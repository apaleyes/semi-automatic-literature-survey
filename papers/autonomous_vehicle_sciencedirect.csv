id,type,publication,publisher,publication_date,database,title,url,abstract,domain
10.1016/j.tins.2021.07.007,journal,Trends in Neurosciences,sciencedirect,2021-10-31,sciencedirect,Learning offline: memory replay in biological and artificial reinforcement learning,https://api.elsevier.com/content/article/pii/S0166223621001442,"
                  Learning to act in an environment to maximise rewards is among the brain’s key functions. This process has often been conceptualised within the framework of reinforcement learning, which has also gained prominence in machine learning and artificial intelligence (AI) as a way to optimise decision making. A common aspect of both biological and machine reinforcement learning is the reactivation of previously experienced episodes, referred to as replay. Replay is important for memory consolidation in biological neural networks and is key to stabilising learning in deep neural networks. Here, we review recent developments concerning the functional roles of replay in the fields of neuroscience and AI. Complementary progress suggests how replay might support learning processes, including generalisation and continual learning, affording opportunities to transfer knowledge across the two fields to advance the understanding of biological and artificial learning and memory.
               ",autonomous vehicle
10.1016/j.imed.2021.10.001,journal,Intelligent Medicine,sciencedirect,2021-11-11,sciencedirect,A comprehensive study on artificial intelligence and machine learning in drug discovery and drug development,https://api.elsevier.com/content/article/pii/S2667102621001066,"The current rise of artificial intelligence and machine learning has been significant. It has reduced the human workload improved quality of life significantly. This article describes the use of artificial intelligence and machine learning to augment drug discovery and development to make them more efficient and accurate. In this study, a systematic evaluation of studies was carried out; these were selected based on prior knowledge of the authors and a keyword search in publicly available databases which were filtered based on related context, abstract, methodology, and full text. This body of work supported the roles of machine learning and artificial intelligence in facilitating drug development and discovery processes, making them more cost-effective or altogether eliminating the need for clinical trials, owing to the ability to conduct simulations using these technologies. They also enabled researchers to study different molecules more extensively, without any trials. The results of this paper demonstrate the prevalent application of machine learning and artificial intelligence methods in drug discovery, and indicate a promising future for these technologies; these results should enable researchers, students, and pharmaceutical industry to dive deeper into machine learning and artificial intelligence in a drug discovery and development context.",autonomous vehicle
10.1016/j.jpowsour.2021.230584,journal,Journal of Power Sources,sciencedirect,2021-12-01,sciencedirect,Self-supervised reinforcement learning-based energy management for a hybrid electric vehicle,https://api.elsevier.com/content/article/pii/S037877532101082X,"
                  Reinforcement learning is a new research hotspot in the energy management strategy. At present, some problems in the application of reinforcement learning to energy management control still exist, including sparse reward, convergence speed, generalization ability, etc. This paper proposes a self-supervised reinforcement learning method based on a Deep Q-learning approach for fuel-saving optimization of a plug-in hybrid electric vehicle (PHEV). First, a detailed vehicle powertrain model of the Prius is built. Second, we use the self-supervised learning model to enrich the reward function. The reward function consists of two parts: internal and external rewards. Finally, to prevent the self-supervised model from falling into the “self-good” situation, a reinforcement learning calibration method is proposed. The vehicle exploration method is more effective because of the enrichment of the reward function. Furthermore, following the characteristics of self-supervised learning, we have also constructed a new driving cycle to verify the generalization ability. Results show that our proposed deep reinforcement learning method based on self-supervised and learning calibration realizes faster training convergence and lower fuel consumption than the traditional policy, and its fuel economy can reach approximately the global optimum under our new driving cycle.
               ",autonomous vehicle
10.1016/j.ptlrs.2021.05.009,journal,Petroleum Research,sciencedirect,2021-06-04,sciencedirect,Application of machine learning and artificial intelligence in oil and gas industry,https://api.elsevier.com/content/article/pii/S2096249521000429,Oil and gas industries are facing several challenges and issues in data processing and handling. Large amount of data bank is generated with various techniques and processes. The proper technical analysis of this database is to be carried out to improve performance of oil and gas industries. This paper provides a comprehensive state-of-art review in the field of machine learning and artificial intelligence to solve oil and gas industry problems. It also narrates the various types of machine learning and artificial intelligence techniques which can be used for data processing and interpretation in different sectors of upstream oil and gas industries. The achievements and developments promise the benefits of machine learning and artificial intelligence techniques towards large data storage capabilities and high efficiency of numerical calculations. In this paper a summary of various researchers work on machine learning and artificial intelligence applications and limitations is showcased for upstream and sectors of oil and gas industry. The existence of this extensive intelligent system could really eliminate the risk factor and cost of maintenance. The development and progress using this emerging technologies have become smart and makes the judgement procedure easy and straightforward. The study is useful to access intelligence of different machine learning methods to declare its application for distinct task in oil and gas sector.,autonomous vehicle
10.1016/S2352-3018(21)00247-2,journal,The Lancet HIV,sciencedirect,2021-11-08,sciencedirect,Application of artificial intelligence and machine learning for HIV prevention interventions,https://api.elsevier.com/content/article/pii/S2352301821002472,"
                  In 2019, the US Government announced its goal to end the HIV epidemic within 10 years, mirroring the initiatives set forth by UNAIDS. Public health prevention interventions are a crucial part of this ambitious goal. However, numerous challenges to this goal exist, including improving HIV awareness, increasing early HIV infection detection, ensuring rapid treatment, optimising resource distribution, and providing efficient prevention services for vulnerable populations. Artificial intelligence has had a pivotal role in revolutionising health care and has shown great potential in developing effective HIV prevention intervention strategies. Although artificial intelligence has been used in a few HIV prevention intervention areas, there are challenges to address and opportunities to explore.
               ",autonomous vehicle
10.1016/j.knosys.2021.107585,journal,Knowledge-Based Systems,sciencedirect,2021-12-25,sciencedirect,RL-DARTS: Differentiable neural architecture search via reinforcement-learning-based meta-optimizer,https://api.elsevier.com/content/article/pii/S0950705121008479,"
                  Differentiable search approaches have attracted extensive attention recently due to their advantages in effectively finding novel neural architectures. However, these methods suffer from shortcomings on heavy computation consumption and low robustness in some cases. In this work, we propose a novel differentiable search method based on reinforcement learning, to further improve the computation efficiency, network precision, and robustness in the neural architecture search area. Our method constructs a reinforcement learning-based meta-optimizer to solve the architecture-parameter optimization problem, which is superior in properties of adaptability and robustness to fixed optimizers. This learnable meta-optimizer can alter its model parameters along with the search process to adapt the optimization procedure, making it possible to find out better structures and parameters with less time. Specifically, we formulate a double-loop algorithm to address the optimization problem in the searched super-network. Through switching between the external and internal loops, our method alternately optimizes the super-network and the meta-optimizer, which converges to the optimal location more rapidly and robustly.
               ",autonomous vehicle
10.1016/j.jmbbm.2021.104728,journal,Journal of the Mechanical Behavior of Biomedical Materials,sciencedirect,2021-11-30,sciencedirect,What can artificial intelligence and machine learning tell us? A review of applications to equine biomechanical research,https://api.elsevier.com/content/article/pii/S1751616121003775,"
                  Artificial intelligence (AI) and machine learning (ML) are fascinating interdisciplinary scientific domains where machines are provided with an approximation of human intelligence. The conjecture is that machines are able to learn from existing examples, and employ this accumulated knowledge to fulfil challenging tasks such as regression analysis, pattern classification, and prediction. The horse biomechanical models have been identified as an alternative tool to investigate the effects of mechanical loading and induced deformations on the tissues and structures in humans. Many reported investigations into bone fatigue, subchondral bone damage in the joints of both humans and animals, and identification of vital parameters responsible for retaining integrity of anatomical regions during normal activities in all species are heavily reliant on equine biomechanical research. Horse racing is a lucrative industry and injury prevention in expensive thoroughbreds has encouraged the implementation of various measurement techniques, which results in massive data generation. ML substantially accelerates analysis and interpretation of data and provides considerable advantages over traditional statistical tools historically adopted in biomechanical research. This paper provides the reader with: a brief introduction to AI, taxonomy and several types of ML algorithms, working principle of a feedforward artificial neural network (ANN), and, a detailed review of the applications of AI, ML, and ANN in equine biomechanical research (i.e. locomotory system function, gait analysis, joint and bone mechanics, and hoof function). Reviewing literature on the use of these data-driven tools is essential since their wider application has the potential to: improve clinical assessments enabling real-time simulations, avoid and/or minimize injuries, and encourage early detection of such injuries in the first place.
               ",autonomous vehicle
10.1016/j.robot.2021.103731,journal,Robotics and Autonomous Systems,sciencedirect,2021-04-30,sciencedirect,Two-stage visual navigation by deep neural networks and multi-goal reinforcement learning,https://api.elsevier.com/content/article/pii/S0921889021000166,"
                  In this paper, we propose a two-stage learning framework for visual navigation in which the experience of the agent during exploration of one goal is shared to learn to navigate to other goals. We train a deep neural network for estimating the robot’s position in the environment using ground truth information provided by a classical localization and mapping approach. The second simpler multi-goal Q-function learns to traverse the environment by using the provided discretized map. Transfer learning is applied to the multi-goal Q-function from a maze structure to a 2D simulator and is finally deployed in a 3D simulator where the robot uses the estimated locations from the position estimator deep network. In the experiments, we first compare different architectures to select the best deep network for location estimation, and then compare the effects of the multi-goal reinforcement learning method to traditional reinforcement learning. The results show a significant improvement when multi-goal reinforcement learning is used. Furthermore, the results of the location estimator show that a deep network can learn and generalize in different environments using camera images with high accuracy in both position and orientation.
               ",autonomous vehicle
10.1016/j.ejmp.2021.04.016,journal,Physica Medica,sciencedirect,2021-03-31,sciencedirect,Artificial intelligence and machine learning for medical imaging: A technology review,https://api.elsevier.com/content/article/pii/S1120179721001733,"
                  Artificial intelligence (AI) has recently become a very popular buzzword, as a consequence of disruptive technical advances and impressive experimental results, notably in the field of image analysis and processing. In medicine, specialties where images are central, like radiology, pathology or oncology, have seized the opportunity and considerable efforts in research and development have been deployed to transfer the potential of AI to clinical applications. With AI becoming a more mainstream tool for typical medical imaging analysis tasks, such as diagnosis, segmentation, or classification, the key for a safe and efficient use of clinical AI applications relies, in part, on informed practitioners. The aim of this review is to present the basic technological pillars of AI, together with the state-of-the-art machine learning methods and their application to medical imaging. In addition, we discuss the new trends and future research directions. This will help the reader to understand how AI methods are now becoming an ubiquitous tool in any medical image analysis workflow and pave the way for the clinical implementation of AI-based solutions.
               ",autonomous vehicle
10.1016/j.desal.2021.115443,journal,Desalination,sciencedirect,2022-01-15,sciencedirect,An efficient deep reinforcement machine learning-based control reverse osmosis system for water desalination,https://api.elsevier.com/content/article/pii/S0011916421005142,"
                  Water scarcity is a permanent problem that faces all over the world. Artificial Intelligence (AI) has many machine learning methods used to solve many problems in all fields. This paper suggests a novel and efficient approach to finding a trans-membrane pressure using Deep Reinforcement Learning (DRL). Our system uses Deep Deterministic Policy Gradient (DDPG) agent to adjust the pressure across the membrane. This adjustment considers the Salt Rejection (SR) to be 99% to investigate the desired water flux. The system takes the maximum height of the water in the tank (h
                     
                        max
                     ), the salt concentration of feed flow (C), the temperature of feed flow (T), the recovery ratio (R), and the salt rejection ratio (SR) as input, and returns the water flux Q
                     
                        p
                     . The results show the effectiveness and the power of the DDPG agent in finding that pressure. The agent is trained in a small number of episodes (150), and the average reward value is high.
               ",autonomous vehicle
10.1016/B978-0-12-819726-4.00078-8,journal,Encyclopedia of Materials: Metals and Alloys,sciencedirect,2022-12-31,sciencedirect,"Data Science, Machine Learning and Artificial Intelligence Applied to Metals and Alloys Research: Past, Present, and Future",https://api.elsevier.com/content/article/pii/B9780128197264000788,"
               In this contribution, we discuss how Data Science, Machine Learning, and Artificial Intelligence have been increasingly used to understand better and design metallic materials systems. We begin by providing some historical perspectives, basic concepts, and significant elements of data-driven materials discovery/design workflows. The contribution concludes by presenting some very recent developments related to closed-loop optimal frameworks for accelerated materials discovery, including the concept of autonomous research platforms.
            ",autonomous vehicle
10.1016/j.knosys.2021.107526,journal,Knowledge-Based Systems,sciencedirect,2021-12-05,sciencedirect,Deep reinforcement learning for transportation network combinatorial optimization: A survey,https://api.elsevier.com/content/article/pii/S0950705121007887,"
                  Traveling salesman and vehicle routing problems with their variants, as classic combinatorial optimization problems, have attracted considerable attention for decades of their theoretical and practical value. Many classic algorithms have been proposed, for example, exact algorithms, heuristic algorithms, solution solvers, etc. Still, due to their complexity, even the most advanced traditional methods require too much computational time or are not well-defined mathematically; algorithm-based decision-making is no exception. Also, these methods cannot be generalized to a larger scale or other similar problems. With the latest developments in machine and deep learning, people believe it is feasible to apply reinforcement learning and other technologies in the decision-making or heuristic for learning combinatorial optimization. In this paper, we first gave an overview on how combinate deep reinforcement learning for the NP-hard combinatorial optimization, emphasizing general optimization problems as data points and exploring the relevant distribution of data used for learning in a given task. We next reviewed state-of-the-art learning techniques related to combinational optimization problems on graphs. Then, we summarized the experimental methods of using reinforcement learning to solve combinatorial optimization problems and analyzed the performance comparison of different algorithms. Lastly, we sorted out the challenges encountered by deep reinforcement learning in solving combinatorial optimization problems and future research directions.
               ",autonomous vehicle
10.1016/j.patcog.2021.107951,journal,Pattern Recognition,sciencedirect,2021-08-31,sciencedirect,Unsupervised meta-learning for few-shot learning,https://api.elsevier.com/content/article/pii/S0031320321001382,"
                  Meta-learning is an effective tool to address the few-shot learning problem, which requires new data to be classified considering only a few training examples. However, when used for classification, it requires large labeled datasets, which are not always available in practice. In this paper, we propose an unsupervised meta-learning algorithm that learns from an unlabeled dataset and adapts to downstream human-specific tasks with few labeled data. The proposed algorithm constructs tasks using clustering embedding methods and data augmentation functions to satisfy two critical class distinction requirements. To alleviate the biases and the weak diversity problem introduced by data augmentation functions, the proposed algorithm uses two methods, which are shifting the feeding data between the inner-outer loops and a novel data augmentation function. We further provide theoretical analysis of the effect of augmentation data in the inner/outer loop. Experiments on the MiniImagenet and Omniglot datasets demonstrate that the proposed unsupervised meta-learning approach outperforms other tested unsupervised representation learning approaches and two recent unsupervised meta-learning baselines. Compared with supervised meta-learning approaches, certain results produced by our method are quite close to those produced by such methods trained on the human-designed labeled tasks.
               ",autonomous vehicle
10.1016/j.jbef.2021.100577,journal,Journal of Behavioral and Experimental Finance,sciencedirect,2021-12-31,sciencedirect,"Artificial intelligence and machine learning in finance: Identifying foundations, themes, and research clusters from bibliometric analysis",https://api.elsevier.com/content/article/pii/S2214635021001210,"
                  Artificial intelligence (AI) and machine learning (ML) are two related technologies that are emergent in financial scholarship. However, no review, to date, has offered a wholistic retrospection of this research. To address this gap, we provide an overview of AI and ML research in finance. Using both co-citation and bibliometric-coupling analyses, we infer the thematic structure of AI and ML research in finance for 1986–April 2021. By uncovering nine (co-citation) and eight (bibliometric coupling) specific clusters of finance that apply AI and ML, we further identify three overarching groups of finance scholarship that are roughly equivalent for both forms of analysis: (1) portfolio construction, valuation, and investor behavior; (2) financial fraud and distress; and (3) sentiment inference, forecasting, and planning. Additionally, using co-occurrence and confluence analyses, we highlight trends and research directions regarding AI and ML in finance research. Our results provide assessment of  AI and ML in finance research.
               ",autonomous vehicle
10.1016/j.ifacol.2021.10.454,journal,IFAC-PapersOnLine,sciencedirect,2021-12-31,sciencedirect,Flight Control of a Multicopter using Reinforcement Learning,https://api.elsevier.com/content/article/pii/S2405896321018917,"
                  Machine Learning, and in particular Reinforcement Learning, is a persistent trend in automation and robotics in recent years. Many researchers worldwide are developing intelligent controllers using Reinforcement Learning techniques. This paper aims to present a proof-of-concept Reinforcement Learning flight controller for a multicopter. The agent has been trained in the Airsim simulation environment to achieve stable flight conditions by controlling its roll, pitch, yaw and throttle. After training, the agent has been tested on the same environment to prove its ability to maintain stable flight conditions while following a determined route.
               ",autonomous vehicle
10.1016/j.neuron.2020.06.014,journal,Neuron,sciencedirect,2020-08-19,sciencedirect,Deep Reinforcement Learning and Its Neuroscientific Implications,https://api.elsevier.com/content/article/pii/S0896627320304682,"The emergence of powerful artificial intelligence (AI) is defining new research directions in neuroscience. To date, this research has focused largely on deep neural networks trained using supervised learning in tasks such as image classification. However, there is another area of recent AI work that has so far received less attention from neuroscientists but that may have profound neuroscientific implications: deep reinforcement learning (RL). Deep RL offers a comprehensive framework for studying the interplay among learning, representation, and decision making, offering to the brain sciences a new set of research tools and a wide range of novel hypotheses. In the present review, we provide a high-level introduction to deep RL, discuss some of its initial applications to neuroscience, and survey its wider implications for research on brain and behavior, concluding with a list of opportunities for next-stage research.",autonomous vehicle
10.1016/j.conbuildmat.2021.124278,journal,Construction and Building Materials,sciencedirect,2021-09-13,sciencedirect,Asphalt pavement maintenance plans intelligent decision model based on reinforcement learning algorithm,https://api.elsevier.com/content/article/pii/S0950061821020377,"
                  Making a proper maintenance plan for the pavement health is the key to maintain a good service level and bearing capacity. To cope with the increasing demand for pavement maintenance, based on the idea of reinforcement learning, an intelligent decision-making model for pavement maintenance plans based on proximal policy optimization algorithm was proposed in this paper. The decision model fully considers the comprehensive maintenance benefit-cost ratio during the whole life cycle of the road. To overcome the problems of the experience-led manual decision-making, it conducted the decision-making between pavement conditions and maintenance plans based on data mining technique. Besides, a method for constructing a reinforcement learning 
                        Environment
                      module based on a deep artificial neural network was proposed, and a reward function is designed for road maintenance decisions. The model was applied to the highway maintenance decision in Jiangsu Province, and verified that the decision overall accuracy of the reinforcement learning model was 82.2%, which was an increase of 17.2% compared with the artificial neural network model.
               ",autonomous vehicle
10.1016/j.procir.2021.10.010,journal,Procedia CIRP,sciencedirect,2021-12-31,sciencedirect,Trends In Machine Learning To Solve Problems In Logistics,https://api.elsevier.com/content/article/pii/S2212827121008519,"With the increase in the number of avenues for production of data everywhere including logistics, using this data has become an obvious next step. In this paper, we look at the major logistics problems in details as discussed in a seminal work in highlighting the problems in logistics by X. Li in 2014. Together with discussing the problems, we also look at the trends in solving these problems using machine learning. We look at the recent work done in the particular fields using machine learning (ML) and develop the correlation of the trends of using supervised, unsupervised and reinforcement learning in solving these problems. This correlation is developed using a table and a diagram showing ML techniques and the trends in the fields of the problems are discussed. COVID-19 has greatly accelerated these trends. This paper serves as a gentle introduction to ML techniques in the field of logistics for researchers who are new to the field.",autonomous vehicle
10.1016/j.ijme.2021.100550,journal,The International Journal of Management Education,sciencedirect,2021-11-30,sciencedirect,Artificial intelligence in business curriculum: The pedagogy and learning outcomes,https://api.elsevier.com/content/article/pii/S1472811721000999,"
                  Artificial intelligence is rapidly reshaping the technological landscape, becoming an important component of work processes and decision-making in many domains. However, AI education has yet to catch up with the challenge of introducing this complex and very important area of technology to audiences beyond the students of computing and engineering disciplines. This is not surprising, given the lack of materials for and experience in teaching AI for non-technical audiences. In this paper we present the challenges in developing an AI course in business schools and propose a curriculum for a graduate-level business course in AI, which balances AI fundamentals with the latest developments in the field. We discuss our choices and present the results of a study, examining students’ experiences, learning outcomes, and perceptions regarding the course. We offer practical guidelines for the curriculum design that other educators may adopt in designing their AI training courses.
               ",autonomous vehicle
10.1016/j.cobeha.2021.02.019,journal,Current Opinion in Behavioral Sciences,sciencedirect,2021-04-30,sciencedirect,Reinforcement-guided learning in frontal neocortex: emerging computational concepts,https://api.elsevier.com/content/article/pii/S2352154621000413,"
                  The classical concepts of reinforcement learning in the mammalian brain focus on dopamine release in the basal ganglia as the neural substrate of reward prediction errors, which drive plasticity in striatal and cortico-striatal synapses to maximize the expected aggregate future reward. This temporal difference framework, however, even when augmented with deep credit assignment, does not fully capture higher-order processes such as the influence of goal representations, planning based on learned internal models, and hierarchical decision-making implemented by diverse neocortical areas. Candidate functions for such neocortical contributions to reinforcement learning are increasingly being considered in artificial intelligence algorithms. Here, we review recent experimental neurophysiological findings focusing on the orbitofrontal cortex, a key higher-order association cortex, and highlight emerging concepts that emphasize the role of the neocortex in reward-driven computation, in addition to its role as an input to striatal structures. In this framework, reward drives plasticity in various neocortical regions, implementing multiple distinct reinforcement learning algorithms.
               ",autonomous vehicle
10.1016/j.neucom.2020.02.004,journal,Neurocomputing,sciencedirect,2020-06-14,sciencedirect,Correlation minimizing replay memory in temporal-difference reinforcement learning,https://api.elsevier.com/content/article/pii/S092523122030179X,"
                  Online reinforcement learning agents are now able to process an increasing amount of data which makes their approximation and compression into value functions a more demanding task. To improve approximation, thus the learning process itself, it has been proposed to select randomly a mini-batch of the past experiences that are stored in the replay memory buffer to be replayed at each learning step. In this work, we present an algorithm that classifies and samples the experiences into separate contextual memory buffers using an unsupervised learning technique. This allows each new experience to be associated to a mini-batch of the past experiences that are not from the same contextual buffer as the current one, thus further reducing the correlation between experiences. Experimental results show that the correlation minimizing sampling improves over Q-learning algorithms with uniform sampling, and that a significant improvement can be observed when coupled with the sampling methods that prioritize on the experience temporal difference error.
               ",autonomous vehicle
10.1016/j.ifacol.2020.12.2317,journal,IFAC-PapersOnLine,sciencedirect,2020-12-31,sciencedirect,Cascade Attribute Network: Decomposing Reinforcement Learning Control Policies using Hierarchical Neural Networks,https://api.elsevier.com/content/article/pii/S2405896320329840,"
                  Reinforcement learning methods have been developed to achieve great success in training control policies in various automation tasks. However, a main challenge of the wider application of reinforcement learning in practical automation is that the training process is hard and the pretrained policy networks are hardly reusable in other similar cases. To address this problem, we propose the cascade attribute network (CAN), which utilizes its hierarchical structure to decompose a complicated control policy in terms of the requirement constraints, which we call attributes, encoded in the control tasks. We validated the effectiveness of our proposed method on two robot control scenarios with various add-on attributes. For some control tasks with more than one add-on attribute attribute, by directly assembling the attribute modules in cascade, the CAN can provide ideal control policies in a zero-shot manner.
               ",autonomous vehicle
10.1016/j.knosys.2021.107439,journal,Knowledge-Based Systems,sciencedirect,2021-11-14,sciencedirect,Local2Global: Unsupervised multi-view deep graph representation learning with Nearest Neighbor Constraint,https://api.elsevier.com/content/article/pii/S0950705121007012,"
                  Multi-view feature fusion is a vital phase for multi-view representation learning. Recently, most Graph Auto-Encoders (GAEs) and their variants focus on multi-view learning. However, most of them ignore deep representation fusion of features of each multi-view. Furthermore, there are scarcely unsupervised constraints guiding to enhance the graph representation capability in training process. In this paper, we propose a novel unsupervised Multi-view Deep Graph Representation Learning (MDGRL) framework on multi-view data which is based on the Graph Auto-Encoders (GAEs) for local feature leaning, a feature fusion module for producing global representation and a valid variant of Variational Graph Auto-Encoder (VGAE) for global deep graph representation learning. To fuse Nearest Neighbor Constraint (NNC) between the maximal degree nodes which represents the most close joining node and their adjacent nodes into VGAE, we propose a new Nearest Neighbor Constraint Variational Graph Auto-Encoder (NNC-VGAE) to enhance the global deep graph representation capability for multi-view data. In the training process of NNC-VGAE, NNC makes the adjacent nodes gradually close to the maximal degree node. Hence, the proposed MDGRL has excellent deep graph representation capability for multi-view data. Experiments on eight non-medical benchmark multi-view data sets and four medical data sets confirm the effectiveness of our MDGRL compared with other state-of-the-art methods for unsupervised clustering.
               ",autonomous vehicle
10.1016/j.neucom.2021.03.123,journal,Neurocomputing,sciencedirect,2021-10-07,sciencedirect,Self-supervised graph representation learning via bootstrapping,https://api.elsevier.com/content/article/pii/S0925231221005154,"
                  Graph neural networks (GNNs) apply deep learning techniques to graph-structured data and have achieved promising performance in graph representation learning. However, existing GNNs rely heavily on labeled data or well-designed negative samples. To address these issues, we propose a new self-supervised graph representation method: deep graph bootstrapping (DGB). DGB consists of two neural networks: online and target networks, and the input of them are different augmented views of the initial graph. The online network is trained to predict the target network while the target network is updated with a slow-moving average of the online network, which means the online and target networks can learn from each other. As a result, the proposed DGB can learn graph representation without negative examples in an unsupervised manner. In addition, we summarize three kinds of augmentation methods for graph-structured data and apply them to the DGB. Experiments on the benchmark datasets show the DGB performs better than the current state-of-the-art methods and how the augmentation methods affect the performances.
               ",autonomous vehicle
10.1016/j.knosys.2020.106685,journal,Knowledge-Based Systems,sciencedirect,2021-02-28,sciencedirect,Explainability in deep reinforcement learning,https://api.elsevier.com/content/article/pii/S0950705120308145,"
                  A large set of the explainable Artificial Intelligence (XAI) literature is emerging on feature relevance techniques to explain a deep neural network (DNN) output or explaining models that ingest image source data. However, assessing how XAI techniques can help understand models beyond classification tasks, e.g. for reinforcement learning (RL), has not been extensively studied. We review recent works in the direction to attain Explainable Reinforcement Learning (XRL), a relatively new subfield of Explainable Artificial Intelligence, intended to be used in general public applications, with diverse audiences, requiring ethical, responsible and trustable algorithms. In critical situations where it is essential to justify and explain the agent’s behaviour, better explainability and interpretability of RL models could help gain scientific insight on the inner workings of what is still considered a black box. We evaluate mainly studies directly linking explainability to RL, and split these into two categories according to the way the explanations are generated: transparent algorithms and post-hoc explainability. We also review the most prominent XAI works from the lenses of how they could potentially enlighten the further deployment of the latest advances in RL, in the demanding present and future of everyday problems.
               ",autonomous vehicle
10.1016/j.engappai.2021.104451,journal,Engineering Applications of Artificial Intelligence,sciencedirect,2021-11-30,sciencedirect,Quantum deep reinforcement learning for rotor side converter control of double-fed induction generator-based wind turbines,https://api.elsevier.com/content/article/pii/S0952197621002992,"
                  The control performance of conventional analytic algorithms for double-fed induction generator-based wind turbines is a fixed feature, which needs to be optimized by optimization processes. To avoid the optimization processes and update control strategies online, this paper proposes an online control algorithm based on the quantum process, deep belief networks, and reinforcement learning for double-fed induction generator-based wind turbines. The proposed approach is named quantum deep reinforcement learning, consisting of deep belief networks, one reinforcement learning framework, and multiple subsidiary reinforcement learning parts with quantum processes. The quantum deep reinforcement learning can update the control strategy online with general initialization for dynamic systems, avoid optimal local solutions, and predict the next systemic states of double-fed induction generator-based wind turbines. The proportional–integral–derivative, fractional-order proportional–integral–derivative, active disturbance rejection controller, reinforcement learning, and the quantum deep reinforcement learning are compared under four cases, i.e., variable wind speed with step magnitude, variable wind speed with sine magnitude, voltage dropout of power grids, and both variable wind speed with random magnitude and voltage dropout of power grids. Consequently, the proposed quantum deep reinforcement learning can obtain better control performance for double-fed induction generator-based wind turbines with smaller integrated absolute error, integral squared error, and integrated time-weighted absolute error of the control error than other compared methods.
               ",autonomous vehicle
10.1016/j.cobeha.2021.02.006,journal,Current Opinion in Behavioral Sciences,sciencedirect,2021-04-30,sciencedirect,Hebbian learning revisited and its inference underlying cognitive function,https://api.elsevier.com/content/article/pii/S2352154621000280,"
                  Despite the recent success of deep learning in artificial intelligence, the lack of biological plausibility and labeled data in natural learning poses a challenge in understanding biological learning. At the other extreme lies Hebbian learning, the simplest local and unsupervised one, yet considered to be computationally less efficient. The recent development of Hebbian learning re-evaluates its contribution to natural learning and memory association. It highlights the efficiency of Hebbian learning combined with supervised learning in forming a low-dimensional and coarse representation, and its role in many cognitive tasks by providing a basis activity patterns and dynamics. Deriving the form of Hebbian learning from in-vivo data bridges the neural and behavioral data and requires utilizing both spatial and temporal activity changes.
               ",autonomous vehicle
10.1016/j.pecs.2021.100967,journal,Progress in Energy and Combustion Science,sciencedirect,2022-01-31,sciencedirect,"Modeling, diagnostics, optimization, and control of internal combustion engines via modern machine learning techniques: A review and future directions",https://api.elsevier.com/content/article/pii/S0360128521000654,"
                  A critical review of the existing Internal Combustion Engine (ICE) modeling, optimization, diagnosis, and control challenges and the promising state-of-the-art Machine Learning (ML) solutions for them is provided in this paper. Some of the major challenges include Real Driving Emission (RDE) modeling and control, combustion knock detection and control, combustion mode transition in multi-mode engines, combustion noise modeling and control, combustion instability and cyclic variability control, costly and time-consuming engine calibration, and fault diagnostics of some ICE components. In this paper, conventional ICE modeling approaches are discussed along with their limitations for realtime ICE optimization and control. Promising ML approaches to address ICE challenges are then classified into three main groups of unsupervised learning, supervised learning, and reinforcement learning. The working principles of each approach along with their advantages and disadvantages in addressing ICE challenges are discussed. ML-based grey-box approach is proposed as a solution that combines the benefits from physics-based and ML-based models to provide robust and high fidelity solutions for ICE modeling and control challenges. This review provides in-depth insight into the applications of ML for ICEs and provides recommendations for future directions to address ICE challenges.
               ",autonomous vehicle
10.1016/B978-0-12-820352-1.00108-5,journal,Reference Module in Materials Science and Materials Engineering,sciencedirect,2021-12-31,sciencedirect,"Artificial Intelligence and Machine Learning: New Age Tools for Augmenting Plastic Materials Designing, Processing, and Manufacturing",https://api.elsevier.com/content/article/pii/B9780128203521001085,"
               Plastic and polymers are late entrants in the repository of engineering materials, compared to bronze and iron (early phases of human civilizations are named after them). However, the extent of usage of the former is growing at an exponential rate because of the near-infinite combinatorial possibilities. In fact, there are hardly few engineering disciplines that can potentially offer so large and endless unexploited possibilities. Plastic industries are widespread across the world due to their easy scalability, favorable economics and extremely diverse applications. Plastic manufacturing and recycling are especially important for the economy of a country and provide livelihood for a large population. It is imperative that the current processes in plastic be improved upon by the advantages offered by computational tools and digital technologies. At this stage, we desperately need new age tools that can properly guide the human enterprise of innovation in designing, perfection in processing while maintaining stringent quality requirements in manufacturing. Artificial intelligence (AI) and machine learning (ML) perfectly fits this bill for the new age tools. We are at a very nascent stage of this AI/ML revolution. This article samples some of the pioneering works from the very discreet space of AI/ML applications in the field of plastic and polymer designing, processing and manufacturing and attempts to tie them up in a cohesive narrative. For the sake of completeness, applications of AI/ML for limiting the adverse environmental impact and future outlook have also been covered.
            ",autonomous vehicle
10.1016/j.procir.2020.04.109,journal,Procedia CIRP,sciencedirect,2020-12-31,sciencedirect,Systematic review on machine learning (ML) methods for manufacturing processes – Identifying artificial intelligence (AI) methods for field application,https://api.elsevier.com/content/article/pii/S2212827120307435,"Artificial Intelligence (AI) and especially machine learning (ML) become increasingly more frequently applicable in factory operations. This paper presents a systematic review of today’s applications of ML techniques in the factory environment. The utilization of ML methods related to manufacturing process planning and control, predictive maintenance, quality control, in situ process control and optimization, logistics, robotics, assistance and learning systems for shopfloor employees are being analyzed. Moreover, an overview of ML training concepts in learning factories is given. Furthermore, these concepts will be analyzed regarding the implemented ML method. Finally, research gaps are identified.",autonomous vehicle
10.1016/j.engappai.2021.104504,journal,Engineering Applications of Artificial Intelligence,sciencedirect,2021-11-30,sciencedirect,Machine learning applications in power system fault diagnosis: Research advancements and perspectives,https://api.elsevier.com/content/article/pii/S0952197621003523,"
                  Newer generation sources and loads are posing new challenges to the conventional power system protection schemes. Adaptive and intelligent protection methodology, based on advanced measurement techniques and intelligent fault diagnosis such as machine learning (ML), is found to be useful to meet these challenges. A large number of research works are reported on ML-based power system fault diagnosis. However, ML techniques are evolving at a very fast pace, and an inclusive, as well as state-of-the-art review on ML-based power system fault diagnosis, is not available in the literature. Given this need and growing trend towards ML, the study presented in this paper aims to provide a comprehensive review of ML-based power system fault diagnosis. At first, efforts have been made to enlist the issues present in conventional fault diagnosis which led to the popularity of ML techniques. Also, a baseline framework and workflow for ML-based fault diagnosis are presented. Next, various unsupervised and supervised learning techniques have been discussed separately which have been used by several researchers for fault diagnosis. The discussion throughout is supported with tabulated facts for fault detection, classification and localization works with techniques used, different simulation tools used, and their application system. The advantages and disadvantages of all the techniques of fault diagnosis have also been discussed which will help the readers in the selection of techniques for their research. A brief review of reinforcement learning and transfer learning is also given as they are gaining popularity in power system-related studies and have the potential to be used for fault diagnosis. Finally, the research trends, some key issues, and directions for future research have been highlighted.
               ",autonomous vehicle
10.1016/j.cma.2020.113452,journal,Computer Methods in Applied Mechanics and Engineering,sciencedirect,2021-01-01,sciencedirect,Hierarchical Deep Learning Neural Network (HiDeNN): An artificial intelligence (AI) framework for computational science and engineering,https://api.elsevier.com/content/article/pii/S004578252030637X,"
                  In this work, a unified AI-framework named Hierarchical Deep Learning Neural Network (HiDeNN) is proposed to solve challenging computational science and engineering problems with little or no available physics as well as with extreme computational demand. The detailed construction and mathematical elements of HiDeNN are introduced and discussed to show the flexibility of the framework for diverse problems from disparate fields. Three example problems are solved to demonstrate the accuracy, efficiency, and versatility of the framework. The first example is designed to show that HiDeNN is capable of achieving better accuracy than conventional finite element method by learning the optimal nodal positions and capturing the stress concentration with a coarse mesh. The second example applies HiDeNN for multiscale analysis with sub-neural networks at each material point of macroscale. The final example demonstrates how HiDeNN can discover governing dimensionless parameters from experimental data so that a reduced set of input can be used to increase the learning efficiency. We further present a discussion and demonstration of the solution for advanced engineering problems that require state-of-the-art AI approaches and how a general and flexible system, such as HiDeNN-AI framework, can be applied to solve these problems.
               ",autonomous vehicle
10.1016/j.media.2021.102193,journal,Medical Image Analysis,sciencedirect,2021-10-31,sciencedirect,Deep reinforcement learning in medical imaging: A literature review,https://api.elsevier.com/content/article/pii/S1361841521002395,"
                  Deep reinforcement learning (DRL) augments the reinforcement learning framework, which learns a sequence of actions that maximizes the expected reward, with the representative power of deep neural networks. Recent works have demonstrated the great potential of DRL in medicine and healthcare. This paper presents a literature review of DRL in medical imaging. We start with a comprehensive tutorial of DRL, including the latest model-free and model-based algorithms. We then cover existing DRL applications for medical imaging, which are roughly divided into three main categories: (i) parametric medical image analysis tasks including landmark detection, object/lesion detection, registration, and view plane localization; (ii) solving optimization tasks including hyperparameter tuning, selecting augmentation strategies, and neural architecture search; and (iii) miscellaneous applications including surgical gesture segmentation, personalized mobile health intervention, and computational model personalization. The paper concludes with discussions of future perspectives.
               ",autonomous vehicle
10.1016/j.jocn.2021.04.043,journal,Journal of Clinical Neuroscience,sciencedirect,2021-07-31,sciencedirect,Machine learning applications to neuroimaging for glioma detection and classification: An artificial intelligence augmented systematic review,https://api.elsevier.com/content/article/pii/S0967586821002241,"
                  Glioma is the most common primary intraparenchymal tumor of the brain and the 5-year survival rate of high-grade glioma is poor. Magnetic resonance imaging (MRI) is essential for detecting, characterizing and monitoring brain tumors but definitive diagnosis still relies on surgical pathology. Machine learning has been applied to the analysis of MRI data in glioma research and has the potential to change clinical practice and improve patient outcomes. This systematic review synthesizes and analyzes the current state of machine learning applications to glioma MRI data and explores the use of machine learning for systematic review automation. Various datapoints were extracted from the 153 studies that met inclusion criteria and analyzed. Natural language processing (NLP) analysis involved keyword extraction, topic modeling and document classification. Machine learning has been applied to tumor grading and diagnosis, tumor segmentation, non-invasive genomic biomarker identification, detection of progression and patient survival prediction. Model performance was generally strong (AUC = 0.87 ± 0.09; sensitivity = 0.87 ± 0.10; specificity = 0.0.86 ± 0.10; precision = 0.88 ± 0.11). Convolutional neural network, support vector machine and random forest algorithms were top performers. Deep learning document classifiers yielded acceptable performance (mean 5-fold cross-validation AUC = 0.71). Machine learning tools and data resources were synthesized and summarized to facilitate future research. Machine learning has been widely applied to the processing of MRI data in glioma research and has demonstrated substantial utility. NLP and transfer learning resources enabled the successful development of a replicable method for automating the systematic review article screening process, which has potential for shortening the time from discovery to clinical application in medicine.
               ",autonomous vehicle
10.1016/j.rser.2020.109899,journal,Renewable and Sustainable Energy Reviews,sciencedirect,2020-09-30,sciencedirect,Artificial intelligence and machine learning approaches to energy demand-side response: A systematic review,https://api.elsevier.com/content/article/pii/S136403212030191X,"Recent years have seen an increasing interest in Demand Response (DR) as a means to provide flexibility, and hence improve the reliability of energy systems in a cost-effective way. Yet, the high complexity of the tasks associated with DR, combined with their use of large-scale data and the frequent need for near real-time decisions, means that Artificial Intelligence (AI) and Machine Learning (ML) — a branch of AI — have recently emerged as key technologies for enabling demand-side response. AI methods can be used to tackle various challenges, ranging from selecting the optimal set of consumers to respond, learning their attributes and preferences, dynamic pricing, scheduling and control of devices, learning how to incentivise participants in the DR schemes and how to reward them in a fair and economically efficient way. This work provides an overview of AI methods utilised for DR applications, based on a systematic review of over 160 papers, 40 companies and commercial initiatives, and 21 large-scale projects. The papers are classified with regards to both the AI/ML algorithm(s) used and the application area in energy DR. Next, commercial initiatives are presented (including both start-ups and established companies) and large-scale innovation projects, where AI methods have been used for energy DR. The paper concludes with a discussion of advantages and potential limitations of reviewed AI techniques for different DR tasks, and outlines directions for future research in this fast-growing area.",autonomous vehicle
10.1016/j.ins.2021.10.025,journal,Information Sciences,sciencedirect,2021-12-31,sciencedirect,Adaptive operator selection with reinforcement learning,https://api.elsevier.com/content/article/pii/S0020025521010331,"
                  Operator selection plays a crucial role in the efficiency of heuristic-based problem solving algorithms, especially, when a pool of operators is used to let algorithms dynamically select operators to produce new candidate solutions. A sequence of selected operators forms up throughout the search which impacts the success of the algorithms. Successive operators in a bespoke sequence can be complementary and therefore diversify the search while randomly selected operators are not expected to behave in this way. State of art adaptive selection schemes have been proposed to select the best next operator without considering the problem state in the process. In this study, a reinforcement learning algorithm is proposed to embed in a standard artificial bee colony algorithm for taking the problem state on board in operator selection process. The proposed approach implies mapping the problem states to the best fitting operators in the pool so as to achieve higher diversity and shape up an optimum operator sequence throughout the search process. The experimental study successfully demonstrates that the proposed idea works towards higher efficiency. The state of art approaches are outperformed with respect to the quality of solution in solving Set Union Knapsack problem over 30 benchmarking instances.
               ",autonomous vehicle
10.1016/j.anclin.2021.03.012,journal,Anesthesiology Clinics,sciencedirect,2021-09-30,sciencedirect,"Machine Learning, Deep Learning, and Closed Loop Devices—Anesthesia Delivery",https://api.elsevier.com/content/article/pii/S1932227521000318,,autonomous vehicle
10.1016/j.tins.2021.04.005,journal,Trends in Neurosciences,sciencedirect,2021-09-30,sciencedirect,Deep learning and the Global Workspace Theory,https://api.elsevier.com/content/article/pii/S0166223621000771,"
                  Recent advances in deep learning have allowed artificial intelligence (AI) to reach near human-level performance in many sensory, perceptual, linguistic, and cognitive tasks. There is a growing need, however, for novel, brain-inspired cognitive architectures. The Global Workspace Theory (GWT) refers to a large-scale system integrating and distributing information among networks of specialized modules to create higher-level forms of cognition and awareness. We argue that the time is ripe to consider explicit implementations of this theory using deep-learning techniques. We propose a roadmap based on unsupervised neural translation between multiple latent spaces (neural networks trained for distinct tasks, on distinct sensory inputs and/or modalities) to create a unique, amodal Global Latent Workspace (GLW). Potential functional advantages of GLW are reviewed, along with neuroscientific implications.
               ",autonomous vehicle
10.1016/j.neucom.2020.03.059,journal,Neurocomputing,sciencedirect,2020-11-27,sciencedirect,"Advances in artificial neural networks, machine learning and computational intelligence",https://api.elsevier.com/content/article/pii/S092523122030432X,,autonomous vehicle
10.1016/j.vehcom.2021.100398,journal,Vehicular Communications,sciencedirect,2021-08-25,sciencedirect,Deep reinforcement learning techniques for vehicular networks: Recent advances and future trends towards 6G,https://api.elsevier.com/content/article/pii/S221420962100067X,"
                  Employing machine learning into 6G vehicular networks to support vehicular application services is being widely studied and a hot topic for the latest research works in the literature. This article provides a comprehensive review of research works that integrated reinforcement and deep reinforcement learning algorithms for vehicular networks management with an emphasis on vehicular telecommunications issues. Vehicular networks have become an important research area due to their specific features and applications such as standardization, efficient traffic management, road safety, and infotainment. In such networks, network entities need to make decisions to maximize network performance under uncertainty. To achieve this goal, Reinforcement Learning (RL) can effectively solve decision-making problems. However, the state and action spaces are massive and complex in large-scale wireless networks. Hence, RL may not be able to find the best strategy in a reasonable time. Therefore, Deep Reinforcement Learning (DRL) has been developed to combine RL with Deep Learning (DL) to overcome this issue. In this survey, we first present vehicular networks and give a brief overview of RL and DRL concepts. Then we review RL and especially DRL approaches to address emerging issues in 6G vehicular networks. We finally discuss and highlight some unresolved challenges for further study.
               ",autonomous vehicle
10.1016/j.psep.2020.09.038,journal,Process Safety and Environmental Protection,sciencedirect,2021-03-31,sciencedirect,Review and analysis of supervised machine learning algorithms for hazardous events in drilling operations,https://api.elsevier.com/content/article/pii/S0957582020317559,"
                  Results of bibliometric analysis and a detailed review are reported on the use of supervised machine learning to study hazardous drilling events. The bibliometric analysis attempts to answer pertinent questions related to progress in the use of supervised machine learning for hazardous events due to drilling fluid density/mud weight. The analysis indicates artificial neural network as the most popular algorithm among researchers. Also, deep learning, random forest and support vector machine have gained momentum in recent use.
                  A critical review of literature on hazardous events and supervised machine learning algorithms are reported. This review was done to observe how the algorithms were used, their relative successes, limitations, as well as input parameters which aided in detection or estimation by the machine learning algorithms. An introduction to deep learning and a review of literature on the use of deep learning with respect to operations involving drilling parameters is presented. The review on deep learning and drilling parameters covered the following operations: lithology identification, drilling rig state determination, generating logging/other drilling parameters and detecting abnormality in data.
                  The study highlights need of publicly accessible large database with data from different oilfields for development of machine learning algorithms. These algorithms could be used globally for the enhancement of machine learning for new fields or fields with limited data. The availability of such large database would aid researchers in improving or customizing deep learning algorithms in line with the unique needs of drilling activities.
               ",autonomous vehicle
10.1016/j.addr.2021.113922,journal,Advanced Drug Delivery Reviews,sciencedirect,2021-11-30,sciencedirect,Artificial intelligence and machine learning assisted drug delivery for effective treatment of infectious diseases,https://api.elsevier.com/content/article/pii/S0169409X2100315X,"
                  In the era of antimicrobial resistance, the prevalence of multidrug-resistant microorganisms that resist conventional antibiotic treatment has steadily increased. Thus, it is now unquestionable that infectious diseases are significant global burdens that urgently require innovative treatment strategies. Emerging studies have demonstrated that artificial intelligence (AI) can transform drug delivery to promote effective treatment of infectious diseases. In this review, we propose to evaluate the significance, essential principles, and popular tools of AI in drug delivery for infectious disease treatment. Specifically, we will focus on the achievements and key findings of current research, as well as the applications of AI on drug delivery throughout the whole antimicrobial treatment process, with an emphasis on drug development, treatment regimen optimization, drug delivery system and administration route design, and drug delivery outcome prediction. To that end, the challenges of AI in drug delivery for infectious disease treatments and their current solutions and future perspective will be presented and discussed.
               ",autonomous vehicle
10.1016/j.cam.2020.112991,journal,Journal of Computational and Applied Mathematics,sciencedirect,2020-12-15,sciencedirect,Analysis on block chain financial transaction under artificial neural network of deep learning,https://api.elsevier.com/content/article/pii/S037704272030282X,"
                  In order to conduct an in-depth study on financial transactions of block chain, the classical back propagation (BP) neural network based on the artificial neural network (ANN) model is selected, and its propagation mode, weight change, and learning process are analyzed. For the problem of slow convergence speed and local minimum value of BP neural network, based on the idea of deep learning, the initial value and training step are changed by auto-encoder and restricted Boltzmann machine, and the theory is analyzed. Taking the stock index futures trading in the block chain financial trading as an example, the stock price trading of stock index futures is studied using the two deep learning neural network models to predict the price changes. The results show that the auto-encoder, as an unsupervised learning system, performs better than the restricted Boltzmann machine in setting the initial weights and thresholds, with fewer iterations, faster convergence rate, and smaller convergence error. The results obtained by the auto-encoder can be used as initialization settings and data analysis. The prediction accuracy of the whole model is around 59%. When the transaction cost is not considered, the transaction can be conducted based on the prediction signal of the deep learning model. Therefore, deep learning neural network model can be applied to block chain financial transactions as a reference for financial transactions, which has a good practical significance for the development of this field.
               ",autonomous vehicle
10.1016/j.cosrev.2020.100317,journal,Computer Science Review,sciencedirect,2021-02-28,sciencedirect,Deep Learning Algorithms for Cybersecurity Applications: A Technological and Status Review,https://api.elsevier.com/content/article/pii/S1574013720304172,"
                  Cybersecurity mainly prevents the hardware, software, and data present in the system that has an active internet connection from external attacks. Organizations mainly deploy cybersecurity for their databases and systems to prevent it from unauthorized access. Different forms of attacks like phishing, spear-phishing, a drive-by attack, a password attack, denial of service, etc. are responsible for these security problems In this survey, we analyzed and reviewed the usage of deep learning algorithms for Cybersecurity applications. Deep learning which is also known as Deep Neural Networks includes machine learning techniques that enable the network to learn from unsupervised data and solve complex problems. Here, 80 papers from 2014 to 2019 have been used and successfully analyzed. Deep learning approaches such as Convolutional Neural Network (CNN), Auto Encoder (AE), Deep Belief Network (DBN), Recurrent Neural Network (RNN), Generative Adversal Network (GAN) and Deep Reinforcement Learning (DIL) are used to categorize the papers referred. Each specific technique is effectively discussed with its algorithms, platforms, dataset, and potential benefits. The paper related to deep learning with cybersecurity is mainly published in the year 2018 in a large number and 18% of published articles originate from the UK. In addition, the papers are selected from a variety of journals, and 30% of papers used are from the Elsevier journal. From the experimental analysis, it is clear that the deep learning model improved the accuracy, scalability, reliability, and performance of the cybersecurity applications when applied in realtime.
               ",autonomous vehicle
10.1016/j.engappai.2021.104362,journal,Engineering Applications of Artificial Intelligence,sciencedirect,2021-09-30,sciencedirect,Spiking neural network-based multi-task autonomous learning for mobile robots,https://api.elsevier.com/content/article/pii/S0952197621002104,"
                  Spiking Neural Networks (SNNs) are the new generation of artificial neural networks that closely mimic the time encoding and information processing aspects of the human brain. In this work, a multi-task autonomous learning paradigm is proposed for the mobile robot application, which employs a SNN to construct the controlling system of the mobile robot. The Reward-modulated Spiking-time-dependent Plasticity learning rule is developed for the SNN-based controller, which aims to achieve the capability of autonomous learning under multiple tasks. Reward signals are generated based on the instantaneous frequencies of pre- and post-synaptic spikes, which adapts to the sensory stimuli and environmental feedback. Meanwhile, inspired by lateral inhibition connections, a task switch mechanism is designed to enable the controller to switch the operations between multiple tasks. Two tasks of obstacle avoidance and target tracking are used for performance evaluation and results demonstrate that the mobile robot with the proposed paradigm is able to autonomously learn, switch and complete the tasks.
               ",autonomous vehicle
10.1016/j.neucom.2019.08.007,journal,Neurocomputing,sciencedirect,2019-11-13,sciencedirect,Ensemble-based deep reinforcement learning for chatbots,https://api.elsevier.com/content/article/pii/S0925231219311269,"
                  Trainable chatbots that exhibit fluent and human-like conversations remain a big challenge in artificial intelligence. Deep Reinforcement Learning (DRL) is promising for addressing this challenge, but its successful application remains an open question. This article describes a novel ensemble-based approach applied to value-based DRL chatbots, which use finite action sets as a form of meaning representation. In our approach, while dialogue actions are derived from sentence clustering, the training datasets in our ensemble are derived from dialogue clustering. The latter aim to induce specialised agents that learn to interact in a particular style. In order to facilitate neural chatbot training using our proposed approach, we assume dialogue data in raw text only – without any manually-labelled data. Experimental results using chitchat data reveal that (1) near human-like dialogue policies can be induced, (2) generalisation to unseen data is a difficult problem, and (3) training an ensemble of chatbot agents is essential for improved performance over using a single agent. In addition to evaluations using held-out data, our results are further supported by a human evaluation that rated dialogues in terms of fluency, engagingness and consistency – which revealed that our proposed dialogue rewards strongly correlate with human judgements.
               ",autonomous vehicle
10.1016/j.comcom.2020.02.069,journal,Computer Communications,sciencedirect,2020-03-15,sciencedirect,Applications of Artificial Intelligence and Machine learning in smart cities,https://api.elsevier.com/content/article/pii/S0140366419320821,"
                  Smart cities are aimed to efficiently manage growing urbanization, energy consumption, maintain a green environment, improve the economic and living standards of their citizens, and raise the people’s capabilities to efficiently use and adopt the modern information and communication technology (ICT). In the smart cities concept, ICT is playing a vital role in policy design, decision, implementation, and ultimate productive services. The primary objective of this review is to explore the role of artificial intelligence (AI), machine learning (ML), and deep reinforcement learning (DRL) in the evolution of smart cities. The preceding techniques are efficiently used to design optimal policy regarding various smart city-oriented complex problems. In this survey, we present in-depth details of the applications of the prior techniques in intelligent transportation systems (ITSs), cyber-security, energy-efficient utilization of smart grids (SGs), effective use of unmanned aerial vehicles (UAVs) to assure the best services of 5G and beyond 5G (B5G) communications, and smart health care system in a smart city. Finally, we present various research challenges and future research directions where the aforementioned techniques can play an outstanding role to realize the concept of a smart city.
               ",autonomous vehicle
10.1016/j.measurement.2021.109616,journal,Measurement,sciencedirect,2021-09-30,sciencedirect,Reinforcement Learning for Statistical Process Control in Manufacturing,https://api.elsevier.com/content/article/pii/S0263224121005881,"The main concept of the authors is to place Reinforcement Learning (RL) into various fields of manufacturing. As one of the first implementations, RL for Statistical Process Control (SPC) in production is introduced in the paper; it is a promising approach owing to its adaptability and the continuous ability to perform. The widely used Q-Table method was applied for get more stable, predictable, and easy to overview results. Therefore, quantization of the values of the time series to stripes inside the control chart was introduced. Detailed elements of the production environment simulation are described and its interaction with the reinforcement learning agent are detailed. Beyond the working concept for adapting RL into SPC in manufacturing, some novel RL extensions are also described, like the epsilon self-control of exploration–exploitation ratio, Reusing Window (RW) and the Measurement Window (MW). In the production related transformation, the main aim of the agent is to optimize the production cost while keeping the ratio of good products on a high level as well. Finally, industrial testing and validation is described that proved the applicability of the proposed concept.",autonomous vehicle
10.1016/j.jksuci.2020.06.013,journal,Journal of King Saud University - Computer and Information Sciences,sciencedirect,2020-07-04,sciencedirect,Machine learning and artificial intelligence based Diabetes Mellitus detection and self-management: A systematic review,https://api.elsevier.com/content/article/pii/S1319157820304134,"Diabetes Mellitus (DM) is a condition induced by unregulated diabetes that may lead to multi-organ failure in patients. Thanks to advances in machine learning and artificial intelligence, which enables theearly detection and diagnosis of DM through an automated process which is more advantageous than a manual diagnosis. Currently, many articles are published on automatic DM detection, diagnosis, and self-management via machine learning and artificial intelligence techniques. This review delivers an analysis of the detection, diagnosis, and self-management techniques of DM from six different facets viz., datasets of DM, pre-processing methods, feature extraction methods, machine learning-based identification, classification, and diagnosis of DM, artificial intelligence-based intelligent DM assistant and performance measures. It also discusses the conclusions of the previous study and the importance of the results of the study. Also, three current research issues in the field of DM detection and diagnosis and self-management and personalization are listed. After a thorough screening procedure, 107 main publications from the Scopus and PubMed repositories are chosen for this study. This review provides a detailed overview of DM detection and self-management techniques which may prove valuable to the community of scientists employed in the area of automatic DM detection and self-management.",autonomous vehicle
10.1016/j.artmed.2020.101964,journal,Artificial Intelligence in Medicine,sciencedirect,2020-09-30,sciencedirect,Reinforcement learning for intelligent healthcare applications: A survey,https://api.elsevier.com/content/article/pii/S093336572031229X,"
                  Discovering new treatments and personalizing existing ones is one of the major goals of modern clinical research. In the last decade, Artificial Intelligence (AI) has enabled the realization of advanced intelligent systems able to learn about clinical treatments and discover new medical knowledge from the huge amount of data collected. Reinforcement Learning (RL), which is a branch of Machine Learning (ML), has received significant attention in the medical community since it has the potentiality to support the development of personalized treatments in accordance with the more general precision medicine vision. This report presents a review of the role of RL in healthcare by investigating past work, and highlighting any limitations and possible future contributions.
               ",autonomous vehicle
10.1016/j.cherd.2021.08.013,journal,Chemical Engineering Research and Design,sciencedirect,2021-10-31,sciencedirect,"Machine learning on sustainable energy: A review and outlook on renewable energy systems, catalysis, smart grid and energy storage",https://api.elsevier.com/content/article/pii/S0263876221003312,"
                  This study presents a broad view of the current state of the art of ML applications in the manufacturing sectors that have a considerable impact on sustainability and the environment, namely renewable energies (solar, wind, hydropower, and biomass), smart grids, the industry of catalysis and power storage and distribution. Artificial neural networks are the most preferred techniques over other ML algorithms because of their generalization capabilities. Demands for ML techniques in the energy sectors will increase considerably in the coming years, since there is a growing demand of academic programmes related to artificial intelligence in science, math, and engineering. Data generation, management, and safety are expected to play a key role for the successful implementation of ML algorithms that can be shared by major stakeholders in the energy sector, thereby promoting the development of ambitious energy management projects. New algorithms for producing reliable data and the addition of other sources of information (e.g., novel sensors) will enhance flow of information between ML and systems. It is expected that unsupervised and reinforcement learning will take a central role in the energy sector, but this will depend on the expansion of other major fields in data science such as big data analytics. Massive implementations, specialized algorithms, and new technologies like 5G will promote the development of sustainable applications of ML in non-industrial applications for energy management.
               ",autonomous vehicle
10.1016/j.enbuild.2021.111085,journal,Energy and Buildings,sciencedirect,2021-09-01,sciencedirect,Sustainability matchmaking: Linking renewable sources to electric water heating through machine learning,https://api.elsevier.com/content/article/pii/S0378778821003698,"
                  A high penetration of renewable energy sources such as wind power generation and photovoltaic generation causes some problems in power systems such as the duck curve and unreliability due to environmental variability. An effective solution to this problem is Demand Response (DR). Electric Water Heaters (EWHs) are considered ideal candidates for DR due to their energy storage capability. Due to the benefits, control strategies or techniques for EWHs have received considerable academic attention. The energy sector has recently tapped into the disruptive artificial intelligence world to learn, among other related priorities, how to enhance operations, maintain energy resilience and improve consumer service. Consequently, this paper reviews the use of machine learning (ML) for optimization and scheduling of EWHs. The main contributions of this review paper are, firstly, to identify state of the art of energy optimization and scheduling of EWHs. Secondly, to review the current ML models for energy optimization and scheduling of EWHs in smart grids and smart building environment. While classical control strategies may deliver substantial improvements, optimum efficiency may not be reached. ML has demonstrated clear advantages over classical control. Based on these conclusions, recommendations for further research topics are drawn.
               ",autonomous vehicle
10.1016/j.gltp.2021.01.004,journal,Global Transitions Proceedings,sciencedirect,2021-06-30,sciencedirect,Machine Learning and Deep Learning Applications-A Vision,https://api.elsevier.com/content/article/pii/S2666285X21000042,"The application of artificial intelligence is machine learning which is one of the current topics in the computer field as well as for the new COVID-19 pandemic. Researchers have given a lot of input to enhance the precision of machine learning algorithms and lot of work is carried out rapidly to enhance the intelligence of machines. Learning, a natural process in human behaviour that also becomes a vital part of machines as well. Besides this, another concept of deep learning comes to play its major role. Deep neural network (deep learning) is a subgroup of machine learning. Deep learning had been analysed and implemented in various applications and had shown remarkable results thus this field needs wider exploration which can be helpful for further real-world applications. The main objective of this paper is to provide insight survey for machine learning along with deep learning applications in various domains. Also, some applications with new normal COVID-19 blues. A review on already present applications and currently going on applications in several domains, for machine learning along with deep neural learning are exemplified.",autonomous vehicle
10.1016/j.retram.2020.01.002,journal,Current Research in Translational Medicine,sciencedirect,2020-11-30,sciencedirect,Machine learning and artificial intelligence in the service of medicine: Necessity or potentiality?,https://api.elsevier.com/content/article/pii/S2452318620300192,"
                  Motivation
                  As a result of the worldwide health care system digitalization trend, the produced healthcare data is estimated to reach as much as 2314 Exabytes of new data generated in 2020.
                  The ongoing development of intelligent systems aims to provide better reasoning and to more efficiently use the data collected. This use is not restricted retrospective interpretation, that is, to provide diagnostic conclusions. It can also be extended to prospective interpretation providing early prognosis. That said, physicians who could be assisted by these systems find themselves standing in the gap between clinical case and deep technical reviews. What they lack is a clear starting point from which to approach the world of machine learning in medicine.
               
                  Methodology and Main Structure
                  This article aims at providing interested physicians with an easy-to-follow insight of Artificial Intelligence (AI) and Machine Learning (ML) use in the medical field, primarily over the last few years.
                  To this end, we first discuss the general developmental paths concerning AI and ML concept usage in healthcare systems. We then list fields where these technologies are already being put to the test or even applied such as in Hematology, Neurology, Cardiology, Oncology, Radiology, Ophthalmology, Cell Biology and Cell Therapy.
               ",autonomous vehicle
10.1016/j.scs.2021.103445,journal,Sustainable Cities and Society,sciencedirect,2022-01-31,sciencedirect,An overview of machine learning applications for smart buildings,https://api.elsevier.com/content/article/pii/S2210670721007186,"The efficiency, flexibility, and resilience of building-integrated energy systems are challenged by unpredicted changes in operational environments due to climate change and its consequences. On the other hand, the rapid evolution of artificial intelligence (AI) and machine learning (ML) has equipped buildings with an ability to learn. A lot of research has been dedicated to specific machine learning applications for specific phases of a building's life-cycle. The reviews commonly take a specific, technological perspective without a vision for the integration of smart technologies at the level of the whole system. Especially, there is a lack of discussion on the roles of autonomous AI agents and training environments for boosting the learning process in complex and abruptly changing operational environments. This review article discusses the learning ability of buildings with a system-level perspective and presents an overview of autonomous machine learning applications that make independent decisions for building energy management. We conclude that the buildings’ adaptability to unpredicted changes can be enhanced at the system level through AI-initiated learning processes and by using digital twins as training environments. The greatest potential for energy efficiency improvement is achieved by integrating adaptability solutions at the timescales of HVAC control and electricity market participation.",autonomous vehicle
10.1016/j.neunet.2021.07.023,journal,Neural Networks,sciencedirect,2021-10-31,sciencedirect,Unsupervised learning of disentangled representations in deep restricted kernel machines with orthogonality constraints,https://api.elsevier.com/content/article/pii/S0893608021002860,"We introduce Constr-DRKM, a deep kernel method for the unsupervised learning of disentangled data representations. We propose augmenting the original deep restricted kernel machine formulation for kernel PCA by orthogonality constraints on the latent variables to promote disentanglement and to make it possible to carry out optimization without first defining a stabilized objective. After discussing a number of algorithms for end-to-end training, we quantitatively evaluate the proposed method’s effectiveness in disentangled feature learning. We demonstrate on four benchmark datasets that this approach performs similarly overall to β -VAE on several disentanglement metrics when few training points are available while being less sensitive to randomness and hyperparameter selection than β -VAE. We also present a deterministic initialization of Constr-DRKM’s training algorithm that significantly improves the reproducibility of the results. Finally, we empirically evaluate and discuss the role of the number of layers in the proposed methodology, examining the influence of each principal component in every layer and showing that components in lower layers act as local feature detectors capturing the broad trends of the data distribution, while components in deeper layers use the representation learned by previous layers and more accurately reproduce higher-level features.",autonomous vehicle
10.1016/j.bios.2021.113666,journal,Biosensors and Bioelectronics,sciencedirect,2021-12-15,sciencedirect,Exploiting machine learning for bestowing intelligence to microfluidics,https://api.elsevier.com/content/article/pii/S095656632100703X,"
                  Intelligent microfluidics is an emerging cross-discipline research area formed by combining microfluidics with machine learning. It uses the advantages of microfluidics, such as high throughput and controllability, and the powerful data processing capabilities of machine learning, resulting in improved systems in biotechnology and chemistry. Compared to traditional microfluidics using manual analysis methods, intelligent microfluidics needs less human intervention, and results in a more user-friendly experience with faster processing. There is a paucity of literature reviewing this burgeoning and highly promising cross-discipline. Therefore, we herein comprehensively and systematically summarize several aspects of microfluidic applications enabled by machine learning. We list the types of microfluidics used in intelligent microfluidic applications over the last five years, as well as the machine learning algorithms and the hardware used for training. We also present the most recent advances in key technologies, developments, challenges, and the emerging opportunities created by intelligent microfluidics.
               ",autonomous vehicle
10.1016/j.neucom.2021.02.058,journal,Neurocomputing,sciencedirect,2021-07-25,sciencedirect,Zero-shot policy generation in lifelong reinforcement learning,https://api.elsevier.com/content/article/pii/S0925231221003143,"
                  Lifelong reinforcement learning (LRL) is an important approach to achieve continual lifelong learning of multiple reinforcement learning tasks. The two major methods used in LRL are task decomposition and policy knowledge extraction. Policy knowledge extraction method in LRL can share knowledge for tasks in different task domains and for tasks in the same task domain with different system environmental coefficients. However, the generalization ability of policy knowledge extraction method is limited on learned tasks rather than learned task domains. In this paper, we propose a cross-domain lifelong reinforcement learning algorithm with zero-shot policy generation ability (CDLRL-ZPG) to improve generalization ability of policy knowledge extraction method from learned tasks to learned task domains. In experiments, we evaluated CDLRL-ZPG performance on four task domains. And our results show that the proposed algorithm can directly generate satisfactory results without needing a trial and error learning process to achieve zero-shot learning in general.
               ",autonomous vehicle
10.1016/j.scs.2021.103207,journal,Sustainable Cities and Society,sciencedirect,2022-01-31,sciencedirect,A novel forecasting based scheduling method for household energy management system based on deep reinforcement learning,https://api.elsevier.com/content/article/pii/S2210670721004856,"
                  The demand response (DR) strategy enables household users to actively optimize and dispatch the household energy management system (HEMS), which may significantly reduce the cost of user energy consumptions. However, the uncertainty of home users behaviors, the diversity of electrical equipment types, as well as the complexity of the working status of various devices have brought severe challenges to the home energy management system. This article proposes a new forecasting based optimization method to deliver the real-time scheduling considering the future environment trend. A recent proposed dueling based deep reinforcement learning approach is adopted to optimally dispatch the HEMS. In addition, due to the delay of heating, ventilation, and air Conditioning (HVAC) indoor and outdoor temperature data non-Gaussian, a new generalized corr-entropy assisted long short-term memory (GC-LSTM) neural network is proposed where the generalized correntropy (GC) loss function is adopted to predict the outdoor temperature. The proposed method is verified in a featured HEMS benchmark problem and the experimental results show that the user costs are effectively reduced while the user satisfaction is maintained utilizing the proposed method.
               ",autonomous vehicle
10.1016/j.neunet.2019.08.020,journal,Neural Networks,sciencedirect,2019-12-31,sciencedirect,Self-organizing neural networks for universal learning and multimodal memory encoding,https://api.elsevier.com/content/article/pii/S0893608019302370,"
                  Learning and memory are two intertwined cognitive functions of the human brain. This paper shows how a family of biologically-inspired self-organizing neural networks, known as fusion Adaptive Resonance Theory (fusion ART), may provide a viable approach to realizing the learning and memory functions. Fusion ART extends the single-channel Adaptive Resonance Theory (ART) model to learn multimodal pattern associative mappings. As a natural extension of ART, various forms of fusion ART have been developed for a myriad of learning paradigms, ranging from unsupervised learning to supervised learning, semi-supervised learning, multimodal learning, reinforcement learning, and sequence learning. In addition, fusion ART models may be used for representing various types of memories, notably episodic memory, semantic memory and procedural memory. In accordance with the notion of embodied intelligence, such neural models thus provide a computational account of how an autonomous agent may learn and adapt in a real-world environment. The efficacy of fusion ART in learning and memory shall be discussed through various examples and illustrative case studies.
               ",autonomous vehicle
10.1016/j.aei.2019.100977,journal,Advanced Engineering Informatics,sciencedirect,2019-10-31,sciencedirect,Intelligent fault diagnosis for rotating machinery using deep Q-network based health state classification: A deep reinforcement learning approach,https://api.elsevier.com/content/article/pii/S1474034619305506,"
                  Fault diagnosis methods for rotating machinery have always been a hot research topic, and artificial intelligence-based approaches have attracted increasing attention from both researchers and engineers. Among those related studies and methods, artificial neural networks, especially deep learning-based methods, are widely used to extract fault features or classify fault features obtained by other signal processing techniques. Although such methods could solve the fault diagnosis problems of rotating machinery, there are still two deficiencies. (1) Unable to establish direct linear or non-linear mapping between raw data and the corresponding fault modes, the performance of such fault diagnosis methods highly depends on the quality of the extracted features. (2) The optimization of neural network architecture and parameters, especially for deep neural networks, requires considerable manual modification and expert experience, which limits the applicability and generalization of such methods. As a remarkable breakthrough in artificial intelligence, AlphaGo, a representative achievement of deep reinforcement learning, provides inspiration and direction for the aforementioned shortcomings. Combining the advantages of deep learning and reinforcement learning, deep reinforcement learning is able to build an end-to-end fault diagnosis architecture that can directly map raw fault data to the corresponding fault modes. Thus, based on deep reinforcement learning, a novel intelligent diagnosis method is proposed that is able to overcome the shortcomings of the aforementioned diagnosis methods. Validation tests of the proposed method are carried out using datasets of two types of rotating machinery, rolling bearings and hydraulic pumps, which contain a large number of measured raw vibration signals under different health states and working conditions. The diagnosis results show that the proposed method is able to obtain intelligent fault diagnosis agents that can mine the relationships between the raw vibration signals and fault modes autonomously and effectively. Considering that the learning process of the proposed method depends only on the replayed memories of the agent and the overall rewards, which represent much weaker feedback than that obtained by the supervised learning-based method, the proposed method is promising in establishing a general fault diagnosis architecture for rotating machinery.
               ",autonomous vehicle
10.1016/j.matpr.2020.11.026,journal,Materials Today: Proceedings,sciencedirect,2021-12-31,sciencedirect,Role of machine learning in the field of Fiber reinforced polymer composites: A preliminary discussion,https://api.elsevier.com/content/article/pii/S2214785320386041,"
                  Artificial Intelligence has become the backbone of almost every domain of science and engineering. Machine learning, the branch of AI adopts probabilistic and statistical methods to learn from the past experience based upon the experimental output data set and detect the possible solution. In this paper, an overview of various machine learning algorithm used so far for the prediction of various problems such as optimization of process parameters, ranking of materials, validation is discussed. The process of design and optimization of the fibre reinforcement in polymer composites with distinguished properties has been redefined by the machine learning approach. This paper also highlights the role of machine learning algorithm, solution techniques and their data bases used in the different stages starting from the selection of raw materials to the end user application for the fiber reinforced polymer composites. This paper also supports readers to understand the future course of action to implement for the development of new product generation in an industry. At the end, a comparison has been made to understand the functionality of machine learning with respective to other technical tools used in the real-world problem.
               ",autonomous vehicle
10.1016/j.energy.2021.122128,journal,Energy,sciencedirect,2022-01-15,sciencedirect,Short-term wind speed forecasting using deep reinforcement learning with improved multiple error correction approach,https://api.elsevier.com/content/article/pii/S0360544221023768,"
                  The safe and stable operation of wind power systems requires the support of wind speed prediction. To ensure the controllability and stability of smart grid dispatching, a novel hybrid model consisting of data-adaptive decomposition, reinforcement learning ensemble, and improved error correction is established for short-term wind speed forecasting. In decomposition module, empirical wavelet transform algorithm is used to adaptively disassemble and reconstruct the wind speed series. In ensemble module, Q-learning is utilized to integrate gated recurrent unit, bidirectional long short-term memory, and deep belief network. In error correction module, wavelet packet decomposition and outlier-robust extreme learning machine are combined to developing predictable components. An appropriate correction shrinkage rate is used to obtain the best correction effect. Ljung-Box Q-Test is utilized to judge the termination of the error correction iteration. Four real data are utilized to validate model performance in the case study. Experimental results show that: (a) The proposed hybrid model can accurately capture the changes of wind data. Taking 1-step prediction results as an example, the mean absolute errors for site #1, #2, #3, and #4 are 0.0829 m/s, 0.0661 m/s, 0.0906 m/s, and 0.0803 m/s, respectively; (b) Compared with several state-of-the-art models, the proposed model has the best prediction performance.
               ",autonomous vehicle
10.1016/j.tics.2020.09.004,journal,Trends in Cognitive Sciences,sciencedirect,2020-12-31,sciencedirect,Embracing Change: Continual Learning in Deep Neural Networks,https://api.elsevier.com/content/article/pii/S1364661320302199,"Artificial intelligence research has seen enormous progress over the past few decades, but it predominantly relies on fixed datasets and stationary environments. Continual learning is an increasingly relevant area of study that asks how artificial systems might learn sequentially, as biological systems do, from a continuous stream of correlated data. In the present review, we relate continual learning to the learning dynamics of neural networks, highlighting the potential it has to considerably improve data efficiency. We further consider the many new biologically inspired approaches that have emerged in recent years, focusing on those that utilize regularization, modularity, memory, and meta-learning, and highlight some of the most promising and impactful directions.",autonomous vehicle
10.1016/j.eng.2019.12.014,journal,Engineering,sciencedirect,2020-03-31,sciencedirect,"Progress in Neural NLP: Modeling, Learning, and Reasoning",https://api.elsevier.com/content/article/pii/S2095809919304928,"Natural language processing (NLP) is a subfield of artificial intelligence that focuses on enabling computers to understand and process human languages. In the last five years, we have witnessed the rapid development of NLP in tasks such as machine translation, question-answering, and machine reading comprehension based on deep learning and an enormous volume of annotated and unannotated data. In this paper, we will review the latest progress in the neural network-based NLP framework (neural NLP) from three perspectives: modeling, learning, and reasoning. In the modeling section, we will describe several fundamental neural network-based modeling paradigms, such as word embedding, sentence embedding, and sequence-to-sequence modeling, which are widely used in modern NLP engines. In the learning section, we will introduce widely used learning methods for NLP models, including supervised, semi-supervised, and unsupervised learning; multitask learning; transfer learning; and active learning. We view reasoning as a new and exciting direction for neural NLP, but it has yet to be well addressed. In the reasoning section, we will review reasoning mechanisms, including the knowledge, existing non-neural inference methods, and new neural inference methods. We emphasize the importance of reasoning in this paper because it is important for building interpretable and knowledge-driven neural NLP models to handle complex tasks. At the end of this paper, we will briefly outline our thoughts on the future directions of neural NLP.",autonomous vehicle
10.1016/j.media.2020.101872,journal,Medical Image Analysis,sciencedirect,2021-01-31,sciencedirect,Unifying neural learning and symbolic reasoning for spinal medical report generation,https://api.elsevier.com/content/article/pii/S136184152030236X,"
                  Automated medical report generation in spine radiology, i.e., given spinal medical images and directly create radiologist-level diagnosis reports to support clinical decision making, is a novel yet fundamental study in the domain of artificial intelligence in healthcare. However, it is incredibly challenging because it is an extremely complicated task that involves visual perception and high-level reasoning processes. In this paper, we propose the neural-symbolic learning (NSL) framework that performs human-like learning by unifying deep neural learning and symbolic logical reasoning for the spinal medical report generation. Generally speaking, the NSL framework firstly employs deep neural learning to imitate human visual perception for detecting abnormalities of target spinal structures. Concretely, we design an adversarial graph network that interpolates a symbolic graph reasoning module into a generative adversarial network through embedding prior domain knowledge, achieving semantic segmentation of spinal structures with high complexity and variability. NSL secondly conducts human-like symbolic logical reasoning that realizes unsupervised causal effect analysis of detected entities of abnormalities through meta-interpretive learning. NSL finally fills these discoveries of target diseases into a unified template, successfully achieving a comprehensive medical report generation. When employed in a real-world clinical dataset, a series of empirical studies demonstrate its capacity on spinal medical report generation and show that our algorithm remarkably exceeds existing methods in the detection of spinal structures. These indicate its potential as a clinical tool that contributes to computer-aided diagnosis.
               ",autonomous vehicle
10.1016/j.cosrev.2021.100379,journal,Computer Science Review,sciencedirect,2021-05-31,sciencedirect,A survey on deep learning and its applications,https://api.elsevier.com/content/article/pii/S1574013721000198,"
                  Deep learning, a branch of machine learning, is a frontier for artificial intelligence, aiming to be closer to its primary goal—artificial intelligence. This paper mainly adopts the summary and the induction methods of deep learning. Firstly, it introduces the global development and the current situation of deep learning. Secondly, it describes the structural principle, the characteristics, and some kinds of classic models of deep learning, such as stacked auto encoder, deep belief network, deep Boltzmann machine, and convolutional neural network. Thirdly, it presents the latest developments and applications of deep learning in many fields such as speech processing, computer vision, natural language processing, and medical applications. Finally, it puts forward the problems and the future research directions of deep learning.
               ",autonomous vehicle
10.1016/j.ijsu.2021.106133,journal,International Journal of Surgery,sciencedirect,2021-10-31,sciencedirect,Perspectives: A surgeon's guide to machine learning,https://api.elsevier.com/content/article/pii/S1743919121002685,"
                  The exponential increase in the volume and complexity of healthcare data presents new challenges to researchers and clinicians in analysis and interpretation. The requirement for new strategies to extract meaningful information from large, noisy datasets has led to the development of the field of big data analytics. Artificial intelligence (AI) is a general-purpose technology in which machines carry out tasks traditionally thought to be only achievable by humans. Machine learning (ML) is an approach to AI in which machines can “learn” to perform tasks in an automated process, rather than being explicitly programmed by a human. Research aiming to apply ML techniques to classification, prediction and decision-making problems in healthcare has increased 61-fold from 2005 to 2019, mirroring this sense of early promise. The field of healthcare ML is relatively young, and many critical steps are needed before adoption into clinical practice, including transparent, unbiased development and reporting of algorithms. Articles claiming that machines can outperform, or replace, doctors in high-level tasks, such as diagnosis or prognostication, must be carefully appraised. It is critical that surgeons have an understanding of the principles and terminology of AI and ML to evaluate these claims and to take an active role in directing research. This article is an up-to-date review and primer for surgeons covering the core tenets of ML applied to surgical problems, including algorithm types and selection, model training and validation, interpretation of common outcome metrics, current and future reporting guidelines and discussion of the challenges and limitations in this field.
               ",autonomous vehicle
10.1016/j.knosys.2021.106967,journal,Knowledge-Based Systems,sciencedirect,2021-06-07,sciencedirect,RL-VAEGAN: Adversarial defense for reinforcement learning agents via style transfer,https://api.elsevier.com/content/article/pii/S0950705121002306,"
                  Reinforcement learning (RL) agents parameterized by deep neural networks have achieved great success in many domains. However, deep RL policies have been shown to be vulnerable to adversarial attacks, i.e., inputs with slight perturbations should result in a substantial agent failure. Inspired by recent advances in deep generative networks that have greatly facilitated the development of adversarial attacks, in this paper, we investigate the adversarial robustness of RL agents and propose a novel defense framework for RL based on the idea of style transfer. More precisely, our defense framework containing variational autoencoders (VAEs) and generative adversarial networks (GANs), called RL-VAEGAN, learns the distribution of the styles of the original and adversarial states, respectively, and naturally eliminates the threat of adversarial attacks for RL agents by transferring adversarial states to unperturbed legitimate one under the shared-content latent space assumption. We empirically show that our methods are effective against the state-of-the-art methods in white-box and black-box scenarios with diverse magnitudes of perturbations.
               ",autonomous vehicle
10.1016/j.neunet.2019.09.007,journal,Neural Networks,sciencedirect,2020-01-31,sciencedirect,A biologically plausible supervised learning method for spiking neural networks using the symmetric STDP rule,https://api.elsevier.com/content/article/pii/S0893608019302680,"
                  Spiking neural networks (SNNs) possess energy-efficient potential due to event-based computation. However, supervised training of SNNs remains a challenge as spike activities are non-differentiable. Previous SNNs training methods can be generally categorized into two basic classes, i.e., backpropagation-like training methods and plasticity-based learning methods. The former methods are dependent on energy-inefficient real-valued computation and non-local transmission, as also required in artificial neural networks (ANNs), whereas the latter are either considered to be biologically implausible or exhibit poor performance. Hence, biologically plausible (bio-plausible) high-performance supervised learning (SL) methods for SNNs remain deficient. In this paper, we proposed a novel bio-plausible SNN model for SL based on the symmetric spike-timing dependent plasticity (sym-STDP) rule found in neuroscience. By combining the sym-STDP rule with bio-plausible synaptic scaling and intrinsic plasticity of the dynamic threshold, our SNN model implemented SL well and achieved good performance in the benchmark recognition task (MNIST dataset). To reveal the underlying mechanism of our SL model, we visualized both layer-based activities and synaptic weights using the t-distributed stochastic neighbor embedding (t-SNE) method after training and found that they were well clustered, thereby demonstrating excellent classification ability. Furthermore, to verify the robustness of our model, we trained it on another more realistic dataset (Fashion-MNIST), which also showed good performance. As the learning rules were bio-plausible and based purely on local spike events, our model could be easily applied to neuromorphic hardware for online training and may be helpful for understanding SL information processing at the synaptic level in biological neural systems.
               ",autonomous vehicle
10.1016/j.compind.2021.103471,journal,Computers in Industry,sciencedirect,2021-09-30,sciencedirect,Planning for automatic product assembly using reinforcement learning,https://api.elsevier.com/content/article/pii/S0166361521000786,"
                  Assembly connects functional modules and components of products. The efficient and accurate assembly can improve performance of the product operation and maintenance. It is therefore essential to have an effective method for product assembly. Existing methods of the mechanical product assembly use mainly manual processes that rely on experience of operators. This paper proposes a reinforcement learning method to enable an automatic operation for improved efficiency and accuracy of the mechanical product assembly. A representation of the product assembly is proposed to build a machine learning model. The automatic assembly of product operations is planned by reinforcement learning agents. Constraints of assembly operations are considered to develop searching strategies of the maximum reward for the optimal solution of assembly operations. A quantitative method is proposed to measure efficiency of assembly operations based on the operation time. The proposed method has been applied in the assembly improvement of function modules of an industrial machine.
               ",autonomous vehicle
10.1016/j.cag.2021.01.011,journal,Computers & Graphics,sciencedirect,2021-04-30,sciencedirect,A robot arm digital twin utilising reinforcement learning,https://api.elsevier.com/content/article/pii/S009784932100011X,"
                  For many industry contexts, the implementation of Artificial Intelligence (AI) has contributed to what has become known as the fourth industrial revolution or “Industry 4.0” and creates an opportunity to deliver significant benefit to both businesses and their stakeholders. Robot arms are one of the most common devices utilised in manufacturing and industrial processes, used for a wide variety of automation tasks on, for example, a factory floor but the effective use of these devices requires AI to be appropriately trained. One approach to support AI training of these devices is the use of a “Digital Twin”. There are, however, a number of challenges that exist within this domain, in particular, success depends upon the ability to collect data of what are considered as observations within the environment and the application of one or many trained AI policies to the task that is to be completed. This project presents a case-study of creating and training a Robot Arm Digital Twin as an approach for AI training in a virtual space and applying this simulation learning within physical space. A virtual space, created using Unity (a contemporary Game Engine), incorporating a virtual robot arm was linked to a physical space, being a 3D printed replica of the virtual space and robot arm. These linked environments were applied to solve a task and provide training for an AI model. The contribution of this work is to provide guidance on training protocols for a digital twin together with details of the necessary architecture to support effective simulation in a virtual space through the use of Tensorflow and hyperparameter tuning. It provides an approach to addressing the mapping of learning in the virtual domain to the physical robot twin.
               ",autonomous vehicle
10.1016/j.inffus.2021.09.022,journal,Information Fusion,sciencedirect,2022-03-31,sciencedirect,EXplainable Neural-Symbolic Learning (<ce:italic>X-NeSyL</ce:italic>) methodology to fuse deep learning representations with expert knowledge graphs: The MonuMAI cultural heritage use case,https://api.elsevier.com/content/article/pii/S1566253521001986,"The latest Deep Learning (DL) models for detection and classification have achieved an unprecedented performance over classical machine learning algorithms. However, DL models are black-box methods hard to debug, interpret, and certify. DL alone cannot provide explanations that can be validated by a non technical audience such as end-users or domain experts. In contrast, symbolic AI systems that convert concepts into rules or symbols – such as knowledge graphs – are easier to explain. However, they present lower generalization and scaling capabilities. A very important challenge is to fuse DL representations with expert knowledge. One way to address this challenge, as well as the performance-explainability trade-off is by leveraging the best of both streams without obviating domain expert knowledge. In this paper, we tackle such problem by considering the symbolic knowledge is expressed in form of a domain expert knowledge graph. We present the eXplainable Neural-symbolic learning (X-NeSyL) methodology, designed to learn both symbolic and deep representations, together with an explainability metric to assess the level of alignment of machine and human expert explanations. The ultimate objective is to fuse DL representations with expert domain knowledge during the learning process so it serves as a sound basis for explainability. In particular, X-NeSyL methodology involves the concrete use of two notions of explanation, both at inference and training time respectively: (1) EXPLANet: Expert-aligned eXplainable Part-based cLAssifier NETwork Architecture, a compositional convolutional neural network that makes use of symbolic representations, and (2) SHAP-Backprop, an explainable AI-informed training procedure that corrects and guides the DL process to align with such symbolic representations in form of knowledge graphs. We showcase X-NeSyL methodology using MonuMAI dataset for monument facade image classification, and demonstrate that with our approach, it is possible to improve explainability at the same time as performance.",autonomous vehicle
10.1016/j.neunet.2020.02.011,journal,Neural Networks,sciencedirect,2020-05-31,sciencedirect,Supervised learning in spiking neural networks: A review of algorithms and evaluations,https://api.elsevier.com/content/article/pii/S0893608020300563,"
                  As a new brain-inspired computational model of the artificial neural network, a spiking neural network encodes and processes neural information through precisely timed spike trains. Spiking neural networks are composed of biologically plausible spiking neurons, which have become suitable tools for processing complex temporal or spatiotemporal information. However, because of their intricately discontinuous and implicit nonlinear mechanisms, the formulation of efficient supervised learning algorithms for spiking neural networks is difficult, and has become an important problem in this research field. This article presents a comprehensive review of supervised learning algorithms for spiking neural networks and evaluates them qualitatively and quantitatively. First, a comparison between spiking neural networks and traditional artificial neural networks is provided. The general framework and some related theories of supervised learning for spiking neural networks are then introduced. Furthermore, the state-of-the-art supervised learning algorithms in recent years are reviewed from the perspectives of applicability to spiking neural network architecture and the inherent mechanisms of supervised learning algorithms. A performance comparison of spike train learning of some representative algorithms is also made. In addition, we provide five qualitative performance evaluation criteria for supervised learning algorithms for spiking neural networks and further present a new taxonomy for supervised learning algorithms depending on these five performance evaluation criteria. Finally, some future research directions in this research field are outlined.
               ",autonomous vehicle
10.1016/j.comnet.2021.108004,journal,Computer Networks,sciencedirect,2021-05-22,sciencedirect,Deep reinforcement learning for blockchain in industrial IoT: A survey,https://api.elsevier.com/content/article/pii/S1389128621001213,"
                  With the ambitious plans of renewal and expansion of industrialization in many countries, the efficiency, agility and cost savings potentially resulting from the application of industrial Internet of Things (IIoT) are drawing attentions. Although blockchain and machine learning technologies (especially deep learning and deep reinforcement learning) may provide the next promising use case for IIoT, they are working in an adversarial way to some extent. While blockchain facilitates the data collection that is critical for machine learning under the premise of data regulation rules like privacy protection, it may suffer from privacy leakage due to big data analytics with the help of machine learning. To make machine learning and blockchain useful and practical for diversified services in industrial sectors, it is of paramount importance to have a comprehensive understanding of the development of both technologies in the context of IIoT. To this end, in this paper we summarize and analyze the applications of blockchain and machine learning in IIoT from three important aspects, i.e., consensus mechanism, storage and communication. This survey provides a deeper understanding on the security and privacy risks of the key components of a blockchain from the perspective of machine learning, which is useful in the design of practical blockchain solutions for IIoT. In addition, we provide useful guidance for future research in the area by identifying interesting open problems that need to be addressed before large-scale deployments of IIoT applications are practicable.
               ",autonomous vehicle
10.1016/j.ifacol.2021.10.162,journal,IFAC-PapersOnLine,sciencedirect,2021-12-31,sciencedirect,Potential of Machine Learning Methods for Robust Performance and Efficient Engine Control Development,https://api.elsevier.com/content/article/pii/S2405896321015640,"
                  Increasingly strict legislation for greenhouse gas and real-world pollutant emissions makes it necessary to develop fuel-efficient and robust control solutions for future automotive engines. Today’s engine control development relies on traditional map-based and model-based control approaches. Due to growing system complexity and real-world requirements, these expert-intensive and time-consuming approaches are facing a turning point, which will lead to unacceptable development time and costs in the near future. Artificial Intelligence (AI) is a disruptive technology, which has interesting features that can tackle these challenges. AI-based methods have received growing interest due to the increasing availability of data and the success of AI applications for complex problems. This paper presents an overview of the state-of-the-art in Machine Learning (ML)-based methods that are applied for engine control development with focus on the time-consuming calibration process. The overview here shows that the vast majority of studies concentrates on regression modelling to model complex processes, to reduce the number of model parameters and to develop real-time, ECU implementable models. The identified promising directions for future ML-based engine control research include the application of reinforcement learning methods to on-line optimize engine performance and guarantee robust performance and unsupervised learning methods for data quality monitoring.
               ",autonomous vehicle
10.1016/j.comnet.2021.108004,journal,Computer Networks,sciencedirect,2021-05-22,sciencedirect,Deep reinforcement learning for blockchain in industrial IoT: A survey,https://api.elsevier.com/content/article/pii/S1389128621001213,"
                  With the ambitious plans of renewal and expansion of industrialization in many countries, the efficiency, agility and cost savings potentially resulting from the application of industrial Internet of Things (IIoT) are drawing attentions. Although blockchain and machine learning technologies (especially deep learning and deep reinforcement learning) may provide the next promising use case for IIoT, they are working in an adversarial way to some extent. While blockchain facilitates the data collection that is critical for machine learning under the premise of data regulation rules like privacy protection, it may suffer from privacy leakage due to big data analytics with the help of machine learning. To make machine learning and blockchain useful and practical for diversified services in industrial sectors, it is of paramount importance to have a comprehensive understanding of the development of both technologies in the context of IIoT. To this end, in this paper we summarize and analyze the applications of blockchain and machine learning in IIoT from three important aspects, i.e., consensus mechanism, storage and communication. This survey provides a deeper understanding on the security and privacy risks of the key components of a blockchain from the perspective of machine learning, which is useful in the design of practical blockchain solutions for IIoT. In addition, we provide useful guidance for future research in the area by identifying interesting open problems that need to be addressed before large-scale deployments of IIoT applications are practicable.
               ",autonomous vehicle
10.1016/j.future.2021.06.036,journal,Future Generation Computer Systems,sciencedirect,2021-12-31,sciencedirect,Textual analysis of traitor-based dataset through semi supervised machine learning,https://api.elsevier.com/content/article/pii/S0167739X21002314,"
                  Insider threats are one of the most challenging and growing security threats which the government agencies, organizations, and institutions face. In such scenarios, malicious (red) activities are performed by the authorized individuals within the company. Because of which, an insider threat has become a taxing and difficult task to identify among other attacks. Along with other monitoring parameters; email logs play a vital role in many research areas such as stalking Insider Threat involving Collaborating Traitors, Textual Analysis, and Social Media exploration. This paper presents a semi-supervised machine learning framework which embraces the pre-processing and classification techniques together for unlabeled dataset i.e. emails. Enron Corporation dataset has been used for experiments and TWOS for evaluation of the proposed framework. Initially, dataset is transformed into vector form using Term Frequency–Inverse Document Frequency (TF–IDF). Thereafter, K-Means is used to classify emails based on message content. Finally, Machine Learning algorithm Decision Tree (DT) is applied to classify the malicious activities. The proposed framework has also been tested with other algorithms such as Logistic Regression (LR), Naive Bayes (NB), KNN, Support Vector Machine (SVM), Random Forest (RF) and Neural Network (NN). However, Decision Tree (DT) combined with pre-processing steps has given the desired results with 99.96% Accuracy and 0.994 AUC for identification of malicious content.
               ",autonomous vehicle
10.1016/j.neunet.2021.05.006,journal,Neural Networks,sciencedirect,2021-11-30,sciencedirect,Learning to recognize while learning to speak: Self-supervision and developing a speaking motor,https://api.elsevier.com/content/article/pii/S0893608021001982,"
                  Traditionally, learning speech synthesis and speech recognition were investigated as two separate tasks. This separation hinders incremental development for concurrent synthesis and recognition, where partially-learned synthesis and partially-learned recognition must help each other throughout lifelong learning. This work is a paradigm shift—we treat synthesis and recognition as two intertwined aspects of a lifelong learning agent. Furthermore, in contrast to existing recognition or synthesis systems, babies do not need their mothers to directly supervise their vocal tracts at every moment during the learning. We argue that self-generated non-symbolic states/actions at fine-grained time level help such a learner as necessary temporal contexts. Here, we approach a new and challenging problem—how to enable an autonomous learning system to develop an artificial speaking motor for generating temporally-dense (e.g., frame-wise) actions on the fly without human handcrafting a set of symbolic states. The self-generated states/actions are Muscles-like, High-dimensional, Temporally-dense and Globally-smooth (MHTG), so that these states/actions are directly attended for concurrent synthesis and recognition for each time frame. Human teachers are relieved from supervising learner’s motor ends. The Candid Covariance-free Incremental (CCI) Principal Component Analysis (PCA) is applied to develop such an artificial speaking motor where PCA features drive the motor. Since each life must develop normally, each Developmental Network-2 (DN-2) reaches the same network (maximum likelihood, ML) regardless of randomly initialized weights, where ML is not just for a function approximator but rather an emergent Turing Machine. The machine-synthesized sounds are evaluated by both the neural network and humans with recognition experiments. Our experimental results showed learning-to-synthesize and learning-to-recognize-through-synthesis for phonemes. This work corresponds to a key step toward our goal to close a great gap toward fully autonomous machine learning directly from the physical world.
               ",autonomous vehicle
10.1016/j.cogr.2021.06.003,journal,Cognitive Robotics,sciencedirect,2021-12-31,sciencedirect,MetaSeg: A survey of meta-learning for image segmentation,https://api.elsevier.com/content/article/pii/S2667241321000070,"Big data-driven deep learning methods have been widely used in image or video segmentation. However, in practical applications, training a deep learning model requires a large amount of labeled data, which is difficult to achieve. Meta-learning, as one of the most promising research areas in the field of artificial intelligence, is believed to be a key tool for approaching artificial general intelligence. Compared with the traditional deep learning algorithm, meta-learning can update the learning task quickly and complete the corresponding learning with less data. To the best of our knowledge, there exist few researches in the meta-learning-based visual segmentation. To this end, this paper summarizes the algorithms and current situation of image or video segmentation technologies based on meta-learning and point out the future trends of meta-learning. Meta-learning has the characteristics of segmentation that based on semi-supervised or unsupervised learning, all the recent novel methods are summarized in this paper. The principle, advantages and disadvantages of each algorithms are also compared and analyzed.",autonomous vehicle
10.1016/B978-0-323-85844-1.00006-4,journal,COVID-19: Tackling Global Pandemics through Scientific and Social Tools,sciencedirect,2022-12-31,sciencedirect,Chapter 3: Vaccine Development Through Reverse Vaccinology Using Artificial Intelligence and Machine Learning Approach,https://api.elsevier.com/content/article/pii/B9780323858441000064,"
               Rapid development of effective vaccine has been one of the cornerstones of outbreak (including epidemic and pandemic) preparedness. Since its inception, vaccine technology and immunization saved billions of lives. Fostered by the knowledge discovery in biological and computational sciences, the basic concepts of vaccine designing have also evolved significantly in the past few decades. The advent of whole-genome sequencing complemented with big-data analysis platforms eventuated omics-based vaccine designing—often referred to as reverse vaccinology (RV). Adopting systemic approach toward the proteome and structural data, RV accomplishes comprehensive profiling of immunogenicity. With the advancement in artificial intelligence and deep learning algorithms, a number of prediction tools have been introduced for precise and accurate prediction of immune-recognition patterns that can be exploited for designing novel vaccine candidates. Considering the fact that vaccines are available for only a few of the infectious diseases, there is an earnest need of rapid development of vaccines for several fatal and emerging pathogens, which can be immensely accentuated by RV. With specific emphasis on immunoinformatics and artificial intelligence algorithms used in it, within the scope of this chapter a comprehensive purview of RV and its application is provided.
            ",autonomous vehicle
10.1016/j.techsoc.2021.101647,journal,Technology in Society,sciencedirect,2021-08-31,sciencedirect,An alliance of humans and machines for machine learning: Hybrid intelligent systems and their design principles,https://api.elsevier.com/content/article/pii/S0160791X21001226,"With the growing number of applications of artificial intelligence such as autonomous cars or smart industrial equipment, the inaccuracy of utilized machine learning algorithms could lead to catastrophic outcomes. Human-in-the-loop computing combines human and machine intelligence resulting in a hybrid intelligence of complementary strengths. Whereas machines are unbeatable in logic and computation speed, humans are contributing with their creative and dynamic minds. Hybrid intelligent systems are necessary to achieve high accuracy and reliability of machine learning algorithms. In a design science research project with a Swedish manufacturing company, this paper presents an application of human-in-the-loop computing to make operational processes more efficient. While conceptualizing a Smart Power Distribution for electric industrial equipment, this research presents a set of principles to design machine-learning algorithms for hybrid intelligence. From being AI-ready as an organization to clearly focusing on the customer benefits of a hybrid intelligent system, designers need to build and strengthen the trust in the human-AI relationship to make future applications successful and reliable. With the growing trends of technological advancements and incorporation of artificial intelligence in more and more applications, the alliance of humans and machines have become even more crucial.",autonomous vehicle
10.1016/j.enbuild.2019.109689,journal,Energy and Buildings,sciencedirect,2020-03-01,sciencedirect,Unsupervised learning for fault detection and diagnosis of air handling units,https://api.elsevier.com/content/article/pii/S0378778819320134,"
                  Supervised learning techniques have witnessed significant successes in fault detection and diagnosis (FDD) for heating ventilation and air-conditioning (HVAC) systems. Despite the good performance, these techniques heavily rely on balanced datasets that contain a large amount of both faulty and normal data points. In real-world scenarios, however, it is often very challenging to collect a sufficient amount of faulty training samples that are necessary for building a balanced training dataset. In this paper, we introduce a framework that utilizes the generative adversarial network (GAN) to address the imbalanced data problem in FDD for air handling units (AHUs). To this end, we first show the necessary procedures of applying GAN to increase the number of faulty training samples in the training pool and re-balance the training dataset. The proposed framework then uses supervised classifiers to train the re-balanced datasets. Finally, we present a comparative study that illustrates the advantages of the proposed method for FDD of AHU with various evaluation metrics. Our work demonstrates the promising prospects of performing robust FDD of AHU with a limited number of faulty training samples.
               ",autonomous vehicle
10.1016/j.datak.2021.101909,journal,Data & Knowledge Engineering,sciencedirect,2021-07-31,sciencedirect,Pairing conceptual modeling with machine learning,https://api.elsevier.com/content/article/pii/S0169023X21000367,"Both conceptual modeling and machine learning have long been recognized as important areas of research. With the increasing emphasis on digitizing and processing large amounts of data for business and other applications, it would be helpful to consider how these areas of research can complement each other. To understand how they can be paired, we provide an overview of machine learning foundations and development cycle. We then examine how conceptual modeling can be applied to machine learning and propose a framework for incorporating conceptual modeling into data science projects. The framework is illustrated by applying it to a healthcare application. For the inverse pairing, machine learning can impact conceptual modeling through text and rule mining, as well as knowledge graphs. The pairing of conceptual modeling and machine learning in this way should help lay the foundations for future research.",autonomous vehicle
10.1016/j.neucom.2021.06.027,journal,Neurocomputing,sciencedirect,2021-10-11,sciencedirect,An autonomous learning mobile robot using biological reward modulate STDP,https://api.elsevier.com/content/article/pii/S0925231221009310,"
                  Recent studies have shown that biologically inspired Spiking Neural Networks (SNNs) has potentials for the mobile robot controls. Based on SNNs, an autonomous learning paradigm for controlling mobile robots is proposed in this work, which can learn specific tasks autonomously. A reward modulated spike-timing-dependent plasticity (R-STDP) learning algorithm is designed to aid implementing the autonomous learning paradigm. It can train the SNN under different environmental states and conditions. The obstacle avoidance in the synthetic and real environments is used as a robotic task example to verify the effectiveness of the proposed paradigm. Results show that the mobile robot can learn autonomously under different environmental conditions and is able to avoid obstacles after learning processes complete.
               ",autonomous vehicle
10.1016/j.autcon.2021.103737,journal,Automation in Construction,sciencedirect,2021-08-31,sciencedirect,Autonomous construction hoist system based on deep reinforcement learning in high-rise building construction,https://api.elsevier.com/content/article/pii/S0926580521001886,"
                  Construction hoists at most building construction sites are manually controlled by human operators using their intuitions; as a result, unnecessary trips are often made when multiple hoists are operating simultaneously and/or when complicated hoist calls are requested. These trips increase a passenger's waiting time and lifting time, reducing the lifting performance of the hoists. To address this issue, the authors develop an autonomous hoist supported by a deep Q-network (DQN), a deep reinforcement learning method. The results show that the DQN algorithm can provide better control policy in complicated real-world hoist control situations than previous control algorithms, reducing the waiting time and lifting time of passengers by up to 86.7%. Such an automated hoist control system helps shorten the project schedule by increasing the lifting performance of multiple hoists at high-rise building construction sites.
               ",autonomous vehicle
10.1016/j.phycom.2020.101184,journal,Physical Communication,sciencedirect,2020-12-31,sciencedirect,Deep reinforcement learning based mobile edge computing for intelligent Internet of Things,https://api.elsevier.com/content/article/pii/S1874490720302615,"
                  In this paper, we investigate mobile edge computing (MEC) networks for intelligent internet of things (IoT), where multiple users have some computational tasks assisted by multiple computational access points (CAPs). By offloading some tasks to the CAPs, the system performance can be improved through reducing the latency and energy consumption, which are the two important metrics of interest in the MEC networks. We devise the system by proposing the offloading strategy intelligently through the deep reinforcement learning algorithm. In this algorithm, Deep Q-Network is used to automatically learn the offloading decision in order to optimize the system performance, and a neural network (NN) is trained to predict the offloading action, where the training data is generated from the environmental system. Moreover, we employ the bandwidth allocation in order to optimize the wireless spectrum for the links between the users and CAPs, where several bandwidth allocation schemes are proposed. In further, we use the CAP selection in order to choose one best CAP to assist the computational tasks from the users. Simulation results are finally presented to show the effectiveness of the proposed reinforcement learning offloading strategy. In particular, the system cost of latency and energy consumption can be reduced significantly by the proposed deep reinforcement learning based algorithm.
               ",autonomous vehicle
10.1016/j.dibe.2021.100045,journal,Developments in the Built Environment,sciencedirect,2021-05-31,sciencedirect,Machine learning in construction: From shallow to deep learning,https://api.elsevier.com/content/article/pii/S2666165921000041,"The development of artificial intelligence technology is currently bringing about new opportunities in construction. Machine learning is a major area of interest within the field of artificial intelligence, playing a pivotal role in the process of making construction “smart”. The application of machine learning in construction has the potential to open up an array of opportunities such as site supervision, automatic detection, and intelligent maintenance. However, the implementation of machine learning faces a range of challenges due to the difficulties in acquiring labeled data, especially when applied in a highly complex construction site environment. This paper reviews the history of machine learning development from shallow to deep learning and its applications in construction. The strengths and weaknesses of machine learning technology in construction have been analyzed in order to foresee the future direction of machine learning applications in this sphere. Furthermore, this paper presents suggestions which may benefit researchers in terms of combining specific knowledge domains in construction with machine learning algorithms so as to develop dedicated deep network models for the industry.",autonomous vehicle
10.1016/j.intmar.2020.04.007,journal,Journal of Interactive Marketing,sciencedirect,2020-08-31,sciencedirect,Artificial Intelligence and Marketing: Pitfalls and Opportunities,https://api.elsevier.com/content/article/pii/S1094996820300888,"
                  This article discusses the pitfalls and opportunities of AI in marketing through the lenses of knowledge creation and knowledge transfer. First, we discuss the notion of “higher-order learning” that distinguishes AI applications from traditional modeling approaches, and while focusing on recent advances in deep neural networks, we cover its underlying methodologies (multilayer perceptron, convolutional, and recurrent neural networks) and learning paradigms (supervised, unsupervised, and reinforcement learning). Second, we discuss the technological pitfalls and dangers marketing managers need to be aware of when implementing AI in their organizations, including the concepts of badly defined objective functions, unsafe or unrealistic learning environments, biased AI, explainable AI, and controllable AI. Third, AI will have a deep impact on predictive tasks that can be automated and require little explainability, we predict that AI will fall short of its promises in many marketing domains if we do not solve the challenges of tacit knowledge transfer between AI models and marketing organizations.
               ",autonomous vehicle
10.1016/B978-0-12-823817-2.00011-5,journal,Mobile Edge Artificial Intelligence,sciencedirect,2022-12-31,sciencedirect,Chapter Two: Primer on artificial intelligence,https://api.elsevier.com/content/article/pii/B9780128238172000115,"
               
                  Artificial intelligence (AI) has induced considerable achievements in various fields such as computer vision, natural language processing, autonomous vehicles, and so on. The AI models refer to the technology presenting human intelligence through computer programs, which could efficiently address the complex optimization problems in the wireless network to enhance the communication quality. On the other hand, the wireless network could also assist the data collection for model training and simplify the deployment of inference models. In this chapter, we shall present the basic concept of various AI models, which contributes to comprehend the following chapters. The machine learning (ML) models are specified by presenting supervised learning, unsupervised learning, and reinforcement learning. Moreover, we particularly present various models of deep learning (DL), which exhibit complex structure and excellent performance.
            ",autonomous vehicle
10.1016/j.jacr.2019.05.047,journal,Journal of the American College of Radiology,sciencedirect,2019-09-30,sciencedirect,"Strengths, Weaknesses, Opportunities, and Threats Analysis of Artificial Intelligence and Machine Learning Applications in Radiology",https://api.elsevier.com/content/article/pii/S1546144019306994,"
                  Currently, the use of artificial intelligence (AI) in radiology, particularly machine learning (ML), has become a reality in clinical practice. Since the end of the last century, several ML algorithms have been introduced for a wide range of common imaging tasks, not only for diagnostic purposes but also for image acquisition and postprocessing. AI is now recognized to be a driving initiative in every aspect of radiology. There is growing evidence of the advantages of AI in radiology creating seamless imaging workflows for radiologists or even replacing radiologists. Most of the current AI methods have some internal and external disadvantages that are impeding their ultimate implementation in the clinical arena. As such, AI can be considered a portion of a business trying to be introduced in the health care market. For this reason, this review analyzes the current status of AI, and specifically ML, applied to radiology from the scope of strengths, weaknesses, opportunities, and threats (SWOT) analysis.
               ",autonomous vehicle
10.1016/j.icte.2021.08.021,journal,ICT Express,sciencedirect,2021-09-03,sciencedirect,A comparative study of classification and prediction of Cardio-Vascular Diseases (CVD) using Machine Learning and Deep Learning techniques,https://api.elsevier.com/content/article/pii/S2405959521001119,"Cardio-Vascular Diseases (CVD) are found to be rampant in the populace leading to fatal death. The statistics of a recent survey reports that the mortality rate is expanding due to obesity, cholesterol, high blood pressure and usage of tobacco among the people. The severity of the disease is piling up due to the above factors. Studying about the variations of these factors and their impact on CVD is the demand of the hour. This necessitates the usage of modern techniques to identify the disease at its outset and to aid a markdown in the mortality rate. Artificial Intelligence and Data Mining domains have a research scope with their enormous techniques that would assist in the prediction of the CVD priory and identify their behavioural patterns in the large volume of data. The results of these predictions will help the clinicians in decision making and early diagnosis, which would reduce the risk of patients becoming fatal. This paper compares and reports the various Classification, Data Mining, Machine Learning, Deep Learning models that are used for prediction of the Cardio-Vascular diseases. The survey is organized as threefold: Classification and Data Mining Techniques for CVD, Machine Learning Models for CVD and Deep Learning Models for CVD prediction. The performance metrics used for reporting the accuracy, the dataset used for prediction and classification, and the tools used for each category of these techniques are also compiled and reported in this survey.",autonomous vehicle
10.1016/j.inffus.2019.10.007,journal,Information Fusion,sciencedirect,2020-04-30,sciencedirect,Emotional editing constraint conversation content generation based on reinforcement learning,https://api.elsevier.com/content/article/pii/S1566253519302234,"
                  In recent years, the generation of conversation content based on deep neural networks has attracted many researchers. However, traditional neural language models tend to generate general replies, lacking logical and emotional factors. This paper proposes a conversation content generation model that combines reinforcement learning with emotional editing constraints to generate more meaningful and customizable emotional replies. The model divides the replies into three clauses based on pre-generated keywords and uses the emotional editor to further optimize the final reply. The model combines multi-task learning with multiple indicator rewards to comprehensively optimize the quality of replies. Experiments shows that our model can not only improve the fluency of the replies, but also significantly enhance the logical relevance and emotional relevance of the replies.
               ",autonomous vehicle
10.1016/j.rser.2021.111685,journal,Renewable and Sustainable Energy Reviews,sciencedirect,2022-01-31,sciencedirect,A review of advanced ground source heat pump control: Artificial intelligence for autonomous and adaptive control,https://api.elsevier.com/content/article/pii/S136403212100959X,"Geothermal energy has the potential to contribute significantly to the CO 2 reduction targets as a renewable source for building heating and cooling but is yet under exploited, mostly due to its high initial investment cost. A lot of research is being carried out to optimise Ground Source Heat Pump (GSHP) systems’ design, but a good control strategy is also fundamental to achieve long-term performance and reduced payback time. GSHP control optimisation is a non-linear dynamic optimisation problem that is influenced by multiple parameters. It can thus not be fully optimised with traditional methods. Artificial Intelligence, and in particular Machine Learning, is suited for this type of optimisation as it can learn implicit relations between parameters and can address non-linearity. This paper reviews the challenges of GSHP control and the strategies for control optimisation found in the literature, from basic rule-based system to artificial neural network-based strategies. Two principal uses of Artificial Intelligence for ground source heat pump control are identified: building a predictive model of the system that reflects its real performances and optimising the control decision in real time. However, the examples found in the literature are limited and the need to further explore the benefits of Machine Learning is identified. The latest developments in the field are reviewed to explore their potential to further improve GSHP control. The challenges of the full implementation of such algorithms are also discussed.",autonomous vehicle
10.1016/j.yofte.2021.102571,journal,Optical Fiber Technology,sciencedirect,2021-07-31,sciencedirect,"Cost-efficient routing, modulation, wavelength and port assignment using reinforcement learning in optical transport networks",https://api.elsevier.com/content/article/pii/S1068520021001206,"
                  With the rapid growth of global network traffic, more and more high-capacity network services are required. Traditional wavelength division multiplexing (WDM) networks are inadequate in terms of service scheduling and network management, so optical transport networks (OTN) have been proposed. OTN offers electric-layer switching ability to support finer granularity and higher spectrum efficiency. In OTN, we need to realize routing, modulation, wavelength, and port assignment (RMWPA) for supporting optical-electric (O/E) conversion (i.e., O/E port). Efficient RMWPA will significantly reduce the cost of operators. In this paper, we explore multi-modal information from OTN, and image the topology and routes to capture the information of the OTN and services. We design a cost-efficient RMWPA (CE-RMWPA) algorithm based on reinforcement learning to realize RMWPA for reducing cost in OTN. The proposed algorithm can interact with the OTN environment and learn how to make improved decisions based on the environment’s feedback. Simulative results demonstratethat the CE-RMWPA algorithm can achieve the optimization of RMWPA for reducing cost.
               ",autonomous vehicle
10.1016/j.ymeth.2020.06.016,journal,Methods,sciencedirect,2020-08-01,sciencedirect,Machine learning and AI-based approaches for bioactive ligand discovery and GPCR-ligand recognition,https://api.elsevier.com/content/article/pii/S1046202319302762,"In the last decade, machine learning and artificial intelligence applications have received a significant boost in performance and attention in both academic research and industry. The success behind most of the recent state-of-the-art methods can be attributed to the latest developments in deep learning. When applied to various scientific domains that are concerned with the processing of non-tabular data, for example, image or text, deep learning has been shown to outperform not only conventional machine learning but also highly specialized tools developed by domain experts. This review aims to summarize AI-based research for GPCR bioactive ligand discovery with a particular focus on the most recent achievements and research trends. To make this article accessible to a broad audience of computational scientists, we provide instructive explanations of the underlying methodology, including overviews of the most commonly used deep learning architectures and feature representations of molecular data. We highlight the latest AI-based research that has led to the successful discovery of GPCR bioactive ligands. However, an equal focus of this review is on the discussion of machine learning-based technology that has been applied to ligand discovery in general and has the potential to pave the way for successful GPCR bioactive ligand discovery in the future. This review concludes with a brief outlook highlighting the recent research trends in deep learning, such as active learning and semi-supervised learning, which have great potential for advancing bioactive ligand discovery.",autonomous vehicle
10.1016/j.ejmp.2021.03.026,journal,Physica Medica,sciencedirect,2021-03-31,sciencedirect,Basic of machine learning and deep learning in imaging for medical physicists,https://api.elsevier.com/content/article/pii/S1120179721001435,"
                  The manuscript aims at providing an overview of the published algorithms/automation tool for artificial intelligence applied to imaging for Healthcare. A PubMed search was performed using the query string to identify the proposed approaches (algorithms/automation tools) for artificial intelligence (machine and deep learning) in a 5-year period. The distribution of manuscript in the various disciplines and the investigated image types according to the AI approaches are presented. The limitation and opportunity of AI application in the clinical practice or in the next future research is discussed.
               ",autonomous vehicle
10.1016/j.ress.2020.106901,journal,Reliability Engineering & System Safety,sciencedirect,2020-07-31,sciencedirect,Deep reinforcement learning-based sampling method for structural reliability assessment,https://api.elsevier.com/content/article/pii/S0951832019300791,"
                  Surrogate model methods are widely used in structural reliability assessment, but conventional sampling methods require a large number of experimental points to construct a surrogate model. Inspired by the learning process of the AlphaGo, which is essentially optimization of sampling, we proposed a deep reinforcement learning (DRL)-based sampling method for structural reliability assessment. First, the sampling space and the existing samples are transformed into an array that is treated as the state in DRL. Second, a deep neural network is designed as the agent to observe the sampling space and select new experimental points, which are treated as actions. Finally, a reward function is proposed to guide the deep neural network to select experimental points along the limit state surface. Two numerical examples including a benchmark problem are employed to illustrate the sampling ability of the proposed method for structural reliability calculation. The simulation results demonstrate that the proposed method can learn to select experimental points along the limit state surface. Comparing with the direct Monte Carlo simulation, AK-MCS, Latin hypercube sampling, and subset simulation methods, the results show that the proposed DRL-based sampling method has an advantage in dealing with highly nonlinear problems.
               ",autonomous vehicle
10.1016/j.arcontrol.2020.03.001,journal,Annual Reviews in Control,sciencedirect,2020-12-31,sciencedirect,Reinforcement learning in sustainable energy and electric systems: a survey,https://api.elsevier.com/content/article/pii/S1367578820300079,"
                  The dynamic nature of sustainable energy and electric systems can vary significantly along with the environment and load change, and they represent the features of multivariate, high complexity and uncertainty of the nonlinear system. Moreover, the integration of intermittent renewable energy sources and energy consumption behaviours of households introduce more uncertainty into sustainable energy and electric systems. The operation, control and decision-making in such an environment definitely require increasing intelligence and flexibility in the control and optimization to ensure the quality of service of sustainable energy and electric systems. Reinforcement learning is a wide class of optimal control strategies that uses estimating value functions from experience, simulation, or search to learn in highly dynamic, stochastic environment. The interactive context enables reinforcement learning to develop strong learning ability and high adaptability. Reinforcement learning does not require the use of the model of system dynamics, which makes it suitable for sustainable energy and electric systems with complex nonlinearity and uncertainty. The use of reinforcement learning in sustainable energy and electric systems will certainly change the traditional energy utilization mode and bring more intelligence into the system. In this survey, an overview of reinforcement learning, the demand for reinforcement learning in sustainable energy and electric systems, reinforcement learning applications in sustainable energy and electric systems, and future challenges and opportunities will be explicitly addressed.
               ",autonomous vehicle
10.1016/j.vehcom.2021.100403,journal,Vehicular Communications,sciencedirect,2021-08-30,sciencedirect,Survey on Artificial Intelligence (AI) techniques for Vehicular Ad-hoc Networks (VANETs),https://api.elsevier.com/content/article/pii/S2214209621000723,"
                  Advances in communications, smart transportation systems, and computer systems have recently opened up vast possibilities of intelligent solutions for traffic safety, convenience, and effectiveness. Artificial Intelligence (AI) is currently being used in various application domains because of its strong potential to help enhance conventional data-driven methods. In the area of Vehicular Ad hoc NETworks (VANETs) data is frequently collected from various sources. This data is used for various purposes which include routing, broadening the awareness of the driver, predicting mobility to avoid hazardous situations, thereby improving passenger comfort, safety, and quality of road experience. We present a comprehensive review of AI techniques that are currently being explored by various research efforts in the area of VANETs. We discuss the strengths and weaknesses of these proposed AI-based proposed approaches for the VANET environment. Finally, we identify future VANET research opportunities that can leverage the full potential of AI.
               ",autonomous vehicle
10.1016/j.imu.2021.100526,journal,Informatics in Medicine Unlocked,sciencedirect,2021-12-31,sciencedirect,Contribution of machine learning approaches in response to SARS-CoV-2 infection,https://api.elsevier.com/content/article/pii/S2352914821000162,"Problem The lately emerged SARS-CoV-2 infection, which has put the whole world in an aberrant demanding situation, has generated an urgent need for developing effective responses through artificial intelligence (AI). Aim This paper aims to overview the recent applications of machine learning techniques contributing to prevention, diagnosis, monitoring, and treatment of coronavirus disease (SARS-CoV-2). Methods A progressive investigation of the recent publications up to November 2020, related to AI approaches towards managing the challenges of COVID-19 infection was made. Results For patient diagnosis and screening, Convolutional Neural Network (CNN) and Support Vector Machine (SVM) are broadly applied for classification purposes. Moreover, Deep Neural Network (DNN) and homology modeling are the most used SARS-CoV-2 drug repurposing models. Conclusion While the fields of diagnosis of the SARS-CoV-2 infection by medical image processing and its dissemination pattern through machine learning have been sufficiently studied, some areas such as treatment outcome in patients and drug development need to be further investigated using AI approaches.",autonomous vehicle
10.1016/j.energy.2020.117794,journal,Energy,sciencedirect,2020-07-01,sciencedirect,A new hybrid ensemble deep reinforcement learning model for wind speed short term forecasting,https://api.elsevier.com/content/article/pii/S0360544220309014,"
                  Wind speed forecasting is a promising solution to improve the efficiency of energy utilization. In this study, a novel hybrid wind speed forecasting model is proposed. The whole modeling process of the proposed model consists of three steps. In stage I, the empirical wavelet transform method reduces the non-stationarity of the original wind speed data by decomposing the original data into several sub-series. In stage II, three kinds of deep networks are utilized to build the forecasting model and calculate prediction results of all sub-series, respectively. In stage III, the reinforcement learning method is used to combine three kinds of deep networks. The forecasting results of each sub-series are combined to obtain the final forecasting results. By comparing all the results of the predictions over three different types of wind speed series, it can be concluded that: (a) the proposed reinforcement learning based ensemble method is effective in integrating three kinds of deep network and works better than traditional optimization based ensemble method; (b) the proposed ensemble deep reinforcement learning based wind speed prediction model can get accurate results in all cases and provide the best accuracy compared with sixteen alternative models and three state-of-the-art models.
               ",autonomous vehicle
10.1016/j.engappai.2020.104000,journal,Engineering Applications of Artificial Intelligence,sciencedirect,2020-11-30,sciencedirect,Deep learning in electrical utility industry: A comprehensive review of a decade of research,https://api.elsevier.com/content/article/pii/S0952197620302943,"
                  Smart-grid (SG) is a new revolution in the electrical utility industry (EUI) over the past decade. With each moving day, some new advanced technologies are coming into the picture which forces the utility engineers to think about its application to make the electrical grid become smarter. Artificial intelligence (AI) techniques such as machine learning (ML), artificial neural network (ANN), deep learning (DL), reinforcement learning (RL), and deep-reinforcement learning (DRL) are the few examples of above-mentioned advanced technologies by which large volume of collected information being processed, and deliver the solution to the complex problems associated with EUI. In recent times, DL for artificial intelligence applications has gained huge attention in the diverse research area. The traditional ML techniques have several constrained for processing the data in raw form. However, the DL provides the options to process the raw data without extracting and selecting the feature vector. The DL techniques belong to a new era of AI development. This article presents the taxonomy of DL algorithms available in the literature applied to different problems in EUI. The main objective of this survey is to provide a comprehensive idea to the researcher/utility engineer about the applications and future research scope of DL methods for power systems studies.
               ",autonomous vehicle
10.1016/j.array.2021.100057,journal,Array,sciencedirect,2021-07-31,sciencedirect,"Deep learning for object detection and scene perception in self-driving cars: Survey, challenges, and open issues",https://api.elsevier.com/content/article/pii/S2590005621000059,"This article presents a comprehensive survey of deep learning applications for object detection and scene perception in autonomous vehicles. Unlike existing review papers, we examine the theory underlying self-driving vehicles from deep learning perspective and current implementations, followed by their critical evaluations. Deep learning is one potential solution for object detection and scene perception problems, which can enable algorithm-driven and data-driven cars. In this article, we aim to bridge the gap between deep learning and self-driving cars through a comprehensive survey. We begin with an introduction to self-driving cars, deep learning, and computer vision followed by an overview of artificial general intelligence. Then, we classify existing powerful deep learning libraries and their role and significance in the growth of deep learning. Finally, we discuss several techniques that address the image perception issues in real-time driving, and critically evaluate recent implementations and tests conducted on self-driving cars. The findings and practices at various stages are summarized to correlate prevalent and futuristic techniques, and the applicability, scalability and feasibility of deep learning to self-driving cars for achieving safe driving without human intervention. Based on the current survey, several recommendations for further research are discussed at the end of this article.",autonomous vehicle
10.1016/j.yamp.2019.07.010,journal,Advances in Molecular Pathology,sciencedirect,2019-11-30,sciencedirect,Machine Learning in Biology and Medicine,https://api.elsevier.com/content/article/pii/S2589408019300110,,autonomous vehicle
10.1016/j.mjafi.2020.10.005,journal,Medical Journal Armed Forces India,sciencedirect,2021-07-31,sciencedirect,Artificial intelligence in critical care: Its about time!,https://api.elsevier.com/content/article/pii/S0377123720301945,"
                  Currently, most critical care information is not expressed automatically at a granular level, rather is continually assessed by overindulged Intensive Care Unit (ICU) staff. Furthermore, due to different confounding morbidities and the uniqueness of the ICU setting, it is difficult to protocolize treatment regimens in the ICU. In highly complex ICU setting where man and resource management becomes extremely challenging, definite advancements are required to implement Artificial Intelligence (AI) for prognosticating the course of the disease to aid in informed decision-making. AI is the intelligence of a computer or computer-supervised robot to execute a piece of work commonly associated with intelligent beings, wherein the machines go beyond the realms of normal information processing by adding the characteristics of learning, sound reasoning, and weighting of the inputs. AI recognizes circuitous, relational time-series blueprint within datasets and this reasoning of analysis transcends conventional threshold-based analysis adapted in ICU protocols. AI works on the principle of a more complex form of Machine Learning by Artificial Neural Networks (ANN). These information-processing paradigms use multidimensional arrays called tensors which aid in ‘learning’ and ‘weighting’ all the information made available to it, thereby converting normal machine learning into Deep Learning. Here, the use of AI for data mining in complex ICU settings for protocol formulation and temporal representation and reasoning is discussed.
               ",autonomous vehicle
10.1016/j.ress.2021.107864,journal,Reliability Engineering & System Safety,sciencedirect,2021-11-30,sciencedirect,Predictive maintenance enabled by machine learning: Use cases and challenges in the automotive industry,https://api.elsevier.com/content/article/pii/S0951832021003835,"Recent developments in maintenance modelling fuelled by data-based approaches such as machine learning (ML), have enabled a broad range of applications. In the automotive industry, ensuring the functional safety over the product life cycle while limiting maintenance costs has become a major challenge. One crucial approach to achieve this, is predictive maintenance (PdM). Since modern vehicles come with an enormous amount of operating data, ML is an ideal candidate for PdM. While PdM and ML for automotive systems have both been covered in numerous review papers, there is no current survey on ML-based PdM for automotive systems. The number of publications in this field is increasing — underlining the need for such a survey. Consequently, we survey and categorize papers and analyse them from an application and ML perspective. Following that, we identify open challenges and discuss possible research directions. We conclude that (a) publicly available data would lead to a boost in research activities, (b) the majority of papers rely on supervised methods requiring labelled data, (c) combining multiple data sources can improve accuracies, (d) the use of deep learning methods will further increase but requires efficient and interpretable methods and the availability of large amounts of (labelled) data.",autonomous vehicle
10.1016/j.cosrev.2020.100254,journal,Computer Science Review,sciencedirect,2020-08-31,sciencedirect,"Julia language in machine learning: Algorithms, applications, and open issues",https://api.elsevier.com/content/article/pii/S157401372030071X,"Machine learning is driving development across many fields in science and engineering. A simple and efficient programming language could accelerate applications of machine learning in various fields. Currently, the programming languages most commonly used to develop machine learning algorithms include Python, MATLAB, and C/C ++. However, none of these languages well balance both efficiency and simplicity. The Julia language is a fast, easy-to-use, and open-source programming language that was originally designed for high-performance computing, which can well balance the efficiency and simplicity. This paper summarizes the related research work and developments in the applications of the Julia language in machine learning. It first surveys the popular machine learning algorithms that are developed in the Julia language. Then, it investigates applications of the machine learning algorithms implemented with the Julia language. Finally, it discusses the open issues and the potential future directions that arise in the use of the Julia language in machine learning.",autonomous vehicle
10.1016/j.csl.2021.101276,journal,Computer Speech & Language,sciencedirect,2022-01-31,sciencedirect,Deep reinforcement and transfer learning for abstractive text summarization: A review,https://api.elsevier.com/content/article/pii/S0885230821000796,"
                  Automatic Text Summarization (ATS) is an important area in Natural Language Processing (NLP) with the goal of shortening a long text into a more compact version by conveying the most important points in a readable form. ATS applications continue to evolve and utilize effective approaches that are being evaluated and implemented by researchers. State-of-the-Art (SotA) technologies that demonstrate cutting-edge performance and accuracy in abstractive ATS are deep neural sequence-to-sequence models, Reinforcement Learning (RL) approaches, and Transfer Learning (TL) approaches, including Pre-Trained Language Models (PTLMs). The graph-based Transformer architecture and PTLMs have influenced tremendous advances in NLP applications. Additionally, the incorporation of recent mechanisms, such as the knowledge-enhanced mechanism, significantly enhanced the results. This study provides a comprehensive review of recent research advances in the area of abstractive text summarization for works spanning the past six years. Past and present problems are described, as well as their proposed solutions. In addition, abstractive ATS datasets and evaluation measurements are also highlighted. The paper concludes by comparing the best models and discussing future research directions.
               ",autonomous vehicle
10.1016/B978-0-12-809633-8.20325-7,journal,Encyclopedia of Bioinformatics and Computational Biology,sciencedirect,2019-12-31,sciencedirect,Artificial Intelligence and Machine Learning in Bioinformatics,https://api.elsevier.com/content/article/pii/B9780128096338203257,"
               This article provides an overview of Artificial Intelligence and Machine Learning methodologies and surveys representative applications in the bioinformatics field, such as for genomics, genetics, proteomics, systems biology and metagenomics. It provides examples of how to choose the appropriate Machine Learning algorithm and discusses Machine Learning in the era of “Big Data”.
            ",autonomous vehicle
10.1053/j.sodo.2021.05.002,journal,Seminars in Orthodontics,sciencedirect,2021-06-30,sciencedirect,"Deep learning and computer vision: Two promising pillars, powering the future in orthodontics",https://api.elsevier.com/content/article/pii/S1073874621000256,"
                  Currently, advances and affordability in digital data have increased the demand for expedite and automate several diagnostic and clinical tasks. In this regard, Artificial Intelligence is an extraordinary tool destined to support diagnosis, treatment plan and prognosis and to monitoring the progresses of orthodontic treatments. This paper is intended to help orthodontists in getting familiar with and to provide a breakdown of several of the pioneering applications of AI with special regard to Deep Learning and Computer Vision in orthodontics for continued innovation.
               ",autonomous vehicle
10.1016/j.istruc.2021.06.110,journal,Structures,sciencedirect,2021-10-31,sciencedirect,Machine learning applied to the design and inspection of reinforced concrete bridges: Resilient methods and emerging applications,https://api.elsevier.com/content/article/pii/S2352012421006214,"
                  Machine learning is one of the key pillars of industry 4.0 that has enabled rapid technological advancement through establishing complex connections among heterogeneous and highly complex engineering data automatically. Once the machine learning model is trained appropriately, it becomes able to effectively predict and make decisions. The technology is rapidly evolving and has found numerous applications in various branches of engineering due to its preponderance. This study is focused on exploring the recent advances of machine learning and its applications in reinforced concrete bridges. It covers a range of different machine learning techniques exploited in structural design, construction quality management, bridge engineering, and the inspection of reinforced concrete bridges. This review demonstrated that machine learning algorithms have established new research directions in bridge engineering, in particular for applications such as the form-finding of innovative long-span structures, structural reinforcement, and structural optimization.
               ",autonomous vehicle
10.1016/j.autcon.2021.103701,journal,Automation in Construction,sciencedirect,2021-07-31,sciencedirect,Reinforcement learning based process optimization and strategy development in conventional tunneling,https://api.elsevier.com/content/article/pii/S0926580521001527,"Reinforcement learning (RL) - a branch of machine learning - refers to the process of an agent learning to achieve a certain goal by interaction with its environment. The process of conventional tunneling shows many similarities, where a geotechnician (agent) tries to achieve a breakthrough (goal) by excavating the rockmass (environment) in an optimum way. In this paper we present a novel RL based framework for strategy development for conventional tunneling. We developed a virtual environment with the goal of a tunnel breakthrough and with a deep Q-network as the agent's architecture. It can choose from different excavation sequences to reach that goal and learns to do so in an economical and safe way by getting feedback from a specially designed reward system. Result analyses show that the optimal policies have great similarities to current practices of sequential tunneling and the framework has the potential to discover new tunneling strategies.",autonomous vehicle
10.1016/j.ijepes.2021.107368,journal,International Journal of Electrical Power & Energy Systems,sciencedirect,2022-01-31,sciencedirect,Battery energy storage control using a reinforcement learning approach with cyclic time-dependent Markov process,https://api.elsevier.com/content/article/pii/S0142061521006074,"
                  Scheduling efficient energy management system operations to respond to the unstable customer demand, electricity prices, and weather increases the complexity of the control systems and requires a flexible and cost-effective control policy. This study develops an intelligent and real-time battery energy storage control based on a reinforcement learning model focused on residential houses connected to the grid and equipped with solar photovoltaic panels and a battery energy storage system. Because the reinforcement learning’s performance is very dependent on the design of the underlying Markov decision process, a cyclic time-dependent Markov Process is uniquely designed to capture existing daily cyclic patterns in demand, electricity price, and solar energy. The Markov Process is successfully used in the Q-learning algorithm, resulting in more efficient battery energy control and saving electricity costs. The proposed Q-learning algorithm is compared with benchmark models of a deterministic equivalent solution and a One-step Roll-out algorithm. Numerical experiments show the gap between the deterministic equivalent solution and Q-learning approaches for one-month electricity cost decreased from 7.99% to 3.63% for house 27 and 6.91% to 3.26% for house 387 when the discrete size of demand, solar energy, price, and battery energy level adjusted to 20. Accordingly, the better performance of the proposed Q-learning is demonstrated compared to the One-step Roll-out algorithm. Moreover, the effect of discrete size of state-space parameters on the adaptive Q-learning performance and computational time are investigated. Variations in the electricity price significantly affect the Q-learning algorithm’s performance more than other parameters.
               ",autonomous vehicle
10.1016/j.knosys.2020.106244,journal,Knowledge-Based Systems,sciencedirect,2020-09-27,sciencedirect,A reinforcement learning approach for optimizing multiple traveling salesman problems over graphs,https://api.elsevier.com/content/article/pii/S0950705120304445,"
                  This paper proposes a learning-based approach to optimize the multiple traveling salesman problem (MTSP), which is one classic representative of cooperative combinatorial optimization problems. The MTSP is interesting to study, because the problem arises from numerous practical applications and efficient approaches to optimize the MTSP can potentially be adapted for other cooperative optimization problems. However, the MTSP is rarely researched in the deep learning domain because of certain difficulties, including the huge search space, the lack of training data that is labeled with optimal solutions and the lack of architectures that extract interactive behaviors among agents. This paper constructs an architecture consisting of a shared graph neural network and distributed policy networks to learn a common policy representation to produce near-optimal solutions for the MTSP. We use a reinforcement learning approach to train the model, overcoming the requirement data labeled with ground truth. We use a two-stage approach, where reinforcement learning is used to learn an allocation of agents to vertices, and a regular optimization method is used to solve the single-agent traveling salesman problems associated with each agent. We introduce a 
                        S
                     -samples batch training method to reduce the variance of the gradient, improving the performance significantly. Experiments demonstrate our approach successfully learns a strong policy representation that outperforms integer linear programming and heuristic algorithms, especially on large scale problems.
               ",autonomous vehicle
10.1016/j.patcog.2021.108174,journal,Pattern Recognition,sciencedirect,2021-12-31,sciencedirect,Graph representation learning for road type classification,https://api.elsevier.com/content/article/pii/S0031320321003617,"We present a novel learning-based approach to graph representations of road networks employing state-of-the-art graph convolutional neural networks. Our approach is applied to realistic road networks of 17 cities from Open Street Map. While edge features are crucial to generate descriptive graph representations of road networks, graph convolutional networks usually rely on node features only. We show that the highly representative edge features can still be integrated into such networks by applying a line graph transformation. We also propose a method for neighborhood sampling based on a topological neighborhood composed of both local and global neighbors. We compare the performance of learning representations using different types of neighborhood aggregation functions in transductive and inductive tasks and in supervised and unsupervised learning. Furthermore, we propose a novel aggregation approach, Graph Attention Isomorphism Network, GAIN 1 . Our results show that GAIN outperforms state-of-the-art methods on the road type classification problem.",autonomous vehicle
10.1016/j.ifacol.2020.06.111,journal,IFAC-PapersOnLine,sciencedirect,2020-12-31,sciencedirect,Reinforcement learning based control of batch polymerisation processes,https://api.elsevier.com/content/article/pii/S2405896320301300,"
                  Control of batch polymerization has been a challenging task. In this work, we have tried to use Reinforcement Learning (RL), and Deep Reinforcement Learning (DRL) based control on addressing the existing challenges. RL is a class of machine learning wherein an agent directly interacts with the environment and learns from its experience. The RL consist of an agent who takes an action, and the action changes the state of the environment. Based on old and new state agent gets a reward, which is reinforcement for its future actions. In this work, we have implemented RL and DRL based control for batch polymerization of Polymethyl methacrylate (PMMA). In both the controllers, the input variable considered was jacket temperature, while the reactor temperature was the output variable. Both the controllers have been found to achieve the given setpoint, while the DRL controller being faster than the RL controllers. Further, RL and DRL control with risk sensitivity were also carried out to accommodate the process constraints.
               ",autonomous vehicle
10.1016/j.csbj.2021.07.003,journal,Computational and Structural Biotechnology Journal,sciencedirect,2021-12-31,sciencedirect,Machine learning in the prediction of cancer therapy,https://api.elsevier.com/content/article/pii/S2001037021002932,"Resistance to therapy remains a major cause of cancer treatment failures, resulting in many cancer-related deaths. Resistance can occur at any time during the treatment, even at the beginning. The current treatment plan is dependent mainly on cancer subtypes and the presence of genetic mutations. Evidently, the presence of a genetic mutation does not always predict the therapeutic response and can vary for different cancer subtypes. Therefore, there is an unmet need for predictive models to match a cancer patient with a specific drug or drug combination. Recent advancements in predictive models using artificial intelligence have shown great promise in preclinical settings. However, despite massive improvements in computational power, building clinically useable models remains challenging due to a lack of clinically meaningful pharmacogenomic data. In this review, we provide an overview of recent advancements in therapeutic response prediction using machine learning, which is the most widely used branch of artificial intelligence. We describe the basics of machine learning algorithms, illustrate their use, and highlight the current challenges in therapy response prediction for clinical practice.",autonomous vehicle
10.1016/j.wpi.2018.07.002,journal,World Patent Information,sciencedirect,2018-12-31,sciencedirect,"The state-of-the-art on Intellectual Property Analytics (IPA): A literature review on artificial intelligence, machine learning and deep learning methods for analysing intellectual property (IP) data",https://api.elsevier.com/content/article/pii/S0172219018300103,"Big data is increasingly available in all areas of manufacturing and operations, which presents an opportunity for better decision making and discovery of the next generation of innovative technologies. Recently, there have been substantial developments in the field of patent analytics, which describes the science of analysing large amounts of patent information to discover trends. We define Intellectual Property Analytics (IPA) as the data science of analysing large amount of IP information, to discover relationships, trends and patterns for decision making. In this paper, we contribute to the ongoing discussion on the use of intellectual property analytics methods, i.e artificial intelligence methods, machine learning and deep learning approaches, to analyse intellectual property data. This literature review follows a narrative approach with search strategy, where we present the state-of-the-art in intellectual property analytics by reviewing 57 recent articles. The bibliographic information of the articles are analysed, followed by a discussion of the articles divided in four main categories: knowledge management, technology management, economic value, and extraction and effective management of information. We hope research scholars and industrial users, may find this review helpful when searching for the latest research efforts pertaining to intellectual property analytics.",autonomous vehicle
10.1016/j.procs.2021.04.175,journal,Procedia Computer Science,sciencedirect,2021-12-31,sciencedirect,Parkinson’s Disease Classification and Medication Adherence Monitoring Using Smartphone-based Gait Assessment and Deep Reinforcement Learning Algorithm,https://api.elsevier.com/content/article/pii/S1877050921010127,"For the diagnosis and classification of Parkinson’s patients, the unified scale of Parkinson’s patients (Unified Parkinson’s Disease Rating Scale – UPDRS) is used, which requires the patient to perform a series of tests among which the biomarkers of speech, the facial expression, the hand movement and walking analysis are considered, after which the doctor diagnoses the patient whether or not he has Parkinson’s disease according to the score obtained. The work proposes a system for monitoring patients with the use of cell phones and their automatic classification according to the data collected by them. The system starts from the budget that Parkinson’s patients have different abnormalities when walking if they do not follow the required medication. The cell phone collects the data passively while the patient has his cell phone in his pocket. After that, the data preprocessor helps to extract the walking cycles that this Parkinson-related biomarker contains. The algorithm proposed for classification and Medication Adherence Monitoring is the Deep Reinforcement Learning. With this work we demonstrate the feasibility of using cell phones to monitor the biomarker walking in Parkinson’s patients and the possibility of Passive Medication Adherence Monitoring and Dynamic Treatment Regimes.",autonomous vehicle
10.1016/j.ijrefrig.2019.07.018,journal,International Journal of Refrigeration,sciencedirect,2019-11-30,sciencedirect,A novel deep reinforcement learning based methodology for short-term HVAC system energy consumption prediction,https://api.elsevier.com/content/article/pii/S0140700719303160,"
                  Short-term energy consumption prediction has fundamental importance in many HVAC system management tasks, such as demand-side management, short-term maintenance, etc. Currently, the prevailing data-driven techniques, especially supervised machine learning methods, are widely applied for short-term energy consumption prediction. Deep reinforcement learning (DRL), as the state-of-the-art machine learning techniques, have been applied for HVAC system control, but rarely for energy consumption prediction. In this paper, a DRL algorithm, namely Deep Deterministic Policy Gradient (DDPG), is firstly introduced for short-term HVAC system energy consumption prediction. Moreover, Autoencoder (AE), which is powerful in processing data in their raw form, is incorporated into DDPG method to extract the high-level features of state space and optimize the prediction model. The operation data of the ground source heat pump (GSHP) system of an office building in Henan province, China is used to train and assess the proposed models. The results demonstrate that the proposed DDPG based models can achieve better prediction performance than common supervised models like BP Neural Network and Support Vector Machine. This study is an enlightening work which may inspire other researchers to tap the potential of DRL algorithms in this field.
               ",autonomous vehicle
10.1016/j.ejmp.2021.02.006,journal,Physica Medica,sciencedirect,2021-03-31,sciencedirect,AI applications to medical images: From machine learning to deep learning,https://api.elsevier.com/content/article/pii/S1120179721000946,"
                  Purpose
                  Artificial intelligence (AI) models are playing an increasing role in biomedical research and healthcare services. This review focuses on challenges points to be clarified about how to develop AI applications as clinical decision support systems in the real-world context.
               
                  Methods
                  A narrative review has been performed including a critical assessment of articles published between 1989 and 2021 that guided challenging sections.
               
                  Results
                  We first illustrate the architectural characteristics of machine learning (ML)/radiomics and deep learning (DL) approaches. For ML/radiomics, the phases of feature selection and of training, validation, and testing are described. DL models are presented as multi-layered artificial/convolutional neural networks, allowing us to directly process images. The data curation section includes technical steps such as image labelling, image annotation (with segmentation as a crucial step in radiomics), data harmonization (enabling compensation for differences in imaging protocols that typically generate noise in non-AI imaging studies) and federated learning. Thereafter, we dedicate specific sections to: sample size calculation, considering multiple testing in AI approaches; procedures for data augmentation to work with limited and unbalanced datasets; and the interpretability of AI models (the so-called black box issue). Pros and cons for choosing ML versus DL to implement AI applications to medical imaging are finally presented in a synoptic way.
               
                  Conclusions
                  Biomedicine and healthcare systems are one of the most important fields for AI applications and medical imaging is probably the most suitable and promising domain. Clarification of specific challenging points facilitates the development of such systems and their translation to clinical practice.
               ",autonomous vehicle
10.1016/j.adhoc.2021.102685,journal,Ad Hoc Networks,sciencedirect,2021-12-01,sciencedirect,Role of machine learning and deep learning in securing 5G-driven industrial IoT applications,https://api.elsevier.com/content/article/pii/S1570870521001906,"
                  The Internet of Things (IoT) connects millions of computing devices and has set a stage for future technology where industrial use cases like smart cities and smart houses will operate with minimal human intervention. IoT’s cross-domain amalgamations with emergent technologies like 5G and blockchain affects human life. Hence, increase in reliance over IoT necessitates focus on its privacy and security concerns. Implementing security through encryption, authentication, access control and communication security is the need of the hour. These needs can be best catered with the use of machine learning (ML) and deep learning (DL) that can help in realizing secure intelligent systems. In this work, the authors present a comprehensive review for securing Industrial-IoT (I-IoT) devices to contribute to the development of security methods for I-IoT deployed over 5G and blockchain. The survey provides a general analysis of the state-of-the-art security implementation and further assesses the product life cycle of IoT devices. The authors present numerous virtues as well as faults in the machine learning and deep learning algorithms deployed over the fog architecture in context with the security solutions. The potential security algorithms can help overcome many challenges in the IoT security and pave way for implementation with emerging technologies like 5G, blockchain, edge computing, fog computing and their use cases for creating smart environments.
               ",autonomous vehicle
10.1016/j.kint.2020.08.026,journal,Kidney International,sciencedirect,2021-04-30,sciencedirect,Prediction modeling—part 2: using machine learning strategies to improve transplantation outcomes,https://api.elsevier.com/content/article/pii/S0085253820310735,"
                  Kidney transplant recipients and transplant physicians face important clinical questions where machine learning methods may help improve the decision-making process. This mini-review explores potential applications of machine learning methods to key stages of a kidney transplant recipient’s journey, from initial waitlisting and donor selection, to personalization of immunosuppression and prediction of post-transplantation events. Both unsupervised and supervised machine learning methods are presented, including k-means clustering, principal components analysis, k-nearest neighbors, and random forests. The various challenges of these approaches are also discussed.
               ",autonomous vehicle
10.1016/j.matpr.2020.09.611,journal,Materials Today: Proceedings,sciencedirect,2020-11-05,sciencedirect,Overview of machine learning and its adaptability in mechanical engineering,https://api.elsevier.com/content/article/pii/S2214785320373363,"
                  Artificial Intelligence unto a mechanical engineer is to take the system to a next level to develop a better device. To better understand the physical phenomenon, engineers design the products that we all interact with. Machine learning (ML) plays a central role in translating that technology to make the world a better place. Many of the tools in ML are embedded with these methods and techniques to design machines which are interactive. The present study is an overview of applications of ML in Mechanical Engineering.
               ",autonomous vehicle
10.1016/j.eswa.2021.114820,journal,Expert Systems with Applications,sciencedirect,2021-08-01,sciencedirect,Machine Learning for industrial applications: A comprehensive literature review,https://api.elsevier.com/content/article/pii/S095741742100261X,"
                  Machine Learning (ML) is a branch of artificial intelligence that studies algorithms able to learn autonomously, directly from the input data. Over the last decade, ML techniques have made a huge leap forward, as demonstrated by Deep Learning (DL) algorithms implemented by autonomous driving cars, or by electronic strategy games. Hence, researchers have started to consider ML also for applications within the industrial field, and many works indicate ML as one the main enablers to evolve a traditional manufacturing system up to the Industry 4.0 level. Nonetheless, industrial applications are still few and limited to a small cluster of international companies. This paper deals with these topics, intending to clarify the real potentialities, as well as potential flaws, of ML algorithms applied to operation management. A comprehensive review is presented and organized in a way that should facilitate the orientation of practitioners in this field. To this aim, papers from 2000 to date are categorized in terms of the applied algorithm and application domain, and a keyword analysis is also performed, to details the most promising topics in the field. What emerges is a consistent upward trend in the number of publications, with a spike of interest for unsupervised and especially deep learning techniques, which recorded a very high number of publications in the last five years. Concerning trends, along with consolidated research areas, recent topics that are growing in popularity were also discovered. Among these, the main ones are production planning and control and defect analysis, thus suggesting that in the years to come ML will become pervasive in many fields of operation management.
               ",autonomous vehicle
10.1016/j.adhoc.2021.102667,journal,Ad Hoc Networks,sciencedirect,2021-12-01,sciencedirect,"Machine learning for 5G security: Architecture, recent advances, and challenges",https://api.elsevier.com/content/article/pii/S1570870521001785,"
                  The granularization of crucial network functions implementation using software-centric, and virtualized approaches in 5G networks have brought forth unprecedented security challenges in general and privacy concerns. Moreover, these software components’ premature deployment and compromised supply chain put the individual network components at risk and have a ripple effect for the rest of the network. Some of the novel threats to 5G assets include tampering in identity and access management, supply-chain poisoning, masquerade and bot attacks, loop-holes in source codes. Machine learning (ML) in this context can help to provide heavily dynamic and robust security mechanisms for the software-centric architecture of 5G Networks. ML models’ development and implementation also rely on programmable environments; hence, they can play a vital role in designing, modelling, and automating efficient security protocols. This article presents the threat landscape across 5G networks and discusses the feasibility and architecture of different ML-based models to counter these threats. Also, we present the architecture for automated threat intelligence using cooperative and coordinated ML to secure 5G assets and infrastructure. We also present the summary of closely related existing works along with future research challenges.
               ",autonomous vehicle
10.1016/j.comnet.2020.107496,journal,Computer Networks,sciencedirect,2020-12-09,sciencedirect,A survey on the computation offloading approaches in mobile edge computing: A machine learning-based perspective,https://api.elsevier.com/content/article/pii/S1389128620311634,"
                  With the rapid developments in emerging mobile technologies, utilizing resource-hungry mobile applications such as media processing, online Gaming, Augmented Reality (AR), and Virtual Reality (VR) play an essential role in both businesses and entertainments. To soften the burden of such complexities incurred by fast developments of such serving technologies, distributed Mobile Edge Computing (MEC) has been developed, aimed at bringing the computation environments near the end-users, usually in one hop, to reach predefined requirements. In the literature, offloading approaches are developed to connect the computation environments to mobile devices by transferring resource-hungry tasks to the near servers. Because of some rising problems such as inherent software and hardware heterogeneity, restrictions, dynamism, and stochastic behavior of the ecosystem, the computation offloading issues consider as the essential challenging problems in the MEC environment. However, to the best of the author's knowledge, in spite of its significance, in machine learning-based (ML-based) computation offloading mechanisms, there is not any systematic, comprehensive, and detailed survey in the MEC environment. In this paper, we provide a review on the ML-based computation offloading mechanisms in the MEC environment in the form of a classical taxonomy to identify the contemporary mechanisms on this crucial topic and to offer open issues as well. The proposed taxonomy is classified into three main fields: Reinforcement learning-based mechanisms, supervised learning-based mechanisms, and unsupervised learning-based mechanisms. Next, these classes are compared with each other based on the essential features such as performance metrics, case studies, utilized techniques, and evaluation tools, and their advantages and weaknesses are discussed, as well. Finally, open issues and uncovered or inadequately covered future research challenges are argued, and the survey is concluded.
               ",autonomous vehicle
10.1016/j.ccep.2021.04.011,journal,Cardiac Electrophysiology Clinics,sciencedirect,2021-09-30,sciencedirect,The Role of Artificial Intelligence in Arrhythmia Monitoring,https://api.elsevier.com/content/article/pii/S1877918221000423,,autonomous vehicle
10.1016/j.neunet.2021.07.021,journal,Neural Networks,sciencedirect,2021-11-30,sciencedirect,Continual learning for recurrent neural networks: An empirical evaluation,https://api.elsevier.com/content/article/pii/S0893608021002847,"
                  Learning continuously during all model lifetime is fundamental to deploy machine learning solutions robust to drifts in the data distribution. Advances in Continual Learning (CL) with recurrent neural networks could pave the way to a large number of applications where incoming data is non stationary, like natural language processing and robotics. However, the existing body of work on the topic is still fragmented, with approaches which are application-specific and whose assessment is based on heterogeneous learning protocols and datasets. In this paper, we organize the literature on CL for sequential data processing by providing a categorization of the contributions and a review of the benchmarks. We propose two new benchmarks for CL with sequential data based on existing datasets, whose characteristics resemble real-world applications.
                  We also provide a broad empirical evaluation of CL and Recurrent Neural Networks in class-incremental scenario, by testing their ability to mitigate forgetting with a number of different strategies which are not specific to sequential data processing. Our results highlight the key role played by the sequence length and the importance of a clear specification of the CL scenario.
               ",autonomous vehicle
10.1016/j.rcim.2021.102132,journal,Robotics and Computer-Integrated Manufacturing,sciencedirect,2021-10-31,sciencedirect,Adaptive optimal control of stencil printing process using reinforcement learning,https://api.elsevier.com/content/article/pii/S073658452100017X,"
                  The stencil printing process (SPP) is a critical operation in surface mount technology (SMT) because it contributes to 60% of soldering defects. The complex relationships between solder paste volume transfer efficiency (TE) and the SPP variables make the control of the solder paste volume TE during production a challenging problem. This research aims to optimize the stencil printing parameters in real time to control the solder paste volume TE and increase the first-pass yields of printed circuit boards (PCBs). A Reinforcement learning (RL) approach, specifically 
                        Q
                     -learning, is used to control and maintain the volume TE within the spec limits in an optimal adaptive control system. RL deals with the problem of building a control system or an autonomous agent that can learn how to take the proper actions to reach its objectives through interaction with its environment. The application of RL in SPP is not yet fully explored; therefore, this study investigates the impacts of applying 
                        Q
                     -learning to control the volume TE in real time. The proposed control systems capture the induced variations in the SPP for two printing directions and consequently adjust the significant and easy-to-change printing parameters in real time. Two types of 
                        Q
                     -learning are explored: 
                        Q
                     -table that uses a tabular format to store the 
                        Q
                     -values and 
                        Q
                     -network that uses an artificial neural network (ANN) to approximate the 
                        Q
                     -value function. Moreover, a new heterogeneous reward function-based clustering is proposed, which is integrated into the 
                        Q
                     -network to enhance its performance. The results show that the developed control systems can learn the optimal policy and take the proper actions to transit from initial states to terminal states. The proposed control systems using 
                        Q
                     -network with a function approximator and heterogeneous reward function converge fully much faster than 
                        Q
                     -table using continuous state space. Moreover, 
                        Q
                     -network control systems are capable to transit more states to terminal states with a lower number of actions when compared to 
                        Q
                     -table control systems.
               ",autonomous vehicle
10.1016/j.neucom.2020.05.113,journal,Neurocomputing,sciencedirect,2020-10-28,sciencedirect,Deep learning for brain disorder diagnosis based on fMRI images,https://api.elsevier.com/content/article/pii/S0925231220316210,"
                  In modern neuroscience and clinical study, neuroscientists and clinicians often use non-invasive imaging techniques to validate theories and computational models, observe brain activities and diagnose brain disorders. The functional Magnetic Resonance Imaging (fMRI) is one of the commonly-used imaging modalities that can be used to understand human brain mechanisms as well as the diagnosis and treatment of brain disorders. The advances in artificial intelligence and the emergence of deep learning techniques have shown promising results to better interpret fMRI data. Deep learning techniques have rapidly become the state of the art for analyzing fMRI data sets and resulted in performance improvements in diverse fMRI applications. Deep learning is normally presented as an end-to-end learning process and can alleviate feature engineering requirements and hence reduce domain knowledge requirements to some extent. Under the framework of deep learning, fMRI data can be considered as images, time series or images series. Hence, different deep learning models such as convolutional neural networks, recurrent neural network, or a combination of both, can be developed to process fMRI data for different tasks. In this review, we discussed the basics of deep learning methods and focused on its successful implementations for brain disorder diagnosis based on fMRI images. The goal is to provide a high-level overview of brain disorder diagnosis with fMRI images from the perspective of deep learning applications.
               ",autonomous vehicle
10.1016/j.isatra.2020.02.017,journal,ISA Transactions,sciencedirect,2020-07-31,sciencedirect,An adaptive deep reinforcement learning approach for MIMO PID control of mobile robots,https://api.elsevier.com/content/article/pii/S0019057820300781,"
                  Intelligent control systems are being developed for the control of plants with complex dynamics. However, the simplicity of the PID (proportional–integrative–derivative) controller makes it still widely used in industrial applications and robotics. This paper proposes an intelligent control system based on a deep reinforcement learning approach for self-adaptive multiple PID controllers for mobile robots. The proposed hybrid control strategy uses an actor–critic structure and it only receives low-level dynamic information as input and simultaneously estimates the multiple parameters or gains of the PID controllers. The proposed approach was tested in several simulated environments and in a real time robotic platform showing the feasibility of the approach for the low-level control of mobile robots. From the simulation and experimental results, our proposed approach demonstrated that it can be of aid by providing with behavior that can compensate or even adapt to changes in the uncertain environments providing a model free unsupervised solution. Also, a comparative study against other adaptive methods for multiple PIDs tuning is presented, showing a successful performance of the approach.
               ",autonomous vehicle
10.1016/j.cep.2021.108671,journal,Chemical Engineering and Processing - Process Intensification,sciencedirect,2021-10-16,sciencedirect,"Process intensification 4.0: A new approach for attaining new, sustainable and circular processes enabled by machine learning",https://api.elsevier.com/content/article/pii/S0255270121003597,"This paper reviews system-level transformations converging into the next generation of Process Intensification strategies defined as PI4.0. Process Intensification 4.0 uses data-driven algorithms to understand other physical and chemical processes that improve equipment design, predictive control, and optimization. Following this, an overview of the use of Artificial Intelligence techniques, particularly Machine Learning for the acceleration of equipment design, process optimization, and streamlining, is presented. This work will highlight and discuss the emerging framework of the integration between Circular Chemistry, Industry 4.0, and Process Intensification and how the data obtained from this integration is at the core of the next generation of Process Intensification strategies. This is supported by a discussion of different cases that apply data-driven models enabled by Machine Learning as a mean to enhance an intensified system (product synthesis, equipment or methods).",autonomous vehicle
10.1016/j.jacc.2020.11.030,journal,Journal of the American College of Cardiology,sciencedirect,2021-01-26,sciencedirect,<ce:marker name=jacmegphn alt=Commentary by Dr. Valentin Fuster altimg-small=jacmegphn_s.svg altimg=jacmegphn_o.svg></ce:marker>Machine Learning and the Future of Cardiovascular Care: <ce:italic>JACC</ce:italic> State-of-the-Art Review,https://api.elsevier.com/content/article/pii/S0735109720378943,"The role of physicians has always been to synthesize the data available to them to identify diagnostic patterns that guide treatment and follow response. Today, increasingly sophisticated machine learning algorithms may grow to support clinical experts in some of these tasks. Machine learning has the potential to benefit patients and cardiologists, but only if clinicians take an active role in bringing these new algorithms into practice. The aim of this review is to introduce clinicians who are not data science experts to key concepts in machine learning that will allow them to better understand the field and evaluate new literature and developments. The current published data in machine learning for cardiovascular disease is then summarized, using both a bibliometric survey, with code publicly available to enable similar analysis for any research topic of interest, and select case studies. Finally, several ways that clinicians can and must be involved in this emerging field are presented.",autonomous vehicle
10.1016/j.apenergy.2021.116808,journal,Applied Energy,sciencedirect,2021-06-01,sciencedirect,"A three-step machine learning framework for energy profiling, activity state prediction and production estimation in smart process manufacturing",https://api.elsevier.com/content/article/pii/S030626192100310X,"
                  The dynamic nature of chemical processes and manufacturing environments, along with numerous machines, their unique activity states, and mutual interactions, render challenges to energy monitoring at a machine level. In this study, we introduce MIGRATE (Machine learnInGfoRsmArTEnergy), a novel three-step framework to predict the machine-specific load profiles via energy disaggregation, which are in turn used to predict the machine’s activity state and the respective production capacities. Various supervised tree-based and recurrent neural network algorithms were evaluated on their capacities to predict load profiles and production capacities of four machines investigated in this study. Light gradient boosting machines and ensemble bi-directional long-term short memory were identified as the respective best performing algorithms with a mean absolute error and root mean squared error of 0.035 and 0.105 (units in Watts) for the disaggregation studies and 1.639 and 11.401 (units in quantities of samples processed) for production estimation. Four unsupervised machine learning algorithms were evaluated to cluster the machine’s activity state from their disaggregated load profiles, where the gaussian mixture model had a superior performance with the V score and Fowlkes Mallows index of 0.852 and 0.983, respectively. The MIGRATE framework is purely data-driven, cross-deployable and serves as promising catalyst to foster smart energy management practices and sustainable productions in the chemical and industrial manufacturing processes.
               ",autonomous vehicle
10.1016/j.cogsys.2019.09.006,journal,Cognitive Systems Research,sciencedirect,2020-01-31,sciencedirect,Mobile robot navigation with the combination of supervised learning in cerebellum and reward-based learning in basal ganglia,https://api.elsevier.com/content/article/pii/S1389041719304723,"
                  Autonomous navigation of mobile robot in unknown environment has attracted much attention of scholars over the past decades, and many bio-inspired heuristic navigation models have been presented, such as the cerebellum and basal ganglia models. Pervious cerebellum and basal ganglia model, however, treats them as parallel and independent system, without interaction between them, or combines their function together, but with only unidirectional communication between them. Based on the cognition and developmental mechanism of the biology, this paper proposes a novel navigation model, which uses the motivated developmental network (MDN) to mimic the supervised learning of the cerebellum and the reinforcement learning based on the radial basis function neural network (RBFNN) to simulate the reward-based learning of the basal ganglia, and integrates them together to construct a hybrid complex cognition model, to navigate a mobile robot in unknown environment. During the environment exploration, for the unexplored places, the artificial agent uses the cerebellum model to choose action, instead of the 
                        
                           ∊
                        
                     -greedy method, to accelerate the learning convergence speed of the basal ganglia. For the explored places, it directly uses the basal ganglia based on the RBFNN to explore, then update and perfect the knowledge base of the cerebellum, which enable the cerebellum to achieve better decision in the following exploration. Hence, it realizes not only the two way communication between the cerebellum and basal ganglia, but also the co-development of them. Experimental results show that this model can enable the agent to autonomously development its intelligence through the hybrid learning. As far as we know, this is the first work to realize the direct communication between the basal ganglia and the cerebellum.
               ",autonomous vehicle
10.1016/j.neunet.2021.09.018,journal,Neural Networks,sciencedirect,2021-12-31,sciencedirect,Natural and Artificial Intelligence: A brief introduction to the interplay between AI and neuroscience research,https://api.elsevier.com/content/article/pii/S0893608021003683,"Neuroscience and artificial intelligence (AI) share a long history of collaboration. Advances in neuroscience, alongside huge leaps in computer processing power over the last few decades, have given rise to a new generation of in silico neural networks inspired by the architecture of the brain. These AI systems are now capable of many of the advanced perceptual and cognitive abilities of biological systems, including object recognition and decision making. Moreover, AI is now increasingly being employed as a tool for neuroscience research and is transforming our understanding of brain functions. In particular, deep learning has been used to model how convolutional layers and recurrent connections in the brain’s cerebral cortex control important functions, including visual processing, memory, and motor control. Excitingly, the use of neuroscience-inspired AI also holds great promise for understanding how changes in brain networks result in psychopathologies, and could even be utilized in treatment regimes. Here we discuss recent advancements in four areas in which the relationship between neuroscience and AI has led to major advancements in the field; (1) AI models of working memory, (2) AI visual processing, (3) AI analysis of big neuroscience datasets, and (4) computational psychiatry.",autonomous vehicle
10.1016/j.physleta.2020.126353,journal,Physics Letters A,sciencedirect,2020-06-15,sciencedirect,Reinforcement learning for optimal error correction of toric codes,https://api.elsevier.com/content/article/pii/S0375960120301638,"
                  We apply deep reinforcement learning techniques to design high threshold decoders for the toric code under uncorrelated noise. By rewarding the agent only if the decoding procedure preserves the logical states of the toric code, and using deep convolutional networks for the training phase of the agent, we observe near-optimal performance for uncorrelated noise around the theoretically optimal threshold of 11%. We observe that, by and large, the agent implements a policy similar to that of minimum weight perfect matchings even though no bias towards any policy is given a priori.
               ",autonomous vehicle
10.1016/j.engappai.2021.104234,journal,Engineering Applications of Artificial Intelligence,sciencedirect,2021-06-30,sciencedirect,Deep Reinforcement Learning for QoS provisioning at the MAC layer: A Survey,https://api.elsevier.com/content/article/pii/S0952197621000816,"Quality of Service (QoS) provisioning is based on various network management techniques including resource management and medium access control (MAC). Various techniques have been introduced to automate networking decisions, particularly at the MAC layer. Deep reinforcement learning (DRL), as a solution to sequential decision making problems, is a combination of the power of deep learning (DL), to represent and comprehend the world, with reinforcement learning (RL), to understand the environment and act rationally. In this paper, we present a survey on the applications of DRL in QoS provisioning at the MAC layer. First, we present the basic concepts of QoS and DRL. Second, we classify the main challenges in the context of QoS provisioning at the MAC layer, including medium access and data rate control, and resource sharing and scheduling. Third, we review various DRL algorithms employed to support QoS at the MAC layer, by analyzing, comparing, and identifying their pros and cons. Furthermore, we outline a number of important open research problems and suggest some avenues for future research.",autonomous vehicle
10.1016/j.jobe.2020.101739,journal,Journal of Building Engineering,sciencedirect,2021-02-28,sciencedirect,Energy-efficient heating control for smart buildings with deep reinforcement learning,https://api.elsevier.com/content/article/pii/S2352710220333726,"
                  Buildings account for roughly 40% of the total energy consumption in the world, out of which heating, ventilation, and air conditioning are the major contributors. Traditional heating controllers are inefficient due to lack of adaptability to dynamic conditions such as changing user preferences and outside temperature patterns. Therefore, it is necessary to design energy-efficient controllers that can improvise occupant thermal comfort (deviation from setpoint temperature) while reducing energy consumption. This research presents a Deep Reinforcement Learning (DRL)-based heating controller to improve thermal comfort and minimize energy costs in smart buildings. We perform extensive simulation experiments using real-world outside temperature data. The results show that the DRL-based smart controller outperforms a traditional thermostat controller by improving thermal comfort between 15% and 30% and reducing energy costs between 5% and 12% in the simulated environment. A second set of experiments is then performed for the case of multiple buildings, each having its own heating equipment. The performance is compared when the buildings are controlled centrally (using a single DRL-based controller) versus decentralized control, where each heater is controlled independently and has its own DRL-based controller. We observe that as the number of buildings and differences in their setpoint temperatures increase, decentralized control performs better than a centralized controller. The results have practical implications for heating control, especially in areas with multiple buildings such as residential complexes with multiple houses.
               ",autonomous vehicle
10.1016/j.cpet.2021.06.005,journal,PET Clinics,sciencedirect,2021-10-31,sciencedirect,Artificial Intelligence-Based Image Enhancement in PET Imaging: Noise Reduction and Resolution Enhancement,https://api.elsevier.com/content/article/pii/S1556859821000444,,autonomous vehicle
10.1016/j.eswa.2021.116100,journal,Expert Systems with Applications,sciencedirect,2022-03-01,sciencedirect,Rule extraction in unsupervised anomaly detection for model explainability: Application to OneClass SVM,https://api.elsevier.com/content/article/pii/S0957417421014329,"OneClass SVM is a popular method for unsupervised anomaly detection. As many other methods, it suffers from the black box problem: it is difficult to justify, in an intuitive and simple manner, why the decision frontier is identifying data points as anomalous or non anomalous. This problem is being widely addressed for supervised models. However, it is still an uncharted area for unsupervised learning. In this paper, we evaluate several rule extraction techniques over OneClass SVM models, while presenting alternative designs for some of those algorithms. Furthermore, we propose algorithms for computing metrics related to eXplainable Artificial Intelligence (XAI) regarding the “comprehensibility”, “representativeness”, “stability” and “diversity” of the extracted rules. We evaluate our proposals with different data sets, including real-world data coming from industry. Consequently, our proposal contributes to extending XAI techniques to unsupervised machine learning models.",autonomous vehicle
10.1016/j.cja.2021.07.027,journal,Chinese Journal of Aeronautics,sciencedirect,2021-10-20,sciencedirect,Recent progress of machine learning in flow modeling and active flow control,https://api.elsevier.com/content/article/pii/S100093612100306X,"In terms of multiple temporal and spatial scales, massive data from experiments, flow field measurements, and high-fidelity numerical simulations have greatly promoted the rapid development of fluid mechanics. Machine Learning (ML) provides a wealth of analysis methods to extract potential information from a large amount of data for in-depth understanding of the underlying flow mechanism or for further applications. Furthermore, machine learning algorithms can enhance flow information and automatically perform tasks that involve active flow control and optimization. This article provides an overview of the past history, current development, and promising prospects of machine learning in the field of fluid mechanics. In addition, to facilitate understanding, this article outlines the basic principles of machine learning methods and their applications in engineering practice, turbulence models, flow field representation problems, and active flow control. In short, machine learning provides a powerful and more intelligent data processing architecture, and may greatly enrich the existing research methods and industrial applications of fluid mechanics.",autonomous vehicle
10.1016/j.jisa.2021.102923,journal,Journal of Information Security and Applications,sciencedirect,2021-09-30,sciencedirect,Attention based multi-agent intrusion detection systems using reinforcement learning,https://api.elsevier.com/content/article/pii/S2214212621001411,"
                  Designing an effective network intrusion system (IDS) is a challenging problem because of the emergence of a large number of novel attacks and heterogeneous network applications. The existing IDSs fail to adapt to the changing attack patterns and unseen attacks that lead to inaccurate detection of network vulnerabilities and system performance degradation. Therefore, there is a need to design robust, scalable, efficient, and adaptive IDS for networks. This paper presents a novel deep reinforcement learning-based IDS that employs Deep Q-Network logic in multiple distributed agents and uses attention mechanisms to efficiently detect and classify advanced network attacks. Our proposed multi-agent IDS is designed as a distributed attack detection platform where agents work in a coordinated manner to provide scalable, fault-tolerant, multi-view architecture guided security system. We have tested our model with extensive experimentation on two benchmark datasets: NSL-KDD and CICIDS2017. It shows improved performance in terms of higher accuracy, precision, recall, F1-Score, and low false-positive rate (FPR) in comparison to the state-of-the-art IDS works. On the other hand, many machine learning systems are found vulnerable to adversarial attacks. Thus, we evaluated our model’s robustness against a practical black-box adversarial attack and observed only a little degradation in performance. We integrated the concept of denoising autoencoder (DAE) with our model to further improve its robustness. Finally, we discuss the usability of our system in real-life applications against zero-day attack patterns.
               ",autonomous vehicle
10.1016/j.neucom.2020.07.020,journal,Neurocomputing,sciencedirect,2020-11-13,sciencedirect,Multi-attention deep reinforcement learning and re-ranking for vehicle re-identification,https://api.elsevier.com/content/article/pii/S0925231220311279,"
                  For solving the vehicle Re-identification (Re-ID) task, we need to focus our attention on the details with arbitrary size in the image, and it’s tough to locate these details accurately. In this paper, we propose a Multi-Attention Deep Reinforcement Learning (MADRL) model to focus on multi-attentional subregions that spreading randomly in the image, and extract the discriminative features for the Re-ID task. First, we obtain multiple attentions from the representative features, then group the feature channels into different parts, then train a deep reinforcement learning model to learn more accurate positions of these fine-grained details with different losses. Unlike existing models with complex strategies to keep the patch-matching constrains, our MADRL model can automatically locate the matching patches (multi-attentional subregions) in different vehicle images with the same identification (ID). Furthermore, based on the fine-grained attention and global features we re-calculate the distance between the inter- and intra- classes, and we get better re-ranking results. Compared with state-of-the-art methods on three large-scale vehicle Re-ID datasets, our algorithm greatly improves the performance of vehicle Re-ID.
               ",autonomous vehicle
10.1016/j.rser.2021.111459,journal,Renewable and Sustainable Energy Reviews,sciencedirect,2021-10-31,sciencedirect,Artificial intelligence techniques for enabling Big Data services in distribution networks: A review,https://api.elsevier.com/content/article/pii/S1364032121007413,"Artificial intelligence techniques lead to data-driven energy services in distribution power systems by extracting value from the data generated by the deployed metering and sensing devices. This paper performs a holistic analysis of artificial intelligence applications to distribution networks, ranging from operation, monitoring and maintenance to planning. The potential artificial intelligence techniques for power system applications and needed data sources are identified and classified. The following data-driven services for distribution networks are analyzed: topology estimation, observability, fraud detection, predictive maintenance, non-technical losses detection, forecasting, energy management systems, aggregated flexibility services and trading. A review of the artificial intelligence methods implemented in each of these services is conducted. Their interdependencies are mapped, proving that multiple services can be offered as a single clustered service to different stakeholders. Furthermore, the dependencies between the AI techniques with each energy service are identified. In recent years there has been a significant rise of deep learning applications for time series prediction tasks. Another finding is that unsupervised learning methods are mainly being applied to customer segmentation, buildings efficiency clustering and consumption profile grouping for non-technical losses detection. Reinforcement learning is being widely applied to energy management systems design, although more testing in real environments is needed. Distribution network sensorization should be enhanced and increased in order to obtain larger amounts of valuable data, enabling better service outcomes. Finally, the future opportunities and challenges for applying artificial intelligence in distribution grids are discussed.",autonomous vehicle
10.1016/j.comcom.2021.07.009,journal,Computer Communications,sciencedirect,2021-10-01,sciencedirect,Blockchain management and machine learning adaptation for IoT environment in 5G and beyond networks: A systematic review,https://api.elsevier.com/content/article/pii/S0140366421002632,"
                  Keeping in view of the constraints and challenges with respect to big data analytics along with security and privacy preservation for 5G and B5G applications, the integration of machine learning and blockchain, two of the most promising technologies of the modern era is inevitable. In comparison to the traditional centralized techniques for security and privacy preservation, blockchain uses decentralized consensus algorithms for verification and validation of different transactions which are supposed to become an integral part of blockchain network. Starting with the existing literature survey, we introduce the basic concepts of blockchain and machine learning in this article. Then, we presented a comprehensive taxonomy for integration of blockchain and machine learning in an IoT environment. We also explored federated learning, reinforcement learning, deep learning algorithms usage in blockchain based applications. Finally, we provide recommendations for future use cases of these emerging technologies in 5G and B5G technologies.
               ",autonomous vehicle
10.1016/j.jii.2021.100224,journal,Journal of Industrial Information Integration,sciencedirect,2021-09-30,sciencedirect,Study on artificial intelligence: The state of the art and future prospects,https://api.elsevier.com/content/article/pii/S2452414X21000248,"
                  In the world, the technological and industrial revolution is accelerating by the widespread application of new generation information and communication technologies, such as AI, IoT (the Internet of Things), and blockchain technology. Artificial intelligence has attracted much attention from government, industry, and academia. In this study, popular articles published in recent years that relate to artificial intelligence are selected and explored. This study aims to provide a review of artificial intelligence based on industry information integration. It presents an overview of the scope of artificial intelligence using background, drivers, technologies, and applications, as well as logical opinions regarding the development of artificial intelligence. This paper may play a role in AI-related research and should provide important insights for practitioners in the real world.The main contribution of this study is that it clarifies the state of the art of AI for future study.
               ",autonomous vehicle
10.1016/j.neucom.2021.06.053,journal,Neurocomputing,sciencedirect,2021-10-07,sciencedirect,Adversarial imitation learning with mixed demonstrations from multiple demonstrators,https://api.elsevier.com/content/article/pii/S0925231221009772,"
                  The aim of generative adversarial imitation learning (GAIL) is to allow an agent to learn an optimal policy from demonstrations via an adversarial training process. However, previous works have not considered a realistic setting for complex continuous control tasks such as robot manipulation, in which the available demonstrations are imperfect and possibly originate from different policies. Such a setting poses significant challenges for the application of the GAIL-related methods. This paper proposes a novel imitation learning (IL) algorithm, MD2-GAIL, to enable an agent to learn effectively from imperfect demonstrations by multiple demonstrators. Instead of training the policy from scratch, unsupervised pretraining is used to speed up the adversarial learning process. Confidence scores representing the quality of the demonstrations are utilized to reconstruct the objective function for off-policy adversarial training, making the policy match the optimal occupancy measure. Based on the Soft Actor Critic (SAC) algorithm, MD2-GAIL incorporates the idea of maximum entropy into the process of optimizing the objective function. Meanwhile, a reshaped reward function is adopted to update the agent policy to avoid falling into local optima.Experiments were conducted based on robotic simulation tasks, and the results show that our method can efficiently learn from the available demonstrations and achieves better performance than other state-of-the-art methods.
               ",autonomous vehicle
10.1016/j.jobe.2020.101816,journal,Journal of Building Engineering,sciencedirect,2021-01-31,sciencedirect,Machine learning applications for building structural design and performance assessment: State-of-the-art review,https://api.elsevier.com/content/article/pii/S2352710220334495,"
                  Machine learning models have been shown to be useful for predicting and assessing structural performance, identifying structural condition and informing preemptive and recovery decisions by extracting patterns from data collected via various sources and media. This paper presents a review of the historical development and recent advances in the application of machine learning to the area of building structural design and performance assessment. To this end, an overview of machine learning theory and the most relevant algorithms is provided with the goal of identifying problems suitable for machine learning and the appropriate models to use. The machine learning applications in building structural design and performance assessment are then reviewed in four main categories: (1) predicting structural response and performance, (2) interpreting experimental data and formulating models to predict component-level structural properties, (3) information retrieval using images and written text and (4) recognizing patterns in structural health monitoring data. The challenges of bringing machine learning into structural engineering practice are identified, and future research opportunities are discussed.
               ",autonomous vehicle
10.1016/j.compbiomed.2021.104932,journal,Computers in Biology and Medicine,sciencedirect,2021-11-30,sciencedirect,A novel self-learning framework for bladder cancer grading using histopathological images,https://api.elsevier.com/content/article/pii/S0010482521007265,"
                  In recent times, bladder cancer has increased significantly in terms of incidence and mortality. Currently, two subtypes are known based on tumour growth: non-muscle invasive (NMIBC) and muscle-invasive bladder cancer (MIBC). In this work, we focus on the MIBC subtype because it has the worst prognosis and can spread to adjacent organs. We present a self-learning framework to grade bladder cancer from histological images stained by immunohistochemical techniques. Specifically, we propose a novel Deep Convolutional Embedded Attention Clustering (DCEAC) which allows for the classification of histological patches into different levels of disease severity, according to established patterns in the literature. The proposed DCEAC model follows a fully unsupervised two-step learning methodology to discern between non-tumour, mild and infiltrative patterns from high-resolution 512 × 512 pixel samples. Our system outperforms previous clustering-based methods by including a convolutional attention module, which enables the refinement of the features of the latent space prior to the classification stage. The proposed network surpasses state-of-the-art approaches by 2–3% across different metrics, reaching a final average accuracy of 0.9034 in a multi-class scenario. Furthermore, the reported class activation maps evidence that our model is able to learn by itself the same patterns that clinicians consider relevant, without requiring previous annotation steps. This represents a breakthrough in MIBC grading that bridges the gap with respect to training the model on labelled data.
               ",autonomous vehicle
10.1016/j.jjcc.2021.10.016,journal,Journal of Cardiology,sciencedirect,2021-11-10,sciencedirect,Prospects for cardiovascular medicine using artificial intelligence,https://api.elsevier.com/content/article/pii/S0914508721002835,"As the importance of artificial intelligence (AI) in the clinical setting increases, the need for clinicians to understand AI is also increasing. This review focuses on the fundamental principles of AI and the current state of cardiovascular AI. Various types of cardiovascular AI have been developed for evaluating examinations such as X-rays, electrocardiogram, echocardiography, computed tomography, and magnetic resonance imaging. Cardiovascular AI achieves high accuracy in diagnostic support and prognosis prediction. Furthermore, it can even detect abnormalities that were previously difficult for cardiologists to detect. Randomized controlled trials begin to be reported to verify the usefulness of cardiovascular AI. The day is approaching when cardiovascular AI will be commonly used in clinical practice. Various types of medical AI will be used for cardiovascular care; however, it will not replace medical doctors. We need to understand the strengths and weaknesses of medical AI so that cardiologists can effectively use AI to improve the medical care of patients.",autonomous vehicle
10.1016/B978-0-12-824477-7.00009-2,journal,Foundations of Artificial Intelligence in Healthcare and Bioscience,sciencedirect,2021-12-31,sciencedirect,3: The science and technologies of artificial intelligence (AI),https://api.elsevier.com/content/article/pii/B9780128244777000092,"
               Artificial intelligence (AI) is a computer science and as such, it relies upon the fundamental elements of basic computing as presented in Chapter 2. Then, building upon that basic technology, AI explodes into a new science as it begins to use algorithms that introduce sophisticated processes called machine learning (ML) and deep learning (DL) creating artificial neural networks (ANN) that “mimic” the neuroscience of the human brain. This Chapter first presents the software of ML and DL, including application programming interface (API) and the multitude of mathematically-based algorithms used to scan countless databases at extraordinary speeds to answer “supervised, labeled” and “unsupervised, unlabeled” input. Also described in detail in the Chapter is the profusion of hardware, graphic processing units (GPU), accelerators, microprocessors, integrated circuitry and specialized systems from natural language processing (NLP) to quantum qubits, all representing the electronics used to drive the speeds and scope of the AI process.
            ",autonomous vehicle
10.1016/j.robot.2020.103555,journal,Robotics and Autonomous Systems,sciencedirect,2020-08-31,sciencedirect,WAGNN: A Weighted Aggregation Graph Neural Network for robot skill learning,https://api.elsevier.com/content/article/pii/S092188902030395X,"
                  Robotic skill learning suffers from the diversity and complexity of robotic tasks in continuous domains, making the learning of transferable skills one of the most challenging issues in this area, especially for the case where robots differ in terms of structure. Aiming at making the policy easier to be generalized or transferred, the graph neural networks (GNN) was previously employed to incorporate explicitly the robot structure into the policy network. In this paper, with the help of graph neural networks, we further investigate the problem of efficient learning transferable policies for robots with serial structure, which commonly appears in various robot bodies, such as robotic arms and the leg of centipede. Based on a kinematics analysis on the serial robotic structure, the policy network is improved by proposing a weighted information aggregation strategy. It is experimentally shown on different robotics structures that in a few-shot policy learning setting, the new aggregation strategy significantly improves the performance not only on the learning speed, but also on the control accuracy.
               ",autonomous vehicle
10.1016/j.rcim.2021.102231,journal,Robotics and Computer-Integrated Manufacturing,sciencedirect,2022-02-28,sciencedirect,A Survey of Robot Learning Strategies for Human-Robot Collaboration in Industrial Settings,https://api.elsevier.com/content/article/pii/S0736584521001137,"
                  Increased global competition has placed a premium on customer satisfaction, and there is a greater demand for manufacturers to be flexible with their products and services. This challenge is usually addressed with the introduction of human operators for precise tasks that require dexterity, flexibility and cognitive decision making. On the other hand, robots, through automation, are very effective in carrying out repetitive, non-ergonomic tasks. Owing to the complementary nature of robots’ and humans’ capabilities, there is an increased interest towards a shared workspace for humans and robots to work together collaboratively, forming the motivation behind the field of human-robot collaboration (HRC). Research in HRC in industry is concerned with the safety of the humans and robots, extent, and modes of collaboration among them, and the level of autonomy and adaptability of robots that can be trained for different tasks. This paper introduces a novel taxonomy of levels of interaction between humans and robots along the lines of SAEs guidelines for autonomous vehicles in response to a need for standard definitions and evolving nature of the field. Research into modes of communication for HRC driven by machine learning are reviewed followed by broad definitions of the types of machine learning. The authors also present a comprehensive review of the machine learning (ML) methodologies and industrial applications of the same in the context of adaptable collaborative robots.
               ",autonomous vehicle
10.1016/j.compchemeng.2020.106886,journal,Computers & Chemical Engineering,sciencedirect,2020-08-04,sciencedirect,A review On reinforcement learning: Introduction and applications in industrial process control,https://api.elsevier.com/content/article/pii/S0098135420300557,"
                  In recent years, reinforcement learning (RL) has attracted significant attention from both industry and academia due to its success in solving some complex problems. This paper provides an overview of RL along with tutorials for practitioners who are interested in implementing RL solutions into process control applications. The paper starts by providing an introduction to different reinforcement learning algorithms. Then, recent successes of RL applications across different industries will be explored, with more emphasis on process control applications. A detailed RL implementation example will also be shown. Afterwards, RL will be compared with traditional optimal control methods, in terms of stability and computational complexity among other factors, and the current shortcomings of RL will be introduced. This paper is concluded with a summary of RL’s potential advantages and disadvantages.
               ",autonomous vehicle
10.1016/j.cosrev.2020.100341,journal,Computer Science Review,sciencedirect,2021-05-31,sciencedirect,Machine Learning and Deep Learning in smart manufacturing: The Smart Grid paradigm,https://api.elsevier.com/content/article/pii/S157401372030441X,"
                  Industry 4.0 is the new industrial revolution. By connecting every machine and activity through network sensors to the Internet, a huge amount of data is generated. Machine Learning (ML) and Deep Learning (DL) are two subsets of Artificial Intelligence (AI), which are used to evaluate the generated data and produce valuable information about the manufacturing enterprise, while introducing in parallel the Industrial AI (IAI). In this paper, the principles of the Industry 4.0 are highlighted, by giving emphasis to the features, requirements, and challenges behind Industry 4.0. In addition, a new architecture for AIA is presented. Furthermore, the most important ML and DL algorithms used in Industry 4.0 are presented and compiled in detail. Each algorithm is discussed and evaluated in terms of its features, its applications, and its efficiency. Then, we focus on one of the most important Industry 4.0 fields, namely the smart grid, where ML and DL models are presented and analyzed in terms of efficiency and effectiveness in smart grid applications. Lastly, trends and challenges in the field of data analysis in the context of the new Industrial era are highlighted and discussed such as scalability, cybersecurity, and big data.
               ",autonomous vehicle
10.1016/j.jnca.2021.103213,journal,Journal of Network and Computer Applications,sciencedirect,2021-11-15,sciencedirect,A survey on deep learning for challenged networks: Applications and trends,https://api.elsevier.com/content/article/pii/S1084804521002149,"
                  Computer networks are dealing with growing complexity, given the ever-increasing volume of data produced by all sorts of network nodes. Performance improvements are a non-stop ambition and require tuning fine-grained details of the system operation. Analyzing such data deluge, however, is not straightforward and sometimes not supported by the system. There are often problems regarding scalability and the predisposition of the involved nodes to understand and transfer the data. This issue is at least partially circumvented by knowledge acquisition from past experiences, which is a characteristic of the herein called “challenged networks”. The addition of intelligence in these scenarios is fundamental to extract linear and non-linear relationships from the data collected by multiple sources. This is undoubtedly an invitation to machine learning and, more particularly, to deep learning.
                  This paper identifies five different challenged networks: IoT and sensor, mobile, industrial, and vehicular networks as typical scenarios that may have multiple and heterogeneous data sources and face obstacles concerning connectivity. As a consequence, deep learning solutions can contribute to system performance by adding intelligence and the ability to interpret data. We start the paper by providing an overview of deep learning, further explaining this approach’s benefits over the cited scenarios. We propose a workflow based on our observations of deep learning applications over challenged networks, and based on it, we strive to survey the literature on deep-learning-based solutions at an application-oriented level using the PRISMA methodology. Afterward, we also discuss new deep learning techniques that show enormous potential for further improvements as well as transversal issues, such as security. Finally, we provide lessons learned raising trends linking all surveyed papers to deep learning approaches. We are confident that the proposed paper contributes to the state of the art and can be a piece of inspiration for beginners and also for enthusiasts on advanced networking research.
               ",autonomous vehicle
10.1016/j.neuroscience.2021.10.001,journal,Neuroscience,sciencedirect,2021-10-14,sciencedirect,Dendritic Computing: Branching Deeper into Machine Learning,https://api.elsevier.com/content/article/pii/S0306452221005017,"
                  In this paper, we discuss the nonlinear computational power provided by dendrites in biological and artificial neurons. We start by briefly presenting biological evidence about the type of dendritic nonlinearities, respective plasticity rules and their effect on biological learning as assessed by computational models. Four major computational implications are identified as improved expressivity, more efficient use of resources, utilizing internal learning signals, and enabling continual learning. We then discuss examples of how dendritic computations have been used to solve real-world classification problems with performance reported on well known data sets used in machine learning. The works are categorized according to the three primary methods of plasticity used—structural plasticity, weight plasticity, or plasticity of synaptic delays. Finally, we show the recent trend of confluence between concepts of deep learning and dendritic computations and highlight some future research directions.
               ",autonomous vehicle
10.1016/j.nic.2020.08.007,journal,Neuroimaging Clinics of North America,sciencedirect,2020-11-30,sciencedirect,Overview of Machine Learning Part 1: Fundamentals and Classic Approaches,https://api.elsevier.com/content/article/pii/S1052514920300629,,autonomous vehicle
10.1016/j.ifacol.2020.12.2093,journal,IFAC-PapersOnLine,sciencedirect,2020-12-31,sciencedirect,Position control of a mobile robot using reinforcement learning,https://api.elsevier.com/content/article/pii/S2405896320327440,"
                  Robotics has been introduced in education at all levels during the last years. In particular, the application of mobile robots for teaching automatic control is becoming more popular in engineering because of the attractive experiments that can be performed. This paper presents the design, development, and implementation of an algorithm to control the position of a wheeled mobile robot using Reinforcement Learning in an advanced 3D simulation environment. In this approach, the learning process occurs when the agent makes some actions in the environment to get some rewards. Trying to make a balance between the new information of the environment and the current knowledge about it. In this way, the algorithm is divided into two phases: 1) the learning stage, and 2) the operational stage. In the first stage, the robot learns how to reach a known destination point from its current position. To do it, it uses the information of the environment and the rewards, to build a learning matrix that is used later during the operational stage. The main advantage of this algorithm concerning traditional control algorithms is that the learning process is carried out automatically with a recursive procedure and the result is a controller that can make the specific task, without the need for a dynamic model. Its main drawback is that the learning stage can take a long time to finish and it depends on the hardware resources of the computer used during the learning process.
               ",autonomous vehicle
10.1016/j.trip.2020.100190,journal,Transportation Research Interdisciplinary Perspectives,sciencedirect,2020-09-30,sciencedirect,Comparison of competing market mechanisms with reinforcement learning in a carpooling scenario,https://api.elsevier.com/content/article/pii/S2590198220301019,"In this paper a multi-agent simulation was implemented to analyze the dynamics of different market mechanisms with a Reinforcement Learning algorithm in the context of a carpooling market. The agents in the simulation, car owners (COs) and non car owners (NCOs), had to sell or buy a car seat for multiple rounds by picking one of two possible mechanisms: Dutch Auction or Fixed Price. In the beginning of the simulation the agents have no information about the efficiency of these mechanisms and they are chosen with the same probability. In the course of the simulation a Reinforcement Learning algorithm alters the agents' preferences for the two mechanisms depending on their cumulative payoffs. The key finding is that sellers have a clear preference for the Dutch auction mechanism with differing degrees dependent on the seller/buyer ratio. Buyers on the other hand have no significant preference for any mechanism. If these results are replicable, they suggest that an increased utilization of the Dutch auction could lead to an expansion of the carpooling market, increasing its impact as an alternative means of transportation.",autonomous vehicle
10.1016/j.xinn.2021.100179,journal,The Innovation,sciencedirect,2021-10-28,sciencedirect,Artificial Intelligence: A Powerful Paradigm for Scientific Research,https://api.elsevier.com/content/article/pii/S2666675821001041,"Artificial Intelligence (AI) coupled with promising machine learning (ML) techniques well known from computer science is broadly affecting many aspects of various fields including science and technology, industry, and even our day to day life. The ML techniques have been developed to analyze high-throughput data with a view to obtaining useful insights, categorizing, predicting and making evidence-based decisions in novel ways, which will promote the growth of novel applications and fuel the sustainable booming of AI. This paper undertakes performs a comprehensive survey on the development and application of AI in different aspects of fundamental sciences, including information science, mathematics, medical science, materials science, geoscience, life science, physics and chemistry. The challenges that each discipline of science meets, and the potentials of AI techniques to handle these challenges, are discussed in detail. Moreover, we shed light on new research trends entailing the integration of AI into each scientific discipline. The goal of this paper is to provide a broad research guideline on fundamental sciences with potential infusion of AI, to help motivate researchers to deeply understand the state-of-the-art applications of AI-based fundamental sciences, and thereby to help promote the continuous development of these fundamental sciences.",autonomous vehicle
10.1016/j.inffus.2021.05.008,journal,Information Fusion,sciencedirect,2021-12-31,sciencedirect,"A review of uncertainty quantification in deep learning: Techniques, applications and challenges",https://api.elsevier.com/content/article/pii/S1566253521001081,"Uncertainty quantification (UQ) methods play a pivotal role in reducing the impact of uncertainties during both optimization and decision making processes. They have been applied to solve a variety of real-world problems in science and engineering. Bayesian approximation and ensemble learning techniques are two widely-used types of uncertainty quantification (UQ) methods. In this regard, researchers have proposed different UQ methods and examined their performance in a variety of applications such as computer vision (e.g., self-driving cars and object detection), image processing (e.g., image restoration), medical image analysis (e.g., medical image classification and segmentation), natural language processing (e.g., text classification, social media texts and recidivism risk-scoring), bioinformatics, etc. This study reviews recent advances in UQ methods used in deep learning, investigates the application of these methods in reinforcement learning, and highlights fundamental research challenges and directions associated with UQ.",autonomous vehicle
10.1016/j.autcon.2021.103760,journal,Automation in Construction,sciencedirect,2021-09-30,sciencedirect,Classification and analysis of deep learning applications in construction: A systematic literature review,https://api.elsevier.com/content/article/pii/S0926580521002119,"
                  In recent years, the construction industry has experienced an expansion in the multitude of projects and emergent information. With the advent of deep learning, new opportunities have emerged for utilizing this vast amount of data to solve construction-related issues. While the use of deep learning has been increasing in construction, there has been no review on these applications to date. Therefore, this paper presents a Systematic Literature Review on the use of deep learning applications in construction. A total of 80 journal papers were identified and analyzed. Among these papers, six application-based topics were identified: equipment tracking, crack detection, construction work management, sewer assessment, 3D point cloud enhancement, and miscellaneous topics. Analysis shows that deep learning has been beneficial in leveraging data in areas such as crack detection and segmentation of infrastructure and sewers; equipment and worker detection and; and analysis and reporting on construction-related operations. Additionally, a discussion of the various deep learning techniques is provided as well as a contrast between deep learning, machine learning, and artificial intelligence.
               ",autonomous vehicle
10.1016/j.eswa.2021.115728,journal,Expert Systems with Applications,sciencedirect,2021-12-30,sciencedirect,A visualized bibliometric analysis of mapping research trends of machine learning in engineering (MLE),https://api.elsevier.com/content/article/pii/S095741742101109X,"
                  In this work, we conducted a visualized bibliometric analysis to map the research trends of machine learning in engineering (MLE) based on articles indexed in the Web of Science Core Collection published between 2000 and 2019. The research distributions, knowledge bases, research hotspots, and research frontiers for MLE studies are revealed by using VOSviewer software and visualization technology. The growth of the literature related to MLE averaged 24.3% in the past two decades. A total of 3057 peer-reviewed papers from 96 countries published in 1299 different journals were identified. The USA was the most productive country, with 23.73% of the overall articles and 32.25% of the overall citations. The most active research organization was MIT, with 41 publications and 1079 citations, and the Journal of Machine Learning Research had the largest number of citations in the field of MLE. In particular, our findings indicate that the research issues of “random forests”, “support vector machine”, “extreme learning machine”, “deep learning”, “statistical learning theory”, and “Python machine learning” formed the knowledge bases of MLE from 2000 to 2019, while the research hotspots focused on applications of machine learning benchmark algorithms. Burst detection analysis results showed that more burst keywords emerged and had a higher frequency of change after 2010. This study provides an insight view of the overall research trends of MLE and may help researchers better understand this research field and predict its dynamic directions.
               ",autonomous vehicle
10.1016/j.neucom.2021.10.004,journal,Neurocomputing,sciencedirect,2022-01-11,sciencedirect,A CNN-based policy for optimizing continuous action control by learning state sequences,https://api.elsevier.com/content/article/pii/S092523122101482X,"
                  Continuous action control is widespread in real-world applications. It controls an agent to take action in continuous space for transiting from one state to another until achieving the desired goal. The optimization of continuous action control is an important issue, which aims to find the optimal policy for the agent to achieve the desired goal with the lowest consumption in continuous action space. A useful tool for this issue is reinforcement learning where an optimal policy is learned for the agent by maximizing the cumulative reward of the state transitions. When updating the policy at each state, most existing reinforcement learning methods consider only the one-step transition of this state. However, for each state in continuous action control, the recognizable information is usually hidden in the sequence of its previous states, thus these methods cannot learn the policy effectively enough for continuous action control. In this paper, we propose a new policy, called convolutional deterministic policy, to solve this problem. Enlightened from the convolutional neural networks used in natural language processing, our convolutional deterministic policy uses convolutional neural networks to learn the recognizable information in the state sequences. Then for each collected state, we update the convolutional deterministic policy by not only the recognizable information in the one-step transition of this state but also the recognizable information in the sequence of its previous states. As a result, our convolutional deterministic policy can make the agent take better action. Based on an effective reinforcement learning method, TD3, the implementation of our convolutional deterministic policy is in CTD3. The theoretical analysis and the experiment illustrate that our CTD3 can learn the policy not only better than but also faster than the existing RL methods for continuous action control. The source code can be downloaded from https://github.com/grcai.
               ",autonomous vehicle
10.1016/j.dss.2018.01.001,journal,Decision Support Systems,sciencedirect,2018-03-31,sciencedirect,Detection of online phishing email using dynamic evolving neural network based on reinforcement learning,https://api.elsevier.com/content/article/pii/S0167923618300010,"
                  Despite state-of-the-art solutions to detect phishing attacks, there is still a lack of accuracy for the detection systems in the online mode which is leading to loopholes in web-based transactions. In this research, a novel framework is proposed which combines a neural network with reinforcement learning to detect phishing attacks in the online mode for the first time. The proposed model has the ability to adapt itself to produce a new phishing email detection system that reflects changes in newly explored behaviours, which is accomplished by adopting the idea of reinforcement learning to enhance the system dynamically over time. The proposed model solves the problem of limited dataset by automatically adding more emails to the offline dataset in the online mode. A novel algorithm is proposed to explore any new phishing behaviours in the new dataset. Through rigorous testing using the well-known data sets, we demonstrate that the proposed technique can handle zero-day phishing attacks with high performance levels achieving high accuracy, TPR, and TNR at 98.63%, 99.07%, and 98.19% respectively. In addition, it shows low FPR and FNR, at 1.81% and 0.93% respectively. Comparison with other similar techniques on the same dataset shows that the proposed model outperforms the existing methods.
               ",autonomous vehicle
10.1016/j.measurement.2021.108974,journal,Measurement,sciencedirect,2021-06-30,sciencedirect,Routing in wireless sensor networks using machine learning techniques: Challenges and opportunities,https://api.elsevier.com/content/article/pii/S0263224121000117,"
                  Energy conservation is the primary task in Wireless Sensor Networks (WSNs) as these tiny sensor nodes are the backbone of today’s Internet of Things (IoT) applications. These nodes rely exclusively on battery power to maneuver in hazardous environments. So, there is a requirement to study and design efficient, robust communication protocols to handle the challenges of the WSNs to make the network operational for a long time. Although traditional technologies solve many issues in WSNs, it may not derive an accurate mathematical model for predicting system behavior. So, some challenging tasks like routing, data fusion, localization, and object tracking are handled by low complexity mathematical models to define system behavior. In this paper, an effort has been made to provide a big outlook to the current “researchers” on machine learning techniques that have been employed to handle various issues in WSNs, and special attention has been given to routing problems.
               ",autonomous vehicle
10.1053/j.sodo.2021.05.007,journal,Seminars in Orthodontics,sciencedirect,2021-06-30,sciencedirect,Artificial Intelligence for radiographic image analysis,https://api.elsevier.com/content/article/pii/S107387462100030X,"
                  Automated identification of landmarks on lateral cephalogram and cone-beam computed tomography (CBCT) scans can save time for the clinicians and act as a second set of eyes for analysis of radiographic images in diagnosis and treatment planning. Several machine-learning techniques have been utilized for this purpose with varying accuracies. However, high degree of variability in the clinical presentation of orthodontic patients, limitations of the algorithms, lack of labelled data, high compute power, etc. are some drawbacks that have limited robust clinical application of such techniques. In recent years, artificial neural networks like deep learning and more specifically deep neural networks are making significant inroads in the true adoption of this technology. YOLOv3 and Single Shot Multibox Detector are some of the deep learning algorithms that have shown promising results. This paper is a theoretical review of the evolution of these technologies and the current state of the art in orthodontic image analysis.
               ",autonomous vehicle
10.1016/j.jksuci.2021.08.007,journal,Journal of King Saud University - Computer and Information Sciences,sciencedirect,2021-08-20,sciencedirect,"Reviewing the application of machine learning methods to model urban form indicators in planning decision support systems: Potential, issues and challenges",https://api.elsevier.com/content/article/pii/S131915782100210X,"Modern cities dynamically face several challenges including digitalization, sustainability, resilience and economic development. Urban planners and designers must develop urban forms that address these challenges. With the integration of new communication and information technologies (Smartphone, GIS, Drones, IoT, Sensors, etc.), urban activities have generated large volumes of urban data. The rapid growth in terms of collection and big data storage capacities combined with the ever-increasing computational power of modern machines have made possible their efficient treatment using machine (ML) and deep learning (DL) algorithms. The emergence of such groundbreaking methods has in turn helped to address the challenges of modern-day cities in several domains (health, security, mobility, etc). ML algorithms have been proposed to model the urban form’s indicators for intelligent urban planning decision making. They have been proven to perform better than the traditional methods. However, the potential of ML has not yet been fully explored in research for urban planning decision support. This paper presents a comprehensive review of ML applications for mitigating the challenges of modern cities planning. First and foremost, an overview of the urban forms, sources of urban data, the ML and DL techniques as well as their potential in solving the aforementioned challenges. For each ML method, we will highlight it working principle, advantages, disadvantages and potential applications using comparative tables. Finally, we will discuss the issues and challenges of ML methods in urban form’s modeling while ultimately advocating some future research directions.",autonomous vehicle
10.1016/j.neucom.2020.05.078,journal,Neurocomputing,sciencedirect,2020-10-14,sciencedirect,"Artificial intelligence within the interplay between natural and artificial computation: Advances in data science, trends and applications",https://api.elsevier.com/content/article/pii/S0925231220309292,"
                  Artificial intelligence and all its supporting tools, e.g. machine and deep learning in computational intelligence-based systems, are rebuilding our society (economy, education, life-style, etc.) and promising a new era for the social welfare state. In this paper we summarize recent advances in data science and artificial intelligence within the interplay between natural and artificial computation. A review of recent works published in the latter field and the state the art are summarized in a comprehensive and self-contained way to provide a baseline framework for the international community in artificial intelligence. Moreover, this paper aims to provide a complete analysis and some relevant discussions of the current trends and insights within several theoretical and application fields covered in the essay, from theoretical models in artificial intelligence and machine learning to the most prospective applications in robotics, neuroscience, brain computer interfaces, medicine and society, in general.
               ",autonomous vehicle
10.1016/j.apenergy.2020.115900,journal,Applied Energy,sciencedirect,2020-12-15,sciencedirect,Online reconfiguration scheme of self-sufficient distribution network based on a reinforcement learning approach,https://api.elsevier.com/content/article/pii/S0306261920313672,"
                  With increasing number of distributed renewable energy sources integrated in power distribution networks, network security issues such as line overloading or bus voltage violations are becoming increasingly common. Traditional capital-intensive system reinforcements could lead to overinvestment. Moreover, active network management solutions, which have emerged as important alternatives, may become a financial burden for distribution system operators or reduce profits for owners of distributed renewable energy sources, or both. To address these limitations, this paper proposes an online network reconfiguration scheme based on a deep reinforcement learning approach. In this scheme, the distribution network operator modifies the network topology to change the power flow when the reliability of network is threatened. Because the variability of distributed renewable energy is large in self-sufficient distribution networks, the reconfiguration process needs to be performed online within short time intervals, which involves the use of conventional algorithms. To solve this problem efficiently, a deep q-learning model is utilized to determine the optimal network topology. Performances of proposed and other algorithms were compared in modified CIGRE 14-bus and IEEE 123-bus test network, as well as varying penalties for frequent switching operation in consideration of physical characteristic of the network. Simulation results demonstrated that the proposed algorithm showed almost identical performances with brute-force search algorithm in both test networks, satisfying network constraints over almost all timespans. Further, the proposed method required very small computation times - under a second per each state and its scalability was verified by comparing the computation time between two test networks.
               ",autonomous vehicle
10.1016/j.compeleceng.2021.107574,journal,Computers & Electrical Engineering,sciencedirect,2021-11-04,sciencedirect,Analysis of machine learning based LEACH robust routing in the Edge Computing systems,https://api.elsevier.com/content/article/pii/S0045790621005139,"
                  Wireless sensor networks (WSN) are used to detect real-time changes in the deployed environment. This dynamic behaviour is either triggered by the deployed environment or by the user from outside. Because of their ability to monitor complex scenarios that change rapidly over time, wireless sensor networks are critical components of most advanced computing systems. These complex activities are influenced by different methods or even by the designers of their networks. Machine learning encourages many real solutions that optimise resource use and increase the network's lifespan in sensor networks. LEACH routing protocol has many limitations due to sudden energy utilisation & cluster head nodes due to direct communication with the base station node. This fast node energy leak creates several black hole structures in the networks, resulting in data redundancy, data packets transmission, node upgrade costs, and end-to-end delay for WSN. The proposed model with LEACH protocol functionality has improved network performance, network (WSN) efficiency, and solving data redundancy issues. By using an independent Recurrent Neural Network (IRNN)-based data fusion algorithm, namely, DFAIRNN. The simulation and comparative results indicate that the mean method & minimum distance method used in the LEACH-DFAIRNN protocol can effectively resolve data redundancy issues caused by the adjacent sensor nodes by flooding data simultaneously to a single node.
               ",autonomous vehicle
10.1016/j.tips.2021.06.002,journal,Trends in Pharmacological Sciences,sciencedirect,2021-09-30,sciencedirect,Disrupting 3D printing of medicines with machine learning,https://api.elsevier.com/content/article/pii/S016561472100119X,"
                  3D printing (3DP) is a progressive technology capable of transforming pharmaceutical development. However, despite its promising advantages, its transition into clinical settings remains slow. To make the vital leap to mainstream clinical practice and improve patient care, 3DP must harness modern technologies. Machine learning (ML), an influential branch of artificial intelligence, may be a key partner for 3DP. Together, 3DP and ML can utilise intelligence based on human learning to accelerate drug product development, ensure stringent quality control (QC), and inspire innovative dosage-form design. With ML’s capabilities, streamlined 3DP drug delivery could mark the next era of personalised medicine. This review details how ML can be applied to elevate the 3DP of pharmaceuticals and importantly, how it can expedite 3DP’s integration into mainstream healthcare.
               ",autonomous vehicle
10.1016/j.enbuild.2019.109675,journal,Energy and Buildings,sciencedirect,2020-02-01,sciencedirect,Study on deep reinforcement learning techniques for building energy consumption forecasting,https://api.elsevier.com/content/article/pii/S0378778819324740,"
                  Reliable and accurate building energy consumption prediction is becoming increasingly pivotal in building energy management. Currently, data-driven approach has shown promising performances and gained lots of research attention due to its efficiency and flexibility. As a combination of reinforcement learning and deep learning, deep reinforcement learning (DRL) techniques are expected to solve nonlinear and complex issues. However, very little is known about DRL techniques in forecasting building energy consumption. Therefore, this paper presents a case study of an office building using three commonly-used DRL techniques to forecast building energy consumption, namely Asynchronous Advantage Actor-Critic (A3C), Deep Deterministic Policy Gradient (DDPG) and Recurrent Deterministic Policy Gradient (RDPG). The objective is to investigate the potential of DRL techniques in building energy consumption prediction field. A comprehensive comparison between DRL models and common supervised models is also provided.
                  The results demonstrate that the proposed DDPG and RDPG models have obvious advantages in forecasting building energy consumption compared to common supervised models, while accounting for more computation time for model training. Their prediction performances measured by mean absolute error (MAE) can be improved by 16%-24% for single-step ahead prediction, and 19%-32% for multi-step ahead prediction. The results also indicate that A3C performs poor prediction accuracy and shows much slower convergence speed than DDPG and RDPG. However, A3C is still the most efficient technique among these three DRL methods. The findings are enlightening and the proposed DRL methodologies can be positively extended to other prediction problems, e.g., wind speed prediction and electricity load prediction.
               ",autonomous vehicle
10.1016/j.tcm.2021.02.002,journal,Trends in Cardiovascular Medicine,sciencedirect,2021-02-10,sciencedirect,Artificial Intelligence and Transcatheter Interventions for Structural Heart Disease: A glance at the (near) future,https://api.elsevier.com/content/article/pii/S1050173821000177,"With innovations in therapeutic technologies and changes in population demographics, transcatheter interventions for structural heart disease have become the preferred treatment and will keep growing. Yet, a thorough clinical selection and efficient pathway from diagnosis to treatment and follow-up are mandatory. In this review we reflect on how artificial intelligence may help to improve patient selection, pre-procedural planning, procedure execution and follow-up so to establish efficient and high quality health care in an increasing number of patients.",autonomous vehicle
10.1016/j.yasu.2020.05.010,journal,Advances in Surgery,sciencedirect,2020-09-30,sciencedirect,The Role of Artificial Intelligence in Surgery,https://api.elsevier.com/content/article/pii/S0065341120300178,,autonomous vehicle
10.1016/j.engappai.2020.103919,journal,Engineering Applications of Artificial Intelligence,sciencedirect,2020-10-31,sciencedirect,Policy-based reinforcement learning for time series anomaly detection,https://api.elsevier.com/content/article/pii/S0952197620302499,"
                  Time series anomaly detection has become a crucial and challenging task driven by the rapid increase of streaming data with the arrival of the Internet of Things. Existing methods are either domain-specific or require strong assumptions that cannot be met in realistic datasets. Reinforcement learning (RL), as an incremental self-learning approach, could avoid the two issues well. However, the current investigation is far from comprehensive. In this paper, we propose a generic policy-based RL framework to address the time series anomaly detection problem. The policy-based time series anomaly detector (PTAD) is progressively learned from the interactions with time-series data in the absence of constraints. Experimental results show that it outperforms the value-based temporal anomaly detector and other state-of-the-art detection methods whether training and test datasets come from the same source or not. Furthermore, the tradeoff between precision and recall is well respected by the PTAD, which is beneficial to fulfill various industrial requirements.
               ",autonomous vehicle
10.1016/j.biotechadv.2021.107739,journal,Biotechnology Advances,sciencedirect,2021-08-31,sciencedirect,Using machine learning approaches for multi-omics data analysis: A review,https://api.elsevier.com/content/article/pii/S0734975021000458,"
                  With the development of modern high-throughput omic measurement platforms, it has become essential for biomedical studies to undertake an integrative (combined) approach to fully utilise these data to gain insights into biological systems. Data from various omics sources such as genetics, proteomics, and metabolomics can be integrated to unravel the intricate working of systems biology using machine learning-based predictive algorithms. Machine learning methods offer novel techniques to integrate and analyse the various omics data enabling the discovery of new biomarkers. These biomarkers have the potential to help in accurate disease prediction, patient stratification and delivery of precision medicine. This review paper explores different integrative machine learning methods which have been used to provide an in-depth understanding of biological systems during normal physiological functioning and in the presence of a disease. It provides insight and recommendations for interdisciplinary professionals who envisage employing machine learning skills in multi-omics studies.
               ",autonomous vehicle
10.1016/j.engappai.2021.104288,journal,Engineering Applications of Artificial Intelligence,sciencedirect,2021-06-30,sciencedirect,Reinforcement learning-based application Autoscaling in the Cloud: A survey,https://api.elsevier.com/content/article/pii/S0952197621001354,"
                  Reinforcement Learning (RL) has demonstrated a great potential for automatically solving decision-making problems in complex, uncertain environments. RL proposes a computational approach that allows learning through interaction in an environment with stochastic behavior, where agents take actions to maximize some cumulative short-term and long-term rewards. Some of the most impressive results have been shown in Game Theory where agents exhibited superhuman performance in games like Go or Starcraft 2, which led to its gradual adoption in many other domains, including Cloud Computing. Therefore, RL appears as a promising approach for Autoscaling in Cloud since it is possible to learn transparent (with no human intervention), dynamic (no static plans), and adaptable (constantly updated) resource management policies to execute applications. These are three important distinctive aspects to consider in comparison with other widely used autoscaling policies that are defined in an ad-hoc way or statically computed as in solutions based on meta-heuristics. Autoscaling exploits the Cloud elasticity to optimize the execution of applications according to given optimization criteria, which demands deciding when and how to scale up/down computational resources and how to assign them to the upcoming processing workload. Such actions have to be taken considering that the Cloud is a dynamic and uncertain environment. Motivated by this, many works apply RL to the autoscaling problem in the Cloud. In this work, we exhaustively survey those proposals from major venues, and uniformly compare them based on a set of proposed taxonomies. We also discuss open problems and prospective research in the area.
               ",autonomous vehicle
10.1016/j.apenergy.2020.115036,journal,Applied Energy,sciencedirect,2020-07-01,sciencedirect,Reinforcement learning for building controls: The opportunities and challenges,https://api.elsevier.com/content/article/pii/S0306261920305481,"
                  Building controls are becoming more important and complicated due to the dynamic and stochastic energy demand, on-site intermittent energy supply, as well as energy storage, making it difficult for them to be optimized by conventional control techniques. Reinforcement Learning (RL), as an emerging control technique, has attracted growing research interest and demonstrated its potential to enhance building performance while addressing some limitations of other advanced control techniques, such as model predictive control. This study conducted a comprehensive review of existing studies that applied RL for building controls. It provided a detailed breakdown of the existing RL studies that use a specific variation of each major component of the Reinforcement Learning: algorithm, state, action, reward, and environment. We found RL for building controls is still in the research stage with limited applications (11%) in real buildings. Three significant barriers prevent the adoption of RL controllers in actual building controls: (1) the training process is time consuming and data demanding, (2) the control security and robustness need to be enhanced, and (3) the generalization capabilities of RL controllers need to be improved using approaches such as transfer learning. Future research may focus on developing RL controllers that could be used in real buildings, addressing current RL challenges, such as accelerating training and enhancing control robustness, as well as developing an open-source testbed and dataset for performance benchmarking of RL controllers.
               ",autonomous vehicle
10.1016/j.isci.2020.101656,journal,iScience,sciencedirect,2020-11-20,sciencedirect,Integrating Machine Learning with Human Knowledge,https://api.elsevier.com/content/article/pii/S2589004220308488,"Machine learning has been heavily researched and widely used in many disciplines. However, achieving high accuracy requires a large amount of data that is sometimes difficult, expensive, or impractical to obtain. Integrating human knowledge into machine learning can significantly reduce data requirement, increase reliability and robustness of machine learning, and build explainable machine learning systems. This allows leveraging the vast amount of human knowledge and capability of machine learning to achieve functions and performance not available before and will facilitate the interaction between human beings and machine learning systems, making machine learning decisions understandable to humans. This paper gives an overview of the knowledge and its representations that can be integrated into machine learning and the methodology. We cover the fundamentals, current status, and recent progress of the methods, with a focus on popular and new topics. The perspectives on future directions are also discussed.",autonomous vehicle
10.1016/j.ijepes.2021.107744,journal,International Journal of Electrical Power & Energy Systems,sciencedirect,2022-03-31,sciencedirect,Review on deep learning applications in frequency analysis and control of modern power system,https://api.elsevier.com/content/article/pii/S0142061521009686,"
                  The penetration of renewable energy resources (RES) generation and the interconnection of regional power grids in wide area and large scale have led the modern power system to exhibit more and more complex dynamic features, such as time-varying nonlinearity, uncertainty, data diversity, and local observability. The increasing complexity of power system’s dynamic characteristics makes the traditional analysis and control methods inefficient, even invalid. As a new technology path of Machine Learning, Deep learning (DL) has distinct advantages in solving complex problems such as power system frequency analysis and control due to its powerful ability of data analysis, prediction, and classification. This paper reviews the history, state of art and the future of the DL’s application in power system frequency analysis and control. Firstly, the basic principle and research progress of DL, the training methods, typical structures, and application peculiarity of DL were introduced. Secondly, the application status of DL in frequency situation awareness, frequency security and stability assessment, and frequency regulation and control were summarized, and the adaptability of DL application to each kind of issue was discussed. Finally, the development trend of DL and its application in power system frequency were prospected.
               ",autonomous vehicle
10.1016/j.ijresmar.2020.04.005,journal,International Journal of Research in Marketing,sciencedirect,2020-09-30,sciencedirect,Machine learning and AI in marketing – Connecting computing power to human insights,https://api.elsevier.com/content/article/pii/S0167811620300410,"
                  Artificial intelligence (AI) agents driven by machine learning algorithms are rapidly transforming the business world, generating heightened interest from researchers. In this paper, we review and call for marketing research to leverage machine learning methods. We provide an overview of common machine learning tasks and methods, and compare them with statistical and econometric methods that marketing researchers traditionally use. We argue that machine learning methods can process large-scale and unstructured data, and have flexible model structures that yield strong predictive performance. Meanwhile, such methods may lack model transparency and interpretability. We discuss salient AI-driven industry trends and practices, and review the still nascent academic marketing literature which uses machine learning methods. More importantly, we present a unified conceptual framework and a multi-faceted research agenda. From five key aspects of empirical marketing research: method, data, usage, issue, and theory, we propose a number of research priorities, including extending machine learning methods and using them as core components in marketing research, using the methods to extract insights from large-scale unstructured, tracking, and network data, using them in transparent fashions for descriptive, causal, and prescriptive analyses, using them to map out customer purchase journeys and develop decision-support capabilities, and connecting the methods to human insights and marketing theories. Opportunities abound for machine learning methods in marketing, and we hope our multi-faceted research agenda will inspire more work in this exciting area.
               ",autonomous vehicle
10.1016/j.jcot.2021.101573,journal,Journal of Clinical Orthopaedics and Trauma,sciencedirect,2021-11-30,sciencedirect,Real-world analysis of artificial intelligence in musculoskeletal trauma,https://api.elsevier.com/content/article/pii/S0976566221004574,"
                  Musculoskeletal trauma accounts for a large percentage of emergency room visits and is amongst the top causes of unscheduled patient visits to the emergency room. Musculoskeletal trauma results in expenditure of billions of dollars and protracted losses of quality-adjusted life years. New and innovative methods are needed to minimise the impact by ensuring quick and accurate assessment. However, each of the currently utilised radiological procedures, such as radiography, ultrasonography, computed tomography, and magnetic resonance imaging, has resulted in implosion of medical imaging data. Deep learning, a recent advancement in artificial intelligence, has demonstrated the potential to analyse medical images with sensitivity and specificity at par with experts. In this review article, we intend to summarise and showcase the various developments which have occurred in the dynamic field of artificial intelligence and machine learning and how their applicability to different aspects of imaging in trauma can be explored to improvise our existing reporting systems and improvise on patient outcomes.
               ",autonomous vehicle
10.1016/j.compenvurbsys.2018.10.008,journal,"Computers, Environment and Urban Systems",sciencedirect,2019-03-31,sciencedirect,Artificial neural networks and deep learning in urban geography: A systematic review and meta-analysis,https://api.elsevier.com/content/article/pii/S0198971518302928,"
                  Artificial neural networks (ANNs) and their latest advancement in deep learning are blooming in computer science. Geography has integrated these artificial intelligence techniques, but not with the same enthusiasm. The main reason for hesitation is that ANNs are still confronted as complex and black boxes. However, ANNs might be more solid methods than conventional approaches when dealing with complex geographical problems. This study considers the great potential of ANNs for research in urban geography. First, using the PRISMA protocol, it provides a statistical review of 140 papers on studies that employed ANNs in urban geography between 1997 and 2016. Second, it performs a quantitative meta-analysis using non-parametric bootstrapping. 45 (of the 140) papers were assessed regarding ANNs' overall accuracy (OA) achieved when used for urban growth prediction or urban land-use classification. Third, a new guideline for reporting ANNs is proposed. Statistical review indicated that ANNs performed better in 75.7% of case studies compared to conventional methods. Meta-analysis found that on bootstrapped averages, the median OA achieved when using, ANNs was higher than the median OA achieved by other techniques by 2.3% (p < .001). ANNs also performed better when used for classification compared to prediction. Analysis also identified inadequate presentation of ANNs and related results when used in urban studies. For this reason, a new guideline for reporting ANNs is suggested in this work to ensure consistency and easier dissemination of individual lessons learned. These findings aim to motivate further studies on ANNs and deep learning in urban geography.
               ",autonomous vehicle
10.1016/j.rser.2021.110889,journal,Renewable and Sustainable Energy Reviews,sciencedirect,2021-06-30,sciencedirect,"Artificial intelligence and internet of things to improve efficacy of diagnosis and remote sensing of solar photovoltaic systems: Challenges, recommendations and future directions",https://api.elsevier.com/content/article/pii/S1364032121001830,"
                  Currently, a huge number of photovoltaic plants have been installed worldwide and these plants should be carefully protected and supervised continually in order to be safe and reliable during their working lifetime. Photovoltaic plants are subject to different types of faults and failures, while available fault detection equipment are mainly used to protect and isolate the photovoltaic plants from some faults (such as arc fault, line-to-line, line-to-ground and ground faults). Although a good number of international standards (IEC, NEC, and UL) exists, undetectable faults continue to create serious problems in photovoltaic plants. Thus, designing smart equipment, including artificial intelligence and internet of things for remote sensing and fault detection and diagnosis of photovoltaic plants, will considerably solve the shortcomings of existing methods and commercialized equipment. This paper presents an overview of artificial intelligence and internet of things applications in photovoltaic plants. This research presents also the most advanced algorithms such as machine and deep learning, in terms of cost implementation, complexity, accuracy, software suitability, and feasibility of real-time applications. The embedding of artificial intelligence and internet of things techniques for fault detection and diagnosis into simple hardware, such as low-cost chips, may be economical and technically feasible for photovoltaic plants located in remote areas, with costly and challenging accessibility for maintenance. Challenging issues, recommendations, and trends of these techniques will also be presented in this paper.
               ",autonomous vehicle
10.1016/j.csbj.2021.10.006,journal,Computational and Structural Biotechnology Journal,sciencedirect,2021-12-31,sciencedirect,"Applied machine learning in cancer research: A systematic review for patient diagnosis, classification and prognosis",https://api.elsevier.com/content/article/pii/S2001037021004281,"Artificial Intelligence (AI) has recently altered the landscape of cancer research and medical oncology using traditional Machine Learning (ML) algorithms and cutting-edge Deep Learning (DL) architectures. In this review article we focus on the ML aspect of AI applications in cancer research and present the most indicative studies with respect to the ML algorithms and data used. The PubMed and dblp databases were considered to obtain the most relevant research works of the last five years. Based on a comparison of the proposed studies and their research clinical outcomes concerning the medical ML application in cancer research, three main clinical scenarios were identified. We give an overview of the well-known DL and Reinforcement Learning (RL) methodologies, as well as their application in clinical practice, and we briefly discuss Systems Biology in cancer research. We also provide a thorough examination of the clinical scenarios with respect to disease diagnosis, patient classification and cancer prognosis and survival. The most relevant studies identified in the preceding year are presented along with their primary findings. Furthermore, we examine the effective implementation and the main points that need to be addressed in the direction of robustness, explainability and transparency of predictive models. Finally, we summarize the most recent advances in the field of AI/ML applications in cancer research and medical oncology, as well as some of the challenges and open issues that need to be addressed before data-driven models can be implemented in healthcare systems to assist physicians in their daily practice.",autonomous vehicle
10.1016/j.jclepro.2021.126536,journal,Journal of Cleaner Production,sciencedirect,2021-05-15,sciencedirect,An empirical analysis of applications of artificial intelligence algorithms in wind power technology innovation during 1980–2017,https://api.elsevier.com/content/article/pii/S0959652621007563,"
                  We investigated the applications of artificial intelligence (AI) algorithms in wind power technology changes over time and found that AI accelerates the automation of wind power systems. This study shows evidence of the evolution of wind technology innovation following the advancement in AI algorithms using the patents data issued in four intellectual property (IP) offices from 1980 through 2017. Artificial intelligence and more advanced data analytics can be effectively applied to increase the efficiency of wind power systems and to optimize wind farm operations. This study empirically analyzes the evolution of applications of AI algorithms in wind power technology by employing machine learning-based text mining and network analysis, demonstrating the dynamic changing pattern of applications of AI algorithms in wind power technology innovation.
               ",autonomous vehicle
10.1016/j.jacc.2018.03.521,journal,Journal of the American College of Cardiology,sciencedirect,2018-06-12,sciencedirect,<ce:marker name=jacmegphn alt=Commentary by Dr. Valentin Fuster altimg-small=jacmegphn_s.gif altimg=jacmegphn_o.gif></ce:marker>Artificial Intelligence in Cardiology,https://api.elsevier.com/content/article/pii/S0735109718344085,"Artificial intelligence and machine learning are poised to influence nearly every aspect of the human condition, and cardiology is not an exception to this trend. This paper provides a guide for clinicians on relevant aspects of artificial intelligence and machine learning, reviews selected applications of these methods in cardiology to date, and identifies how cardiovascular medicine could incorporate artificial intelligence in the future. In particular, the paper first reviews predictive modeling concepts relevant to cardiology such as feature selection and frequent pitfalls such as improper dichotomization. Second, it discusses common algorithms used in supervised learning and reviews selected applications in cardiology and related disciplines. Third, it describes the advent of deep learning and related methods collectively called unsupervised learning, provides contextual examples both in general medicine and in cardiovascular medicine, and then explains how these methods could be applied to enable precision cardiology and improve patient outcomes.",autonomous vehicle
10.1016/j.neunet.2018.12.002,journal,Neural Networks,sciencedirect,2019-03-31,sciencedirect,Deep learning in spiking neural networks,https://api.elsevier.com/content/article/pii/S0893608018303332,"
                  In recent years, deep learning has revolutionized the field of machine learning, for computer vision in particular. In this approach, a deep (multilayer) artificial neural network (ANN) is trained, most often in a supervised manner using backpropagation. Vast amounts of labeled training examples are required, but the resulting classification accuracy is truly impressive, sometimes outperforming humans.
                  Neurons in an ANN are characterized by a single, static, continuous-valued activation. Yet biological neurons use discrete spikes to compute and transmit information, and the spike times, in addition to the spike rates, matter. Spiking neural networks (SNNs) are thus more biologically realistic than ANNs, and are arguably the only viable option if one wants to understand how the brain computes at the neuronal description level. The spikes of biological neurons are sparse in time and space, and event-driven. Combined with bio-plausible local learning rules, this makes it easier to build low-power, neuromorphic hardware for SNNs. However, training deep SNNs remains a challenge. Spiking neurons’ transfer function is usually non-differentiable, which prevents using backpropagation.
                  Here we review recent supervised and unsupervised methods to train deep SNNs, and compare them in terms of accuracy and computational cost. The emerging picture is that SNNs still lag behind ANNs in terms of accuracy, but the gap is decreasing, and can even vanish on some tasks, while SNNs typically require many fewer operations and are the better candidates to process spatio-temporal data.
               ",autonomous vehicle
10.1016/j.engappai.2020.103612,journal,Engineering Applications of Artificial Intelligence,sciencedirect,2020-05-31,sciencedirect,Incorporating domain knowledge into reinforcement learning to expedite welding sequence optimization,https://api.elsevier.com/content/article/pii/S0952197620300804,"
                  Welding Sequence Optimization (WSO) is very effective to minimize the structural deformation, however selecting proper welding sequence leads to a combinatorial optimization problem. State-of-the-art algorithms could take more than one week to compute the best sequence for an assembly of eight weld beads which is unrealistic for the early stages of Product Delivery Process (PDP). In this article, we develop and implement a novel Reinforcement Q-learning algorithm for WSO where structural deformation is used to compute reward function. We utilize a thermo-mechanical Finite Element Analysis (FEA) to predict deformation. The exploration–exploitation dilemma has been tackled by domain knowledge driven 
                        ε
                     -greedy algorithm into Q-RL which helps to expedite the WSO and we call this novel algorithm as DKQRL. We run welding simulation experiment using well-known Simufact® software on a typical widely used mounting bracket which contains eight welding beads. DKQRL allows the reduction of structural deformation up to 
                        ∼
                     71% and it substantially speeds up the computational time over Modified Lowest Cost Search (MLCS), Genetic Algorithm (GA), exhaustive search, and standard RL algorithm. Results of welding simulation demonstrate a reasonable agreement with real experiment in terms of structural deformation.
               ",autonomous vehicle
10.1016/j.neucom.2020.02.127,journal,Neurocomputing,sciencedirect,2020-09-10,sciencedirect,Reducing human efforts in video segmentation annotation with reinforcement learning,https://api.elsevier.com/content/article/pii/S0925231220308110,"Manual annotation of video segmentation datasets requires an immense amount of human effort, thus, reduction of human annotation costs is an active topic of research. While many papers deal with the propagation of masks through frames of a video, only a few results attempt to optimize annotation task selection. In this paper we present a deep learning based solution to the latter problem and train it using Reinforcement Learning. Our approach utilizes a modified version of the Dueling Deep Q-Network sharing weight parameters across the temporal axis of the video. This technique enables the trained agent to select annotation tasks from the whole video. We evaluate our annotation task selection method by means of a hierarchical supervoxel segmentation based mask propagation algorithm.",autonomous vehicle
10.1016/j.ejrad.2021.109717,journal,European Journal of Radiology,sciencedirect,2021-06-30,sciencedirect,Artificial intelligence in ultrasound,https://api.elsevier.com/content/article/pii/S0720048X21001972,"
                  Ultrasound (US), a ﬂexible green imaging modality, is expanding globally as a ﬁrst-line imaging technique in various clinical fields following with the continual emergence of advanced ultrasonic technologies and the well-established US-based digital health system. Actually, in US practice, qualified physicians should manually collect and visually evaluate images for the detection, identification and monitoring of diseases. The diagnostic performance is inevitably reduced due to the intrinsic property of high operator-dependence from US. In contrast, artificial intelligence (AI) excels at automatically recognizing complex patterns and providing quantitative assessment for imaging data, showing high potential to assist physicians in acquiring more accurate and reproducible results. In this article, we will provide a general understanding of AI, machine learning (ML) and deep learning (DL) technologies; We then review the rapidly growing applications of AI-especially DL technology in the field of US-based on the following anatomical regions: thyroid, breast, abdomen and pelvis, obstetrics heart and blood vessels, musculoskeletal system and other organs by covering image quality control, anatomy localization, object detection, lesion segmentation, and computer-aided diagnosis and prognosis evaluation; Finally, we offer our perspective on the challenges and opportunities for the clinical practice of biomedical AI systems in US.
               ",autonomous vehicle
10.1016/j.csbj.2021.07.021,journal,Computational and Structural Biotechnology Journal,sciencedirect,2021-12-31,sciencedirect,A primer on machine learning techniques for genomic applications,https://api.elsevier.com/content/article/pii/S2001037021003111,"High throughput sequencing technologies have enabled the study of complex biological aspects at single nucleotide resolution, opening the big data era. The analysis of large volumes of heterogeneous “omic” data, however, requires novel and efficient computational algorithms based on the paradigm of Artificial Intelligence. In the present review, we introduce and describe the most common machine learning methodologies, and lately deep learning, applied to a variety of genomics tasks, trying to emphasize capabilities, strengths and limitations through a simple and intuitive language. We highlight the power of the machine learning approach in handling big data by means of a real life example, and underline how described methods could be relevant in all cases in which large amounts of multimodal genomic data are available.",autonomous vehicle
10.1016/B978-0-12-823337-5.00005-6,journal,Intelligence-Based Medicine,sciencedirect,2020-12-31,sciencedirect,Chapter 5: Machine and Deep Learning,https://api.elsevier.com/content/article/pii/B9780128233375000056,"
               Machine learning along with data mining comprise sub-disciplines under data science. There are several schools of machine learning, including symbolists, connectionists, revolutionaries, Bayesians, and analogizers. Machine learning with its sometimes tedious workflow differs significantly from conventional programming. Classical machine learning consists of supervised (classification and regression) and unsupervised (clustering and generalization) learning but also semi-supervised and ensemble learning. Deep learning consists of a range of methods including convolutional neural networks, recurrent neural networks, generative adversarial networks, and their derivatives. Deep reinforcement learning (such as deep Q network, or DQN) is becoming a valuable deep learning tool in biomedicine. Evaluation of these models includes methods such as receiver operating characteristic, precision-recall curve, and the F-1 measure in the confusion matrix. Finally, issues such as explainability, bias and variance, fitting, curse of dimensionality, and correlation vs causation are discussed.
            ",autonomous vehicle
10.1016/B978-0-12-818662-6.00005-4,journal,Spinal Cord Injury Pain,sciencedirect,2022-12-31,sciencedirect,Chapter 9: Decoding nociception in the spinal cord: Computer modeling and machine learning,https://api.elsevier.com/content/article/pii/B9780128186626000054,"
               Spinal cord injury (SCI) is a significant neurologic illness that can result in devastating disability due to numerous complications of the disease. One of the most common and debilitating complications of SCI is chronic pain. Researchers have made considerable efforts to develop assessment tools for SCI-related pain and uncover factors that predispose individuals to develop chronic pain. Artificial Intelligence (AI) represents a new horizon in advanced computing technology that shows promise as an analytical tool in medicine. In recent years, AI, and in particular AI systems that utilize Machine Learning (ML) have been an active area of focus in SCI and pain research. This chapter highlights some of the advances that have been made using ML to allow computer-based algorithms to predict SCI clinical outcomes, analyze imaging data to accurately diagnose and identify neural injury, and predict whether individuals have chronic pain based on brain imaging or EEG patterns. This chapter aims to highlight how these technological advances utilizing AI and ML can be leveraged to address the unmet medical need of diagnosing and managing pain following SCI.
            ",autonomous vehicle
10.1016/j.jmoneco.2021.07.004,journal,Journal of Monetary Economics,sciencedirect,2021-09-30,sciencedirect,Deep learning for solving dynamic economic models.,https://api.elsevier.com/content/article/pii/S0304393221000799,"
                  We introduce a unified deep learning method that solves dynamic economic models by casting them into nonlinear regression equations. We derive such equations for three fundamental objects of economic dynamics – lifetime reward functions, Bellman equations and Euler equations. We estimate the decision functions on simulated data using a stochastic gradient descent method. We introduce an all-in-one integration operator that facilitates approximation of high-dimensional integrals. We use neural networks to perform model reduction and to handle multicollinearity. Our deep learning method is tractable in large-scale problems, e.g., Krusell and Smith (1998). We provide a TensorFlow code that accommodates a variety of applications.
               ",autonomous vehicle
10.1016/j.cub.2019.02.034,journal,Current Biology,sciencedirect,2019-04-01,sciencedirect,Neural network models and deep learning,https://api.elsevier.com/content/article/pii/S0960982219302040,"Originally inspired by neurobiology, deep neural network models have become a powerful tool of machine learning and artificial intelligence. They can approximate functions and dynamics by learning from examples. Here we give a brief introduction to neural network models and deep learning for biologists. We introduce feedforward and recurrent networks and explain the expressive power of this modeling framework and the backpropagation algorithm for setting the parameters. Finally, we consider how deep neural network models might help us understand brain computation.",autonomous vehicle
10.1016/j.compeleceng.2021.107373,journal,Computers & Electrical Engineering,sciencedirect,2021-10-31,sciencedirect,Potential technologies and applications based on deep learning in the 6G networks,https://api.elsevier.com/content/article/pii/S0045790621003426,"
                  The fifth generation (5G) networks provide high speed, low latency, and reliable communication services, and have been making great differences in our daily life. However, it might be difficult to meet the growing demands of Internet of Things (IoT) devices based on the existing technologies in the future, so the concept of sixth generation (6G) networks were raised to improve the existing 5G networks and promote the development of intelligent applications furthermore. Therefore, we introduce the characteristics, challenges, and potential technologies of 6G networks, and find that deep learning is potential to overcome these challenges with the help of big data and cloud computing. In addition, Deep learning approaches and potential applications in 6G networks are also discussed. We summarize the important viewpoints and studies, and provide some new ideas for developing 6G networks.
               ",autonomous vehicle
10.1016/j.bushor.2019.10.005,journal,Business Horizons,sciencedirect,2020-04-30,sciencedirect,"Machine learning for enterprises: Applications, algorithm selection, and challenges",https://api.elsevier.com/content/article/pii/S0007681319301521,"
                  Machine learning holds great promise for lowering product and service costs, speeding up business processes, and serving customers better. It is recognized as one of the most important application areas in this era of unprecedented technological development, and its adoption is gaining momentum across almost all industries. In view of this, we offer a brief discussion of categories of machine learning and then present three types of machine-learning usage at enterprises. We then discuss the trade-off between the accuracy and interpretability of machine-learning algorithms, a crucial consideration in selecting the right algorithm for the task at hand. We next outline three cases of machine-learning development in financial services. Finally, we discuss challenges all managers must confront in deploying machine-learning applications.
               ",autonomous vehicle
10.1016/j.ifacol.2021.06.012,journal,IFAC-PapersOnLine,sciencedirect,2021-12-31,sciencedirect,Three-Tier Control of Energy Costs in Transport Corporation with Self-Learning,https://api.elsevier.com/content/article/pii/S2405896321004407,"
                  According to the concept of Industry 4.0, the control of energy costs in a large transport corporation should be based on the use of artificial intelligence, the main focus of which is machine learning. It is also necessary to take into account the human factor - the unwanted activity of corporation employees who use information available only to them to achieve their own goals. The article examines the problem of reducing energy costs in a 3-tier transport corporation, which has regional offices, which are subordinate to local enterprises. The plant management knows own abilities of energy savings better than the regional management. Respectively, regional management knows these abilities better than the corporate governance. Therefore, both regional and plant management can manipulate energy expenditures to get more stimuli from the corporate governance. Such unwanted activity can result in excess energy consumption by the corporation. The goal is to minimize the energy costs of the corporation. To achieve this goal, a mechanism for organizational control of energy costs in a 3-tier transport corporation is proposed. This mechanism includes multilevel machine learning and stimulating procedures. Sufficient conditions are found for the synthesis of such a mechanism in which the existing possibilities of reducing energy costs in a transport corporation are used. The advantage of this mechanism over existing methods of learning classification in organizational systems (such as artificial neural networks or self-organizing maps) is to stimulate energy savings. The use of such a mechanism is illustrated by the example of 3-tier control system for energy consumption of JSC Russian Railways.
               ",autonomous vehicle
10.1016/j.pmatsci.2021.100797,journal,Progress in Materials Science,sciencedirect,2022-01-31,sciencedirect,"Machine learning for design, phase transformation and mechanical properties of alloys",https://api.elsevier.com/content/article/pii/S0079642521000219,"
                  Machine learning is now applied in virtually every sphere of life for data analysis and interpretation. The main strengths of the method lie in the relative ease of the construction of its structures and its ability to model complex non-linear relationships and behaviours. While application of existing materials have enabled significant technological advancement there are still needs for novel materials that will enable even greater achievement at lower cost and higher effectiveness. The physics underlining the phenomena involved in materials processing and behaviour however still pose considerable challenge and yet require solving. Machine learning can facilitate the achievement of these new aspirations and desires by learning from existing knowledge and data to fill in gaps that have so far been intractable for various reasons including cost and time. This paper reviews the applications of machine learning to various aspects of materials design, processing, characterisation, and some aspects of fabrication and environmental impact evaluation.
               ",autonomous vehicle
10.1016/j.procir.2017.12.268,journal,Procedia CIRP,sciencedirect,2018-12-31,sciencedirect,Fault pattern identification in multi-stage assembly processes with non-ideal sheet-metal parts based on reinforcement learning architecture,https://api.elsevier.com/content/article/pii/S2212827117312143,"A reinforcement learning-based architecture to address the fault detection on body in white assembly processes is introduced in this paper. During the research were addressed: (i) generation of a random defect pattern database using a multi-physics variation modeling for multi-stage assembly systems; (ii) design and implementation of a fault pattern identification reinforcement learning-based architecture, combining neural network, genetic algorithm and Q-learning algorithms; and (iii) validation based on non-ideal sheet-metal parts case study generated by the Variation Response Method toolkit. Finally, a comparative study between the different topologies is done, highlighting the influence of the Q-learning in the default identification process.",autonomous vehicle
10.1016/j.procs.2021.09.170,journal,Procedia Computer Science,sciencedirect,2021-12-31,sciencedirect,Integrating Experience-Based Knowledge Representation and Machine Learning for Efficient Virtual Engineering Object Performance,https://api.elsevier.com/content/article/pii/S1877050921019098,"Machine learning and Artificial Intelligence have grown significant attention from industry and academia during the past decade. The key reason behind interest is such technologies capabilities to revolutionize human life since they seamlessly integrate classical networks, networked objects and people to create more efficient environments. In this paper, the Knowledge Representation technique of Set of Experience Knowledge Structure (SOEKS) and Decisional DNA (DDNA) is applied to facilitate Machine Learning. For effective and efficient decision-making in Machine Learning, the environment’s own experience is captured, stored and reused using the DDNA technique. The proposed approach is implemented on practical test cases like a Chatbot. Decisional DNA gathers explicit experiential knowledge based on formal decision events and uses this knowledge to support decision-making processes. The experimental test and results of the presented implementation of Decisional DNA Chatbot case studies support it as a technology that can improve and be applied to the technology, enhancing intelligence by predicting capabilities and facilitating knowledge engineering processes.",autonomous vehicle
10.1016/j.aquaculture.2021.736724,journal,Aquaculture,sciencedirect,2021-07-15,sciencedirect,Application of machine learning in intelligent fish aquaculture: A review,https://api.elsevier.com/content/article/pii/S0044848621003860,"
                  Among the background of developments in automation and intelligence, machine learning technology has been extensively applied in aquaculture in recent years, providing a new opportunity for the realization of digital fishery farming. In the present paper, the machine learning algorithms and techniques adopted in intelligent fish aquaculture in the past five years are expounded, and the application of machine learning in aquaculture is explored in detail, including the information evaluation of fish biomass, the identification and classification of fish, behavioral analysis and prediction of water quality parameters. Further, the application of machine learning algorithms in aquaculture is outlined, and the results are analyzed. Finally, several current problems in aquaculture are highlighted, and the development trend is considered.
               ",autonomous vehicle
10.1016/j.tcm.2020.11.007,journal,Trends in Cardiovascular Medicine,sciencedirect,2020-11-23,sciencedirect,Artificial intelligence in cardiology,https://api.elsevier.com/content/article/pii/S1050173820301511,"
                  This review examines the current state and application of artificial intelligence (AI) and machine learning (ML) in cardiovascular medicine. AI is changing the clinical practice of medicine in other specialties. With progress continuing in this emerging technology, the impact for cardiovascular medicine is highlighted to provide insight for the practicing clinician and to identify potential patient benefits.
               ",autonomous vehicle
10.1016/j.survophthal.2021.08.004,journal,Survey of Ophthalmology,sciencedirect,2021-08-25,sciencedirect,Artificial intelligence applications in different imaging modalities for corneal topography,https://api.elsevier.com/content/article/pii/S0039625721001764,"
                  Interpretation of topographical maps used to detect corneal ectasias requires a high level of expertise. Several artificial intelligence (AI) technologies have attempted to interpret topographic maps. The purpose of this study is to provide a review of AI algorithms in corneal topography from the perspectives of an eye care professional, a biomedical engineer, and a data scientist. A systematic literature review using Web of Science, Pubmed, and Google Scholar was performed from 2010 to 2020 on themes regarding imaging modalities, their parameters, purpose, and conclusions and their samples and performance related to AI in corneal topography. We provide a comprehensive summary of advances in corneal imaging and its applications in AI. Combined metrics from the Dual Scheimpflug and Placido device could be a good starting point to try AI models in corneal imaging systems. The range of area under the receiving operating curve for AI in keratoconus detection and classification was from 0.87 to 1, sensitivity was from 0.89 to 1, and specificity was from 0.82 to 1. A combination of different types of AI applications to corneal ectasia diagnosis is recommended.
               ",autonomous vehicle
10.1016/j.neucom.2021.06.070,journal,Neurocomputing,sciencedirect,2021-10-07,sciencedirect,Direct training of hardware-friendly weight binarized spiking neural network with surrogate gradient learning towards spatio-temporal event-based dynamic data recognition,https://api.elsevier.com/content/article/pii/S0925231221009942,"
                  The spiking neural network (SNN)-based neuromorphic hardware has been extensively studied in dynamic information processing. However, there is still a lack of training algorithms to drive or support the implementation of compact spatio-temporal neuromorphic hardware; and the existing neuromorphic hardware uses excessive on-chip memory to store parameters, which limits its neuron and synapse scale. Here, we introduce a hardware-friendly weight binarized spiking neural network (BSNN) to efficiently recognize the spatio-temporal event-based data. The neuron of the spike response model (SRM) is used in BSNN due to its rich spatio-temporal characteristics. In the training process, a surrogate gradient method is used to replace the derivative of the spike train, and the weights are binarized. Moreover, combined with the spiking characteristics of SNN (i.e., the input/output of SNN and the communications of neurons in SNN are binary spikes), it is possible to replace the hardware-expensive matrix–vector multiplication (MVM) with the hardware-friendly “Signed AND” operation during inference, which is favored for constructing compact neuromorphic hardware. The trained BSNN has competitive recognition accuracies of 99.52% and 62.1%, 97.57%, and 90.35% on the dynamic images dataset N-MNIST and DVS-CIFAR10, dynamic gestures dataset DvsGesture, and dynamic audio dataset N-TIDIGITS18, respectively, which are related to human vision or hearing. The proposed compact SNN training method paves the way for real-time dynamic information processing oriented hardware-saving and power-efficient neuromorphic hardware.
               ",autonomous vehicle
10.1016/j.neucom.2017.09.054,journal,Neurocomputing,sciencedirect,2018-01-31,sciencedirect,On neural networks and learning systems for business computing,https://api.elsevier.com/content/article/pii/S0925231217315734,"
                  Artificial intelligence, including neural networks, deep learning and machine learning, has made numerous progress and offered new opportunity for academic research and applications in many fields, especially for business activities and firm development. This paper summarizes different applications of artificial intelligence technologies in several domains of business administration. Finance, retail industry, manufacturing industry, and enterprise management are all included. In spite of all the existing challenges, we conclude that the rapid development of artificial intelligence will show its great impact on more fields.
               ",autonomous vehicle
10.1016/j.nic.2020.04.004,journal,Neuroimaging Clinics of North America,sciencedirect,2020-08-31,sciencedirect,Artificial Intelligence in Head and Neck Imaging: A Glimpse into the Future,https://api.elsevier.com/content/article/pii/S1052514920300277,,autonomous vehicle
10.1016/j.spinee.2021.04.019,journal,The Spine Journal,sciencedirect,2021-10-31,sciencedirect,Artificial intelligence for adult spinal deformity: current state and future directions,https://api.elsevier.com/content/article/pii/S1529943021002163,"As we experience a technological revolution unlike any other time in history, spinal surgery as a discipline is poised to undergo a dramatic transformation. As enormous amounts of data become digitized and more readily available, medical professionals approach a critical juncture with respect to how advanced computational techniques may be incorporated into clinical practices. Within neurosurgery, spinal disorders in particular, represent a complex and heterogeneous disease entity that can vary dramatically in its clinical presentation and how it may impact patients’ lives. The spectrum of pathologies is extremely diverse, including many different etiologies such as trauma, oncology, spinal deformity, infection, inflammatory conditions, and degenerative disease among others. The decision to perform spine surgery, especially complex spine surgery, involves several nuances due to the interplay of biomechanical forces, bony composition, neurologic deficits, and the patient's desired goals. Adult spinal deformity as an example is one of the most complex, given its involvement of not only the spine, but rather the entirety of the skeleton in order to appreciate radiographic completeness. With the vast array of variables contributing to spinal disorders, treatment algorithms can vary significantly, and it is very difficult for surgeons to predict how patients will respond to surgery. As such, it will become imperative for spine surgeons to utilize the burgeoning availability of advanced computational tools to process unprecedented amounts of data and provide novel insights into spinal disease. These tools range from predictive models built using machine learning algorithms, to deep learning methods for imaging analysis, to natural language processing that can mine text from electronic medical records or transcribed patient visits – all to better treat the intricacies of spinal disorders. The adoption of such techniques will empower patients and propel spine surgeons into the era of personalized medicine, by allowing clinical plans to be tailored to address individual patients’ needs. This paper, which exists in the context of a larger body of literatutre, provides a comprehensive review of the current state and future of artificial intelligence and machine learning with a particular emphasis on Adult spinal deformity surgery.",autonomous vehicle
10.1016/j.clindermatol.2021.03.011,journal,Clinics in Dermatology,sciencedirect,2021-03-19,sciencedirect,Artificial intelligence in dermatology,https://api.elsevier.com/content/article/pii/S0738081X21000560,"
                  Dermatology and medicine are producing data at an increasing rate that are progressively difficult to sort and manage. Artificial intelligence (AI) and machine learning are examples of tools that may have the capability to produce significant and meaningful results from these data. Currently, AI and machine learning have a variety of applications in medicine including, but not limited to, diagnostics, patient management, preventive medicine, and genomic analysis. Although the role of AI in dermatology is greater than ever, its use is still extremely limited. As AI is continually developed and implemented, it is essential that stakeholders understand AI terminology, applications, limitations, and projected uses in dermatology. With the continued development of AI technology, however, its implementation may afford greater dermatologist efficiency, greater increased patient access to dermatologic care, and improved patient outcomes.
               ",autonomous vehicle
10.1016/j.drudis.2021.09.006,journal,Drug Discovery Today,sciencedirect,2021-09-20,sciencedirect,Artificial intelligence-enhanced drug design and development: Toward a computational precision medicine,https://api.elsevier.com/content/article/pii/S1359644621003962,"
                  Artificial Intelligence (AI) relies upon a convergence of technologies with further synergies with life science technologies to capture the value of massive multi-modal data in the form of predictive models supporting decision-making. AI and machine learning (ML) enhance drug design and development by improving our understanding of disease heterogeneity, identifying dysregulated molecular pathways and therapeutic targets, designing and optimizing drug candidates, as well as evaluating in silico clinical efficacy. By providing an unprecedented level of knowledge on both patient specificities and drug candidate properties, AI is fostering the emergence of a computational precision medicine allowing the design of therapies or preventive measures tailored to the singularities of individual patients in terms of their physiology, disease features, and exposure to environmental risks.
               ",autonomous vehicle
10.1016/j.jmrt.2021.07.004,journal,Journal of Materials Research and Technology,sciencedirect,2021-10-31,sciencedirect,Machine learning in predicting mechanical behavior of additively manufactured parts,https://api.elsevier.com/content/article/pii/S2238785421006670,"Although applications of additive manufacturing (AM) have been significantly increased in recent years, its broad application in several industries is still under progress. AM also known as three-dimensional (3D) printing is layer by layer manufacturing process which can be used for fabrication of geometrically complex customized functional end-use products. Since AM processing parameters have significant effects on the performance of the printed parts, it is necessary to tune these parameters which is a difficult task. Today, different artificial intelligence techniques have been utilized to optimize AM parameters and predict mechanical behavior of 3D-printed components. In the present study, applications of machine learning (ML) in prediction of structural performance and fracture of additively manufactured components has been presented. This study first outlines an overview of ML and then summarizes its applications in AM. The main part of this review, focuses on applications of ML in prediction of mechanical behavior and fracture of 3D-printed parts. To this aim, previous research works which investigated application of ML in characterization of polymeric and metallic 3D-printed parts have been reviewed and discussed. Moreover, the review and analysis indicate limitations, challenges, and perspectives for industrial applications of ML in the field of AM. Considering advantages of ML increase in applications of ML in optimization of 3D printing parameters, prediction of mechanical performance, and evaluation of 3D-printed products is expected.",autonomous vehicle
10.1016/j.vlsi.2020.11.006,journal,Integration,sciencedirect,2021-03-31,sciencedirect,"Review: Machine learning techniques in analog/RF integrated circuit design, synthesis, layout, and test",https://api.elsevier.com/content/article/pii/S0167926020302947,"
                  Rapid developments in semiconductor technology have substantially increased the computational capability of computers. As a result of this and recent developments in theory, machine learning (ML) techniques have become attractive in many new applications. This trend has also inspired researchers working on integrated circuit (IC) design and optimization. ML-based design approaches have gained importance to challenge/aid conventional design methods since they can be employed at different design levels, from modeling to test, to learn any nonlinear input-output relationship of any analog and radio frequency (RF) device or circuit; thus, providing fast and accurate responses to the task that they have learned. Furthermore, employment of ML techniques in analog/RF electronic design automation (EDA) tools boosts the performance of such tools. In this paper, we summarize the recent research and present a comprehensive review on ML techniques for analog/RF circuit modeling, design, synthesis, layout, and test.
               ",autonomous vehicle
10.1016/B978-0-12-821229-5.00005-7,journal,Machine Learning and the Internet of Medical Things in Healthcare,sciencedirect,2021-12-31,sciencedirect,Chapter 1: Machine learning architecture and framework,https://api.elsevier.com/content/article/pii/B9780128212295000057,"
               Machine Learning (ML) is a branch of Artificial Intelligence that enables computer systems to learn from past experiences and improve accordingly without the direct intervention of the programmer. ML enables machines to behave very similarly to human beings. In order to extract the required information from the huge amount of data, ML can be used to design algorithms based on the trends of data and relationships among the data. ML can be applied in various fields such as intrusion detection, bioinformatics, health care, marketing, game playing, and so on. It enables the computers or the machines to make data-driven decisions rather than being explicitly programmed for carrying out a certain task. These programs or algorithms are designed in a way that they learn and improve over time when they are exposed to new or unseen data. Due to the huge amount of data, the significance of ML can be seen in various sections of the society. Especially in industries, ML is assisting exploration of the hidden patterns of the data, and through this the overall performance of the business can be improved. It is cost-effective, affordable, and simple computing techniques allow the analysis and handling of a huge amount of complex data. ML is not only helping to understand and identify the hidden patterns of a diverse set of data but also encourages automation in analysis in place of humans. Also, ML is helping industries to avail of the opportunities and make it profitable in future endeavors.
               In this chapter, we first review the fundamental concepts of machine learning such as feature assessment, unsupervised versus supervised learning, and types of classification. Then, details of the ML architecture and framework are discussed.
            ",autonomous vehicle
10.1016/j.jnca.2020.102576,journal,Journal of Network and Computer Applications,sciencedirect,2020-05-01,sciencedirect,Machine learning for intelligent optical networks: A comprehensive survey,https://api.elsevier.com/content/article/pii/S1084804520300503,"
                  With the rapid development of Internet and communication systems, both in the aspect of services and technologies, communication networks have been suffering increasing complexity. It is imperative to improve intelligence in communication networks, and several aspects have been incorporating with Artificial Intelligence (AI) and Machine Learning (ML). The optical network, which plays an important role both in core and access network in communication networks, also faces great challenges of system complexity and the requirement of manual operations. To overcome the current limitations and address the issues of future optical networks, it is essential to deploy more intelligence capability to enable autonomous and flexible network operations. ML techniques are proved to have superiority on solving complex problems, and thus recently, ML techniques have been used for many optical network applications. In this paper, a detailed survey of existing applications of ML for intelligent optical networks is presented. The applications of ML are classified in terms of their use cases, which are categorised into optical network control and resource management, and optical network monitoring and survivability. These applications are analyzed and compared according to the used ML techniques. Besides, a tutorial for ML applications is provided from the aspects of the introduction of common ML algorithms, paradigms of ML, and motivations of applying ML. Lastly, challenges and possible solutions of ML application in intelligent optical networks are also discussed, which intends to inspire future innovations in leveraging ML to build intelligent optical networks.
               ",autonomous vehicle
10.1016/j.dcan.2021.09.001,journal,Digital Communications and Networks,sciencedirect,2021-09-05,sciencedirect,Machine learning-based zero-touch network and service management: A survey,https://api.elsevier.com/content/article/pii/S2352864821000614,"The exponential growth of mobile applications and services during the last years has challenged the existing network infrastructures. Consequently, the arrival of multiple management solutions to cope with this explosion along the end-to-end network chain has increased the complexity in the coordinated orchestration of different segments composing the whole infrastructure. The Zero-touch Network and Service Management (ZSM) concept has recently emerged to automatically orchestrate and manage network resources while assuring the Quality of Experience (QoE) demanded by users. Machine Learning (ML) is one of the key enabling technologies that many ZSM frameworks are adopting to bring intelligent decision making to the network management system. This paper presents a comprehensive survey of the state-of-the-art application of ML-based techniques to improve ZSM performance. To this end, the main related standardization activities and the aligned international projects and research efforts are deeply examined. From this dissection, the skyrocketing growth of the ZSM paradigm can be observed. Concretely, different standardization bodies have already designed reference architectures to set the foundations of novel automatic network management functions and resource orchestration. Aligned with these advances, diverse ML techniques are being currently exploited to build further ZSM developments in different aspects, including multi-tenancy management, traffic monitoring, and architecture coordination, among others. However, different challenges, such as the complexity, scalability, and security of ML mechanisms, are also identified, and future research guidelines are provided to accomplish a firm development of the ZSM ecosystem.",autonomous vehicle
10.1016/j.cognition.2020.104365,journal,Cognition,sciencedirect,2020-10-31,sciencedirect,Deep learning and cognitive science,https://api.elsevier.com/content/article/pii/S0010027720301840,"
                  In recent years, the family of algorithms collected under the term “deep learning” has revolutionized artificial intelligence, enabling machines to reach human-like performances in many complex cognitive tasks. Although deep learning models are grounded in the connectionist paradigm, their recent advances were basically developed with engineering goals in mind. Despite of their applied focus, deep learning models eventually seem fruitful for cognitive purposes. This can be thought as a kind of biological exaptation, where a physiological structure becomes applicable for a function different from that for which it was selected. In this paper, it will be argued that it is time for cognitive science to seriously come to terms with deep learning, and we try to spell out the reasons why this is the case. First, the path of the evolution of deep learning from the connectionist project is traced, demonstrating the remarkable continuity, and the differences as well. Then, it will be considered how deep learning models can be useful for many cognitive topics, especially those where it has achieved performance comparable to humans, from perception to language. It will be maintained that deep learning poses questions that cognitive sciences should try to answer. One of such questions is the reasons why deep convolutional models that are disembodied, inactive, unaware of context, and static, are by far the closest to the patterns of activation in the brain visual system.
               ",autonomous vehicle
10.1016/j.neunet.2014.09.003,journal,Neural Networks,sciencedirect,2015-01-31,sciencedirect,Deep learning in neural networks: An overview,https://api.elsevier.com/content/article/pii/S0893608014002135,"
                  In recent years, deep artificial neural networks (including recurrent ones) have won numerous contests in pattern recognition and machine learning. This historical survey compactly summarizes relevant work, much of it from the previous millennium. Shallow and Deep Learners are distinguished by the depth of their credit assignment paths, which are chains of possibly learnable, causal links between actions and effects. I review deep supervised learning (also recapitulating the history of backpropagation), unsupervised learning, reinforcement learning & evolutionary computation, and indirect search for short programs encoding deep and large networks.
               ",autonomous vehicle
10.1016/B978-0-12-821777-1.00023-9,journal,"Machine Learning, Big Data, and IoT for Medical Informatics",sciencedirect,2021-12-31,sciencedirect,Chapter 1: Predictive analytics and machine learning for medical informatics: A survey of tasks and techniques,https://api.elsevier.com/content/article/pii/B9780128217771000239,"
               In this chapter, we survey machine learning and predictive analytics methods for medical informatics. We begin by surveying the current state of practice, key task definitions, and open research problems related to predictive modeling in diagnostic medicine. This follows the traditional supervised, unsupervised, and reinforcement learning taxonomy. Next, we review current research on semisupervised, active, and transfer learning, and on differentiable computing methods such as deep learning. The focus of this chapter is on deep neural networks with common use cases in computational medicine, including self-supervised learning scenarios: these include convolutional neural networks for image analysis, recurrent neural networks for time series, and generative adversarial models for correction of class imbalance in differential diagnosis and anomaly detection. We then continue by assessing salient connections between the current state of machine learning research and data-centric healthcare analytics, focusing specifically on diagnostic imaging and multisensor integration as crucial research topics within predictive analytics. This section includes synthesis experiments on analytics and multisensor data fusion within a diagnostic test bed. Finally, we conclude by relating open problems of machine learning for prediction-based medical informatics surveyed in this chapter to the impact of big data and its associated challenges, trends, and limitations of current work, including privacy and security of sensitive patient data.
            ",autonomous vehicle
10.1016/j.isatra.2020.01.016,journal,ISA Transactions,sciencedirect,2020-06-30,sciencedirect,Fuzzy reinforcement learning based intelligent classifier for power transformer faults,https://api.elsevier.com/content/article/pii/S0019057820300161,"
                  In this work a fuzzy reinforcement learning (RL) based intelligent classifier for power transformer incipient faults is proposed. Fault classifiers proposed till date have low identification accuracy and do not identify all types of transformer faults. Herein, an attempt has been made to design an adaptive, intelligent transformer fault classifier that progressively learns to identify faults on-line with high accuracy for all fault types. In the proposed approach, dissolved gas analysis (DGA) data of oil samples collected from real power transformers (and from credible sources) has been used, which serves as input to a fuzzy RL based classifier. Typically, classification accuracy is heavily dependent on the number of input variables chosen. This has been resolved by using the J48 algorithm to select 8 most appropriate input variables from the 24 variables obtained using DGA. Proposed fuzzy RL approach achieves a fault identification accuracy of 99.7%, which is significantly higher than other contemporary soft computing based identifiers. Experimental results and comparison with other state-of-the-art approaches, highlights superiority and efficacy of the proposed fuzzy RL technique for transformer fault classification.
               ",autonomous vehicle
10.1016/B978-0-12-821777-1.00023-9,journal,"Machine Learning, Big Data, and IoT for Medical Informatics",sciencedirect,2021-12-31,sciencedirect,Chapter 1: Predictive analytics and machine learning for medical informatics: A survey of tasks and techniques,https://api.elsevier.com/content/article/pii/B9780128217771000239,"
               In this chapter, we survey machine learning and predictive analytics methods for medical informatics. We begin by surveying the current state of practice, key task definitions, and open research problems related to predictive modeling in diagnostic medicine. This follows the traditional supervised, unsupervised, and reinforcement learning taxonomy. Next, we review current research on semisupervised, active, and transfer learning, and on differentiable computing methods such as deep learning. The focus of this chapter is on deep neural networks with common use cases in computational medicine, including self-supervised learning scenarios: these include convolutional neural networks for image analysis, recurrent neural networks for time series, and generative adversarial models for correction of class imbalance in differential diagnosis and anomaly detection. We then continue by assessing salient connections between the current state of machine learning research and data-centric healthcare analytics, focusing specifically on diagnostic imaging and multisensor integration as crucial research topics within predictive analytics. This section includes synthesis experiments on analytics and multisensor data fusion within a diagnostic test bed. Finally, we conclude by relating open problems of machine learning for prediction-based medical informatics surveyed in this chapter to the impact of big data and its associated challenges, trends, and limitations of current work, including privacy and security of sensitive patient data.
            ",autonomous vehicle
10.1016/j.renene.2020.05.060,journal,Renewable Energy,sciencedirect,2020-09-30,sciencedirect,Improving response of wind turbines by pitch angle controller based on gain-scheduled recurrent ANFIS type 2 with passive reinforcement learning,https://api.elsevier.com/content/article/pii/S0960148120307588,"
                  In this paper, passive reinforcement learning (RL) solved by particle swarm optimization policy (PSO–P) is used to handle an adaptive neuro-fuzzy inference system (ANFIS) type-2 structure with unsupervised clustering for controlling the pitch angle of a real wind turbine (WT). The proposed control scheme is based on gain-scheduled reinforcement learning recurrent ANFIS type 2 (GS-RL-RANFIST2) pitch angle controller to maintain the rotor speed at its rated value while smoothing the output power and the performance of the pitch angle system. The practical application of the proposed controller is evaluated by using FAST tool for a real 600 kW WT equipped with a synchronous generator with a full-size power converter (CART3, located at the National Renewable Energy Laboratory, NREL), whose results are compared with those obtained by a gain corrected proportional integral (GC-PI) controller. The results demonstrate that the GS-RL-RANFIST2, which sets the nonlinear characteristics of the system automatically and waves more uncertainties in the windy conditions, allows to increase the energy capture and smooth the output power fluctuation, and therefore, to improve the control and response of the WT.
               ",autonomous vehicle
10.1016/j.asoc.2021.107391,journal,Applied Soft Computing,sciencedirect,2021-08-31,sciencedirect,A machine learning approach combining expert knowledge with genetic algorithms in feature selection for credit risk assessment,https://api.elsevier.com/content/article/pii/S1568494621003148,"
                  Most credit scoring algorithms are designed with the assumption to be executed in an environment characterized by an automatic processing of credit applications, without considering the input of expert opinions. Since in credit scoring applications expert opinions have been proved very helpful, in this work, we propose a combination strategy of integrating soft computing methods with expert knowledge. The ability of interpretation of the predictive power of each feature in the credit dataset is strengthened by the engagement of experts in the credit scoring process, and the proposed wrapper-based feature selection approach which explores how the features contributing most towards the classification of borrowers. In particular, an unsupervised machine learning algorithm allows experts to create one or more clustering scenarios regarding the features by defining a desired number of clusters per scenario. For each clustering scenario, the Analytic Hierarchy Process is applied for helping experts to make preferences for features of each cluster, set pair-wise constraints for features of equal importance, and evaluate their subjective judgments in terms of consistency. Then, expert opinions are taken into consideration for solving a credit scoring problem in the form of an optimization problem subject to constraints via soft computing methods based on supervised machine learning and evolutionary optimization algorithms. Testing instances on standard credit dataset are established to verify the effectiveness of the proposed methodology.
               ",autonomous vehicle
10.1016/j.measurement.2021.110460,journal,Measurement,sciencedirect,2021-11-14,sciencedirect,Autoencoder-based Representation Learning and Its Application in Intelligent Fault Diagnosis: A Review,https://api.elsevier.com/content/article/pii/S0263224121013464,"
                  With the increase of the scale and complexity of mechanical equipment, traditional intelligent fault diagnosis (IFD) based on shallow machine learning methods is unable to meet the demand of coupling faults. In the past decades, the vigorous development of deep learning (DL) brings new opportunities for IFD, especially the representation learning based on Autoencoder (AE) theory has been widely applied. To provide a more comprehensive reference, the theoretical foundations of multi-type AEs and the training method of stacked autoencoder (SAE) are briefly introduced. Then the application advances of AE are reviewed from optimization and combination aspects, which are aiming at improving the representation learning ability. To provide ways for the application of AE-based methods, two typical study cases for ideal and complex engineering systems are illustrated respectively. Finally, the challenges and prospects of AE-based representation learning are reported from four aspects, which give a guidance for the future research direction.
               ",autonomous vehicle
10.1016/j.procir.2020.05.163,journal,Procedia CIRP,sciencedirect,2020-12-31,sciencedirect,Deep reinforcement learning-based dynamic scheduling in smart manufacturing,https://api.elsevier.com/content/article/pii/S2212827120307708,"Scheduling problems are a classic type of optimization problems in the manufacturing domain, such as job shop scheduling, flexible job shop scheduling, and distributed job shop scheduling problems. Especially, the dynamic task scheduling problem is closer to the requirements of real manufacturing systems than the static scheduling problem. In recent years, with the deeper application of the Internet of Things, big data, and cloud platform technologies, manufacturing systems have evolved from job shops to networked, collaborative and intelligent manufacturing systems. Smart manufacturing scheduling has some characteristics compared with job shop scheduling not only because of the larger number of tasks and services, but also because of the dynamic state of services and uncertainties. In this paper, we analyze the smart manufacturing service scheduling problem and give its mathematical description. Then a deep reinforcement learning-based method is proposed to minimize the maximum completion time of all tasks. In the system framework of the proposed method, the agent, environment, as well as the interaction between them, are designed. The queue times of all candidate services are considered as the system state, and the maximum queue time at the current moment is considered as the target value. Besides, we use two networks the prediction network and the target network to learn the prediction value and the target value, respectively. Two case studies are used to show the efficiency of the considered problem and the proposed method.",autonomous vehicle
10.1016/j.arcontrol.2018.09.005,journal,Annual Reviews in Control,sciencedirect,2018-12-31,sciencedirect,"Reinforcement learning for control: Performance, stability, and deep approximators",https://api.elsevier.com/content/article/pii/S1367578818301184,"
                  Reinforcement learning (RL) offers powerful algorithms to search for optimal controllers of systems with nonlinear, possibly stochastic dynamics that are unknown or highly uncertain. This review mainly covers artificial-intelligence approaches to RL, from the viewpoint of the control engineer. We explain how approximate representations of the solution make RL feasible for problems with continuous states and control actions. Stability is a central concern in control, and we argue that while the control-theoretic RL subfield called adaptive dynamic programming is dedicated to it, stability of RL largely remains an open question. We also cover in detail the case where deep neural networks are used for approximation, leading to the field of deep RL, which has shown great success in recent years. With the control practitioner in mind, we outline opportunities and pitfalls of deep RL; and we close the survey with an outlook that – among other things – points out some avenues for bridging the gap between control and artificial-intelligence RL techniques.
               ",autonomous vehicle
10.1016/j.tej.2020.106880,journal,The Electricity Journal,sciencedirect,2021-02-28,sciencedirect,Artificial intelligence for resilience enhancement of power distribution systems,https://api.elsevier.com/content/article/pii/S104061902030172X,"
                  The threat of high impact low probability (HILP) events on power distribution system is substantial but quite unpredictable. Enhancing the resilience of power distribution grids against such events requires solving combinatorial planning and operational problems in stochastic spaces, as well as classifying system conditions based on high-dimensional input data. Since traditional mathematical solutions struggle with both uncertainty and the curse of dimensionality, data-driven techniques based on artificial intelligence (AI) are gaining momentum for solving those problems. This paper reviews AI capabilities for decision making in uncertain and high-dimensional spaces in general, and their particular application in resilient enhancement problems such as damage detection and estimation, cyber-physical anomaly detection, stochastic operation, and cyber security enhancement. Efficient data structures and AI approaches are suggested for each problem, which depend on the type of input signals, search-based or game-based structure of the problem, as well as the uncertainty sources involved. In particular, potential applications of supervised and unsupervised deep learning combined with Monte Carlo Tree Search and 
                        ε
                     -greedy search is explored to find near optimal operational decisions that help enhance the resilience of power distribution systems.
               ",autonomous vehicle
10.1016/j.matpr.2021.07.089,journal,Materials Today: Proceedings,sciencedirect,2021-07-17,sciencedirect,Investigations on optimizing performance of the distributed computing in heterogeneous environment using machine learning technique for large scale data set,https://api.elsevier.com/content/article/pii/S2214785321049415,"
                  Enforcing admired machine learning approaches to huge data enhanced novel issues for researchers. Conventional libraries could not suitably fulfil the requirement of complex model with wide variety of data and system parameters. Therefore new methodologies are required performing the computation on more than one machine over distributed environment. Some distributed frameworks on huge data such as MapReduce and TensorFlow have been deployed to solve various machine learning problems in heterogeneous distributed environment. The objective of this paper is providing a wide variety of useful information about platforms, approaches, problems, datasets, and optimization approaches in distributed systems. So researchers have been used the beneficial information to develop new approaches for efficient machine learning. This paper also covers the various formats of data like structured, semi structured and unstructured big data. A brief review of previous works is also represented in text and tabular format to provide motivation to the researchers for developing new paradigm of distributed computing environment.
               ",autonomous vehicle
10.1016/j.ifacol.2020.12.712,journal,IFAC-PapersOnLine,sciencedirect,2020-12-31,sciencedirect,Deep Learning in Mining and Mineral Processing Operations: A Review,https://api.elsevier.com/content/article/pii/S2405896320310296,"
                  In this paper, the application of deep learning in the mining and processing of ores is reviewed. Deep learning is strongly impacting the development of sensor systems, particularly computer vision systems used in mining and mineral processing automation, where it is filling a gap not currently achievable by traditional approaches. To a lesser extent, deep learning is also being considered in the automation of decision support systems. There is significant scope for the application of deep learning to improve operations, but access to industrial data and big data infrastructure in operational environments are critical bottlenecks to the development and deployment of the technology.
               ",autonomous vehicle
10.1016/j.jobe.2021.103299,journal,Journal of Building Engineering,sciencedirect,2021-12-31,sciencedirect,"Artificial intelligence in the construction industry: A review of present status, opportunities and future challenges",https://api.elsevier.com/content/article/pii/S2352710221011578,"The growth of the construction industry is severely limited by the myriad complex challenges it faces such as cost and time overruns, health and safety, productivity and labour shortages. Also, construction industry is one the least digitized industries in the world, which has made it difficult for it to tackle the problems it currently faces. An advanced digital technology, Artificial Intelligence (AI), is currently revolutionising industries such as manufacturing, retail, and telecommunications. The subfields of AI such as machine learning, knowledge-based systems, computer vision, robotics and optimisation have successfully been applied in other industries to achieve increased profitability, efficiency, safety and security. While acknowledging the benefits of AI applications, numerous challenges which are relevant to AI still exist in the construction industry. This study aims to unravel AI applications, examine AI techniques being used and identify opportunites and challenges for AI applications in the construction industry. A critical review of available literature on AI applications in the construction industry such as activity monitoring, risk management, resource and waste optimisation was conducted. Furthermore, the opportunities and challenges of AI applications in construction were identified and presented in this study. This study provides insights into key AI applications as it applies to construction-specific challenges, as well as the pathway to realise the acrueable benefits of AI in the construction industry.",autonomous vehicle
10.1016/j.elerap.2020.100987,journal,Electronic Commerce Research and Applications,sciencedirect,2020-10-31,sciencedirect,Learning pareto optimal solution of a multi-attribute bilateral negotiation using deep reinforcement,https://api.elsevier.com/content/article/pii/S1567422320300648,"
                  This paper aims to design an intelligent buyer to learn how to decide in an incomplete information multi-attribute bilateral simultaneous negotiation. The buyer does not know the negotiation strategy of the seller and only have access to the historical data of the previous negotiations. Using the historical data and clustering method, the type of seller is identified online during the negotiation. Then, the deep reinforcement learning method is utilized to support the buyer to learn its optimal decision. In the complete information case, we prove that the negotiation admits a unique Nash bargaining solution with possibly asymmetric negotiation powers. In comprehensive simulation studies, the efficiency of the proposed learning agent is evaluated in different scenarios and we show that the learning negotiation with incomplete information is converged to a Pareto optimal solution. Then, using the concept of the Nash bargaining solution, the negotiation power of the buyer is assessed in negotiation.
               ",autonomous vehicle
10.1016/j.procir.2021.03.050,journal,Procedia CIRP,sciencedirect,2021-12-31,sciencedirect,Automatised quality assessment in additive layer manufacturing using layer-by-layer surface measurements and deep learning,https://api.elsevier.com/content/article/pii/S2212827121003267,"Additive manufacturing (AM) has gained high research interests in the past but comes with some drawbacks, such as the difficulty to do in-situ quality monitoring. In this paper, deep learning is used on electron-optical images taken during the Electron Beam Melting (EBM) process to classify the quality of AM layers to achieve automatized quality assessment. A comparative study of several mainstream Convolutional Neural Networks to classify the images has been conducted. The classification accuracy is up to 95 %, which demonstrates the great potential to support in-process layer quality control of EBM.And the error analysis has shown that some human misclassification were correctly classified by the Convolutional Neural Networks.",autonomous vehicle
10.1016/B978-0-12-818234-5.00149-8,journal,Reference Module in Earth Systems and Environmental Sciences,sciencedirect,2021-12-31,sciencedirect,Machine Learning for Environmental Sensing,https://api.elsevier.com/content/article/pii/B9780128182345001498,"
               Machine learning has found many applications in Earth Science. These applications range from retrieval algorithms, from code acceleration to calibration of low cost sensors, from classification of dust sources, to rock type classification. As a broad sub-field of artificial intelligence, machine learning is concerned with algorithms and techniques that allow computers to “learn” by example. The major focus of machine learning is to extract information from data automatically by computational and statistical methods. Over the last decade there has been considerable progress in developing a machine-learning methodology for a variety of science applications involving trace gases, retrievals, aerosol products, land-surface products, vegetation indices, and most recently, ocean applications. In this article, we will review some examples of how machine learning has already been used in science. Machine learning can and has been used for a variety of applications including new data product creation, to bias correction, to data classification, for software defined sensing and in autonomous robotic teams.
            ",autonomous vehicle
10.1016/j.egyai.2021.100049,journal,Energy and AI,sciencedirect,2021-03-31,sciencedirect,Machine learning for advanced energy materials,https://api.elsevier.com/content/article/pii/S2666546821000033,"The screening of advanced materials coupled with the modeling of their quantitative structural-activity relationships has recently become one of the hot and trending topics in energy materials due to the diverse challenges, including low success probabilities, high time consumption, and high computational cost associated with the traditional methods of developing energy materials. Following this, new research concepts and technologies to promote the research and development of energy materials become necessary. The latest advancements in artificial intelligence and machine learning have therefore increased the expectation that data-driven materials science would revolutionize scientific discoveries towards providing new paradigms for the development of energy materials. Furthermore, the current advances in data-driven materials engineering also demonstrate that the application of machine learning technology would not only significantly facilitate the design and development of advanced energy materials but also enhance their discovery and deployment. In this article, the importance and necessity of developing new energy materials towards contributing to the global carbon neutrality are presented. A comprehensive introduction to the fundamentals of machine learning is also provided, including open-source databases, feature engineering, machine learning algorithms, and analysis of machine learning model. Afterwards, the latest progress in data-driven materials science and engineering, including alkaline ion battery materials, photovoltaic materials, catalytic materials, and carbon dioxide capture materials, is discussed. Finally, relevant clues to the successful applications of machine learning and the remaining challenges towards the development of advanced energy materials are highlighted.",autonomous vehicle
10.1016/j.compchemeng.2019.04.003,journal,Computers & Chemical Engineering,sciencedirect,2019-07-12,sciencedirect,Advances and opportunities in machine learning for process data analytics,https://api.elsevier.com/content/article/pii/S0098135419302248,"
                  In this paper we introduce the current thrust of development in machine learning and artificial intelligence, fueled by advances in statistical learning theory over the last 20 years and commercial successes by leading big data companies. Then we discuss the characteristics of process manufacturing systems and briefly review the data analytics research and development in the last three decades. We give three attributes for process data analytics to make machine learning techniques applicable in the process industries. Next we provide a perspective on the currently active topics in machine learning that could be opportunities for process data analytics research and development. Finally we address the importance of a data analytics culture. Issues discussed range from technology development to workforce education and from government initiatives to curriculum enhancement.
               ",autonomous vehicle
10.1016/j.matpr.2021.07.196,journal,Materials Today: Proceedings,sciencedirect,2021-07-24,sciencedirect,Diagnosis of diabetes using machine learning algorithms,https://api.elsevier.com/content/article/pii/S2214785321050550,"
                  One of the chronic diseases is Diabetes, Diabetes is a metabolic disorder category caused by continued high levels of blood sugar. It is regarded as one of the most deadly diseases in the world. If accurate early prediction is possible, Diabetes severity and risk factor can be significantly lowered. Machine learning has become more popular in the medical community as a result of its ascent, and in the field of diseases in particular. In this paper, we propose a model that can predict where the patient has or hasn't have diabetes. Our model is based on the prediction precision of certain powerful machine learning (ML) algorithms based on different measures such as precision, recall, and F1-measure. The Pima Indian Diabetes (PIDD) dataset has been used, that can predict diabetic onset based on diagnostics manner. The results we obtained using Logistic Regression (LR), Naïve Bayes (NB), and K-nearest Neighbor (KNN) algorithms were 94%, 79%, and 69% respectively. The results show that LR is more efficient at predicting diabetes Compared to other algorithms.
               ",autonomous vehicle
10.1016/j.neucom.2021.04.112,journal,Neurocomputing,sciencedirect,2021-10-12,sciencedirect,Online learning: A comprehensive survey,https://api.elsevier.com/content/article/pii/S0925231221006706,"
                  
                     Online learning represents a family of machine learning methods, where a learner attempts to tackle some predictive (or any type of decision-making) task by learning from a sequence of data instances one by one at each time. The goal of online learning is to maximize the accuracy/correctness for the sequence of predictions/decisions made by the online learner given the knowledge of correct answers to previous prediction/learning tasks and possibly additional information. This is in contrast to traditional batch or offline machine learning methods that are often designed to learn a model from the entire training data set at once. Online learning has become a promising technique for learning from continuous streams of data in many real-world applications. This survey aims to provide a comprehensive survey of the online machine learning literature through a systematic review of basic ideas and key principles and a proper categorization of different algorithms and techniques. Generally speaking, according to the types of learning tasks and the forms of feedback information, the existing online learning works can be classified into three major categories: (i) online supervised learning where full feedback information is always available, (ii) online learning with limited feedback, and (iii) online unsupervised learning where no feedback is available. Due to space limitation, the survey will be mainly focused on the first category, but also briefly cover some basics of the other two categories. Finally, we also discuss some open issues and attempt to shed light on potential future research directions in this field.
               ",autonomous vehicle
10.1016/B978-0-323-85235-7.00013-2,journal,Methodological Approaches for Sleep and Vigilance Research,sciencedirect,2022-12-31,sciencedirect,Chapter 13: Clinical psychoinformatics: a novel approach to behavioral states and mental health care driven by machine learning,https://api.elsevier.com/content/article/pii/B9780323852357000132,"
               Machine learning (ML) is a branch of artificial intelligence technology that has received considerable attention in recent years. It is a computational strategy to discover the regularities inherent in multidimensional data sets, allowing us to build predictive models focused on individual states. Therefore, it may help increase the efficiency and sophistication of assessment and aid the selection of optimal intervention methods in clinical practice, including cognitive behavioral therapy. In this paper, we first review the framework of the ML approach, its differences from statistics, and its features. Subsequently, we summarize the main research topics where ML approaches have been applied in the field of mental health and introduce some examples of their applications that may contribute to research in clinical psychology and cognitive behavioral therapy. Finally, the limitations of the ML approach are discussed, as well as its potential for future applications.
            ",autonomous vehicle
10.1016/j.matt.2020.08.034,journal,Matter,sciencedirect,2020-12-02,sciencedirect,Intelligent Microfluidics: The Convergence of Machine Learning and Microfluidics in Materials Science and Biomedicine,https://api.elsevier.com/content/article/pii/S2590238520304987,"
                  Microfluidics permit the automated manipulation of fluids at the microscale with high throughput and spatiotemporal precision, enabling the generation of large, multidimensional datasets. Machine intelligence provides powerful predictive tools with the ability to learn from data. The analysis of microfluidics-generated data via machine learning has been applied in a variety of contexts, achieving impressive results. Here, we elaborate on the potential of operating microfluidic platforms via closed-loop data-driven models by leveraging multimodal monitoring and data-acquisition instrumentation. We believe this approach will provide a robust framework for fundamental explorations in materials science and biomedicine, with implications in fields such as drug discovery, nanomaterials, in vitro organ modeling, and developmental biology. We identify challenges and propose research strategies in the context of the prediction and optimization of chemical reactions and materials syntheses and the development of the next generation of more robust and functional organs-on-chips and emerging organoids-on-chips.
               ",autonomous vehicle
10.1016/j.procs.2021.06.028,journal,Procedia Computer Science,sciencedirect,2021-12-31,sciencedirect,Hybrid Bionic Cognitive Architecture for Artificial General Intelligence Agents,https://api.elsevier.com/content/article/pii/S1877050921012710,"The article describes the author’s proposal on cognitive architecture for the development of a general-level artificial intelligent agent («strong» artificial intelligence). New principles for the development of such an architecture are proposed — a hybrid approach in artificial intelligence and bionics. The architecture diagram of the proposed solution is given and descriptions of possible areas of application are described. Strong artificial intelligence is a technical solution that can solve arbitrary cognitive tasks available to humans (human-level artificial intelligence) and even surpass the capabilities of human intelligence (artificial superintelligence). The fields of application of strong artificial intelligence are limitless — from solving current problems facing the human to completely new problems that are not yet available to human civilization or are still waiting for their discoverer. The novelty of the work lies in the author’s approach to the construction of cognitive architecture, which has absorbed the results of many years of research in the field of artificial intelligence and the results of the analysis of cognitive architectures of other researchers.",autonomous vehicle
10.1016/j.ijpe.2021.108250,journal,International Journal of Production Economics,sciencedirect,2021-11-30,sciencedirect,Artificial intelligence applications in supply chain management,https://api.elsevier.com/content/article/pii/S0925527321002267,"
                  This paper presents a systematic review of studies related to artificial intelligence (AI) applications in supply chain management (SCM). Our systematic search of the related literature identifies 150 journal articles published between 1998 and 2020. A thorough bibliometric analysis is completed to develop the past and present state of this literature. A co-citation analysis on this pool of articles provides an understanding of the clusters of knowledge that constitute this research area. To further direct our discussions, we develop and validate an AI taxonomy which we use as a scale to conduct our bibliometric and co-citation analyses. The proposed taxonomy consists of three research categories of (a) sensing and interacting, (b) learning, and (c) decision making. These categories collectively establish the basis for present and future research on the application of AI methods in SCM literature and practice. Our analysis of the primary research clusters finds that learning methods are slowly getting momentum and sensing and interacting methods offer an emerging area of research. Finally, we provide a roadmap into future studies on AI applications in SCM. Our analysis underpins the importance of behavioral considerations in future studies.
               ",autonomous vehicle
10.1016/j.rcim.2020.101991,journal,Robotics and Computer-Integrated Manufacturing,sciencedirect,2021-02-28,sciencedirect,Logistics-involved QoS-aware service composition in cloud manufacturing with deep reinforcement learning,https://api.elsevier.com/content/article/pii/S0736584520302027,"
                  Cloud manufacturing is a new manufacturing model that aims to provide on-demand manufacturing services to consumers over the Internet. Service composition is an essential issue as well as an important technique in cloud manufacturing (CMfg) that supports construction of larger-granularity, value-added services by combining a number of smaller-granularity services to satisfy consumers’ complex requirements. Meta-heuristics algorithms such as genetic algorithm, particle swarm optimization, and ant colony algorithm are frequently employed for addressing service composition issues in cloud manufacturing. These algorithms, however, require complex design flows and painstaking parameter tuning, and lack adaptability to dynamic environment. Deep reinforcement learning (DRL) provides an alternative approach for solving cloud manufacturing service composition (CMfg-SC) issues. DRL as model-free artificial intelligent methods enables a system to learn optimal service composition solutions through training, which can therefore circumvent the aforementioned problems with meta-heuristics algorithms. This paper is dedicated to exploring possible applications of DRL in CMfg-SC. A logistics-involved QoS-aware DRL-based CMfg-SC is proposed. A dueling Deep Q-Network (DQN) with prioritized replay named PD-DQN is designed as the DRL algorithm. Effectiveness, robustness, adaptability, and scalability of PD-DQN are investigated, and compared with that of the basic DQN and Q-learning. Experimental results indicate that PD-DQN is able to effectively address the CMfg-SC problem.
               ",autonomous vehicle
10.1016/j.patrec.2018.01.018,journal,Pattern Recognition Letters,sciencedirect,2018-04-01,sciencedirect,Special issue MLAAI: Machine learning and applications in artificial intelligence,https://api.elsevier.com/content/article/pii/S0167865518300242,,autonomous vehicle
10.1016/j.jbusres.2020.09.068,journal,Journal of Business Research,sciencedirect,2021-02-28,sciencedirect,"Augmenting organizational decision-making with deep learning algorithms: Principles, promises, and challenges",https://api.elsevier.com/content/article/pii/S0148296320306512,"The current expansion of theory and research on artificial intelligence in management and organization studies has revitalized the theory and research on decision-making in organizations. In particular, recent advances in deep learning (DL) algorithms promise benefits for decision-making within organizations, such as assisting employees with information processing, thereby augment their analytical capabilities and perhaps help their transition to more creative work. We conceptualize the decision-making process in organizations augmented with DL algorithm outcomes (such as predictions or robust patterns from unstructured data) as deep learning–augmented decision-making (DLADM). We contribute to the understanding and application of DL for decision-making in organizations by (a) providing an accessible tutorial on DL algorithms and (b) illustrating DLADM with two case studies drawing on image recognition and sentiment analysis tasks performed on datasets from Zalando, a European e-commerce firm, and Rotten Tomatoes, a review aggregation website for movies, respectively. Finally, promises and challenges of DLADM as well as recommendations for managers in attending to these challenges are also discussed.",autonomous vehicle
10.1016/j.sna.2021.113096,journal,Sensors and Actuators A: Physical,sciencedirect,2021-12-01,sciencedirect,Droplet based microfluidics integrated with machine learning,https://api.elsevier.com/content/article/pii/S0924424721005616,"
                  Droplet based microfluidics (DBMF) has gained huge recognition in the recent years for performing micro-reactions in droplets with high throughput, sensitivity, specificity and minimum cross-contaminations. This technology enables the researchers to realize highly reliable and rapid detection and screening applications in various fields. The high-throughput nature of droplet microfluidics generates large amounts of valuable but complex droplet dataset. Deeper analysis of this intricate droplet data is very essential for detection, classification, characterization and quantification of reactions/content inside the droplets. This can be carried out by Machine Learning (ML), which has proven itself in processing and providing deeper insights and precise predictions of relatively large amounts of complex data with shorter analysis times and exceptional accuracy. The analytical tools of ML enable to imbibe automation and control of many such diagnostic platforms, including DBMF, with minimum human intervention. In recent times, the potential of ML has been explored in microfluidic technology as well to tackle challenges in biomedical and biotechnological applications. The synergy of both the fields, DBMF and ML, helps in development of optimized and automated tools with higher accuracy for numerous applications. Specifically, this enables complete comprehension of the field to eventually realize a truly microfluidic total analysis system (µTAS). This work comprehends a general review emphasizing the implementation of different ML models with DBMF to automate various activities such as fluid control, droplet size prediction, recognition of flow pattern and identification, classification and sorting of droplets in a microfluidic device.
               ",autonomous vehicle
10.1016/B978-0-12-821379-7.00003-5,journal,Practical Machine Learning for Data Analysis Using Python,sciencedirect,2020-12-31,sciencedirect,Chapter 3: Machine learning techniques,https://api.elsevier.com/content/article/pii/B9780128213797000035,"
               Machine learning uses the theory of statistics to build mathematical models, as the main objective is to yield inferences from a sample. Once a model is built, its representation and algorithmic solution for interpretation needs to be competent as well. In some applications, the competence of the machine learning algorithm might be as important as its classification accuracy. Machine learning is used in several fields, including forecasting, anomaly detection, and biomedical data analysis as a decision support element. The purpose of this chapter is to help scientists select an appropriate machine learning technique and guide them using optimal strategies by employing real-time databases, in addition to familiarizing readers with the basics of machine learning before taking a deep dive into solving real-world problems with machine learning techniques. Basic concepts are covered in areas such as artificial intelligence, data mining, computer science, data science, natural language processing, deep learning, mathematics, and statistics. Topics related to the various machine learning techniques will be explored, including supervised, unsupervised, and reinforcement learning, hence important machine learning algorithms are discussed in this chapter. Toward the end of every section, suitable Python functions will be illustrated as examples. Most of the examples are taken from Python–scikit-learn library (https://scikit-learn.org/stable/) and TensorFlow, and then adapted.
            ",autonomous vehicle
10.1016/j.sna.2021.113096,journal,Sensors and Actuators A: Physical,sciencedirect,2021-12-01,sciencedirect,Droplet based microfluidics integrated with machine learning,https://api.elsevier.com/content/article/pii/S0924424721005616,"
                  Droplet based microfluidics (DBMF) has gained huge recognition in the recent years for performing micro-reactions in droplets with high throughput, sensitivity, specificity and minimum cross-contaminations. This technology enables the researchers to realize highly reliable and rapid detection and screening applications in various fields. The high-throughput nature of droplet microfluidics generates large amounts of valuable but complex droplet dataset. Deeper analysis of this intricate droplet data is very essential for detection, classification, characterization and quantification of reactions/content inside the droplets. This can be carried out by Machine Learning (ML), which has proven itself in processing and providing deeper insights and precise predictions of relatively large amounts of complex data with shorter analysis times and exceptional accuracy. The analytical tools of ML enable to imbibe automation and control of many such diagnostic platforms, including DBMF, with minimum human intervention. In recent times, the potential of ML has been explored in microfluidic technology as well to tackle challenges in biomedical and biotechnological applications. The synergy of both the fields, DBMF and ML, helps in development of optimized and automated tools with higher accuracy for numerous applications. Specifically, this enables complete comprehension of the field to eventually realize a truly microfluidic total analysis system (µTAS). This work comprehends a general review emphasizing the implementation of different ML models with DBMF to automate various activities such as fluid control, droplet size prediction, recognition of flow pattern and identification, classification and sorting of droplets in a microfluidic device.
               ",autonomous vehicle
10.1016/j.chroma.2021.461900,journal,Journal of Chromatography A,sciencedirect,2021-02-08,sciencedirect,Deep Q-learning for the selection of optimal isocratic scouting runs in liquid chromatography,https://api.elsevier.com/content/article/pii/S0021967321000248,"
                  An important challenge in chromatography is the development of adequate separation methods. Accurate retention models can significantly simplify and expedite the development of adequate separation methods for complex mixtures. The purpose of this study was to introduce reinforcement learning to chromatographic method development, by training a double deep Q-learning algorithm to select optimal isocratic scouting runs to generate accurate retention models. These scouting runs were fit to the Neue-Kuss retention model, which was then used to predict retention factors both under isocratic and gradient conditions. The quality of these predictions was compared to experimental data points, by computing a mean relative percentage error (MRPE) between the predicted and actual retention factors. By providing the reinforcement learning algorithm with a reward whenever the scouting runs led to accurate retention models and a penalty when the analysis time of a selected scouting run was too high (> 1h); it was hypothesized that the reinforcement learning algorithm should by time learn to select good scouting runs for compounds displaying a variety of characteristics. The reinforcement learning algorithm developed in this work was first trained on simulated data, and then evaluated on experimental data for 57 small molecules – each run at 10 different fractions of organic modifier (0.05 to 0.90) and four different linear gradients. The results showed that the MRPE of these retention models (3.77% for isocratic runs and 1.93% for gradient runs), mostly obtained via 3 isocratic scouting runs for each compound, were comparable in performance to retention models obtained by fitting the Neue-Kuss model to all (10) available isocratic datapoints (3.26% for isocratic runs and 4.97% for gradient runs) and retention models obtained via a “chromatographer's selection” of three scouting runs (3.86% for isocratic runs and 6.66% for gradient runs). It was therefore concluded that the reinforcement learning algorithm learned to select optimal scouting runs for retention modeling, by selecting 3 (out of 10) isocratic scouting runs per compound, that were informative enough to successfully capture the retention behavior of each compound.
               ",autonomous vehicle
10.1016/j.procs.2020.04.299,journal,Procedia Computer Science,sciencedirect,2020-12-31,sciencedirect,Data Traffic Classification in Software Defined Networks (SDN) using supervised-learning,https://api.elsevier.com/content/article/pii/S1877050920312928,"Traffic classification with accuracy is of prime importance in network activities such as security monitoring, traffic engineering, fault detection, accounting of network usage, billing and for providing differentiation in Quality of Service (QoS) parameters of the various network services. Network Traffic Classification is significant in recent days due to rapid growth in the number of internet consumers. The different primitive techniques of network traffic classification have failed to provide reliable accuracy because of 1000 fold scaling in the amount of devices as well as flows. To overcome this drawback, the integration of Software Defined Network (SDN) architecture and machine learning technology is proposed in this paper. Three different supervised learning models, namely Support Vector Machine (SVM), nearest centroid and Naïve Bayes (NB), are applied to classify the data traffic based on the applications in a software-defined network platform. The network traffic traces are captured and flows features are generated, which is sent to the classifier for prediction. The accuracy obtained for SVM is 92.3%, NB is 96.79% and the nearest centroid is 91.02%. The challenges faced are in the live network data traffic capture and classification of the applications in the SDN platform.",autonomous vehicle
10.1016/j.crfs.2021.01.002,journal,Current Research in Food Science,sciencedirect,2021-12-31,sciencedirect,Machine learning techniques for analysis of hyperspectral images to determine quality of food products: A review,https://api.elsevier.com/content/article/pii/S2665927121000034,"Non-destructive testing techniques have gained importance in monitoring food quality over the years. Hyperspectral imaging is one of the important non-destructive quality testing techniques which provides both spatial and spectral information. Advancement in machine learning techniques for rapid analysis with higher classification accuracy have improved the potential of using this technique for food applications. This paper provides an overview of the application of different machine learning techniques in analysis of hyperspectral images for determination of food quality. It covers the principle underlying hyperspectral imaging, the advantages, and the limitations of each machine learning technique. The machine learning techniques exhibited rapid analysis of hyperspectral images of food products with high accuracy thereby enabling robust classification or regression models. The selection of effective wavelengths from the hyperspectral data is of paramount importance since it greatly reduces the computational load and time which enhances the scope for real time applications. Due to the feature learning nature of deep learning, it is one of the most promising and powerful techniques for real time applications. However, the field of deep learning is relatively new and need further research for its full utilization. Similarly, lifelong machine learning paves the way for real time HSI applications but needs further research to incorporate the seasonal variations in food quality. Further, the research gaps in machine learning techniques for hyperspectral image analysis, and the prospects are discussed.",autonomous vehicle
10.1016/j.iot.2020.100314,journal,Internet of Things,sciencedirect,2020-12-31,sciencedirect,"A survey on machine learning in Internet of Things: Algorithms, strategies, and applications",https://api.elsevier.com/content/article/pii/S2542660520301451,"
                  In the IoT and WSN era, large number of connected objects and sensing devices are dedicated to collect, transfer, and generate a huge amount of data for a wide variety of fields and applications. To effectively run these complex networks of connected objects, there are several challenges like topology changes, link failures, memory constraints, interoperability, network congestion, coverage, scalability, network management, security, and privacy to name a few. Thus, to overcome these challenges and exploiting them to support this technological outbreak would be one of the most crucial tasks of modern world. In the recent years, the development of Artificial Intelligence (AI) led to the emergence of Machine Learning (ML) which has become the key enabler to figure out solutions and learning models in an attempt to enhance the QoS parameters of IoT and WSNs. By learning from past experiences, ML techniques aim to resolve issues in the WSN and IoT's fields by building algorithmic models. In this paper, we are going to highlight the most fundamental concepts of ML categories and Algorithms. We start by providing a thorough overview of the WSN and IoT's technologies. We also discuss the vital role of ML techniques in driving up the evolution of these technologies. Then, as the key contribution of this paper, a new taxonomy of ML algorithms is provided. We also summarize the major applications and research challenges that leveraged ML techniques in the WSN and IoT. Eventually, we analyze the critical issues and list some future research directions.
               ",autonomous vehicle
10.1016/j.rpor.2020.03.015,journal,Reports of Practical Oncology & Radiotherapy,sciencedirect,2020-08-31,sciencedirect,Artificial intelligence in radiotherapy,https://api.elsevier.com/content/article/pii/S1507136720300444,"
                  Artificial intelligence (AI) has already been implemented widely in the medical field in the recent years. This paper first reviews the background of AI and radiotherapy. Then it explores the basic concepts of different AI algorithms and machine learning methods, such as neural networks, that are available to us today and how they are being implemented in radiotherapy and diagnostic processes, such as medical imaging, treatment planning, patient simulation, quality assurance and radiation dose delivery. It also explores the ongoing research on AI methods that are to be implemented in radiotherapy in the future. The review shows very promising progress and future for AI to be widely used in various areas of radiotherapy. However, basing on various concerns such as availability and security of using big data, and further work on polishing and testing AI algorithms, it is found that we may not ready to use AI primarily in radiotherapy at the moment.
               ",autonomous vehicle
10.1016/j.jobe.2020.101827,journal,Journal of Building Engineering,sciencedirect,2020-11-30,sciencedirect,Deep learning in the construction industry: A review of present status and future innovations,https://api.elsevier.com/content/article/pii/S2352710220334604,"The construction industry is known to be overwhelmed with resource planning, risk management and logistic challenges which often result in design defects, project delivery delays, cost overruns and contractual disputes. These challenges have instigated research in the application of advanced machine learning algorithms such as deep learning to help with diagnostic and prescriptive analysis of causes and preventive measures. However, the publicity created by tech firms like Google, Facebook and Amazon about Artificial Intelligence and applications to unstructured data is not the end of the field. There abound many applications of deep learning, particularly within the construction sector in areas such as site planning and management, health and safety and construction cost prediction, which are yet to be explored. The overall aim of this article was to review existing studies that have applied deep learning to prevalent construction challenges like structural health monitoring, construction site safety, building occupancy modelling and energy demand prediction. To the best of our knowledge, there is currently no extensive survey of the applications of deep learning techniques within the construction industry. This review would inspire future research into how best to apply image processing, computer vision, natural language processing techniques of deep learning to numerous challenges in the industry. Limitations of deep learning such as the black box challenge, ethics and GDPR, cybersecurity and cost, that can be expected by construction researchers and practitioners when adopting some of these techniques were also discussed.",autonomous vehicle
10.1016/j.comnet.2020.107478,journal,Computer Networks,sciencedirect,2020-12-09,sciencedirect,UAVs joint optimization problems and machine learning to improve the 5G and Beyond communication,https://api.elsevier.com/content/article/pii/S1389128620311518,"
                  Recently, unmanned aerial vehicles (UAVs) have gained notable interest in various applications such as wireless coverage, aerial surveillance, precision agriculture, construction, power lines monitoring and blood delivery, etc. The UAVs implicit attributes e.g., rapid deployment, quick mobility, increase in flight duration, improvements in payload capacities, etc. , place it as an effective candidate for many applications in 5G and Beyond communications. The UAVs-assisted next-generation communications are determined to be highly influenced by various techniques and technologies like artificial intelligence (AI), machine learning (ML), deep reinforcement learning (DRL), mobile edge computing (MEC), and software-defined networks (SDN). In this article, we develop a review to investigate the UAVs joint optimization problems to enhance system efficiency. We classify the joint optimization problems based on the number of parameters used in proposed optimization problems. Moreover, we explore the impact of AI, ML, DRL, MEC, and SDN over UAVs joint optimization problems and present future research challenges and directions.
               ",autonomous vehicle
10.1016/B978-0-12-822226-3.00003-9,journal,Trends in Deep Learning Methodologies,sciencedirect,2021-12-31,sciencedirect,"Chapter 3: An overview of deep learning in big data, image, and signal processing in the modern digital age",https://api.elsevier.com/content/article/pii/B9780128222263000039,"
               Nowadays, data is generated all the time on the internet. Faced with this scenario, technologies have emerged to take advantage of this feature, so that in addition to just being able to measure and understand where they come from, it is possible for them to be collected, quantified, decoded, and analyzed, allowing the understanding of behaviors and trends, the definition of strategies, and the process of insight generation. Big data leverages resources that organize and catalog this information, increasing the availability of relevant data for informed decision making. Machine learning is an aspect of artificial intelligence that competently performs automation in the process of building analytical models that allow machines to adapt independently to new scenarios, enabling software to successfully predict and react to the deployment of scenarios based on past results. Deep learning has this nomenclature because it deals with neural networks having multiple (deep) layers that allow learning; therefore it is a subset of machine learning, which considers algorithms inspired by the human brain, the artificial neural networks, which learn from large amounts of data. Deep learning techniques are especially useful for analyzing complex, rich, and multidimensional data such as voice, images, and videos. In short, all deep learning is machine learning, but not all machine learning is deep learning. This chapter examines the technology of deep learning and machine learning in big data by addressing its evolution and fundamental concepts and its integration into new technologies, by approaching its success, and by categorizing and synthesizing the potential of both technologies.
            ",autonomous vehicle
10.1016/j.smrv.2021.101512,journal,Sleep Medicine Reviews,sciencedirect,2021-10-31,sciencedirect,Artificial intelligence and sleep: Advancing sleep medicine,https://api.elsevier.com/content/article/pii/S1087079221000976,"
                  Artificial intelligence (AI) allows analysis of “big data” combining clinical, environmental and laboratory based objective measures to allow a deeper understanding of sleep and sleep disorders. This development has the potential to transform sleep medicine in coming years to the betterment of patient care and our collective understanding of human sleep. This review addresses the current state of the field starting with a broad definition of the various components and analytic methods deployed in AI. We review examples of AI use in screening, endotyping, diagnosing, and treating sleep disorders and place this in the context of precision/personalized sleep medicine. We explore the opportunities for AI to both facilitate and extend providers’ clinical impact and present ethical considerations regarding AI derived prognostic information. We cover early adopting specialties of AI in the clinical realm, such as radiology and pathology, to provide a road map for the challenges sleep medicine is likely to face when deploying this technology. Finally, we discuss pitfalls to ensure clinical AI implementation proceeds in the safest and most effective manner possible.
               ",autonomous vehicle
10.1016/j.image.2019.03.012,journal,Signal Processing: Image Communication,sciencedirect,2019-07-31,sciencedirect,Quality-aware dual-modal saliency detection via deep reinforcement learning,https://api.elsevier.com/content/article/pii/S0923596518308683,"
                  Incorporating various modes of information into the machine learning procedure is becoming a new trend And data from various source can provide more information than single one no matter they are heterogeneous or homogeneous. Existing deep learning based algorithms usually directly concatenate features from each domain to represent the input data. Seldom of them take the quality of data into consideration which is a key issue in related multimodal problems. In this paper, we propose an efficient quality-aware deep neural network to model the weight of data from each domain using deep reinforcement learning (DRL). Specifically, we take the weighting of each domain as a decision-making problem and teach an agent learn to interact with the environment. The agent can tune the weight of each domain through discrete action selection and obtain a positive reward if the saliency results are improved. The target of the agent is to achieve maximum rewards after finished its sequential action selection. We validate the proposed algorithms on dual-modal saliency detection in a coarse-to-fine way. The coarse saliency maps are generated from an encoder–decoder framework which is trained with content loss and adversarial loss. The final results can be obtained via adaptive weighting of maps from each domain. Extensive experiments on two kinds of salient object detection benchmarks all obtained a significant gain by fusing two-modalities with our proposed quality-aware deep neural network.
               ",autonomous vehicle
10.1016/j.wpi.2020.102002,journal,World Patent Information,sciencedirect,2020-12-31,sciencedirect,An analysis of the effects of artificial intelligence on electric vehicle technology innovation using patent data,https://api.elsevier.com/content/article/pii/S0172219020300946,"
                  This study empirically analyzes the effects of artificial intelligence (AI) on electric vehicle technology innovation by employing a machine learning-based text mining model and the international patent classification (IPC) co-occurrence network analysis, using patent data filed from 1980 to 2017. Based on artificial intelligence algorithms classified, the study demonstrates the dynamic changing pattern of the convergence of artificial intelligence and electric vehicle technology and reveals how artificial intelligence has affected electric vehicle technology innovation over time. This study reveals that artificial intelligence accelerates the automation of electric vehicle driving, and that artificial intelligence algorithms that are widely used in electric vehicles have changed over time, and that technology areas of electric vehicles that AI affects also have been changed.
               ",autonomous vehicle
10.1016/j.yaoo.2018.04.001,journal,Advances in Ophthalmology and Optometry,sciencedirect,2018-08-31,sciencedirect,Artificial Intelligence and Its Applications in Vision and Eye Care,https://api.elsevier.com/content/article/pii/S2452176018300015,,autonomous vehicle
10.1016/j.knosys.2020.105596,journal,Knowledge-Based Systems,sciencedirect,2020-04-22,sciencedirect,"A review of deep learning with special emphasis on architectures, applications and recent trends",https://api.elsevier.com/content/article/pii/S095070512030071X,"
                  Deep learning (DL) has solved a problem that a few years ago was thought to be intractable — the automatic recognition of patterns in spatial and temporal data with an accuracy superior to that of humans. It has solved problems beyond the realm of traditional, hand-crafted machine learning algorithms and captured the imagination of practitioners who are inundated with all types of data. As public awareness of the efficacy of DL increases so does the desire to make use of it. But even for highly trained professionals it can be daunting to approach the rapidly increasing body of knowledge in the field. Where does one start? How does one determine if a particular DL model is applicable to their problem? How does one train and deploy them? With these questions in mind, we present an overview of some of the key DL architectures. We also discuss some new automatic architecture optimization protocols that use multi-agent approaches. Further, since guaranteeing system uptime is critical to many applications, a section dwells on using DL for fault detection and mitigation. This is followed by an exploratory survey of several areas where DL emerged as a game-changer: fraud detection in financial applications, financial time-series forecasting, predictive and prescriptive analytics, medical image processing, power systems research and recommender systems. The thrust of this review is to outline emerging applications of DL and provide a reference to researchers seeking to use DL in their work for pattern recognition with unparalleled learning capacity and the ability to scale with data.
               ",autonomous vehicle
10.1016/j.apsb.2021.02.007,journal,Acta Pharmaceutica Sinica B,sciencedirect,2021-02-11,sciencedirect,Applying artificial intelligence for cancer immunotherapy,https://api.elsevier.com/content/article/pii/S2211383521000459,"Artificial intelligence (AI) is a general term that refers to the use of a machine to imitate intelligent behavior for performing complex tasks with minimal human intervention, such as machine learning; this technology is revolutionizing and reshaping medicine. AI has considerable potential to perfect health-care systems in areas such as diagnostics, risk analysis, health information administration, lifestyle supervision, and virtual health assistance. In terms of immunotherapy, AI has been applied to the prediction of immunotherapy responses based on immune signatures, medical imaging and histological analysis. These features could also be highly useful in the management of cancer immunotherapy given their ever-increasing performance in improving diagnostic accuracy, optimizing treatment planning, predicting outcomes of care and reducing human resource costs. In this review, we present the details of AI and the current progression and state of the art in employing AI for cancer immunotherapy. Furthermore, we discuss the challenges, opportunities and corresponding strategies in applying the technology for widespread clinical deployment. Finally, we summarize the impact of AI on cancer immunotherapy and provide our perspectives about underlying applications of AI in the future.",autonomous vehicle
10.1016/j.neunet.2019.01.012,journal,Neural Networks,sciencedirect,2019-05-31,sciencedirect,Continual lifelong learning with neural networks: A review,https://api.elsevier.com/content/article/pii/S0893608019300231,"Humans and animals have the ability to continually acquire, fine-tune, and transfer knowledge and skills throughout their lifespan. This ability, referred to as lifelong learning, is mediated by a rich set of neurocognitive mechanisms that together contribute to the development and specialization of our sensorimotor skills as well as to long-term memory consolidation and retrieval. Consequently, lifelong learning capabilities are crucial for computational learning systems and autonomous agents interacting in the real world and processing continuous streams of information. However, lifelong learning remains a long-standing challenge for machine learning and neural network models since the continual acquisition of incrementally available information from non-stationary data distributions generally leads to catastrophic forgetting or interference. This limitation represents a major drawback for state-of-the-art deep neural network models that typically learn representations from stationary batches of training data, thus without accounting for situations in which information becomes incrementally available over time. In this review, we critically summarize the main challenges linked to lifelong learning for artificial learning systems and compare existing neural network approaches that alleviate, to different extents, catastrophic forgetting. Although significant advances have been made in domain-specific learning with neural networks, extensive research efforts are required for the development of robust lifelong learning on autonomous agents and robots. We discuss well-established and emerging research motivated by lifelong learning factors in biological systems such as structural plasticity, memory replay, curriculum and transfer learning, intrinsic motivation, and multisensory integration.",autonomous vehicle
10.1016/j.jallcom.2020.158018,journal,Journal of Alloys and Compounds,sciencedirect,2021-05-05,sciencedirect,Optimizing laser powder bed fusion of Ti-5Al-5V-5Mo-3Cr by artificial intelligence,https://api.elsevier.com/content/article/pii/S0925838820343826,"
                  The prerequisite for exploiting the full potential of additive manufacturing (AM) is the rapid and cost-effective fabrication of defect-free components. However, each newly processed material usually requires the identification of the optimal parameter set, a cost and time-consuming process, mostly conducted by trial and error. Here, an optimization strategy based on artificial intelligence (AI) is developed for predicting the density of additively manufactured Ti-5Al-5V-5Mo-3Cr components from experimental data. The present approach opens the way to a faster identification of the optimum set of processing parameters via AI.
               ",autonomous vehicle
10.1016/j.nicl.2020.102376,journal,NeuroImage: Clinical,sciencedirect,2020-12-31,sciencedirect,Identifying controllable cortical neural markers with machine learning for adaptive deep brain stimulation in Parkinson’s disease,https://api.elsevier.com/content/article/pii/S2213158220302138,"The identification of oscillatory neural markers of Parkinson’s disease (PD) can contribute not only to the understanding of functional mechanisms of the disorder, but may also serve in adaptive deep brain stimulation (DBS) systems. These systems seek online adaptation of stimulation parameters in closed-loop as a function of neural markers, aiming at improving treatment’s efficacy and reducing side effects. Typically, the identification of PD neural markers is based on group-level studies. Due to the heterogeneity of symptoms across patients, however, such group-level neural markers, like the beta band power of the subthalamic nucleus, are not present in every patient or not informative about every patient’s motor state. Instead, individual neural markers may be preferable for providing a personalized solution for the adaptation of stimulation parameters. Fortunately, data-driven bottom-up approaches based on machine learning may be utilized. These approaches have been developed and applied successfully in the field of brain-computer interfaces with the goal of providing individuals with means of communication and control. In our contribution, we present results obtained with a novel supervised data-driven identification of neural markers of hand motor performance based on a supervised machine learning model. Data of 16 experimental sessions obtained from seven PD patients undergoing DBS therapy show that the supervised patient-specific neural markers provide improved decoding accuracy of hand motor performance, compared to group-level neural markers reported in the literature. We observed that the individual markers are sensitive to DBS therapy and thus, may represent controllable variables in an adaptive DBS system.",autonomous vehicle
10.1016/j.knosys.2021.107688,journal,Knowledge-Based Systems,sciencedirect,2021-11-15,sciencedirect,Improved prior selection using semantics in maximum a posteriori for few-shot learning,https://api.elsevier.com/content/article/pii/S0950705121009485,"
                  Few-shot learning is to recognize novel concepts with few labeled samples. Recently, significant progress has been made to address the overfitting caused by data scarcity, especially those on modeling the distribution of novel categories given a single point. However, they often deeply rely on the prior knowledge from base set, which is generally hard to define, and its selection can easily bias the learning. A popular pipeline is to pretrain a feature extractor with base set and generate statistics from them as prior information. Since pretrained feature extractor cannot extract accurate representations for categories have never seen, and there is only 1 or 5 support images from novel categories, making it hard to acquire accurate priors, especially when they are far away from the class center. To address these issues, in this paper, we base our network on Maximum a posteriori (MAP), proposing a strategy for better prior selection from base set. We specially introduce semantic information, which are learned from unsupervised text corpora and easily available, to alleviate biases caused by unrepresentative support samples. Our intuition is that when the support from visual information is biased, semantics can provide strong prior knowledge to assist learning. Experimental results on four few-shot benchmarks also show that it outperforms the state-of-the-art methods by a large margin, improves around 2.08%
                        ∼
                     12.68% than the best results in each dataset on both 1- and 5-shot tasks.
               ",autonomous vehicle
10.1016/j.tips.2020.12.004,journal,Trends in Pharmacological Sciences,sciencedirect,2021-03-31,sciencedirect,"Machine Learning for Biologics: Opportunities for Protein Engineering, Developability, and Formulation",https://api.elsevier.com/content/article/pii/S0165614720302856,"
                  Successful biologics must satisfy multiple properties including activity and particular physicochemical features that are globally defined as developability. These multiple properties must be simultaneously optimized in a very broad design space of protein sequences and buffer compositions. In this context, artificial intelligence (AI), and especially machine learning (ML), have great potential to accelerate and improve the optimization of protein properties, increasing their activity and safety as well as decreasing their development time and manufacturing costs. We highlight the emerging applications of ML in biologics discovery and development, focusing on protein engineering, early biophysical screening, and formulation. We discuss the power of ML in extracting information from complex datasets and in reducing the necessary experimental effort to simultaneously achieve multiple quality targets. We finally anticipate possible future interventions of AI in several steps of the biological landscape.
               ",autonomous vehicle
10.1016/j.pcd.2021.02.005,journal,Primary Care Diabetes,sciencedirect,2021-06-30,sciencedirect,A review on current advances in machine learning based diabetes prediction,https://api.elsevier.com/content/article/pii/S175199182100019X,"
                  Diabetes is a metabolic disorder comprising of high glucose level in blood over a prolonged period in the body as it is not capable of using it properly. The severe complications associated with diabetes include diabetic ketoacidosis, nonketotic hypersmolar coma, cardiovascular disease, stroke, chronic renal failure, retinal damage and foot ulcers. There is a huge increase in the number of patients with diabetes globally and it is considered a major health problem worldwide. Early diagnosis of diabetes is helpful for treatment and reduces the chance of severe complications associated with it. Machine learning algorithms (such as ANN, SVM, Naive Bayes, PLS-DA and deep learning) and data mining techniques are used for detecting interesting patterns for diagnosing and treatment of disease. Current computational methods for diabetes diagnosis have some limitations and are not tested on different datasets or peoples from different countries which limits the practical use of prediction methods. This paper is an effort to summarize the majority of the literature concerned with machine learning and data mining techniques applied for the prediction of diabetes and associated challenges. This report would be helpful for better prediction of disease and improve in understanding the pattern of diabetes. Consequently, the report would be helpful for treatment and reduce risk of other complications of diabetes.
               ",autonomous vehicle
10.1016/B978-0-12-818503-2.00010-1,journal,Heat Transfer Engineering,sciencedirect,2021-12-31,sciencedirect,Chapter 10: Machine learning in heat transfer,https://api.elsevier.com/content/article/pii/B9780128185032000101,"
               This chapter introduces the application of machine learning in heat transfer. We discuss the distinction between physics and data-based models and the importance of the latter in heat transfer engineering. We then give an overview of various types of algorithms within machine learning. This is followed by a detailed description of the supervised learning process. This process is then applied to two learning algorithms—linear regression and artificial neural networks. A detailed discussion of neural networks is presented next, along with some sample problems. We follow this with a discussion on practical considerations in the application of neural networks to engineering heat transfer. We end with a description of some modern techniques and the current and future applications of machine learning in engineering heat transfer.
            ",autonomous vehicle
10.1016/j.compag.2020.105709,journal,Computers and Electronics in Agriculture,sciencedirect,2020-10-31,sciencedirect,Crop yield prediction using machine learning: A systematic literature review,https://api.elsevier.com/content/article/pii/S0168169920302301,"Machine learning is an important decision support tool for crop yield prediction, including supporting decisions on what crops to grow and what to do during the growing season of the crops. Several machine learning algorithms have been applied to support crop yield prediction research. In this study, we performed a Systematic Literature Review (SLR) to extract and synthesize the algorithms and features that have been used in crop yield prediction studies. Based on our search criteria, we retrieved 567 relevant studies from six electronic databases, of which we have selected 50 studies for further analysis using inclusion and exclusion criteria. We investigated these selected studies carefully, analyzed the methods and features used, and provided suggestions for further research. According to our analysis, the most used features are temperature, rainfall, and soil type, and the most applied algorithm is Artificial Neural Networks in these models. After this observation based on the analysis of machine learning-based 50 papers, we performed an additional search in electronic databases to identify deep learning-based studies, reached 30 deep learning-based papers, and extracted the applied deep learning algorithms. According to this additional analysis, Convolutional Neural Networks (CNN) is the most widely used deep learning algorithm in these studies, and the other widely used deep learning algorithms are Long-Short Term Memory (LSTM) and Deep Neural Networks (DNN).",autonomous vehicle
10.1016/j.cosrev.2020.100280,journal,Computer Science Review,sciencedirect,2020-08-31,sciencedirect,When machine learning meets medical world: Current status and future challenges,https://api.elsevier.com/content/article/pii/S157401372030126X,"
                  Imagine the enormous amounts of data that can be generated in the medical field. Each patient has his own medical record which contains valuable information like patient allergy, chronic diseases and vaccinations. Healthcare can profit from this data when it is properly analyzed. The more data gathered, the more complicated data analytics become, therefore, machine learning can be a very useful solution not only to facilitate analysis but also to save time.
                  In this paper basic concepts of the medical field and machine learning will be described. We will show how data analytic can help in the healthcare process. Finally, we will present some challenges that must be carefully studied in order to obtain effective solutions in medical diagnosis.
               ",autonomous vehicle
10.1016/j.engappai.2019.02.013,journal,Engineering Applications of Artificial Intelligence,sciencedirect,2019-05-31,sciencedirect,Reinforcement Learning based scheduling in a workflow management system,https://api.elsevier.com/content/article/pii/S0952197619300351,"
                  Any computational process from simple data analytics tasks to training a machine learning model can be described by a workflow. Many workflow management systems (WMS) exist that undertake the task of scheduling workflows across distributed computational resources. In this work, we introduce a WMS that leverages machine learning to predict workflow task runtime and the probability of failure of task assignments to execution sites. The expected runtime of workflow tasks can be used to approximate the weight of the workflow graph branches in respect to the total workflow workload and the ability to anticipate task failures can discourage task assignments that are unlikely to succeed. We demonstrate that the proposed machine learning models can lead to significantly more informed scheduling decisions that minimize task failures and utilize execution sites more efficiently, thus leading to reduced workflow runtime. Additionally, we train a modified sequence-to-sequence neural network architecture via reinforcement learning to perform scheduling decisions as part of a WMS. Our approach introduces a WMS that can drastically improve its scheduling performance by independently learning over time, without external intervention or reliance on any specific heuristic or optimization technique. Finally, we test our approach in real-world scenarios utilizing computationally demanding and data intensive workflows and evaluate its performance against existing scheduling methodologies traditionally used in WMSes. The performance evaluation outcome confirms that the proposed approach significantly outperforms the other scheduling algorithms in a consistent manner and achieves the best execution runtime with the lowest number of failed tasks and communication costs.
               ",autonomous vehicle
10.1016/j.knosys.2021.107646,journal,Knowledge-Based Systems,sciencedirect,2022-01-10,sciencedirect,"Meta-learning as a promising approach for few-shot cross-domain fault diagnosis: Algorithms, applications, and prospects",https://api.elsevier.com/content/article/pii/S0950705121009084,"
                  The advances of intelligent fault diagnosis in recent years show that deep learning has strong capability of automatic feature extraction and accurate identification for fault signals. Nevertheless, data scarcity and varying working conditions can degrade the performance of the model. More recently, a tool has been proposed to address the above challenges simultaneously. Meta-learning, also known as learning to learn, uses a small sample to quickly adapt to a new task. It has great application potential in few-shot and cross-domain fault diagnosis, and thus has become a promising tool. However, there is a lack of a survey to conclude existing work and look into the future. This paper comprehensively investigates deep meta-learning in fault diagnosis from three views: (i) what to use, (ii) how to use, and (iii) how to develop, i.e. algorithms, applications, and prospects. Algorithms are illustrated by optimization-, metric-, and model-based methods, the applications are concluded in few-shot cross-domain fault diagnosis, and open challenges, as well as prospects, are given to motivate the future work. Additionally, we demonstrate the performance of three approaches on two few-shot cross-domain tasks. Typical meta-learning methods are implemented and available at https://github.com/fyancy/MetaFD.
               ",autonomous vehicle
10.1016/j.eswa.2020.114060,journal,Expert Systems with Applications,sciencedirect,2021-03-15,sciencedirect,Machine learning and data mining in manufacturing,https://api.elsevier.com/content/article/pii/S095741742030823X,"
                  Manufacturing organizations need to use different kinds of techniques and tools in order to fulfill their foundation goals. In this aspect, using machine learning (ML) and data mining (DM) techniques and tools could be very helpful for dealing with challenges in manufacturing. Therefore, in this paper, a comprehensive literature review is presented to provide an overview of how machine learning techniques can be applied to realize manufacturing mechanisms with intelligent actions. Furthermore, it points to several significant research questions that are unanswered in the recent literature having the same target. Our survey aims to provide researchers with a solid understanding of the main approaches and algorithms used to improve manufacturing processes over the past two decades. It presents the previous ML studies and recent advances in manufacturing by grouping them under four main subjects: scheduling, monitoring, quality, and failure. It comprehensively discusses existing solutions in manufacturing according to various aspects, including tasks (i.e., clustering, classification, regression), algorithms (i.e., support vector machine, neural network), learning types (i.e., ensemble learning, deep learning), and performance metrics (i.e., accuracy, mean absolute error). Furthermore, the main steps of knowledge discovery in databases (KDD) process to be followed in manufacturing applications are explained in detail. In addition, some statistics about the current state are also given from different perspectives. Besides, it explains the advantages of using machine learning techniques in manufacturing, expresses the ways to overcome certain challenges, and offers some possible further research directions.
               ",autonomous vehicle
10.1016/j.matpr.2021.04.349,journal,Materials Today: Proceedings,sciencedirect,2021-05-21,sciencedirect,A research on prediction of bat-borne disease infection through segmentation using diffusion-weighted MR imaging in deep-machine learning approach,https://api.elsevier.com/content/article/pii/S2214785321032697,"
                  The theme of this study is to provide a detailed description of its recent improvements in image segmentation and lesion classification in disease prognosis. Previous studies have shown that gray-white matter hyperintensities (GWMH) is one of the hallmarks of Nipah encephalitis, which sometimes occurs during the incubation period. Predicting this type of inflammation is a challenging task because it involves some unknown medical risk factors. A typical Magnetic Resonance Imaging (MRIs) is the best non-invasive system to analyze the anatomical structure of the brain. In-depth analysis of the defined pathological structure from isolated MR imaging leads to a reduction in the processing time of the prognostic model. Modern learning techniques such as Machine Learning, Computer Vision, and Deep Learning are the most promising techniques for determining the optimal outcome, computer can able to learn and extract useful information from historical data using various algorithms. Disease prognosis based on deep learning is sophisticated, so it can handle a variety of difficult tasks including image processing, classification, and feature extraction, noise and object detection. Diffusion-weighted imaging (DWI) in MRIs is a clinical prototype that can be used to diagnose brain abnormalities and to evaluate the microscopic architectural and molecular function of human organs or tissues. In this study, we summarize the results of diagnosing Nipah encephalitis using some publicly available brain encephalitis and encephalopathy databases.
               ",autonomous vehicle
10.1016/j.knosys.2020.105622,journal,Knowledge-Based Systems,sciencedirect,2020-03-15,sciencedirect,Predicting concentration levels of air pollutants by transfer learning and recurrent neural network,https://api.elsevier.com/content/article/pii/S0950705120300873,"
                  Air pollution (AP) poses a great threat to human health, and people are paying more attention than ever to its prediction. Accurate prediction of AP helps people to plan for their outdoor activities and aids protecting human health. In this paper, long–short term memory (LSTM) recurrent neural networks (RNNs) have been used to predict the future concentration of air pollutants (APS) in Macau. Additionally, meteorological data and data on the concentration of APS have been utilized. Moreover, in Macau, some air quality monitoring stations (AQMSs) have less observed data in quantity, and, at the same time, some AQMSs recorded less observed data of certain types of APS. Therefore, the transfer learning and pre-trained neural networks have been employed to assist AQMSs with less observed data to build a neural network with high prediction accuracy. The experimental sample covers a period longer than 12-year and includes daily measurements from several APS as well as other more classical meteorological values. Records from five stations, four out of them are AQMSs and the remaining one is an automatic weather station, have been prepared from the aforesaid period and eventually underwent to computational intelligence techniques to build and extract a prediction knowledge-based system. As shown by experimentation, LSTM RNNs initialized with transfer learning methods have higher prediction accuracy; it incurred shorter training time than randomly initialized recurrent neural networks.
               ",autonomous vehicle
10.1016/j.jechem.2021.07.020,journal,Journal of Energy Chemistry,sciencedirect,2022-03-31,sciencedirect,Is machine learning redefining the perovskite solar cells?,https://api.elsevier.com/content/article/pii/S2095495621004101,"
                  Development of novel materials with desirable properties remains at the forefront of modern scientific research. Machine learning (ML), a branch of artificial intelligence, has recently emerged as a powerful technology in optoelectronic devices for the prediction of various properties and rational design of materials. Metal halide perovskites (MHPs) have been at the centre of attraction owing to their outstanding photophysical properties and rapid development in solar cell application. Therefore, the application of ML in the field of MHPs is also getting much attention to optimize the fabrication process and reduce the cost of processing. Here, we comprehensively reviewed different applications of ML in the designing of both MHP absorber layers as well as complete perovskite solar cells (PSCs). At the end, the challenges of ML along with the possible future direction of research are discussed. We believe that this review becomes an indispensable roadmap for optimizing materials composition and predicting design strategies in the field of perovskite technology in the future.
               ",autonomous vehicle
10.1016/j.knosys.2019.05.020,journal,Knowledge-Based Systems,sciencedirect,2019-09-15,sciencedirect,Adaptive and large-scale service composition based on deep reinforcement learning,https://api.elsevier.com/content/article/pii/S0950705119302266,"
                  In a service-oriented system, simple services are combined to form value-added services to meet users’ complex requirements. As a result, service composition has become a common practice in service computing. With the rapid development of web service technology, a massive number of web services with the same functionality but different non-functional attributes (e.g., QoS) are emerging. The increasingly complex user requirements and the large number of services lead to a significant challenge to select the optimal services from numerous candidates to achieve an optimal composition. Meanwhile, web services accessible via computer networks are inherently dynamic and the environment of service composition is also complex and unstable. Thus, service composition solutions need to be adaptable to the dynamic environment. To address these key challenges, we propose a new service composition scheme based on Deep Reinforcement Learning (DRL) for adaptive and large-scale service composition. The proposed approach is more suitable for the partially observable service environment, making it work better for real-world settings. A recurrent neural network is adopted to improve reinforcement learning, which can predict the objective function and enhance the ability to express and generalize. In addition, we employ the heuristic behavior selection strategy, in which the state set is divided into the hidden and fully observable state sets, to perform the targeted behavior selection strategy when facing with different types of states. The experimental results justify the effectiveness and efficiency, scalability, and adaptability of our methods by showing obvious advantages in composition results and efficiency for service composition.
               ",autonomous vehicle
10.1016/j.ijsu.2021.106151,journal,International Journal of Surgery,sciencedirect,2021-11-30,sciencedirect,A systematic review on artificial intelligence in robot-assisted surgery,https://api.elsevier.com/content/article/pii/S1743919121002867,"
                  Background
                  Despite the extensive published literature on the significant potential of artificial intelligence (AI) there are no reports on its efficacy in improving patient safety in robot-assisted surgery (RAS). The purposes of this work are to systematically review the published literature on AI in RAS, and to identify and discuss current limitations and challenges.
               
                  Materials and methods
                  A literature search was conducted on PubMed, Web of Science, Scopus, and IEEExplore according to PRISMA 2020 statement. Eligible articles were peer-review studies published in English language from January 1, 2016 to December 31, 2020. Amstar 2 was used for quality assessment. Risk of bias was evaluated with the Newcastle Ottawa Quality assessment tool. Data of the studies were visually presented in tables using SPIDER tool.
               
                  Results
                  Thirty-five publications, representing 3436 patients, met the search criteria and were included in the analysis. The selected reports concern: motion analysis (n = 17), urology (n = 12), gynecology (n = 1), other specialties (n = 1), training (n = 3), and tissue retraction (n = 1). Precision for surgical tools detection varied from 76.0% to 90.6%. Mean absolute error on prediction of urinary continence after robot-assisted radical prostatectomy (RARP) ranged from 85.9 to 134.7 days. Accuracy on prediction of length of stay after RARP was 88.5%. Accuracy on recognition of the next surgical task during robot-assisted partial nephrectomy (RAPN) achieved 75.7%.
               
                  Conclusion
                  The reviewed studies were of low quality. The findings are limited by the small size of the datasets. Comparison between studies on the same topic was restricted due to algorithms and datasets heterogeneity. There is no proof that currently AI can identify the critical tasks of RAS operations, which determine patient outcome. There is an urgent need for studies on large datasets and external validation of the AI algorithms used. Furthermore, the results should be transparent and meaningful to surgeons, enabling them to inform patients in layman's words.
               
                  Registration
                  Review Registry Unique Identifying Number: reviewregistry1225.
               ",autonomous vehicle
10.1016/B978-0-12-819154-5.00023-0,journal,Knowledge Discovery in Big Data from Astronomy and Earth Observation,sciencedirect,2020-12-31,sciencedirect,Chapter 12: Learning in Big Data: Introduction to Machine Learning,https://api.elsevier.com/content/article/pii/B9780128191545000230,"
               
                  Machine learning (ML) is a subset of artificial intelligence that develops dynamic algorithms capable of data-driven decisions, in contrast to models that follow static programming instructions. ML is concerned with enabling computer programs automatically to improve their performance at some tasks through experience. Astronomy and geosciences are two areas where the application of ML can be very fruitful. While the adoption of ML methods in astronomy and geosciences has been slow, there are several published studies using ML in these disciplines.
               This chapter introduces and evaluates several ML techniques. Special attention is given to inductive learning, which is among the most mature of the ML approaches currently available. The supervised, unsupervised, semisupervised and reinforcement learning types are described.
               ML algorithms are programs of data-driven inference tools that offer an automated means of recognizing patterns in high-dimensional data. Current trends and recent developments in ML algorithms are discussed. Scalable ML algorithms and frameworks are also described.
               Selected case study applications in which ML techniques have been successfully deployed in astronomy and geosciences are described.
               The chapter concludes with a summary of some of the key research issues in ML related to astronomy and geosciences, with emphasis on the scope for the application of ML algorithms to the rapidly increasing volumes of astronomical and remotely sensed geophysical data for geological mapping and other problems.
            ",autonomous vehicle
10.1016/j.tej.2020.106879,journal,The Electricity Journal,sciencedirect,2021-02-28,sciencedirect,A review of machine learning applications in IoT-integrated modern power systems,https://api.elsevier.com/content/article/pii/S1040619020301718,"
                  This paper aims to provide a systematic and comprehensive survey of state-of-the-art machine learning techniques and their potential applications in IoT-integrated power systems.
               ",autonomous vehicle
10.1016/B978-0-12-822828-9.00005-8,journal,Somatosensory Feedback for Neuroprosthetics,sciencedirect,2021-12-31,sciencedirect,Chapter 19: Prospect of data science and artificial intelligence for patient-specific neuroprostheses,https://api.elsevier.com/content/article/pii/B9780128228289000058,"
               Machine learning and its subfield deep learning have recently gained interest in scientific research community due to their ability to analyze and learn from big data. In this chapter, we discuss the capabilities, limitations, and current applications of unsupervised and supervised machine learning methods in addition to more recent deep learning techniques for the design and control of patient-specific neuroprostheses. Furthermore, we speculate on what they could promise for future applications.
            ",autonomous vehicle
10.1016/j.asoc.2021.107763,journal,Applied Soft Computing,sciencedirect,2021-11-30,sciencedirect,The algorithmic composition for music copyright protection under deep learning and blockchain,https://api.elsevier.com/content/article/pii/S1568494621006840,"
                  To strengthen music copyright protection effectively, a new deep learning neural network music composition neural network (MCNN) is proposed. The probability distribution of LSTM generation is adjusted by constructing a reasonable reward function. Music theory rules are used to constrain the generated music style to realize the intelligent generation of specific music style. Then, the digital music copyright protection system based on blockchain is constructed from three perspectives of confirming right, using right, and protecting right. The validity of the model is further verified by relevant data. The results show that the composition algorithm based on deep learning can realize music creation, and the qualified rate reaches 95.11%. Compared with the composition algorithm in the latest study, the model achieves 62.4 percent satisfaction with subjective samples and a recognition rate of 75.6 percent for musical sentiment classification. It is proved that the music copyright protection model based on block chain can ensure that the copyright owners of works obtain corresponding economic benefits from various distribution channels, which is helpful to build a harmonious music market environment. In short, the innovation of this study is reflected in that it fills in the gap of detailed comparative study of the differences in the application of different models, realizes the framework of music copyright protection system, and provides convenient conditions for composers.
               ",autonomous vehicle
10.1016/j.cosrev.2021.100389,journal,Computer Science Review,sciencedirect,2021-05-31,sciencedirect,A systematic review on Deep Learning approaches for IoT security,https://api.elsevier.com/content/article/pii/S1574013721000290,"
                  The constant spread of smart devices in many aspects of our daily life goes hand in hand with the ever-increasing demand for appropriate mechanisms to ensure they are resistant against various types of threats and attacks in the Internet of Things (IoT) environment. In this context, Deep Learning (DL) is emerging as one of the most successful and suitable techniques to be applied to different IoT security aspects.
                  This work aims at systematically reviewing and analyzing the research landscape about DL approaches applied to different IoT security scenarios. The contributions we reviewed are classified according to different points of view into a coherent and structured taxonomy in order to identify the gap in this pivotal research area.
                  The research focused on articles related to the keywords ’deep learning’, ’security’ and ’Internet of Things’ or ’IoT’ in four major databases, namely IEEEXplore, ScienceDirect, SpringerLink, and the ACM Digital Library.
                  We selected and reviewed 69 articles in the end. We have characterized these studies according to three main research questions, namely, the involved security aspects, the used DL network architectures, and the engaged datasets. A final discussion highlights the research gaps still to be investigated as well as the drawbacks and vulnerabilities of the DL approaches in the IoT security scenario.
               ",autonomous vehicle
10.1016/B978-0-12-820168-8.00015-8,journal,New Technologies for Power System Operation and Analysis,sciencedirect,2021-12-31,sciencedirect,Chapter nine: Automated optimal control in energy systems: the reinforcement learning approach,https://api.elsevier.com/content/article/pii/B9780128201688000158,"
               With the development of smart grid technologies an increasing number of new devices and participants have joined modern energy systems and are inevitably making them more complicated and interdependent than ever. Optimally controlling such a complex energy system and maintaining its operation in a high-efficient, secure, and resilient manner are challenging tasks to the system operators. Fortunately, the revolution in deep learning and artificial intelligence (AI), both from hardware and algorithms perspectives, has provided new ideas and solutions to many previously intractable problems. As a result, this advance in computer science also sparked great research interests in utilizing AI in solving engineering problems related to the modern energy systems.
               Among many AI techniques, deep reinforcement learning (DRL) has demonstrated great potential for solving sequential optimization problems, which are very common in the engineering domains. Its ability to handle nonlinearity and stochasticity in controlled systems has out-competed many traditional optimal control algorithms. Therefore in this chapter, we focus on the state-of-the-art of DRL concepts and related algorithms, compare their pros and cons with traditional optimal control approaches and discuss the typical workflow for leveraging RL in solving complex problems in modern energy systems.
            ",autonomous vehicle
10.1016/j.compag.2018.10.024,journal,Computers and Electronics in Agriculture,sciencedirect,2018-12-31,sciencedirect,Forecasting yield by integrating agrarian factors and machine learning models: A survey,https://api.elsevier.com/content/article/pii/S0168169918311529,"
                  The advancement in science and technology has led to a substantial amount of data from various fields of agriculture to be incremented in the public domain. Hence a desideratum arises from the investigation of the available data and integrating them with a process like a crop improvement, yield prediction, crop disease analysis, identifying water stress, and so on. Computing techniques like Machine learning is a new advent for the analysis and resoluteness of these intricate issues. Various analytical models like Decision Trees, Random Forests, Support Vector Machines, Bayesian Networks, and Artificial Neural Networks, and so on, have been utilized for engendering the models and analyze the results. These methods enable to analyze soil, climate, and water regime which are significantly involved in crop growth and precision farming. This survey incorporates an overview of some of the existing supervised and unsupervised machine learning models associated with the crop yield in literature. Moreover, this survey compares one approach with other using various error measures like Root Mean Square Error (RMSE), Relative Root Mean Square Error (RRMSE), Mean Absolute Error (MAE), and Coefficient of Determination (R2).
               ",autonomous vehicle
10.1016/j.procs.2020.03.127,journal,Procedia Computer Science,sciencedirect,2020-12-31,sciencedirect,Open Source Machine Learning Frameworks for Industrial Internet of Things,https://api.elsevier.com/content/article/pii/S1877050920305652,"Information and communication technology has revolutionized the industrial operations and productions. The industries irrespective of size, whether small or large, have felt the need of artificial intelligence and machine learning techniques to process the terabytes of data generated through sensors, actuators, industrial management systems, and web applications. These data have the characteristics of volume (terabyte) and variety (image, audio, video, graphics) and thus customized models and techniques are required for analysis and management. The advancement in computer hardware, processing power, storage capacity, and cloud computing have led to experimentation and implementation of machine learning models in industrial domain for resource optimization, operation management, and quality control. However, the industrial Data Analysts face the dilemma of selecting the affordable and easy to use machine learning frameworks that suite their need and expectations. The study investigates the open source machine learning frameworks, aligned with the industrial domain (processing data generated from Industrial Internet of Things), in terms of usage, programming languages, implementations, and future prospects.",autonomous vehicle
10.1016/B978-0-12-816637-6.00009-9,journal,Big Data Analytics for Cyber-Physical Systems,sciencedirect,2019-12-31,sciencedirect,Chapter 9: Reinforcement learning and deep neural network for autonomous driving,https://api.elsevier.com/content/article/pii/B9780128166376000099,"
               This chapter deals with a behavioral decision model for autonomous driving. Such a model aims at an application in which reinforcement learning and deep neural networks are used for autonomous driving. The training and evaluation takes place in a simulation. For this purpose, SUMO Simulation will be used to create own scenarios. An own sensor modeling will also be developed. The Deep Deterministic Policy Gradient (DDPG) method by Lillicrap et al. (2015) is used as a reinforcement learning algorithm and is adapted to this usecase. The work shows that the learned model can react to different types of drivers and the surrounding traffic. It also supports a safe and fast response to driving reactions.
            ",autonomous vehicle
10.1016/j.autcon.2021.103892,journal,Automation in Construction,sciencedirect,2021-11-30,sciencedirect,Deep-learning-based visual data analytics for smart construction management,https://api.elsevier.com/content/article/pii/S0926580521003435,"
                  Visual data captured at construction sites is a rich source of information for the day-to-day operation of construction projects. The development of deep-learning-based methods has demonstrated their capabilities in analyzing complex visual data and inferring valuable insights. Recent applications of these methods in construction have also shown promising performance in making the construction management process smarter. To understand the current research trends and to highlight future research directions, this study reviews state-of-the-art deep-learning applications on visual data analytics in the context of construction project management. This in-depth review identifies six major fields and fifty-two subfields of construction management where deep-learning-based visual data analytics have been applied. It also proposes a generalized workflow for applying deep-learning-based visual data analytics methods for solving construction management problems. In addition, the study highlights three future research directions where deep-learning-based visual data analytics can be applied on relatively less explored 3D visual data.
               ",autonomous vehicle
10.1016/j.jss.2021.110941,journal,Journal of Systems and Software,sciencedirect,2021-06-30,sciencedirect,Multilayered review of safety approaches for machine learning-based systems in the days of AI,https://api.elsevier.com/content/article/pii/S0164121221000388,"
                  The unprecedented advancement of artificial intelligence (AI) in recent years has altered our perspectives on software engineering and systems engineering as a whole. Nowadays, software-intensive intelligent systems rely more on a learning model than thousands of lines of codes. Such alteration has led to new research challenges in the engineering process that can ensure the safe and beneficial behavior of AI systems. This paper presents a literature survey of the significant efforts made in the last fifteen years to foster safety in complex intelligent systems. This survey covers relevant aspects of AI safety research including safety requirements engineering, safety-driven design at both system and machine learning (ML) component level, validation and verification from the perspective of software and system engineers. We categorize these research efforts based on a three-layered conceptual framework for developing and maintaining AI systems. We also perform a gap analysis to emphasize the open research challenges in ensuring safe AI. Finally, we conclude the paper by providing future research directions and a road map for AI safety.
               ",autonomous vehicle
10.1016/j.wear.2021.203902,journal,Wear,sciencedirect,2021-08-15,sciencedirect,Research on tool wear prediction based on temperature signals and deep learning,https://api.elsevier.com/content/article/pii/S004316482100291X,"
                  Tool condition monitoring is an important part of tool prediagnosis and health management systems. Accurate prediction of tool wear is greatly important for making full use of tool life, improving the production efficiency and product quality, and reducing tool costs. In this paper, an intelligent cutting tool embedded with a thin-film thermocouple collected the temperature signals during the tool life cycle. As a deep learning method, stacked sparse autoencoders model with a backpropagation neural network for regression was proposed to predict tool wear based on raw temperature signals. An improved loss function with sparse and weight penalty terms was used to enhance the robustness and generalizability of the stacked sparse autoencoders model. To confirm the superiority of the proposed model, the predictive performance was compared with that of traditional machine learning methods, such as backpropagation neural network and support vector regression with manual features extraction. The root mean square error and coefficient of determination were calculated to evaluate the prediction model. The experimental results showed that the proposed model outperformed the traditional methods with a higher prediction accuracy and better prediction stability. The feasibility and accuracy of temperature signals for tool wear prediction were also confirmed.
               ",autonomous vehicle
10.1016/j.tej.2020.106890,journal,The Electricity Journal,sciencedirect,2021-02-28,sciencedirect,Artificial intelligence for operation and control: The case of microgrids,https://api.elsevier.com/content/article/pii/S1040619020301822,"
                  Research on artificial intelligence (AI) has advanced significantly in recent years. A variety of AI algorithms have shown great promise in a large number of applications for power system operation and control. This article examines the potential of applying AI in microgrids (MGs). Specifically, as MGs commonly employ onsite generation including an increasing penetration of non-dispatchable distributed energy resources (DERs) and require seamless transition between operation modes (e.g., grid-connected and island) for different operation scenarios, the energy management within an MG is particularly complicated. Many factors including lack of inertia needed for system stability, generation uncertainty from DERs, and complex MG network topology composition (e.g., AC, DC, and hybrid AC/DC MGs) contribute to the difficulty of microgrid energy management. AI techniques such as deep learning (DL) and deep reinforcement learning (DRL) have recently demonstrated their excellence in tackling problems pertinent to decision making, providing a possible solution to overcome the above-mentioned challenges. This article discusses the applications of AI to MG operation and control, with an emphasis on DL and DRL. We survey the available DL and DRL technologies and their applications to power grids. We also investigate the unique issues associated with MGs including their layered control architecture, single vs. networked structure, and topology optimization. Perspectives on the ongoing challenges and viable AI solutions to MG operation and control are presented.
               ",autonomous vehicle
10.1016/j.cbi.2021.109533,journal,Chemico-Biological Interactions,sciencedirect,2021-08-25,sciencedirect,Application of artificial intelligence for detection of chemico-biological interactions associated with oxidative stress and DNA damage,https://api.elsevier.com/content/article/pii/S0009279721001691,"
                  In recent years, various AI-based methods have been developed in order to uncover chemico-biological interactions associated with DNA damage and oxidative stress. Various decision trees, bayesian networks, random forests, logistic regression models, support vector machines as well as deep learning tools, have great potential in the area of molecular biology and toxicology, and it is estimated that in the future, they will greatly contribute to our understanding of molecular and cellular mechanisms associated with DNA damage and repair. In this concise review, we discuss recent attempts to build machine learning tools for assessment of radiation – induced DNA damage as well as algorithms that can analyze the data from the most frequently used DNA damage assays in molecular biology. We also review recent works on the detection of antioxidant proteins with machine learning, and the use of AI-related methods for prediction and evaluation of noncoding DNA sequences. Finally, we discuss previously published research on the potential application of machine learning tools in aging research.
               ",autonomous vehicle
10.1016/bs.apcsb.2021.02.003,journal,Advances in Protein Chemistry and Structural Biology,sciencedirect,2021-12-31,sciencedirect,Chapter Five: Proteome analysis using machine learning approaches and its applications to diseases,https://api.elsevier.com/content/article/pii/S1876162321000213,"
                  With the tremendous developments in the fields of biological and medical technologies, huge amounts of data are generated in the form of genomic data, images in medical databases or as data on protein sequences, and so on. Analyzing this data through different tools sheds light on the particulars of the disease and our body's reactions to it, thus, aiding our understanding of the human health. Most useful of these tools is artificial intelligence and deep learning (DL). The artificially created neural networks in DL algorithms help extract viable data from the datasets, and further, to recognize patters in these complex datasets. Therefore, as a part of machine learning, DL helps us face all the various challenges that come forth during protein prediction, protein identification and their quantification. Proteomics is the study of such proteins, their structures, features, properties and so on. As a form of data science, Proteomics has helped us progress excellently in the field of genomics technologies. One of the major techniques used in proteomics studies is mass spectrometry (MS). However, MS is efficient with analysis of large datasets only with the added help of informatics approaches for data analysis and interpretation; these mainly include machine learning and deep learning algorithms. In this chapter, we will discuss in detail the applications of deep learning and various algorithms of machine learning in proteomics.
               ",autonomous vehicle
10.1016/j.eswa.2020.113650,journal,Expert Systems with Applications,sciencedirect,2020-12-30,sciencedirect,Towards integrated dialogue policy learning for multiple domains and intents using Hierarchical Deep Reinforcement Learning,https://api.elsevier.com/content/article/pii/S0957417420304747,"
                  Creation of Expert and Intelligent Dialogue/Virtual Agent (VA) that can serve complicated and intricate tasks (need) of the user related to multiple domains and its various intents is indeed quite challenging as it necessitates the agent to concurrently handle multiple subtasks in different domains. This paper presents an expert, unified and a generic Deep Reinforcement Learning (DRL) framework that creates dialogue managers competent for managing task-oriented conversations embodying multiple domains along with their various intents and provide the user with an expert system which is a one stop for all queries. In order to address these multiple aspects, the dialogue exchange between the user and the VA is split into hierarchies, so as to isolate and identify subtasks belonging to different domains. The notion of Hierarchical Reinforcement Learning (HRL) specifically options is employed to learn optimal policies in these hierarchies that operate at varying time steps to accomplish the user goal. The dialogue manager encompasses a top-level domain meta-policy, intermediate-level intent meta-policies in order to select amongst varied and multiple subtasks or options and low-level controller policies to select primitive actions to complete the subtask given by the higher-level meta-policies in varying intents and domains. Sharing of controller policies among overlapping subtasks enables the meta-policies to be generic. The proposed expert framework has been demonstrated in the domains of “Air Travel” and “Restaurant”. Experiments as compared to several strong baselines and a state of the art model establish the efficiency of the learned policies and the need for such expert models capable of handling complex and composite tasks.
               ",autonomous vehicle
10.1016/j.enganabound.2021.09.032,journal,Engineering Analysis with Boundary Elements,sciencedirect,2022-01-01,sciencedirect,Machine Learning to approximate free-surface Green's function and its application in wave-body interactions,https://api.elsevier.com/content/article/pii/S0955799721002848,"
                  Efficient and accurate evaluation of free-surface Green's function is the key to hydrodynamic problems solved by boundary element method (
                        BEM
                     ). However, so far, there is still no unified numerical method that can accurately approximate all kinds of free-surface Green's functions. In theory, machine learning can be used to approximate any function with high accuracy. In the present study, neural networks are used for the numerical approximation of pulsating source Green's function, and the corresponding optimization algorithms for gradient descent are adopted. Regularization is used to prevent overfitting. Double-precision numerical results obtained by Romberg quadrature are used as training set and validation set. To improve the accuracy of present numerical approximation, the calculation domain of both Green's function and its gradient are divided into 4 zones, and various network structures are adopted in each zone. Finally, a machine model, called ZeroGF, is obtained by machine learning that can predict Green's function and its derivatives. The numerical results show that ZeroGF owns at least 4 digits of accuracy in above 99% area of all zones. BEM program incorporating with ZeroGF is validated in the hydrodynamic calculation of the hemisphere, Wigley III and the Barge. Good accuracy and reliability of ZeroGF is shown.
               ",autonomous vehicle
10.1016/j.eswa.2021.114774,journal,Expert Systems with Applications,sciencedirect,2021-07-15,sciencedirect,Learning style detection in E-learning systems using machine learning techniques,https://api.elsevier.com/content/article/pii/S0957417421002153,"
                  Learning style plays a vital role in helping students retain learned concepts for a longer time and also improves the understanding of the concepts. Learning styles in offline and online scenarios are recognized using questionnaires. The recent trend is to identify and use attributes to detect the learning style of the learner automatically without disturbing the learner. The paper is an extension of the authors' earlier work with some changes to the methodology. In this paper, the authors have identified new attributes and scaled-down the attributes identified earlier, which would help identify the learner's learning style. The authors implemented classification algorithms and compared the accuracy of the different algorithms on the dataset. Various interesting patterns are observed in learner's behaviour while learning different types of concepts in different situations.
               ",autonomous vehicle
10.1016/j.eswa.2021.115076,journal,Expert Systems with Applications,sciencedirect,2021-10-15,sciencedirect,The impact of artificial intelligence and big data on end-stage kidney disease treatments,https://api.elsevier.com/content/article/pii/S0957417421005170,"
                  In the field of medicine, decision-making has traditionally been carried out based on the best available scientific information and the experience of specialists using data found in analog formats such as radiographies, medical reports, and handwritten notes, among others. In this sense, the Big Data phenomenon is changing the world of medicine since the technologies that have been developed have made available to researchers and clinicians enormous amounts of data in digital formats that can be used to complement or help in complex tasks such as mentioned decision making. A key element in this process is data analysis techniques, since without them it is not possible to exploit the information. Currently the most used techniques are based on algorithms in the area of artificial intelligence and more specifically machine learning. This paper focuses on a specific domain of medicine, renal replacement therapies for end-stage renal disease, where machine learning is beginning to be used as a complementary tool to predict or make decisions. This paper provides a narrative review of the main machine learning methods that are being used to conduct end-stage renal disease treatment analyses.
               ",autonomous vehicle
10.1016/j.addr.2021.05.015,journal,Advanced Drug Delivery Reviews,sciencedirect,2021-08-31,sciencedirect,Harnessing artificial intelligence for the next generation of 3D printed medicines,https://api.elsevier.com/content/article/pii/S0169409X21001794,"
                  Artificial intelligence (AI) is redefining how we exist in the world. In almost every sector of society, AI is performing tasks with super-human speed and intellect; from the prediction of stock market trends to driverless vehicles, diagnosis of disease, and robotic surgery. Despite this growing success, the pharmaceutical field is yet to truly harness AI. Development and manufacture of medicines remains largely in a ‘one size fits all’ paradigm, in which mass-produced, identical formulations are expected to meet individual patient needs. Recently, 3D printing (3DP) has illuminated a path for on-demand production of fully customisable medicines. Due to its flexibility, pharmaceutical 3DP presents innumerable options during formulation development that generally require expert navigation. Leveraging AI within pharmaceutical 3DP removes the need for human expertise, as optimal process parameters can be accurately predicted by machine learning. AI can also be incorporated into a pharmaceutical 3DP ‘Internet of Things’, moving the personalised production of medicines into an intelligent, streamlined, and autonomous pipeline. Supportive infrastructure, such as The Cloud and blockchain, will also play a vital role. Crucially, these technologies will expedite the use of pharmaceutical 3DP in clinical settings and drive the global movement towards personalised medicine and Industry 4.0.
               ",autonomous vehicle
10.1016/B978-0-323-85064-3.00009-1,journal,Image Processing for Automated Diagnosis of Cardiac Diseases,sciencedirect,2021-12-31,sciencedirect,Chapter 6: Impetus to machine learning in cardiac disease diagnosis,https://api.elsevier.com/content/article/pii/B9780323850643000091,"
               Machine learning is a branch of computer science, and it is a subset of artificial intelligence. It comprises many algorithms based on statistical methods to build automated systems for solving a particular problem. Due to its versatility, it is popular in many fields in real life, including scientific researches, healthcare field, industries, pharmaceutical field for drug discovery, social anomalies such as epidemic, and pandemic diseases spread. This chapter aims to identify the impact of machine learning techniques in the diagnosis of cardiac diseases. This chapter starts with the justification of the need for machine learning technology in the healthcare field. The basics of machine learning technology and its various algorithms are explained in the next section. The applications of these algorithms, which includes the diagnosis of various diseases such as diabetics, coronary artery disease (CAD), coronary heart disease (CHD), liver ailments, cancer detection and prevention, radiology, pathology, clinical trials, robotic surgery, drug discovery, and personalized treatments, are described from the contemporary researches. The challenges it faces in the healthcare field are also listed. In the end, the constraints of machine learning techniques in the healthcare field are explained with the suggestions to make accurate and efficient diagnoses in the future.
            ",autonomous vehicle
10.1016/j.ces.2021.117224,journal,Chemical Engineering Science,sciencedirect,2022-02-02,sciencedirect,"Machine learning in solid heterogeneous catalysis: Recent developments, challenges and perspectives",https://api.elsevier.com/content/article/pii/S0009250921007892,"
                  Recently, the availability of extensive catalysis-related data generated by experimental data and theoretical calculations has promoted the development of machine learning (ML) techniques for novel heterogeneous catalyst development. ML is an effective tool in automating the generation, processing, and interpretation of large catalyst datasets with superior properties than the conventional statistical approaches. Also, ML have enabled the identification of accurate data-driven models that have been used to establish key relationships between the features of materials and targeted catalytic performance, such as activity, selectivity, and stability. These advances have resulted in the development of efficient design or screening guidelines for solid-state catalysts with targeted properties. However, extending the existing ML approaches to obtain accurate predictions of catalyst performance or design strategies for high-performance catalysts still poses several challenges. In this review, we discuss the recent milestones on the application of ML for solid heterogeneous catalysis and present the limitations and challenges of ML in this field. We also discuss potential future directions for the effective use of ML in solid heterogeneous catalyst design.
               ",autonomous vehicle
10.1016/j.medj.2021.04.006,journal,Med,sciencedirect,2021-06-11,sciencedirect,Machine learning in clinical decision making,https://api.elsevier.com/content/article/pii/S2666634021001550,"
                  Machine learning is increasingly integrated into clinical practice, with applications ranging from pre-clinical data processing, bedside diagnosis assistance, patient stratification, treatment decision making, and early warning as part of primary and secondary prevention. However, a multitude of technological, medical, and ethical considerations are critical in machine-learning utilization, including the necessity for careful validation of machine-learning-based technologies in real-life contexts, unbiased evaluation of benefits and risks, and avoidance of technological over-dependence and associated loss of clinical, ethical, and social-related decision-making capacities. Other challenges include the need for careful benchmarking and external validations, dissemination of end-user knowledge from computational experts to field users, and responsible code and data sharing, enabling transparent assessment of pipelines. In this review, we highlight key promises and achievements in integration of machine-learning platforms into clinical medicine while highlighting limitations, pitfalls, and challenges toward enhanced integration of learning systems into the medical realm.
               ",autonomous vehicle
10.1016/j.jclepro.2020.124022,journal,Journal of Cleaner Production,sciencedirect,2021-01-01,sciencedirect,Artificial intelligence in nuclear industry: Chimera or solution?,https://api.elsevier.com/content/article/pii/S0959652620340671,"
                  Nuclear industry is in crisis and innovation is the central theme of its survival in future. Artificial intelligence has made a quantum leap in last few years. This paper comprehensively analyses recent advancement in artificial intelligence for its applications in nuclear power industry. A brief background of machine learning techniques researched and proposed in this domain is outlined. A critical assessment of various nuances of artificial intelligence for nuclear industry is provided. Lack of operational data from real power plant especially for transients and accident scenario is a major concern regarding the accuracy of intelligent systems. There is no universally agreed opinion among researchers for selecting the best artificial intelligence techniques for a specific purpose as intelligent systems developed by various researchers are based on different data set. Interlaboratory work frame or round-robin programme to develop the artificial intelligent tool for any specific purpose, based on the same data base, can be crucial in claiming the accuracy and thus the best technique. The black box nature of artificial techniques also poses a serious challenge for its implementation in nuclear industry, as it makes them prone to fooling.
               ",autonomous vehicle
10.1016/j.eswa.2021.114598,journal,Expert Systems with Applications,sciencedirect,2021-07-01,sciencedirect,Predictive maintenance system for production lines in manufacturing: A machine learning approach using IoT data in real-time,https://api.elsevier.com/content/article/pii/S0957417421000397,"
                  In this study, a data driven predictive maintenance system was developed for production lines in manufacturing. By utilizing the data generated from IoT sensors in real-time, the system aims to detect signals for potential failures before they occur by using machine learning methods. Consequently, it helps address the issues by notifying operators early such that preventive actions can be taken prior to a production stop. In current study, the effectiveness of the system was also assessed using real-world manufacturing system IoT data. The evaluation results indicated that the predictive maintenance system was successful in identifying the indicators of potential failures and it can help prevent some production stops from happening. The findings of comparative evaluations of machine learning algorithms indicated that models of Random Forest, a bagging ensemble algorithm, and XGBoost, a boosting method, appeared to outperform the individual algorithms in the assessment. The best performing machine learning models in this study have been integrated into the production system in the factory.
               ",autonomous vehicle
10.1016/j.apergo.2021.103574,journal,Applied Ergonomics,sciencedirect,2022-01-31,sciencedirect,The role of machine learning in the primary prevention of work-related musculoskeletal disorders: A scoping review,https://api.elsevier.com/content/article/pii/S0003687021002210,"To determine the applications of machine learning (ML) techniques used for the primary prevention of work-related musculoskeletal disorders (WMSDs), a scoping review was conducted using seven literature databases. Of the 4,639 initial results, 130 primary research studies were deemed relevant for inclusion. Studies were reviewed and classified as a contribution to one of six steps within the primary WMSD prevention research framework by van der Beek et al. (2017). ML techniques provided the greatest contributions to the development of interventions (48 studies), followed by risk factor identification (33 studies), underlying mechanisms (29 studies), incidence of WMSDs (14 studies), evaluation of interventions (6 studies), and implementation of effective interventions (0 studies). Nearly a quarter (23.8%) of all included studies were published in 2020. These findings provide insight into the breadth of ML techniques used for primary WMSD prevention and can help identify areas for future research and development.",autonomous vehicle
10.1053/j.jvca.2020.08.048,journal,Journal of Cardiothoracic and Vascular Anesthesia,sciencedirect,2021-01-31,sciencedirect,Artificial Intelligence in Echocardiography for Anesthesiologists,https://api.elsevier.com/content/article/pii/S1053077020308430,"
                  Echocardiography is a unique diagnostic tool for intraoperative monitoring and assessment of patients with cardiovascular diseases. However, there are high levels of interoperator variations in echocardiography interpretations that could lead to inaccurate diagnosis and incorrect treatment. Furthermore, anesthesiologists are faced with the additional challenge to interpret echocardiography and make decisions in a limited timeframe from these complex data. The need for an automated, less operator-dependent process that enhances speed and accuracy of echocardiography analysis is crucial for anesthesiologists. Artificial intelligence is playing an increasingly important role in the medical field and could help anesthesiologists analyze complex echocardiographic data while adding increased accuracy and consistency to interpretation. This review aims to summarize practical use of artificial intelligence in echocardiography and discusses potential limitations and challenges in the future for anesthesiologists.
               ",autonomous vehicle
10.1016/j.compchemeng.2019.05.029,journal,Computers & Chemical Engineering,sciencedirect,2019-08-04,sciencedirect,Reinforcement Learning – Overview of recent progress and implications for process control,https://api.elsevier.com/content/article/pii/S0098135419300754,"
                  This paper provides an introduction to Reinforcement Learning (RL) technology, summarizes recent developments in this area, and discusses their potential implications for the field of process control, and more generally, of operational decision-making. The paper begins with an introduction to RL that allows an agent to learn, through trial and error, the best way to accomplish a task. We then highlight new developments in RL that have led to the recent wave of applications and media interest. A comparison of the key features of RL and mathematical programming based methods (e.g., model predictive control) is then presented to clarify their similarities and differences. This is followed by an assessment of several ways that RL technology can potentially be used in process control and operational decision applications. A final section summarizes our conclusions and lists directions for future RL research that may improve its relevance for the process systems engineering field.
               ",autonomous vehicle
10.1016/j.neucom.2021.08.096,journal,Neurocomputing,sciencedirect,2021-11-13,sciencedirect,Skin disease diagnosis with deep learning: A review,https://api.elsevier.com/content/article/pii/S0925231221012935,"
                  Skin cancer is one of the most threatening diseases worldwide. However, diagnosing skin cancer correctly is challenging. Recently, deep learning algorithms have emerged to achieve excellent performance in various tasks. Particularly, they have been applied to the skin disease diagnosis tasks. In this paper, we present a review on deep learning methods and their applications in skin disease diagnosis. We first present a brief introduction to skin diseases and image acquisition methods in dermatology, and list several publicly available skin datasets. Then, we introduce the conception of deep learning, and review popular deep learning architectures and popular frameworks facilitating the implementation of deep learning algorithms. Thereafter, performance evaluation metrics are presented. As an important part of this article, we then review the literature involving deep learning methods for skin disease diagnosis from several aspects according to the specific tasks. Additionally, we discuss the challenges faced in the area and suggest possible future research directions. The major purpose of this article is to provide a conceptual and systematically review of the recent works on skin disease diagnosis with deep learning. Given the popularity of deep learning, there remains great challenges in the area, as well as opportunities that we can explore in the future.
               ",autonomous vehicle
10.1016/j.cmi.2020.02.003,journal,Clinical Microbiology and Infection,sciencedirect,2020-10-31,sciencedirect,"Machine learning in infection management using routine electronic health records: tools, techniques, and reporting of future technologies",https://api.elsevier.com/content/article/pii/S1198743X20300823,"Background Machine learning (ML) is increasingly being used in many areas of health care. Its use in infection management is catching up as identified in a recent review in this journal. We present here a complementary review to this work. Objectives To support clinicians and researchers in navigating through the methodological aspects of ML approaches in the field of infection management. Sources A Medline search was performed with the keywords artificial intelligence, machine learning, infection∗, and infectious disease∗ for the years 2014–2019. Studies using routinely available electronic hospital record data from an inpatient setting with a focus on bacterial and fungal infections were included. Content Fifty-two studies were included and divided into six groups based on their focus. These studies covered detection/prediction of sepsis (n = 19), hospital-acquired infections (n = 11), surgical site infections and other postoperative infections (n = 11), microbiological test results (n = 4), infections in general (n = 2), musculoskeletal infections (n = 2), and other topics (urinary tract infections, deep fungal infections, antimicrobial prescriptions; n = 1 each). In total, 35 different ML techniques were used. Logistic regression was applied in 18 studies followed by random forest, support vector machines, and artificial neural networks in 18, 12, and seven studies, respectively. Overall, the studies were very heterogeneous in their approach and their reporting. Detailed information on data handling and software code was often missing. Validation on new datasets and/or in other institutions was rarely done. Clinical studies on the impact of ML in infection management were lacking. Implications Promising approaches for ML use in infectious diseases were identified. But building trust in these new technologies will require improved reporting. Explainability and interpretability of the models used were rarely addressed and should be further explored. Independent model validation and clinical studies evaluating the added value of ML approaches are needed.",autonomous vehicle
10.1016/j.neunet.2019.09.004,journal,Neural Networks,sciencedirect,2020-01-31,sciencedirect,Spiking Neural Networks and online learning: An overview and perspectives,https://api.elsevier.com/content/article/pii/S0893608019302655,"
                  Applications that generate huge amounts of data in the form of fast streams are becoming increasingly prevalent, being therefore necessary to learn in an online manner. These conditions usually impose memory and processing time restrictions, and they often turn into evolving environments where a change may affect the input data distribution. Such a change causes that predictive models trained over these stream data become obsolete and do not adapt suitably to new distributions. Specially in these non-stationary scenarios, there is a pressing need for new algorithms that adapt to these changes as fast as possible, while maintaining good performance scores. Unfortunately, most off-the-shelf classification models need to be retrained if they are used in changing environments, and fail to scale properly. Spiking Neural Networks have revealed themselves as one of the most successful approaches to model the behavior and learning potential of the brain, and exploit them to undertake practical online learning tasks. Besides, some specific flavors of Spiking Neural Networks can overcome the necessity of retraining after a drift occurs. This work intends to merge both fields by serving as a comprehensive overview, motivating further developments that embrace Spiking Neural Networks for online learning scenarios, and being a friendly entry point for non-experts.
               ",autonomous vehicle
10.1016/j.eswa.2020.113820,journal,Expert Systems with Applications,sciencedirect,2021-02-28,sciencedirect,Multi-DQN: An ensemble of Deep Q-learning agents for stock market forecasting,https://api.elsevier.com/content/article/pii/S0957417420306321,"
                  The stock market forecasting is one of the most challenging application of machine learning, as its historical data are naturally noisy and unstable. Most of the successful approaches act in a supervised manner, labeling training data as being of positive or negative moments of the market. However, training machine learning classifiers in such a way may suffer from over-fitting, since the market behavior depends on several external factors like other markets trends, political events, etc. In this paper, we aim at minimizing such problems by proposing an ensemble of reinforcement learning approaches which do not use annotations (i.e. market goes up or down) to learn, but rather learn how to maximize a return function over the training stage. In order to achieve this goal, we exploit a Q-learning agent trained several times with the same training data and investigate its ensemble behavior in important real-world stock markets. Experimental results in intraday trading indicate better performance than the conventional Buy-and-Hold strategy, which still behaves well in our setups. We also discuss qualitative and quantitative analyses of these results.
               ",autonomous vehicle
10.1016/j.procir.2019.03.041,journal,Procedia CIRP,sciencedirect,2019-12-31,sciencedirect,"Design, Implementation and Evaluation of Reinforcement Learning for an Adaptive Order Dispatching in Job Shop Manufacturing Systems",https://api.elsevier.com/content/article/pii/S2212827119303464,"Modern production systems tend to have smaller batch sizes, a larger product variety and more complex material flow systems. Since a human oftentimes can no longer act in a sufficient manner as a decision maker under these circumstances, the demand for efficient and adaptive control systems is rising. This paper introduces a methodical approach as well as guideline for the design, implementation and evaluation of Reinforcement Learning (RL) algorithms for an adaptive order dispatching. Thereby, it addresses production engineers willing to apply RL. Moreover, a real-world use case shows the successful application of the method and remarkable results supporting real-time decision-making. These findings comprehensively illustrate and extend the knowledge on RL.",autonomous vehicle
10.1016/j.procir.2021.01.086,journal,Procedia CIRP,sciencedirect,2021-12-31,sciencedirect,Selection of Suitable Machine Learning Algorithms for Classification Tasks in Reverse Logistics,https://api.elsevier.com/content/article/pii/S2212827121001141,"The use of machine learning (ML) for data analysis is constantly increasing in industry. Reverse logistics, which struggles with many uncertainties related to complex processes and can benefit from implementing ML. Yet, such solutions are often not applied due to lack of common knowledge. The goal of this paper is to support a preselection of ML algorithms by developing a concept that provides a comprehensive overview of basic ML algorithms and their characteristics. Therefore, basic supervised ML algorithms and a set of criteria are selected and described, matching both in a table. The applicability of the concept is reviewed on an exemplary use case from reverse logistics.",autonomous vehicle
10.1016/j.cirp.2018.04.041,journal,CIRP Annals,sciencedirect,2018-12-31,sciencedirect,Reinforcement learning for adaptive order dispatching in the semiconductor industry,https://api.elsevier.com/content/article/pii/S0007850618300659,"
                  The digitalization of production systems tends to provide a huge amount of data from heterogeneous sources. This is particularly true for the semiconductor industry wherein real time process monitoring is inherently required to achieve a high yield of good parts. An application of data-driven algorithms in production planning to enhance operational excellence for complex semiconductor production systems is currently missing. This paper shows the successful implementation of a reinforcement learning-based adaptive control system for order dispatching in the semiconductor industry. Furthermore, a performance comparison of the learning-based control system with the traditionally used rule-based system shows remarkable results. Since a strict rulebook does not bind the learning-based control system, a flexible adaption to changes in the environment can be achieved through a combination of online and offline learning.
               ",autonomous vehicle
10.1016/j.jmst.2020.12.010,journal,Journal of Materials Science & Technology,sciencedirect,2021-07-20,sciencedirect,Accelerating materials discovery using machine learning,https://api.elsevier.com/content/article/pii/S100503022031029X,"
                  The discovery of new materials is one of the driving forces to promote the development of modern society and technology innovation, the traditional materials research mainly depended on the trial-and-error method, which is time-consuming and laborious. Recently, machine learning (ML) methods have made great progress in the researches of materials science with the arrival of the big-data era, which gives a deep revolution in human society and advance science greatly. However, there exist few systematic generalization and summaries about the applications of ML methods in materials science. In this review, we first provide a brief account of the progress of researches on materials science with ML employed, the main ideas and basic procedures of this method are emphatically introduced. Then the algorithms of ML which were frequently used in the researches of materials science are classified and compared. Finally, the recent meaningful applications of ML in metal materials, battery materials, photovoltaic materials and metallic glass are reviewed.
               ",autonomous vehicle
10.1016/j.imed.2021.06.004,journal,Intelligent Medicine,sciencedirect,2021-07-19,sciencedirect,The application of artificial intelligence to chest medical image analysis,https://api.elsevier.com/content/article/pii/S2667102621000358,"The aim of this article is to review recent progress in the application of artificial intelligence to chest medical image analysis. The lungs, bone, and mediastinum were included in terms of anatomy, while X-ray and computed tomography (CT), with and without contrast enhancement, were considered regarding imaging modalities. Four key components of deep learning were summarized, namely, network architectures, learning strategies, optimization methods, and vision tasks. Disease-specific applications were discussed in detail with respect to the dimension of the data input, network architecture, and modality: lung cancer, pneumonia, tuberculosis, pulmonary embolism, chronic obstructive pulmonary disease, and interstitial lung disease for lung; traumatic fractures, osteoporosis, osteoporotic fractures, and bone metastases for bone; and coronary artery calcification and aortic dissection for vascular diseases. Finally, five promising research directions and possible solutions were presented for future work.",autonomous vehicle
10.1016/B978-0-12-815739-8.00001-8,journal,Machine Learning,sciencedirect,2020-12-31,sciencedirect,Chapter 1: Introduction to machine learning,https://api.elsevier.com/content/article/pii/B9780128157398000018,"
               Machine learning is becoming increasingly popular in the neuroscientific literature. However, navigating the literature can easily become overwhelming, especially for the nonexpert. In this chapter, we provide an introduction to machine learning aimed at researchers, clinicians, and students with an interest in brain disorders, including psychiatry and neurology. We first provide a brief overview of how the most prominent theories of human learning from the fields of psychology and neuroscience influenced the development of modern cutting-edge machine learning methods. Second, we discuss how these methods differ from classical statistics and why they could be particularly suited to the investigation of brain disorders. In the final section of this chapter, we introduce a high-level taxonomy of the main approaches used in the machine learning literature: supervised learning, unsupervised learning, semisupervised learning, and reinforcement learning.
            ",autonomous vehicle
10.1016/j.comnet.2020.107556,journal,Computer Networks,sciencedirect,2020-12-24,sciencedirect,"Towards artificial intelligence enabled 6G: State of the art, challenges, and opportunities",https://api.elsevier.com/content/article/pii/S138912862031207X,"
                  6G is expected to support the unprecedented Internet of everything scenarios with extremely diverse and challenging requirements. To fulfill such diverse requirements efficiently, 6G is envisioned to be space-aerial-terrestrial-ocean integrated three-dimension networks with different types of slices enabled by new technologies and paradigms to make the system more intelligent and flexible. As 6G networks are increasingly complex, heterogeneous and dynamic, it is very challenging to achieve efficient resource utilization, seamless user experience, automatic management and orchestration. With the advancement of big data processing technology, computing power and the availability of rich data, it is natural to tackle complex 6G network issues by leveraging artificial intelligence (AI). In this paper, we make a comprehensive survey about AI-empowered networks evolving towards 6G. We first present the vision of AI-enabled 6G system, the driving forces of introducing AI into 6G and the state of the art in machine learning. Then applying machine learning techniques to major 6G network issues including advanced radio interface, intelligent traffic control, security protection, management and orchestration, and network optimization is extensively discussed. Moreover, the latest progress of major standardization initiatives and industry research programs on applying machine learning to mobile networks evolving towards 6G are reviewed. Finally, we identify important open issues to inspire further studies towards an intelligent, efficient and secure 6G system.
               ",autonomous vehicle
10.1016/j.comcom.2020.01.043,journal,Computer Communications,sciencedirect,2020-03-01,sciencedirect,Learning paradigms for communication and computing technologies in IoT systems,https://api.elsevier.com/content/article/pii/S0140366419312411,"
                  Wireless communication and computation technologies are becoming increasingly complex and dynamic due to the sophisticated and ubiquitous Internet of things (IoT) applications. Therefore, future wireless networks and computation solutions must be able to handle these challenges and dynamic user requirements for the success of IoT systems. Recently, learning strategies (particularly deep learning and reinforcement learning) are explored immensely to deal with the complexity and dynamic nature of communication and computation technologies for IoT systems, mainly because of their power to predict and efficient data analysis. Learning strategies can significantly enhance the performance of IoT systems at different stages, including at IoT node level, local communication, long-range communication, edge gateway, cloud platform, and corporate data centers. This paper presents a comprehensive overview of learning strategies for IoT systems. We categorize learning paradigms for communication and computing technologies in IoT systems into reinforcement learning, Bayesian algorithms, stochastic learning, and miscellaneous. We then present research in IoT with the integration of learning strategies from the optimization perspective where the optimization objectives are categorized into maximization and minimization along with corresponding applications. Learning strategies are discussed to illustrate how these strategies can enhance the performance of IoT applications. We also identify the key performance indicators (KPIs) used to evaluate the performance of IoT systems and discuss learning algorithms for these KPIs. Lastly, we provide future research directions to further enhance IoT systems using learning strategies
               ",autonomous vehicle
10.1016/j.clinimag.2020.09.005,journal,Clinical Imaging,sciencedirect,2021-01-31,sciencedirect,Artificial intelligence in stroke imaging: Current and future perspectives,https://api.elsevier.com/content/article/pii/S0899707120303466,"
                  Artificial intelligence (AI) is a fast-growing research area in computer science that aims to mimic cognitive processes through a number of techniques. Supervised machine learning, a subfield of AI, includes methods that can identify patterns in high-dimensional data using labeled ‘ground truth’ data and apply these learnt patterns to analyze, interpret, or make predictions on new datasets. Supervised machine learning has become a significant area of interest within the medical community. Radiology and neuroradiology in particular are especially well suited for application of machine learning due to the vast amount of data that is generated. One devastating disease for which neuroimaging plays a significant role in the clinical management is stroke. Within this context, AI techniques can play pivotal roles for image-based diagnosis and management of stroke. This overview focuses on the recent advances of artificial intelligence methods – particularly supervised machine learning and deep learning – with respect to workflow, image acquisition and reconstruction, and image interpretation in patients with acute stroke, while also discussing potential pitfalls and future applications.
               ",autonomous vehicle
10.1016/j.jeconbus.2018.05.003,journal,Journal of Economics and Business,sciencedirect,2018-12-31,sciencedirect,Some financial regulatory implications of artificial intelligence,https://api.elsevier.com/content/article/pii/S0148619517302618,"
                  Artificial intelligence has been playing an increasingly large role in the economy and this trend seems likely to continue. This paper begins with a high-level overview of artificial intelligence, including some of its important strengths and weaknesses. It then discusses some of the ways that AI affect the evolution of the financial system and financial regulation.
               ",autonomous vehicle
10.1016/j.neucom.2021.07.011,journal,Neurocomputing,sciencedirect,2021-10-14,sciencedirect,Emotion model of associative memory possessing variable learning rates with time delay,https://api.elsevier.com/content/article/pii/S0925231221010456,"
                  Lots of researchers have used memristors to realize the emotion model of associative memory. In previous works, researchers analyzed this associative memory from two perspectives—forgetting and variable learning rate. In the previous emotion model, neutral stimulus(message notification) and unconditioned reflex(good or bad message) were applied simultaneously. But the variable learning rate with time delay is not considered in the emotion model. When the unconditioned reflex lags behind the neutral stimulus, the associative memory can also be formed. This article proposes an emotion model of variable learning rate with time delay. We also consider three kinds of forgetting: only a stimulus of unconditioned reflex applied, only a neutral stimulus applied and neither stimulus of unconditioned reflex nor neutral stimulus applied. In the end, the software PSPICE is used to simulate the whole circuit. This paper provides an option to realize emotional learning based on memristor.
               ",autonomous vehicle
10.1016/B978-0-12-809633-8.20352-X,journal,Encyclopedia of Bioinformatics and Computational Biology,sciencedirect,2019-12-31,sciencedirect,Deep Learning,https://api.elsevier.com/content/article/pii/B978012809633820352X,"
               The article is devoted at illustrating the basic principles and the current results which characterize the research on Deep Learning. The term refers to the theory and practice of devising and training complex neural networks for supervised and unsupervised tasks. Within the article, we illustrate the basic principle underlying the idea of a single neural unit, and will show how these units can be combined to realize a complex network. We shall discuss the basic algorithms for training a network and the recent advances proposed by the literature for scaling up the training to deep architectures. The article concludes by an overview of the most successful deep architectures proposed in the literature, both for supervised and unsupervised learning.
            ",autonomous vehicle
10.1016/j.scs.2020.102526,journal,Sustainable Cities and Society,sciencedirect,2021-01-31,sciencedirect,Machine learning for geographically differentiated climate change mitigation in urban areas,https://api.elsevier.com/content/article/pii/S2210670720307423,"
                  Artificial intelligence and machine learning are transforming scientific disciplines, but their full potential for climate change mitigation remains elusive. Here, we conduct a systematic review of applied machine learning studies that are of relevance for climate change mitigation, focusing specifically on the fields of remote sensing, urban transportation, and buildings. The relevant body of literature spans twenty years and is growing exponentially. We show that the emergence of big data and machine learning methods enables climate solution research to overcome generic recommendations and provide policy solutions at urban, street, building and household scale, adapted to specific contexts, but scalable to global mitigation potentials. We suggest a meta-algorithmic architecture and framework for using machine learning to optimize urban planning for accelerating, improving and transforming urban infrastructure provision.
               ",autonomous vehicle
10.1016/j.patcog.2018.12.002,journal,Pattern Recognition,sciencedirect,2019-04-30,sciencedirect,Improving classification with semi-supervised and fine-grained learning,https://api.elsevier.com/content/article/pii/S0031320318304230,"
                  In this paper, we propose a novel and efficient multi-stage approach, which combines both semi-supervised learning and fine-grained learning to improve the performance of classification model learned only from a few samples. The fine-grained category recognition process utilized in our method is dubbed as MSR. In this process, we cut images into multi-scaled parts to feed into the network to learn more fine-grained features. By assigning these image cuts with dynamic weights, we can reduce the negative impact of background information and thus achieve a more accurate prediction. Furthermore, we present the voted pseudo label (VPL) which is an efficient method of semi-supervised learning. In this approach, for unlabeled data, VPL picks up the classes with non-confused labels verified by the consensus prediction of different classification models. These two methods can be applied to most neural network models and training methods. Inspired from classifier-based adaptation, we also propose a mix deep CNN architecture (MixDCNN). Both the VPL and MSR are integrated with the MixDCNN. Comprehensive experiments demonstrate the effectiveness of VPL and MSR. Without bottles and jars, we achieve the state-of-the-art or even better performance in two fine-grained recognition tasks on the datasets of Stanford Dogs and CUB Birds, with the accuracy of 95.6% and 85.2%, respectively.
               ",autonomous vehicle
10.1016/j.scs.2019.101748,journal,Sustainable Cities and Society,sciencedirect,2019-11-30,sciencedirect,A review of reinforcement learning methodologies for controlling occupant comfort in buildings,https://api.elsevier.com/content/article/pii/S2210670719307589,"
                  Classical building control systems are becoming vulnerable with increasing complexities in contemporary built environments and energy systems. Due to this, the reinforcement learning (RL) method is becoming more distinctive and applicable in control networks for buildings. This paper, therefore, conducts a comprehensive review of RL techniques applied in control systems for occupant comfort in indoor built environments. The empirical applications of RL-based control systems are presented, depending on comfort objectives (thermal comfort, indoor air quality, and lighting) along with other objectives which invariably includes energy consumption. The class of RL algorithms and implementation details regarding how the value functions have been represented and how the policies are improved are also illustrated. This paper shows there are limited works for which RL has been explored for controlling occupant comfort, especially in indoor air quality and lighting. Relatively few of the reviewed works incorporate occupancy patterns and/or occupant feedback into the control loop. Moreover, this paper identifies a gap with regard to the performance of implementing cooperative multi-agent RL (MARL). Based on our findings, current challenges and further opportunities are discussed. We expect to clarify the feasible theory and functions of RL for building control systems, which would promote their wider-spread application in built environments.
               ",autonomous vehicle
10.1016/j.compbiomed.2021.104949,journal,Computers in Biology and Medicine,sciencedirect,2021-12-31,sciencedirect,Deep learning for neuroimaging-based diagnosis and rehabilitation of Autism Spectrum Disorder: A review,https://api.elsevier.com/content/article/pii/S0010482521007435,"
                  Accurate diagnosis of Autism Spectrum Disorder (ASD) followed by effective rehabilitation is essential for the management of this disorder. Artificial intelligence (AI) techniques can aid physicians to apply automatic diagnosis and rehabilitation procedures. AI techniques comprise traditional machine learning (ML) approaches and deep learning (DL) techniques. Conventional ML methods employ various feature extraction and classification techniques, but in DL, the process of feature extraction and classification is accomplished intelligently and integrally. DL methods for diagnosis of ASD have been focused on neuroimaging-based approaches. Neuroimaging techniques are non-invasive disease markers potentially useful for ASD diagnosis. Structural and functional neuroimaging techniques provide physicians substantial information about the structure (anatomy and structural connectivity) and function (activity and functional connectivity) of the brain. Due to the intricate structure and function of the brain, proposing optimum procedures for ASD diagnosis with neuroimaging data without exploiting powerful AI techniques like DL may be challenging. In this paper, studies conducted with the aid of DL networks to distinguish ASD are investigated. Rehabilitation tools provided for supporting ASD patients utilizing DL networks are also assessed. Finally, we will present important challenges in the automated detection and rehabilitation of ASD and propose some future works.
               ",autonomous vehicle
10.1016/j.neucom.2020.06.012,journal,Neurocomputing,sciencedirect,2020-10-21,sciencedirect,N3-CPL: Neuroplasticity-based neuromorphic network cell proliferation learning,https://api.elsevier.com/content/article/pii/S0925231220309814,"
                  In general, Spiking Neural Networks (SNNs) have a network structure with special methods applied to neuron models and information transmission to mimic humans biologically. However, the existing SNN structures have two problems, such as fixed existing Artificial Neural Network structures and difficulty in learning due to lack of spike information during information transfer. Recently, many approaches of learning SNNs have been proposed in order to alleviate those two drawbacks. However, it is very difficult to overcome the drawbacks only by the learning method without the fundamental solution for the structure. In order to solve the problem of structure and learning method, we propose a novel flexible network construction method using neurogenesis-based cell proliferation concept and Triple Simultaneous- Spike Timing Dependent Plasticity (TS-STDP) which is improved learning method through neuroplasticity-based spike timing. We build the network flexibly and automatically by employing the concept that not only one neuron exists in the neural network, but also that various cells proliferate and transform from stem cells to function. In addition, TS-STDP is designed by considering the correlation of signal response among several neurons to solve the lack of information due to spike sparsity, which is a disadvantage of STDP. In the experimental section, we demonstrate and analyze our method using Mixed National Institute of Standards and Technology image data. Our method is 2.7× better in memory efficiency and 1.7× better in computational efficiency than the existing method. In particular, the research that automatically constructs the network structure is the first to my knowledge.
               ",autonomous vehicle
10.1016/j.oceaneng.2021.110180,journal,Ocean Engineering,sciencedirect,2021-12-15,sciencedirect,An enhanced intelligent model: To protect marine IoT sensor environment using ensemble machine learning approach,https://api.elsevier.com/content/article/pii/S0029801821014980,"
                  The research in marine sensors and the Internet of Things (IoT) has grown exponentially with the ample warehouse of natural materials in the sea. The growing activities in the marine sensor environment increased the threat of anomalies and cyber-attacks. Many Intrusion Detection Systems (IDS) and classical machine learning-based models have been proposed to secure the sensor-based IoT infrastructure. Still, these mechanisms have failed to achieve significant results for securing the marine sensor environment due to the discriminant requirements of the IoT appliances in deep oceans, such as distribution, information complexity, scalability, higher network bandwidth requirements, and low computational capacity. Hence, we propose a lightweight and robust ensemble model to secure the marine IoT environment from cyber-attacks and malicious activities. This paper established an optimized Light Gradient Boosting Machine (Light-GBM) algorithm for ocean IoT attack detection. The experiments were conducted on Distributed Smart Space Orchestration System (DS2OS) dataset. The proposed methodology includes a label encoding technique for best feature selection, hyper-parameter tuning, ensemble function, and a novel algorithm to develop an ocean IoT attack detection model. As an extension of traditional methods, the optimized Light-GBM model can handle the distributed IoT attacks in the deeper marine environments with low computational cost and with 98.52% detection accuracy. The comparative analysis confirms the effectiveness of the proposed model for marine sensor safety. Conclusively, the proposed model mitigates the threat of cyber-attacks in the marine sensor environment and presenting a promising future in real-time ocean-based IoT applications.
               ",autonomous vehicle
10.1016/j.tej.2020.106881,journal,The Electricity Journal,sciencedirect,2021-02-28,sciencedirect,Machine learning for power system protection and control,https://api.elsevier.com/content/article/pii/S1040619020301731,"
                  Since the power system is undergoing a transition into a more flexible and complex system, it urges improvements in fault diagnosis techniques for the power system protection to avoid cascading damages at the occurrence of faults. Facing with challenges of massive data, several machine-learning based methods for identifying faults were proposed over the past years. In this paper, an overview of conventional and trending machine learning applications for the fault diagnosis are summarized.
               ",autonomous vehicle
10.1016/j.neucom.2014.11.022,journal,Neurocomputing,sciencedirect,2015-03-25,sciencedirect,"Biological context of Hebb learning in artificial neural networks, a review",https://api.elsevier.com/content/article/pii/S0925231214015239,"
                  In 1949 Donald Olding Hebb formulated a hypothesis describing how neurons excite each other and how the efficiency of this excitation subsequently changes with time. In this paper we present a review of this idea. We evaluate its influences on the development of artificial neural networks and the way we describe biological neural networks. We explain how Hebb׳s hypothesis fits into the research both of that time and of present. We highlight how it has gone on to inspire many researchers working on artificial neural networks. The underlying biological principles that corroborate this hypothesis, that were discovered much later, are also discussed in addition to recent results in the field and further possible directions of synaptic learning research.
               ",autonomous vehicle
10.1016/j.comcom.2021.01.021,journal,Computer Communications,sciencedirect,2021-03-15,sciencedirect,Deep Learning for Network Traffic Monitoring and Analysis (NTMA): A Survey,https://api.elsevier.com/content/article/pii/S0140366421000426,"Modern communication systems and networks, e.g., Internet of Things (IoT) and cellular networks, generate a massive and heterogeneous amount of traffic data. In such networks, the traditional network management techniques for monitoring and data analytics face some challenges and issues, e.g., accuracy, and effective processing of big data in a real-time fashion. Moreover, the pattern of network traffic, especially in cellular networks, shows very complex behavior because of various factors, such as device mobility and network heterogeneity. Deep learning has been efficiently employed to facilitate analytics and knowledge discovery in big data systems to recognize hidden and complex patterns. Motivated by these successes, researchers in the field of networking apply deep learning models for Network Traffic Monitoring and Analysis (NTMA) applications, e.g., traffic classification and prediction. This paper provides a comprehensive review on applications of deep learning in NTMA. We first provide fundamental background relevant to our review. Then, we give an insight into the confluence of deep learning and NTMA, and review deep learning techniques proposed for NTMA applications. Finally, we discuss key challenges, open issues, and future research directions for using deep learning in NTMA applications.",autonomous vehicle
10.1016/S2352-3026(20)30121-6,journal,The Lancet Haematology,sciencedirect,2020-07-31,sciencedirect,Machine learning in haematological malignancies,https://api.elsevier.com/content/article/pii/S2352302620301216,"
                  Machine learning is a branch of computer science and statistics that generates predictive or descriptive models by learning from training data rather than by being rigidly programmed. It has attracted substantial attention for its many applications in medicine, both as a catalyst for research and as a means of improving clinical care across the cycle of diagnosis, prognosis, and treatment of disease. These applications include the management of haematological malignancy, in which machine learning has created inroads in pathology, radiology, genomics, and the analysis of electronic health record data. As computational power becomes cheaper and the tools for implementing machine learning become increasingly democratised, it is likely to become increasingly integrated into the research and practice landscape of haematology. As such, machine learning merits understanding and attention from researchers and clinicians alike. This narrative Review describes important concepts in machine learning for unfamiliar readers, details machine learning's current applications in haematological malignancy, and summarises important concepts for clinicians to be aware of when appraising research that uses machine learning.
               ",autonomous vehicle
10.1016/j.future.2021.06.040,journal,Future Generation Computer Systems,sciencedirect,2021-12-31,sciencedirect,DroneCOCoNet: Learning-based edge computation offloading and control networking for drone video analytics,https://api.elsevier.com/content/article/pii/S0167739X21002351,"
                  Multi-Unmanned Aerial Vehicle (UAV) systems with high-resolution cameras have been found useful for operations such as smart city and disaster management. These systems feature Flying Ad-Hoc Networks (FANETs) that connect the computation edge with UAVs and a Ground Control Station (GCS) through air-to-ground wireless network links. Leveraging the edge/fog computation resources effectively with energy-latency-awareness, and handling intermittent failures of FANETs are the major challenges in supporting video processing applications. In this paper, we propose a novel “DroneCOCoNet” framework for drone video analytics that coordinates intelligent processing of large video datasets using edge computation offloading and performs network protocol selection based on resource-awareness. We present two edge computation offloading approaches, i.e., heuristic-based and reinforcement learning-based approaches. These approaches provide intelligent task sharing and co-ordination for dynamic offloading decision-making among UAVs. Our scheme handles the problem of computation offloading tasks in two separate ways: (i) heuristic decision-making process, and (ii) Markov decision process; wherein we aim to minimize the total computation costs as well as latency in the edge/fog resources while minimizing video processing times to meet application requirements. Our experimental results show that our heuristic-based offloading decision-making scheme enables lower scheduling time and energy consumption for low drone-to-ground server ratios. In comparison, our dynamic reinforcement learning-based decision-making approach increases the accuracy and saves overall time periodically. Notably, these results also hold in various other multi-UAV scenarios involving largely different numbers of detected objects in e.g., smart farming, transportation traffic flow monitoring and disaster response.
               ",autonomous vehicle
10.1016/j.ymssp.2021.108487,journal,Mechanical Systems and Signal Processing,sciencedirect,2022-03-15,sciencedirect,"A perspective survey on deep transfer learning for fault diagnosis in industrial scenarios: Theories, applications and challenges",https://api.elsevier.com/content/article/pii/S088832702100830X,"
                  Deep Transfer Learning (DTL) is a new paradigm of machine learning, which can not only leverage the advantages of Deep Learning (DL) in feature representation, but also benefit from the superiority of Transfer Learning (TL) in knowledge transfer. As a result, DTL techniques can make DL-based fault diagnosis methods more reliable, robust and applicable, and they have been widely developed and investigated in the field of Intelligent Fault Diagnosis (IFD). Although several systematic and valuable review articles have been published on the topic of IFD, they summarized relevant research only from an algorithm perspective and overlooked practical applications in industry scenarios. Furthermore, a comprehensive review on DTL-based IFD methods is still lacking. From this insight, it is particularly important and more necessary to comprehensively survey the relevant publications of DTL-based IFD, which will help readers to conveniently understand the current state-of-the-art techniques and to quickly design an effective solution for solving IFD problems in practice. First, theoretical backgrounds of DTL are briefly introduced to present how the transfer learning techniques can be integrated with deep learning models. Then, major applications of DTL and their recent developments in the field of IFD are detailed and discussed. More importantly, suggestions on how to select DTL algorithms in practical applications, and some future challenges are shared. Finally, conclusions of this survey are given. At last, we have reason to believe that the works done in this article can provide convenience and inspiration for the researchers who want to devote their efforts in the progress and advance of IFD.
               ",autonomous vehicle
10.1016/j.eswa.2017.01.021,journal,Expert Systems with Applications,sciencedirect,2017-06-01,sciencedirect,Learning style Identifier: Improving the precision of learning style identification through computational intelligence algorithms,https://api.elsevier.com/content/article/pii/S0957417417300301,"
                  Identifying students’ learning styles has several benefits such as making students aware of their strengths and weaknesses when it comes to learning and the possibility to personalize their learning environment to their learning styles. While there exist learning style questionnaires for identifying a student's learning style, such questionnaires have several disadvantages and therefore, research has been conducted on automatically identifying learning styles from students’ behavior in a learning environment. Current approaches to automatically identify learning styles have an average precision between 66% and 77%, which shows the need for improvements in order to use such automatic approaches reliably in learning environments. In this paper, four computational intelligence algorithms (artificial neural network, genetic algorithm, ant colony system and particle swarm optimization) have been investigated with respect to their potential to improve the precision of automatic learning style identification. Each algorithm was evaluated with data from 75 students. The artificial neural network shows the most promising results with an average precision of 80.7%, followed by particle swarm optimization with an average precision of 79.1%. Improving the precision of automatic learning style identification allows more students to benefit from more accurate information about their learning styles as well as more accurate personalization towards accommodating their learning styles in a learning environment. Furthermore, teachers can have a better understanding of their students and be able to provide more appropriate interventions.
               ",autonomous vehicle
10.1016/j.neuron.2020.09.005,journal,Neuron,sciencedirect,2020-09-23,sciencedirect,Artificial Neural Networks for Neuroscientists: A Primer,https://api.elsevier.com/content/article/pii/S0896627320307054,"Artificial neural networks (ANNs) are essential tools in machine learning that have drawn increasing attention in neuroscience. Besides offering powerful techniques for data analysis, ANNs provide a new approach for neuroscientists to build models for complex behaviors, heterogeneous neural activity, and circuit connectivity, as well as to explore optimization in neural systems, in ways that traditional models are not designed for. In this pedagogical Primer, we introduce ANNs and demonstrate how they have been fruitfully deployed to study neuroscientific questions. We first discuss basic concepts and methods of ANNs. Then, with a focus on bringing this mathematical framework closer to neurobiology, we detail how to customize the analysis, structure, and learning of ANNs to better address a wide range of challenges in brain research. To help readers garner hands-on experience, this Primer is accompanied with tutorial-style code in PyTorch and Jupyter Notebook, covering major topics.",autonomous vehicle
10.1016/j.techfore.2020.120392,journal,Technological Forecasting and Social Change,sciencedirect,2021-01-31,sciencedirect,"Artificial intelligence and innovation management: A review, framework, and research agenda<ce:sup loc=post>✰</ce:sup>",https://api.elsevier.com/content/article/pii/S004016252031218X,"Artificial Intelligence (AI) reshapes companies and how innovation management is organized. Consistent with rapid technological development and the replacement of human organization, AI may indeed compel management to rethink a company's entire innovation process. In response, we review and explore the implications for future innovation management. Using ideas from the Carnegie School and the behavioral theory of the firm, we review the implications for innovation management of AI technologies and machine learning-based AI systems. We outline a framework showing the extent to which AI can replace humans and explain what is important to consider in making the transformation to the digital organization of innovation. We conclude our study by exploring directions for future research.",autonomous vehicle
10.1016/j.ins.2020.11.048,journal,Information Sciences,sciencedirect,2021-04-30,sciencedirect,A novel lifelong learning model based on cross domain knowledge extraction and transfer to classify underwater images,https://api.elsevier.com/content/article/pii/S0020025520311464,"
                  Artificial intelligence based autonomous systems interacting with dynamic environment are required to continuously learn, accumulate and improve the learned knowledge. Currently, most artificial intelligence based systems lack this ability and work in isolated learning paradigm. Human beings follow the continuous learning process by retaining and accumulating the learnt knowledge, and by using the learnt knowledge to solve the problem at hand. In this paper, we present a lifelong learning model, to solve challenging problem of real world underwater image classification. The proposed model is capable to learn from simple problems, accumulates the learnt knowledge by continual learning and uses the learnt knowledge to solve future complex problems of the same or related domain, in a similar way as humans do. In the proposed model, firstly, a deep classification convolutional autoencoder is presented to extract spatially localized features from images by utilizing convolution filters, then a code fragment based learning classifier system, with rich knowledge encoding scheme, is proposed for knowledge representation and transfer. In order to validate the model, experiments are conducted on two underwater images datasets and one in-air images dataset. Experiments results demonstrate that the proposed method outperforms base line method and state-of-the-art convolution neural network (CNN) methods.
               ",autonomous vehicle
10.1016/j.yacr.2020.04.006,journal,Advances in Clinical Radiology,sciencedirect,2020-09-30,sciencedirect,Artificial Intelligence in Cardiopulmonary Imaging,https://api.elsevier.com/content/article/pii/S2589870120300067,,autonomous vehicle
10.1016/j.ahjo.2021.100050,journal,American Heart Journal Plus: Cardiology Research and Practice,sciencedirect,2021-11-30,sciencedirect,Emerging role of machine learning in cardiovascular disease investigation and translations,https://api.elsevier.com/content/article/pii/S2666602221000483,"Unexpected insights and practical advances in cardiovascular disease (CVD) are being discovered by rapidly advancing developments in supercomputers and machine learning (ML) software algorithms. These have been accelerated during the COVID-19 pandemic, and the resulting CVD translational implications of ML are steering new measures of prevention and treatment, new tools for objective clinical diagnosis, and even opportunities for rethinking basic foundations of CVD nosology. As the usual cardiovascular specialist may not be familiar with these tools, the editor has invited this brief overview.",autonomous vehicle
10.1016/j.buildenv.2019.03.038,journal,Building and Environment,sciencedirect,2019-05-15,sciencedirect,Energy optimization associated with thermal comfort and indoor air control via a deep reinforcement learning algorithm,https://api.elsevier.com/content/article/pii/S0360132319302008,"
                  The aim of this work is to propose an artificial intelligence algorithm that maintains thermal comfort and air quality within optimal levels while consuming the least amount of energy from air-conditioning units and ventilation fans. The proposed algorithm is first trained with 10 years of simulated past experiences in a subtropical environment in Taiwan. The simulations are carried out in a laboratory room having around 2–10 occupants and a classroom with up to 60 occupants. The proposed agent was first selected among different configurations of itself, with the 10th-year of training data set, then it was tested in real environments. Finally, a comparison between the current control methods and this new strategy is performed. It was found that the proposed AI agent can satisfactorily control and balance the needs of thermal comfort, indoor air quality (in terms of CO2 levels) and energy consumption caused by air-conditioning units and ventilation fans. For both environments, the AI agent can successfully manipulate the indoor environment within the accepted PMV values, ranging from about −0.1 to +0.07 during all the operating time. In regards to the indoor air quality, in terms of the CO2 levels, the results are also satisfactory. By utilizing the agent, the average CO2 levels fall below 800 ppm all the time. The results show that the proposed agent has a superior PMV and 10% lower CO2 levels than the current control system while consuming about 4–5% less energy.
               ",autonomous vehicle
10.1016/j.ajpath.2021.05.023,journal,The American Journal of Pathology,sciencedirect,2021-10-31,sciencedirect,<ce:marker name=asip alt=ASIP member publication altimg-small=ajpaasip_s.svg altimg=ajpaasip_o.svg></ce:marker>Dealing with Multi-Dimensional Data and the Burden of Annotation: Easing the Burden of Annotation,https://api.elsevier.com/content/article/pii/S0002944021002625,"
                  The need for huge data sets represents a bottleneck for the application of artificial intelligence. Substantially fewer annotated target lesions than normal tissues for comparison present an additional problem in the field of pathology. Organic brains overcome these limitations by utilizing large numbers of specialized neural nets arranged in both linear and parallel fashion, with each solving a restricted classification problem. They rely on local Hebbian error corrections as compared to the nonlocal back-propagation used in most artificial neural nets, and leverage reinforcement. For these reasons, even toddlers are able to classify objects after only a few examples. Rather than provide an overview of current AI research in pathology, this review focuses on general strategies for overcoming the data bottleneck. These include transfer learning, zero-shot learning, Siamese networks, one-class models, generative networks, and reinforcement learning. Neither an extensive mathematic background nor advanced programing skills are needed to make these subjects accessible to pathologists. However, some familiarity with the basic principles of deep learning, briefly reviewed here, is expected to be useful in understanding both the current limitations of machine learning and determining ways to address them.
               ",autonomous vehicle
10.1016/j.jocs.2020.101111,journal,Journal of Computational Science,sciencedirect,2020-10-31,sciencedirect,On semi-supervised multiple representation behavior learning,https://api.elsevier.com/content/article/pii/S1877750319313389,"
                  Since Shahshahani and Landgrebe published their seminal paper (Shahshahani and Landgrebe, 1994) [1] in 1994, the study on semi-supervised learning (SSL) developed fast and has already become one of the main streams of machine learning (ML) research. However, there are still some areas or problems where the capability of SSL remains seriously limited. Firstly, according to our observation, almost all SSL researches are towards classification, regression or clustering tasks. More difficult tasks such as planning, construction, summarization, argumentation, etc. are rarely seen studied with SSL methods. Secondly, most SSL researches use only simple labels (e.g. a string, an identifier, a numerical value, etc.) to mark the text data. It is difficult to use such simple labels to characterize data with delicate information. This limitation might be the reason why current SSL technique is not appropriate in processing complex tasks. Thirdly, after entering the age of big data and big knowledge, SSL, like the other branches of ML, is now facing the challenge of learning big knowledge from big data. The shortage of traditional SSL as mentioned above became even more serious and we are looking forward to new technology of SSL.
                  In this paper, we propose and discuss a novel paradigm of SSL: the semi-supervised multiple representation behavior learning (SSMRBL). It is towards matching the challenge to SSL stated above. SSMRBL should extend current SSL techniques to support complex task learning such as planning, construction, summarization, argumentation etc. In order to meet the challenge, SSMRBL introduces compound structured labels such as trees, graphs, lattices, etc. to represent complicated information of objects and tasks to be learned. Thus, to label an unlabeled datum is to construct a compound structured label for it. As a consequence, SSMRBL needs to have multiple representations. There may be one representation for compound structured labels, one for the target model which is the unification of all local models (labels), one for representing the process (behavior) of label construction, and one for the efficient computation during the learning process. This paper introduces also a typical circumstance of SSMRBL—semi-supervised grammar learning (SSGL), which learns a grammar from a set of natural language texts and then applies this grammar to parse new texts and to summarize its content. We provide also experimental results based on a variety of algorithms to show the reasonability of our ideas.
               ",autonomous vehicle
10.1016/j.clinthera.2020.12.014,journal,Clinical Therapeutics,sciencedirect,2021-02-28,sciencedirect,Artificial Intelligence in Pharmacovigilance: Scoping Points to Consider,https://api.elsevier.com/content/article/pii/S0149291820305634,"
                  Artificial intelligence (AI), a highly interdisciplinary science, is an increasing presence in pharmacovigilance (PV). A better understanding of the scope of artificial intelligence in pharmacovigilance (AIPV) may be advantageous to more sharply defining, for example, which terms, methods, tasks, and data sets are suitably subsumed under the application of AIPV. Accordingly, this article explores relevant points to consider regarding defining the scope of AIPV and offers a potential working definition of the scope of AIPV.
               ",autonomous vehicle
10.1016/j.cam.2020.112723,journal,Journal of Computational and Applied Mathematics,sciencedirect,2020-07-31,sciencedirect,Blockchain financial investment based on deep learning network algorithm,https://api.elsevier.com/content/article/pii/S0377042720300145,"
                  In order to study the use of in-depth learning to process financial data, it is proposed that the related technology of neural network and deep learning can be applied to financial data, and real stock index and futures data can be used to explore the application effect of neural network and in-depth learning. Firstly, the theory and model of in-depth learning and neural network are introduced in detail. Secondly, a simple neural network and in-depth learning model are used in the stock index and futures price forecast. The data used in the input of the model include the price of a stock in the current trading and some data indicators, and the closing price of a stock in the next time. The decline will be reflected in output. If the output is up for 1 and down for 0, new data will be input after the training of the model. Finally, the output data can be compared with the real data to judge the application effect of the model, after comparing e and analyzing the application effect of the model. The results show that the research in this study can help investors to build an automated investment model and the investment strategy of the stock market. The construction can be used for reference to improve investors’ investment strategy and return rate.
               ",autonomous vehicle
10.1016/j.jksuci.2019.07.010,journal,Journal of King Saud University - Computer and Information Sciences,sciencedirect,2019-07-30,sciencedirect,Performance optimization of criminal network hidden link prediction model with deep reinforcement learning,https://api.elsevier.com/content/article/pii/S1319157819308584,"The scale of criminal networks (e.g. drug syndicates and terrorist networks) extends globally and poses national security threat to many nations as they also tend to be technologically advance (e.g. Dark Web and Silk Road cryptocurrency). Therefore, it is critical for law enforcement agencies to be equipped with the latest tools in criminal network analysis (CNA) to obtain key hidden links (relationships) within criminal networks to preempt and disrupt criminal network structures and activities. Current hidden or missing link predictive models that are based on Social Network Analysis models rely on ML techniques to improve the performance of the models in terms of predictive accuracy and computing power. Given the improvement in the recent performance of Deep Reinforcement Learning (DRL) techniques which could train ML models through self-generated dataset, DRL can be usefully applied to domains with relatively smaller dataset such as criminal networks. The objective of this study is to assess the comparative performance of a CNA hidden link prediction model developed using DRL techniques against classical ML models such as gradient boosting machine (GBM), random forest (RF) and support vector machine (SVM). The experiment results exhibit an improvement in the performance of the DRL model of about 7.4% over the next best performing classical RF model trained within 1500 iterations. The performance of these link prediction models can be scaled up with the parallel processing capabilities of graphical processing units (GPUs), to significantly improve the speed of training the model and the prediction of hidden links.",autonomous vehicle
10.1016/j.semcancer.2021.04.013,journal,Seminars in Cancer Biology,sciencedirect,2021-04-26,sciencedirect,Artificial intelligence in oncology: From bench to clinic,https://api.elsevier.com/content/article/pii/S1044579X21001140,"
                  In the past few years, Artificial Intelligence (AI) techniques have been applied to almost every facet of oncology, from basic research to drug development and clinical care. In the clinical arena where AI has perhaps received the most attention, AI is showing promise in enhancing and automating image-based diagnostic approaches in fields such as radiology and pathology. Robust AI applications, which retain high performance and reproducibility over multiple datasets, extend from predicting indications for drug development to improving clinical decision support using electronic health record data. In this article, we review some of these advances. We also introduce common concepts and fundamentals of AI and its various uses, along with its caveats, to provide an overview of the opportunities and challenges in the field of oncology. Leveraging AI techniques productively to provide better care throughout a patient’s medical journey can fuel the predictive promise of precision medicine.
               ",autonomous vehicle
10.1016/j.neucom.2019.07.009,journal,Neurocomputing,sciencedirect,2019-11-06,sciencedirect,Fast and robust learning in Spiking Feed-forward Neural Networks based on Intrinsic Plasticity mechanism,https://api.elsevier.com/content/article/pii/S0925231219309348,"
                  In this paper, the computational performance of a Spiking Feed-forward Neural Network (SFNN) is investigated based on a brain-inspired Intrinsic Plasticity (IP) mechanism, which is a membrane potential adaptive tuning scheme used to change the intrinsic excitability of individual neuron. This learning rule has the ability of regulating neural activity in a relative homeostatic level even if the external input of a neuron is extremely low or extremely high. The effectiveness of IP on SFNN model has been studied and evaluated through the MNIST handwritten digits classification. The training of network weights starts from a conventional artificial neural network by backpropagation and then the rate-based neurons are transformed into spiking neuron models with IP learning. Our results show that both over-activation and under-activation of neuronal response which commonly exist during the computation of neural networks can be effectively avoided. Without loss of accuracy, the calculation speed of SFNN with IP learning is extremely higher than that of the other models. Besides, when the input intensity and data noise are taken into account, both of the learning speed and accuracy of the model can be greatly improved by the application of IP learning. This biologically inspired SFNN model is simple and effective which may give insights for the optimization of neural computation.
               ",autonomous vehicle
10.1016/j.ymben.2020.10.005,journal,Metabolic Engineering,sciencedirect,2021-01-31,sciencedirect,Machine learning for metabolic engineering: A review,https://api.elsevier.com/content/article/pii/S109671762030166X,"Machine learning provides researchers a unique opportunity to make metabolic engineering more predictable. In this review, we offer an introduction to this discipline in terms that are relatable to metabolic engineers, as well as providing in-depth illustrative examples leveraging omics data and improving production. We also include practical advice for the practitioner in terms of data management, algorithm libraries, computational resources, and important non-technical issues. A variety of applications ranging from pathway construction and optimization, to genetic editing optimization, cell factory testing, and production scale-up are discussed. Moreover, the promising relationship between machine learning and mechanistic models is thoroughly reviewed. Finally, the future perspectives and most promising directions for this combination of disciplines are examined.",autonomous vehicle
10.1016/j.apenergy.2021.116601,journal,Applied Energy,sciencedirect,2021-04-01,sciencedirect,"Artificial intelligence based anomaly detection of energy consumption in buildings: A review, current trends and new perspectives",https://api.elsevier.com/content/article/pii/S0306261921001409,"Enormous amounts of data are being produced everyday by sub-meters and smart sensors installed in residential buildings. If leveraged properly, that data could assist end-users, energy producers and utility companies in detecting anomalous power consumption and understanding the causes of each anomaly. Therefore, anomaly detection could stop a minor problem becoming overwhelming. Moreover, it will aid in better decision-making to reduce wasted energy and promote sustainable and energy efficient behavior. In this regard, this paper is an in-depth review of existing anomaly detection frameworks for building energy consumption based on artificial intelligence. Specifically, an extensive survey is presented, in which a comprehensive taxonomy is introduced to classify existing algorithms based on different modules and parameters adopted, such as machine learning algorithms, feature extraction approaches, anomaly detection levels, computing platforms and application scenarios. To the best of the authors’ knowledge, this is the first review article that discusses anomaly detection in building energy consumption. Moving forward, important findings along with domain-specific problems, difficulties and challenges that remain unresolved are thoroughly discussed, including the absence of: (i) precise definitions of anomalous power consumption, (ii) annotated datasets, (iii) unified metrics to assess the performance of existing solutions, (iv) platforms for reproducibility and (v) privacy-preservation. Following, insights about current research trends are discussed to widen the applications and effectiveness of the anomaly detection technology before deriving future directions attracting significant attention. This article serves as a comprehensive reference to understand the current technological progress in anomaly detection of energy consumption based on artificial intelligence.",autonomous vehicle
10.1016/j.promfg.2018.10.018,journal,Procedia Manufacturing,sciencedirect,2018-12-31,sciencedirect,Online control of stencil printing parameters using reinforcement learning approach,https://api.elsevier.com/content/article/pii/S235197891831134X,"This research proposes a novel approach to control the stencil printing process (SPP) parameters online in surface mount technology (SMT) of printed circuit boards (PCBs). Several external variables induce variations in stencil printing quality including environment conditions, operator faults, and others. This research aims to build an optimal adaptive controller that captures these variations and consequently adjusts the controllable and significant printing parameters to enhance the solder paste volume transfer efficiency (TE) during actual production run. Q-learning which is a reinforcement learning (RL) approach is used to control the main printing parameters (printing speed and pressure, and the separation speed) online. The results show that Q-learning converges to the optimal policy for the SPP problem, and the optimal sets of actions for different states are retrieved using Q-table. Moreover, the developed controller is capable to reach the terminal state for several testing examples with taking few actions.",autonomous vehicle
10.1016/j.imu.2021.100564,journal,Informatics in Medicine Unlocked,sciencedirect,2021-12-31,sciencedirect,"Machine learning approaches in COVID-19 diagnosis, mortality, and severity risk prediction: A review",https://api.elsevier.com/content/article/pii/S235291482100054X,"The existence of widespread COVID-19 infections has prompted worldwide efforts to control and manage the virus, and hopefully curb it completely. One important line of research is the use of machine learning (ML) to understand and fight COVID-19. This is currently an active research field. Although there are already many surveys in the literature, there is a need to keep up with the rapidly growing number of publications on COVID-19-related applications of ML. This paper presents a review of recent reports on ML algorithms used in relation to COVID-19. We focus on the potential of ML for two main applications: diagnosis of COVID-19 and prediction of mortality risk and severity, using readily available clinical and laboratory data. Aspects related to algorithm types, training data sets, and feature selection are discussed. As we cover work published between January 2020 and January 2021, a few key points have come to light. The bulk of the machine learning algorithms used in these two applications are supervised learning algorithms. The established models are yet to be used in real-world implementations, and much of the associated research is experimental. The diagnostic and prognostic features discovered by ML models are consistent with results presented in the medical literature. A limitation of the existing applications is the use of imbalanced data sets that are prone to selection bias.",autonomous vehicle
10.1016/j.mlwa.2021.100164,journal,Machine Learning with Applications,sciencedirect,2021-12-15,sciencedirect,Autonomous Driving Architectures: Insights of Machine Learning and Deep Learning Algorithms,https://api.elsevier.com/content/article/pii/S2666827021000827,"Research in Autonomous Driving is taking momentum due to the inherent advantages of autonomous driving systems. The main advantage being the disassociation of the driver from the vehicle reducing the human intervention. However, the Autonomous Driving System involves many subsystems which need to be integrated as a whole system. Some of the tasks include Motion Planning, Vehicle Localization, Pedestrian Detection, Traffic Sign Detection, Road-marking Detection, Automated Parking, Vehicle Cybersecurity, and System Fault Diagnosis. This paper aims to the overview of various Machine Learning and Deep Learning Algorithms used in Autonomous Driving Architectures for different tasks like Motion Planning, Vehicle Localization, Pedestrian Detection, Traffic Sign Detection, Road-marking Detection, Automated Parking, Vehicle Cybersecurity and Fault Diagnosis. This paper surveys the technical aspects of Machine Learning and Deep Learning Algorithms used for Autonomous Driving Systems. Comparison of these algorithms is done based on the metrics like mean Intersect in over Union (mIoU), Average Precision (AP)missed detection rate, miss rate False Positives Per Image (FPPI), and average number for false frame detection. This study contributes to picture a review of the Machine Learning and Deep Learning Algorithms used for Autonomous Driving Systems and is organized based on the different tasks of the system.",autonomous vehicle
10.1016/B978-0-12-820273-9.00002-6,journal,Machine Learning in Cardiovascular Medicine,sciencedirect,2021-12-31,sciencedirect,Chapter 2: An overview of artificial intelligence: basics and state-of-the-art algorithms,https://api.elsevier.com/content/article/pii/B9780128202739000026,"
               Recently, deep convolutional neural networks have been successfully applied to medical image tasks by taking advantage of a large amount of training data with gold standard annotations. The first part of this chapter introduces several basic, yet critical, concepts regarding machine learning and deep learning, which provide a foundation for understanding of both the development of these algorithms as well as appreciating their current cardiovascular applications. Thereafter, this chapter provides an overview of state-of-the-art machine learning algorithms including Bayesian, Decision Tree, Support Vector Machine, and Convolutional Neural Network (CNN).
            ",autonomous vehicle
10.1016/j.cosrev.2021.100395,journal,Computer Science Review,sciencedirect,2021-05-31,sciencedirect,Machine learning algorithms for social media analysis: A survey,https://api.elsevier.com/content/article/pii/S1574013721000356,"
                  Social Media (SM) are the most widespread and rapid data generation applications on the Internet increase the study of these data. However, the efficient processing of such massive data is challenging, so we require a system that learns from these data, like machine learning. Machine learning methods make the systems to learn itself. Many papers are published on SM using machine learning approaches over the past few decades. In this paper, we provide a comprehensive survey of multiple applications of SM analysis using robust machine learning algorithms. Initially, we discuss a summary of machine learning algorithms, which are used in SM analysis. After that, we provide a detailed survey of machine learning approaches to SM analysis. Furthermore, we summarize the challenges and benefits of Machine Learning usages in SM analysis. Finally, we presented open issues and consequences in SM analysis for further research.
               ",autonomous vehicle
10.1016/j.aca.2021.338403,journal,Analytica Chimica Acta,sciencedirect,2021-05-29,sciencedirect,Taking the leap between analytical chemistry and artificial intelligence: A tutorial review,https://api.elsevier.com/content/article/pii/S0003267021002294,"
                  The last 10 years have witnessed the growth of artificial intelligence into different research areas, emerging as a vibrant discipline with the capacity to process large amounts of information and even intuitively interact with humans. In the chemical world, these innovations in both hardware and algorithms have allowed the development of revolutionary approaches in organic synthesis, drug discovery, and materials’ design. Despite these advances, the use of AI to support analytical purposes has been mostly limited to data-intensive methodologies linked to image recognition, vibrational spectroscopy, and mass spectrometry but not to other technologies that, albeit simpler, offer promise of greatly enhanced analytics now that AI is becoming mature enough to take advantage of them. To address the imminent opportunity of analytical chemists to use AI, this tutorial review aims to serve as a first step for junior researchers considering integrating AI into their programs. Thus, basic concepts related to AI are first discussed followed by a critical assessment of representative reports integrating AI with various sensors, spectroscopies, and separation techniques. For those with the courage (and the time) needed to get started, the review also provides a general sequence of steps to begin integrating AI into their programs.
               ",autonomous vehicle
10.1016/j.ress.2021.107530,journal,Reliability Engineering & System Safety,sciencedirect,2021-07-31,sciencedirect,Machine learning for reliability engineering and safety applications: Review of current status and future opportunities,https://api.elsevier.com/content/article/pii/S0951832021000892,"
                  Machine learning (ML) pervades an increasing number of academic disciplines and industries. Its impact is profound, and several fields have been fundamentally altered by it, autonomy and computer vision for example; reliability engineering and safety will undoubtedly follow suit. There is already a large but fragmented literature on ML for reliability and safety applications, and it can be overwhelming to navigate and integrate into a coherent whole. In this work, we facilitate this task by providing a synthesis of, and a roadmap to this ever-expanding analytical landscape and highlighting its major landmarks and pathways. We first provide an overview of the different ML categories and sub-categories or tasks, and we note several of the corresponding models and algorithms. We then look back and review the use of ML in reliability and safety applications. We examine several publications in each category/sub-category, and we include a short discussion on the use of Deep Learning to highlight its growing popularity and distinctive advantages. Finally, we look ahead and outline several promising future opportunities for leveraging ML in service of advancing reliability and safety considerations. Overall, we argue that ML is capable of providing novel insights and opportunities to solve important challenges in reliability and safety applications. It is also capable of teasing out more accurate insights from accident datasets than with traditional analysis tools, and this in turn can lead to better informed decision-making and more effective accident prevention.
               ",autonomous vehicle
10.1016/j.preteyeres.2021.100965,journal,Progress in Retinal and Eye Research,sciencedirect,2021-11-30,sciencedirect,Artificial intelligence in OCT angiography,https://api.elsevier.com/content/article/pii/S1350946221000264,"
                  Optical coherence tomographic angiography (OCTA) is a non-invasive imaging modality that provides three-dimensional, information-rich vascular images. With numerous studies demonstrating unique capabilities in biomarker quantification, diagnosis, and monitoring, OCTA technology has seen rapid adoption in research and clinical settings. The value of OCTA imaging is significantly enhanced by image analysis tools that provide rapid and accurate quantification of vascular features and pathology. Today, the most powerful image analysis methods are based on artificial intelligence (AI). While AI encompasses a large variety of techniques, machine-learning-based, and especially deep-learning-based, image analysis provides accurate measurements in a variety of contexts, including different diseases and regions of the eye. Here, we discuss the principles of both OCTA and AI that make their combination capable of answering new questions. We also review contemporary applications of AI in OCTA, which include accurate detection of pathologies such as choroidal neovascularization, precise quantification of retinal perfusion, and reliable disease diagnosis.
               ",autonomous vehicle
10.1016/j.robot.2018.05.016,journal,Robotics and Autonomous Systems,sciencedirect,2018-09-30,sciencedirect,Adaptive low-level control of autonomous underwater vehicles using deep reinforcement learning,https://api.elsevier.com/content/article/pii/S0921889018301519,"
                  Low-level control of autonomous underwater vehicles (AUVs) has been extensively addressed by classical control techniques. However, the variable operating conditions and hostile environments faced by AUVs have driven researchers towards the formulation of adaptive control approaches. The reinforcement learning (RL) paradigm is a powerful framework which has been applied in different formulations of adaptive control strategies for AUVs. However, the limitations of RL approaches have lead towards the emergence of deep reinforcement learning which has become an attractive and promising framework for developing real adaptive control strategies to solve complex control problems for autonomous systems. However, most of the existing applications of deep RL use video images to train the decision making artificial agent but obtaining camera images only for an AUV control purpose could be costly in terms of energy consumption. Moreover, the rewards are not easily obtained directly from the video frames. In this work we develop a deep RL framework for adaptive control applications of AUVs based on an actor-critic goal-oriented deep RL architecture, which takes the available raw sensory information as input and as output the continuous control actions which are the low-level commands for the AUV’s thrusters. Experiments on a real AUV demonstrate the applicability of the stated deep RL approach for an autonomous robot control problem.
               ",autonomous vehicle
10.1016/j.neucom.2021.01.078,journal,Neurocomputing,sciencedirect,2021-06-07,sciencedirect,SpaceNet: Make Free Space for Continual Learning,https://api.elsevier.com/content/article/pii/S0925231221001545,"The continual learning (CL) paradigm aims to enable neural networks to learn tasks continually in a sequential fashion. The fundamental challenge in this learning paradigm is catastrophic forgetting previously learned tasks when the model is optimized for a new task, especially when their data is not accessible. Current architectural-based methods aim at alleviating the catastrophic forgetting problem but at the expense of expanding the capacity of the model. Regularization-based methods maintain a fixed model capacity; however, previous studies showed the huge performance degradation of these methods when the task identity is not available during inference (e.g. class incremental learning scenario). In this work, we propose a novel architectural-based method referred as SpaceNet 1 1 Code available at: https://github.com/GhadaSokar/SpaceNet for class incremental learning scenario where we utilize the available fixed capacity of the model intelligently. SpaceNet trains sparse deep neural networks from scratch in an adaptive way that compresses the sparse connections of each task in a compact number of neurons. The adaptive training of the sparse connections results in sparse representations that reduce the interference between the tasks. Experimental results show the robustness of our proposed method against catastrophic forgetting old tasks and the efficiency of SpaceNet in utilizing the available capacity of the model, leaving space for more tasks to be learned. In particular, when SpaceNet is tested on the well-known benchmarks for CL: split MNIST, split Fashion-MNIST, CIFAR-10/100, and iCIFAR100, it outperforms regularization-based methods by a big performance gap. Moreover, it achieves better performance than architectural-based methods without model expansion and achieves comparable results with rehearsal-based methods, while offering a huge memory reduction.",autonomous vehicle
10.1016/j.ahj.2020.07.009,journal,American Heart Journal,sciencedirect,2020-11-30,sciencedirect,"Clinical applications of machine learning in the diagnosis, classification, and prediction of heart failure",https://api.elsevier.com/content/article/pii/S0002870320302155,"
                  Machine learning and artificial intelligence are generating significant attention in the scientific community and media. Such algorithms have great potential in medicine for personalizing and improving patient care, including in the diagnosis and management of heart failure. Many physicians are familiar with these terms and the excitement surrounding them, but many are unfamiliar with the basics of these algorithms and how they are applied to medicine. Within heart failure research, current applications of machine learning include creating new approaches to diagnosis, classifying patients into novel phenotypic groups, and improving prediction capabilities. In this paper, we provide an overview of machine learning targeted for the practicing clinician and evaluate current applications of machine learning in the diagnosis, classification, and prediction of heart failure.
               ",autonomous vehicle
10.1016/j.pcl.2020.04.016,journal,Pediatric Clinics of North America,sciencedirect,2020-08-31,sciencedirect,Creating a Learning Televillage and Automated Digital Child Health Ecosystem,https://api.elsevier.com/content/article/pii/S0031395520300456,,autonomous vehicle
10.1016/j.ijheatmasstransfer.2020.120684,journal,International Journal of Heat and Mass Transfer,sciencedirect,2021-02-28,sciencedirect,Supervised learning method for the physical field reconstruction in a nanofluid heat transfer problem,https://api.elsevier.com/content/article/pii/S0017931020336206,"
                  This paper presents a supervised learning method for the physical field reconstruction in a specific heat transfer problem. The deep convolutional neural network (CNN) is applied to predict fields from a few measurable information, while heat transfer characteristics of interest can be then easily inferred from the fields. This data-driven method can establish an end to end mapping from low-dimensional measurable information to full physical fields. Two modes of measurable information are considered as inputs of the network. When the measurable information is an accurate structure or work condition parameters, this method is equivalent as an efficient surrogate model instead of computational fluid dynamics (CFD) simulation. This network can also reconstruct the full-field from local information with several measuring points as inputs. To our best knowledge, this is the first time a CNN based model has been used as a high-fidelity field predicator for the flow heat transfer. To validate this method, the fields of Al2O3-water nanofluid laminar flow in a grooved microchannel are employed to be reconstructed from a set of reduced parameters. It indicates that the reconstruction model enables accurate results for all the temperature, velocity and pressure fields. Meanwhile, the characteristics concerned in a heat transfer process, such as Nu and f, can also be extracted from the reconstructed fields with high precision. Furthermore, the reconstruction performance and stability are verified from several perspectives, including the loss function, train-data size, measuring noise and points layout. At last, the comparison of computational costs shows that a well-trained CNN model has three orders of magnitude faster than CFD solver. The proposed approach can provide an efficient analysis tool with acceptable accuracy for heat transfer research.
               ",autonomous vehicle
10.1016/j.rser.2021.111051,journal,Renewable and Sustainable Energy Reviews,sciencedirect,2021-07-31,sciencedirect,"Scientometric review of artificial intelligence for operations & maintenance of wind turbines: The past, present and future",https://api.elsevier.com/content/article/pii/S1364032121003403,"
                  Wind energy has emerged as a highly promising source of renewable energy in recent times. However, wind turbines regularly suffer from operational inconsistencies, leading to significant costs and challenges in operations and maintenance (O&M). Condition-based monitoring (CBM) and performance assessment/analysis of turbines are vital aspects for ensuring efficient O&M planning and cost minimisation. Data-driven decision making techniques have witnessed rapid evolution in the wind industry for such O&M tasks during the last decade, from applying signal processing methods in early 2010 to artificial intelligence (AI) techniques, especially deep learning in 2020. In this article, we utilise statistical computing to present a scientometric review of the conceptual and thematic evolution of AI in the wind energy sector, providing evidence-based insights into present strengths and limitations of data-driven decision making in the wind industry. We provide a perspective into the future and on current key challenges in data availability and quality, lack of transparency in black box-natured AI models, and prevailing issues in deploying models for real-time decision support, along with possible strategies to overcome these problems. We hope that a systematic analysis of the past, present and future of CBM and performance assessment can encourage more organisations to adopt data-driven decision making techniques in O&M towards making wind energy sources more reliable, contributing to the global efforts of tackling climate change.
               ",autonomous vehicle
10.1016/j.drudis.2020.12.003,journal,Drug Discovery Today,sciencedirect,2021-03-31,sciencedirect,Advanced machine-learning techniques in drug discovery,https://api.elsevier.com/content/article/pii/S1359644620305213,"The popularity of machine learning (ML) across drug discovery continues to grow, yielding impressive results. As their use increases, so do their limitations become apparent. Such limitations include their need for big data, sparsity in data, and their lack of interpretability. It has also become apparent that the techniques are not truly autonomous, requiring retraining even post deployment. In this review, we detail the use of advanced techniques to circumvent these challenges, with examples drawn from drug discovery and allied disciplines. In addition, we present emerging techniques and their potential role in drug discovery. The techniques presented herein are anticipated to expand the applicability of ML in drug discovery.",autonomous vehicle
10.1016/j.cosrev.2020.100303,journal,Computer Science Review,sciencedirect,2020-11-30,sciencedirect,Leveraging Deep Learning and IoT big data analytics to support the smart cities development: Review and future directions,https://api.elsevier.com/content/article/pii/S1574013720304032,"
                  The rapid growth of urban populations worldwide imposes new challenges on citizens’ daily lives, including environmental pollution, public security, road congestion, etc. New technologies have been developed to manage this rapid growth by developing smarter cities. Integrating the Internet of Things (IoT) in citizens’ lives enables the innovation of new intelligent services and applications that serve sectors around the city, including healthcare, surveillance, agriculture, etc. IoT devices and sensors generate large amounts of data that can be analyzed to gain valuable information and insights that help to enhance citizens’ quality of life. Deep Learning (DL), a new area of Artificial Intelligence (AI), has recently demonstrated the potential for increasing the efficiency and performance of IoT big data analytics. In this survey, we provide a review of the literature regarding the use of IoT and DL to develop smart cities. We begin by defining the IoT and listing the characteristics of IoT-generated big data. Then, we present the different computing infrastructures used for IoT big data analytics, which include cloud, fog, and edge computing. After that, we survey popular DL models and review the recent research that employs both IoT and DL to develop smart applications and services for smart cities. Finally, we outline the current challenges and issues faced during the development of smart city services.
               ",autonomous vehicle
10.1016/j.triboint.2021.107326,journal,Tribology International,sciencedirect,2022-01-31,sciencedirect,Machine learning models of the transition from solid to liquid lubricated friction and wear in aluminum-graphite composites,https://api.elsevier.com/content/article/pii/S0301679X21004746,"
                  We study wear and friction of dry and lubricated aluminum-graphite composites and the transition between lubrication regimes. Using Principal Component Analysis, we perform dimensionality reduction for the 14 material and tribological variables to find clusters in friction and wear data. Five standalone and one hybrid supervised regression models were developed to predict friction and wear of lubricated composites. ML analysis identifies lubrication condition and lubricant viscosity as the most important variables. Unlike dry, graphite content has a reduced impact on the tribological behavior with liquid lubricants. The incorporation of graphite in the matrix of aluminum alloys enables them to run under boundary lubrication and run for more extended periods with lower friction even after the lubricant is drained out.
               ",autonomous vehicle
10.1016/B978-0-12-821092-5.00010-3,journal,Applications of Artificial Intelligence in Process Systems Engineering,sciencedirect,2021-12-31,sciencedirect,Chapter 1: Artificial intelligence in process systems engineering,https://api.elsevier.com/content/article/pii/B9780128210925000103,"
               Accompanied by the great advances in computer hardware and the widespread commercial application in big data, the artificial intelligence (AI) represented by the machine learning technology has gained popular applications in the past two decades. On the other side, some challenges such as multiscale modeling, simulation, optimization, control, and supply chain management have been encountered in the research of process system engineering (PSE). Advanced AI technology like deep learning, reinforcement learning, etc. provides a promising way to solve the above-mentioned problems in the PSE from a different perspective. Therefore, the background about PSE and typical branching of AI are introduced to give us an overall grasp about both disciplines. In addition, some work related to the AI applications in PSE have been reviewed hopefully to provide some inspirations in the relative fields.
            ",autonomous vehicle
10.1016/j.neucom.2013.01.008,journal,Neurocomputing,sciencedirect,2013-12-09,sciencedirect,Advances in artificial neural networks and machine learning,https://api.elsevier.com/content/article/pii/S0925231213001549,"
                  This work aims at a reflection on the evolution of the field of Neurocomputing along the last 20 years that have witnessed the sequence of editions of the International Work-Conference on Artificial Neural Networks (IWANN). This reflection arises inextricably of the evolution of connectionist networks themselves, describing their features and most remarkable particularities, most of which have prevailed in time.
                  Another trend that is worth mentioning is the development of a strong interconnection with other paradigms comprised under the so-called Computational Intelligence, which can be understood as a set of nature-inspired computational methodologies and approaches to address complex real-world problems, which traditional approaches are ineffective or unfeasible to deal with. Indeed, many hybrid computational intelligence schemes have been developed that efficiently combine procedures from the domains of artificial neural networks, machine learning, evolutionary computation and fuzzy logic to be applied in complex domains.
                  Finally, a brief description of the diverse contributions that have been included in this special issue is presented. These papers stem from previous versions presented at IWANN2011.
               ",autonomous vehicle
10.1016/B978-0-12-820273-9.00004-X,journal,Machine Learning in Cardiovascular Medicine,sciencedirect,2021-12-31,sciencedirect,Chapter 4: Deep learning for biomedical applications,https://api.elsevier.com/content/article/pii/B978012820273900004X,"
               The types of data most commonly used for machine learning in biomedical research, including electronic health records, imaging, -omics, sensor data, and medical text, are complex, heterogeneous, poorly annotated, and generally unstructured. However, gaining knowledge and actionable insights from all these data sources is a key challenge in implementing personalized medicine and next-generation healthcare. Deep learning, which describes a class of machine learning algorithms based on neural networks, provides effective opportunities to model, represent, and learn from such complex and heterogeneous sources. Here, we review how this paradigm has already affected healthcare, we note limitations and needs for improved methods and applications, and we discuss the challenges to implement and deploy augmented human intelligence based on deep learning in the clinical domain.
            ",autonomous vehicle
10.1016/j.asoc.2020.106582,journal,Applied Soft Computing,sciencedirect,2020-11-30,sciencedirect,"Deep learning architectures in emerging cloud computing architectures: Recent development, challenges and next research trend",https://api.elsevier.com/content/article/pii/S1568494620305202,"
                  The challenges of the conventional cloud computing paradigms motivated the emergence of the next generation cloud computing architectures. The emerging cloud computing architectures generate voluminous amount of data that are beyond the capability of the shallow intelligent algorithms to process. Deep learning algorithms, with their ability to process large-scale datasets, have recently started gaining tremendous attentions from researchers to solve problem in the emerging cloud computing architectures. However, no comprehensive literature review exists on the applications of deep learning architectures to solve complex problems in emerging cloud computing architectures. To fill this gap, we conducted a comprehensive literature survey on the applications of deep learning architectures in emerging cloud computing architectures. The survey shows that the adoption of deep learning architectures in emerging cloud computing architectures are increasingly becoming an interesting research area. We introduce a new taxonomy of deep learning architectures for emerging cloud computing architectures and provide deep insights into the current state-of-the-art active research works on deep learning to solve complex problems in emerging cloud computing architectures. The synthesis and analysis of the articles as well as their limitation are presented. A lot of challenges were identified in the literature and new future research directions to solve the identified challenges are presented. We believed that this article can serve as a reference guide to new researchers and an update for expert researchers to explore and develop more deep learning applications in the emerging cloud computing architectures.
               ",autonomous vehicle
10.1016/j.neucom.2021.10.021,journal,Neurocomputing,sciencedirect,2022-01-16,sciencedirect,Online continual learning in image classification: An empirical survey,https://api.elsevier.com/content/article/pii/S0925231221014995,"
                  Online continual learning for image classification studies the problem of learning to classify images from an online stream of data and tasks, where tasks may include new classes (class incremental) or data nonstationarity (domain incremental). One of the key challenges of continual learning is to avoid catastrophic forgetting (CF), i.e., forgetting old tasks in the presence of more recent tasks. Over the past few years, a large range of methods and tricks have been introduced to address the continual learning problem, but many have not been fairly and systematically compared under a variety of realistic and practical settings.
                  To better understand the relative advantages of various approaches and the settings where they work best, this survey aims to (1) compare state-of-the-art methods such as Maximally Interfered Retrieval (MIR), iCARL, and GDumb (a very strong baseline) and determine which works best at different memory and data settings as well as better understand the key source of CF; (2) determine if the best online class incremental methods are also competitive in the domain incremental setting; and (3) evaluate the performance of 7 simple but effective tricks such as the ”review” trick and the nearest class mean (NCM) classifier to assess their relative impact. Regarding (1), we observe that iCaRL remains competitive when the memory buffer is small; GDumb outperforms many recently proposed methods in medium-size datasets and MIR performs the best in larger-scale datasets. For (2), we note that GDumb performs quite poorly while MIR – already competitive for (1) – is also strongly competitive in this very different (but important) continual learning setting. Overall, this allows us to conclude that MIR is overall a strong and versatile online continual learning method across a wide variety of settings. Finally for (3), we find that all tricks are beneficial, and when augmented with the “review” trick and NCM classifier, MIR produces performance levels that bring online continual learning much closer to its ultimate goal of matching offline training. Our codes are available at https://github.com/RaptorMai/online-continual-learning.
               ",autonomous vehicle
10.1016/j.asej.2021.05.018,journal,Ain Shams Engineering Journal,sciencedirect,2021-06-10,sciencedirect,Prosumer in smart grids based on intelligent edge computing: A review on Artificial Intelligence Scheduling Techniques,https://api.elsevier.com/content/article/pii/S2090447921002409,"Smart Grid technology has been considered an attractive research issue due to its efficiency in solving energy demand, storage, and power transmission. The integration of IoT technology in the Smart Grids is a critical way to accelerate the digitization of the power grid and is useful for the efficient performance of the energy grid infrastructure. For efficient real-time data analysis and decision-making, the Internet of Things will incorporate various communication systems seamlessly. To achieve efficient communication between all Internet of Things, devices are expected to use multiple means, including smart sensors, cable and wireless communication. Improved Internet of Things sensor technologies and connectivity could theoretically prevent or minimize the potential to natural disaster transmission lines, improve transmission power capacity and reduce economic losses. A smart grid is a variety of sensors, devices, and data sets that continuously capture high-resolution data equal to individual IoT conditions. A vast amount of data is one of the biggest challenges on the Internet of Things. Edge Computing is trying to process data close to linked sensors to address this problem, where the data is gathered and processed. This paper aims to investigate the edge computing solutions for the smart grid. A comprehensive review of both emerging issues and edge computing in the Smart Grid environment is discussed and explained. There are two primary components to the energy sharing process among Prosumers: information/digital technologies and Artificial Intelligence Scheduling Techniques. Each of them is mentioned in detail to discuss the Prosumer smart Grid. Furthermore, Edge Computing and classifications (cloudlet, Fog computing and Multi-Access) are among the suitable network methods mentioned in this paper. Some techniques and methodologies have been extensively covered to improve reader awareness of the Prosumer smart grid system.",autonomous vehicle
10.1016/j.energy.2021.120842,journal,Energy,sciencedirect,2021-09-01,sciencedirect,An investigation on deep learning and wavelet transform to nowcast wind power and wind power ramp: A case study in Brazil and Uruguay,https://api.elsevier.com/content/article/pii/S0360544221010902,"
                  Large variations in wind energy production over a period of minutes to hours is a challenge for electricity balancing authorities. The use of reliable tools for the prediction of wind power and wind power ramp events is essential for the operator of the electrical system. The main objective of this work is to analyze the wind power and wind power ramp forecasting at Brazil and Uruguay. To achieve this goal the wavelet decomposition applying 48 different mother wavelet functions and deep learning techniques are used. The recurrent neural network was trained to perform the forecasting of 1 h ahead, and then, using it, the trained network was applied to recursively infer the forecasting for the next hours of the wind speed. After this computational procedure, the wind power and the wind power ramp were predicted. The results showed good accuracy and can be used as a tool to help national grid operators for the energy supply. The wavelet discrete Meyer family (dmey) demonstrates greater precision in the decomposition of the wind speed signals. Therefore, it is proven that the wavelet dmey is the most accurate in the decomposition of temporal wind signals, whether using signals from tropical or subtropical regions.
               ",autonomous vehicle
10.1016/B978-0-12-809414-3.00011-5,journal,Human Genome Informatics,sciencedirect,2018-12-31,sciencedirect,"Chapter 11: Artificial Intelligence: The Future Landscape of Genomic Medical Diagnosis: Dataset, In Silico Artificial Intelligent Clinical Information, and Machine Learning Systems",https://api.elsevier.com/content/article/pii/B9780128094143000115,,autonomous vehicle
10.1016/j.canlet.2020.03.032,journal,Cancer Letters,sciencedirect,2020-07-01,sciencedirect,Machine Learning in oncology: A clinical appraisal,https://api.elsevier.com/content/article/pii/S0304383520301658,"
                  Machine learning (ML) is a branch of artificial intelligence centered on algorithms which do not need explicit prior programming to function but automatically learn from available data, creating decision models to complete tasks. ML-based tools have numerous promising applications in several fields of medicine. Its use has grown following the increased availability of patient data due to technological advances such as digital health records and high-volume information extraction from medical images. Multiple ML algorithms have been proposed for applications in oncology. For instance, they have been employed for oncological risk assessment, automated segmentation, lesion detection, characterization, grading and staging, prediction of prognosis and therapy response.
                  In the near future, ML could become essential part of every step of oncological screening strategies and patients’ management thus leading to precision medicine.
               ",autonomous vehicle
10.1016/j.jcsr.2021.106856,journal,Journal of Constructional Steel Research,sciencedirect,2021-10-31,sciencedirect,Application of machine learning models for designing CFCFST columns,https://api.elsevier.com/content/article/pii/S0143974X21003382,"
                  In this study, two machine learning (ML) algorithms including support vector regression (SVR) and artificial neural network (ANN) are employed to predict the ultimate strength of rectangular and circular concrete-filled cold-formed steel tubular (CFCFST) columns under concentric and eccentric loading. In total, 730 test results on CFCFST columns are compiled and used to train the algorithms. In addition, 720 rectangular and circular CFCFST columns subjected to concentric and eccentric loading are modelled and analysed using finite element (FE) method to expand the training data. The accuracy of the developed FE models is verified by comparing the simulation results with existing experimental results. To develop the predictive models with the best performance, hyperparameter tuning is performed through random search. The results show that the ANN models have relatively better accuracy than that of SVR ones. Using the well-trained ANN models, a graphical user interface (GUI) is developed to predict the ultimate strength of CFCFST columns. To make the GUI applicable to practical application, reduction factors are proposed by performing reliability analysis using Monte Carlo simulation (MCS). The reduction factors are in accordance with load standards of America, Europe and Australia/New Zealand.
               ",autonomous vehicle
10.1016/j.jbi.2021.103791,journal,Journal of Biomedical Informatics,sciencedirect,2021-06-30,sciencedirect,Comparative study of machine learning methods for COVID-19 transmission forecasting,https://api.elsevier.com/content/article/pii/S1532046421001209,"
                  Within the recent pandemic, scientists and clinicians are engaged in seeking new technology to stop or slow down the COVID-19 pandemic. The benefit of machine learning, as an essential aspect of artificial intelligence, on past epidemics offers a new line to tackle the novel Coronavirus outbreak. Accurate short-term forecasting of COVID-19 spread plays an essential role in improving the management of the overcrowding problem in hospitals and enables appropriate optimization of the available resources (i.e., materials and staff).This paper presents a comparative study of machine learning methods for COVID-19 transmission forecasting. We investigated the performances of deep learning methods, including the hybrid convolutional neural networks-Long short-term memory (LSTM-CNN), the hybrid gated recurrent unit-convolutional neural networks (GAN-GRU), GAN, CNN, LSTM, and Restricted Boltzmann Machine (RBM), as well as baseline machine learning methods, namely logistic regression (LR) and support vector regression (SVR). The employment of hybrid models (i.e., LSTM-CNN and GAN-GRU) is expected to eventually improve the forecasting accuracy of COVID-19 future trends. The performance of the investigated deep learning and machine learning models was tested using confirmed and recovered COVID-19 cases time-series data from seven impacted countries: Brazil, France, India, Mexico, Russia, Saudi Arabia, and the US. The results reveal that hybrid deep learning models can efficiently forecast COVID-19 cases. Also, results confirmed the superior performance of deep learning models compared to the two considered baseline machine learning models. Furthermore, results showed that LSTM-CNN achieved improved performances with an averaged mean absolute percentage error of 3.718%, among others.
               ",autonomous vehicle
10.1016/j.chaos.2020.110338,journal,"Chaos, Solitons & Fractals",sciencedirect,2021-01-31,sciencedirect,Applications of artificial intelligence in battling against covid-19: A literature review,https://api.elsevier.com/content/article/pii/S0960077920307335,"
                  Colloquially known as coronavirus, the Severe Acute Respiratory Syndrome CoronaVirus 2 (SARS-CoV-2), that causes CoronaVirus Disease 2019 (COVID-19), has become a matter of grave concern for every country around the world. The rapid growth of the pandemic has wreaked havoc and prompted the need for immediate reactions to curb the effects. To manage the problems, many research in a variety of area of science have started studying the issue. Artificial Intelligence is among the area of science that has found great applications in tackling the problem in many aspects. Here, we perform an overview on the applications of AI in a variety of fields including diagnosis of the disease via different types of tests and symptoms, monitoring patients, identifying severity of a patient, processing covid-19 related imaging tests, epidemiology, pharmaceutical studies, etc. The aim of this paper is to perform a comprehensive survey on the applications of AI in battling against the difficulties the outbreak has caused. Thus we cover every way that AI approaches have been employed and to cover all the research until the writing of this paper. We try organize the works in a way that overall picture is comprehensible. Such a picture, although full of details, is very helpful in understand where AI sits in current pandemonium. We also tried to conclude the paper with ideas on how the problems can be tackled in a better way and provide some suggestions for future works.
               ",autonomous vehicle
10.1016/j.ebiom.2019.08.027,journal,EBioMedicine,sciencedirect,2019-09-30,sciencedirect,Looking beyond the hype: Applied AI and machine learning in translational medicine,https://api.elsevier.com/content/article/pii/S2352396419305493,"Big data problems are becoming more prevalent for laboratory scientists who look to make clinical impact. A large part of this is due to increased computing power, in parallel with new technologies for high quality data generation. Both new and old techniques of artificial intelligence (AI) and machine learning (ML) can now help increase the success of translational studies in three areas: drug discovery, imaging, and genomic medicine. However, ML technologies do not come without their limitations and shortcomings. Current technical limitations and other limitations including governance, reproducibility, and interpretation will be discussed in this article. Overcoming these limitations will enable ML methods to be more powerful for discovery and reduce ambiguity within translational medicine, allowing data-informed decision-making to deliver the next generation of diagnostics and therapeutics to patients quicker, at lowered costs, and at scale.",autonomous vehicle
10.1016/j.compchemeng.2021.107308,journal,Computers & Chemical Engineering,sciencedirect,2021-06-30,sciencedirect,Performance prediction of trace metals and cod in wastewater treatment using artificial neural network,https://api.elsevier.com/content/article/pii/S0098135421000867,"
                  Artificial intelligence is finding its ways into the mainstream of day-to-day operations. Novel AI application techniques such as the artificial neural network (ANN), fuzzy logic, genetic algorithms and expert systems have gained popularity in the fourth industrial revolution era. Due to the chemical composition, inherent complexity, incoherent flow rate and higher safety factor in the effective operation of the biological wastewater treatment process, the AI-based model was extensively tested in managing the wastewater treatment operations. The interrelationship between COD and trace metals was studied using an AI-based prediction model with ANNs incorporated in MATLAB. A supervised learning algorithm was used for training the ANNs and to relate input data to output data. The training was aimed at estimating, validating, predicting the parameters by an error function minimization. The goodness of the prediction was attained with the coefficient of determination (R2) of 0.98–0.99, a sum of square error (SSE) 0.00029–0.1598, room mean-square error (RMSE) of 0.0049–0.8673, mean squared error (MSE) 2.7059e-14 to 2.3175e-15. The ANNs models were found to be a robust tool for predicting WWTP performance. The predictive approaches can be used in the prediction of environmental management and other emerging technologies. This will meet the cost-effectiveness, effective environmental and technical criteria with a wide range of big-data support and implementation of the sustainable development goals, circular bio-economy and industry 4.0.
                  
               ",autonomous vehicle
10.1016/j.tust.2021.103946,journal,Tunnelling and Underground Space Technology,sciencedirect,2021-07-31,sciencedirect,State-of-the-art review of geotechnical-driven artificial intelligence techniques in underground soil-structure interaction,https://api.elsevier.com/content/article/pii/S0886779821001371,"
                  There has been an increasing demand for underground construction due to urbanization and limited land in metropolitan cities in the recent years. However, the behavior of underground structures in soils and rocks is often not completely understood. The emergence of Artificial Intelligence (AI) techniques is envisaged to have a huge potential in addressing geotechnical problems that involve complex soil-structure interaction. This paper thus aims at reviewing the applications of AI techniques in studying underground soil-structure interaction, which focuses on aspects such as characterization of soils and rocks, pile foundations, deep excavations and tunneling. An overview of different AI techniques is provided and a list of key AI applications in underground works that have been published in the last ten years is also compiled to study the recent trend of machine learning techniques in underground construction. The capabilities and limitations of these techniques are discussed throughout the paper, to help readers understand various techniques that are suitable for different underground geotechnical applications. Lastly, some of the challenges that may be faced when applying the techniques are identified, and recent development of AI in geotechnical engineering is discussed in which possible countermeasures to overcome these limitations are highlighted.
               ",autonomous vehicle
10.1016/j.drudis.2019.07.006,journal,Drug Discovery Today,sciencedirect,2019-10-31,sciencedirect,"Deep learning in drug discovery: opportunities, challenges and future prospects",https://api.elsevier.com/content/article/pii/S135964461930282X,"
                  Artificial Intelligence (AI) is an area of computer science that simulates the structures and operating principles of the human brain. Machine learning (ML) belongs to the area of AI and endeavors to develop models from exposure to training data. Deep Learning (DL) is another subset of AI, where models represent geometric transformations over many different layers. This technology has shown tremendous potential in areas such as computer vision, speech recognition and natural language processing. More recently, DL has also been successfully applied in drug discovery. Here, I analyze several relevant DL applications and case studies, providing a detailed view of the current state-of-the-art in drug discovery and highlighting not only the problematic issues, but also the successes and opportunities for further advances.
               ",autonomous vehicle
10.1016/j.jcmg.2020.07.015,journal,JACC: Cardiovascular Imaging,sciencedirect,2020-09-30,sciencedirect,Proposed Requirements for Cardiovascular Imaging-Related Machine Learning Evaluation (PRIME): A Checklist: Reviewed by the American College of Cardiology Healthcare Innovation Council,https://api.elsevier.com/content/article/pii/S1936878X20306367,"Machine learning (ML) has been increasingly used within cardiology, particularly in the domain of cardiovascular imaging. Due to the inherent complexity and flexibility of ML algorithms, inconsistencies in the model performance and interpretation may occur. Several review articles have been recently published that introduce the fundamental principles and clinical application of ML for cardiologists. This paper builds on these introductory principles and outlines a more comprehensive list of crucial responsibilities that need to be completed when developing ML models. This paper aims to serve as a scientific foundation to aid investigators, data scientists, authors, editors, and reviewers involved in machine learning research with the intent of uniform reporting of ML investigations. An independent multidisciplinary panel of ML experts, clinicians, and statisticians worked together to review the theoretical rationale underlying 7 sets of requirements that may reduce algorithmic errors and biases. Finally, the paper summarizes a list of reporting items as an itemized checklist that highlights steps for ensuring correct application of ML models and the consistent reporting of model specifications and results. It is expected that the rapid pace of research and development and the increased availability of real-world evidence may require periodic updates to the checklist.",autonomous vehicle
10.1016/j.neucom.2021.07.098,journal,Neurocomputing,sciencedirect,2021-11-20,sciencedirect,Federated learning on non-IID data: A survey,https://api.elsevier.com/content/article/pii/S0925231221013254,"
                  Federated learning is an emerging distributed machine learning framework for privacy preservation. However, models trained in federated learning usually have worse performance than those trained in the standard centralized learning mode, especially when the training data are not independent and identically distributed (Non-IID) on the local devices. In this survey, we provide a detailed analysis of the influence of Non-IID data on both parametric and non-parametric machine learning models in both horizontal and vertical federated learning. In addition, current research work on handling challenges of Non-IID data in federated learning are reviewed, and both advantages and disadvantages of these approaches are discussed. Finally, we suggest several future research directions before concluding the paper.
               ",autonomous vehicle
10.1016/S1001-9294(21)00057-2,journal,Chinese Medical Sciences Journal,sciencedirect,2021-09-30,sciencedirect,Advances of Artificial Intelligence Application in Medical Imaging of Ovarian Cancers,https://api.elsevier.com/content/article/pii/S1001929421000572,"
                  Ovarian cancer is one of the three most common gynecological cancers in the world, and is regarded as a priority in terms of women's cancer. In the past few years, many researchers have attempted to develop and apply artificial intelligence (AI) techniques to multiple clinical scenarios of ovarian cancer, especially in the field of medical imaging. AI-assisted imaging studies have involved computer tomography (CT), ultrasonography (US), and magnetic resonance imaging (MRI). In this review, we perform a literature search on the published studies that using AI techniques in the medical care of ovarian cancer, and bring up the advances in terms of four clinical aspects, including medical diagnosis, pathological classification, targeted biopsy guidance, and prognosis prediction. Meanwhile, current status and existing issues of the researches on AI application in ovarian cancer are discussed.
               ",autonomous vehicle
10.1016/B978-0-12-823504-1.00011-8,journal,Deep Learning Models for Medical Imaging,sciencedirect,2022-12-31,sciencedirect,Chapter 1: Introduction,https://api.elsevier.com/content/article/pii/B9780128235041000118,"
               
                  This chapter provides basic machine learning mechanisms. It includes supervised, unsupervised, semisupervised, and reinforcement learning algorithms. Besides, different learning techniques are discussed, namely rule-based learning, feature-based learning, and representational learning. Deep learning fundamental and its importance are discussed in the medical imaging field by taking recent works in the domain. Lastly, the scope of the book is neatly discussed.
            ",autonomous vehicle
10.1016/j.cocom.2021.e00597,journal,Computational Condensed Matter,sciencedirect,2021-12-31,sciencedirect,Recent machine learning guided material research - A review,https://api.elsevier.com/content/article/pii/S2352214321000605,"
                  Sustainable development of modern society demands discovering new materials with superior properties in different applications such as aerospace, wind, civil, automotive, etc. Characterizing and predicting material properties using traditional methods are time consuming and expensive. Therefore, advanced methods have been developed to meet the need for quick and reliable design and characterizing of materials properties. ML methods have made it possible to optimize and automate design performance and discover new materials. This review paper gives an overview of the implementation of ML in i) discovery of new materials, and ii) characterization of materials ML. Various ML models for materials manufacturing as well as how ML is applied to model materials are discussed. Recent advances, ML applications, as well as upcoming challenges and perspectives are discussed.
               ",autonomous vehicle
10.1016/j.jretai.2020.12.001,journal,Journal of Retailing,sciencedirect,2020-12-23,sciencedirect,The role of machine learning analytics and metrics in retailing research,https://api.elsevier.com/content/article/pii/S0022435920300932,"
                  This research presents the use of machine learning analytics and metrics in the retailing context. We first discuss what is machine learning and explain the field’s origins. We then demonstrate the strengths of machine learning methods using an online retailing dataset, noting key areas of divergence from the traditional explanatory approach to data analysis. We then provide a review of the current state of machine learning in top-level retailing and marketing research, integrating ideas for future research and showcasing potential applications for practitioners. We propose that the explanatory and machine learning approaches need not be mutually exclusive. Particularly, we discuss four key areas in the general scientific research process that can benefit from machine learning: data exploration/theory building, variable creation, estimation, and predicting an outcome metric. Due to the customer-facing nature of retailing, we anticipate several challenges researchers and practitioners might face in the adoption and implementation of machine learning, such as ethical prediction and customer privacy issues. Overall, our belief is that machine learning can enhance customer experience and, accordingly, we advance opportunities for future research.
               ",autonomous vehicle
10.1016/j.cma.2018.11.026,journal,Computer Methods in Applied Mechanics and Engineering,sciencedirect,2019-04-01,sciencedirect,"Meta-modeling game for deriving theory-consistent, microstructure-based traction–separation laws via deep reinforcement learning",https://api.elsevier.com/content/article/pii/S0045782518305851,"
                  This paper presents a new meta-modeling framework that employs deep reinforcement learning (DRL) to generate mechanical constitutive models for interfaces. The constitutive models are conceptualized as information flow in directed graphs. The process of writing constitutive models is simplified as a sequence of forming graph edges with the goal of maximizing the model score (a function of accuracy, robustness and forward prediction quality). Thus meta-modeling can be formulated as a Markov decision process with well-defined states, actions, rules, objective functions and rewards. By using neural networks to estimate policies and state values, the computer agent is able to efficiently self-improve the constitutive model it generated through self-playing, in the same way AlphaGo Zero (the algorithm that outplayed the world champion in the game of Go) improves its gameplay. Our numerical examples show that this automated meta-modeling framework does not only produces models which outperform existing cohesive models on benchmark traction–separation data, but is also capable of detecting hidden mechanisms among micro-structural features and incorporating them in constitutive models to improve the forward prediction accuracy, both of which are difficult tasks to do manually.
               ",autonomous vehicle
10.1016/j.epsr.2021.107436,journal,Electric Power Systems Research,sciencedirect,2021-10-31,sciencedirect,Energy management of hybrid energy system sources based on machine learning classification algorithms,https://api.elsevier.com/content/article/pii/S037877962100417X,"
                  Hybrid energy systems (HES) that contain renewable energy sources, such as wind and solar energy help to minimize CO2 emissions. Therefore, studying these systems to improve their performance has become one of the critical needs these days due to the environmental crisis. Within HES, energy management (EM) of HES is an essential topic that has been covered in detail by numerous studies, as errors in EM can lead to HES blackouts. Recent research has experimented with energy management strategy (EMS) to achieve optimal EM. This work aims to generate a robust forecasting model for one hour ahead of EM. The present research work has two main objectives. The first objective is to determine which energy source should supply the demand side, using different machine-learning algorithms such as Random Forest (RF), Decision Tree (DT), Gaussian Naive Bayes (Gaussian NB) and K-Nearest Neighbors (KNN). The second objective is to compare the results of these algorithms to choose the algorithm with the best performance and to rank them based on performance as well as accuracy. The work is validated using different algorithms. The results show that DT algorithm has achieved the best performance compared to the RF and Gaussian NB algorithms. KNN algorithm gives the lowest accuracy especially over class 3. The results proof that RF, DT, and Gaussian NB algorithms are reliable.
               ",autonomous vehicle
10.1016/j.patcog.2021.108312,journal,Pattern Recognition,sciencedirect,2022-02-28,sciencedirect,Learning multiscale hierarchical attention for video summarization,https://api.elsevier.com/content/article/pii/S0031320321004921,"
                  In this paper, we propose a multiscale hierarchical attention approach for supervised video summarization. Different from most existing supervised methods which employ bidirectional long short-term memory networks, our method exploits the underlying hierarchical structure of video sequences and learns both the short-range and long-range temporal representations via a intra-block and a inter-block attention. Specifically, we first separate each video sequence into blocks of equal length and employ the intra-block and inter-block attention to learn local and global information, respectively. Then, we integrate the frame-level, block-level, and video-level representations for the frame-level importance score prediction. Next, we conduct shot segmentation and compute shot-level importance scores. Finally, we perform key shot selection to produce video summaries. Moreover, we extend our method into a two-stream framework, where appearance and motion information is leveraged. Experimental results on the SumMe and TVSum datasets validate the effectiveness of our method against state-of-the-art methods.
               ",autonomous vehicle
10.1016/j.engappai.2020.103840,journal,Engineering Applications of Artificial Intelligence,sciencedirect,2020-10-31,sciencedirect,Computation offloading in Edge Computing environments using Artificial Intelligence techniques,https://api.elsevier.com/content/article/pii/S0952197620302050,"
                  Edge Computing (EC) is a recent architectural paradigm that brings computation close to end-users with the aim of reducing latency and bandwidth bottlenecks, which 5G technologies are committed to further reduce, while also achieving higher reliability. EC enables computation offloading from end devices to edge nodes. Deciding whether a task should be offloaded, or not, is not trivial. Moreover, deciding when and where to offload a task makes things even harder and making inadequate or off-time decisions can undermine the EC approach. Recently, Artificial Intelligence (AI) techniques, such as Machine Learning (ML), have been used to help EC systems cope with this problem. AI promises accurate decisions, higher adaptability and portability, thus diminishing the cost of decision-making and the probability of error. In this work, we perform a literature review on computation offloading in EC systems with and without AI techniques. We analyze several AI techniques, especially ML-based, that display promising results, overcoming the shortcomings of current approaches for computing offloading coordination We sorted the ML algorithms into classes for better analysis and provide an in-depth analysis on the use of AI for offloading, in particular, in the use case of offloading in Vehicular Edge Computing Networks, actually one technology that gained more relevance in the last years, enabling a vast amount of solutions for computation and data offloading. We also discuss the main advantages and limitations of offloading, with and without the use of AI techniques.
               ",autonomous vehicle
10.1016/j.knosys.2015.01.010,journal,Knowledge-Based Systems,sciencedirect,2015-05-31,sciencedirect,Transfer learning using computational intelligence: A survey,https://api.elsevier.com/content/article/pii/S0950705115000179,"
                  Transfer learning aims to provide a framework to utilize previously-acquired knowledge to solve new but similar problems much more quickly and effectively. In contrast to classical machine learning methods, transfer learning methods exploit the knowledge accumulated from data in auxiliary domains to facilitate predictive modeling consisting of different data patterns in the current domain. To improve the performance of existing transfer learning methods and handle the knowledge transfer process in real-world systems, computational intelligence has recently been applied in transfer learning. This paper systematically examines computational intelligence-based transfer learning techniques and clusters related technique developments into four main categories: (a) neural network-based transfer learning; (b) Bayes-based transfer learning; (c) fuzzy transfer learning, and (d) applications of computational intelligence-based transfer learning. By providing state-of-the-art knowledge, this survey will directly support researchers and practice-based professionals to understand the developments in computational intelligence-based transfer learning research and applications.
               ",autonomous vehicle
10.1016/j.ijpvp.2021.104471,journal,International Journal of Pressure Vessels and Piping,sciencedirect,2021-10-31,sciencedirect,Applications of machine learning in pipeline integrity management: A state-of-the-art review,https://api.elsevier.com/content/article/pii/S0308016121001666,"
                  Despite being considered the safest means to transport oil and gas, pipelines are susceptible to degradation. Pipeline integrity management (PIM) is implemented to lower the risk of failure due to degradation and to maintain the functionality and safety of pipelines. PIM consists of a set of activities for assessing the operational conditions of pipelines. These activities generate data with high volume, velocity, and variety, due to the length of a pipeline and the number of sensors and tools used to assess the pipeline's condition. This paper provides a comprehensive review in relation to the applications of machine learning (ML) in managing and processing data generated from PIM activities. ML applications in the elements of a PIM process (e.g., inspection, monitoring, and maintenance) are investigated. The aspects of ML techniques (i.e., type of input, pre-processing, learning algorithm, output and evaluation metric) applied in each element of PIM are examined. Current research challenges and future research opportunities in the application of ML in PIM are also discussed.
               ",autonomous vehicle
10.1016/B978-0-12-820601-0.00001-X,journal,Artificial Intelligence in Data Mining,sciencedirect,2021-12-31,sciencedirect,5: Deep learning methods for data classification,https://api.elsevier.com/content/article/pii/B978012820601000001X,"
               Deep learning is the key aspect of machine learning and artificial intelligence. In the past decades the methods introduced from the research of deep learning concepts impact an extensive range of information and signal processing task. The hierarchical models in the deep learning have the facility to learn various levels of data representation corresponding to different abstraction levels that enable the concept of representation in a dense way. Hence, the deep learning methods are extensively used in the last decades in various automatic classification processes. Various deep learning methods developed to perform the data classification process in the data mining activity are discussed in this chapter. Data classification is a data mining technique, where the training samples or database tuples are effectively analyzed to generate a generalized data. However, the classification scheme is used to sort out the future data samples and to provide superior understanding with the contents in the database.
            ",autonomous vehicle
10.1016/j.gloei.2020.05.009,journal,Global Energy Interconnection,sciencedirect,2020-04-30,sciencedirect,Research and application of artificial intelligence service platform for the power field,https://api.elsevier.com/content/article/pii/S2096511720300517,"Conventional analysis methods cannot fully meet the business needs of power grids. At present, several artificial intelligence (AI) projects in a single business field are competing with each other, and the interfaces between the systems lack unified specifications. Therefore, it is imperative to establish a comprehensive service platform. In this paper, an AI platform framework for power fields is proposed; it adopts the deep learning technology to support natural language processing and computer vision services. On one hand, it can provide an algorithm, a model, and service support for power- enterprise applications, and on the other hand, it can provide a large number of heterogeneous data processing, algorithm libraries, intelligent services, model managements, typical application scenarios, and other services for different levels of business personnel. The establishment of the platform framework could break data barrier, improve portability of technology, avoid the investment waste caused by repeated constructions, and lay the foundation for the construction of “platform + application + service” ecological chain.",autonomous vehicle
10.1016/j.gr.2020.08.007,journal,Gondwana Research,sciencedirect,2021-12-31,sciencedirect,Pathways and challenges of the application of artificial intelligence to geohazards modelling,https://api.elsevier.com/content/article/pii/S1342937X20302458,"
                  The application of artificial intelligence (AI) and machine learning in geohazard modelling has been rapidly growing in recent years, a trend that is observed in several research and application areas thanks to recent advances in AI. As a result, the increasing dependence on data driven studies has made its practical applications towards geohazards (landslides, debris flows, earthquakes, droughts, floods, glacier studies) an interesting prospect. These aforementioned geohazards were responsible for roughly 80% of the economic loss in the past two decades caused by all natural hazards. The present study analyses the various domains of geohazards which have benefited from classical machine learning approaches and highlights the future course of direction in this field. The emergence of deep learning has fulfilled several gaps in: i) classification; ii) seasonal forecasting as well as forecasting at longer lead times; iii) temporal based change detection. Apart from the usual challenges of dataset availability, climate change and anthropogenic activities, this review paper emphasizes that the future studies should focus on consecutive events along with integration of physical models. The recent catastrophe in Japan and Australia makes a compelling argument to focus towards consecutive events. The availability of higher temporal resolution and multi-hazard dataset will prove to be essential, but the key would be to integrate it with physical models which would improve our understanding of the mechanism involved both in single and consecutive hazard scenario. Geohazards would eventually be a data problem, like geosciences, and therefore it is essential to develop models that would be capable of handling large voluminous data. The future works should also revolve towards interpretable models with the hope of providing a reasonable explanation of the results, thereby achieving the ultimate goal of Explainable AI.
               ",autonomous vehicle
10.1016/B978-0-12-821229-5.00011-2,journal,Machine Learning and the Internet of Medical Things in Healthcare,sciencedirect,2021-12-31,sciencedirect,"Chapter 2: Machine learning in healthcare: review, opportunities and challenges",https://api.elsevier.com/content/article/pii/B9780128212295000112,"
               Machine learning technology is a prominent research field aiming to build a system which imitates human intelligence. Machine learning can be applied in the healthcare domain. It cannot replace human physicians, but it can make better solutions to healthcare problems. Machine learning is the most important area to develop computational approaches automatically. In this chapter, we review the recent literature on applying machine learning technology to promote healthcare solutions. However, we also deliberate limitations, challenges, and opportunities in the healthcare domain using machine learning technology. To monitor and observe the effectiveness of treatment in the healthcare field, machine learning application can be used for diagnosis, prognosis, and perfect treatment plan for the detected disease. Machine learning technology can assist medical practitioner by empowering them with faster and more accurate solutions. In this chapter, readers will find the fundamentals with the progressive developments in the state-of-the-art machine learning-based system for healthcare. However, the evolving nature of medical science and technology creates an innovative scenario that must be studied in an interdisciplinary and holistic way. This chapter aims to obtain novel and quality research-work offerings in healthcare, which are facilitated by the machine learning procedures and techniques. Healthcare industries are focused on enhancing the power of machine learning because it considers a large amount of data daily.
            ",autonomous vehicle
10.1016/j.matpr.2021.01.847,journal,Materials Today: Proceedings,sciencedirect,2021-12-31,sciencedirect,CNN algorithm for plant classification in deep learning,https://api.elsevier.com/content/article/pii/S2214785321009445,"
                  The prior methodology of characterizing the plants for dependent on surface based order and another strategy depends on KNN classifier. This paper presents qualities examination of plants utilizing picture preparing methods for robotized vision framework utilized at horticultural field. In farming examination, the programmed plant attributes recognition is fundamental one in observing huge field. The proposed dynamic framework uses picture content portrayal and regulated classifier sort of neural organization. This will naturally distinguish the plant species when we import its picture as info. Picture preparing strategies for this sort of choice investigation includes pre-processing and characterization stage. At Processing, an info picture will be resized and commotion expulsion procedure is applied. At definite stage the neural organization orders the pictures as farming plant, harmful plant and therapeutic plant separately. At that point it will show the attributes of each plant.
               ",autonomous vehicle
10.1016/j.inffus.2019.12.004,journal,Information Fusion,sciencedirect,2020-06-30,sciencedirect,"Continual learning for robotics: Definition, framework, learning strategies, opportunities and challenges",https://api.elsevier.com/content/article/pii/S1566253519307377,"
                  Continual learning (CL) is a particular machine learning paradigm where the data distribution and learning objective change through time, or where all the training data and objective criteria are never available at once. The evolution of the learning process is modeled by a sequence of learning experiences where the goal is to be able to learn new skills all along the sequence without forgetting what has been previously learned. CL can be seen as an online learning where knowledge fusion needs to take place in order to learn from streams of data presented sequentially in time. Continual learning also aims at the same time at optimizing the memory, the computation power and the speed during the learning process. An important challenge for machine learning is not necessarily finding solutions that work in the real world but rather finding stable algorithms that can learn in real world. Hence, the ideal approach would be tackling the real world in a embodied platform: an autonomous agent. Continual learning would then be effective in an autonomous agent or robot, which would learn autonomously through time about the external world, and incrementally develop a set of complex skills and knowledge.Robotic agents have to learn to adapt and interact with their environment using a continuous stream of observations. Some recent approaches aim at tackling continual learning for robotics, but most recent papers on continual learning only experiment approaches in simulation or with static datasets. Unfortunately, the evaluation of those algorithms does not provide insights on whether their solutions may help continual learning in the context of robotics. This paper aims at reviewing the existing state of the art of continual learning, summarizing existing benchmarks and metrics, and proposing a framework for presenting and evaluating both robotics and non robotics approaches in a way that makes transfer between both fields easier. We put light on continual learning in the context of robotics to create connections between fields and normalize approaches.
               ",autonomous vehicle
10.1016/j.compind.2019.02.003,journal,Computers in Industry,sciencedirect,2019-06-30,sciencedirect,Deep neural networks with transfer learning in millet crop images,https://api.elsevier.com/content/article/pii/S0166361518305888,"
                  Plant or crop diseases are important items in the reduction of quality and quantity in agriculture. Therefore, the detection and diagnosis of these diseases are very necessary. The appropriate classification with small datasets in Deep Learning is a major scientific challenge. Furthermore, it is difficult and expensive to generate labeled data manually according to certain selection criteria. The approaches using transfer learning aims to resolve this problem by recognizing and applying knowledge and abilities learned in previous tasks to novel tasks (in new domains).
                  In this paper, we propose an approach using transfer learning with feature extraction to build an identification system of mildew disease in pearl millet. The deep learning facilitates a practically fast and interesting data analysis in precision agriculture. The expected advantage of the proposal is to provide support to stakeholders (researchers and farmers) through the information and knowledge generated by the reasoning process. The experimental result gives an encouraging performance that is the accuracy of 95.00%, the precision of 90.50%, the recall of 94.50% and the f1-score of 91.75%.
               ",autonomous vehicle
10.1016/j.mlwa.2021.100046,journal,Machine Learning with Applications,sciencedirect,2021-09-15,sciencedirect,Public policymaking for international agricultural trade using association rules and ensemble machine learning,https://api.elsevier.com/content/article/pii/S2666827021000232,"International economics has a long history of improving our understanding of factors causing trade, and the consequences of free flow of goods and services across countries. The recent shocks to the free-trade regime, especially trade disputes among major economies, as well as black swan events (such as trade wars and pandemics), raise the need for improved predictions to inform policy decisions. Artificial Intelligence (AI) methods are allowing economists to solve such prediction problems in new ways. In this manuscript, we present novel methods that predict and associate food and agricultural commodities traded internationally. Association Rules (AR) analysis has been deployed successfully for economic scenarios at the consumer or store level (such as for market basket analysis). In our work however; we present analysis of imports/exports associations and their effects on country–commodity trade flows. Moreover, Ensemble Machine Learning (EML) methods are developed to provide improved agricultural trade predictions, outlier events’ implications, and quantitative pointers to policy makers.",autonomous vehicle
10.1016/j.patcog.2018.11.006,journal,Pattern Recognition,sciencedirect,2019-04-30,sciencedirect,Incremental semi-supervised learning on streaming data,https://api.elsevier.com/content/article/pii/S0031320318303923,"
                  In streaming data classification, most of the existing methods assume that all arrived evolving data are completely labeled. One challenge is that some applications where only small amount of labeled examples are available for training. Incremental semi-supervised learning algorithms have been proposed for regularizing neural networks by incorporating various side information, such as pairwise constraints or user-provided labels. However, it is hard to put them into practice, especially for non-stationary environments due to the effectiveness and parameter sensitivity of such algorithms. In this paper, we propose a novel incremental semi-supervised learning framework on streaming data. Each layer of model is comprised of a generative network, a discriminant structure and the bridge. The generative network uses dynamic feature learning based on autoencoders to learn generative features from streaming data which has been demonstrated its potential in learning latent feature representations. In addition, the discriminant structure regularizes the network construction via building pairwise similarity and dissimilarity constraints. It is also used for facilitating the parameter learning of the generative network. The network and structure are integrated into a joint learning framework and bridged by enforcing the correlation of their parameters, which balances the flexible incorporation of supervision information and numerical tractability for non-stationary environments as well as explores the intrinsic data structure. Moreover, an efficient algorithm is designed to solve the proposed optimization problem and we also give an ensemble method. Particularly, when multiple layers of model are stacked, the performance is significantly boosted. Finally, to validate the effectiveness of the proposed method, extensive experiments are conducted on synthetic and real-life datasets. The experimental results demonstrate that the performance of the proposed algorithms is superior to some state-of-the-art approaches.
               ",autonomous vehicle
10.1016/j.engappai.2020.103678,journal,Engineering Applications of Artificial Intelligence,sciencedirect,2020-06-30,sciencedirect,"Potential, challenges and future directions for deep learning in prognostics and health management applications",https://api.elsevier.com/content/article/pii/S0952197620301184,"Deep learning applications have been thriving over the last decade in many different domains, including computer vision and natural language understanding. The drivers for the vibrant development of deep learning have been the availability of abundant data, breakthroughs of algorithms and the advancements in hardware. Despite the fact that complex industrial assets have been extensively monitored and large amounts of condition monitoring signals have been collected, the application of deep learning approaches for detecting, diagnosing and predicting faults of complex industrial assets has been limited. The current paper provides a thorough evaluation of the current developments, drivers, challenges, potential solutions and future research needs in the field of deep learning applied to Prognostics and Health Management (PHM) applications.",autonomous vehicle
10.1016/j.compmedimag.2021.101933,journal,Computerized Medical Imaging and Graphics,sciencedirect,2021-07-31,sciencedirect,"Digital imaging, technologies and artificial intelligence applications during COVID-19 pandemic",https://api.elsevier.com/content/article/pii/S0895611121000823,"
                  The advancement of technology remained an immersive interest for humankind throughout the past decades. Tech enterprises offered a stream of innovation to address the universal healthcare concerns. The novel coronavirus holds a substantial foothold of planet earth which is combatted by digital interventions across afflicted geographical boundaries and territories. This study aims to explore the trends of modern healthcare technologies and Artificial Intelligence (AI) during COVID-19 crisis, define the concepts and clinical role of AI in the mitigation of COVID-19, investigate and correlate the efficacy of AI-enabled technology in medical imaging during COVID-19 and determine advantages, drawbacks, and challenges of artificial intelligence during COVID-19 pandemic.
                  The paper applied systematic review approach using a deliberated research protocol and Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) flow chart.
                  Digital technologies can coordinate COVID-19 responses in a cascade fashion that extends from the clinical care facility to the exterior of the pending viral epicenter. With cases of healthcare robotics, aerial drones, and the internet of things as evidentiary examples. PCR tests and medical imaging are the frontier diagnostics of COVID-19. Computed tomography helped to correct the accuracy variation of PCR tests at a clinical sensitivity of 98 %. Artificial intelligence can enable autonomous COVID-19 responses using techniques like machine learning.
                  Technology could be an endless system of innovation and opportunities when sourced effectively. Scientists can utilize technology to resolve global concerns challenging the history of tangible possibility. Digital interventions have enhanced the responses to COVID-19, magnified the role of medical imaging amid the COVID-19 crisis and have exposed healthcare professionals to the opportunity of contactless care.
               ",autonomous vehicle
10.1016/j.cosrev.2021.100413,journal,Computer Science Review,sciencedirect,2021-08-31,sciencedirect,A systematic literature review on machine learning applications for consumer sentiment analysis using online reviews,https://api.elsevier.com/content/article/pii/S1574013721000538,"
                  Consumer sentiment analysis is a recent fad for social media-related applications such as healthcare, crime, finance, travel, and in academia. Disentangling consumer perception to gain insight into the desired objective and reviews is significant. With the advancement of technology, a massive amount of social web data increasing in volume, subjectivity, and heterogeneity becomes challenging to process manually. Machine learning (ML) techniques have been utilized to handle this difficulty in real-life applications. This paper presents a study to determine the usefulness, scope, and applicability of this alliance of ML techniques for consumer sentiment analysis (CSA) for online reviews in the domain of hospitality and tourism. We show a systematic literature review to compare, analyse, explore, and understand the attempts and directions to find research gaps in illustrating the future scope of this pairing. The primary objective is to read and analyse the use of ML techniques for consumer sentiment analysis on online reviews in the domain of hospitality and tourism. This research has significant implications for service providers in terms of developing managerial strategies for consumers in terms of selecting services that meet their needs. Furthermore, there is high impact for researchers in terms of prospective research directions.
               ",autonomous vehicle
10.3182/20060906-3-IT-2910.00013,journal,IFAC Proceedings Volumes,sciencedirect,2006-12-31,sciencedirect,A NEURAL ARCHITECTURE FOR ONLINE PATH LEARNING IN MAZE NAVIGATION,https://api.elsevier.com/content/article/pii/S1474667016384920,"
                  This paper describes a neural network architecture and the online learning policies that permits to an autonomous robot navigates though a maze in order to memorize a path that explores the entire environment, while avoiding obstacles. The state space representation is constructed by unsupervised and competitive learning as well as the mapping state-action is constructed by means of reinforcement learning, during the maze exploration. The result of learning creates a memory of states-actions that emerges an intelligent behavior, such as the path learning. The robot uses only its own infrared distance-sensors to perform obstacle detection, used as pattern recognition cues, while moving in a maze environment. In order to demonstrate the effectiveness and real-time ability of the proposed neural controller, we report a number of simulation results of navigation in unknown maze environments.
               ",autonomous vehicle
10.1016/j.robot.2016.08.021,journal,Robotics and Autonomous Systems,sciencedirect,2016-12-31,sciencedirect,Hierarchical reinforcement learning as creative problem solving,https://api.elsevier.com/content/article/pii/S0921889016305371,"
                  Although creativity is studied from philosophy to cognitive robotics, a definition has proven elusive. We argue for emphasizing the creative process (the cognition of the creative agent), rather than the creative product (the artifact or behavior). Owing to developments in experimental psychology, the process approach has become an increasingly attractive way of characterizing creative problem solving. In particular, the phenomenon of insight, in which an individual arrives at a solution through a sudden change in perspective, is a crucial component of the process of creativity.
                  These developments resonate with advances in machine learning, in particular hierarchical and modular approaches, as the field of artificial intelligence aims for general solutions to problems that typically rely on creativity in humans or other animals. We draw a parallel between the properties of insight according to psychology and the properties of Hierarchical Reinforcement Learning (HRL) systems for embodied agents. Using the Creative Systems Framework developed by Wiggins and Ritchie, we analyze both insight and HRL, establishing that they are creative in similar ways. We highlight the key challenges to be met in order to call an artificial system “insightful”.
               ",autonomous vehicle
10.1016/j.aeue.2021.153739,journal,AEU - International Journal of Electronics and Communications,sciencedirect,2021-06-30,sciencedirect,Efficient modelling of compact microstrip antenna using machine learning,https://api.elsevier.com/content/article/pii/S1434841121001369,"
                  In this article, an application of regression-based machine learning (ML) approaches to compute resonant frequency at dominant mode 
                        
                           
                              
                                 TM
                              
                              
                                 10
                              
                           
                        
                     , slot dimensions of square patch, and patch dimensions of compact microstrip antenna (SPCMA) in the frequency band of 0.4856–7.8476 GHz is presented. In the design process, a squared patch microstrip antenna with two identical slots at the opposite side of a radiating edge of the antenna is loaded. The resonant frequencies of three thousand eight hundred and twenty-two SPCMAs have simulated with CST microwave studio 2019 by varying slot size, the thickness of the material, patch length, and dielectric materials is in accordance with specification of VHF, ULF, L, S, and C band applications. A comparison of 20 regression-based machine learning algorithms including artificial neural network is presented, and it is observed that the Gaussian Process Regression(GPR) model predicts physical or electrical parameters more accurately. The proposed GPR model is validated by fabricating and characterizing a prototype of a microstrip antenna. The fabricated antenna performance is very close to the designed antenna and predicted by GPR.
               ",autonomous vehicle
10.1016/j.jnca.2021.103005,journal,Journal of Network and Computer Applications,sciencedirect,2021-05-01,sciencedirect,Applying machine learning techniques for caching in next-generation edge networks: A comprehensive survey,https://api.elsevier.com/content/article/pii/S1084804521000321,"
                  Edge networking is a complex and dynamic computing paradigm that aims to push cloud re-sources closer to the end user improving responsiveness and reducing backhaul traffic. User mobility, preferences, and content popularity are the dominant dynamic features of edge networks. Temporal and social features of content, such as the number of views and likes are leveraged to estimate the popularity of content from a global perspective. However, such estimates should not be mapped to an edge network with particular social and geographic characteristics. In next generation edge networks, i.e., 5G and beyond 5G, machine learning techniques can be applied to predict content popularity based on user preferences, cluster users based on similar content interests, and optimize cache placement and replacement strategies provided a set of constraints and predictions about the state of the network. These applications of machine learning can help identify relevant content for an edge network. This article investigates the application of machine learning techniques for in-network caching in edge networks. We survey recent state-of-the-art literature and formulate a comprehensive taxonomy based on (a) machine learning technique (method, objective, and features), (b) caching strategy (policy, location, and replacement), and (c) edge network (type and delivery strategy). A comparative analysis of the state-of-the-art literature is presented with respect to the parameters identified in the taxonomy. Moreover, we debate research challenges and future directions for optimal caching decisions and the application of machine learning in edge networks.
               ",autonomous vehicle
10.1016/j.jngse.2021.104134,journal,Journal of Natural Gas Science and Engineering,sciencedirect,2021-10-31,sciencedirect,The development of leak detection model in subsea gas pipeline using machine learning,https://api.elsevier.com/content/article/pii/S1875510021003371,"
                  Pipelines are mainly used to transport crude and refined petroleum, such as natural gas, worldwide. Monitoring pipeline health condition at offshore locations is challenging. Despite several attempts to develop leak detection systems, few can simultaneously detect the leak location and size. It is extremely difficult to obtain abnormal data such as actual leaks from a long-distance subsea pipeline. Dynamic modeling can be a good alternative to overcome this limitation. In this study, based on the dynamic model matched with the field, we conducted various flow simulations and selected the most sensitive variables. By changing these variables within an appropriate range, a machine-learning data set was generated. We used deep neural network methods to train the data and derived the optimal learning model. To improve the model accuracy, we adjusted the pipeline model section size not to exceed 20 m from the initial 50 m and designed models with a more detailed pipeline structure. The mean absolute error for each leak size was separately calculated to assess its effect on learning itself. Overall, the model showed excellent accuracy. However, for leak sizes of 0.5 cm, the accuracy appeared too low because the leak effect on mass flow, pressure, and the temperature was minimal. These parameters have been reported to have a great impact on the accuracy of machine-learning models. Therefore, the leak size detected was rearranged to perform data learning again. As a result, the model accuracy was improved by 80% compared to the initial learning model. Based on our study results, we proposed a flowchart for leak detection in the gas pipeline. The proposed procedure can be applied to various pipelines and support more efficient operation by detecting leaks in real-time.
               ",autonomous vehicle
10.1016/j.artint.2015.05.008,journal,Artificial Intelligence,sciencedirect,2015-09-30,sciencedirect,Transferring knowledge as heuristics in reinforcement learning: A case-based approach,https://api.elsevier.com/content/article/pii/S000437021500082X,"The goal of this paper is to propose and analyse a transfer learning meta-algorithm that allows the implementation of distinct methods using heuristics to accelerate a Reinforcement Learning procedure in one domain (the target) that are obtained from another (simpler) domain (the source domain). This meta-algorithm works in three stages: first, it uses a Reinforcement Learning step to learn a task on the source domain, storing the knowledge thus obtained in a case base; second, it does an unsupervised mapping of the source-domain actions to the target-domain actions; and, third, the case base obtained in the first stage is used as heuristics to speed up the learning process in the target domain. A set of empirical evaluations were conducted in two target domains: the 3D mountain car (using a learned case base from a 2D simulation) and stability learning for a humanoid robot in the Robocup 3D Soccer Simulator (that uses knowledge learned from the Acrobot domain). The results attest that our transfer learning algorithm outperforms recent heuristically-accelerated reinforcement learning and transfer learning algorithms.",autonomous vehicle
10.1016/j.nucengdes.2019.110479,journal,Nuclear Engineering and Design,sciencedirect,2020-04-01,sciencedirect,Status of research and development of learning-based approaches in nuclear science and engineering: A review,https://api.elsevier.com/content/article/pii/S0029549319305102,"Nuclear technology industries have increased their interest in using data-driven methods to improve safety, reliability, and availability of assets. To do so, it is important to understand the fundamentals between the disciplines to effectively develop and deploy such systems. This survey presents an overview of the fundamentals of artificial intelligence and the state of development of learning-based methods in nuclear science and engineering to identify the risks and opportunities of applying such methods to nuclear applications. This paper focuses on applications related to three key subareas related to safety and decision-making. These are reactor health and monitoring, radiation detection, and optimization. The principles of learning-based methods in these applications are explained and recent studies are explored. Furthermore, as these methods have become more practical during the past decade, it is foreseen that the popularity of learning-based methods in nuclear science and technology will increase; consequently, understanding the benefits and barriers of implementing such methodologies can help create better research plans, and identify project risks and opportunities.",autonomous vehicle
10.1016/j.procs.2020.09.087,journal,Procedia Computer Science,sciencedirect,2020-12-31,sciencedirect,"Evaluation of Machine Learning Techniques for Inflow Prediction in Lake Como, Italy",https://api.elsevier.com/content/article/pii/S1877050920319852,"Accurate streamflow prediction is a fundamental task for integrated water resources management and flood risk mitigation. The purpose of this study is to forecast the water inflow to lake Como, (Italy) using different machine learning algorithms. The forecast is done for different days ranging from one day to three days. These models are evaluated by three statistical measures including Mean Absolute Error, Root Mean Squared Error, and the Nash-Sutcliffe Efficiency Coefficient. The experimental results show that Neural Network performs better for streamflow estimation with MAE and RMSE followed by Support Vector Regression and Random Forest.",autonomous vehicle
10.1016/j.artmed.2019.06.005,journal,Artificial Intelligence in Medicine,sciencedirect,2019-07-31,sciencedirect,Deep multiphysics: Coupling discrete multiphysics with machine learning to attain self-learning in-silico models replicating human physiology,https://api.elsevier.com/content/article/pii/S0933365718305852,"
                  Objectives
                  The objective of this study is to devise a modelling strategy for attaining in-silico models replicating human physiology and, in particular, the activity of the autonomic nervous system.
               
                  Method
                  Discrete Multiphysics (a multiphysics modelling technique) and Reinforcement Learning (a Machine Learning algorithm) are combined to achieve an in-silico model with the ability of self-learning and replicating feedback loops occurring in human physiology. Computational particles, used in Discrete Multiphysics to model biological systems, are associated to (computational) neurons: Reinforcement Learning trains these neurons to behave like they would in real biological systems.
               
                  Results
                  As benchmark/validation, we use the case of peristalsis in the oesophagus. Results show that the in-silico model effectively learns by itself how to propel the bolus in the oesophagus.
               
                  Conclusions
                  The combination of first principles modelling (e.g. multiphysics) and machine learning (e.g. Reinforcement Learning) represents a new powerful tool for in-silico modelling of human physiology. Biological feedback loops occurring, for instance, in peristaltic or metachronal motion, which until now could not be accounted for in in-silico models, can be tackled by the proposed technique.
               ",autonomous vehicle
10.1016/j.ddtec.2020.07.003,journal,Drug Discovery Today: Technologies,sciencedirect,2019-12-31,sciencedirect,Current methods and challenges for deep learning in drug discovery,https://api.elsevier.com/content/article/pii/S1740674920300081,"
                  Driven by rapid advances in computer hardware and publicly available datasets over the past decade, deep learning has achieved tremendous success in the transformation of many computational disciplines. These novel technologies have had considerable impact on computer-aided drug design as well, throughout all stages of the development pipeline. A flexible toolbox of neural architectures has been developed that are well-suited to represent the sequential, topological, or geometrical concepts of chemistry and biology; and that are able to either discriminate existing molecules or to generate new ones from scratch. For some biochemical prediction tasks, the state of the art has been advanced; however, for complex and practically relevant projects, the outcomes are less clear-cut. Current deep learning methods rely on massive amounts of labeled examples, but drug discovery data is comparatively limited in quantity and quality. These problems need to be resolved and existing sources used more effectively to demonstrate that deep learning can revolutionize the field in general.
               ",autonomous vehicle
10.1016/j.inffus.2020.10.014,journal,Information Fusion,sciencedirect,2021-03-31,sciencedirect,"Lights and shadows in Evolutionary Deep Learning: Taxonomy, critical methodological analysis, cases of study, learned lessons, recommendations and challenges",https://api.elsevier.com/content/article/pii/S1566253520303833,"
                  Much has been said about the fusion of bio-inspired optimization algorithms and Deep Learning models for several purposes: from the discovery of network topologies and hyperparametric configurations with improved performance for a given task, to the optimization of the model’s parameters as a replacement for gradient-based solvers. Indeed, the literature is rich in proposals showcasing the application of assorted nature-inspired approaches for these tasks. In this work we comprehensively review and critically examine contributions made so far based on three axes, each addressing a fundamental question in this research avenue: (a) optimization and taxonomy (Why?), including a historical perspective, definitions of optimization problems in Deep Learning, and a taxonomy associated with an in-depth analysis of the literature, (b) critical methodological analysis (How?), which together with two case studies, allows us to address learned lessons and recommendations for good practices following the analysis of the literature, and (c) challenges and new directions of research (What can be done, and what for?). In summary, three axes – optimization and taxonomy, critical analysis, and challenges – which outline a complete vision of a merger of two technologies drawing up an exciting future for this area of fusion research.
               ",autonomous vehicle
10.1016/B978-0-323-90231-1.00001-7,journal,Methods for Petroleum Well Optimization,sciencedirect,2022-12-31,sciencedirect,Chapter Seven: Data-driven machine learning solutions to real-time ROP prediction,https://api.elsevier.com/content/article/pii/B9780323902311000017,"
               In recent years, machine learning (ML) algorithms have aided in solving the rate of penetration (ROP) problem in drilling engineering. This chapter shows that ML for ROP prediction could be classified under five methods: artificial neural network, support vector machine, fuzzy inference system, neurofuzzy, and ensemble. The future of drilling operations lies in the ever-increasing implementation of data-driven modeling and its applications in predicting and optimizing highly uncertain downhole environments. With various wellbore complexities exaggerating the overall well cost, achieving drilling efficiency with the highest possible ROP is now more imperative than ever.
            ",autonomous vehicle
10.1016/B978-0-323-67538-3.00004-X,journal,Artificial Intelligence and Deep Learning in Pathology,sciencedirect,2021-12-31,sciencedirect,Chapter 4: Complexity in the use of artificial intelligence in anatomic pathology,https://api.elsevier.com/content/article/pii/B978032367538300004X,"
               Over and above the characterization of an image in terms of classification labels, artificial intelligence can detect specific features within that image that contribute to an understanding of its clinical behavior. The analysis of an image can be more robust and provide more information than that which can be obtained by unaided morphological interpretation by the human eye.
               However, the capability is hampered by the huge annotated datasets that are required for supervised deep learning training as currently implemented. The kinds of unsupervised experiential training that occurs in humans are not currently helpful. However, there are a variety of weakly supervised strategies that allow for substantial reduction in the amount of specific annotation necessary for training. In any training regimen, care must be taken to preserve and detect information about the relationships among patterns in the data. New approaches toward this goal will also be discussed.
            ",autonomous vehicle
10.1016/B978-0-12-817216-2.00015-6,journal,Introduction to Algorithms for Data Mining and Machine Learning,sciencedirect,2019-12-31,sciencedirect,8: Neural networks and deep learning,https://api.elsevier.com/content/article/pii/B9780128172162000156,"
               
                  This chapter introduces the key concepts of artificial neural networks and their extension to deep learning. Topics include neural networks, network architecture, optimizers, convolutionary neural network, and Boltzmann machines.
            ",autonomous vehicle
10.1016/j.cie.2020.106773,journal,Computers & Industrial Engineering,sciencedirect,2020-11-30,sciencedirect,Machine learning applications in production lines: A systematic literature review,https://api.elsevier.com/content/article/pii/S036083522030485X,"A production line is a set of sequential operations established in a factory where materials are put through a refining process to produce an end-product that is suitable for further usage. Monitoring production lines is essential to ensure that the targeted quality of the production process and the products are achieved. With the increased digitalization, lots of data can now be generated in the overall production line process. In parallel, the generated data sets are used by machine learning techniques for analytics of the production line to improve quality control, evaluate risks, and save cost. This paper aims to identify, assess, and synthesize the reported studies related to the application of machine learning in production lines, to provide a systematic overview of the current state-of-the-art and, as such, paving the way for further research. To this end, we have performed a Systematic Literature Review (SLR) in which we retrieved 271 papers, of which 39 primary studies were selected for a detailed analysis. This SLR presents and categorizes the production line problems addressed by machine learning, identifies the targeted industrial domains, discusses which machine learning algorithms have been used, and explains the adopted independent and dependent variables of the models. The study highlights the open problems that need to be solved and provides the identified research directions.",autonomous vehicle
10.1016/j.xcrp.2021.100482,journal,Cell Reports Physical Science,sciencedirect,2021-07-21,sciencedirect,The data-intensive scientific revolution occurring where two-dimensional materials meet machine learning,https://api.elsevier.com/content/article/pii/S266638642100182X,"Machine learning (ML) has experienced rapid development in recent years and been widely applied to assist studies in various research areas. Two-dimensional (2D) materials, due to their unique chemical and physical properties, have been receiving increasing attention since the isolation of graphene. The combination of ML and 2D materials science has significantly accelerated the development of new functional 2D materials, and a timely review may inspire further ML-assisted 2D materials development. In this review, we provide a horizontal and vertical summary of the recent advances at the intersection of the fields of ML and 2D materials, discussing ML-assisted 2D materials preparation (design, discovery, and synthesis of 2D materials), atomistic structure analysis (structure identification and formation mechanism), and properties prediction (electronic properties, thermodynamic properties, mechanical properties, and other properties) and revealing their connections. Finally, we highlight current research challenges and provide insight into future research opportunities.",autonomous vehicle
10.1016/j.spinee.2020.10.006,journal,The Spine Journal,sciencedirect,2021-10-31,sciencedirect,Fostering reproducibility and generalizability in machine learning for clinical prediction modeling in spine surgery,https://api.elsevier.com/content/article/pii/S1529943020311438,"
                  As the use of machine learning algorithms in the development of clinical prediction models has increased, researchers are becoming more aware of the deleterious effects that stem from the lack of reporting standards. One of the most obvious consequences is the insufficient reproducibility found in current prediction models. In an attempt to characterize methods to improve reproducibility and to allow for better clinical performance, we utilize a previously proposed taxonomy that separates reproducibility into 3 components: technical, statistical, and conceptual reproducibility. By following this framework, we discuss common errors that lead to poor reproducibility, highlight the importance of generalizability when evaluating a ML model's performance, and provide suggestions to optimize generalizability to ensure adequate performance. These efforts are a necessity before such models are applied to patient care.
               ",autonomous vehicle
10.1016/j.rbmo.2021.11.003,journal,Reproductive BioMedicine Online,sciencedirect,2021-11-12,sciencedirect,Artificial Intelligence in the embryology laboratory: A review,https://api.elsevier.com/content/article/pii/S1472648321005575,"
                  The goal of an in vitro fertilization (IVF) cycle is a healthy live-born baby. Despite the many advances in the field of assisted reproductive technologies, accurately predicting the outcome of an IVF cycle has yet to be achieved. One reason for this is the method of selecting an embryo for transfer. Morphological assessment of embryos is the traditional method of evaluating embryo quality and selecting which embryo to transfer. However, this subjective method of assessing embryos leads to inter- and intra-observer variability, resulting in less-than-optimal IVF success rates. To overcome this, it is common practice to transfer more than one embryo, potentially resulting in high-risk multiple pregnancies. Although time-lapse incubators and preimplantation genetic testing for aneuploidy have been introduced to help increase the chances of live birth, the outcomes remain less than ideal. Utilization of artificial intelligence (AI) has become increasingly popular in the medical field and is increasingly being leveraged in the embryology laboratory to help improve IVF outcomes. Many studies have been published investigating the use of AI as an unbiased, automated approach to embryo assessment. This review summarizes recent AI advancements in the embryology laboratory.
               ",autonomous vehicle
10.1016/j.matt.2020.08.023,journal,Matter,sciencedirect,2020-11-04,sciencedirect,Machine Learning for Advanced Additive Manufacturing,https://api.elsevier.com/content/article/pii/S2590238520304501,"Increasing demand for the fabrication of components with complex designs has spurred a revolution in manufacturing methods. Additive manufacturing stands out as a promising technology when it comes to prototyping multi-functional and multi-material designs. However, challenges still exist in the additive manufacturing process, such as mismatched material properties, lack of build consistency, and pervasive imperfections in the printed part. These inherent challenges can be avoided by implementing algorithms to detect imperfections and modulate printing parameters in real time. In this paper, several algorithms, with a focus on machine learning methods, are reviewed and explored to systematically tackle the three main stages of the additive manufacturing process: geometrical design, process parameter configuration, and in situ anomaly detection. Current challenges and future opportunities for algorithmically driven additive manufacturing processes, as well as potential applications to other manufacturing methods, are also discussed.",autonomous vehicle
10.1016/j.media.2020.101905,journal,Medical Image Analysis,sciencedirect,2021-02-28,sciencedirect,A review of machine learning methods for retinal blood vessel segmentation and artery/vein classification,https://api.elsevier.com/content/article/pii/S1361841520302693,"
                  The eye affords a unique opportunity to inspect a rich part of the human microvasculature non-invasively via retinal imaging. Retinal blood vessel segmentation and classification are prime steps for the diagnosis and risk assessment of microvascular and systemic diseases. A high volume of techniques based on deep learning have been published in recent years. In this context, we review 158 papers published between 2012 and 2020, focussing on methods based on machine and deep learning (DL) for automatic vessel segmentation and classification for fundus camera images. We divide the methods into various classes by task (segmentation or artery-vein classification), technique (supervised or unsupervised, deep and non-deep learning, hand-crafted methods) and more specific algorithms (e.g. multiscale, morphology). We discuss advantages and limitations, and include tables summarising results at-a-glance. Finally, we attempt to assess the quantitative merit of DL methods in terms of accuracy improvement compared to other methods. The results allow us to offer our views on the outlook for vessel segmentation and classification for fundus camera images.
               ",autonomous vehicle
10.1016/j.adengl.2021.11.007,journal,Actas Dermo-Sifiliográficas (English Edition),sciencedirect,2021-11-10,sciencedirect,Artificial Intelligence in Dermatology: A Threat or an Opportunity?,https://api.elsevier.com/content/article/pii/S1578219021003115,"The worldwide explosion of interest in artificial intelligence (AI) has created a before-and-after moment in our lives by generating great improvements in such sectors as the automotive and food production industries. AI has even been called the fourth industrial revolution. Machine learning through AI is helping to improve professional processes and promises to transform the health care sector as we know it in various ways: 1) through applications able to promote health in the general population by providing high-quality information and offering advice for different segments of the population based on prediction models; 2) by developing prediction models based on anonymized clinical data, for preventive purposes in primary care; 3) by analyzing images to provide additional decision-making support for health care providers, for improving specialist care at the secondary level; and 4) through robotics applied to processes that promote health and well-being. However, the medical profession harbors doubts about whether this revolution is a threat or an opportunity owing to a lack of understanding of AI technology and the methods used to validate its applications. This article outlines basic aspects of AI as it is applied in dermatology and reviews the main advances achieved in the last 5 years.",autonomous vehicle
10.1016/j.thorsurg.2019.03.011,journal,Thoracic Surgery Clinics,sciencedirect,2019-08-31,sciencedirect,Artificial Intelligence: Can Information be Transformed into Intelligence in Surgical Education?,https://api.elsevier.com/content/article/pii/S1547412719300222,,autonomous vehicle
10.1016/j.isci.2020.101936,journal,iScience,sciencedirect,2021-01-22,sciencedirect,Machine learning toward advanced energy storage devices and systems,https://api.elsevier.com/content/article/pii/S2589004220311330,"Technology advancement demands energy storage devices (ESD) and systems (ESS) with better performance, longer life, higher reliability, and smarter management strategy. Designing such systems involve a trade-off among a large set of parameters, whereas advanced control strategies need to rely on the instantaneous status of many indicators. Machine learning can dramatically accelerate calculations, capture complex mechanisms to improve the prediction accuracy, and make optimized decisions based on comprehensive status information. The computational efficiency makes it applicable for real-time management. This paper reviews recent progresses in this emerging area, especially new concepts, approaches, and applications of machine learning technologies for commonly used energy storage devices (including batteries, capacitors/supercapacitors, fuel cells, other ESDs) and systems (including battery ESS, hybrid ESS, grid and microgrid-containing energy storage units, pumped-storage system, thermal ESS). The perspective on future directions is also discussed.",autonomous vehicle
10.1016/B978-0-323-67538-3.00002-6,journal,Artificial Intelligence and Deep Learning in Pathology,sciencedirect,2021-12-31,sciencedirect,Chapter 2: The basics of machine learning: strategies and techniques,https://api.elsevier.com/content/article/pii/B9780323675383000026,"
               Machine learning refers to a class of computer algorithms that learn from examples rather than being explicitly programmed to perform a task. It learns to formulate a general rule from a set of concrete examples. Thus, like human learning, the computer becomes capable of improving its performance from acquired knowledge. The difference is that, at the current state of our knowledge, the computer needs many more learning examples than people do. Machine learning is the basis of artificial intelligence. It can be subdivided into shallow and deep learning, depending upon the structure and complexity of the algorithm. Several examples are given, both because of their own importance and because they help to introduce some of the concepts and principals involved in deep learning. It is important to recognize that both forms of machine learning are in common use, as there are situations in which one or the other is optimal for a given task. For deep learning, additional concepts such as multilayer connectivity, backpropagation, and convolution are described in detail, as these are the factors that must be taken into account when deploying these models.
            ",autonomous vehicle
10.1016/j.rcim.2020.101975,journal,Robotics and Computer-Integrated Manufacturing,sciencedirect,2020-10-31,sciencedirect,A self-learning and self-optimizing framework for the fault diagnosis knowledge base in a workshop,https://api.elsevier.com/content/article/pii/S0736584519305666,"
                  The knowledge base is an essential part of the fault diagnosis system, which is crucial to the performance of fault recognition. As the intelligence of the fault diagnosis system has made persistent advance, the increasing demands for diversity and dynamic update have posed challenges to the knowledge base. In this paper, a framework for the fault diagnosis knowledge base is proposed to address the challenges mentioned above. Firstly, a dynamic clustering model is designed using the proposed semi-supervised multi-spatial manifold clustering method to recognize attribute clusters and aggregate new types. When new types are added to this model, it is constantly updated to achieve the automatic evolution of the knowledge base for the diversity of fault. Then, a knowledge evolution model is established by the generative adversarial network algorithm to achieve self-learning and self-optimizing capabilities of the knowledge base. This method learns the distribution of knowledge elements and generates new knowledge elements to optimize the clustering model. Finally, a series of comparative experiments are carried out on bearing datasets to verify the validity of the mentioned framework and models. The comparison results indicate that the proposed method has better performance in fault diagnosis. This research can not only update the knowledge base, but also provide a feasible approach for designing an autonomous knowledge base with self-optimizing and self-learning capabilities.
               ",autonomous vehicle
10.1016/j.conbuildmat.2020.120109,journal,Construction and Building Materials,sciencedirect,2020-12-10,sciencedirect,Deep learning models for bridge deck evaluation using impact echo,https://api.elsevier.com/content/article/pii/S0950061820321140,"
                  Impact echo (IE) is a common nondestructive evaluation (NDE) method to detect subsurface defects in concrete bridge decks. The conventional approach for analyzing the IE data requires user expertise to define analysis parameters that could hinder broad field implementation. In this paper, the feasibility of using deep learning models (DLMs) for autonomous subsurface defect detection in bridge decks using IE has been investigated. A set of eight laboratory-made reinforced concrete bridge specimens with artificial defects were constructed at the Federal Highway Administration Advanced Sensing Technology NDE laboratory. A total number of 2016 of IE data was collected from these specimens. A one-dimensional (1D) convolutional neural network (CNN), and a 1D recurrent neural network using bidirectional long-short term memory units, were developed and applied on the IE data. In addition, two-dimensional (2D) world renowned CNNs were applied on the 2D representatives of the IE data, i.e., spectrograms. The proposed 1D CNN was the most accurate model achieving an overall accuracy of 0.88 by classifying 0.70 of the defects and 0.95 of the sound regions correctly. The proposed 1DCNN was superior to previous machine learning models that were previously used for IE classification. The results of this study showed the feasibility and the potentials of the DLMs for subsurface defect detection.
               ",autonomous vehicle
10.1016/B978-0-12-822260-7.00015-7,journal,Handbook of Computational Intelligence in Biomedical Engineering and Healthcare,sciencedirect,2021-12-31,sciencedirect,Chapter 2: Computational intelligence in healthcare and biosignal processing,https://api.elsevier.com/content/article/pii/B9780128222607000157,"
               In this new era, technological advancement toward the mission of a better tomorrow is reaching its limit because the exploration of the advanced possibilities of Artificial Intelligence is bounded with certain limitations. The application of analyzing various features of biosignal processing is key in the fields of medicine and healthcare. Biosignals such as Electroencephalogram (EEG), Electrocardiogram (ECG), Electromyography (EMG), Electrooculography (EOG), Galvanic Skin Response(GSR), and Magnetoencephalography (MEG) is already giving deep insight into the human body toward the identification of diverse nature and disorders. In recent years, the research toward analyzing biosignal gained interest among many researchers. The primary limitation for the algorithms to analyze these signals for more possibilities of insight is its uncertainty. Even though the algorithms of Artificial Intelligence have the capabilities to unravel the mysteries, it is bounded with specific difficulties. The machine learning algorithms designed to manage uncertain data but lacks accuracy due to many factors. Also, complete supervision is needed in a training process that involves the extraction and selection of adequate features for the training. The deep learning method (a subset of machine learning) comes into the picture due to one of these facts. This, indeed, as a supervised learning method, needs a massive volume of data to train to reach the accuracy goal. The deep learning algorithm plays a significant role in today's Artificial Intelligence–based applications. However, this platform needs many requirements, such as (a) high computational power like graphical processing units (GPU); (b) similar to machine learning methods, a massive labeled dataset for supervised learning; (c) adequate parameter selection to avoid overfitting or underfitting. To overcome the problems highlighted, the strategy of adopting the behaviors of unsupervised learning (performed by the clustering algorithm) in the deep learning methodology is needed. To achieve the goal, two-phase operations were processed, such as (1) transformation of the data elements into a latent feature space (Z) is processed through a nonlinear mapping of deep learning networks; (2) clustering the latent feature space to k-clusters, and simultaneously, the clustering loss is fed to the deep learning network for the next iteration of operation concerning the objective function convergence analyzed by the Kullback–Leibler divergence. Various strategies of enhancing the nature of deep learning methods and clustering methodologies for an unsupervised learning process are addressed in this chapter.
            ",autonomous vehicle
10.1016/j.jechem.2020.05.044,journal,Journal of Energy Chemistry,sciencedirect,2021-03-31,sciencedirect,Recent progress on discovery and properties prediction of energy materials: Simple machine learning meets complex quantum chemistry,https://api.elsevier.com/content/article/pii/S2095495620303806,"
                  In nature, the properties of matter are ultimately governed by the electronic structures. Quantum chemistry (QC) at electronic level matches well with a few simple physical assumptions in solving simple problems. To date, machine learning (ML) algorithm has been migrated to this field to simplify calculations and improve fidelity. This review introduces the basic information on universal electron structures of emerging energy materials and ML algorithms involved in the prediction of material properties. Then, the structure-property relationships based on ML algorithm and QC theory are reviewed. Especially, the summary of recently reported applications on classifying crystal structure, modeling electronic structure, optimizing experimental method, and predicting performance is provided. Last, an outlook on ML assisted QC calculation towards identifying emerging energy materials is also presented.
               ",autonomous vehicle
10.1016/j.pcl.2020.06.010,journal,Pediatric Clinics of North America,sciencedirect,2020-10-31,sciencedirect,The Next Frontier in Pediatric Cardiology: Artificial Intelligence,https://api.elsevier.com/content/article/pii/S0031395520300705,,autonomous vehicle
10.1016/j.egyai.2020.100014,journal,Energy and AI,sciencedirect,2020-08-31,sciencedirect,"Fundamentals, materials, and machine learning of polymer electrolyte membrane fuel cell technology",https://api.elsevier.com/content/article/pii/S2666546820300148,"Polymer electrolyte membrane (PEM) fuel cells are electrochemical devices that directly convert the chemical energy stored in fuel into electrical energy with a practical conversion efficiency as high as 65%. In the past years, significant progress has been made in PEM fuel cell commercialization. By 2019, there were over 19,000 fuel cell electric vehicles (FCEV) and 340 hydrogen refueling stations (HRF) in the U.S. (~8,000 and 44, respectively), Japan (~3,600 and 112, respectively), South Korea (~5,000 and 34, respectively), Europe (~2,500 and 140, respectively), and China (~110 and 12, respectively). Japan, South Korea, and China plan to build approximately 3,000 HRF stations by 2030. In 2019, Hyundai Nexo and Toyota Mirai accounted for approximately 63% and 32% of the total sales, with a driving range of 380 and 312 miles and a mile per gallon (MPGe) of 65 and 67, respectively. Fundamentals of PEM fuel cells play a crucial role in the technological advancement to improve fuel cell performance/durability and reduce cost. Several key aspects for fuel cell design, operational control, and material development, such as durability, electrocatalyst materials, water and thermal management, dynamic operation, and cold start, are briefly explained in this work. Machine learning and artificial intelligence (AI) have received increasing attention in material/energy development. This review also discusses their applications and potential in the development of fundamental knowledge and correlations, material selection and improvement, cell design and optimization, system control, power management, and monitoring of operation health for PEM fuel cells, along with main physics in PEM fuel cells for physics-informed machine learning. The objective of this review is three fold: (1) to present the most recent status of PEM fuel cell applications in the portable, stationary, and transportation sectors; (2) to describe the important fundamentals for the further advancement of fuel cell technology in terms of design and control optimization, cost reduction, and durability improvement; and (3) to explain machine learning, physics-informed deep learning, and AI methods and describe their significant potentials in PEM fuel cell research and development (R&D).",autonomous vehicle
10.1016/j.knosys.2021.107046,journal,Knowledge-Based Systems,sciencedirect,2021-07-19,sciencedirect,Gradient estimation of information measures in deep learning,https://api.elsevier.com/content/article/pii/S0950705121003099,"
                  Information measures including entropy and mutual information (MI) have been widely applied in deep learning. Despite the successes, exiting estimation methods suffer from either high variance or high bias. This may lead to unstable training or poor performance in deep learning. Since estimating information measures in themselves is very difficult, we explore an alternative appealing strategy, by directly estimating the gradients of information measures with respect to model parameters. We propose a general gradient estimation method for information measures based on the score estimation. In detail, we establish the Entropy Gradient Estimator (EGE) and the Mutual Information Gradient Estimator (MIGE) to estimate the gradient of entropy and mutual information with respect to model parameters, respectively. For dealing with the optimization of entropy and mutual information, we can directly plug in their gradient approximation with relevant parameters to enable stochastic backpropagation for stability and efficiency. Our proposed method exhibits higher accuracy and lower variance for gradient estimation of information measures. Extensive experiments on various deep learning tasks have demonstrated the superiority of our method.
               ",autonomous vehicle
10.1016/j.compstruct.2021.114328,journal,Composite Structures,sciencedirect,2021-10-01,sciencedirect,Machine learning and materials informatics approaches for evaluating the interfacial properties of fiber-reinforced composites,https://api.elsevier.com/content/article/pii/S026382232100790X,"
                  Fiber pullout tests have been frequently performed to determine the interfacial properties of fiber-reinforced composites. However, traditional experimental approaches and numerical investigations are restrained by being both labor-intensive and time-consuming. Hence, an accurate and effectual prediction of the interfacial properties is of paramount importance for composite design and tailoring. This work for the first time presents machine learning-assisted models to determine the interfacial properties based on previous micro-bond tests. Through a comparison between the pullout test results and prediction results, the effectiveness of the proposed model in the prediction of the interfacial shear strength and the maximum force is verified. The relationship between influencing attributes and interfacial properties can be reliably captured. It can be referred from the mean impact value analysis of the proposed models that the interfacial properties are significantly dependent on the fiber’s diameters. This work reveals that gradient boosting regressor (GBR) and artificial neural networks (ANN) exhibit adequate generalization and interpretation abilities. Besides, both ANN and GBR, with small datasets, have tremendous potential for a wide array of applications in predicting the shear resistance properties in fiber-reinforced composites.
               ",autonomous vehicle
10.1067/j.cpradiol.2020.05.006,journal,Current Problems in Diagnostic Radiology,sciencedirect,2021-04-30,sciencedirect,Artificial Intelligence-Based Clinical Decision Support Systems Using Advanced Medical Imaging and Radiomics,https://api.elsevier.com/content/article/pii/S0363018820301080,"
                  Artificial intelligence (AI) is poised to make a veritable impact in medicine. Clinical decision support (CDS) is an important area where AI can augment the clinician's capability to collect, understand and make inferences on an overwhelming volume of patient data to reach the optimal clinical decision. Advancements in medical image analysis, such as Radiomics, and data computation, such as machine learning, have expanded our understanding of disease processes and their management. In this article, we review the most relevant concepts of AI as applicable to advanced imaging-based clinical decision support systems.
               ",autonomous vehicle
10.1016/j.nanoen.2020.105380,journal,Nano Energy,sciencedirect,2020-12-31,sciencedirect,Machine learning for halide perovskite materials,https://api.elsevier.com/content/article/pii/S2211285520309575,"
                  Halide perovskite materials serve as excellent candidates for solar cell and optoelectronic devices. Recently, the design of the halide perovskite materials is greatly facilitated by machine learning techniques, which effectively identify suitable halide perovskite candidates and unveil hidden relationships by algorithms that mimic the human cognitive functions. In this manuscript, we review recent progresses on the machine learning studies of the halide perovskite materials, including the prediction and understanding of lead-free and stable halide perovskite materials. The structural descriptors to describe the property and performance of the halide perovskite materials are discussed. In addition, the design strategy of the additive species for the halide perovskite materials via the machine learning technique is provided. Suggestions to further develop the halide perovskite-based systems via the machine learning methods in the future are provided.
               ",autonomous vehicle
10.1016/j.cmi.2020.02.006,journal,Clinical Microbiology and Infection,sciencedirect,2020-10-31,sciencedirect,Machine learning in the clinical microbiology laboratory: has the time come for routine practice?,https://api.elsevier.com/content/article/pii/S1198743X20300859,"Background Machine learning (ML) allows the analysis of complex and large data sets and has the potential to improve health care. The clinical microbiology laboratory, at the interface of clinical practice and diagnostics, is of special interest for the development of ML systems. Aims This narrative review aims to explore the current use of ML In clinical microbiology. Sources References for this review were identified through searches of MEDLINE/PubMed, EMBASE, Google Scholar, biorXiv, arXiV, ACM Digital Library and IEEE Xplore Digital Library up to November 2019. Content We found 97 ML systems aiming to assist clinical microbiologists. Overall, 82 ML systems (85%) targeted bacterial infections, 11 (11%) parasitic infections, nine (9%) viral infections and three (3%) fungal infections. Forty ML systems (41%) focused on microorganism detection, identification and quantification, 36 (37%) evaluated antimicrobial susceptibility, and 21 (22%) targeted the diagnosis, disease classification and prediction of clinical outcomes. The ML systems used very diverse data sources: 21 (22%) used genomic data of microorganisms, 19 (20%) microbiota data obtained by metagenomic sequencing, 19 (20%) analysed microscopic images, 17 (18%) spectroscopy data, eight (8%) targeted gene sequencing, six (6%) volatile organic compounds, four (4%) photographs of bacterial colonies, four (4%) transcriptome data, three (3%) protein structure, and three (3%) clinical data. Most systems used data from high-income countries (n = 71, 73%) but a significant number used data from low- and middle-income countries (n = 36, 37%). Performance measures were reported for the 97 ML systems, but no article described their use in clinical practice or reported impact on processes or clinical outcomes. Implications In clinical microbiology, ML has been used with various data sources and diverse practical applications. The evaluation and implementation processes represent the main gap in existing ML systems, requiring a focus on their interpretability and potential integration into real-world settings.",autonomous vehicle
10.1016/j.matpr.2021.04.643,journal,Materials Today: Proceedings,sciencedirect,2021-12-31,sciencedirect,Optimal feature selection for machine learning based intrusion detection system by exploiting attribute dependence,https://api.elsevier.com/content/article/pii/S2214785321041523,"
                  Feature Engineering plays an important role in the development of a Machine Learning-based Classifier; especially for Intrusion Detection Systems. It helps in reducing the dimensions of the available datasets, training time, and computation costs; yet improves the performance and detection accuracy of the model. Feature Selection is the most common technique used for reducing the dimensionality of the available dataset. The higher the dimensions of the dataset; the more will be the training time required by the Machine Learning model to process (train and test) the dataset. This paper proposes two approaches for constructing an optimal feature subset, termed Dense_FR and Sparse_FR; to reduce the dimensions of the dataset, based on Kendall’s Correlation Coefficient and Mutual Information. Mutual Information is an important and common metric used for Feature Selection. It tries to reduce the amount of uncertainty by incorporating additional attributes. Kendall’s Correlation Coefficient is a stricter and consistent correlation coefficient when compared to Pearson’s Coefficient or Spearman’s Coefficient. The names Dense_FR and Sparse_FR justify the number of features generated in the optimal feature subsets; there are fewer features in the optimal subset generated by the Sparse_FR approach when compared to the Dense_FR approach. Results show that the proposed approaches improve the performance of classification.
               ",autonomous vehicle
10.1016/j.procs.2020.09.190,journal,Procedia Computer Science,sciencedirect,2020-12-31,sciencedirect,Machine Learning for Hydropower Scheduling: State of the Art and Future Research Directions,https://api.elsevier.com/content/article/pii/S1877050920320925,"This paper investigates and discusses the current and future role of machine learning (ML) within the hydropower sector. An overview of the main applications of ML in the field of hydropower operations is presented to show the most common topics that have been addressed in the scientific literature in the last years. The objective is to provide recommendations for novel research directions that can be taken in the near future to cover those areas that have not been studied so far. The key contribution of this paper lies in a critical investigation of the state of the art of ML applications in hydropower scheduling. In light of the established literature available in the last years, this study identifies and discusses new roles that can be covered by ML, coupled with cyber-physical systems (CPSs), with a particular focus on short-term hydropower scheduling (STHS) challenges.",autonomous vehicle
10.1016/j.leaqua.2020.101426,journal,The Leadership Quarterly,sciencedirect,2020-09-30,sciencedirect,Determining causal relationships in leadership research using Machine Learning: The powerful synergy of experiments and data science,https://api.elsevier.com/content/article/pii/S1048984320300539,"
                  Machine Learning (ML) techniques offer exciting new avenues for leadership research. In this paper we discuss how ML techniques can be used to inform predictive and causal models of leadership effects and clarify why both types of model are important for leadership research. We propose combining ML and experimental designs to draw causal inferences by introducing a recently developed technique to isolate “heterogeneous treatment effects.” We provide a step-by-step guide on how to design studies that combine field experiments with the application of ML to establish causal relationships with maximal predictive power. Drawing on examples in the leadership literature, we illustrate how the suggested approach can be applied to examine the impact of, for example, leadership behavior on follower outcomes. We also discuss how ML can be used to advance leadership research from theoretical, methodological and practical perspectives and consider limitations.
               ",autonomous vehicle
10.1016/j.procs.2020.09.190,journal,Procedia Computer Science,sciencedirect,2020-12-31,sciencedirect,Machine Learning for Hydropower Scheduling: State of the Art and Future Research Directions,https://api.elsevier.com/content/article/pii/S1877050920320925,"This paper investigates and discusses the current and future role of machine learning (ML) within the hydropower sector. An overview of the main applications of ML in the field of hydropower operations is presented to show the most common topics that have been addressed in the scientific literature in the last years. The objective is to provide recommendations for novel research directions that can be taken in the near future to cover those areas that have not been studied so far. The key contribution of this paper lies in a critical investigation of the state of the art of ML applications in hydropower scheduling. In light of the established literature available in the last years, this study identifies and discusses new roles that can be covered by ML, coupled with cyber-physical systems (CPSs), with a particular focus on short-term hydropower scheduling (STHS) challenges.",autonomous vehicle
10.1016/j.mlwa.2021.100134,journal,Machine Learning with Applications,sciencedirect,2021-12-15,sciencedirect,Deep learning in computer vision: A critical review of emerging techniques and application scenarios,https://api.elsevier.com/content/article/pii/S2666827021000670,"Deep learning has been overwhelmingly successful in computer vision (CV), natural language processing, and video/speech recognition. In this paper, our focus is on CV. We provide a critical review of recent achievements in terms of techniques and applications. We identify eight emerging techniques, investigate their origins and updates, and finally emphasize their applications in four key scenarios, including recognition, visual tracking, semantic segmentation, and image restoration. We recognize three development stages in the past decade and emphasize research trends for future works. The summarizations, knowledge accumulations, and creations could benefit researchers in the academia and participators in the CV industries.",autonomous vehicle
10.1016/j.compbiomed.2018.05.018,journal,Computers in Biology and Medicine,sciencedirect,2018-07-01,sciencedirect,Survey on deep learning for radiotherapy,https://api.elsevier.com/content/article/pii/S0010482518301318,"
                  More than 50% of cancer patients are treated with radiotherapy, either exclusively or in combination with other methods. The planning and delivery of radiotherapy treatment is a complex process, but can now be greatly facilitated by artificial intelligence technology. Deep learning is the fastest-growing field in artificial intelligence and has been successfully used in recent years in many domains, including medicine.
                  In this article, we first explain the concept of deep learning, addressing it in the broader context of machine learning. The most common network architectures are presented, with a more specific focus on convolutional neural networks. We then present a review of the published works on deep learning methods that can be applied to radiotherapy, which are classified into seven categories related to the patient workflow, and can provide some insights of potential future applications. We have attempted to make this paper accessible to both radiotherapy and deep learning communities, and hope that it will inspire new collaborations between these two communities to develop dedicated radiotherapy applications.
               ",autonomous vehicle
10.1016/j.robot.2012.05.019,journal,Robotics and Autonomous Systems,sciencedirect,2012-11-30,sciencedirect,Real-world reinforcement learning for autonomous humanoid robot docking,https://api.elsevier.com/content/article/pii/S0921889012000814,"
                  Reinforcement learning (RL) is a biologically supported learning paradigm, which allows an agent to learn through experience acquired by interaction with its environment. Its potential to learn complex action sequences has been proven for a variety of problems, such as navigation tasks. However, the interactive randomized exploration of the state space, common in reinforcement learning, makes it difficult to be used in real-world scenarios. In this work we describe a novel real-world reinforcement learning method. It uses a supervised reinforcement learning approach combined with Gaussian distributed state activation. We successfully tested this method in two real scenarios of humanoid robot navigation: first, backward movements for docking at a charging station and second, forward movements to prepare grasping. Our approach reduces the required learning steps by more than an order of magnitude, and it is robust and easy to be integrated into conventional RL techniques.
               ",autonomous vehicle
10.1016/j.rec.2019.05.014,journal,Revista Española de Cardiología (English Edition),sciencedirect,2019-12-31,sciencedirect,Applications of Artificial Intelligence in Cardiology. The Future is Already Here,https://api.elsevier.com/content/article/pii/S1885585719302609,"
                  There is currently no other hot topic like the ability of current technology to develop capabilities similar to those of human beings, even in medicine. This ability to simulate the processes of human intelligence with computer systems is known as artificial intelligence (AI). This article aims to clarify the various terms that still sound foreign to us, such as AI, machine learning (ML), deep learning (DL), and big data. It also provides an in-depth description of the concept of AI and its types; the learning techniques and technology used by ML; cardiac imaging analysis with DL; and the contribution of this technological revolution to classical statistics, as well as its current limitations, legal aspects, and initial applications in cardiology. To do this, we conducted a detailed PubMed search on the evolution of original contributions on AI to the various areas of application in cardiology in the last 5 years and identified 673 research articles. We provide 19 detailed examples from distinct areas of cardiology that, by using AI, have shown diagnostic and therapeutic improvements, and which will aid understanding of ML and DL methodology.
               ",autonomous vehicle
10.1016/j.matre.2021.100047,journal,Materials Reports: Energy,sciencedirect,2021-08-31,sciencedirect,Computational discovery of energy materials in the era of big data and machine learning: A critical review,https://api.elsevier.com/content/article/pii/S2666935821000823,"The discovery of novel materials with desired properties is essential to the advancements of energy-related technologies. Despite the rapid development of computational infrastructures and theoretical approaches, progress so far has been limited by the empirical and serial nature of experimental work. Fortunately, the situation is changing thanks to the maturation of theoretical tools such as density functional theory, high-throughput screening, crystal structure prediction, and emerging approaches based on machine learning. Together these recent innovations in computational chemistry, data informatics, and machine learning have acted as catalysts for revolutionizing material design and hopefully will lead to faster kinetics in the development of energy-related industries. In this report, recent advances in material discovery methods are reviewed for energy devices. Three paradigms based on empiricism-driven experiments, database-driven high-throughput screening, and data informatics-driven machine learning are discussed critically. Key methodological advancements involved are reviewed including high-throughput screening, crystal structure prediction, and generative models for target material design. Their applications in energy-related devices such as batteries, catalysts, and photovoltaics are selectively showcased.",autonomous vehicle
10.1016/j.enbuild.2020.109831,journal,Energy and Buildings,sciencedirect,2020-04-01,sciencedirect,State-of-the-art on research and applications of machine learning in the building life cycle,https://api.elsevier.com/content/article/pii/S0378778819337879,"
                  Fueled by big data, powerful and affordable computing resources, and advanced algorithms, machine learning has been explored and applied to buildings research for the past decades and has demonstrated its potential to enhance building performance. This study systematically surveyed how machine learning has been applied at different stages of building life cycle. By conducting a literature search on the Web of Knowledge platform, we found 9579 papers in this field and selected 153 papers for an in-depth review. The number of published papers is increasing year by year, with a focus on building design, operation, and control. However, no study was found using machine learning in building commissioning. There are successful pilot studies on fault detection and diagnosis of HVAC equipment and systems, load prediction, energy baseline estimate, load shape clustering, occupancy prediction, and learning occupant behaviors and energy use patterns. None of the existing studies were adopted broadly by the building industry, due to common challenges including (1) lack of large scale labeled data to train and validate the model, (2) lack of model transferability, which limits a model trained with one data-rich building to be used in another building with limited data, (3) lack of strong justification of costs and benefits of deploying machine learning, and (4) the performance might not be reliable and robust for the stated goals, as the method might work for some buildings but could not be generalized to others. Findings from the study can inform future machine learning research to improve occupant comfort, energy efficiency, demand flexibility, and resilience of buildings, as well as to inspire young researchers in the field to explore multidisciplinary approaches that integrate building science, computing science, data science, and social science.
               ",autonomous vehicle
10.1016/j.procs.2021.05.037,journal,Procedia Computer Science,sciencedirect,2021-12-31,sciencedirect,Deep Learning and Remote Sensing: Detection of Dumping Waste Using UAV,https://api.elsevier.com/content/article/pii/S1877050921011224,"An important success and use of Deep Learning in recent years has been in the field of image processing. Research on Deep Learning has shown that these architectures particularly convolution neuron network (CNN) can learn solutions with human-level capability for certain visual tasks. These techniques have been used in particular in remote sensing image analysis tasks, including object detection on images, image fusion, image recording, scene classification, segmentation, object-based image analysis, land use and land cover classification (LULC). In this paper we present an automatic solution for the detection of clandestine waste dumps using unmanned aerial vehicle (UAV) images in the Saint Louis area of Senegal, West Africa. This is a challenging task given the very high spatial resolution of UAV images (on the order of a few centimeters) and the extremely high level of detail, which require suitable automatic analysis methods. Our proposed method begins by 1) segmenting image into four (4) regions, which can be used as an input image 2) Reduce size of input images into 300x300x3 for the CNN entries 3) Labelling the image by determining region of interest. Next Single shot detector SSD is used to mine highly descriptive features from these datasets. The results show that the model recognizes well the areas concerned but presents difficulties on some areas lacking clear ground truths.",autonomous vehicle
10.1016/j.tgie.2019.150642,journal,Techniques and Innovations in Gastrointestinal Endoscopy,sciencedirect,2020-04-30,sciencedirect,A primer of artificial intelligence in medicine,https://api.elsevier.com/content/article/pii/S1096288319300816,"
                  As AI becomes increasingly ubiquitous in our world, it is set to transform every aspect of how we do medical care, research and education. Physicians as a profession need to be active leaders and participants in this technology-driven transformation in order to ensure that the potential to dramatically improve health care is fulfilled. This article is focused on enabling that active participation by helping physicians gain understanding of the core concepts, issues, and trends related to AI (using the common board use of the term which includes Machine Learning, Deep Learning, Augmented Intelligence, and Artificial General Intelligence).
               ",autonomous vehicle
10.1016/j.dsx.2020.06.068,journal,Diabetes & Metabolic Syndrome: Clinical Research & Reviews,sciencedirect,2020-10-31,sciencedirect,Application of Artificial Intelligence in COVID-19 drug repurposing,https://api.elsevier.com/content/article/pii/S187140212030237X,"
                  Background and aim
                  COVID-19 outbreak has created havoc and a quick cure for the disease will be a therapeutic medicine that has usage history in patients to resolve the current pandemic. With technological advancements in Artificial Intelligence (AI) coupled with increased computational power, the AI-empowered drug repurposing can prove beneficial in the COVID-19 scenario.
               
                  Methods
                  The recent literature is studied and analyzed from various sources such as Scopus, Google Scholar, PubMed, and IEEE Xplore databases. The search terms used are ‘COVID-19′, ’ AI ′, and ‘Drug Repurposing’.
               
                  Results
                  AI is implemented in the field design through the generation of the learning-prediction model and performs a quick virtual screening to accurately display the output. With a drug-repositioning strategy, AI can quickly detect drugs that can fight against emerging diseases such as COVID-19. This technology has the potential to improve the drug discovery, planning, treatment, and reported outcomes of the COVID-19 patient, being an evidence-based medical tool.
               
                  Conclusions
                  Thus, there are chances that the application of the AI approach in drug discovery is feasible. With prior usage experiences in patients, few of the old drugs, if shown active against SARS-CoV-2, can be readily applied to treat the COVID-19 patients. With the collaboration of AI with pharmacology, the efficiency of drug repurposing can improve significantly.
               ",autonomous vehicle
10.1016/j.jterra.2021.04.005,journal,Journal of Terramechanics,sciencedirect,2021-10-31,sciencedirect,Machine learning in planetary rovers: A survey of learning versus classical estimation methods in terramechanics for in situ exploration,https://api.elsevier.com/content/article/pii/S0022489821000380,"
                  For the design of space missions in the Moon and planets, analysis of mobility in robots is crucial and poor planning has led to abortion of missions in the past. To mitigate the risk of mission failure, improved algorithms relying intrinsically on fusing visual odometry with other sensory inputs are developed for slip detection and navigation. However, these approaches are significantly expensive computationally and difficult to meet for future space exploration robots. Hence, today the central question in the field is how to develop a novel framework for in situ estimation of rover mobility with available space hardware and low-computational demanding terramechanics predictors. Ranging from pure simulations up to experimentally validated studies, this paper surveys dozens of existing methodologies for detection of vehicle motion performance (wheel forces and torques), surface hazards (slip-sinkage) and other parameters (soil strenght constants) using classical terramechanics maps, and compare them with novel approaches introduced by machine learning, allowing to establish future directions of research towards distributed exteroceptive and proprioceptive sensing for visionless exploration in dynamic environments. To avoid making it challenging to collect all relevant studies expeditiously, we propose a global classification of terramechanics according most common practices in the field, allowing to form an structured framework that condense most works in the domain within three estimator categories (direct/forward or inverse terramechanics, and slip estimators). Likewise, from the experiences collected in previous MER (Mars Exploration Rover) missions, five overlooked problems are documented that will need to be addressed in next generation of planetary vehicles, along three research questions and few hypothesis that will pave the road towards future applications of machine learning-based terramechanics.
               ",autonomous vehicle
10.1016/j.measurement.2020.108202,journal,Measurement,sciencedirect,2020-12-15,sciencedirect,Few-shot transfer learning for intelligent fault diagnosis of machine,https://api.elsevier.com/content/article/pii/S0263224120307405,"
                  Rotating machinery intelligent diagnosis with large data has been researched comprehensively, while there is still a gap between the existing diagnostic model and the practical application, due to the variability of working conditions and the scarcity of fault samples. To address this problem, few-shot transfer learning method is constructed utilizing meta-learning for few-shot samples diagnosis in variable conditions in this paper. We consider two transfer situations of rotating machinery intelligent diagnosis named conditions transfer and artificial-to-natural transfer, and construct seven few-shot transfer learning methods based on a unified 1D convolution network for few-shot diagnosis of three datasets. Baseline accuracy under different sample capacity and transfer situations are provided for comprehensive comparison and guidelines. What is more, data dependency, transferability, and task plasticity of various methods in the few-shot scenario are discussed in detail, the data analysis result shows meta-learning holds the advantage for machine fault diagnosis with extremely few-shot instances on the relatively simple transfer task. Our code is available at https://github.com/a1018680161/Few-shot-Transfer-Learning.
               ",autonomous vehicle
10.1016/j.patcog.2021.107936,journal,Pattern Recognition,sciencedirect,2021-08-31,sciencedirect,A survey on heterogeneous network representation learning,https://api.elsevier.com/content/article/pii/S0031320321001230,"
                  Heterogeneous information networks usually contain different kinds of nodes and distinguishing types of relations, which can preserve more information than homogeneous information networks. Heterogeneous network representation learning attempts to learn a low-dimensional representation for each node and capture rich semantic information of the given network. Most of existing surveys focus on heterogeneous information network analysis and homogeneous information network representation learning. Although considerable research efforts concentrate on heterogeneous network representation learning, there are few surveys that systematically review the state-of-the-art heterogeneous network representation learning techniques. Motivated by this, we propose a taxonomy of heterogeneous network representation learning algorithms according to different approaches of capturing semantic information in heterogeneous networks, including path based algorithms and semantic unit based algorithms. Moreover, we introduce the typical heterogeneous network representation learning techniques in detail and make a comparative analysis of these techniques. In addition, the research challenges in terms of semantics preserving, data sparsity and scalability are discussed. To tackle these challenges, several potential future research directions for heterogeneous network representation learning are pointed out, including semantic relations extraction, dynamic heterogeneous networks, very large heterogeneous networks and heterogeneous networks construction.
               ",autonomous vehicle
10.1016/j.compbiomed.2021.104665,journal,Computers in Biology and Medicine,sciencedirect,2021-09-30,sciencedirect,Artificial intelligence-driven assessment of radiological images for COVID-19,https://api.elsevier.com/content/article/pii/S0010482521004595,"
                  Artificial Intelligence (AI) methods have significant potential for diagnosis and prognosis of COVID-19 infections. Rapid identification of COVID-19 and its severity in individual patients is expected to enable better control of the disease individually and at-large. There has been remarkable interest by the scientific community in using imaging biomarkers to improve detection and management of COVID-19. Exploratory tools such as AI-based models may help explain the complex biological mechanisms and provide better understanding of the underlying pathophysiological processes. The present review focuses on AI-based COVID-19 studies as applies to chest x-ray (CXR) and computed tomography (CT) imaging modalities, and the associated challenges. Explicit radiomics, deep learning methods, and hybrid methods that combine both deep learning and explicit radiomics have the potential to enhance the ability and usefulness of radiological images to assist clinicians in the current COVID-19 pandemic. The aims of this review are: first, to outline COVID-19 AI-analysis workflows, including acquisition of data, feature selection, segmentation methods, feature extraction, and multi-variate model development and validation as appropriate for AI-based COVID-19 studies. Secondly, existing limitations of AI-based COVID-19 analyses are discussed, highlighting potential improvements that can be made. Finally, the impact of AI and radiomics methods and the associated clinical outcomes are summarized. In this review, pipelines that include the key steps for AI-based COVID-19 signatures identification are elaborated. Sample size, non-standard imaging protocols, segmentation, availability of public COVID-19 databases, combination of imaging and clinical information and full clinical validation remain major limitations and challenges. We conclude that AI-based assessment of CXR and CT images has significant potential as a viable pathway for the diagnosis, follow-up and prognosis of COVID-19.
               ",autonomous vehicle
10.1016/j.neucom.2006.06.010,journal,Neurocomputing,sciencedirect,2007-10-31,sciencedirect,A new hybrid evolutionary mechanism based on unsupervised learning for Connectionist Systems,https://api.elsevier.com/content/article/pii/S0925231207001555,"
                  Recent studies have confirmed that the modulation of synaptic efficacy affects emergent behaviour of brain cells assemblies. We report the first results of adding up the behaviour of particular brain circuits to Artificial Neural Networks. A new hybrid learning method has emerged. In order to find the best solution to a given problem, this method combines the use of Genetic Algorithms with particular changes to connection weights based on this behaviour. We show this combination in feed-forward multilayer architectures initially created to solve classification problems and we illustrate the benefits obtained with this new method.
               ",autonomous vehicle
10.1016/j.amjmed.2019.01.017,journal,The American Journal of Medicine,sciencedirect,2019-07-31,sciencedirect,Artificial Intelligence Transforms the Future of Health Care,https://api.elsevier.com/content/article/pii/S0002934319301202,"
                  Life sciences researchers using artificial intelligence (AI) are under pressure to innovate faster than ever. Large, multilevel, and integrated data sets offer the promise of unlocking novel insights and accelerating breakthroughs. Although more data are available than ever, only a fraction is being curated, integrated, understood, and analyzed. AI focuses on how computers learn from data and mimic human thought processes. AI increases learning capacity and provides decision support system at scales that are transforming the future of health care. This article is a review of applications for machine learning in health care with a focus on clinical, translational, and public health applications with an overview of the important role of privacy, data sharing, and genetic information.
               ",autonomous vehicle
10.1016/B978-0-12-809556-0.00004-6,journal,Leveraging Biomedical and Healthcare Data,sciencedirect,2019-12-31,sciencedirect,"Chapter 4: Big Data, Artificial Intelligence, and Machine Learning in Neurotrauma",https://api.elsevier.com/content/article/pii/B9780128095560000046,"
               Rapid advances in the collection, storage, and analysis of large volumes of data—Big Data—offer the much-needed help to identify and treat the various pathological conditions triggered by traumatic brain injury (TBI). Big Data (BD) is defined as extremely large, complex, and mostly unstructured data that cannot be analyzed using traditional approaches. BD can be only analyzed by using text mining (TM), artificial intelligence (AI), or machine learning (ML). These approaches can reveal patterns, trends, and associations, critical for understanding the “most complex disease of the most complex organ.” While powerful and successfully tested computational tools are available, using BD approaches in TBI is currently hampered by the limited availability of legacy and/or primary data, by incompatible data formats and standards. This chapter introduces Big Data and Big Data approaches such as text mining, artificial intelligence, and machine learning; outlines the benefits of using BD approaches; and suggests potential solutions that can help using the full potential of BD in TBI. It also identifies necessary changes of how researchers can help ushering in a new era of preclinical and clinical TBI research by recording and storing ALL the data generated and making ALL the data available for BD approaches—text mining, artificial intelligence, and machine learning so new correlations, relationships, and trends can be identified. In turn, these new information will help to develop novel diagnostics, evidence-based treatments, and improve outcomes.
            ",autonomous vehicle
10.1016/j.asoc.2020.106612,journal,Applied Soft Computing,sciencedirect,2020-11-30,sciencedirect,Application of artificial intelligence methods in vital signs analysis of hospitalized patients: A systematic literature review,https://api.elsevier.com/content/article/pii/S1568494620305500,"
                  In a hospital environment, patients are monitored continuously by electronic devices and health professionals. Therefore, a large amount of data is collected and stored in electronic health records systems for each patient. Among such data, vital signs are one of the most common and relevant types of information monitored to assess a patient’s health status. Artificial intelligence techniques can be used to analyze and learn useful standards from clinical datasets to provide better evidence to support the decisions of health professionals and thus help to improve patient health outcomes in hospitals. This systematic literature review aims to provide an updated computational perspective of how artificial intelligence has been applied to analyze the vital signs of adult hospitalized patients and the outcomes obtained. To this end, we reviewed 2899 scientific articles published between 2008 and 2018 and selected 78 articles that met our inclusion criteria to answer the research questions. Moreover, we used the information found in the reviewed articles to propose a taxonomy and identified the main concerns, challenges, and opportunities in this field. Our findings demonstrate that many researchers are exploring the use of artificial intelligence methods in tasks related to improving the health outcomes of hospitalized patients in distinct units. Additionally, although vital signs are significant predictors of clinical deterioration, they are not analyzed in isolation to predict or identify a clinical outcome. Our taxonomy and discussion contribute to the achievement of a significant degree of coverage regarding the aspects related to using machine learning to improve health outcomes in hospital environments, while highlighting gaps in the literature for future research.
               ",autonomous vehicle
10.1016/j.procir.2020.04.039,journal,Procedia CIRP,sciencedirect,2020-12-31,sciencedirect,Automated machine learning for predictive quality in production,https://api.elsevier.com/content/article/pii/S2212827120306016,"Applications that leverage the benefits of applying machine learning (ML) in production have been successfully realized. A fundamental hurdle to scale ML-based projects is the necessity of expertise from manufacturing and data science. One possible solution lies in automating the ML pipeline: integration, preparation, modeling and model deployment. This paper shows the possibilities and limits of applying AutoML in production, including a benchmarking of available systems. Furthermore, AutoML is compared to manual implementation in a predictive quality use case: AutoML still requires programming knowledge and is outperformed by manual implementation - but sufficient results are available in a shorter timespan.",autonomous vehicle
10.1016/j.dcan.2021.10.007,journal,Digital Communications and Networks,sciencedirect,2021-10-28,sciencedirect,Machine learning in vehicular networking: an overview,https://api.elsevier.com/content/article/pii/S2352864821000870,"As vehicle complexity and road congestion increase, combined with the emergence of electric vehicles, the need for intelligent transportation systems to improve on-road safety and transportation efficiency using vehicular networks has become essential. The evolution of high mobility wireless networks will provide improved support for connected vehicles through highly dynamic heterogeneous networks. Particularly, 5G deployment introduces new features and technologies that enable operators to capitalize on emerging infrastructure capabilities. Machine Learning (ML), a powerful methodology for adaptive and predictive system development, has emerged in both vehicular and conventional wireless networks. Adopting data-centric methods enables ML to address highly dynamic vehicular network issues faced by conventional solutions, such as traditional control loop design and optimization techniques. This article provides a short survey of ML applications in vehicular networks from the networking aspect. Research topics covered in this article include network control containing handover management and routing decision making, resource management, and energy efficiency in vehicular networks. The findings of this paper suggest more attention should be paid to network forming/deforming decision making. ML applications in vehicular networks should focus on researching multi-agent cooperated oriented methods and overall complexity reduction while utilizing enabling technologies, such as mobile edge computing for real-world deployment. Research datasets, simulation environment standardization, and method interpretability also require more research attention.",autonomous vehicle
10.1016/j.crphar.2021.100042,journal,Current Research in Pharmacology and Drug Discovery,sciencedirect,2021-12-31,sciencedirect,Artificial intelligence-driven drug repurposing and structural biology for SARS-CoV-2,https://api.elsevier.com/content/article/pii/S2590257121000298,"It has been said that COVID-19 is a generational challenge in many ways. But, at the same time, it becomes a catalyst for collective action, innovation, and discovery. Realizing the full potential of artificial intelligence (AI) for structure determination of unknown proteins and drug discovery are some of these innovations. Potential applications of AI include predicting the structure of the infectious proteins, identifying drugs that may be effective in targeting these proteins, and proposing new chemical compounds for further testing as potential drugs. AI and machine learning (ML) allow for rapid drug development including repurposing existing drugs. Algorithms were used to search for novel or approved antiviral drugs capable of inhibiting SARS-CoV-2. This paper presents a survey of AI and ML methods being used in various biochemistry of SARS-CoV-2, from structure to drug development, in the fight against the deadly COVID-19 pandemic. It is envisioned that this study will provide AI/ML researchers and the wider community an overview of the current status of AI applications particularly in structural biology, drug repurposing, and development, and motivate researchers in harnessing AI potentials in the fight against COVID-19.",autonomous vehicle
10.1016/j.jmsy.2020.08.009,journal,Journal of Manufacturing Systems,sciencedirect,2021-01-31,sciencedirect,Artificial intelligence and internet of things in small and medium-sized enterprises: A survey,https://api.elsevier.com/content/article/pii/S0278612520301424,"
                  Internet of things (IoT) and artificial intelligence (AI) are popular topics of Industry 4.0. Many publications regarding these topics have been published, but they are primarily focused on larger enterprises. However, small and medium-sized enterprises (SMEs) are considered the economic backbone of many countries, which is why it is increasingly important that these kinds of companies also have easy access to these technologies and can make them operational. This paper presents a comprehensive survey and investigation of how widespread AI and IoT are among manufacturing SMEs, and discusses the current limitations and opportunities towards enabling predictive analytics. Firstly, an overview of the enablers for AI and IoT is provided along with the four analytics capabilities. Hereafter a comprehensive literature review is conducted and its findings showcased. Finally, emerging topics of research and development, making AI and IoT accessible technologies to SMEs, and the associated future trends and challenges are summarised.
               ",autonomous vehicle
10.1016/B978-0-12-821986-7.00023-8,journal,Nature-Inspired Optimization Algorithms,sciencedirect,2021-12-31,sciencedirect,Chapter 16: Data Mining and Deep Learning,https://api.elsevier.com/content/article/pii/B9780128219867000238,"
               
                  Both data mining and machine learning are becoming popular with many different applications. This chapter introduces some techniques in data mining and machine learning, including clustering, support vector machine, and neural networks. We also discuss their links to nature-inspired algorithms for optimization.
            ",autonomous vehicle
10.1016/j.physrep.2019.03.001,journal,Physics Reports,sciencedirect,2019-05-30,sciencedirect,"A high-bias, low-variance introduction to Machine Learning for physicists",https://api.elsevier.com/content/article/pii/S0370157319300766,"Machine Learning (ML) is one of the most exciting and dynamic areas of modern research and application. The purpose of this review is to provide an introduction to the core concepts and tools of machine learning in a manner easily understood and intuitive to physicists. The review begins by covering fundamental concepts in ML and modern statistics such as the bias–variance tradeoff, overfitting, regularization, generalization, and gradient descent before moving on to more advanced topics in both supervised and unsupervised learning. Topics covered in the review include ensemble models, deep learning and neural networks, clustering and data visualization, energy-based models (including MaxEnt models and Restricted Boltzmann Machines), and variational methods. Throughout, we emphasize the many natural connections between ML and statistical physics. A notable aspect of the review is the use of Python Jupyter notebooks to introduce modern ML/statistical packages to readers using physics-inspired datasets (the Ising Model and Monte-Carlo simulations of supersymmetric decays of proton–proton collisions). We conclude with an extended outlook discussing possible uses of machine learning for furthering our understanding of the physical world as well as open problems in ML where physicists may be able to contribute.",autonomous vehicle
10.1016/j.neunet.2018.02.010,journal,Neural Networks,sciencedirect,2018-11-30,sciencedirect,An adaptive deep Q-learning strategy for handwritten digit recognition,https://api.elsevier.com/content/article/pii/S0893608018300492,"
                  Handwritten digits recognition is a challenging problem in recent years. Although many deep learning-based classification algorithms are studied for handwritten digits recognition, the recognition accuracy and running time still need to be further improved. In this paper, an adaptive deep Q-learning strategy is proposed to improve accuracy and shorten running time for handwritten digit recognition. The adaptive deep Q-learning strategy combines the feature-extracting capability of deep learning and the decision-making of reinforcement learning to form an adaptive Q-learning deep belief network (Q-ADBN). First, Q-ADBN extracts the features of original images using an adaptive deep auto-encoder (ADAE), and the extracted features are considered as the current states of Q-learning algorithm. Second, Q-ADBN receives Q-function (reward signal) during recognition of the current states, and the final handwritten digits recognition is implemented by maximizing the Q-function using Q-learning algorithm. Finally, experimental results from the well-known MNIST dataset show that the proposed Q-ADBN has a superiority to other similar methods in terms of accuracy and running time.
               ",autonomous vehicle
10.1016/j.knosys.2020.106445,journal,Knowledge-Based Systems,sciencedirect,2020-12-27,sciencedirect,A descriptive framework for the field of deep learning applications in medical images,https://api.elsevier.com/content/article/pii/S0950705120305748,"
                  Deep learning in medical image analysis is a typical interdisciplinary application, which needs support and cooperation of computer techniques and medical experience, and has broad application prospects. Since 2017, the number of related published articles has increased exponentially, imposing a burden on the literature review in this field. In this survey, we clustered 2068 retrieved articles into 15 topics through Latent Dirichlet Allocation (LDA) and provided a rough overview on the application of deep learning in medical images. On this basis, we conducted a detailed review with 77 top representative articles. We built a descriptive review framework based on LDA and discussed classification, object detection, segmentation, and image generation applications in medical images for the field of deep learning from the perspective of image modalities. We ended with discussing current challenges and future research directions of deep learning in medical images analysis.
               ",autonomous vehicle
10.1016/j.neunet.2009.03.013,journal,Neural Networks,sciencedirect,2009-04-30,sciencedirect,Coordinated machine learning and decision support for situation awareness,https://api.elsevier.com/content/article/pii/S0893608009000367,"
                  Domains such as force protection require an effective decision maker to maintain a high level of situation awareness. A system that combines humans with neural networks is a desirable approach. Furthermore, it is advantageous for the calculation engine to operate in three learning modes: supervised for initial training and known updating, reinforcement for online operational improvement, and unsupervised in the absence of all external signaling. An Adaptive Resonance Theory based architecture capable of seamlessly switching among the three types of learning is discussed that can be used to help optimize the decision making of a human operator in such a scenario. This is followed by a situation assessment module.
               ",autonomous vehicle
10.1016/j.eswa.2020.114161,journal,Expert Systems with Applications,sciencedirect,2021-04-01,sciencedirect,Deep and machine learning techniques for medical imaging-based breast cancer: A comprehensive review,https://api.elsevier.com/content/article/pii/S0957417420309015,"
                  Breast cancer is the second leading cause of death for women, so accurate early detection can help decrease breast cancer mortality rates. Computer-aided detection allows radiologists to detect abnormalities efficiently. Medical images are sources of information relevant to the detection and diagnosis of various diseases and abnormalities. Several modalities allow radiologists to study the internal structure, and these modalities have been met with great interest in several types of research. In some medical fields, each of these modalities is of considerable significance. This study aims at presenting a review that shows the new applications of machine learning and deep learning technology for detecting and classifying breast cancer and provides an overview of progress in this area. This review reflects on the classification of breast cancer utilizing multi-modalities medical imaging. Details are also given on techniques developed to facilitate the classification of tumors, non-tumors, and dense masses in various medical imaging modalities. It first provides an overview of the different approaches to machine learning, then an overview of the different deep learning techniques and specific architectures for the detection and classification of breast cancer. We also provide a brief overview of the different image modalities to give a complete overview of the area. In the same context, this review was performed using a broad variety of research databases as a source of information for access to various field publications. Finally, this review summarizes the future trends and challenges in the classification and detection of breast cancer.
               ",autonomous vehicle
10.1016/j.compchemeng.2017.10.008,journal,Computers & Chemical Engineering,sciencedirect,2018-06-09,sciencedirect,Machine learning: Overview of the recent progresses and implications for the process systems engineering field,https://api.elsevier.com/content/article/pii/S0098135417303538,"
                  Machine learning (ML) has recently gained in popularity, spurred by well-publicized advances like deep learning and widespread commercial interest in big data analytics. Despite the enthusiasm, some renowned experts of the field have expressed skepticism, which is justifiable given the disappointment with the previous wave of neural networks and other AI techniques. On the other hand, new fundamental advances like the ability to train neural networks with a large number of layers for hierarchical feature learning may present significant new technological and commercial opportunities. This paper critically examines the main advances in deep learning. In addition, connections with another ML branch of reinforcement learning are elucidated and its role in control and decision problems is discussed. Implications of these advances for the fields of process and energy systems engineering are also discussed.
               ",autonomous vehicle
10.1016/j.future.2019.09.060,journal,Future Generation Computer Systems,sciencedirect,2020-09-30,sciencedirect,Saving time and cost on the scheduling of fog-based IoT applications using deep reinforcement learning approach,https://api.elsevier.com/content/article/pii/S0167739X19308702,"
                  Due to the rapid growth of intelligent devices and the Internet of Things (IoT) applications in recent years, the volume of data that is generated by these devices is increasing ceaselessly. Hence, moving all of these data to cloud datacenters would be impossible and would lead to more bandwidth usage, latency, cost, and energy consumption. In such cases, the fog layer would be the best place for data processing. In the fog layer, the computing equipment dedicates parts of its limited resources to process the IoT application tasks. Therefore, efficient utilization of computing resources is of great importance and requires an optimal and intelligent strategy for task scheduling. In this paper, we have focused on the task scheduling of fog-based IoT applications with the aim of minimizing long-term service delay and computation cost under the resource and deadline constraints. To address this problem, we have used the reinforcement learning approach and have proposed a Double Deep Q-Learning (DDQL)-based scheduling algorithm using the target network and experience replay techniques. The evaluation results reveal that our proposed algorithm outperforms some baseline algorithms in terms of service delay, computation cost, energy consumption and task accomplishment and also handles the Single Point of Failure (SPoF) and load balancing challenges.
               ",autonomous vehicle
10.1016/j.apenergy.2019.114416,journal,Applied Energy,sciencedirect,2020-03-15,sciencedirect,Machine-learning based hybrid demand-side controller for high-rise office buildings with high energy flexibilities,https://api.elsevier.com/content/article/pii/S0306261919321038,"
                  The accurate demand prediction with high efficiency and advanced demand-side controller are essential for the enhancement of energy flexibility provided by buildings, whereas the current literature fails to present the mechanism on modelling development and demand-side control. This paper aims to deal with the complexity of building demand prediction with supervised machine learning method, including the multiple linear regression, the support vector regression and the backpropagation neural network. The regularization, adding the sum of the weights to the learning function, is utilized to improve the training speed and to solve the overfitting by eliminating the unnecessary connections with small weights. The configuration of the artificial neural network was presented, and sensitivity analysis has been conducted on the learning performance regarding different training times. Energy flexibilities of sophisticated building energy systems (including renewable system, electric and thermal demands and building services systems) were quantitatively characterised with a series of quantifiable indicators. Moreover, several advanced controllers have been developed and contrasted, in regard to the flexibility utilisation of building energy systems. Results showed that, the developed hybrid controller with short-term prediction through the cross-entropy function is more technically competitive than other controllers. With the implementation of the developed hybrid controller, the peak power of the grid importation can be reduced from 500.3 to 195 kW by 61%. This study formulates a data-driven model with an advanced machine learning algorithm for the accurate building demand prediction and a hybrid advanced controller with short-term prediction for the energy management, which are critical for the promotion of energy flexible buildings.
               ",autonomous vehicle
10.1016/j.biotechadv.2020.107631,journal,Biotechnology Advances,sciencedirect,2020-11-15,sciencedirect,"Microalgae with artificial intelligence: A digitalized perspective on genetics, systems and products",https://api.elsevier.com/content/article/pii/S0734975020301336,"
                  With recent advances in novel gene-editing tools such as RNAi, ZFNs, TALENs, and CRISPR-Cas9, the possibility of altering microalgae toward designed properties for various application is becoming a reality. Alteration of microalgae genomes can modify metabolic pathways to give elevated yields in lipids, biomass, and other components. The potential of such genetically optimized microalgae can give a “domino effect” in further providing optimization leverages down the supply chain, in aspects such as cultivation, processing, system design, process integration, and revolutionary products. However, the current level of understanding the functional information of various microalgae gene sequences is still primitive and insufficient as microalgae genome sequences are long and complex. From this perspective, this work proposes to link up this knowledge gap between microalgae genetic information and optimized bioproducts using Artificial Intelligence (AI). With the recent acceleration of AI research, large and complex data from microalgae research can be properly analyzed by combining the cutting-edge of both fields. In this work, the most suitable class of AI algorithms (such as active learning, semi-supervised learning, and meta-learning) are discussed for different cases of microalgae applications. This work concisely reviews the current state of the research milestones and highlight some of the state-of-art that has been carried out, providing insightful future pathways. The utilization of AI algorithms in microalgae cultivation, system optimization, and other aspects of the supply chain is also discussed. This work opens the pathway to a digitalized future for microalgae research and applications.
               ",autonomous vehicle
10.1016/j.joule.2021.07.012,journal,Joule,sciencedirect,2021-09-15,sciencedirect,The role of artificial intelligence in the mass adoption of electric vehicles,https://api.elsevier.com/content/article/pii/S2542435121003500,"
                  The electrification of mass transportation is hailed as a solution for reducing global greenhouse-gas emissions and dependence on unsustainable energy sources. The annual sales of electric vehicles (EVs) has continued to rise since 2011, with a global sale of EVs of 2.1 million in 2019. This increase in sales is mainly due to continued improvement in the cost and performance of commercial EVs, increased EV options available to consumers, and environmental awareness. However, despite the positive outlook, EVs still face major
                  challenges that hinder their rapid and widespread adoption: limited driving range, long charging times, and a lack of sufficient charging infrastructure. This review outlines the recent advances in EVs and related infrastructure, mainly from artificial intelligence (AI), which makes EVs a more attractive consumer option. The application of AI in improving EVs, facilitating EV charging stations, and EV integration with the smart grid is critically analyzed and reviewed. Finally, future trends and prospects in the area are discussed.
               ",autonomous vehicle
10.1016/j.ast.2020.106100,journal,Aerospace Science and Technology,sciencedirect,2020-11-30,sciencedirect,Horizontal trajectory control of stratospheric airships in wind field using Q-learning algorithm,https://api.elsevier.com/content/article/pii/S1270963820307823,"
                  This paper proposes an adaptive horizontal trajectory control method for stratospheric airships in uncertain wind field using Q-learning algorithm. Firstly, horizontal trajectory control of the airships is decomposed into the target tracking, and the observation model of airships is constructed. Then, the Markov decision process (MDP) model of airships is established, in which the action strategy is determined by the wind direction, and a cerebellar model articulation controller (CMAC) neural network is designed to optimize the action strategy for each state. Finally, numerical simulations demonstrate that the proposed control method performs well stability and intelligent decision-making ability in the process of horizontal trajectory control for stratospheric airships.
               ",autonomous vehicle
10.1016/j.micpro.2018.09.008,journal,Microprocessors and Microsystems,sciencedirect,2019-02-28,sciencedirect,Autonomous power management in mobile devices using dynamic frequency scaling and reinforcement learning for energy minimization,https://api.elsevier.com/content/article/pii/S0141933118301273,"
                  Embedded systems execute applications that execute hardware differently depending on the computation task, generating time-varying workloads. Energy minimization can be reached by using the low-power central processing unit (CPU) frequency for each workload. We propose an autonomous and online approach, capable of reducing energy consumption from adaptation to workload variations even in an unknown environment. In this approach, we improved the AEWMA algorithm into a new algorithm called AEWMA-MSE, adding new functionality to detect workload changes and demonstrating why it is better to use statistical analysis for real user cases in a mobile environment. Also, a new power model for mobile devices based on k-NN algorithm for regression was proposed and validated proving to have a better trade-off between execution time and precision than neural networks and linear regression-based models. AEWMA-MSE and the proposed power model are integrated into a novel algorithm for energy management based on reinforcement learning that suitably selects the appropriate CPU frequency based on workload predictions to minimize energy consumption. The proposed approach is validated through simulation by using real smartphone data from an ARM Cortex A7 processor used in a commercial smartphone. Our proposal proved to have an improvement in the Q-learning cost function and can effectively minimize the average energy consumption by 21% and up to 29% when compared to the already existing approaches.
               ",autonomous vehicle
10.1016/j.enconman.2019.111799,journal,Energy Conversion and Management,sciencedirect,2019-10-15,sciencedirect,A review of deep learning for renewable energy forecasting,https://api.elsevier.com/content/article/pii/S0196890419307812,"
                  As renewable energy becomes increasingly popular in the global electric energy grid, improving the accuracy of renewable energy forecasting is critical to power system planning, management, and operations. However, this is a challenging task due to the intermittent and chaotic nature of renewable energy data. To date, various methods have been developed, including physical models, statistical methods, artificial intelligence techniques, and their hybrids to improve the forecasting accuracy of renewable energy. Among them, deep learning, as a promising type of machine learning capable for discovering the inherent nonlinear features and high-level invariant structures in data, has been frequently reported in the literature. This paper provides a comprehensive and extensive review of renewable energy forecasting methods based on deep learning to explore its effectiveness, efficiency and application potential. We divide the existing deterministic and probabilistic forecasting methods based on deep learning into four groups, namely deep belief network, stack auto-encoder, deep recurrent neural network and others. We also dissect the feasible data preprocessing techniques and error post-correction methods to improve the forecasting accuracy. Extensive analysis and discussion of various deep learning based forecasting methods are given. Finally, we explore the current research activities, challenges and potential future research directions in this topic.
               ",autonomous vehicle
10.1016/B978-0-12-801238-3.11388-1,journal,Systems Medicine,sciencedirect,2021-12-31,sciencedirect,Implementation of Machine Learning-Aided Imaging Analytics for Histopathological Image Diagnosis,https://api.elsevier.com/content/article/pii/B9780128012383113881,"
               In the last few decades, rapid advances in image-digitizing technologies have led to the development of scanners capable of producing digitalized slides for use in pathology. Digital slides are more interactive, simultaneously available to multiple remote users, can be easily annotated, and enables more standardized education in pathology. Increase in computer processing power, data transfer speeds, advances in software development, and cloud storage solutions have also enabled the use of machine learning-based approaches for a wide variety of applications in pathology. Machine learning-aided imaging analytics usually begins with the system computing image features that are assumed to be of importance in making the prediction or diagnosis of interest. Following which, it identifies the best combination of these image features for classifying the image or computing some metric for a given region of interest. There are now several learning algorithms that can be used, each with their own strengths and weaknesses in the computational settings. There are also open-source platforms that make it easier for the user to employ machine learning-based methods for image analysis. In summary, machine learning-based imaging analytics is now widely used for histopathological image diagnoses and this article will introduce its utilization in the clinical settings to illustrate in detail how such method could be applied in the routine practice of histopathology.
            ",autonomous vehicle
10.1016/j.csbj.2021.08.011,journal,Computational and Structural Biotechnology Journal,sciencedirect,2021-12-31,sciencedirect,A review on machine learning approaches and trends in drug discovery,https://api.elsevier.com/content/article/pii/S2001037021003421,"Drug discovery aims at finding new compounds with specific chemical properties for the treatment of diseases. In the last years, the approach used in this search presents an important component in computer science with the skyrocketing of machine learning techniques due to its democratization. With the objectives set by the Precision Medicine initiative and the new challenges generated, it is necessary to establish robust, standard and reproducible computational methodologies to achieve the objectives set. Currently, predictive models based on Machine Learning have gained great importance in the step prior to preclinical studies. This stage manages to drastically reduce costs and research times in the discovery of new drugs. This review article focuses on how these new methodologies are being used in recent years of research. Analyzing the state of the art in this field will give us an idea of where cheminformatics will be developed in the short term, the limitations it presents and the positive results it has achieved. This review will focus mainly on the methods used to model the molecular data, as well as the biological problems addressed and the Machine Learning algorithms used for drug discovery in recent years.",autonomous vehicle
10.1016/j.apenergy.2020.115237,journal,Applied Energy,sciencedirect,2020-08-15,sciencedirect,Machine learning driven smart electric power systems: Current trends and new perspectives,https://api.elsevier.com/content/article/pii/S0306261920307492,"
                  The current power systems are undergoing a rapid transition towards their more active, flexible, and intelligent counterpart smart grid, which brings about tremendous challenges in many domains, e.g., integration of various distributed renewable energy sources, cyberspace security, demand-side management, and decision-making of system planning and operation. The fulfillment of advanced functionalities in the smart grid firmly relies on the underlying information and communication infrastructure, and the efficient handling of a massive amount of data generated from various sources, e.g., smart meters, phasor measurement units, and various forms of sensors. In this paper, a comprehensive survey of over 200 recent publications is conducted to review the state-of-the-art practices and proposals of machine learning techniques and discuss the trend in a wide range of smart grid application domains. This study demonstrates the increasing interest and rapid expansion in the use of machine learning techniques to successfully address the technical challenges of the smart grid from various aspects. It is also revealed that some issues still remain open and worth further research efforts, such as the high-performance data processing and analysis for intelligent decision-making in large-scale complex multi-energy systems, lightweight machine learning-based solutions, and so forth. Moreover, the future perspectives of utilizing advanced computing and communication technologies, e.g., edge computing, ubiquitous internet of things and 5G wireless networks, in the smart grid are also highlighted. To the best of our knowledge, this is the first review of machine learning-driven solutions covering almost all the smart grid application domains. Machine learning will be one of the major drivers of future smart electric power systems, and this study can provide a preliminary foundation for further exploration and development of related knowledge and insights.
               ",autonomous vehicle
10.1016/j.robot.2020.103630,journal,Robotics and Autonomous Systems,sciencedirect,2020-11-30,sciencedirect,Improving robot dual-system motor learning with intrinsically motivated meta-control and latent-space experience imagination,https://api.elsevier.com/content/article/pii/S092188902030470X,"Combining model-based and model-free learning systems has been shown to improve the sample efficiency of learning to perform complex robotic tasks. However, dual-system approaches fail to consider the reliability of the learned model when it is applied to make multiple-step predictions, resulting in a compounding of prediction errors and performance degradation. In this paper, we present a novel dual-system motor learning approach where a meta-controller arbitrates online between model-based and model-free decisions based on an estimate of the local reliability of the learned model. The reliability estimate is used in computing an intrinsic feedback signal, encouraging actions that lead to data that improves the model. Our approach also integrates arbitration with imagination where a learned latent-space model generates imagined experiences, based on its local reliability, to be used as additional training data. We evaluate our approach against baseline and state-of-the-art methods on learning vision-based robotic grasping in simulation and real world. The results show that our approach outperforms the compared methods and learns near-optimal grasping policies in dense- and sparse-reward environments.",autonomous vehicle
10.1016/j.vehcom.2019.100184,journal,Vehicular Communications,sciencedirect,2019-12-31,sciencedirect,"Deep learning models for traffic flow prediction in autonomous vehicles: A review, solutions, and challenges",https://api.elsevier.com/content/article/pii/S2214209619302311,"
                  In the last few years, there has been an exponential increase in the usage of the autonomous vehicles across the globe. It is due to an exponential increase in the popularity and usage of the artificial intelligence techniques in various applications. Traffic flow predication is important for autonomous vehicles using which they decide their itinerary and take adaptive decisions (for example, turn let or right, move straight, lane change, stop, or accelerate) with respect to their surrounding objects. From the existing literature, it has been observed that research on autonomous vehicles has shifted from the traditional statistical models to adaptive machine learning techniques. However, existing machine learning models may not be directly applicable in this environment due to non-linear complex relationship between spatial and temporal data collected from the surroundings during the aforementioned adaptive decisions taken by the vehicles. So, with focus on these issues, in this article, we explore various deep learning models for traffic flow prediction in autonomous vehicles and compared these models with respect to their applicability in modern smart transportation systems. Various parameters are chosen to have a relative comparison among different deep learning models. Moreover, challenges and future research directions are also discussed in the article.
               ",autonomous vehicle
10.1016/j.patter.2021.100328,journal,Patterns,sciencedirect,2021-10-08,sciencedirect,Machine learning applications for therapeutic tasks with genomics data,https://api.elsevier.com/content/article/pii/S2666389921001768,"Thanks to the increasing availability of genomics and other biomedical data, many machine learning algorithms have been proposed for a wide range of therapeutic discovery and development tasks. In this survey, we review the literature on machine learning applications for genomics through the lens of therapeutic development. We investigate the interplay among genomics, compounds, proteins, electronic health records, cellular images, and clinical texts. We identify 22 machine learning in genomics applications that span the whole therapeutics pipeline, from discovering novel targets, personalizing medicine, developing gene-editing tools, all the way to facilitating clinical trials and post-market studies. We also pinpoint seven key challenges in this field with potentials for expansion and impact. This survey examines recent research at the intersection of machine learning, genomics, and therapeutic development.",autonomous vehicle
10.1016/B978-0-323-89861-4.00027-0,journal,Computers in Earth and Environmental Sciences,sciencedirect,2022-12-31,sciencedirect,Chapter 43: Application of machine learning algorithms in hydrology,https://api.elsevier.com/content/article/pii/B9780323898614000270,"
               Hydrology is the science of studying the natural flow of water and the effect of human activity on the water. Hydrological modeling is essential for the management and conservation of water. In recent decades, machine learning (ML) has been applied efficiently in hydrology. In this study, the application of ML in four subfields of hydrology, including flood, precipitation estimation, water quality, and groundwater, is presented. This review shows that ML performs better in flood prediction than traditional data-driven and physical hydrology modeling, particularly in short-term flood forecasting. In addition, using the ML technique helps to estimate precipitation from satellite datasets. This study provides a review of the potential of ML in water quality and groundwater modeling. The study shows that using an optimization algorithm for parameter selection can improve the performance of ML. Moreover, modeling accuracy is often improved through ML hybridization. Finally, it is recommended that hydrologists use ML in their modeling owing to their low computational cost and high performance.
            ",autonomous vehicle
10.1016/j.abb.2020.108730,journal,Archives of Biochemistry and Biophysics,sciencedirect,2021-02-15,sciencedirect,Artificial intelligence in the early stages of drug discovery,https://api.elsevier.com/content/article/pii/S0003986120307384,"
                  Although the use of computational methods within the pharmaceutical industry is well established, there is an urgent need for new approaches that can improve and optimize the pipeline of drug discovery and development. In spite of the fact that there is no unique solution for this need for innovation, there has recently been a strong interest in the use of Artificial Intelligence for this purpose. As a matter of fact, not only there have been major contributions from the scientific community in this respect, but there has also been a growing partnership between the pharmaceutical industry and Artificial Intelligence companies. Beyond these contributions and efforts there is an underlying question, which we intend to discuss in this review: can the intrinsic difficulties within the drug discovery process be overcome with the implementation of Artificial Intelligence? While this is an open question, in this work we will focus on the advantages that these algorithms provide over the traditional methods in the context of early drug discovery.
               ",autonomous vehicle
10.1016/j.crme.2019.11.009,journal,Comptes Rendus Mécanique,sciencedirect,2019-11-30,sciencedirect,Data-driven modeling and learning in science and engineering,https://api.elsevier.com/content/article/pii/S1631072119301809,"In the past, data in which science and engineering is based, was scarce and frequently obtained by experiments proposed to verify a given hypothesis. Each experiment was able to yield only very limited data. Today, data is abundant and abundantly collected in each single experiment at a very small cost. Data-driven modeling and scientific discovery is a change of paradigm on how many problems, both in science and engineering, are addressed. Some scientific fields have been using artificial intelligence for some time due to the inherent difficulty in obtaining laws and equations to describe some phenomena. However, today data-driven approaches are also flooding fields like mechanics and materials science, where the traditional approach seemed to be highly satisfactory. In this paper we review the application of data-driven modeling and model learning procedures to different fields in science and engineering.",autonomous vehicle
10.1016/j.ress.2019.04.036,journal,Reliability Engineering & System Safety,sciencedirect,2019-11-30,sciencedirect,Managing engineering systems with large state and action spaces through deep reinforcement learning,https://api.elsevier.com/content/article/pii/S0951832018313309,"
                  Decision-making for engineering systems management can be efficiently formulated using Markov Decision Processes (MDPs) or Partially Observable MDPs (POMDPs). Typical MDP/POMDP solution procedures utilize offline knowledge about the environment and provide detailed policies for relatively small systems with tractable state and action spaces. However, in large multi-component systems the dimensions of these spaces easily explode, as system states and actions scale exponentially with the number of components, whereas environment dynamics are difficult to be described explicitly for the entire system and may, often, only be accessible through computationally expensive numerical simulators. In this work, to address these issues, an integrated Deep Reinforcement Learning (DRL) framework is introduced. The Deep Centralized Multi-agent Actor Critic (DCMAC) is developed, an off-policy actor-critic DRL algorithm that directly probes the state/belief space of the underlying MDP/POMDP, providing efficient life-cycle policies for large multi-component systems operating in high-dimensional spaces. Apart from deep network approximators parametrizing complex functions with vast state spaces, DCMAC also adopts a factorized representation of the system actions, thus being able to designate individualized component- and subsystem-level decisions, while maintaining a centralized value function for the entire system. DCMAC compares well against Deep Q-Network and exact solutions, where applicable, and outperforms optimized baseline policies that incorporate time-based, condition-based, and periodic inspection and maintenance considerations.
               ",autonomous vehicle
10.1016/j.ijepes.2021.107176,journal,International Journal of Electrical Power & Energy Systems,sciencedirect,2021-11-30,sciencedirect,A taxonomical review on recent artificial intelligence applications to PV integration into power grids,https://api.elsevier.com/content/article/pii/S0142061521004154,"
                  The exponential growth of solar power has been witnessed in the past decade and is projected by the ambitious policy targets. Nevertheless, the proliferation of solar energy poses challenges to power system operations, mostly due to its uncertainty, locational specificity, and variability. The prevalence of smart grids enables artificial intelligence (AI) techniques to mitigate solar integration problems with massive amounts of solar energy data. Different AI subfields (e.g., machine learning, deep learning, ensemble learning, and metaheuristic learning) have brought breakthroughs in solar energy, especially in its grid integration. However, AI research in solar integration is still at the preliminary stage, and is lagging behind the AI mainstream. Aiming to inspire deep AI involvement in the solar energy domain, this paper presents a taxonomical overview of AI applications in solar photovoltaic (PV) systems. Text mining techniques are first used as an assistive tool to collect, analyze, and categorize a large volume of literature in this field. Then, based on the constructed literature infrastructure, recent advancements in AI applications to solar forecasting, PV array detection, PV system fault detection, design optimization, and maximum power point tracking control problems are comprehensively reviewed. Current challenges and future trends of AI applications in solar integration are also discussed for each application theme.
               ",autonomous vehicle
10.1016/j.cej.2020.124072,journal,Chemical Engineering Journal,sciencedirect,2020-05-01,sciencedirect,"Applying machine learning algorithms in estimating the performance of heterogeneous, multi-component materials as oxygen carriers for chemical-looping processes",https://api.elsevier.com/content/article/pii/S1385894720300632,"Heterogeneous, multi-component materials such as industrial tailings or by-products, along with naturally occurring materials, such as ores, have been intensively investigated as candidate oxygen carriers for chemical-looping processes. However, these materials have highly variable compositions, and this strongly influences their chemical-looping performance. Here, using machine learning techniques, we estimate the performance of heterogeneous, multi-component materials as oxygen carriers for chemical-looping. Experimental data for 19 manganese ores chosen as potential chemical-looping oxygen carriers were used to create a so-called training database. This database has been used to train several supervised artificial neural network models (ANN), which were used to predict the reactivity of the oxygen carriers with different fuels and the oxygen transfer capacity with only the knowledge of reactor bed temperature, elemental composition, and mechanical properties of the manganese ores. This novel approach explores ways of dealing with the training dataset, learning algorithms and topology of ANN models to achieve enhanced prediction precision. Stacked neural networks with a bootstrap resampling technique have been applied to achieve high precision and robustness on new input data, and the confidence intervals were used to assess the precision of these predictions. The current results indicate that the best trained ANNs can produce highly accurate predictions for both the training database and the unseen data with the high coefficient of determination (R2 = 0.94) and low mean absolute error (MAE = 0.057). We envision that the application of these ANNs and other machine learning algorithms will accelerate the development of oxygen carrying materials for a range of chemical-looping applications and offer a rapid screening tool for new potential oxygen carriers.",autonomous vehicle
10.1016/j.caeai.2020.100002,journal,Computers and Education: Artificial Intelligence,sciencedirect,2020-12-31,sciencedirect,Application and theory gaps during the rise of Artificial Intelligence in Education,https://api.elsevier.com/content/article/pii/S2666920X20300023,"Considering the increasing importance of Artificial Intelligence in Education (AIEd) and the absence of a comprehensive review on it, this research aims to conduct a comprehensive and systematic review of influential AIEd studies. We analyzed 45 articles in terms of annual distribution, leading journals, institutions, countries/regions, the most frequently used terms, as well as theories and technologies adopted. We also evaluated definitions of AIEd from broad and narrow perspectives and clarified the relationship among AIEd, Educational Data Mining, Computer-Based Education, and Learning Analytics. Results indicated that: 1) there was a continuingly increasing interest in and impact of AIEd research; 2) little work had been conducted to bring deep learning technologies into educational contexts; 3) traditional AI technologies, such as natural language processing were commonly adopted in educational contexts, while more advanced techniques were rarely adopted, 4) there was a lack of studies that both employ AI technologies and engage deeply with educational theories. Findings suggested scholars to 1) seek the potential of applying AI in physical classroom settings; 2) spare efforts to recognize detailed entailment relationships between learners’ answers and the desired conceptual understanding within intelligent tutoring systems; 3) pay more attention to the adoption of advanced deep learning algorithms such as generative adversarial network and deep neural network; 4) seek the potential of NLP in promoting precision or personalized education; 5) combine biomedical detection and imaging technologies such as electroencephalogram, and target at issues regarding learners’ during the learning process; and 6) closely incorporate the application of AI technologies with educational theories.",autonomous vehicle
10.1016/j.ecoenv.2020.111470,journal,Ecotoxicology and Environmental Safety,sciencedirect,2021-01-15,sciencedirect,Predicting the consequences of accidents involving dangerous substances using machine learning,https://api.elsevier.com/content/article/pii/S0147651320313075,"A new dimension of learning lessons from the occurrence of hazardous events involving dangerous substances is considered relying on the availability of representative data and the significant evolution of a wide range of machine learning tools. The importance of such a dimension lies in the possibility of predicting the associated nature of damages without imposing any unrealistic simplifications or restrictions. To provide the best possible modeling framework, several implementations are tested using logistic regression, decision trees, neural networks, support vector machine, naive Bayes classifier and random forests to forecast the occurrence of the human, environmental and material consequences of industrial accidents based on the EU Major Accident Reporting System’s records. Many performance metrics are estimated to select the most suitable model in each treated case. The obtained results show the distinctive ability of random forests and neural networks to predict the occurrence of specific consequences of accidents in the industrial installations, with an obvious exception concerning the performance of this latter algorithm when the involved datasets are highly unbalanced.",autonomous vehicle
10.1053/j.gastro.2019.08.058,journal,Gastroenterology,sciencedirect,2020-01-31,sciencedirect,<ce:marker name=globe alt=Additional Online Content Available altimg-small=globe_s.svg altimg=globe_o.svg></ce:marker>Application of Artificial Intelligence to Gastroenterology and Hepatology,https://api.elsevier.com/content/article/pii/S0016508519414121,"
                  Since 2010, substantial progress has been made in artificial intelligence (AI) and its application to medicine. AI is explored in gastroenterology for endoscopic analysis of lesions, in detection of cancer, and to facilitate the analysis of inflammatory lesions or gastrointestinal bleeding during wireless capsule endoscopy. AI is also tested to assess liver fibrosis and to differentiate patients with pancreatic cancer from those with pancreatitis. AI might also be used to establish prognoses of patients or predict their response to treatments, based on multiple factors. We review the ways in which AI may help physicians make a diagnosis or establish a prognosis and discuss its limitations, knowing that further randomized controlled studies will be required before the approval of AI techniques by the health authorities.
               ",autonomous vehicle
10.1016/j.jnca.2021.103084,journal,Journal of Network and Computer Applications,sciencedirect,2021-08-15,sciencedirect,A Systematic Review of Quality of Service in Wireless Sensor Networks using Machine Learning: Recent Trend and Future Vision,https://api.elsevier.com/content/article/pii/S1084804521001065,"
                  Wireless Sensor Network (WSN) is used in different research areas such as military, industry, healthcare, agriculture, Internet of Things (IoT), transportation, and smart cities. The reason behind this increased usage is the rapid development of smart sensors. There is a challenging need to satisfy the Quality of Service (QoS) requirements in different applications due to the dynamic network condition, heterogeneous traffic flows and resource-constrained behaviour of sensor nodes. Optimizing the QoS in terms of performance, privacy and security levels is an open issue in the WSN. It has limited resources and is deployed in hostile environment where achieving high performance is difficult. This performance level is categorized into four subcategories: deployment phase, layered architecture, measurability, network and application specific parameter. Privacy and security levels are divided into four parameters: security, confidentiality, integrity and safety. A systematic review is presented in this paper based on QoS parameters in the light of Machine Learning (ML) techniques. It also provides a methodological framework for the performance parameters. This study presents a statistical analysis of the past ten years ranging from 2011 to 2021 on various ML techniques used for the QoS parameters. Finally, the author's vision is highlighted with some discussion on the open issues which forms the baseline for the future research directions.
               ",autonomous vehicle
10.1016/j.copbio.2021.07.024,journal,Current Opinion in Biotechnology,sciencedirect,2022-02-28,sciencedirect,Applications of artificial intelligence to enzyme and pathway design for metabolic engineering,https://api.elsevier.com/content/article/pii/S0958166921001361,"
                  Metabolic engineering for developing industrial strains capable of overproducing bioproducts requires good understanding of cellular metabolism, including metabolic reactions and enzymes. However, metabolic pathways and enzymes involved are still unknown for many products of interest, which presents a key challenge in their biological production. This challenge can be partly overcome by constructing novel biosynthetic pathways through enzyme and pathway design approaches. With the increase in bio-big data, data-driven approaches using artificial intelligence (AI) techniques are allowing more advanced protein and pathway design. In this paper, we review recent studies on AI-aided protein engineering and design, focusing on directed evolution that uses AI approaches to efficiently construct mutant libraries. Also, recent works of AI-aided pathway design strategies, including template-based and template-free approaches, are discussed.
               ",autonomous vehicle
10.1016/j.ins.2020.06.025,journal,Information Sciences,sciencedirect,2020-10-31,sciencedirect,An accurate and dynamic predictive model for a smart M-Health system using machine learning,https://api.elsevier.com/content/article/pii/S0020025520306113,"
                  Nowadays, new highly-developed technologies are changing traditional processes related to medical and healthcare systems. Emerging Mobile Health (M-Health) systems are examples of novel technologies based on advanced data communication, deep learning, artificial intelligence, cloud computing, big data, and other machine learning methods. Data are collected from sensor nodes and forwarded to local databases through new technologies that enable cellular networks and then store the information in cloud storage systems. From cloud computing services or medical centres, the data are collected for further analysis. Furthermore, machine learning techniques are being used for accurate prediction of disease analysis and for purposes of classification. This paper presents a detailed overview of M-Health systems, their model and architecture, technologies and applications and also discusses statistical and machine learning approaches. We also propose a secure Android-based architecture to collect patient data, a reliable cloud-based model for data storage. Finally, a predictive model able to classify cardiovascular diseases according to their seriousness will be discussed. Moreover, the proposed prediction model has been compared with existing models in terms of accuracy, sensitivity, and specificity. The experimental results show encouraging results in terms of the proposed predictive model for an M-Health system. Keywords: Machine Learning, Predictive, Models, M-Health, Classification, SVM, Decision Tree, Accuracy
               ",autonomous vehicle
10.1016/j.neunet.2013.02.003,journal,Neural Networks,sciencedirect,2013-07-31,sciencedirect,A supervised multi-spike learning algorithm based on gradient descent for spiking neural networks,https://api.elsevier.com/content/article/pii/S0893608013000440,"
                  We use a supervised multi-spike learning algorithm for spiking neural networks (SNNs) with temporal encoding to simulate the learning mechanism of biological neurons in which the SNN output spike trains are encoded by firing times. We first analyze why existing gradient-descent-based learning methods for SNNs have difficulty in achieving multi-spike learning. We then propose a new multi-spike learning method for SNNs based on gradient descent that solves the problems of error function construction and interference among multiple output spikes during learning. The method could be widely applied to single spiking neurons to learn desired output spike trains and to multilayer SNNs to solve classification problems. By overcoming learning interference among multiple spikes, our method has high learning accuracy when there are a relatively large number of output spikes in need of learning. We also develop an output encoding strategy with respect to multiple spikes for classification problems. This effectively improves the classification accuracy of multi-spike learning compared to that of single-spike learning.
               ",autonomous vehicle
10.1016/B978-0-323-85498-6.00010-1,journal,Artificial Intelligence for Future Generation Robotics,sciencedirect,2021-12-31,sciencedirect,Chapter Eight: Integrated deep learning for self-driving robotic cars,https://api.elsevier.com/content/article/pii/B9780323854986000101,"
               In recent years, autonomous driving has become a hotbed of research in academia and industry. However, the high cost of the infrastructure as well as of the specialized equipment makes it very difficult for academia as well as industry to pursue in-house research. This study presents a laboratory floor software model for learning self-driving using a robotic vehicle with only vision cameras and no other expensive sensors or equipment. The low-cost autonomous driving learning program is based on an analysis of the human driving cycle which consists of perception, scene generation, planning, and action. The software platform is an integration of deep supervision and reinforcement learning. Perception is done through deep supervision learning using the latest convolutional neural networks, while the driving actions such as steering, accelerating, slowing down, and braking are achieved through Deep Q learning. The major components of the autonomous driving agents are (1) fundamental driving functions, (2) hazard detection, and (3) warning systems, which are further composed of several independent subroutine modules. The learned agent is integrated and embedded in a miniature robotic vehicle and further trained manually through remotely controlled steering, accelerator, and brakes. Simulation on a multi-GPU workstation and laboratory floor verifications have produced encouraging results, leading to the development of an economically viable low-cost self-driving prototype.
            ",autonomous vehicle
10.1016/j.comnet.2020.107743,journal,Computer Networks,sciencedirect,2021-02-11,sciencedirect,Deep learning for privacy preservation in autonomous moving platforms enhanced 5G heterogeneous networks,https://api.elsevier.com/content/article/pii/S138912862031327X,"
                  5G heterogeneous networks have become a promising platform to connect a growing number of Internet-of-Things (IoT) devices and accommodate a wide variety of vertical services. IoT has not been limited to traditional sensing systems since the introduction of 5G, but also includes a range of autonomous moving platforms, e.g., autonomous flying vehicles, autonomous underwater vehicles, autonomous surface vehicles as well as autonomous land vehicles. These platforms can be used as an effective means to connect air, space, ground, and sea mobile networks for providing a wider diversity of Internet services. Deep learning has been widely used to extract useful information from network big data for enhancing network quality-of-service and user quality-of-experience. Privacy preservation for user and network data is a burning concern in 5G heterogeneous networks due to various attacks in this environment. In this paper, we conduct an in-depth investigation on how deep learning can cope with privacy preservation issues in 5G heterogeneous networks, in terms of heterogeneous radio access networks (RANs), beyond-RAN networks, and end-to-end network slices, followed by a set of key research challenges and open issues that aim to guide future research.
               ",autonomous vehicle
10.1016/j.compeleceng.2020.106558,journal,Computers & Electrical Engineering,sciencedirect,2020-03-31,sciencedirect,Adaptive cache pre-forwarding policy for distributed deep learning,https://api.elsevier.com/content/article/pii/S0045790619313266,"
                  With the rapid growth of deep learning algorithms, several high-accuracy models have been developed and applied to many real-world domains. Deep learning is parallel and suitable for distributed computing, which can significantly improve the system throughput. However, there is a bottleneck for cross-machine training, that is, network latency. Nodes frequently need to wait for synchronization, and the content of each synchronization may range from several megabytes to hundred megabytes. Thus, network communication takes considerable time in the training process, which reduces system performance. Therefore, many computing architectures have been proposed. This paper proposes a type of distributed computing system for deep learning. Our design aims to reduce synchronization times and network blocking times by using a new cache mechanism, called cache pre-forwarding. The design concept of cache pre-forwarding aims to exploit reinforcement learning to train a pre-forwarding policy to increase the cache hit rate. Because of the features of reinforcement learning, our policy is adaptive and applicable to different computing environments. Finally, we experimentally demonstrate that our system is feasible.
               ",autonomous vehicle
10.1016/j.neucom.2016.02.005,journal,Neurocomputing,sciencedirect,2016-06-05,sciencedirect,"Special issue: Advances in artificial neural networks, machine learning and computational intelligenceSelected papers from the 23rd European Symposium on Artificial Neural Networks (ESANN 2015)",https://api.elsevier.com/content/article/pii/S0925231216001831,,autonomous vehicle
10.1016/j.crfs.2021.03.009,journal,Current Research in Food Science,sciencedirect,2021-12-31,sciencedirect,Deep learning and machine vision for food processing: A survey,https://api.elsevier.com/content/article/pii/S2665927121000228,"The quality and safety of food is an important issue to the whole society, since it is at the basis of human health, social development and stability. Ensuring food quality and safety is a complex process, and all stages of food processing must be considered, from cultivating, harvesting and storage to preparation and consumption. However, these processes are often labour-intensive. Nowadays, the development of machine vision can greatly assist researchers and industries in improving the efficiency of food processing. As a result, machine vision has been widely used in all aspects of food processing. At the same time, image processing is an important component of machine vision. Image processing can take advantage of machine learning and deep learning models to effectively identify the type and quality of food. Subsequently, follow-up design in the machine vision system can address tasks such as food grading, detecting locations of defective spots or foreign objects, and removing impurities. In this paper, we provide an overview on the traditional machine learning and deep learning methods, as well as the machine vision techniques that can be applied to the field of food processing. We present the current approaches and challenges, and the future trends.",autonomous vehicle
10.1016/j.cor.2020.104926,journal,Computers & Operations Research,sciencedirect,2020-07-31,sciencedirect,A systematic literature review on machine learning applications for sustainable agriculture supply chain performance,https://api.elsevier.com/content/article/pii/S0305054820300435,"
                  Agriculture plays an important role in sustaining all human activities. Major challenges such as overpopulation, competition for resources poses a threat to the food security of the planet. In order to tackle the ever-increasing complex problems in agricultural production systems, advancements in smart farming and precision agriculture offers important tools to address agricultural sustainability challenges. Data analytics hold the key to ensure future food security, food safety, and ecological sustainability. Disruptive information and communication technologies such as machine learning, big data analytics, cloud computing, and blockchain can address several problems such as productivity and yield improvement, water conservation, ensuring soil and plant health, and enhance environmental stewardship. The current study presents a systematic review of machine learning (ML) applications in agricultural supply chains (ASCs). Ninety three research papers were reviewed based on the applications of different ML algorithms in different phases of the ASCs. The study highlights how ASCs can benefit from ML techniques and lead to ASC sustainability. Based on the study findings an ML applications framework for sustainable ASC is proposed. The framework identifies the role of ML algorithms in providing real-time analytic insights for pro-active data-driven decision-making in the ASCs and provides the researchers, practitioners, and policymakers with guidelines on the successful management of ASCs for improved agricultural productivity and sustainability.
               ",autonomous vehicle
10.1016/j.ins.2015.01.019,journal,Information Sciences,sciencedirect,2015-05-20,sciencedirect,An unsupervised learning algorithm for membrane computing,https://api.elsevier.com/content/article/pii/S0020025515000572,"
                  This paper focuses on the unsupervised learning problem within membrane computing, and proposes an innovative solution inspired by membrane computing techniques, the fuzzy membrane clustering algorithm. An evolution–communication P system with nested membrane structure is the core component of the algorithm. The feasible cluster centers are represented by means of objects, and three types of membranes are considered: evolution, local store, and global store. Based on the designed membrane structure and the inherent communication mechanism, a modified differential evolution mechanism is developed to evolve the objects in the system. Under the control of the evolution–communication mechanism of the P system, the proposed fuzzy clustering algorithm achieves good fuzzy partitioning for a data set. The proposed fuzzy clustering algorithm is compared to three recently-developed and two classical clustering algorithms for five artificial and five real-life data sets.
               ",autonomous vehicle
10.1016/j.cger.2020.04.009,journal,Clinics in Geriatric Medicine,sciencedirect,2020-08-31,sciencedirect,Artificial Intelligence and Digital Tools: Future of Diabetes Care,https://api.elsevier.com/content/article/pii/S074906902030029X,,autonomous vehicle
10.1016/j.engstruct.2018.05.084,journal,Engineering Structures,sciencedirect,2018-09-15,sciencedirect,Emerging artificial intelligence methods in structural engineering,https://api.elsevier.com/content/article/pii/S0141029617335526,"
                  Artificial intelligence (AI) is proving to be an efficient alternative approach to classical modeling techniques. AI refers to the branch of computer science that develops machines and software with human-like intelligence. Compared to traditional methods, AI offers advantages to deal with problems associated with uncertainties and is an effective aid to solve such complex problems. In addition, AI-based solutions are good alternatives to determine engineering design parameters when testing is not possible, thus resulting in significant savings in terms of human time and effort spent in experiments. AI is also able to make the process of decision making faster, decrease error rates, and increase computational efficiency. Among the different AI techniques, machine learning (ML), pattern recognition (PR), and deep learning (DL) have recently acquired considerable attention and are establishing themselves as a new class of intelligent methods for use in structural engineering. The objective of this review paper is to summarize techniques concerning applications of the noted AI methods in structural engineering developed over the last decade. First, a general introduction to AI is presented and the importance of AI in structural engineering is described. Thereafter, a review of recent applications of ML, PR, and DL in the field is provided, and the capability of such methods to address the restrictions of conventional models are discussed. Further, the advantages of employing such algorithmic methods are discussed in detail. Finally, potential research avenues and emerging trends for employing ML, PR, and DL are presented, and their limitations are discussed.
               ",autonomous vehicle
10.1016/j.media.2020.101813,journal,Medical Image Analysis,sciencedirect,2021-01-31,sciencedirect,Deep neural network models for computational histopathology: A survey,https://api.elsevier.com/content/article/pii/S1361841520301778,"
                  Histopathological images contain rich phenotypic information that can be used to monitor underlying mechanisms contributing to disease progression and patient survival outcomes. Recently, deep learning has become the mainstream methodological choice for analyzing and interpreting histology images. In this paper, we present a comprehensive review of state-of-the-art deep learning approaches that have been used in the context of histopathological image analysis. From the survey of over 130 papers, we review the field’s progress based on the methodological aspect of different machine learning strategies such as supervised, weakly supervised, unsupervised, transfer learning and various other sub-variants of these methods. We also provide an overview of deep learning based survival models that are applicable for disease-specific prognosis tasks. Finally, we summarize several existing open datasets and highlight critical challenges and limitations with current deep learning approaches, along with possible avenues for future research.
               ",autonomous vehicle
10.1016/j.clsr.2021.105564,journal,Computer Law & Security Review,sciencedirect,2021-09-30,sciencedirect,Legal evaluation of the attacks caused by artificial intelligence-based lethal weapon systems within the context of Rome statute,https://api.elsevier.com/content/article/pii/S0267364921000376,"
                  Artificial intelligence (AI) as of the level of development reached today has become a scientific reality that is subject to study in the fields of law, political science, and other social sciences besides computer and software engineering. AI systems which perform relatively simple tasks in the early stages of the development period are expected to become fully or largely autonomous in the near future. Thanks to this, AI which includes the concepts of machine learning, deep learning, and autonomy, has begun to play an important role in producing and using smart arms. However, questions about AI-Based Lethal Weapon Systems (AILWS) and attacks that can be carried out by such systems have not been fully answered under legal aspect. More particularly, it is a controversial issue who will be responsible for the actions that an AILWS has committed. In this article, we discussed whether AILWS can commit offense in the context of the Rome Statute, examined the applicable law regarding the responsibility of AILWS, and tried to assess whether these systems can be held responsible in the context of international law, crime of aggression, and individual responsibility. It is our finding that international legal rules including the Rome Statute can be applied regarding the responsibility for the act/crime of aggression caused by AILWS. However, no matter how advanced the cognitive capacity of an AI software, it will not be possible to resort to the personal responsibility of this kind of system since it has no legal personality at all. In such a case, responsibility will remain with the actors who design, produce, and use the system. Last but not least, since no AILWS software does have specific codes of conduct that can make legal and ethical reasonings for today, at the end of the study it was recommended that states and non-governmental organizations together with manifacturers should constitute the necessary ethical rules written in software programs to prevent these systems from unlawful acts and to develop mechanisms that would restrain AI from working outside human control.
               ",autonomous vehicle
10.1016/j.mineng.2018.12.004,journal,Minerals Engineering,sciencedirect,2019-03-01,sciencedirect,Machine learning applications in minerals processing: A review,https://api.elsevier.com/content/article/pii/S0892687518305430,"
                  Machine learning and artificial intelligence techniques have an ever-increasing presence and impact on a wide-variety of research and commercial fields. Disappointed by previous hype cycles, researchers and industrial practitioners may be wary of overpromising and underdelivering techniques. This review aims at equipping researchers and industrial practitioners with structured knowledge on the state of machine learning applications in mineral processing: the supplementary material provides a searchable summary of all techniques reviewed, with fields including nature of case study data (synthetic/laboratory/industrial), level of success, area of application (e.g. milling, flotation, etc), and major problem category (data-based modelling, fault detection and diagnosis, and machine vision). Future directions are proposed, including suggestions on data collection, technique comparison, industrial participation, cost-benefit analyses and the future of mineral engineering training.
               ",autonomous vehicle
10.1016/j.eswa.2010.09.001,journal,Expert Systems with Applications,sciencedirect,2011-05-31,sciencedirect,Stock trading with cycles: A financial application of ANFIS and reinforcement learning,https://api.elsevier.com/content/article/pii/S095741741000905X,"
                  Based on the principles of technical analysis, this paper proposes an artificial intelligence model, which employs the Adaptive Network Fuzzy Inference System (ANFIS) supplemented by the use of reinforcement learning (RL) as a non-arbitrage algorithmic trading system. The novel intelligent trading system is capable of identifying a change in a primary trend for trading and investment decisions. It dynamically determines the periods for momentum and moving averages using the RL paradigm and also appropriately shifting the cycle using ANFIS-RL to address the delay in the predicted cycle. This is used as a proxy to determine the best point in time to go LONG and visa versa for SHORT. When this is coupled with a group of stocks, we derive a simple form of “riding the cycles – waves”. These are the derived features of the underlying stock movement. It provides a learning framework to trade on cycles. Initial experimental results are encouraging. Firstly, the proposed framework is able to outperform DENFIS and RSPOP in terms of true error and correlation. Secondly, based on the test trading with five US stocks, the proposed trading system is able to beat the market by about 50 percentage points over a period of 13years.
               ",autonomous vehicle
10.1016/j.neucom.2019.11.023,journal,Neurocomputing,sciencedirect,2020-03-14,sciencedirect,Deep learning in video multi-object tracking: A survey,https://api.elsevier.com/content/article/pii/S0925231219315966,"
                  The problem of Multiple Object Tracking (MOT) consists in following the trajectory of different objects in a sequence, usually a video. In recent years, with the rise of Deep Learning, the algorithms that provide a solution to this problem have benefited from the representational power of deep models. This paper provides a comprehensive survey on works that employ Deep Learning models to solve the task of MOT on single-camera videos. Four main steps in MOT algorithms are identified, and an in-depth review of how Deep Learning was employed in each one of these stages is presented. A complete experimental comparison of the presented works on the three MOTChallenge datasets is also provided, identifying a number of similarities among the top-performing methods and presenting some possible future research directions.
               ",autonomous vehicle
10.1016/j.jss.2021.111031,journal,Journal of Systems and Software,sciencedirect,2021-10-31,sciencedirect,A software engineering perspective on engineering machine learning systems: State of the art and challenges,https://api.elsevier.com/content/article/pii/S016412122100128X,"
                  Context:
                  Advancements in machine learning (ML) lead to a shift from the traditional view of software development, where algorithms are hard-coded by humans, to ML systems materialized through learning from data. Therefore, we need to revisit our ways of developing software systems and consider the particularities required by these new types of systems.
               
                  Objective:
                  The purpose of this study is to systematically identify, analyze, summarize, and synthesize the current state of software engineering (SE) research for engineering ML systems.
               
                  Method:
                  I performed a systematic literature review (SLR). I systematically selected a pool of 141 studies from SE venues and then conducted a quantitative and qualitative analysis using the data extracted from these studies.
               
                  Results:
                  The non-deterministic nature of ML systems complicates all SE aspects of engineering ML systems. Despite increasing interest from 2018 onwards, the results reveal that none of the SE aspects have a mature set of tools and techniques. Testing is by far the most popular area among researchers. Even for testing ML systems, engineers have only some tool prototypes and solution proposals with weak experimental proof. Many of the challenges of ML systems engineering were identified through surveys and interviews. Researchers should conduct experiments and case studies, ideally in industrial environments, to further understand these challenges and propose solutions.
               
                  Conclusion:
                  The results may benefit (1) practitioners in foreseeing the challenges of ML systems engineering; (2) researchers and academicians in identifying potential research questions; and (3) educators in designing or updating SE courses to cover ML systems engineering.
               ",autonomous vehicle
10.1016/j.jobe.2021.102636,journal,Journal of Building Engineering,sciencedirect,2021-12-31,sciencedirect,A BIM and machine learning integration framework for automated property valuation,https://api.elsevier.com/content/article/pii/S2352710221004940,"Property valuation contributes significantly to market economic activities, while it has been continuously questioned on its low transparency, inaccuracy and inefficiency. With Big Data applications in real estate domain growing fast, computer-aided valuation systems such as AI-enhanced automated valuation models (AVMs) have the potential to address these issues. While a plethora of research has focused on improving predictive performance of AVMs, little effort has been made on information requirements for valuation models. As the amount of data in BIM is rising exponentially, the value-relevant design information has not been widely utilized for property valuation. This paper presents a system that leverages a holistic data interpretation, improves information exchange between AEC projects and property valuation, and automates specific workflows for property valuation. A mixed research method was adopted combining the archival literature research, qualitative and quantitative data analysis. A BIM and Machine learning (ML) integration framework for automated property valuation was proposed which contains a fundamental database interpretation, an IFC-based information extraction and an automated valuation model based on genetic algorithm optimized machine learning (GA-GBR). The main findings indicated: (1) Partial information requirements can be extracted from BIM models, (2) Property valuation can be performed in a more accurate and efficient way. This research contributes to managing information exchange between AEC projects and property valuation and supporting automated property valuation. It was suggested that the infusion of BIM, ML and other emerging digital technologies might add values to property valuation and the construction industry.",autonomous vehicle
10.1016/B978-0-12-821259-2.00004-1,journal,Artificial Intelligence in Medicine,sciencedirect,2021-12-31,sciencedirect,Chapter 4: Biomedical imaging and analysis through deep learning,https://api.elsevier.com/content/article/pii/B9780128212592000041,"
               The material presented here will expand upon the deep learning techniques introduced in the prior chapter to address imaging-related issues. Recently, the use of deep learning has gained tremendous popularity within the realm of medical imaging research and development. This chapter will give a general overview of artificial intelligence applications with an emphasis on the areas of tomographic image reconstruction, image segmentation, image registration, and radiomics. Given the large scope of this chapter, it mainly describes key applications at a conceptual level but not at a technical level, with many details left to the references.
            ",autonomous vehicle
10.1016/j.parco.2018.03.006,journal,Parallel Computing,sciencedirect,2018-07-31,sciencedirect,Machine Learning in Multi-Agent Systems using Associative Arrays,https://api.elsevier.com/content/article/pii/S0167819118300814,"
                  In this paper, a new machine learning algorithm for multi-agent systems is introduced. The algorithm is based on associative arrays, thus it becomes less complex and more efficient substitute of artificial neural networks and Bayesian networks, which is confirmed by performance measurements. Implementation of machine learning algorithm in multi-agent system for aided design of selected control systems allowed to improve the performance by reducing time of processing requests, that were previously acknowledged and stored in learning module. This article contains an insight into different machine learning algorithms and includes the classification of learning techniques regarding the criteria depicted by multi-agent systems. The publication is also an attempt to provide the answer for a question posted by Shoham, Powers and Grenager: “If multi-agent learning is the answer, what is the question?”
               ",autonomous vehicle
10.1016/j.patrec.2020.05.031,journal,Pattern Recognition Letters,sciencedirect,2020-08-31,sciencedirect,Multi-task learning for natural language processing in the 2020s: Where are we going?,https://api.elsevier.com/content/article/pii/S0167865520302087,"
                  Multi-task learning (MTL) significantly pre-dates the deep learning era, and it has seen a resurgence in the past few years as researchers have been applying MTL to deep learning solutions for natural language tasks. While steady MTL research has always been present, there is a growing interest driven by the impressive successes published in the related fields of transfer learning and pre-training, such as BERT, and the release of new challenge problems, such as GLUE and the NLP Decathlon (decaNLP). These efforts place more focus on how weights are shared across networks, evaluate the re-usability of network components and identify use cases where MTL can significantly outperform single-task solutions. This paper strives to provide a comprehensive survey of the numerous recent MTL contributions to the field of natural language processing and provide a forum to focus efforts on the hardest unsolved problems in the next decade. While novel models that improve performance on NLP benchmarks are continually produced, lasting MTL challenges remain unsolved which could hold the key to better language understanding, knowledge discovery and natural language interfaces.
               ",autonomous vehicle
10.1016/j.neucom.2020.09.091,journal,Neurocomputing,sciencedirect,2021-10-07,sciencedirect,Domain generalization via optimal transport with metric similarity learning,https://api.elsevier.com/content/article/pii/S0925231221002009,"
                  Generalizing knowledge to unseen domains, where data and labels are unavailable, is crucial for machine learning models. We tackle the domain generalization problem to learn from multiple source domains and generalize to a target domain with unknown statistics. The crucial idea is to extract the underlying invariant features across all the domains. Previous domain generalization approaches mainly focused on learning invariant features and stacking the learned features from each source domain to generalize to a new target domain while ignoring the label information, which will lead to indistinguishable features with an ambiguous classification boundary. One possible solution is to constrain the label-similarity when extracting the invariant features and take advantage of the label similarities for class-specific cohesion and separation of features across domains. Therefore we adopt optimal transport with Wasserstein distance, which could constrain the class label similarity, for adversarial training and also further deploy a metric learning objective to leverage the label information for achieving distinguishable classification boundary. Empirical results show that our proposed method could outperform most of the baselines. Furthermore, ablation studies also demonstrate the effectiveness of each component of our method.
               ",autonomous vehicle
10.1016/j.neunet.2018.07.013,journal,Neural Networks,sciencedirect,2018-12-31,sciencedirect,"Born to learn: The inspiration, progress, and future of evolved plastic artificial neural networks",https://api.elsevier.com/content/article/pii/S0893608018302120,"
                  Biological neural networks are systems of extraordinary computational capabilities shaped by evolution, development, and lifelong learning. The interplay of these elements leads to the emergence of biological intelligence. Inspired by such intricate natural phenomena, Evolved Plastic Artificial Neural Networks (EPANNs) employ simulated evolution in-silico to breed plastic neural networks with the aim to autonomously design and create learning systems. EPANN experiments evolve networks that include both innate properties and the ability to change and learn in response to experiences in different environments and problem domains. EPANNs’ aims include autonomously creating learning systems, bootstrapping learning from scratch, recovering performance in unseen conditions, testing the computational advantages of particular neural components, and deriving hypotheses on the emergence of biological learning. Thus, EPANNs may include a large variety of different neuron types and dynamics, network architectures, plasticity rules, and other factors. While EPANNs have seen considerable progress over the last two decades, current scientific and technological advances in artificial neural networks are setting the conditions for radically new approaches and results. Exploiting the increased availability of computational resources and of simulation environments, the often challenging task of hand-designing learning neural networks could be replaced by more autonomous and creative processes. This paper brings together a variety of inspiring ideas that define the field of EPANNs. The main methods and results are reviewed. Finally, new opportunities and possible developments are presented.
               ",autonomous vehicle
10.1016/j.cie.2020.106682,journal,Computers & Industrial Engineering,sciencedirect,2021-01-31,sciencedirect,An analytical and a Deep Learning model for solving the inverse kinematic problem of an industrial parallel robot,https://api.elsevier.com/content/article/pii/S0360835220304162,"
                  This paper proposes two solutions for the inverse kinematic problem of an industrial parallel robot: a closed analytical form and a Deep Learning approximation model based on three different networks. The analytical form is found and compared against the three Neural Network models: MLP (Multi-Layer Perceptron), deep LSTM (Long-Short Term Memory) and GRU (Gated Recurrent Unit) networks. Algorithms based on these three machine learning (ML) techniques were implemented in a tensorflow environment, using a Deep Learning server machine. Analysis of inverse kinematics is complex and in most cases it pursues multiple solutions; furthermore, an analytic solution exists only for an ideal robot model when the structure of the robot meets certain conditions. Therefore, soft-computing alternatives, along with the Deep Learning concept are qualified candidates due to decreased calculation and processing times compared with other conventional methods. The solution proposed here includes a prediction accuracy comparison between three ML techniques, as well as the validation with the nominal kinematic model of the parallel industrial robot. It is a novel alternative for solving and validating parallel robot models.
               ",autonomous vehicle
10.1016/j.procir.2020.01.035,journal,Procedia CIRP,sciencedirect,2019-12-31,sciencedirect,"Machine Learning in Production – Potentials, Challenges and Exemplary Applications",https://api.elsevier.com/content/article/pii/S2212827120300445,"Recent trends like autonomous driving, natural language processing, service robotics or Industry 4.0 are mainly based on the tremendous progress made in the field of machine learning (ML). The increased data availability coupled with affordable computing power and easy-to-use software tools have laid the foundation for using such algorithms in a wide range of industrial applications, e.g. for predictive maintenance, predictive quality or machine vision. However, a systematic guideline for identifying and implementing economically viable ML use cases in manufacturing industry is still missing. In particular, there is still a lack of a structured overview of concrete, industry-specific best practices that can be easily transferred to one’ s own production. Hence, this paper aims to summarize various existing application scenarios of ML from a process and an industry sector perspective. The process point of view mainly covers the main manufacturing process groups of DIN 8580, handling operations according to VDI 2860 as well as selected cross-process approaches. From an industry sector perspective, application scenarios from various subsectors such as the production of electronics, electric motors, transmission components and medical devices are outlined.",autonomous vehicle
10.1016/j.cosrev.2019.08.002,journal,Computer Science Review,sciencedirect,2019-11-30,sciencedirect,Machine learning and multi-agent systems in oil and gas industry applications: A survey,https://api.elsevier.com/content/article/pii/S1574013719300486,"
                  The oil and gas industry (OGI) has always been associated with challenges and complexities. It involves many processes and stakeholders, each generating a huge amount of data. Due to the global and distributed nature of the business, processing and managing this information is an arduous task. Many issues such as orchestrating different data sources, owners and formats; verifying, validating and securing data streams as they move along the complex business process pipeline; and getting insights from data for improving business efficiency, scheduling maintenance and preventing theft and fraud are to be addressed. Artificial intelligence (AI), and machine learning (ML) in particular, have gained huge acceptance in many areas recently, including the OGI, to help humans tackle such complex tasks. Furthermore, multi-agent systems (MAS) as a sub-field of distributed AI meet the requirement of distributed systems and have been utilised successfully in a vast variety of disciplines. Several studies have explored the use of ML and MAS to increase operational efficiency, manage supply chain and solve various production- and maintenance-related tasks in the OGI. However, ML has only been applied to isolated tasks, and while MAS have yielded good performance in simulated environments, they have not gained the expected popularity among oil and gas companies yet. Further research in the fields is necessary to realise the potential of ML and MAS and encourage their wider acceptance in the OGI. In particular, embedding ML into MAS can bring many benefits for the future development of the industry. This paper aims to summarise the efforts to date of applying ML and MAS to OGI tasks, identify possible reasons for their low and slow uptake and suggest ways to ensure a greater adoption of these technologies in the OGI.
               ",autonomous vehicle
10.1016/j.neucom.2020.09.017,journal,Neurocomputing,sciencedirect,2021-01-15,sciencedirect,A review on transfer learning in EEG signal analysis,https://api.elsevier.com/content/article/pii/S0925231220314223,"
                  Electroencephalogram (EEG) signal analysis, which is widely used for human-computer interaction and neurological disease diagnosis, requires a large amount of labeled data for training. However, the collection of substantial EEG data could be difficult owing to its randomness and non-stationary. Moreover, there is notable individual difference in EEG data, which affects the reusability and generalization of models. For mitigating the adverse effects from the above factors, transfer learning is applied in this field to transfer the knowledge learnt in one domain into a different but related domain. Transfer learning adjusts models with small-scale data of the task, and also maintains the learning ability with individual difference. This paper describes four main methods of transfer learning and explores their practical applications in EEG signal analysis in recent years. Finally, we discuss challenges and opportunities of transfer learning and suggest areas for further study.
               ",autonomous vehicle
10.1016/j.csbj.2021.09.025,journal,Computational and Structural Biotechnology Journal,sciencedirect,2021-12-31,sciencedirect,Machine learning applications in RNA modification sites prediction,https://api.elsevier.com/content/article/pii/S2001037021004104,"Ribonucleic acid (RNA) modifications are post-transcriptional chemical composition changes that have a fundamental role in regulating the main aspect of RNA function. Recently, large datasets have become available thanks to the recent development in deep sequencing and large-scale profiling. This availability of transcriptomic datasets has led to increased use of machine learning based approaches in epitranscriptomics, particularly in identifying RNA modifications. In this review, we comprehensively explore machine learning based approaches used for the prediction of 11 RNA modification types, namely, m 1 A , m 6 A , m 5 C , 5 hmC , ψ , 2 ′ - O - Me , ac 4 C , m 7 G , A - to - I , m 2 G , and D . This review covers the life cycle of machine learning methods to predict RNA modification sites including available benchmark datasets, feature extraction, and classification algorithms. We compare available methods in terms of datasets, target species, approach, and accuracy for each RNA modification type. Finally, we discuss the advantages and limitations of the reviewed approaches and suggest future perspectives.",autonomous vehicle
10.1016/j.aap.2018.06.002,journal,Accident Analysis & Prevention,sciencedirect,2018-09-30,sciencedirect,Factors influencing unsafe behaviors: A supervised learning approach,https://api.elsevier.com/content/article/pii/S0001457518302173,"
                  Despite its potential, the use of machine learning in safety studies had been limited. Considering machine learning’s advantage in predictive accuracy, this study used a supervised learning approach to evaluate the relative importance of different cognitive factors within the Theory of Reasoned Action (TRA) in influencing safety behavior. Data were collected from 80 workers in a tunnel construction project using a TRA-based questionnaire. At the same time, behavior-based safety (BBS) observation data, % unsafe behavior, was collected. Subsequently, with the TRA cognitive factors as the input attributes, six widely-used machine learning algorithms and logistic regression were used to develop models to predict % unsafe behavior. The receiver operating characteristic (ROC) curves show that decision tree provides the best prediction. It was found that intention and social norms have the biggest influence on whether a worker was observed to work safely or not. Thus, managers aiming to improve safety behaviors need to pay specific attention to social norms in the worksite. The study also showed that a TRA survey can be used to extend a BBS to facilitate more effective interventions. Lastly, the study showed that machine learning algorithms provide an alternative approach for analyzing the relationship between the cognitive factors and behavioral data.
               ",autonomous vehicle
10.1016/j.neucom.2020.12.131,journal,Neurocomputing,sciencedirect,2021-05-07,sciencedirect,Supervised and semi-supervised deep probabilistic models for indoor positioning problems,https://api.elsevier.com/content/article/pii/S0925231221000229,"
                  WiFi fingerprint-based indoor localization has been a popular research topic recently. In this work, we propose two novel deep learning-based models, the convolutional mixture density recurrent neural network and the variational autoencoder-based semi-supervised learning model. The convolutional mixture density recurrent neural network is designed for indoor next location prediction, in which the advantages of convolutional neural networks, recurrent neural networks and mixture density networks are combined. Furthermore, since most of real-world WiFi fingerprint data are not labeled, we devise a variational autoencoder-based model to compute accurate user location in a semi-supervised learning manner. Finally, in order to evaluate the proposed models, we conduct the validation experiments on two real-world datasets. The final results are compared to other existing methods and verify the effectiveness of our approaches.
               ",autonomous vehicle
10.1016/j.buildenv.2021.108164,journal,Building and Environment,sciencedirect,2021-11-30,sciencedirect,Learning-based CO2 concentration prediction: Application to indoor air quality control using demand-controlled ventilation,https://api.elsevier.com/content/article/pii/S0360132321005655,"
                  There have been increasing concerns over the air quality inside buildings as high levels of bio-effluents can cause nausea, dizziness, headaches, and fatigue to the people working in those spaces. First published in 2004 as Standard 62.1, ASHRAE Standard 62.2-2019 requires highly occupied spaces to implement heating, ventilation, and air conditioning (HVAC) that can dilute contaminants produced by occupants. In this regard, occupant-centric ventilation control has been regarded as an effective practice to maintain a satisfactory indoor air quality (IAQ) when dealing with highly variable occupancy environments. However, few established models in current literature and practice consider dynamic occupancy behavior and adaptive IAQ control. To address this gap, a dynamic indoor CO2 model is constructed using machine learning algorithms to forecast CO2 concentrations across a range of forecasting horizons. Herein, we tuned and compared six state-of-the-art learning algorithms—including Support Vector Machine, AdaBoost, Random Forest, Gradient Boosting, Logistic Regression, and Multilayer Perceptron. The algorithms’ performances are validated using CO2 and historical meteorological data collected from a campus classroom with a variable occupancy rate. Simulation results showed that Multilayer Perceptron can strongly predict the volatile CO2 behavior and also outperforms other algorithms in terms of accuracy. Furthermore, a control strategy capable of modeling and detecting dynamic patterns of CO2 level is utilized to modulate the ventilation rate in real-time and also reduce the energy consumption. The proposed controller reduced the HVAC fan’s energy consumption by 51.4% and provided ventilation as needed per the ASHRAE standards.
               ",autonomous vehicle
10.1016/B978-0-12-820783-3.00013-0,journal,Handbook of Nanomaterials for Sensing Applications,sciencedirect,2021-12-31,sciencedirect,"Chapter 24: The brain-machine interface, nanosensor technology, and artificial intelligence: Their convergence with a novel frontier",https://api.elsevier.com/content/article/pii/B9780128207833000130,"
               A confluence of technological capabilities is creating an opportunity for machine learning and artificial intelligence (AI) to enable “smart” nanoengineered brain-machine interfaces (BMI). This new generation of technologies will be able to communicate with the brain in ways that support contextual learning and adaptation to change functional requirements. This applies to both invasive technologies aimed at restoring neurological function, as in the case of neural prosthesis, as well as noninvasive technologies enabled by signals such as electroencephalograph (EEG). Advances in computation, hardware, and algorithms that learn and adapt in a contextually dependent way will be able to leverage the capabilities that nanoengineering offers the design and functionality of BMI. We explore the enabling capabilities that these devices may exhibit, why they matter, and the state of the technologies necessary to build them. We also discuss a number of open technical challenges and problems that will need to be solved to achieve this.
            ",autonomous vehicle
10.1016/j.comcom.2021.07.025,journal,Computer Communications,sciencedirect,2021-10-01,sciencedirect,GraphNET: Graph Neural Networks for routing optimization in Software Defined Networks,https://api.elsevier.com/content/article/pii/S0140366421002887,"
                  In this paper, a graph neural net-based routing algorithm is designed which leverages global information from controller of a software-defined network to predict optimal path with minimum average delay between source and destination nodes in software-defined networks. Graph nets are used because of their generalization capability which allows the routing algorithm to scale across varying topologies, traffic schemes and changing conditions. A deep reinforcement learning framework is developed to train the Graph Neural Networks using prioritized experience replay from the experiences learnt by the controllers. The algorithm is tested on various small and large topologies in terms of packets successfully routed and average packet delay time. Experiments are performed to check robustness of routing algorithms to changes in network structure and effects of varying hyperparameters. The proposed algorithm shows impressive results when compared to q-routing and shortest path routing algorithm in terms of above experiments and is robust to varying graphical structure of the network.
               ",autonomous vehicle
10.1016/j.eswa.2021.114702,journal,Expert Systems with Applications,sciencedirect,2021-07-01,sciencedirect,Artificial intelligence applications in supply chain: A descriptive bibliometric analysis and future research directions,https://api.elsevier.com/content/article/pii/S0957417421001433,"
                  Today’s supply chains are very different from those of just a few years ago, and they continue to evolve within an extremely competitive economy. Dynamic supply chain processes require a technology that can cope with their increasing complexity. In recent years, several functional supply chain applications based on artificial intelligence (AI) have emerged, yet very few studies have addressed the applications of AI in supply chain processes. Machine learning, natural language processing, and robotics are all potential enablers of supply chain transformation. Aware of the potential advantages of AI implementation in supply chains and of the paucity of work done regarding it, we explore what researchers have done so far with respect to AI and what needs further exploration. We reviewed 136 research papers published between 1996 and 2020 from the Scopus database and provided a classification of the research material according to four critical structural dimensions (level of analytics, AI algorithms or techniques, sector or industry of application, and supply chain processes). This study is the first attempt to study the AI applications in SC from a process perspective and provides a decisional framework for adequate use of AI techniques in the different SC processes.
               ",autonomous vehicle
10.1016/B978-0-12-823337-5.00001-9,journal,Intelligence-Based Medicine,sciencedirect,2020-12-31,sciencedirect,Chapter 1: Basic Concepts of Artificial Intelligence,https://api.elsevier.com/content/article/pii/B9780128233375000019,"
               The definition of artificial intelligence (AI) is elucidated as there is sometimes confusion with terminologies like AI, machine and deep learning, analytics, and data science. The data-to-intelligence continuum concept is essential to understand in the framework of AI in medicine as data are the foundational layer of work in this domain. There are several methods of categorizing AI: weak versus strong; narrow versus general; and assisted, augmented, and autonomous (the latter as part of a human–machine intelligence continuum). The entire portfolio of AI is briefly presented: natural language processing, cognitive computing, machine and deep learning, robotics, and reinforcement learning. The analytics continuum, ranging from descriptive to cognitive, is also explained along with an analytics maturity model. Finally, AI in the context of neuroscience is particularly relevant as we head into an era of cognitive architecture in AI and as we aim to better understand how doctors think during their day-to-day clinical work.
            ",autonomous vehicle
10.1016/B978-0-12-823928-5.00038-4,journal,Medical Epigenetics,sciencedirect,2021-12-31,sciencedirect,Chapter 24: Machine learning in epigenetic diseases,https://api.elsevier.com/content/article/pii/B9780128239285000384,"
               Machine learning of DNA methylation patterns and other epigenetic phenomena possesses great potential in the prediction of disease. Machine learning has emerged as a powerful tool that enables the discovery of unknown features in the epigenome to predict phenotypes of interest and to suggest diagnoses and clinical courses. It has been successfully applied to select DNA methylation features to identify biomarkers for complex diseases and to predict treatment outcomes. Epigenetic classifiers can be used in combination with current tools to complement and assist in diagnosis, and several epigenetic biomarkers are currently used clinically. Applications of machine learning in clinical epigenetics hold forth the possibility of molecular diagnostics for specific diseases, complex conditions, or physiologic abnormalities.
            ",autonomous vehicle
10.1016/j.drudis.2021.01.013,journal,Drug Discovery Today,sciencedirect,2021-04-30,sciencedirect,Use of artificial intelligence to enhance phenotypic drug discovery,https://api.elsevier.com/content/article/pii/S1359644621000404,"
                  Research and development (R&D) productivity across the pharmaceutical industry has received close scrutiny over the past two decades, especially taking into consideration reports of attrition rates and the colossal cost for drug development. The respective merits of the two main drug discovery approaches, phenotypic and target based, have divided opinion across the research community, because each hold different advantages for identifying novel molecular entities with a successful path to the market. Nevertheless, both have low translatability in the clinic. Artificial intelligence (AI) and adoption of machine learning (ML) tools offer the promise of revolutionising drug development, and overcoming obstacles in the drug discovery pipeline. Here, we assess the potential of target-driven and phenotypic-based approaches and offer a holistic description of the current state of the field, from both a scientific and industry perspective. With the emerging partnerships between AI/ML and pharma still in their relative infancy, we investigate the potential and current limitations with a particular focus on phenotypic drug discovery. Finally, we emphasise the value of public–private partnerships (PPPs) and cross-disciplinary collaborations to foster innovation and facilitate efficient drug discovery programmes.
               ",autonomous vehicle
10.1016/j.ifacol.2020.12.2763,journal,IFAC-PapersOnLine,sciencedirect,2020-12-31,sciencedirect,Automating nut tightening using Machine Learning,https://api.elsevier.com/content/article/pii/S2405896320335266,"
                  At the Volvo Truck assembly plant the repetitive task of nut tightening is not ideal regarding quality and ergonomic. The solution to both these issues would be to significantly increase the level of automation. However, automating this specific station requires solutions to two specific problems. The first problem is to find and identify what nuts that need to be tightened, since they are not always on the same position for this highly customized product. The second problem is that the automated solution needs to accommodate the working space which is a moving assembly line with human operators. This paper investigates how these two problems ban be solved using machine learning and collaborative robots. A realistic mockup of the assembly station has been created at Stena Industry Innovation Laboratory (SII-Lab) where all the testing has been done.
                  The problem to identify the nuts to tighten is further complicated by the fact that some nuts are placed backwards for future further assembly which must be avoided. Therefore, the selected solution is to use supervised machine learning for object recognition. This way, the system can be trained to recognize both nuts that need to be tightened and those mounted backwards, and possible other objects needed. Tests have been conducted with different types of CNN (Convolutional Neural Network) algorithms. Results have been very successful, and the test setup has successfully managed to connect the whole task of identifying the correct nuts and move the collaborative robot to that specific position.
               ",autonomous vehicle
10.1016/j.zemedi.2018.11.002,journal,Zeitschrift für Medizinische Physik,sciencedirect,2019-05-31,sciencedirect,An overview of deep learning in medical imaging focusing on MRI,https://api.elsevier.com/content/article/pii/S0939388918301181,"What has happened in machine learning lately, and what does it mean for the future of medical image analysis? Machine learning has witnessed a tremendous amount of attention over the last few years. The current boom started around 2009 when so-called deep artificial neural networks began outperforming other established models on a number of important benchmarks. Deep neural networks are now the state-of-the-art machine learning models across a variety of areas, from image analysis to natural language processing, and widely deployed in academia and industry. These developments have a huge potential for medical imaging technology, medical data analysis, medical diagnostics and healthcare in general, slowly being realized. We provide a short overview of recent advances and some associated challenges in machine learning applied to medical image processing and image analysis. As this has become a very broad and fast expanding field we will not survey the entire landscape of applications, but put particular focus on deep learning in MRI. Our aim is threefold: (i) give a brief introduction to deep learning with pointers to core references; (ii) indicate how deep learning has been applied to the entire MRI processing chain, from acquisition to image retrieval, from segmentation to disease prediction; (iii) provide a starting point for people interested in experimenting and perhaps contributing to the field of deep learning for medical imaging by pointing out good educational resources, state-of-the-art open-source code, and interesting sources of data and problems related medical imaging.",autonomous vehicle
10.1016/j.conbuildmat.2020.119889,journal,Construction and Building Materials,sciencedirect,2020-11-10,sciencedirect,Machine learning prediction of mechanical properties of concrete: Critical review,https://api.elsevier.com/content/article/pii/S0950061820318948,"
                  Accurate prediction of the mechanical properties of concrete has been a concern since these properties are often required by design codes. The emergence of new concrete mixtures and applications has motivated researchers to pursue reliable models for predicting mechanical strength. Empirical and statistical models, such as linear and nonlinear regression, have been widely used. However, these models require laborious experimental work to develop, and can provide inaccurate results when the relationships between concrete properties and mixture composition and curing conditions are complex. To overcome such drawbacks, several Machine Learning (ML) models have been proposed as an alternative approach for predicting the mechanical strength of concrete. The present study examines ML models for forecasting the mechanical properties of concrete, including artificial neural networks, support vector machine, decision trees, and evolutionary algorithms. The application of each model and its performance are critically discussed and analyzed, thus identifying practical recommendations, current knowledge gaps, and needed future research.
               ",autonomous vehicle
10.1016/j.ymeth.2019.04.008,journal,Methods,sciencedirect,2019-08-15,sciencedirect,"Deep learning in bioinformatics: Introduction, application, and perspective in the big data era",https://api.elsevier.com/content/article/pii/S1046202318303256,"
                  Deep learning, which is especially formidable in handling big data, has achieved great success in various fields, including bioinformatics. With the advances of the big data era in biology, it is foreseeable that deep learning will become increasingly important in the field and will be incorporated in vast majorities of analysis pipelines. In this review, we provide both the exoteric introduction of deep learning, and concrete examples and implementations of its representative applications in bioinformatics. We start from the recent achievements of deep learning in the bioinformatics field, pointing out the problems which are suitable to use deep learning. After that, we introduce deep learning in an easy-to-understand fashion, from shallow neural networks to legendary convolutional neural networks, legendary recurrent neural networks, graph neural networks, generative adversarial networks, variational autoencoder, and the most recent state-of-the-art architectures. After that, we provide eight examples, covering five bioinformatics research directions and all the four kinds of data type, with the implementation written in Tensorflow and Keras. Finally, we discuss the common issues, such as overfitting and interpretability, that users will encounter when adopting deep learning methods and provide corresponding suggestions. The implementations are freely available at https://github.com/lykaust15/Deep_learning_examples.
               ",autonomous vehicle
10.1016/B978-0-12-820352-1.00171-1,journal,Reference Module in Materials Science and Materials Engineering,sciencedirect,2021-12-31,sciencedirect,Machine Learning Guided Optimization of Biomimetic Polymeric Lattice Structures,https://api.elsevier.com/content/article/pii/B9780128203521001711,"
               In this article, we report that, machine learning, an artificial intelligent technique, is used to optimize biomimetic rods and lattice structures. Various structures available in nature such as plant stems and roots that exhibit better buckling resistance are mimicked and modeled using finite element analysis, to obtain a training dataset. For validating the finite element analysis, uniaxial compression to buckling of additive manufactured biomimetic rods using a polymeric ink is performed. These model results are then formed into a dataset. Forward design and data filtering are conducted by machine learning to optimize the biomimetic rods from the dataset. The results show that the machine learning assisted rod designs have 150% better buckling resistance than all the rods in the training dataset, i.e., better than the nature’s counterparts. These optimal rods can be used in designing structures with superior buckling resistance such as in bridges, buildings, lattice structures, etc. Using these biomimetic rods, lattice structures with better structural performance are manufactured. While lattice unit cells such as octahedron, tetrahedron, octet, etc., have been previously proposed for lightweight structures, it is plausible that more optimal unit cells exist which might perform better than the existing counterparts. Machine learning technique is used to discover new optimal cells. Uniaxial compression tests using ANSYS are performed to form a dataset, which is used to train machine learning algorithms and form predictive model. The predictive model is then used to identify a total of 20 optimal symmetric unit cells. These new unit cells show 51%–57% higher capacity than octet cell. Particularly, if the porous biomimetic rods are used to construct the unit cells, an additional 130%–160% increase in buckling resistance is achieved. New lattice unit cells exhibit a buckling load of 261%–308% higher than the classical octet unit cell. Sandwich structures manufactured by 3D printing these optimal symmetric unit cells show 13%–35% higher flexural strength. This study opens up new opportunities to design high-performance metamaterials combining biomimetics and machine learning.
            ",autonomous vehicle
10.1016/j.ymssp.2021.107915,journal,Mechanical Systems and Signal Processing,sciencedirect,2021-11-30,sciencedirect,Machine learning based frequency modelling,https://api.elsevier.com/content/article/pii/S0888327021003101,"
                  Detection of cracks in structures has always been an important research topic in the industrial domain closely associated with aerospace, mechanical, marine and civil engineering. The presence of the cracks alters the dynamic response properties. Hence, it becomes crucial to locate these cracks in the structures to avoid any catastrophic failures and maintain structural integrity and performance. The study's objective is to propose two distinct statistical procedures for conducting the machine learning experiment for modelling the frequency and show the effect of experiment design on the results. In the study, the predictive performance of machine learning models and their ensembles is compared within each experiment design and between two experimental designs for the task of prediction of first six natural frequencies of a fixed ended cracked beam. The study highlights the significance of more than one experimental design to reduce the confirmation bias in the research and discusses the proposed methods' generalizability over the different modelling constraints and modelling parameters. The study also discusses a real-world implementation of the learned machine learning models from the perspective of Bayesian optimization.
               ",autonomous vehicle
10.1016/B978-0-12-822226-3.00008-8,journal,Trends in Deep Learning Methodologies,sciencedirect,2021-12-31,sciencedirect,Chapter 8: Deep similarity learning for disease prediction,https://api.elsevier.com/content/article/pii/B9780128222263000088,"
               Deep learning is helping many medical experts as well as researchers to discover important insights from healthcare data and provide better medical facilities. Patients' risk assessment of developing a certain disease and providing personalized healthcare is an important research area. The current study presents a comprehensive review of deep learning architectures and how they can be used in learning representation and similarity between a pair of patients for disease prediction. We call this model a deep similarity learning model for disease prediction. As a demonstration of functionality, the deep learning architectures are trained on electronic health records to perform disease prediction. The experimental results obtained encourage us to use one of the suitable models in calculating similarity as future work.
            ",autonomous vehicle
10.1016/j.cosrev.2021.100376,journal,Computer Science Review,sciencedirect,2021-05-31,sciencedirect,"Strategies based on various aspects of clustering in wireless sensor networks using classical, optimization and machine learning techniques: Review, taxonomy, research findings, challenges and future directions",https://api.elsevier.com/content/article/pii/S1574013721000162,"
                  Wireless Sensor Networks (WSNs) have attracted various academic researchers, engineers, science, and technology communities. This attraction is due to their broad research areas such as energy efficiency, data communication, coverage, connectivity, load balancing, security, reliability, scalability, and network lifetime. Researchers are looking towards cost-effective approaches to improve the existing solutions that reveal novel schemes, methods, concepts, protocols, and algorithms in the desired domain. Generally, review studies provide complete, easy access or solution to these concepts. Considering this as a driving force and the impact of clustering on the deterioration of energy consumption in wireless sensor networks, this review focus on clustering methods based on different aspects. This study’s significant contribution is to provide a brief review in the field of clustering in wireless sensor networks based on three different categories, such as classical, optimization, and machine learning techniques. For each of these categories, various performance metrics and parameters are provided, and a comparative assessment of the corresponding aspects like cluster head selection, routing protocols, reliability, security, and unequal clustering are discussed. Various advantages, limitations, applications of each method, research gaps, challenges, and research directions are considered in this study, motivating the researchers to carry out further research by providing relevant information in cluster-based wireless sensor networks.
               ",autonomous vehicle
10.1016/j.neucom.2021.03.085,journal,Neurocomputing,sciencedirect,2021-08-11,sciencedirect,A hybrid deep segmentation network for fundus vessels via deep-learning framework,https://api.elsevier.com/content/article/pii/S0925231221004719,"
                  High-precision segmentation of fundus vessels is a fundamental step in the diagnosis and treatment of fundus diseases, in which both thick and thin vessels are important features for symptom detection. With the rapid development of artificial intelligence, the deep convolutional neural network (DCNN) has been widely applied into image analysis of fundus vessels. Nevertheless, due to the imbalanced ratio between thick and thin vessels, the existing segmentation methods are weak in the task of microvessel extraction from fundus images. To address this problem, this paper proposes a new hybrid deep image segmentation method for fundus vessels that consists of a multitask segmentation network and a fusion network. For the proposed method, a multitask segmentation network is developed to precisely segment both thick vessels and thin vessels from fundus images separately. In addition, an effective loss function is designed to adapt to the two different vessel segmentation tasks and ultimately solve the imbalanced ratio between these two vessels. Furthermore, an improved U-net network model is proposed to serve as the basic segmentation network to ensure the segmentation performance of the multitask segmentation network. Together with these networks, a fusion network is also proposed to fuse these two kinds of blood vessels to obtain the fusion images as the final segmentation results of fundus vessels. The proposed segmentation method is validated on many different public data sets of fundus images, such as DRIVE, STARE and CHASE_DB1. Experimental results show that the proposed method obtains a better segmentation performance on fundus images and acquires a higher recall, F_1 value, and accuracy than other advanced segmentation methods.
               ",autonomous vehicle
10.1016/j.cogsys.2018.08.023,journal,Cognitive Systems Research,sciencedirect,2018-12-31,sciencedirect,Deep learning: Evolution and expansion,https://api.elsevier.com/content/article/pii/S1389041717303546,"
                  This paper historically attempts to map the significant success of deep neural networks in notably varied classification problems and application domains with near human-level performance. The paper also addresses the various doubts surrounding the acceptance of deep learning as a science of future. The manuscript attempts to unveil the hidden capabilities of deep neural networks in enabling machines perform the human way tasks which can be learned through what we call observation and experience.
               ",autonomous vehicle
10.1016/j.jcsr.2021.106682,journal,Journal of Constructional Steel Research,sciencedirect,2021-07-31,sciencedirect,Capacity prediction of cold-formed stainless steel tubular columns using machine learning methods,https://api.elsevier.com/content/article/pii/S0143974X21001644,"
                  Current methods of designing cold-formed stainless-steel tubular columns provide a series of design formulae for columns to adapt to various grades of materials and failure modes. In this study, machine learning algorithms were used to establish a unified capacity prediction method. Test data on cold-formed stainless-steel tubular columns were collected from the literature, and Pearson correlation analysis was performed to establish the relations among the design parameters. Seven classical machine learning algorithms were introduced and developed to predict column capacity in a variety of cases used for the analysis. The results show that the random forest performs the best, followed by another gradient boosted tree-based ensemble algorithm, XGBoost. The non-dimensional slenderness of the member and of the cross-section (denoted as comprehensive parameters) are the two key parameters for capacity prediction in the current design methods, and are used as the feature input to acquire the base machine learning model. More models are evaluated by adding one or more material and geometric variables into the feature input. The analysis results show that the performance of the random forest algorithm can be significantly improved by considering the comprehensive parameters together with the member slenderness, ratio of tensile strength to yield strength, and strain hardening exponent. The predictions of the machine learning-based method were compared with those of the current design method in Eurocode. It is concluded that the proposed unified machine learning model provides more accurate results owing to the consideration of the diversities of both material properties and member failure modes.
               ",autonomous vehicle
10.1016/j.watres.2021.117666,journal,Water Research,sciencedirect,2021-10-15,sciencedirect,Machine learning in natural and engineered water systems,https://api.elsevier.com/content/article/pii/S0043135421008617,"
                  Water resources of desired quality and quantity are the foundation for human survival and sustainable development. To better protect the water environment and conserve water resources, efficient water management, purification, and transportation are of critical importance. In recent years, machine learning (ML) has exhibited its practicability, reliability, and high efficiency in numerous applications; furthermore, it has solved conventional and emerging problems in both natural and engineered water systems. For example, ML can predict various water quality indicators in situ and real-time by considering the complex interactions among water-related variables. ML approaches can also solve emerging pollution problems with proven rules or universal mechanisms summarized from the related research. Moreover, by applying image recognition technology to analyze the relationships between image information and physicochemical properties of the research object, ML can effectively identify and characterize specific contaminants. In view of the bright prospects of ML, this review comprehensively summarizes the development of ML applications in natural and engineered water systems. First, the concept and modeling steps of ML are briefly introduced, including data preparation, algorithm selection and model evaluation. In addition, comprehensive applications of ML in recent studies, including predicting water quality, mapping groundwater contaminants, classifying water resources, tracing contaminant sources, and evaluating pollutant toxicity in natural water systems, as well as modeling treatment techniques, assisting characterization analysis, purifying and distributing drinking water, and collecting and treating sewage water in engineered water systems, are summarized. Finally, the advantages and disadvantages of commonly used algorithms are analyzed according to their structures and mechanisms, and recommendations on the selection of ML algorithms for different studies, as well as prospects on the application and development of ML in water science are proposed. This review provides references for solving a wider range of water-related problems and brings further insights into the intelligent development of water science.
               ",autonomous vehicle
10.1016/j.procs.2015.02.016,journal,Procedia Computer Science,sciencedirect,2015-12-31,sciencedirect,Distributed Optimization of Solar Micro-grid Using Multi Agent Reinforcement Learning,https://api.elsevier.com/content/article/pii/S1877050915000800,"In the distributed optimization of micro-grid, we consider grid connected solar micro-grid system which contains a local consumer, a solar photovoltaic system and a battery. The consumer as an agent continuously interacts with the environment and learns to take optimal actions. Each agent uses a model-free reinforcement learning algorithm, namely Q Learning, to optimize the battery scheduling in dynamic environment of load and available solar power. Multiple agents sense the states of the environment components and make collective decisions about how to respond to randomness in load, intermittent solar power using a Multi-Agent Reinforcement Learning algorithm, called Coordinated Q Learning (CQL). The goals of each agent are to increase the utility of the battery and solar power in order to achieve the long term objective of reducing the power consumption from grid.",autonomous vehicle
10.1016/j.arr.2018.11.003,journal,Ageing Research Reviews,sciencedirect,2019-01-31,sciencedirect,Artificial intelligence for aging and longevity research: Recent advances and perspectives,https://api.elsevier.com/content/article/pii/S156816371830240X,"The applications of modern artificial intelligence (AI) algorithms within the field of aging research offer tremendous opportunities. Aging is an almost universal unifying feature possessed by all living organisms, tissues, and cells. Modern deep learning techniques used to develop age predictors offer new possibilities for formerly incompatible dynamic and static data types. AI biomarkers of aging enable a holistic view of biological processes and allow for novel methods for building causal models—extracting the most important features and identifying biological targets and mechanisms. Recent developments in generative adversarial networks (GANs) and reinforcement learning (RL) permit the generation of diverse synthetic molecular and patient data, identification of novel biological targets, and generation of novel molecular compounds with desired properties and geroprotectors. These novel techniques can be combined into a unified, seamless end-to-end biomarker development, target identification, drug discovery and real world evidence pipeline that may help accelerate and improve pharmaceutical research and development practices. Modern AI is therefore expected to contribute to the credibility and prominence of longevity biotechnology in the healthcare and pharmaceutical industry, and to the convergence of countless areas of research.",autonomous vehicle
10.1016/j.asoc.2020.106913,journal,Applied Soft Computing,sciencedirect,2021-02-28,sciencedirect,Robot learning through observation via coarse-to-fine grained video summarization,https://api.elsevier.com/content/article/pii/S1568494620308516,"
                  Learning human daily behavior is important for enabling robots to perform tasks and assist people. However, most prior work either requires specific sensors for capturing data or heavily relies on prior knowledge of human motion, which can be difficult to obtain. To alleviate the above problems, we propose a novel pipeline for robots to learn human behavior based on coarse-to-fine video summarization using a single Kinect camera. Specifically, the robot first retrieves information of general interest followed by a task-specific content retrieval, then focuses on fine-grained motion clips of human behavior, and guides itself by using an object-centric learning method to complete the desired task. Our work has three unique advantages: (1) it enables the robot to effectively capture granularity hierarchies of human behavior which efficiently exploits multi-stage information while alleviating disturbances and redundancies in visual data; (2) it obtains knowledge by focusing on object movements in summarized motion clips which does not require any prior knowledge of human motion; (3) it only requires a single Kinect sensor for the robot to learn human behavior which is fully accessible and easy to equip. Experiments in an office environment were performed to validate the efficiency and effectiveness of the proposed framework, and the results indicate that this approach exhibits good learning efficacy for the robot to understand human behavior and learn to perform tasks.
               ",autonomous vehicle
10.1016/B978-0-12-820239-5.00002-4,journal,Precision Medicine and Artificial Intelligence,sciencedirect,2021-12-31,sciencedirect,"Chapter 1: History, current status, and future directions of artificial intelligence",https://api.elsevier.com/content/article/pii/B9780128202395000024,"
               Artificial intelligence (AI) as a technology concept is making a major impact on a wide range of industries and sectors. This is largely attributed to technical advancements in machine and especially deep learning methodologies fueled by improved computational capabilities which have led to sophisticated approaches in applying AI to various scenarios. These AI applications aim to improve productivity, decrease cost, and comprehend the ever-increasing volumes of data available to ultimately provide actionable insights. Included in this paradigm shift is medicine, where AI is beginning to enable clinical assistance, decision support, improved management and accelerated scientific discovery and development.
            ",autonomous vehicle
10.1016/j.patcog.2020.107763,journal,Pattern Recognition,sciencedirect,2021-04-30,sciencedirect,Compact class-conditional domain invariant learning for multi-class domain adaptation,https://api.elsevier.com/content/article/pii/S0031320320305665,"
                  Neural network-based models have recently shown excellent performance in various kinds of tasks. However, a large amount of labeled data is required to train deep networks, and the cost of gathering labeled training data for every kind of domain is prohibitively expensive. Domain adaptation tries to solve this problem by transferring knowledge from labeled source domain data to unlabeled target domain data. Previous research tried to learn domain-invariant features of source and target domains to address this problem, and this approach has been used as a key concept in various methods. However, domain-invariant features do not mean that a classifier trained on source data can be directly applied to target data because it does not guarantee that data distribution of the same classes will be aligned across two domains. In this paper, we present novel generalization upper bounds for domain adaptation that motivates the need for class-conditional domain invariant learning. Based on this theoretical framework, we then propose a class-conditional domain invariant learning method that can learn a feature space in which features in the same class are expected to be mapped nearby. We empirically experimented that our model showed state-of-the-art performance on standard datasets and showed effectiveness by visualization of latent space.
               ",autonomous vehicle
10.1016/j.cobeha.2019.04.006,journal,Current Opinion in Behavioral Sciences,sciencedirect,2019-10-31,sciencedirect,Toward evolutionary and developmental intelligence,https://api.elsevier.com/content/article/pii/S2352154618302225,"Given the phenomenal advances in artificial intelligence in specific domains like visual object recognition and game playing by deep learning, expectations are rising for building artificial general intelligence (AGI) that can flexibly find solutions in unknown task domains. One approach to AGI is to set up a variety of tasks and design AI agents that perform well in many of them, including those the agent faces for the first time. One caveat for such an approach is that the best performing agent may be just a collection of domain-specific AI agents switched for a given domain. Here we propose an alternative approach of focusing on the process of acquisition of intelligence through active interactions in an environment. We call this approach evolutionary and developmental intelligence (EDI). We first review the current status of artificial intelligence, brain-inspired computing and developmental robotics and define the conceptual framework of EDI. We then explore how we can integrate advances in neuroscience, machine learning, and robotics to construct EDI systems and how building such systems can help us understand animal and human intelligence.",autonomous vehicle
10.1016/j.egyai.2021.100054,journal,Energy and AI,sciencedirect,2021-06-30,sciencedirect,Predicting entropy and heat capacity of hydrocarbons using machine learning,https://api.elsevier.com/content/article/pii/S2666546821000082,"Chemical substances are essential in all aspects of human life, and understanding their properties is essential for developing chemical systems. The properties of chemical species can be accurately obtained by experiments or ab initio computational calculations; however, these are time-consuming and costly. In this work, machine learning models (ML) for estimating entropy, S, and constant pressure heat capacity, Cp, at 298.15 K, are developed for alkanes, alkenes, and alkynes. The training data for entropy and heat capacity are collected from the literature. Molecular descriptors generated using alvaDesc software are used as input features for the ML models. Support vector regression (SVR), v-support vector regression (v-SVR), and random forest regression (RFR) algorithms were trained with K-fold cross-validation on two levels. The first level assessed the models' performance, and the second level generated the final models. Between the three ML models chosen, SVR shows better performance on the test dataset. The SVR model was then compared against traditional Benson's group additivity to illustrate the advantages of using the ML model. Finally, a sensitivity analysis is performed to find the most critical descriptors in the property estimations.",autonomous vehicle
10.1016/j.measurement.2021.110146,journal,Measurement,sciencedirect,2021-12-31,sciencedirect,Identification of inlet pipe blockage level in centrifugal pump over a range of speeds by deep learning algorithm using multi-source data,https://api.elsevier.com/content/article/pii/S0263224121010654,"
                  This paper focusses on the identification of the blockage fault in the inlet pipe of centrifugal pump over a range of speed using data obtained from different types of sensors. Acceleration, pressure and motor line current signatures taken using accelerometer, pressure transducer and current probe, respectively, and are used for identification of the pipe blockage level. Methodology is given for the blockage detection based on the multiclass classification using the deep learning algorithm at different blockage levels and speed of rotation of the pump. Importance of the multi-source data collection is emphasized based on the obtained results. Effect of the motor speed is also discussed when considered as an input feature to the classifier. It is observed that the use of combinations of different types of sensors help to identify the blockage level with better accuracy (close to 100% for many combinations). Blockage level prediction at each speed separately, is also given, which can be used for the fault diagnosis of single speed pumps. Finally, the performance of the classifier is tested using some unknown data (different from the data at training speed) to check the blockage prediction accuracy of the classifier.
               ",autonomous vehicle
10.1016/j.autcon.2020.103372,journal,Automation in Construction,sciencedirect,2020-12-31,sciencedirect,Artificial intelligence-empowered pipeline for image-based inspection of concrete structures,https://api.elsevier.com/content/article/pii/S0926580520309523,"
                  Inspection of civil infrastructure is a major challenge to engineers due to the limitations in existing practice, which are as laborious, time-consuming and prone to error. To address these issues, we have applied deep learning for image-based inspection of concrete defects of civil infrastructure, and have established an artificial intelligence-empowered inspection pipeline methodology. This innovative approach comprises anomaly detection, anomaly extraction and defect classification. The anomaly detection and extraction are used to identify defect regions from the enormous volume of image datasets, which used to be the common challenges encountered in automated visual inspections. The search space of defects is substantially reduced, i.e., at least 60% of the original volume of image datasets, with an average hit rate of ~88.7% and an average false alarm rate of ~14.2%. Following that, deep learning-based classifiers are used to categorize defects into appropriate classes. The assessment results show that the proposed inspection pipeline exhibits great capability in detecting, extracting and classifying defects subjected to various environmental and operational conditions, including lighting condition, camera distance and capturing angle, with an average testing accuracy of 95.6%.
               ",autonomous vehicle
10.1016/j.comnet.2020.107684,journal,Computer Networks,sciencedirect,2021-02-11,sciencedirect,The two-phase scheduling based on deep learning in the Internet of Things,https://api.elsevier.com/content/article/pii/S1389128620312925,"
                  With the growth of data generation speed and its huge volume, the cloud-based infrastructure alone does not meet the needs of the Internet of Things (IoT), thereby leading to inefficiency. By combining fog and cloud computing on the IoT, some processing occurs near the data generation location with a higher speed and without needing a large bandwidth. Cloud and fog computing requires proper management in scheduling, increasing resource efficiency, and reducing consumption costs. In this article, a two-phase scheduling algorithm is presented based on deep learning methods in the field of IoT. The first phase is to decide on the location of the task execution using the clustering method. The second phase involves scheduling the task according to the execution location. In the clustering section, three ideas based on the Self-Organizing Map (SOM) clustering method have been proposed. In the first and second ideas, the SOM and hierarchical SOM are used to cluster the features of the tasks received from the IoT layer. In the third idea, the feature is extracted, and its dimensions are reduced using the Autoencoder, which is one of the deep learning methods, after which clustering is done. After scheduling each cluster's tasks, comparing the methods presented in the article, it is shown that the feature extraction using deep learning can improve clustering in such a way to reduce the missed rate of tasks in the cloud and fog, as well as their costs.
               ",autonomous vehicle
10.1016/j.eswa.2020.114195,journal,Expert Systems with Applications,sciencedirect,2021-04-01,sciencedirect,A state-of-the-art review on mobile robotics tasks using artificial intelligence and visual data,https://api.elsevier.com/content/article/pii/S095741742030926X,"
                  Nowadays, the field of mobile robotics has experienced an important evolution and these robots are more commonly proposed to solve different tasks autonomously. The use of visual sensors has played an important role in mobile robotics tasks during the past few years due to the advances in computer vision hardware and algorithms. It is worth remarking the use of AI tools to solve a variety of problems in mobile robotics based on the use of images either as the only source of information or combining them with other sensors such as laser or GPS. The improvement of the autonomy of mobile robots has attracted the attention of the scientific community. A considerable amount of works have been proposed over the past few years, leading to an extensive variety of approaches. Building a robust model of the environment (mapping), estimating the position within the model (localization) and controlling the movement of the robot from one place to another (navigation) are important abilities that any mobile robot must have. Considering this, this review focuses on analyzing these problems; how researchers have addressed them by means of AI tools and visual information; and how these approaches have evolved in recent years. This topic is currently open and a large number of works can be found in the related literature. Therefore, it can be of interest making an analysis of the current state of the topic. From this review, we can conclude that AI has provided robust solutions to some specific tasks in mobile robotics, such as information retrieval from scenes, mapping, localization and exploration. However, it is worth continuing to develop this line of research to find more integral solutions to the navigation problem so that mobile robots can increase their autonomy in large, complex and heterogeneous environments.
               ",autonomous vehicle
10.1016/j.measurement.2020.107929,journal,Measurement,sciencedirect,2020-10-15,sciencedirect,"Deep learning for prognostics and health management: State of the art, challenges, and opportunities",https://api.elsevier.com/content/article/pii/S026322412030467X,"
                  Improving the reliability of engineered systems is a crucial problem in many applications in various engineering fields, such as aerospace, nuclear energy, and water declination industries. This requires efficient and effective system health monitoring methods, including processing and analyzing massive machinery data to detect anomalies and performing diagnosis and prognosis. In recent years, deep learning has been a fast-growing field and has shown promising results for Prognostics and Health Management (PHM) in interpreting condition monitoring signals such as vibration, acoustic emission, and pressure due to its capacity to mine complex representations from raw data. This paper provides a systematic review of state-of-the-art deep learning-based PHM frameworks. It emphasizes on the most recent trends within the field and presents the benefits and potentials of state-of-the-art deep neural networks for system health management. In addition, limitations and challenges of the existing technologies are discussed, which leads to opportunities for future research.
               ",autonomous vehicle
10.1016/j.jcin.2019.04.048,journal,JACC: Cardiovascular Interventions,sciencedirect,2019-07-22,sciencedirect,Impact of Artificial Intelligence on Interventional Cardiology: From Decision-Making Aid to Advanced Interventional Procedure Assistance,https://api.elsevier.com/content/article/pii/S1936879819310957,"Access to big data analyzed by supercomputers using advanced mathematical algorithms (i.e., deep machine learning) has allowed for enhancement of cognitive output (i.e., visual imaging interpretation) to previously unseen levels and promises to fundamentally change the practice of medicine. This field, known as “artificial intelligence” (AI), is making significant progress in areas such as automated clinical decision making, medical imaging analysis, and interventional procedures, and has the potential to dramatically influence the practice of interventional cardiology. The unique nature of interventional cardiology makes it an ideal target for the development of AI-based technologies designed to improve real-time clinical decision making, streamline workflow in the catheterization laboratory, and standardize catheter-based procedures through advanced robotics. This review provides an introduction to AI by highlighting its scope, potential applications, and limitations in interventional cardiology.",autonomous vehicle
10.1016/B978-0-12-818576-6.00018-6,journal,Artificial Intelligence to Solve Pervasive Internet of Things Issues,sciencedirect,2021-12-31,sciencedirect,Chapter 18: Machine Learning for Optical Communication to Solve Pervasive Issues of Internet of Things,https://api.elsevier.com/content/article/pii/B9780128185766000186,"
               Over the past decade, machine learning (ML) is being used as a new direction of innovation to transform optical communication systems. Due to the significant improvement and advancement in computing capabilities of ML techniques, it helps in issues related to optical communication and networking. In this chapter we describe numerous ML-based approaches to deal with various issues at physical and network layers. We also provide an outline of existing ML applications in optical communication systems with an emphasis on physical and network layer.
            ",autonomous vehicle
10.1016/j.iot.2021.100365,journal,Internet of Things,sciencedirect,2021-06-30,sciencedirect,Machine learning approaches to IoT security: A systematic literature review,https://api.elsevier.com/content/article/pii/S2542660521000093,"
                  With the continuous expansion and evolution of IoT applications, attacks on those IoT applications continue to grow rapidly. In this systematic literature review (SLR) paper, our goal is to provide a research asset to researchers on recent research trends in IoT security. As the main driver of our SLR paper, we proposed six research questions related to IoT security and machine learning. This extensive literature survey on the most recent publications in IoT security identified a few key research trends that will drive future research in this field. With the rapid growth of large scale IoT attacks, it is important to develop models that can integrate state of the art techniques and technologies from big data and machine learning. Accuracy and efficiency are key quality factors in finding the best algorithms and models to detect IoT attacks in real or near real-time
               ",autonomous vehicle
10.1016/j.suscom.2021.100511,journal,Sustainable Computing: Informatics and Systems,sciencedirect,2021-06-30,sciencedirect,Machine intelligence approach: To solve load balancing problem with high quality of service performance for multi-controller based Software Defined Network,https://api.elsevier.com/content/article/pii/S2210537921000044,"
                  In this paper, we have proposed a DRL based method to obtain the route based on an optimized load of SDN which is based on self-learning of human intelligence. In this proposal, the Bio-Inspired RBM is used for Bio-Inspired Deep Belief Architecture (BDBA) for implementing deep learning to obtain the optimized route. This Bio-Inspired RBM has two parts one is simple RBM and another part is inspired by self-learning of human intelligence based on emotion learning of the limbic system of the brain. Every Bio-Inspired RBM is fined tune using the reward function R which captures the environmental dynamics in the form of network policies.
                  Software Defined Network (SDN) concept resolves several problems of network infrastructure to decouple the responsibilities of the control plane and data plane. The single controller improves control on the network but decreases the reliability of the system in the case of failure of the controller. The distributed controller improves reliability and also reduces system failure. The route optimization is a big challenge in distributed SDN. Some route optimization techniques have been proposed which requires some prior knowledge. Deep Reinforcement Learning (DRL) is one of the techniques for route optimization which does not require any prior knowledge and runs in real-time. This technique learns from environment dynamics and optimizes anonymously. The high demand for internet usages and business activities using distributed SDN also facing complex problems that can be resolved using the self-learning methodology of human intelligence. Self-learning of human intelligence plays a dominant role in making complex decisions for any sudden problem.
               ",autonomous vehicle
10.1016/B978-0-12-820488-7.00021-9,journal,Computing in Communication Networks,sciencedirect,2020-12-31,sciencedirect,Chapter 8: Machine learning,https://api.elsevier.com/content/article/pii/B9780128204887000219,"
               In the last few years, especially thanks to the recent advancements in the field of Deep Learning, Machine Learning has drawn a lot of attention. One of the main driving factors of the machine learning hype is related to the fact that it offers a unified framework for introducing intelligent decision-making into many domains. In the following chapters, we will introduce examples of possible applications of machine learning to networking scenarios. Here we will lay the foundation to start diving into the machine learning world. We start by discussing various categories of machine learning algorithms. Then we introduce the needed mathematical notation. Finally, we introduce and discuss the most common algorithms for supervised learning and reinforcement learning.
            ",autonomous vehicle
10.1016/j.neunet.2019.09.012,journal,Neural Networks,sciencedirect,2019-12-31,sciencedirect,A survey of adaptive resonance theory neural network models for engineering applications,https://api.elsevier.com/content/article/pii/S0893608019302734,"
                  This survey samples from the ever-growing family of adaptive resonance theory (ART) neural network models used to perform the three primary machine learning modalities, namely, unsupervised, supervised and reinforcement learning. It comprises a representative list from classic to contemporary ART models, thereby painting a general picture of the architectures developed by researchers over the past 30 years. The learning dynamics of these ART models are briefly described, and their distinctive characteristics such as code representation, long-term memory, and corresponding geometric interpretation are discussed. Useful engineering properties of ART (speed, configurability, explainability, parallelization and hardware implementation) are examined along with current challenges. Finally, a compilation of online software libraries is provided. It is expected that this overview will be helpful to new and seasoned ART researchers.
               ",autonomous vehicle
10.1016/j.rec.2020.12.008,journal,Revista Española de Cardiología (English Edition),sciencedirect,2021-07-31,sciencedirect,A new approach to modelling in adult congenital heart disease: artificial intelligence,https://api.elsevier.com/content/article/pii/S1885585720305466,,autonomous vehicle
10.1016/j.asoc.2015.09.017,journal,Applied Soft Computing,sciencedirect,2015-12-31,sciencedirect,Application of reinforcement learning for security enhancement in cognitive radio networks,https://api.elsevier.com/content/article/pii/S156849461500589X,"
                  Cognitive radio network (CRN) enables unlicensed users (or secondary users, SUs) to sense for and opportunistically operate in underutilized licensed channels, which are owned by the licensed users (or primary users, PUs). Cognitive radio network (CRN) has been regarded as the next-generation wireless network centered on the application of artificial intelligence, which helps the SUs to learn about, as well as to adaptively and dynamically reconfigure its operating parameters, including the sensing and transmission channels, for network performance enhancement. This motivates the use of artificial intelligence to enhance security schemes for CRNs. Provisioning security in CRNs is challenging since existing techniques, such as entity authentication, are not feasible in the dynamic environment that CRN presents since they require pre-registration. In addition these techniques cannot prevent an authenticated node from acting maliciously. In this article, we advocate the use of reinforcement learning (RL) to achieve optimal or near-optimal solutions for security enhancement through the detection of various malicious nodes and their attacks in CRNs. RL, which is an artificial intelligence technique, has the ability to learn new attacks and to detect previously learned ones. RL has been perceived as a promising approach to enhance the overall security aspect of CRNs. RL, which has been applied to address the dynamic aspect of security schemes in other wireless networks, such as wireless sensor networks and wireless mesh networks can be leveraged to design security schemes in CRNs. We believe that these RL solutions will complement and enhance existing security solutions applied to CRN To the best of our knowledge, this is the first survey article that focuses on the use of RL-based techniques for security enhancement in CRNs.
               ",autonomous vehicle
10.1016/j.adhoc.2019.101913,journal,Ad Hoc Networks,sciencedirect,2019-10-31,sciencedirect,Machine learning for wireless communications in the Internet of Things: A comprehensive survey,https://api.elsevier.com/content/article/pii/S1570870519300812,"
                  The Internet of Things (IoT) is expected to require more effective and efficient wireless communications than ever before. For this reason, techniques such as spectrum sharing, dynamic spectrum access, extraction of signal intelligence and optimized routing will soon become essential components of the IoT wireless communication paradigm. In this vision, IoT devices must be able to not only learn to autonomously extract spectrum knowledge on-the-fly from the network but also leverage such knowledge to dynamically change appropriate wireless parameters (e.g., frequency band, symbol modulation, coding rate, route selection, etc.) to reach the network’s optimal operating point. Given that the majority of the IoT will be composed of tiny, mobile, and energy-constrained devices, traditional techniques based on a priori network optimization may not be suitable, since (i) an accurate model of the environment may not be readily available in practical scenarios; (ii) the computational requirements of traditional optimization techniques may prove unbearable for IoT devices. To address the above challenges, much research has been devoted to exploring the use of machine learning to address problems in the IoT wireless communications domain. The reason behind machine learning’s popularity is that it provides a general framework to solve very complex problems where a model of the phenomenon being learned is too complex to derive or too dynamic to be summarized in mathematical terms.
                  This work provides a comprehensive survey of the state of the art in the application of machine learning techniques to address key problems in IoT wireless communications with an emphasis on its ad hoc networking aspect. First, we present extensive background notions of machine learning techniques. Then, by adopting a bottom-up approach, we examine existing work on machine learning for the IoT at the physical, data-link and network layer of the protocol stack. Thereafter, we discuss directions taken by the community towards hardware implementation to ensure the feasibility of these techniques. Additionally, before concluding, we also provide a brief discussion of the application of machine learning in IoT beyond wireless communication. Finally, each of these discussions is accompanied by a detailed analysis of the related open problems and challenges.
               ",autonomous vehicle
10.1016/B978-0-12-820604-1.00011-X,journal,Computational Intelligence and Its Applications in Healthcare,sciencedirect,2020-12-31,sciencedirect,11: Alzheimer’s disease classification using deep learning,https://api.elsevier.com/content/article/pii/B978012820604100011X,"
               Computational intelligence (CI) is an emerging field that deals with the design of computer systems replicating the function of the brain. Deep learning is one of the techniques of CI that has gained attention among several learning techniques, since it encapsulates the neural networks with automatic feature extraction stages, in turn resulting in more efficient performance. Health care is a popular research area that extensively utilizes the technique of deep learning in various stages of diagnosis. Alzheimer’s disease (AD) is one of the most challenging diseases, requiring an early and accurate diagnosis in order to extend an effective treatment. Classification of the AD data is the initial and crucial stage in treatment, but this stage is prone to many misclassified cases due to the high degree of similarity within the brain images. This led to the idea of incorporating deep learning in the classification of brain images, since it had attractive results in image classification, even outperforming human skills. The chapter discusses the relevance of deep learning in the diagnosis and treatment of AD. Starting from the concept of CI, the chapter is framed to deliver a clear-cut overview of the technique of deep learning and the benefits of using deep learning in the diagnosis and treatment of AD. Numerous works have been initiated for this purpose and the constantly increasing number of projects in this area illustrates the importance of utilizing CI for AD diagnosis, which is also briefly analyzed in this chapter.
            ",autonomous vehicle
10.1016/j.swevo.2021.100840,journal,Swarm and Evolutionary Computation,sciencedirect,2021-04-30,sciencedirect,A cooperative coevolution framework for evolutionary learning and instance selection,https://api.elsevier.com/content/article/pii/S2210650221000018,"
                  Data science becomes an emerging topic in developing computational and artificial intelligence techniques due to the rapid growth of information and data over past decade. Data-driven evaluation, through scanning data to validate the performance of a method, is the crux of effectively retrieving information. Ordinarily, the computational cost at data-driven evaluation is proportional to the volume, determined by the number of instances and the number of attributes, of the data. One straightforward way of reducing the volume of data is instance selection. Past studies have adopted evolutionary algorithm (EA) for instance selection, yet these studies are mainly based on 
                        k
                      nearest neighbors classifier due to the high computational cost at learning process. Leveraging EA to learn patterns, models, or classifiers forms a famous branch of learning method known as evolutionary learning (EL). Nevertheless, EL requires immense amount of data-driven fitness evaluations, which detracts EL from efficiency and causes slow convergence. The issue of high computational cost at data-driven evaluation restricts the capability of evolutionary learning and evolutionary instance selection. This study proposes a novel framework called cooperative evolutionary learning and instance selection (CELIS) for addressing the above issues. There are three main features in the proposed CELIS: 1) cooperative evolutionary learning and instance selection in an adaptive manner, 2) fast solution evaluation based on the selected subset of data for evolutionary learning, and 3) effective data evaluation of representativeness as per promising solution for evolutionary instance selection. This study conducts a series of experiments for testing the effectiveness and efficiency of the proposed CELIS on data mining, classification, and symbolic regression problems. The results show that the proposed method can significantly enhance the performance of state-of-the-art EAs on the three types of problems in terms of solution quality and convergence speed. Further analysis also validates that CELIS is able to gain a small and representative subset of data.
               ",autonomous vehicle
10.1016/j.swevo.2021.100840,journal,Swarm and Evolutionary Computation,sciencedirect,2021-04-30,sciencedirect,A cooperative coevolution framework for evolutionary learning and instance selection,https://api.elsevier.com/content/article/pii/S2210650221000018,"
                  Data science becomes an emerging topic in developing computational and artificial intelligence techniques due to the rapid growth of information and data over past decade. Data-driven evaluation, through scanning data to validate the performance of a method, is the crux of effectively retrieving information. Ordinarily, the computational cost at data-driven evaluation is proportional to the volume, determined by the number of instances and the number of attributes, of the data. One straightforward way of reducing the volume of data is instance selection. Past studies have adopted evolutionary algorithm (EA) for instance selection, yet these studies are mainly based on 
                        k
                      nearest neighbors classifier due to the high computational cost at learning process. Leveraging EA to learn patterns, models, or classifiers forms a famous branch of learning method known as evolutionary learning (EL). Nevertheless, EL requires immense amount of data-driven fitness evaluations, which detracts EL from efficiency and causes slow convergence. The issue of high computational cost at data-driven evaluation restricts the capability of evolutionary learning and evolutionary instance selection. This study proposes a novel framework called cooperative evolutionary learning and instance selection (CELIS) for addressing the above issues. There are three main features in the proposed CELIS: 1) cooperative evolutionary learning and instance selection in an adaptive manner, 2) fast solution evaluation based on the selected subset of data for evolutionary learning, and 3) effective data evaluation of representativeness as per promising solution for evolutionary instance selection. This study conducts a series of experiments for testing the effectiveness and efficiency of the proposed CELIS on data mining, classification, and symbolic regression problems. The results show that the proposed method can significantly enhance the performance of state-of-the-art EAs on the three types of problems in terms of solution quality and convergence speed. Further analysis also validates that CELIS is able to gain a small and representative subset of data.
               ",autonomous vehicle
10.1016/j.jmir.2019.09.010,journal,Journal of Medical Imaging and Radiation Sciences,sciencedirect,2019-12-31,sciencedirect,Developing an Artificial Intelligence–Enabled Health Care Practice: Rewiring Health Care Professions for Better Care,https://api.elsevier.com/content/article/pii/S1939865419305430,"
                  Artificial intelligence (AI) has the potential to impact almost every aspect of health care, from detection to prediction and prevention. The adoption of new technologies in health care, however, lags far behind the emergence of new technologies. Health care professionals and organizations must be prepared to change and evolve to adopt these new technologies. A basic understanding of emerging AI technologies will be essential for all health care professionals. These technologies include expert systems, robotic process automation, natural language processing, machine learning, and deep learning. Health care professionals and organizations must build their capacity and capabilities to understand and appropriately adopt these technologies. This understanding starts with basic AI literacy, including data governance principles, basic statistics, data visualization, and the impact on clinical processes. Health care professionals and organizations will need to overcome several challenges and tackle core structural issues, such as access to data and the readiness of algorithms for clinical practice. However, health care professionals have an opportunity to shape the way that AI will be used and the outcomes that will be achieved. There is an urgent and emerging need for education and training so that appropriate technologies can be rapidly adopted, resulting in a healthier world for our patients and our communities.
               ",autonomous vehicle
10.1016/j.cpet.2021.06.002,journal,PET Clinics,sciencedirect,2021-10-31,sciencedirect,Role of Artificial Intelligence in Theranostics: Toward Routine Personalized Radiopharmaceutical Therapies,https://api.elsevier.com/content/article/pii/S1556859821000419,,autonomous vehicle
10.1016/j.jweia.2020.104361,journal,Journal of Wind Engineering and Industrial Aerodynamics,sciencedirect,2020-11-30,sciencedirect,Modified fuzzy Q-learning based wind speed prediction,https://api.elsevier.com/content/article/pii/S0167610520302713,"
                  Renewable energy has taken a center stage in sustainable and environmentally safe power generation. In this work, a novel model free Reinforcement Learning based wind speed forecasting technique has been proposed. Our technique uses modified fuzzy Q learning (MFQL) framework to accurately predict 1-min ahead wind speed from the publicly available data online. Empirical Mode Decomposition (EMD) and Pearson’s correlation coefficient have been used in the pre-processing stages to identify seven most relevant intrinsic mode functions (IMF) from the raw wind speed data. These IMFs form the inputs to an MFQL based forecaster which accurately forecasts wind speed using a reward/punishment approach. MFQL predictor is put to test on wind speed data obtained from National Institute of Wind Energy and Wind Resource Assessment data portal for 10 Indian cities, i.e., Bhogat, Chandori, Kotada, Charanka, Gandhi Nagar, Jambua, Keshod, Sadodar, Surat and Vartej located in the state of Gujarat, India. Our predictor is able to achieve an accuracy of 96.23% for Gandhi Nagar, 94.84% for Bhogat, 94.12% for Kotada and similar results are obtained for other locations. MFQL approach has been compared with SVR and k-NN. Results show that MFQL approach has higher accuracy than SVR and k-NN techniques.
               ",autonomous vehicle
10.1016/j.conbuildmat.2019.02.136,journal,Construction and Building Materials,sciencedirect,2019-05-20,sciencedirect,Prediction and validation of alternative fillers used in micro surfacing mix-design using machine learning techniques,https://api.elsevier.com/content/article/pii/S0950061819304301,"
                  In this study regression analysis using machine learning models was investigated to predict and validate the composition of alternative mineral filler in micro surfacing mix design. To generate the data, 168 experiments were conducted with mixing time (sec), cohesion (30 min) kg.cm, cohesion (60 min) kg.cm, set time (sec), wet track abrasion loss (g/m2) as an additives for the design of alternative fillers such as Copper Slag, Fly Ash and High Calcium Fly Ash. Training and testing of feature vector which were formed after conducting experiment was fed into machine learning regression models for prediction of composition of fillers. Support vector machine with polynomial, radial basis function and PUK kernel, Artificial neural network with RBF kernel and Isotonic regression models were considered in the present study. Machine learning regression models were evaluated using three parameters Correlation coefficient, Spearman rho’s and Mean absolute error. Excellent agreement between regression models and experimental results observed. The methodology used will be useful for prediction of micro surfacing mix design for alternative fillers used in the construction industry.
               ",autonomous vehicle
10.1016/B978-0-12-821229-5.00003-3,journal,Machine Learning and the Internet of Medical Things in Healthcare,sciencedirect,2021-12-31,sciencedirect,Chapter 5: Diagnosing of disease using machine learning,https://api.elsevier.com/content/article/pii/B9780128212295000033,"
               The role of machine learning in the healthcare industry is inevitable due to its power to use in disease detection and management. Disease diagnosis using machine-learning techniques can enhance the quickness of decision-making, and it can reduce the rate of false positives. This chapter discusses recent developments in machine-learning algorithms that have made a substantial impact on the detection and diagnosis of several diseases. This chapter firstly introduces challenges in the conventional healthcare system and then describes various machine-learning algorithms like SVM, KNN, Naive Bayes, and Decision tree. The practical implementation of these algorithms is discussed with Python. These algorithms are used to diagnose various diseases like cancer, diabetes, epilepsy, heart attack, and other prominent diseases. The text offers a conceptual and mathematical background of accuracy, precision, recall, and F1 score metrics of a machine-learning algorithm in order to diagnose disease. Finally, the chapter discusses the impact of machine learning on the healthcare industry.
            ",autonomous vehicle
10.1016/j.neucom.2013.02.061,journal,Neurocomputing,sciencedirect,2014-08-22,sciencedirect,Reinforcement learning and dopamine in the striatum: A modeling perspective,https://api.elsevier.com/content/article/pii/S0925231214003439,"
                  The recent research evidences show that the dopamine (DA) system in the brain is involved in various functions like reward-related learning, exploration, preparation, and execution in goal directed behavior. It is suggested that dopaminergic neurons provide a prediction error akin to the error computed in the temporal difference learning (TDL) models of reinforcement learning (RL). Houk et al. (1995) [26] proposed a biochemical model in the spine head of neurons at the striatum in the basal ganglia which generates and uses neural signals to predict reinforcement. The model explains how the DA neurons are able to predict reinforcement and how the output from these neurons might then be used to reinforce the behaviors that lead to primary reinforcement. They proposed a scheme drawing that parallels between actor–critic architecture and dopamine activity in the basal ganglia. Houk et al. (1995) [26] also proposed a biochemical model of interactions between protein molecules which supports learning earlier predictions of reinforcement in the spine head of medium spiny neurons at the striatum. However, Houk׳s proposed cellular model fails to account for the time delay between the dopaminergic and glutamatergic activity required for reward-related learning and also fails to explain the ‘eligibility trace’ condition needed in delayed tasks of associative conditioning in which a memory trace of the antecedent signal is needed at the time of a succeeding reward. In this article, we review various models of RL with an emphasis on the cellular models of RL. In particular, we emphasize biochemical models of RL, and point out the future directions.
               ",autonomous vehicle
10.1016/j.asoc.2020.106181,journal,Applied Soft Computing,sciencedirect,2020-05-31,sciencedirect,Financial time series forecasting with deep learning : A systematic literature review: 2005–2019,https://api.elsevier.com/content/article/pii/S1568494620301216,"
                  Financial time series forecasting is undoubtedly the top choice of computational intelligence for finance researchers in both academia and the finance industry due to its broad implementation areas and substantial impact. Machine Learning (ML) researchers have created various models, and a vast number of studies have been published accordingly. As such, a significant number of surveys exist covering ML studies on financial time series forecasting. Lately, Deep Learning (DL) models have appeared within the field, with results that significantly outperform their traditional ML counterparts. Even though there is a growing interest in developing models for financial time series forecasting, there is a lack of review papers that solely focus on DL for finance. Hence, the motivation of this paper is to provide a comprehensive literature review of DL studies on financial time series forecasting implementation. We not only categorized the studies according to their intended forecasting implementation areas, such as index, forex, and commodity forecasting, but we also grouped them based on their DL model choices, such as Convolutional Neural Networks (CNNs), Deep Belief Networks (DBNs), and Long-Short Term Memory (LSTM). We also tried to envision the future of the field by highlighting its possible setbacks and opportunities for the benefit of interested researchers.
               ",autonomous vehicle
10.1016/j.asoc.2020.106384,journal,Applied Soft Computing,sciencedirect,2020-08-31,sciencedirect,Deep learning for financial applications : A survey,https://api.elsevier.com/content/article/pii/S1568494620303240,"
                  Computational intelligence in finance has been a very popular topic for both academia and financial industry in the last few decades. Numerous studies have been published resulting in various models. Meanwhile, within the Machine Learning (ML) field, Deep Learning (DL) started getting a lot of attention recently, mostly due to its outperformance over the classical models. Lots of different implementations of DL exist today, and the broad interest is continuing. Finance is one particular area where DL models started getting traction, however, the playfield is wide open, a lot of research opportunities still exist. In this paper, we tried to provide a state-of-the-art snapshot of the developed DL models for financial applications. We not only categorized the works according to their intended subfield in finance but also analyzed them based on their DL models. In addition, we also aimed at identifying possible future implementations and highlighted the pathway for the ongoing research within the field.
               ",autonomous vehicle
10.1016/S1006-1266(07)60009-1,journal,Journal of China University of Mining and Technology,sciencedirect,2007-03-31,sciencedirect,A Proposal of Adaptive PID Controller Based on Reinforcement Learning,https://api.elsevier.com/content/article/pii/S1006126607600091,"
                  Aimed at the lack of self-tuning PID parameters in conventional PID controllers, the structure and learning algorithm of an adaptive PID controller based on reinforcement learning were proposed. Actor-Critic learning was used to tune PID parameters in an adaptive way by taking advantage of the model-free and on-line learning properties of re-inforcement learning effectively. In order to reduce the demand of storage space and to improve the learning efficiency, a single RBF neural network was used to approximate the policy function of Actor and the value function of Critic si-multaneously. The inputs of RBF network are the system error, as well as the first and the second-order differences of error. The Actor can realize the mapping from the system state to PID parameters, while the Critic evaluates the outputs of the Actor and produces TD error. Based on TD error performance index and gradient descent method, the updating rules of RBF kernel function and network weights were given. Simulation results show that the proposed controller is efficient for complex nonlinear systems and it is perfectly adaptable and strongly robust, which is better than that of a conventional PID controller.
               ",autonomous vehicle
10.1016/B978-0-12-821633-0.00010-6,journal,"Demystifying Big Data, Machine Learning, and Deep Learning for Healthcare Analytics",sciencedirect,2021-12-31,sciencedirect,Chapter 11: Machine learning algorithms for prediction of heart disease,https://api.elsevier.com/content/article/pii/B9780128216330000106,"
               The field of healthcare has continuously been under the radar over the last few decades and, as such, has seen advancements by leaps and bounds, especially in the sector of disease prediction and potential data analysis. The quick and timely prediction of diseases is of paramount importance in healthcare systems. The complexity of disease prediction far surpasses the cognitive abilities of the human species. That, combined with the time constraints, make this task an extremely challenging one. The mission of analyzing high-degree medical data concerning all dimensions is monotonous and prone to human errors. This monotonous and tedious task can be efficiently performed by machines equipped with advanced hardware units (such as GPUs). The ability of machines to work tirelessly without error can be harnessed for better accuracy. Recently, Machine learning (ML)/deep Learning (DL) techniques have gained a lot of momentum in the healthcare industry for the fast prediction of diseases with minimal human intervention. Some contributory risk factors such as diabetes, high blood pressure, high cholesterol, etc., make the task of identifying heart disease a difficult one. Owing to this, the heart disease dataset has been chosen for the case study for this chapter. The steps to be followed in using various basic ML algorithms and ensemble ML along with the comparison of results are presented. In our experimentation with base learners, the support vector machine had the highest accuracy of up to 83%. In ensemble learning models, we achieved higher accuracy for weighted average voting of 83%.
            ",autonomous vehicle
10.1016/j.neunet.2018.07.006,journal,Neural Networks,sciencedirect,2018-12-31,sciencedirect,State representation learning for control: An overview,https://api.elsevier.com/content/article/pii/S0893608018302053,"
                  Representation learning algorithms are designed to learn abstract features that characterize data. State representation learning (SRL) focuses on a particular kind of representation learning where learned features are in low dimension, evolve through time, and are influenced by actions of an agent. The representation is learned to capture the variation in the environment generated by the agent’s actions; this kind of representation is particularly suitable for robotics and control scenarios. In particular, the low dimension characteristic of the representation helps to overcome the curse of dimensionality, provides easier interpretation and utilization by humans and can help improve performance and speed in policy learning algorithms such as reinforcement learning.
                  This survey aims at covering the state-of-the-art on state representation learning in the most recent years. It reviews different SRL methods that involve interaction with the environment, their implementations and their applications in robotics control tasks (simulated or real). In particular, it highlights how generic learning objectives are differently exploited in the reviewed algorithms. Finally, it discusses evaluation methods to assess the representation learned and summarizes current and future lines of research.
               ",autonomous vehicle
10.3182/20050703-6-CZ-1902.01138,journal,IFAC Proceedings Volumes,sciencedirect,2005-12-31,sciencedirect,GENERALIZATION OF REINFORCEMENT LEARNING WITH CMAC,https://api.elsevier.com/content/article/pii/S1474667016371506,"
                  To implement a generalization of value functions in Adaptive Search Element (ASE)-reinforcement learning, CMAC is integrated into ASE controller. ASE-reinforcement learning scheme is briefly studied to discuss how CMAC is integrated into ASE controller. Neighbourhood Sequential Training concept is utilized to establish the look-up table of CMAC and to produce discrete control outputs. In computer simulation, an ASE controller and a couple of ASE-CMAC neural network are trained to balance the inverted pendulum on a cart. The number of trials until the controllers are established and the learning performance of the controllers are evaluated to find that generalization ability of the CMAC improves the speed of the ASE-reinforcement learning enough to realize the cartpole control system.
               ",autonomous vehicle
10.1016/j.eswa.2017.12.020,journal,Expert Systems with Applications,sciencedirect,2018-05-01,sciencedirect,The use of machine learning algorithms in recommender systems: A systematic review,https://api.elsevier.com/content/article/pii/S0957417417308333,"
                  Recommender systems use algorithms to provide users with product or service recommendations. Recently, these systems have been using machine learning algorithms from the field of artificial intelligence. However, choosing a suitable machine learning algorithm for a recommender system is difficult because of the number of algorithms described in the literature. Researchers and practitioners developing recommender systems are left with little information about the current approaches in algorithm usage. Moreover, the development of recommender systems using machine learning algorithms often faces problems and raises questions that must be resolved. This paper presents a systematic review of the literature that analyzes the use of machine learning algorithms in recommender systems and identifies new research opportunities. The goals of this study are to (i) identify trends in the use or research of machine learning algorithms in recommender systems; (ii) identify open questions in the use or research of machine learning algorithms; and (iii) assist new researchers to position new research activity in this domain appropriately. The results of this study identify existing classes of recommender systems, characterize adopted machine learning approaches, discuss the use of big data technologies, identify types of machine learning algorithms and their application domains, and analyzes both main and alternative performance metrics.
               ",autonomous vehicle
10.1016/j.procs.2019.09.052,journal,Procedia Computer Science,sciencedirect,2019-12-31,sciencedirect,A Comparative Research of Machine Learning Impact to Future of Maritime Transportation,https://api.elsevier.com/content/article/pii/S1877050919312128,"Machine Learning (ML) can be defined as a level of algorithm which may allow software applications to create more accurate in forecasting outputs without being external programmed. Since maritime transportation requires smart technologies, adaptation of machine learning tools might provide utmost benefit for efficiency, sustainability and reduction of operational costs. As the data is core element to unlocking the uncertainty, it may help to improve shipping. So far, the data acquisition on maritime transportation is quite limited. Therefore, adaptation of machine learning techniques in the maritime transportation is narrow as compared to other industries. The aim of this paper is to discuss machine learning applications and their impacts to future of maritime transportation industry. A sets of comparative researches will be undertaken to present current situation and potential impacts to future in maritime transportation. With the help of this research, the maritime practitioners and professionals will gain an idea on focusing appropriate algorithm for a specific shipping problem such as voyage optimization and economics, sustainability of transportation, controlling of freight rates, maintenance forecasting, digitalization on bridge and engine control room, etc.",autonomous vehicle
10.1016/j.compeleceng.2009.12.005,journal,Computers & Electrical Engineering,sciencedirect,2010-05-31,sciencedirect,Performance evaluation of artificial neural network-based learning schemes for cognitive radio systems,https://api.elsevier.com/content/article/pii/S0045790609001128,"
                  Over the last decade the world of wireless communications has been undergoing some crucial changes, which have brought it at the forefront of international research and development interest, eventually resulting in the advent of a multitude of innovative technologies and associated products such as WiFi, WiMax, 802.20, 802.22, wireless mesh networks and software defined radio. Such a disparate and highly varying radio environment calls for intelligent management, allocation and usage of a scarce resource, namely the radio spectrum. One of the most prominent emerging technologies that promise to handle such situations is cognitive radio. Cognitive radio systems are based on software defined radio technology and utilize intelligent software packages that enrich their transceivers with the highly attractive properties of self-awareness, adaptability and capability to learn. A cognitive radio system has the ability to adjust its operating parameters, observe the results and, eventually take actions, that is to say, decide to operate in a specific radio configuration (i.e. radio access technology, carrier frequency, modulation type, etc.), expecting to move the radio toward some optimized operational state. In such a process, learning mechanisms that are capable of exploiting measurements sensed from the environment, gathered experience and stored knowledge, are judged as rather beneficial for guiding decisions and actions. Framed within this statement, this paper introduces and evaluates learning schemes that are based on artificial neural networks and can be used for discovering the performance (e.g. data rate) that can be achieved by a specific radio configuration in a cognitive radio system. Interesting scenarios, which include both commercial off-the-shelf and simulation hardware/software products, are mobilized for the performance assessment work, conducted in order to design and use an appropriate neural network structure, while indicative results are presented and discussed in order to showcase the benefits of incorporating such learning schemes into cognitive radio systems.
               ",autonomous vehicle
10.1016/j.patter.2020.100142,journal,Patterns,sciencedirect,2020-12-11,sciencedirect,Deep Learning in Protein Structural Modeling and Design,https://api.elsevier.com/content/article/pii/S2666389920301902,"Deep learning is catalyzing a scientific revolution fueled by big data, accessible toolkits, and powerful computational resources, impacting many fields, including protein structural modeling. Protein structural modeling, such as predicting structure from amino acid sequence and evolutionary information, designing proteins toward desirable functionality, or predicting properties or behavior of a protein, is critical to understand and engineer biological systems at the molecular level. In this review, we summarize the recent advances in applying deep learning techniques to tackle problems in protein structural modeling and design. We dissect the emerging approaches using deep learning techniques for protein structural modeling and discuss advances and challenges that must be addressed. We argue for the central importance of structure, following the “sequence → structure → function” paradigm. This review is directed to help both computational biologists to gain familiarity with the deep learning methods applied in protein modeling, and computer scientists to gain perspective on the biologically meaningful problems that may benefit from deep learning techniques.",autonomous vehicle
10.1016/j.isci.2018.06.010,journal,iScience,sciencedirect,2018-07-27,sciencedirect,Data and Power Efficient Intelligence with Neuromorphic Learning Machines,https://api.elsevier.com/content/article/pii/S2589004218300865,"The success of deep networks and recent industry involvement in brain-inspired computing is igniting a widespread interest in neuromorphic hardware that emulates the biological processes of the brain on an electronic substrate. This review explores interdisciplinary approaches anchored in machine learning theory that enable the applicability of neuromorphic technologies to real-world, human-centric tasks. We find that (1) recent work in binary deep networks and approximate gradient descent learning are strikingly compatible with a neuromorphic substrate; (2) where real-time adaptability and autonomy are necessary, neuromorphic technologies can achieve significant advantages over main-stream ones; and (3) challenges in memory technologies, compounded by a tradition of bottom-up approaches in the field, block the road to major breakthroughs. We suggest that a neuromorphic learning framework, tuned specifically for the spatial and temporal constraints of the neuromorphic substrate, will help guiding hardware algorithm co-design and deploying neuromorphic hardware for proactive learning of real-world data.",autonomous vehicle
10.1016/B978-0-12-818697-8.00001-7,journal,Reference Module in Neuroscience and Biobehavioral Psychology,sciencedirect,2020-12-31,sciencedirect,Artificial Intelligence in Clinical Psychology,https://api.elsevier.com/content/article/pii/B9780128186978000017,"
               The chapter focuses on the current applications of machine learning in clinical psychology, clinical psychiatry and psychotherapy.
               Clustering, classification and prediction algorithms can be used for different purposes in the realm of clinical psychology, such as diagnosis and prognosis. Current diagnostic criteria, therapies and statistical procedures show different limitations that hamper the understanding and management of complex and multidimensional conditions. The potential of this new technology can be exploited to improve the assessment and the treatment of psychiatric disorders and successfully added as a new tool for clinicians and researchers.
            ",autonomous vehicle
10.1016/j.amjsurg.2020.11.023,journal,The American Journal of Surgery,sciencedirect,2021-06-30,sciencedirect,The present and future state of machine learning for predictive analytics in surgery,https://api.elsevier.com/content/article/pii/S0002961020307467,,autonomous vehicle
10.1016/B978-0-323-67538-3.00008-7,journal,Artificial Intelligence and Deep Learning in Pathology,sciencedirect,2021-12-31,sciencedirect,Chapter 8: Precision medicine in digital pathology via image analysis and machine learning,https://api.elsevier.com/content/article/pii/B9780323675383000087,"
               In this chapter, we present the concept that digital pathology and artificial intelligence can add value and speed to a pathologist's diagnosis while striving toward precision medicine. We describe how image analysis and machine learning can segment images and compute object- and spatial-based data prior to analysis. This data analysis can identify patients who are at a high risk of succumbing to a disease, who may need more detailed clinical follow-up, or who will respond to specific therapy. We also describe how deep learning algorithms can learn complex morphological patterns from both human- and data-led input in order to perform diagnostic or prognostic tasks. Finally, we discuss the theory behind some commonly used machine learning algorithms and how they may attain regulatory approval.
            ",autonomous vehicle
10.1016/j.scs.2020.102177,journal,Sustainable Cities and Society,sciencedirect,2020-09-30,sciencedirect,Data mining and machine learning methods for sustainable smart cities traffic classification: A survey,https://api.elsevier.com/content/article/pii/S2210670720301645,"
                  This survey paper describes the significant literature survey of Sustainable Smart Cities (SSC), Machine Learning (ML), Data Mining (DM), datasets, feature extraction and selection for network traffic classification. Considering relevance and most cited methods and datasets of features were identified, read and summarized. As data and data features are essential in Internet traffic classification using machine learning techniques, some well-known and most used datasets with details statistical features are described. Different classification techniques for SSC network traffic classification are presented with more information. The complexity of data set, features extraction and machine learning methods are addressed. In the end, challenges and recommendations for SSC network traffic classification with the dataset of features are presented.
               ",autonomous vehicle
10.1016/j.jnca.2020.102871,journal,Journal of Network and Computer Applications,sciencedirect,2021-01-01,sciencedirect,Distributed real-time SlowDoS attacks detection over encrypted traffic using Artificial Intelligence,https://api.elsevier.com/content/article/pii/S1084804520303362,"
                  SlowDoS attacks exploit slow transmissions on application-level protocols like HTTP to carry out denial of service against web-servers. These attacks are difficult to be detected with traditional signature-based intrusion detection approaches, even more when the HTTP traffic is encrypted. To cope with this challenge, this paper describes and AI-based anomaly detection system for real-time detection of SlowDoS attacks over application-level encrypted traffic. Our system monitors in real-time the network traffic, analyzing, processing and aggregating packets into conversation flows, getting valuable features and statistics that are dynamically analyzed in streaming for AI-based anomaly detection. The distributed AI model running in Apache Spark-streaming, combines clustering analysis for anomaly detection, along with deep learning techniques to increase detection accuracy in those cases where clustering obtains ambiguous probabilities. The proposal has been implemented and validated in a real testbed, showing its feasibility, performance and accuracy for detecting in real-time different kinds of SlowDoS attacks over encrypted traffic. The achieved results are close to the optimal precision value with a success rate 98%, while the false negative rate takes a value below 0.5%.
               ",autonomous vehicle
10.1016/j.aei.2006.01.001,journal,Advanced Engineering Informatics,sciencedirect,2006-07-31,sciencedirect,Reinforcement learning in a distributed market-based production control system,https://api.elsevier.com/content/article/pii/S1474034606000024,"
                  The paper presents an adaptive iterative distributed scheduling algorithm that operates in a market-based production control system. The manufacturing system is agentified, thus, every machine and job is associated with its own software agent. Each agent learns how to select presumably good schedules, by this way the size of the search space can be reduced. In order to get adaptive behavior and search space reduction, a triple-level learning mechanism is proposed. The top level of learning incorporates a simulated annealing algorithm, the middle (and the most important) level contains a reinforcement learning system, while the bottom level is done by a numerical function approximator, such as an artificial neural network. The paper suggests a cooperation technique for the agents, as well. It also analyzes the time and space complexity of the solution and presents some experimental results.
               ",autonomous vehicle
10.1016/B978-0-12-815630-8.00009-0,journal,Algorithmic Trading Methods,sciencedirect,2021-12-31,sciencedirect,Chapter 9: Machine Learning Techniques,https://api.elsevier.com/content/article/pii/B9780128156308000090,,autonomous vehicle
10.1016/j.rser.2020.110287,journal,Renewable and Sustainable Energy Reviews,sciencedirect,2020-11-30,sciencedirect,Machine learning applications in urban building energy performance forecasting: A systematic review,https://api.elsevier.com/content/article/pii/S136403212030575X,"
                  In developed countries, buildings are involved in almost 50% of total energy use and 30% of global green-house gas emissions. Buildings' operational energy is highly dependent on various building physical, operational, and functional characteristics, as well as meteorological and temporal properties. Besides physics-based building energy modeling, machine learning techniques can provide faster and higher accuracy estimates, given buildings' historic energy consumption data. Looking beyond individual building levels, forecasting buildings’ energy performance helps city and community managers have a better understanding of their future energy needs, and plan for satisfying them more efficiently. Focusing on an urban-scale, this study systematically reviews 70 journal articles, published in the field of building energy performance forecasting between 2015 and 2018. The recent literature have been categorized according to five criteria: 1. Learning Method, 2. Building Type, 3. Energy Type, 4. Input Data, and 5. Time-scale. The scarcity of building energy performance forecasting studies in urban-scale versus individual level is considerable. There is no study incorporating building functionality in terms of space functionality share percentages, nor assessing the effects of climate change on urban buildings energy performance using machine learning approaches and future weather scenarios. There is no optimal criteria combination for achieving the most accurate machine learning-based forecast, as there is no universal measure able to provide such global comparison. Accuracy levels are highly correlated with the characteristics of forecasting problems. The goal is to provide a comprehensive status of machine learning applications in urban building energy performance forecasting, during 2015–2018.
               ",autonomous vehicle
10.1016/j.enbuild.2020.109807,journal,Energy and Buildings,sciencedirect,2020-03-15,sciencedirect,The use of artificial intelligence (AI) methods in the prediction of thermal comfort in buildings: energy implications of AI-based thermal comfort controls,https://api.elsevier.com/content/article/pii/S0378778819336527,"
                  Buildings consume about 40 % of globally-produced energy. A notable amount of this energy is used to provide sufficient comfort levels to the building occupants. Moreover, given recent increases in global temperatures as a result of climate change and the associated decrease in comfort levels, providing adequate comfort levels in indoor spaces has become increasingly important. However, striking a balance between reducing building energy use and providing adequate comfort levels is a significant challenge. Conventional control methods for indoor spaces, such as on/off, proportional-integral (PI), and proportional-integral-derivative (PID) controllers, display significant instabilities and frequently overshoot thermostats, resulting in unnecessary energy use. Additionally, conventional building control methods rarely include comfort regulatory schemes. Consequently, recent research efforts have focused on the use of advanced artificial intelligence (AI) methods to optimize building energy usage while maintaining occupant thermal comfort. We present a review of the current AI-based methodologies being used to enhance thermal comfort in indoor spaces. we focus on thermal comfort predictive models using diverse machine learning (ML) algorithms and their deployment in building control systems for energy saving purposes. We then discuss gaps in the existing literature and highlight potential future research directions.
               ",autonomous vehicle
10.1016/j.inffus.2020.01.002,journal,Information Fusion,sciencedirect,2020-07-31,sciencedirect,Urban flow prediction from spatiotemporal data using machine learning: A survey,https://api.elsevier.com/content/article/pii/S1566253519303094,"
                  Urban spatiotemporal flow prediction is of great importance to traffic management, land use, public safety. This prediction task is affected by several complex and dynamic factors, such as patterns of human activities, weather, events, and holidays. Datasets evaluated the flow come from various sources in different domains, e.g. mobile phone data, taxi trajectories data, metro/bus swiping data, bike-sharing data. To summarize these methodologies of urban flow prediction, in this paper, we first introduced four main factors affecting urban flow. Second, in order to further analyze urban flow, we partitioned the preparation process of multi-source spatiotemporal data related with urban flow into three groups. Third, we chose the spatiotemporal dynamic data as a case study for the urban flow prediction task. Fourth, we analyzed and compared some representative flow prediction methods in detail, classifying them into five categories: statistics-based, traditional machine learning-based, deep learning-based, reinforcement learning-based, and transfer learning-based methods. Finally, we showed open challenges of urban flow prediction and discussed many recent research works on urban flow prediction. This paper will facilitate researchers to find suitable methods and public datasets for addressing urban spatiotemporal flow forecast problems.
               ",autonomous vehicle
10.1016/j.conb.2017.08.020,journal,Current Opinion in Neurobiology,sciencedirect,2017-10-31,sciencedirect,Learning with three factors: modulating Hebbian plasticity with errors,https://api.elsevier.com/content/article/pii/S0959438817300612,"Synaptic plasticity is a central theme in neuroscience. A framework of three-factor learning rules provides a powerful abstraction, helping to navigate through the abundance of models of synaptic plasticity. It is well-known that the dopamine modulation of learning is related to reward, but theoretical models predict other functional roles of the modulatory third factor; it may encode errors for supervised learning, summary statistics of the population activity for unsupervised learning or attentional feedback. Specialized structures may be needed in order to generate and propagate third factors in the neural network.",autonomous vehicle
10.1016/j.techsoc.2020.101396,journal,Technology in Society,sciencedirect,2020-11-30,sciencedirect,Deep learning diffusion by infusion into preexisting technologies – Implications for users and society at large,https://api.elsevier.com/content/article/pii/S0160791X20302694,"Artificial Intelligence (AI) in the form of Deep Learning (DL) technology has diffused in the consumer domain in a unique way as compared to previous general-purpose technologies. DL has often spread by infusion, i.e., by being added to preexisting technologies that are already in use. We find that DL-algorithms for recommendations or ranking have been infused into all the 15 most popular mobile applications (apps) in the U.S. (as of May 2019). DL-infusion enables fast and vast diffusion. For example, when a DL-system was infused into YouTube, it almost immediately reached a third of the world's population. We argue that existing theories of innovation diffusion and adoption have limited relevance for DL-infusion, because it is a process that is driven by enterprises rather than individuals. We also discuss its social and ethical implications. First, consumers have a limited ability to detect and evaluate an infused technology. DL-infusion may thus help to explain why AI's presence in society has not been challenged by many. Second, the DL-providers are likely to face conflicts of interest, since consumer and supplier goals are not always aligned. Third, infusion is likely to be a particularly important diffusion process for DL-technologies as compared to other innovations, because they need large data sets to function well, which can be drawn from preexisting users. Related, it seems that larger technology companies comparatively benefit more from DL-infusion, because they already have many users. This suggests that the value drawn from DL is likely to follow a Matthew Effect of accumulated advantage online: many preexisting users provide a lot of behavioral data, which bring about better DL-driven features, which attract even more users, etc. Such a self-reinforcing process could limit the possibilities for new companies to compete. This way, the notion of DL-infusion may put light on the power shift that comes with the presence of AI in society.",autonomous vehicle
10.1016/j.conbuildmat.2021.125279,journal,Construction and Building Materials,sciencedirect,2021-12-06,sciencedirect,Estimating compressive strength of modern concrete mixtures using computational intelligence: A systematic review,https://api.elsevier.com/content/article/pii/S0950061821030208,"
                  The mixture proportioning of conventional concrete is commonly established using regression analysis of experimental data. However, such traditional empirical procedures have proven less accurate for modern complex cementitious composites. The lack of robust predictive tools for estimating the mixture composition and engineering properties of novel concretes led to deploying machine learning techniques. Although these versatile computational algorithms have proven successful in diverse applications, their performance is highly dependent on the data structure and appropriate selection of hyperparameters. Therefore, this paper demystifies the use of ML in concrete technology by systematically surveying and critically reviewing ML algorithms employed to predict the compressive strength of modern concrete mixtures. The hyperparameters of various machine learning models along with the achieved accuracy are critically analyzed and discussed. The main findings regarding machine learning predictions of compressive strength for various concrete types are presented, recommendations for best practice are made, and needed future research is identified.
               ",autonomous vehicle
10.1016/j.ocarto.2020.100069,journal,Osteoarthritis and Cartilage Open,sciencedirect,2020-09-30,sciencedirect,Machine learning in knee osteoarthritis: A review,https://api.elsevier.com/content/article/pii/S2665913120300583,"Objective The purpose of present review paper is to introduce the reader to key directions of Machine Learning techniques on the diagnosis and predictions of knee osteoarthritis. Design This survey was based on research articles published between 2006 and 2019. The articles were divided into four categories, namely (i) predictions/regression, (ii) classification, (iii) optimum post-treatment planning techniques and (iv) segmentation. The grouping was based on the application domain of each study. Results The survey findings are reported outlining the main characteristics of the proposed learning algorithms, the application domains, the data sources investigated and the quality of the results. Conclusions Knee osteoarthritis is a big data problem in terms of data complexity, heterogeneity and size as it has been commonly considered in the literature. Machine Learning has attracted significant interest from the scientific community to cope with the aforementioned challenges and thus lead to new automated pre- or post-treatment solutions that utilize data from the greatest possible variety of sources.",autonomous vehicle
10.1016/j.promfg.2018.06.006,journal,Procedia Manufacturing,sciencedirect,2018-12-31,sciencedirect,Application Scenarios of Artificial Intelligence in Electric Drives Production,https://api.elsevier.com/content/article/pii/S2351978918305237,"Artificial intelligence (AI) is the overall term for technologies used to build intelligent systems, no matter whether utilized in an industrial or private environment. However, hardly any AI-based approaches have been proposed for the increasingly important electric drives production yet. By identifying and presenting exemplary application scenarios for knowledge-based systems (KBS) and machine learning (ML), the paper serves as a starting point for further research in the respective fields. Among others, the systematic overview reveals that KBS are especially suited for supporting the planning of electric drives production systems, whereas ML-based approaches have great potential for optimizing single production processes.",autonomous vehicle
10.1016/j.procir.2021.01.056,journal,Procedia CIRP,sciencedirect,2021-12-31,sciencedirect,Towards an intelligent linear winding process through sensor integration and machine learning techniques,https://api.elsevier.com/content/article/pii/S2212827121000809,"Industry 4.0 is associated with numerous technologies, which offer great potential for optimizing linear winding processes as used in electric motor manufacturing. To further increase flexibility and quality in the production of mass products like coils, data-driven techniques such as machine learning (ML) are increasingly moving into focus. To generate the required data, however, linear winding machines must be equipped with suitable sensors. Accordingly, this paper will first present all known influencing and quality parameters in linear winding based on earlier research work. Based on this, suitable data sources and sensor systems are identified and a concept for applying ML techniques is derived.",autonomous vehicle
10.1016/j.cmi.2019.09.009,journal,Clinical Microbiology and Infection,sciencedirect,2020-05-31,sciencedirect,Machine learning for clinical decision support in infectious diseases: a narrative review of current applications,https://api.elsevier.com/content/article/pii/S1198743X1930494X,"Background Machine learning (ML) is a growing field in medicine. This narrative review describes the current body of literature on ML for clinical decision support in infectious diseases (ID). Objectives We aim to inform clinicians about the use of ML for diagnosis, classification, outcome prediction and antimicrobial management in ID. Sources References for this review were identified through searches of MEDLINE/PubMed, EMBASE, Google Scholar, biorXiv, ACM Digital Library, arXiV and IEEE Xplore Digital Library up to July 2019. Content We found 60 unique ML-clinical decision support systems (ML-CDSS) aiming to assist ID clinicians. Overall, 37 (62%) focused on bacterial infections, 10 (17%) on viral infections, nine (15%) on tuberculosis and four (7%) on any kind of infection. Among them, 20 (33%) addressed the diagnosis of infection, 18 (30%) the prediction, early detection or stratification of sepsis, 13 (22%) the prediction of treatment response, four (7%) the prediction of antibiotic resistance, three (5%) the choice of antibiotic regimen and two (3%) the choice of a combination antiretroviral therapy. The ML-CDSS were developed for intensive care units (n = 24, 40%), ID consultation (n = 15, 25%), medical or surgical wards (n = 13, 20%), emergency department (n = 4, 7%), primary care (n = 3, 5%) and antimicrobial stewardship (n = 1, 2%). Fifty-three ML-CDSS (88%) were developed using data from high-income countries and seven (12%) with data from low- and middle-income countries (LMIC). The evaluation of ML-CDSS was limited to measures of performance (e.g. sensitivity, specificity) for 57 ML-CDSS (95%) and included data in clinical practice for three (5%). Implications Considering comprehensive patient data from socioeconomically diverse healthcare settings, including primary care and LMICs, may improve the ability of ML-CDSS to suggest decisions adapted to various clinical contexts. Currents gaps identified in the evaluation of ML-CDSS must also be addressed in order to know the potential impact of such tools for clinicians and patients.",autonomous vehicle
10.1016/j.isci.2020.101809,journal,iScience,sciencedirect,2020-12-18,sciencedirect,Integration and Co-design of Memristive Devices and Algorithms for Artificial Intelligence,https://api.elsevier.com/content/article/pii/S2589004220310063,"Memristive devices share remarkable similarities to biological synapses, dendrites, and neurons at both the physical mechanism level and unit functionality level, making the memristive approach to neuromorphic computing a promising technology for future artificial intelligence. However, these similarities do not directly transfer to the success of efficient computation without device and algorithm co-designs and optimizations. Contemporary deep learning algorithms demand the memristive artificial synapses to ideally possess analog weighting and linear weight-update behavior, requiring substantial device-level and circuit-level optimization. Such co-design and optimization have been the main focus of memristive neuromorphic engineering, which often abandons the “non-ideal” behaviors of memristive devices, although many of them resemble what have been observed in biological components. Novel brain-inspired algorithms are being proposed to utilize such behaviors as unique features to further enhance the efficiency and intelligence of neuromorphic computing, which calls for collaborations among electrical engineers, computing scientists, and neuroscientists.",autonomous vehicle
10.1016/j.artmed.2020.101878,journal,Artificial Intelligence in Medicine,sciencedirect,2020-06-30,sciencedirect,Deep learning in generating radiology reports: A survey,https://api.elsevier.com/content/article/pii/S0933365719302635,"
                  Substantial progress has been made towards implementing automated radiology reporting models based on deep learning (DL). This is due to the introduction of large medical text/image datasets. Generating radiology coherent paragraphs that do more than traditional medical image annotation, or single sentence-based description, has been the subject of recent academic attention. This presents a more practical and challenging application and moves towards bridging visual medical features and radiologist text. So far, the most common approach has been to utilize publicly available datasets and develop DL models that integrate convolutional neural networks (CNN) for image analysis alongside recurrent neural networks (RNN) for natural language processing (NLP) and natural language generation (NLG). This is an area of research that we anticipate will grow in the near future. We focus our investigation on the following critical challenges: understanding radiology text/image structures and datasets, applying DL algorithms (mainly CNN and RNN), generating radiology text, and improving existing DL based models and evaluation metrics. Lastly, we include a critical discussion and future research recommendations. This survey will be useful for researchers interested in DL, particularly those interested in applying DL to radiology reporting.
               ",autonomous vehicle
10.1016/j.asoc.2020.106068,journal,Applied Soft Computing,sciencedirect,2020-03-31,sciencedirect,Optimizing hyperparameters of deep learning in predicting bus passengers based on simulated annealing,https://api.elsevier.com/content/article/pii/S1568494620300089,"
                  Bus is certainly one of the most widely used public transportation systems in a modern city because it provides an inexpensive solution to public transportation users, such as commuters and tourists. Most people would like to avoid taking a crowded bus on the way. That is why forecasting the number of bus passengers has been a critical problem for years. The proposed method is inspired by the fact that there is no easy way to know the suitable parameters for most of the deep learning methods in solving the optimization problem of forecasting the number of passengers on a bus. To address this issue, the proposed algorithm uses a simulated annealing (SA) to find out a suitable number of neurons for each layer of a fully connected deep neural network (DNN) to enhance the accuracy rate in solving this particular optimization problem. The proposed method is compared with support vector machine, random forest, eXtreme gradient boosting, deep neural network, and deep neural network with dropout for the data provided by the Taichung city smart transportation big data research center, Taiwan (TSTBDRC). Our simulation results indicate that the proposed method outperforms all the other forecasting methods for forecasting the number of bus passengers in terms of the accuracy rate and the prediction time.
               ",autonomous vehicle
10.1016/j.jnca.2020.102630,journal,Journal of Network and Computer Applications,sciencedirect,2020-07-01,sciencedirect,Machine learning based solutions for security of Internet of Things (IoT): A survey,https://api.elsevier.com/content/article/pii/S1084804520301041,"
                  Over the last decade, IoT platforms have been developed into a global giant that grabs every aspect of our daily lives by advancing human life with its unaccountable smart services. Because of easy accessibility and fast-growing demand for smart devices and network, IoT is now facing more security challenges than ever before. There are existing security measures that can be applied to protect IoT. However, traditional techniques are not as efficient with the advancement booms as well as different attack types and their severeness. Thus, a strong-dynamically enhanced and up to date security system is required for next-generation IoT system. A huge technological advancement has been noticed in Machine Learning (ML) which has opened many possible research windows to address ongoing and future challenges in IoT. In order to detect attacks and identify abnormal behaviors of smart devices and networks, ML is being utilized as a powerful technology to fulfill this purpose. In this survey paper, the architecture of IoT is discussed, following a comprehensive literature review on ML approaches the importance of security of IoT in terms of different types of possible attacks. Moreover, ML-based potential solutions for IoT security has been presented and future challenges are discussed.
               ",autonomous vehicle
10.1016/j.apenergy.2021.117766,journal,Applied Energy,sciencedirect,2021-12-15,sciencedirect,A review of wind speed and wind power forecasting with deep neural networks,https://api.elsevier.com/content/article/pii/S0306261921011053,"
                  The use of wind power, a pollution-free and renewable form of energy, to generate electricity has attracted increasing attention. However, intermittent electricity generation resulting from the random nature of wind speed poses challenges to the safety and stability of electric power grids when wind power is integrated into grids on large scales. Therefore, accurate forecasting of wind speed and wind power (WS/WP) has gradually taken on a key role in reducing wind power fluctuations in system dispatch planning. With the development of artificial intelligence technologies, especially deep learning, increasing numbers of deep learning-based models are being considered for WS/WP forecasting due to their superior ability to deal with complex nonlinear problems. This paper comprehensively reviews the various deep learning technologies being used in WS/WP forecasting, including the stages of data processing, feature extraction, and relationship learning. The forecasting performance of some popular models is tested and compared using two real-world wind datasets. In this review, three challenges to accurate WS/WP forecasting under complex conditions are identified, namely, data uncertainties, incomplete features, and intricate nonlinear relationships. Moreover, future research directions are summarized as a guide to improve the accuracy of WS/WP forecasts.
               ",autonomous vehicle
10.1016/j.cie.2020.106854,journal,Computers & Industrial Engineering,sciencedirect,2020-11-30,sciencedirect,A review of applications in federated learning,https://api.elsevier.com/content/article/pii/S0360835220305532,"
                  Federated Learning (FL) is a collaboratively decentralized privacy-preserving technology to overcome challenges of data silos and data sensibility. Exactly what research is carrying the research momentum forward is a question of interest to research communities as well as industrial engineering. This study reviews FL and explores the main evolution path for issues exist in FL development process to advance the understanding of FL. This study aims to review prevailing application in industrial engineering to guide for the future landing application. This study also identifies six research fronts to address FL literature and help advance our understanding of FL for future optimization. This study contributes to conclude application in industrial engineering and computer science and summarize a review of applications in FL.
               ",autonomous vehicle
10.1016/j.suscom.2021.100578,journal,Sustainable Computing: Informatics and Systems,sciencedirect,2021-09-30,sciencedirect,Forecasting energy generation in large photovoltaic plants using radial belief neural network,https://api.elsevier.com/content/article/pii/S221053792100069X,"
                  Forecasting the energy generation from the solar power is considered challenging due to inaccuracies in forecasting, reliability issues and substantial economic losses in power systems. Hence, it is necessary to consider wide features from the solar power generation point of view. In this paper, the study uses large features set to feed the deep learning classifier for optimal prediction of energy generation from the photovoltaic (PV) plants. The features selection and prediction modules automates the process of optimal prediction of energy using Radial Belief Neural Network (RBNN). The Restricted Boltzmann Machines (RBM) is used for rule set generation based on the feature extracted and the rule set generation is powered by action-reward based Reinforcement Learning (RL) method. The experiments are conducted with rich set of input features on large PV plants that ranges between 1, 50, 100 and 1000. The performance of the proposed model is compared with various metrics that includes: Root mean squared error (RMSE), normalized root mean squared error (NRMSE), mean bias error (MBE), Mean absolute error (MAE), Maximum absolute error (MaxAE), mean absolute percentage error (MAPE), Kolmogorov–Smirnov test integral (KSI) and OVER metrics, Skewness and kurtosis and variability estimation metrics. The simulation results show that the RBNN offers improved prediction ability with reduced errors than other deep and machine learning classifiers.
               ",autonomous vehicle
10.1016/j.compag.2018.12.006,journal,Computers and Electronics in Agriculture,sciencedirect,2019-01-31,sciencedirect,Current and future applications of statistical machine learning algorithms for agricultural machine vision systems,https://api.elsevier.com/content/article/pii/S0168169918304289,"
                  With being rapid increasing population in worldwide, the need for satisfactory level of crop production with decreased amount of agricultural lands. Machine vision would ensure the increase of crop production by using an automated, non-destructive and cost-effective technique. In last few years, remarkable results have been achieved in different sectors of agriculture. These achievements are integrated with machine learning techniques on machine vision approach that cope with colour, shape, texture and spectral analysis from the image of objects. Despite having many applications of different machine learning techniques, this review only described the statistical machine learning technologies with machine vision systems in agriculture due to broad area of machine learning applications. Two types of statistical machine learning techniques such as supervised and unsupervised learning have been utilized for agriculture. This paper comprehensively surveyed current application of statistical machine learning techniques in machine vision systems, analyses each technique potential for specific application and represents an overview of instructive examples in different agricultural areas. Suggestions of specific statistical machine learning technique for specific purpose and limitations of each technique are also given. Future trends of statistical machine learning technology applications are discussed.
               ",autonomous vehicle
10.1016/j.pnucene.2019.103183,journal,Progress in Nuclear Energy,sciencedirect,2020-01-31,sciencedirect,Feasibility study on application of an artificial neural network for automatic design of a reactor core at the Kyoto University Critical Assembly,https://api.elsevier.com/content/article/pii/S0149197019302926,"
                  Designing reactor cores by means of an artificial neural network is a difficult challenge, because there are many variables in the core configuration. Especially, for designing a new type of reactor core with an artificial neural network, little (if any) previous data exists, and the appropriate number of results, such as multiplication factors and neutron fluxes, which require a large computational time for a single calculation, should be previously obtained for training the machine learning of the artificial neural network. This paper presents a feasibility study on the automatic design of a research reactor core (a simplified core based on the Kyoto University Critical Assembly) using an artificial neural network. By imitating conventional design procedure, a way to design the core is developed by means of the artificial neural network and automatic machine learning. After setting a design goal of the reactor core, the fuel assembly and core are designed by the proposed method and compared with those designed by conventional design procedure. The results reveal that the reactor core designed by the proposed method performs well and will, therefore, provide a clue to innovation in future reactor design with artificial intelligence.
               ",autonomous vehicle
10.1016/j.ijpe.2020.107621,journal,International Journal of Production Economics,sciencedirect,2020-08-31,sciencedirect,The impact of entrepreneurship orientation on project performance: A machine learning approach,https://api.elsevier.com/content/article/pii/S0925527320300098,"
                  Recent studies in project management have shown the important role of entrepreneurship orientation of the individuals in project performance. Although identifying the role of entrepreneurship orientation as a critical success factor in project performance has been considered as an important issue, it is also important to develop a measurement system for predicting performance based on the degree of an individual's entrepreneurial orientation. In this study, we use predictive analytics by proposing a machine learning approach to predict individuals' project performance based on measures of several aspects of entrepreneurial orientation and entrepreneurial attitude of the individuals. We investigated this relationship using a sample of 185 observations and a range of machine learning algorithms including lasso, ridge, support vector machines, neural networks, and random forest. Our results showed that the best method for predicting project performance is lasso. After identifying the best predictive model, we then used the Bayesian Information Criterion and the Akaike Information Criterion to identify the most significant factors. Our results identify all three aspects of entrepreneurial attitude (social self-efficacy, appearance self-efficacy, and comparativeness) and one aspect of entrepreneurial orientation (proactiveness) as the most important factors. This study contributes to the relationship between entrepreneurship skills and project performance and provides insights into the application of emerging tools in data science and machine learning in operations management and project management research.
               ",autonomous vehicle
10.1016/j.neucom.2021.03.125,journal,Neurocomputing,sciencedirect,2021-08-18,sciencedirect,Supervised deep convolutional generative adversarial networks,https://api.elsevier.com/content/article/pii/S0925231221005178,"
                  Generative adversarial networks (GANs) are one of the most important generative network models. Using real samples, the GAN generates fake samples from the noise given as input to the network. This popular network model, which has recently emerged and consists of several variants, has different applications in many areas. Some of the studies have been implemented by applying GANs to real-world problems. Another part is aimed at improving the performance of GANs or eliminating the disadvantages observed over time. One of these studies is DCGAN. The importance of DCGAN is that it contributes significantly to balancing GAN training with its convolutional architecture. GAN and naturally DCGAN have an unsupervised network structure. While the network is informed that the samples given as input are real or fake, the category label information is not given to the network. In the present study, a method is proposed, which enables creating a supervised network structure when using multi-categories data set with DCGAN structure. The proposed method ensures that noise can be given a category label and this generated category label information can be used in the output layer. This method, which is easily applicable and effective, is named as Supervised DCGAN (SDCGAN).
               ",autonomous vehicle
10.1016/j.nima.2020.164652,journal,"Nuclear Instruments and Methods in Physics Research Section A: Accelerators, Spectrometers, Detectors and Associated Equipment",sciencedirect,2021-01-01,sciencedirect,Machine learning for beam dynamics studies at the CERN Large Hadron Collider,https://api.elsevier.com/content/article/pii/S0168900220310494,"Machine learning entails a broad range of techniques that have been widely used in Science and Engineering since decades. High-energy physics has also profited from the power of these tools for advanced analysis of colliders data. It is only up until recently that Machine Learning has started to be applied successfully in the domain of Accelerator Physics, which is testified by intense efforts deployed in this domain by several laboratories worldwide. This is also the case of CERN, where recently focused efforts have been devoted to the application of Machine Learning techniques to beam dynamics studies at the Large Hadron Collider (LHC). This implies a wide spectrum of applications from beam measurements and machine performance optimisation to analysis of numerical data from tracking simulations of non-linear beam dynamics. In this paper, the LHC-related applications that are currently pursued are presented and discussed in detail, paying also attention to future developments.",autonomous vehicle
10.1016/j.inffus.2019.12.012,journal,Information Fusion,sciencedirect,2020-06-30,sciencedirect,"Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI",https://api.elsevier.com/content/article/pii/S1566253519308103,"
                  In the last few years, Artificial Intelligence (AI) has achieved a notable momentum that, if harnessed appropriately, may deliver the best of expectations over many application sectors across the field. For this to occur shortly in Machine Learning, the entire community stands in front of the barrier of explainability, an inherent problem of the latest techniques brought by sub-symbolism (e.g. ensembles or Deep Neural Networks) that were not present in the last hype of AI (namely, expert systems and rule based models). Paradigms underlying this problem fall within the so-called eXplainable AI (XAI) field, which is widely acknowledged as a crucial feature for the practical deployment of AI models. The overview presented in this article examines the existing literature and contributions already done in the field of XAI, including a prospect toward what is yet to be reached. For this purpose we summarize previous efforts made to define explainability in Machine Learning, establishing a novel definition of explainable Machine Learning that covers such prior conceptual propositions with a major focus on the audience for which the explainability is sought. Departing from this definition, we propose and discuss about a taxonomy of recent contributions related to the explainability of different Machine Learning models, including those aimed at explaining Deep Learning methods for which a second dedicated taxonomy is built and examined in detail. This critical literature analysis serves as the motivating background for a series of challenges faced by XAI, such as the interesting crossroads of data fusion and explainability. Our prospects lead toward the concept of Responsible Artificial Intelligence, namely, a methodology for the large-scale implementation of AI methods in real organizations with fairness, model explainability and accountability at its core. Our ultimate goal is to provide newcomers to the field of XAI with a thorough taxonomy that can serve as reference material in order to stimulate future research advances, but also to encourage experts and professionals from other disciplines to embrace the benefits of AI in their activity sectors, without any prior bias for its lack of interpretability.
               ",autonomous vehicle
10.1016/j.compind.2021.103509,journal,Computers in Industry,sciencedirect,2021-11-30,sciencedirect,Detecting cyberattacks using anomaly detection in industrial control systems: A Federated Learning approach,https://api.elsevier.com/content/article/pii/S0166361521001160,"
                  In recent years, the rapid development and wide application of advanced technologies have profoundly impacted industrial manufacturing, leading to smart manufacturing (SM). However, the Industrial IoT (IIoT)-based manufacturing systems are now one of the top industries targeted by a variety of attacks. In this research, we propose detecting Cyberattacks in Industrial Control Systems using Anomaly Detection. An anomaly detection architecture for the IIoT-based SM is proposed to deploy one of the top most concerned networking technique - a Federated Learning architecture - that can detect anomalies for time series data typically running inside an industrial system. The architecture achieves higher detection performance compared to the current detection solution for time series data. It also shows the feasibility and efficiency to be deployed on top of edge computing hardware of an IIoT-based SM that can save 35% of bandwidth consumed in the transmission link between the edge and the cloud. At the expense, the architecture needs to trade off with the computing resource consumed at edge devices for implementing the detection task. However, findings in maximal CPU usage of 85% and average Memory usage of 37% make this architecture totally realizable in an IIoT-based SM.
               ",autonomous vehicle
10.1016/j.promfg.2020.01.025,journal,Procedia Manufacturing,sciencedirect,2019-12-31,sciencedirect,One-Shot Learning for Custom Identification Tasks; A Review,https://api.elsevier.com/content/article/pii/S2351978920300263,"Deep Learning has great achievements in computer vision for various classification and regression tasks. The automation of tasks such as component sorting, bin-picking and anomaly detection may be of great use in the process industry. However, most machine learning-based object categorization algorithms require training on hundreds or thousands of images and very large datasets. The requirement for large training datasets presents a barrier to the adoption of deep learning methodologies in many custom object classification tasks. For example, in defect detection, positive instances of a defect, take for instance a tank leakage, may seldom occur and therefore creating a dataset of sufficient size for conventional deep learning procedures is not always possible. One-shot learning aims to learn information about object categories from only a handful of labelled examples per category. One-shot learning has received the most attention in face-recognition and person re-identification (re-id) tasks due to their potential practical applications in surveillance security. This research will review these one-shot learning methodologies and investigate how they may be transferred to other domains. Concepts such as Siamese Networks and triplet loss which are commonly used for one-shot learning will be examined. Challenges such as variations in illumination conditions, object pose, camera resolution and partial occlusion will be discussed. Finally, the implications and advantages of deploying such techniques to practical applications in the process industry will be analysed.",autonomous vehicle
10.1016/j.drudis.2018.10.005,journal,Drug Discovery Today,sciencedirect,2019-01-31,sciencedirect,Artificial intelligence and its potential in oncology,https://api.elsevier.com/content/article/pii/S1359644618301636,"
                  The two main branches associated with Artificial Intelligence (AI) in medicine are virtual and physical. The virtual component includes machine learning (ML) and algorithms, whereas physical AI includes medical devices and robots for delivering care. AI is used successfully in tumour segmentation, histopathological diagnosis, tracking tumour development, and prognosis prediction. CURATE.AI, developed at the National University of Singapore, is a platform that automatically decides the optimum dose of drugs for a durable response, allowing the patient to resume a completely normal life. With the involvement of technology multinationals, such as Google and Microsoft, in AI and healthcare in association with leading healthcare companies, the future of AI in healthcare looks very promising.
               ",autonomous vehicle
10.1016/B978-0-12-822830-2.00003-9,journal,Unmanned Driving Systems for Smart Trains,sciencedirect,2021-12-31,sciencedirect,Chapter 3: Train unmanned driving algorithm based on reasoning and learning strategy,https://api.elsevier.com/content/article/pii/B9780128228302000039,"
               In this chapter, train unmanned driving algorithms based on the reasoning and learning strategy are introduced. Mainly including three aspects, namely the current status and technical progress of train unmanned controlling algorithm, the connotation and composition of train unmanned driving algorithm, and the calculation process and analysis of train unmanned driving algorithm. To comprehensively evaluate the unmanned train algorithm used, the positioning and navigation phase, the path planning phase, and the object detection phase are described. With the rapid development of artificial intelligence technology, more and more machine learning algorithms have been applied to unmanned driving applications. In the future development process, the train unmanned and automated degree will be greatly improved.
            ",autonomous vehicle
10.1016/B978-0-12-824495-1.00011-5,journal,Sustainable Natural Gas Reservoir and Production Engineering,sciencedirect,2022-12-31,sciencedirect,Chapter Three: Machine learning to improve natural gas reservoir simulations,https://api.elsevier.com/content/article/pii/B9780128244951000115,"
               Natural gas reservoir simulation, as a physics-based numerical method, needs to be carried out with a high level of precision. If not, it may be highly misleading and cause substantial losses, poor estimation of ultimate recovery factor, and wasted effort. Although simple simulations often provide acceptable approximations, there is a continued desire to develop more sophisticated simulation strategies and techniques. Given the capabilities of Machine Learning (ML) and their general acceptance in recent decades, this chapter considers the application of these techniques to gas reservoir simulations. The aspiration ML technics should be capable of providing some improvements in terms of both accuracy and speed. The simulation of gas reservoirs (dry gas, wet gas, and retrograde gas-condensate) is introduced along with its fundamental concepts and governing equations. More specific and advanced concepts of applying ML in modern reservoir simulation models are described and justified, particularly with respect to history matching and proxy models. Reservoir simulation assisted by machine learning is becoming increasingly applied to assess suitably of reservoirs for carbon capture and sequestration associated with enhanced gas recovery. Such applications, and the ability to improve reservoir performance via production efficiency, make ML-assisted reservoir simulation a valuable approach for improving the sustainability of natural gas reservoirs. The concepts are reinforced using a case study applying two ML models providing dew point pressure predictions for gas condensate reservoirs.
            
               Banner headline
               Reservoir simulation methods applied to gas reservoirs are reviewed and the key influencing variables identified. Machine Learning (ML) methods can be applied in various ways to improve the performance of gas reservoir simulations, especially in respect to history matching and proxy modeling. Additionally, ML can assist the CO2 sequestration and enhanced gas recovery, well placement optimization, production optimization, estimation of gas production, dew point prediction in gas condensate reservoirs, and pressure and rate transient analysis.
            ",autonomous vehicle
10.1016/j.jweia.2020.104320,journal,Journal of Wind Engineering and Industrial Aerodynamics,sciencedirect,2020-11-30,sciencedirect,"Emerging frontiers in wind engineering: Computing, stochastics, machine learning and beyond",https://api.elsevier.com/content/article/pii/S0167610520302300,"
                  Over the last several decades, wind engineering a multi-disciplinary subject involving engineering meteorology, fluid dynamics, structural dynamics, structural engineering, probabilistic methods, and design has addressed the challenges posed by winds of synoptic and non-synoptic origins. Combined computational approaches and laboratory to full-scale experiments have enhanced our ability to design and construct wind-resistant structures that range from low-rise to supertall buildings, footbridges to super long-span bridges and, wind turbines on the ground and floating foundations and floating offshore drilling and production systems. During this period, we have seen extraordinary advances in experimental facilities, instrumentation and data acquisition and management. At the laboratory scale, new wind tunnels have emerged with added features like extra-wide cross-sections, from passive to active driving systems, from boundary layer to flow simulators with vortical flows mimicking non-synoptic winds features. While at full-scale, we have been able to use deployable sensing networks in the path of landfalling hurricanes/typhoons to monitoring in real-time the performance of tall buildings and long-span bridges during extreme wind events. Advanced technologies like aerial surveying using drones and satellite imagery have been employed to enhance the post-storm surveillance capabilities. These advances have enabled us to build a cadre of civil infrastructure that meets some of the challenges posed by the extreme winds. Yet there remain several frontiers that still need to be addressed for example the three “Nons,” the triple emerging fronts, i.e., non-stationarity, non-Gaussianity, non-linearity prevalent in the changing dynamic of winds prevailing in gust fronts, vortical and convective systems, rolls, meso-scale features and intermittent turbulence. In the face of these challenges, increasing heights, spans, and depths of structures exposed to these winds pose additional challenges as their performance becomes more sensitive to their dynamics, thus necessitating new tools and perspectives that go beyond customary analysis and modeling norms. Fortunately, amidst these challenges, there are new opportunities to complement our existing capabilities as the burgeoning growth in computational resources and parallel computational advances coupled with data analytics and AI-based schemes, e.g., machine learning hold the promise of expanding our modeling and simulation capacity far beyond our current conventional schemes offer. All these advances can be couched in a Generalized Wind Loading Chain to capture the three “Nons” by building upon the wind loading chain proposed by Davenport based on linear and stationary conditions. An example of the Gust Front Factor in this framework is an effective means for designing under non-synoptic winds. This paper expands on these new computational opportunities and ways to take advantage of their added capabilities to address emerging challenges in building a resilient and sustainable civil infrastructure and beyond to stability and safety of high-speed trains.
               ",autonomous vehicle
10.1016/j.psep.2018.08.021,journal,Process Safety and Environmental Protection,sciencedirect,2018-11-30,sciencedirect,Supervised machine learning techniques in the desulfurization of oil products for environmental protection: A review,https://api.elsevier.com/content/article/pii/S0957582018303641,"
                  Desulfurization, known as the removal of sulfur from oil, is extremely important in the petroleum processing industry and in the environmental protection. Several oil-upgrading processes such as desulfurization and catalysts such as alumina loaded with molybdenum have been proposed to deal with the problem of removing sulfur-containing compounds from light oil. Thus, several parameters are required to be experimentally optimized which demands a lot of work including reagents. Advanced mathematical tools can be used to optimize the desulfurization process and to study the related factors. The modeling and simulation of the desulfurization process have been proposed in several studies in order to facilitate a better understanding of the process operations. Machine Learning (ML) is regarded as a promising methodological area to perform such optimization and analysis. This review describes the relevant methods for dealing with the applications of ML for desulfurization in oil. Although a good number of research papers have appeared in recent years, the application of ML for desulfurization is still a promising area of research. The review presents an overview of the ML methods and their categories in desulfurization. It discusses and compares the methods that employ ML to optimize the desulfurization process. The review also highlights the findings and possible research directions.
               ",autonomous vehicle
10.1016/j.neucom.2018.07.101,journal,Neurocomputing,sciencedirect,2020-07-05,sciencedirect,Deep Learning Clusters in the Cognitive Packet Network,https://api.elsevier.com/content/article/pii/S0925231219304473,"
                  The Cognitive Packet Network (CPN) bases its routing decisions and flow control on the Random Neural Network (RNN) Reinforcement Learning algorithm; this paper proposes the addition of a Deep Learning (DL) Cluster management structure to the CPN for Quality of Service metrics (Delay Loss and Bandwidth), Cyber Security keys (User, Packet and Node) and Management decisions (QoS, Cyber and CEO). The RNN already models how neurons transmit information using positive and negative impulsive signals whereas the proposed additional Deep Learning structure emulates the way the brain learns and takes decisions; this paper presents a brain model as the combination of both learning algorithms, RNN and DL. The proposed model has been simulated under different network sizes and scenarios and it has been validated against the CPN itself without DL clusters. The simulation results are promising; the presented CPN with DL clusters as a mechanism to transmit, learn and make packet routing decisions is a step closer to emulate the way the brain transmits information, learns the environment and takes decisions.
               ",autonomous vehicle
10.1016/j.apenergy.2020.115733,journal,Applied Energy,sciencedirect,2020-11-15,sciencedirect,"Artificial intelligence techniques for stability analysis and control in smart grids: Methodologies, applications, challenges and future directions",https://api.elsevier.com/content/article/pii/S0306261920312228,"
                  Smart grid is the new trend for clean, sustainable, efficient and reliable energy generation, delivery and use. To ensure stable and secure operation is essential for the smart grid, which needs effective stability analysis and control. As the smart grid has evolved through a growing scale of interconnection, increasing integration of renewable energy, widespread operation of direct current power transmission systems, and liberalization of electricity markets, the stability characteristics of it are much more complex than the past. Due to these changes, conventional stability analysis and control approaches have a series of drawbacks in terms of speed, effectiveness and economy. On the contrary, the emerging artificial intelligence (AI) techniques provide powerful and promising tools for stability analysis and control in smart grids and have attracted growing attention. This paper aims to give a comprehensive and clear picture of recent advances in this research area. First, we present a general overview of AI, including its definitions, history and state-of-the-art methodologies. And then, this paper gives a comprehensive review of its applications to security assessment, stability assessment, fault diagnosis, and stability control in smart grids. These applications have achieved impressive results. Nevertheless, we also identify some major challenges these applications face in practice: high requirements on data, imbalanced learning, interpretability of AI, difficulties in transfer learning, the robustness of AI to communication quality, and the robustness against attack or adversarial examples. Furthermore, we provide suggestions for potential important future investigation directions to overcome these challenges and bridge the gap between research and practice.
               ",autonomous vehicle
10.1016/j.solener.2020.03.104,journal,Solar Energy,sciencedirect,2020-05-15,sciencedirect,A review on machine learning algorithms to predict daylighting inside buildings,https://api.elsevier.com/content/article/pii/S0038092X20303509,"
                  Steep increases in air temperatures and CO2 emissions have been associated with the global demand for energy. This is coupled with population growth and improved living standards that encourages the reliance on mechanical acclimatization. Lighting energy alone is responsible for a large portion of total energy consumption in office buildings; and the demand for artificial light is expected to grow in the next years. One of sustainable approaches to enhance energy-efficiency is to incorporate daylighting strategies, which entail the controlled use of daylight inside buildings. Daylight simulation is an active area of research that offers accurate estimations, yet requires a complex set of inputs. Even with today’s computers, simulations are computationally expensive and time-consuming, hindering to acquire accelerated preliminary approximations in acceptable timeframes, especially for the iterative design alternatives. Alternatively, predictive models that build on machine learning algorithms have granted much interest from the building design community due to their ability to handle such complex non-linear problems, acting as proxies to heavy simulations. This research presents a review on the growing directions that exploit machine learning to rapidly predict daylighting performance inside buildings, putting a particular focus on scopes of prediction, used algorithms, data sources and sizes, besides evaluation metrics. This work should improve architects’ decision-making and increase the applicability to predict daylighting. Another implication is to point towards knowledge gaps and missing opportunities in the related research domain, revealing future trends that allow for such innovative approaches to be exploited more commonly in Architectural practice.
               ",autonomous vehicle
10.1016/B978-0-444-64241-7.50008-2,journal,Computer Aided Chemical Engineering,sciencedirect,2018-12-31,sciencedirect,Reinforcement Learning – Overview of Recent Progress and Implications for Process Control,https://api.elsevier.com/content/article/pii/B9780444642417500082,"
                  
                     This paper provides a brief introduction to Reinforcement Learning (RL) technology, summarizes recent developments in this area, and discusses their potential implications for the field of process control. The paper begins with a brief introduction to RL, a machine learning technology that allows an agent to learn, through trial and error, the best way to accomplish a task. We then highlight two new developments in RL that have led to the recent wave of applications and media interest. A comparison of the key features of RL and Model Predictive Control (MPC) is then presented in order to clarify their similarities and differences. This is followed by an assessment of five ways that RL technology can potentially be used in process control applications. A final section summarizes our conclusions and lists directions for future RL research that may improve its relevance for process control applications.
               ",autonomous vehicle
10.1016/B978-0-12-821982-9.00007-1,journal,"Quantum Information Processing, Quantum Computing, and Quantum Error Correction",sciencedirect,2021-12-31,sciencedirect,Chapter 14: Quantum Machine Learning,https://api.elsevier.com/content/article/pii/B9780128219829000071,"
               The chapter starts by reviewing relevant classical machine learning algorithms, categorized as supervised, unsupervised, and reinforcement learning algorithms. The following topics from classical machine learning are described: principal component analysis (PCA), support vector machines (SVMs), clustering, boosting, regression analysis, and neural networks. In the PCA section we describe how to determine the principal components from the correlation matrix, followed by a description of singular value decomposition-based PCA and scoring phase. We describe SVMs from both geometric and Lagrangian method-based points of view and introduce both hard and soft margins as well as the kernel method. In the clustering section we describe K-means, expectation maximization, and K-nearest neighbor algorithms. We also describe how to evaluate the clustering quality. In the boosting section, we describe how to build strong learners from weak learners, with special attention devoted to the AdaBoost algorithm. In the regression analysis section we describe the least-squares estimation, the pseudoinverse approach, and the ridge regression method. In the neural networks' section we describe in detail: perceptron, activation functions, and feedforward networks. The focus then moves to the quantum machine learning (QML) algorithms. We first describe the Ising model and relate it to the quadratic unconstrained binary optimization (QUBO) problem. We then study how to solve the QUBO problem by adiabatic quantum computing and quantum annealing. To perform QML using imperfect and noisy quantum circuits we describe the variational quantum eigensolver and quantum approximate optimization algorithm (QAOA). To illustrate the impact of QAOA we describe how to use it in combinatorial optimization problems and how to solve the MAX-CUT problem. Next, quantum boosting is discussed, and is related to the QUBO problem. Quantum random access memory is described next, allowing us to address the superposition of memory cells with the help of a quantum register. Quantum matrix inversion, also known as the Harrow–Hassidim–Lloyd algorithm, is then described, which is used as a basic ingredient for other QML algorithms such as quantum PCA, which is described as well. In the quantum optimization-based clustering section we describe how the MAX-CUT problem can be related to clustering, and thus be solved by adiabatic computing, quantum annealing, and QAOA. Grover algorithm-based quantum optimization is discussed next. In the quantum K-means section we describe how to calculate the dot product and quantum distance, followed by the Grover search-based K-means algorithm. In the quantum SVM section we formulate the SVM problem using least squares and describe how to solve it using quantum matrix inversion. In the quantum neural networks (QNNs) section we describe feedforward QNNs, quantum perceptron, and quantum convolutional networks.
            ",autonomous vehicle
10.1016/j.addma.2020.101538,journal,Additive Manufacturing,sciencedirect,2020-12-31,sciencedirect,Machine learning in additive manufacturing: State-of-the-art and perspectives,https://api.elsevier.com/content/article/pii/S2214860420309106,"
                  Additive manufacturing (AM) has emerged as a disruptive digital manufacturing technology. However, its broad adoption in industry is still hindered by high entry barriers of design for additive manufacturing (DfAM), limited materials library, various processing defects, and inconsistent product quality. In recent years, machine learning (ML) has gained increasing attention in AM due to its unprecedented performance in data tasks such as classification, regression and clustering. This article provides a comprehensive review on the state-of-the-art of ML applications in a variety of AM domains. In the DfAM, ML can be leveraged to output new high-performance metamaterials and optimized topological designs. In AM processing, contemporary ML algorithms can help to optimize process parameters, and conduct examination of powder spreading and in-process defect monitoring. On the production of AM, ML is able to assist practitioners in pre-manufacturing planning, and product quality assessment and control. Moreover, there has been an increasing concern about data security in AM as data breaches could occur with the aid of ML techniques. Lastly, it concludes with a section summarizing the main findings from the literature and providing perspectives on some selected interesting applications of ML in research and development of AM.
               ",autonomous vehicle
10.1016/j.comnet.2021.107859,journal,Computer Networks,sciencedirect,2021-04-22,sciencedirect,Application of Internet of Things and artificial intelligence for smart fitness: A survey,https://api.elsevier.com/content/article/pii/S1389128621000360,"
                  The revolution of Internet of Things (IoT) is pervading many facets of our everyday life. Among the multiple IoT application domains, well-being is becoming one of the popular scenarios in IoT which aims to offer new services including smart fitness. This paper focuses on smart fitness covering IoT-based solutions for this domain as well as the impacts of artificial intelligence and social-IoT. IoT-based smart fitness is divided into three categories: Fitness trackers (including wearable and non-wearable sensors), movement analysis and fitness applications. Data collected from IoT-based smart fitness and users could be used for enhancing training performance by Artificial Intelligence (AI)-based algorithms. Sensor to sensor relationship is another notable topic which can be implemented by social-IoT that can share data, information and experiences of users’ training from different places and times. In this his study a comprehensive review on different types of fitness trackers and fitness applications in provided and followed by a review of AI algorithms used in smart fitness scenarios. Lastly detail discussions on the benefits and the potential problems of smart fitness are presented and a shortlist of existing gaps and potential future work have been identified and proposed.
               ",autonomous vehicle
10.1016/B978-0-12-819043-2.00006-X,journal,Innovation in Health Informatics,sciencedirect,2020-12-31,sciencedirect,Chapter 6: Application of machine learning and image processing for detection of breast cancer,https://api.elsevier.com/content/article/pii/B978012819043200006X,"
               Information technology is playing an important role in healthcare systems. Communication technologies enable the user to stay in contact with the physician. The physician is also staying in contact with his patient. Smart wearable sensing devices are easily available in a smart environment. The patient wears these sensing devices and performs daily life routine actions. If there is any disturbance in the patient body than sensing devices sense it and send the message to the doctor about the patient condition. Through this process, the physician can give proper treatment to a patient according to the condition of the patient at that time. Patient life can be saved using these innovations in smart healthcare systems. Breast cancer is a critical healthcare problem. The computer can solve this problem using advanced algorithms of artificial intelligence (AI). Breast cancer is the second leading disease causes woman deaths. According to a report, 2.5 million breast cancer cases identified in the United States in 2017. According to IARC, 8.2 million deaths caused by cancer per year. Deaths rate from cancer is increasing developed countries every year. Cancer is the name of uncontrolled cell growth in the body. Breast cancer occurs due growth of cells in the breast. Group of extra tissues is known as tumor. Cancer is the deadliest disease in the whole world. Cancer has stages. It can be cured in the first stage. After the first stage, it can never be cured. But in the first stage of cancer, it is very difficult to diagnose it. Early stage detection of cancer can save millions of lives. Computer-aided detection is used for early detection of cancer. X-ray image of the breast is known as a mammogram. Breast is compressed between the two plates and the x-ray beam is applied to take mammogram. The technique which is applied before symptoms occur in woman breast is known as screening mammography. Mammography images have noise. Image processing techniques are used to remove noise from a mammogram. In the first-phase preprocessing is done. Filters are applied to remove noise from mammograms. After preprocessing noise-free image is passed through the segmentation phase. In segmentation, the region of interest (ROI) is found. ROI is the region in the image in which a tumor exists. After the segmentation feature is extracted. The whole image’s dataset is passed from above phases. After this, text or comma-separated values (CSV) file is obtained which has the features of mammogram dataset. Dataset has two labels “0” and “1.” Label “1” indicates cancer and label “0” indicates noncancer image. Feature extracted dataset is also labeled using mammogram labeled. Subfield of AI is machine learning (ML). It has two major types that are supervised and unsupervised. Supervised learning has labeled data for training. Unsupervised learning is trained using unlabeled data. Dataset is divided into two parts. One is used for training is known as training dataset. Second is used for testing is known as test data. Training data are used for training and testing data are used for the testing model. In the end, a different measure is calculated using different formulas such as precision, accuracy, and f-score using confusing matrix. If the reasonable accuracy is achieved when the model is considered for prediction. Feature is extracted from new samples and label is predicted for that case. Either case has cancer or not. Our work is beneficial to save human lives. We applied ML algorithms to detect cancer from a mammogram. We achieved reasonable accuracy for the prediction of cancer.
            ",autonomous vehicle
10.1016/j.compind.2021.103485,journal,Computers in Industry,sciencedirect,2021-10-31,sciencedirect,Deep learning-based visual control assistant for assembly in Industry 4.0,https://api.elsevier.com/content/article/pii/S0166361521000920,"
                  Product assembly is a crucial process in manufacturing plants. In Industry 4.0, the offer of mass-customized products is expanded, thereby increasing the complexity of the assembling phase. This implies that operators should pay close attention to small details, potentially resulting in errors during the manufacturing process owing to its high level of complexity. To mitigate this, we propose a novel architecture that evaluates the activities of an operator during manual assembly in a production cell so that errors in the manufacturing process can be identified, thus avoiding low quality in the final product and reducing rework and waste of raw materials or time. To perform this assessment, it is necessary to use state-of-the-art computer vision techniques, such as deep learning, so that tools, components, and actions may be identified by visual control systems. We develop a deep-learning-based visual control assembly assistant that enables real-time evaluation of the activities in the assembly process so that errors can be identified. A general-use language is developed to describe the actions in assembly processes, which can also be used independently of the proposed architecture. Finally, we generate two datasets with annotated data to be fed to the deep learning methods, the first for the recognition of tools and accessories and the second for the identification of basic actions in manufacturing processes. To validate the proposed method, a set of experiments are conducted, and high accuracy is obtained.
               ",autonomous vehicle
10.1016/j.asoc.2020.106855,journal,Applied Soft Computing,sciencedirect,2021-01-31,sciencedirect,Standardized Variable Distances: A distance-based machine learning method,https://api.elsevier.com/content/article/pii/S1568494620307936,"
                  Today, machine learning algorithms are an important research area capable of analyzing and modeling data in any field. Information obtained through machine learning methods helps researchers and planners to understand and review systematic problems of their current strategies. Thus, it is very important to work fully in every field that facilitates human life, such as early and correct diagnosis, correct choice, fully functioning autonomous systems. In this paper, a novel machine learning algorithm for multiclass classification is presented. The proposed method is designed based on the Minimum Distance Classifier (MDC) algorithm. The MDC is variance-insensitive because it classifies input vectors by calculating their distances/similarities with respect to class-centroids (average value of input vectors of a class). As it is known, real-world data contains certain proportions of noise. This situation negatively affects the performance of the MDC. To overcome this problem, we developed a variance-sensitive model, which we call Standardized Variable Distances (SVD), considering the standard deviation and z-score (standardized variable) factors. To ensure the accuracy of the SVD, we used Wisconsin Breast Cancer Original (WBCO) and LED Display Domain (led7digit) datasets, which we obtained from UCI machine learning repository, with 5-fold cross validation. It was compared and analyzed classification performance of the SVD with Decision Tree (DT), Random Forest (RF), k-Nearest Neighbor (k-NN), Multinomial Logistic Regression (MLR), Naïve Bayes (NB), Support Vector Machine (SVM), and the Minimum Distance Classifier (MDC), which are well-known in the literature. It has also been compared thirteen different studies using the same datasets over the past five years. Our results in the experimental studies have shown that the SVD can classify better than traditional and state-of-the-art methods, compared in this study. The proposed method reached over 97% classification accuracy (CACC), F-measure (FM) and area under the curve (AUC) on the WBCO dataset. On the led7digit dataset, approximately 74% CACC, 75.1% FM and 82.2% AUC scores were obtained. It has been observed that the classification scores obtained with the SVD are higher than other ML algorithms used in the experimental studies.
               ",autonomous vehicle
10.1016/j.clp.2020.05.002,journal,Clinics in Perinatology,sciencedirect,2020-09-30,sciencedirect,Machine Learning to Support Hemodynamic Intervention in the Neonatal Intensive Care Unit,https://api.elsevier.com/content/article/pii/S0095510820300427,,autonomous vehicle
10.1016/j.neucom.2018.01.020,journal,Neurocomputing,sciencedirect,2018-04-05,sciencedirect,Deep reinforcement learning for extractive document summarization,https://api.elsevier.com/content/article/pii/S0925231218300377,"
                  We present a novel extractive document summarization approach based on a Deep Q-Network (DQN), which can model salience and redundancy of sentences in the Q-value approximation and learn a policy that maximize the Rouge score with respect to gold summaries. We design two hierarchical network architectures to not only generate informative features from the document to represent the states of DQN, but also create a list of potential actions from sentences in the document for the DQN. At training time, our model is directly trained on reference summaries generated by human, eliminating the need for sentence-level extractive labels. For testing, we evaluate this model on the CNN/Daily corpus, the DUC 2002 dataset and the DUC 2004 dataset using Rouge metric. Our experiments show that our approach achieves performance which is better than or comparable to state-of-the-art models on these corpora without any access to linguistic annotation. This is the first time DQN has been applied to extractive summarization tasks.
               ",autonomous vehicle
10.1016/j.rser.2021.111530,journal,Renewable and Sustainable Energy Reviews,sciencedirect,2021-11-30,sciencedirect,A systematic literature review on the use of artificial intelligence in energy self-management in smart buildings,https://api.elsevier.com/content/article/pii/S136403212100808X,"Buildings are one of the main consumers of energy in cities, which is why a lot of research has been generated around this problem. Especially, the buildings energy management systems must improve in the next years. Artificial intelligence techniques are playing and will play a fundamental role in these improvements. This work presents a systematic review of the literature on researches that have been done in recent years to improve energy management systems for smart building using artificial intelligence techniques. An originality of the work is that they are grouped according to the concept of “Autonomous Cycles of Data Analysis Tasks”, which defines that an autonomous management system requires specialized tasks, such as monitoring, analysis, and decision-making tasks for reaching objectives in the environment, like improve the energy efficiency. This organization of the work allows us to establish not only the positioning of the researches, but also, the visualization of the current challenges and opportunities in each domain. We have identified that many types of researches are in the domain of decision-making (a large majority on optimization and control tasks), and defined potential projects related to the development of autonomous cycles of data analysis tasks, feature engineering, or multi-agent systems, among others.",autonomous vehicle
10.1016/j.trc.2021.103225,journal,Transportation Research Part C: Emerging Technologies,sciencedirect,2021-08-31,sciencedirect,Hierarchical integrated machine learning model for predicting flight departure delays and duration in series,https://api.elsevier.com/content/article/pii/S0968090X21002394,"
                  Flight delays may propagate through the entire aviation network and are becoming an important research topic. This paper proposes a novel hierarchical integrated machine learning model for predicting flight departure delays and duration in series rather than in parallel to avoid ambiguity in decision making. The paper analyses the proposed model using various machine learning algorithms in combination with different sampling techniques. The highly noisy, unbalanced, dispersed, and skewed historical high dimensional data provided by an international airline operating in Hong Kong was used to demonstrate the practical application of the model. The result shows that for a 4-h forecast horizon, a constructive neural network machine learning algorithm with the Synthetic Minority Over Sampling Technique-Tomek Links (SMOTETomek) sampling technique was able to achieve better average balanced recall accuracies of 65.5%, 61.5%, 59% for classifying delay status and predicting delay duration at thresholds of 60 min and 30 min, respectively. Similarly, for minority labels, the precision-recall and area under the curve showed that the proposed model achieved better results of 32.44% and 35.14% compared to the parallel model of 26.43% and 21.02% for thresholds of 60 min and 30 min, respectively. The effect of different sampling techniques, sampling approaches, and estimation mechanisms on prediction performance is also studied.
               ",autonomous vehicle
10.1016/j.neunet.2008.06.013,journal,Neural Networks,sciencedirect,2008-08-31,sciencedirect,Two forms of immediate reward reinforcement learning for exploratory data analysis,https://api.elsevier.com/content/article/pii/S0893608008001342,"
                  We review two forms of immediate reward reinforcement learning: in the first of these, the learner is a stochastic node while in the second the individual unit is deterministic but has stochastic synapses. We illustrate the first method on the problem of Independent Component Analysis. Four learning rules have been developed from the second perspective and we investigate the use of these learning rules to perform linear projection techniques such as principal component analysis, exploratory projection pursuit and canonical correlation analysis. The method is very general and simply requires a reward function which is specific to the function we require the unit to perform. We also discuss how the method can be used to learn kernel mappings and conclude by illustrating its use on a topology preserving mapping.
               ",autonomous vehicle
10.1016/j.compeleceng.2020.106767,journal,Computers & Electrical Engineering,sciencedirect,2020-10-31,sciencedirect,Medical image registration using deep neural networks: A comprehensive review,https://api.elsevier.com/content/article/pii/S0045790620306224,"
                  Image-guided interventions are saving the lives of a large number of patients where the image registration should indeed be considered as the most complex and complicated issue to be tackled. On the other hand, a huge progress in the field of machine learning has recently made by the possibility of implementing deep neural networks on the contemporary many-core GPUs. It has opened up a promising window to challenge with many medical applications in more efficient and effective ways, where the registration is not an exception. In this paper, a comprehensive review on the state-of-the-art literature known as medical image registration using deep neural networks is presented. The review is systematic and encompasses all the related works previously published in the field. Key concepts, statistical analysis from different points of view, confining challenges, novelties and main contributions, key-enabling techniques, future directions, and prospective trends all are discussed and surveyed in details in this comprehensive review. This review allows a deep understanding and insight for the readers active in the field who are investigating the state-of-the-art and seeking to contribute the future literature.
               ",autonomous vehicle
10.1016/j.engappai.2021.104398,journal,Engineering Applications of Artificial Intelligence,sciencedirect,2021-09-30,sciencedirect,Fair classification via Monte Carlo policy gradient method,https://api.elsevier.com/content/article/pii/S0952197621002463,"
                  Artificial intelligence is steadily increasing its impact on everyday life. Therefore, the societal issues of artificial intelligence have become an important concern in the AI research. The presence of data that reflects human biases towards historically discriminated groups defined by sensitive features such as race and gender, results in machine learning models which discriminate against these groups. In order to tackle the impact of bias in data, researchers developed a variety of specialized machine learning algorithms which are able to satisfy different fairness constraints imposed on the model. Group fairness constraints do not fit standard machine learning formulations easily due to their non-differentiable nature. In this paper we developed a technique for learning a fair classifier by Monte Carlo policy gradient method which naturally deals with such non-differentiable constraints. Our methodology focuses on direct optimization of both group fairness metric and predictive performance of the model. In addition, we propose two different variance reduction techniques of gradient estimation. We compare our models to seven other related and state-of-the-art models and demonstrate that they are able to achieve better trade-off between accuracy and unfairness. To the best of our knowledge, this is the first fair classification algorithm which solves the issue of non-differentiable constraints by reinforcement learning techniques.
               ",autonomous vehicle
10.1016/B978-0-12-822844-9.00032-3,journal,Recent Trends in Computational Intelligence Enabled Research,sciencedirect,2021-12-31,sciencedirect,Chapter 6: Computational intelligence techniques for cancer diagnosis,https://api.elsevier.com/content/article/pii/B9780128228449000323,"
               Computational intelligence (CI) is a group of techniques for building machine intelligence. These techniques follow a pragmatic approach to learning and decision making rather than a hard approach as in expert systems or rule-based systems. These techniques use soft computing, fuzzy logic, and evolutionary methods to enable a machine to learn and make a decision under different situations. This is different from artificial intelligence which is based on crisp logic rules. In this chapter, we discuss the applications of important techniques under CI that have been applied to the computational diagnosis of cancers. These important techniques include fuzzy logic, artificial neural networks, evolutionary computation based on principles of natural selection, learning theory which is the study of learning mechanism of natural organisms, probabilistic or random methods that inherently account for uncertainty in input or events. Fuzzy logic is applied in the identification of tumors in medical imaging reports where approximate reasoning is helpful. Neural networks like convolutional neural networks have been demonstrated to accurately identify and classify the various type of tumors. An evolutionary computation or natural computation methods apply the principle of natural selection to solve a multiobjective optimization problem. These methods, including swarm intelligence, etc., have wide applications in the areas of genomic data analysis. CI heavily uses the principles of learning theory to understand the cognitive processes of various natural organisms. Learning theory involves the study of how the processes of cognition, emotion regulation, accounting for environmental influences, and experience aid in gathering, improving, or changing knowledge. It also examines how these are processed and used for the prediction of future events based on experience. The application of learning theory to the prediction of the incidence of a disease is an interesting area of research. Probabilistic methods are characterized by randomness. Many probabilistic methods like Bayesian networks are used in the diagnosis of cancers. The mathematical basis is mainly machine learning methods like regression, support vector machines, decision trees, etc. In this chapter, we have analyzed commonly used CI techniques as applied to research into the classification and diagnosis of cancerous tumors and compiled a list of the most promising techniques that have improved the accuracy of diagnosis or have led to better outcomes for cancer patients.
            ",autonomous vehicle
10.1016/j.neunet.2018.03.009,journal,Neural Networks,sciencedirect,2018-07-31,sciencedirect,Personalized response generation by Dual-learning based domain adaptation,https://api.elsevier.com/content/article/pii/S0893608018300947,"
                  Open-domain conversation is one of the most challenging artificial intelligence problems, which involves language understanding, reasoning, and the utilization of common sense knowledge. The goal of this paper is to further improve the response generation, using personalization criteria. We propose a novel method called PRGDDA (Personalized Response Generation by Dual-learning based Domain Adaptation) which is a personalized response generation model based on theories of domain adaptation and dual learning. During the training procedure, PRGDDA first learns the human responding style from large general data (without user-specific information), and then fine-tunes the model on a small size of personalized data to generate personalized conversations with a dual learning mechanism. We conduct experiments to verify the effectiveness of the proposed model on two real-world datasets in both English and Chinese. Experimental results show that our model can generate better personalized responses for different users.
               ",autonomous vehicle
10.1016/j.ecoser.2018.04.004,journal,Ecosystem Services,sciencedirect,2018-10-31,sciencedirect,Machine learning for ecosystem services,https://api.elsevier.com/content/article/pii/S2212041617306423,"Recent developments in machine learning have expanded data-driven modelling (DDM) capabilities, allowing artificial intelligence to infer the behaviour of a system by computing and exploiting correlations between observed variables within it. Machine learning algorithms may enable the use of increasingly available ‘big data’ and assist applying ecosystem service models across scales, analysing and predicting the flows of these services to disaggregated beneficiaries. We use the Weka and ARIES software to produce two examples of DDM: firewood use in South Africa and biodiversity value in Sicily, respectively. Our South African example demonstrates that DDM (64–91% accuracy) can identify the areas where firewood use is within the top quartile with comparable accuracy as conventional modelling techniques (54–77% accuracy). The Sicilian example highlights how DDM can be made more accessible to decision makers, who show both capacity and willingness to engage with uncertainty information. Uncertainty estimates, produced as part of the DDM process, allow decision makers to determine what level of uncertainty is acceptable to them and to use their own expertise for potentially contentious decisions. We conclude that DDM has a clear role to play when modelling ecosystem services, helping produce interdisciplinary models and holistic solutions to complex socio-ecological issues.",autonomous vehicle
10.1016/B978-0-12-813086-5.00006-2,journal,Biomedical Signal Analysis for Connected Healthcare,sciencedirect,2021-12-31,sciencedirect,6: Machine learning for biomedical signal analysis,https://api.elsevier.com/content/article/pii/B9780128130865000062,"
               In this chapter, we have investigated the conceptual framework of some popular machine learning (ML) algorithms and how they could be extended to biomedical signal analysis applications. Signal feature extraction techniques provide some form of systematic and compact representation of biomedical signals, and they form as the input to most ML models except in cases such as deep learning (DL) in which explicit feature extraction is not a necessary step. It's important to select features carefully that are suitable for an ethical and fair ML, and be as representative in ensuring equity, diversity, and inclusion in the data collection and training phases of ML algorithm, so that unwanted biases could be avoided. The right selection of ML algorithm depends on the application in hand, and how well a dataset has been curated and labeled. Also, the hardware platform plays a role in the appropriate selection of the ML algorithm. If the biomedical application has large computational resources and bandwidth to process, then cloud-based ML such as DL could be pursued. In cases where resources are constrained and/or speed is important, then edge ML or tiny ML approaches could be deployed. In case of tiny ML applications, the DL or classical ML algorithm could be pruned, quantized, and optimized by using criteria to suit microcontroller implementations that are inbuilt in most wearable biomedical devices. Guidelines on how the train and test datasets should be split and what forms of metrics should be considered in assessing the performances of the ML outcomes are also covered in this chapter.
            ",autonomous vehicle
10.1016/j.future.2020.04.041,journal,Future Generation Computer Systems,sciencedirect,2020-10-31,sciencedirect,A machine learning forensics technique to detect post-processing in digital videos,https://api.elsevier.com/content/article/pii/S0167739X20306786,"
                  Technology has brought great benefits to human beings and has served to improve the quality of life and carry out great discoveries. However, its use can also involve many risks. Examples include mobile devices, digital cameras and video surveillance cameras, which offer excellent performance and generate a large number of images and video. These files are generally shared on social platforms and are exposed to any manipulation, compromising their authenticity and integrity. In a legal process, a manipulated video can provide the necessary elements to accuse an innocent person of a crime or to exempt a guilty person from criminal acts. Therefore, it is essential to create robust forensic methods, which will strengthen the justice administration systems and thus make fair decisions. This paper presents a novel forensic technique to detect the post-processing of digital videos with MP4, MOV and 3GP formats. Concretely, detect the social platform and editing program used to execute possible manipulation attacks. The proposed method is focused on supervised machine learning techniques. To achieve our goal, we take advantage that the social platforms and editing programs, execute filtering and compression processes on the videos when they are shared or manipulated. The result of these transformations leaves a characteristic pattern in the videos that allow us to detect the social platform or editing program efficiently. Three phases are involved in the method: 1) Dataset preparation; 2) data features extraction; 3) Supervised model creation. To evaluate the scalability of the technique in real scenarios, we used a robust, heterogeneous and far superior dataset than that used in the literature.
               ",autonomous vehicle
10.1016/j.artint.2007.08.001,journal,Artificial Intelligence,sciencedirect,2008-03-31,sciencedirect,Restricted gradient-descent algorithm for value-function approximation in reinforcement learning,https://api.elsevier.com/content/article/pii/S0004370207001257,"This work presents the restricted gradient-descent (RGD) algorithm, a training method for local radial-basis function networks specifically developed to be used in the context of reinforcement learning. The RGD algorithm can be seen as a way to extract relevant features from the state space to feed a linear model computing an approximation of the value function. Its basic idea is to restrict the way the standard gradient-descent algorithm changes the hidden units of the approximator, which results in conservative modifications that make the learning process less prone to divergence. The algorithm is also able to configure the topology of the network, an important characteristic in the context of reinforcement learning, where the changing policy may result in different requirements on the approximator structure. Computational experiments are presented showing that the RGD algorithm consistently generates better value-function approximations than the standard gradient-descent method, and that the latter is more susceptible to divergence. In the pole-balancing and Acrobot tasks, RGD combined with SARSA presents competitive results with other methods found in the literature, including evolutionary and recent reinforcement-learning algorithms.",autonomous vehicle
10.1016/j.cattod.2020.07.074,journal,Catalysis Today,sciencedirect,2021-07-01,sciencedirect,Machine learning in experimental materials chemistry,https://api.elsevier.com/content/article/pii/S0920586120305502,"
                  The development of advanced materials is an important aspect of modern life. However, the discovery of novel materials involves searching the vast chemical space to find materials with desired properties. Recent developments in the applications of Machine Learning (ML) in materials chemistry show promise to accelerate the material discovery process. In this perspective article, we highlight the importance of ML in materials chemistry. We discuss few examples of ML applications in synthesis, characterization, and predicting activities of materials. Finally, we discuss the challenges in this field and how the progress in ML in chemistry is leveraged together with advanced robotics to perform automated optimization of material discovery.
               ",autonomous vehicle
10.1016/j.seppur.2013.10.023,journal,Separation and Purification Technology,sciencedirect,2014-02-10,sciencedirect,Cost reduction of CO<ce:inf loc=post>2</ce:inf> capture processes using reinforcement learning based iterative design: A pilot-scale absorption–stripping system,https://api.elsevier.com/content/article/pii/S1383586613006163,"
                  An economical and practical way of designing the optimal condition for CO2 capture processes is proposed. This learning strategy, called reinforcement learning based iterative design, is developed to learn the optimal condition from hybrid information. One is from simulation data, the other, from real plant data. The simulation data is easily accessible, but the optimal condition is restricted to a simulated model being selected. To make up the mismatch, new info from the real process is explored. Only fewer operating data supplemented from the real process is used to update the learning scheme, so time, costs, and efforts can be saved. The fused info from the two kinds of data is also proposed. To demonstrate the effectiveness of the proposed method, design of a pilot-scale CO2 absorption–stripping experiment is conducted for recovery of CO2 from flue gases.
               ",autonomous vehicle
10.1016/bs.host.2018.07.004,journal,Handbook of Statistics,sciencedirect,2018-12-31,sciencedirect,Chapter 8: Machine Learning,https://api.elsevier.com/content/article/pii/S0169716118300191,"
                  The objective of this chapter is to provide the reader with an overview of machine learning concepts and different types of learning techniques which include supervised, unsupervised, semi-supervised, and reinforcement learning. Learning algorithms discussed in this chapter help the reader to easily move from the equations of the book to a computer program. Various metrics like accuracy, precision, confusion matrix, recall, RMSE, and quantile of errors used to evaluate machine learning algorithms are outlined in this chapter. At the end of this chapter we present various applications of machine learning techniques followed by future trends and challenges.
               ",autonomous vehicle
10.1016/j.future.2020.10.026,journal,Future Generation Computer Systems,sciencedirect,2021-03-31,sciencedirect,Generating knowledge graphs by employing Natural Language Processing and Machine Learning techniques within the scholarly domain,https://api.elsevier.com/content/article/pii/S0167739X2033003X,"
                  The continuous growth of scientific literature brings innovations and, at the same time, raises new challenges. One of them is related to the fact that its analysis has become difficult due to the high volume of published papers for which manual effort for annotations and management is required. Novel technological infrastructures are needed to help researchers, research policy makers, and companies to time-efficiently browse, analyse, and forecast scientific research. Knowledge graphs i.e., large networks of entities and relationships, have proved to be effective solution in this space. Scientific knowledge graphs focus on the scholarly domain and typically contain metadata describing research publications such as authors, venues, organizations, research topics, and citations. However, the current generation of knowledge graphs lacks of an explicit representation of the knowledge presented in the research papers. As such, in this paper, we present a new architecture that takes advantage of Natural Language Processing and Machine Learning methods for extracting entities and relationships from research publications and integrates them in a large-scale knowledge graph. Within this research work, we (i) tackle the challenge of knowledge extraction by employing several state-of-the-art Natural Language Processing and Text Mining tools, (ii) describe an approach for integrating entities and relationships generated by these tools, (iii) show the advantage of such an hybrid system over alternative approaches, and (vi) as a chosen use case, we generated a scientific knowledge graph including 109,105 triples, extracted from 26,827 abstracts of papers within the Semantic Web domain. As our approach is general and can be applied to any domain, we expect that it can facilitate the management, analysis, dissemination, and processing of scientific knowledge.
               ",autonomous vehicle
10.1016/j.landurbplan.2021.104239,journal,Landscape and Urban Planning,sciencedirect,2021-12-31,sciencedirect,Leveraging machine learning to understand urban change with net construction,https://api.elsevier.com/content/article/pii/S0169204621002024,"
                  A key indicator of urban change is construction, demolition, and renovation. Although these development activities are often interrelated, they are typically studied independent of one another. Analytic methods relying on a strict set of modeling assumptions limit our ability to understand this change holistically. Machine learning has demonstrated the potential when combined with big data to discover patterns and relationships between seemingly unrelated variables. This research explores urban change through net construction, a composite value that treats demolition as a deductive process that is subtracted from construction activity which provides for a more holistic and nuanced understanding of development activity. Once validated through a visual analysis of its reliability as a measure of urban change, we then used a series of random forest regression models to evaluate the predictive accuracy of net construction compared with independent models of construction and demolition. Applying the approaches to an urban county in the United States, we compiled 122 independent variables to provide a comprehensive view of individual neighborhoods from multi-disciplinary data sources such as socioeconomic, built environment characteristics, and landscape metrics. We then analyze the feature importance scores derived from the random forest models in an effort to assess the similarities and differences between the variables that have the greatest influence on model accuracy. The net construction model produced more accurate results than models that used construction and demolition activity independently. While many of the most important features aligned with those from the independent models, land use mix drawn from landscape metrics appeared as the most important, representing a departure from previous studies. This study provides a scalable method for modeling urban change using machine learning techniques and reveals the importance of applying data-driven algorithms that can help communities become more informed about their pressing issues.
               ",autonomous vehicle
10.1016/j.aei.2020.101101,journal,Advanced Engineering Informatics,sciencedirect,2020-08-31,sciencedirect,Predictive model-based quality inspection using Machine Learning and Edge Cloud Computing,https://api.elsevier.com/content/article/pii/S1474034620300707,"The supply of defect-free, high-quality products is an important success factor for the long-term competitiveness of manufacturing companies. Despite the increasing challenges of rising product variety and complexity and the necessity of economic manufacturing, a comprehensive and reliable quality inspection is often indispensable. In consequence, high inspection volumes turn inspection processes into manufacturing bottlenecks. In this contribution, we investigate a new integrated solution of predictive model-based quality inspection in industrial manufacturing by utilizing Machine Learning techniques and Edge Cloud Computing technology. In contrast to state-of-the-art contributions, we propose a holistic approach comprising the target-oriented data acquisition and processing, modelling and model deployment as well as the technological implementation in the existing IT plant infrastructure. A real industrial use case in SMT manufacturing is presented to underline the procedure and benefits of the proposed method. The results show that by employing the proposed method, inspection volumes can be reduced significantly and thus economic advantages can be generated.",autonomous vehicle
10.1016/j.matpr.2021.04.241,journal,Materials Today: Proceedings,sciencedirect,2021-05-19,sciencedirect,Artificial intelligence techniques for cancer detection in medical image processing: A review,https://api.elsevier.com/content/article/pii/S2214785321031618,"
                  Cancer is the uncontrolled growth of abnormal cells in any part of a body. Cancer is a broad term for a group of diseases caused when abnormal cells grows in different body parts. There are more than hundred types of Cancer such as Lung cancer, Breast cancer, Skin cancer, Oral cancer, Colon cancer and Prostate cancer. Delay in treatment can cause serious health issues, even cause loss of life. This paper gives the review on methods of detection of lung cancer and brain cancer and liver using image processing. The methods used for detection are Automated and computer-aided detection system (CAD) with artificial intelligence and these methods are good to process a large datasets to provide accurate and efficient results in the detection of cancer. However, these processing system have to face many challenges to implement on large scale including image acquisition, pre-processing, segmentation, and data management and classification strategies to be compatible with AI. This paper reviews the various image acquisition and segmentation techniques. These techniques become the need of an hour to cater the growing patient population and for the improvement in the Healthcare system.
               ",autonomous vehicle
10.1016/j.ins.2013.08.037,journal,Information Sciences,sciencedirect,2014-03-10,sciencedirect,Reinforcement learning algorithms with function approximation: Recent advances and applications,https://api.elsevier.com/content/article/pii/S0020025513005975,"
                  In recent years, the research on reinforcement learning (RL) has focused on function approximation in learning prediction and control of Markov decision processes (MDPs). The usage of function approximation techniques in RL will be essential to deal with MDPs with large or continuous state and action spaces. In this paper, a comprehensive survey is given on recent developments in RL algorithms with function approximation. From a theoretical point of view, the convergence and feature representation of RL algorithms are analyzed. From an empirical aspect, the performance of different RL algorithms was evaluated and compared in several benchmark learning prediction and learning control tasks. The applications of RL with function approximation are also discussed. At last, future works on RL with function approximation are suggested.
               ",autonomous vehicle
10.1016/j.comnet.2020.107468,journal,Computer Networks,sciencedirect,2020-12-09,sciencedirect,BaaS: Broadcast as a service cross-layer learning-based approach in cloud assisted VANETs,https://api.elsevier.com/content/article/pii/S1389128620311464,"
                  Broadcasting plays a significant role for data dissemination in Vehicular Ad hoc Network (VANET). Many VANET services rely on broadcast communication mode to disseminate data packets over the network. However, due to high mobility of vehicles, it stills being a challenging task. In this paper, a BaaS (Broadcast as a Service ) broadcasting solution is proposed for VANET to disseminate the data efficiently to the vehicles in the network using cloud computing. Due to the limited storage and computing capacity of vehicles, all information about road traffic and the broadcasting protocol is both maintained and processed in the cloud. So, we introduce a new cloud assisted VANET (CAV) architecture organizing the different levels of communications between the VANET components and the cloud, as well as between the three cloud layers. The source vehicle broadcasts the message by requesting the best relay set from the nearby road side unit (RSU) which plays the role of intermediate to the cloud. To perform the message forwarding, the cloud provides cross layer services: infrastructure as a service for traffic data storage and relay set selection processing, and application layer service for communication with VANET entities. By integrating cloud services many broadcasting problems may be reduced such as the delay and link breakage.
               ",autonomous vehicle
10.1016/B978-0-12-815480-9.00006-2,journal,Artificial Intelligence in the Age of Neural Networks and Brain Computing,sciencedirect,2019-12-31,sciencedirect,Chapter 6: Evolving and Spiking Connectionist Systems for Brain-Inspired Artificial Intelligence,https://api.elsevier.com/content/article/pii/B9780128154809000062,"
               Artificial neural networks have now a long history as major techniques in computational intelligence with a wide range of application for learning from data and for artificial intelligence (AI). This chapter starts with a brief review of AI methods, from Aristotle's logic to the classical artificial neural networks (ANN) and hybrid systems that are used for AI now. A main focus of the paper is a class of ANN called evolving connectionist systems (ECOS) that evolve their structure and functionality through learning from data. Methods, systems, and a wide range of applications of ECOS are presented and referenced. Principles and applications of spiking neural networks (SNN) and evolving SNN (eSNN) are presented and illustrated as the third generation of ANN. The chapter includes a section on future brain-like SNN for brain-like AI. An example is the NeuCube SNN architecture, which is illustrated with deep learning algorithms and with two case study applications for brain data and seismic data modeling. The chapter concludes that knowing and combining various methods of AI and ANN and getting more inspiration from neuroscience to create new methods is the way for future research.
            ",autonomous vehicle
10.1016/j.rcl.2021.07.006,journal,Radiologic Clinics of North America,sciencedirect,2021-11-30,sciencedirect,Separating Hope from Hype: Artificial Intelligence Pitfalls and Challenges in Radiology,https://api.elsevier.com/content/article/pii/S0033838921001007,,autonomous vehicle
10.1016/j.jksuci.2021.07.020,journal,Journal of King Saud University - Computer and Information Sciences,sciencedirect,2021-07-30,sciencedirect,Intelligent transportation systems: A survey on modern hardware devices for the era of machine learning,https://api.elsevier.com/content/article/pii/S1319157821001877,"The increasing complexity of Intelligent Transportation Systems (ITS), that comprise a wide variety of applications and services, has imposed a necessity for high-performance Modern Hardware Devices (MHDs). The performance challenge has become more noticeable with the integration of Machine Learning (ML) techniques deployed in large-scale settings. ML has effectively supported the field of ITS by providing efficient and optimized solutions to problems that were otherwise tackled using traditional statistical and analytical approaches. Addressing the hardware deployment needs of ITS in the era of ML is a challenging problem that involves temporal, spatial, environmental, and economical factors. This survey reviews the recent literature of ML-driven ITS, in which MHDs were utilized, with a focus on performance indicators. A taxonomy is then synthesized, giving a complete representation of what the current capabilities of the surveyed ITS rely on in terms of ML techniques and technological infrastructure. To alleviate the difficulties faced in the non-trivial task of selecting suitable ML techniques and MHDs for an ITS with a specific complexity level, a performance evaluation framework is proposed. The presented survey sets the basis for developing suitable hardware, facilitating the integration of ML within ITS, and bridging the gap between research and real-world deployments.",autonomous vehicle
10.1016/B978-0-08-102295-5.10410-X,journal,International Encyclopedia of Human Geography,sciencedirect,2020-12-31,sciencedirect,"Networks, Neural",https://api.elsevier.com/content/article/pii/B978008102295510410X,"
               Neural networks (NNs), also referred to as artificial neural networks (ANNs), are a form of artificial intelligence (AI) model that mimics the way the human brain works. They were introduced as an alternative to solve geographic problems in the 1990s and recently have flourished with advancements in computer power, AI technology, and data availability, among others. This article provides a general background of NNs, their definition, architecture, and learning mechanisms. Potential applications of NNs to human geography as exploratory and explanatory models follow. Next, a detailed example of the steps to implement a NN model for mode choice is outlined, concluding with a motivation to further explore NN models.
            ",autonomous vehicle
10.1016/j.joule.2021.10.001,journal,Joule,sciencedirect,2021-10-25,sciencedirect,Machine learning for high-throughput experimental exploration of metal halide perovskites,https://api.elsevier.com/content/article/pii/S2542435121004451,"
                  Metal halide perovskites (MHPs) have catapulted to the forefront of energy research due to the unique combination of high device performance, low materials cost, and facile solution processability. A remarkable merit of these materials is their compositional flexibility allowing for multiple substitutions at all crystallographic sites, and hence thousands of possible pure compounds and virtually a near-infinite number of multicomponent solid solutions. Harnessing the full potential of MHPs necessitates rapid exploration of multidimensional chemical space toward desired functionalities. Recent advances in laboratory automation, ranging from bespoke fully automated robotic labs to microfluidic systems and to pipetting robots, have enabled high-throughput experimental workflows for synthesizing MHPs. Here, we provide an overview of the state of the art in the automated MHP synthesis and existing methods for navigating multicomponent compositional space. We highlight the limitations and pitfalls of the existing strategies and formulate the requirements for necessary machine learning tools including causal and Bayesian methods, as well as strategies based on co-navigation of theoritical and experimental spaces. We argue that ultimately the goal of automated experiments is to simultaneously optimize the materials synthesis and refine the theoretical models that underpin target functionalities. Furthermore, the near-term development of automated experimentation will not lead to the full exclusion of human operator but rather automatization of repetitive operations, deferring human role to high-level slow decisions. We also discuss the emerging opportunities leveraging machine learning-guided automated synthesis to the development of high-performance perovskite optoelectronics.
               ",autonomous vehicle
10.1016/j.tust.2020.103677,journal,Tunnelling and Underground Space Technology,sciencedirect,2021-02-28,sciencedirect,"BIM, machine learning and computer vision techniques in underground construction: Current status and future perspectives",https://api.elsevier.com/content/article/pii/S0886779820306313,"
                  The architecture, engineering and construction (AEC) industry is experiencing a technological revolution driven by booming digitisation and automation. Advances in research fields of information technology and computer science, such as building information modelling (BIM), machine learning and computer vision have attracted growing attention owing to their useful applications. At the same time, population-driven underground development has been accelerated with digital transformation as a strategic imperative. Urban underground infrastructures are valuable assets and thus demanding effective planning, construction and maintenance. While enabling greater visibility and reliability into the processes and subsystems of underground construction, applications of BIM, machine learning and computer vision in underground construction represent different sets of opportunities and challenges from their use in above-ground construction. Therefore, this paper aims to present the state-of-the-art development and future trends of BIM, machine learning, computer vision and their related technologies in facilitating the digital transition of tunnelling and underground construction. Section 1 presents the global demand for adopting these technologies. Section 2 introduces the related terminologies, standardisations and fundamentals. Section 3 reviews BIM in traditional and mechanised tunnelling and highlights the importance of integrating 3D geological modelling and geographic information system (GIS) databases with BIM. Section 4 examines the key applications of machine learning and computer vision at different stages of underground construction. Section 5 discusses the challenges and perspectives of existing research on leveraging these emerging technologies for escalating digitisation, automation and information integration throughout underground project lifecycle. Section 6 summarises the current state of development, identified gaps and future directions.
               ",autonomous vehicle
10.1016/B978-0-12-812594-6.00006-8,journal,Applied Biomechatronics using Mathematical Models,sciencedirect,2018-12-31,sciencedirect,Chapter 6: Application of mathematical models in biomechatronics: artificial intelligence and time-frequency analysis,https://api.elsevier.com/content/article/pii/B9780128125946000068,"
               Mathematical models for kinematics, kinetics, and muscles potentials activities are deducted of data signals analysis, using time-frequency domain and non-classic methods from pattern recognitions to computational learning theory of Artificial Intelligence (AI) based on Machine Learning algorithms. Covering decision theory for supervised, and unsupervised learning as: Partitional Clustering (k-means algorithm), Hierarchical Clustering, Artificial Neural Network (ANN), and others approaches. Applying them in practical Biomedical research examples for Classification, and Regression using: Classification Tree Analysis (CTA), and Regression Tree Analysis (RTA). Finalizing, with Mathematical models of Soft Computing for Fuzzy Inference Systems (FIS), Fuzzy Relations and Fuzzy Similarities deducted from the dynamics of human body, using synchronized signals from: Ground Reaction Forces (GRF) during the gait phases, muscles activities of Surface Electromyography (SEMG), and Joint Angles.
            ",autonomous vehicle
10.1016/j.scs.2021.103189,journal,Sustainable Cities and Society,sciencedirect,2021-11-30,sciencedirect,Artificial intelligence and internet of things in screening and management of autism spectrum disorder,https://api.elsevier.com/content/article/pii/S2210670721004674,"
                  Autism is a disability that obstructs the process of a person’s development. Autistic individuals find it extremely difficult to cope with the world’s pace, can not communicate properly, and unable to express their feelings appropriately. Artificial Intelligence (AI), Machine Learning (ML), and Internet of Things (IoT) are used in several medical applications, and autistic individuals can be assisted using the proper use of automated systems. In this paper, some of the research works in the field of application of AI, ML, and IoT in autism were reviewed. State-of-the-art articles were collected and around 58 articles were selected which have significant contribution in this field. The selected research works were analyzed, represented, and compared. Finally, incorporation of the autism facilities in smart city environment is described, some research gaps and challenges were pointed out, and recommendations were provided for further research work.
               ",autonomous vehicle
10.1016/j.suc.2020.08.012,journal,Surgical Clinics of North America,sciencedirect,2021-02-28,sciencedirect,Evolution of Risk Calculators and the Dawn of Artificial Intelligence in Predicting Patient Complications,https://api.elsevier.com/content/article/pii/S0039610920301080,,autonomous vehicle
10.1016/j.aei.2020.101126,journal,Advanced Engineering Informatics,sciencedirect,2020-08-31,sciencedirect,Failure mode classification and bearing capacity prediction for reinforced concrete columns based on ensemble machine learning algorithm,https://api.elsevier.com/content/article/pii/S1474034620300951,"
                  Failure mode (FM) and bearing capacity of reinforced concrete (RC) columns are key concerns in structural design and/or performance assessment procedures. The failure types, i.e., flexure, shear, or mix of the above two, will greatly affect the capacity and ductility of the structure. Meanwhile, the design methodologies for structures of different failure types will be totally different. Therefore, developing efficient and reliable methods to identify the FM and predict the corresponding capacity is of special importance for structural design/assessment management. In this paper, an intelligent approach is presented for FM classification and bearing capacity prediction of RC columns based on the ensemble machine learning techniques. The most typical ensemble learning method, adaptive boosting (AdaBoost) algorithm, is adopted for both classification and regression (prediction) problems. Totally 254 cyclic loading tests of RC columns are collected. The geometric dimensions, reinforcing details, material properties are set as the input variables, while the failure types (for classification problem) and peak capacity forces (for regression problem) are set as the output variables. The results indicate that the model generated by the AdaBoost learning algorithm has a very high accuracy for both FM classification (accuracy = 0.96) and capacity prediction (R2 = 0.98). Different learning algorithms are also compared and the results show that ensemble learning (especially AdaBoost) has better performance than single learning. In addition, the bearing capacity predicted by the AdaBoost is also compared to that by the empirical formulas provided by the design codes, which shows an obvious superior of the proposed method. In summary, the machine learning technique, especially the ensemble learning, can provide an alternate to the conventional mechanics-driven models in structural design in this big data time.
               ",autonomous vehicle
10.1016/j.ejor.2021.04.032,journal,European Journal of Operational Research,sciencedirect,2022-01-16,sciencedirect,Machine learning at the service of meta-heuristics for solving combinatorial optimization problems: A state-of-the-art,https://api.elsevier.com/content/article/pii/S0377221721003623,"In recent years, there has been a growing research interest in integrating machine learning techniques into meta-heuristics for solving combinatorial optimization problems. This integration aims to lead meta-heuristics toward an efficient, effective, and robust search and improve their performance in terms of solution quality, convergence rate, and robustness. Since various integration methods with different purposes have been developed, there is a need to review the recent advances in using machine learning techniques to improve meta-heuristics. To the best of our knowledge, the literature is deprived of having a comprehensive yet technical review. To fill this gap, this paper provides such a review on the use of machine learning techniques in the design of different elements of meta-heuristics for different purposes including algorithm selection, fitness evaluation, initialization, evolution, parameter setting, and cooperation. First, we describe the key concepts and preliminaries of each of these ways of integration. Then, the recent advances in each way of integration are reviewed and classified based on a proposed unified taxonomy. Finally, we provide a technical discussion on the advantages, limitations, requirements, and challenges of implementing each of these integration ways, followed by promising future research directions.",autonomous vehicle
10.1016/j.ijinfomgt.2019.03.004,journal,International Journal of Information Management,sciencedirect,2019-12-31,sciencedirect,A supervised machine learning approach to data-driven simulation of resilient supplier selection in digital manufacturing,https://api.elsevier.com/content/article/pii/S0268401219301422,"
                  There has been an increased interest in resilient supplier selection in recent years, much of it focusing on forecasting the disruption probabilities. We conceptualize an entirely different approach to analyzing the risk profiles of supplier performance under uncertainty by utilizing the data analytics capabilities in digital manufacturing. Digital manufacturing peculiarly challenge the supplier selection by the dynamic order allocations, and opens new opportunities to exploit the digital data to improve sourcing decisions. We develop a hybrid technique, combining simulation and machine learning and examine its applications to data-driven decision-making support in resilient supplier selection. We consider on-time delivery as an indicator for supplier reliability, and explore the conditions surrounding the formation of resilient supply performance profiles. We theorize the notions of risk profile of supplier performance and resilient supply chain performance. We show that the associations of the deviations from the resilient supply chain performance profile with the risk profiles of supplier performance can be efficiently deciphered by our approach. The results suggest that a combination of supervised machine learning and simulation, if utilized properly, improves the delivery reliability. Our approach can also be of value when analyzing the supplier base and uncovering the critical suppliers, or combinations of suppliers the disruption of which result in the adverse performance decreases. The results of this study advance our understanding about how and when machine learning and simulation can be combined to create digital supply chain twins, and through these twins improve resilience. The proposed data-driven decision-making model for resilient supplier selection can be further exploited for design of risk mitigation strategies in supply chain disruption management models, re-designing the supplier base or investing in most important and risky suppliers.
               ",autonomous vehicle
10.1016/j.jacc.2018.12.054,journal,Journal of the American College of Cardiology,sciencedirect,2019-03-26,sciencedirect,<ce:marker name=jacmegphn alt=Commentary by Dr. Valentin Fuster altimg-small=jacmegphn_s.gif altimg=jacmegphn_o.gif></ce:marker>Artificial Intelligence in Cardiovascular Imaging: <ce:italic>JACC</ce:italic> State-of-the-Art Review,https://api.elsevier.com/content/article/pii/S0735109719302360,"Data science is likely to lead to major changes in cardiovascular imaging. Problems with timing, efficiency, and missed diagnoses occur at all stages of the imaging chain. The application of artificial intelligence (AI) is dependent on robust data; the application of appropriate computational approaches and tools; and validation of its clinical application to image segmentation, automated measurements, and eventually, automated diagnosis. AI may reduce cost and improve value at the stages of image acquisition, interpretation, and decision-making. Moreover, the precision now possible with cardiovascular imaging, combined with “big data” from the electronic health record and pathology, is likely to better characterize disease and personalize therapy. This review summarizes recent promising applications of AI in cardiology and cardiac imaging, which potentially add value to patient care.",autonomous vehicle
10.1016/j.neucom.2020.07.025,journal,Neurocomputing,sciencedirect,2020-11-13,sciencedirect,Detector neural network vs connectionist ANNs,https://api.elsevier.com/content/article/pii/S0925231220311322,"
                  Most widely used modern artificial neural networks are based on the connectionist paradigm of building and learning. The authors propose an alternative detector approach. The basis of this approach is the original architecture of the neural network, as well as a new procedure for its learning. The developed neural network is called the detector neural network. This network consists of two layers of neurons. The neurons of the first layer are called neurons-pre-detectors and they do not learn. They are designed to highlight the structural elements of recognizable images, as well as to determine their measured parameters. The types of structural elements and their parameters are set a priori and depend on the type and complexity of recognizable images. Neurons of the second layer can be trained. They recognize individual complex images. These neurons are called neurons-detectors (ND). The model of the ND is significantly different from all known models of neurons and has important features: the presence of a dendritic tree model, a new looks at the role and value of synaptic coupling coefficients, an original approach to the formation of a neuron reaction. The training procedure for the ND is called counter learning. In the process of counter learning, an information model – template of the most simplified image structure of a particular class is formed and remembered by the ND. This template is called a concept. The main role in the formation of the concept is played by the model of the dendritic tree of the neuron. During the classification process, the ND tries to simplify the recognizable image until it coincides with the concept. The article provides a comparative analysis of the detector approach and the connectionist paradigm. The advantages of the detector approach, according to the authors, open up new possibilities in the study of the problem of constructing Artificial General Intelligence.
               ",autonomous vehicle
10.1016/B978-0-12-809633-8.20331-2,journal,Encyclopedia of Bioinformatics and Computational Biology,sciencedirect,2019-12-31,sciencedirect,Machine Learning in Bioinformatics,https://api.elsevier.com/content/article/pii/B9780128096338203312,"
               Machine Learning (ML) is an adaptive process of interpreting information from real-world data sets by making use of computers that learn from experiences. Bioinformatics is emerging as an interdisciplinary science of interpreting biological data using such adaptive computational procedures. This article provides an overview on the ML tools and techniques, which are useful in bioinformatics and computational biology.
            ",autonomous vehicle
10.1016/j.procs.2020.05.020,journal,Procedia Computer Science,sciencedirect,2020-12-31,sciencedirect,Curriculum Design Using Artificial Intelligence (AI) Back Propagation Method,https://api.elsevier.com/content/article/pii/S1877050920313430,"With the rapid changes in technologies and industry requirements, goal for the engineering education can be set as the job roles and defining the curriculum of the Program as required to meet the job role. The curriculum design can be done with designing the curriculum and defining the expected outcome of the Program. Then based on the expected outcome, the gap can be identified and the curriculum can be defined to bridge the gap between the Program and the expected outcome. Instead, the Job Roles and the skills required skills can be taken as the as the starting point and the courses required are identified to attain this outcome. In turn, the pre requisite courses can be identified and further prerequisite courses can be identified. The technology of Artificial Intelligence (AI) back propagation method can be effectively used to achieve this. This paper gives an example of such method applied for identifying the curriculum required to meet the job roles in the area of Internet of Things (IoT).",autonomous vehicle
10.1016/j.rser.2021.110969,journal,Renewable and Sustainable Energy Reviews,sciencedirect,2021-07-31,sciencedirect,Intelligent building control systems for thermal comfort and energy-efficiency: A systematic review of artificial intelligence-assisted techniques,https://api.elsevier.com/content/article/pii/S1364032121002616,"
                  Building operations represent a significant percentage of the total primary energy consumed in most countries due to the proliferation of Heating, Ventilation and Air-Conditioning (HVAC) installations in response to the growing demand for improved thermal comfort. Reducing the associated energy consumption while maintaining comfortable conditions in buildings are conflicting objectives and represent a typical optimization problem that requires intelligent system design. Over the last decade, different methodologies based on the Artificial Intelligence (AI) techniques have been deployed to find the sweet spot between energy use in HVAC systems and suitable indoor comfort levels to the occupants. This paper performs a comprehensive and an in-depth systematic review of AI-based techniques used for building control systems by assessing the outputs of these techniques, and their implementations in the reviewed works, as well as investigating their abilities to improve the energy-efficiency, while maintaining thermal comfort conditions. This enables a holistic view of (1) the complexities of delivering thermal comfort to users inside buildings in an energy-efficient way, and (2) the associated bibliographic material to assist researchers and experts in the field in tackling such a challenge. Among the 20 AI tools developed for both energy consumption and comfort control, functions such as identification and recognition patterns, optimization, predictive control. Based on the findings of this work, the application of AI technology in building control is a promising area of research and still an ongoing, i.e., the performance of AI-based control is not yet completely satisfactory. This is mainly due in part to the fact that these algorithms usually need a large amount of high-quality real-world data, which is lacking in the building or, more precisely, the energy sector. Based on the current study, from 1993 to 2020, the application of AI techniques and personalized comfort models has enabled energy savings on average between 21.81 and 44.36%, and comfort improvement on average between 21.67 and 85.77%. Finally, this paper discusses the challenges faced in the use of AI for energy productivity and comfort improvement, and opens main future directions in relation with AI-based building control systems for human comfort and energy-efficiency management.
               ",autonomous vehicle
10.1016/j.procir.2021.01.138,journal,Procedia CIRP,sciencedirect,2021-12-31,sciencedirect,Artificial intelligence system for enhancing product’s performance during its life cycle in a railcar industry,https://api.elsevier.com/content/article/pii/S2212827121001682,"The management of the activities which characterize the lifecycle of a product could be challenging, hence, a well-structured Product Lifecycle Management System (PLMS) with Artificial Intelligence (AI) capability can offer a sustainable solution in this regard. This will promote data driven maintenance because AI and data analysis can drive the activities of the entire product’s lifecycle. The aim of this study is to demonstrate the use of AI for enhancing products’ performance during its use phase in the products’ life cycle and to develop a framework for the proposed PLMS and AI capability. The bearing component of a railcar was used as a case study. The temperature data of the bearing component employed were obtained from primary and secondary sources and were pre-processed in order to extract features and indicators for the training and predictive model development. The acquisition of the input data was followed by data pre-processing to remove noise and iterative training to obtain the predictive model. The training was done using the Levenberg Marquardt algorithm in a MATLAB 2018a environment in order to predict future temperature variations and the remaining useful life of the bearing. The results obtained indicated that the AI is suitable for condition based monitoring and prediction of the time to failure as well as the Remaining Useful Life (RUL) of the railcar bearing. It is envisaged that with the AI capability integrated into the PLMS will enhance components performance through effective monitoring during its use phase.",autonomous vehicle
10.1016/j.tej.2020.106883,journal,The Electricity Journal,sciencedirect,2021-02-28,sciencedirect,Distributed machine learning for energy trading in electric distribution system of the future,https://api.elsevier.com/content/article/pii/S1040619020301755,"
                  Machine Learning (ML) has seen a great potential to solve many power system problems along with its transition into Smart Grid. Specifically, electric distribution systems have witnessed a rapid integration of distributed energy resources (DERs), including photovoltaic (PV) panels, electric vehicles (EV), and smart appliances, etc. Electricity consumers, equipped with such DERs and advanced metering/sensing/computing devices, are becoming self-interested prosumers who can behave more actively for their electric energy consumption. In this paper, the potential of distributed ML in solving the energy trading problem among prosumers of a future electric distribution system - building DC grid cell, is explored, while considering the limited computation, communication, and data privacy issues of the edge entities. A fully distributed energy trading framework based on ML is proposed to optimize the load and price prediction accuracy and energy trading efficiency. Computation resource allocation, communication schemes, ML task scheduling, as well as user sensitive data preserving issues in the distributed ML framework are addressed with consideration of all the economic and physical constraints of the electric distribution systems.
               ",autonomous vehicle
10.1016/j.jacc.2017.03.571,journal,Journal of the American College of Cardiology,sciencedirect,2017-05-30,sciencedirect,<ce:marker name=jacmegphn alt=Commentary by Dr. Valentin Fuster altimg-small=jacmegphn_s.gif altimg=jacmegphn_o.gif></ce:marker>Artificial Intelligence in Precision Cardiovascular Medicine,https://api.elsevier.com/content/article/pii/S0735109717368456,"Artificial intelligence (AI) is a field of computer science that aims to mimic human thought processes, learning capacity, and knowledge storage. AI techniques have been applied in cardiovascular medicine to explore novel genotypes and phenotypes in existing diseases, improve the quality of patient care, enable cost-effectiveness, and reduce readmission and mortality rates. Over the past decade, several machine-learning techniques have been used for cardiovascular disease diagnosis and prediction. Each problem requires some degree of understanding of the problem, in terms of cardiovascular medicine and statistics, to apply the optimal machine-learning algorithm. In the near future, AI will result in a paradigm shift toward precision cardiovascular medicine. The potential of AI in cardiovascular medicine is tremendous; however, ignorance of the challenges may overshadow its potential clinical impact. This paper gives a glimpse of AI’s application in cardiovascular clinical care and discusses its potential role in facilitating precision cardiovascular medicine.",autonomous vehicle
10.1016/B978-0-323-85380-4.00007-5,journal,Intelligence Science,sciencedirect,2021-12-31,sciencedirect,Chapter 7: Learning,https://api.elsevier.com/content/article/pii/B9780323853804000075,"
               People learn from the objective environment constantly over their whole life span. A person’s cognitive ability, wisdom, and lifelong learning is subject to gradual formation, development, and improvement.
            ",autonomous vehicle
10.1016/j.epsr.2020.106904,journal,Electric Power Systems Research,sciencedirect,2021-03-31,sciencedirect,Ensemble machine learning models for the detection of energy theft,https://api.elsevier.com/content/article/pii/S0378779620307021,"
                  Advanced metering infrastructure allows the two-way sharing of information between smart meters and utilities. However, it makes smart grids more vulnerable to cyber-security threats such as energy theft. This study suggests ensemble machine learning (ML) models for the detection of energy theft in smart grids using customers’ consumption patterns. Ensemble ML models are meta-algorithms that create a pool of several ML approaches and combine them smartly into one predictive model to reduce variance and bias. A number of algorithms, including adaptive boosting, categorical boosting, extreme-boosting, light boosting, random forest, and extra trees, were tested to find their false positive and detection rates. A data pre-processing method was employed to improve detection performance. The statistical approach of minority over-sampling was also employed to tackle over-fitting. An extensive analysis based on a practical dataset of 5000 customers reveals that bagging models outperform other algorithms. The random forest and extra trees models achieve the highest area under the curve score of 0.90. The precision analysis shows that the proposed bagging methods perform better.
               ",autonomous vehicle
10.1016/j.oceaneng.2020.107922,journal,Ocean Engineering,sciencedirect,2020-12-01,sciencedirect,Ship collision avoidance anthropomorphic decision-making for structured learning based on AIS with Seq-CGAN,https://api.elsevier.com/content/article/pii/S0029801820308805,"
                  The ship handling decision-making process constitutes the most critical step in intelligent ship collision avoidance. This study utilized the ship encounter azimuth map to extract the data of 12 types of ship encounter modes from the automatic identification system (AIS) big data. The sliding window algorithm was used to identify the ship encounter behaviors, which were used as the training data. The sequence conditional generative adversarial network (Seq-CGAN) was proposed using the sequence-to-sequence (Seq2Seq) model to learn how to generate appropriate anthropomorphic collision avoidance decisions and bypass the process of ship collision risk assessment based on the quantification of a series of functions. The long short-term memory (LSTM) cell was simultaneously combined with Seq-CGAN to improve the memory capacity and the current adaptability of the environment of the overall system. The Seq-CGAN was trained by 2018 Zhoushan Port AIS data to achieve structured learning regarding human experiences. The results indicate that the Seq-CGAN effectively formulates the sequence of ship handling behaviors. This study contributes significantly to the increased efficiency and safety of sea operations. The proposed method could be potentially applied to determine the predictive framework for various intelligent systems, including intelligent collision avoidance, route planning, and operational efficiency estimation.
               ",autonomous vehicle
10.1016/j.cmpb.2021.106190,journal,Computer Methods and Programs in Biomedicine,sciencedirect,2021-08-31,sciencedirect,A review of risk prediction models in cardiovascular disease: conventional approach vs. artificial intelligent approach,https://api.elsevier.com/content/article/pii/S0169260721002649,"
                  Cardiovascular disease (CVD) is the leading cause of death worldwide and is a global health issue. Traditionally, statistical models are used commonly in the risk prediction and assessment of CVD. However, the adoption of artificial intelligent (AI) approach is rapidly taking hold in the current era of technology to evaluate patient risks and predict the outcome of CVD. In this review, we outline various conventional risk scores and prediction models and do a comparison with the AI approach. The strengths and limitations of both conventional and AI approaches are discussed. Besides that, biomarker discovery related to CVD are also elucidated as the biomarkers can be used in the risk stratification as well as early detection of the disease. Moreover, problems and challenges involved in current CVD studies are explored. Lastly, future prospects of CVD risk prediction and assessment in the multi-modality of big data integrative approaches are proposed.
               ",autonomous vehicle
10.1016/j.ins.2018.05.005,journal,Information Sciences,sciencedirect,2018-08-31,sciencedirect,Privacy preserving multi-party computation delegation for deep learning in cloud computing,https://api.elsevier.com/content/article/pii/S0020025518303608,"
                  The recent advances in deep learning have improved the state of the art in artificial intelligence, and one of the most important stimulants of this success is the large volume of data. Although collaborative learning can improve the learning accuracy by incorporating more datasets into the learning process, serious privacy issues have also emerged from the training data. In this paper, we propose a new framework for privacy-preserving multi-party deep learning in cloud computing, where the large volume of training data is distributed among many parties. Our system enables multiple parties to learn the same neural network model, which is generated based on the aggregate dataset, and the privacy of the local dataset and learning model is protected against the cloud server. Extensive analysis shows that our schemes satisfy the security requirements of verifiability and privacy. Our implementation and experiments demonstrate that our system has a manageable computational efficiency and can be applied to a wide range of privacy-sensitive areas in deep learning.
               ",autonomous vehicle
10.1016/j.future.2018.06.042,journal,Future Generation Computer Systems,sciencedirect,2019-01-31,sciencedirect,CPS data streams analytics based on machine learning for Cloud and Fog Computing: A survey,https://api.elsevier.com/content/article/pii/S0167739X17330613,"
                  Cloud and Fog computing has emerged as a promising paradigm for the Internet of things (IoT) and cyber–physical systems (CPS). One characteristic of CPS is the reciprocal feedback loops between physical processes and cyber elements (computation, software and networking), which implies that data stream analytics is one of the core components of CPS. The reasons for this are: (i) it extracts the insights and the knowledge from the data streams generated by various sensors and other monitoring components embedded in the physical systems; (ii) it supports informed decision making; (iii) it enables feedback from the physical processes to the cyber counterparts; (iv) it eventually facilitates the integration of cyber and physical systems. There have been many successful applications of data streams analytics, powered by machine learning techniques, to CPS systems. Thus, it is necessary to have a survey on the particularities of the application of machine learning techniques to the CPS domain. In particular, we explore how machine learning methods should be deployed and integrated in Cloud and Fog architectures for better fulfilment of the requirements of mission criticality and time criticality arising in CPS domains. To the best of our knowledge, this paper is the first to systematically study machine learning techniques for CPS data stream analytics from various perspectives, especially from a perspective that leads to the discussion and guidance of how the CPS machine learning methods should be deployed in a Cloud and Fog architecture.
               ",autonomous vehicle
10.1016/j.ejmp.2021.05.010,journal,Physica Medica,sciencedirect,2021-05-31,sciencedirect,Artificial Intelligence in magnetic Resonance guided Radiotherapy: Medical and physical considerations on state of art and future perspectives,https://api.elsevier.com/content/article/pii/S1120179721001915,"
                  Over the last years, technological innovation in Radiotherapy (RT) led to the introduction of Magnetic Resonance-guided RT (MRgRT) systems.
                  Due to the higher soft tissue contrast compared to on-board CT-based systems, MRgRT is expected to significantly improve the treatment in many situations. MRgRT systems may extend the management of inter- and intra-fraction anatomical changes, offering the possibility of online adaptation of the dose distribution according to daily patient anatomy and to directly monitor tumor motion during treatment delivery by means of a continuous cine MR acquisition.
                  Online adaptive treatments require a multidisciplinary and well-trained team, able to perform a series of operations in a safe, precise and fast manner while the patient is waiting on the treatment couch.
                  Artificial Intelligence (AI) is expected to rapidly contribute to MRgRT, primarily by safely and efficiently automatising the various manual operations characterizing online adaptive treatments. Furthermore, AI is finding relevant applications in MRgRT in the fields of image segmentation, synthetic CT reconstruction, automatic (on-line) planning and the development of predictive models based on daily MRI.
                  This review provides a comprehensive overview of the current AI integration in MRgRT from a medical physicist’s perspective. Medical physicists are expected to be major actors in solving new tasks and in taking new responsibilities: their traditional role of guardians of the new technology implementation will change with increasing emphasis on the managing of AI tools, processes and advanced systems for imaging and data analysis, gradually replacing many repetitive manual tasks.
               ",autonomous vehicle
10.1016/j.trc.2018.12.004,journal,Transportation Research Part C: Emerging Technologies,sciencedirect,2019-02-28,sciencedirect,Enhancing transportation systems via deep learning: A survey,https://api.elsevier.com/content/article/pii/S0968090X18304108,"
                  Machine learning (ML) plays the core function to intellectualize the transportation systems. Recent years have witnessed the advent and prevalence of deep learning which has provoked a storm in ITS (Intelligent Transportation Systems). Consequently, traditional ML models in many applications have been replaced by the new learning techniques and the landscape of ITS is being reshaped. Under such perspective, we provide a comprehensive survey that focuses on the utilization of deep learning models to enhance the intelligence level of transportation systems. By organizing multiple dozens of relevant works that were originally scattered here and there, this survey attempts to provide a clear picture of how various deep learning models have been applied in multiple transportation applications.
               ",autonomous vehicle
10.1016/j.compbiomed.2018.03.014,journal,Computers in Biology and Medicine,sciencedirect,2018-05-01,sciencedirect,A survey on Barrett's esophagus analysis using machine learning,https://api.elsevier.com/content/article/pii/S0010482518300714,"
                  This work presents a systematic review concerning recent studies and technologies of machine learning for Barrett's esophagus (BE) diagnosis and treatment. The use of artificial intelligence is a brand new and promising way to evaluate such disease. We compile some works published at some well-established databases, such as Science Direct, IEEEXplore, PubMed, Plos One, Multidisciplinary Digital Publishing Institute (MDPI), Association for Computing Machinery (ACM), Springer, and Hindawi Publishing Corporation. Each selected work has been analyzed to present its objective, methodology, and results. The BE progression to dysplasia or adenocarcinoma shows a complex pattern to be detected during endoscopic surveillance. Therefore, it is valuable to assist its diagnosis and automatic identification using computer analysis. The evaluation of the BE dysplasia can be performed through manual or automated segmentation through machine learning techniques. Finally, in this survey, we reviewed recent studies focused on the automatic detection of the neoplastic region for classification purposes using machine learning methods.
               ",autonomous vehicle
10.1016/j.mlwa.2021.100052,journal,Machine Learning with Applications,sciencedirect,2021-09-15,sciencedirect,Machine learning-based malicious user detection for reliable cooperative radio spectrum sensing in Cognitive Radio-Internet of Things,https://api.elsevier.com/content/article/pii/S266682702100027X,"The Cognitive Radio based Internet of Things (CR-IoT) is a promising technology that provides IoT endpoints, i.e., CR-IoT users the capability to share the radio spectrum otherwise allocated to licensed Primary Users (PUs). Cooperative Spectrum Sensing (CSS) improves spectrum sensing accuracy in a CR-IoT network. However, its performance may be degraded by potential attacks of the malicious CR-IoT users that send their incorrect sensing information to the corresponding Fusion Center (FC). This study presents a promising Machine Learning (ML)-based malicious user detection scheme for a CR-IoT network that uses a Support Vector Machine (SVM) algorithm to identify and classify malicious CR-IoT users. The classification allows the FC to make a more robust global decision based on the sensing results (i.e., energy vectors) which are reported only by the normal CR-IoT users. The effectiveness of the proposed SVM algorithm based ML in a CR-IoT network with the malicious CR-IoT users is verified via simulations.",autonomous vehicle
10.1016/j.ajpath.2021.07.011,journal,The American Journal of Pathology,sciencedirect,2021-10-31,sciencedirect,<ce:marker name=asip alt=ASIP member publication altimg-small=ajpaasip_s.svg altimg=ajpaasip_o.svg></ce:marker>Artificial Intelligence in Pathology,https://api.elsevier.com/content/article/pii/S0002944021003497,,autonomous vehicle
10.1016/j.matpr.2020.02.720,journal,Materials Today: Proceedings,sciencedirect,2021-12-31,sciencedirect,Design and simulation of handwritten recognition system,https://api.elsevier.com/content/article/pii/S2214785320314863,"
                  In the field of computer science involving with the huge amount of role in artificial intelligence, the recent improvements in this have provided various effective solutions for the problems being solved by the human in the today’s world. The main aim of this paper is to describe the recognition of handwritten characters for the ease use of it by using the various machine learning algorithms. This paper involves with the use of supervised and unsupervised machine learning algorithms to provide the betterment of result in the form of accuracy level. Algorithms such as Random forest, Logistic regression, Support Vector Machine (SVM) and K-Nearest Neighbor (KNN) algorithms are compared to provide its efficiency with the result comparison. Out of all the above mentioned algorithms, KNN seems to provide the better accuracy of around 98%.
               ",autonomous vehicle
10.1016/j.nepr.2021.103224,journal,Nurse Education in Practice,sciencedirect,2021-10-31,sciencedirect,Artificial intelligence and predictive analytics in nursing education,https://api.elsevier.com/content/article/pii/S1471595321002602,,autonomous vehicle
10.1016/j.robot.2004.06.001,journal,Robotics and Autonomous Systems,sciencedirect,2004-09-30,sciencedirect,Adaptive neurofuzzy control of a robotic gripper with on-line machine learning,https://api.elsevier.com/content/article/pii/S0921889004000880,"
                  Pre-programming complex robotic systems to operate in unstructured environments is extremely difficult because of the programmer’s inability to predict future operating conditions in the face of unforeseen environmental conditions, mechanical wear of parts, etc. The solution to this problem is for the robot controller to learn on-line about its own capabilities and limitations when interacting with its environment. At the present state of technology, this poses a challenge to existing machine learning methods. We study this problem using a simple two-fingered gripper which learns to grasp an object with appropriate force, without slip while minimising chances of damage to the object. Three machine learning methods are used to produce a neurofuzzy controller for the gripper. These are off-line supervised neurofuzzy learning and two on-line methods, namely unsupervised reinforcement learning and an unsupervised/supervised hybrid. With the two on-line methods, we demonstrate that the controller can learn through interaction with its environment to overcome simulated failure of its sensors. Further, the hybrid is shown to out perform reinforcement learning alone in terms of faster adaptation to the changing circumstances of sensor failure. The hybrid learning scheme allows us to make best use of such pre-labeled datasets as might exist and to remember effectively good control actions discovered by reinforcement learning.
               ",autonomous vehicle
10.1016/j.inffus.2018.09.013,journal,Information Fusion,sciencedirect,2019-09-30,sciencedirect,Machine learning algorithms for wireless sensor networks: A survey,https://api.elsevier.com/content/article/pii/S156625351830277X,"
                  Wireless sensor network (WSN) is one of the most promising technologies for some real-time applications because of its size, cost-effective and easily deployable nature. Due to some external or internal factors, WSN may change dynamically and therefore it requires depreciating dispensable redesign of the network. The traditional WSN approaches have been explicitly programmed which make the networks hard to respond dynamically. To overcome such scenarios, machine learning (ML) techniques can be applied to react accordingly. ML is the process of self-learning from the experiences and acts without human intervention or re-program. The survey of the ML techniques for WSNs is presented in [1], covering period of 2002–2013. In this survey, we present various ML-based algorithms for WSNs with their advantages, drawbacks, and parameters effecting the network lifetime, covering the period from 2014–March 2018. In addition, we also discuss ML algorithms for synchronization, congestion control, mobile sink scheduling and energy harvesting. Finally, we present a statistical analysis of the survey, the reasons for selection of a particular ML techniques to address an issue in WSNs followed by some discussion on the open issues.
               ",autonomous vehicle
10.1016/j.neucom.2021.07.037,journal,Neurocomputing,sciencedirect,2021-10-21,sciencedirect,ZstGAN: An adversarial approach for Unsupervised Zero-Shot Image-to-image Translation,https://api.elsevier.com/content/article/pii/S092523122101081X,"
                  Image-to-image translation models have shown remarkable ability on transferring images among different domains. Most of existing work follows the setting that the source domain and target domain keep the same at training and inference phases, which cannot be generalized to the scenarios for translating an image from an unseen domain to another unseen domain. In this work, we propose the Unsupervised Zero-Shot Image-to-image Translation (UZSIT) problem, which aims to learn a model that can translate samples from image domains that are not observed during training. Accordingly, we propose a framework called ZstGAN: By introducing an adversarial training scheme, ZstGAN learns to model each domain with domain-specific feature distribution that is semantically consistent on vision and attribute modalities. Then the domain-invariant features are disentangled with an shared encoder for image generation. We carry out extensive experiments on CUB and FLO datasets, and the results demonstrate the effectiveness of proposed method on UZSIT task. Moreover, ZstGAN shows significant accuracy improvements over state-of-the-art zero-shot learning methods on CUB and FLO.
               ",autonomous vehicle
10.1016/bs.agph.2020.08.002,journal,Advances in Geophysics,sciencedirect,2020-12-31,sciencedirect,Chapter One: 70 years of machine learning in geoscience in review,https://api.elsevier.com/content/article/pii/S0065268720300054,"
                  This review gives an overview of the development of machine learning in geoscience. A thorough analysis of the codevelopments of machine learning applications throughout the last 70 years relates the recent enthusiasm for machine learning to developments in geoscience. I explore the shift of kriging toward a mainstream machine learning method and the historic application of neural networks in geoscience, following the general trend of machine learning enthusiasm through the decades. Furthermore, this chapter explores the shift from mathematical fundamentals and knowledge in software development toward skills in model validation, applied statistics, and integrated subject matter expertise. The review is interspersed with code examples to complement the theoretical foundations and illustrate model validation and machine learning explainability for science. The scope of this review includes various shallow machine learning methods, e.g., decision trees, random forests, support-vector machines, and Gaussian processes, as well as, deep neural networks, including feed-forward neural networks, convolutional neural networks, recurrent neural networks, and generative adversarial networks. Regarding geoscience, the review has a bias toward geophysics but aims to strike a balance with geochemistry, geostatistics, and geology, however, excludes remote sensing, as this would exceed the scope. In general, I aim to provide context for the recent enthusiasm surrounding deep learning with respect to research, hardware, and software developments that enable successful application of shallow and deep machine learning in all disciplines of Earth science.
               ",autonomous vehicle
10.1016/S0079-6123(06)65033-4,journal,Progress in Brain Research,sciencedirect,2007-12-31,sciencedirect,On the challenge of learning complex functions,https://api.elsevier.com/content/article/pii/S0079612306650334,"
                  A common goal of computational neuroscience and of artificial intelligence research based on statistical learning algorithms is the discovery and understanding of computational principles that could explain what we consider adaptive intelligence, in animals as well as in machines. This chapter focuses on what is required for the learning of complex behaviors. We believe it involves the learning of highly varying functions, in a mathematical sense. We bring forward two types of arguments which convey the message that many currently popular machine learning approaches to learning flexible functions have fundamental limitations that render them inappropriate for learning highly varying functions. The first issue concerns the representation of such functions with what we call shallow model architectures. We discuss limitations of shallow architectures, such as so-called kernel machines, boosting algorithms, and one-hidden-layer artificial neural networks. The second issue is more focused and concerns kernel machines with a local kernel (the type used most often in practice) that act like a collection of template-matching units. We present mathematical results on such computational architectures showing that they have a limitation similar to those already proved for older non-parametric methods, and connected to the so-called curse of dimensionality. Though it has long been believed that efficient learning in deep architectures is difficult, recently proposed computational principles for learning in deep architectures may offer a breakthrough.
               ",autonomous vehicle
10.1016/j.indmarman.2020.12.001,journal,Industrial Marketing Management,sciencedirect,2021-01-31,sciencedirect,An integrated artificial intelligence framework for knowledge creation and B2B marketing rational decision making for improving firm performance,https://api.elsevier.com/content/article/pii/S0019850120309044,"
                  This study examines the effect of big data powered artificial intelligence on customer knowledge creation, user knowledge creation and external market knowledge creation to better understand its impact on B2B marketing rational decision making to influence firm performance. The theoretical model is grounded in Knowledge Management Theory (KMT) and the primary data was collected from B2B companies functioning in the South African mining industry. Findings point out that big data powered artificial intelligence and the path customer knowledge creation is significant. Secondly, big data powered artificial intelligence and the path user knowledge creation is significant. Thirdly, big data powered artificial intelligence and the path external market knowledge creation is significant. It was observed that customer knowledge creation, user knowledge creation and external market knowledge creation have significant effect on the B2B marketing-rational decision making. Finally, the path B2B marketing rational decision making has a significant effect on firm performance.
               ",autonomous vehicle
10.1016/j.comnet.2020.107484,journal,Computer Networks,sciencedirect,2020-12-09,sciencedirect,Artificial intelligence-based vehicular traffic flow prediction methods for supporting intelligent transportation systems,https://api.elsevier.com/content/article/pii/S1389128620311567,"
                  In recent years, the Intelligent transportations system (ITS) has received considerable attention, due to higher demands for road safety and efficiency in highly interconnected road networks. As an essential part of ITS, traffic prediction can provide support in many aspects, such as road routing, traffic congestion control, etc. To provide a more comprehensive overview of the role of traffic forecasting in ITS systems, we will first introduce the corresponding ITS applications and discuss how traffic forecasting can improve the performance of these applications. Next, we will introduce the general prediction procedure as well as some basic concepts of traffic flow prediction, followed by a description of a general framework for implementing the traffic flow prediction. In this survey, mainly two sorts of prediction methods are focused, statistics-based and machine learning (ML)-based. These two types of approaches are more used in ITS traffic flow predictions these years, and service for different contexts. Generally speaking, the statistics-based models have better model interpretability, but the rigorous model structure limits the adaptability, while ML-based models are more flexible. Accordingly, we will introduce the characteristics of these two types of methods through specific examples of state-of-the-art approaches. Last but not least, some potential and meaningful development directions corresponding to this domain are introduced to do a great favor for future research.
               ",autonomous vehicle
10.1016/j.jbankfin.2021.106290,journal,Journal of Banking & Finance,sciencedirect,2021-08-28,sciencedirect,Artificial intelligence and systemic risk,https://api.elsevier.com/content/article/pii/S0378426621002466,"Artificial intelligence (AI) is rapidly changing how the financial system is operated, taking over core functions for both cost savings and operational efficiency reasons. AI will assist both risk managers and the financial authorities. However, it can destabilize the financial system, creating new tail risks and amplifying existing ones due to procyclicality, unknown-unknowns, the need for trust, and optimization against the system.",autonomous vehicle
10.1016/j.yofte.2020.102355,journal,Optical Fiber Technology,sciencedirect,2020-12-31,sciencedirect,Overview on routing and resource allocation based machine learning in optical networks,https://api.elsevier.com/content/article/pii/S106852002030345X,"For optical networks, routing and resource allocation which considerably determines the resource efficiency and network capacity is one of the most important works. It has been widely studied and many excellent algorithms have been developed. However, theoretical analysis shows that routing and resource allocation belongs to the Nondeterministic Polynomial Complete (NP-C) problem no matter in wavelength division multiplexing (WDM) optical networks, elastic optical networks (EONs), or space division multiplexing (SDM) optical networks. At presents, there doesn't exist a polynomial-time algorithm for routing and resource allocation. In recent years, machine learning which shows great advantages in solving complex problems has been widely concerned and researched. Using machine learning to conduct routing and resource allocation has aroused a great interest of researchers. This paper provides an overview on routing and resource allocation based on machine learning in optical networks. At first, we briefly introduce the routing and wavelength allocation (RWA) problem in WDM optical networks, the routing and spectrum allocation (RSA) problem in EONs, and the routing, core, spectrum allocation (RCSA) problem in SDM optical networks respectively. Commonly used machine learning techniques in optical networks are briefly elaborated. Then, the problems of quality of transmission (QoT) estimation, traffic estimation, and crosstalk prediction which can help to routing and resource allocation are also elaborated. The machine learning enabled RWA algorithms, RSA algorithms, and RCSA algorithms are elaborated, analyzed and compared in detail. In addition, the applications of machine learning in the QoT estimation, traffic estimation, and crosstalk prediction, etc., are also elaborated. Based on the existing research results, we present future research directions about how to use machine learning techniques to conduct routing and resource allocation in multidimensional time–space-frequency optical networks and satellite optical networks.",autonomous vehicle
10.1016/j.jbusres.2020.09.013,journal,Journal of Business Research,sciencedirect,2021-05-31,sciencedirect,Artificial intelligence to design collaborative strategy: An application to urban destinations,https://api.elsevier.com/content/article/pii/S0148296320305932,"
                  Organisations currently compete within contexts that require collaboration with other players (suppliers, customers, competitors), which is central to achieving sustainable competitive advantages. This new perspective, which is centred on relationships, has changed the way companies design and implement their competitive strategies, while also challenging traditional tools of strategy analysis. Artificial intelligence, particularly artificial neural networks, can help address these challenges. This paper proposes an innovative application of the Auto-Contractive Map method, which is a deep non-supervised Artificial Neural Network algorithm that has already been widely applied to bio-medical, security, insurance, and financial studies, but has not yet been used in the domains of tourism and strategy. Our study demonstrates the effectiveness of this method, compared to other methods that have been applied to tourism studies. This method successfully addresses issues in the complex and dynamic competitive settings of tourism destinations, which are characterised by the inclusion of many stakeholders. Specifically, the Auto-Contractive Map method allows both scholars and practitioners to significantly advance formulations of collaborative strategies in a destination, at three levels: (i) defining priority areas of action, (ii) identifying relevant stakeholders and governance levels that have control over these areas, and (iii) profiling key features of the destination’s positioning, compared with its competitors.
               ",autonomous vehicle
10.1016/j.comnet.2020.107646,journal,Computer Networks,sciencedirect,2021-01-15,sciencedirect,Massive connectivity with machine learning for the Internet of Things,https://api.elsevier.com/content/article/pii/S1389128620312652,"
                  Driven by the need to ensure the connectivity of an unprecedentedly huge number of IoT devices with no human intervention the issues of massive connectivity have recently become one of the main research areas in IoT studies. Conventional wireless communication technologies are designed for Human-to-Human (H2H) communication which leads to major problems in primary access, channel utilization and spectrum efficiency when massive numbers of devices require connectivity. Current random access procedures are based on a four-step handshaking with control messages which contradicts the requirements of IoT applications in terms of small data payloads and low complexity. Targeted channel utilization and spectrum efficiency cannot be achieved using traditional orthogonal approaches. Thus the goal of our work is to review the most recent developments and critically evaluate the existing work related to the evolution of network access methods in the new communication era. The paper covers three major aspects: first the primary random access procedures, proposed for IoT communications are discussed. The second aspect focuses on the approaches for integration of existing random multiple access schemes with non-orthogonal multiple access methods (NOMA). This integration of random access procedures with NOMA opens a new research trend in the field of massive connectivity. Operating on space domains additional to the physical domain such as code and power domains, NOMA integration targets increased channel utilization and spectrum efficiency to complement the flexibility of random access. On the other hand, the design of efficient algorithms for massive connectivity in IoT is also challenged by the highly application and environmentally dependent traffic model. A new angle of tackling this problem has emerged thanks to the extensive developments in machine learning and the possibilities of their incorporation in communication networks. Thus, the final aspect this review paper addresses are the newly emerging research directions of incorporating machine learning (ML) methods for providing efficient IoT connectivity. Breakthrough ML techniques allow wireless networking devices to perform transmissions by learning and building knowledge about the communication and networking environment. A critical evaluation of the large body of work accumulated in this area in the most recent years and outlining of some major open research issues concludes the paper.
               ",autonomous vehicle
10.1016/j.cognition.2017.11.008,journal,Cognition,sciencedirect,2018-04-30,sciencedirect,Cognitive science in the era of artificial intelligence: A roadmap for reverse-engineering the infant language-learner,https://api.elsevier.com/content/article/pii/S0010027717303013,"
                  Spectacular progress in the information processing sciences (machine learning, wearable sensors) promises to revolutionize the study of cognitive development. Here, we analyse the conditions under which ’reverse engineering’ language development, i.e., building an effective system that mimics infant’s achievements, can contribute to our scientific understanding of early language development. We argue that, on the computational side, it is important to move from toy problems to the full complexity of the learning situation, and take as input as faithful reconstructions of the sensory signals available to infants as possible. On the data side, accessible but privacy-preserving repositories of home data have to be setup. On the psycholinguistic side, specific tests have to be constructed to benchmark humans and machines at different linguistic levels. We discuss the feasibility of this approach and present an overview of current results.
               ",autonomous vehicle
10.1016/j.inffus.2021.07.016,journal,Information Fusion,sciencedirect,2022-01-31,sciencedirect,"Unbox the black-box for the medical explainable AI via multi-modal and multi-centre data fusion: A mini-review, two showcases and beyond",https://api.elsevier.com/content/article/pii/S1566253521001597,"Explainable Artificial Intelligence (XAI) is an emerging research topic of machine learning aimed at unboxing how AI systems’ black-box choices are made. This research field inspects the measures and models involved in decision-making and seeks solutions to explain them explicitly. Many of the machine learning algorithms cannot manifest how and why a decision has been cast. This is particularly true of the most popular deep neural network approaches currently in use. Consequently, our confidence in AI systems can be hindered by the lack of explainability in these black-box models. The XAI becomes more and more crucial for deep learning powered applications, especially for medical and healthcare studies, although in general these deep neural networks can return an arresting dividend in performance. The insufficient explainability and transparency in most existing AI systems can be one of the major reasons that successful implementation and integration of AI tools into routine clinical practice are uncommon. In this study, we first surveyed the current progress of XAI and in particular its advances in healthcare applications. We then introduced our solutions for XAI leveraging multi-modal and multi-centre data fusion, and subsequently validated in two showcases following real clinical scenarios. Comprehensive quantitative and qualitative analyses can prove the efficacy of our proposed XAI solutions, from which we can envisage successful applications in a broader range of clinical questions.",autonomous vehicle
10.1016/j.csbj.2020.11.007,journal,Computational and Structural Biotechnology Journal,sciencedirect,2020-12-31,sciencedirect,Homology modeling in the time of collective and artificial intelligence,https://api.elsevier.com/content/article/pii/S2001037020304748,"Homology modeling is a method for building protein 3D structures using protein primary sequence and utilizing prior knowledge gained from structural similarities with other proteins. The homology modeling process is done in sequential steps where sequence/structure alignment is optimized, then a backbone is built and later, side-chains are added. Once the low-homology loops are modeled, the whole 3D structure is optimized and validated. In the past three decades, a few collective and collaborative initiatives allowed for continuous progress in both homology and ab initio modeling. Critical Assessment of protein Structure Prediction (CASP) is a worldwide community experiment that has historically recorded the progress in this field. Folding@Home and Rosetta@Home are examples of crowd-sourcing initiatives where the community is sharing computational resources, whereas RosettaCommons is an example of an initiative where a community is sharing a codebase for the development of computational algorithms. Foldit is another initiative where participants compete with each other in a protein folding video game to predict 3D structure. In the past few years, contact maps deep machine learning was introduced to the 3D structure prediction process, adding more information and increasing the accuracy of models significantly. In this review, we will take the reader in a journey of exploration from the beginnings to the most recent turnabouts, which have revolutionized the field of homology modeling. Moreover, we discuss the new trends emerging in this rapidly growing field.",autonomous vehicle
10.1016/j.ensm.2019.06.011,journal,Energy Storage Materials,sciencedirect,2019-09-30,sciencedirect,"A perspective on inverse design of battery interphases using multi-scale modelling, experiments and generative deep learning",https://api.elsevier.com/content/article/pii/S2405829719302193,"Understanding and controlling the complex and dynamic processes at battery interfaces holds the key to developing more durable and ultra high performance secondary batteries. Interfacial processes like dendrite and Solid Electrolyte Interphase (SEI) formation span numerous time- and length scales, and despite decades of research, their formation, composition,structure and function still pose a conundrum. Consequently, ”inverse design” of high-performance interfaces and interphases like the SEI, remains an elusive dream. Here, we present a perspective and possible blueprint for a future battery research strategy to reach this ambitious goal. Semi-supervised generative deep learning models trained on all sources of available data, i.e., extensive multi-fidelity datasets from multi-scale computer simulations and databases, operando characterization from large-scale research facilities, high-throughput synthesis and laboratory testing, need to work closely together to unlock this dream. We show how understanding and tracking different types of uncertainties in the experimental and simulation methods, as well as the machine learning framework for the generative model, is crucial for controlling and improving the fidelity in the predictive design of battery interfaces and interphases. We argue that simultaneous utilization of data from multiple domains, including data from failed experiments, will play a critical role in accelerating the development of reliable generative models to enable accelerated discovery and inverse design of durable ultra high performance batteries based on novel materials, structures and designs.",autonomous vehicle
10.1016/j.comcom.2020.02.008,journal,Computer Communications,sciencedirect,2020-03-01,sciencedirect,Machine Learning Models for Secure Data Analytics: A taxonomy and threat model,https://api.elsevier.com/content/article/pii/S0140366419318493,"
                  In recent years, rapid technological advancements in smart devices and their usage in a wide range of applications exponentially increases the data generated from these devices. So, the traditional data analytics techniques may not be able to handle this extreme volume of data known as Big Data (BD) generated by different devices. However, this exponential increase of data opens the doors for the different type of attackers to launch various attacks by exploiting various vulnerabilities (SQL injection, OS fingerprinting, malicious code execution, etc.) during data analytics. Motivated from the aforementioned discussion, in this paper, we explored Machine Learning (ML) and Deep Learning (DL)-based models and techniques which are capable off to identify and mitigate both the known as well as unknown attacks. ML and DL-based techniques have the capabilities to learn from the traffic pattern using training and testing datasets in the extensive network domains to make intelligent decisions concerning attack identification and mitigation. We also proposed a DL and ML-based Secure Data Analytics (SDA) architecture to classify normal or attack input data. A detailed taxonomy of SDA is abstracted into a threat model. This threat model addresses various research challenges in SDA using multiple parameters such as-efficiency, latency, accuracy, reliability, and attacks launched by the attackers. Finally, a comparison of existing SDA proposals with respect to various parameters is presented, which allows the end users to select one of the SDA proposals in comparison to its merits over the others.
               ",autonomous vehicle
10.1016/j.matpr.2019.11.320,journal,Materials Today: Proceedings,sciencedirect,2020-12-31,sciencedirect,Estimation of seismic wave velocities of metamorphic rocks using artificial neural network,https://api.elsevier.com/content/article/pii/S2214785319340064,"
                  Effective physical parameter evaluation of rocks is of engineering and scientific importance and bears application in conventional and non-conventional energy production. Based on the laboratory and field-scale experiments, many mathematical correlations, empirical and semiempirical are developed considering mineralogy, microstructure and chemical composition. Laboratory experiments measuring the seismic velocities offers natural correlation among physical parameters. However, retrieving an intact lab scale size sample from depth is a challenging task. In this paper, we present an artificial neural network (ANN) trained with chemical composition, temperature and pressure of metamorphic rocks recovered from surface to 10 km depth. The training data is collected from literature, and the network is trained for several combinations of hidden layers and neurons. The best predicting network is used, and predictions are made for rocks from a different location. The results of the model predicted velocities are compared with laboratory measurements reported in the literature. A good agreement is found between the model prediction and measurements. The method could be a viable option for early prediction of seismic wave velocities parameters from chemical composition, temperature and pressure for drilling application and Martian or Lunar planetary heat and seismic studies.
               ",autonomous vehicle
10.1016/B978-0-12-822314-7.00010-9,journal,Learning Control,sciencedirect,2021-12-31,sciencedirect,Chapter 5: Deep learning approaches in face analysis,https://api.elsevier.com/content/article/pii/B9780128223147000109,"
               
                  Although face analysis algorithms have changed over the decades, almost in every face-related algorithm it is still usually the case that the order of the problem solving algorithm is the same. The analysis task is relatively easy on frontal and clear faces. However, when it comes to in-the-wild objects with spontaneous expressions, it is a challenging issue due to the changes in illumination, pose variation, expression intensity, subtle deformations, occlusion, etc. The recent success in deep neural networks makes it inevitable to ignore the technique in face analysis to automatically learn the discriminative representations of the face. In a deep network, the input data is run through several hidden layers that decompose the features of the input. The features are then classified using a function to retrieve the class probabilities to predict the output class. This chapter will investigate the deep learning approaches used to detect and pre-process the face, estimate its attributes, classify the expression and recognize the face.
            ",autonomous vehicle
10.1016/j.cose.2019.101635,journal,Computers & Security,sciencedirect,2020-01-31,sciencedirect,Is image-based CAPTCHA secure against attacks based on machine learning? An experimental study,https://api.elsevier.com/content/article/pii/S0167404818312185,"
                  The completely automated public Turing test to tell computers and humans apart (CAPTCHA) is among the most common methods of authentication used by websites and web services. It is intended to protect online services from automated scripts and malicious programs. Text-based and audio CAPTCHA are two of the earliest such methods, and have been shown to be inadequate at protecting systems and services. Image-based CAPTCHA has been introduced to address the limitations of previous CAPTCHA methods. It uses image recognition tasks to determine whether the user is a human or a malicious program. In light of the sensitivity of protected resources, challenges to their security arising from advances in machine learning algorithms are investigated here. This study examines the strength of image-based CAPTCHA by proposing an image-based CAPTCHA breaking system. The proposed system can automatically answer challenges posed by the recently proposed Google image reCAPTCHA with minimal human intervention. It employs deep learning technologies and machine learning algorithms, including random forest, classification and regression trees (CART), bagging with CART, and Naïve Bayes to automatically answer challenges. The proposed attack mechanism achieved an average accuracy of 85.32% while successfully solving 56.29% of reCAPTCHA challenges posed to it. The results show current image-based CAPTCHAs to deter automated scripts and malicious programs provide a false sense of security.
               ",autonomous vehicle
10.1016/B978-0-12-821360-5.00012-9,journal,Power Electronics and Motor Drives,sciencedirect,2021-12-31,sciencedirect,Chapter 12: Neural network and applications,https://api.elsevier.com/content/article/pii/B9780128213605000129,"
               Neural networks (NNWs), the most generic form of AI, are discussed very comprehensively with example applications in this chapter. Artificial neuron structure with different activation functions is explained in the beginning, followed by a simple NNW structure for sine-wave fabrication. Feed-forward perceptrons and Adaline/Madaline structures are explained with their performance features. Then, the most commonly used back-propagation network is explained in detail along with its performance features, flowchart for training, and theory of supervised training. Then, principles of dynamic networks are discussed followed by a discussion of adaptive neuro-fuzzy networks. Example applications of feed-forward NNWs, such as selected harmonic elimination PWM; delayless harmonic filtering; feedback signal estimation of induction motor drives; estimation of distorted current waves; space vector PWM of two-level, three-level, and five-level NPC; and ANFIS-based DTC, are given. Dynamic networks for emulation of dynamic systems are explained followed by applications in PCLPF programmable filter, machine stator flux estimation, sensorless vector drive, neuro-fuzzy efficiency optimization control, and inverse dynamics-based MRAC system. The MATLAB Neural Network Toolbox is discussed with example applications and examples of back-propagation training. Finally, advances and trends in the technology are summarized. In the recent years, the terminology of machine learning (ML) and deep learning (DL) has evolved. ML is a general term in AI which may be based on supervised, unsupervised or reinforcement learning. DL (also known as structured learning) is part of broader family of ML. Both are generally implemented by NNWs.
            ",autonomous vehicle
10.1016/j.neucom.2010.11.012,journal,Neurocomputing,sciencedirect,2011-03-31,sciencedirect,Continuous state/action reinforcement learning: A growing self-organizing map approach,https://api.elsevier.com/content/article/pii/S092523121000473X,"
                  This paper proposes an algorithm to deal with continuous state/action space in the reinforcement learning (RL) problem. Extensive studies have been done to solve the continuous state RL problems, but more research should be carried out for RL problems with continuous action spaces. Due to non-stationary, very large size, and continuous nature of RL problems, the proposed algorithm uses two growing self-organizing maps (GSOM) to elegantly approximate the state/action space through addition and deletion of neurons. It has been demonstrated that GSOM has a better performance in topology preservation, quantization error reduction, and non-stationary distribution approximation than the standard SOM. The novel algorithm proposed in this paper attempts to simultaneously find the best representation for the state space, accurate estimation of Q-values, and appropriate representation for highly rewarded regions in the action space. Experimental results on delayed reward, non-stationary, and large-scale problems demonstrate very satisfactory performance of the proposed algorithm.
               ",autonomous vehicle
10.1016/0959-4388(94)90138-4,journal,Current Opinion in Neurobiology,sciencedirect,1994-12-31,sciencedirect,Reinforcement learning control,https://api.elsevier.com/content/article/pii/0959438894901384,"
                  Reinforcement learning refers to improving performance through trial-and-error. Despite recent progress in developing artificial learning systems, including new learning methods for artificial neural networks, most of these systems learn under the tutelage of a knowledgeable ‘teacher’ able to tell them how to respond to a set of training stimuli. Learning under these conditions is not adequate, however, when it is costly, or even impossible, to obtain this kind of training information. Reinforcement learning is attracting increasing attention in computer science and engineering because it can be used by autonomous systems to learn from their experiences instead of from knowledgeable teachers, and it is attracting attention in computational neuroscience because it is consonant with biological principles. Recent research has improved the efficiency of reinforcement learning and has provided some striking examples of its capabilities.
               ",autonomous vehicle
10.1016/j.ifacol.2020.11.020,journal,IFAC-PapersOnLine,sciencedirect,2020-12-31,sciencedirect,A comparison of machine learning techniques for LNG pumps fault prediction in regasification plants,https://api.elsevier.com/content/article/pii/S2405896320301646,"
                  We present a comparative study on the most popular machine learning methods applied to the challenging problem of Liquefied Natural Gas pumps fault prediction in regasification plants. The proposed solution tries to address the problem of pump failure during operation, this failure makes the pump unavailable, with a high cost of corrective maintenance. It must be taken into account that the condition monitoring may be insufficient because they are cryogenic and inaccessible equipment once the tanks have been started up. The use of machine learning techniques allows us to anticipate the response time by detecting anomalies in the operation, and to be able to do the maintenance before the failure occurs. In our experiments, we predict the power consumption based on the parameters captured in real time during operation. For the composition of the dataset, data was collected between 2007 and 2017, resulting in a dataset of over 15,000 lines for training and validation. First, all models were applied and evaluated on a dataset collected from a real case study. In the second phase, the performance improvement offered by boosting was studied. In order to determine the most efficient parameter combinations we compare Root Mean Squared Error, Absolute Error, Relative Error, Squared Error, Correlation, Training Time and Scoring Time. Our results demonstrate clear superiority of the boosted versions of the models against the plain (non-boosted) versions. The fastest scoring and total time was the Decision Tree and the best overall was Gradient Boosted Trees.
               ",autonomous vehicle
10.1016/j.patrec.2019.12.006,journal,Pattern Recognition Letters,sciencedirect,2020-03-31,sciencedirect,A comprehensive review on multi-organs tumor detection based on machine learning,https://api.elsevier.com/content/article/pii/S0167865519303691,"
                  Tumor is comprised of abnormally growing regions that is dangerous for human survival. Therefore, early stage tumor detection is useful for increase of survival rate although it is challenging because of numerous prolific image factors including poor contrast, complex background, brightness issues, shape of infected region and fuzzy borders. Practically in recent times, numerous computerized aided systems (CAD) perform better for the detection of various sorts of tumors in brain, breast, skin and lung. These body organs are analyzed through different modalities for exampple magnetic resonance imaging (MRI), computed tomography (CT), dermoscopic, mammography and positron emission tomography (PET) etc. The major objective is to extensively analyze existing methodologies for brain, breast, skin and lung cancer/tumor detection. The survey is organized in terms of computer vision employed for cancer/tumor detection. The deep learning approaches are also discussed.
               ",autonomous vehicle
10.1016/j.jmsy.2021.05.008,journal,Journal of Manufacturing Systems,sciencedirect,2021-05-21,sciencedirect,A Review on Recent Advances in Vision-based Defect Recognition towards Industrial Intelligence,https://api.elsevier.com/content/article/pii/S0278612521001059,"
                  In modern manufacturing, vision-based defect recognition is an essential technology to guarantee product quality, and it plays an important role in industrial intelligence. With the developments of industrial big data, defect images can be captured by ubiquitous sensors. And, how to realize accuracy recognition has become a research hotspot. In the past several years, many vision-based defect recognition methods have been proposed, and some newly-emerged techniques, such as deep learning, have become increasingly popular and have addressed many challenging problems effectively. Hence, a comprehensive review is urgently needed, and it can promote the development and bring some insights in this area. This paper surveys the recent advances in vision-based defect recognition and presents a systematical review from a feature perspective. This review divides the recent methods into designed-feature based methods and learned-feature based methods, and summarizes the advantages, disadvantages and application scenarios. Furthermore, this paper also summarizes the performance metrics for vision-based defect recognition methods. And some challenges and development trends are also discussed.
               ",autonomous vehicle
10.1016/j.smhl.2018.07.015,journal,Smart Health,sciencedirect,2018-12-31,sciencedirect,New attacks on RNN based healthcare learning system and their detections,https://api.elsevier.com/content/article/pii/S2352648318300503,"
                  Advances in machine learning (ML) in recent years have enabled a wide range of applications such as data analytics, autonomous systems, and security diagnostics. For example, recurrent neural networks (RNNs), such as long short-term memory networks (LSTMs), serve as a fundamental building block for many sequence learning tasks. Many researchers construct learning based inference and decision making models using primitive learning modules (PLMs) hosted in popular developer platforms, e.g., github. However, most of the publicly available primitive learning modules (PLMs) are maintained by third parties and lack proper checking to ensure they have not been maliciously modified by adversaries. In this paper, we articulate a particular threat to Recurrent Neural Network (RNN) based ML systems by introducing a new attack, which adjusts the weights of a RNN-based model causing it to produce wrong prediction results. Via synthetic and real world datasets, we demonstrate that such an attack is feasible. Next, we propose a detection scheme which can be used to infer if a particular PLM used in a RNN-based ML system contains such malicious behaviors. Experimental results show that our RNN based attack algorithm decreases the system performance when weights of important features are modified during training. In addition, our results also show that our detection mechanism is useful in identifying such malicious PLM.
               ",autonomous vehicle
10.1016/j.future.2021.05.030,journal,Future Generation Computer Systems,sciencedirect,2021-11-30,sciencedirect,Asymmetric cryptographic functions based on generative adversarial neural networks for Internet of Things,https://api.elsevier.com/content/article/pii/S0167739X21001801,"
                  Increasingly, one should assume that the (digital) environment, e.g., Internet-of-Things (IoT) systems, we operate in is untrusted. In other words, this is a zero trust environment, in the sense that all devices and systems can be compromised and hence, untrusted. However, information sharing in a zero trust environment is more challenging, in comparison to an environment where we can rely on some trusted third-party. To address this challenge, we propose a blockchain-enabled zero trust information sharing protocol that is able to support the filtering of fabricated information and protect participant privacy during information sharing. We then prove the security of our protocol in the universally composable secure framework, and also evaluate its performance using a series of experiments. The evaluation results show that the average execution times of the three key steps in our protocol are 0.059 s, 0.060 s and 0.032 s, which demonstrates its potential for deployment in a real-world setting.
               ",autonomous vehicle
10.1016/B978-0-12-816034-3.00005-5,journal,Biomedical Information Technology,sciencedirect,2020-12-31,sciencedirect,Chapter Five: Machine learning in medical imaging,https://api.elsevier.com/content/article/pii/B9780128160343000055,"
               Medical imaging is an indispensable component of modern healthcare, playing a critical role in diagnosis, staging, and the assessment of treatment response for most major medical conditions. Advancements in image acquisition technologies have resulted in imaging modalities that capture more detailed and diverse visual information, leading to challenges in image analysis and interpretation. Within this context, machine learning (ML) algorithms offer the opportunity to enhance clinical analytics and decision-making through the creation of computer tools that can be trained to automatically analyze the visual details of medical image data. This chapter examines the current field of ML methods and their utility in clinical applications in decision support systems, with particular emphasis on advances from developments in deep learning. These methods are discussed with respect to the challenges in ML for medical imaging including limited annotated data, imbalanced class distributions, and labels that may be subjective or uncertain.
            ",autonomous vehicle
10.1016/B978-0-12-823337-5.00006-8,journal,Intelligence-Based Medicine,sciencedirect,2020-12-31,sciencedirect,Chapter 6: Other Key Concepts in Artificial Intelligence,https://api.elsevier.com/content/article/pii/B9780128233375000068,"
               While machine and deep learning are garnering most of the attention in the artificial intelligence (AI) domain at the present time, other essential tools are also important in this portfolio of AI resources. Cognitive computing, as exemplified by IBM Watson, is a framework of machine learning, pattern recognition, and natural language processing (NLP) to mimic the human brain. The DeepQA technology of Watson (a massively parallel probabilistic evidence-based architecture) can effectively deconstruct a question and result in a hypothesis with evidence and confidence scoring. There are key differences between characteristics of AI and cognitive computing even though cognitive computing is considered within the realm of AI. NLP, the machine’s capability to understand language and is the intersection of AI, computer science, and linguistics. NLP consists of natural language understanding and generation and is used in chatbots. Autonomous systems, including robotics, is an AI domain that has a significant role in health care, with surgery and rehabilitation being leading areas. Other key areas of AI technology for health care include robotic process automation, extended reality (augmented, virtual, and mixed reality), blockchain, cloud computing, and internet of things.
            ",autonomous vehicle
10.1016/j.neucom.2020.07.154,journal,Neurocomputing,sciencedirect,2021-10-07,sciencedirect,Optimizing CNN-LSTM neural networks with PSO for anomalous query access control,https://api.elsevier.com/content/article/pii/S0925231221009474,"
                  Database security focuses on protecting most organization’s virtual data storage unit and confidential information from malicious threats and external attacks. To keep out data secure, we need to use a role-based access control (RBAC) approach to accurately differentiate access permissions, but SQL queries written by an authorized user have very similar characteristics and are difficult to distinguish. In this paper, we propose a method of optimizing CNN-LSTM neural networks with particle swarm optimization (PSO) to classify the roles in RBAC system. Convolutional neural network (CNN) can extract parsed SQL queries into smaller details and features through an analysis mechanism. Long short-term memory (LSTM) is also suitable for modeling the temporal information of SQL queries to recognize the context of user authorities. PSO repeatedly searches and optimizes the complex hyperparameter space of the CNN-LSTM. Our PSO-based CNN-LSTM neural networks outperform other deep learning and machine learning models in the TPC-E benchmark SQL query statement. Finally, experiments and analysis show the usefulness of PSO and identify the important SQL query features that affect user role classification.
               ",autonomous vehicle
10.3182/20060522-3-FR-2904.00051,journal,IFAC Proceedings Volumes,sciencedirect,2006-12-31,sciencedirect,APPROACH OF IMPLEMENTATION OF AN INTELLIGENT ENTERPRISE STAFF ADAPTIVE E-LEARNING,https://api.elsevier.com/content/article/pii/S1474667015330706,"
                  This paper presents the principles of development of an intelligent e-curricula model for the employees of the future generations of manufacturing or service processing systems. To introduce an adaptive e-learning property for the employees of an intelligent future enterprise, the proposed e-curricula model is constructed. It is based both on using an adaptive multi – agent system and on applying the following unsupervised learning algorithms: the Q-learning, and a special type of artificial neural network, the Kohonen's Self-Organizing Maps.
               ",autonomous vehicle
10.1016/j.enbuild.2020.110013,journal,Energy and Buildings,sciencedirect,2020-08-01,sciencedirect,"A state-of-the-art-review on phase change materials integrated cooling systems for deterministic parametrical analysis, stochastic uncertainty-based design, single and multi-objective optimisations with machine learning applications",https://api.elsevier.com/content/article/pii/S037877882030178X,"
                  Renewable energy utilisation, latent energy storage, optimal system design, and robust system operation are critical elements for carbon-free buildings and communities. Machine learning methods are effective to assist the energy-efficient renewable systems during multi-criteria design and multi-level uncertainty-based operation periods. However, the current literature provides little knowledge on this topic. In this study, a state-of-the-art-review on phase change materials for cooling applications is presented, in terms of smart ventilations, intelligent PCMs charging/discharging, deterministic parametrical analysis, stochastic uncertainty-based performance prediction and optimisation. Furthermore, technical effectiveness of machine learning methods in single and multi-objective optimisations has been presented, through hybrid PCMs integrated renewable systems. Multivariables involved in the review include thermo-physical, geometrical and operating parameters of PCMs. Multi-criteria employed in the review include heat transfer rate, cooling energy storage density, heat storage and release efficiency, and indoor thermal comfort. The literature review presents technical challenges, such as tradeoff solutions between computational accuracy and efficiency, generic methods for effective selection amongst multi-diversified optimal solutions along the Pareto front, the general methodology for multi-level uncertainty quantification, smart controllers with accurate predictions under high-level parameters’ uncertainty and stochastic occupants’ behaviors. The future outlook and recommendations of machine learning methods in PCMs integrated cooling systems have also been presented as avenues for upcoming research.
               ",autonomous vehicle
10.1016/j.comcom.2008.05.040,journal,Computer Communications,sciencedirect,2008-09-05,sciencedirect,Neural network-based learning schemes for cognitive radio systems,https://api.elsevier.com/content/article/pii/S0140366408003393,"
                  Intelligence is needed to keep up with the rapid evolution of wireless communications, especially in terms of managing and allocating the scarce, radio spectrum in the highly varying and disparate modern environments. Cognitive radio systems promise to handle this situation by utilizing intelligent software packages that enrich their transceiver with radio-awareness, adaptability and capability to learn. A cognitive radio system participates in a continuous process, the “cognition cycle”, during which it adjusts its operating parameters, observes the results and, eventually takes actions, that is to say, decides to operate in a specific radio configuration (i.e., radio access technology, carrier frequency, modulation type, etc.) expecting to move the radio toward some optimized operational state. In such a process, learning mechanisms that are capable of exploiting measurements sensed from the environment, gathered experience and stored knowledge, are judged as rather beneficial for guiding decisions and actions. Framed within this statement, this paper introduces and evaluates learning schemes that are based on artificial neural networks and can be used for predicting the capabilities (e.g. data rate) that can be achieved by a specific radio configuration. In particular, the focus in this work is placed on obtaining insight on the behavior of the presented, learning schemes, whereas useful, indicative results from the benchmarking work, conducted in order to design and use an appropriate neural network structure, are also presented and discussed. In the near future, such learning schemes are expected to assist a cognitive radio system to compare among the whole of available, candidate radio configurations and finally select the best one to operate in.
               ",autonomous vehicle
10.1016/j.drudis.2018.05.010,journal,Drug Discovery Today,sciencedirect,2018-08-31,sciencedirect,Machine learning in chemoinformatics and drug discovery,https://api.elsevier.com/content/article/pii/S1359644617304695,"Chemoinformatics is an established discipline focusing on extracting, processing and extrapolating meaningful data from chemical structures. With the rapid explosion of chemical ‘big’ data from HTS and combinatorial synthesis, machine learning has become an indispensable tool for drug designers to mine chemical information from large compound databases to design drugs with important biological properties. To process the chemical data, we first reviewed multiple processing layers in the chemoinformatics pipeline followed by the introduction of commonly used machine learning models in drug discovery and QSAR analysis. Here, we present basic principles and recent case studies to demonstrate the utility of machine learning techniques in chemoinformatics analyses; and we discuss limitations and future directions to guide further development in this evolving field.",autonomous vehicle
10.1016/j.commatsci.2019.109474,journal,Computational Materials Science,sciencedirect,2020-03-31,sciencedirect,Catalytic materials and chemistry development using a synergistic combination of machine learning and <ce:italic>ab initio</ce:italic> methods,https://api.elsevier.com/content/article/pii/S0927025619307736,"
                  First principles-based molecular modelling plays a crucial role in the development of novel catalytic materials and in the investigation of catalytic chemical reactions. However, the computational cost and/or the accuracy of these models remains a bottleneck in carrying out these simulations for complex or large scale systems, as in the case of catalysis. Over the past two decades, machine learning (ML) has made an impact in the field of computational catalysis. Modern-day researchers have started using machine learning-based data-driven techniques to overcome the limitations of these molecular simulations. In this review, we summarize the recent progress in the utilization of ML algorithms to assist molecular simulations, followed by its applications in the field of catalysis. Furthermore, we provide our perspective on promising avenues for research in the future regarding the incorporation of ML in molecular simulations in catalysis.
               ",autonomous vehicle
10.1016/j.acra.2019.10.009,journal,Academic Radiology,sciencedirect,2020-02-29,sciencedirect,On High Grade Kidney Cancer and Machine Learning,https://api.elsevier.com/content/article/pii/S1076633219304921,,autonomous vehicle
10.1016/j.neunet.2015.07.004,journal,Neural Networks,sciencedirect,2015-12-31,sciencedirect,Neuromorphic implementations of neurobiological learning algorithms for spiking neural networks,https://api.elsevier.com/content/article/pii/S0893608015001410,"
                  The application of biologically inspired methods in design and control has a long tradition in robotics. Unlike previous approaches in this direction, the emerging field of neurorobotics not only mimics biological mechanisms at a relatively high level of abstraction but employs highly realistic simulations of actual biological nervous systems. Even today, carrying out these simulations efficiently at appropriate timescales is challenging. Neuromorphic chip designs specially tailored to this task therefore offer an interesting perspective for neurorobotics. Unlike Von Neumann CPUs, these chips cannot be simply programmed with a standard programming language. Like real brains, their functionality is determined by the structure of neural connectivity and synaptic efficacies. Enabling higher cognitive functions for neurorobotics consequently requires the application of neurobiological learning algorithms to adjust synaptic weights in a biologically plausible way. In this paper, we therefore investigate how to program neuromorphic chips by means of learning. First, we provide an overview over selected neuromorphic chip designs and analyze them in terms of neural computation, communication systems and software infrastructure. On the theoretical side, we review neurobiological learning techniques. Based on this overview, we then examine on-die implementations of these learning algorithms on the considered neuromorphic chips. A final discussion puts the findings of this work into context and highlights how neuromorphic hardware can potentially advance the field of autonomous robot systems. The paper thus gives an in-depth overview of neuromorphic implementations of basic mechanisms of synaptic plasticity which are required to realize advanced cognitive capabilities with spiking neural networks.
               ",autonomous vehicle
10.1016/S0954-1810(98)00020-X,journal,Artificial Intelligence in Engineering,sciencedirect,1999-04-30,sciencedirect,Autonomous agent based on reinforcement learning and adaptive shadowed network,https://api.elsevier.com/content/article/pii/S095418109800020X,"
                  The planning of intelligent robot behavior plays an important role in the development of flexible automated systems. The robot’s intelligence comprises its capability to act in unpredictable and chaotic situations, which requires not just a change but the creation of the robot’s working knowledge. Planning of intelligent robot behavior addresses three main issues: finding task solutions in unknown situations, learning from experience and recognizing the similarity of problem paradigms. This article outlines a planning system which integrates the reinforcement learning method and a neural network approach with the aim to ensure autonomous robot behavior in unpredictable working conditions.
                  The assumption is that the robot is a tabula rasa and has no knowledge of the work space structure. Initially, it has just basic strategic knowledge of searching for solutions, based on random attempts, and a built-in learning system. The reinforcement learning method is used here to evaluate robot behavior and to induce new, or improve the existing, knowledge. The acquired action (task) plan is stored as experience which can be used in solving similar future problems. To provide the recognition of problem similarities, the Adaptive Fuzzy Shadowed neural network is designed. This novel network concept with a fuzzy learning rule and shadowed hidden layer architecture enables the recognition of slightly translated or rotated patterns and does not forget already learned structures.
                  The intelligent planning system is simulated using object-oriented techniques and verified on planned and random examples, proving the main advantages of the proposed approach: autonomous learning, which is invariant with regard to the order of training samples, and single iteration learning progress.
               ",autonomous vehicle
10.1016/B978-0-12-821259-2.00025-9,journal,Artificial Intelligence in Medicine,sciencedirect,2021-12-31,sciencedirect,Chapter 25: Outlook of the future landscape of artificial intelligence in medicine and new challenges,https://api.elsevier.com/content/article/pii/B9780128212592000259,"
               Basic and technical aspects and important applications of artificial intelligence (AI) in medicine (AIM) have been presented in previous chapters. Given the broad scope and incredible depth of potential impact that AI promises to bring, it is quite clear that medicine is on the verge of a revolution. Looking ahead, much needs to be done to optimize the pathway for clinical translation and to maximally utilize the capacity of AI to benefit the well-being of patients. In this final chapter we highlight some important trends in research and development in AIM and their indications to the future of health care. In particular, the urgent demands in advanced big data analytics, practically feasible data curation and sharing schemes, quantitative imaging tools, and more intelligent and broadly applicable machine learning algorithms will be elaborated. New opportunities and challenges in AIM will also be discussed.
            ",autonomous vehicle
10.1016/j.engappai.2014.05.012,journal,Engineering Applications of Artificial Intelligence,sciencedirect,2014-09-30,sciencedirect,Aggregate Reinforcement Learning for multi-agent territory division: The Hide-and-Seek game,https://api.elsevier.com/content/article/pii/S0952197614001109,"
                  In many applications in Robotics such as disaster rescuing, mine detection, robotic surveillance and warehouse systems, it is crucial to build multi-agent systems (MAS) in which agents cooperate to complete a sequence of tasks. For better performance in such systems, e.g. minimizing duplicate work, agents need to agree on how to divide and plan that sequence of tasks among themselves. This paper targets the problem of territory division in the children’s game of Hide-and-Seek as a test-bed for our proposed approach. The problem is solved in a hierarchical learning scheme using Reinforcement Learning (RL). Based on Q-learning, our learning model is presented in detail; definition of composite states, actions, and reward function to deal with multiple agent learning. In addition, a revised version of the standard updating rule of the Q-learning is proposed to cope with multiple seekers. The model is examined on a set of different maps, on which it converges to the optimal solutions. After the complexity analysis of the algorithm, we enhanced it by using state aggregation (SA) to alleviate the state space explosion. Two levels of aggregation are devised: topological and hiding aggregation. After elaborating how the learning model is modified to handle the aggregation technique, the enhanced model is examined by some experiments. Results indicate promising performance with higher convergence rate and up to 10× space reduction.
               ",autonomous vehicle
10.1016/B978-0-12-823337-5.00008-1,journal,Intelligence-Based Medicine,sciencedirect,2020-12-31,sciencedirect,Chapter 8: Artificial Intelligence in Subspecialties,https://api.elsevier.com/content/article/pii/B9780128233375000081,"
               The present state of artificial intelligence in subspecialties is summarized to provide a backdrop for this chapter on the state of artificial intelligence in each subspecialty. First, a broad discussion of artificial intelligence application for all subspecialties reviews the main areas of focus such as use of machine and deep learning for medical image interpretation and decision support, use of artificial intelligence tools for administrative support, use of natural language processing for communication, use of artificial intelligence for data mining, and use of machine and deep learning for risk assessment and intervention. For each of the subspecialties or health care sector (such as medical education and training or health-care administration), the current published reviews and selected works are first discussed, followed by present assessment and future strategy for that subspecialty. For each subspecialty, a table of clinical relevance versus current artificial intelligence availability is presented to visualize potential application in the future.
            ",autonomous vehicle
10.1016/j.osn.2017.12.006,journal,Optical Switching and Networking,sciencedirect,2018-04-30,sciencedirect,Artificial intelligence (AI) methods in optical networks: A comprehensive survey,https://api.elsevier.com/content/article/pii/S157342771730231X,"Artificial intelligence (AI) is an extensive scientific discipline which enables computer systems to solve problems by emulating complex biological processes such as learning, reasoning and self-correction. This paper presents a comprehensive review of the application of AI techniques for improving performance of optical communication systems and networks. The use of AI-based techniques is first studied in applications related to optical transmission, ranging from the characterization and operation of network components to performance monitoring, mitigation of nonlinearities, and quality of transmission estimation. Then, applications related to optical network control and management are also reviewed, including topics like optical network planning and operation in both transport and access networks. Finally, the paper also presents a summary of opportunities and challenges in optical networking where AI is expected to play a key role in the near future.",autonomous vehicle
10.1016/j.neunet.2009.06.049,journal,Neural Networks,sciencedirect,2009-08-31,sciencedirect,Goal-directed learning of features and forward models,https://api.elsevier.com/content/article/pii/S0893608009001245,"
                  The brain is able to perform actions based on an adequate internal representation of the world, where task-irrelevant features are ignored and incomplete sensory data are estimated. Traditionally, it is assumed that such abstract state representations are obtained purely from the statistics of sensory input for example by unsupervised learning methods. However, more recent findings suggest an influence of the dopaminergic system, which can be modeled by a reinforcement learning approach. Standard reinforcement learning algorithms act on a single layer network connecting the state space to the action space. Here, we involve in a feature detection stage and a memory layer, which together, construct the state space for a learning agent. The memory layer consists of the state activation at the previous time step as well as the previously chosen action. We present a temporal difference based learning rule for training the weights from these additional inputs to the state layer. As a result, the performance of the network is maintained both, in the presence of task-irrelevant features, and at randomly occurring time steps during which the input is invisible. Interestingly, a goal-directed forward model emerges from the memory weights, which only covers the state–action pairs that are relevant to the task. The model presents a link between reinforcement learning, feature detection and forward models and may help to explain how reward systems recruit cortical circuits for goal-directed feature detection and prediction.
               ",autonomous vehicle
10.1016/j.procs.2019.06.016,journal,Procedia Computer Science,sciencedirect,2019-12-31,sciencedirect,Deep Learning-Based Power Usage Forecast Modeling and Evaluation,https://api.elsevier.com/content/article/pii/S1877050919307859,"The growing Internet of Things (IoT) provides significant resources to be integrated with critical infrastructures to enable cyber-physical systems. More specifically, the deployment of smart meters for electricity usage monitoring in the smart grid can provide granular and detailed information from which power load forecasting can be carried out. However, the accurate prediction of long-term power usage remains a challenging issue. In light of many recent advances, deep learning has the potential to significantly improve the ability to assess data and make predictions, and is already rapidly changing the world we live in. As such, in this paper, we consider the use of deep learning, via Recursive Neural Network (RNN) and Long Short-Term Memory layers, for the long-term prediction of localized power consumption. In particular, we consider the optimization of both data feature sets and neural network models, developing three model-feature combinations to maximize prediction accuracy and minimize error. Through detailed experimental evaluation, our results demonstrate the ability to achieve highly accurate predictions over periods as large as 21 days through the integration of correlated features.",autonomous vehicle
10.1016/j.promfg.2021.06.061,journal,Procedia Manufacturing,sciencedirect,2021-12-31,sciencedirect,Application of Artificial Intelligence in Incremental Sheet Metal Forming: A Review,https://api.elsevier.com/content/article/pii/S2351978921000718,"Artificial Intelligence (AI) has been widely used in manufacturing, healthcare, sports, finance, and other areas to model nonlinearities and make reliable predictions. In manufacturing, AI has been applied to improve processes, reduce costs, and increase reliability. A novel manufacturing process that has been augmented with AI is Incremental Sheet Forming (ISF), a technology that applies a step-by-step incremental feed to a sheet metal or polymer blank using a CNC machine. The quality of the produced part is affected by parameters related to four process elements: the blank, the blank holder, the forming tool, and the CNC machine (applied force). The ISF process is greatly affected by forming process parameters, material property parameters, and geometric parameters. Numerous research efforts have correlated the relationship between the ISF parameters to final product attributes using analytical, experimental, and numerical techniques. However, these techniques are not efficient due to the nonlinearities and complexities of the relationships, the time-consuming nature of the process, and extensive computational time needed to simulate the process. To compensate for these shortcomings, researchers have started to apply AI techniques in analysis of ISF. The aim of this paper is to review the application of AI in the ISF process in forming of metal sheets in order to summarize the contributions of prior research efforts and identify potential opportunities for future research.",autonomous vehicle
10.1016/j.jbi.2018.10.008,journal,Journal of Biomedical Informatics,sciencedirect,2018-12-31,sciencedirect,Toward analyzing and synthesizing previous research in early prediction of cardiac arrest using machine learning based on a multi-layered integrative framework,https://api.elsevier.com/content/article/pii/S153204641830203X,"Background One of the significant problems in the field of healthcare is the low survival rate of people who have experienced sudden cardiac arrest. Early prediction of cardiac arrest can provide the time required for intervening and preventing its onset in order to reduce mortality. Traditional statistical methods have been used to predict cardiac arrest. They have often analyzed group-level differences using a limited number of variables. On the other hand, machine learning approach, which is part of a growing trend of predictive medical analysis, has provided personalized predictive analyses on more complex data and produced remarkable results. Objective This paper has two aims. First, it offers a systematic review to evaluate the capability and performance of machine learning techniques in predicting the risk of cardiac arrest. Second, it offers an integrative framework to synthesize the researches in this field. Method A systematic review of cardiac arrest prediction studies was carried out through Pubmed, ScienceDirect, Google Scholar and SpringerLink databases. These studies used machine learning techniques and were conducted between the years 2000 and 2018. Results From a total of 1617 papers retrieved from the literature search, 75 studies were included in the final analysis. In order to explore how machine learning techniques were employed to predict cardiac arrest, a multi-layered framework was proposed. Each layer of the framework represents a classification of the current literature and contains taxonomies of relevant observed information. The framework integrates these classifications and illustrates the relative influence of a layer on other layers. The included papers were analyzed and synthesized through this framework. The used machine learning techniques were evaluated in terms of application and efficiency. The results illustrated the prediction capability of machine learning methods in predicting cardiac arrest. Conclusion According to the results, machine learning techniques can improve the outcome of cardiac arrest prediction. However, future research should be carried out to evaluate the efficiency of rarely-used algorithms and to address the challenges of external validation, implementation and adoption of machine learning models in real clinical environments.",autonomous vehicle
10.1016/j.cobeha.2016.05.012,journal,Current Opinion in Behavioral Sciences,sciencedirect,2016-10-31,sciencedirect,Does computational neuroscience need new synaptic learning paradigms?,https://api.elsevier.com/content/article/pii/S2352154616301048,"Computational neuroscience is dominated by a few paradigmatic models, but it remains an open question whether the existing modelling frameworks are sufficient to explain observed behavioural phenomena in terms of neural implementation. We take learning and synaptic plasticity as an example and point to open questions, such as one-shot learning and acquiring internal representations of the world for flexible planning.",autonomous vehicle
10.1016/j.neuron.2017.06.011,journal,Neuron,sciencedirect,2017-07-19,sciencedirect,Neuroscience-Inspired Artificial Intelligence,https://api.elsevier.com/content/article/pii/S0896627317305093,"The fields of neuroscience and artificial intelligence (AI) have a long and intertwined history. In more recent times, however, communication and collaboration between the two fields has become less commonplace. In this article, we argue that better understanding biological brains could play a vital role in building intelligent machines. We survey historical interactions between the AI and neuroscience fields and emphasize current advances in AI that have been inspired by the study of neural computation in humans and other animals. We conclude by highlighting shared themes that may be key for advancing future research in both fields.",autonomous vehicle
10.1016/j.cogsys.2021.10.002,journal,Cognitive Systems Research,sciencedirect,2022-01-31,sciencedirect,ECNN: Enhanced convolutional neural network for efficient diagnosis of autism spectrum disorder,https://api.elsevier.com/content/article/pii/S1389041721000759,"
                  This paper aims to apply deep learning to identify autism spectrum disorder (ASD) patients from a large brain imaging dataset based on the patients’ brain activation patterns. The brain images are collected from the ABIDE (Autism Brain Imaging Data Exchange) database. The proposed convolutional neural network (CNN) architecture investigates functional connectivity patterns between different brain areas to identify specifics patterns to diagnose ASD. The enhanced CNN uses blocks of temporal convolutional layers that employ casual convolutions and dilations; hence, it is suitable for sequential data with temporality large receptive fields. Experimental results show that the proposed ECNN achieves an accuracy of up to 80% accuracy. These patterns show an anticorrelation of brain function between anterior and posterior areas of the brain; that is, the disruption in brain connectivity is one primary evidence of ASD.
               ",autonomous vehicle
10.3182/20020721-6-ES-1901.00827,journal,IFAC Proceedings Volumes,sciencedirect,2002-12-31,sciencedirect,REINFORCEMENT LEARNING OF FUZZY LOGIC CONTROLLERS FOR QUADRUPED,https://api.elsevier.com/content/article/pii/S147466701539248X,"
                  This paper presents a fuzzy logic controller (FLC) for the implementation of some behaviour of Sony legged robots. The adaptive heuristic Critic (AHC) reinforcement learning is employed to refine the FLC. The actor part of AHC is a conventional FLC in which the parameters of input membership functions are learned by an immediate internal reinforcement signal. This internal reinforcement signal comes from a prediction of the evaluation value of a policy and the external reinforcement signal. The evaluation value of a policy is learned by temporal difference (TD) learning in the critic part that is also represented by a FLC. A genetic algorithm (GA) is employed for learning internal reinforcement of the actor part because it is more efficient in searching than other trial and error search approaches.
               ",autonomous vehicle
10.1016/j.aei.2020.101209,journal,Advanced Engineering Informatics,sciencedirect,2021-01-31,sciencedirect,Evolutionary digital twin: A new approach for intelligent industrial product development,https://api.elsevier.com/content/article/pii/S1474034620301786,"
                  To fulfill increasingly difficult and demanding tasks in the ever-changing complex world, intelligent industrial products are to be developed with higher flexibility and adaptability. Digital twin (DT) brings about a possible means, due to its ability to provide candidate behavior adjustments based on received “feedbacks” from its physical part. However, such candidate adjustments are deterministic, and thus lack of flexibility and adaptability. To address such problem, in this paper an extended concept – evolutionary digital twin (EDT) and an EDT-based new mode for intelligent industrial product development has been proposed. With our proposed EDT, a more precise approximated model of the physical world could be established through supervised learning, based on which the collaborative exploration for optimal policies via parallel simulation in multiple cyberspaces could be performed through reinforcement learning. Hence, more flexibility and adaptability could be brought to industrial products through machine learning (such as supervised learning and reinforcement learning) based self-evolution. As a primary verification of the effectiveness of our proposed approach, a case study has been carried out. The experimental results have well confirmed the effectiveness of our EDT based development mode.
               ",autonomous vehicle
10.1016/j.neunet.2014.12.006,journal,Neural Networks,sciencedirect,2015-04-30,sciencedirect,Editorial introduction to the Neural Networks special issue on Deep Learning of Representations,https://api.elsevier.com/content/article/pii/S089360801400286X,,autonomous vehicle
10.1016/j.rser.2021.111341,journal,Renewable and Sustainable Energy Reviews,sciencedirect,2021-10-31,sciencedirect,Thermo-physical properties prediction of carbon-based magnetic nanofluids based on an artificial neural network,https://api.elsevier.com/content/article/pii/S1364032121006274,"
                  Nanostructured magnetic suspensions have superior thermophysical properties, which have attracted widespread attention owing to their industrial applications for heat transfer enhancement and thermal management. However, experimental measurements of the thermophysical properties of magnetic-based nanofluids, especially under an external magnetic field, are significantly complicated, expensive, and time consuming. Currently, the method of predicting and summarizing material properties through machine learning has accelerated the development of materials and practical industrial applications. This study aims to predict the thermophysical properties of magnetic nanofluids by establishing an artificial neural network (ANN) using experimental data on viscosity, thermal conductivity, and specific heat. The results based on the ANN model agree with the experimental results according to the different evaluation criteria. Different previous theoretical thermophysical models are reviewed, and the ANN model is proven to be more accurate by comparing the values of the ANN model and previous thermophysical models, which can also provide a theoretical basis for explaining the heat transfer of magnetic nanofluids. In the present study, a neural network model was developed for predicting the thermophysical properties of magnetic nanofluids and using material informatics to study functional materials.
               ",autonomous vehicle
10.1016/j.engappai.2021.104484,journal,Engineering Applications of Artificial Intelligence,sciencedirect,2021-11-30,sciencedirect,Towards dense people detection with deep learning and depth images,https://api.elsevier.com/content/article/pii/S0952197621003328,"
                  This paper describes a novel DNN-based system, named PD3net, that detects multiple people from a single depth image, in real time. The proposed neural network processes a depth image and outputs a likelihood map in image coordinates, where each detection corresponds to a Gaussian-shaped local distribution, centered at each person’s head. This likelihood map encodes both the number of detected people as well as their position in the image, from which the 3D position can be computed. The proposed DNN includes spatially separated convolutions to increase performance, and runs in real-time with low budget GPUs. We use synthetic data for initially training the network, followed by fine tuning with a small amount of real data. This allows adapting the network to different scenarios without needing large and manually labeled image datasets. Due to that, the people detection system presented in this paper has numerous potential applications in different fields, such as capacity control, automatic video-surveillance, people or groups behavior analysis, healthcare or monitoring and assistance of elderly people in ambient assisted living environments. In addition, the use of depth information does not allow recognizing the identity of people in the scene, thus enabling their detection while preserving their privacy. The proposed DNN has been experimentally evaluated and compared with other state-of-the-art approaches, including both classical and DNN-based solutions, under a wide range of experimental conditions. The achieved results allows concluding that the proposed architecture and the training strategy are effective, and the network generalize to work with scenes different from those used during training. We also demonstrate that our proposal outperforms existing methods and can accurately detect people in scenes with significant occlusions.
               ",autonomous vehicle
10.1016/B978-0-12-801238-3.10876-1,journal,Encyclopedia of Biomedical Engineering,sciencedirect,2019-12-31,sciencedirect,Machine Learning in Biomedical Informatics,https://api.elsevier.com/content/article/pii/B9780128012383108761,"
               Biomedical informatics has skyrocketed in the last years by reducing sequencing costs with next-generation sequencing techniques. Thus, the amount of available data to study is increasing excessively, and more recently, is even open access for researchers. Due to this, biomedical informatics researchers, with different profiles, are using machine learning algorithms for knowledge extraction and, despite the great amount of benefits this entails, it also requires to take into account a series of particularities of inexcusable compliance in order to achieve a solution which is real.
            ",autonomous vehicle
10.1016/j.tig.2021.04.004,journal,Trends in Genetics,sciencedirect,2021-09-30,sciencedirect,Opportunities and challenges for artificial intelligence in clinical cardiovascular genetics,https://api.elsevier.com/content/article/pii/S0168952521000858,"
                  A combination of emerging genomic and artificial intelligence (AI) techniques may ultimately unlock a deeper understanding of heterogeneity and biological complexities in cardiovascular diseases (CVDs), leading to advances in prognostic guidance and personalized therapies. We discuss the state of AI in cardiovascular genetics, current applications, limitations, and future directions of the field.
               ",autonomous vehicle
10.1016/j.future.2020.07.038,journal,Future Generation Computer Systems,sciencedirect,2020-12-31,sciencedirect,Using machine learning techniques to analyze the performance of concurrent kernel execution on GPUs,https://api.elsevier.com/content/article/pii/S0167739X19312658,"
                  Heterogeneous systems employing CPUs and GPUs are becoming increasingly popular in large-scale data centers and cloud environments. In these platforms, sharing a GPU across different applications is an important feature to improve hardware utilization and system throughput. However, under scenarios where GPUs are competitively shared, some challenges arise. The decision on the simultaneous execution of different kernels is made by the hardware and depends on the kernels resource requirements. Besides that, it is very difficult to understand all the hardware variables involved in the simultaneous execution decisions in order to describe a formal allocation method. In this work, we use machine learning techniques to understand how the resource requirements of the kernels from the most important GPU benchmarks impact their concurrent execution. We focus on making the machine learning algorithms capture the hidden patterns that make a kernel interfere in the execution of another one when they are submitted to run at the same time. The techniques analyzed were 
                        k
                     -NN, Logistic Regression, Multilayer Perceptron and XGBoost (which obtained the best results) over the GPU benchmark suites, Rodinia, Parboil and SHOC. Our results showed that, from the features selected in the analysis, the number of blocks per grid, number of threads per block, and number of registers are the resource consumption features that most affect the performance of the concurrent execution.
               ",autonomous vehicle
10.1016/j.procs.2020.10.071,journal,Procedia Computer Science,sciencedirect,2020-12-31,sciencedirect,Current and Future RL’s Contribution to Emerging Network Security,https://api.elsevier.com/content/article/pii/S1877050920323413,"Reinforcement learning is a machine-learning paradigm, which learns the best actions an agent needs to perform to maximize its rewards in a particular environment. Research into RL has been proven to have made a real contribution to the protection of emerging network systems against malware. In this paper, a systematic review of this research was performed in regard to various attacks and an analysis of the trends and future fields of interest for the RL-based research in network security was completed.",autonomous vehicle
10.1016/j.wpi.2020.101988,journal,World Patent Information,sciencedirect,2020-09-30,sciencedirect,Patenting patterns in Artificial Intelligence: Identifying national and international breeding grounds,https://api.elsevier.com/content/article/pii/S0172219020300806,"This paper identifies countries at the forefront of Artificial Intelligence (AI) development and proposes two novel patent-based indicators to differentiate structural differences in the patterns of intellectual property (IP) protection observed for AI across countries. In particular, we consider (i) the extent to which countries specialise in AI and are relevant markets for corresponding IP protection (‘National Breeding Ground’); and (ii) the extent to which countries attract AI from abroad for IP protection and extend the protection of their AI-related IP to foreign markets (‘International Breeding Ground’). Our investigation confirms prior findings regarding substantial changes in the technological leadership in AI, besides drastic changes in the relevance of AI techniques over time. Particularly, we find that National and International Breeding Grounds overlap only partially. China and the US can be characterised as dominant National Breeding Grounds. Australia and selected European countries, but primarily the US, are major International Breeding Grounds. We conclude that China promotes AI development with a major focus on IP protection in its domestic market, whereas the US sustains its AI progress in the international context as well. This might indicate a considerable bifurcation in the structural patterns of IP protection in global AI development.",autonomous vehicle
10.1016/j.neucom.2004.11.025,journal,Neurocomputing,sciencedirect,2005-10-31,sciencedirect,A parallel growing architecture for self-organizing maps with unsupervised learning,https://api.elsevier.com/content/article/pii/S0925231204005247,"
                  Self-organizing maps (SOMs) have become popular for tasks in data visualization, pattern classification or natural language processing and can be seen as one of the major concepts for artificial neural networks of today. Their general idea is to approximate a high dimensional and previously unknown input distribution by a lower-dimensional neural network structure with the goal to model the topology of the input space as close as possible. Classical SOMs read the input values in random but sequential order one by one and thus adjust the network structure over space: the network will be built while reading larger and larger parts of the input. In contrast to this approach, we present a SOM that processes the whole input in parallel and organizes itself over time. The main reason for parallel input processing lies in the fact that knowledge can be used to recognize parts of patterns in the input space that have already been learned. This way, networks can be developed that do not reorganize their structure from scratch every time a new set of input vectors is presented, but rather adjust their internal architecture in accordance with previous mappings. One basic application could be a modeling of the whole-part relationship through layered architectures.
               ",autonomous vehicle
10.1016/j.trc.2019.09.006,journal,Transportation Research Part C: Emerging Technologies,sciencedirect,2019-11-30,sciencedirect,A tailored machine learning approach for urban transport network flow estimation,https://api.elsevier.com/content/article/pii/S0968090X18317650,"
                  This study deals with urban transport network flow estimation based on Cellphone Location (CL) and License Plate Recognition (LPR) data. We first propose two methods to filter CL data and extract the spatio-temporal traffic features for a specific road segment. A tailored machine learning approach is developed, including two components: a tangible multi-grained scanning ensemble learning model and a novel two-stage zero-shot learner. The former aims to estimate traffic flow on a single link with both filtered CL data, extracted spatio-temporal traffic features, and LPR data by incorporating the unique merits thereof. The latter is capable of estimating traffic flow on those links with only CL data by considering the spatial features of these links and relevant land-use information. Finally, case studies are analysed to demonstrate the impressive performance of the tailored machine learning approach.
               ",autonomous vehicle
10.1016/j.aci.2018.01.004,journal,Applied Computing and Informatics,sciencedirect,2019-07-31,sciencedirect,Deep belief networks and cortical algorithms: A comparative study for supervised classification,https://api.elsevier.com/content/article/pii/S2210832717300947,"The failure of shallow neural network architectures in replicating human intelligence led the machine learning community to focus on deep learning, to computationally match human intelligence. The wide availability of increasing computing power coupled with the development of more efficient training algorithms have allowed the implementation of deep learning principles in a manner and span that had not been previously possible. This has led to the inception of deep architectures that capitalize on recent advances in artificial intelligence and insights from cognitive neuroscience to provide better learning solutions. In this paper, we discuss two such algorithms that represent different approaches to deep learning with varied levels of maturity. The more mature but less biologically inspired Deep Belief Network (DBN) and the more biologically grounded Cortical Algorithms (CA) are first introduced to give readers a bird’s eye view of the higher-level concepts that make up these algorithms, as well as some of their technical underpinnings and applications. Their theoretical computational complexity is then derived before comparing their empirical performance on some publicly available classification datasets. Multiple network architectures were compared and showed that CA outperformed DBN on most datasets, with the best network architecture consisting of six hidden layers.",autonomous vehicle
10.1016/j.jocs.2017.09.008,journal,Journal of Computational Science,sciencedirect,2018-01-31,sciencedirect,Learning Automata Clustering,https://api.elsevier.com/content/article/pii/S1877750317302247,"
                  Clustering of data points has been a profound research avenue in the history of machine learning algorithms. Using learning automata which are autonomous decision making entities, in this paper, the learning automata clustering algorithm is proposed. In learning automata clustering, each data point is affiliated with a learning automaton where the learning automaton determines the cluster membership of that data point. The cluster rectification is done through a reinforcement signal for each learning automaton which is fabricated from the Euclidean distance of that data point and the mean value of its designated cluster. Finally, the learning automata clustering is compared with four centroid-based clustering algorithms, K-means, K-means++, K-medians, and K-medoids and results demonstrate the high clustering accuracy and comparable Silhouette coefficient of the proposed method.
               ",autonomous vehicle
10.1016/j.compeleceng.2018.03.015,journal,Computers & Electrical Engineering,sciencedirect,2019-03-31,sciencedirect,BCI cinematics – A pre-release analyser for movies using H<ce:inf loc=post>2</ce:inf>O deep learning platform,https://api.elsevier.com/content/article/pii/S0045790617315318,"
                  Entertainment industry has seen a phenomenal growth throughout the globe in recent times and movie industry enjoys a crucial role in the above emergence. A movie can capture the attention of a viewer and can trigger cognitive and emotional processes in the brain. In this article we assess the emotional outcome of the viewer while they watch the movie before its actual release that is, during its preview. Traditionally FMRI was used to assess the activity of brain but proved to be non-feasible and costly so we used EEG Sensors to monitor and record the functioning of the brain of movie viewer for further analysis. The collected data through EEG sensor were analysed using deep learning framework. H2O package of deep learning was employed to find high and low of different brain waves mapping to the emotions depicted in the every scene of the movie. Our proposed system named BCI cinematics obtained 85% accuracy and results were validated by obtaining the feedback from the stake holders. The outcome of this work will assist the creators to understand the emotional impact of movie over a normal viewer impartially thus enable them to modify certain scenes or change sequence of scenes and so on. When deployed in real time our system prove to be a cost saver for movie makers.
               ",autonomous vehicle
10.1016/B978-0-12-817133-2.00005-7,journal,Artificial Intelligence in Precision Health,sciencedirect,2020-12-31,sciencedirect,"Chapter 5: Machine learning in digital health, recent trends, and ongoing challenges",https://api.elsevier.com/content/article/pii/B9780128171332000057,"
               As a result of a growing and aging population, as well as an increase in associated costs, there is a continual stretching of health care services worldwide. This issue is motivating researchers all over the world to optimize medical resources by utilizing digital tools explicitly addressed to health care and well-being. One of the main fields of research in this regard is artificial intelligence (AI), the endowment of machines with human-like learning, reasoning, and decision-making abilities. Combined with high penetration of sensor-based technologies—such as smartphones and wearables—in modern society, advancements in AI mean we are entering a new age of health care. Soon, we will be able to monitor vital signs and lifestyle habits, in real-time, in such a way that will help clinicians to monitor patients’ evolution and progress in a nonintrusive and remote manner. This chapter intended to be an introductory, higher-level overview, of the core concepts relating to the branch of AI known as machine learning (ML). Readers are introduced to the ML train-test pipeline and given an overview of commonly used ML algorithms. The chapter finishes by discussing challenges that need to be overcome to help fully realize the potential of ML in everyday digital health settings.
            ",autonomous vehicle
10.1016/j.ins.2020.10.009,journal,Information Sciences,sciencedirect,2021-05-31,sciencedirect,A strong coreset algorithm to accelerate OPF as a graph-based machine learning in large-scale problems,https://api.elsevier.com/content/article/pii/S0020025520309968,"
                  Optimum-path forest (OPF) is one of the efficient graph-based frameworks that can determine the patterns of input dataset by extracting the optimal partitions of graph obtained through encoding data into a graph. Since OPF was introduced based on simple assumptions without considering the requirements of large-scale problems, this machine learning is an effective algorithm only for a reasonable size of input datasets. To provide a scalable OPF, this study introduces a strong coreset for accelerating OPF algorithm. Applying this approach can expedite OPF procedure, especially when it is working on massive datasets. Accordingly, a novel algebra is developed to represent the problem of OPF as an optimization problem for the proposed coreset definition. A novel coreset construction algorithm that can approximate the OPF solutions is subsequently proposed in order to improve the OPF construction speed. The simulation results of diverse experiments on various benchmark datasets illustrate computation gain and superiority of the proposed algorithm in terms of the construction and classification speeds as compared to the original algorithm while displaying reliably accurate performance. The presented coreset construction algorithm performs the training and testing phases of OPF up to 6.1 and 4.9 times faster than before, respectively.
               ",autonomous vehicle
10.1016/j.procs.2014.11.111,journal,Procedia Computer Science,sciencedirect,2014-12-31,sciencedirect,"Modeling Mechanisms of Cognition-emotion Interaction in Artificial Neural Networks, since 1981",https://api.elsevier.com/content/article/pii/S1877050914015567,"The paper describes modeling of cognition-emotion interaction implemented in a neural network named Crossbar Adaptive Array in 1981. The architecture was proposed to meet two challenges: solving the delayed reinforcement learning problem for neural networks, and building a self-learning system (no advice and no reinforcement from environment) based on a neural network. The architecture introduced computation of feelings, and their interaction with learning and decision making mechanisms. It also introduced genetic environment as provider of initial emotions to the neural network. Receiving initial emotions from the genetic environment, the architecture learns using the proposed emotion backpropagation mechanism. Some developments after 1981 are also discussed.",autonomous vehicle
10.1016/j.conb.2015.07.006,journal,Current Opinion in Neurobiology,sciencedirect,2015-12-31,sciencedirect,Learning with hidden variables,https://api.elsevier.com/content/article/pii/S0959438815001245,"
                  Learning and inferring features that generate sensory input is a task continuously performed by cortex. In recent years, novel algorithms and learning rules have been proposed that allow neural network models to learn such features from natural images, written text, audio signals, etc. These networks usually involve deep architectures with many layers of hidden neurons. Here we review recent advancements in this area emphasizing, amongst other things, the processing of dynamical inputs by networks with hidden nodes and the role of single neuron models. These points and the questions they arise can provide conceptual advancements in understanding of learning in the cortex and the relationship between machine learning approaches to learning with hidden nodes and those in cortical circuits.
               ",autonomous vehicle
10.1016/0167-8191(90)90089-R,journal,Parallel Computing,sciencedirect,1990-08-31,sciencedirect,"Neural networks for learning in the real world: representation, reinforcement and dynamics",https://api.elsevier.com/content/article/pii/016781919090089R,"
                  It is argued that the backpropagation learning algorithm is unsuited to tackling real world problems such as sensory-motor coordination learning or the encoding of large amounts of background knowledge in neural networks. One difficulty in the real world - the unavailability of ‘teachers’ who already know the solution to problems, may be overcome by the use of reinforcement learning algorithms in place of backpropagation. It is suggested that the complexity of search space in real world neural network learning problems may be reduced if learning is divided into two components. One component is concerned with abstracting structure from the environment and hence with developing representations of stimuli. The other component involves associating and refining these representations on the basis of feedback from the environment. Time-dependent learning problems are also considered in this hybrid framework. Finally, an ‘open systems’ approach in which subsets of a network may adapt independently on the basis of spatio-temporal patterns is briefly discussed.
               ",autonomous vehicle
10.1016/j.patter.2020.100050,journal,Patterns,sciencedirect,2020-07-10,sciencedirect,When Autonomous Systems Meet Accuracy and Transferability through AI: A Survey,https://api.elsevier.com/content/article/pii/S2666389920300611,"With widespread applications of artificial intelligence (AI), the capabilities of the perception, understanding, decision-making, and control for autonomous systems have improved significantly in recent years. When autonomous systems consider the performance of accuracy and transferability, several AI methods, such as adversarial learning, reinforcement learning (RL), and meta-learning, show their powerful performance. Here, we review the learning-based approaches in autonomous systems from the perspectives of accuracy and transferability. Accuracy means that a well-trained model shows good results during the testing phase, in which the testing set shares a same task or a data distribution with the training set. Transferability means that when a well-trained model is transferred to other testing domains, the accuracy is still good. Firstly, we introduce some basic concepts of transfer learning and then present some preliminaries of adversarial learning, RL, and meta-learning. Secondly, we focus on reviewing the accuracy or transferability or both of these approaches to show the advantages of adversarial learning, such as generative adversarial networks, in typical computer vision tasks in autonomous systems, including image style transfer, image super-resolution, image deblurring/dehazing/rain removal, semantic segmentation, depth estimation, pedestrian detection, and person re-identification. We furthermore review the performance of RL and meta-learning from the aspects of accuracy or transferability or both of them in autonomous systems, involving pedestrian tracking, robot navigation, and robotic manipulation. Finally, we discuss several challenges and future topics for the use of adversarial learning, RL, and meta-learning in autonomous systems.",autonomous vehicle
10.1016/B978-0-12-819314-3.00010-0,journal,Data Analytics in Biomedical Engineering and Healthcare,sciencedirect,2021-12-31,sciencedirect,9: Disease diagnosis using machine learning: A comparative study,https://api.elsevier.com/content/article/pii/B9780128193143000100,"
               With time new diseases are coming up, there are diseases where the symptoms are considered to be harmless in 90% of the cases like cold, sneezing, and fever. People usually avoid going to the doctor for such symptoms and prefer taking some normal medicines like paracetamol. Such medicines won’t cure the diseases, and if not treated in time, it can be dangerous for the patient. To help provide people with the diagnoses of the disease, this chapter presents a comparative study of disease diagnosis that would require the sufferers to input the basic symptoms they are facing and would inform them about the disease they may be suffering from along with the probability of other diseases. The evaluation of the algorithm developed will help in the diagnosis of the diseases so that proper care can be taken. The developed model contains symptoms that would be displayed on the screen in groups of 10 starting from mere mild symptoms to serious ones, the user will have to select the appropriate symptoms, and the diagnosis prototype analyzes various combination of the symptoms occurring together through the machine learning model and displays the most probable disease diagnosed. The prototype proposed uses decision tree classification and deep neural networks to classify symptoms of diseases. The classification makes it easier for the user to identify the disease he/she is suffering from. It further compares the results of both techniques.
            ",autonomous vehicle
10.1016/j.cosrev.2019.100199,journal,Computer Science Review,sciencedirect,2019-11-30,sciencedirect,A taxonomy and survey of attacks against machine learning,https://api.elsevier.com/content/article/pii/S1574013718303289,"
                  The majority of machine learning methodologies operate with the assumption that their environment is benign. However, this assumption does not always hold, as it is often advantageous to adversaries to maliciously modify the training (poisoning attacks) or test data (evasion attacks). Such attacks can be catastrophic given the growth and the penetration of machine learning applications in society. Therefore, there is a need to secure machine learning enabling the safe adoption of it in adversarial cases, such as spam filtering, malware detection, and biometric recognition. This paper presents a taxonomy and survey of attacks against systems that use machine learning. It organizes the body of knowledge in adversarial machine learning so as to identify the aspects where researchers from different fields can contribute to. The taxonomy identifies attacks which share key characteristics and as such can potentially be addressed by the same defence approaches. Thus, the proposed taxonomy makes it easier to understand the existing attack landscape towards developing defence mechanisms, which are not investigated in this survey. The taxonomy is also leveraged to identify open problems that can lead to new research areas within the field of adversarial machine learning.
               ",autonomous vehicle
10.1016/B978-0-12-824536-1.00027-7,journal,Data Science for COVID-19,sciencedirect,2021-12-31,sciencedirect,20: Forecast and prediction of COVID-19 using machine learning,https://api.elsevier.com/content/article/pii/B9780128245361000277,"
               COVID-19 outbreaks only affect the lives of people, they result in a negative impact on the economy of the country. On Jan. 30, 2020, it was declared as a health emergency for the entire globe by the World Health Organization (WHO). By Apr. 28, 2020, more than 3 million people were infected by this virus and there was no vaccine to prevent. The WHO released certain guidelines for safety, but they were only precautionary measures. The use of information technology with a focus on fields such as data Science and machine learning can help in the fight against this pandemic. It is important to have early warning methods through which one can forecast how much the disease will affect society, on the basis of which the government can take necessary actions without affecting its economy. In this chapter, we include methods for forecasting future cases based on existing data. Machine learning approaches are used and two solutions, one for predicting the chance of being infected and other for forecasting the number of positive cases, are discussed. A trial was done for different algorithms, and the algorithm that gave results with the best accuracy are covered in the chapter. The chapter discusses autoregressive integrated moving average time series for forecasting confirmed cases for various states of India. Two classifiers, random forest and extra tree classifiers, were selected; both have an accuracy of more than 90%. Of the two, the extra tree classifier has 93.62% accuracy. These results can be used to take corrective measures by different governmental bodies. The availability of techniques for forecasting infectious disease can make it easier to fight COVID-19.
            ",autonomous vehicle
10.1016/j.cogsys.2017.03.005,journal,Cognitive Systems Research,sciencedirect,2017-08-31,sciencedirect,Re-framing the characteristics of concepts and their relation to learning and cognition in artificial agents,https://api.elsevier.com/content/article/pii/S1389041717300402,"
                  In this work, the problems of knowledge acquisition and information processing are explored in relation to the definitions of concepts and conceptual processing, and their implications for artificial agents.
                  The discussion focuses on views of cognition as a dynamic property in which the world is actively represented in grounded mental states which only have meaning in the action context. Reasoning is understood as an emerging property consequence of actions-environment couplings achieved through experience, and concepts as situated and dynamic phenomena enabling behaviours.
                  Re-framing the characteristics of concepts is considered crucial to overcoming settled beliefs and reinterpreting new understandings in artificial systems.
                  The first part presents a review of concepts from cognitive sciences. Support is found for views on grounded and embodied cognition, describing concepts as dynamic, flexible, context-dependent, and distributedly coded.
                  That is argued to contrast with many technical implementations assuming concepts as categories, whilst explains limitations when grounding amodal symbols, or in unifying learning, perception and reasoning.
                  The characteristics of concepts are linked to methods of active inference, self-organization, and deep learning to address challenges posed and to reinterpret emerging techniques.
                  In a second part, an architecture based on deep generative models is presented to illustrate arguments elaborated. It is evaluated in a navigation task, showing that sufficient representations are created regarding situated behaviours with no semantics imposed on data. Moreover, adequate behaviours are achieved through a dynamic integration of perception and action in a single representational domain and process.
               ",autonomous vehicle
10.1016/S0952-1976(01)00031-8,journal,Engineering Applications of Artificial Intelligence,sciencedirect,2001-10-31,sciencedirect,Reinforcement learning control of nonlinear multi-link system,https://api.elsevier.com/content/article/pii/S0952197601000318,"
                  In this paper, the effects of basic parameters in reinforcement learning control such as eligibility, action and critic network constrained weights, system nonlinearities, gradient information, state-space partitioning, variance of exploration are studied in detail. It is attempted to increase feasibility for practical applications, implementation, learning efficiency, and enhance performance. Also, a novel adaptive grid algorithm is proposed to overcome the difficulty in partitioning the input space to achieve better performance. Reinforcement learning is applied for control of a nonlinear one and two-link robots. This problem dictates that the learning is performed on-line, based on a binary or real-valued reinforcement signal from a critic network, without knowing the system model or nonlinearity.
               ",autonomous vehicle
10.1016/bs.host.2020.01.002,journal,Handbook of Statistics,sciencedirect,2020-12-31,sciencedirect,"Chapter 3: Machine learning algorithms, applications, and practices in data science",https://api.elsevier.com/content/article/pii/S0169716120300225,"
                  Data science is an umbrella term used for referring to concepts and practices of subset of the topics under artificial intelligence (AI) methodologies. AI is actually a framework to define notion of intelligence in software systems or devices in terms of knowledge representation and reasoning methodologies. There are two main types of reasoning methods deductive and inductive over data. The major class of machine learning and deep learning methods come under inductive reasoning where essentially, missing pieces of information are interpolated based on existing data through numerical transformations. However, today AI is mostly identified with deduction systems while it is actually a comprehensive school of thought and formal framework. The AI framework offers rigor and robustness to the solutions developed and there is still scope for onboarding today's deep learning solutions and reap benefits of sturdiness. Data science is about end to end development of a smart solution that involves creation of pipelines for activities for data generation, business decision making and solution maintenance with humans in loop. Data generation is a cycle of activities involving collection, refinement, feature transformations, devising more insightful heuristic measures based on domain peculiarities and iterations to enhance quality of data driven decisions. Business decision making is pipeline of activities involving designing mappers from data to business decisions. The mappers are typically machine learning methods which are fine tuned to give best possible performance in a given period of study subject to business constraints. The mappers are fine tuned based on quality and magnitude of data and subdata. Solution maintenance is a critical component that involves setting up alarms to detect when a given decision maker model no longer works as desired. The maintenance work calls for repair actions such as identifying data to gather, comparative metrics of different models and monitoring the patterns and trends in the input data.
               ",autonomous vehicle
10.1016/j.bushor.2018.08.004,journal,Business Horizons,sciencedirect,2019-02-28,sciencedirect,"Siri, Siri, in my hand: Who’s the fairest in the land? On the interpretations, illustrations, and implications of artificial intelligence",https://api.elsevier.com/content/article/pii/S0007681318301393,"
                  Artificial intelligence (AI)—defined as a system’s ability to correctly interpret external data, to learn from such data, and to use those learnings to achieve specific goals and tasks through flexible adaptation—is a topic in nearly every boardroom and at many dinner tables. Yet, despite this prominence, AI is still a surprisingly fuzzy concept and a lot of questions surrounding it are still open. In this article, we analyze how AI is different from related concepts, such as the Internet of Things and big data, and suggest that AI is not one monolithic term but instead needs to be seen in a more nuanced way. This can either be achieved by looking at AI through the lens of evolutionary stages (artificial narrow intelligence, artificial general intelligence, and artificial super intelligence) or by focusing on different types of AI systems (analytical AI, human-inspired AI, and humanized AI). Based on this classification, we show the potential and risk of AI using a series of case studies regarding universities, corporations, and governments. Finally, we present a framework that helps organizations think about the internal and external implications of AI, which we label the Three C Model of Confidence, Change, and Control.
               ",autonomous vehicle
10.1016/j.eswa.2021.116209,journal,Expert Systems with Applications,sciencedirect,2021-11-14,sciencedirect,Semantic and explainable research-related recommendation system based on semi-supervised methodology using BERT and LDA models,https://api.elsevier.com/content/article/pii/S0957417421015232,"
                  With the launch of academic search engines such as Google Scholar, Microsoft Academic, and Scopus, researchers are using them to access scholarly materials on a large scale without making any payment. In this optimal research environment, academic libraries or community databases have naturally experienced a rapid increase in volume; however, the excessive quantitative growth of information, or information overload, acts as a double-edged sword that prevents researchers from finding relevant prior studies or researchers with similar interests. Existing keyword and rule-based search systems carry the risk of recommending research literature with a high citation frequency rather than recommending contextually similar documents. Therefore, this study proposes a semi-supervised semantic-based research literature and researcher recommendation system using LDA and BERT. Since a semi-supervised method is used, research literature can be embedded based on contextual and classification information, and the global topic information of the research literature can also be captured. In addition, a research literature information extractor system has been implemented, which comprises classification network of our model and an explainable keywords extractor system implemented using BERT's self-attention structure. Based on the experimental results, it can be confirmed that the proposed study model shows better performance than other baselines. This model can help users to find information and contextual matching data quickly and accurately from the database. In addition, when implementing an academic library and community database, such a powerful research-related recommendation system and research literature information extractor are expected to have an enormous effect.
               ",autonomous vehicle
10.1016/j.procir.2013.05.059,journal,Procedia CIRP,sciencedirect,2013-12-31,sciencedirect,Learning Defect Classifiers for Textured Surfaces Using Neural Networks and Statistical Feature Representations,https://api.elsevier.com/content/article/pii/S2212827113002667,"Detecting surface defects is a challenging visual recognition problem arising in many processing steps during manufacturing. These defects occur with arbitrary size, shape and orientation. The challenges posed by this complexity have been combated with very special, runtime intensive and hand-designed feature representations. In this paper we present a machine vision system which uses basic patch statistics from raw image data combined with a two layer neural network to detect surface defects on arbitrary textured and weakly labeled image data. Evaluation on an artificial dataset with more than 6000 examples in addition to a real micro-cold forming process showed excellent classification results.",autonomous vehicle
10.1016/j.eswa.2020.113272,journal,Expert Systems with Applications,sciencedirect,2020-07-15,sciencedirect,Shop floor simulation optimization using machine learning to improve parallel metaheuristics,https://api.elsevier.com/content/article/pii/S095741742030097X,"
                  Simulation optimization is a tool commonly used as a decision-making support system on industrial problems in order to find the best resource allocation, which has a direct influence on costs and revenues. The present study proposed an open-source framework developed on Python, integrating different strategies for a novel optimization algorithm. The framework includes multicore parallelism (tested on two different types of computer sets), (two) population-based metaheuristics, and 33 machine learning methods. Moreover, the study tested the framework to optimize resource allocation on a theoretical shop floor case study, evaluating 12 optimization scenarios. The use of metaheuristic with parallelism reduced 88.3% the processing time compared with the serial metaheuristic, while the integration of metaheuristic with the selected machine learning generated an additional reduction of 59.0% on the necessary processing time. The combination of the optimization methods created a solution of 95.3% near the global optimum and time reduction of 95.2%.
               ",autonomous vehicle
10.1016/j.neucom.2018.05.005,journal,Neurocomputing,sciencedirect,2018-10-02,sciencedirect,Speech emotion recognition based on an improved brain emotion learning model,https://api.elsevier.com/content/article/pii/S0925231218305344,"
                  Human-robot emotional interaction has developed rapidly in recent years, in which speech emotion recognition plays a significant role. In this paper, a speech emotion recognition method based on an improved brain emotional learning (BEL) model is proposed, which is inspired by the emotional processing mechanism of the limbic system in the brain. The reinforcement learning rule of BEL model, however, makes it have poor adaptation and affects its performance. To solve these problems, Genetic Algorithm (GA) is employed to update the weights of BEL model. The proposal is tested on the CASIA Chinese emotion corpus, SAVEE emotion corpus, and FAU Aibo dataset, in which MFCC related features and their 1st order delta coefficients are extracted. In addition, the proposal is tested on INTERSPEECH 2009 standard feature set, in which three dimensionality reduction methods of Linear Discriminant Analysis (LDA), Principal Component Analysis (PCA), and PCA+LDA are used to reduce the dimension of feature set. The experimental results show that the proposed method obtains average recognition accuracy of 90.28% (CASIA), 76.40% (SAVEE), and 71.05% (FAU Aibo) for speaker-dependent (SD) speech emotion recognition and the highest average accuracy of 38.55% (CASIA), 44.18% (SAVEE), 64.60% (FAU Aibo) for speaker-independent (SI) speech emotion recognition are obtained, which shows that the proposal is feasible in speech emotion recognition.
               ",autonomous vehicle
10.1016/j.kint.2021.01.015,journal,Kidney International,sciencedirect,2021-06-30,sciencedirect,AI applications in renal pathology,https://api.elsevier.com/content/article/pii/S0085253821001812,"
                  The explosive growth of artificial intelligence (AI) technologies, especially deep learning methods, has been translated at revolutionary speed to efforts in AI-assisted healthcare. New applications of AI to renal pathology have recently become available, driven by the successful AI deployments in digital pathology. However, synergetic developments of renal pathology and AI require close interdisciplinary collaborations between computer scientists and renal pathologists. Computer scientists should understand that not every AI innovation is translatable to renal pathology, while renal pathologists should capture high-level principles of the relevant AI technologies. Herein, we provide an integrated review on current and possible future applications in AI-assisted renal pathology, by including perspectives from computer scientists and renal pathologists. First, the standard stages, from data collection to analysis, in full-stack AI-assisted renal pathology studies are reviewed. Second, representative renal pathology-optimized AI techniques are introduced. Last, we review current clinical AI applications, as well as promising future applications with the recent advances in AI.
               ",autonomous vehicle
10.1016/j.trsl.2017.10.010,journal,Translational Research,sciencedirect,2018-04-30,sciencedirect,Digital image analysis in breast pathology—from image processing techniques to artificial intelligence,https://api.elsevier.com/content/article/pii/S1931524417302955,"
                  Breast cancer is the most common malignant disease in women worldwide. In recent decades, earlier diagnosis and better adjuvant therapy have substantially improved patient outcome. Diagnosis by histopathology has proven to be instrumental to guide breast cancer treatment, but new challenges have emerged as our increasing understanding of cancer over the years has revealed its complex nature. As patient demand for personalized breast cancer therapy grows, we face an urgent need for more precise biomarker assessment and more accurate histopathologic breast cancer diagnosis to make better therapy decisions. The digitization of pathology data has opened the door to faster, more reproducible, and more precise diagnoses through computerized image analysis. Software to assist diagnostic breast pathology through image processing techniques have been around for years. But recent breakthroughs in artificial intelligence (AI) promise to fundamentally change the way we detect and treat breast cancer in the near future. Machine learning, a subfield of AI that applies statistical methods to learn from data, has seen an explosion of interest in recent years because of its ability to recognize patterns in data with less need for human instruction. One technique in particular, known as deep learning, has produced groundbreaking results in many important problems including image classification and speech recognition. In this review, we will cover the use of AI and deep learning in diagnostic breast pathology, and other recent developments in digital image analysis.
               ",autonomous vehicle
10.1016/j.opelre.2019.02.003,journal,Opto-Electronics Review,sciencedirect,2019-03-31,sciencedirect,A deep learning ball tracking system in soccer videos,https://api.elsevier.com/content/article/pii/S123034021830146X,"
                  Increasing interest, enthusiasm of sport lovers, and economics involved offer high importance to sports video recording and analysis. Being crucial for decision making, ball detection and tracking in soccer has become a challenging research area. This paper presents a novel deep learning approach for 2D ball detection and tracking (DLBT) in soccer videos posing various challenges. A new 2-stage buffer median filtering background modelling is used for moving objects blob detection. A deep learning approach for classification of an image patch into three classes, i.e. ball, player, and background is initially proposed. Probabilistic bounding box overlapping technique is proposed further for robust ball track validation. Novel full and boundary grid concepts resume tracking in ball_track_lost and ball_out_of_frame situations. DLBT does not require human intervention to identify ball from the initial frames unlike the most published algorithms. DLBT yields extraordinary accurate and robust tracking results compared to the other contemporary 2D trackers even in presence of various challenges including very small ball size and fast movements.
               ",autonomous vehicle
10.1016/S0893-6080(99)00064-7,journal,Neural Networks,sciencedirect,1999-12-31,sciencedirect,A self-supervised learning system for pattern recognition by sensory integration,https://api.elsevier.com/content/article/pii/S0893608099000647,"
                  Artificial neural networks are useful tools for pattern recognition because they realize nonlinear mapping between input and output spaces. This ability is tuned by supervised learning methods such as back-propagation. In the supervised learning methods, desired outputs of the neural network are needed. However, the desired outputs are usually unknown in unpredictable environments. To solve this problem, this paper presents a self-supervised learning system for category detection. This system learns categories of objects automatically by integrating information from several sensors. We assume that these sensory inputs are always ambiguous patterns that include some noises according to deformations of the objects. After the learning, the system recognizes objects, also controlling the priority of each sensor, according to the deformation of the sensory input pattern.
                  In the simulation, the system is applied to several learning and recognition tasks using artificial or actual sensory inputs. In all tasks, the system found the categories. Particularly, we applied the new system to the learning of five Japanese vowels with the corresponding shapes of the mouth. As result, the system became to yield specific outputs corresponding to each vowel.
               ",autonomous vehicle
10.1016/j.eswa.2021.115584,journal,Expert Systems with Applications,sciencedirect,2021-12-15,sciencedirect,Data-driven modeling of technology acceptance: A machine learning perspective,https://api.elsevier.com/content/article/pii/S0957417421009866,"
                  Understanding, explaining, and predicting technology acceptance have dominated the research of information systems (IS) for more than two decades. Past research has favored explanatory modeling, considering it a prediction-oriented approach; until recently, predictive analytics has been poorly understood and widely underappreciated. Research on IS for prediction-oriented modeling like predictive analytics remains rare, despite its potential for development and utility. Our research addresses the capacity of predictive analytics for advancing technology acceptance modeling by assessing predictive power, evaluating the current frameworks, and introducing new constructs.
                  This research formulates a unique data-driven approach that utilizes machine learning (ML) and predictive analytics-based modeling to empirically predict end users’ acceptance of consumer-use technology in a non-organizational setting using the following steps. First, a thorough analysis of IS literature was conducted to explore the constructs of technology use in various contexts. Second, the Twitter API and interviews were utilized to extract new constructs and evaluate the content of current models of technology acceptance. Third, a unique technology acceptance model of thirty-seven constructs was developed and tested on thirty-two personal technologies with heterogeneous subjects. Fourth, ML algorithms estimated the predictive power of the model and ranked the influence of its variables, achieving an 
                        
                           R
                        
                     
                     2 of 0.97 and an error rate of 0.04. Thirteen new constructs were successfully introduced, including eight technology characteristics. Four other constructs were reinstated, presenting the utility of the ML approach to contribute to research. Ranking the thirty-seven constructs by applying a sensitivity analysis on the basis of partial derivatives showed the differences between the predictive model and the explanatory model of personal technology acceptance. The proposed approach demonstrates the capacity of ML to formulate a complex model of personal technology acceptance, which further develops technology acceptance models.
               ",autonomous vehicle
10.1016/j.matcom.2008.01.028,journal,Mathematics and Computers in Simulation,sciencedirect,2008-07-31,sciencedirect,Artificial Intelligence techniques: An introduction to their use for modelling environmental systems,https://api.elsevier.com/content/article/pii/S0378475408000505,"
                  Knowledge-based or Artificial Intelligence techniques are used increasingly as alternatives to more classical techniques to model environmental systems. We review some of them and their environmental applicability, with examples and a reference list. The techniques covered are case-based reasoning, rule-based systems, artificial neural networks, fuzzy models, genetic algorithms, cellular automata, multi-agent systems, swarm intelligence, reinforcement learning and hybrid systems.
               ",autonomous vehicle
10.1016/j.ijinfomgt.2020.102104,journal,International Journal of Information Management,sciencedirect,2020-08-31,sciencedirect,"Artificial intelligence for sustainability: Challenges, opportunities, and a research agenda",https://api.elsevier.com/content/article/pii/S0268401220300967,"
                  Artificial intelligence (AI) will transform business practices and industries and has the potential to address major societal problems, including sustainability. Degradation of the natural environment and the climate crisis are exceedingly complex phenomena requiring the most advanced and innovative solutions. Aiming to spur groundbreaking research and practical solutions of AI for environmental sustainability, we argue that AI can support the derivation of culturally appropriate organizational processes and individual practices to reduce the natural resource and energy intensity of human activities. The true value of AI will not be in how it enables society to reduce its energy, water, and land use intensities, but rather, at a higher level, how it facilitates and fosters environmental governance. A comprehensive review of the literature indicates that research regarding AI for sustainability is challenged by (1) overreliance on historical data in machine learning models, (2) uncertain human behavioral responses to AI-based interventions, (3) increased cybersecurity risks, (4) adverse impacts of AI applications, and (5) difficulties in measuring effects of intervention strategies. The review indicates that future studies of AI for sustainability should incorporate (1) multilevel views, (2) systems dynamics approaches, (3) design thinking, (4) psychological and sociological considerations, and (5) economic value considerations to show how AI can deliver immediate solutions without introducing long-term threats to environmental sustainability.
               ",autonomous vehicle
10.1016/j.medine.2019.06.012,journal,Medicina Intensiva (English Edition),sciencedirect,2019-10-31,sciencedirect,Big Data Analysis and Machine Learning in Intensive Care Units,https://api.elsevier.com/content/article/pii/S2173572719301420,"
                  Intensive care is an ideal environment for the use of Big Data Analysis (BDA) and Machine Learning (ML), due to the huge amount of information processed and stored in electronic format in relation to such care. These tools can improve our clinical research capabilities and clinical decision making in the future.
                  The present study reviews the foundations of BDA and ML, and explores possible applications in our field from a clinical viewpoint. We also suggest potential strategies to optimize these new technologies and describe a new kind of hybrid healthcare-data science professional with a linking role between clinicians and data.
               ",autonomous vehicle
10.1016/j.matpr.2021.02.167,journal,Materials Today: Proceedings,sciencedirect,2021-03-10,sciencedirect,Detection of critical diagnostic faults in automobiles using Convolutional Neural network architecture,https://api.elsevier.com/content/article/pii/S2214785321012499,"
                  Predictive Maintenance is an important solution to the rising maintenance costs in the industries. With the advent of intelligent computer and availability of data, predictive maintenance is seen as a solution to predict and prevent the occurrence of the faults in the different types of machines. This work provides a detailed methodology to predict the occurrence of critical Diagnostic Trouble codes that are observed in a vehicle in order to take necessary maintenance actions before occurrence of the fault in automobiles using Convolutional Neural Network architecture.
               ",autonomous vehicle
10.1016/B978-0-323-67538-3.00003-8,journal,Artificial Intelligence and Deep Learning in Pathology,sciencedirect,2021-12-31,sciencedirect,Chapter 3: Overview of advanced neural network architectures,https://api.elsevier.com/content/article/pii/B9780323675383000038,"
               Though initially outclassed by more computationally efficient machine learning algorithms, neural network–based deep learning strategies have come of age with modern implementations and hardware and are now capable of addressing a range of previously intractable problems. These include methods for dealing with complex spatial or sequential data, strategies for coping with sets of data in which not all of the examples are annotated, and ways of generalizing across different databases. Other methods take inspiration from biological strategies such as operant conditioning or evolutionary principles. These and other techniques described here form the basis for many of the advances in the use of artificial intelligence in the service of pathology, as well as medicine in general.
            ",autonomous vehicle
10.1016/j.eneco.2021.105494,journal,Energy Economics,sciencedirect,2021-10-31,sciencedirect,Machine learning and oil price point and density forecasting,https://api.elsevier.com/content/article/pii/S0140988321003807,"
                  The purpose of this paper is to explore machine learning techniques to forecast the oil price. In the era of big data, we investigate whether new automated tools can improve over traditional approaches in terms of forecast accuracy. Oil price point and density forecasts are built from 23 methods, including regression trees (random forest, quantile regression forest, xgboost), regularization procedures (elastic net, lasso, ridge), standard econometric models and forecast combinations, besides the structural factor model of Schwartz and Smith (2000). The database contains 315 macroeconomic and financial variables, used to build high-dimensional models. To evaluate the predictive power of each method, an extensive pseudo out-of-sample forecasting exercise is built, in monthly and quarterly frequencies, with horizons from one month up to five years. Overall, the results indicate a good performance of the machine learning methods in the short-run. Up to six months, lasso-based models, oil future prices, VECM and the Schwartz–Smith model provide the best forecasts. At longer horizons, forecast combinations also become relevant. In several cases, the accuracy gains in respect to the random walk forecast are statistically significant and reach two-digit figures, in percentage terms, using the 
                        
                           
                              R
                           
                           
                              2
                           
                        
                      out-of-sample statistic; an expressive achievement compared to the previous literature.
               ",autonomous vehicle
10.1016/j.matpr.2020.03.774,journal,Materials Today: Proceedings,sciencedirect,2020-12-31,sciencedirect,Investigations on System Modeling Simulations for Solving Heterogeneous WSN Task Assignment Problem using Multilayer Feed Forward Neural Networks,https://api.elsevier.com/content/article/pii/S2214785320325566,"
                  In the modern smart era, various real-time applications are achieved through Heterogeneous Wireless Sensor Networks (HeWSN) comprising a set of multi-functional wireless sensor nodes. In comparison to legacy nodes, the contemporary nodes are smaller and faster with multi-sensing capabilities. The real-time application missions are achieved by effectively assigning tasks to the ideal nodes in the HeWSN by employing a task assignment algorithm. Though various WSN tasks assignment algorithms have been proposed from time to time, it requires maximal human intervention to achieve the application missions. To mitigate human intervention, the task assignment in WSN could be powered by machine learning techniques. Despite the technological advancements, WSN is still assumed to be resource-constrained and employing a powerful machine learning technique may not be ideal. Investigations of the existing machine learning techniques catering to the needs of the WSN have been carried out and have led to the selection of Multilayer Feed Forward Network (MLFFN). Further, a HeWSN application is modelled and simulated to train the system. A total of 39,000 individual experimentations was performed by varying the hidden layers and training sets. Performance analysis was performed on the successful task assignment ratio. Investigations performed are (i) finding the highest successful task assignment ratio, (ii) configuration of the hidden layer and training set ranges in which the maximum successful task assignment ratio is obtained. In a nutshell, in-depth discussions to adapt MLFFN in a HeWSN application has been presented along with the future scope of the work
               ",autonomous vehicle
10.1016/j.optlastec.2020.106721,journal,Optics & Laser Technology,sciencedirect,2021-03-31,sciencedirect,A review on applications of artificial intelligence in modeling and optimization of laser beam machining,https://api.elsevier.com/content/article/pii/S0030399220313542,"
                  Laser beam machining (LBM) as an efficient tool for material removal has attracted the attention of manufacturing industries. Accordingly, there is a great motivation in the modeling and optimization of this non-conventional machining process. In this paper, the focus is on the most common LBM process, including cutting, grooving, turning, milling, and drilling. The development of an accurate model between the input and output variables of the LBM process is difficult and complex due to the non-linear behavior of the process under various conditions. In the case of LBM, the input variables are system, material, and process parameters, and the output variables are the quality characteristics of laser machined workpiece, including geometry characteristics, metallurgical characteristics, surface roughness, and material removal rate (MRR). Recently, among computational methods, artificial intelligence (AI) has been studied by scientists as a pioneer in the field of modeling and optimizing quality features of LBM. AI techniques utilize the empirical findings and existing knowledge for modeling, optimization, monitoring, and controlling of the LBM process. In this paper, the applications of AI techniques, including artificial neural network (ANN), fuzzy logic (FL), metaheuristic optimization algorithms, and hybrid approaches in modeling and optimization of the quality characteristics of LBM are reviewed. It is shown that AI techniques are successfully capable of predicting and improving the features of the laser machined workpiece. It is also demonstrated that AI can be used as a powerful tool to obtain a comprehensive model and optimal setting parameters of LBM. In addition, according to the potential and capability of AI techniques, several ideas have been offered for future studies.
               ",autonomous vehicle
10.1016/j.asoc.2021.107169,journal,Applied Soft Computing,sciencedirect,2021-06-30,sciencedirect,Influence-aware graph neural networks,https://api.elsevier.com/content/article/pii/S1568494621000922,"
                  Network representation learning endeavors to learn low-dimensional dense representations for nodes in a network. With the rapid development of online social platforms, the analysis of social networks has become increasingly significant. Although network representation learning can facilitate the social network analysis, most existing algorithms merely exploit the explicit structure among nodes to obtain the node representations. Besides, traditional network representation learning techniques ignore the influence of nodes in a network when generating the representations of nodes. Motivated by this, we innovatively propose an influence-aware graph neural network (IAGNN) framework, which can learn the latent feature representations of nodes by incorporating both node influence and global structure information into the embedding process for encoding graph-structured data. The generated low-dimensional dense representations of the nodes in a network can be used for subsequent tasks such as user classification and user behavior prediction. Specifically, we assign different weights to each node according to different types of topology between their neighbors, and integrate with the basic influence of each node to generate an intermediate matrix with influence information. The intermediate matrix is encoded into low-dimensional and dense vector spaces by leveraging the attention mechanism and the graph convolution operation. Extensive experiments are conducted on five datasets, and IAGNN achieves an average accuracy of 3% higher than the comparison algorithms on the node classification and link prediction tasks. The experimental results demonstrate that our model can significantly outperform the state-of-the-art network embedding methods such as GCN, GAT, GraphSage, AGNN on node classification and link prediction tasks.
               ",autonomous vehicle
10.1016/j.visinf.2017.01.006,journal,Visual Informatics,sciencedirect,2017-03-31,sciencedirect,Towards better analysis of machine learning models: A visual analytics perspective,https://api.elsevier.com/content/article/pii/S2468502X17300086,"Interactive model analysis, the process of understanding, diagnosing, and refining a machine learning model with the help of interactive visualization, is very important for users to efficiently solve real-world artificial intelligence and data mining problems. Dramatic advances in big data analytics have led to a wide variety of interactive model analysis tasks. In this paper, we present a comprehensive analysis and interpretation of this rapidly developing area. Specifically, we classify the relevant work into three categories: understanding, diagnosis, and refinement. Each category is exemplified by recent influential work. Possible future research opportunities are also explored and discussed.",autonomous vehicle
10.1016/B978-0-12-821777-1.00016-1,journal,"Machine Learning, Big Data, and IoT for Medical Informatics",sciencedirect,2021-12-31,sciencedirect,Chapter 7: Machine learning-enabled Internet of Things for medical informatics,https://api.elsevier.com/content/article/pii/B9780128217771000161,"
               The exponential increase in wireless data traffic is intensifying human-machine interaction. Imperceptible diagnostics, ubiquitous monitoring, and the availability of digital assistive systems are conceptualized as essential milestones in revolutionizing the modes, by which the Internet of Things (IoT) is transforming healthcare applications. This is referred as healthcare IoT (H-IoT) systems. H-IoT is continuously evolving, driven by the advances in the underlying technologies in wireless body area network (WBAN). The machine learning (ML) is considered as a pivotal solution in fulfilling the needs of H-IoT applications and devices. This chapter serves as an introductory guideline to address the challenges and opportunities, while designing ML-enabled H-IoT networks. Section 1 provides a discussion on traditional H-IoT, challenges, and opportunities in the Network 2030 paradigm. Section 2 focuses on the applications of H-IoT. Section 3 provides a detailed comparison of types of ML approaches. Moreover, this section discusses potential ML techniques compatible with H-IoT. Finally, Section 4 points out open issues and future research directions.
            ",autonomous vehicle
10.1016/j.asoc.2021.107082,journal,Applied Soft Computing,sciencedirect,2021-04-30,sciencedirect,Computational Intelligence in the hospitality industry: A systematic literature review and a prospect of challenges,https://api.elsevier.com/content/article/pii/S1568494621000053,"
                  This research work presents a detailed survey about Computational Intelligence (CI) applied to various Hotel and Travel Industry areas. Currently, the hospitality industry’s interest in data science is growing exponentially because of their expected margin of profit growth. In order to provide precise state of the art content, this survey analyzes more than 160 research works from which a detailed categorization and taxonomy have been produced. We have studied the different approaches on the various forecasting methods and subareas where CI is currently being used. This research work also shows an actual distribution of these research efforts in order to enhance the understanding of the reader about this topic and to highlight unexploited research niches. A set of guidelines and recommendations for future research areas and promising applications are also presented in a final section.
               ",autonomous vehicle
10.1016/j.drudis.2020.10.010,journal,Drug Discovery Today,sciencedirect,2021-01-31,sciencedirect,Artificial intelligence in drug discovery and development,https://api.elsevier.com/content/article/pii/S1359644620304256,,autonomous vehicle
10.1016/j.sysarc.2020.101861,journal,Journal of Systems Architecture,sciencedirect,2021-01-31,sciencedirect,A survey on machine learning-based malware detection in executable files,https://api.elsevier.com/content/article/pii/S1383762120301442,"
                  In last decade, a proliferation growth in the development of computer malware has been done. Nowadays, cybercriminals (attacker) use malware as a weapon to carry out the attacks on the computer systems. Internet is the main media to execute the malware attack on the computer systems through emails, malicious websites and by drive and download software. Malicious software can be a virus, trojan horse, worms, rootkits, adware or ransomware. Malware and benign samples are analyzed using static or dynamic analysis techniques. After analysis unique features are extracted to distinguish the malware and benign files. The efficiency of the malware detection system depends on how effectively discriminative malware features are extracted through the analysis techniques. There are various methods to set up the analysis environments using various static and dynamic tools. The second phase is to train the malware classifiers. Earlier traditional methods were used but nowadays machine learning algorithms are used for malware classification which can cope with complexity and pace of malware development. In this paper detailed study of malware detection techniques using machine learning algorithms are presented. In addition, this paper discusses various challenges for developing malware classifiers. At last future directive is discussed to develop an effective malware detection system by handling various issues in malware detection.
               ",autonomous vehicle
10.1016/B978-0-12-814761-0.00010-1,journal,Data Science,sciencedirect,2019-12-31,sciencedirect,Chapter 10: Deep Learning,https://api.elsevier.com/content/article/pii/B9780128147610000101,"
               Deep learning is one of the fastest growing areas of data science. Deep learning refers to a class of algorithms which are based on artificial neural networks optimized to work with unstructured data such as images, voice, videos and text. While the techniques of deep learning date back to the mid-80s, their true potential has been realized only in the last 5 years or so. This chapter covers some of the early history, develops a conceptual framework for introducing deep learning in general. It also describes a practical implementation of deep learning.
            ",autonomous vehicle
10.1016/j.biosystems.2020.104313,journal,Biosystems,sciencedirect,2021-01-31,sciencedirect,Extraction of the molecular level biomedical event trigger based on gene ontology using radial belief neural network techniques,https://api.elsevier.com/content/article/pii/S0303264720301878,"
                  Detection of molecular level biomedical event extraction plays a vital role in creating and visualizing the applications related to natural language processing. Cystic Fibrosis is an inherited genetic and debilitating pathology involving the respiratory and digestive systems. The excessive production of thick sticky mucus on the outside of the cells is the main consequence of such disease. This includes disease prevention and medical search to signify the occurrence and detection of event triggers, which is regarded as a proper step in an event extraction of molecular level in biomedical applications. In this model, use a rich set of extracted features to feed the machine learning classifier that helps in better extraction of events. The study uses an automatic feature selection and a classification model using Radial Belief Neural Network (RBNN) for the optimal detection of molecular biomedical event detection. The Radial Belief Neural Network (RBNN) is the proposed system is implemented and it is the classifier to give accurate result of the disease detection. These three algorithms are used to enhance the generalization performance and scalability of detecting the molecular event triggers. The validation is conducted on the cystic fibrosis event trigger based on the gene ontology bio system using the RBNN model with a lung molecular event-level extraction dataset. The extensive computation shows that the Radial Belief Neural Network (RBNN) is proposed to given the better performance results like Accuracy, Sensitivity, Specificity, F-measure and Execution time.
               ",autonomous vehicle
10.1016/B978-0-12-815715-2.00013-0,journal,Smart Delivery Systems,sciencedirect,2020-12-31,sciencedirect,Chapter 7: Where machine learning meets smart delivery systems,https://api.elsevier.com/content/article/pii/B9780128157152000130,"
               
                  The big data revolution is here and now. We are currently observing the explosion of data generated in virtually all fields of science, business, and engineering, including—among others—medical image analysis, satellite imaging, banking, and agriculture. Transportation systems are not an exception here—collecting and analyzing data can help build better (meta)heuristics for solving rich vehicle routing problems (VRPs) using data-driven algorithms. In this chapter, we review the current advances in machine learning-powered approaches applied to solve different variants of VRPs and show how effective data mining can boost the capabilities of existent optimization techniques. Finally, we discuss how machine learning and deep learning in particular are exploited in smart delivery systems.
            ",autonomous vehicle
10.1016/B978-0-12-814391-9.00002-9,journal,Intelligent Data Mining and Fusion Systems in Agriculture,sciencedirect,2020-12-31,sciencedirect,Chapter 2: Artificial intelligence in agriculture,https://api.elsevier.com/content/article/pii/B9780128143919000029,"
               This chapter provides a detailed description regarding the scientific background of One Class Classification, One Class Support Vector Machines Hierarchical Self Organizing Maps and Active learning algorithms and their application in the field of Precision Agriculture.
            ",autonomous vehicle
10.1016/j.neucom.2011.03.020,journal,Neurocomputing,sciencedirect,2011-09-30,sciencedirect,Dynamic topology learning with the probabilistic self-organizing graph,https://api.elsevier.com/content/article/pii/S0925231211002360,"
                  Self-organizing neural networks are usually focused on prototype learning, while the topology is held fixed during the learning process. Here a method to adapt the topology of the network so that it reflects the internal structure of the input distribution is proposed. This leads to a self-organizing graph, where each unit is a mixture component of a mixture of Gaussians (MoG). The corresponding update equations are derived from the stochastic approximation framework. This approach combines the advantages of probabilistic mixtures with those of self-organization. Experimental results are presented to show the self-organization ability of our proposal and its performance when used with multivariate datasets in classification and image segmentation tasks.
               ",autonomous vehicle
10.1016/j.canlet.2016.05.033,journal,Cancer Letters,sciencedirect,2016-11-01,sciencedirect,Big Data and machine learning in radiation oncology: State of the art and future prospects,https://api.elsevier.com/content/article/pii/S0304383516303469,"
                  Precision medicine relies on an increasing amount of heterogeneous data. Advances in radiation oncology, through the use of CT Scan, dosimetry and imaging performed before each fraction, have generated a considerable flow of data that needs to be integrated. In the same time, Electronic Health Records now provide phenotypic profiles of large cohorts of patients that could be correlated to this information. In this review, we describe methods that could be used to create integrative predictive models in radiation oncology. Potential uses of machine learning methods such as support vector machine, artificial neural networks, and deep learning are also discussed.
               ",autonomous vehicle
10.1016/j.ifacol.2019.12.692,journal,IFAC-PapersOnLine,sciencedirect,2019-12-31,sciencedirect,Classification of user attitudes in Twitter -beginners guide to selected Machine Learning libraries,https://api.elsevier.com/content/article/pii/S2405896319326412,"
                  This paper presents an interesting use case for learning as well as teaching basics of Machine Learning systems. Starting from a brief historical outline of the ML, the authors propose and compare a set of popular ML libraries in an interesting exemplary implementation, to present their usability. The paper also describes text classification methods, the aim of which is to distinguish positive and negative labels of particular messages within the Twitter social network. The study is summarized by a brief comparison of the quality of the classification of the libraries and methods used, as an assessment of their suitability. Final thoughts on the importance of teaching ML are included.
               ",autonomous vehicle
10.1016/j.avsg.2019.11.037,journal,Annals of Vascular Surgery,sciencedirect,2020-05-31,sciencedirect,Fundamentals in Artificial Intelligence for Vascular Surgeons,https://api.elsevier.com/content/article/pii/S0890509619310337,"
                  Artificial intelligence (AI) corresponds to a broad discipline that aims to design systems, which display properties of human intelligence. While it has led to many advances and applications in daily life, its introduction in medicine is still in its infancy. AI has created interesting perspectives for medical research and clinical practice but has been sometimes associated with hype leading to a misunderstanding of its real capabilities. Here, we aim to introduce the fundamental notions of AI and to bring an overview of its potential applications for medical and surgical practice. In the limelight of current knowledge, limits and challenges to face as well as future directions are discussed.
               ",autonomous vehicle
10.1016/B978-0-12-815585-1.00028-0,journal,Biotechnology Entrepreneurship,sciencedirect,2020-12-31,sciencedirect,Chapter 28: Artificial Intelligence: Emerging Applications in Biotechnology and Pharma,https://api.elsevier.com/content/article/pii/B9780128155851000280,"
               Artificial intelligence (AI), specifically machine learning (ML) models that make biological or clinical predictions, may indeed be transformative within the realms of drug and biologic discovery and development. ML has already made inroads within these fields. The extent to which AI dramatically improves the efficiency and rate of success of drug R&D remains to be determined, but many promising approaches have already been developed. The use of these tools, however, should be undertaken with precision and care by a cross-disciplinary team to maximize the likelihood of utility. This chapter summarizes, at a high level, the basic principles of ML, state-of-the-art techniques, and potential applications within drug discovery and development.
            ",autonomous vehicle
10.1016/j.ifacol.2016.11.099,journal,IFAC-PapersOnLine,sciencedirect,2016-12-31,sciencedirect,Deep Learning vs. Wise Learning: A Critical and Challenging Overview,https://api.elsevier.com/content/article/pii/S240589631632537X,"
                  Learning is the most important thing that living creatures do. An organism cannot properly animate itself without first learning how to. Knowledge is the basis for all natural and human made systems. Using knowledge to model and control a complex dynamic system (CDS) is considered. Wisdom is carefully reviewed and related to knowledge. Deep Learning (DL) and Wise Learning (WL) are presented and analyzed as two approaches to address the challenging problem of modelling and controlling CDS. Strong and weak issues of both methods are discussed. Future research directions are provided.
               ",autonomous vehicle
10.1016/j.clinmicnews.2019.06.004,journal,Clinical Microbiology Newsletter,sciencedirect,2019-07-15,sciencedirect,Machine Learning in Microbiology: Finding the Signal in the Noise,https://api.elsevier.com/content/article/pii/S0196439919300649,"
                  Electronic health records (EHRs) capture data that is common to healthcare organizations to document the care they provide to their patients. EHR data are discrete or structured data that include laboratory values and diagnosis codes. Other examples include unstructured data like physician notes, nursing notes, and dictations. As access to EHR data expands, significant opportunities arise for clinical laboratories to combine EHR data with data internal to the laboratory to answer questions about laboratory impact. For example, microbiology laboratories can collect clinical, financial and operational data variables to document downstream improvements to survival, stewardship, test utilization after the implementation of a diagnostic intervention or a clinical decision tree that requires results of a laboratory test [1]. As clinical decision trees are adopted, data can be used to risk-stratify patients and to guide care for patients with high-complexity conditions and develop diagnostic or clinical pathways that improve care. For new diagnostic pathways to occur, the applications of computational techniques, such as machine learning (ML) fit well with data mining the EHR. ML is a branch of artificial intelligence and is defined as an analytic model that can handle large complex data sets and “learn” from the data at hand to identify and predict patterns with minimal human intervention. Application of ML has the potential to transform patient risk stratification, which could then transform care. In this review, the basics of ML and examples of relevant ML applications are described. Finally, relevant factors for the design and implementation of ML in microbiology are discussed.
               ",autonomous vehicle
10.1016/j.jclepro.2021.125834,journal,Journal of Cleaner Production,sciencedirect,2021-03-20,sciencedirect,"Artificial intelligence in sustainable energy industry: Status Quo, challenges and opportunities",https://api.elsevier.com/content/article/pii/S0959652621000548,"
                  The energy industry is at a crossroads. Digital technological developments have the potential to change our energy supply, trade, and consumption dramatically. The new digitalization model is powered by the artificial intelligence (AI) technology. The integration of energy supply, demand, and renewable sources into the power grid will be controlled autonomously by smart software that optimizes decision-making and operations. AI will play an integral role in achieving this goal. This study focuses on the use of AI techniques in the energy sector. This study aims to present a realistic baseline that allows researchers and readers to compare their AI efforts, ambitions, new state-of-the-art applications, challenges, and global roles in policymaking. We covered three major aspects, including: i) the use of AI in solar and hydrogen power generation; (ii) the use of AI in supply and demand management control; and (iii) recent advances in AI technology. This study explored how AI techniques outperform traditional models in controllability, big data handling, cyberattack prevention, smart grid, IoT, robotics, energy efficiency optimization, predictive maintenance control, and computational efficiency. Big data, the development of a machine learning model, and AI will play an important role in the future energy market. Our study’s findings show that AI is becoming a key enabler of a complex, new and data-related energy industry, providing a key magic tool to increase operational performance and efficiency in an increasingly cut-throat environment. As a result, the energy industry, utilities, power system operators, and independent power producers may need to focus more on AI technologies if they want meaningful results to remain competitive. New competitors, new business strategies, and a more active approach to customers would require informed and flexible regulatory engagement with the associated complexities of customer safety, privacy, and information security. Given the pace of development in information technology, AI and data analysis, regulatory approvals for new services and products in the new Era of digital energy markets can be enforced as quickly and efficiently as possible.
               ",autonomous vehicle
10.1016/j.procir.2020.02.207,journal,Procedia CIRP,sciencedirect,2020-12-31,sciencedirect,Identification of evaluation criteria for algorithms used within the context of product development,https://api.elsevier.com/content/article/pii/S2212827120308581,"Manufacturing companies are facing multiple challenges within the product development process nowadays. A dynamic environment due to new technologies, political influences on products as well as unstable markets are raising the complexity of product development processes. To address these uncertainties, the use Artificial Intelligence is increasing. Based on the availability of large data sets and computing capacity to analyse these data, especially the field of Machine Learning algorithms, as a subsection of Artificial Intelligence, it has high potentials to increase effectiveness and efficiency within a wide variety of different fields. One of these fields, in which Machine Learning algorithms can be used, is a variety of activities within the product development process. The paper presents an evaluation method, which helps to identify specific strengths and weaknesses of selected algorithms to identify possible usage within this process. First, the work describes different algorithms, categorizes them, and selects the most relevant for further evaluation. Second, a method to evaluate different Machine Learning algorithms is developed based on a literature review on influence factors for those algorithms. The objective of the evaluation method is to give users without in-depth knowledge of Machine Learning algorithms an applicable possibility to choose a potential algorithm. Finally, the most relevant algorithms are identified based on the literature findings. Those are evaluated based on the identified criteria to simplify the selection of a Machine Learning algorithm for product development tasks.",autonomous vehicle
10.1016/j.neucom.2021.06.093,journal,Neurocomputing,sciencedirect,2021-10-12,sciencedirect,Routing optimization meets Machine Intelligence: A perspective for the future network,https://api.elsevier.com/content/article/pii/S0925231221010298,"
                  The future network is expected to support extremely large bandwidth, ultra-low latency or deterministic delay, extremely high reliability, and massive connectivity for novel forward-looking scenarios. As one of the most fundamental parts of the network, routing plays a vital role in a well-performed network. Recently, some new techniques using machine intelligence to optimize the network routing have been proposed. Although they have demonstrated great potential to improve the network performance, it is still a great challenge to apply machine intelligence-based routing in real network environment due to the limitations of current network architectures and protocols. Fortunately, the future network research program is on-going for designing new network paradigms, which provides an opportunity to address those limitations. In this paper, we investigate state-of-the-art techniques in machine intelligence-enabled network routing and discuss the development trends of machine intelligence-enabled routing optimization techniques for the future network.
               ",autonomous vehicle
10.1016/j.csbj.2016.12.005,journal,Computational and Structural Biotechnology Journal,sciencedirect,2017-12-31,sciencedirect,Machine Learning and Data Mining Methods in Diabetes Research,https://api.elsevier.com/content/article/pii/S2001037016300733,"The remarkable advances in biotechnology and health sciences have led to a significant production of data, such as high throughput genetic data and clinical information, generated from large Electronic Health Records (EHRs). To this end, application of machine learning and data mining methods in biosciences is presently, more than ever before, vital and indispensable in efforts to transform intelligently all available information into valuable knowledge. Diabetes mellitus (DM) is defined as a group of metabolic disorders exerting significant pressure on human health worldwide. Extensive research in all aspects of diabetes (diagnosis, etiopathophysiology, therapy, etc.) has led to the generation of huge amounts of data. The aim of the present study is to conduct a systematic review of the applications of machine learning, data mining techniques and tools in the field of diabetes research with respect to a) Prediction and Diagnosis, b) Diabetic Complications, c) Genetic Background and Environment, and e) Health Care and Management with the first category appearing to be the most popular. A wide range of machine learning algorithms were employed. In general, 85% of those used were characterized by supervised learning approaches and 15% by unsupervised ones, and more specifically, association rules. Support vector machines (SVM) arise as the most successful and widely used algorithm. Concerning the type of data, clinical datasets were mainly used. The title applications in the selected articles project the usefulness of extracting valuable knowledge leading to new hypotheses targeting deeper understanding and further investigation in DM.",autonomous vehicle
10.1016/B978-0-12-812970-8.00002-6,journal,"Mobility Patterns, Big Data and Transport Analytics",sciencedirect,2019-12-31,sciencedirect,Chapter 2: Machine Learning Fundamentals,https://api.elsevier.com/content/article/pii/B9780128129708000026,"
               This chapter aims to be a smooth introduction to the basic concepts of machine learning, and, building on them, explain some to the latest advanced techniques. After a brief historical perspective, we overview the two currently most popular machine learning frameworks—deep learning and probabilistic graphical models. We conclude the chapter with practical advices about machine learning experiments which are necessary to know for a beginner. A good understanding of these fundamentals opens up a wide portfolio of opportunities for predictive models in transportation, and is hopefully a good basis for the remainder of this book.
            ",autonomous vehicle
10.1016/j.promfg.2020.01.331,journal,Procedia Manufacturing,sciencedirect,2019-12-31,sciencedirect,"Framework for Customized, Machine Learning Driven Condition Monitoring System for Manufacturing",https://api.elsevier.com/content/article/pii/S235197892030398X,"Knowledge about the technical state of a complex machine is crucial regarding the reliability and availability within the entire usage phase of a certain technical product. By gathering and gaining information about the exact health condition, production stops can be minimized while optimizing the efficiency of a machine leading to increased customer satisfaction. Since more and more data is available and gets recorded by plenty of sensors, classical statistical methods are not applicable anymore or are just not designed to process such large amount of data. Due to that purpose, many methods from the field of machine learning emerged in the past years. Machine learning in general addresses the issue of recognizing patterns and rules within large amounts of data with the goal to make predictions. Within this paper, the focus is laid upon a detailed step-by-step guide on the development of customized condition monitoring solutions in manufacturing since the application of these still poses a major problem for lots of users. The entire concept of such solutions starting with data collection, through signal-interpretation and index development, finishing with reliable monitoring of the machine state during the manufacturing process is presented throughout. This is done on a real-life application of the concept using synthesized yet realistic data. Moreover, an in-depth analysis of advantages, possibilities and opportunities of the presented concept as well as its weaknesses and boundaries is performed.",autonomous vehicle
10.1016/j.jcp.2018.05.031,journal,Journal of Computational Physics,sciencedirect,2019-03-01,sciencedirect,Sharp interface approaches and deep learning techniques for multiphase flows,https://api.elsevier.com/content/article/pii/S0021999118303371,"
                  We present a review on numerical methods for simulating multiphase and free surface flows. We focus in particular on numerical methods that seek to preserve the discontinuous nature of the solutions across the interface between phases. We provide a discussion on the Ghost-Fluid and Voronoi Interface methods, on the treatment of surface tension forces that avoid stringent time step restrictions, on adaptive grid refinement techniques for improved efficiency and on parallel computing approaches. We present the results of some simulations obtained with these treatments in two and three spatial dimensions. We also provide a discussion of Machine Learning and Deep Learning techniques in the context of multiphase flows and propose several future potential research thrusts for using deep learning to enhance the study and simulation of multiphase flows.
               ",autonomous vehicle
10.1016/j.cose.2020.101947,journal,Computers & Security,sciencedirect,2020-10-31,sciencedirect,SoK: Machine vs. machine – A systematic classification of automated machine learning-based CAPTCHA solvers,https://api.elsevier.com/content/article/pii/S0167404820302236,"Internet services heavily rely on CAPTCHAs for determining whether or not a user is a human being. The recent advances in ML and AI make the efficacy of CAPTCHAs in strengthening Internet services against bots questionable. In this paper, we conduct a systematic analysis and classification of the state-of-the-art ML-based techniques for the automated text-based CAPTCHA breaking problem. The current state and robustness of text-based CAPTCHAs as are utilized by modern Internet applications, against ML-based automated breaking tools, is examined and reported. Our study suggests that ML can be very effective in increasing: (a) accuracy, (b) speed, and (c) abstraction in CAPTCHA solving. Especially, as far as (c) is concerned, ML-based techniques are easier to be applied in different classes of text-based CAPTCHA schemes. To assess the importance of ML in breaking CAPTCHAs, we build our own ML-only classifiers. Surprisingly, an ML-only approach for solving CAPTCHAs is not sufficient. Overall, our study suggests that fundamentally different ways of conducting reverse Turing test, that will be painless for legitimate users (i.e., humans) but at the same time challenging for automated systems (i.e., software), should be considered for ensuring the healthy operation of current Internet services.",autonomous vehicle
10.1016/j.jrmge.2020.05.010,journal,Journal of Rock Mechanics and Geotechnical Engineering,sciencedirect,2021-02-28,sciencedirect,Application of artificial intelligence to rock mechanics: An overview,https://api.elsevier.com/content/article/pii/S1674775520301426,"Different artificial intelligence (AI) methods have been applied to various aspects of rock mechanics, but the fact that none of these methods have been used as a standard implies that doubt as to their generality and validity still exists. For this, a literature review of application of AI to the field of rock mechanics is presented. Comprehensive studies of the researches published in the top journals relative to the fields of rock mechanics, computer applications in engineering, and the textbooks were conducted. The performances of the AI methods that have been used in rock mechanics applications were evaluated. The literature review shows that AI methods have successfully been used to solve various problems in the rock mechanics field and they performed better than the traditional empirical, mathematical or statistical methods. However, their practical applicability is still an issue of concern as many of the existing AI models require some level of expertise before they can be used, because they are not in the form of tractable mathematical equations. Thus some advanced AI methods are still yet to be explored. The limited availability of dataset for the AI simulations is also identified as a major problem. The solutions to the identified problems and the possible future research focus were proposed in the study subsequently.",autonomous vehicle
10.1016/j.coco.2021.100812,journal,Composites Communications,sciencedirect,2021-08-31,sciencedirect,Fatigue life prediction of glass fiber reinforced epoxy composites using artificial neural networks,https://api.elsevier.com/content/article/pii/S2452213921001881,"
                  In this work, fiber-reinforced composites, consisting of glass fibers with an epoxy matrix, have been considered to assess their fatigue life under combined rotating bending loads. Rotating bending fatigue tests were performed under fully reversed loading cycles at an operating frequency of 10 Hz. Fatigue test results revealed that the fatigue life of glass fiber reinforced with epoxy (GFRE) matrix composite decreased with an increase in stress levels. In addition, an artificial neural network approach was used in this study to demonstrate the impact of stress levels on the fatigue strength and failure modes of GFRE composites. The neural network was trained by the Levenberg-Marquardt algorithm using MATLAB. The feasibility of artificial neural networks for modeling and predicting the fatigue behavior of GFRE composites was verified by the correlation results (R2 = 0.99857) between the experimental data and ANN outputs. Three failure modes viz. matrix cracking, interfacial debonding and interfacial splitting of fibers were observed under low and high cycle fatigue tests. Further, ANN was successful in classifying these failure modes using a conjugate gradient backpropagation algorithm with 100% accuracy.
               ",autonomous vehicle
10.1016/j.ifacol.2019.11.203,journal,IFAC-PapersOnLine,sciencedirect,2019-12-31,sciencedirect,Machine Learning in Predicting Demand for Fast-Moving Consumer Goods: An Exploratory Research,https://api.elsevier.com/content/article/pii/S240589631931153X,"
                  More accurate prediction of the demand for fast-moving consumer goods is a competitive factor for manufacturers and retailers, especially in the fashion, technology and fresh food sectors. This exploratory research presents the benefits of Machine Learning in sales forecasting for short shelf-life and highly-perishable products, as it surpasses the accuracy level of traditional statistical techniques and, as a result, improves inventory balancing throughout the chain, reducing stockout rates at points of sale, improving availability to consumers and increasing profitability.
               ",autonomous vehicle
10.1016/j.cosrev.2019.100221,journal,Computer Science Review,sciencedirect,2020-02-29,sciencedirect,Relational intelligence recognition in online social networks — A survey,https://api.elsevier.com/content/article/pii/S1574013718303575,"
                  Information networks today play an important, fundamental role in regulating real life activities. However, many methods developed on this framework lack the capacity to adequately represent sophistication contained within the information it carries. As a result, they suffer from problems such as inaccuracies, reliability and performance. We define relational intelligence as a combination of affective (Cambria, 2016; 2015 [1,2]; Hidalgo et al., 2015 [3]), sentimental (Ferrara and Yang, 2015 [4]; Wang et al., 2013 [5]; Madhoushi et al., 2015 [6]) and ethical (Vayena et al., 2015 [7]; Nunan and Di Domenico, 2013 [8]; Anderson and Guyton, 2013 [9]) developments reflected in the evolving patterns of online social structures. These developments involve the ability of actors to adaptively regulate emotions, values, interest and demands between each other in an online social scene. In this paper, we provide a state-of-the-art overview of approaches used in recognizing relational intelligence — with special focus given to Online Social Networks (OSNs). The important core processes of data mining, identification (extraction), detection (labeling), classification, prediction and learning which empower machine recognition tasks will be discussed in detail. In addition, widely affected applications like recommending, ranking, influence, topic modeling, evolution, etc. will also be introduced along with their basic concepts uncovered to a detailed degree. We also include some discussions on more advanced topics that point to further interesting future research directions.
               ",autonomous vehicle
10.1016/j.autcon.2020.103517,journal,Automation in Construction,sciencedirect,2021-02-28,sciencedirect,Roles of artificial intelligence in construction engineering and management: A critical review and future trends,https://api.elsevier.com/content/article/pii/S0926580520310979,"
                  With the extensive adoption of artificial intelligence (AI), construction engineering and management (CEM) is experiencing a rapid digital transformation. Since AI-based solutions in CEM has become the current research focus, it needs to be comprehensively understood. In this regard, this paper presents a systematic review under both scientometric and qualitative analysis to present the current state of AI adoption in the context of CEM and discuss its future research trends. To begin with, a scientometric review is performed to explore the characteristics of keywords, journals, and clusters based on 4,473 journal articles published in 1997–2020. It is found that there has been an explosion of relevant papers especially in the past 10 years along with the change in keyword popularity from expert systems to building information modeling (BIM), digital twins, and others. Then, a brief understanding of CEM is provided, which can be benefited from the emerging trend of AI in terms of automation, risk mitigation, high efficiency, digitalization, and computer vision. Special concerns have been put on six hot research topics that amply the advantage of AI in CEM, including (1) knowledge representation and reasoning, (2) information fusion, (3) computer vision, (4) natural language processing, (5) intelligence optimization, and (6) process mining. The goal of these topics is to model, predict, and optimize issues in a data-driven manner throughout the whole lifecycle of the actual complex project. To further narrow the gap between AI and CEM, six key directions of future researches, such as smart robotics, cloud virtual and augmented reality (cloud VR/AR), Artificial Intelligence of Things (AIoT), digital twins, 4D printing, and blockchains, are highlighted to constantly facilitate the automation and intelligence in CEM.
               ",autonomous vehicle
10.1016/j.jnca.2021.103078,journal,Journal of Network and Computer Applications,sciencedirect,2021-07-01,sciencedirect,Application placement in Fog computing with AI approach: Taxonomy and a state of the art survey,https://api.elsevier.com/content/article/pii/S1084804521000989,"
                  With the increasing use of the Internet of Things (IoT) in various fields and the need to process and store huge volumes of generated data, Fog computing was introduced to complement Cloud computing services. Fog computing offers basic services at the network for supporting IoT applications with low response time requirements. However, Fogs are distributed, heterogeneous, and their resources are limited, therefore efficient distribution of IoT applications tasks in Fog nodes, in order to meet quality of service (QoS) and quality of experience (QoE) constraints is challenging. In this survey, at first, we have an overview of basic concepts of Fog computing, and then review the application placement problem in Fog computing with focus on Artificial intelligence (AI) techniques. We target three main objectives with considering a characteristics of AI-based methods in Fog application placement problem: (i) categorizing evolutionary algorithms, (ii) categorizing machine learning algorithms, and (iii) categorizing combinatorial algorithms into subcategories includes a combination of machine learning and heuristic, a combination of evolutionary and heuristic, and a combinations of evolutionary and machine learning. Then the security considerations of application placement have been reviewed. Finally, we provide a number of open questions and issues as future works.
               ",autonomous vehicle
10.1016/j.chemosphere.2018.02.111,journal,Chemosphere,sciencedirect,2018-06-30,sciencedirect,A review on experimental design for pollutants removal in water treatment with the aid of artificial intelligence,https://api.elsevier.com/content/article/pii/S0045653518303230,"
                  Water pollution occurs mainly due to inorganic and organic pollutants, such as nutrients, heavy metals and persistent organic pollutants. For the modeling and optimization of pollutants removal, artificial intelligence (AI) has been used as a major tool in the experimental design that can generate the optimal operational variables, since AI has recently gained a tremendous advance. The present review describes the fundamentals, advantages and limitations of AI tools. Artificial neural networks (ANNs) are the AI tools frequently adopted to predict the pollutants removal processes because of their capabilities of self-learning and self-adapting, while genetic algorithm (GA) and particle swarm optimization (PSO) are also useful AI methodologies in efficient search for the global optima. This article summarizes the modeling and optimization of pollutants removal processes in water treatment by using multilayer perception, fuzzy neural, radial basis function and self-organizing map networks. Furthermore, the results conclude that the hybrid models of ANNs with GA and PSO can be successfully applied in water treatment with satisfactory accuracies. Finally, the limitations of current AI tools and their new developments are also highlighted for prospective applications in the environmental protection.
               ",autonomous vehicle
10.1016/j.inffus.2018.10.005,journal,Information Fusion,sciencedirect,2019-10-31,sciencedirect,Data fusion and machine learning for industrial prognosis: Trends and perspectives towards Industry 4.0,https://api.elsevier.com/content/article/pii/S1566253518304706,"
                  The so-called “smartization” of manufacturing industries has been conceived as the fourth industrial revolution or Industry 4.0, a paradigm shift propelled by the upsurge and progressive maturity of new Information and Communication Technologies (ICT) applied to industrial processes and products. From a data science perspective, this paradigm shift allows extracting relevant knowledge from monitored assets through the adoption of intelligent monitoring and data fusion strategies, as well as by the application of machine learning and optimization methods. One of the main goals of data science in this context is to effectively predict abnormal behaviors in industrial machinery, tools and processes so as to anticipate critical events and damage, eventually causing important economical losses and safety issues. In this context, data-driven prognosis is gradually gaining attention in different industrial sectors. This paper provides a comprehensive survey of the recent developments in data fusion and machine learning for industrial prognosis, placing an emphasis on the identification of research trends, niches of opportunity and unexplored challenges. To this end, a principled categorization of the utilized feature extraction techniques and machine learning methods will be provided on the basis of its intended purpose: analyze what caused the failure (descriptive), determine when the monitored asset will fail (predictive) or decide what to do so as to minimize its impact on the industry at hand (prescriptive). This threefold analysis, along with a discussion on its hardware and software implications, intends to serve as a stepping stone for future researchers and practitioners to join the community investigating on this vibrant field.
               ",autonomous vehicle
10.1016/j.comnet.2021.108177,journal,Computer Networks,sciencedirect,2021-08-04,sciencedirect,"Task offloading in Edge and Cloud Computing: A survey on mathematical, artificial intelligence and control theory solutions",https://api.elsevier.com/content/article/pii/S1389128621002322,"
                  Next generation communication networks are expected to accommodate a high number of new and resource-voracious applications that can be offered to a large range of end users. Even though end devices are becoming more powerful, the available local resources cannot cope with the requirements of these applications. This has created a new challenge called task offloading, where computation intensive tasks need to be offloaded to more resource powerful remote devices. Naturally, the Cloud Computing is a well-tested infrastructure that can facilitate the task offloading. However, Cloud Computing as a centralized and distant infrastructure creates significant communication delays that cannot satisfy the requirements of the emerging delay-sensitive applications. To this end, the concept of Edge Computing has been proposed, where the Cloud Computing capabilities are repositioned closer to the end devices at the edge of the network. This paper provides a detailed survey of how the Edge and/or Cloud can be combined together to facilitate the task offloading problem. Particular emphasis is given on the mathematical, artificial intelligence and control theory optimization approaches that can be used to satisfy the various objectives, constraints and dynamic conditions of this end-to-end application execution approach. The survey concludes with identifying open challenges and future directions of the problem at hand.
               ",autonomous vehicle
10.1016/j.asoc.2020.106876,journal,Applied Soft Computing,sciencedirect,2021-02-28,sciencedirect,Resource provisioning in scalable cloud using bio-inspired artificial neural network model,https://api.elsevier.com/content/article/pii/S1568494620308140,"
                  Resource assignment is one of the emerging research area in the cloud scenario. Cloud computing provides a shared pool of resources in a distributed environment. It supports the features of utility-based computing. Efficient task provisioning on virtual machines is the major concern in an extensible cloud computing environment. The task provisioning minimizes the performance metrics total completion time (ms), average start time, average finish time, average execution time, scheduling time, and simulation time respectively. The scheduling is an important problem which becomes more complicated when various parameters consider. The key issue in virtual machine level scheduling is execution time overhead and scalability in a real-time scenario. Our objective is to make an optimal schedule of tasks on a virtual machine inside the datacenter using neural-bio inspired GA-ANN technique. This work presents a scheduler based on a genetic approach and an artificial neural network. The presented approach performs optimal scheduling of tasks on an appropriate virtual machine. The reliability of the system improves by reducing the number of tasks failed. The presented work uses a genetic algorithm to generated huge data sets and trains the neural model using the data set generated by using a genetic approach. The accuracy of the model is improved using back propagation with 98% accuracy. The set of experiments are performed using a scalable cloud computing environment. The presented bio-inspired technique is compared against nature-inspired, bio-inspired cost-aware BB–BC, GA-Cost, and GA-Exe based efficient task scheduling techniques. The results are obtained using real workload logs and synthetic data sets. Results indicate that the proposed GA-ANN bio-inspired predictive approach outperforms the considered nature-inspired scheduling approaches. The proposed algorithm is compared using various performance metrics total completion time, average start time, average finish time, and the fault rate, execution time, and scheduling time respectively. The proposed model reduces the fault rate by 82.63%, successfully completed tasks count improves by 26.81% and execution time improves by 10.66% and scheduling time improves by 69.94%. The scheduling time improves by 85.76% with an increasing number of iterations and constant numbers of tasks. Hence the presented GA-ANN scheduling technique outperformed the GA cost, GA EXE, and BB–BC COST scheduling approaches.
               ",autonomous vehicle
10.1016/j.eswa.2017.03.002,journal,Expert Systems with Applications,sciencedirect,2017-09-01,sciencedirect,Incremental <ce:italic>Q</ce:italic>-learning strategy for adaptive PID control of mobile robots,https://api.elsevier.com/content/article/pii/S0957417417301513,"
                  Expert and intelligent systems are being developed to control many technological systems including mobile robots. However, the PID (Proportional-Integral-Derivative) controller is a fast low-level control strategy widely used in many control engineering tasks. Classic control theory has contributed with different tuning methods to obtain the gains of PID controllers for specific operation conditions. Nevertheless, when the system is not fully known and the operative conditions are variable and not previously known, classical techniques are not entirely suitable for the PID tuning. To overcome these drawbacks many adaptive approaches have been arisen, mainly from the field of artificial intelligent. In this work, we propose an incremental Q-learning strategy for adaptive PID control. In order to improve the learning efficiency we define a temporal memory into the learning process. While the memory remains invariant, a non-uniform specialization process is carried out generating new limited subspaces of learning. An implementation on a real mobile robot demonstrates the applicability of the proposed approach for a real-time simultaneous tuning of multiples adaptive PID controllers for a real system operating under variable conditions in a real environment.
               ",autonomous vehicle
10.1016/j.promfg.2019.04.103,journal,Procedia Manufacturing,sciencedirect,2019-12-31,sciencedirect,A survey of Artificial Neural Network-based Prediction Models for Thermal Properties of Biomass,https://api.elsevier.com/content/article/pii/S2351978919305840,"The global community has supported the need for sustainable and renewable energy due to environmental concerns from the greenhouse gas emission. Biomass stands as one of the most abundant and predictable sources of renewable energy. Therefore, to explore the maximum potential of biomass, a detailed understanding of its embedded potential is needed. However, most experimental procedures require equipment that is highly sophisticated and expensive. The advancement of knowledge in artificial intelligence and blockchain technology is unlocking new potential prediction accuracy for biomass thermal properties. Artificial Neural Network (ANN) is proving to be a vital tool that can enhance the research development in biomass energy prediction. This review highlights the stages in ANN modeling and the application of ANN in Biomass thermal value prediction. It identifies the research gaps in the current status of research on ANN as related to biomass and the direction for further study.",autonomous vehicle
10.1016/j.eswa.2017.03.002,journal,Expert Systems with Applications,sciencedirect,2017-09-01,sciencedirect,Incremental <ce:italic>Q</ce:italic>-learning strategy for adaptive PID control of mobile robots,https://api.elsevier.com/content/article/pii/S0957417417301513,"
                  Expert and intelligent systems are being developed to control many technological systems including mobile robots. However, the PID (Proportional-Integral-Derivative) controller is a fast low-level control strategy widely used in many control engineering tasks. Classic control theory has contributed with different tuning methods to obtain the gains of PID controllers for specific operation conditions. Nevertheless, when the system is not fully known and the operative conditions are variable and not previously known, classical techniques are not entirely suitable for the PID tuning. To overcome these drawbacks many adaptive approaches have been arisen, mainly from the field of artificial intelligent. In this work, we propose an incremental Q-learning strategy for adaptive PID control. In order to improve the learning efficiency we define a temporal memory into the learning process. While the memory remains invariant, a non-uniform specialization process is carried out generating new limited subspaces of learning. An implementation on a real mobile robot demonstrates the applicability of the proposed approach for a real-time simultaneous tuning of multiples adaptive PID controllers for a real system operating under variable conditions in a real environment.
               ",autonomous vehicle
10.1016/j.robot.2019.03.001,journal,Robotics and Autonomous Systems,sciencedirect,2019-06-30,sciencedirect,A deep learning gated architecture for UGV navigation robust to sensor failures,https://api.elsevier.com/content/article/pii/S0921889018305645,"
                  In this paper, we introduce a novel methodology for fusing sensors and improving robustness to sensor failures in end-to-end learning based autonomous navigation of ground vehicles in unknown environments. We propose the first learning based camera–LiDAR fusion methodology for autonomous in-door navigation. Specifically, we develop a multimodal end-to-end learning system, which maps raw depths and pixels from LiDAR and camera, respectively, to the steering commands. A novel gating based dropout regularization technique is introduced which effectively performs multimodal sensor fusion and reliably predicts steering commands even in the presence of various sensor failures. The robustness of our network architecture is demonstrated by experimentally evaluating its ability to autonomously navigate in the indoor corridor environment. Specifically, we show through various empirical results that our framework is robust to sensor failures, partial image occlusions, modifications of the camera image intensity, and the presence of noise in the camera or LiDAR range images. Furthermore, we show that some aspects of obstacle avoidance are implicitly learned (while not being specifically trained for it); these learned navigation capabilities are shown in ground vehicle navigation around static and dynamic obstacles.
               ",autonomous vehicle
10.1016/j.jbi.2021.103820,journal,Journal of Biomedical Informatics,sciencedirect,2021-07-31,sciencedirect,Causal relationship extraction from biomedical text using deep neural models: A comprehensive survey,https://api.elsevier.com/content/article/pii/S1532046421001490,"
                  The identification of causal relationships between events or entities within biomedical texts is of great importance for creating scientific knowledge bases and is also a fundamental natural language processing (NLP) task. A causal (cause-effect) relation is defined as an association between two events in which the first must occur before the second. Although this task is an open problem in artificial intelligence, and despite its important role in information extraction from the biomedical literature, very few works have considered this problem. However, with the advent of new techniques in machine learning, especially deep neural networks, research increasingly addresses this problem. This paper summarizes state-of-the-art research, its applications, existing datasets, and remaining challenges. For this survey we have implemented and evaluated various techniques including a Multiview CNN (MVC), attention-based BiLSTM models and state-of-the-art word embedding models, such as those obtained with bidirectional encoder representations (ELMo) and transformer architectures (BioBERT). In addition, we have evaluated a graph LSTM as well as a baseline rule based system. We have investigated the class imbalance problem as an innate property of annotated data in this type of task. The results show that a considerable improvement of the results of state-of-the-art systems can be achieved when a simple random oversampling technique for data augmentation is used in order to reduce class imbalance.
               ",autonomous vehicle
10.1016/j.neucom.2013.11.040,journal,Neurocomputing,sciencedirect,2014-10-02,sciencedirect,Least-squares temporal difference learning based on an extreme learning machine,https://api.elsevier.com/content/article/pii/S0925231214003890,"
                  Reinforcement learning (RL) is a general class of algorithms for solving decision-making problems, which are usually modeled using the Markov decision process (MDP) framework. RL can find exact solutions only when the MDP state space is discrete and small enough. Due to the fact that many real-world problems are described by continuous variables, approximation is essential in practical applications of RL. This paper is focused on learning the value function of a fixed policy in continuous MPDs. This is an important subproblem of several RL algorithms. We propose a least-squares temporal difference (LSTD) algorithm based on the extreme learning machine. LSTD is typically combined with local function approximators, which scale poorly with the problem dimensionality. Our approach allows us to approximate value functions using single-hidden layer feedforward networks (SLFNs), a type of artificial neural network extensively used in many fields. Due to the global nature of SLFNs, the proposed approach is more suitable than traditional methods for high-dimensional problems. The method was empirically evaluated on a set of MDPs whose dimensionality varies from 1 to 6. For comparison purposes, experiments were replicated using a standard LSTD algorithm combined with Gaussian radial basis functions. Experimental results suggest that, although both methods can approximate accurately value functions, the proposed approach requires considerably fewer resources for the same degree of accuracy.
               ",autonomous vehicle
10.1016/j.ifacol.2019.12.411,journal,IFAC-PapersOnLine,sciencedirect,2019-12-31,sciencedirect,Deep Learning-Based Dependability Assessment Method for Industrial Wireless Network,https://api.elsevier.com/content/article/pii/S2405896319323195,"
                  Techniques on 5G and Internet of things bring a strong potential paradigm shift to wireless communication applications in industrial domain. Hence, there is a strong need for quantitative dependability assessment in these applications. However, with the evergrowing complexity and amount of wireless communication systems, their dependability relevant parameters also increase rapidly. In addition, the deep neural network has advantages on high dimensional data process. Hence, a deep learning-based dependability assessment method is proposed to address the issue, wherein a deep auto-encoder based approach is proposed to reduce data dimension and to obtain the data codes, and DBSCAN is used to cluster these codes. An experimental environment is built for collecting data set on the Multifaces, and a rough classification method is proposed to obtain a superior deep encoder model. Based on the superior model and DBSCAN, the data set are mainly divided into four dependability clusters.
               ",autonomous vehicle
10.1016/j.enconman.2019.111859,journal,Energy Conversion and Management,sciencedirect,2019-10-01,sciencedirect,"Artificial neural network based multivariable optimization of a hybrid system integrated with phase change materials, active cooling and hybrid ventilations",https://api.elsevier.com/content/article/pii/S0196890419308416,"
                  Utilising diversified forms of energy in combination with advanced energy conversions and thermal energy storages is an effective way of developing high energy-efficient renewable systems for green buildings. In this study, a novel hybrid system for the energy cascade utilisation has been proposed, integrating the hybrid ventilations, the active photovoltaic cooling, the radiative cooling and the phase change materials’ storages. An enthalpy-based numerical modelling using the finite-difference method, which has been developed earlier, was used to characterize the sophisticated heat transfer process. A generic optimization methodology with competitive computational efficiency was applied by implementing the supervised machine learning and the advanced optimization algorithm. Multivariable optimizations for geometrical and operating parameters have been conducted and contrasted between the teaching-learning-based optimization and the particle swarm optimization. The results illustrate that the developed artificial neural network-based data-driven learning algorithm is more accurate and more computational-efficient than the traditional ‘lsqcurvefit’ fitting methodology for the characterization of the optimization function. In addition, the optimal case through the teaching-learning-based optimization is more robust than the optimal case through the particle swarm optimization in terms of the equivalent overall energy generation. This study presents a novel hybrid system for the energy cascade utilisation and a new generic optimization methodology, which are important for the promotion of green buildings with high efficiency of renewable energy utilisation.
               ",autonomous vehicle
10.1016/j.echo.2018.10.003,journal,Journal of the American Society of Echocardiography,sciencedirect,2018-12-31,sciencedirect,The Mechanics of Machine Learning: From a Concept to Value,https://api.elsevier.com/content/article/pii/S0894731718305704,,autonomous vehicle
10.1016/B978-0-12-816718-2.00009-9,journal,Deep Learning and Parallel Computing Environment for Bioengineering Systems,sciencedirect,2019-12-31,sciencedirect,Chapter 2: Big Data Analytics and Deep Learning in Bioinformatics With Hadoop,https://api.elsevier.com/content/article/pii/B9780128167182000099,"
               
                  Bioinformatics research is regarded as an area which encompasses voluminous, expanding and complex datasets. Nowadays, with the use of high-throughput next-generation sequencing technologies, there is significant expansion of biological big data, which presents storage and processing challenges. Performing data analytics to harvest the wealth of data from biological and biomedical data, such as genetic mapping on the DNA sequence, will only help to advance our understanding of the human condition, health and disease; which will consequently allow curing diseases and improving human health and lives by supporting the development of precision methods for healthcare. In this chapter, big data analytics with regards to the Hadoop big data framework for storing and processing big data is described in the context of bioinformatics. Moreover, machine learning is an important approach for performing predictive and prescriptive analytics. Thus, machine learning and deep learning approaches currently being used in the context of big data analytics in the Hadoop framework are also presented, as well as the current uses of such techniques and tools in bioinformatics.
            ",autonomous vehicle
10.1016/j.engappai.2006.05.007,journal,Engineering Applications of Artificial Intelligence,sciencedirect,2006-10-31,sciencedirect,Imitation learning with spiking neural networks and real-world devices,https://api.elsevier.com/content/article/pii/S0952197606000959,"
                  This article is about a new approach in robotic learning systems. It provides a method to use a real-world device that operates in real-time, controlled through a simulated recurrent spiking neural network for robotic experiments. A randomly generated network is used as the main computational unit. Only the weights of the output units of this network are changed during training. It will be shown, that this simple type of a biological realistic spiking neural network—also known as a neural microcircuit—is able to imitate robot controllers like that incorporated in Braitenberg vehicles. A more non-linear type controller is imitated in a further experiment. In a different series of experiments that involve temporal memory reported in Burgsteiner et al. [2005. In: Proceedings of the 18th International Conference IEA/AIE. Lecture Notes in Artificial Intelligence. Springer, Berlin, pp. 121–130.] this approach also provided a basis for a movement prediction task. The results suggest that a neural microcircuit with a simple learning rule can be used as a sustainable robot controller for experiments in computational motor control.
               ",autonomous vehicle
10.1016/j.joi.2020.101047,journal,Journal of Informetrics,sciencedirect,2020-08-31,sciencedirect,Understanding hierarchical structural evolution in a scientific discipline: A case study of artificial intelligence,https://api.elsevier.com/content/article/pii/S1751157719302925,"
                  Detecting what type of knowledge constitutes a discipline, tracking how the knowledge changes, and understanding why the changes are triggered are the key issues in analyzing scientific development from a macro perspective, which is usually analyzed by the topic of evolution. However, traditional methods assume that the disciplinary structure is flat with only one-layer topics, rather than a tree-like structure with hierarchical topics, which leads to the inability of existing methods to effectively examine the details of the evolution, such as the interactions between different research directions. In this paper, we take artificial intelligence (AI) as a case in which we study its hierarchical structural evolution, more precisely inspecting disciplinary development, by analyzing 65,887 AI-related research papers published during a 10-year period from 2009 to 2018. From a hierarchical topic model that can construct a topic-tree with multi-layer organizations, we design a visual analysis model for the topic-tree to systematically and visually investigate how knowledge transfers along the topic-tree and how the topic-tree changes over time. Moreover, some assistant indicators are employed to help in the exploration of the complicated structural evolution. Then, we discover the latent relationship between the sub-structures within a topic as well as the triggering reason for the knowledge migration. Based on these results, we conclude that different topics have different development patterns and that the recent artificial intelligence revolution stems from the interactions among the different topics.
               ",autonomous vehicle
10.1016/j.inffus.2020.10.001,journal,Information Fusion,sciencedirect,2021-03-31,sciencedirect,Smart anomaly detection in sensor systems: A multi-perspective review,https://api.elsevier.com/content/article/pii/S1566253520303717,"
                  Anomaly detection is concerned with identifying data patterns that deviate remarkably from the expected behavior. This is an important research problem, due to its broad set of application domains, from data analysis to e-health, cybersecurity, predictive maintenance, fault prevention, and industrial automation. Herein, we review state-of-the-art methods that may be employed to detect anomalies in the specific area of sensor systems, which poses hard challenges in terms of information fusion, data volumes, data speed, and network/energy efficiency, to mention but the most pressing ones. In this context, anomaly detection is a particularly hard problem, given the need to find computing-energy-accuracy trade-offs in a constrained environment. We taxonomize methods ranging from conventional techniques (statistical methods, time-series analysis, signal processing, etc.) to data-driven techniques (supervised learning, reinforcement learning, deep learning, etc.). We also look at the impact that different architectural environments (Cloud, Fog, Edge) can have on the sensors ecosystem. The review points to the most promising intelligent-sensing methods, and pinpoints a set of interesting open issues and challenges.
               ",autonomous vehicle
10.1016/j.jfranklin.2015.04.014,journal,Journal of the Franklin Institute,sciencedirect,2015-08-31,sciencedirect,Spiking neural controllers in multi-agent competitive systems for adaptive targeted motor learning,https://api.elsevier.com/content/article/pii/S001600321500174X,"
                  The proposed work introduces a neural control strategy for guiding adaptation in spiking neural structures acting as nonlinear controllers in a group of bio-inspired robots which compete in reaching targets in a virtual environment. The neural structures embedded into each agent are inspired by a specific part of the insect brain, namely Central Complex, devoted to detect, learn and memorize visual features for targeted motor control. A reduced-order model of a spiking neuron is used as the basic building block for the neural controller. The control methodology employs bio-inspired, correlation based learning mechanisms like Spike timing dependent plasticity with the addition of a reward/punishment-based method experimentally found in insects. The reference signal for the overall multi-agent control system is imposed by a global reward, which guides motor learning to direct each agent towards specific visual targets. The neural controllers within the agents start from identical conditions: the learning strategy induces each robot to show anticipated targeting actions upon specific visual stimuli. The whole control structure also contributes to make the robots refractory or more sensitive to specific visual stimuli, showing distinct preferences in future choices. This leads to an environmentally induced, targeted motor control, even without a direct communication among the agents, giving robots, while running, the ability to perform adaptation in real-time. Experiments, carried out in a dynamic simulation environment, show the suitability of the proposed approach. Specific performance indexes, like Shannon׳s Entropy, are adopted to quantitatively analyze diversity and specialization within the group.
               ",autonomous vehicle
10.1016/j.cose.2018.07.003,journal,Computers & Security,sciencedirect,2018-09-30,sciencedirect,Unsupervised intrusion detection through skip-gram models of network behavior,https://api.elsevier.com/content/article/pii/S0167404818302700,"
                  Detecting intrusions is one of the main objectives of computer security. Attacks have become overly sophisticated over the years in order to remain effective and stealthy. Major breaches are typically perpetrated using techniques that are polymorphic, multi-vector, multi-stage and targeted, that is, adopting forms that were never seen before. Anomaly detection, which does not make any assumption about the shape of a potential attack but instead on legitimate behavior, seems to be a suitable approach in order to defeat sophisticated intrusions. Skip-gram modeling, a word2vec algorithm variant, was leveraged to model systems’ legitimate network behavior. The resulting model was then used to spot intrusions in a test dataset. The optimal configuration led to 99.20% precision, 82.07% recall, and 91.02% accuracy, with a false positive rate of 0.61%, which is significantly lower than most state-of-the-art methods. These metrics were achieved under a fully unsupervised setting, that is, without any prior knowledge of what constitutes an attack. Furthermore, the approach provides benefits in terms of interpretability and log storage requirements, as it requires a small amount of input features. It also produces information about systems behavior and their relationships, that can be reused by other analysis techniques to obtain further insights.
               ",autonomous vehicle
10.1016/S0019-0578(07)60032-9,journal,ISA Transactions,sciencedirect,2004-04-30,sciencedirect,Reinforcement learning algorithms for robotic navigation in dynamic environments,https://api.elsevier.com/content/article/pii/S0019057807600329,"
                  The purpose of this study was to examine improvements to reinforcement learning (RL) algorithms in order to successfully interact within dynamic environments. The scope of the research was that of RL algorithms as applied to robotic navigation. Proposed improvements include: addition of a forgetting mechanism, use of feature based state inputs, and hierarchical structuring of an RL agent. Simulations were performed to evaluate the individual merits and flaws of each proposal, to compare proposed methods to prior established methods, and to compare proposed methods to theoretically optimal solutions. Incorporation of a forgetting mechanism did considerably improve the learning times of RL agents in a dynamic environment. However, direct implementation of a feature-based RL agent did not result in any performance enhancements, as pure feature-based navigation results in a lack of positional awareness, and the inability of the agent to determine the location of the goal state. Inclusion of a hierarchical structure in an RL agent resulted in significantly improved performance, specifically when one layer of the hierarchy included a feature-based agent for obstacle avoidance, and a standard RL agent for global navigation. In summary, the inclusion of a forgetting mechanism, and the use of a hierarchically structured RL agent offer substantially increased performance when compared to traditional RL agents navigating in a dynamic environment.
               ",autonomous vehicle
10.1016/j.buildenv.2020.106786,journal,Building and Environment,sciencedirect,2020-05-31,sciencedirect,A review on cooling performance enhancement for phase change materials integrated systems—flexible design and smart control with machine learning applications,https://api.elsevier.com/content/article/pii/S036013232030144X,"
                  Climate-adaptive design, smart control, latent thermal storages, multi-dimensional uncertainty analysis, and multi-objective optimisations are effective solutions for cooling performance enhancement of buildings through integrated techniques, such as hybrid ventilations, nocturnal sky radiation, radiative cooling and active PV cooling for the self-consumption. However, there is no systematic and in-depth analysis on this topic in the academia. In this study, a state-of-the-art review on novel PCMs based strategies to reduce cooling load of buildings has been presented. The investigated strategies include the structural configuration, systematic control and the multi-criteria for assessment. The roles of ventilations, radiative cooling and the underlying heat transfer mechanism have been characterized for the in-depth understanding. In order to realise the multivariable optimal design and robust operations under multi-level scenario uncertainties, parametric and uncertainty analysis, single- and multi-objective optimisations have been comprehensively reviewed, together with technical challenges for each solution. Research results show that, integrated passive and active systems with flexible transitions on operating modes are full of prospects for the multi-criteria performance improvement. Trade-off solutions along the multi-objective Pareto frontier are multi-diversified, dependent on the adopted approach and the studied scenario. Furthermore, machine learning methods are promising for the thermal and energy performances improvement, through the surrogate model development, the model predictive control and the optimisation function. Future studies and prospects have been demonstrated as avenues for future research. This study presents a systematic overview on novel PCMs based strategies, together with the application of machine-learning methods for cooling performance enhancement, which are critical for the promotion of novel PCMs based cooling strategies in buildings.
               ",autonomous vehicle
10.1016/j.ijinfomgt.2019.08.002,journal,International Journal of Information Management,sciencedirect,2021-04-30,sciencedirect,"Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy",https://api.elsevier.com/content/article/pii/S026840121930917X,"
                  As far back as the industrial revolution, significant development in technical innovation has succeeded in transforming numerous manual tasks and processes that had been in existence for decades where humans had reached the limits of physical capacity. Artificial Intelligence (AI) offers this same transformative potential for the augmentation and potential replacement of human tasks and activities within a wide range of industrial, intellectual and social applications. The pace of change for this new AI technological age is staggering, with new breakthroughs in algorithmic machine learning and autonomous decision-making, engendering new opportunities for continued innovation. The impact of AI could be significant, with industries ranging from: finance, healthcare, manufacturing, retail, supply chain, logistics and utilities, all potentially disrupted by the onset of AI technologies. The study brings together the collective insight from a number of leading expert contributors to highlight the significant opportunities, realistic assessment of impact, challenges and potential research agenda posed by the rapid emergence of AI within a number of domains: business and management, government, public sector, and science and technology. This research offers significant and timely insight to AI technology and its impact on the future of industry and society in general, whilst recognising the societal and industrial influence on pace and direction of AI development.
               ",autonomous vehicle
10.1016/B978-0-12-817665-8.00024-2,journal,Hydraulic Fracturing in Unconventional Reservoirs,sciencedirect,2019-12-31,sciencedirect,Chapter Twenty-Four: Application of machine learning in hydraulic fracture optimization,https://api.elsevier.com/content/article/pii/B9780128176658000242,"
               The subject of artificial intelligence (AI) in general and application of machine learning (ML) has gained lots of popularity in the past few years throughout various industries. This rise in popularity is due to new technologies such as sensors and high-performance computing services (e.g., Apache Hadoop, NoSQL, etc.) that enable big-data acquisition and storage in different fields of study. Big data refers to a quantity of data that is too large to be handled (i.e., gathered, stored, and analyzed) using common tools and techniques, for example, Terabytes of data. In the oil and gas industry, in addition to pressure, rate, and surface and downhole seismic measurements, we are now able to collect information using fiber optics that provide high-resolution temperature and acoustic measurements in time and space. The oil and gas industry has also collected large amounts of data corresponding to evaluation, drilling, completion, stimulation, and operation of the wells. This valuable and expensive data has not been studied and analyzed in detail, simply due to the lack of knowledge and the complexity of the data collected. The application of AI in the oil and gas industry, using different data mining and ML techniques, has enabled us to use this information not only to optimize drilling, completions, stimulation, and operation procedures but also to make real-time decisions to avoid any failure or malfunction, that is, real-time operation center or RTOC. The application of AI will empower our industry to take advantage of new technologies developed in industrial monitoring systems such as sensor technologies, high-performance computing, and use our current and previously collected data to increase the NPV of different projects.
            ",autonomous vehicle
10.1016/j.neucom.2019.09.044,journal,Neurocomputing,sciencedirect,2020-01-29,sciencedirect,"BreakHis based breast cancer automatic diagnosis using deep learning: Taxonomy, survey and insights",https://api.elsevier.com/content/article/pii/S0925231219313128,"
                  There are several breast cancer datasets for building Computer Aided Diagnosis systems (CADs) using either deep learning or traditional models. However, most of these datasets impose various trade-offs on practitioners related to their availability or inner clinical value. Recently, a public dataset called BreakHis has been released to overcome these limitations. BreakHis is organized into four magnification levels, each image is labeled according to its main category (Benign/Malignant) and its subcategory (A/F/PT/TA/PC/DC/LC/MC). This organization allows practitioners to address this problem either as a binary or a multi-category classification task with either a magnification dependent or independent training approach. In this work, we define a taxonomy that categorize this problem into four different reformulations: Magnification-Specific Binary (MSB), Magnification-Independent Binary (MIB), Magnification-Specific Multi-category (MSM) and Magnification-Independent Multi-category (MIM) classifications. We provide a comprehensive survey of all related works. We identify the best reformulation from clinical and practical standpoints. Finally, we explore for the first time the MIM approach using deep learning and draw the learnt lessons.
               ",autonomous vehicle
10.1016/B978-0-12-818803-3.00010-6,journal,Machine Learning,sciencedirect,2020-12-31,sciencedirect,Chapter 1: Introduction,https://api.elsevier.com/content/article/pii/B9780128188033000106,"
               
                  This chapter serves as an introduction to the book and an overview of machine learning. It starts with a historical framework of what is known as the fourth industrial revolution and the role of automation and learning from data as one of its driving forces. A short discussion related to the terms of “machine learning” and “artificial intelligence” is presented. Some notable typical application areas of machine learning are listed and are briefly discussed together with some of the future challenges. In the sequel, supervised learning is introduced via its two major pillars, i.e., the classification and regression tasks. Definitions of unsupervised and semisupervised learning are briefly presented. The chapter also outlines the structure of the book and provides a road map for students and instructors. A summary of each of the subsequent chapters is provided. Finally, suggestions are offered on which chapters to be covered during a course depending on its specific focus and the students' background.
            ",autonomous vehicle
10.1016/j.gie.2019.12.018,journal,Gastrointestinal Endoscopy,sciencedirect,2020-04-30,sciencedirect,Artificial intelligence in endoscopy,https://api.elsevier.com/content/article/pii/S001651071932560X,,autonomous vehicle
10.1016/j.knosys.2004.03.012,journal,Knowledge-Based Systems,sciencedirect,2004-05-31,sciencedirect,Robot docking with neural vision and reinforcement,https://api.elsevier.com/content/article/pii/S0950705104000139,"
                  We present a solution for robotic docking, i.e. approach of a robot toward a table so that it can grasp an object. One constraint is that our PeopleBot robot has a short non-extendable gripper and wide ‘shoulders’. Therefore, it must approach the table at a perpendicular angle so that the gripper can reach over it. Another constraint is the use of vision to locate the object. Only the angle is supplied as additional input. We present a solution based solely on neural networks: object recognition and localisation is trained, motivated by insights from the lower visual system. Based on the hereby obtained perceived location, we train a value function unit and four motor units via reinforcement learning. After training the robot can approach the table at the correct position and in a perpendicular angle. This is to be used as part of a bigger system where the robot acts according to verbal instructions based on multi-modal neuronal representations as found in language and motor cortex (mirror neurons).
               ",autonomous vehicle
10.1016/j.dcan.2017.10.002,journal,Digital Communications and Networks,sciencedirect,2018-08-31,sciencedirect,Machine learning for internet of things data analysis: a survey,https://api.elsevier.com/content/article/pii/S235286481730247X,"Rapid developments in hardware, software, and communication technologies have facilitated the emergence of Internet-connected sensory devices that provide observations and data measurements from the physical world. By 2020, it is estimated that the total number of Internet-connected devices being used will be between 25 and 50 billion. As these numbers grow and technologies become more mature, the volume of data being published will increase. The technology of Internet-connected devices, referred to as Internet of Things (IoT), continues to extend the current Internet by providing connectivity and interactions between the physical and cyber worlds. In addition to an increased volume, the IoT generates big data characterized by its velocity in terms of time and location dependency, with a variety of multiple modalities and varying data quality. Intelligent processing and analysis of this big data are the key to developing smart IoT applications. This article assesses the various machine learning methods that deal with the challenges presented by IoT data by considering smart cities as the main use case. The key contribution of this study is the presentation of a taxonomy of machine learning algorithms explaining how different techniques are applied to the data in order to extract higher level information. The potential and challenges of machine learning for IoT data analytics will also be discussed. A use case of applying a Support Vector Machine (SVM) to Aarhus smart city traffic data is presented for a more detailed exploration.",autonomous vehicle
10.1016/j.jisa.2020.102647,journal,Journal of Information Security and Applications,sciencedirect,2020-12-31,sciencedirect,Adaptable feature-selecting and threshold-moving complete autoencoder for DDoS flood attack mitigation,https://api.elsevier.com/content/article/pii/S2214212620308061,"DDoS attacks remain one of the top cyber threats targeting the financial, health care, retail, gaming, and political sectors, which affects Internet service disruption, data or monetary loss. Security experts have predicted that the development of 5G technology will increase the frequency and the vector of DDoS attacks. Moreover, enhanced DDoS attack technology utilises artificial intelligence [1], which will escalate the level of difficulty to identify malicious traffic correctly to mitigate the attack effectively. The Internet service provider (ISP) is the connector between the users and the Internet. Deploying DDoS mitigation systems within the ISP domain can offer an efficient solution. Therefore, we propose a dynamic learning system (DLS) for the ISP. The DLS is an unsupervised ensemble model using the Complete Autoencoder (CA) as base learners to classify network traffic. The utmost difference between the CA and the regular Autoencoder is that the CA exploits the imbalanced characteristic of the attack data to generate a binary classification via a class switch. When the predicted number of normal IP addresses is over 50% of the total IP addresses, the CA swaps the class of the IP addresses. The CA is directed by a reference object (RO), which is either a reference limit or the mean of a reference error function ( R L 1 ¯ ), to furnish the automation to the DLS. The DLS was trained with a TCP-ICMP flood attack and tested with a UDP-TCP and a UDP-TCP-ICMP flood attack data set. The average Recall, Precision and F1 Score are all above 0.97. Additionally, the DLS outperformed the K-means and the Self-Organising Map models on a UDP flood attack data set.",autonomous vehicle
10.1016/j.engeos.2021.01.001,journal,Energy Geoscience,sciencedirect,2021-07-31,sciencedirect,"Feedforward Neural Network for joint inversion of geophysical data to identify geothermal sweet spots in Gandhar, Gujarat, India",https://api.elsevier.com/content/article/pii/S2666759221000019,"Artificial Neural Networks (ANNs) are used in numerous engineering and scientific disciplines as an automated approach to resolve a number of problems. However, to build an artificial neural network that is prudent enough to rely on, vast quantities of relevant data have to be fed. In this study, we analysed the scope of artificial neural networks in geothermal reservoir architecture. In particular, we attempted to solve joint inversion problem through Feedforward Neural Network (FNN) technique. In order to identify geothermal sweet spots in the subsurface, an extensive geophysical studies were conducted in Gandhar area of Gujarat, India. The data were acquired along six profile lines for gravity, magnetics and magnetotellurics. Initially low velocity zone was identified using refraction seismic technique in order to set a common datum level for other potential data. The depth of low velocity zone in Gandhar was identified at 11 m. The FNN backpropagation method was applied to gain the global minima of the data space and model space as desired. The input dataset fed to the inversion algorithm in the form of gravity, magnetic susceptibility and resistivity helped to predict the suitable model after network training in multiple steps. The joint inversion of data is conducive to understanding the subsurface geological and lithological features along with probable geothermal sweet spots. The results of this study show the geothermal sweet spots at depth ranging from 200 m to 300 m. The results from our study can be used for targeted zones for geothermal water exploitation.",autonomous vehicle
10.1016/j.comcom.2018.07.015,journal,Computer Communications,sciencedirect,2018-09-30,sciencedirect,From 4G to 5G: Self-organized network management meets machine learning,https://api.elsevier.com/content/article/pii/S0140366418300380,"
                  Self-organization as applied to cellular networks is usually referred to Selforganizing Networks (SONs), and it is a key driver for improving Operations,Administration, and Management (OAM) activities. SON aims at reducing the cost of installation and management of 4G and future 5G networks, by simplifying operational tasks through the capability to configure, optimize and heal itself. To satisfy 5G network management requirements, this autonomous management vision has to be extended to the end to end network. In literature and also in some instances of products available in the market, Machine Learning (ML) has been identified as the key tool to implement autonomous adaptability and take advantage of experience when making decisions. In this paper, we survey how 5G network management, with an end-to-end perspective of the network, can significantly benefit from ML solutions. We review and provide the basic concepts and taxonomy for SON, network management and ML. We analyze the available state of the art in the literature, standardization, and in the market. We pay special attention to 3rd Generation Partnership Project (3GPP) evolution in the area of network management and to the data that can be extracted from 3GPP networks, in order to gain knowledge and experience in how the network is working, and improve network performance in a proactive way. Finally, we go through the main challenges associated with this line of research, in both 4G and in what 5G is getting designed, while identifying new directions for research.
               ",autonomous vehicle
10.1016/0921-8890(94)90020-5,journal,Robotics and Autonomous Systems,sciencedirect,1994-04-30,sciencedirect,Learning locomotion reflexes: A self-supervised neural system for a mobile robot,https://api.elsevier.com/content/article/pii/0921889094900205,"
                  This article is concerned with an artificial neural system for a mobile robot reactive navigation in an unknown, cluttered environment. Reactive navigation is a process of immediately choosing locomotion actions in response to measured spatial situations, while no planning occurs. A task of a presented system is to provide a steering angle signal letting a robot reach a goal while avoiding collisions with obstacles. Basic reactive navigation methods are briefly characterized, special attention is paid to a neural approach to the considered problem. The authors describe the system's architecture and important details of the algorithm. The main parts of the system are: the Fuzzy ART neural self-organizing classifier, performing a perceptual space partitioning, and a neural associative memory, memorizing the system's experience and superposing influences of different behaviors. Tests show that the learning process, starting from zero, is efficient, despite some initial fluctuations of its effectiveness.
               ",autonomous vehicle
10.1016/j.dsx.2021.06.017,journal,Diabetes & Metabolic Syndrome: Clinical Research & Reviews,sciencedirect,2021-08-31,sciencedirect,Modern computational intelligence based drug repurposing for diabetes epidemic,https://api.elsevier.com/content/article/pii/S1871402121002009,"
                  Background and aim
                  Objectives are to explore recent advances in discovery of new antidiabetic agents using repurposing strategies and to discuss modern technologies used for drug repurposing highlighting diabetic specific web portal.
               
                  Methods
                  Recent literature were studied and analyzed from various sources such as Scopus, PubMed, and IEEE Xplore databases.
               
                  Results
                  Drugs like Niclosamideethanolamine, Methazolamide, Diacerein, Berberine, Clobetasol, etc. with possibility of repurposing to curb diabetes can be potential late-stage clinical candidates, providing access to information on pharmacology, formulation, and probable toxicity if any.
               
                  Conclusions
                  With collaboration of artificial intelligence (AI) with pharmacology, the efficiency of drug repurposing can improve significantly.
               ",autonomous vehicle
10.1016/j.dcan.2017.10.002,journal,Digital Communications and Networks,sciencedirect,2018-08-31,sciencedirect,Machine learning for internet of things data analysis: a survey,https://api.elsevier.com/content/article/pii/S235286481730247X,"Rapid developments in hardware, software, and communication technologies have facilitated the emergence of Internet-connected sensory devices that provide observations and data measurements from the physical world. By 2020, it is estimated that the total number of Internet-connected devices being used will be between 25 and 50 billion. As these numbers grow and technologies become more mature, the volume of data being published will increase. The technology of Internet-connected devices, referred to as Internet of Things (IoT), continues to extend the current Internet by providing connectivity and interactions between the physical and cyber worlds. In addition to an increased volume, the IoT generates big data characterized by its velocity in terms of time and location dependency, with a variety of multiple modalities and varying data quality. Intelligent processing and analysis of this big data are the key to developing smart IoT applications. This article assesses the various machine learning methods that deal with the challenges presented by IoT data by considering smart cities as the main use case. The key contribution of this study is the presentation of a taxonomy of machine learning algorithms explaining how different techniques are applied to the data in order to extract higher level information. The potential and challenges of machine learning for IoT data analytics will also be discussed. A use case of applying a Support Vector Machine (SVM) to Aarhus smart city traffic data is presented for a more detailed exploration.",autonomous vehicle
10.1016/j.engappai.2010.06.007,journal,Engineering Applications of Artificial Intelligence,sciencedirect,2010-12-31,sciencedirect,Artificial immune classifier with swarm learning,https://api.elsevier.com/content/article/pii/S0952197610001302,"
                  Artificial immune systems are computational systems inspired by the principles and processes of the natural immune system. The various applications of artificial immune systems have been used for pattern recognition and classification problems; however, these artificial immune systems have three major problems, which are growing of the memory cell population, eliminating of the useful memory cells in next the steps, and randomly using cloning and mutation operators. In this study, a new artificial immune classifier with swarm learning is proposed to solve these three problems. The proposed algorithm uses the swarm learning to evolve the antibody population. In each step, the antibodies that belong to the same class move to the same way according to their affinities. The size of the memory cell population does not grow during the training stage of the algorithm. Therefore, the method is faster than other artificial immune classifiers. The classifier was tested on two case studies. In the first case study, the algorithm was used to diagnose the faults of induction motors. In the second case study, five benchmark data sets were used to evaluate the performance of the algorithm. The results of second case studies show that the proposed method gives better results than two well-known artificial immune systems for real word data sets. The results were compared to other classification techniques, and the method is competitive to other classifiers.
               ",autonomous vehicle
10.1016/j.gltp.2021.01.002,journal,Global Transitions Proceedings,sciencedirect,2021-06-30,sciencedirect,Development of classification system for LULC using remote sensing and GIS,https://api.elsevier.com/content/article/pii/S2666285X21000029,This article demonstrations the techniques for land classification and development stages that began in 1950 and till now. It highlights the findings of the research efforts from 220 studies that worked in this domain. The land classification was manual till classification processes evolved into numerical and digital with the emergence of technology and the revolution in Artificial Intelligence algorithms. It included an inventory of all the methods traditional and recent used in land classification. Most land use and land cover classification classifiers have been comparing to determine the best classifiers and the characteristics of each to determine points that will help develop classification accuracy. This article will be significant for the upcoming researchers to understand the land classification and various techniques. It will help determine the efficient classifier and motivates to development of new classifiers.,autonomous vehicle
10.1016/j.compeleceng.2011.03.010,journal,Computers & Electrical Engineering,sciencedirect,2011-05-31,sciencedirect,An educational tool for artificial neural networks,https://api.elsevier.com/content/article/pii/S0045790611000371,"
                  Artificial neural networks are some kind of data processing systems, which try to simulate features of the human brain and its learning process. So, they are widely used by researchers to solve different problems in optimization, classification, pattern recognition, associative memory and control. In this paper, an educational tool, which can be used to work on different kinds of neural network models and learn fundamentals of the artificial neural network, is described. At this point, the whole tool environment provides an advanced system to ensure mentioned functions. The developed system supports using MLP, LVQ and SOM models and related learning algorithms. It employs some visual, interactive tools, which enable users to compose their own neural networks and work on the developed networks easily. By using these tools, users can also understand and learn working mechanism of a typical artificial neural network, using features of different models and related learning algorithms.
               ",autonomous vehicle
10.1016/j.neucom.2021.08.131,journal,Neurocomputing,sciencedirect,2021-11-20,sciencedirect,"Towards information-rich, logical dialogue systems with knowledge-enhanced neural models",https://api.elsevier.com/content/article/pii/S0925231221013424,"
                  Dialogue systems have made massive promising progress contributed by deep learning techniques and have been widely applied in our life. However, existing end-to-end neural models suffer from the problem of tending to generate uninformative and generic responses because they cannot ground dialogue context with background knowledge. In order to solve this problem, many researchers begin to consider combining external knowledge in dialogue systems, namely knowledge-enhanced dialogue systems. The challenges of knowledge-enhanced dialogue systems include how to select the appropriate knowledge from large-scale knowledge bases, how to read and understand extracted knowledge, and how to integrate knowledge into responses generation process. Combined with external knowledge, dialogue systems can deeply understand the dialogue context, and generate more informative and logical responses. This survey gives a comprehensive review of knowledge-enhanced dialogue systems, summarizes research progress to solve these challenges and proposes some open issues and research directions.
               ",autonomous vehicle
10.1016/j.neucom.2019.02.047,journal,Neurocomputing,sciencedirect,2019-05-07,sciencedirect,Deep Radial Intelligence with Cumulative Incarnation approach for detecting Denial of Service attacks,https://api.elsevier.com/content/article/pii/S0925231219302711,"
                  Nowadays Denial of Service (DoS) attack is one of the security threats that make the online services unavailable for legitimate users. Therefore, a DoS attack detection system is needed to protect online services against malicious activities. Machine learning approaches are widely used to detect cyber-attacks. The lacunae in the existing machine learning based attack detection systems are more learning time due to vanishing gradient and getting stuck in the local minima due to selection of random weights. In this paper, these issues have been addressed and Deep Radial Intelligence (DeeRaI) with Cumulative Incarnation (CuI) approach is proposed to detect the DoS attacks. The proposed DeeRaI approach learns the intelligence extracted from the radial basis function with different levels of abstraction. The proposed CuI optimizes the weights of the DeeRaI network in which the knowledge gained is progressed to the next generation. Experiments were conducted on benchmark datasets and the proposed approach is compared with existing classifiers and state-of-the-art attack detection systems. It is seen from the performance evaluation that the proposed approach gives promising results than the other existing approaches. Further, it is evident that the proposed approach converges faster and provides best weights compared to the existing optimization methods.
               ",autonomous vehicle
10.1016/j.giq.2021.101577,journal,Government Information Quarterly,sciencedirect,2021-07-31,sciencedirect,Implications of the use of artificial intelligence in public governance: A systematic literature review and a research agenda,https://api.elsevier.com/content/article/pii/S0740624X21000137,"To lay the foundation for the special issue that this research article introduces, we present 1) a systematic review of existing literature on the implications of the use of Artificial Intelligence (AI) in public governance and 2) develop a research agenda. First, an assessment based on 26 articles on this topic reveals much exploratory, conceptual, qualitative, and practice-driven research in studies reflecting the increasing complexities of using AI in government – and the resulting implications, opportunities, and risks thereof for public governance. Second, based on both the literature review and the analysis of articles included in this special issue, we propose a research agenda comprising eight process-related recommendations and seven content-related recommendations. Process-wise, future research on the implications of the use of AI for public governance should move towards more public sector-focused, empirical, multidisciplinary, and explanatory research while focusing more on specific forms of AI rather than AI in general. Content-wise, our research agenda calls for the development of solid, multidisciplinary, theoretical foundations for the use of AI for public governance, as well as investigations of effective implementation, engagement, and communication plans for government strategies on AI use in the public sector. Finally, the research agenda calls for research into managing the risks of AI use in the public sector, governance modes possible for AI use in the public sector, performance and impact measurement of AI use in government, and impact evaluation of scaling-up AI usage in the public sector.",autonomous vehicle
10.1016/j.engappai.2019.103312,journal,Engineering Applications of Artificial Intelligence,sciencedirect,2020-01-31,sciencedirect,The application of machine learning techniques for driving behavior analysis: A conceptual framework and a systematic literature review,https://api.elsevier.com/content/article/pii/S0952197619302672,"
                  Driving Behavior (DB) is a complex concept describing how the driver operates the vehicle in the context of the driving scene and surrounding environment. Recently, DB assessment has become an emerging topic of great importance. However, in view of to the stochastic nature of driving, measuring and modeling, DB continues to be a challenging topic today. As such, this paper argues that to move forward in understanding the individual and organizational mechanisms influencing DB, a conceptual framework is outlined whereby DB is viewed in terms of different dimensions established within the Driver–Vehicle–Environment (DVE) system. Moreover, DB assessment has been approached by various machine learning (ML) models. Still, there has been no attempt to analyze the empirical evidence on ML models in a systematic way, furthermore, ML based DB models often face problems and raise questions that must be resolved. This article presents a systematic literature review (SLR) of the DB investigation concept; In the first phase, a framework for conceptualizing a holistic approach of the different facets in DB analysis is presented, as well as a scheme to guide the future development and implementation of DB assessment strategies. In the second phase, an overview of the literature on ML is designed, revealing a premier and unbiased survey of the existing empirical research of ML techniques that have been applied to DB analysis. The results of this study identify an interpretive framework incorporating multiple dimensions influencing the driver’s conduct, in an attempt to achieve a thorough understanding of the DB concept within the DVE system in which the drivers operate. Additionally, 82 primary studies published during the last decade and eight broadly used ML models were identified. The findings of this review prove the performance capability of the ML techniques for assessing DB. The models using the ML techniques outperform other conventional approaches. However, the application of ML models in DB analysis is still limited and more effort is needed to obtain well-formed and generalizable results. To this end, and based on the outcomes obtained in this work, future guidelines have been provided to practitioners and researchers to grasp the major contributions and challenges in the state-of-the-art research.
               ",autonomous vehicle
10.1016/j.compbiomed.2021.104365,journal,Computers in Biology and Medicine,sciencedirect,2021-06-30,sciencedirect,Precision nutrition: A systematic literature review,https://api.elsevier.com/content/article/pii/S0010482521001591,"Precision Nutrition research aims to use personal information about individuals or groups of individuals to deliver nutritional advice that, theoretically, would be more suitable than generic advice. Machine learning, a subbranch of Artificial Intelligence, has promise to aid in the development of predictive models that are suitable for Precision Nutrition. As such, recent research has applied machine learning algorithms, tools, and techniques in precision nutrition for different purposes. However, a systematic overview of the state-of-the-art on the use of machine learning in Precision Nutrition is lacking. Therefore, we carried out a Systematic Literature Review (SLR) to provide an overview of where and how machine learning has been used in Precision Nutrition from various aspects, what such machine learning models use as input features, what the availability status of the data used in the literature is, and how the models are evaluated. Nine research questions were defined in this study. We retrieved 4930 papers from electronic databases and 60 primary studies were selected to respond to the research questions. All of the selected primary studies were also briefly discussed in this article. Our results show that fifteen problems spread across seven domains of nutrition and health are present. Four machine learning tasks are seen in the form of regression, classification, recommendation and clustering, with most of these utilizing a supervised approach. In total, 30 algorithms were used, with 19 appearing more than once. Models were through the use of four groups of approaches and 23 evaluation metrics. Personalized approaches are promising to reduce the burden of these current problems in nutrition research, and the current review shows Machine Learning can be incorporated into Precision Nutrition research with high performance. Precision Nutrition researchers should consider incorporating Machine Learning into their methods to facilitate the integration of many complex features, allowing for the development of high-performance Precision Nutrition approaches.",autonomous vehicle
10.1016/B978-0-12-823014-5.00002-8,journal,Handbook of Deep Learning in Biomedical Engineering,sciencedirect,2021-12-31,sciencedirect,10: Deep neural network in medical image processing,https://api.elsevier.com/content/article/pii/B9780128230145000028,"
               Computer-assisted analysis for better interpretation of biomedical images has been a long-standing concern in the field of medical imaging. On the image interpretation front, recent developments in machine learning, especially in the field of deep learning, have made a major leap forward to help with the identification, classification, and quantification of patterns in biomedical images. Specifically, the leveraging of hierarchical structures based entirely on data, instead of handcrafted features mostly based on domain-specific knowledge, lies at the heart of the advances. In this way, deep learning is rapidly proving to be the state-of-the-art platform for achieving superior performance in various biomedical applications. The most important point in efficacious treatment of a disease is an accurate diagnosis, which by and large is not an easy feat to achieve. There are many reasons for an inaccurate diagnosis such as inexperienced doctors, scarcity of trained radiologists, lack of proper equipment, and so on. But technology (specifically machine learning) provides a silver lining, as it has been proven quite successful in aiding and assisting diagnosis by interpreting biomedical images such as X-ray, computerized tomography scans, mammograms, and so on. The unprecedented success of deep learning frameworks in the interpretation of biomedical images is mainly due to the following factors:
               •The progress of high-tech central processing units, graphics processing units, and most recently tensor processing units.
               •The availability of a large amount of data
               •Advances in deep learning algorithms
               There has been tremendous interest in the use of image processing and analysis techniques for computer-aided detection (CAD)/diagnostics (CADx) in biomedical images. The goal has been to increase diagnostic accuracy as well as the reproducibility of biomedical image interpretation.
               CAD/CADx systems can aid radiologists/doctors by providing a second opinion and may be used in the first stage of examination in the near future, providing the reduction of the variability among radiologists in the interpretation of biomedical images.
               In this chapter, we aim to discuss fundamentals of image processing first including segmentation and edge detection followed by identifying critical areas in biomedical images, denoising, and applying image processing technique on various available biomedical image data sets. The authors focus on tissue segmentation, application of convolutional neural network in interpreting biomedical images, usage of different deep learning libraries for identifying areas of interest in a biomedical image, computer-aided disease diagnosis or prognosis, and so on. We will conclude by raising research issues and suggesting future directions for further improvements.
            ",autonomous vehicle
10.1016/j.ymeth.2018.05.015,journal,Methods,sciencedirect,2018-12-01,sciencedirect,"m-Health 2.0: New perspectives on mobile health, machine learning and big data analytics",https://api.elsevier.com/content/article/pii/S1046202318300860,"
                  Mobile health (m-Health) has been repeatedly called the biggest technological breakthrough of our modern times. Similarly, the concept of big data in the context of healthcare is considered one of the transformative drivers for intelligent healthcare delivery systems. In recent years, big data has become increasingly synonymous with mobile health, however key challenges of ‘Big Data and mobile health’, remain largely untackled. This is becoming particularly important with the continued deluge of the structured and unstructured data sets generated on daily basis from the proliferation of mobile health applications within different healthcare systems and products globally.
                  The aim of this paper is of twofold. First we present the relevant big data issues from the mobile health (m-Health) perspective. In particular we discuss these issues from the technological areas and building blocks (communications, sensors and computing) of mobile health and the newly defined (m-Health 2.0) concept. The second objective is to present the relevant rapprochement issues of big m-Health data analytics with m-Health. Further, we also present the current and future roles of machine and deep learning within the current smart phone centric m-health model.
                  The critical balance between these two important areas will depend on how different stakeholder from patients, clinicians, healthcare providers, medical and m-health market businesses and regulators will perceive these developments. These new perspectives are essential for better understanding the fine balance between the new insights of how intelligent and connected the future mobile health systems will look like and the inherent risks and clinical complexities associated with the big data sets and analytical tools used in these systems. These topics will be subject for extensive work and investigations in the foreseeable future for the areas of data analytics, computational and artificial intelligence methods applied for mobile health.
               ",autonomous vehicle
10.1016/j.compbiomed.2020.104043,journal,Computers in Biology and Medicine,sciencedirect,2020-11-30,sciencedirect,Artificial intelligence framework for predictive cardiovascular and stroke risk assessment models: A narrative review of integrated approaches using carotid ultrasound,https://api.elsevier.com/content/article/pii/S0010482520303747,"
                  Recent findings
                  Cardiovascular disease (CVD) is the leading cause of mortality and poses challenges for healthcare providers globally. Risk-based approaches for the management of CVD are becoming popular for recommending treatment plans for asymptomatic individuals. Several conventional predictive CVD risk models based do not provide an accurate CVD risk assessment for patients with different baseline risk profiles. Artificial intelligence (AI) algorithms have changed the landscape of CVD risk assessment and demonstrated a better performance when compared against conventional models, mainly due to its ability to handle the input nonlinear variations. Further, it has the flexibility to add risk factors derived from medical imaging modalities that image the morphology of the plaque. The integration of noninvasive carotid ultrasound image-based phenotypes with conventional risk factors in the AI framework has further provided stronger power for CVD risk prediction, so-called “integrated predictive CVD risk models.”
               
                  Purpose
                  of the review: The objective of this review is (i) to understand several aspects in the development of predictive CVD risk models, (ii) to explore current conventional predictive risk models and their successes and challenges, and (iii) to refine the search for predictive CVD risk models using noninvasive carotid ultrasound as an exemplar in the artificial intelligence-based framework.
               
                  Conclusion
                  Conventional predictive CVD risk models are suboptimal and could be improved. This review examines the potential to include more noninvasive image-based phenotypes in the CVD risk assessment using powerful AI-based strategies.
               ",autonomous vehicle
10.1016/j.neunet.2019.03.010,journal,Neural Networks,sciencedirect,2019-08-31,sciencedirect,Continuous learning in single-incremental-task scenarios,https://api.elsevier.com/content/article/pii/S0893608019300838,"
                  It was recently shown that architectural, regularization and rehearsal strategies can be used to train deep models sequentially on a number of disjoint tasks without forgetting previously acquired knowledge. However, these strategies are still unsatisfactory if the tasks are not disjoint but constitute a single incremental task (e.g., class-incremental learning). In this paper we point out the differences between multi-task and single-incremental-task scenarios and show that well-known approaches such as LWF, EWC and SI are not ideal for incremental task scenarios. A new approach, denoted as AR1, combining architectural and regularization strategies is then specifically proposed. AR1 overhead (in terms of memory and computation) is very small thus making it suitable for online learning. When tested on CORe50 and iCIFAR-100, AR1 outperformed existing regularization strategies by a good margin.
               ",autonomous vehicle
10.1016/j.accpm.2018.09.008,journal,Anaesthesia Critical Care & Pain Medicine,sciencedirect,2019-08-31,sciencedirect,Big data and targeted machine learning in action to assist medical decision in the ICU,https://api.elsevier.com/content/article/pii/S2352556818302169,"
                  Historically, personalised medicine has been synonymous with pharmacogenomics and oncology. We argue for a new framework for personalised medicine analytics that capitalises on more detailed patient-level data and leverages recent advances in causal inference and machine learning tailored towards decision support applicable to critically ill patients. We discuss how advances in data technology and statistics are providing new opportunities for asking more targeted questions regarding patient treatment, and how this can be applied in the intensive care unit to better predict patient-centred outcomes, help in the discovery of new treatment regimens associated with improved outcomes, and ultimately how these rules can be learned in real-time for the patient.
               ",autonomous vehicle
10.1016/B978-012526430-3/50003-9,journal,Neural Systems for Control,sciencedirect,1997-12-31,sciencedirect,Chapter 2: Reinforcement Learning,https://api.elsevier.com/content/article/pii/B9780125264303500039,"
               
                  Reinforcement learning refers to ways of improving performance through trial-and-error experience. Despite recent progress in developing artificial learning systems, including new learning methods for artificial neural networks, most of these systems learn under the tutelage of a knowledgeable “teacher” able to tell them how to respond to a set of training stimuli. But systems restricted to learning under these conditions are not adequate when it is costly, or even impossible, to obtain the required training examples. Reinforcement learning allows autonomous systems to learn from their experiences instead of exclusively from knowledgeable teachers. Although its roots are in experimental psychology, this chapter provides an overview of modern reinforcement learning research directed toward developing capable artificial learning systems.
            ",autonomous vehicle
10.1016/j.conb.2019.05.002,journal,Current Opinion in Neurobiology,sciencedirect,2019-04-30,sciencedirect,"Editorial overview: Machine learning, big data, and neuroscience",https://api.elsevier.com/content/article/pii/S0959438819300522,,autonomous vehicle
10.1016/B978-0-12-818803-3.00030-1,journal,Machine Learning,sciencedirect,2020-12-31,sciencedirect,Chapter 18: Neural Networks and Deep Learning,https://api.elsevier.com/content/article/pii/B9780128188033000301,"
               
                  This chapter deals with neural networks, starting from the early days of the perceptron and perceptron rule, and then moves on to the multilayer feed-forward neural networks and their training via the backpropagation algorithmic concept. A number of algorithms are discussed, starting from the basic gradient descent scheme up to very recently proposed popular variants. The palette of possible nonlinearities, including the ReLU, is presented and the effects of the choice of the nonlinearity on the convergence of the training algorithm are discussed, also in relation to the cost function that is selected. Regularization techniques, including the dropout method, are presented. In the sequel, convolutional neural networks (CNNs) and recurrent neural networks (RNNs) are introduced in some depth, alongside some very popular deep networks that are currently in use. In the sequel, generative networks are presented, starting from the more classical models based on Boltzmann machines and deep belief networks and moving on to the more recent advances including variational autoencoders and generative adversarial networks (GANs). Finally, capsule networks are introduced and discussed. A case study from the field of natural language processing (NLP) completes the chapter.
            ",autonomous vehicle
10.1016/B978-0-12-821092-5.00004-8,journal,Applications of Artificial Intelligence in Process Systems Engineering,sciencedirect,2021-12-31,sciencedirect,Chapter 12: Integrated machine learning framework for computer-aided chemical product design,https://api.elsevier.com/content/article/pii/B9780128210925000048,"
               Chemical industry is continuously looking for opportunities to manufacture the necessary commodity chemicals as well as to convert them into higher value-added chemicals-based products. Current global business environment encourages a short time-to-market for any potential chemical product. With the help of computer-aided product design methods, the R&D cycle of new products is able to be shortened, the financial resource as well as manpower can be saved. Toward current challenges existing in chemical product design methods, machine learning (ML) is regarded as a promising solution technique for computer-aided property prediction and product design. In this chapter, first, the integrated ML framework for computer-aided molecular design (CAMD) is presented. Then, the ML-CAMD framework is discussed in detail for the establishment of ML models for property prediction as well as chemical product design. Finally, two case studies are presented for the application of the proposed framework.
            ",autonomous vehicle
10.1016/j.advwatres.2021.104064,journal,Advances in Water Resources,sciencedirect,2021-12-31,sciencedirect,A hybrid of statistical and conditional generative adversarial neural network approaches for reconstruction of 3D porous media (ST-CGAN),https://api.elsevier.com/content/article/pii/S0309170821002177,"
                  A coupled statistical and conditional generative adversarial neural network is used for 3D reconstruction of both homogeneous and heterogeneous porous media from a single two-dimensional image. A statistical approach feeds the deep network with conditional data, and then the reconstruction is trained on a deep generative network. The conditional nature of the generative model helps in network stability and convergence which has been optimized through a gradient-descent-based optimization method. Moreover, this coupled approach allows the reconstruction of heterogeneous samples, a critical and serious challenge in conventional reconstruction methods. The main contribution of this work is to develop an adaptable framework that can efficiently reconstitute heterogeneous porous media using the power of conditional generative adversarial networks. The reconstruction time is accelerated approximately 1000-fold compared to traditional statistical reconstruction methods. Various matching criteria in both morphological and physical characteristics are used to evaluate the model performance. To validate the approach, the reconstructed realizations have been compared to the models generated by a conventional 3D GAN along with a well-known statistical method. The results confirm that the proposed approach is a reliable framework for extracting information from a single 2D image to reconstruct 3D microstructures.
               ",autonomous vehicle
10.1016/j.jmsy.2021.07.021,journal,Journal of Manufacturing Systems,sciencedirect,2021-07-31,sciencedirect,Artificial intelligence for throughput bottleneck analysis – State-of-the-art and future directions,https://api.elsevier.com/content/article/pii/S0278612521001588,"Identifying, and eventually eliminating throughput bottlenecks, is a key means to increase throughput and productivity in production systems. In the real world, however, eliminating throughput bottlenecks is a challenge. This is due to the landscape of complex factory dynamics, with several hundred machines operating at any given time. Academic researchers have tried to develop tools to help identify and eliminate throughput bottlenecks. Historically, research efforts have focused on developing analytical and discrete event simulation modelling approaches to identify throughput bottlenecks in production systems. However, with the rise of industrial digitalisation and artificial intelligence (AI), academic researchers explored different ways in which AI might be used to eliminate throughput bottlenecks, based on the vast amounts of digital shop floor data. By conducting a systematic literature review, this paper aims to present state-of-the-art research efforts into the use of AI for throughput bottleneck analysis. To make the work of the academic AI solutions more accessible to practitioners, the research efforts are classified into four categories: (1) identify, (2) diagnose, (3) predict and (4) prescribe. This was inspired by real-world throughput bottleneck management practice. The categories, identify and diagnose focus on analysing historical throughput bottlenecks, whereas predict and prescribe focus on analysing future throughput bottlenecks. This paper also provides future research topics and practical recommendations which may help to further push the boundaries of the theoretical and practical use of AI in throughput bottleneck analysis.",autonomous vehicle
10.1016/j.comcom.2020.08.024,journal,Computer Communications,sciencedirect,2020-10-01,sciencedirect,A secure edge monitoring approach to unsupervised energy disaggregation using mean shift algorithm in residential buildings,https://api.elsevier.com/content/article/pii/S0140366420319113,"
                  Compared to Intrusive Load Monitoring which uses smart power meters at each level to be monitored, Non-Intrusive Load Monitoring (NILM) is an ingenious way that relies on signal readings at a single point to deduce the share of the devices that have contributed to the overall load. This reliable technique that guarantees the safety and privacy of individual users has recently become an increasingly popular topic, as it turns out to be a major solution to assist household users in the process of obtaining details of their electricity consumption. The detailed consumption promotes better management of the electrical power on the consumer side by helping to eliminate any waste of energy. In this paper, an edge gateway has been implemented to safely monitor the overall load in a smart energy system. A load separation method has been introduced based on events detected on a low-frequency power signal, which allows the consumption profile of On/Off and multi-state devices to be generated without relying on the knowledge of the cardinality of these devices Following the extraction of significant features contained in the aggregate signal, an appliance profile recognition approach is presented based on the non-parametric Mean Shift algorithm. The ability of the proposed method to learn and deduce devices profile is validated using the Reference Energy Disaggregation Dataset (REDD). The experimental results show that the proposed approach is efficient in detecting events of binary state and finite state appliances.
               ",autonomous vehicle
10.1016/j.eswa.2019.01.015,journal,Expert Systems with Applications,sciencedirect,2019-06-01,sciencedirect,W-MetaPath2Vec: The topic-driven meta-path-based model for large-scaled content-based heterogeneous information network representation learning,https://api.elsevier.com/content/article/pii/S0957417419300156,"
                  Recently, heterogeneous network representation learning has attracted a lot of attentions due to its potential applications. Our works in this paper are concentrated on how to leverage the output of network representation learning by combining with the topic similarity between nodes in content-based heterogeneous information network (CHIN). These unique challenges come from the shortage of topic similarity evaluation between text-based nodes which limit the accuracy of the similarity search as well other network mining tasks. Moreover, the massive sizes of current real-world network also raises challenges for traditional standalone-based heterogeneous network analysis models. Different from previous network representation learning models, such as: Node2Vec or Metapath2Vec, our proposed W-MethPath2Vec model uses the topic-driven meta-path-based random walk mechanism for generating heterogeneous neighborhood of nodes as the learning features. Then, these learning nodes’ features are used to train the learning model which is used for solving various heterogeneous network mining tasks such as: node similarity search, clustering, classification, link prediction, etc. The W-MethPath2Vec model enables the simultaneous modeling of structural and topic correlations between nodes in heterogeneous networks. Moreover, the W-MethPath2Vec model is implemented in the Apache Spark-based distributed framework which enables the capability of handling large-scaled networks. We tested our W-MethPath2Vec model with the previous state-of-the-art approaches in the real-world datasets to demonstrate the effectiveness of our proposed model.
               ",autonomous vehicle
10.1016/j.drudis.2018.11.014,journal,Drug Discovery Today,sciencedirect,2019-03-31,sciencedirect,Artificial intelligence in drug development: present status and future prospects,https://api.elsevier.com/content/article/pii/S1359644618300916,"
                  Artificial intelligence (AI) uses personified knowledge and learns from the solutions it produces to address not only specific but also complex problems. Remarkable improvements in computational power coupled with advancements in AI technology could be utilised to revolutionise the drug development process. At present, the pharmaceutical industry is facing challenges in sustaining their drug development programmes because of increased R&D costs and reduced efficiency. In this review, we discuss the major causes of attrition rates in new drug approvals, the possible ways that AI can improve the efficiency of the drug development process and collaboration of pharmaceutical industry giants with AI-powered drug discovery firms.
               ",autonomous vehicle
10.1016/j.jclepro.2020.122574,journal,Journal of Cleaner Production,sciencedirect,2020-10-20,sciencedirect,Artificial intelligence in the design of the transitions to sustainable food systems,https://api.elsevier.com/content/article/pii/S0959652620326214,"
                  Food systems and our ability to secure food and nutrition for current and future generations is challenged by population growth, climate change, resource depletion and pollution. The current agricultural and supply chain systems are one of the main contributors to the issues. Transformational, not incremental change is needed to transition to sustainable food systems capable of feeding close to 10 billion people in less than 30 years.
                  Artificial intelligence (AI) is pervading all parts of food systems in ways that indicate transformative system changes are possible. Designers, as mediators between people, technology and the environment have a responsibility to recognise and reflect on ways AI could bring the change needed to move to sustainable food systems.
                  This literature review is situated at the intersection of Food systems, Design, Artificial Intelligence and Sustainability. The transdisciplinary approach reveals what exists across the disciplines, what can be done with AI to transition to sustainable food systems, how Design proposes to approach the change, and which ethical or philosophical considerations start to emerge. The discussion reflects on AI as a potential leverage point to bring changes in the system and on the designer's role in establishing the human-technology-environmental relationships. Further research and recommendations are provided.
               ",autonomous vehicle
10.1016/j.firesaf.2019.02.002,journal,Fire Safety Journal,sciencedirect,2019-04-30,sciencedirect,Fire resistance evaluation through artificial intelligence - A case for timber structures,https://api.elsevier.com/content/article/pii/S0379711218304302,"
                  With the ever-growing surge of new technologies, there seems to be an ongoing inertia towards integrating automation and cognition into various engineering applications. Despite a number of initiatives, and oddly enough of all civil engineering sub-disciplines, the structural fire engineering and fire safety community continues to embrace a classical stance to tackle the problem of fire. In support of growing demands to adopt performance-based solutions, this paper showcases the potential of integrating Artificial Intelligence (AI) as a unique technology to assess performance and fire resistance of structures. More specifically, this study sheds light on the proper use of AI to derive temperature-dependent material models for wood, together with simple expressions that can be used to trace thermo-structural response of timber elements/components (i.e. floor assemblies, beams, columns, and connections). These expressions comprehend the naturally complex temperature-induced physio-chemical changes to timber properties, including creep and charring, and hence do not require input of such properties nor special computing software. The outcome of this study clearly shows the merit of utilizing AI to modernize fire resistance evaluation given that the developed AI-models have high degree of perception (i.e. learn from past behaviors) and ability to improve their prediction capability through independent and unsupervised learning.
               ",autonomous vehicle
10.1016/j.knosys.2021.107568,journal,Knowledge-Based Systems,sciencedirect,2021-12-25,sciencedirect,An interpretable deep neural network for colorectal polyp diagnosis under colonoscopy,https://api.elsevier.com/content/article/pii/S0950705121008303,"
                  Colorectal cancer (CRC) is the third leading cause of cancer deaths in the world, which mostly stems from precancerous polyps. Early detection and accurate classification of polyps play a vital role in colonoscopy. It makes sense to automatically detect the polyp and give a real-time classification feedback according to popular Yamada classification guidance during colonoscopy progress. We propose an interpretable deep neural network method, called multi-task real-time deep neural network with Shapley additive explanations, for polyp detection, polyp classification and polyp segmentation under colonoscopy. To the best of our knowledge, this is the first time to perform polyp classification according to Yamada classification guidance under colonoscopy with a deep learning method. To validate the performance of our proposed method, we conduct various comparative experiments on popular CVC-CLINIC and CVC-COLON datasets. We adopt various performance indicators, including area under receiver operating characteristics curve (AUC), precision, recall, F1 score, accuracy, and mean intersection over union (mIoU). The proposed method achieves satisfactory real-time performance in terms of polyp detection module, polyp classification module and polyp segmentation module. The experimental results show the overwhelming performance of our proposed method compared with other deep learning methods. We have achieved satisfying operating efficiency and interpretable feedback to meet the requirements of the colorectal surgeon, which provides an valuable decision support and reduces the rate of missed diagnosis and misdiagnosis of polyps in the process of colonoscopy.
               ",autonomous vehicle
10.1016/j.inffus.2021.01.008,journal,Information Fusion,sciencedirect,2021-07-31,sciencedirect,Towards multi-modal causability with Graph Neural Networks enabling information fusion for explainable AI,https://api.elsevier.com/content/article/pii/S1566253521000142,"AI is remarkably successful and outperforms human experts in certain tasks, even in complex domains such as medicine. Humans on the other hand are experts at multi-modal thinking and can embed new inputs almost instantly into a conceptual knowledge space shaped by experience. In many fields the aim is to build systems capable of explaining themselves, engaging in interactive what-if questions. Such questions, called counterfactuals, are becoming important in the rising field of explainable AI (xAI). Our central hypothesis is that using conceptual knowledge as a guiding model of reality will help to train more explainable, more robust and less biased machine learning models, ideally able to learn from fewer data. One important aspect in the medical domain is that various modalities contribute to one single result. Our main question is “How can we construct a multi-modal feature representation space (spanning images, text, genomics data) using knowledge bases as an initial connector for the development of novel explanation interface techniques?”. In this paper we argue for using Graph Neural Networks as a method-of-choice, enabling information fusion for multi-modal causability (causability – not to confuse with causality – is the measurable extent to which an explanation to a human expert achieves a specified level of causal understanding). The aim of this paper is to motivate the international xAI community to further work into the fields of multi-modal embeddings and interactive explainability, to lay the foundations for effective future human–AI interfaces. We emphasize that Graph Neural Networks play a major role for multi-modal causability, since causal links between features can be defined directly using graph structures.",autonomous vehicle
10.1016/B978-0-12-823014-5.00004-1,journal,Handbook of Deep Learning in Biomedical Engineering,sciencedirect,2021-12-31,sciencedirect,5: Depression discovery in cancer communities using deep learning,https://api.elsevier.com/content/article/pii/B9780128230145000041,"
               We are said to be living in the “information age,” and data are the capital of the new economy. With the continuous explosion in the extent of data being created every day on online portals and social networking websites, industries today are collecting and analyzing more data than ever before. Data are readily available, finding valuable insights are the struggle. The easy accessibility of data, new cutting-edge technologies, and a cultural shift toward data-driven decision-making drive the demand for sentiment analysis (SA) and make it relevant in each and every domain such as politics, marketing, healthcare, and so on. In the healthcare domain, cancer is a deadly disease that claims almost 10 million lives every year. The alarming numbers of fatalities are caused due to privation of timely cancer detection, tardy medical attention, or in some cases from patients losing the will to live due to a protracted and unending treatment procedure. Governments across the world are taking steps to ensure timely cancer detection and treatment. However, little attention is being paid to the seemingly unending treatment course taking a toll on the patient's mental health, thus crushing the patient's spirit to continue. In this chapter, we investigate the use of different deep neural network architectures and natural language processing for depression detection in cancer communities. Depression detection using SA can be of great assistance to the doctors treating cancer patients and aid them in deciding whether along with the cancer treatment their patients need help from psychologists or psychiatrists.
            ",autonomous vehicle
10.1016/j.comcom.2017.08.005,journal,Computer Communications,sciencedirect,2017-11-01,sciencedirect,A reinforcement learning-based link quality estimation strategy for RPL and its impact on topology management,https://api.elsevier.com/content/article/pii/S0140366417305704,"
                  Over the last few years, standardisation efforts are consolidating the role of the Routing Protocol for Low-Power and Lossy Networks (RPL) as the standard routing protocol for IPv6-based Wireless Sensor Networks (WSNs). Although many core functionalities are well defined, others are left implementation dependent. Among them, the definition of an efficient link-quality estimation (LQE) strategy is of paramount importance, as it influences significantly both the quality of the selected network routes and nodes’ energy consumption. In this paper, we present RL-Probe, a novel strategy for link quality monitoring in RPL, which accurately measures link quality with minimal overhead and energy waste. To achieve this goal, RL-Probe leverages both synchronous and asynchronous monitoring schemes to maintain up-to-date information on link quality and to promptly react to sudden topology changes, e.g. due to mobility. Our solution relies on a reinforcement learning model to drive the monitoring procedures in order to minimise the overhead caused by active probing operations. The performance of the proposed solution is assessed by means of simulations and real experiments. Results demonstrated that RL-Probe helps in effectively improving packet loss rates, allowing nodes to promptly react to link quality variations as well as to link failures due to node mobility.
               ",autonomous vehicle
10.1016/j.radphyschem.2021.109636,journal,Radiation Physics and Chemistry,sciencedirect,2021-11-30,sciencedirect,Experimental and DBN-Based neural network extraction of radiation attenuation coefficient of dry mixture shotcrete produced using different additives,https://api.elsevier.com/content/article/pii/S0969806X21002863,"
                  In this study, the radiation attenuation coefficients (μm) of different proportions of additives were produced in dry mixture shotcrete both by experimental processes and by deep neural network based on DBN. Fly ash, silica fume, and polypropylene fiber were used as additives of dry mix shotcrete. In the first part of the two-part study, μm values were obtained from seven samples produced and a data set was created along with the input parameters of the experiment. In the second part, a model was developed for predicting the value of μm with input parameters using the DBN deep neural network Algorithm. Experimental data obtained in accordance with both applications and data generated by the Deep Belief Network (DBN) model were analyzed. As a result, the DBN model prediction μm values with an accuracy performance of 87.86%.
               ",autonomous vehicle
10.1016/j.pneurobio.2003.12.001,journal,Progress in Neurobiology,sciencedirect,2003-12-31,sciencedirect,"Information processing, dimensionality reduction and reinforcement learning in the basal ganglia",https://api.elsevier.com/content/article/pii/S0301008203001928,"
                  Modeling of the basal ganglia has played a major role in our understanding of this elusive group of nuclei. Models of the basal ganglia have undergone evolutionary and revolutionary changes over the last 20 years, as new research in the fields of anatomy, physiology and biochemistry of these nuclei has yielded new information. Early models dealt with a single pathway through the nuclei and focused on the nature of the processing performed within it, convergence of information versus parallel processing of information. Later, the Albin–DeLong “box-and-arrow” model characterized the inter-nuclei interaction as multiple pathways while maintaining a simplistic scalar representation of the nuclei themselves. This model made a breakthrough by providing key insights into the behavior of these nuclei in hypo- and hyper-kinetic movement disorders. The next generation of models elaborated the intra-nuclei interactions and focused on the role of the basal ganglia in action selection and sequence generation which form the most current consensus regarding basal ganglia function in both normal and pathological conditions. However, new findings challenge these models and point to a different neural network approach to information processing in the basal ganglia. Here, we take an in-depth look at the reinforcement driven dimensionality reduction (RDDR) model which postulates that the basal ganglia compress cortical information according to a reinforcement signal using optimal extraction methods. The model provides new insights and experimental predictions on the computational capacity of the basal ganglia and their role in health and disease.
               ",autonomous vehicle
10.1016/j.buildenv.2021.108057,journal,Building and Environment,sciencedirect,2021-10-31,sciencedirect,"An explainable one-dimensional convolutional neural networks based fault diagnosis method for building heating, ventilation and air conditioning systems",https://api.elsevier.com/content/article/pii/S0360132321004595,"
                  Due to the frequently changed outdoor weather conditions and indoor requirements, heating, ventilation and air conditioning (HVAC) experiences faulty operations inevitably throughout its lifespan. Therefore, it is important to monitor and diagnose HVAC fault operations. Recently, deep learning methods have attracted more attentions for their guarantee of better diagnosis performance under various system configurations and operating conditions. However, these methods are black-box models which though highly accurate for fault diagnosis but are extremely hard to explain. To overcome the disadvantage of poor interpretability of deep learning black-box models, this study therefore proposes a novel explainable deep learning based fault diagnosis method that is suitable for HVACs. To maintain HVAC operational information and variable locations of all chiller input data samples, proposed method is established with three characteristics: 1) the pooling layer is excluded, 2) the size of convolution filter kernel is set as 1, and 3) use softsign as an activation function. Considering the resulting impacts of HVAC faults on system operating variables, a new Absolute Gradient-weighted Class Activation Mapping (Grad-Absolute-CAM) method is proposed to visualize the fault diagnosis criteria and make the model explainable by providing the fault-discriminative information. The proposed method is validated using fault experimental dataset of a typical building HVAC system (i.e., chiller) from the ASHRAE research project 1043 (RP-1043). The fault diagnosis accuracy is over 98.5% for seven chiller faults. Results indicates that it is capable of interpreting the model work mechanism by activation feature maps and explaining the fault diagnosis criteria by Grad-Absolute-CAM.
               ",autonomous vehicle
10.1016/j.tele.2020.101525,journal,Telematics and Informatics,sciencedirect,2021-05-31,sciencedirect,Futures of artificial intelligence through technology readiness levels,https://api.elsevier.com/content/article/pii/S0736585320301842,"Artificial Intelligence (AI) offers the potential to transform our lives in radical ways. However, the main unanswered questions about this foreseen transformation are its depth, breadth and timelines. To answer them, not only do we lack the tools to determine what achievements will be attained in the near future, but we even ignore what various technologies in present-day AI are capable of. Many so-called breakthroughs in AI are associated with highly-cited research papers or good performance in some particular benchmarks. However, research breakthroughs do not directly translate into a technology that is ready to use in real-world environments. In this paper, we present a novel exemplar-based methodology to categorise and assess several AI technologies, by mapping them onto Technology Readiness Levels (TRL) (representing their depth in maturity and availability). We first interpret the nine TRLs in the context of AI, and identify several categories in AI to which they can be assigned. We then introduce a generality dimension, which represents increasing layers of breadth of the technology. These two dimensions lead to the new readiness-vs-generality charts, which show that higher TRLs are achievable for low-generality technologies, focusing on narrow or specific abilities, while high TRLs are still out of reach for more general capabilities. We include numerous examples of AI technologies in a variety of fields, and show their readiness-vs-generality charts, serving as exemplars. Finally, we show how the timelines of several AI technology exemplars at different generality layers can help forecast some short-term and mid-term trends for AI.",autonomous vehicle
10.3182/20020721-6-ES-1901.01644,journal,IFAC Proceedings Volumes,sciencedirect,2002-12-31,sciencedirect,"AI AND MACHINE LEARNING TECHNIQUES FOR MANAGING COMPLEXITY, CHANGES AND UNCERTAINTIES IN MANUFACTURING",https://api.elsevier.com/content/article/pii/S1474667015400655,"
                  The application of pattern recognition (PR) techniques, expert systems (ESs), artificial neural networks (ANNs), fuzzy systems (FSs) and nowadays hybrid artificial intelligence (AI) techniques in manufacturing can be regarded as consecutive elements of a process started two decades ago. On the one hand, the paper outlines the most important steps of this process and introduces some new results with special emphasis on hybrid AI and multistrategy machine learning (ML) approaches. On the other hand, agent-based (holonic) systems are highlighted as promising tools for managing complexity, changes and disturbances in production systems. Further integration of approaches is predicted.
               ",autonomous vehicle
10.1016/0921-8890(95)00006-2,journal,Robotics and Autonomous Systems,sciencedirect,1995-10-31,sciencedirect,Skillful control under uncertainty via direct reinforcement learning,https://api.elsevier.com/content/article/pii/0921889095000062,"
                  Complexity and uncertainty in modern robots and other autonomous systems make it difficult to design controllers for such systems that can achieve desired levels of precision and robustness. Therefore learning methods are being incorporated into controllers for such systems, thereby providing the adaptibility necessary to meet the performance demands of the task. We argue that for learning tasks arising frequently in control applications, the most useful methods in practice probably are those we call direct associative reinforcement learning methods. We describe direct reinforcement learning methods and also illustrate with an example the utility of these methods for learning skilled robot control under uncertainty.
               ",autonomous vehicle
10.1016/j.procir.2020.03.047,journal,Procedia CIRP,sciencedirect,2020-12-31,sciencedirect,Performance assessment methodology for AI-supported decision-making in production management,https://api.elsevier.com/content/article/pii/S2212827120306508,"Artificial intelligence (AI) gains importance in many domains and may soon be transferred decision-making responsibilities in production management from production managers. For the future, it will be vital to identify each entity’s domain of decision-making superiority. Therefore, this paper proposes and applies a model to assess AI performance in contrast to human decision-making. Relying on reinforcement learning and item response theory, the approach describes a minimum viable setup for AI systems to identify opportunities for AI systems in manufacturing. The model is based on operative production management decisions (job-shop scheduling) and validated through a series of academic scheduling instances.",autonomous vehicle
10.1016/j.ifacol.2018.09.525,journal,IFAC-PapersOnLine,sciencedirect,2018-12-31,sciencedirect,Fault Tolerant Deep Neural Networks for Detection of Unrecognizable Situations,https://api.elsevier.com/content/article/pii/S240589631832216X,"
                  Deep Neural Networks are achieving great success in various fields. However, their use remains limited to non critical applications because their behavior is unpredictable and unsafe. In this paper we propose some fault tolerant approaches based on diversifying learning in order to improve DNNs dependability and particularly safety. Our main goal is to increase trust in the outcome of deep learning mechanisms by recognizing the unlearned inputs and preventing misclassification.
               ",autonomous vehicle
10.1016/j.neucom.2021.07.081,journal,Neurocomputing,sciencedirect,2021-10-21,sciencedirect,A semi-supervised semantic-enhanced framework for scientific literature retrieval,https://api.elsevier.com/content/article/pii/S0925231221011632,"
                  Scientific literature retrieval provides convenience for researchers to find scientific literature related to the query. It is an important part of scientific research to search related papers given a paper title as query. However, for scientific literature retrieval tasks, most of the existing retrieval methods do not consider sentence-level semantic matching so that the retrieval performance is limited. With the success of neural networks, neural information retrieval methods have been widely studied and achieved good retrieval results. In this paper, we propose a semi-supervised semantic-enhanced scientific literature retrieval framework. The framework is composed of two networks: a self-attention convolutional encoder-decoder network and a sentence-level attention scientific literature retrieval network. By joint training of the two networks, the proposed semi-supervised semantic-enhanced scientific literature retrieval model can fully capture the rich semantic information of scientific text data and leverages human labeled scientific text data to improve the discriminativeness of the learned semantic representation. The retrieval results on two scientific literature datasets demonstrate that the proposed method significantly and consistently outperforms the other baseline methods.
               ",autonomous vehicle
10.1016/S0952-1976(03)00078-2,journal,Engineering Applications of Artificial Intelligence,sciencedirect,2003-06-30,sciencedirect,"AI and machine learning techniques for managing complexity, changes and uncertainties in manufacturing",https://api.elsevier.com/content/article/pii/S0952197603000782,"
                  The application of pattern recognition techniques, expert systems, artificial neural networks, fuzzy systems and nowadays hybrid artificial intelligence (AI) techniques in manufacturing can be regarded as consecutive elements of a process started two decades ago. The paper outlines the most important steps of this process and introduces some new results with special emphasis on hybrid AI and multistrategy machine learning approaches. Agent-based (holonic) systems are highlighted as promising tools for managing complexity, changes and disturbances in production systems. Further integration of approaches is predicted.
               ",autonomous vehicle
10.1016/B978-0-12-374176-9.00022-1,journal,Neuroeconomics,sciencedirect,2009-12-31,sciencedirect,Chapter 22: Theoretical and Empirical Studies of Learning,https://api.elsevier.com/content/article/pii/B9780123741769000221,"
               This chapter introduces the reinforcement learning framework and gives a brief background to the origins and history of reinforcement learning models of decision-making. Reinforcement learning provides a normative framework, within which conditioning can be analyzed. That is, this suggests a means by which optimal prediction and action selection can be achieved, and exposes explicitly the computations that must be realized in the service of these. In contrast to descriptive models that describe behavior as it is, normative models study behavior from the point of view of its hypothesized function—that is, they study behavior, as it should be if it were to accomplish specific goals in an optimal way. The appeal of normative models derives from several sources. Historically, the core ideas in reinforcement learning arose from two separate and parallel lines of research. One axis is mainly associated with Richard Sutton, formerly an undergraduate psychology major, and his PhD advisor, Andrew Barto, a computer scientist. Interested in artificial intelligence and agent-based learning, Sutton and Barto developed algorithms for reinforcement learning that were inspired by the psychological literature on Pavlovian and instrumental conditioning.
            ",autonomous vehicle
10.1016/j.heliyon.2018.e00938,journal,Heliyon,sciencedirect,2018-11-30,sciencedirect,State-of-the-art in artificial neural network applications: A survey,https://api.elsevier.com/content/article/pii/S2405844018332067,"This is a survey of neural network applications in the real-world scenario. It provides a taxonomy of artificial neural networks (ANNs) and furnish the reader with knowledge of current and emerging trends in ANN applications research and area of focus for researchers. Additionally, the study presents ANN application challenges, contributions, compare performances and critiques methods. The study covers many applications of ANN techniques in various disciplines which include computing, science, engineering, medicine, environmental, agriculture, mining, technology, climate, business, arts, and nanotechnology, etc. The study assesses ANN contributions, compare performances and critiques methods. The study found that neural-network models such as feedforward and feedback propagation artificial neural networks are performing better in its application to human problems. Therefore, we proposed feedforward and feedback propagation ANN models for research focus based on data analysis factors like accuracy, processing speed, latency, fault tolerance, volume, scalability, convergence, and performance. Moreover, we recommend that instead of applying a single method, future research can focus on combining ANN models into one network-wide application.",autonomous vehicle
10.1016/j.pmcj.2017.03.016,journal,Pervasive and Mobile Computing,sciencedirect,2017-09-30,sciencedirect,Unsupervised understanding of location and illumination changes in egocentric videos,https://api.elsevier.com/content/article/pii/S1574119217301700,"
                  Wearable cameras stand out as one of the most promising devices for the upcoming years, and as a consequence, the demand of computer algorithms to automatically understand the videos recorded with them is increasing quickly. An automatic understanding of these videos is not an easy task, and its mobile nature implies important challenges to be faced, such as the changing light conditions and the unrestricted locations recorded. This paper proposes an unsupervised strategy based on global features and manifold learning to endow wearable cameras with contextual information regarding the light conditions and the location captured. Results show that non-linear manifold methods can capture contextual patterns from global features without compromising large computational resources. The proposed strategy is used, as an application case, as a switching mechanism to improve the hand-detection problem in egocentric videos.
               ",autonomous vehicle
10.1016/j.procs.2020.06.032,journal,Procedia Computer Science,sciencedirect,2020-12-31,sciencedirect,Review of Clustering Techniques in Control System: Review of Clustering Techniques in Control System,https://api.elsevier.com/content/article/pii/S1877050920315362,"Data clustering is an important tool in data mining, that helps to retrieve useful data from large amount of available data. In this digital era data is available in abundance, but finding useful data has become a challenging task. For this, data clustering is an effective and common approach where we can group data by seeing some pattern or inherent data similarity in one group. Clustering is an unsupervised learning method of linearly separable and nonlinearly separable clusters widely used for different nature of application [1]. Data clustering finds application in classification of patterns in different areas such as artificial intelligence, summarization, learning, segmentation, speech recognition, pattern recognition, image segmentation, biology, marketing, data mining, modelling and system identification etc [5][24][25]. No one clustering technique can be said as best or better than other, because different clustering algorithms co-exists and are application specific. This paper majorly emphasises on critical review of clustering algorithms used in control systems, but a brief overview is also given about all major algorithms.",autonomous vehicle
10.1016/j.procs.2020.02.220,journal,Procedia Computer Science,sciencedirect,2020-12-31,sciencedirect,The Perception–Action Hierarchy and its Implementation Using Binons (Binary Neurons),https://api.elsevier.com/content/article/pii/S1877050920303434,"The perception–action hierarchy contains a model of the environment as experienced based on what has been recognized and done. Binons (binary neurons) can be used to represent and implement this hierarchy. Binons are simple deterministic artificial neural nodes that represent relationships. They have two source nodes and are reused by zero or more target nodes. Binons are general purpose components that interact in an object-oriented fashion. The two types of binons are spatial and temporal. Spatial binons represent simultaneously occurring patterns of percepts and actions. Temporal binons represent sequential patterns of percepts and actions. Two kinds of temporal binons are used to learn and control behaviour. They are the action and expectation control binons. They are equivalent to command neurons in neuroscience, production rules in cognitive architectures, or the forward model in motor control when combined together. Learning takes place in the three stages of babbling, practicing and automaticity. The resulting hierarchy is a transparent, compositional, unsupervised, continuously growing, deep learning artificial neural network. The hierarchy is part of the Adaptron cognitive architecture.",autonomous vehicle
10.1016/j.artint.2021.103521,journal,Artificial Intelligence,sciencedirect,2021-10-31,sciencedirect,Making sense of raw input,https://api.elsevier.com/content/article/pii/S0004370221000722,"How should a machine intelligence perform unsupervised structure discovery over streams of sensory input? One approach to this problem is to cast it as an apperception task [1]. Here, the task is to construct an explicit interpretable theory that both explains the sensory sequence and also satisfies a set of unity conditions, designed to ensure that the constituents of the theory are connected in a relational structure. However, the original formulation of the apperception task had one fundamental limitation: it assumed the raw sensory input had already been parsed using a set of discrete categories, so that all the system had to do was receive this already-digested symbolic input, and make sense of it. But what if we don't have access to pre-parsed input? What if our sensory sequence is raw unprocessed information? The central contribution of this paper is a neuro-symbolic framework for distilling interpretable theories out of streams of raw, unprocessed sensory experience. First, we extend the definition of the apperception task to include ambiguous (but still symbolic) input: sequences of sets of disjunctions. Next, we use a neural network to map raw sensory input to disjunctive input. Our binary neural network is encoded as a logic program, so the weights of the network and the rules of the theory can be solved jointly as a single SAT problem. This way, we are able to jointly learn how to perceive (mapping raw sensory information to concepts) and apperceive (combining concepts into declarative rules).",autonomous vehicle
10.1016/j.compeleceng.2020.106815,journal,Computers & Electrical Engineering,sciencedirect,2020-07-31,sciencedirect,Introduction to the special section on artificial intelligence in renewable energy (SI-aires),https://api.elsevier.com/content/article/pii/S0045790620306662,,autonomous vehicle
10.1016/B0-08-043076-7/00648-3,journal,International Encyclopedia of the Social & Behavioral Sciences,sciencedirect,2001-12-31,sciencedirect,Motor Control Models: Learning and Performance,https://api.elsevier.com/content/article/pii/B0080430767006483,"
               The article analyzes the complexity of motor control patterns and in particular the ecological nature of the action–perception cycle, which is the computational equivalent of Piaget's Circular reaction: purposive action is required making predictions about the dynamics of environmental processes; consistent perception is only possible if our brain takes into account the organization of our movements; the physics of the outside world is a source of energy/information to take advantage of. The problem of trajectory formation is discussed in some detail, examining the complexity of the underlying computational processes as regards planning, kinematics, dynamics, and redundancy. The role of the mechanical properties of muscles is emphasized, in relation with the theory of equilibrium-point control. Learning paradigms in neural network models are reviewed for addressing general problems in the field of adaptive behavior and motor learning such as: internal representation of space (cortical maps); self-supervised learning of dynamic internal models (cerebellum); reinforcement learning of action sequences (basal ganglia).
            ",autonomous vehicle
10.1016/B978-0-12-818438-7.00001-0,journal,Artificial Intelligence in Healthcare,sciencedirect,2020-12-31,sciencedirect,"Chapter 1: Current healthcare, big data, and machine learning",https://api.elsevier.com/content/article/pii/B9780128184387000010,"
               Today, our lifestyles, the global demographics, and our needs as individuals are rapidly changing and more people need healthcare than ever before. Healthcare costs are also becoming increasingly expensive and with increased demands and the technological developments, major changes in the healthcare value chain and business models are on their way to disrupt the healthcare system as we know it. It is time to change the way we see health and disease in a static sense and instead view life as a dynamic process where the maintenance of health is followed from far before symptoms of any sort start to appear. Technological developments within data science have begun having an impact on the healthcare ecosystem but are yet to show their full potential. The digitization of the healthcare system in the last decade has led to an ever-increasing source for data production. The amount of data generated is increasing exponentially and collection of healthcare data from various sources is rapidly on the rise. Collection, management, and storage of all this data is very costly and may not be of value unless converted into insights that can be used by the healthcare ecosystem. For maximum optimization and usefulness, the data needs to be analyzed, interpreted, and acted upon. ML and DL allow for the generation of new knowledge from all the collected data and can help reduce the cost/time-based burden on all healthcare systems via learning, better prediction, and diagnosis. These tools serve to improve the overall clinical workflow from the preventive and diagnostic phase to the prescriptive and restorative phase in health management.
            ",autonomous vehicle
10.1016/B978-0-444-89488-5.50021-X,journal,Artificial Neural Networks,sciencedirect,1992-12-31,sciencedirect,Reinforcement Learning: On Being Wise During the Event,https://api.elsevier.com/content/article/pii/B978044489488550021X,"
               A backchaining algorithm, an example of unsupervised sequential decision learning in an unknown environment, is defined. The algorithm is described as a neural network implementation and its behaviour is demonstrated through a simulation of a goal finding system in a two-dimensional world. The relationship between this model and some important concepts in animal learning theory are briefly discussed.
            ",autonomous vehicle
10.1016/j.patcog.2021.108120,journal,Pattern Recognition,sciencedirect,2021-12-31,sciencedirect,Real-time and light-weighted unsupervised video object segmentation network,https://api.elsevier.com/content/article/pii/S0031320321003071,"
                  Video object segmentation is one of the most practical computer vision tasks, especially in the unsupervised case, which has no manually labeled segmentation mask at the beginning of a video sequence. In this paper, we propose a new real-time unsupervised video object segmentation network. Based on the encoder-decoder framework, we present a Dynamic ASPP module and a RNN-Conv module. The former adds a dynamic selection mechanism into the Astrous Spatial Pyramid Pooling structure, and then the dilated convolutional kernels adaptively select appropriate features according to the scales by the channel attention mechanism. Compared with directly concatenating the dilated convolutional features, dynamically selecting feature maps reduces the amount of parameters and makes the module more efficient. The RNN-Conv module incorporates the RNN units with external convolutional blocks, aggregating the temporal features of a video sequence with the spatial information extracted by the convolutional network. We stack this module to extract deeper spatiotemporal features than the traditional RNN network. This module helps to avoid the gradient disappearance and explosion during network training. We test our network on the popular video object segmentation datasets. The experiment results demonstrate the effectiveness of our model.
                        1
                     
                     
                        1
                        Our code is available at https://github.com/Sanyuan-Zhao/Real-Time-and-Light-Weighted-UVOS
                     
                  
               ",autonomous vehicle
10.1016/j.neunet.2021.03.035,journal,Neural Networks,sciencedirect,2021-07-31,sciencedirect,Quantifying the separability of data classes in neural networks,https://api.elsevier.com/content/article/pii/S0893608021001234,"We introduce the Generalized Discrimination Value (GDV) that measures, in a non-invasive manner, how well different data classes separate in each given layer of an artificial neural network. It turns out that, at the end of the training period, the GDV in each given layer L attains a highly reproducible value, irrespective of the initialization of the network’s connection weights. In the case of multi-layer perceptrons trained with error backpropagation, we find that classification of highly complex data sets requires a temporal reduction of class separability, marked by a characteristic ‘energy barrier’ in the initial part of the GDV(L) curve. Even more surprisingly, for a given data set, the GDV(L) is running through a fixed ‘master curve’, independently from the total number of network layers. Finally, due to its invariance with respect to dimensionality, the GDV may serve as a useful tool to compare the internal representational dynamics of artificial neural networks with different architectures for neural architecture search or network compression; or even with brain activity in order to decide between different candidate models of brain function.",autonomous vehicle
10.1016/j.aiopen.2020.11.001,journal,AI Open,sciencedirect,2020-12-31,sciencedirect,"Neural machine translation: A review of methods, resources, and tools",https://api.elsevier.com/content/article/pii/S2666651020300024,"Machine translation (MT) is an important sub-field of natural language processing that aims to translate natural languages using computers. In recent years, end-to-end neural machine translation (NMT) has achieved great success and has become the new mainstream method in practical MT systems. In this article, we first provide a broad review of the methods for NMT and focus on methods relating to architectures, decoding, and data augmentation. Then we summarize the resources and tools that are useful for researchers. Finally, we conclude with a discussion of possible future research directions.",autonomous vehicle
10.1016/j.knosys.2014.12.032,journal,Knowledge-Based Systems,sciencedirect,2015-05-31,sciencedirect,Evolving connectionist systems for adaptive learning and knowledge discovery: Trends and directions,https://api.elsevier.com/content/article/pii/S0950705115000040,"
                  This paper follows the 25years of development of methods and systems for knowledge-based neural network systems and more specifically the recent evolving connectionist systems (ECOS). ECOS combine the adaptive/evolving learning ability of neural networks and the approximate reasoning and linguistically meaningful explanation features of symbolic representation, such as fuzzy rules. This review paper presents the classical now hybrid expert systems and evolving neuro-fuzzy systems, along with new developments in spiking neural networks, neurogenetic systems, and quantum inspired systems, all discussed from the point of few of their adaptability, model interpretability and knowledge discovery. The paper discusses new directions for the integration of principles from neural networks, fuzzy systems, bio- and neuroinformatics, and nature in general.
               ",autonomous vehicle
10.1016/j.seta.2021.101370,journal,Sustainable Energy Technologies and Assessments,sciencedirect,2021-10-31,sciencedirect,A review on virtual power plant for energy management,https://api.elsevier.com/content/article/pii/S2213138821003805,"
                  A Virtual Power Plant (VPP) is a practical concept that aggregates various Renewable Energy Sources (RESs) to improve energy management efficiency and facilitate energy trading. Operation scheduling for all energy components in VPPs plays a vital role from an energy management perspective. Technical and economic constraints and uncertainties that significantly affect the scheduling program must be considered simultaneously. This paper provides a comprehensive review of the scheduling problem in the VPP concept, following Kitchenham's guidelines, to address questions such as: What are the most frequent scheduling techniques in VPPs? How technical and economic aspects of scheduling have been considered to optimize the problem? Moreover, how to deal with different types of uncertainties? To that end, all previous studies on this topic have been extracted and analyzed, focusing on the scheduling algorithm's necessity. Several optimal scheduling methods are investigated that show learning-based approaches have not been well studied. Then, joint technical and economic limitations and dealing with various types of uncertainties are appraised. Contribute to better knowledge for future studies on VPPs, the research gaps regarding optimization techniques, joint techno-economic, and various kinds of uncertainties have been introduced. Finally, this paper also suggests utilizing a Deep Reinforcement Learning (DRL)-based technique to address the mentioned concerns due to generalization, scalability, and feature extraction, which are originated from a combination of reinforcement learning and deep learning.
               ",autonomous vehicle
10.1016/j.procs.2019.05.052,journal,Procedia Computer Science,sciencedirect,2019-12-31,sciencedirect,Artificial intelligence analytics with Multi-Attribute Tradespace Exploration and Set-Based Design,https://api.elsevier.com/content/article/pii/S1877050919307112,"Data-driven design approaches such as Multi-Attribute Tradespace Exploration and Set-Based Design are increasing in popularity due to their ability to capture broader decision spaces than traditional point-based design. These methods share many of the same features and have complementary goals. Artificial intelligence offers a way to process the large amounts of data created by these methods in a fast and objective manner, supporting the insights of subject matter experts. This paper discusses the intersection of these three research areas and demonstrates an approach for combining these techniques to rapidly identify the most value-driving decisions available to the design team.",autonomous vehicle
10.1016/j.dcan.2020.06.005,journal,Digital Communications and Networks,sciencedirect,2020-08-31,sciencedirect,On recommendation-aware content caching for 6G: An artificial intelligence and optimization empowered paradigm,https://api.elsevier.com/content/article/pii/S2352864820301802,"Recommendation-aware Content Caching (RCC) at the edge enables a significant reduction of the network latency and the backhaul load, thereby invigorating ubiquitous latency-sensitive innovative services. However, the effectiveness of RCC strategies is highly dependent on explicit information as regards subscribers’ content request patterns, the sophisticated caching placement policy, and the personalized recommendation tactics. In this article, we investigate how the potentials of Artificial Intelligence (AI) and optimization techniques can be harnessed to address those core issues and facilitate the full implementation of RCC for the upcoming intelligent 6G era. Towards this end, we first elaborate on the hierarchical RCC network architecture. Then, the devised AI and optimization empowered paradigm is introduced, whereas AI and optimization techniques are leveraged to predict the users’ content preferences in real-time situations with the assistance of their historical behavior data and determine the cache pushing and recommendation decision, respectively. Through extensive case studies, we validate the effectiveness of AI-based predictors in estimating users’ content preference and the superiority of optimized RCC policies over the conventional benchmarks. At last, we shed light on the opportunities and challenges in the future.",autonomous vehicle
10.1016/j.neucom.2020.07.053,journal,Neurocomputing,sciencedirect,2020-12-05,sciencedirect,Survey on Deep Neural Networks in Speech and Vision Systems,https://api.elsevier.com/content/article/pii/S0925231220311619,"
                  This survey presents a review of state-of-the-art deep neural network architectures, algorithms, and systems in speech and vision applications. Recent advances in deep artificial neural network algorithms and architectures have spurred rapid innovation and development of intelligent speech and vision systems. With availability of vast amounts of sensor data and cloud computing for processing and training of deep neural networks, and with increased sophistication in mobile and embedded technology, the next-generation intelligent systems are poised to revolutionize personal and commercial computing. This survey begins by providing background and evolution of some of the most successful deep learning models for intelligent speech and vision systems to date. An overview of large-scale industrial research and development efforts is provided to emphasize future trends and prospects of intelligent speech and vision systems. Robust and efficient intelligent systems demand low-latency and high fidelity in resource-constrained hardware platforms such as mobile devices, robots, and automobiles. Therefore, this survey also provides a summary of key challenges and recent successes in running deep neural networks on hardware-restricted platforms, i.e. within limited memory, battery life, and processing capabilities. Finally, emerging applications of speech and vision across disciplines such as affective computing, intelligent transportation, and precision medicine are discussed. To our knowledge, this paper provides one of the most comprehensive surveys on the latest developments in intelligent speech and vision applications from the perspectives of both software and hardware systems. Many of these emerging technologies using deep neural networks show tremendous promise to revolutionize research and development for future speech and vision systems.
               ",autonomous vehicle
10.1016/j.dcan.2021.11.002,journal,Digital Communications and Networks,sciencedirect,2021-11-15,sciencedirect,Reconfigurable intelligent surface: Design the channel – A new opportunity for future wireless networks,https://api.elsevier.com/content/article/pii/S2352864821000912,"In this paper, we survey state-of-the-art research outcomes in the burgeoning field of Reconfigurable Intelligent Surface (RIS) given its potential for significant performance enhancement of next-generation wireless communication networks by means of adapting a propagation environment. Emphasis has been placed on several aspects gating the commercially viability of a future network deployment. Comprehensive summaries are provided for practical hardware design considerations and broad implications of artificial intelligence techniques, as are in-depth outlooks on the salient aspects of system models, use cases, and physical layer optimization techniques.",autonomous vehicle
10.1016/j.ijepes.2019.02.023,journal,International Journal of Electrical Power & Energy Systems,sciencedirect,2019-07-31,sciencedirect,Effective bulk energy consumption control and management for power utilities using artificial intelligence techniques under conventional and renewable energy resources,https://api.elsevier.com/content/article/pii/S0142061518329776,"
                  Increasing sustainability demands initiate estimating various design and control opportunities for classifying energy-efficient plan ever more significant. These conditions demand simulation algorithms which are not only fast, but also accurate. Artificial intelligence (AI) enables efficient mimicry of bulk energy consumption control while producing results much faster than data-mining and machine learning models. This study proposes two AI based approaches for utilities bulk energy consumption prediction, control and management. Two different zones actual environmental and energy consumption data are obtained for input feature selection and modeling analysis. Each zone is categorized into five features parameter selection (PS) states. Each PS state is further divided into four different hidden neurons (HD) and hidden layers of the model’s network. The forecasting duration is based on 1-month and 1-year ahead intervals for medium-term (MT) and long-term (LT) respectively. Further the current proposed model’s performance is compared with three existing models. One of the promising findings in this research is that substantial improvement in prediction accuracy applying features extracted by PS-3 and PS-5. Results show that AI models are powerful in solving complex and nonlinear patterns of raw data. This study renders optimal decisions can be projected while utilities energy supply strategy & control, capacity expansion, capital investment research market management, revenue analysis and future load requirement forecasting.
               ",autonomous vehicle
10.1016/B978-0-12-815665-0.00009-6,journal,Release and Bioavailability of Nanoencapsulated Food Ingredients,sciencedirect,2020-12-31,sciencedirect,Chapter Nine: Release modeling of nanoencapsulated food ingredients by artificial intelligence algorithms,https://api.elsevier.com/content/article/pii/B9780128156650000096,"
               This chapter introduces the concept of artificial intelligence algorithms and the potential for their application in the field of biomedicine and nutrition, especially in the context of nanoencapsulated food ingredients. Amongst many artificial intelligence algorithms, several are selected and their potential for release modeling is described in more detail. These are namely artificial neural networks, adaptive neuro-fuzzy inference systems, and genetic algorithms. Apart from the theoretical background on their operating principles, numerous examples are selected and briefly discussed in order to present to reader different aspects for their potential application. In spite of usually being treated as the “black-box” modeling tools, these algorithms actually do provide the opportunities to analyze the causal relationships between the data or perform sensitivity analysis, which is of great importance and value for the release modeling.
            ",autonomous vehicle
10.1016/j.procs.2011.07.029,journal,Procedia Computer Science,sciencedirect,2011-12-31,sciencedirect,How Wireless Sensor Networks Can Benefit from Brain Emotional Learning Based Intelligent Controller (BELBIC),https://api.elsevier.com/content/article/pii/S1877050911003541,"Wireless sensor networks (WSNs) are composed of small sensing and actuating devices that collaboratively monitor a phenomena, process and reason about sensor measurements, and provide adequate feedback or take actions. One of WSNs tasks is event detection, in which occurrence of events of interest is detected in situ whenever and wherever they occur. Some examples of these events include environmental (e.g. fire), personal (e.g. activities), and data-related (e.g. outlier) events. Simply speaking, event detection is a classification process, in which membership of data measurements to each event class is determined. Neural network is one of the classifiers that have often been used for detecting events with known patterns. One of the techniques to maximise the neural network performance during classification process is enabling a learning process. Through this learning process, neural network can learn from errors generated in each round of classification to gradually improve its performance. In this paper we investigate applicability of Brain Emotional Based Intelligent Controller (BELBIC) to improve neural network performance. Empirical results show that incorporating the BELBIC with neural networks improves the accuracy of event detection in many circumstances.",autonomous vehicle
10.1016/j.cie.2016.10.022,journal,Computers & Industrial Engineering,sciencedirect,2017-10-31,sciencedirect,Integrating estimation of distribution algorithms versus Q-learning into Meta-RaPS for solving the 0-1 multidimensional knapsack problem,https://api.elsevier.com/content/article/pii/S0360835216304077,"
                  Finding near-optimal solutions in an acceptable amount of time is a challenge when developing sophisticated approximate approaches. A powerful answer to this challenge might be reached by incorporating intelligence into metaheuristics. We propose integrating two methods into Meta-RaPS (Metaheuristic for Randomized Priority Search), which is currently classified as a memoryless metaheuristic. The first method is the Estimation of Distribution Algorithms (EDA), and the second is utilizing a machine learning algorithm known as Q-Learning. To evaluate their performance, the proposed algorithms are tested on the 0-1 Multidimensional Knapsack Problem (MKP). Meta-RaPS EDA appears to perform better than Meta-RaPS Q-Learning. However, both showed promising results compared to other approaches presented in the literature for the 0-1 MKP.
               ",autonomous vehicle
10.1016/j.neucom.2006.06.008,journal,Neurocomputing,sciencedirect,2007-08-31,sciencedirect,New trends in Cognitive Science: Integrative approaches to learning and development,https://api.elsevier.com/content/article/pii/S0925231206005078,"
                  A new trend in Cognitive Science is the use of artificial agents and systems to investigate learning and development of complex organisms in natural environments. This work, in contrast with traditional AI work, takes into account principles of neural development, problems of embodiment, and complexities of the environment. Current and future promises and challenges for this approach are defined and outlined.
               ",autonomous vehicle
10.1016/j.future.2019.09.002,journal,Future Generation Computer Systems,sciencedirect,2020-09-30,sciencedirect,BlockIoTIntelligence: A Blockchain-enabled Intelligent IoT Architecture with Artificial Intelligence,https://api.elsevier.com/content/article/pii/S0167739X19316474,"
                  In the recent year, Internet of Things (IoT) is industrializing in several real-world applications such as smart transportation, smart city to make human life reliable. With the increasing industrialization in IoT, an excessive amount of sensing data is producing from various sensors devices in the Industrial IoT. To analyzes of big data, Artificial Intelligence (AI) plays a significant role as a strong analytic tool and delivers a scalable and accurate analysis of data in real-time. However, the design and development of a useful big data analysis tool using AI have some challenges, such as centralized architecture, security, and privacy, resource constraints, lack of enough training data. Conversely, as an emerging technology, Blockchain supports a decentralized architecture. It provides a secure sharing of data and resources to the various nodes of the IoT network is encouraged to remove centralized control and can overcome the existing challenges in AI. The main goal of our research is to design and develop an IoT architecture with blockchain and AI to support an effective big data analysis. In this paper, we propose a Blockchain-enabled Intelligent IoT Architecture with Artificial Intelligence that provides an efficient way of converging blockchain and AI for IoT with current state-of-the-art techniques and applications. We evaluate the proposed architecture and categorized into two parts: qualitative analysis and quantitative analysis. In qualitative evaluation, we describe how to use AI and Blockchain in IoT applications with “AI-driven Blockchain” and “Blockchain-driven AI.” In quantitative analysis, we present a performance evaluation of the BlockIoTIntelligence architecture to compare existing researches on device, fog, edge and cloud intelligence according to some parameters such as accuracy, latency, security and privacy, computational complexity and energy cost in IoT applications. The evaluation results show that the proposed architecture performance over the existing IoT architectures and mitigate the current challenges.
               ",autonomous vehicle
10.1016/j.ejor.2019.04.013,journal,European Journal of Operational Research,sciencedirect,2019-10-01,sciencedirect,Large data sets and machine learning: Applications to statistical arbitrage,https://api.elsevier.com/content/article/pii/S0377221719303339,"
                  Machine learning algorithms and big data are transforming all industries including the finance and portfolio management sectors. While these techniques, such as Deep Belief Networks or Random Forests, are becoming more and more popular on the market, the academic literature is relatively sparse. Through a series of applications involving hundreds of variables/predictors and stocks, this article presents some of the state-of-the-art techniques and how they can be implemented to manage a long-short portfolio. Numerous practical and empirical issues are developed. One of the main questions beyond big data use is the value of information. Does an increase in the number of predictors improve the portfolio performance? Which features are the most important? A large number of predictors means, potentially, a high level of noise. How do the algorithms manage this? This article develops an application using a 22-year trading period, up to 300 U.S. large caps and around 600 predictors. The empirical results underline the ability of these techniques to generate useful trading signals for portfolios with important turnovers and short holding periods (one or five days). Positive excess returns are reported between 1993 and 2008. They are strongly reduced after accounting for transaction costs and traditional risk factors. When these machine learning tools were readily available in the market, excess returns turned into the negative in most recent times. Results also show that adding features is far from being a guarantee to boost the alpha of the portfolio.
               ",autonomous vehicle
10.1016/j.autcon.2017.01.016,journal,Automation in Construction,sciencedirect,2017-05-31,sciencedirect,Machine learning for durability and service-life assessment of reinforced concrete structures: Recent advances and future directions,https://api.elsevier.com/content/article/pii/S0926580517300559,"
                  Accurate service-life prediction of structures is vital for taking appropriate measures in a time- and cost-effective manner. However, the conventional prediction models rely on simplified assumptions, leading to inaccurate estimations. The paper reviews the capability of machine learning in addressing the limitations of classical prediction models. This is due to its ability to capture the complex physical and chemical process of the deterioration mechanism. The paper also presents previous researches that proposed the applicability of machine learning in assisting durability assessment of reinforced concrete structures. The advantages of employing machine learning for durability and service-life assessment of reinforced concrete structures are also discussed in detail. The growing trend of collecting more and more in-service data using wireless sensors facilitates the use of machine learning for durability and service-life assessment. The paper concludes by recommending the future directions based on examination of recent advances and current practices in this specific area.
               ",autonomous vehicle
10.1016/j.neucom.2015.02.074,journal,Neurocomputing,sciencedirect,2015-09-21,sciencedirect,Advances in computational intelligence: Selected and improved papers of the 12th International Work-Conference on Artificial Neural Networks (IWANN 2013),https://api.elsevier.com/content/article/pii/S0925231215003069,,autonomous vehicle
10.1016/j.adhoc.2021.102565,journal,Ad Hoc Networks,sciencedirect,2021-09-01,sciencedirect,Deep Q-network based fog node offloading strategy for 5 G vehicular Adhoc Network,https://api.elsevier.com/content/article/pii/S1570870521001086,"
                  The research on the Vehicular ad-hoc network (VANET) has been accelerated by the 5 G technology. The software-defined network and fog nodes near the vehicles have improved the throughput and latency in the processing of requests. However, the fog nodes are limited with computational resources like memories, RAM, etc. and need to be optimally managed. The estimation of vehicles' future locations can help in the optimal offloading of vehicles' processing requests. This paper has introduced the Kalman filter prediction scheme to estimate the vehicle's next location so that the future availability of fog resources can help in the offloading decision. The deep Q network-based reinforcement learning is used to select the resources-rich fog node in VANET. The Long Term Short Memory-based Deep Q-Network optimally offloads the tasks of the fog nodes as per their available resources thus giving much better performance. The proposed Deep Q-Network algorithm is an efficient solution to offload the request optimally which improves the overall performance of the network. It is found that the average reward by proposed Deep Q-Network is 56.889% more than SARSA learning and is 44.727% more than Q learning.
               ",autonomous vehicle
10.1016/j.compag.2018.08.001,journal,Computers and Electronics in Agriculture,sciencedirect,2018-10-31,sciencedirect,Computer vision and artificial intelligence in precision agriculture for grain crops: A systematic review,https://api.elsevier.com/content/article/pii/S0168169918305829,"
                  Grain production plays an important role in the global economy. In this sense, the demand for efficient and safe methods of food production is increasing. Information Technology is one of the tools to that end. Among the available tools, we highlight computer vision solutions combined with artificial intelligence algorithms that achieved important results in the detection of patterns in images. In this context, this work presents a systematic review that aims to identify the applicability of computer vision in precision agriculture for the production of the five most produced grains in the world: maize, rice, wheat, soybean, and barley. In this sense, we present 25 papers selected in the last five years with different approaches to treat aspects related to disease detection, grain quality, and phenotyping. From the results of the systematic review, it is possible to identify great opportunities, such as the exploitation of GPU (Graphics Processing Unit) and advanced artificial intelligence techniques, such as DBN (Deep Belief Networks) in the construction of robust methods of computer vision applied to precision agriculture.
               ",autonomous vehicle
10.1016/B978-0-12-822420-5.00013-1,journal,Introduction to Machine Olfaction Devices,sciencedirect,2022-12-31,sciencedirect,9: Tests and training,https://api.elsevier.com/content/article/pii/B9780128224205000131,"
               In this chapter, tests on the metal oxide sensor and training, using artificial intelligence methods such as machine learning and deep learning, are presented. Prior to the above, the completed new machine olfaction device (MOD) hardware is examined and basic characterizations for standard procedures and protocols for the device are established. This process is to enable quantitative characterization of the MOD instrumentation, since specific parameters need to be addressed, including repeatability, reproducibility, uncertainty, range, sensitivity, and traceability of the measurements, together with associated calibration, as part of important experimental tests.
               Various samples have been chosen to start basic experiments on the MOD. Concentrations of aqueous acetone, ethanol, and methanol have been used, as well as HPLC water (HPLC—High Performance Liquid Chromatography) implementing different and repeatable experiments. The methodology applied to the samples covers four steps; these are: sample concentration, time (related to water bath, injection, change in sensor resistance), temperature (related to water bath), and sensor resistance. All the experimental results have been illustrated in graphs and tables. In a comparison of all the samples results, the measurement modes of the device produce the highest correlation with the highest concentration, which suggests that the MOD is in good working order.
               Further tests and training are also performed. Consideration is given for the outline design of a Python software program, prior to the programming, but not providing the program code.
            ",autonomous vehicle
10.1016/j.suscom.2020.100373,journal,Sustainable Computing: Informatics and Systems,sciencedirect,2020-06-30,sciencedirect,An artificial neural network based approach for energy efficient task scheduling in cloud data centers,https://api.elsevier.com/content/article/pii/S2210537918302798,"
                  Energy efficiency is considered as a crucial objective in cloud data centers as it reduces cost and meets the standard set in green computing. Task scheduling an important problem becomes more complex and critical under energy efficiency consideration. Key issues in recent research on energy efficient task scheduling are execution overhead and scalability. Machine learning has been widely employed for energy efficient task scheduling problem but mostly used to predict resource consumption only instead of deciding the schedule itself. However, we used the neural network to decide which resource should be assigned to given task independently. In this paper, we proposed an energy efficient independent task scheduler using supervised neural networks with the aim to reduce makespan, energy consumption, execution overhead and number of active racks. Proposed artificial neural network-based scheduler takes incoming task and current cloud environment state as input and predict the best computing resource for given task as output which compiles our aim. We used genetic algorithm to generate a huge dataset (∼18 million training instances) and trained our neural network on this dataset using back propagation algorithm with 99.9% accuracy. We simulated experiments on heavily loaded and lightly loaded cloud environment and compared with well-known approaches: Genetic algorithm, MinMIN-MINMin heuristic and Linear regression based energy efficient task schedulers. Results clearly indicate that proposed work outperforms considered algorithms. In heavily (lightly) loaded environment, it improves makespan by 59% (64%), energy consumption by 45% (71%), execution overhead by 88% (43%) respectively and number of active racks by 70%.
               ",autonomous vehicle
10.1016/j.dcan.2021.07.009,journal,Digital Communications and Networks,sciencedirect,2021-07-30,sciencedirect,Poisoning attacks and countermeasures in intelligent networks: Status quo and prospects,https://api.elsevier.com/content/article/pii/S235286482100050X,"Over the past years, the emergence of intelligent networks empowered by machine learning techniques has brought great facilitates to different aspects of human life. However, using machine learning in intelligent networks also presents potential security and privacy threats. A common practice is the so-called poisoning attacks where malicious users inject fake training data with the aim of corrupting the learned model. In this survey, we comprehensively review existing poisoning attacks as well as the countermeasures in intelligent networks for the first time. We emphasize and compare the principles of the formal poisoning attacks employed in different categories of learning algorithms, and analyze the strengths and limitations of corresponding defense methods in a compact form. We also highlight some remaining challenges and future directions in the attack-defense confrontation to promote further research in this emerging yet promising area.",autonomous vehicle
10.1016/S1474-6670(17)50864-2,journal,IFAC Proceedings Volumes,sciencedirect,1992-06-30,sciencedirect,Reinforcement Learning and Recruitment Mechanism for Adaptive Distributed Control,https://api.elsevier.com/content/article/pii/S1474667017508642,"
                  The work presented in this paper is an attempt to spread further the inspiration gained from the knowledge of biological systems into the field of adaptive control. After the neural controllers and the evolutionary based mechanisms, new hints for the control of complex processes might be derived from other biological domains such as immunology or the study of conditioning learning. The conception of a system equipped with a complex controller, interacting with an uncertain and varying environment, and basing its learning on its own experiences entails quite naturally the integration of a reinforcement learning mechanism. Two learning processes characterized by two different time scales will be introduced, will be connected to their respective biological origins and will be illustrated on the classical cart-pole control problem. These two learning processes are the rapid reinforcement learning and the slower recruitment mechanism.
               ",autonomous vehicle
10.1016/j.neucom.2018.11.067,journal,Neurocomputing,sciencedirect,2019-02-28,sciencedirect,Least squares support vector machine with self-organizing multiple kernel learning and sparsity,https://api.elsevier.com/content/article/pii/S0925231218314139,"
                  In recent years, least squares support vector machines (LSSVMs) with various kernel functions have been widely used in the field of machine learning. However, the selection of kernel functions is often ignored in practice. In this paper, an improved LSSVM method based on self-organizing multiple kernel learning is proposed for black-box problems. To strengthen the generalization ability of the LSSVM, some appropriate kernel functions are selected and the corresponding model parameters are optimized using a differential evolution algorithm based on an improved mutation strategy. Due to the large computation cost, a sparse selection strategy is developed to extract useful data and remove redundant data without loss of accuracy. To demonstrate the effectiveness of the proposed method, some benchmark problems from the UCI machine learning repository are tested. The results show that the proposed method performs better than other state-of-the-art methods. In addition, to verify the practicability of the proposed method, it is applied to a real-world converter steelmaking process. The results illustrate that the proposed model can precisely predict the molten steel quality and satisfy the actual production demand.
               ",autonomous vehicle
10.1016/j.engappai.2020.104112,journal,Engineering Applications of Artificial Intelligence,sciencedirect,2021-02-28,sciencedirect,Multi-agent hierarchical policy gradient for Air Combat Tactics emergence via self-play,https://api.elsevier.com/content/article/pii/S0952197620303547,"
                  Air-to-air confrontation has attracted wide attention from artificial intelligence scholars. However, in the complex air combat process, operational strategy selection depends heavily on aviation expert knowledge, which is usually expensive and difficult to obtain. Moreover, it is challenging to select optimal action sequences efficiently and accurately with existing methods, due to the high complexity of action selection when involving hybrid actions, e.g., discrete/continuous actions. In view of this, we propose a novel Multi-Agent Hierarchical Policy Gradient algorithm (MAHPG), which is capable of learning various strategies and transcending expert cognition by adversarial self-play learning. Besides, a hierarchical decision network is adopted to deal with the complicated and hybrid actions. It has a hierarchical decision-making ability similar to humankind, and thus, reduces the action ambiguity efficiently. Extensive experimental results demonstrate that the MAHPG outperforms the state-of-the-art air combat methods in terms of both defense and offense ability. Notably, it is discovered that the MAHPG has the ability of Air Combat Tactics Interplay Adaptation, and new operational strategies emerged that surpass the level of experts.
               ",autonomous vehicle
10.1016/j.neucom.2020.01.003,journal,Neurocomputing,sciencedirect,2020-04-28,sciencedirect,Bi-Decoder Augmented Network for Neural Machine Translation,https://api.elsevier.com/content/article/pii/S0925231220300229,"
                  Neural Machine Translation (NMT) has become a popular technology in recent years, and the encoder–decoder framework is the mainstream among all the methods. It is obvious that the quality of the semantic representations from encoding is very crucial and can significantly affect the performance of the model. However, existing unidirectional source-to-target architectures may hardly produce a language-independent representation of the text because they rely heavily on the specific relations of the given language pairs. To alleviate this problem, in this paper, we propose a novel Bi-Decoder Augmented Network (BiDAN) for the neural machine translation task. Besides the original decoder which generates the target language sequence, we add an auxiliary decoder to generate back the source language sequence at the training time. Since each decoder transforms the representations of the input text into its corresponding language, jointly training with two target ends can make the shared encoder has the potential to produce a language-independent semantic space. We conduct extensive experiments on several NMT benchmark datasets and the results demonstrate the effectiveness of our proposed approach.
               ",autonomous vehicle
10.1016/j.procir.2018.12.019,journal,Procedia CIRP,sciencedirect,2019-12-31,sciencedirect,Predictive Maintenance of Machine Tool Systems Using Artificial Intelligence Techniques Applied to Machine Condition Data,https://api.elsevier.com/content/article/pii/S2212827118312988,"Often, manufacturing equipment is utilized without a planned maintenance approach. Such a strategy frequently results in unplanned downtime, owing to unexpected failures. Scheduled maintenance replaces components frequently to avoid unexpected equipment stoppages, but increases the time associated with machine non-operation and maintenance cost. The emergence of Industry 4.0 and smart systems is leading to increasing attention to predictive maintenance (PdM) strategies that can decrease the cost of downtime and increase the availability (utilization rate) of manufacturing equipment. PdM also has the potential to foster sustainable practices in manufacturing by maximizing the useful lives of components. In this paper, the AI-based algorithms for predictive maintenance are presented, and are applied to monitor two critical machine tool system elements: the cutting tool and the spindle motor. A data-driven modeling approach will be described, and it will be utilized to investigate the tool wear and the bearing failures.",autonomous vehicle
10.1016/B978-0-12-816502-7.00029-4,journal,Optical Fiber Telecommunications VII,sciencedirect,2020-12-31,sciencedirect,Chapter 21: Machine learning methods for optical communication systems and networks,https://api.elsevier.com/content/article/pii/B9780128165027000294,"
               Machine learning (ML) is being hailed as a new direction of innovation to transform future optical communication systems. Signal processing paradigms based on ML are being considered to solve certain critical problems in optical communications that cannot be easily tackled using conventional approaches. Recent applications of ML in various aspects of optical communications and networking such as nonlinear transmission systems, network planning and performance prediction, cross-layer network optimizations for software-defined networks, and autonomous and reliable network operations have shown promising results. However, to comprehend true potential of ML in optical communications, a basic understanding of the nature of ML concepts is indispensable. In this chapter we describe mathematical foundations of several key ML methods from communication theory and signal processing perspectives and highlight the types of problems in optical communications and networking where they can be useful. We also provide an overview of existing ML applications in optical communication systems with an emphasis on physical layer.
            ",autonomous vehicle
10.1016/j.ssmph.2018.03.007,journal,SSM - Population Health,sciencedirect,2018-04-30,sciencedirect,Machine learning in social epidemiology: Learning from experience,https://api.elsevier.com/content/article/pii/S2352827318300405,,autonomous vehicle
10.1016/j.jtcvs.2019.08.141,journal,The Journal of Thoracic and Cardiovascular Surgery,sciencedirect,2020-09-30,sciencedirect,<ce:marker name=ymtcperimanage alt=Perioperative Management altimg-small=ymtcperimanage_s.svg altimg=ymtcperimanage_o.svg></ce:marker>Wireless monitoring and artificial intelligence: A bright future in cardiothoracic surgery,https://api.elsevier.com/content/article/pii/S0022522319331216,,autonomous vehicle
10.1016/bs.host.2016.07.010,journal,Handbook of Statistics,sciencedirect,2016-12-31,sciencedirect,Chapter 5: Cognitive Analytics: Going Beyond Big Data Analytics and Machine Learning,https://api.elsevier.com/content/article/pii/S0169716116300517,"
                  This chapter defines analytics and traces its evolution from its origin in 1988 to its current stage—cognitive analytics. We discuss types of learning and describe classes of machine learning algorithms. Given this backdrop, we propose a reference architecture for cognitive analytics and indicate ways to implement the architecture. A few cognitive analytics applications are briefly described. The chapter concludes by indicating current trends and future research direction.
               ",autonomous vehicle
10.1016/j.neucom.2019.03.036,journal,Neurocomputing,sciencedirect,2019-07-25,sciencedirect,Green virtual network embedding with supervised self-organizing map,https://api.elsevier.com/content/article/pii/S0925231219303765,"
                  Virtualization in the data center network is used to overcome the resistance of Internet ossification. Node mapping in virtual network embedding (VNE) process is more important than link mapping because each node mapping has a lot of choices with high probability. Historical data always have valuable information which displays how to be mapped virtual networks with special features. Previous works use physical network historical information, and they do not pay attention to the characteristics of virtual networks during the embedding process. To address this problem, this paper proposes SoGVNE (Self-organized Green Virtual Network Embedding) which uses previous successful virtual node mapping information to train a self-organizing map (SOM) neural network. Then, it will respond to future requests quickly and with fewer resources. SoGVNE uses virtual networks historical data and is able to embed more than one virtual network concurrently. Using supervised SOM makes the SoGVNE aware of resources with renewable energy and uses them to manage energy consumption. This paper evaluates SoGVNE with reinforcement-learning, Monte-Carlo and Presto methods. The results indicate the high efficiency of the proposed method in learning the node mapping process and achieving high profit and acceptance ratio with low cost.
               ",autonomous vehicle
10.1016/j.comcom.2021.10.034,journal,Computer Communications,sciencedirect,2022-01-01,sciencedirect,A survey on improving the wireless communication with adaptive antenna selection by intelligent method,https://api.elsevier.com/content/article/pii/S014036642100414X,"
                  Transmission applications in wireless networks have brought unprecedented demands. The demand for high-performance wireless transmission is increasing day by day. Antenna technology is an indispensable part of the development of wireless communication. One potential solution is to resort t intelligent learning techniques to help breakthroughs in the limited antenna technical field. It is based on an adaptive antenna using intelligent learning. It has laid the foundation for signal strength adjustment to enhance wireless transmission efficiency. This paper evaluates the most advanced literature and techniques. A comprehensive description from different perspectives covers several adaptive antenna structures, including diversity antennas, phased array antennas, and beamforming specific learning methods. After that, this paper divides it into different categories, from intelligent learning algorithms and feature data perspectives in a different light to analyze and discuss. This article expects to help readers understand the latest intelligent technology based on adaptive antennas. Further, it sheds novel light on future research directions to meet the development needs of adaptive antennas for future wireless networks.
               ",autonomous vehicle
10.1016/B978-0-12-821092-5.00006-1,journal,Applications of Artificial Intelligence in Process Systems Engineering,sciencedirect,2021-12-31,sciencedirect,"Chapter 11: Application of artificial intelligence in modeling, control, and fault diagnosis",https://api.elsevier.com/content/article/pii/B9780128210925000061,"
               Advanced control systems are becoming more and more sophisticated every day. For safety-critical systems such as chemical processes, nuclear reactors, aircraft and spacecraft, the issue of reliability, acceptable performance, and environmental protection are of particular importance. If a fault occurs, the damages to financial, human, and environmental could be severe. As a result, there is a growing need for online monitoring and Fault Detection, or to identify system faults to improve reliability. Accordingly, it is viable to apply the preliminary signs and to prevent the system from stopping and the occurrence of a catastrophe to a large extent. Fault diagnostic methods can be divided into two general categories of model-based and data-based methods. In this chapter, we will look at the second category, which includes fuzzy logic, neural network, and support vector machines.
            ",autonomous vehicle
10.1016/B978-0-12-821092-5.00009-7,journal,Applications of Artificial Intelligence in Process Systems Engineering,sciencedirect,2021-12-31,sciencedirect,Chapter 7: Artificial intelligence algorithm application in wastewater treatment plants: Case study for COD load prediction,https://api.elsevier.com/content/article/pii/B9780128210925000097,"
               Most modern urban wastewater treatment plants adopt a treatment technology based on an activated sludge process. Because the composition and content of pollutants in sewage cannot be known, excessive aeration and chemicals are often used to ensure that the quality of the treated sewage meets the requisite standard. Such extensive operations also result in energy wastage and potential secondary pollution. Therefore, this chapter proposes a chemical oxygen demand load prediction model based on the gradient boosting decision tree (GBDT) algorithm. Based on the prediction results, the aeration and chemical dosage can be accurately controlled to reduce energy consumption and eliminate the potential secondary chemical contamination risk.
            ",autonomous vehicle
10.1016/j.apenergy.2020.116069,journal,Applied Energy,sciencedirect,2021-01-01,sciencedirect,Time series generative adversarial network controller for long-term smart generation control of microgrids,https://api.elsevier.com/content/article/pii/S0306261920314975,"
                  The conventional combined generation control framework of microgrids, which contains two time-scales, i.e., the time slot of economic dispatch is set to 15 min; and the total time slot of smart generation control and generation command dispatch is set to 4 s, could lead to uncoordinated problems. To avoid uncoordinated problems, this paper proposes a long-term smart generation control framework with a single time-scale to replace the conventional combined generation control framework with two time-scales, and then proposes time series generative adversarial network controller for long-term smart generation control of microgrids. The proposed time series generative adversarial network controller contains reinforcement learning, generator deep neural networks, and discriminator deep neural networks. The generator deep neural networks generate predicted states from multiple historical states, multiple historical actions, and multiple long-term actions. The discriminator deep neural networks judge whether the data from the generator deep neural networks or real-life data. This paper compares the proposed controller with conventional optimization algorithms and control algorithms, which are applied for economic dispatch, smart generation control, and generation commands dispatch in microgrids. The numerical simulation results under Hainan Power Grid, IEEE 300-bus power system, and IEEE 1951-bus power system verify that the proposed time series generative adversarial network controller can simultaneously obtain higher control performance and smaller economic cost than conventional combined control algorithm and optimization algorithms in the long-term. Consequently, the uncoordinated problem of economic dispatch, smart generation control, and generation commands dispatch can be solved by the proposed approach with one single long-term time-scale.
               ",autonomous vehicle
10.1016/j.cognition.2021.104779,journal,Cognition,sciencedirect,2021-08-31,sciencedirect,SCALa: A blueprint for computational models of language acquisition in social context,https://api.elsevier.com/content/article/pii/S0010027721001980,"
                  Theories and data on language acquisition suggest a range of cues are used, ranging from information on structure found in the linguistic signal itself, to information gleaned from the environmental context or through social interaction. We propose a blueprint for computational models of the early language learner (SCALa, for Socio-Computational Architecture of Language Acquisition) that makes explicit the connection between the kinds of information available to the social learner and the computational mechanisms required to extract language-relevant information and learn from it. SCALa integrates a range of views on language acquisition, further allowing us to make precise recommendations for future large-scale empirical research.
               ",autonomous vehicle
10.1016/j.neuroimage.2021.118048,journal,NeuroImage,sciencedirect,2021-08-01,sciencedirect,A unified framework for personalized regions selection and functional relation modeling for early MCI identification,https://api.elsevier.com/content/article/pii/S1053811921003256,"Resting-state functional magnetic resonance imaging (rs-fMRI) has been widely adopted to investigate functional abnormalities in brain diseases. Rs-fMRI data is unsupervised in nature because the psychological and neurological labels are coarse-grained, and no accurate region-wise label is provided along with the complex co-activities of multiple regions. To the best of our knowledge, most studies regarding univariate group analysis or multivariate pattern recognition for brain disease identification have focused on discovering functional characteristics shared across subjects; however, they have paid less attention to individual properties of neural activities that result from different symptoms or degrees of abnormality. In this work, we propose a novel framework that can identify subjects with early-stage mild cognitive impairment (eMCI) and consider individual variability by learning functional relations from automatically selected regions of interest (ROIs) for each subject concurrently. In particular, we devise a deep neural network composed of a temporal embedding module, an ROI selection module, and a disease-identification module. Notably, the ROI selection module is equipped with a reinforcement learning mechanism so it adaptively selects ROIs to facilitate the learning of discriminative feature representations from a temporally embedded blood-oxygen-level-dependent signals. Furthermore, our method allows us to capture the functional relations of a subject-specific ROI subset through the use of a graph-based neural network. Our method considers individual characteristics for diagnosis, as opposed to most conventional methods that identify the same biomarkers across subjects within a group. Based on the ADNI cohort, we validate the effectiveness of our method by presenting the superior performance of our network in eMCI identification. Furthermore, we provide insightful neuroscientific interpretations by analyzing the regions selected for the eMCI classification.",autonomous vehicle
10.1016/j.patcog.2017.10.009,journal,Pattern Recognition,sciencedirect,2018-05-31,sciencedirect,Multiple instance learning: A survey of problem characteristics and applications,https://api.elsevier.com/content/article/pii/S0031320317304065,"
                  Multiple instance learning (MIL) is a form of weakly supervised learning where training instances are arranged in sets, called bags, and a label is provided for the entire bag. This formulation is gaining interest because it naturally fits various problems and allows to leverage weakly labeled data. Consequently, it has been used in diverse application fields such as computer vision and document classification. However, learning from bags raises important challenges that are unique to MIL. This paper provides a comprehensive survey of the characteristics which define and differentiate the types of MIL problems. Until now, these problem characteristics have not been formally identified and described. As a result, the variations in performance of MIL algorithms from one data set to another are difficult to explain. In this paper, MIL problem characteristics are grouped into four broad categories: the composition of the bags, the types of data distribution, the ambiguity of instance labels, and the task to be performed. Methods specialized to address each category are reviewed. Then, the extent to which these characteristics manifest themselves in key MIL application areas are described. Finally, experiments are conducted to compare the performance of 16 state-of-the-art MIL methods on selected problem characteristics. This paper provides insight on how the problem characteristics affect MIL algorithms, recommendations for future benchmarking and promising avenues for research. Code is available on-line at https://github.com/macarbonneau/MILSurvey.
               ",autonomous vehicle
10.1016/j.artint.2012.04.002,journal,Artificial Intelligence,sciencedirect,2012-08-31,sciencedirect,Learning from others: Exchange of classification rules in intelligent distributed systems,https://api.elsevier.com/content/article/pii/S0004370212000410,"Learning by an exchange of knowledge and experiences enables humans to act efficiently in a very dynamic environment. Thus, it would be highly desirable to enable intelligent distributed systems to behave in a way which follows that biological archetype. We believe that knowledge exchange will become increasingly important in many application areas such as intrusion detection, driver assistance, or robotics. Constituents of a distributed system such as software agents, cars equipped with smart sensors, or intelligent robots may learn from each other by exchanging knowledge in form of classification rules, for instance. This article proposes techniques for the exchange of classification rules that represent uncertain knowledge. For that purpose, we introduce methods for knowledge acquisition in dynamic environments, for gathering and using meta-knowledge about rules (i.e., experience), and for rule exchange in distributed systems. The methods are based on a probabilistic knowledge modeling approach. We describe the results of two case studies where we show that knowledge exchange (exchange of learned rules) may be superior to information exchange (exchange of raw observations, i.e. samples) and demonstrate that the use of experiences (meta-knowledge concerning the rules) may improve that rule exchange process further. Some possible real application scenarios are sketched briefly and an application in the field of intrusion detection in computer networks is elaborated in more detail.",autonomous vehicle
10.1016/j.mehy.2018.12.001,journal,Medical Hypotheses,sciencedirect,2019-02-28,sciencedirect,A theory of general intelligence,https://api.elsevier.com/content/article/pii/S0306987718310636,"
                  This paper proposes a theoretical framework for the biological learning mechanism as a general learning system. The proposal is as follows. The bursting and tonic modes of firing patterns found in many neuron types in the brain correspond to two separate modes of information processing, with one mode resulting in awareness, and another mode being subliminal. In such a coding scheme, a neuron in bursting state codes for the highest level of perceptual abstraction representing a pattern of sensory stimuli, or volitional abstraction representing a pattern of muscle contraction sequences. Within the 50–250 ms minimum integration time of experience, the bursting neurons form synchrony ensembles to allow for binding of related percepts. The degree which different bursting neurons can be merged into the same synchrony ensemble depends on the underlying cortical connections that represent the degree of perceptual similarity. These synchrony ensembles compete for selective attention to remain active. The dominant synchrony ensemble triggers episodic memory recall in the hippocampus, while forming new episodic memory with current sensory stimuli, resulting in a stream of thoughts. Neuromodulation modulates both top-down selection of synchrony ensembles, and memory formation. Episodic memory stored in the hippocampus is transferred to semantic and procedural memory in the cortex during rapid eye movement sleep, by updating cortical neuron synaptic weights with spike timing dependent plasticity. With the update of synaptic weights, new neurons become bursting while previous bursting neurons become tonic, allowing bursting neurons to move up to a higher level of perceptual abstraction. Finally, the proposed learning mechanism is compared with the back-propagation algorithm used in deep neural networks, and a proposal of how the credit assignment problem can be addressed by the current theory is presented.
               ",autonomous vehicle
10.1016/S1364-6613(99)01327-3,journal,Trends in Cognitive Sciences,sciencedirect,1999-06-01,sciencedirect,Is imitation learning the route to humanoid robots?,https://api.elsevier.com/content/article/pii/S1364661399013273,"
                  This review investigates two recent developments in artificial intelligence and neural computation: learning from imitation and the development of humanoid robots. It is postulated that the study of imitation learning offers a promising route to gain new insights into mechanisms of perceptual motor control that could ultimately lead to the creation of autonomous humanoid robots. Imitation learning focuses on three important issues: efficient motor learning, the connection between action and perception, and modular motor control in the form of movement primitives. It is reviewed here how research on representations of, and functional connections between, action and perception have contributed to our understanding of motor acts of other beings. The recent discovery that some areas in the primate brain are active during both movement perception and execution has provided a hypothetical neural basis of imitation. Computational approaches to imitation learning are also described, initially from the perspective of traditional AI and robotics, but also from the perspective of neural network models and statistical-learning research. Parallels and differences between biological and computational approaches to imitation are highlighted and an overview of current projects that actually employ imitation learning for humanoid robots is given.
               ",autonomous vehicle
10.1016/j.compeleceng.2021.107099,journal,Computers & Electrical Engineering,sciencedirect,2021-05-31,sciencedirect,A semantic malware detection model based on the GMDH neural networks,https://api.elsevier.com/content/article/pii/S0045790621001075,"
                  There are several approaches for preventing mobile devices from malware intrusion, but most of them suffer from the insufficient accuracy required for detecting Trojan malware. A combination of semantic and machine learning techniques can be effective in preventing intrusions. In this paper, we have used a hierarchical semantic approach to convert numerical and string data to meaningful values, Subgraph Semantic Homomorphism Coefficient (SSHC) to select optimal features, and Group Method of Data Handling (GMDH) deep neural network (DNN) algorithm to detect malware via a cloud-computing infrastructure. To evaluate our model, Android Trojan Dataset has been used. After evaluation, the accuracy reached 99.91%, which was improved by about 5.25% compared to StormDroid, Drebin, and KuafuDet models. Also, the accuracy was improved by about 10.4% and 31.9% compared to machine learning based approaches of Random Forest (RF), Support Vector Machine (SVM), and K-Nearest Neighbor (KNN), in the state-of-the-art KuafuDet model, respectively.
               ",autonomous vehicle
10.1016/j.media.2012.02.005,journal,Medical Image Analysis,sciencedirect,2012-07-31,sciencedirect,Machine learning and radiology,https://api.elsevier.com/content/article/pii/S1361841512000333,"
                  In this paper, we give a short introduction to machine learning and survey its applications in radiology. We focused on six categories of applications in radiology: medical image segmentation, registration, computer aided detection and diagnosis, brain function or activity analysis and neurological disease diagnosis from fMR images, content-based image retrieval systems for CT or MRI images, and text analysis of radiology reports using natural language processing (NLP) and natural language understanding (NLU). This survey shows that machine learning plays a key role in many radiology applications. Machine learning identifies complex patterns automatically and helps radiologists make intelligent decisions on radiology data such as conventional radiographs, CT, MRI, and PET images and radiology reports. In many applications, the performance of machine learning-based automatic detection and diagnosis systems has shown to be comparable to that of a well-trained and experienced radiologist. Technology development in machine learning and radiology will benefit from each other in the long run. Key contributions and common characteristics of machine learning techniques in radiology are discussed. We also discuss the problem of translating machine learning applications to the radiology clinical setting, including advantages and potential barriers.
               ",autonomous vehicle
10.1016/0921-8890(95)00005-Z,journal,Robotics and Autonomous Systems,sciencedirect,1995-10-31,sciencedirect,Reinforcement learning of multiple tasks using a hierarchical CMAC architecture,https://api.elsevier.com/content/article/pii/092188909500005Z,"
                  A reinforcement learning approach based on modular function approximation is presented. Cerebellar Model Articulation Controller (CMAC) networks are incorporated in the Hierarchical Mixtures of Experts (HME) architecture and the resulting architecture is referred to as HME-CMAC. A computationally efficient on-line learning algorithm based on the Expectation Maximization (EM) algorithm is proposed in order to achieve fast function approximation with the HME-CMAC architecture.
                  The Compositional Q-Learning (CQ-L) framework establishes the relationship between the Q-values of composite tasks and those of elemental tasks in its decomposition. This framework is extended here to allow rewards in non-terminal states. An implementation of the extended CQ-L framework using the HME-CMAC architecture is used to perform task decomposition in a realistic simulation of a two-linked manipulator having non-linear dynamics. The context-dependent reinforcement learning achieved by adopting this approach has advantages over monolithic approaches in terms of speed of learning, storage requirements and the ability to cope with changing goals.
               ",autonomous vehicle
10.1016/j.scs.2019.101533,journal,Sustainable Cities and Society,sciencedirect,2019-07-31,sciencedirect,Modeling and forecasting building energy consumption: A review of data-driven techniques,https://api.elsevier.com/content/article/pii/S2210670718323862,"
                  Building energy consumption modeling and forecasting is essential to address buildings energy efficiency problems and take up current challenges of human comfort, urbanization growth and the consequent energy consumption increase. In a context of integrated smart infrastructures, data-driven techniques rely on data analysis and machine learning to provide flexible methods for building energy prediction. The present paper offers a review of studies developing data-driven models for building scale applications. The prevalent methods are introduced with a focus on the input data characteristics and data pre-processing methods, the building typologies considered, the targeted energy end-uses and forecasting horizons, and accuracy assessment. A special attention is also given to different machine learning approaches. Based on the results of this review, the latest technical improvements and research efforts are synthesized. The key role of occupants’ behavior integration in data-driven modeling is discussed. Limitations and research gaps are highlighted. Future research opportunities are also identified.
               ",autonomous vehicle
10.1016/j.conbuildmat.2015.09.058,journal,Construction and Building Materials,sciencedirect,2015-12-15,sciencedirect,CaPrM: Carbonation prediction model for reinforced concrete using machine learning methods,https://api.elsevier.com/content/article/pii/S0950061815304165,"
                  Reliable carbonation depth prediction of concrete structures is crucial for optimizing their design and maintenance. The challenge of conventional carbonation prediction models is capturing the complex relationship between governing parameters. To improve the accuracy and methodology of the prediction a machine learning based carbonation prediction model which integrates four learning methods is introduced. The model developed considers parameters influencing the carbonation process and enables the user to choose the best alternative of the machine based methods. The applicability of the method is demonstrated by an example where the carbonation depths are estimated using the developed model and verified with unseen data. The evaluation proofs that the model predicts the carbonation depth with a high accuracy.
               ",autonomous vehicle
10.1016/S0065-2458(08)60167-9,journal,Advances in Computers,sciencedirect,1991-12-31,sciencedirect,Neurocomputing Formalisms for Computational Learning and Machine Intelligence,https://api.elsevier.com/content/article/pii/S0065245808601679,"
                  The chapter discusses the capabilities of neural-network learning that is central to the deeper question of its feasibility to artificial intelligence. It focuses on machine learning in the context of neural networks from the standpoints of computational complexity and algorithms information theory, and on the emerging area of learning theory in the context of dynamic systems. Engineered intelligent systems behave with remarkable rigidity when compared with, their biological counterparts, especially in their ability to recognize objects or speech, to manipulate and adapt in an unstructured environment, and to learn from past experience. A major reason for this limited technical success in emulating some of the more fundamental aspects of human intelligence lies in the differences between the organization and structuring of knowledge, and the dynamics of biological neuronal circuitry and its emulation using the symbolic-processing paradigm. The chapter rederives a theoretical framework for neural learning of nonlinear mappings, wherein both the topology of the network and synaptic interconnection strengths are evolved adaptively. The proposed methodology exploits a new class of mathematical constructs, terminal attractors, which provide unique information-processing capabilities to artificial neural systems. Terminal attractor representations are used not only to ensure infinite local stability of the encoded information, but also to provide a qualitative as well as quantitative change in the nature of the learning process. The chapter also draws from mathematical constructs in sensitivity theory for nonlinear systems to illustrate the notion of forward and adjoint-operators. The formalism exploits the concept of adjoint-operators to enable a fast global computation of the network's response to perturbations in all system parameters. This formalism eliminates the heuristic overtones of the proposed framework.
               ",autonomous vehicle
10.1016/j.neuroscience.2020.06.019,journal,Neuroscience,sciencedirect,2021-05-10,sciencedirect,"50 Years Since the Marr, Ito, and Albus Models of the Cerebellum",https://api.elsevier.com/content/article/pii/S0306452220303961,"Fifty years have passed since David Marr, Masao Ito, and James Albus proposed seminal models of cerebellar functions. These models share the essential concept that parallel-fiber-Purkinje-cell synapses undergo plastic changes, guided by climbing-fiber activities during sensorimotor learning. However, they differ in several important respects, including holistic versus complementary roles of the cerebellum, pattern recognition versus control as computational objectives, potentiation versus depression of synaptic plasticity, teaching signals versus error signals transmitted by climbing-fibers, sparse expansion coding by granule cells, and cerebellar internal models. In this review, we evaluate different features of the three models based on recent computational and experimental studies. While acknowledging that the three models have greatly advanced our understanding of cerebellar control mechanisms in eye movements and classical conditioning, we propose a new direction for computational frameworks of the cerebellum, that is, hierarchical reinforcement learning with multiple internal models.",autonomous vehicle
10.1016/B978-0-12-815822-7.00005-4,journal,Spatiotemporal Analysis of Air Pollution and Its Application in Public Health,sciencedirect,2020-12-31,sciencedirect,Chapter 5: Machine learning for spatiotemporal big data in air pollution,https://api.elsevier.com/content/article/pii/B9780128158227000054,"
               An accurate understanding of air pollutants in a continuous space-time domain is critical for meaningful assessment of the quantitative relationship between the adverse health effects and the concentrations of air pollutants. Traditional interpolation methods, including various statistic and nonstatistic regression models, typically involve restrictive assumptions regarding independence of observations and distributions of outcomes. Moreover, a set of relationships among variables need to be defined strictly in advance. Machine learning opens a new door to understand the air pollution data based on the exposing data-driven relationships and predicting outcomes without empirical models. In this chapter, the state-of-the-art machine learning methods will be introduced to unlock the full potential of the air pollutant data, that is, to estimate the PM2.5 concentration more accurately in the spatiotemporal domain. The methods can be extended to the other air pollutants.
            ",autonomous vehicle
10.1016/j.ijcip.2021.100436,journal,International Journal of Critical Infrastructure Protection,sciencedirect,2021-09-30,sciencedirect,Fault-tolerant AI-driven Intrusion Detection System for the Internet of Things,https://api.elsevier.com/content/article/pii/S1874548221000287,"
                  Internet of Things (IoT) has emerged as a key component of all advanced critical infrastructures. However, with the challenging nature of IoT, new security breaches have been introduced, especially against the Routing Protocol for Low-power and Lossy Networks (RPL). Artificial-Intelligence-based technologies can be used to provide insights to deal with IoT’s security issues. In this paper, we describe the initial stages of developing, a new Intrusion Detection System using Machine Learning (ML) to detect routing attacks against RPL. We first simulate the routing attacks and capture the traffic for different topologies. We then process the traffic and generate large 2-class and multi-class datasets. We select a set of significant features for each attack, and we use this set to train different classifiers to make the IDS. The experiments with 5-fold cross-validation demonstrated that decision tree (DT), random forests (RF), and K-Nearest Neighbours (KNN) achieved good results of more than 99% value for accuracy, precision, recall, and F1-score metrics, and RF has achieved the lowest fitting time. On the other hand, Deep Learning (DL) model, MLP, Naïve Bayes (NB), and Logistic Regression (LR) have shown significantly lower performance.
               ",autonomous vehicle
10.1016/j.dsx.2020.05.008,journal,Diabetes & Metabolic Syndrome: Clinical Research & Reviews,sciencedirect,2020-08-31,sciencedirect,A review of modern technologies for tackling COVID-19 pandemic,https://api.elsevier.com/content/article/pii/S1871402120301272,"
                  Objective
                  Science and technology sector constituting of data science, machine learning and artificial intelligence are contributing towards COVID-19. The aim of the present study is to discuss the various aspects of modern technology used to fight against COVID-19 crisis at different scales, including medical image processing, disease tracking, prediction outcomes, computational biology and medicines.
               
                  Methods
                  A progressive search of the database related to modern technology towards COVID-19 is made. Further, a brief review is done on the extracted information by assessing the various aspects of modern technologies for tackling COVID-19 pandemic.
               
                  Results
                  We provide a window of thoughts on review of the technology advances used to decrease and smother the substantial impact of the outburst. Though different studies relating to modern technology towards COVID-19 have come up, yet there are still constrained applications and contributions of technology in this fight.
               
                  Conclusions
                  On-going progress in the modern technology has contributed in improving people’s lives and hence there is a solid conviction that validated research plans including artificial intelligence will be of significant advantage in helping people to fight this infection.
               ",autonomous vehicle
10.1016/j.engappai.2020.103894,journal,Engineering Applications of Artificial Intelligence,sciencedirect,2020-10-31,sciencedirect,Artificial neural networks in microgrids: A review,https://api.elsevier.com/content/article/pii/S0952197620302372,"
                  In this work it is shown that artificial neural networks have certain characteristics that make them advantageous in the development of controllers in the different levels of control that microgrids must include to be economic, efficient and able to satisfy the energy power quality and quantity requirements. An objective of this paper is to bring attention to the promising applicability of artificial neural networks applied to the control of microgrid distributed generation sources, as well as in scheduling, power sharing, supervisory control and optimization.
               ",autonomous vehicle
10.1016/j.eswa.2017.12.043,journal,Expert Systems with Applications,sciencedirect,2018-05-01,sciencedirect,A machine learning approach to synchronization of automata,https://api.elsevier.com/content/article/pii/S0957417417308655,"
                  We present a novel method to predict the length of the shortest synchronizing words of a finite automaton by applying the machine learning approach. We introduce several so-called automata features which depict the structure of an automaton, and use them with machine learning algorithms. The article discusses effectiveness of the machine learning approach in predicting the length of the shortest synchronizing words. We also examine the impact of particular features on this length, which may be helpful in methods of constructing automata as models of real systems, algorithms finding synchronizing words, and further theoretical research on synchronizing automata and the Černý conjecture.
               ",autonomous vehicle
10.1016/j.enbuild.2020.110520,journal,Energy and Buildings,sciencedirect,2021-01-01,sciencedirect,Generation of whole building renovation scenarios using variational autoencoders,https://api.elsevier.com/content/article/pii/S0378778820304904,"
                  Buildings consume a huge amount of energy, resulting in a considerable impact on the environment. In Canada, almost 70% of the total energy used by the commercial and institutional sectors was consumed by Heating, Ventilation and Air-Conditioning (HVAC) and lighting systems, which makes them the main targets of energy performance optimization methods. Furthermore, based on a governmental report, 40% of Quebec university buildings are in poor or very poor shape regarding structure and materials, and require immediate renovation. Therefore, it is of utmost importance to reduce energy consumption, and this can be accomplished by improving the design of new buildings or by renovating existing ones. Moreover, Simulation-Based Multi-Objective Optimization (SBMO) models can be used for optimizing and assessing different renovation scenarios considering Total Energy Consumption (TEC) and Life Cycle Cost (LCC). The time-consuming nature of SBMO has triggered the development of simplified and surrogate models within the design process. This study proposes a generative deep learning building energy model using Variational Autoencoders (VAEs), which could potentially overcome the current limitations. The proposed VAEs extract deep features from a whole building renovation dataset and generate renovation scenarios considering TEC and LCC of the existing institutional buildings. The proposed model also has the generalization ability due to its potential to reuse the dataset from a specific case in similar situations. The performance of the developed model has been demonstrated using a simulated renovation dataset to prove its potential. The results show that using generative VAEs is acceptable considering computational time and accuracy.
               ",autonomous vehicle
10.1016/j.neucom.2021.01.091,journal,Neurocomputing,sciencedirect,2021-05-21,sciencedirect,Semi-supervised point cloud segmentation using self-training with label confidence prediction,https://api.elsevier.com/content/article/pii/S0925231221001739,"
                  Point cloud segmentation is a key problem in 3D content understanding. The existing methods based on deep neural network for point cloud segmentation mainly train the network in a supervised fashion, which heavily rely on a large amount of high-quality manual-labeled training point clouds. However, it is very tedious and time-consuming to manually assign part labels for each point in point clouds. Meanwhile, a lot of unlabeled point clouds can easily be obtained from 3D scanners, Internet or reconstruction. Therefore, we introduce self-training to utilize these unlabeled point clouds. So the proposed semi-supervised point cloud segmentation method can employ both labeled point clouds and unlabeled point clouds for training. Moreover, in order to make better use of unlabeled point clouds, the adopted adversarial architecture proposes confidence discrimination of label prediction for unlabeled point clouds. Thus, pseudo labels on unlabeled point clouds with higher reliability can be picked out to participate the network training, which further improves segmentation performance. The experiments show that the proposed method can make full use of the unlabeled point clouds in training. In addition, segmentation performance improves by self-training with label confidence prediction.
               ",autonomous vehicle
10.1016/j.biosystems.2021.104430,journal,Biosystems,sciencedirect,2021-08-31,sciencedirect,Living systems are smarter bots: Slime mold semiosis versus AI symbol manipulation,https://api.elsevier.com/content/article/pii/S030326472100085X,"
                  Although machines may be good at mimicking, they are not currently able, as organisms are, to act creatively. We offer an understanding of the emergent qualities of biological sign processing in terms of generalization, association, and encryption. We use slime mold as a model of minimal cognition and compare it to deep-learning video game bots, which some claim have evolved beyond their merely quantitative algorithms. We find that these discrete Turing machine bots are not able to make productive, yet unanticipated, “errors”—necessary for biological learning—which, based on the physicality of signs, their relatively similar shapes, and relative physical positions spatially and temporally, lead to emergent effects and make learning and evolution possible. In organisms, stochastic resonance at the local level can be leveraged for self-organization at the global level. We contrast all this to the symbolic processing of today's machine learning, whereby each logic node and memory state is discrete. Computer codes are produced by external operators, whereas biological symbols are evolved through an internal encryption process.
               ",autonomous vehicle
10.1016/j.iot.2019.100118,journal,Internet of Things,sciencedirect,2019-12-31,sciencedirect,"Transformative effects of IoT, Blockchain and Artificial Intelligence on cloud computing: Evolution, vision, trends and open challenges",https://api.elsevier.com/content/article/pii/S2542660519302331,"
                  Cloud computing plays a critical role in modern society and enables a range of applications from infrastructure to social media. Such system must cope with varying load and evolving usage reflecting societies’ interaction and dependency on automated computing systems whilst satisfying Quality of Service (QoS) guarantees. Enabling these systems are a cohort of conceptual technologies, synthesized to meet demand of evolving computing applications. In order to understand current and future challenges of such system, there is a need to identify key technologies enabling future applications. In this study, we aim to explore how three emerging paradigms (Blockchain, IoT and Artificial Intelligence) will influence future cloud computing systems. Further, we identify several technologies driving these paradigms and invite international experts to discuss the current status and future directions of cloud computing. Finally, we proposed a conceptual model for cloud futurology to explore the influence of emerging paradigms and technologies on evolution of cloud computing.
               ",autonomous vehicle
10.1016/j.procs.2019.01.118,journal,Procedia Computer Science,sciencedirect,2019-12-31,sciencedirect,Automatic data labeling by neural networks for the counting of objects in videos,https://api.elsevier.com/content/article/pii/S1877050919301255,"The paper proposes an efficient method for training a neural network to count moving objects in a video, while another neural network concurrently prepares a labeled dataset for the first one. The detection, tracking, and counting of objects is crucial for effective Intelligence Transportation Systems (ITS), which should reduce congestion and recognize traffic offenders on highways and in urban areas. Creation of labeled data for training a neural network is one of the essential prerequisites for successful application of supervised machine learning. In this paper, the experimental results of the automatic labeling and counting of vehicles under real world conditions are shown. The method shows that by using the Convolutional Neural Network (CNN), the computing power and speed-up time for training a Recurrent Neural Network (RNN) with a Long Short-Term Memory (LSTM) cell for counting moving objects can be decreased.",autonomous vehicle
10.1016/B978-0-12-814425-1.00003-6,journal,Biopharmaceutics and Pharmacokinetics Considerations,sciencedirect,2021-12-31,sciencedirect,Chapter 23: Artificial intelligence in preventive and managed healthcare,https://api.elsevier.com/content/article/pii/B9780128144251000036,"
               Increasing application on account of the rapid progress made by artificial intelligence (AI) in healthcare has brought upon a progressive paradigm shift. By combining relevant AI architectures with digitized data acquisition and sophisticated data validation techniques, AI-based technologies are expanding to unchartered areas. This chapter focuses on understanding recent novel innovations and practical clinical applications of AI in aiding the conventional healthcare industry. Breakthrough AI-based platforms that aid in critical sectors of the healthcare industry like disease diagnosis, robot-assisted surgery, patient rehabilitation, and use of smartphones/smart wearables for health monitoring and AI’s role in controlling and tackling COVID-19 like pandemics are summarized.
            ",autonomous vehicle
10.1016/j.neucom.2003.10.005,journal,Neurocomputing,sciencedirect,2004-03-31,sciencedirect,An integrated learning approach to environment modelling in mobile robot navigation,https://api.elsevier.com/content/article/pii/S0925231203005125,"
                  We extend the approach to learning a topological description of the environment with recurrent neural networks. Usually, a predetermined reactive behavior and a predefined criterion for decision points are used. In our extended approach, both the reactive behavior and the criterion for the decision points are adaptive and therefore more flexible. The reactive behavior is learnt using reinforcement learning supplemented by a new, psychologically grounded mechanism that enables the robot to autonomously explore the environment in a useful way for the purposes of modelling. Decision points or situations where a deviation from the reactive behavior is allowed are learnt on-line using a novel criterion based on the information theory. Results of experiments conducted with a simulated mobile robot equipped with proximity sensors and a color video camera show applicability of the proposed approach.
               ",autonomous vehicle
10.1016/j.neucom.2017.02.090,journal,Neurocomputing,sciencedirect,2017-11-01,sciencedirect,Unsupervised identification and recognition of situations for high-dimensional sensori-motor streams,https://api.elsevier.com/content/article/pii/S0925231217309840,"
                  An important question in self-learning robots is how robots can autonomously learn about and act in their environment in an on-line and unsupervised manner. This paper introduces and evaluates Context Recognition in Data Streams (CoRDS), a method that enables a robot to identify and recognise different situations in its environment. CoRDS achieves this by processing the data stream from the robot’s sensors to distinguish different patterns that identify different environmental situations. We evaluated the CoRDS method by means of quantitative and qualitative analysis on three different data streams: one synthetic and two data sets with actual sensor data generated by Thymio II robots. Our analyses showed that CoRDS created active cluster patterns that, for all three data streams, corresponded with the experimenters’ expectations. Experiments varying the parameters of the CoRDS method indicated a consistent response over all three data streams. These findings suggest that CoRDS may provide a basis for data stream clustering techniques that can be applied for the task of situation recognition.
               ",autonomous vehicle
10.1016/j.neucom.2015.02.092,journal,Neurocomputing,sciencedirect,2016-01-22,sciencedirect,Review of advances in neural networks: Neural design technology stack,https://api.elsevier.com/content/article/pii/S0925231215010358,"
                  This review provides a high-level synthesis of significant recent advances in artificial neural network research, as well as multi-disciplinary concepts connected to the far-reaching goal of obtaining intelligent systems. We assume that a global outlook of these interconnected fields can benefit researchers by providing alternative viewpoints. Therefore, we present different network and neuron models, we discuss model parameters and the means to obtain them, and we draw a quick outline of information encoding, before proceeding to an overview of the relevant learning mechanisms, ranging from established approaches to novel ideas. We specifically focus on comparing the classical artificial model with the biologically-feasible spiking neuron, and we take this comparison further into a discussion on the biological plausibility of various learning approaches.
               ",autonomous vehicle
10.1016/j.cja.2021.10.006,journal,Chinese Journal of Aeronautics,sciencedirect,2021-11-09,sciencedirect,Rotating machinery fault detection and diagnosis based on deep domain adaptation: A survey,https://api.elsevier.com/content/article/pii/S100093612100368X,"In practical mechanical fault detection and diagnosis, it is difficult and expensive to collect enough large-scale supervised data to train deep networks. Transfer learning can reuse the knowledge obtained from the source task to improve the performance of the target task, which performs well on small data and reduces the demand for high computation power. However, the detection performance is significantly reduced by the direct transfer due to the domain difference. Domain adaptation (DA) can transfer the distribution information from the source domain to the target domain and solve a series of problems caused by the distribution difference of data. In this survey, we review various current DA strategies combined with deep learning (DL) and analyze the principles, advantages, and disadvantages of each method. We also summarize the application of DA combined with DL in the field of fault diagnosis. This paper provides a summary of the research results and proposes future work based on analysis of the key technologies.",autonomous vehicle
10.1016/j.cma.2020.113442,journal,Computer Methods in Applied Mechanics and Engineering,sciencedirect,2020-12-01,sciencedirect,Development of an algorithm for reconstruction of droplet history based on deposition pattern using computational fluid dynamics and convolutional neural network,https://api.elsevier.com/content/article/pii/S0045782520306277,"
                  Liquids are one of the fundamental components for the functionality of a wide range of mechanical equipment (e.g. for lubrication, cooling, hydraulic, etc.). However, they could lead to secondary issues such as corrosion or contamination, caused, for instance, due to leakage of the liquid. As investigating the equipment during their operation is not always possible (e.g. for rotary machinery or in case of high-temperature working conditions), locating the origin of the leakage could be a challenging task, especially if the only traces left behind are a few droplets. In the present work, an algorithm for prediction of the leakage position, i.e. position of the injector, is developed. In order to guarantee intelligence and enhance flexibility, the designed algorithm is powered by a machine learning approach, Convolutional Neural Network. The developed algorithm is based upon using different deposition patterns, calculated by numerical simulations, as the input. The robust algorithm is designed to be so intelligent that it could ideally traceback (predict) the leakage location (i.e. reconstruct the droplet history) by only an image of the deposition.
               ",autonomous vehicle
10.1016/j.ymssp.2020.107108,journal,Mechanical Systems and Signal Processing,sciencedirect,2021-01-15,sciencedirect,Fusing convolutional generative adversarial encoders for 3D printer fault detection with only normal condition signals,https://api.elsevier.com/content/article/pii/S0888327020304945,"
                  Collecting data from mechanical systems in abnormal conditions is expensive and time consuming. Consequently, fault detection approaches based on classical supervised learning working with both normal and abnormal data are not applicable in some condition-based maintenance tasks. To address this problem, this paper proposes Fusing Convolutional Generative Adversarial Encoders (fCGAE) method to create fault detection models from only normal data. Firstly, to obtain an adequate deep feature space, encoder models based on 1D convolutional neural networks are created. Then, these encoders are optimized in an unsupervised way through Bidirectional Generative Adversarial Networks. Finally, the multi-channel features collected from the system are merged with One-Class Support Vector Machine. fCGAE is applied to fault detection in 3D printers, where experimental results in two fault detection cases show excellent generalization capabilities and better performance compared to peer methods.
               ",autonomous vehicle
10.1016/B978-0-12-814455-8.00003-7,journal,The Future of Pharmaceutical Product Development and Research,sciencedirect,2020-12-31,sciencedirect,Chapter 3: Artificial intelligence in the pharmaceutical sector: current scene and future prospect,https://api.elsevier.com/content/article/pii/B9780128144558000037,"
               Recently, artificial intelligence is growing rapidly in the pharmaceutical sector as well as the healthcare system. This new system showed its potential benefits in different pharmaceutical sectors like drug discovery, continuous manufacturing, dosage form design, quality control, and many more. This book chapter discusses different artificial intelligence tools and different applications of artificial intelligence in the pharmaceutical sector. Moreover, it also gives an idea about the implementation of artificial intelligence in the healthcare system and its potential benefits for the community. Lastly, it evokes some challenges and hurdles that are associated with the implementation of artificial intelligence in the pharmaceutical sector.
            ",autonomous vehicle
10.1016/j.rser.2015.11.064,journal,Renewable and Sustainable Energy Reviews,sciencedirect,2016-04-30,sciencedirect,A review on performance of artificial intelligence and conventional method in mitigating PV grid-tied related power quality events,https://api.elsevier.com/content/article/pii/S1364032115013313,"
                  Integration of renewable energy resources into power networks is the trend in power distribution system. It is to reduce burden of centralized power plant and global emissions, increase usage of renewable energy, and diverse energy supply market. However, solar photovoltaic which is a type of renewable energy resource, is found to generate peak capacity for a short duration only. Next, its output is intermittent and randomness. In addition, it changes behavior of power distribution system from unidirectional to bidirectional. As a result, it causes different types of power quality events to the power networks. Therefore, these power quality events are urged to be mitigated to further explore the potential of solar photovoltaic system. This paper aims to investigate negative impacts of photovoltaic (PV) grid-tied system to the power networks, and study on performance of artificial intelligence (AI) and conventional methods in mitigating power quality event. According to the surveys, power system monitoring, inverter, dynamic voltage regulator, static synchronous compensator, unified power quality conditioner and energy storage system are able to compensate power quality events which are caused by PV grid-tied system. From the studies, AI methods usually outperform conventional methods in terms of response time and controllability. They also show talent in multi-mode operation, which is to switch to different operation modes according to the environment. However, they require memory to achieve abovementioned tasks. It is believed that unsupervised learning AI is the future trend as it can adapt to the environment without the need of collecting large amount of data before the AI is implemented.
               ",autonomous vehicle
10.1016/j.euf.2021.04.006,journal,European Urology Focus,sciencedirect,2021-04-30,sciencedirect,Ethical implications of AI in robotic surgical training: A Delphi consensus statement,https://api.elsevier.com/content/article/pii/S2405456921001127,"
                  Context
                  As the role of AI in healthcare continues to expand there is increasing awareness of the potential pitfalls of AI and the need for guidance to avoid them.
               
                  Objectives
                  To provide ethical guidance on developing narrow AI applications for surgical training curricula. We define standardised approaches to developing AI driven applications in surgical training that address current recognised ethical implications of utilising AI on surgical data. We aim to describe an ethical approach based on the current evidence, understanding of AI and available technologies, by seeking consensus from an expert committee.
               
                  Evidence acquisition
                  The project was carried out in 3 phases: (1) A steering group was formed to review the literature and summarize current evidence. (2) A larger expert panel convened and discussed the ethical implications of AI application based on the current evidence. A survey was created, with input from panel members. (3) Thirdly, panel-based consensus findings were determined using an online Delphi process to formulate guidance. 30 experts in AI implementation and/or training including clinicians, academics and industry contributed. The Delphi process underwent 3 rounds. Additions to the second and third-round surveys were formulated based on the answers and comments from previous rounds. Consensus opinion was defined as ≥ 80% agreement.
               
                  Evidence synthesis
                  There was 100% response from all 3 rounds. The resulting formulated guidance showed good internal consistency, with a Cronbach alpha of >0.8. There was 100% consensus that there is currently a lack of guidance on the utilisation of AI in the setting of robotic surgical training. Consensus was reached in multiple areas, including: 1. Data protection and privacy; 2. Reproducibility and transparency; 3. Predictive analytics; 4. Inherent biases; 5. Areas of training most likely to benefit from AI.
               
                  Conclusions
                  Using the Delphi methodology, we achieved international consensus among experts to develop and reach content validation for guidance on ethical implications of AI in surgical training. Providing an ethical foundation for launching narrow AI applications in surgical training. This guidance will require further validation.
               
                  Patient summary
                  As the role of AI in healthcare continues to expand there is increasing awareness of the potential pitfalls of AI and the need for guidance to avoid them.In this paper we provide guidance on ethical implications of AI in surgical training.
               ",autonomous vehicle
10.1016/j.specom.2016.10.010,journal,Speech Communication,sciencedirect,2017-02-28,sciencedirect,An online model for vowel imitation learning,https://api.elsevier.com/content/article/pii/S0167639315300728,"
                  When infants learn to pronounce speech sounds of their native language, they face the so-called correspondence problem – how to know which articulatory gestures lead to acoustic sounds that are recognized as native speech sounds by other speakers? Previous research suggests that infants might not learn to imitate their parents via autonomous babbling because direct evaluation of the acoustic similarity between the speech sounds of the two is not possible due to different spectral characteristics of the voices caused by differing vocal tract morphologies. We present a novel robust model of infant vowel imitation learning, following a hypothesis that an infant learns to match their productions to their caregiver's speech sounds when the caregiver imitates the infant's babbles. Adapting a cross-situational associative learning technique, evidently present in infant word learning, our simulated language learner can cope with ambiguity in caregiver's responses to babbling as well as with the imprecision of the articulatory gestures of the infant itself. Our fully online learning model also combines vocal exploration and imitative interaction into a single process. Learning performance is evaluated in experiments using Finnish adults as caregivers for a virtual infant, responding to the infant's babbles with lexical words and, after a learning stage, evaluating the quality of the vowels produced by the learner. After 1000 babble-response pairs, our virtual infant is seen to reach a satisfying vowel imitation accuracy of 70–80%.
               ",autonomous vehicle
10.1016/j.patcog.2020.107582,journal,Pattern Recognition,sciencedirect,2021-01-31,sciencedirect,Local minima found in the subparameter space can be effective for ensembles of deep convolutional neural networks,https://api.elsevier.com/content/article/pii/S003132032030385X,"
                  Ensembles of deep convolutional neural networks (CNNs), which integrate multiple deep CNN models to achieve better generalization for an artificial intelligence application, now play an important role in ensemble learning due to the dominant position of deep learning. However, the usage of ensembles of deep CNNs is still not adequate because the increasing complexity of deep CNN architectures and the emerging data with large dimensionality have made the training stage and testing stage of ensembles of deep CNNs inevitably expensive. To alleviate this situation, we propose a new approach that finds multiple models converging to local minima in subparameter space for ensembles of deep CNNs. The subparameter space here refers to the space constructed by a partial selection of parameters, instead of the entire set of parameters, of a deep CNN architecture. We show that local minima found in the subparameter space of a deep CNN architecture can in fact be effective for ensembles of deep CNNs to achieve better generalization. Moreover, finding local minima in the subparameter space of a deep CNN architecture is more affordable at the training stage, and the multiple models at the found local minima can also be selectively fused to achieve better ensemble generalization while limiting the expense to a single deep CNN model at the testing stage. Demonstrations of MobilenetV2, Resnet50 and InceptionV4 (deep CNN architectures from lightweight to complex) on ImageNet, CIFAR-10 and CIFAR-100, respectively, lead us to believe that finding local minima in the subparameter space of a deep CNN architecture could be leveraged to broaden the usage of ensembles of deep CNNs.
               ",autonomous vehicle
10.1016/j.eswa.2021.115742,journal,Expert Systems with Applications,sciencedirect,2021-12-30,sciencedirect,"A review on social spam detection: Challenges, open issues, and future directions",https://api.elsevier.com/content/article/pii/S0957417421011209,"
                  Online Social Networks are perpetually evolving and used in plenteous applications such as content sharing, chatting, making friends/followers, customer engagements, commercials, product reviews/promotions, online games, and news, etc. The substantial issues related to the colossal flood of social spam in social media are polarizing sentiments, impacting users’ online interaction time, degrading available information quality, network bandwidth, computing power, and speed. Simultaneously, groups of coordinated automated accounts/bots often use social networking sites to spread spam, rumors, bogus reviews, and fake news for targeted users or mass communication. The latest developments in the form of artificial intelligence-enabled Deepfakes have exacerbated these issues at large. Consequently, it becomes extremely relevant to review recent work concerning social spam and spammer detection to counter this issue and its effect. This paper provides a brief introduction to social spam, the spamming process, and social spam taxonomy. The comprehensive review entails several dimensionality reduction techniques used for feature selection/extraction, features used, various machine learning and deep learning techniques used for social spam and spammer detection, and their merits and demerits. Artificial intelligence and deep learning empowered Deepfake (text, image, and video) spam, and their countermeasures are also explored. Furthermore, meticulous discussions, existing challenges, and emerging issues such as robustness of detection systems, scalability, real-time datasets, evade strategies used by spammers, coordinated inauthentic behavior, and adversarial attacks on machine learning-based spam detectors, etc., have been discussed with possible directions for future research.
               ",autonomous vehicle
10.1016/j.specom.2016.10.010,journal,Speech Communication,sciencedirect,2017-02-28,sciencedirect,An online model for vowel imitation learning,https://api.elsevier.com/content/article/pii/S0167639315300728,"
                  When infants learn to pronounce speech sounds of their native language, they face the so-called correspondence problem – how to know which articulatory gestures lead to acoustic sounds that are recognized as native speech sounds by other speakers? Previous research suggests that infants might not learn to imitate their parents via autonomous babbling because direct evaluation of the acoustic similarity between the speech sounds of the two is not possible due to different spectral characteristics of the voices caused by differing vocal tract morphologies. We present a novel robust model of infant vowel imitation learning, following a hypothesis that an infant learns to match their productions to their caregiver's speech sounds when the caregiver imitates the infant's babbles. Adapting a cross-situational associative learning technique, evidently present in infant word learning, our simulated language learner can cope with ambiguity in caregiver's responses to babbling as well as with the imprecision of the articulatory gestures of the infant itself. Our fully online learning model also combines vocal exploration and imitative interaction into a single process. Learning performance is evaluated in experiments using Finnish adults as caregivers for a virtual infant, responding to the infant's babbles with lexical words and, after a learning stage, evaluating the quality of the vowels produced by the learner. After 1000 babble-response pairs, our virtual infant is seen to reach a satisfying vowel imitation accuracy of 70–80%.
               ",autonomous vehicle
10.1016/j.procs.2021.06.098,journal,Procedia Computer Science,sciencedirect,2021-12-31,sciencedirect,Contemporary trends in privacy-preserving data pattern recognition,https://api.elsevier.com/content/article/pii/S1877050921013533,"The article is devoted to the recent scientific problem of privacy-preserving data pattern recognition. The purposes of the work are to systematize the security models for such tasks, to identify algorithmic tools that can be used to ensure the privacy of the data processing, and application of models and to analyze the privacy-preserving data pattern recognition systems. The article presents the main concepts and some definitions related to privacy-preserving machine learning, gives a systematization of related problems, and notes modern and promising areas of development of machine learning. Special cryptographic methods and protocols are correlated to the solved problems. A brief description of the known privacy-preserving data pattern recognition systems is given. Unsolved problems in the field of privacy-preserving data pattern recognition are considered.",autonomous vehicle
10.1016/j.jjimei.2020.100004,journal,International Journal of Information Management Data Insights,sciencedirect,2021-04-30,sciencedirect,Generative adversarial network: An overview of theory and applications,https://api.elsevier.com/content/article/pii/S2667096820300045,"In recent times, image segmentation has been involving everywhere including disease diagnosis to autonomous vehicle driving. In computer vision, this image segmentation is one of the vital works and it is relatively complicated than other vision undertakings as it needs low-level spatial data. Especially, Deep Learning has impacted the field of segmentation incredibly and gave us today different successful models. The deep learning associated Generated Adversarial Networks (GAN) has presenting remarkable outcomes on image segmentation. In this study, the authors have presented a systematic review analysis on recent publications of GAN models and their applications. Three libraries such as Embase (Scopus), WoS, and PubMed have been considered for searching the relevant papers available in this area. Search outcomes have identified 2084 documents, after two-phase screening 52 potential records are included for final review. The following applications of GAN have been emerged: 3D object generation, medicine, pandemics, image processing, face detection, texture transfer, and traffic controlling. Before 2016, research in this field was limited and thereafter its practical usage came into existence worldwide. The present study also envisions the challenges associated with GAN and paves the path for future research in this realm.",autonomous vehicle
10.1016/j.ygeno.2017.01.004,journal,Genomics,sciencedirect,2017-03-31,sciencedirect,Gene selection for microarray cancer classification using a new evolutionary method employing artificial intelligence concepts,https://api.elsevier.com/content/article/pii/S0888754317300046,"Gene selection is a demanding task for microarray data analysis. The diverse complexity of different cancers makes this issue still challenging. In this study, a novel evolutionary method based on genetic algorithms and artificial intelligence is proposed to identify predictive genes for cancer classification. A filter method was first applied to reduce the dimensionality of feature space followed by employing an integer-coded genetic algorithm with dynamic-length genotype, intelligent parameter settings, and modified operators. The algorithmic behaviors including convergence trends, mutation and crossover rate changes, and running time were studied, conceptually discussed, and shown to be coherent with literature findings. Two well-known filter methods, Laplacian and Fisher score, were examined considering similarities, the quality of selected genes, and their influences on the evolutionary approach. Several statistical tests concerning choice of classifier, choice of dataset, and choice of filter method were performed, and they revealed some significant differences between the performance of different classifiers and filter methods over datasets. The proposed method was benchmarked upon five popular high-dimensional cancer datasets; for each, top explored genes were reported. Comparing the experimental results with several state-of-the-art methods revealed that the proposed method outperforms previous methods in DLBCL dataset.",autonomous vehicle
10.3182/20080706-5-KR-1001.02614,journal,IFAC Proceedings Volumes,sciencedirect,2008-12-31,sciencedirect,Learning and Adaptation of Skills in Autonomous Physical Agents,https://api.elsevier.com/content/article/pii/S1474667016414795,"
                  A skills learning methodology is presented for autonomous physical agents. Adaptation of skills and learning is a fundamental part of the simple agent behaviours outlined. A general framework of skills learning is described that uses skill macros to define simple behaviours by agents that communicate, sense and act in the physical world. Programmed playfulness can be easily implemented in this framework that plays an important part in acquiring sophisticated skills. Reusability of results in learning algorithms is supported by ontology based classification of learning in skills. Ontologies provide references to object instances that enable modularization of software and easy interfacing of skills with learning algorithms.
               ",autonomous vehicle
10.1016/j.radmp.2020.11.002,journal,Radiation Medicine and Protection,sciencedirect,2020-12-31,sciencedirect,A review on 3D deformable image registration and its application in dose warping,https://api.elsevier.com/content/article/pii/S2666555720300629,"Deformable image registration (DIR) has been well explored in recent decades, and it is widely utilized in clinical tasks, especially dose warping. Nowadays, as deep learning (DL) develops rapidly, many DL-based methods were also applied in DIR. This paper reviews DL-based DIR methods in recent years and the application of DIR in dose warping. We collected and categorized the latest DL-based DIR studies. A thorough review of each category was presented, in which studies were discussed based on their supervision, advantage, and challenges. Then, we reviewed DIR-based dose warping and discussed its rationale, feasibility, successes, and difficulties. Lastly, we summarized the review on both parts and discussed their future development trend.",autonomous vehicle
10.1016/j.neucom.2020.01.043,journal,Neurocomputing,sciencedirect,2020-05-07,sciencedirect,Integration of an actor-critic model and generative adversarial networks for a Chinese calligraphy robot,https://api.elsevier.com/content/article/pii/S0925231220300886,"
                  As a combination of robotic motion planning and Chinese calligraphy culture, robotic calligraphy plays a significant role in the inheritance and education of Chinese calligraphy culture. Most existing calligraphy robots focus on enabling the robots to learn writing through human participation, such as human–robot interactions and manually designed evaluation functions. However, because of the subjectivity of art aesthetics, these existing methods require a large amount of implementation work from human engineers. In addition, the written results cannot be accurately evaluated. To overcome these limitations, in this paper, we propose a robotic calligraphy model that combines a generative adversarial network (GAN) and deep reinforcement learning to enable a calligraphy robot to learn to write Chinese character strokes directly from images captured from Chinese calligraphic textbooks. In our proposed model, to automatically establish an aesthetic evaluation system for Chinese calligraphy, a GAN is first trained to understand and reconstruct stroke images. Then, the discriminator network is independently extracted from the trained GAN and embedded into a variant of the reinforcement learning method, the “actor-critic model”, as a reward function. Thus, a calligraphy robot adopts the improved actor-critic model to learn to write multiple character strokes. The experimental results demonstrate that the proposed model successfully allows a calligraphy robot to write Chinese character strokes based on input stroke images. The performance of our model, compared with the state-of-the-art deep reinforcement learning method, shows the efficacy of the combination approach. In addition, the key technology in this work shows promise as a solution for robotic autonomous assembly.
               ",autonomous vehicle
10.1016/j.knosys.2021.107007,journal,Knowledge-Based Systems,sciencedirect,2021-06-21,sciencedirect,Self-organizing deep belief modular echo state network for time series prediction,https://api.elsevier.com/content/article/pii/S0950705121002707,"
                  A deep belief echo state network is an effective deep learning framework for solving time series prediction problems. The suitable structure of the hidden layers determines the prediction performance of the neural network. However, a neural network structure designed using artificial experience has difficulty meeting application requirements. To address this problem, this paper proposes a self-organizing deep belief modular echo state network (SDBMESN) model for time series prediction with high accuracy. The basic framework of this model includes two parts: a deep belief network for deep feature extraction and a modular echo state network with subreservoirs for time series prediction. To find a suitable neural network structure, a neuron significance based on mutual information is designed to measure the degree of information of the neurons, and then a self-organizing mechanism is designed to realize the dynamic adjustment of the hidden layer neurons and subreservoirs. In addition, the robust loss function is used to improve the robustness of the prediction. The simulation results of nonlinear system modeling, sunspot prediction and algal bloom prediction demonstrate that the SDBMESN has good prediction performance and robustness.
               ",autonomous vehicle
10.1016/B978-0-12-817356-5.00009-7,journal,Internet of Things in Biomedical Engineering,sciencedirect,2019-12-31,sciencedirect,"Chapter 7: Artificial Intelligence Based Diagnostics, Therapeutics and Applications in Biomedical Engineering and Bioinformatics",https://api.elsevier.com/content/article/pii/B9780128173565000097,"
               The field of medicine demands precision and accuracy in diagnostics and treatment, which provides continuous motivation for improvements in diagnostic and treatment technologies in use for various diseases under study. Also, the need to improve upon the human abilities in medical regimens for the physical medical procedures is only practical, so as to improve upon the human errors that might prove fatal, and hence improve upon medical decision making and therapeutic procedure methodologies. The introduction of artificial intelligence and computer vision to biomedical sciences has opened opportunities for exploring new domains in medical research as well as improving existing technology employed for various medical procedures. In this chapter, we shall explore the various applications in which the use of such technologies has been taken up or is proposed for future implementation, with a brief survey of artificial intelligence techniques that are employed in such applications.
            ",autonomous vehicle
10.1016/j.asoc.2015.09.040,journal,Applied Soft Computing,sciencedirect,2016-01-31,sciencedirect,Artificial neural networks in business: Two decades of research,https://api.elsevier.com/content/article/pii/S1568494615006122,"
                  In recent two decades, artificial neural networks have been extensively used in many business applications. Despite the growing number of research papers, only few studies have been presented focusing on the overview of published findings in this important and popular area. Moreover, the majority of these reviews were introduced more than 15 years ago. The aim of this work is to expand the range of earlier surveys and provide a systematic overview of neural network applications in business between 1994 and 2015. We have covered a total of 412 articles and classified them according to the year of publication, application area, type of neural network, learning algorithm, benchmark method, citations and journal. Our investigation revealed that most of the research has aimed at financial distress and bankruptcy problems, stock price forecasting, and decision support, with special attention to classification tasks. Besides conventional multilayer feedforward network with gradient descent backpropagation, various hybrid networks have been developed in order to improve the performance of standard models. Even though neural networks have been established as well-known method in business, there is enormous space for additional research in order to improve their functioning and increase our understanding of this influential area.
               ",autonomous vehicle
10.1016/j.neucom.2021.09.003,journal,Neurocomputing,sciencedirect,2021-11-20,sciencedirect,Simulated annealing for optimization of graphs and sequences,https://api.elsevier.com/content/article/pii/S0925231221013576,"
                  Optimization of discrete structures aims at generating a new structure with the better property given an existing one, which is a fundamental problem in machine learning. Different from the continuous optimization, the realistic applications of discrete optimization (e.g., text generation) are very challenging due to the complex and long-range constraints, including both syntax and semantics, in discrete structures. In this work, we present SAGS, a novel Simulated Annealing framework for Graph and Sequence optimization. The key idea is to integrate powerful neural networks into metaheuristics (e.g., simulated annealing, SA) to restrict the search space in discrete optimization. We start by defining a sophisticated objective function, involving the property of interest and pre-defined constraints (e.g., grammar validity). SAGS searches from the discrete space towards this objective by performing a sequence of local edits, where deep generative neural networks propose the editing content and thus can control the quality of editing. We evaluate SAGS on paraphrase generation and molecule generation for sequence optimization and graph optimization, respectively. Extensive results show that our approach achieves state-of-the-art performance compared with existing paraphrase generation methods in terms of both automatic and human evaluations. Further, SAGS also significantly outperforms all the previous methods in molecule generation.
               ",autonomous vehicle
10.1016/S1088-467X(98)00027-4,journal,Intelligent Data Analysis,sciencedirect,1998-12-31,sciencedirect,A review of the fourteenth international conference on machine learning,https://api.elsevier.com/content/article/pii/S1088467X98000274,"
                  We briefly review each paper of the Fourteenth International Conference on Machine Learning, along with some general observations on the conference as a whole. The major topics of papers include data reduction, feature selection, ensembles of classifiers, natural language learning, text categorization, inductive logic programming, stochastic models, and reinforcement learning.
               ",autonomous vehicle
10.1016/B978-0-12-817133-2.00009-4,journal,Artificial Intelligence in Precision Health,sciencedirect,2020-12-31,sciencedirect,Chapter 9: Artificial intelligence for management of patients with intracranial neoplasms,https://api.elsevier.com/content/article/pii/B9780128171332000094,"
               Although rare, intracranial neoplasms are associated with high morbidity and often poor prognosis. After a brief epidemiologic introduction, artificial intelligence (AI) applications in the fields of neurosurgery and neuro-oncology are reviewed, with a focus on machine learning (ML).
               AI can play an important role in the diagnostic process of brain tumors, through imaging-related applications, from segmentation to prediction of clinical features and patient outcome. AI decision support systems are promising to aid the physician in defining the best treatment options, based on predicted outcomes. Important technological advances have provided neurosurgeons with innovative equipment that can assist in surgical resection of brain lesions: while neuronavigation is now standard for most procedures, new systems can help differentiate neoplastic tissue from normal brain parenchyma and robotics has found specific applications. Assessment of prognosis through ML algorithms has been shown to be at least as accurate as normally used prognostic indices and the opinion of clinical experts.
               Although extremely promising, AI applications in neurosurgical practice still present several limitations—from quantity and quality of training data, to concerns of ethical implications—highlighting the need for continued research in this growing field.
               This chapter provides an overview of the applications AI and ML in the habitual steps of clinical management of a patient with an intracranial neoplasm, discussing the present and future AI tools available to assist diagnosis, treatment, and prognosis.
            ",autonomous vehicle
10.1016/j.tifs.2021.08.012,journal,Trends in Food Science & Technology,sciencedirect,2021-10-31,sciencedirect,Recent advances in assessing qualitative and quantitative aspects of cereals using nondestructive techniques: A review,https://api.elsevier.com/content/article/pii/S0924224421004994,"
                  Background
                  Cereals around the globe are consumed as a staple food owing to the provision of essential nutrients. Their quality attributes are increasingly attracting the attention of nutritionists and scientists. Emerging nondestructive techniques offers great perspectives due to the special advantages of noninvasive and rapid detection of qualitative and quantitative properties. Furthermore, no review article has been found covering all the nondestructive techniques coupled chemometrics in cereal. Taking this into consideration, current effort was made to provide an in-depth and up-to-date review article.
               
                  Scope and methods
                  Traditional methods and nondestructive techniques utilized for the quality monitoring of cereals play a significant role. Traditional techniques accompanying the limitations of time-consuming, laborious, offline and destructive nature considered not good as compared to nondestructive techniques.
               
                  Key findings
                  In the current review article, near-infrared (NIR), infrared (IR), Raman spectroscopy, and fluorescence spectroscopy, along with colorimetric sensor array (CSA), imaging-based techniques and data fusion strategies have been introduced as promising techniques for the quality, authenticity and discrimination of cereals. The use of chemometrics based on artificial intelligence and machine learning are also documented. This review article also covers the challenges related to cereal processing which need to be resolved or investigated in future studies.
               ",autonomous vehicle
10.1016/j.adhoc.2016.11.007,journal,Ad Hoc Networks,sciencedirect,2017-04-30,sciencedirect,Classification of node degree based on deep learning and routing method applied for virtual route assignment,https://api.elsevier.com/content/article/pii/S1570870516303092,"
                  In recent years, the importance of various wireless network technologies has increased. Specifically, in communication environments noted for severe conditions, such as disasters, war, and terrorism, collaboration between fixed communication infrastructure and wireless ad-hoc networks is indispensable. In this paper, the node degree of wireless communication is classified for disaster situations, and virtual routes are set according to the predetermined node degree. Then, the proposed routing method is employed with base stations as the infrastructure, such that a route may be assigned, maintained, and recovered. Our classification of wireless degree nodes uses deep learning, and virtual routes are created by employing the Viterbi algorithm. The proposed routing method is compared with existing methods (AODV, OLSR, and ZRP) from the viewpoint of route discovery times and reachability via simulations.
               ",autonomous vehicle
10.1016/j.energy.2021.120268,journal,Energy,sciencedirect,2021-06-15,sciencedirect,Rejectable deep differential dynamic programming for real-time integrated generation dispatch and control of micro-grids,https://api.elsevier.com/content/article/pii/S036054422100517X,"
                  With the application of renewable energy and distributed power generation in micro-grids, conventional artificial intelligent control strategies have shown deficiencies for the frequency control and economic dispatch of micro-grids. Conventional deep learning controllers could provide outputs although when the predicted probability is not high, which will lead to micro-grid system divergence. This paper proposes a rejectable deep differential dynamic programming for the real-time integrated generation dispatch and control of micro-grids. The rejectable deep differential dynamic programming can provide an action from an analytic control algorithm when the predicted probability is not high enough. The deep differential dynamic programming contains four deep neural networks, i.e., “deep differential prediction network”, “deep differential evaluation network 1”, “deep differential evaluation network 2” and “deep differential execution network”. To verify the feasibility and effectiveness of the proposed rejectable deep differential dynamic programming, a total of 25 combined conventional optimization and control algorithms are compared under a micro-grid based on Hainan Power Grid. The numeric simulation results show that the proposed approach can obtain high control performance for the real-time integrated generation dispatch and control framework, which can replace the conventional combined “economic dispatch + automatic generation control + droop control” framework of micro-grids.
               ",autonomous vehicle
10.1016/j.jmsy.2020.06.020,journal,Journal of Manufacturing Systems,sciencedirect,2020-07-31,sciencedirect,Intelligent welding system technologies: State-of-the-art review and perspectives,https://api.elsevier.com/content/article/pii/S0278612520301102,"
                  Welding systems are being transformed by the advent of modern information technologies such as the internet of things, big data, artificial intelligence, cloud computing, and intelligent manufacturing. Intelligent welding systems (IWS), making use of these technologies, are drawing attention from academic and industrial communities. Intelligent welding is the use of computers to mimic, strengthen, and/or replace human operators in sensing, learning, decision-making, monitoring and control, etc. This is accomplished by integrating the advantages of humans and physical systems into intelligent cyber systems. While intelligent welding has found pilot applications in industry, a systematic analysis of its components, applications, and future directions will help provide a unified definition of intelligent welding systems. This paper examines fundamental components and techniques necessary to make welding systems intelligent, including sensing and signal processing, feature extraction and selection, modeling, decision-making, and learning. Emerging technologies and their application potential to IWS will also be surveyed, including Industry 4.0, cyber-physical system (CPS), digital twins, etc. Typical applications in IWS will be surveyed, including weld design, task sequencing, robot path planning, robot programming, process monitoring and diagnosis, prediction, process control, quality inspection and assessment, human-robot collaboration, and virtual welding. Finally, conclusions and suggestions for future development will be proposed. This review is intended to provide a reference of the state-of-the-art for those seeking to introduce intelligent welding capabilities as they modernize their traditional welding stations, systems, and factories.
               ",autonomous vehicle
10.1016/j.mlwa.2020.100011,journal,Machine Learning with Applications,sciencedirect,2021-03-15,sciencedirect,"Smart healthcare disease diagnosis and patient management: Innovation, improvement and skill development",https://api.elsevier.com/content/article/pii/S2666827020300116,"Data mining (DM) is an instrument of pattern detection and retrieval of knowledge from a large quantity of data. Many robust early detection services and other health-related technologies have developed from clinical and diagnostic evidence in both the DM and healthcare sectors. Artificial Intelligence (AI) is commonly used in the research and health care sectors. Classification or predictive analytics is a key part of AI in machine learning (ML). Present analyses of new predictive models founded on ML methods demonstrate promise in the area of scientific research. Healthcare professionals need accurate predictions of the outcomes of various illnesses that patients suffer from. In addition, timing is another significant aspect that affects clinical choices for precise predictions. In this regard, the authors have reviewed numerous publications in this area in terms of method, algorithms, and performance. This review paper summarized the documentation examined in accordance with approaches, styles, activities, and processes. The analyses and assessment techniques of the selected papers are discussed and an appraisal of the findings is presented to conclude the article. Present statistical models of healthcare remedies have been scientifically reviewed in this article. The uncertainty between statistical methods and ML has now been clarified. The study of related research reveals that the prediction of existing forecasting models differs even if the same dataset is used. Predictive models are also essential, and new approaches need to be improved.",autonomous vehicle
10.1016/B978-0-12-818279-6.00011-6,journal,Thinking Machines,sciencedirect,2021-12-31,sciencedirect,Chapter 1: Introduction,https://api.elsevier.com/content/article/pii/B9780128182796000116,"
               
                  This chapter describes how machine learning is applied, the dawn of machine learning, and an example use case of Industry4.0 and transaction processing. In addition, this chapter introduces the types of machine learning being studied and what problems or issues they are used for. Further, previously deployed services and applications are also discussed. First, remarkable examples of machine learning are introduced. IBM's Watson AI machine, which indicated the advent of machine learning in the research domain, and Google's Go Playing machine, are introduced. Next, definitions of inference and learning are introduced, and a full perspective is provided through examples. The inference obtained from learning is then described. Before the learning of a neural network model, input data are cleaned and modified to effectively improve the learning task; the techniques used for this are also provided. In addition, a common learning method, taxonomy of learning, and its performance metrics and verifications are introduced.
               Then, Industry4.0 is introduced as a use case of machine learning. Two typical example models of factory automation are applied in the explanation. Finally, the use of transaction processing is introduced. Machine learning is applied to process huge datasets, and a transaction is a procedure applied between each data processing. Although there is no direct relationship between machine learning and the transaction process, a block chain used in the transaction process can be applied anywhere and combined with machine learning. Therefore, a block chain is introduced in this chapter.
            ",autonomous vehicle
10.1016/B978-0-12-821259-2.00016-8,journal,Artificial Intelligence in Medicine,sciencedirect,2021-12-31,sciencedirect,Chapter 16: Prospect and adversity of artificial intelligence in urology,https://api.elsevier.com/content/article/pii/B9780128212592000168,"
               The emergence of artificial intelligence (AI) has opened a new avenue for tackling existing challenges in clinical routine. This chapter will briefly introduce potential applications of AI in urology and focus on its benefits and barriers in solving real clinical problems. First, the introduction section will generally discuss AI and existing data resources. Then, the chapter will explain the potential application of AI in urological endoscopy, urine, stone and andrology, imaging and the robotic surgery. Further, this chapter will briefly discuss some tools of risk predictions for urological cancer. Finally, the author will discuss the potential future direction of AI in urology.
            ",autonomous vehicle
10.1016/j.ipm.2019.102088,journal,Information Processing & Management,sciencedirect,2019-11-30,sciencedirect,Textual keyword extraction and summarization: State-of-the-art,https://api.elsevier.com/content/article/pii/S0306457319300044,"
                  With the advent of Web 2.0, there exist many online platforms that results in massive textual data production such as social networks, online blogs, magazines etc. This textual data carries information that can be used for betterment of humanity. Hence, there is a dire need to extract potential information out of it. This study aims to present an overview of approaches that can be applied to extract and later present these valuable information nuggets residing within text in brief, clear and concise way. In this regard, two major tasks of automatic keyword extraction and text summarization are being reviewed. To compile the literature, scientific articles were collected using major digital computing research repositories. In the light of acquired literature, survey study covers early approaches up to all the way till recent advancements using machine learning solutions. Survey findings conclude that annotated benchmark datasets for various textual data-generators such as twitter and social forms are not available. This scarcity of dataset has resulted into relatively less progress in many domains. Also, applications of deep learning techniques for the task of automatic keyword extraction are relatively unaddressed. Hence, impact of various deep architectures stands as an open research direction. For text summarization task, deep learning techniques are applied after advent of word vectors, and are currently governing state-of-the-art for abstractive summarization. Currently, one of the major challenges in these tasks is semantic aware evaluation of generated results.
               ",autonomous vehicle
10.3182/20080706-5-KR-1001.01548,journal,IFAC Proceedings Volumes,sciencedirect,2008-12-31,sciencedirect,Development of User-Adaptive Value System of Learning Function using Interactive EC,https://api.elsevier.com/content/article/pii/S1474667016404258,"
                  Our goal is to create a user-adaptive communication-robot. We are developing a system for evaluating human-robot interactions. Although such evaluation is indispensable for learning algorithms, users’ preferences are too difficult to model because they are subjective. In this study, we used the interactive evolutionary computation (IEC) to configure the value system of a learning communication-robot. The IEC is a genetic algorithm whose fitness function is performed by the user. In our experiment, we encoded the values of sensors (reward or punishment) into genes, and subjects interacted with the learning robot. Through the interaction, the subjects evaluated the robot by touching its sensors, and the robot learned appropriate combinations between input and output. Afterward, the subjects gave their scores to the experimenter, and the scores were regarded as the fitness values of the corresponding genes. These sequences were continued until the 4 generation, and then the subjects compared three of their best genes and two of the experimenter's. We found that the user-adaptive value system is suitable for the communication-robot.
               ",autonomous vehicle
10.1016/B978-0-12-816222-4.00021-6,journal,Genetics and Genomics of Eye Disease,sciencedirect,2020-12-31,sciencedirect,Chapter 19: Advancing to precision medicine through big data and artificial intelligence,https://api.elsevier.com/content/article/pii/B9780128162224000216,"
               According to the Precision Medicine Initiative, a long-term research measure involving the National Institutes of Health and other centers, precision medicine is defined as “an emerging approach for disease treatment and prevention that takes into account individual variability in genes, environment, and lifestyle for each person.” By contributing to the prevention and treatment of disease and improved health, big data and artificial intelligence play critical roles in precision medicine. The idea of harnessing all available data and techniques can be traced to the Huangdi Neijing, a book about ancient Chinese medicine practices. Currently, we have the tools to collect vast amounts of genomics data along with state-of-the-art data analytics for interpretation. With the availability of new resources and techniques, we are advancing our understanding of genomics and medicine with increasing precision.
            ",autonomous vehicle
10.1016/j.neucom.2016.12.038,journal,Neurocomputing,sciencedirect,2017-04-19,sciencedirect,A survey of deep neural network architectures and their applications,https://api.elsevier.com/content/article/pii/S0925231216315533,"
                  Since the proposal of a fast learning algorithm for deep belief networks in 2006, the deep learning techniques have drawn ever-increasing research interests because of their inherent capability of overcoming the drawback of traditional algorithms dependent on hand-designed features. Deep learning approaches have also been found to be suitable for big data analysis with successful applications to computer vision, pattern recognition, speech recognition, natural language processing, and recommendation systems. In this paper, we discuss some widely-used deep learning architectures and their practical applications. An up-to-date overview is provided on four deep learning architectures, namely, autoencoder, convolutional neural network, deep belief network, and restricted Boltzmann machine. Different types of deep neural networks are surveyed and recent progresses are summarized. Applications of deep learning techniques on some selected areas (speech recognition, pattern recognition and computer vision) are highlighted. A list of future research topics are finally given with clear justifications.
               ",autonomous vehicle
10.1016/j.neucom.2016.06.014,journal,Neurocomputing,sciencedirect,2016-11-19,sciencedirect,"Neural networks: An overview of early research, current frameworks and new challenges",https://api.elsevier.com/content/article/pii/S0925231216305550,"
                  This paper presents a comprehensive overview of modelling, simulation and implementation of neural networks, taking into account that two aims have emerged in this area: the improvement of our understanding of the behaviour of the nervous system and the need to find inspiration from it to build systems with the advantages provided by nature to perform certain relevant tasks. The development and evolution of different topics related to neural networks is described (simulators, implementations, and real-world applications) showing that the field has acquired maturity and consolidation, proven by its competitiveness in solving real-world problems. The paper also shows how, over time, artificial neural networks have contributed to fundamental concepts at the birth and development of other disciplines such as Computational Neuroscience, Neuro-engineering, Computational Intelligence and Machine Learning. A better understanding of the human brain is considered one of the challenges of this century, and to achieve it, as this paper goes on to describe, several important national and multinational projects and initiatives are marking the way to follow in neural-network research.
               ",autonomous vehicle
10.1016/j.biosystems.2014.04.003,journal,Biosystems,sciencedirect,2014-11-30,sciencedirect,Computational modeling of neural plasticity for self-organization of neural networks,https://api.elsevier.com/content/article/pii/S0303264714000446,"
                  Self-organization in biological nervous systems during the lifetime is known to largely occur through a process of plasticity that is dependent upon the spike-timing activity in connected neurons. In the field of computational neuroscience, much effort has been dedicated to building up computational models of neural plasticity to replicate experimental data. Most recently, increasing attention has been paid to understanding the role of neural plasticity in functional and structural neural self-organization, as well as its influence on the learning performance of neural networks for accomplishing machine learning tasks such as classification and regression. Although many ideas and hypothesis have been suggested, the relationship between the structure, dynamics and learning performance of neural networks remains elusive. The purpose of this article is to review the most important computational models for neural plasticity and discuss various ideas about neural plasticity's role. Finally, we suggest a few promising research directions, in particular those along the line that combines findings in computational neuroscience and systems biology, and their synergetic roles in understanding learning, memory and cognition, thereby bridging the gap between computational neuroscience, systems biology and computational intelligence.
               ",autonomous vehicle
10.1016/j.neucom.2021.04.128,journal,Neurocomputing,sciencedirect,2021-10-21,sciencedirect,A generative adversarial network for single and multi-hop distributional knowledge base completion,https://api.elsevier.com/content/article/pii/S0925231221009577,"
                  Knowledge bases (KBs) inherently lack reasoning ability, limiting their effectiveness for tasks such as question–answering and query expansion. Machine-learning is hence commonly employed for representation learning in order to learn semantic features useful for generalization. Most existing methods utilize discriminative models that require both positive and negative samples to learn a decision boundary. KBs, by contrast, contain only positive samples, necessitating that negative samples are generated by replacing the head/tail of predicates with randomly-chosen entities. They are thus frequently easily discriminable from positive samples, which can prevent learning of sufficiently robust classifiers.
                  Generative models, however, do not require negative samples to learn the distribution of positive samples; stimulated by recent developments in Generative Adversarial Networks (GANs), we propose a novel framework, Knowledge Completion GANs (KCGANs), for competitively training generative link prediction models against discriminative belief prediction models. KCGAN thus invokes a game between generator-network 
                        
                           G
                        
                      and discriminator-network 
                        
                           D
                        
                      in which 
                        
                           G
                        
                      aims to understand underlying KB structure by learning to perform link prediction while 
                        
                           D
                        
                      tries to gain knowledge about the KB by learning predicate/triplet classification. Two key challenges are addressed: 1) Classical GAN architectures’ inability to easily generate samples over discrete entities; 2) the inefficiency of softmax for learning distributions over large sets of entities. As a step toward full first-order logical reasoning we further extend KCGAN to learn multi-hop logical entailment relations between entities by enabling 
                        
                           G
                        
                      to compose a multi-hop relational path between entities and 
                        
                           D
                        
                      to discriminate between real and fake paths.
                  KCGAN is tested on benchmarks WordNet and FreeBase datasets and evaluated on link prediction and belief prediction tasks using MRR and HIT@ 10, achieving best-in-class performance.
               ",autonomous vehicle
10.1016/j.neucom.2020.04.157,journal,Neurocomputing,sciencedirect,2021-07-15,sciencedirect,"Convolutional neural networks for medical image analysis: State-of-the-art, comparisons, improvement and perspectives",https://api.elsevier.com/content/article/pii/S0925231221001314,"
                  Convolutional neural networks, are one of the most representative deep learning models. CNNs were extensively used in many aspects of medical image analysis, allowing for great progress in computer-aided diagnosis in recent years. In this paper, we provide a survey on convolutional neural networks in medical image analysis. First, we review the commonly used CNNs in medical image processing, including AlexNet, GoogleNet, ResNet, R-CNN, and FCNN. Then, we present an overview of the use of CNNs, for image classification, segmentation, detection, and other tasks such as registration, content-based image retrieval, image generation and enhancement, in some typical medical diagnosis areas such as brain, breast, and abdominal. Finally, we discuss the remaining challenges of CNNs in medical image analysis, and accordingly we present some ideas for future research directions.
               ",autonomous vehicle
10.1016/j.suscom.2020.100439,journal,Sustainable Computing: Informatics and Systems,sciencedirect,2020-12-31,sciencedirect,ML based sustainable precision agriculture: A future generation perspective,https://api.elsevier.com/content/article/pii/S2210537920301669,"
                  Agriculture is an essential source of survival and also accounts for the economic growth of any country. Rapid advancement in precision agriculture has helped agriculture and its ancillary enter into the era of machine learning and big data technologies. The advanced technology would help in the improvement in various dimensions of the agro-sector. The agro science community needs to create its architectural model to enjoy the advantages of parallel computing and storage management of large datasets, which would help to discover novel analytical structure to extract useful information from the data patterns. These patterns would help understand the field and various issues and also help in identifying the solutions to eradicate multiple problems. Machine learning offers favorable computational as well as analytical solutions for the integrated study of different types of datasets from various sources. This study introduces the core concept of machine learning and systematic processes to comprehend its application in agriculture. It also discusses various machine learning algorithms that can be utilized to build models to address various agricultural issues.
               ",autonomous vehicle
10.1016/j.asoc.2019.105551,journal,Applied Soft Computing,sciencedirect,2019-09-30,sciencedirect,An integrated framework of genetic network programming and multi-layer perceptron neural network for prediction of daily stock return: An application in Tehran stock exchange market,https://api.elsevier.com/content/article/pii/S156849461930331X,"
                  Evolutionary algorithms are generally used to find or generate the best individuals in a population. Whenever these algorithms are applied to agent systems, they will lead to optimal solutions. Genetic Network Programming (GNP), which contains graph networks, is one of the developed evolutionary algorithms. When the aim is to forecast the share price or return, ascending and descending trends, volatilities, recent returns, fundamental and technical factors have remarkable impacts on the prediction. This is why technical indicators are used to constitute a set of trading rules. In this paper, we apply an integrated framework consisting of GNP model along with a reinforcement learning and Multi-Layer Perceptron (MLP) neural network to classify data and also time series models to forecast the stock return. Moreover, we utilize rules of accumulation based on the GNP model’s results to forecast the return. The aim of using these models alongside one another is to estimate one-day return. The results derived from 9 stocks with regard to the Tehran Stock Exchange Market. GNP extracts a prodigious number of rules on the basis of 5 technical indicators with 3 times period. Next, MLP network classifies data and finds the similarity between future data and past data concerning a stock (5 sub-period) through classification. Subsequently, a number of conditions are established, in order to choose the best estimation between GNP-RL and ARMA. Distinct comparison with the ARMA–GARCH model, which is operated for return estimation and risk measurement in many researches, demonstrates an extended forecasting power of the proposed model, by the name of GNP–ARMA, reducing error by a mean of 16%.
               ",autonomous vehicle
10.1016/B978-0-12-815585-1.00029-2,journal,Biotechnology Entrepreneurship,sciencedirect,2020-12-31,sciencedirect,Chapter 29: Artificial Intelligence in Biotechnology: A Framework for Commercialization,https://api.elsevier.com/content/article/pii/B9780128155851000292,"
               Developing and commercializing new biotechnology products that rely on artificial intelligence (AI) require compliance with multiple regulatory structures that govern (1) the use and disclosure of individually identifiable information, (2) the sales and marketing of AI as a medical device, and (3) coverage and reimbursement for items and services that incorporate AI. In this chapter we will review some principles to help provide a framework for the biotech entrepreneur to consider, as products from AI and their resulting regulatory guidance evolves.
            ",autonomous vehicle
10.1016/j.neucom.2012.12.023,journal,Neurocomputing,sciencedirect,2013-07-02,sciencedirect,Designing of on line intrusion detection system using rough set theory and Q-learning algorithm,https://api.elsevier.com/content/article/pii/S092523121300060X,"
                  Development of an efficient real time intrusion detection system (IDS) has been proposed in the paper by integrating Q-learning algorithm and rough set theory (RST). The objective of the work is to achieve maximum classification accuracy while detecting intrusions by classifying NSL-KDD network traffic data either ‘normal’ or ‘anomaly’. Since RST processes discrete data only, by applying cut operation attributes in training data are discretized. Using indiscernibility concept of RST, reduced attribute sets, called reducts are obtained and among the reducts a single reduct is chosen which provides highest classification accuracy. However, for the test data the same reduct would not provide highest classification accuracy due to change of discretized attribute values. Therefore, to overcome the problem discretization and feature selection processes are dealt in a comprehensive and systematic way in the paper using machine learning approach. The Q-learning algorithm has been modified to learn optimum cut value for different attributes so that corresponding reduct produces maximum classification accuracy while classifying network traffic data. Since, not all attributes but reduct only take part to detect intrusions, the proposed algorithm is faster than Q-learning and reduces complexity of the IDS. Classification accuracy with 98% success rate has been obtained using real time data, which demonstrates superior performance compared to other classifiers.
               ",autonomous vehicle
10.1016/j.jobe.2021.102536,journal,Journal of Building Engineering,sciencedirect,2021-11-30,sciencedirect,A review on application of soft computing techniques for the rapid visual safety evaluation and damage classification of existing buildings,https://api.elsevier.com/content/article/pii/S2352710221003934,"
                  Seismic vulnerability assessment of existing buildings is of great concern around the world. Different countries develop various approaches and methodologies to overcome the disastrous effects of earthquakes on the structural parameters of the building and the human losses. There are structures still in service with a high seismic vulnerability, which proposes an urgent need for a screening system's damageability grading system. Rapid urbanization and the proliferation of slums give rise to improper construction practices that make the building stock's reliability ambiguous, including old structures that were constructed either when the seismic codes were not advanced or not enforced by law. Despite having a good knowledge of structural analysis, it is impractical to conduct detailed nonlinear analysis on each building in the target area to define their seismic vulnerability. This indicates the necessity of developing a rapid, reliable, and computationally easy method of seismic vulnerability assessment, more commonly known as Rapid Visual Screening (RVS). This method begins with a walk-down survey by a trained evaluator, and an initial score is assigned to the structure. Further, the vulnerability parameters are defined (predictor variables), and the damage grades are defined. Various methods are then adopted to develop an optimum correlation between the parameters and damage grades. Soft Computing (SC) techniques including probabilistic approaches, meta-heuristics, and Artificial Intelligence (AI) theories such as artificial neural networks, machine learning, fuzzy logic, etc. due to their capabilities in targeting inherent imprecision of phenomena in real-world are among the most important and widely used approaches in this regard. In this paper, a comprehensive literature review of the most commonly used and newly developed innovative methodologies in RVS using powerful SC techniques has been presented to shed light on key factors, strengths, and applications of each SC technique in advancing the RVS field of study.
               ",autonomous vehicle
10.1016/S0007-8506(18)30216-6,journal,CIRP Annals,sciencedirect,1996-12-31,sciencedirect,Machine Learning Approaches to Manufacturing,https://api.elsevier.com/content/article/pii/S0007850618302166,"
                  Continuous, steady improvement is a key requirement to manufacturing enterprises that necessitates flat and flexible organizations, life-long learning of employees on the one side, and information and material processing systems with adaptive, learning abilities on the other side. On the basis of two Workshops on Learning in Intelligent Manufacturing Systems, a thorough-going analysis of the literature, and with numerous contributions, the paper surveys machine learning techniques that seem to be applicable in realizing systems with intelligent behavior. Symbolic, subsymbolic approaches and their applications in manufacturing are equally treated, together with hybrid solutions which try to integrate the benefits of the individual techniques. In order to find appropriate techniques for given problems, the strengths, weaknesses and limitations of the methods are described on a wide range of manufacturing fields. Finally, future trends are enumerated.
               ",autonomous vehicle
10.1016/j.knosys.2007.01.005,journal,Knowledge-Based Systems,sciencedirect,2007-06-30,sciencedirect,Effective learning system techniques for human–robot interaction in service environment,https://api.elsevier.com/content/article/pii/S0950705107000135,"
                  HRI (Human–Robot Interaction) is often frequent and intense in assistive service environment and it is known that realizing human-friendly interaction is a very difficult task because of human presence as a subsystem of the interaction process. After briefly discussing typical HRI models and characteristics of human, we point out that learning aspect would play an important role for designing the interaction process of the human-in-the loop system. We then show that the soft computing toolbox approach, especially with fuzzy set-based learning techniques, can be effectively adopted for modeling human behavior patterns as well as for processing human bio-signals including facial expressions, hand/ body gestures, EMG and so forth. Two project works are briefly described to illustrate how the fuzzy logic-based learning techniques and the soft computing toolbox approach are successfully applied for human-friendly HRI systems. Next, we observe that probabilistic fuzzy rules can handle inconsistent data patterns originated from human, and show that combination of fuzzy logic, fuzzy clustering, and probabilistic reasoning in a single frame leads to an algorithm of iterative fuzzy clustering with supervision. Further, we discuss a possibility of using the algorithm for inductively constructing probabilistic fuzzy rule base in a learning system of a smart home. Finally, we propose a life-long learning system architecture for the HRI type of human-in-the-loop systems.
               ",autonomous vehicle
10.1016/B978-0-12-820604-1.00010-8,journal,Computational Intelligence and Its Applications in Healthcare,sciencedirect,2020-12-31,sciencedirect,10: Convolutional neural network for biomedical applications,https://api.elsevier.com/content/article/pii/B9780128206041000108,"
               The emergence of deep learning techniques has created a new era in the medical field related to disease diagnosis and detection. The equipment developed for the diagnosis of diseases should be more accurate and precise and should stand as a second opinion for the doctors. For this purpose, convolutional neural networks are trained with large datasets for accurate learning of features and to produce optimal accuracy. From the pretrained network, due to advances in technology, new networks can be designed for specific applications. Hence this technology has permeated all other technologies and inventions and is a nonreplaceable technique in the medical field. However, designing such a network is a straightforward process. The network designed for a specific application should be optimum in terms of both accuracy and processing time. A large dataset is required for exact training, which may lead to more processing time. Preprocessing of images to a particular size without losing needed features can reduce the processing time to an extent. But the ideal accuracy of deep convolutional networks has outweighed all their drawbacks and they now stand as a true solution for biomedical applications. A case of lung nodule classification described in this chapter has shown that CNN can outperform other techniques and can analyze medical images in their raw format.
            ",autonomous vehicle
10.1016/j.comnet.2021.108149,journal,Computer Networks,sciencedirect,2021-07-20,sciencedirect,Generative Adversarial Networks (GANs) in networking: A comprehensive survey & evaluation,https://api.elsevier.com/content/article/pii/S1389128621002139,"
                  Despite the recency of their conception, Generative Adversarial Networks (GANs) constitute an extensively-researched machine learning sub-field for the creation of synthetic data through deep generative modeling. GANs have consequently been applied in a number of domains, most notably computer vision, in which they are typically used to generate or transform synthetic images. Given their relative ease of use, it is therefore natural that researchers in the field of networking (which has seen extensive application of deep learning methods) should take an interest in GAN-based approaches. The need for a comprehensive survey of such activity is therefore urgent. In this paper, we demonstrate how this branch of machine learning can benefit multiple aspects of computer and communication networks, including mobile networks, network analysis, internet of things, physical layer, and cybersecurity. In doing so, we shall provide a novel evaluation framework for comparing the performance of different models in non-image applications, applying this to a number of reference network datasets.
               ",autonomous vehicle
10.1016/j.ifacol.2020.12.540,journal,IFAC-PapersOnLine,sciencedirect,2020-12-31,sciencedirect,"On Data Science for Process Systems Modeling, Control and Operations",https://api.elsevier.com/content/article/pii/S2405896320308375,"
                  Data science is emerging as a multidisciplinary field with tremendous recent development in theoretical foundations and expanded applications in both science and engineering. Engineering applications include industrial data analytics, autonomous systems, energy analytics, environmental applications, economic data modeling, image sequence modeling, and other high dimensional time-series data analytics. This paper is concerned with the integration of data science with dynamic systems, monitoring and control. The development of machine learning is reviewed in both a neural-mimic learning route and a learning control route, which deals with intrinsically uncertain dynamic systems. The paper then reviews the interaction of data with process manufacturing systems modeling and control, involving both data and first principles models with varying proportions. Problems include data reconciliation, state and disturbance estimation, system identification, process monitoring, and inferential property estimation. For time series data in process manufacturing systems, we present latent dynamic variable modeling methods to extract the principal dynamics in a low dimensional subspace of the data. The approaches effectively distill latent dynamic features from the data for easy interpretation, prediction, and visualization. Case studies are presented to illustrate how these latent dynamic analytics extract important features for process interpretation, troubleshooting, and monitoring.
               ",autonomous vehicle
10.1016/j.asoc.2020.106831,journal,Applied Soft Computing,sciencedirect,2020-12-31,sciencedirect,A refreshing view of soft computing models for predicting the deflection of reinforced concrete beams,https://api.elsevier.com/content/article/pii/S1568494620307699,"
                  The efforts of this study are to address an essential technical issue in construction and civil engineering, namely predicting the deflection of reinforced concrete beams. Indeed, six new hybrid models (ensemble models) were developed to address this critical technical problem based on artificial intelligence models as well as machine learning algorithms, such as artificial neural network (ANN), support vector machine (SVM), and adaptive neuro-fuzzy inference system (ANFIS). Accordingly, the bagging (BA) technique was applied to create new ensemble models, including BA-SVM, BA-ANN, BA-ANFIS, SVM-ANN, SVM-ANFIS, and ANN-ANFIS models. They were developed based on 120 practical experiments on the deflection of reinforced concrete beams. A series of indicators of error, accuracy, as well as the statistical significance of the models, were analyzed to assess the overall efficiency of the forecasting models. The results showed that the ensemble models are capable of predicting the deflection of reinforced concrete beams with high accuracy, especially the SVM-ANFIS model. The results of this study have opened up many new research directions in the design and optimization of the structure of buildings, dangerous warning systems, and timely solutions to ensure the safety of buildings.
               ",autonomous vehicle
10.1016/j.bushor.2019.12.002,journal,Business Horizons,sciencedirect,2020-04-30,sciencedirect,From data to action: How marketers can leverage AI,https://api.elsevier.com/content/article/pii/S0007681319301624,"
                  Artificial intelligence (AI) is at the forefront of a revolution in business and society. AI affords companies a host of ways to better understand, predict, and engage customers. Within marketing, AI’s adoption is increasing year-on-year and in varied contexts, from providing service assistance during customer interactions to assisting in the identification of optimal promotions. But just as questions about AI remain with regard to job automation, ethics, and corporate responsibility, the marketing domain faces its own concerns about AI. With this article, we seek to consolidate the growing body of knowledge about AI in marketing. We explain how AI can enhance the marketing function across nine stages of the marketing planning process. We also provide examples of current applications of AI in marketing.
               ",autonomous vehicle
10.1016/j.matt.2020.06.011,journal,Matter,sciencedirect,2020-08-05,sciencedirect,AI Applications through the Whole Life Cycle of Material Discovery,https://api.elsevier.com/content/article/pii/S2590238520303015,"We provide a review of machine learning (ML) tools for material discovery and sophisticated applications of different ML strategies. Although there have been a few published reviews on artificial intelligence (AI) for materials with an emphasis on a single material system or individual methods, this paper focuses on an application-based perspective in AI-enhanced material discovery. It shows how AI strategies are applied through material discovery stages (including characterization, property prediction, synthesis, and theory paradigm discovery). Also, by referring to the ML tutorial, readers can acquire a better understanding of the exact functions of ML methods in each application and how these methods work to realize the targets. We are aiming to enable a better integration of AI methods with the material discovery process. The keys to successful applications of AI in material discovery and challenges to be addressed are also highlighted.",autonomous vehicle
10.1016/j.neucom.2021.05.011,journal,Neurocomputing,sciencedirect,2021-09-30,sciencedirect,Recent advances of single-object tracking methods: A brief survey,https://api.elsevier.com/content/article/pii/S0925231221007220,"
                  Single-object tracking is regarded as a challenging task in computer vision, especially in complex spatio-temporal contexts. The changes in the environment and object deformation make it difficult to track. In the last 10 years, the application of correlation filters and deep learning enhances the performance of trackers to a large extent. This paper summarizes single-object tracking algorithms based on correlation filters and deep learning. Firstly, we explain the definition of single-object tracking and analyze the components of general object tracking algorithms. Secondly, the single-object tracking algorithms proposed in the past decade are summarized according to different categories. Finally, this paper summarizes the achievements and problems of existing algorithms by analyzing experimental results and discusses the development trends.
               ",autonomous vehicle
10.1016/j.eswa.2021.114800,journal,Expert Systems with Applications,sciencedirect,2021-09-01,sciencedirect,"A comprehensive survey on deep neural networks for stock market: The need, challenges, and future directions",https://api.elsevier.com/content/article/pii/S0957417421002414,"
                  The stock market has been an attractive field for a large number of organizers and investors to derive useful predictions. Fundamental knowledge of stock market can be utilised with technical indicators to investigate different perspectives of the financial market; also, the influence of various events, financial news, and/or opinions on investors’ decisions and hence, market trends have been observed. Such information can be exploited to make reliable predictions and achieve higher profitability. Computational intelligence has emerged with various deep neural network (DNN) techniques to address complex stock market problems. In this article, we aim to review the significance and need of DNNs in the field of stock price and trend prediction; we discuss the applicability of DNN variations to the temporal stock market data and also extend our survey to include hybrid, as well as metaheuristic, approaches with DNNs. We observe the potential limitations for stock market prediction using various DNNs. To provide an experimental evaluation, we also conduct a series of experiments for stock market prediction using nine deep learning-based models; we analyse the impact of these models on forecasting the stock market data. We also evaluate the performance of individual models with different number of features. We discuss challenges, as well as potential future research directions, and conclude our survey with the experimental study. This survey can be referred for the recent perspectives of DNN-based stock market prediction, primarily covering research spanning over years 
                        
                           2017
                           -
                           2020
                        
                     .
               ",autonomous vehicle
10.1016/j.artint.2020.103438,journal,Artificial Intelligence,sciencedirect,2021-04-30,sciencedirect,Making sense of sensory input,https://api.elsevier.com/content/article/pii/S0004370220301855,"This paper attempts to answer a central question in unsupervised learning: what does it mean to “make sense” of a sensory sequence? In our formalization, making sense involves constructing a symbolic causal theory that both explains the sensory sequence and also satisfies a set of unity conditions. The unity conditions insist that the constituents of the causal theory – objects, properties, and laws – must be integrated into a coherent whole. On our account, making sense of sensory input is a type of program synthesis, but it is unsupervised program synthesis. Our second contribution is a computer implementation, the Apperception Engine, that was designed to satisfy the above requirements. Our system is able to produce interpretable human-readable causal theories from very small amounts of data, because of the strong inductive bias provided by the unity conditions. A causal theory produced by our system is able to predict future sensor readings, as well as retrodict earlier readings, and impute (fill in the blanks of) missing sensory readings, in any combination. In fact, it is able to do all three tasks simultaneously. We tested the engine in a diverse variety of domains, including cellular automata, rhythms and simple nursery tunes, multi-modal binding problems, occlusion tasks, and sequence induction intelligence tests. In each domain, we test our engine's ability to predict future sensor values, retrodict earlier sensor values, and impute missing sensory data. The Apperception Engine performs well in all these domains, significantly out-performing neural net baselines. We note in particular that in the sequence induction intelligence tests, our system achieved human-level performance. This is notable because our system is not a bespoke system designed specifically to solve intelligence tests, but a general-purpose system that was designed to make sense of any sensory sequence.",autonomous vehicle
10.1016/j.jnca.2016.04.006,journal,Journal of Network and Computer Applications,sciencedirect,2016-06-30,sciencedirect,"Neural networks in wireless networks: Techniques, applications and guidelines",https://api.elsevier.com/content/article/pii/S1084804516300492,"
                  The design of modern wireless networks, which involves decision making and parameter optimization, is quite challenging due to the highly dynamic, and often unknown, environmental conditions that characterize wireless networks. There is a common trend in modern networks to incorporate artificial intelligence (AI) techniques to cope with this design complexity. While a number of AI techniques have been profitably employed in the wireless networks community, the well-established AI framework of neural networks (NNs), well known for their remarkable generality and versatility, has been applied in a wide variety of settings in wireless networks. In particular, NNs are especially popular for tasks involving classification, learning, or optimization. In this paper, we provide both an exposition of common NN models and a comprehensive survey of the applications of NNs in wireless networks. We also identify pitfalls and challenges of implementing NNs especially when we consider alternative AI models and techniques. While various surveys on NNs exist in the literature, our paper is the first paper, to the best of our knowledge, which focuses on the applications of NNs in wireless networks.
               ",autonomous vehicle
10.1016/B978-0-12-816639-0.00015-6,journal,Smart Cities: Issues and Challenges,sciencedirect,2019-12-31,sciencedirect,Chapter 15: An outlook of a future smart city in Taiwan from post–Internet of things to artificial intelligence Internet of things,https://api.elsevier.com/content/article/pii/B9780128166390000156,"
               Artificial intelligence (AI) and the Internet of things (IoT) are progressively entering our daily lives, from food, medicine, housing, transportation, education, and entertainment to smart manufacturing, smart security, and smart retail. In this chapter, we discuss the artificial intelligence of things (AIoT) (AI+IoT) to analyze the objectives of future smart city construction in Taiwan and explain the supporting technologies. Subsequently, we detail the technical framework of a future smart city and use information collected by the IoT through the AI open collaboration mechanism to discuss model evaluation and optimization, which provide functions such as AI application and facilitating interactive relationships. Next, we analyze the needs of future smart cities and the advantages and challenges they bring. The future development of smart cities in Taiwan that are under planning is then analyzed, and the formation, operation, and development of the smart city system are revealed.
            ",autonomous vehicle
10.1016/j.patcog.2021.108368,journal,Pattern Recognition,sciencedirect,2022-03-31,sciencedirect,Discrete embedding for attributed graphs,https://api.elsevier.com/content/article/pii/S0031320321005483,"
                  Attributed graphs refer to graphs where both node links and node attributes are observable for analysis. Attributed graph embedding enables joint representation learning of node links and node attributes. Different from classical graph embedding methods such as Deepwalk and node2vec that first project node links into low-dimensional vectors which are then linearly concatenated with node attribute vectors as node representation, attributed graph embedding fully explores data dependence between node links and attributes by either using node attributes as class labels to supervise structure learning from node links, or reversely using node links to supervise the learning from node attributes. However, existing attributed graph embedding models are designed in continuous Euclidean spaces which often introduce data redundancy and impose challenges to storage and computation costs. In this paper, we study a new problem of discrete embedding for attributed graphs that can learn succinct node representations. Specifically, we present a Binarized Attributed Network Embedding model (BANE for short) to learn binary node representation by factorizing a Weisfeiler-Lehman proximity matrix under the constraint of binary node representation. Furthermore, based on BANE, we propose a new Low-bit Quantization for Attributed Network Representation learning model (LQANR for short) to learn even more compact node representation of bit-width values. Theoretical analysis and empirical studies on real-world datasets show that the new discrete embedding models outperform benchmark methods.
               ",autonomous vehicle
10.1016/j.neucom.2019.11.090,journal,Neurocomputing,sciencedirect,2020-03-14,sciencedirect,Neuroevolutionary based convolutional neural network with adaptive activation functions,https://api.elsevier.com/content/article/pii/S0925231219316947,"
                  Deep convolutional neural networks are one of the most successful types of neural networks widely used in image processing and pattern recognition. These networks involve many tunable parameters that influence network performance drastically. Among them, the proposed method of this paper focuses on the role of activation function in these networks, while the idea of adaptive activation functions is further developed by utilizing the neuroevolutionary technique. Considering several basic function to be combined in a non-linear manner, the proposed method attempts to construct an adaptive function by the help of Genetics Algorithm (GA) technique, while the selected basic functions by GA and the learned combination coefficients are adapted to the input data. As the network optimizer and the learning rate parameter are tightly related to the network activation functions, they are also included in the GA evolutionary process to be selected such that they are highly in coherence with the selected basic functions. Experiments done on the classification of CT brain images and the MNIST hand written digits dataset clearly confirm the efficiency of the proposed idea and the role of proper adaptive activation functions in extending the capabilities of neural networks.
               ",autonomous vehicle
10.1016/B978-1-55860-307-3.50047-2,journal,Machine Learning Proceedings 1993,sciencedirect,1993-12-31,sciencedirect,Online Learning with Random Representations,https://api.elsevier.com/content/article/pii/B9781558603073500472,"
               We consider the requirements of online learning—learning which must be done incrementally and in realtime, with the results of learning available soon after each new example is acquired. Despite the abundance of methods for learning from examples, there are few that can be used effectively for online learning, e.g., as components of reinforcement learning systems. Most of these few, including radial basis functions, CMACs, Kohonen's self-organizing maps, and those developed in this paper, share the same structure. All expand the original input representation into a higher dimensional representation in an unsupervised way, and then map that representation to the final answer using a relatively simple supervised learner, such as a perceptron or LMS rule. Such structures learn very rapidly and reliably, but have been thought either to scale poorly or to require extensive domain knowledge. To the contrary, some researchers (Rosenblatt, 1962; Gallant & Smith, 1987; Kanerva, 1988; Prager & Fallside, 1988) have argued that the expanded representation can be chosen largely at random with good results. The main contribution of this paper is to develop and test this hypothesis. We show that simple random-representation methods can perform as well as nearest-neighbor methods (while being more suited to online learning), and significantly better than backpropagation. We find that the size of the random representation does increase with the dimensionality of the problem, but not unreasonably so, and that the required size can be reduced substantially using unsupervised-learning techniques. Our results suggest that randomness has a useful role to play in online supervised learning and constructive induction.
            ",autonomous vehicle
10.1016/j.matpr.2021.07.376,journal,Materials Today: Proceedings,sciencedirect,2021-07-27,sciencedirect,A framework on driving behavior and pattern using On-Board diagnostics (OBD-II) tool,https://api.elsevier.com/content/article/pii/S2214785321052354,"
                  The concept of DB means “Driver Behaviour” is a detailed analysis of driver pattern or style as to how the vehicle is operated by the driver in different conditions such as environ and dynamics of road conditions. Hence, it has become one of the most sought-after subject having pivotal importance these days. Here in this paper, the chief aim of detailed work on DB analysis methods is rendered in an elaborated manner. The DB is essentially consists of driver and his body dynamics with respect to the vehicle such as dynamics of upper and lower body and lower body including motion and gestures of hands, feet motion, head rotation, staring/gazing behaviour etc.. All this detailed feed is in turn utilized in predicting driver’s attention with respect to the surroundings and the vehicle and also his distraction and fatigue with respect to the same. Most of the accidents are a result of risky DB, which in turn cause great bodily and fiscal damages. Moreover, it has become necessary to recognise risky DB and treat people accordingly on the basis of their driving behaviour in order to counter the ongoing growth of road accidents. Positively, with the help of advanced sensors integrated into embedded systems or OBD systems support, it is now possible to propose and develop a fool proof DB.
                  The efficacy of the DB and further scope of its improvement, based on the information obtained by the vehicle diagnostic system “OBD-II” and ML means “Machine Learning” algorithms is explored in this paper. Almost complete behaviour of drivers, that is, whether the driver practises safe driving methods/patterns or is following unsafe/risky driving methods including rash driving or drunk driving or even violating the traffic rules, can be distinguished by utilizing specific inputs from OBD-II and then with application of certain ML techniques. Ultimately, the mankind is heading towards a range of autonomous vehicles, hence the need, scope and potential of DB is gaining momentum now a days.
               ",autonomous vehicle
10.1016/j.knosys.2021.107134,journal,Knowledge-Based Systems,sciencedirect,2021-08-17,sciencedirect,"A comprehensive survey on sentiment analysis: Approaches, challenges and trends",https://api.elsevier.com/content/article/pii/S095070512100397X,"
                  Sentiment analysis (SA), also called Opinion Mining (OM) is the task of extracting and analyzing people’s opinions, sentiments, attitudes, perceptions, etc., toward different entities such as topics, products, and services. The fast evolution of Internet-based applications like websites, social networks, and blogs, leads people to generate enormous heaps of opinions and reviews about products, services, and day-to-day activities. Sentiment analysis poses as a powerful tool for businesses, governments, and researchers to extract and analyze public mood and views, gain business insight, and make better decisions. This paper presents a complete study of sentiment analysis approaches, challenges, and trends, to give researchers a global survey on sentiment analysis and its related fields. The paper presents the applications of sentiment analysis and describes the generic process of this task. Then, it reviews, compares, and investigates the used approaches to have an exhaustive view of their advantages and drawbacks. The challenges of sentiment analysis are discussed next to clarify future directions.
               ",autonomous vehicle
10.1016/j.procs.2020.09.036,journal,Procedia Computer Science,sciencedirect,2020-12-31,sciencedirect,A Generic approach for Pronominal Anaphora and Zero Anaphora resolution in Arabic language,https://api.elsevier.com/content/article/pii/S1877050920319311,"This paper deals with the resolution of pronominal anaphora and zero anaphora in Arabic language. While researchers have treated the two phenomena separately, we propose a generic approach for both of them. Our resolution system combines a Q-learning reinforcement method and Word Embedding models. The Q-learning method uses syntactic criteria as preference factors to select candidate antecedents. It reinforces the best combination criteria for evaluating candidate antecedents. The Word Embedding models provide semantic similarity measures that help to validate the best antecedent. Our approach is evaluated on different type of Arabic texts and the obtained precision can reach79.37%.",autonomous vehicle
10.1016/B978-0-323-90184-0.00003-5,journal,Deep Learning for Chest Radiographs,sciencedirect,2021-12-31,sciencedirect,Chapter 1: Introduction,https://api.elsevier.com/content/article/pii/B9780323901840000035,"
               This chapter highlights the motivation behind designing computer-aided classification (CAC) systems for chest radiographs. It gives a brief overview of the concepts of artificial intelligence, deep learning, medical images, medical imaging techniques, and diagnostic features of medical images. The chapter discusses the different medical imaging techniques beneficial in diagnosis of chest abnormalities, primarily pneumonia, and the diagnostic features that a medical practitioner looks for to identify normal, pneumonia, and COVID19 chest radiographs.
            ",autonomous vehicle
10.1016/j.stlm.2021.100034,journal,Annals of 3D Printed Medicine,sciencedirect,2021-12-31,sciencedirect,3D printing in cardiology: A review of applications and roles for advanced cardiac imaging,https://api.elsevier.com/content/article/pii/S2666964121000291,"With the rate of cardiovascular diseases in the U.S increasing throughout the years, there is a need for developing more advanced treatment plans that can be tailored to specific patients and scenarios. The development of 3D printing is rapidly gaining acceptance into clinical cardiology. In this review, key technologies used in 3D printing are briefly summarized, particularly, the use of artificial intelligence (AI), open-source tools like MeshLab and MeshMixer, and 3D printing techniques such as fused deposition molding (FDM) and polyjet are reviewed. The combination of 3D printing, multiple image integration, and augmented reality may greatly enhance data visualization during diagnosis, treatment planning, and surgical procedures for cardiology.",autonomous vehicle
10.1016/j.jrmge.2015.06.008,journal,Journal of Rock Mechanics and Geotechnical Engineering,sciencedirect,2015-10-31,sciencedirect,Prediction of roadheaders' performance using artificial neural network approaches (MLP and KOSFM),https://api.elsevier.com/content/article/pii/S1674775515000839,"Application of mechanical excavators is one of the most commonly used excavation methods because it can bring the project more productivity, accuracy and safety. Among the mechanical excavators, roadheaders are mechanical miners which have been extensively used in tunneling, mining and civil industries. Performance prediction is an important issue for successful roadheader application and generally deals with machine selection, production rate and bit consumption. The main aim of this research is to investigate the cutting performance (instantaneous cutting rates (ICRs)) of medium-duty roadheaders by using artificial neural network (ANN) approach. There are different categories for ANNs, but based on training algorithm there are two main kinds: supervised and unsupervised. The multi-layer perceptron (MLP) and Kohonen self-organizing feature map (KSOFM) are the most widely used neural networks for supervised and unsupervised ones, respectively. For gaining this goal, a database was primarily provided from roadheaders' performance and geomechanical characteristics of rock formations in tunnels and drift galleries in Tabas coal mine, the largest and the only fully-mechanized coal mine in Iran. Then the database was analyzed in order to yield the most important factor for ICR by using relatively important factor in which Garson equation was utilized. The MLP network was trained by 3 input parameters including rock mass properties, rock quality designation (RQD), intact rock properties such as uniaxial compressive strength (UCS) and Brazilian tensile strength (BTS), and one output parameter (ICR). In order to have more validation on MLP outputs, KSOFM visualization was applied. The mean square error (MSE) and regression coefficient (R) of MLP were found to be 5.49 and 0.97, respectively. Moreover, KSOFM network has a map size of 8 × 5 and final quantization and topographic errors were 0.383 and 0.032, respectively. The results show that MLP neural networks have a strong capability to predict and evaluate the performance of medium-duty roadheaders in coal measure rocks. Furthermore, it is concluded that KSOFM neural network is an efficient way for understanding system behavior and knowledge extraction. Finally, it is indicated that UCS has more influence on ICR by applying the best trained MLP network weights in Garson equation which is also confirmed by KSOFM.",autonomous vehicle
10.1016/j.eswa.2021.114666,journal,Expert Systems with Applications,sciencedirect,2021-06-15,sciencedirect,NeuroEvolution of augmenting topologies for solving a two-stage hybrid flow shop scheduling problem: A comparison of different solution strategies,https://api.elsevier.com/content/article/pii/S095741742100107X,"The article investigates the application of NeuroEvolution of Augmenting Topologies (NEAT) to generate and parameterize artificial neural networks (ANN) on determining allocation and sequencing decisions in a two-stage hybrid flow shop scheduling environment with family setup times. NEAT is a machine-learning and neural architecture search algorithm, which generates both, the structure and the hyper-parameters of an ANN. Our experiments show that NEAT can compete with state-of-the-art approaches in terms of solution quality and outperforms them regarding computational efficiency. The main contributions of this article are: (i) A comparison of five different strategies, evaluated with 14 different experiments, on how ANNs can be applied for solving allocation and sequencing problems in a hybrid flow shop environment, (ii) a comparison of the best identified NEAT strategy with traditional heuristic and metaheuristic approaches concerning solution quality and computational efficiency.",autonomous vehicle
10.1016/j.engappai.2013.04.010,journal,Engineering Applications of Artificial Intelligence,sciencedirect,2013-10-31,sciencedirect,An appraisal and design of a multi-agent system based cooperative wireless intrusion detection computational intelligence technique,https://api.elsevier.com/content/article/pii/S0952197613000766,"
                  The deployment of wireless sensor networks and mobile ad-hoc networks in applications such as emergency services, warfare and health monitoring poses the threat of various cyber hazards, intrusions and attacks as a consequence of these networks’ openness. Among the most significant research difficulties in such networks safety is intrusion detection, whose target is to distinguish between misuse and abnormal behavior so as to ensure secure, reliable network operations and services. Intrusion detection is best delivered by multi-agent system technologies and advanced computing techniques. To date, diverse soft computing and machine learning techniques in terms of computational intelligence have been utilized to create Intrusion Detection and Prevention Systems (IDPS), yet the literature does not report any state-of-the-art reviews investigating the performance and consequences of such techniques solving wireless environment intrusion recognition issues as they gain entry into cloud computing. The principal contribution of this paper is a review and categorization of existing IDPS schemes in terms of traditional artificial computational intelligence with a multi-agent support. The significance of the techniques and methodologies and their performance and limitations are additionally analyzed in this study, and the limitations are addressed as challenges to obtain a set of requirements for IDPS in establishing a collaborative-based wireless IDPS (Co-WIDPS) architectural design. It amalgamates a fuzzy reinforcement learning knowledge management by creating a far superior technological platform that is far more accurate in detecting attacks. In conclusion, we elaborate on several key future research topics with the potential to accelerate the progress and deployment of computational intelligence based Co-WIDPSs.
               ",autonomous vehicle
10.1016/j.engappai.2014.03.007,journal,Engineering Applications of Artificial Intelligence,sciencedirect,2014-06-30,sciencedirect,An unsupervised feature selection algorithm based on ant colony optimization,https://api.elsevier.com/content/article/pii/S0952197614000621,"
                  Feature selection is a combinatorial optimization problem that selects most relevant features from an original feature set to increase the performance of classification or clustering algorithms. Most feature selection methods are supervised methods and use the class labels as a guide. On the other hand, unsupervised feature selection is a more difficult problem due to the unavailability of class labels. In this paper, we present an unsupervised feature selection method based on ant colony optimization, called UFSACO. The method seeks to find the optimal feature subset through several iterations without using any learning algorithms. Moreover, the feature relevance will be computed based on the similarity between features, which leads to the minimization of the redundancy. Therefore, it can be classified as a filter-based multivariate method. The proposed method has a low computational complexity, thus it can be applied for high dimensional datasets. We compare the performance of UFSACO to 11 well-known univariate and multivariate feature selection methods using different classifiers (support vector machine, decision tree, and naïve Bayes). The experimental results on several frequently used datasets show the efficiency and effectiveness of the UFSACO method as well as improvements over previous related methods.
               ",autonomous vehicle
10.1016/j.arcontrol.2021.04.001,journal,Annual Reviews in Control,sciencedirect,2021-12-31,sciencedirect,Robustness of AI-based prognostic and systems health management,https://api.elsevier.com/content/article/pii/S1367578821000195,"
                  Prognostic and systems Health Management (PHM) is an integral part of a system. It is used for solving reliability problems that often manifest due to complexities in design, manufacturing, operating environment and system maintenance. For safety-critical applications, using a model-based development process for complex systems might not always be ideal but it is equally important to establish the robustness of the solution. The information revolution has allowed data-driven methods to diffuse within this field to construct the requisite process (or system models) to cope with the so-called big data phenomenon. This is supported by large datasets that help machine-learning models achieve impressive accuracy. AI technologies are now being integrated into many PHM related applications including aerospace, automotive, medical robots and even autonomous weapon systems. However, with such rapid growth in complexity and connectivity, a systems’ behaviour is influenced in unforeseen ways by cyberattacks, human errors, working with incorrect or incomplete models and even adversarial phenomena. Many of these models depend on the training data and how well the data represents the test data. These issues require fine-tuning and even retraining the models when there is even a small change in operating conditions or equipment. Yet, there is still ambiguity associated with their implementation, even if the learning algorithms classify accordingly. Uncertainties can lie in any part of the AI-based PHM model, including in the requirements, assumptions, or even in the data used for training and validation. These factors lead to sub-optimal solutions with an open interpretation as to why the requirements have not been met. This warrants the need for achieving a level of robustness in the implemented PHM, which is a challenging task in a machine learning solution.
                  This article aims to present a framework for testing the robustness of AI-based PHM. It reviews some key milestones achieved in the AI research community to deal with three particular issues relevant for AI-based PHM in safety-critical applications: robustness to model errors, robustness to unknown phenomena and empirical evaluation of robustness during deployment. To deal with model errors, many techniques from probabilistic inference and robust optimisation are often used to provide some robustness guarantee metric. In the case of unknown phenomena, techniques include anomaly detection methods, using causal models, the construction of ensembles and reinforcement learning. It elicits from the authors’ work on fault diagnostics and robust optimisation via machine learning techniques to offer guidelines to the PHM research community. Finally, challenges and future directions are also examined; on how to better cope with any uncertainties as they appear during the operating life of an asset.
               ",autonomous vehicle
10.1016/S0921-8890(96)00068-1,journal,Robotics and Autonomous Systems,sciencedirect,1997-06-30,sciencedirect,Learning social behavior,https://api.elsevier.com/content/article/pii/S0921889096000681,"
                  This paper discusses the challenges of learning to behave socially in the dynamic, noisy, situated and embodied mobile multi-robot domain. Using the methodology for synthesizing basis behaviors as a substrate for generating a large repertoire of higher-level group interactions, in this paper we describe how, given the substrate, greedy agents can learn social rules that benefit the group as a whole. We describe three sources of reinforcement and show their effectiveness in learning non-greedy social rules. We then demonstrate the learning approach on a group of four mobile robots learning to yield and share information in a foraging task.
               ",autonomous vehicle
10.1016/j.kint.2019.11.037,journal,Kidney International,sciencedirect,2020-04-30,sciencedirect,Electronic health records for the diagnosis of rare diseases,https://api.elsevier.com/content/article/pii/S0085253820300120,"
                  With the emergence of electronic health records, the reuse of clinical data offers new perspectives for the diagnosis and management of patients with rare diseases. However, there are many obstacles to the repurposing of clinical data. The development of decision support systems depends on the ability to recruit patients, extract and integrate the patients’ data, mine and stratify these data, and integrate the decision support algorithm into patient care. This last step requires an adaptability of the electronic health records to integrate learning health system tools. In this literature review, we examine the research that provides solutions to unlock these barriers and accelerate translational research: structured electronic health records and free-text search engines to find patients, data warehouses and natural language processing to extract phenotypes, machine learning algorithms to classify patients, and similarity metrics to diagnose patients. Medical informatics is experiencing an impellent request to develop decision support systems, and this requires ethical considerations for clinicians and patients to ensure appropriate use of health data.
               ",autonomous vehicle
10.1016/j.jisa.2020.102722,journal,Journal of Information Security and Applications,sciencedirect,2021-03-31,sciencedirect,Weaponized AI for cyber attacks,https://api.elsevier.com/content/article/pii/S2214212620308620,"
                  Artificial intelligence (AI)-based technologies are actively used for purposes of cyber defense. With the passage of time and with decreasing complexity in implementing AI-based solutions, the usage of AI-based technologies for offensive purposes has begun to appear in the world. These attacks vary from tampering with medical images using adversarial machine learning for false identification of cancer to the generation of adversarial traffic signals for influencing the safety of autonomous vehicles. In this research, we investigated recent cyberattacks that utilize AI-based techniques and identified various mitigation strategies that are helpful in handling such attacks. Further, we identified existing methods and techniques that are used in executing AI-based cyberattacks and what probable future scenarios will be plausible to control such attacks by identifying existing trends in AI-based cyberattacks.
               ",autonomous vehicle
10.1016/j.infsof.2018.04.010,journal,Information and Software Technology,sciencedirect,2018-09-30,sciencedirect,The use of artificial neural networks for extracting actions and actors from requirements document,https://api.elsevier.com/content/article/pii/S0950584918300752,"
                  Context
                  The automatic extraction of actors and actions (i.e., use cases) of a system from natural language-based requirement descriptions, is considered a common problem in requirements analysis. Numerous techniques have been used to resolve this problem. Examples include rule-based (e.g., inference), keywords, query (e.g., bi-grams), library maintenance, semantic business vocabularies, and rules. The question remains: can combination of natural language processing (NLP) and artificial neural networks (ANNs) perform this job successfully and effectively?
               
                  Objective
                  This paper proposes a new approach to automatically identify actors and actions in a natural language-based requirements’ description of a system. Included are descriptions of how NLP plays an important role in extracting actors and actions, and how ANNs can be used to provide definitive identification.
               
                  Method
                  We used an NLP parser with a general architecture for text engineering, producing lexicons, syntaxes, and semantic analyses. An ANN was developed using five different use cases, producing different results due to their complexity and linguistic formation.
               
                  Results
                  Binomial classification accuracy techniques were used to evaluate the effectiveness of this approach. Based on the five use cases, the results were 17–63% for precision, 5–6100% for recall, and 29–71% for F-measure.
               
                  Conclusion
                  We successfully used a combination of NLP and ANN artificial intelligence techniques to reveal specific domain semantics found in a software requirements specification. An Intelligent Technique for Requirements Engineering (IT4RE) was developed to provide a semi-automated approach, classified as Intelligent Computer Aided Software Engineering (I-CASE).
               ",autonomous vehicle
10.1016/j.heliyon.2021.e08123,journal,Heliyon,sciencedirect,2021-10-31,sciencedirect,Process monitoring for quality — A multiple classifier system for highly unbalanced data,https://api.elsevier.com/content/article/pii/S240584402102226X,"In big data-based analyses, because of hyper-dimensional feature spaces, there has been no previous distinction between machine learning algorithms (MLAs). Therefore, multiple diverse algorithms should be included in the analysis to develop an adequate model for detecting/recognizing patterns exhibited by classes. If multiple classifiers are developed, the next natural step is to determine whether the prediction benchmark set by the top performer can be improved by combining them. In this context, multiple classifier systems (MCSs) are powerful solutions for difficult pattern recognition problems because they usually outperform the best individual classifier, and their diversity tends to improve resilience and robustness to high-dimensional and noisy data. To design an MCS, an appropriate fusion method is required to optimally combine the individual classifiers and determine the final decision. Process monitoring for quality is a Quality 4.0 initiative aimed at defect detection via binary classification. Because most mature organizations have merged traditional quality philosophies, their processes generate only a few defects per million of opportunities. Therefore, manufacturing data sets for binary classification of quality tends to be highly/ultra-unbalanced. Detecting these rare quality events is one of the most relevant intellectual challenges posed to the fourth industrial revolution, Industry 4.0 (I 4.0). A new MCS aimed at analyzing these data structures is presented. It is based on eight well-known MLAs, an ad hoc fitness function, and a novel meta-learning algorithm. For predicting the final quality class, this algorithm considers the prediction from a set of classifiers as input and determines which classifiers are reliable and which are not. Finally, to demonstrate the superiority of the MLAs over extensively used fusion rules, multiple publicly available data sets are analyzed.",autonomous vehicle
10.1016/S0921-8890(97)80708-7,journal,Robotics and Autonomous Systems,sciencedirect,1997-06-30,sciencedirect,Living in a partially structured environment: How to bypass the limitations of classical reinforcement techniques,https://api.elsevier.com/content/article/pii/S0921889097807087,"
                  In this paper, we propose an unsupervised neural network allowing a robot to learn sensory-motor associations with a delayed reward. The robot task is to learn the “meaning” of pictograms in order to “survive” in a maze. First, we introduce a new neural conditioning rule probabilistic conditioning rule (PCR) allowing us to test hypotheses (associations between visual categories and movements) during a given time span. Second, we describe a real maze experiment with our mobile robot. We propose a neural architecture overcoming the difficulty to build visual categories dynamically while associating them to movements. Third, we propose to use our algorithm on a simulation in order to test it exhaustively. We give the results for different kinds of mazes and we compare our system to an adapted version of the Q-learning algorithm. Finally, we conclude by showing the limitations of approaches that do not take into account the intrinsic complexity of a reasoning based on image recognition.
               ",autonomous vehicle
10.1016/j.ijpsycho.2017.11.017,journal,International Journal of Psychophysiology,sciencedirect,2018-10-31,sciencedirect,Feedback information and the reward positivity,https://api.elsevier.com/content/article/pii/S0167876017303690,"
                  The reward positivity is a component of the event-related brain potential (ERP) sensitive to neural mechanisms of reward processing. Multiple studies have demonstrated that reward positivity amplitude indices a reward prediction error signal that is fundamental to theories of reinforcement learning. However, whether this ERP component is also sensitive to richer forms of performance information important for supervised learning is less clear. To investigate this question, we recorded the electroencephalogram from participants engaged in a time estimation task in which the type of error information conveyed by feedback stimuli was systematically varied across conditions. Consistent with our predictions, we found that reward positivity amplitude decreased in relation to increasing information content of the feedback, and that reward positivity amplitude was unrelated to trial-to-trial behavioral adjustments in task performance. By contrast, a series of exploratory analyses revealed frontal-central and posterior ERP components immediately following the reward positivity that related to these processes. Taken in the context of the wider literature, these results suggest that the reward positivity is produced by a neural mechanism that motivates task performance, whereas the later ERP components apply the feedback information according to principles of supervised learning.
               ",autonomous vehicle
10.1016/j.asoc.2021.107806,journal,Applied Soft Computing,sciencedirect,2021-11-30,sciencedirect,Jamming detection at the edge of drone networks using Multi-layer Perceptrons and Decision Trees,https://api.elsevier.com/content/article/pii/S1568494621007274,"
                  As wireless networks play an increasingly key role in everyday life, it is necessary to secure them from radio frequency attacks, such as jamming, which are hard to detect, especially because they may be easily mistaken for other network conditions. Within this challenging context, the paper proposes a framework for jamming detection in drone networks, relying on a distributed approach based on supervised machine learning techniques, namely, Multi-layer Perceptrons and Decision Trees. Given a reference data packet trace set, our framework computes the features of some predefined metrics, such as throughput, PDR and RSSI, which vary during a jamming attack, and that can therefore be used to detect it. We evaluate our framework using datasets from publicly available standardized jamming attack scenarios with IEEE 802.11p radio data, and via ns3-based simulation datasets from networks of drones using WiFi. We show that the performance of the classifiers improves as the sampling time of the packets decreases. We also show that the Multi-layer Perceptron can be effectively generalized to achieve jamming detection accuracy superior to that of Decision Trees even when applied to communication scenarios for which it has not been specifically trained. Our proposed framework reaches a satisfactory accuracy level of 96%, while requiring low computational and hardware capabilities, thus proving to be suitable for resource-constrained drone networks.
               ",autonomous vehicle
10.1016/j.tics.2019.07.012,journal,Trends in Cognitive Sciences,sciencedirect,2019-10-31,sciencedirect,Where Does Value Come From?,https://api.elsevier.com/content/article/pii/S1364661319302001,"
                  The computational framework of reinforcement learning (RL) has allowed us to both understand biological brains and build successful artificial agents. However, in this opinion, we highlight open challenges for RL as a model of animal behaviour in natural environments. We ask how the external reward function is designed for biological systems, and how we can account for the context sensitivity of valuation. We summarise both old and new theories proposing that animals track current and desired internal states and seek to minimise the distance to a goal across multiple value dimensions. We suggest that this framework readily accounts for canonical phenomena observed in the fields of psychology, behavioural ecology, and economics, and recent findings from brain-imaging studies of value-guided decision-making.
               ",autonomous vehicle
10.1016/j.ijhcs.2009.03.004,journal,International Journal of Human-Computer Studies,sciencedirect,2009-08-31,sciencedirect,Interacting meaningfully with machine learning systems: Three experiments,https://api.elsevier.com/content/article/pii/S1071581909000457,"
                  Although machine learning is becoming commonly used in today's software, there has been little research into how end users might interact with machine learning systems, beyond communicating simple “right/wrong” judgments. If the users themselves could work hand-in-hand with machine learning systems, the users’ understanding and trust of the system could improve and the accuracy of learning systems could be improved as well. We conducted three experiments to understand the potential for rich interactions between users and machine learning systems. The first experiment was a think-aloud study that investigated users’ willingness to interact with machine learning reasoning, and what kinds of feedback users might give to machine learning systems. We then investigated the viability of introducing such feedback into machine learning systems, specifically, how to incorporate some of these types of user feedback into machine learning systems, and what their impact was on the accuracy of the system. Taken together, the results of our experiments show that supporting rich interactions between users and machine learning systems is feasible for both user and machine. This shows the potential of rich human–computer collaboration via on-the-spot interactions as a promising direction for machine learning systems and users to collaboratively share intelligence.
               ",autonomous vehicle
10.1016/j.future.2021.06.021,journal,Future Generation Computer Systems,sciencedirect,2021-12-31,sciencedirect,Optimizing makespan and resource utilization for multi-DNN training in GPU cluster,https://api.elsevier.com/content/article/pii/S0167739X21002168,"
                  Deep neural network (DNN) has been widely applied in many fields of artificial intelligence (AI), gaining great popularity both in industry and academia. Increasing the size of DNN models does dramatically improve the learning accuracy. However, training large-scale DNN models on a single GPU takes unacceptable waiting time. In order to speed up the training process, many distributed deep learning (DL) systems and frameworks have been published and designed for parallel DNN training with multiple GPUs. However, most of the existing studies concentrate only on improving the training speed of a single DNN model under centralized or decentralized systems with synchronous or asynchronous approaches. Few works consider the issue of multi-DNN training on the GPU cluster, which is the joint optimization problem of job scheduling and resource allocation. This paper proposes an optimizing makespan and resource utilization (OMRU) approach to minimize job completion time and improve resource utilization for multi-DNN training in a GPU cluster. Specifically, we first collect the training speed/time data of all DNN models by running a job for one epoch on a different number of GPUs. The OMRU algorithm, integrating job scheduling, resource allocation, and GPU reuse strategies, is then devised to minimize the total job completion time (also called makespan) and improve GPU cluster resource utilization. The linear scaling rule (LSR) is adopted for adjusting the learning rate when a DNN model is trained on multiple GPUs with large minibatch size, which can guarantee model accuracy without the other hyper-parameters tune-up. We implement the OMRU algorithm on the Pytorch with Ring-Allreduce communication architecture and a GPU cluster with 8 nodes, each of which has 4 NVIDIA V100 GPUs. Experimental results on image classification and action recognition show that OMRU achieves a makespan reduction of up to 30% compared to the baseline scheduling algorithms and reach an average of 98.4% and 99.2% resource utilization on image classification and action recognition, respectively, with the state-of-the-art model accuracy.
               ",autonomous vehicle
10.1016/0893-6080(94)00063-R,journal,Neural Networks,sciencedirect,1995-12-31,sciencedirect,"A real-time, unsupervised neural network for the low-level control of a mobile robot in a nonstationary environment",https://api.elsevier.com/content/article/pii/089360809400063R,"
                  This article introduces a real-time, unsupervised neural network that learns to control a two-degree-of-freedom mobile robot in a nonstationary environment. The neural controller, which is termed neural NETwork MObile Robot Controller (NETMORC), combines associative learning and Vector Associative Map (VAM) learning to generate transformations between spatial and velocity coordinates. As a result, the controller learns the wheel velocities required to reach a target at an arbitrary distance and angle. The transformations are learned in an unsupervised training phase, during which the robot moves as a result of randomly selected wheel velocities. The robot learns the relationship between these velocities and the resulting incremental movements. Aside from being able to reach stationary or moving targets, the NETMORC structure also enables the robot to perform successfully in spite of disturbances in the environment, such as wheel slippage, or changes in the robot's plant, including changes in wheel radius, changes in interwheel distance, or changes in the internal time step of the system. Finally, the controller is extended to include a module that learns an internal odometric transformation, allowing the robot to reach targets when visual input is sporadic or unreliable.
               ",autonomous vehicle
10.1016/j.patcog.2020.107677,journal,Pattern Recognition,sciencedirect,2021-03-31,sciencedirect,Exploring global diverse attention via pairwise temporal relation for video summarization,https://api.elsevier.com/content/article/pii/S0031320320304805,"
                  Video summarization is an effective way to facilitate video searching and browsing. Most of existing systems employ encoder-decoder based recurrent neural networks, which fail to explicitly diversify the system-generated summary frames while requiring intensive computations. In this paper, we propose an efficient convolutional neural network architecture for video SUMmarization via Global Diverse Attention called SUM-GDA, which adapts attention mechanism in a global perspective to consider pairwise temporal relations of video frames. Particularly, the GDA module has two advantages: (1) it models the relations within paired frames as well as the relations among all pairs, thus capturing the global attention across all frames of one video; (2) it reflects the importance of each frame to the whole video, leading to diverse attention on these frames. Thus, SUM-GDA is beneficial for generating diverse frames to form satisfactory video summary. Extensive experiments on three data sets, i.e., SumMe, TVSum, and VTW, have demonstrated that SUM-GDA and its extension outperform other competing state-of-the-art methods with remarkable improvements. In addition, the proposed models can be run in parallel with significantly less computational costs, which helps the deployment in highly demanding applications.
               ",autonomous vehicle
10.1016/j.eursup.2004.02.016,journal,European Urology Supplements,sciencedirect,2004-03-31,sciencedirect,Artificial Neural Networks in Urology 2004,https://api.elsevier.com/content/article/pii/S1569905604000429,"
                  At the First Meeting of the European Society for Oncological Urology (ESOU) in Vienna the value of artificial neural networks in urology (ANN) were discussed. An ANN is an artificial intelligence tool that identifies arbitrary nonlinear multiparametric discriminant functions directly from clinical data. The use of ANNs has gained increasing popularity for applications where description of the dependency between dependent and independent variables is either unknown or very complex. This learning technique can be roughly described as a universal algebraic function that will distinguish signal from noise directly from clinical data. The application of ANNs to complex relationships makes them highly attractive for the study of complexed medical decision making. Recent applications include diagnosis, staging and progression of prostate cancer, progression of benign prostate hyperplasia, and bladder cancer recurrence in Ta/T1 bladder cancers.
                  Accuracy of ANNs are between 77–91%, 81–88% and 80–90% for prostate cancer diagnosis, staging, and prediction of prognosis after radical prostatectomy, respectively.
               ",autonomous vehicle
10.1016/0004-3702(92)90058-6,journal,Artificial Intelligence,sciencedirect,1992-06-30,sciencedirect,Automatic programming of behavior-based robots using reinforcement learning,https://api.elsevier.com/content/article/pii/0004370292900586,"
                  This paper describes a general approach for automatically programming a behavior-based robot. New behaviors are learned by trial and error using a performance feedback function as reinforcement. Two algorithms for behavior learning are described that combine Q learning, a well-known scheme for propagating reinforcement values temporally across actions, with statistical clustering and Hamming distance, two ways of propagating reinforcement values spatially across states. A real behavior-based robot called OBELIX is described that learns several component behaviors in an example task involving pushing boxes. A simulator for the box pushing task is also used to gather data on the learning techniques. A detailed experimental study using the real robot and the simulator suggests two conclusions. 
                        
                           1.
                           (1) The learning techniques are able to learn the individual behaviors, sometimes outperforming a handcoded program.
                        
                        
                           2.
                           (2) Using a behavior-based architecture speeds up reinforcement learning by converting the problem of learning a complex task into that of learning a simpler set of special-purpose reactive subtasks.
                        
                     
                  
               ",autonomous vehicle
10.1016/B978-0-08-041898-8.50080-3,journal,Artificial Intelligence in Real-Time Control 1992,sciencedirect,1993-12-31,sciencedirect,REINFORCEMENT LEARNING AND RECRUITMENT MECHANISM FOR ADAPTIVE DISTRIBUTED CONTROL,https://api.elsevier.com/content/article/pii/B9780080418988500803,"
               Keywords Process Control; Reinforcement Learning; Dynamic Programming; Distributed Control; Immune System; Recruitment Mechanism",autonomous vehicle
10.1016/B978-0-08-097086-8.43005-9,journal,International Encyclopedia of the Social & Behavioral Sciences,sciencedirect,2015-12-31,sciencedirect,Artificial Intelligence: Connectionist and Symbolic Approaches,https://api.elsevier.com/content/article/pii/B9780080970868430059,"
               This article describes the two competing paradigms of artificial intelligence: connectionist and symbolic approaches. Each paradigm has its strengths and weaknesses. No single existing paradigm currently can fully address all major issues concerning intelligence and cognition. This condition indicates the need to integrate these existing paradigms.
            ",autonomous vehicle
10.1016/j.fuel.2021.121202,journal,Fuel,sciencedirect,2021-10-15,sciencedirect,Developing a model for prediction of the combustion performance and emissions of a turboprop engine using the long short-term memory method,https://api.elsevier.com/content/article/pii/S0016236121010814,"
                  In this study, the exhaust emissions index and combustion efficiency of the single shaft T56-A-15 engine are modeled using the Long-Short Term Memory (LSTM) method, one of the recent artificial intelligence algorithms. For this purpose, emissions data based on air–fuel ratio (AFR), engine speed (RPM) and different fuel flow rate parameters are used experimentally under different loads. In the designed LSTM models, fuel flow, engine speed and AFR are used as input parameters for the prediction of exhaust emission indices, engine speed and AFR data is used as an input parameter for the prediction of combustion efficiency. In the designed system, the experimental data is divided into two, 80% training and 20% test, by crossing according to the k-fold 5 value. Mean Absolute Error (MAE), Mean Square Error (MSE), Root Mean Square Error (RMSE), Mean Absolute Percentage Error (MAPE) error functions and the R squared (R2) function are used in the evaluation of the designed LSTM models. The originality of this study is the prediction of the exhaust emissions index and combustion efficiency values for the T56-A-15 turboprop engine using the LSTM method, which is an artificial intelligence method. This study attempts to address the literature gap in the calculation of the CO, CO2, UHC, NO2 exhaust emissions index and combustion efficiency values with an accuracy of over 95%, without the need for hundreds of experimental studies required for intermediate values.
               ",autonomous vehicle
10.1016/B978-0-08-097086-8.43068-0,journal,International Encyclopedia of the Social & Behavioral Sciences,sciencedirect,2015-12-31,sciencedirect,Motor Control Models: Learning and Performance,https://api.elsevier.com/content/article/pii/B9780080970868430680,"
               The focus of the article is on the variety of attempts that have been investigated for capturing the complexity of purposive action and adaptive behavior, having defined a coordinated action as a class of movements plus a goal. Redundancy is a side effect of this connection, and thus redundancy is necessarily task oriented, something to be managed ‘online’ and rapidly updated as the action unfolds. The article then analyzes two main computational mechanisms that have been proposed as candidates of how the brain may deal with motor redundancy: (1) the force-field-based solution known as Equilibrium-Point Hypothesis (EPH) and (2) the cost-function-based solution to the degrees of freedom problem, namely Optimal Control Theory. However, both theories apply only to overt actions where force fields and cost functions are directly related to the interaction of the body with the physical world in the course of a real action. Considering that overt actions are just the tip of the iceberg, hiding the vast domain of covert actions that are the skeleton of motor cognition, an extension of EPH is described, Passive Motion Paradigm (PMP). The relationships between PMP, the simulation theory of covert actions, internal models, and the body schema concept are also analyzed. Finally, general learning mechanisms that may support the acquisition of internal computational modules are briefly summarized.
            ",autonomous vehicle
10.1016/j.jnca.2021.103244,journal,Journal of Network and Computer Applications,sciencedirect,2021-12-15,sciencedirect,Towards development of IoT-ML driven healthcare systems: A survey,https://api.elsevier.com/content/article/pii/S1084804521002423,"
                  The impact of IoT-ML in the healthcare sector is very significant and it has helped us to change our view at the traditional treatment methods. In IoT-ML-based healthcare applications, the sensing layer is responsible for collecting information from humans and transferring it to the storage layer through communication technology. ML is implemented to make intelligent decisions for healthcare applications. This survey shows all the fields starting from the IoT sensor devices to the deployment of ML in the healthcare sector. We have conducted a comprehensive survey of the existing literature covering IoT and ML strategies from a healthcare perspective. We also provide insights into the different types of network storage and computing strategies used for other health-based applications. We believe that the presented work is innovative as no other survey is furnished in such manner. From this survey, researchers can get an overview of IoT-ML and cloud-based healthcare applications under the single system. We have proposed a unique taxonomy from an IoT-ML-based healthcare perspective where we have highlighted key steps in developing healthcare systems. We have culminated the most striking technologies in IoT, communications, network storage and computing, and ML for healthcare systems. Another contribution of our survey is that we have collected and discussed surveys and scientific literature based on the proposed taxonomy and their sub-taxonomy throughout this paper. Besides that we have reviewed several types of popularly used sensors, development boards in healthcare with various examples. We also show the mapping of communication technology with the protocols used by IoT sensors. In the ML section, we have shown an ML pipeline centering on healthcare application and discussed every step of it. Finally, we have identified a number of research challenges including exploration of Deep Learning based models, proper data acquisition and handling of data, privacy and ethics, security issues in WBAN, etc. These research challenges will provide the researchers the necessary future research directions while developing IoT-ML-based healthcare applications.
               ",autonomous vehicle
10.1016/S0893-6080(05)80007-3,journal,Neural Networks,sciencedirect,1992-12-31,sciencedirect,CALM: Categorizing and learning module,https://api.elsevier.com/content/article/pii/S0893608005800073,"
                  A new procedure (CALM: Categorizing and Learning Module) is introduced for unsupervised learning in modular neural networks. The work described addresses a number of problems in connectionist modeling, such as lack of speed, lack of stability, inability to learn either with or without supervision, and the inability to both discriminate between and generalize over patterns. CALM is a single module that can be used to construct larger networks. A CALM module consists of pairs of excitatory Representation- and inhibitory Veto-nodes, and an Arousal-node. Because of the fixed internal wiring pattern of a module, the Arousal-node is sensitive to the novelty of the input pattern. The activation of the Arousal-node determines two psychologically motivated types of learning operating in the module: elaboration learning, which implies a high learning rate and the distribution of nonspecific, random activations in the module, and activation learning, which has only base rate learning without random activations. The learning rule used is a modified version of a rule described by Grossberg. The workings of CALM networks are illustrated in a number of simulations. It is shown that a CALM module quickly reaches a categorization, even with new patterns. Though categorization and learning are relatively fast compared to other models, CALM modules do not suffer from excessive plasticity. They are also shown to be capable of both discriminating between and generalizing over patterns. When presented with a pattern set exceeding the number of Representation-nodes, similar patterns are assigned to the same node. Multi-modular simulations showed that with supervised learning an average of 1.6 presentations sufficed to learn the EXOR function. Moreover, an unsupervised learning version of the McClelland and Rumelhart model successfully simulated a word superiority effect. It is concluded that the incorporation of psychologically and biologically plausible structural and functional characteristics, like modularity, unsupervised (competitive) learning, and a novelty dependent learning rate, may contribute to solving some of the problems often encountered in connectionist modeling.
               ",autonomous vehicle
10.1016/0005-7916(95)00050-X,journal,Journal of Behavior Therapy and Experimental Psychiatry,sciencedirect,1995-12-31,sciencedirect,Synthesizing animal and human behavior research via neural network learning theory,https://api.elsevier.com/content/article/pii/000579169500050X,"
                  Animal and human research have been “divorced” since approximately 1968. Several recent articles have tried to persuade behavior therapists of the merits of animal research. Three reasons are given concerning why disinterest in animal research is so widespread: (1) functional explanations are given for animals, and cognitive explanations are given for humans; (2) serial symbol manipulating models are used to explain human behavior; and (3) human learning was assumed, thereby removing it as something to be explained. Brain-inspired connectionist neural networks, collectively referred to as neural network learning theory (NNLT), are briefly described, and a spectrum of their accomplishments from simple conditioning through speech is outlined. Five benefits that behavior therapists can derive from NNLT are described. They include (a) enhanced professional identity derived from a comprehensive learning theory, (b) improved interdisciplinary collaboration both clinically and scientifically, (c) renewed perceived relevance of animal research, (d) access to plausible proximal causal mechanisms capable of explaining operant conditioning, and (e) an inherently developmental perspective.
               ",autonomous vehicle
10.1016/j.neucom.2019.08.003,journal,Neurocomputing,sciencedirect,2019-11-20,sciencedirect,Complete vector quantization of feedforward neural networks,https://api.elsevier.com/content/article/pii/S0925231219311129,"
                  Deep neural networks are widely used to solve several difficult machine learning tasks due to their impressive performance on standard benchmarking datasets. Most of the state of the art neural architectures contain a staggering amount of parameters and have many layers. Thus, they are computationally intensive and memory demanding models. This fact prohibits their deployment in devices with limited computational resources such as smartphones and unmanned vehicles. Recently, a growing interest in developing methods that can compress and accelerate these networks emerged. In this paper, we propose the use of complete vector quantization for neural model compression and acceleration. More specifically, we show that it is possible to use product quantization with common subdictionaries to quantize both the parameters and the activations of the neural network without compromising significantly the network accuracy. The proposed method removes the need for multiplications in order to compute the neural preactivations and provides opportunities for acceleration using lookup tables.
               ",autonomous vehicle
10.3182/20020721-6-ES-1901.00972,journal,IFAC Proceedings Volumes,sciencedirect,2002-12-31,sciencedirect,A NEW METHOD OF THE EVOLUTIONARY ALGORITHM FOR ADAPTIVE LEARNING CONTROL,https://api.elsevier.com/content/article/pii/S1474667015393939,"
                  In case of restricting the relation between the parents and the offspring as each one in order to learn in real time, a new numerical formula is proposed to solve the difficulty of adjusting the search region as reducing the individual by using the error generated from the sensor of the dynamic system. The mutation equation of evolutionary strategy uses the error that is generated from the dynamic system. Competitive individuals among total population are reduced with automatic adjustments of the search region in accord with the error. Therefore, it is possible to control the control object varied as time changes because control signal of the learning is generated in real-time. As the state value of the control object is generated, applied evolutionary strategy each sampling time because the learning process of an estimation, selection and mutation is done in real-time. These algorithms can be applied; the people who do not have knowledge about the technical tuning of dynamic systems could design the controller or problems in which the characteristics of the system dynamics are slightly varied as time changes.
               ",autonomous vehicle
10.1016/j.bdr.2018.04.002,journal,Big Data Research,sciencedirect,2018-12-31,sciencedirect,A Dynamic Neural Network Architecture with Immunology Inspired Optimization for Weather Data Forecasting,https://api.elsevier.com/content/article/pii/S2214579617303696,"
                  Recurrent neural networks are dynamical systems that provide for memory capabilities to recall past behaviour, which is necessary in the prediction of time series. In this paper, a novel neural network architecture inspired by the immune algorithm is presented and used in the forecasting of naturally occurring signals, including weather big data signals. Big Data Analysis is a major research frontier, which attracts extensive attention from academia, industry and government, particularly in the context of handling issues related to complex dynamics due to changing weather conditions. Recently, extensive deployment of IoT, sensors, and ambient intelligence systems led to an exponential growth of data in the climate domain. In this study, we concentrate on the analysis of big weather data by using the Dynamic Self Organized Neural Network Inspired by the Immune Algorithm. The learning strategy of the network focuses on the local properties of the signal using a self-organised hidden layer inspired by the immune algorithm, while the recurrent links of the network aim at recalling previously observed signal patterns. The proposed network exhibits improved performance when compared to the feedforward multilayer neural network and state-of-the-art recurrent networks, e.g., the Elman and the Jordan networks. Three non-linear and non-stationary weather signals are used in our experiments. Firstly, the signals are transformed into stationary, followed by 5-steps ahead prediction. Improvements in the prediction results are observed with respect to the mean value of the error (RMS) and the signal to noise ratio (SNR), however to the expense of additional computational complexity, due to presence of recurrent links.
               ",autonomous vehicle
10.1016/j.neucom.2020.08.087,journal,Neurocomputing,sciencedirect,2021-07-15,sciencedirect,Bridge health anomaly detection using deep support vector data description,https://api.elsevier.com/content/article/pii/S0925231221001211,"
                  As an extremely important part of traffic arteries, bridge structure plays an essential role in national economic construction, social development and smart city. Thus the monitoring of the bridge structure health are increasingly concerned by the bridge industry scholars and engineering people at home and aboard. In this paper, we propose a deep learning framework to evaluate the safety of the bridge structural state. More specifically, the proposed system generates a learnable transformation which attempts to map most of the data network representations into a hypersphere characterized of minimum volume. During inference, mappings of normal examples fall within the learned hypersphere, whereas mappings of anomalies fall outside the hypersphere. The whole system is end-to-end trainable and outperforms other advanced methods in real-world dataset.
               ",autonomous vehicle
10.1016/j.jbi.2005.04.002,journal,Journal of Biomedical Informatics,sciencedirect,2006-04-30,sciencedirect,A machine learning perspective on the development of clinical decision support systems utilizing mass spectra of blood samples,https://api.elsevier.com/content/article/pii/S1532046405000432,"Currently, the best way to reduce the mortality of cancer is to detect and treat it in the earliest stages. Technological advances in genomics and proteomics have opened a new realm of methods for early detection that show potential to overcome the drawbacks of current strategies. In particular, pattern analysis of mass spectra of blood samples has attracted attention as an approach to early detection of cancer. Mass spectrometry provides rapid and precise measurements of the sizes and relative abundances of the proteins present in a complex biological/chemical mixture. This article presents a review of the development of clinical decision support systems using mass spectrometry from a machine learning perspective. The literature is reviewed in an explicit machine learning framework, the components of which are preprocessing, feature extraction, feature selection, classifier training, and evaluation.",autonomous vehicle
10.1016/B978-0-444-88400-8.50028-5,journal,Advanced Neural Computers,sciencedirect,1990-12-31,sciencedirect,THE ROLE OF TIME IN NATURAL INTELLIGENCE: IMPLICATIONS FOR NEURAL NETWORK AND ARTIFICIAL INTELLIGENCE RESEARCH,https://api.elsevier.com/content/article/pii/B9780444884008500285,"
               Real-time, closed-loop interactions between biological systems and their environments appear to be fundamental to the mechanisms that underlie natural intelligence. Such a theoretical view has given rise to neuronal models that predict a vide range of Pavlovian animal learning phenomena and to neural networks that may model instrumental conditioning phenomena. The theoretical framework that is emerging suggests an alternative to the current paradigms for neural network and artificial intelligence research.
            ",autonomous vehicle
10.1016/j.cogsys.2016.08.001,journal,Cognitive Systems Research,sciencedirect,2017-03-31,sciencedirect,Value systems for developmental cognitive robotics: A survey,https://api.elsevier.com/content/article/pii/S1389041716301280,"
                  This paper surveys value systems for developmental cognitive robotics. A value system permits a biological brain to increase the likelihood of neural responses to selected external phenomena. Many machine learning algorithms capture the essence of this learning process. However, computational value systems aim not only to support learning, but also autonomous attention focus to direct learning. This combination of unsupervised attention focus and learning aims to address the grand challenge of autonomous mental development for machines. This survey examines existing value systems for developmental cognitive robotics in this context. We examine the definitions of value used—including recent pioneering work in intrinsic motivation as value—as well as initialisation strategies for innate values, update strategies for acquired value and the data structures used for storing value. We examine the extent to which existing value systems support attention focus, learning and prediction in an unsupervised setting. The types of robots and applications in which these value systems are used are also examined, as well as the ways that these applications are evaluated. Finally, we study the strengths and limitations of current value systems for developmental cognitive robots and conclude with a set of research challenges for this field.
               ",autonomous vehicle
10.1016/j.neucom.2021.05.070,journal,Neurocomputing,sciencedirect,2021-09-30,sciencedirect,DDQN-TS: A novel bi-objective intelligent scheduling algorithm in the cloud environment,https://api.elsevier.com/content/article/pii/S0925231221008237,"
                  Task scheduling has always been one of the crucial problem in cloud computing. With the transition of task types from static batch processing to dynamic stream processing, the dynamic online task scheduling problem has attracted widespread attention. At this stage, explore an effective task scheduling method to implement high quality of service (QoS) requests with limited resources is a considerable challenge. This paper proposes a novel scheduling algorithm called double deep Q-network task scheduling (DDQN-TS), which uses the adaptive learning ability of double deep Q-network (DDQN) to explore the optimal task scheduling strategy. Experiments conducted using the Random, Google, and Alibaba benchmarks to compare several classic algorithms show that the proposed DDQN-TS can guarantee a high task completion rate and efficiently reduce the task average response time.
               ",autonomous vehicle
10.1016/j.neucom.2013.08.021,journal,Neurocomputing,sciencedirect,2014-03-27,sciencedirect,Autonomous intelligent decision-making system based on Bayesian SOM neural network for robot soccer,https://api.elsevier.com/content/article/pii/S0925231213008977,"
                  The complex confrontation in robot soccer match requires the decision-making system to learn the priori-knowledge given by humans and learn from its own experience. The two learning issues are usually addressed in two phases: off-line learning and on-line learning. Though lots of methods have been developed to address the two issues separately, the construction of a fully autonomous intelligent decision-making system remains challenging because of the difficulty of connecting the two phases. Most existing intelligent decision-making systems focus on only one of the two phases consequently. The model and algorithms of the Bayesian SOM neural network are proposed in this paper, based on which a fully autonomous intelligent decision-making system for robot soccer is built. This model provides a knowledge structure which can be shared by the off-line learning and on-line learning algorithms. By integrating the Bayesian classifier into each neuron, the whole neural network is equivalent to a multi-agent decision-making system. In the on-line learning phase, the Bayesian method is used to update each neuron's beliefs and the whole network's estimation of the state space. In matches with different opponents, this Bayesian SOM intelligent decision-making system showed outstanding learning ability and great adaptivity.
               ",autonomous vehicle
10.1016/j.bica.2018.10.006,journal,Biologically Inspired Cognitive Architectures,sciencedirect,2018-10-31,sciencedirect,Multi-level metacognition for adaptive behavior,https://api.elsevier.com/content/article/pii/S2212683X18301439,"
                  Behavior adaptation is an integral aspect for autonomous agents to survive in a world where change is normal. Animals change their foraging routines and socializing habits based on predator risks in their environment. Humans adapt their behavior based on current interests, social norms, stress level, health conditions, upcoming deadlines and various other factors. Artificial agents need to effectively adapt to changes in their environment such that they can quickly adjust their behavior to maintain performance in the changed environment. In this paper, we present a multi-level metacognitive model that allows agents to adapt their behavior in various ways based on the resources available for metacognitive processing. As the agent operates at higher levels of this model, the agent is better equipped to adapt to a wider range of changes. The model has been tested on 2 different applications: (i) a reinforcement learner-based agent trying to navigate and collect rewards in a seasonal grid-world environment and (ii) a convolutional neural network-based agent trying to classify the signals in a radio frequency spectrum world and separate them into known modulations and unknown modulations.
               ",autonomous vehicle
10.1016/j.comnet.2021.108435,journal,Computer Networks,sciencedirect,2021-11-09,sciencedirect,Comprehensive survey on self-organizing cellular network approaches applied to 5G networks,https://api.elsevier.com/content/article/pii/S1389128621003960,"
                  Self-Organizing Network (SON) stands for a key concept characterizing the behavior of the future mobile networks. The evolution of telecom infrastructures towards 5G transforms the network management from the traditional and static processes to automatic and dynamic ones. SON was proposed to offer agile on-demand services to the users through providing self-adaptation capabilities to mobile networks on different categories. This paper presents a detailed and exhaustive survey on SON evolution from 4G towards 5G networks. The central focus of this survey is upon providing a deep understanding of SON mechanisms along with the architectural changes associated with 5G networks. Within this framework, the approaches and trends in self-organizing cellular networks are discussed. Additionally, the main functionalities of SON, namely self-configuration, self-optimization and self-healing are displayed. Our work serves as an enlightening guideline for future research works on SON as far as cellular networks domain is concerned.
               ",autonomous vehicle
10.1016/j.addma.2021.102089,journal,Additive Manufacturing,sciencedirect,2021-10-31,sciencedirect,Towards developing multiscale-multiphysics models and their surrogates for digital twins of metal additive manufacturing,https://api.elsevier.com/content/article/pii/S2214860421002542,"Artificial intelligence (AI) embedded within digital models of manufacturing processes can be used to improve process productivity and product quality significantly. The application of such advanced capabilities particularly to highly digitalized processes such as metal additive manufacturing (AM) is likely to make those processes commercially more attractive. AI capabilities will reside within Digital Twins (DTs) which are living virtual replicas of the physical processes. DTs will be empowered to operate autonomously in a diagnostic control capacity to supervise processes and can be interrogated by the practitioner to inform the optimal processing route for any given product. The utility of the information gained from the DTs would depend on the quality of the digital models and, more importantly, their faster-solving surrogates which dwell within DTs for consultation during rapid decision-making. In this article, we point out the exceptional value of DTs in AM and focus on the need to create high-fidelity multiscale-multiphysics models for AM processes to feed the AI capabilities. We identify technical hurdles for their development, including those arising from the multiscale and multiphysics characteristics of the models, the difficulties in linking models of the subprocesses across scales and physics, and the scarcity of experimental data. We discuss the need for creating surrogate models using machine learning approaches for real-time problem-solving. We further identify non-technical barriers, such as the need for standardization and difficulties in collaborating across different types of institutions. We offer potential solutions for all these challenges, after reflecting on and researching discussions held at an international symposium on the subject in 2019. We argue that a collaborative approach can not only help accelerate their development compared with disparate efforts, but also enhance the quality of the models by allowing modular development and linkages that account for interactions between the various sub-processes in AM. A high-level roadmap is suggested for starting such a collaboration.",autonomous vehicle
10.1016/j.cities.2019.102481,journal,Cities,sciencedirect,2020-01-31,sciencedirect,Understanding cities with machine eyes: A review of deep computer vision in urban analytics,https://api.elsevier.com/content/article/pii/S0264275119308443,"
                  Modelling urban systems has interested planners and modellers for decades. Different models have been achieved relying on mathematics, cellular automation, complexity, and scaling. While most of these models tend to be a simplification of reality, today within the paradigm shifts of artificial intelligence across the different fields of science, the applications of computer vision show promising potential in understanding the realistic dynamics of cities. While cities are complex by nature, computer vision shows progress in tackling a variety of complex physical and non-physical visual tasks. In this article, we review the tasks and algorithms of computer vision and their applications in understanding cities. We attempt to subdivide computer vision algorithms into tasks, and cities into layers to show evidence of where computer vision is intensively applied and where further research is needed. We focus on highlighting the potential role of computer vision in understanding urban systems related to the built environment, natural environment, human interaction, transportation, and infrastructure. After showing the diversity of computer vision algorithms and applications, the challenges that remain in understanding the integration between these different layers of cities and their interactions with one another relying on deep learning and computer vision. We also show recommendations for practice and policy-making towards reaching AI-generated urban policies.
               ",autonomous vehicle
10.1016/j.technovation.2021.102375,journal,Technovation,sciencedirect,2021-08-17,sciencedirect,Prerequisites for the adoption of AI technologies in manufacturing – Evidence from a worldwide sample of manufacturing companies,https://api.elsevier.com/content/article/pii/S0166497221001565,"
                  In current discussions, Artificial Intelligence (AI) is ascribed great influence on production processes. Research on AI has seen tremendous growth in recent years. However, most of the research has focused primarily on various AI technologies, and less on prerequisites and enablers for adoption of AI at firm-level. This is surprising, given the fact that many companies are still struggling to establish AI in their production and to drive their AI adoption forward.
                  To close this gap, this study analyses the impact of various technological, organizational and environmental (TOE) prerequisites for a successful adoption of AI technologies in manufacturing. Based on a cross-national survey of 655 company representatives from the manufacturing industry, our results contribute to a better understanding of why some companies are more determined than others when it comes to implementing AI in their production. We find evidence that organizational factors, such as digital skills, company size, and R&D intensity, have the greatest impact on the adoption of AI in manufacturing. Furthermore, in order to gain new insights into the interplay of new technology adoption and global production strategies, this paper addresses the question of which factors explain a primarily domestic or globally oriented technology adoption. We find that especially research-intensive, knowledge-based and service-oriented companies tend to roll out AI technologies not only at their domestic but also at their foreign production sites.
               ",autonomous vehicle
10.1016/j.clsr.2021.105532,journal,Computer Law & Security Review,sciencedirect,2021-07-31,sciencedirect,AI research and data protection: Can the same rules apply for commercial and academic research under the GDPR?,https://api.elsevier.com/content/article/pii/S0267364921000054,"
                  The paper examines how the EU General Data Protection Regulation (GDPR) is applied to the development of AI products and services, drawing attention to the differences between academic and commercial research. The GDPR aims to encourage innovation by providing several exemptions from its strict rules for scientific research. Still, the GDPR defines scientific research in a broad manner, which includes academic and commercial research. However, corporations conducting commercial research might not have in place a similar level of ethical and institutional safeguards as academic researchers. Furthermore, corporate secrecy and opaque algorithms in AI research might pose barriers to oversight. The aim of this paper is to stress the limits of the GDPR research exemption and to find the proper balance between privacy and innovation. The paper argues that commercial AI research should not benefit from the GDPR research exemption unless there is a public interest and has similar safeguards to academic research, such as review by research ethics committees. Since the GDPR provides this broad exemption, it is crucial to clarify the limits and requirements of scientific research, before the application of AI drastically transforms this field.
               ",autonomous vehicle
10.1016/j.enbuild.2021.111343,journal,Energy and Buildings,sciencedirect,2021-11-15,sciencedirect,"A review on physical and data-driven modeling of buildings hygrothermal behavior: Models, approaches and simulation tools",https://api.elsevier.com/content/article/pii/S0378778821006277,"
                  The hygrothermal simulation of hygroscopic building materials is a real challenge, in terms of regulations and labelling, but also in decision-making. Today, we lack reference models for the hygrothermal behavior of a whole building. A scoping literature review is conducted to provide an overview of current state-of-the-art methods in order to address of these simulation methods. The most comprehensive studies are selected and examined in detail. These include physical models (White-box), which focus on solving equations that simulate the hygrothermal behavior of buildings, and data driven models, which involve implementing a prediction model using machine learning techniques (Black-box). On one hand, the white-box models are reviewed according to a two-category classification: CFD and Nodal approaches. On the other hand, the principal model used for black-box models is neural network models. The study highlights the need for a recognized method for hygrothermal simulations of hygroscopic envelopes. It provides a better understanding of the hygrothermal simulation, which helps to choose the most suitable tool or model. In addition, this review points out that there is limited application of data-driven methods to simulate the hygrothermal behavior of hygroscopic envelopes. This analysis study highlights future research gaps to overcome in order to stimulate data-driven building performance design.
               ",autonomous vehicle
10.1016/j.engappai.2017.01.013,journal,Engineering Applications of Artificial Intelligence,sciencedirect,2017-04-30,sciencedirect,Metaheuristic design of feedforward neural networks: A review of two decades of research,https://api.elsevier.com/content/article/pii/S0952197617300234,"
                  Over the past two decades, the feedforward neural network (FNN) optimization has been a key interest among the researchers and practitioners of multiple disciplines. The FNN optimization is often viewed from the various perspectives: the optimization of weights, network architecture, activation nodes, learning parameters, learning environment, etc. Researchers adopted such different viewpoints mainly to improve the FNN's generalization ability. The gradient-descent algorithm such as backpropagation has been widely applied to optimize the FNNs. Its success is evident from the FNN's application to numerous real-world problems. However, due to the limitations of the gradient-based optimization methods, the metaheuristic algorithms including the evolutionary algorithms, swarm intelligence, etc., are still being widely explored by the researchers aiming to obtain generalized FNN for a given problem. This article attempts to summarize a broad spectrum of FNN optimization methodologies including conventional and metaheuristic approaches. This article also tries to connect various research directions emerged out of the FNN optimization practices, such as evolving neural network (NN), cooperative coevolution NN, complex-valued NN, deep learning, extreme learning machine, quantum NN, etc. Additionally, it provides interesting research challenges for future research to cope-up with the present information processing era.
               ",autonomous vehicle
10.1016/j.mspro.2014.07.208,journal,Procedia Materials Science,sciencedirect,2014-12-31,sciencedirect,"Estimation of Circularity, Cylindricity and Surface Roughness in Drilling Al-Si<ce:inf loc=post>3</ce:inf>N<ce:inf loc=post>4</ce:inf> Metal Matrix Composites Using Artificial Neural Network",https://api.elsevier.com/content/article/pii/S2211812814005732,"The experiment work consists of drilling an aluminium silicon nitride composite forged plate with 6% and 10% Si3N4 reinforcement material using high-speed steel drill bit. The experiments were carried by varying cutting speed and feed rate. The experiment work involved measuring, machining time, circularity, cylindricity and surface roughness for each hole at different cutting conditions and to estimate the output parameters by sophisticated method of signal analysis like Artificial Neural Network (ANN). Machining was stopped at regular intervals of time to measure tool flank wear. The influence of network architecture is used to know the drilled hole status with different training sets viz. 30%, 50% and 70%. The optimum value was obtained at 70% training set. Result shows, ANN is reliable method for estimating.",autonomous vehicle
10.1016/j.energy.2017.10.121,journal,Energy,sciencedirect,2018-01-15,sciencedirect,Intelligent parameter optimization of Savonius rotor using Artificial Neural Network and Genetic Algorithm,https://api.elsevier.com/content/article/pii/S0360544217318273,"
                  Power coefficient, the most significant criterion for evaluating the performance of Savonius rotor is a multi-dimensional function of numerous parameters like overlap ratio, number of stages, blade rotation, etc. All these parameters have been examined separately and an approximate span in which optimum performance can be attained is proposed for each one. Furthermore, neither any attempt on scrutinizing this range accurately nor any investigations on probing the probability of existence of any interacting relation among these parameters have been reported so far. Using computational intelligence, an accurate study toward this span and a probable relation among these parameters has been conducted. Power coefficient is considered as a function of six independent input parameters, according to experimental data extracted from a related paper. An Artificial Neural Network has been assigned to investigate a logical interaction among dependent and independent variables and define a cost function based on same empirical data. This function is then optimized by Genetic Algorithm and best amount for each parameter has been determined. Suggested geometry and flow field conditions have then been simulated by Computational Fluid Dynamics and acceptable agreement is detected.
               ",autonomous vehicle
10.1016/j.energy.2017.10.121,journal,Energy,sciencedirect,2018-01-15,sciencedirect,Intelligent parameter optimization of Savonius rotor using Artificial Neural Network and Genetic Algorithm,https://api.elsevier.com/content/article/pii/S0360544217318273,"
                  Power coefficient, the most significant criterion for evaluating the performance of Savonius rotor is a multi-dimensional function of numerous parameters like overlap ratio, number of stages, blade rotation, etc. All these parameters have been examined separately and an approximate span in which optimum performance can be attained is proposed for each one. Furthermore, neither any attempt on scrutinizing this range accurately nor any investigations on probing the probability of existence of any interacting relation among these parameters have been reported so far. Using computational intelligence, an accurate study toward this span and a probable relation among these parameters has been conducted. Power coefficient is considered as a function of six independent input parameters, according to experimental data extracted from a related paper. An Artificial Neural Network has been assigned to investigate a logical interaction among dependent and independent variables and define a cost function based on same empirical data. This function is then optimized by Genetic Algorithm and best amount for each parameter has been determined. Suggested geometry and flow field conditions have then been simulated by Computational Fluid Dynamics and acceptable agreement is detected.
               ",autonomous vehicle
10.1016/B978-075067952-7/50013-1,journal,Cognitive Radio Technology,sciencedirect,2006-12-31,sciencedirect,Chapter 12: Cognitive Research: Knowledge Representation and Learning,https://api.elsevier.com/content/article/pii/B9780750679527500131,"
               This chapter provides a brief overview of knowledge representation and reasoning paradigms. Understanding the different representation mechanisms is an important prerequisite to discussing learning algorithms because the algorithm may be inextricably tied to the underlying representation and reasoning mechanism. The chapter presents several approaches to machine learning and their application within a cognitive radio system. It also presents some of the issues and trade-offs related to the implementation aspects of the learning systems. The chapter provides a representative cross section of machine learning with an emphasis on application within the radio communications domain. There is no single approach to knowledge representation and reasoning or learning that will address all aspects of cognitive radio systems. The level of intelligence exhibited by a cognitive radio system will be dynamic and varied, depending on the implementation platform, the computational resources available, and the specific mission of the system. The type of knowledge representation and reasoning system employed as well as the type of knowledge may “prefer” a particular learning method.
            ",autonomous vehicle
10.1016/S0893-6080(05)80160-1,journal,Neural Networks,sciencedirect,1994-12-31,sciencedirect,Mobile robot visual mapping and localization: A view-based neurocomputational architecture that emulates hippocampal place learning,https://api.elsevier.com/content/article/pii/S0893608005801601,"
                  We propose a real-time, view-based neurocomputational architecture for unsupervised 2-D mapping and localization within a 3-D environment defined by a spatially distributed set of visual landmarks. This architecture emulates place learning by hippocampal place cells in rats, and draws from anatomy of the primate object (“What”) and spatial (“Where”) processing streams. It extends by analogy, principles for learning characteristic views of 3-D objects (i.e., “aspects”), to learning characteristic views of environments (i.e., “places”). Places are defined by the identities and approximate poses (the What) of landmarks, as provided by visible landmark aspects. They are also defined by prototypical locations (the Where) within the landmark constellation, as indicated by the panoramic spatial distribution of landmark gaze directions. Combining these object and spatial definitions results in place nodes whose activity profiles define decision boundaries that parcel a 2-D area of the environment into place regions. These profiles resemble the spatial firing patterns over hippocampal place fields observed in rat experiments. A realtime demonstration of these capabilities on the binocular mobile robot MAVIN (the mobile adaptive visual navigator) illustrates the potential of this approach for qualitative mapping and fine localization.
               ",autonomous vehicle
10.1016/j.neunet.2019.08.010,journal,Neural Networks,sciencedirect,2019-12-31,sciencedirect,Adaptive Resonance Theory in the time scales calculus,https://api.elsevier.com/content/article/pii/S0893608019302278,"
                  Engineering applications of algorithms based on Adaptive Resonance Theory have proven to be fast, reliable, and scalable solutions to modern industrial machine learning problems. A key emerging area of research is in the combination of different kinds of inputs within a single learning architecture along with ensuring the systems have the capacity for lifelong learning. We establish a dynamic equation model of ART in the time scales calculus capable of handling inputs in such mixed domains. We prove theorems establishing that the orienting subsystem can affect learning in the long-term memory storage unit as well as that those remembered exemplars result in stable categories. Further, we contribute to the mathematics of time scales literature itself with novel takes on logic functions in the calculus as well as new representations for the action of weight matrices in generalized domains. Our work extends the core ART theory and algorithms to these important mixed input domains and provides the theoretical foundation for further extensions of ART-based learning strategies for applied engineering work.
               ",autonomous vehicle
10.1016/j.robot.2005.09.015,journal,Robotics and Autonomous Systems,sciencedirect,2006-01-31,sciencedirect,Navigation with memory in a partially observable environment,https://api.elsevier.com/content/article/pii/S0921889005001442,"
                  The paper presents an architecture that allows the reactive visual navigation via an unsupervised reinforcement learning. This objective is reached using 
                        Q
                     -learning and a hierarchical approach to the developed architecture. Using these techniques requires a deviation from the Partially Observable Markov Decision Processes (POMDP) and some innovations: heuristic techniques for generalizing the experience and for treating the partial observability; a technique for the speed adjournment of the 
                        Q
                      function; the definition of a special reinforcement policy adequate for learning a complex task without supervision. The result is a satisfactory learning of the navigation assignment in a simulated environment.
               ",autonomous vehicle
10.1016/j.scs.2021.102804,journal,Sustainable Cities and Society,sciencedirect,2021-06-30,sciencedirect,Does historical data still count? Exploring the applicability of smart building applications in the post-pandemic period,https://api.elsevier.com/content/article/pii/S2210670721000949,"
                  The emergence of COVID-19 pandemic is causing tremendous impact on our daily lives, including the way people interact with buildings. Leveraging the advances in machine learning and other supporting digital technologies, recent attempts have been sought to establish exciting smart building applications that facilitates better facility management and higher energy efficiency. However, relying on the historical data collected prior to the pandemic, the resulting smart building applications are not necessarily effective under the current ever-changing situation due to the drifts of data distribution. This paper investigates the bidirectional interaction between human and buildings that leads to dramatic change of building performance data distributions post-pandemic, and evaluates the applicability of typical facility management and energy management applications against these changes. According to the evaluation, this paper recommends three mitigation measures to rescue the applications and embedded machine learning algorithms from the data inconsistency issue in the post-pandemic era. Among these measures, incorporating occupancy and behavioural parameters as independent variables in machine learning algorithms is highlighted. Taking a Bayesian perspective, the value of data is exploited, historical or recent, pre- and post-pandemic, under a people-focused view.
               ",autonomous vehicle
10.1016/j.ijrmms.2016.05.003,journal,International Journal of Rock Mechanics and Mining Sciences,sciencedirect,2016-07-31,sciencedirect,Modeling the load-displacement curve for fully-grouted cable bolts using Artificial Neural Networks,https://api.elsevier.com/content/article/pii/S1365160916300806,,autonomous vehicle
10.1016/0743-7315(89)90060-9,journal,Journal of Parallel and Distributed Computing,sciencedirect,1989-04-30,sciencedirect,Relaxation and neural learning: Points of convergence and divergence,https://api.elsevier.com/content/article/pii/0743731589900609,"
                  The fields of relaxation labeling and adaptive neural networks are surveyed using a common formalism that permits highlighting their coincidences and divergences. Two alternative views are proposed: one in which short-term neural net functioning is interpreted as relaxation and thus neural learning amounts to discovering the constraints between the states of connected neurons, and another in which neural learning itself is interpreted as an extended form of relaxation. Some transfer of results and suggestions from one field to the other are pointed out.
               ",autonomous vehicle
10.1016/B978-0-12-812939-5.00005-7,journal,Biomechatronics,sciencedirect,2019-12-31,sciencedirect,5: Control and Physical Intelligence,https://api.elsevier.com/content/article/pii/B9780128129395000057,"
               Control and physical intelligence are reviewed in the context of engineered and biological systems. First, the general control problem is revised. Then, the popular topics of engineered systems are addressed including proportional-integral-derivative control approach, time delays, various sources of errors and uncertainties, stability concept, feedback linearization, sliding control, adaptive control, Bellman's curse of dimensionality and various attempts to resolve it, artificial intelligence, machine learning, data mining, etc. The rest of chapter is focused on control aspects of biological system, that is, central and peripheral nervous system and its constituents.
            ",autonomous vehicle
10.1016/j.matpr.2021.02.166,journal,Materials Today: Proceedings,sciencedirect,2021-02-16,sciencedirect,Evaluation of performance of an LR and SVR models to predict COVID-19 pandemic,https://api.elsevier.com/content/article/pii/S2214785321012487,"
                  Recently, in December 2019 the Coronavirus disease surprisingly influenced the lives of millions of people in the world with its swift spread. To support medical experts/doctors with the overpowering challenge of prediction of total cases in India, a machine-learning algorithm was developed. In this research article, the author describes the possibility of predicting the COVID-19 total, active cases, death and cured cases in India up to 25th June 2020 by applying linear regression and support vector machine. It is extremely tricky to manage the occurrence of corona virus since it is expanding exponentially day to day and is difficult to handle with a limited number of doctors and beds to treat the infected individuals with limited time. Hence, it is essential to develop a machine learning based computerized predicting model. The development effort in this article is based on publicly available data that is downloaded from KAGGLE to estimate the spread of the disease within a short period. We have calculated the RMSE, R2, MAE of LR and SVR models and concluded that the RMSE of linear regression is less than the SVR. Therefore, the LR will help doctors to forecast for the next few days.
               ",autonomous vehicle
10.1016/j.eswa.2019.06.018,journal,Expert Systems with Applications,sciencedirect,2019-11-30,sciencedirect,A Recursive General Regression Neural Network (R-GRNN) Oracle for classification problems,https://api.elsevier.com/content/article/pii/S095741741930418X,"
                  This research introduces the Recursive General Regression Neural Network Oracle (R-GRNN Oracle) and is demonstrated on several binary classification datasets. The traditional GRNN Oracle classifier (Masters et al., 1998) combines the predictive powers of several machine learning classifiers by weighing the amount of error each classifier has on the final predictions. Each classifier is assigned a weight based on the percentage of errors it contributes to the final predictions as the classifiers evaluate the dataset. The proposed R-GRNN Oracle is an enhancement to the GRNN Oracle in which the proposed algorithm consists of an oracle within an oracle – where the inner oracle acts as a classifier with its own predictions and error contribution. By combining the inner oracle with other classifiers, the R-GRNN Oracle produces superior results. The classifiers considered in this study are: Support Vector Machine (SVM), Multilayer Perceptron (MLP), Probabilistic Neural Network (PNN), Gaussian Naïve Bayes (GNB), K-Nearest Neighbor (KNN), and Random Forest (RF). To demonstrate the effectiveness of the proposed approach, several datasets were used, with the primary one being the publicly available Spambase dataset. The predictions of SVM, MLP, KNN, and RF were used to create the first GRNN Oracle, which was then enhanced with the high performances of SVM and RF to create the second oracle, the R-GRNN Oracle. The combined recursive model was 93.24% accurate using 10-fold cross validation, higher than the 91.94% of the inner GRNN Oracle and the 91.29% achieved by RF, the highest performance by a stand-alone classifier. The R-GRNN Oracle was not only the most accurate, but it also had the highest AUC, sensitivity, specificity, precision, and F1-score (97.99%, 91.86%, 94.40%, 93.28%, and 92.57%, respectively). The research contribution of this paper is introducing the concept of recursion (a concept not fully explored in machine learning models and applications) and testing this structure's ability on further enhancing the performance of the traditional oracle. The recursive model has also been applied to several other datasets: The Human Resources, Bank Marketing, and Monoclonal Gammopathy of Undetermined Significance (MGUS) datasets. The results of these implementations are summarized in this paper.
               ",autonomous vehicle
10.1016/j.comnet.2021.107930,journal,Computer Networks,sciencedirect,2021-05-08,sciencedirect,6G networks: Beyond Shannon towards semantic and goal-oriented communications,https://api.elsevier.com/content/article/pii/S1389128621000773,"
                  The goal of this paper is to promote the idea that including semantic and goal-oriented aspects in future 6G networks can produce a significant leap forward in terms of system effectiveness and sustainability. Semantic communication goes beyond the common Shannon paradigm of guaranteeing the correct reception of each single transmitted bit, irrespective of the meaning conveyed by the transmitted bits. The idea is that, whenever communication occurs to convey meaning or to accomplish a goal, what really matters is the impact that the received bits have on the interpretation of the meaning intended by the transmitter or on the accomplishment of a common goal. Focusing on semantic and goal-oriented aspects, and possibly combining them, helps to identify the relevant information, i.e. the information strictly necessary to recover the meaning intended by the transmitter or to accomplish a goal. Combining knowledge representation and reasoning tools with machine learning algorithms paves the way to build semantic learning strategies enabling current machine learning algorithms to achieve better interpretation capabilities and contrast adversarial attacks. 6G semantic networks can bring semantic learning mechanisms at the edge of the network and, at the same time, semantic learning can help 6G networks to improve their efficiency and sustainability.
               ",autonomous vehicle
10.1016/j.comnet.2021.108322,journal,Computer Networks,sciencedirect,2021-10-09,sciencedirect,A comprehensive survey on DNS tunnel detection,https://api.elsevier.com/content/article/pii/S1389128621003248,"
                  Domain Name System (DNS) tunnels, established between the controlled host and master server disguised as the authoritative domain name server, can be used as a secret data communication channel for malicious activities. Owing to the ready evasion of the DNS traffic to bypass the network security mechanism, DNS tunnelling can cause severe damage. Thus, an in-depth and comprehensive understanding of the various detection technologies is of considerable importance when facing this type of threat. However, most of the existing reviews focus on a single aspect of the DNS tunnel detection technologies, such as methods based on machine learning, traffic, and payload analysis. In addition, few studies have conducted comprehensive investigation that includes a sequentially integrated range of literature in this researchfield, or have analysed the latest literature on DNS tunnels. This paper reviews these detection technologies from a novel perspective of rule-based and model-based methods with descriptions and analyses of the DNS-based tools and their corresponding features. To the best of our knowledge, this is the first study to comprehensively discuss and analyse DNS tunnel detection in a novel and specific classification fashion from various aspects in detail, covering almost all the detection methods developed from 2006 to 2020. Furthermore, a comparative analysis of detection methods and several suggestions for future research directions are presented.
               ",autonomous vehicle
10.1016/S0957-4174(96)00076-0,journal,Expert Systems with Applications,sciencedirect,1997-12-31,sciencedirect,Learning symbolic descriptions of shape for object recognition in X-ray images,https://api.elsevier.com/content/article/pii/S0957417496000760,"
                  In this paper we describe a method for learning shape descriptions of objects in X-ray images. The descriptions are induced from shape examples using the AQ15c inductive learning system. The method has been experimentally compared to k-nearest neighbor, a statistical pattern recognition technique, the C4.5 decision tree learning program, and a multilayer feed-forward neural network. Experimental results demonstrate strong advantages of the AQ methodology over the other methods. Specifically, the method has higher predictive accuracy and faster learning and recognition rates. AQ's representation language, VL1, was better suited for this problem, which can be seen by examining the empirical results and the learned rules. The method was applied to the problem of detecting blasting caps in X-ray images of luggage. An intelligent system performing this detection task can be used to assist airport security personnel with luggage screening.
               ",autonomous vehicle
10.1016/j.optlastec.2021.107404,journal,Optics & Laser Technology,sciencedirect,2021-12-31,sciencedirect,Addressing the challenges in remanufacturing by laser-based material deposition techniques,https://api.elsevier.com/content/article/pii/S0030399221004928,"
                  Increased focus on reduction of impact on the environment has put the aspect of remanufacturing in the spotlight, and remanufacturing of high-value engineering components is gradually becoming a mainstream practice. Out of different alternatives, laser-based deposition has been the central choice for remanufacturing, thanks to its accuracy, and precision. However, considering the complex process physics involved in laser-based remanufacturing processes, it is essential to establish the reliability of the process so that certifiable remanufactured parts can be produced. This work provides a comprehensive analysis of the issues encountered during laser-based remanufacturing, and the different approaches to address them. Apart from covering the state-of-the-art of remanufacturing by laser-based deposition, this article also discusses tools like deep learning, and digital twin which are still in their early phases in terms of applications in the remanufacturing domain.
               ",autonomous vehicle
10.1016/j.drudis.2021.02.007,journal,Drug Discovery Today,sciencedirect,2021-06-30,sciencedirect,Driving success in personalized medicine through AI-enabled computational modeling,https://api.elsevier.com/content/article/pii/S1359644621000738,"
                  The development of successful drugs is expensive and time-consuming because of high clinical attrition rates. This is caused partially by the rupture seen in the translatability of the drug from the bench to the clinic in the context of personalized medicine. Artificial intelligence (AI)-driven platforms integrated with mechanistic modeling have become instrumental in accelerating the drug development process by leveraging data ubiquitously across the various phases. AI can counter the deficiencies and ambiguities that arise during the classical drug development process while reducing human intervention and bridging the translational gap in discovering the connections between drugs and diseases.
               ",autonomous vehicle
10.1016/S1574-1400(08)00003-0,journal,Annual Reports in Computational Chemistry,sciencedirect,2008-12-31,sciencedirect,Chapter 3: Machine Learning for Protein Structure and Function Prediction,https://api.elsevier.com/content/article/pii/S1574140008000030,"
                  Solving biological problems in silico has posed many daunting challenges ranging from simulating the protein dynamics to function assignment for new sequences. One approach to such problems, machine learning, is to learn the solution from a set of examples. Machine learning has found wide application in many problem domains such as medical diagnosis, market analysis, traffic analysis among others. The key idea in machine learning is to direct the computer to learn how to solve a problem rather than explicitly give the solution to the computer. This chapter reviews a number of popular machine learning formulations and gives an example for each formulation using the modeling of protein–DNA interactions. Then we present recent applications of machine learning to a number of biological problems related to protein structure and function prediction. These include protein-RNA interactions, protein-membrane interactions, protein-protein interactions, protein-peptide interactions, single amino acid polymorphisms, subcellular localization, and protein structure predictions.
               ",autonomous vehicle
10.1016/j.optlastec.2017.10.011,journal,Optics & Laser Technology,sciencedirect,2018-03-01,sciencedirect,[INVITED] Computational intelligence for smart laser materials processing,https://api.elsevier.com/content/article/pii/S0030399217303286,"
                  Computational intelligence (CI) involves using a computer algorithm to capture hidden knowledge from data and to use them for training “intelligent machine” to make complex decisions without human intervention. As simulation is becoming more prevalent from design and planning to manufacturing and operations, laser material processing can also benefit from computer generating knowledge through soft computing.
                  This work is a review of the state-of-the-art on the methodology and applications of CI in laser materials processing (LMP), which is nowadays receiving increasing interest from world class manufacturers and 4.0 industry. The focus is on the methods that have been proven effective and robust in solving several problems in welding, cutting, drilling, surface treating and additive manufacturing using the laser beam.
                  After a basic description of the most common computational intelligences employed in manufacturing, four sections, namely, laser joining, machining, surface, and additive covered the most recent applications in the already extensive literature regarding the CI in LMP. Eventually, emerging trends and future challenges were identified and discussed.
               ",autonomous vehicle
10.1016/j.arcontrol.2019.07.003,journal,Annual Reviews in Control,sciencedirect,2019-12-31,sciencedirect,A survey on artificial neural networks application for identification and control in environmental engineering: Biological and chemical systems with uncertain models,https://api.elsevier.com/content/article/pii/S1367578819300094,"
                  Artificial neural networks (ANNs) are considered efficient tools for modeling complex, non-linear processes with uncertain dynamic models. ANNs were originally applied as effective predictors of diverse processes with static dependence on the input-output information. However, when the ANN must be applied to characterize an approximate model of time-dependent input-output relationships, then it is necessary to introduce the time effect as part of the ANN, yielding to the construction of dynamic ANN or DNN. This review establishes the variants of recurrent and differential forms of DNN, their mathematically formulation as well as the methods to adjust the network weights. The characteristics of DNNs motivate their use to represent the dynamics of decontamination processes. This review details recent findings on the DNN application for the modeling and control of treatment systems based on either biological and chemical processes. The modeling application of DNN for some common methods used in the treatment of wastewater, contaminated soil and atmosphere is described. The major benefits of using the approximate DNN-based model instead of designing the complex mathematical description for each treatment are analyzed in the context of enhancing the efficiency of the decontamination treatment. This review also highlights the remarkable efficiency of DNNs as a keystone tool for modeling and control sequence of treatments. The last section in the review introduces several open researching areas for the application of DNN for decontamination systems based on biochemical and chemical treatments.
               ",autonomous vehicle
10.1016/j.paerosci.2020.100617,journal,Progress in Aerospace Sciences,sciencedirect,2020-05-31,sciencedirect,Advances in intelligent and autonomous navigation systems for small UAS,https://api.elsevier.com/content/article/pii/S0376042120300294,"
                  A significant growth in Unmanned Aircraft System (UAS) operations has been observed over the past decade, largely driven by the emergence of new commercial opportunities and use-cases. This has posed new technological and regulatory challenges in order to address the complex safety, efficiency and sustainability requirements associated with UAS operations in an increasingly congested airspace. The growing need for trusted autonomy in UAS operations imposes demanding performance requirements on Navigation and Guidance Systems (NGS), both in terms of accuracy, integrity, continuity and availability. In most current NGS implementations, system autonomy is tightly constrained within a specified set of operational and environmental conditions through a large number of explicit rules. Recent breakthroughs in Artificial Intelligence (AI)-based methods and the emergence of highly-parallelized processor boards with low form-factor has led to the opportunity to employ Machine Learning (ML) techniques to enhance navigation system performance, particularly for small UAS (sUAS), which account for the majority of current and future unmanned aircraft use-cases. sUAS navigation systems typically employ diverse low Size, Weight, Power and Cost (SWaP-C) sensors such as Global Navigation Satellite System (GNSS) receivers, MEMS-IMUs, magnetometers, cameras and Lidars for localization, obstacle detection and avoidance. This paper presents a comprehensive review of conventional sUAS navigation systems, including aspects such as system architecture, sensing modalities and data-fusion algorithms. Additionally, performance monitoring and augmentation strategies are critically reviewed and assessed against current and future UAS Traffic Management (UTM) requirements. The primary focus is on the identification of key gaps in the literature where the use of AI-based methods can potentially enhance navigation performance. A critical review of AI-based methods and their application to sUAS navigation is conducted, along with an assessment of the performance benefits they provide over conventional navigation systems. Reviewed methods include but are not restricted to Artificial Neural Networks (ANN) such as Convolutional and Recurrent Neural Networks (CNN and RNN), Support Vector Machines (SVM) and ensemble techniques. The key challenges associated with adapting these methods to address sUAS operational objectives are clearly identified. The review also covers the assurance of predictable, deterministic system behaviour which is a key requirement to support system certification. The review and analysis will inform the reader of the applicability of various AI/ML methods to sUAS navigation and autonomous system integrity monitoring, and its likely role in the ongoing UTM evolution.
               ",autonomous vehicle
10.1016/0004-3702(94)90047-7,journal,Artificial Intelligence,sciencedirect,1994-12-31,sciencedirect,Robot shaping: developing autonomous agents through learning,https://api.elsevier.com/content/article/pii/0004370294900477,"
                  Learning plays a vital role in the development of autonomous agents. In this paper, we explore the use of reinforcement learning to “shape” a robot to perform a predefined target behavior. We connect both simulated and real robots to Alecsys, a parallel implementation of a learning classifier system with an extended genetic algorithm. After classifying different kinds of Animat-like behaviors, we explore the effects on learning of different types of agent's architecture and training strategies. We show that the best results are achieved when both the agent's architecture and the training strategy match the structure of the behavior pattern to be learned. We report the results of a number of experiments carried out both in simulated and in real environments, and show that the results of simulations carry smoothly to physical robots. While most of our experiments deal with simple reactive behavior, in one of them we demonstrate the use of a simple and general memory mechanism. As a whole, our experimental activity demonstrates that classifier systems with genetic algorithms can be practically employed to develop autonomous agents.
               ",autonomous vehicle
10.1016/j.rser.2021.111521,journal,Renewable and Sustainable Energy Reviews,sciencedirect,2021-11-30,sciencedirect,Driving conditions-driven energy management strategies for hybrid electric vehicles: A review,https://api.elsevier.com/content/article/pii/S1364032121007991,"
                  Motivated by the concerns on transported fuel consumption and global air pollution, industrial engineers and academic researchers have made many efforts to construct more efficient and environment-friendly vehicles. Hybrid electric vehicles (HEVs) are the representative ones because they can satisfy the power demand by coordinating energy supplements among different energy storage devices. To achieve this goal, energy management approaches are crucial technology, and driving cycles are the critical influence factor. Therefore, this paper aims to summarize driving cycle-driven energy management strategies (EMSs) for HEVs. First, the definition and significance of driving cycles in the energy management field are clarified, and the recent literature in this research domain is reviewed and revisited. In addition, according to the known information of driving cycles, the EMSs are divided into three categories, and the relevant study directions, such as standard driving cycles, long-term driving cycle generation, and short-term driving cycle prediction are illuminated and analyzed. Furthermore, the existing database of driving cycles in highway and urban aspects is displayed and discussed. Finally, this article also elaborates on the future prospects of energy management technologies related to driving cycles. This paper focusing on helping the relevant researchers realize the state-of-the-art of HEVs’ energy management field and also recognize its future development direction.
               ",autonomous vehicle
10.1016/j.neunet.2019.08.033,journal,Neural Networks,sciencedirect,2020-01-31,sciencedirect,"Distributed dual vigilance fuzzy adaptive resonance theory learns online, retrieves arbitrarily-shaped clusters, and mitigates order dependence",https://api.elsevier.com/content/article/pii/S0893608019302606,"
                  This paper presents a novel adaptive resonance theory (ART)-based modular architecture for unsupervised learning, namely the distributed dual vigilance fuzzy ART (DDVFA). DDVFA consists of a global ART system whose nodes are local fuzzy ART modules. It is equipped with distributed higher-order activation and match functions and a dual vigilance mechanism. Together, these allow DDVFA to perform unsupervised modularization, create multi-prototype cluster representations, retrieve arbitrarily-shaped clusters, and reduce category proliferation. Another important contribution is the reduction of order-dependence, an issue that affects any agglomerative clustering method. This paper demonstrates two approaches for mitigating order-dependence: pre-processing using visual assessment of cluster tendency (VAT) or post-processing using a novel Merge ART module. The former is suitable for batch processing, whereas the latter also works for online learning. Experimental results in online mode carried out on 30 benchmark data sets show that DDVFA cascaded with Merge ART statistically outperformed the best other ART-based systems when samples were randomly presented. Conversely, they were found to be statistically equivalent in offline mode when samples were pre-processed using VAT. Remarkably, performance comparisons to non-ART-based clustering algorithms show that DDVFA (which learns incrementally) was also statistically equivalent to the non-incremental (offline) methods of density-based spatial clustering of applications with noise (DBSCAN), single linkage hierarchical agglomerative clustering (SL-HAC), and k-means, while retaining the appealing properties of ART. Links to the source code and data are provided. Considering the algorithm’s simplicity, online learning capability, and performance, it is an ideal choice for many agglomerative clustering applications.
               ",autonomous vehicle
10.1016/j.martra.2020.100003,journal,Maritime Transport Research,sciencedirect,2020-12-31,sciencedirect,Improving ship yard ballast pumps’ operations: A PCA approach to predictive maintenance,https://api.elsevier.com/content/article/pii/S2666822X20300034,"This paper investigates a predictive maintenance approach for marine mechanical systems via an early warning system. A machine learning methodology was used to process and analyze the dock pump back pressure, flow rate, amperage and suction pressure data. Operating parameters for a dock pump were monitored for 40 weeks and the values were manually input into the tool. Unsupervised machine learning was used in order to draw inferences from data via MATLAB. A principal component analysis (PCA) algorithm was used to improve on the selection of the key operating parameters of the dock pumps. The dock pump flow rate and suction pressure, were the principal components that were 99.707% sufficient to explain the variation in the data. Using the dataset explained by the PCA, two data classes were later used in the SVM algorithm for a binary classification approach. The developed tool predicted that the dock pump may fail/requires maintenance between seventh and eighth weeks. This prediction deviated from the actual ten weeks that it took the dock pump to fail. A prediction deviation from the actual failure time to failure could be attributed to the quality of the historical failure and maintenance data. Nevertheless, with less ambiguity of the data, the maintenance prediction tool can be used as a basis before sensor technology on the dock pumps is implemented.",autonomous vehicle
10.1016/j.jallcom.2008.03.035,journal,Journal of Alloys and Compounds,sciencedirect,2009-02-20,sciencedirect,Prediction of tribological behavior of aluminum–copper based composite using artificial neural network,https://api.elsevier.com/content/article/pii/S0925838808004428,"
                  The potential of using neural network in prediction of wear loss quantities of some aluminum–copper–silicon carbide composite materials has been studied in the present work. Effects of addition of copper as alloying element and silicon carbide as reinforcement particles to Al–4wt.%Mg metal matrix have been investigated. Different Al–Cu alloys and composites were subjected to dry sliding wear test using pin-on-disk apparatus under 40N normal load with rotational speed of counter face disk of 150rpm at room conditions (∼20°C and ∼50% relative humidity). The experimental results were firstly coded prior to training in a feed forward back propagation artificial neural network (ANN) and the results were compared with experimental results. The average value of absolute relative error of un-coded values reaches 2.40%.
               ",autonomous vehicle
10.1016/B0-12-227240-4/00107-6,journal,Encyclopedia of Information Systems,sciencedirect,2003-12-31,sciencedirect,Machine Learning,https://api.elsevier.com/content/article/pii/B0122272404001076,,autonomous vehicle
10.1016/j.procs.2021.01.290,journal,Procedia Computer Science,sciencedirect,2021-12-31,sciencedirect,Taxonomy of generative adversarial networks for digital immunity of Industry 4.0 systems,https://api.elsevier.com/content/article/pii/S1877050921003392,"Industry 4.0 systems are extensively using artificial intelligence (AI) to enable smartness, automation and flexibility within variety of processes. Due to the importance of the systems, they are potential targets for attackers trying to take control over the critical processes. Attackers use various vulnerabilities of such systems including specific vulnerabilities of AI components. It is important to make sure that inappropriate adversarial content will not break the security walls and will not harm the decision logic of critical systems. We believe that the corresponding security toolset must be organized as a trainable self-protection mechanism similar to immunity. We found certain similarities between digital vs. biological immunity and we study the possibilities of Generative Adversarial Networks (GANs) to provide the basis for the digital immunity training. We suggest the taxonomy of GANs (including new architectures) suitable to simulate various aspects of the immunity for Industry 4.0 applications.",autonomous vehicle
10.1016/B978-0-12-374731-0.00007-4,journal,Artificial Intelligence for Games,sciencedirect,2009-12-31,sciencedirect,7: Learning,https://api.elsevier.com/content/article/pii/B9780123747310000074,"Learning is a hot topic in games. In principle, learning AI has the potential to adapt to each player, learning their tricks and techniques and providing a consistent challenge. It has the potential to produce more believable characters: characters that can learn about their environment and use it to the best effect. It also has the potential to reduce the effort needed to create game-specific AI: characters should be able to learn about their surroundings and the tactical options that they provide.",autonomous vehicle
10.1016/j.jmatprotec.2008.02.066,journal,Journal of Materials Processing Technology,sciencedirect,2009-01-19,sciencedirect,"Prediction of density, porosity and hardness in aluminum–copper-based composite materials using artificial neural network",https://api.elsevier.com/content/article/pii/S0924013608002082,"
                  The potential of using feed forward backpropagation neural network in prediction of some physical properties and hardness of aluminium–copper/silicon carbide composites synthesized by compocasting method has been studied in the present work. Two input vectors were used in the construction of proposed network; namely weight percentage of the copper and volume fraction of the reinforced particles. Density, porosity and hardness were the three outputs developed from the proposed network. Effects of addition of copper as alloying element and silicon carbide as reinforcement particles to Al–4wt.% Mg metal matrix have been investigated by using artificial neural networks. The maximum absolute relative error for predicted values does not exceed 5.99%. Therefore, by using ANN outputs, satisfactory results can be estimated rather than measured and hence reduce testing time and cost.
               ",autonomous vehicle
10.1016/B978-0-12-823519-5.00007-5,journal,Generative Adversarial Networks for Image-to-Image Translation,sciencedirect,2021-12-31,sciencedirect,Chapter 10: Image generation using generative adversarial networks,https://api.elsevier.com/content/article/pii/B9780128235195000075,"
               Ever heard of generation of image datasets, human faces, cartoon characters, 3D objects, image-to-image and text-to-image translation, face aging, photo blending, and others? How are the computers able to perform the tasks by achieving mastery results? Yes, the answer to all these tasks is generative adversarial networks (GANs). GANs are an amazing artificial intelligence innovation fit for making pictures, sound, and recordings that are unclear from the real thing. GANs employ self-supervised learning consisting of two neural networks. The setup includes two neural network systems in opposition to one another—one to create fakes (generator) and one to spot them (discriminator). The term generative indicates the idea of creating new data depending on the training data. The term adversarial indicates a gamelike framework with two networks, i.e., generator and discriminator. The generator produces the realistic data, which is similar to the training data, whereas the discriminator's task is to identify fake data produced by the generator from the real data coming from the training sample. The chapter is divided into three sections providing a brief introduction about generative deep learning and variational autoencoder giving the flexibility of data generation with slight variations compared to the original data followed by the workflow, training problems, and real-world applications using different flavors of GANs.
            ",autonomous vehicle
10.1016/S0893-6080(99)00046-5,journal,Neural Networks,sciencedirect,1999-11-30,sciencedirect,"What are the computations of the cerebellum, the basal ganglia and the cerebral cortex?",https://api.elsevier.com/content/article/pii/S0893608099000465,"
                  The classical notion that the cerebellum and the basal ganglia are dedicated to motor control is under dispute given increasing evidence of their involvement in non-motor functions. Is it then impossible to characterize the functions of the cerebellum, the basal ganglia and the cerebral cortex in a simplistic manner? This paper presents a novel view that their computational roles can be characterized not by asking what are the “goals” of their computation, such as motor or sensory, but by asking what are the “methods” of their computation, specifically, their learning algorithms. There is currently enough anatomical, physiological, and theoretical evidence to support the hypotheses that the cerebellum is a specialized organism for supervised learning, the basal ganglia are for reinforcement learning, and the cerebral cortex is for unsupervised learning.
                  This paper investigates how the learning modules specialized for these three kinds of learning can be assembled into goal-oriented behaving systems. In general, supervised learning modules in the cerebellum can be utilized as “internal models” of the environment. Reinforcement learning modules in the basal ganglia enable action selection by an “evaluation” of environmental states. Unsupervised learning modules in the cerebral cortex can provide statistically efficient representation of the states of the environment and the behaving system. Two basic action selection architectures are shown, namely, reactive action selection and predictive action selection. They can be implemented within the anatomical constraint of the network linking these structures. Furthermore, the use of the cerebellar supervised learning modules for state estimation, behavioral simulation, and encapsulation of learned skill is considered. Finally, the usefulness of such theoretical frameworks in interpreting brain imaging data is demonstrated in the paradigm of procedural learning.
               ",autonomous vehicle
10.1016/j.neucom.2021.05.016,journal,Neurocomputing,sciencedirect,2021-09-24,sciencedirect,SRGCN: Graph-based multi-hop reasoning on knowledge graphs,https://api.elsevier.com/content/article/pii/S0925231221007530,"
                  Learning to infer missing links is one of the fundamental tasks in the knowledge graph. Instead of reasoning based on separate paths in the existing methods, in this paper, we propose a new model, Sequential Relational Graph Convolutional Network (SRGCN), which treats the multiple paths between an entity pair as a sequence of subgraphs. Specifically, to reason the relationship between two entities, we first construct a graph for the entities based on the knowledge graph and serialize the graph to a sequence. For each hop in the sequence, Relational Graph Convolutional Network (R-GCN) is then applied to update the embeddings of the entities. The updated embedding of the tail entity contains information of the entire graph, hence the relationship between two entities can be inferred from it. Compared to the existing approaches that deal with paths separately, SRGCN treats the graph as a whole, which can encode structural information and interactions between paths better. Experiments show that SRGCN outperforms path-based baselines on both link and fact prediction tasks. We also show that SRGCN is highly efficient in the sense that only one epoch of training is enough to achieve high accuracy, and even partial datasets can lead to competitive performance.
               ",autonomous vehicle
10.1016/B978-0-12-373594-2.00007-1,journal,Building Intelligent Interactive Tutors,sciencedirect,2009-12-31,sciencedirect,Chapter 7: Machine Learning,https://api.elsevier.com/content/article/pii/B9780123735942000071,"
               Machine learning (ML) refers to a system's ability to acquire, and integrate knowledge through large-scale observations, and to improve, and extend itself by learning new knowledge rather than by being programmed with that knowledge. ML techniques are used in intelligent tutors to acquire new knowledge about students, identify their skills, and learn new teaching approaches. They improve teaching by repeatedly observing how students react and generalize rules about the domain or student. The role of ML techniques in a tutor is to independently observe and evaluate the tutor's actions. ML tutors customize their teaching by reasoning about large groups of students, and tutor-student interactions, generated through several components. A performance element is responsible for making improvements in the tutor, using perceptions of tutor/student interactions, and knowledge about the student's reaction to decide how to modify the tutor to perform better in the future. ML techniques are used to identify student learning strategies, such as, which activities do students select most frequently and in which order. Analysis of student behavior leads to greater student learning outcome by providing tutors with useful diagnostic information for generating feedback.
            ",autonomous vehicle
10.1016/j.knosys.2021.107587,journal,Knowledge-Based Systems,sciencedirect,2022-01-10,sciencedirect,Explainability in supply chain operational risk management: A systematic literature review,https://api.elsevier.com/content/article/pii/S0950705121008492,"
                  It is important to manage operational disruptions to ensure the success of supply chain operations. To achieve this aim, researchers have developed techniques that determine the occurrence of operational risk events which assists supply chain operational risk managers develop plans to manage them by detection/monitoring, mitigation/management, or optimization techniques. Various artificial intelligence (AI) approaches have been used to develop such techniques in the broad activities of operational risk management. However, all of these techniques are black box in their working nature. This means that the chosen technique cannot explain why it has given that output and whether it is correct and free from bias. To address this, researchers argue the need for supply chain management professionals to move towards using explainable AI methods for operational risk management. In this paper, we conduct a systematic literature review on the techniques used to determine operational risks and analyse whether they satisfy the requirement of them being explainable. The findings highlight the shortcomings and inspires directions for future research. From a managerial perspective, the paper encourages risk managers to choose techniques for supply chain operational risk management that can be auditable as this will ensure that the risk managers know why they should take a particular risk management action rather than just what they should do to manage the operational risks.
               ",autonomous vehicle
10.1016/j.asoc.2020.106275,journal,Applied Soft Computing,sciencedirect,2020-07-31,sciencedirect,Fuzzy neural networks and neuro-fuzzy networks: A review the main techniques and applications used in the literature,https://api.elsevier.com/content/article/pii/S1568494620302155,"
                  This paper presents a review of the central theories involved in hybrid models based on fuzzy systems and artificial neural networks, mainly focused on supervised methods for training hybrid models. The basic concepts regarding the history of hybrid models, from the first proposed model to the current advances, the composition and the functionalities in their architecture, the data treatment and the training methods of these intelligent models are presented to the reader so that the evolution of this category of intelligent systems can be evidenced. Finally, the features of the leading models and their applications are presented to the reader. We conclude that the fuzzy neural network models and their derivations are efficient in constructing a system with a high degree of accuracy and an appropriate level of interpretability working in a wide range of areas of economics and science.
               ",autonomous vehicle
10.1016/j.jclepro.2015.12.082,journal,Journal of Cleaner Production,sciencedirect,2016-03-10,sciencedirect,The optimized artificial neural network model with Levenberg–Marquardt algorithm for global solar radiation estimation in Eastern Mediterranean Region of Turkey,https://api.elsevier.com/content/article/pii/S0959652615019071,"
                  An accurate knowledge on global solar radiation is particularly required for proper placement and design of solar energy conversion systems. While the meteorological data are measured at most of the weather stations, global solar radiation measurement is not always performed due to high cost of the measurement devices and their operation and maintenance requirements. Therefore, several linear, non-linear and soft computing models are developed to estimate the solar radiation owing to being more economical when compared to installing pyranometers and these models provide satisfactory results. However, it is crucial to choose the most appropriate model for a specific purpose and region. The primary objective of this study is to optimize the performance of the artificial neural network model in order to realize an efficient estimation of solar radiation for Eastern Mediterranean Region of Turkey. Estimation performances are discussed for different structures of neural network by taking into account the number and quality of input features, learning algorithms, number of hidden neurons, correlation between network outputs and targets, and statistical error analysis methods. The presented model indicates that the artificial neural network models illustrate promising in the estimation of monthly mean daily global solar radiation by using commonly available data. In order to indicate the superiority of the performance of the model, it is evaluated with various test years, which are not used for training stage of the model. The presented model provides superior relationship between the estimated and measured values. The test results showed that the coefficient of determination and mean absolute percentage error between the optimized artificial neural network estimations and measured values for testing datasets are higher than 99% and %5, respectively.
               ",autonomous vehicle
10.1016/B978-0-323-67538-3.00005-1,journal,Artificial Intelligence and Deep Learning in Pathology,sciencedirect,2021-12-31,sciencedirect,Chapter 5: Dealing with data: strategies of preprocessing data,https://api.elsevier.com/content/article/pii/B9780323675383000051,"
               A machine learning model is only as good as the data it is given to work on. Data may have redundancies, missing values, artifactual outliers, irrelevant features (noise), and so on. In addition, the feature vectors that characterize data are usually of high dimension (each sample has a very large number of features or attributes). For a variety of technical reasons, this may also compromise performance. Direct observation of the data, in addition to cleaning up the data, may lead to removal of some of the unimportant or compromised features, thus reducing dimensionality as well. In addition, there are a number of mathematical techniques by which machine learning algorithms can reduce dimensionality in an automatic and unsupervised manner. These considerations are an important part of the armamentarium of every data scientist and machine learning specialist and must be understood by the pathologists partnering with them to ensure the clinical applicability and validity of the final result. Of equal importance is the need to avoid bias from contamination or limitations of the dataset. In addition to ethical and moral issues, such bias can lead to incorrect conclusions from that data.
            ",autonomous vehicle
10.1016/j.aiia.2020.09.002,journal,Artificial Intelligence in Agriculture,sciencedirect,2020-12-31,sciencedirect,A review on computer vision systems in monitoring of poultry: A welfare perspective,https://api.elsevier.com/content/article/pii/S2589721720300258,"Monitoring of poultry welfare-related bio-processes and bio-responses is vital in welfare assessment and management of welfare-related factors. With the current development in information technologies, computer vision has become a promising tool in the real-time automation of poultry monitoring systems due to its non-intrusive and non-invasive properties, and its ability to present a wide range of information. Hence, it can be applied to monitor several bio-processes and bio-responses. This review summarizes the current advances in poultry monitoring techniques based on computer vision systems, i.e., conventional machine learning-based and deep learning-based systems. A detailed presentation on the machine learning-based system was presented, i.e., pre-processing, segmentation, feature extraction, feature selection, and dimension reduction, and modeling. Similarly, deep learning approaches in poultry monitoring were also presented. Lastly, the challenges and possible solutions presented by researches in poultry monitoring, such as variable illumination conditions, occlusion problems, and lack of augmented and labeled poultry datasets, were discussed.",autonomous vehicle
10.1016/j.vlsi.2017.11.001,journal,Integration,sciencedirect,2018-03-31,sciencedirect,"Neuromorphic computing's yesterday, today, and tomorrow – an evolutional view",https://api.elsevier.com/content/article/pii/S0167926017304674,"
                  Neuromorphic computing was originally referred to as the hardware that mimics neuro-biological architectures to implement models of neural systems. The concept was then extended to the computing systems that can run bio-inspired computing models, e.g., neural networks and deep learning networks. In recent years, the rapid growth of cognitive applications and the limited processing capability of conventional von Neumann architecture on these applications motivated worldwide research on neuromorphic computing systems. In this paper, we review the evolution of neuromorphic computing technique in both computing model and hardware implementation from a historical perspective. Various implementation methods and practices are also discussed. Finally, we present some emerging technologies that may potentially change the landscape of neuromorphic computing in the future, e.g., new devices and interdisciplinary computing architectures.
               ",autonomous vehicle
10.1016/B978-0-12-820074-2.00007-1,journal,Local Electricity Markets,sciencedirect,2021-12-31,sciencedirect,Chapter 14: Forecasting,https://api.elsevier.com/content/article/pii/B9780128200742000071,"
               Local energy markets require various types of forecasting. Even if the existing methods are more and more accurate, there is a continuous search for more advanced methods able to quantify the uncertainty of various electrical and price signals. Although a wide range of machine learning methods has been applied to electricity forecasting, in this chapter we will pass from linear models to state-of-the-art deep learning methods in an attempt to understand which are their most interesting challenges and limitations. The day-ahead electricity load forecast performance is analyzed for five EU countries. Consequently, we perform a comparison between Ordinary Least Squares, Ridge Regression, Bayesian Ridge Regression, Kernel Ridge Regression, Support Vector Regression, Nearest Neighbors Regression, Gaussian Process, Decision Trees, AdaBoost, Random Trees, and dense Multilayer Perceptron (MLP). Moreover, in an attempt to have accurate and fast methods with good generalization power, we introduce sparse neural networks and sparse training methods for electricity forecasting through the means of sparse MLPs trained with the Sparse Evolutionary Training procedure (SET-MLP).
            ",autonomous vehicle
10.1016/j.vlsi.2019.07.005,journal,Integration,sciencedirect,2019-11-30,sciencedirect,Computer vision algorithms and hardware implementations: A survey,https://api.elsevier.com/content/article/pii/S0167926019301762,"The field of computer vision is experiencing a great-leap-forward development today. This paper aims at providing a comprehensive survey of the recent progress on computer vision algorithms and their corresponding hardware implementations. In particular, the prominent achievements in computer vision tasks such as image classification, object detection and image segmentation brought by deep learning techniques are highlighted. On the other hand, review of techniques for implementing and optimizing deep-learning-based computer vision algorithms on GPU, FPGA and other new generations of hardware accelerators are presented to facilitate real-time and/or energy-efficient operations. Finally, several promising directions for future research are presented to motivate further development in the field.",autonomous vehicle
10.1016/j.aap.2021.106164,journal,Accident Analysis & Prevention,sciencedirect,2021-07-31,sciencedirect,A performance analysis of prediction techniques for impacting vehicles in hit-and-run road accidents,https://api.elsevier.com/content/article/pii/S0001457521001950,"
                  Road accidents are globally accepted challenges. They are one of the significant causes of deaths and injuries besides other direct and indirect losses. Countries and international organizations have designed technologies, systems, and policies to prevent accidents. However, hit-and-run accidents remain one of the most dangerous types of road accidents as the information about the vehicle responsible for the accident remain unknown. Therefore, any mechanism which can provide information about the impacting vehicle in hit-and-run accidents will be useful in planning and executing preventive measures to address this road menace. Since there exist several models to predict the impacting unknown vehicle, it becomes important to find which is the most accurate amongst those available. This research applies a process-based approach that identifies the most accurate model out of six supervised learning classification models viz. Logistic Reasoning, Linear Discriminant Analysis, Naïve Bayes, Classification and Regression Trees, k-Nearest Neighbor and Support Vector Machine. These models are implemented using five-fold and ten-fold cross validation, on road accident data collected from five mid-sized Indian cities: Agra, Amritsar, Bhopal, Ludhiana, and Vizag (Vishakhapatnam).This study investigates the possible input factors that may have effect on the performance of applied models. Based on the results of the experiment conducted in this study, Support Vector Machine has been found to have the maximum potentiality to predict unknown impacting vehicle type in hit-and-run accidents for all the cities except Amritsar. The result indicates that, Classification and Regression Trees have maximum accuracy, for Amritsar. Naïve Bayes performed very poorly for the five cities. These recommendations will help in predicting unknown impacting vehicles in hit-and-run accidents. The outcome is useful for transportation authorities and policymakers to implement effective road safety measures for the safety of road users.
               ",autonomous vehicle
10.1016/j.neucom.2021.04.127,journal,Neurocomputing,sciencedirect,2021-10-21,sciencedirect,Distant supervised relation extraction with position feature attention and selective bag attention,https://api.elsevier.com/content/article/pii/S0925231221009541,"
                  Distant supervision greatly reduces manual consumption by automatically labeling data. The relation extraction methods under distant supervision divide sentences with the same entity pair into a bag, and perform training and testing on these sentence bags. The existing distant supervised relation extraction methods ignore two facts. First, there are many sentences where the target entity pairs appear multiple times. Second, the noise between sentence bags is different, and the sentences of some bags are even all mislabeled. To solve these two problems, we propose a novel relation extraction method with position feature attention and selective bag attention. The position feature attention is employed to obtain the weighted sentence representation with different position features by calculating all position combinations of the target entity pair. A bag with large noise and a bag with small noise are selected through the selective bag attention mechanism to form a bag pair, and training is performed at the level of the bag pair, which denoises at the bag level and at the same time balances the noise between different bag pairs. The experimental results show that our method is effective and outperforms several competitive baseline methods.
               ",autonomous vehicle
10.1016/S0952-1976(02)00043-X,journal,Engineering Applications of Artificial Intelligence,sciencedirect,2002-08-31,sciencedirect,A neural network based obstacle-navigation animat in a virtual environment,https://api.elsevier.com/content/article/pii/S095219760200043X,"
                  Recently, much research has been done on the possibilities of autonomous robots navigating and performing obstacle avoidance. Vision is an important method of gathering information about obstacles and other objects within an environment. A virtual environment was created, in which an animat (virtual organism) navigates using a visual system comparable to that used by biological organisms. A spike-based neural network model was applied to learning. Various applicable rules on network topology were used, and examination was made of how well the animat learnt under varying sizes of visual input layer and intermediate ‘processing’ layer. In addition, three different training regimes were applied and their different merits discussed. It was discovered that increasing layer size in general improved the performance of the animat, provided that the sizes of the input array and the processing arrays correspond. The learning curve of the animat over time was investigated, allowing an optimal training time to be determined. These findings allow an insight into how well such a system can perform and how its design and training may be optimized.
               ",autonomous vehicle
10.1016/j.eng.2019.04.011,journal,Engineering,sciencedirect,2019-08-31,sciencedirect,From Intelligence Science to Intelligent Manufacturing,https://api.elsevier.com/content/article/pii/S2095809919301821,,autonomous vehicle
10.1016/j.drudis.2021.01.008,journal,Drug Discovery Today,sciencedirect,2021-04-30,sciencedirect,Integration of AI and traditional medicine in drug discovery,https://api.elsevier.com/content/article/pii/S1359644621000350,,autonomous vehicle
10.1016/j.apgeochem.2021.105072,journal,Applied Geochemistry,sciencedirect,2021-09-30,sciencedirect,"The processing methods of geochemical exploration data: past, present, and future",https://api.elsevier.com/content/article/pii/S0883292721002031,"
                  Geochemical exploration data is popular in mineral exploration in that it plays a notable role in discovering unknown mineral deposits. In this study, we review the state-of-the-art popular methods for processing geochemical exploration data and for identifying geochemical anomalies associated with mineralization. The distribution laws of geochemical elements concentrations, including normal, log-normal, power-law, and multimodal and complex distributions, have been extensively studied over the past several decades. Accordingly, methods for processing geochemical exploration data have shifted from classic statistics, multivariate statistics, geostatistics, to fractal/multifractal models and machine learning algorithms. Geochemical exploration data, as compositional data, suffer from the closure problem. We need first to open them using logratio transformation. In the future, deep learning algorithms will become a popular technique for mining geochemical exploration data and for extracting targets associated with mineralization in mineral exploration.
               ",autonomous vehicle
10.1016/j.apenergy.2020.115524,journal,Applied Energy,sciencedirect,2020-11-01,sciencedirect,Microgrid Energy Management Systems Design by Computational Intelligence Techniques,https://api.elsevier.com/content/article/pii/S0306261920310369,"
                  With the capillary spread of multi-energy systems such as microgrids, nanogrids, smart homes and hybrid electric vehicles, the design of a suitable Energy Management System (EMS) able to schedule the local energy flows in real time has a key role for the development of Renewable Energy Sources (RESs) and for reducing pollutant emissions. In the literature, most EMSs proposed are based on the implementation of energy systems prediction which enable to run a specific optimization algorithm. Such strategy, known as Rolling Time Horizon (RTH), demonstrated very effective when the supporting prediction system performs well. However, it is featured by high operational times. In this work, different lightweight EMS models synthesized through machine learning algorithms have been compared considering six different simulation scenarios. Results shows that an RTH-based EMS owns the best overall performances. However, in some case studies, also other EMSs show competitive results, especially those based on Adaptive Neuro Fuzzy Inference Systems (ANFIS) trained by clustering, which in one case outperform RTH EMSs, and in other 3 cases (out of 6) yields performances close to RTH EMSs within 
                        
                           5
                           %
                        
                     . A second contribution concerns the RTH EMS implementation on a small micro-controller, highlighting the high computational effort which can range in the order of minutes. Conversely, the ANFIS EMS shows always almost negligible computational costs (less than one second) and therefore can be used in realistic scenarios on cheap devices at run time. The paper also proposed a novel graphic tool to better represent, observe and analyze microgrid energy flows in each time slot or along the overall considered dataset.
               ",autonomous vehicle
10.1016/B978-0-323-85064-3.00002-9,journal,Image Processing for Automated Diagnosis of Cardiac Diseases,sciencedirect,2021-12-31,sciencedirect,Chapter 8: AI-based diagnosis techniques for cardiac disease analysis and predictions,https://api.elsevier.com/content/article/pii/B9780323850643000029,"
               Artificial intelligence (AI) has developed speedily since the late 1980s. Enhancement of medical datasets and outcomes in the last twenty years has resulted in unprecedented improvement in AI-based journals. In addition, with the introduction of unparalleled computational efficiency, the accessibility of AI tools has improved. There are two fundamental tools in AI. The first is machine learning (ML), where organized information like electrophysiology (EP), images, and genetic information are broken down and examined. The second is natural language processing (NLP), where unorganized information is scrutinized. These two AI tools have enhanced strategies, calculations, and applications. Different endeavors and new techniques of AI have been utilized for ailments like cardiovascular disease (CVD), neural disorders, and cancer, among others. Presently, a sophisticated deep learning (DL) technique has instigated exceptional growth of AI in clinical imaging diagnostic frameworks. Thus, this chapter presents pivotal and specialized information about AI-based techniques for predicting, diagnosing, and analyzing cardiac diseases.
            ",autonomous vehicle
10.1016/B978-0-12-820341-5.00006-0,journal,A Beginner's Guide to Data Agglomeration and Intelligent Sensing,sciencedirect,2020-12-31,sciencedirect,Chapter six: Intelligent sensor network,https://api.elsevier.com/content/article/pii/B9780128203415000060,"
               If we search for the word “intelligent” related to computer science we will come across various terms related to automation, machine learning, artificial intelligence, embedded system, and many more. An intelligent system can be classified as an agglomeration of all the stated words that is a system which is able to perform certain computation without any human intervention and is able to take decisions accordingly.
            ",autonomous vehicle
10.1016/B978-0-323-85769-7.00012-4,journal,Cognitive Computing for Human-Robot Interaction,sciencedirect,2021-12-31,sciencedirect,Chapter 2: Recent trends towards cognitive science: from robots to humanoids,https://api.elsevier.com/content/article/pii/B9780323857697000124,"
               Each upsurge of new computational creativity has been cultivated to guide new kinds of systems, new ways of creating tools, a new form of data, and so on, which have often overturned their predecessors. In this regard, cognitive computing (CC) is described as the merger of artificial intelligence and signal processing disciplines. The objective of cognitive processing is to replicate the human thought processes in a computerized model. Employing self-learning algorithms that use data mining, pattern recognition, and natural language processing concerning this specific subject in which the computer can mimic the human brain function.
               CC makes a new class of glitches computable. The problem can address the complex situations characterized by vagueness as well as doubt. The diverse, ample-data, and frequently changing circumstances, data manage to shift continuously, moreover it is contradictory also. Another objective of CC is to evaluate users’ needs through learning and reframing their goals and objectives. Following the diversity of user’s interpretation of problems, the CC system presents the reorganization of types of data and sways meaning and analysis. When arriving at a conclusion the CC system usually relies on considering contradictory proof and proposing a solution which is “best” more than precisely “right.” The systems with CC ability create quantifiable situations. Machines with cognitive systems recognize as well as excerpt factor characteristics for example location, time, history, work, or description to portray the data set suitable for a person or for a relying implementation involved in a particular method at an exact schedule and location. These systems reframe the scenery involving the association of individuals and their progressively all-encompassing digital environment. This has been shown to play a larger role as the mentor or assistant for the user, similarly they may behave digitally independent for resolving various problems.
            ",autonomous vehicle
10.1016/S0004-3702(97)00063-5,journal,Artificial Intelligence,sciencedirect,1997-12-31,sciencedirect,Selection of relevant features and examples in machine learning,https://api.elsevier.com/content/article/pii/S0004370297000635,"In this survey, we review work in machine learning on methods for handling data sets containing large amounts of irrelevant information. We focus on two key issues: the problem of selecting relevant features, and the problem of selecting relevant examples. We describe the advances that have been made on these topics in both empirical and theoretical work in machine learning, and we present a general framework that we use to compare different methods. We close with some challenges for future work in this area.",autonomous vehicle
10.1016/B978-0-12-819043-2.00011-3,journal,Innovation in Health Informatics,sciencedirect,2020-12-31,sciencedirect,Chapter 11: Artificial intelligence–assisted detection of diabetic retinopathy on digital fundus images: concepts and applications in the National Health Service,https://api.elsevier.com/content/article/pii/B9780128190432000113,"
               Diabetic retinopathy (DR), one of the most devastating manifestations of diabetes, is a leading cause of blindness among working-age adults. World Health Organization predicts the prevalence of diabetes to increase substantially in the future, leading to an increasing pressure on public health services. In the context of smart healthcare, DR screening has been widely adopted by utilizing fundus imaging with manual input. In this chapter, we review the potential of artificial intelligence enabled automated screening for the detection and classification of DR in the context of the National Health Service. We propose an integrated multimodal approach to enable the combination of manual (through human graders) and automated DR screening. Furthermore, we discuss how this multimodal approach can be enhanced by the integration of additional modalities such as optical coherence tomography. Artificial intelligence in that setting can complement and upskill human graders by acting in an assistive way rather than replacing their role.
            ",autonomous vehicle
10.1016/j.scs.2020.102052,journal,Sustainable Cities and Society,sciencedirect,2020-04-30,sciencedirect,A review on renewable energy and electricity requirement forecasting models for smart grid and buildings,https://api.elsevier.com/content/article/pii/S2210670720300391,"
                  The benefits of renewable energy are that it is sustainable and is low in environmental pollution. Growing load requirement, global warming, and energy crisis need energy-intensive management to give sincere attempts to promote high accuracy energy monitoring techniques in order to enhance energy system efficiency and performance. The energy consumption data of domestic, commercial and industrial are becoming accessible to estimate the notable share of various sectors in the energy market. Energy forecasting algorithms play a vital role in energy sector development and policy formulation. Energy prediction and power supply management are the key roots of energy planning. A large number of prediction models have been used in the recent past. The selection of a prediction model usually based on available data, the objectives of the model network mechanism and energy planning operation. In this review, we conduct a critical and systematic review of renewable energy and electricity prediction models applied as an energy planning tool. The forecasting intervals is divided into three sections including: i) short-term; ii) medium-term; iii) and long-term. Three renewable energy resources, i.e. wind, solar, and geothermal energy, and electricity load demand requirement are considered for review forecasting analysis. Three major states-of-art forecasting classifications: i) machine learning algorithms; ii) ensemble-based approaches; iii) and artificial neural networks are analyzed. These approaches are investigated for prediction applicability; accuracy for spatial and temporal forecasting; and relevance to policy and planning objectives. The machine learning models can handle large amount of data with accurate forecasting analysis. Applying ensemble techniques enables us to obtain higher forecasting accuracy by combining different models. Artificial neural networks if used in the right way can contribute a robust choice, given that it is capible to extract and model unseen relationships and features. Furthermore, unlike these conventional techniques, artificial neural networks do not force any limitation on residual and input distributions. Findings from this review would help professionals and researchers in obtaining recognition of the prediction approaches and allow them to choose the relevant methods to satisfy their desired tasks and forecasting requirements.
               ",autonomous vehicle
10.1016/j.jss.2020.110574,journal,Journal of Systems and Software,sciencedirect,2020-07-31,sciencedirect,Adaptive metamorphic testing with contextual bandits,https://api.elsevier.com/content/article/pii/S0164121220300558,"
                  Metamorphic Testing is a software testing paradigm which aims at using necessary properties of a system under test, called metamorphic relations, to either check its expected outputs, or to generate new test cases. Metamorphic Testing has been successful to test programs for which a full oracle is not available or to test programs for which there are uncertainties on expected outputs such as learning systems. In this article, we propose Adaptive Metamorphic Testing as a generalization of a simple yet powerful reinforcement learning technique, namely contextual bandits, to select one of the multiple metamorphic relations available for a program. By using contextual bandits, Adaptive Metamorphic Testing learns which metamorphic relations are likely to transform a source test case, such that it has higher chance to discover faults. We present experimental results over two major case studies in machine learning, namely image classification and object detection, and identify weaknesses and robustness boundaries. Adaptive Metamorphic Testing efficiently identifies weaknesses of the tested systems in context of the source test case.
               ",autonomous vehicle
10.1016/B978-012161964-0/50006-6,journal,Artificial Intelligence,sciencedirect,1996-12-31,sciencedirect,Chapter 4: Machine Learning,https://api.elsevier.com/content/article/pii/B9780121619640500066,"
               This chapter gives an account of machine learning, which is the subfield of artificial intelligence (AI) concerned with intelligent systems that can learn. This chapter adopts a view of intelligent systems as agents. Learning is often viewed as the most fundamental aspect of intelligence because it enables the agent to become independent of its creator. It is an essential component of an agent design whenever the designer has incomplete knowledge of the task environment. Therefore, learning provides autonomy in that the agent is not dependent on the designer's knowledge for its success and can free itself from the assumptions built into its initial configuration. Learning in intelligent agents is essential both as a construction process and as a way to deal with unknown environments. Learning agents can be divided conceptually into a performance element, which is responsible for selecting actions, and a learning element, which is responsible for modifying the performance element. The nature of the performance element and the kind of feedback available from the environment determine the form of the learning algorithm. The chapter develops a comprehensive theory of the complexity of induction, which analyzes the inherent difficulty of various kinds of learning problems in terms of sample complexity and computational complexity.
            ",autonomous vehicle
10.1016/j.neucom.2016.03.067,journal,Neurocomputing,sciencedirect,2016-09-26,sciencedirect,Self-adjusting feature maps network and its applications,https://api.elsevier.com/content/article/pii/S0925231216303009,"
                  This paper, proposes a novel artificial neural network, called self-adjusting feature map (SAM), and develop its unsupervised learning ability with self-adjusting mechanism. The trained network structure of representative connected neurons not only displays the spatial relation of the input data distribution but also quantizes the data well. The SAM can automatically isolate a set of connected neurons, in which, the used number of the sets may indicate the number of clusters. The idea of self-adjusting mechanism is based on combining of mathematical statistics and neurological advantages and retreat of waste. In the training process, for each representative neuron has are three phases, growth, adaptation and decline. The network of representative neurons, first create the necessary neurons according to the local density of the input data in the growth phase. In the adaption phase, it adjusts neighborhood neuron pair׳s connected/disconnected topology constantly according to the statistics of input feature data. Finally, the unnecessary neurons of the network are merged or remove in the decline phase. In this paper, we exploit the SAM to handle some peculiar cases that cannot be handled easily by classical unsupervised learning networks such as self-organizing map (SOM) network. The remarkable characteristics of the SAM can be seen on various real world cases in the experimental results.
               ",autonomous vehicle
10.1016/j.cviu.2017.01.009,journal,Computer Vision and Image Understanding,sciencedirect,2017-11-30,sciencedirect,Harnessing noisy Web images for deep representation,https://api.elsevier.com/content/article/pii/S1077314217300255,"
                  The keep-growing content of Web images is probably the next important data source to scale up deep neural networks which recently surpass human in image classification tasks. The fact that deep networks are hungry for labelled data limits themselves from extracting valuable information of Web images which are abundant and cheap. There have been efforts to train neural networks such as autoencoders with respect to either unsupervised or semi-supervised settings. Nonetheless they are less performant than supervised methods partly because the loss function used in unsupervised methods, for instance Euclidean loss, failed to guide the network to learn discriminative features and ignore unnecessary details. We instead train convolutional networks in a supervised setting but use weakly labelled data which are large amounts of unannotated Web images downloaded from Flickr and Bing. Our experiments are conducted at several data scales, with different choices of network architecture, and alternating between different data preprocessing techniques. The effectiveness of our approach is shown by the good generalization of the learned representations with new six public datasets.
               ",autonomous vehicle
10.1016/j.neubiorev.2004.09.009,journal,Neuroscience & Biobehavioral Reviews,sciencedirect,2005-01-31,sciencedirect,Donald O. Hebb's synapse and learning rule: a history and commentary,https://api.elsevier.com/content/article/pii/S0149763404000995,"
                  This year sees the anniversary of Donald O. Hebb's birth, in July 1904. The impact of his work, especially through his neurophysiological postulate, as described in his magnum opus, The organization of behaviour (1949), has been profound in contemporary neuroscience. Hebb's life, and the scientific milieu in psychology and neurophysiology which preceded and informed Hebb's work are described. His core postulate, which gave rise to such eponymous expressions as the Hebbian synapse and the Hebbian learning rule, is examined in some detail, as well as the part it played in his higher-order theoretical constructs concerned with neocortical structure and function. Early models which made use of the Hebbian synapse are described, and then illustrative examples are given detailing the impact of Hebb's idea in relation to learning and memory, synaptic plasticity and stability, and the question of persistent cortical activity underlying forms of short-term memory.
               ",autonomous vehicle
10.1016/j.cie.2021.107400,journal,Computers & Industrial Engineering,sciencedirect,2021-08-31,sciencedirect,Random mask-based estimation of the distribution algorithm for stacked auto-encoder one-step pre-training,https://api.elsevier.com/content/article/pii/S0360835221003041,"
                  The deep learning techniques have received great achievements in computer vision, natural language processing, etc. The success of deep neural networks depends on the sufficient training of parameters. The traditional way of neural network training is a gradient-based algorithm, which suffers the disadvantage of gradient disappearing, especially for the deeper neural network. Recently, a heuristic algorithm has been proposed for deeper neural network optimization. In this paper, a random mask and elitism univariate continuous estimation of distribution algorithm based on the Gaussian model is proposed to pre-train staked auto-encoder, and then a Stochastic Gradient Descent (SGD) based fine-tuning process is carried out for local searching. In the improved estimation of the distribution algorithm, two individual update strategies are defined; one group of individuals is generated according to the constructed probabilistic model, and another is updated according to the statistics of advanced individuals that aim to reduce the probability of combination explosion and time consumption according to the mask information. In the simulations, different architectures, different mask ratios and different promising individual ratios are adopted to testify the effectiveness of the improved algorithm. According to simulation results, the estimation of thr distribution algorithm has a steady optimization ability for the shallow and stacked auto-encoder by one-step pre-training combining SGD based fine-tuning for the MNIST dataset. The proposed model will achieve a state-of-the-art performance on Fashion-MNIST.
               ",autonomous vehicle
10.1016/j.micpro.2020.103115,journal,Microprocessors and Microsystems,sciencedirect,2020-07-31,sciencedirect,Detecting android malware using an improved filter based technique in embedded software,https://api.elsevier.com/content/article/pii/S0141933120302829,"
                  The technological advancements have led to evolution of sophisticated devices called smartphones. By providing extensive capabilities, they are becoming more and more popular. The Android based smartphones are preferred furthermore, due to their open-source nature. This has also led to the development of large number of malwares targeting these smartphones. Thus to protect the devices, some countermeasures are needed. Machine learning methods have gained popularity in detection of malware. This work proposes a malware detection technique in Android devices based on static analysis carried out using the Manifest files extracted from the apk files. The feature selection is performed using the proposed KNN based Relief algorithm and detection of malware is done using the proposed optimized SVM algorithm. The proposed method achieves a True Positive Rate greater than 0.70 and much reduced False Positive Rate values were obtained, with the values of False Positive Rate being very close to zero. The proposed KNN based feature selection is found to select better features in comparison with some popular existing feature selection techniques. The proposed optimized SVM technique achieves a performance that is on par with the performance of Neural Networks.
               ",autonomous vehicle
10.1016/j.future.2019.08.009,journal,Future Generation Computer Systems,sciencedirect,2020-01-31,sciencedirect,Intelligence and security in big 5G-oriented IoNT: An overview,https://api.elsevier.com/content/article/pii/S0167739X19301074,"
                  Internet of Nano-Things (IoNT) overcomes critical difficulties and additionally open doors for wearable sensor based huge information examination. Conventional computing and/or communication systems do not offer enough flexibility and adaptability to deal with the gigantic amount of assorted information nowadays. This creates the need for legitimate components that can efficiently investigate and communicate the huge data while maintaining security and quality of service. In addition, while developing the ultra-wide Heterogeneous Networks (HetNets) associated with the ongoing Big Data project and 5G-based IoNT, it is required to resolve the emerging difficulties as well. Accordingly, these difficulties and other relevant design issues have been comprehensively reported in this survey. It mainly focuses on security issues and associated intelligence to be considered while managing these issues.
               ",autonomous vehicle
10.1016/B978-0-12-805095-8.00014-4,journal,The Five Technological Forces Disrupting Security,sciencedirect,2018-12-31,sciencedirect,Chapter 14: Faster,https://api.elsevier.com/content/article/pii/B9780128050958000144,"
               Abstract
               Smart data is a means of providing answers much more quickly than has been possible with older technologies. The speed of these systems relies on parallel advances in artificial intelligence and machine learning that have made great strides in real-time comprehension of complex data streams. Network and cybersecurity systems are presented as models of big data systems for physical security because they operate with significantly higher volumes of data and must be able to extract threat information in real time. Relevant aspects of artificial intelligence are discussed in the context of their application to physical security data streams. Machine learning techniques are outlined, with examples of how they can be useful for security analytics. Real-time analytics are discussed as a special case.
            ",autonomous vehicle
10.1016/j.comcom.2006.08.009,journal,Computer Communications,sciencedirect,2007-01-15,sciencedirect,Neural-based downlink scheduling algorithm for broadband wireless networks,https://api.elsevier.com/content/article/pii/S0140366406003161,"
                  Wireless local area networks are becoming very popular in many scenarios because they are very simple, convenient and cheap. This paper focuses on multimedia traffic management in wireless networks, where we consider to provide differentiated Quality of Service (QoS) levels. We address the complex task of traffic scheduling with multi-objective requirements in the presence of errors introduced by the radio channel. In particular, we focus on managing downlink traffic in both wireless ATM and WiFi scenarios, referring to an infrastructure wireless access network where a central coordinator takes scheduling decisions for the mobile users in its cell. Our scheduler is based on an Artificial Neural Network (ANN) with reinforcement learning. The ANN is trained from examples to behave as an “optimal” scheduler, according to an Actor-Critic model. The results obtained in scheduling concomitant voice, video and Web traffic classes permit to show the significant capacity improvement that can be achieved by our scheme with respect to other techniques previously proposed in the literature.
               ",autonomous vehicle
10.1016/j.expneurol.2021.113647,journal,Experimental Neurology,sciencedirect,2021-06-30,sciencedirect,Automation of training and testing motor and related tasks in pre-clinical behavioural and rehabilitative neuroscience,https://api.elsevier.com/content/article/pii/S0014488621000522,"
                  Testing and training animals in motor and related tasks is a cornerstone of pre-clinical behavioural and rehabilitative neuroscience. Yet manually testing and training animals in these tasks is time consuming and analyses are often subjective. Consequently, there have been many recent advances in automating both the administration and analyses of animal behavioural training and testing. This review is an in-depth appraisal of the history of, and recent developments in, the automation of animal behavioural assays used in neuroscience. We describe the use of common locomotor and non-locomotor tasks used for motor training and testing before and after nervous system injury. This includes a discussion of how these tasks help us to understand the underlying mechanisms of neurological repair and the utility of some tasks for the delivery of rehabilitative training to enhance recovery. We propose two general approaches to automation: automating the physical administration of behavioural tasks (i.e., devices used to facilitate task training, rehabilitative training, and motor testing) and leveraging the use of machine learning in behaviour analysis to generate large volumes of unbiased and comprehensive data. The advantages and disadvantages of automating various motor tasks as well as the limitations of machine learning analyses are examined. In closing, we provide a critical appraisal of the current state of automation in animal behavioural neuroscience and a prospective on some of the advances in machine learning we believe will dramatically enhance the usefulness of these approaches for behavioural neuroscientists.
               ",autonomous vehicle
10.1016/B0-08-043076-7/00537-4,journal,International Encyclopedia of the Social & Behavioral Sciences,sciencedirect,2001-12-31,sciencedirect,Connectionist Approaches,https://api.elsevier.com/content/article/pii/B0080430767005374,"
               Connectionist approaches to cognitive modeling make use of large networks of simple computational units, which communicate by means of simple quantitative signals. Higher-level information processing emerges from the massively-parallel interaction of these units by means of their connections, and a network may adapt its behavior by means of local changes in the strength of the connections. Connectionist approaches are related to neural networks and provide a distinct alternative to cognitive models inspired by the digital computer. After defining key terms, a short history of connectionism is presented, first in the narrower context of cognitive science and artificial intelligence, then in the broader context of epistemology, linguistics, and the philosophy of mind. Next the article touches on the principles of adaptation and learning in connectionist systems, discussing informally the principles of correlational and gradient-descent learning (including the delta rule and backpropagation). The concepts of supervised and unsupervised learning are defined. Then a single example of the connectionist approach is presented: training a network to learn the past tenses of English verbs. Finally, a number of issues in connectionism are discussed briefly: the relation of the symbolic and subsymbolic, distributed representations, computability and Turing machines, the uninterpretability of connectionist networks, their ability to account for sentential and hierarchical knowledge, and their relation to biological neural nets.
            ",autonomous vehicle
10.1016/j.infsof.2019.106241,journal,Information and Software Technology,sciencedirect,2020-03-31,sciencedirect,Intelligent software engineering in the context of agile software development: A systematic literature review,https://api.elsevier.com/content/article/pii/S0950584919302587,"
                  
                     CONTEXT: Intelligent Software Engineering (ISE) refers to the application of intelligent techniques to software engineering. We define an “intelligent technique” as a technique that explores data (from digital artifacts or domain experts) for knowledge discovery, reasoning, learning, planning, natural language processing, perception or supporting decision-making.
                  
                     OBJECTIVE: The purpose of this study is to synthesize and analyze the state of the art of the field of applying intelligent techniques to Agile Software Development (ASD). Furthermore, we assess its maturity and identify adoption risks.
                  
                     METHOD: Using a systematic literature review, we identified 104 primary studies, resulting in 93 unique studies. RESULTS: We identified that there is a positive trend in the number of studies applying intelligent techniques to ASD. Also, we determined that reasoning under uncertainty (mainly, Bayesian network), search-based solutions, and machine learning are the most popular intelligent techniques in the context of ASD. In terms of purposes, the most popular ones are effort estimation, requirements prioritization, resource allocation, requirements selection, and requirements management. Furthermore, we discovered that the primary goal of applying intelligent techniques is to support decision making. As a consequence, the adoption risks in terms of the safety of the current solutions are low. Finally, we highlight the trend of using explainable intelligent techniques.
                  
                     CONCLUSION: Overall, although the topic area is up-and-coming, for many areas of application, it is still in its infancy. So, this means that there is a need for more empirical studies, and there are a plethora of new opportunities for researchers.
               ",autonomous vehicle
10.1016/j.asoc.2009.06.019,journal,Applied Soft Computing,sciencedirect,2010-01-31,sciencedirect,The use of computational intelligence in intrusion detection systems: A review,https://api.elsevier.com/content/article/pii/S1568494609000908,"
                  Intrusion detection based upon computational intelligence is currently attracting considerable interest from the research community. Characteristics of computational intelligence (CI) systems, such as adaptation, fault tolerance, high computational speed and error resilience in the face of noisy information, fit the requirements of building a good intrusion detection model. Here we want to provide an overview of the research progress in applying CI methods to the problem of intrusion detection. The scope of this review will encompass core methods of CI, including artificial neural networks, fuzzy systems, evolutionary computation, artificial immune systems, swarm intelligence, and soft computing. The research contributions in each field are systematically summarized and compared, allowing us to clearly define existing research challenges, and to highlight promising new research directions. The findings of this review should provide useful insights into the current IDS literature and be a good source for anyone who is interested in the application of CI approaches to IDSs or related fields.
               ",autonomous vehicle
10.1016/j.procs.2021.05.080,journal,Procedia Computer Science,sciencedirect,2021-12-31,sciencedirect,Automatic Detection of Cyberbullying and Abusive Language in Arabic Content on Social Networks: A Survey,https://api.elsevier.com/content/article/pii/S1877050921011959,"As a key player in today’s world, online social networks are emerging, providing a platform for expression and content distribution. This technology enables users to communicate easily with each other and share their data instantly. However, the internet isn’t generally protected; it can be a source for abusive and harmful content and causing harm to others. There is a great need for approaches and strategies to solve these issues due to the negative effect of abusive language and cyberbullying. Arabic text is known for its challenges, complexity, and scarcity of its resources. Many languages have made many efforts to find automated solutions for detecting abusive language and cyberbullying, but not much for the Arabic language. This work analyzes 27 studies on automatic Arabic abusive language and cyberbullying and its related detection approaches. The goal of this paper is to review the findings of the previous studies about cyberbullying and abusive detection in Arabic content on online social networks and help researcher in the future to develop automatic detection systems that are effective and realistic.",autonomous vehicle
10.1016/B978-0-323-89861-4.00036-1,journal,Computers in Earth and Environmental Sciences,sciencedirect,2022-12-31,sciencedirect,"Chapter 19: Soft computing applications in rainfall-induced landslide analysis and protection—Recent trends, techniques, and opportunities",https://api.elsevier.com/content/article/pii/B9780323898614000361,"
               Among the natural hazards that affect humans, landslides play a key role in the devastation it creates and the economic loss it produces. Landslides mostly occur in hilly regions with unstable slopes. It is a complex nonlinear natural dynamical system with built-in uncertainty. The Evolution of landslides is influenced by multiple parameters, including tectonic, rainfall, water table fluctuation, soil condition, nature of the slope, vegetation availability, undercutting, and human activities in the region. Slow-moving landslides cause huge damage in terms of economy and ravaging facilities across their travel path, resulting in huge losses to human life. However, the quantification of fatalities caused by landslides is always underestimated and mainly incomplete; thus, the estimation of landslide risk is rather ambitious. Moreover, there is a growing demand for a detailed, and accurate estimation of landslides and their damage. It is also important to provide hazard mapping that shows the real vulnerability in an area, which will facilitate planners to plan and act. Quantification and estimation of landslide hazards is an hour in many countries where the quantum and frequency of landslides occurring in the last few decades are of major concern. Several methods have been studied by researchers. One such technique employed is machine learning, which is suitable for handling a large amount of data to predict the occurrence of landslides using various attributes. It is found in many studies that the critical role in landslide initiation is not only played by rainfall and earthquakes but also by intrinsic factors such as slope steepness, soil properties, lithology, etc. This work focuses on reviewing recent work done on landslide mapping, rainfall-induced landslides, and predictive methods of machine learning algorithms to map hazard vulnerability in tandem with GIS techniques.
            ",autonomous vehicle
10.1016/j.neunet.2012.09.017,journal,Neural Networks,sciencedirect,2013-01-31,sciencedirect,"Adaptive Resonance Theory: How a brain learns to consciously attend, learn, and recognize a changing world",https://api.elsevier.com/content/article/pii/S0893608012002584,"
                  Adaptive Resonance Theory, or ART, is a cognitive and neural theory of how the brain autonomously learns to categorize, recognize, and predict objects and events in a changing world. This article reviews classical and recent developments of ART, and provides a synthesis of concepts, principles, mechanisms, architectures, and the interdisciplinary data bases that they have helped to explain and predict. The review illustrates that ART is currently the most highly developed cognitive and neural theory available, with the broadest explanatory and predictive range. Central to ART’s predictive power is its ability to carry out fast, incremental, and stable unsupervised and supervised learning in response to a changing world. ART specifies mechanistic links between processes of consciousness, learning, expectation, attention, resonance, and synchrony during both unsupervised and supervised learning. ART provides functional and mechanistic explanations of such diverse topics as laminar cortical circuitry; invariant object and scenic gist learning and recognition; prototype, surface, and boundary attention; gamma and beta oscillations; learning of entorhinal grid cells and hippocampal place cells; computation of homologous spatial and temporal mechanisms in the entorhinal–hippocampal system; vigilance breakdowns during autism and medial temporal amnesia; cognitive–emotional interactions that focus attention on valued objects in an adaptively timed way; item–order–rank working memories and learned list chunks for the planning and control of sequences of linguistic, spatial, and motor information; conscious speech percepts that are influenced by future context; auditory streaming in noise during source segregation; and speaker normalization. Brain regions that are functionally described include visual and auditory neocortex; specific and nonspecific thalamic nuclei; inferotemporal, parietal, prefrontal, entorhinal, hippocampal, parahippocampal, perirhinal, and motor cortices; frontal eye fields; supplementary eye fields; amygdala; basal ganglia: cerebellum; and superior colliculus. Due to the complementary organization of the brain, ART does not describe many spatial and motor behaviors whose matching and learning laws differ from those of ART. ART algorithms for engineering and technology are listed, as are comparisons with other types of models.
               ",autonomous vehicle
10.1016/j.inffus.2021.07.009,journal,Information Fusion,sciencedirect,2022-01-31,sciencedirect,Multimodal research in vision and language: A review of current and emerging trends,https://api.elsevier.com/content/article/pii/S1566253521001512,"
                  Deep Learning and its applications have cascaded impactful research and development with a diverse range of modalities present in the real-world data. More recently, this has enhanced research interests in the intersection of the Vision and Language arena with its numerous applications and fast-paced growth. In this paper, we present a detailed overview of the latest trends in research pertaining to visual and language modalities. We look at its applications in their task formulations and how to solve various problems related to semantic perception and content generation. We also address task-specific trends, along with their evaluation strategies and upcoming challenges. Moreover, we shed some light on multi-disciplinary patterns and insights that have emerged in the recent past, directing this field toward more modular and transparent intelligent systems. This survey identifies key trends gravitating recent literature in VisLang research and attempts to unearth directions that the field is heading toward.
               ",autonomous vehicle
10.1016/j.jbi.2021.103687,journal,Journal of Biomedical Informatics,sciencedirect,2021-03-31,sciencedirect,Enhanced childhood diseases treatment using computational models: Systematic review of intelligent experiments heading to precision medicine,https://api.elsevier.com/content/article/pii/S1532046421000162,"
                  Introduction
                  Precision or personalized Medicine (PM) is used for the prevention and treatment of diseases by considering a huge amount of information about individuals variables. Due to high volume of information, AI-based computational models are required. A large set of studies conducted to examine the PM approach to improve childhood clinical outcomes. Thus, the main goal of this study was to review the application of health information technology and especially artificial intelligence (AI) methods for the treatment of childhood disease using PM.
               
                  Methods
                  PubMed, Scopus, Web of Science, and EMBASE databases were searched up to December 18, 2019. Articles that focused on informatics applications for childhood disease PM included in this study. Included papers were classified for qualitative analysis and interpreting results. The results were analyzed using Microsoft Excel 2019.
               
                  Results
                  From 341 citations, 62 papers met our inclusion criteria. The number of published papers that used AI methods to apply for PM in childhood diseases increased from 2010 to 2019. Our results showed that most applied methods were related to machine learning discipline. In terms of clinical scope, the largest number of clinical articles are devoted to oncology. Besides, the analysis showed that genomics was the most PM approach used regarding childhood disease.
               
                  Conclusion
                  This systematic review examined papers that used AI methods for applying PM approaches in childhood diseases from medical informatics perspectives. Thus, it provided new insight to researchers who are interested in knowing research needs in this field.
               ",autonomous vehicle
10.1016/S1474-6670(17)48394-7,journal,IFAC Proceedings Volumes,sciencedirect,1993-07-31,sciencedirect,A Reward/Punishment Learning Method to Swing Up a Pendulum into its Upright Position,https://api.elsevier.com/content/article/pii/S1474667017483947,"
                  For a pendulum-cart-DC-motor system, an on-line self-learning control scheme has been developed which is based on a reward/punishment approach Unlike in previous work where balancing the pendulum has been the main issue, our goal is to get the pendulum From us hanging position into its upright position. Besides the final position, no further requirements concerning the motion are specified. No information about the equations of motion is required for the proposed learning scheme lis reinforcement signal is calculated from most basic physical measurements which appear to be similar to human perception. Learning is both structural and parametric. According to its acquired experience, the learning system will determine on its own how many parameters have to be used and to which values they have to be set. These parameters constitute the memory of ihe control algorithm. Computer simulations are presented to clarify the main ideas from above.
               ",autonomous vehicle
10.1016/j.procs.2019.09.163,journal,Procedia Computer Science,sciencedirect,2019-12-31,sciencedirect,Towards automated joining element design,https://api.elsevier.com/content/article/pii/S1877050919313419,"Product variety and its induced manufacturing complexity remains to increase and therefore greatens challenges for design of joining elements. Historically, joining element design was a paper-based process with incomplete variety documentation and is digitalized only by replacing paper for 3D space. Currently, joining element design remains an ambiguous manual task with limited automation, resulting in long iterative, error prone development trajectories and costly reworks. Thus, processes in practice conflict with required capabilities. Artificial intelligence helps to solve such conflicts by taking over repetitive tasks, preventing human errors, optimizing designs and enabling designers to focus on their core competencies. This paper proposes a novel artificial intelligence method toolbox as a foundation to automate joining element design in manufacturing industries. The methodology aims to incorporate multiple lifecycle requirements including large product variety.",autonomous vehicle
10.1016/B978-0-12-415801-6.00023-2,journal,"Universe, Human Immortality and Future Human Evaluation",sciencedirect,2012-12-31,sciencedirect,Appendix 2: Current Artificial Intelligence,https://api.elsevier.com/content/article/pii/B9780124158016000232,Unknown,autonomous vehicle
10.1016/j.compbiomed.2020.104035,journal,Computers in Biology and Medicine,sciencedirect,2020-12-31,sciencedirect,Computer-aided diagnosis of liver lesions using CT images: A systematic review,https://api.elsevier.com/content/article/pii/S0010482520303668,"
                  Background
                  Medical image processing has a strong footprint in radio diagnosis for the detection of diseases from the images. Several computer-aided systems were researched in the recent past to assist the radiologist in diagnosing liver diseases and reducing the interpretation time. The aim of this paper is to provide an overview of the state-of-the-art techniques in computer-assisted diagnosis systems to predict benign and malignant lesions using computed tomography images.
               
                  Methods
                  The research articles published between 1998 and 2020 obtained from various standard databases were considered for preparing the review. The research papers include both conventional as well as deep learning-based systems for liver lesion diagnosis. The paper initially discusses the various hepatic lesions that are identifiable on computed tomography images, then the computer-aided diagnosis systems and their workflow. The conventional and deep learning-based systems are presented in stages wherein the various methods used for preprocessing, liver and lesion segmentation, radiological feature extraction and classification are discussed.
               
                  Conclusion
                  The review suggests the scope for future, work as efficient and effective segmentation methods that work well with diverse images have not been developed. Furthermore, unsupervised and semi-supervised deep learning models were not investigated for liver disease diagnosis in the reviewed papers. Other areas to be explored include image fusion and inclusion of essential clinical features along with the radiological features for better classification accuracy.
               ",autonomous vehicle
10.1016/j.acalib.2021.102450,journal,The Journal of Academic Librarianship,sciencedirect,2021-12-31,sciencedirect,Exploring adaptive boosting (AdaBoost) as a platform for the predictive modeling of tangible collection usage,https://api.elsevier.com/content/article/pii/S0099133321001415,"
                  Low-use tangible print collections represent a long-standing problem for academic libraries. Expanding on the previous research aimed at leveraging machine learning (ML) toward predicting patterns of collection use, this study explores the potential for adaptive boosting (AdaBoost) as a foundation for developing actionable predictive models of print title use. This study deploys the AdaBoost algorithm, with random forests used as the base classifier, via the adabag package for R. Methodological considerations associated with dataset congruence, as well as sample-based modeling versus novel data modeling, are explored in relation to four AdaBoost models that are trained and tested. Results of this study show AdaBoost as a promising ML solution for predictive modeling of print collections, with the central model of interest able to accurately predict use in over 85% of cases. This research also explores peripheral questions of interest related to general considerations when evaluating ML models, as well as the compatibility of similar models trained with e-book versus print book usage data.
               ",autonomous vehicle
10.1016/j.cogsys.2020.09.001,journal,Cognitive Systems Research,sciencedirect,2021-01-31,sciencedirect,Using spotted hyena optimizer for training feedforward neural networks,https://api.elsevier.com/content/article/pii/S1389041720300577,"
                  Spotted hyena optimizer (SHO) is a novel metaheuristic optimization algorithm based on the behavior of spotted hyena and their collaborative behavior in nature. In this paper, we design a spotted hyena optimizer for training feedforward neural network (FNN), which is regarded as a challenging task since it is easy to fall into local optima. Our objective is to apply metaheuristic optimization algorithm to tackle this problem better than the mathematical and deterministic methods. In order to confirm that using SHO to train FNN is more effective, five classification datasets and three function-approximations are applied to benchmark the performance of the proposed method. The experimental results show that the proposed SHO algorithm for optimization FNN has the best comprehensive performance and has more outstanding performance than other the state-of-the-art metaheuristic algorithms in terms of the performance measures.
               ",autonomous vehicle
10.1016/j.ecoinf.2021.101212,journal,Ecological Informatics,sciencedirect,2021-03-31,sciencedirect,Advances in image acquisition and processing technologies transforming animal ecological studies,https://api.elsevier.com/content/article/pii/S1574954121000030,"
                  Images and videos have become pervasive in ecological research and the ease of acquiring image data and its subsequent processing can provide answers in research areas such as species recognition, animal behaviour, and population studies which are critical for animal conservation and biodiversity. Technological advances in imaging are enabling data collection from new areas such as from underwater, new modalities such as thermal and new ways of processing such as deep learning. These advances are accelerating due to ease of data collection, better storage and processing technologies with associated lowering costs. The advancements in state-of-the-art machine learning for image and video classification and analysis can directly be applied in ecology. Ecological applications are generally conducted in remote and harsh deployment environments, and therefore present formidable challenges that require appreciation of the limitations of such technologies. The ecological field is poised to make use of images acquired through drones, robotics, and satellites through machine learning for rapid advancements in critical research areas. Timely insights from such data help to understand and protect the species and environment. This paper provides a review of the advancements in image acquisition and processing technologies used in animal ecological studies. We also discuss concepts and technologies that would help foster future ecological research methodologies potentially opening new insights and quickening growth to an already rich and data-intensive field.
               ",autonomous vehicle
10.1016/j.matpr.2020.08.386,journal,Materials Today: Proceedings,sciencedirect,2020-12-31,sciencedirect,Detection and prognosis of diabetes based on data science techniques,https://api.elsevier.com/content/article/pii/S2214785320362581,"
                  Diabetes is such a complicated disease that it is not just chronic but damages the blood glucose processing system of human body also. This requires a regular medication. As per International Diabetes Federation, there were about 463 million patients diagnosed with diabetes in 2019 and the figure may reach 700 million by 2045 approximately. If diabetes is not noticed in time, it will result in fatal effect in a person by damaging the eyes, blood vessels, kidneys, nerves, and heart. To stop progress this illness to the said complication level, it must be timely detected. In this task, a neural network algorithm is developed combined with machine learning applications to detect the diabetes timely. A data sample was used for training the neural network based on the various classification algorithms which are K-Nearest Neighbours, Support Vector Machine, Decision Tree, Gradient Boost, Linear Regression, and Random Forest, and various implicit layers to detect the disease possibility in advance. Python programming language was used for developing the model. Different sets of test data were used to check the accuracy of different algorithms. Error percentages were reported for all the algorithms with different hidden layers, to choose the best algorithm with optimality of hidden layers which can be used for diabetic prognosis with less possible error.
               ",autonomous vehicle
10.1016/j.promfg.2018.07.143,journal,Procedia Manufacturing,sciencedirect,2018-12-31,sciencedirect,Interdisciplinary Data Driven Production Process Analysis for the Internet of Production,https://api.elsevier.com/content/article/pii/S2351978918308199,"Recent developments in the industrial field are strongly influenced by requirements of the fourth industrial revolution (I4.0) for modern Cyber-Physical Production Systems (CPPS) and the coherent phenomenon of industrial big data (IBD). I4.0 is characterized by a growing amount of interdisciplinary work and cross-domain exchange of methods and knowledge. Similar to the development of the Internet of Things (IoT) for the consumer market, the emergence of an Internet of Production (IoP) in the industrial field is imminent. The future vision for an IoP is based on aggregated, multi-perspective and persistent data sets that can be seamlessly and semantically integrated to allow diagnosis and prediction in domain-specific real-time. In this paper, we demonstrate an exemplary scenario of collaborative cross-domain work, in which domain-experts from largely different fields of expertise, i.e. heavy plate rolling (HPR), injection molding (IM) and machine learning (ML), generate insights through data driven process analysis in two use cases. Specifically, in the HPR use case, reinforcement-learning was utilized to support the planning phase of the process aiming to reduce manual work load and to ultimately generate process plans that serve as a foundation for a simulation to calculate process results. On the contrary, in the IM use case, supervised-learning was utilized to learn a complex and computationally demanding finite element simulation model in order to predict process results for unknown process configurations, which can be used to optimize the process planning phase. While both use cases had the overall goal to utilize ML to gain new insights about the respective process, the actual ML application was utilized with reversed purpose. Particularly, in the HPR use case, ML was used to learn the process planning in order to calculate process results while in the IM use case, ML was used to predict process results in order to improve the process planning. We facilitate the communication between physically separated domain experts and the exchange of gained insights in the respective use cases by a framework that addresses the specific needs of cross-domain collaboration. We show that the insights gained from two largely different use cases are valuable to the domain experts of the other respective use case, facilitating cross-domain data driven production process analysis for future IoP scenarios.",autonomous vehicle
10.1016/S1474-6670(17)50880-0,journal,IFAC Proceedings Volumes,sciencedirect,1992-05-31,sciencedirect,Intelligent Estimation and Prediction for Systems Control and Decision - Methodology and Applications,https://api.elsevier.com/content/article/pii/S1474667017508800,"
                  This paper presents the concept, architecture and methodology of intelligent estimation and prediction (IEP) in systems control and decision, particularly the applications of both supervised and unsupervised learning in IEP. The IEP as described is mathematical model free estimation and prediction technique, and mainly based on knowledge base, fuzzy logic, artificial neural network and their combination. The major characteristics of the intelligent” in the proposed IEP system are learning and self-organizing in order to provide robust and adaptive system behavior. A few working examples and potential applications of IEP are also addressed in this paper.
               ",autonomous vehicle
10.1016/j.neucom.2020.04.132,journal,Neurocomputing,sciencedirect,2020-09-10,sciencedirect,Deep attentive and semantic preserving video summarization,https://api.elsevier.com/content/article/pii/S0925231220307797,"
                  Video summarization shortens a lengthy video into a succinct version, whose challenges mainly originate from the difficulties of discovering the inherent relations between the original video and its summary, meanwhile minimizing the semantic information loss. Supervised approaches, especially those in deep learning framework, have demonstrated their effectiveness in video summarization. However, these approaches mainly focus on one of the challenges, and seldom pay close attention to both challenges simultaneously. To this end, we propose to pay close attention to this deficiency by incorporating the ideas of both the encoder-decoder attention and semantic preserving loss in a deep Seq2Seq framework for video summarization. Moreover, we also introduce Huber loss to replace the popular mean square error loss to enhance the robustness of the model to outliers. Extensive experiments on two benchmark video summarization datasets demonstrate that the proposed approach consistently outperforms the state-of-the-art ones.
               ",autonomous vehicle
10.1016/j.neunet.2009.03.015,journal,Neural Networks,sciencedirect,2009-04-30,sciencedirect,Exploiting co-adaptation for the design of symbiotic neuroprosthetic assistants,https://api.elsevier.com/content/article/pii/S0893608009000483,"
                  The success of brain–machine interfaces (BMI) is enabled by the remarkable ability of the brain to incorporate the artificial neuroprosthetic ‘tool’ into its own cognitive space and use it as an extension of the user’s body. Unlike other tools, neuroprosthetics create a shared space that seamlessly spans the user’s internal goal representation of the world and the external physical environment enabling a much deeper human–tool symbiosis. A key factor in the transformation of ‘simple tools’ into ‘intelligent tools’ is the concept of co-adaptation where the tool becomes functionally involved in the extraction and definition of the user’s goals. Recent advancements in the neuroscience and engineering of neuroprosthetics are providing a blueprint for how new co-adaptive designs based on reinforcement learning change the nature of a user’s ability to accomplish tasks that were not possible using conventional methodologies. By designing adaptive controls and artificial intelligence into the neural interface, tools can become active assistants in goal-directed behavior and further enhance human performance in particular for the disabled population. This paper presents recent advances in computational and neural systems supporting the development of symbiotic neuroprosthetic assistants.
               ",autonomous vehicle
10.1016/B978-0-12-819620-5.00008-4,journal,"Implementing Biomedical Innovations into Health, Education, and Practice",sciencedirect,2020-12-31,sciencedirect,Chapter 8: Technology and computing,https://api.elsevier.com/content/article/pii/B9780128196205000084,"
               Advances in technology have touched virtually every aspect of our daily lives. Well chronicled are the disruptions to industries ranging from banking to retail to information dissemination. While medicine has a long history of embracing technology, the clinical practice of medicine has not experienced major disruption. This will change. Under the encompassing term artificial intelligence—AI—are an array of technologies. The development of ever more sophisticated sensors will allow 24×7×365 monitoring of chronic diseases. Advances in computing will enable the widespread integration of machine learning derived algorithms into clinical settings, ranging from intensive care units to risk identification in ambulatory populations. The continued development of not only macro robots but also micro and nano bots will facilitate diagnostic and therapeutic interventions while minimizing the need for invasive procedures.
               While it is anticipated that AI will disrupt the practice of medicine, the potential for harm to patients requires that robust safeguards in the development of these technologies be followed.
            ",autonomous vehicle
10.1016/j.pecs.2008.01.001,journal,Progress in Energy and Combustion Science,sciencedirect,2008-10-31,sciencedirect,Artificial intelligence techniques for photovoltaic applications: A review,https://api.elsevier.com/content/article/pii/S0360128508000026,"
                  Artificial intelligence (AI) techniques are becoming useful as alternate approaches to conventional techniques or as components of integrated systems. They have been used to solve complicated practical problems in various areas and are becoming more popular nowadays. They can learn from examples, are fault tolerant in the sense that they are able to handle noisy and incomplete data, are able to deal with nonlinear problems and once trained can perform prediction and generalization at high speed. AI-based systems are being developed and deployed worldwide in a wide variety of applications, mainly because of their symbolic reasoning, flexibility and explanation capabilities. AI has been used in different sectors, such as engineering, economics, medicine, military, marine, etc. They have also been applied for modeling, identification, optimization, prediction, forecasting and control of complex systems. The paper outlines an understanding of how AI systems operate by way of presenting a number of problems in photovoltaic systems application. Problems presented include three areas: forecasting and modeling of meteorological data, sizing of photovoltaic systems and modeling, simulation and control of photovoltaic systems. Published literature presented in this paper show the potential of AI as design tool in photovoltaic systems.
               ",autonomous vehicle
10.1016/j.asoc.2021.107977,journal,Applied Soft Computing,sciencedirect,2021-12-31,sciencedirect,The landscape of soft computing applications for terrorism analysis: A review,https://api.elsevier.com/content/article/pii/S1568494621008991,"
                  Terrorism is a globally prevalent dreaded form of crime against humanity in modern civil society. The nature of surprise, casualties caused, and the panic involved in terrorist activities compels improvisation of efforts to counter them. These counter-terrorism efforts require precise and reliable techniques to analyze the patterns existing in data of previous terrorist activities. Such patterns can reveal vital information for predicting details of upcoming attacks. Structures of terrorist networks and their operational specifics are among such attack details that deserve critical analysis by specialized applications. Most of these applications used for analyzing terrorism data are based on computational methods articulated under the broad term of soft computing techniques. In this paper, we review various aspects of soft computing applications developed for the analysis of terrorism data. Initiating with an in-depth discussion on the databases of terrorist event data, we propose 6 criteria for their quality evaluation. We proceed by elaborating the utilities of a prospective terrorism analysis application. These utilities include forecasting, detection and link mapping of terrorist activities. In the core of this review, we present a categorization of soft computing techniques into 3 major components; approximate reasoning, metaheuristic optimization and machine learning. A rich volume of applications for terrorism analysis has been discussed and compared on the scale of these techniques and their subcategories. Among these applications, while metaheuristic approaches present results to a precision of 90%, machine learning classifier methods also depict a classification accuracy of up to 93% in their outputs. Later, we discuss the perceived challenges in current literature, their consequential inclinations of research, and suggestions for directions of possible future developments. Finally, we conclude this review with a summary of the current state-of-art and critical comment on open opportunities in terrorism analysis.
               ",autonomous vehicle
10.1016/B978-0-12-804642-5.00004-9,journal,Intelligent Digital Oil and Gas Fields,sciencedirect,2018-12-31,sciencedirect,Chapter Four: Components of Artificial Intelligence and Data Analytics,https://api.elsevier.com/content/article/pii/B9780128046425000049,"
               As asset yields become harder to assess, extract, and forecast, oil and gas operating companies and service providers must enable real-time decision-making to better predict business outcomes that drive higher efficiencies and utilization to achieve improved bottom-line results and profitability. With the continued worldwide expansion of the digital oil field (DOF), the exploration and production (E&P) industry is rapidly becoming an information- and data-driven business. If we accept the prediction that the DOF market will exceed $30 billion by the year 2020 along with exponential growth in volume and complexity of acquired data, the E&P industry needs to rapidly adopt the new generation of digital transformation, technology, and processes that include:
                     
                        •
                        implementation of large-scale, big data-driven advanced analytics, integrated into role-centric, and relevant time workflows;
                     
                     
                        •
                        delivery of holistic ability for capture, classification, integration, and interpretation of all relevant and disparate data sources (geological, engineering, production, equipment, performance, etc.), regardless of origin or structure; and
                     
                     
                        •
                        ability to understand advanced analytical trends and correlation models to quickly and efficiently unlock the “hidden” knowledge from all datasets—from small data to large-scale and complex data as well as from historic repositories and databases or from fast streaming data.
                     
                  
               
            ",autonomous vehicle
10.1016/S0022-3115(98)00464-4,journal,Journal of Nuclear Materials,sciencedirect,1999-01-01,sciencedirect,Nuclear fuel pellet inspection using artificial neural networks,https://api.elsevier.com/content/article/pii/S0022311598004644,"
                  Nuclear fuel must be of high quality before being placed into service in a reactor. Fuel vendors currently use manual inspection for quality control of fabricated nuclear fuel pellets. In order to reduce workers' exposure to radiation and increase the inspection accuracy and speed, the feasibility of automation of fuel pellet inspection using artificial neural networks (ANNs) is studied in this paper. Three kinds of neural network architectures are examined for evaluation of the ANN performance in proper classification of good versus bad pellets. Two supervised neural networks, backpropagation and fuzzy ARTMAP, and one unsupervised neural network called ART2-A are applied. The results indicate that a supervised ANN with adequate training can achieve a high success rate in classification of fuel pellets.
               ",autonomous vehicle
10.1016/B978-012443875-0/50010-0,journal,Knowledge-Based Systems,sciencedirect,2000-12-31,sciencedirect,9: Knowledge Acquisition Via Bottom-up Learning,https://api.elsevier.com/content/article/pii/B9780124438750500100,"
               Different from existing models of skill learning that use a top-down approach—that is, from declarative knowledge to procedural knowledge or from explicit knowledge to implicit knowledge—this chapter proposes a bottom-up approach toward skill learning, which goes from procedural to declarative knowledge. The approach is demonstrated in the chapter using the CLARION (connectionist learning with adaptive rule induction online) model. The model based on the two-level dual-representation framework proposed that learns both types of knowledge in a bottom-up fashion by integrating connectionist, reinforcement, and symbolic learning methods to perform online learning and by using a combination of localist and distributed representation. It taps into the synergy of the two types of processes. Analyses of the literature on implicit versus explicit learning as well as other types of skill acquisition, including those in developmental psychology, are carried out to highlight the cognitive plausibility of our approach and its fit with human data. This chapter reviews human experiments that involve bottom-up skill learning while also describing bottom-up skill learning, as captured in a (simplified) computational model of the bottom-up approach, based on many related empirical and theoretical considerations. The chapter analyzes the approach (and especially the model) in terms of its ability to support important phenomena in bottom-up skill learning. The chapter highlights the commonalities and differences between the present approach and existing models of skill learning.
            ",autonomous vehicle
10.1016/j.robot.2017.04.005,journal,Robotics and Autonomous Systems,sciencedirect,2017-07-31,sciencedirect,Neural networks to increase the autonomy of interplanetary nanosatellite missions,https://api.elsevier.com/content/article/pii/S0921889016304419,"
                  To accomplish more ambitious scientific goals of interplanetary nanosatellite missions, a certain set of technological challenges need to be addressed to enhance systems performance. An area of particular interest is mission autonomy. Increasing the degree of mission autonomy might help overcoming the limitations imposed by the typical low data rate and the simplified ground segment of small missions. This research aims at supporting the development of more autonomous spacecraft by exploiting the potentialities of artificial intelligence. An artificial neural network is proposed to enable a set of autonomous operations, for example to select what payload data are useful for the mission and must be sent to Earth, and what data can be discarded. The algorithm is developed and tested on a case study represented by a CubeSat mission to a near Earth asteroid that requires the autonomous detection of an impact event on the asteroid surface. The proposed algorithm demonstrates the feasibility of a novel training approach based on optimized datasets created directly in-situ using images taken by the spacecraft on-board camera. The validity of the algorithm is demonstrated through several simulations, considering different scenarios and disturbances. The research presented in this paper can be extended to other applications of the artificial neural networks, such as autonomous failure detection and isolation, also in conjunction with other artificial intelligence approaches.
               ",autonomous vehicle
10.1016/j.chemolab.2021.104305,journal,Chemometrics and Intelligent Laboratory Systems,sciencedirect,2021-06-15,sciencedirect,Clustering-based hybrid feature selection approach for high dimensional microarray data,https://api.elsevier.com/content/article/pii/S0169743921000733,"
                  The DNA microarrays are used to monitor the expression levels of significant genes. Most of the microarray data are assumed to be high dimensional, redundant, and noisy. This paper proposed a clustering-based hybrid gene selection approach to reduce the high dimensionality and increase the classification accuracy of cancer microarray data. The proposed approach uses the combined method of k-means clustering algorithm and signal-to-noise-ratio ranking method as a primary filtering method to reduce the high dimensionality of the microarray dataset. A cellular learning automaton combined with ant colony optimization is then applied on the reduced dataset as a wrapper method to get the optimized gene subset. The classifiers adopted to evaluate the proposed method are support vector machine, K-nearest neighbor, and Naive Bayes. The experiments showed promising results in gene subset selection and classification.
               ",autonomous vehicle
10.1016/S0004-3702(97)00078-7,journal,Artificial Intelligence,sciencedirect,1998-02-28,sciencedirect,Learning metric-topological maps for indoor mobile robot navigation,https://api.elsevier.com/content/article/pii/S0004370297000787,"Autonomous robots must be able to learn and maintain models of their environments. Research on mobile robot navigation has produced two major paradigms for mapping indoor environments: grid-based and topological. While grid-based methods produce accurate metric maps, their complexity often prohibits efficient planning and problem solving in large-scale indoor environments. Topological maps, on the other hand, can be used much more efficiently, yet accurate and consistent topological maps are often difficult to learn and maintain in large-scale environments, particularly if momentary sensor data is highly ambiguous. This paper describes an approach that integrates both paradigms: grid-based and topological. Grid-based maps are learned using artificial neural networks and naive Bayesian integration. Topological maps are generated on top of the grid-based maps, by partitioning the latter into coherent regions. By combining both paradigms, the approach presented here gains advantages from both worlds: accuracy/consistency and efficiency. The paper gives results for autonomous exploration, mapping and operation of a mobile robot in populated multi-room environments.",autonomous vehicle
10.1016/j.asoc.2020.106236,journal,Applied Soft Computing,sciencedirect,2020-06-30,sciencedirect,Evaluating deep models for absenteeism prediction of public security agents,https://api.elsevier.com/content/article/pii/S1568494620301769,"
                  Absenteeism is a complex phenomenon characterized by the physical absence of the individual, usually at his workplace. Such absences generally lead to innumerable personal, social, and economic losses, particularly in public security institutions, where incidence is higher than the one verified in other occupational categories. Identifying preponderant absenteeism factors and allowing preventive actions to be carried out effectively may be beneficial to these institutions and their agents. Such knowledge could be acquired hypothetically by exploiting large human resources data sets. In this paper, we investigate the potential of machine learning classifiers to identify security workers prone to long-term absenteeism. Such predictors shall make decisions based on the professional history of each agent, which is extracted from databases of public security institutions. In our study, we performed experiments on a database comprised of 6 years of professional data from workers of the Military Police of Alagoas, Brazil. We evaluated deep models, including variations of Multilayer Perceptrons (MLP), Recurrent Neural Networks (RNN) and Long Short-Term Memory (LSTM), and compared with baseline Support-Vector Machines (SVM) classifiers. We show results revealing that the best architectures achieve up to 78% of accuracy. Also, experiments indicated that the use of data accumulated over several years improves the accuracy of the prediction of absenteeism. Finally, we conclude that such results encourage the usage of deep learning techniques to predict absenteeism and support the implementation of effective prevention measures in these institutions.
               ",autonomous vehicle
10.1016/j.ijpharm.2021.120554,journal,International Journal of Pharmaceutics,sciencedirect,2021-06-01,sciencedirect,Industry 4.0 for pharmaceutical manufacturing: Preparing for the smart factories of the future,https://api.elsevier.com/content/article/pii/S0378517321003598,"Over the last two centuries, medicines have evolved from crude herbal and botanical preparations into more complex manufacturing of sophisticated drug products and dosage forms. Along with the evolution of medicines, the manufacturing practices for their production have advanced from small-scale manual processing with simple tools to large-scale production as part of a trillion-dollar pharmaceutical industry. Today’s pharmaceutical manufacturing technologies continue to evolve as the internet of things, artificial intelligence, robotics, and advanced computing begin to challenge the traditional approaches, practices, and business models for the manufacture of pharmaceuticals. The application of these technologies has the potential to dramatically increase the agility, efficiency, flexibility, and quality of the industrial production of medicines. How these technologies are deployed on the journey from data collection to the hallmark digital maturity of Industry 4.0 will define the next generation of pharmaceutical manufacturing. Acheiving the benefits of this future requires a vision for it and an understanding of the extant regulatory, technical, and logistical barriers to realizing it.",autonomous vehicle
10.1016/S0149-1970(00)00158-X,journal,Progress in Nuclear Energy,sciencedirect,2001-12-31,sciencedirect,Traditional signal pattern recognition versus artificial neural networks for nuclear plant diagnostics,https://api.elsevier.com/content/article/pii/S014919700000158X,"
                  This paper introduces a new tool based on a traditional noise analysis techniques for monitoring reactor components' signal condition. It also presents the performance of artificial neural networks for pattern recognition to the same set of reactor signals and provides a comparison of these two techniques. Reactor pump signals from the Experimental Breeder Reactor (EBR-II) are utilized here. Collected signals such as pump power, pump speed, and pump pressure are obtained from already installed sensors in the reactor. The signals utilized are collected signals as well as generated signals simulating the pump shaft degradation progress. From the study of time series analysis and regression modeling of these signals, a parameter related to degradation and material buildup in the shaft is identified and used in the development of a monitoring tool. The results are then used as a benchmark against which to test the performance of artificial neural networks as a tool for reactor diagnostics.
                  Several neural networks are examined in this study, including Restricted Coulomb Energy (RCE), Cascade Correlation, and Backpropagation paradigms of artificial neural networks. RCE is selected due to its unique design and speed, Backpropagation is selected because it is widely used and well accepted in the neural network research community, and Cascade correlation is selected because it overcomes some of the problems associated with the Backpropagation paradigm. Similar study is performed using the Adaptive Resonance Theory (ART) family of neural network paradigms.
                  The results of this study indicate that artificial neural networks are simpler techniques for pattern recognition than noise analysis techniques such as the one introduced here. Neural networks do not require prior fault related parameter identification; they generate their own rules by learning from being shown examples. On the other hand, noise analysis and regression modeling can provide very sensitive techniques for monitoring of a detected problem in a component.
               ",autonomous vehicle
10.1016/j.trac.2020.116094,journal,TrAC Trends in Analytical Chemistry,sciencedirect,2020-12-31,sciencedirect,ICP-MS and trace element analysis as tools for better understanding medical conditions,https://api.elsevier.com/content/article/pii/S016599362030323X,"
                  Element constitution and distribution in tissues and body fluids have increasingly become key pieces of information in life sciences and medicine, and trace elements may be successfully used as disease biomarkers. Here, we review the most recent advances in inductively coupled plasma mass spectrometry (ICP-MS) and the related state-of-the-art instrumentation and methods (e.g. single-particle and single-cell determination capabilities) used to expand the application of trace element information to the study of diseases. Advanced statistical tools and machine learning used for evaluating, diagnosing, and treating different diseases has highlighted the importance of trace elements in clinical research. In this manuscript, we review recently published studies involving trace element analysis and machine learning applied to better understanding clinical conditions and pathologies, and discuss some perspectives for this field.
               ",autonomous vehicle
10.1016/j.solener.2020.11.051,journal,Solar Energy,sciencedirect,2021-01-15,sciencedirect,Self-organizing profiles to characterize representative temporal settings for daylight simulations,https://api.elsevier.com/content/article/pii/S0038092X2031210X,"
                  While daylight studies vary in scale and complexity, they follow almost identical procedure that yields higher spatial and temporal dimensionality of luminance/illuminance values, which should be reduced into simpler performance metrics. Several complexities arise from depending solely on performance metrics that do not neither agree on unified thresholds, nor report luminous variations spaces might experience due to weather fluctuations. Few research attempts addressed those issues using predefined timesteps. However, they capture instantaneous performance samples of spaces to which they can only be applied, considering fractions of sky conditions that do not offer generalization. This research expands previous endeavors and presents an unprecedented approach that employs unsupervised machine learning to characterize the most representative temporal settings of given locations from widely accessible weather datasets to evaluate internal luminous conditions. The strengths of three algorithms are combined: the ability of Principal Component Analysis to reduce dimensionality, with Self-Organizing Maps and K-means to cluster the reduced data. To exemplify the proposed approach, three locations are investigated, expressing diverse sky conditions. Each of which reflected unique clustering patterns that were translated into temporal profile maps for visualization. This would provide architects and policy makers with methodology to facilitate building performance simulation and design.
               ",autonomous vehicle
10.1016/j.neucom.2018.02.100,journal,Neurocomputing,sciencedirect,2019-02-07,sciencedirect,Review of classical dimensionality reduction and sample selection methods for large-scale data processing,https://api.elsevier.com/content/article/pii/S0925231218309469,"
                  In the era of big data, all types of data with increasing samples and high-dimensional attributes are demonstrating their important roles in various fields, such as data mining, pattern recognition and machine learning, etc. Meanwhile, machine learning algorithms are being effectively applied in large-scale data processing. This paper mainly reviews the classical dimensionality reduction and sample selection methods based on machine learning algorithms for large-scale data processing. Firstly, the paper provides a brief overview to the classical sample selection and dimensionality reduction methods. Then, it pays attention to the applications of those methods and their combinations with the classical machine learning methods, such as clustering, random forest, fuzzy set, and heuristic algorithms, particularly deep leaning methods. Furthermore, the paper primarily introduces the application frameworks that combine sample selection and dimensionality reduction in the context of two aspects: sequential and simultaneous, which almost all get the ideal results in the processing of the large-scale training data contrasting to the original models. Lastly, we further conclude that sample selection and dimensionality reduction methods are essential and effective for the modern large-scale data processing. In the future work, the machine learning algorithms, especially the deep learning methods, will play a more important role in the processing of large-scale data.
               ",autonomous vehicle
10.1016/j.ins.2021.08.059,journal,Information Sciences,sciencedirect,2021-11-30,sciencedirect,Rumor knowledge embedding based data augmentation for imbalanced rumor detection,https://api.elsevier.com/content/article/pii/S0020025521008677,"
                  Rumor detection aims to detect rumors in a timely manner to prevent malicious rumors from misleading the public and disrupting social order. However, rumor detection suffers from the problem of imbalanced data. Existing methods of text generation and imbalanced learning are insufficient in addressing this imbalance because they are not specialized in rumor tasks. We propose a knowledge graph-based rumor data augmentation method: Graph Embedding-based Rumor Data Augmentation (GERDA), which simulates the generation process of rumor from the perspective of knowledge. To model the generation process of false information, we introduce knowledge representation in the process of text generation. To better learn the graph structured rumor data, we propose a graph-based rumor text generative model G2S-AT-GAN, which uses an attention-based graph convolutional neural network and a generative adversarial network for rumor text generation. Experiments show that our method is able to generate high-quality rumors of diverse topics and the generated rumors can further address rumor data imbalance for better performance in rumor detection.
               ",autonomous vehicle
10.1016/S0167-7012(00)00201-3,journal,Journal of Microbiological Methods,sciencedirect,2000-12-01,sciencedirect,"Artificial neural networks: fundamentals, computing, design, and application",https://api.elsevier.com/content/article/pii/S0167701200002013,"
                  Artificial neural networks (ANNs) are relatively new computational tools that have found extensive utilization in solving many complex real-world problems. The attractiveness of ANNs comes from their remarkable information processing characteristics pertinent mainly to nonlinearity, high parallelism, fault and noise tolerance, and learning and generalization capabilities. This paper aims to familiarize the reader with ANN-based computing (neurocomputing) and to serve as a useful companion practical guide and toolkit for the ANNs modeler along the course of ANN project development. The history of the evolution of neurocomputing and its relation to the field of neurobiology is briefly discussed. ANNs are compared to both expert systems and statistical regression and their advantages and limitations are outlined. A bird’s eye review of the various types of ANNs and the related learning rules is presented, with special emphasis on backpropagation (BP) ANNs theory and design. A generalized methodology for developing successful ANNs projects from conceptualization, to design, to implementation, is described. The most common problems that BPANNs developers face during training are summarized in conjunction with possible causes and remedies. Finally, as a practical application, BPANNs were used to model the microbial growth curves of S. flexneri. The developed model was reasonably accurate in simulating both training and test time-dependent growth curves as affected by temperature and pH.
               ",autonomous vehicle
10.1016/j.fmre.2021.08.017,journal,Fundamental Research,sciencedirect,2021-09-30,sciencedirect,Combating emerging financial risks in the big data era: A perspective review,https://api.elsevier.com/content/article/pii/S2667325821001722,"Big data technology has had a significant impact on new business and financial services: for example, GPS and Bluetooth inspire location-based services, and search and web technologies motivate online shopping, reviews, and payments. These business services have become more connected than ever, and as a result, financial frauds have become a significant challenge. Therefore, combating financial risks in the big data era requires breaking the borders of traditional data, algorithms, and systems. An increasing number of studies have addressed these challenges and proposed new methods for risk detection, assessment, and forecasting. As a key contribution, we categorize these works in a rational framework: first, we identify the data that can be used to identify risks. We then discuss how big data can be combined with the emerging tools to effectively learn or analyze financial risk. Finally, we highlight the effectiveness of these methods in real-world applications. Furthermore, we stress on the importance of utilizing multi-channel information, graphs, and networks of long-range dependence for the effective identification of financial risks. We conclude our survey with a discussion on the new challenges faced by the financial sector, namely, deep fake technology, adversaries, causal and interpretable inference, privacy protection, and microsimulations.",autonomous vehicle
10.1016/j.artmed.2010.05.004,journal,Artificial Intelligence in Medicine,sciencedirect,2010-11-30,sciencedirect,Multi-step dimensionality reduction and semi-supervised graph-based tumor classification using gene expression data,https://api.elsevier.com/content/article/pii/S0933365710000692,"
                  Objective
                  Both supervised methods and unsupervised methods have been widely used to solve the tumor classification problem based on gene expression profiles. This paper introduces a semi-supervised graph-based method for tumor classification. Feature extraction plays a key role in tumor classification based on gene expression profiles, and can greatly improve the performance of a classifier. In this paper we propose a novel multi-step dimensionality reduction method for extracting tumor-related features.
               
                  Methods and materials
                  First the Wilcoxon rank-sum test is used for gene selection. Then gene ranking and discrete cosine transform are combined with principal component analysis for feature extraction. Finally, the performance is evaluated by semi-supervised learning algorithms.
               
                  Results
                  To show the validity of the proposed method, we apply it to classify four tumor datasets involving various human normal and tumor tissue samples. The experimental results show that the proposed method is efficient and feasible. Compared with other methods, our method can achieve relatively higher prediction accuracy. Particularly, it is found that semi-supervised method is superior to support vector machines in classification performance.
               
                  Conclusions
                  The proposed approach can effectively improve the performance of tumor classification based on gene expression profiles. This work is a meaningful attempt to explore and apply multi-step dimensionality reduction and semi-supervised learning methods in the field of tumor classification. Considering the high classification accuracy, there should be much room for the application of multi-step dimensionality reduction and semi-supervised learning methods to perform tumor classification.
               ",autonomous vehicle
10.1016/j.eng.2020.01.011,journal,Engineering,sciencedirect,2020-03-31,sciencedirect,"Dark, Beyond Deep: A Paradigm Shift to Cognitive AI with Humanlike Common Sense",https://api.elsevier.com/content/article/pii/S2095809920300345,"Recent progress in deep learning is essentially based on a “big data for small tasks” paradigm, under which massive amounts of data are used to train a classifier for a single narrow task. In this paper, we call for a shift that flips this paradigm upside down. Specifically, we propose a “small data for big tasks” paradigm, wherein a single artificial intelligence (AI) system is challenged to develop “common sense,” enabling it to solve a wide range of tasks with little training data. We illustrate the potential power of this new paradigm by reviewing models of common sense that synthesize recent breakthroughs in both machine and human vision. We identify functionality, physics, intent, causality, and utility (FPICU) as the five core domains of cognitive AI with humanlike common sense. When taken as a unified concept, FPICU is concerned with the questions of “why” and “how,” beyond the dominant “what” and “where” framework for understanding vision. They are invisible in terms of pixels but nevertheless drive the creation, maintenance, and development of visual scenes. We therefore coin them the “dark matter” of vision. Just as our universe cannot be understood by merely studying observable matter, we argue that vision cannot be understood without studying FPICU. We demonstrate the power of this perspective to develop cognitive AI systems with humanlike common sense by showing how to observe and apply FPICU with little training data to solve a wide range of challenging tasks, including tool use, planning, utility inference, and social learning. In summary, we argue that the next generation of AI must embrace “dark” humanlike common sense for solving novel tasks.",autonomous vehicle
10.1016/j.sysarc.2019.06.001,journal,Journal of Systems Architecture,sciencedirect,2019-09-30,sciencedirect,Empirical model-based performance prediction for application mapping on multicore architectures,https://api.elsevier.com/content/article/pii/S1383762118306465,"
                  Application mapping in multicore embedded systems plays a central role in their energy-efficiency. The present paper deals with this issue by focusing on the prediction of performance and energy consumption, induced by task and data allocation on computing resources. It proposes a solution by answering three fundamental questions as follows: (i) how to encode mappings for training performance prediction models? (ii) how to define an adequate criterion for assessing the quality of mapping performance predictors? and (iii) which technique among regression and classification enables the best predictions? Here, the prediction models are obtained by applying carefully selected supervised machine learning techniques on raw data, generated off-line from system executions. These techniques are Support Vector Machines, Adaptive Boosting (AdaBoost) and Artificial Neural Networks (ANNs). Our study is validated on an automotive application case study. The experimental results show that with a limited set of training information, AdaBoost and ANNs can provide very good outcomes (up to 84.8% and 89.05% correct prediction score in some cases, respectively), making them attractive enough for the addressed problem.
               ",autonomous vehicle
10.1016/j.measurement.2018.11.031,journal,Measurement,sciencedirect,2019-03-31,sciencedirect,Application of neural networks and fuzzy systems for the intelligent prediction of CO<ce:inf loc=post>2</ce:inf>-induced strength alteration of coal,https://api.elsevier.com/content/article/pii/S0263224118310856,"
                  CO2 sequestration and enhanced coal bed methane (ECBM) extraction necessitate CO2 injection into coal reservoirs that affect the coal strength properties and long-term integrity of the seam. Evaluation of CO2-induced coal strength alterations is essential to minimize the reservoir damage. Advanced soft computing models have become prevalent in rock mechanics field, as they are capable of learning trends from complex data sets, preserving the experience and using it for predictions. We present two models viz. artificial neural network (ANN) and adoptive neuro-fuzzy inference system (ANFIS) to predict the strength alterations of coal, under various CO2 saturation conditions. Model performances are compared with linear and non-linear multivariate regression analyses (L-MRA and NL-MRA). We consider three effective input parameters (i.e. coal type, CO2 saturation pressure and CO2 interaction time) and one output parameter (i.e. unconfined compressive strength (UCS)) in the models. ANN consists of a three-layer feed-forward back-propagation network with a 3-5-1 architecture and ANFIS consists of [4 4 4] Gaussian type membership functions. Model results confirm that ANFIS has the highest prediction capacity followed by ANN, with R2 equal to 0.9954 and 0.9933, respectively. Both L-MRA and NL-MRA prediction performances are not satisfactory, as R2 values are only 0.7854 and 0.7821 for two models, respectively. Thus, general statistical models like MRA fail to precisely predict the complex strength alterations. From the verified models, we show that well-trained ANN and ANFIS models can successfully fit and forecast the experimental data, and are able to predict the long-term CO2 saturation effect on coal strength.
               ",autonomous vehicle
10.1016/B978-0-12-821229-5.00004-5,journal,Machine Learning and the Internet of Medical Things in Healthcare,sciencedirect,2021-12-31,sciencedirect,Chapter 9: Cancer prediction and diagnosis hinged on HCML in IOMT environment,https://api.elsevier.com/content/article/pii/B9780128212295000045,"
               Machine learning (ML) is a postulation of artificial intelligence (AI) to facilitate the supply system of rules with the capability to routinely learn and improve from occurrences without being unambiguously programmed. ML centers on the improvement of computer programs that are able to enter information. The basic assertion of ML is that algorithms can collect input data and use statistical investigation to predict an output at the same time as updating outputs as fresh data becomes accessible. Health care restores health by the treatment and prevention of disease particularly by trained and licensed professionals. The value of HCML is its facility to progress on huge datasets ahead of the scope of human capability, and then reliably convert analysis of that data into clinical insights that assist the medical practitioner in the preparation and furnishing of care, finally leading to improved outcomes. Applications of ML in healthcare are identifying diseases and diagnosis, drug discovery and manufacturing, medical imaging diagnosis, ML-based behavioral modification (MLBBM), smart health records, better radiotherapy, and outbreak prediction. Breast cancer (BC) is one of the most perilous types of diseases in the world and detecting this cancer in its initial stage helps in saving lives. Numerous women die every year of BC. ML algorithms can be accessible used for anticipation as well as designation of BC. Various ML algorithms are Naïve Bayes, Support Vector Machine, and K-Nearest Neighbor.
            ",autonomous vehicle
10.1016/j.future.2019.01.049,journal,Future Generation Computer Systems,sciencedirect,2019-09-30,sciencedirect,Evolution-based configuration optimization of a Deep Neural Network for the classification of Obstructive Sleep Apnea episodes,https://api.elsevier.com/content/article/pii/S0167739X1832805X,"
                  Deep Neural Networks (DNNs) may be very effective for the classification over highly-sized data sets, especially in the medical domain, where the recognition of the occurrence of a specific event related to a disease is of high importance. Unfortunately, DNNs suffer from the drawback that a good set of values for their configuration hyper-parameters must be found. Currently, this is done through the use of either trial-and-error methods or sampling-based ones. In this paper we propose a new approach to find the most suitable structure for a DNN used for a classification problem in terms of achievement of the highest classification accuracy. This approach is based on a distributed version of Differential Evolution (DE), a variety of an Evolutionary Algorithm. To evaluate the approach, in this paper we investigate this issue with reference to Obstructive Sleep Apnea (OSA). OSA is an important medical problem consisting of episodes taking place during night in which a subject stops breathing due to a constriction of the upper airways. This deteriorates the quality of life and may have dangerous, and even lethal, consequences on both short and long term. An accurate classification is a very crucial step for the OSA treatment, because understanding automatically that a subject is experiencing such an episode may be decisive if prompt medical action is needed. In our experiments, classification takes place on a data set in which each item contains the values of 17 Heart Rate Variability parameters, extracted from ElectroCardiography signals, and the annotation of OSA events. We have extracted this data set from the real-world Sleep Heart Health Study database. The results obtained by the distributed DE are compared against those of the Grid Search as well as against those achieved by 13 well-known classification tools. The use of a distributed DE version turns out to be very effective in automatically obtaining DNN structures with higher classification accuracy with respect to Grid Search (72.95% versus 72.61%), and allows saving a high amount of time (three hours as opposed to 65 h and 40 min). Moreover, the proposed method outperforms in terms of higher accuracy all the other classifiers investigated, as it is evidenced also by statistical analysis. Numerically, the runner-up, i.e., JRip, achieves as its best value 72.01% and 71.50% on average over 25 runs, both values being lower than 72.95% and 72.74% obtained by our dDE.
               ",autonomous vehicle
10.1016/B978-0-12-820472-6.00116-X,journal,Reference Module in Biomedical Sciences,sciencedirect,2021-12-31,sciencedirect,Magic bullets: Drug repositioning and drug combinations,https://api.elsevier.com/content/article/pii/B978012820472600116X,"
               Discovery and development of novel pharmaceuticals continue to be a very costly, time-consuming and uncertain process impacting negatively not only the research and development of pharmaceutical industry but also health care. Although the number of novel drugs approved each year has grown over by as much as 60% compared to the past decade, there are still many diseases that do not have any approved drug. Recent technological advances in the biomedical, genomics, and computational science domains accompanied by multisource and multidimensional data opened new opportunities and challenges. The drug discovery paradigm is increasingly shifting from hypothesis-driven to data-driven approaches. While the search for the magic bullets of medicine continues, the magic—crunching the data deluge into knowledge and hypotheses nuggets—is mostly driven by machines and machine intelligence. This review will primarily focus on three facets of computational drug discovery approaches, namely, drug repositioning, de novo drug discovery, and drug combinations, and reflect on computational approaches which are reproducible and seem most promising for the machine learning-driven drug discovery. Finally, using COVID-19 as an example, we discuss how the computational approaches are aiding and accelerating the process of discovery of magic bullet(s) for this dreadful pandemic.
            ",autonomous vehicle
10.1016/j.patcog.2019.107109,journal,Pattern Recognition,sciencedirect,2020-04-30,sciencedirect,Training data independent image registration using generative adversarial networks and domain adaptation,https://api.elsevier.com/content/article/pii/S0031320319304108,"
                  Medical image registration is an important task in automated analysis of multimodal images and temporal data involving multiple patient visits. Conventional approaches, although useful for different image types, are time consuming. Of late, deep learning (DL) based image registration methods have been proposed that outperform traditional methods in terms of accuracy and time. However, DL based methods are heavily dependent on training data and do not generalize well when presented with images of different scanners or anatomies. We present a DL based approach that can perform medical image registration of one image type despite being trained with images of a different type. This is achieved by unsupervised domain adaptation in the registration process and allows for easier application to different datasets without extensive retraining. To achieve our objective we train a network that transforms the given input image pair to a latent feature space vector using autoencoders. The resultant encoded feature space is used to generate the registered images with the help of generative adversarial networks (GANs). This feature transformation ensures greater invariance to the input image type. Experiments on chest X-ray, retinal and brain MR images show that our method, trained on one dataset gives better registration performance for other datasets, outperforming conventional methods that do not incorporate domain adaptation.
               ",autonomous vehicle
10.1533/9780857090195.1.125,journal,Colour Measurement,sciencedirect,2010-12-31,sciencedirect,5: Use of artificial neural networks (ANNs) in colour measurement,https://api.elsevier.com/content/article/pii/B9781845695590500042,"
               
                  An artificial neural network (ANN) is an information processing paradigm that is inspired by the way biological nervous systems, such as the brain, process information. ANNs are used for modelling non-linear problems and to predict the output values for given input parameters from their training values. Most of the coloration processes and the related quality assessments are non-linear in nature and hence neural networks find application in colour science. The conventional approaches used for assessing and predicting the colour parameters are based on the approximations to the physical processes actually taking place and this leads to inaccuracy. It has been suggested that ANNs could be used to predict the colour parameters and recipe, controlling of dyeing process, classification of dyes, etc., based on the colorant concentrations and spectral reflectances. The formulation of colour parameters and assessments using ANNs is claimed to have better accuracy compared to any other models.
            ",autonomous vehicle
10.1016/S1474-6670(17)50003-8,journal,IFAC Proceedings Volumes,sciencedirect,1992-08-31,sciencedirect,Artificial Fuzzy Neural Networks and their Application to Intelligent Robot Control Systems,https://api.elsevier.com/content/article/pii/S1474667017500038,"
                  Machine learning in an uncertain or unknown environment is of vital interest to those working with intelligent systems. The ability to gamer new information, process it, and increase the understanding/ capability of the machine is crucial to the performance of autonomous systems. The field of artificial intelligence provides two major approaches to the problem of knowledge engineering - expert systems and neural networks. Harnessing the power of these two techniques in a hybrid, cooperating system holds great promise.
               ",autonomous vehicle
10.1016/j.future.2017.10.021,journal,Future Generation Computer Systems,sciencedirect,2020-04-30,sciencedirect,Big Data analytics and Computational Intelligence for Cyber–Physical Systems: Recent trends and state of the art applications,https://api.elsevier.com/content/article/pii/S0167739X17323282,"
                  Big data is fuelling the digital revolution in an increasingly knowledge driven and connected society by offering big data analytics and computational intelligence based solutions to reduce the complexity and cognitive burden on accessing and processing large volumes of data. In this paper, we discuss the importance of big data analytics and computational intelligence techniques applied to data produced from the myriad of pervasively connected machines and personalized devices offering embedded and distributed information processing capabilities. We provide a comprehensive survey of computational intelligence techniques appropriate for the effective processing and analysis of big data. We discuss a number of exemplar application areas that generate big data and can hence benefit from its effective processing. State of the art research and novel applications in health-care, intelligent transportation and social network sentiment analysis, are presented and discussed in the context of Big data, Cyber–Physical Systems (CPS), and Computational Intelligence (CI). We present a data modelling methodology, which introduces a novel biologically inspired universal generative modelling approach called Hierarchical Spatial–Temporal State Machine (HSTSM). The HSTSM modelling approach incorporates a number of soft computing techniques such as: deep belief networks, auto-encoders, agglomerative hierarchical clustering and temporal sequence processing, in order to address the computational challenges arising from analysing and processing large volumes of diverse data to provide an effective big data analytics tool for diverse application areas. A conceptual cyber–physical architecture, which can accommodate and benefit from the proposed methodology, is further presented.
               ",autonomous vehicle
10.1016/j.engappai.2012.09.017,journal,Engineering Applications of Artificial Intelligence,sciencedirect,2013-03-31,sciencedirect,Machine learning of syntactic parse trees for search and classification of text,https://api.elsevier.com/content/article/pii/S0952197612002552,"
                  We build an open-source toolkit which implements deterministic learning to support search and text classification tasks. We extend the mechanism of logical generalization towards syntactic parse trees and attempt to detect weak semantic signals from them. Generalization of syntactic parse tree as a syntactic similarity measure is defined as the set of maximum common sub-trees and performed at a level of paragraphs, sentences, phrases and individual words. We analyze semantic features of such similarity measure and compare it with semantics of traditional anti-unification of terms. Nearest-neighbor machine learning is then applied to relate a sentence to a semantic class. Using syntactic parse tree-based similarity measure instead of bag-of-words and keyword frequency approach, we expect to detect a weak semantic signal otherwise unobservable. The proposed approach is evaluated in a four distinct domains where a lack of semantic information makes classification of sentences rather difficult. We describe a toolkit which is a part of Apache Software Foun-dation project OpenNLP, designed to aid search engineers in tasks requiring text relevance assessment.
               ",autonomous vehicle
10.1016/j.ijepes.2013.03.007,journal,International Journal of Electrical Power & Energy Systems,sciencedirect,2013-10-31,sciencedirect,Current state of neural networks applications in power system monitoring and control,https://api.elsevier.com/content/article/pii/S014206151300104X,"
                  For over two decades Neural Network (NN) has been applied to power system monitoring and control. Conventional controllers suffer from certain limitations which NN as an Artificial Intelligence (AI) technique is able to overcome. Therefore, many researchers prefer to use NN technique in the monitoring and control of power systems. This paper reviews published recently schemes for control and monitoring based on NN. The performance of various NN controllers is compared with one another as well as to the performance of other types of controllers. This review further reveals that the design of a proper NN control can maintain first-swing stability, damp oscillation, ensure voltage stability and the reliable supply of electric power.
               ",autonomous vehicle
10.1016/j.commtr.2021.100011,journal,Communications in Transportation Research,sciencedirect,2021-12-31,sciencedirect,Emerging approaches applied to maritime transport research: Past and future,https://api.elsevier.com/content/article/pii/S2772424721000111,"Maritime transport is the backbone of international trade and globalization. Maritime transport research can be roughly divided into two categories, namely the shipping side and the port side. Most of the classic approaches adopted to address practical problems in these research topics are based on long-term observations and expert knowledge, while few of them are based on historical data accumulated from practice. In recent years, emerging approaches, which we refer to as machine learning and deep learning techniques in this essay, have been receiving a wider attention to solve practical problems. As a relatively conservative industry, there are some initial trials of applying the emerging approaches to solve practical problems in the maritime sector. The objective of this essay is to review the application of emerging approaches to maritime transport research. The main research topics in maritime transport and classic methods developed to solve them are first presented. The introduction of emerging approaches and their suitability to be applied in maritime transport research is then discussed. Related existing studies are then reviewed according to problem settings, main data sources, and emerging approaches adopted. Challenges and solutions in the process are also discussed from the perspectives of data, model, users, and targets. Finally, promising future research directions are identified. This essay is the first to give a comprehensive review of existing studies on developing machine learning and deep learning models together with popular data sources used to address practical problems in maritime transport.",autonomous vehicle
10.1016/j.compeleceng.2020.106766,journal,Computers & Electrical Engineering,sciencedirect,2020-10-31,sciencedirect,Imputation of Missing Values Affecting the Software Performance of Component-based Robots,https://api.elsevier.com/content/article/pii/S0045790620306212,"
                  Intelligent robots are foreseen as a technology that would be soon present in most public and private environments. In order to increase the trust of humans, robotic systems must be reliable while both response and down times are minimized. In keeping with this idea, present paper proposes the application of machine learning (regression models more precisely) to preprocess data in order to improve the detection of failures. Such failures deeply affect the performance of the software components embedded in human-interacting robots. To address one of the most common problems of real-life datasets (missing values), some traditional (such as linear regression) as well as innovative (decision tree and neural network) models are applied. The aim is to impute missing values with minimum error in order to improve the quality of data and consequently maximize the failure-detection rate. Experiments are run on a public and up-to-date dataset and the obtained results support the viability of the proposed models.
               ",autonomous vehicle
10.1016/S1474-6670(17)46108-8,journal,IFAC Proceedings Volumes,sciencedirect,1994-06-30,sciencedirect,Combining Neural and Fuzzy Techniques in Monitoring and Control of Manufacturing Processes,https://api.elsevier.com/content/article/pii/S1474667017461088,"
                  Real-time nature, uncertainty handling and learning ability are essential requirements for knowledge representation and processing techniques to be applied at lower levels of intelligent manufacturing systems. The paper demonstrates and compares the applicability of neural networks and fuzzy techniques for monitoring and control of manufacturing processes. Combined use of neural and fuzzy techniques is illustrated, and the integration of such a hybrid system in an intelligent manufacturing environment is investigated.
               ",autonomous vehicle
10.1016/j.aei.2021.101246,journal,Advanced Engineering Informatics,sciencedirect,2021-01-31,sciencedirect,"A systematic literature review on intelligent automation: Aligning concepts from theory, practice, and future perspectives",https://api.elsevier.com/content/article/pii/S147403462100001X,"
                  With the recent developments in robotic process automation (RPA) and artificial intelligence (AI), academics and industrial practitioners are now pursuing robust and adaptive decision making (DM) in real-life engineering applications and automated business workflows and processes to accommodate context awareness, adaptation to environment and customisation. The emerging research via RPA, AI and soft computing offers sophisticated decision analysis methods, data-driven DM and scenario analysis with regard to the consideration of decision choices and provides benefits in numerous engineering applications. The emerging intelligent automation (IA) – the combination of RPA, AI and soft computing – can further transcend traditional DM to achieve unprecedented levels of operational efficiency, decision quality and system reliability. RPA allows an intelligent agent to eliminate operational errors and mimic manual routine decisions, including rule-based, well-structured and repetitive decisions involving enormous data, in a digital system, while AI has the cognitive capabilities to emulate the actions of human behaviour and process unstructured data via machine learning, natural language processing and image processing. Insights from IA drive new opportunities in providing automated DM processes, fault diagnosis, knowledge elicitation and solutions under complex decision environments with the presence of context-aware data, uncertainty and customer preferences. This sophisticated review attempts to deliver the relevant research directions and applications from the selected literature to the readers and address the key contributions of the selected literature, IA’s benefits, implementation considerations, challenges and potential IA applications to foster the relevant research development in the domain.
               ",autonomous vehicle
10.1016/j.jnca.2020.102537,journal,Journal of Network and Computer Applications,sciencedirect,2020-05-01,sciencedirect,Artificial Immune Systems approaches to secure the internet of things: A systematic review of the literature and recommendations for future research,https://api.elsevier.com/content/article/pii/S1084804520300114,"
                  As the Internet of Things (IoT) recently attains tremendous popularity, this promising technology leads to a variety of security challenges. The traditional solutions do not fit the new challenges brought by the IoT ecosystem. Although the development's area of Artificial Immune Systems (AIS) provides an opportunity to improve security issues and create a fertile and exciting environment for further research and experiments, there is not any systematic and comprehensive study about analyzing its importance for IoT environment. Therefore, this work aims to identify, evaluate, and perform a comprehensive study of empirical research on the studies of AIS approaches to secure the IoT environment. The relevant and high-quality studies are addressing using three research questions about the main research motivations, existing solutions, and future gaps and directions. The AIS approaches have been divided into three main categories based on IoT layers, and detailed classifications have also been included based on different parameters. To achieve this aim, the authors use a systematic literature review (SLR) as a powerful method to collect and critically analyze the research papers. Also, the authors discuss the selected studies and their main techniques, as well as their benefits and drawbacks in general. This research process strives to build a knowledge base for AIS solutions under the umbrella of IoT security and suggest directions for future research.
               ",autonomous vehicle
10.1016/j.compeleceng.2021.107044,journal,Computers & Electrical Engineering,sciencedirect,2021-05-31,sciencedirect,Intrusion detection in cyber-physical systems using a generic and domain specific deep autoencoder model,https://api.elsevier.com/content/article/pii/S0045790621000628,"
                  The rapid growth of network-related services in the last decade has produced a huge amount of sensitive data on the internet. But networks are very much prone to intrusions where unauthorized users attempt to access sensitive information and even disrupt the system. Building a competent network intrusion detection system (IDS) is necessary to prevent such attacks. IDSs generally use machine learning algorithms for classifying the attacks. But the features used for classification are not always suitable or sufficient. Besides, the number of intrusions is much less than the number of non-intrusions. Hence naive approaches may fail to provide acceptable performance due to this class imbalance. To counter this problem, in this paper, we propose a model that extracts useful features from the given features and then uses a deep learning algorithm to classify the intrusions. It is to be noted that underlying data points cannot be thought of as sampled from the same distribution, rather from two different distributions - one generic to all network intrusions, and the other specific to the domain. Keeping this fact in mind, we propose a unique Generic-Specific autoencoder architecture where the generic one learns the features that are common across all forms of network intrusions, and the specific ones learn features that are pertaining only to that domain. The model has been evaluated on the CICIDS2017 dataset, which is the largest dataset of this type available online, and we have set new benchmark results on this dataset. Source code of this work is available at: https://github.com/SoumyadeepThakur/Intrusion-AE
                  
               ",autonomous vehicle
10.1016/j.jallcom.2008.11.155,journal,Journal of Alloys and Compounds,sciencedirect,2009-06-10,sciencedirect,Artificial neural network modeling of the drilling process of self-lubricated aluminum/alumina/graphite hybrid composites synthesized by powder metallurgy technique,https://api.elsevier.com/content/article/pii/S0925838808021178,"
                  In recent years, the consumption of metal matrix composites (MMCs) materials in many engineering fields has increased enormously. Most industries are usually looking for replacement of ferrous components with lighter and high strength alloys like Al metal matrix composites. Despite the superior mechanical and thermal properties of particulate metal matrix composites (PMMCs), their poor machinability is the main drawback to their substitution to other metallic parts. Machining is a material removal process which is important for many stages prior to the application or assembling of the components. Accordingly, the need for accurate machining of composites has also increased tremendously. This study addresses the modeling of the machinability of self-lubricated aluminum/alumina/graphite hybrid composites synthesized by powder metallurgy (P/M). In the present work, a feed forward back propagation artificial neural network (ANN) system is used to investigate the influence of some parameters on the thrust force and cutting torque in the drilling processes. Experimental data collected were tested with artificial neural network technique. Multilayer perceptron model has been constructed with feed forward back propagation algorithm using the input parameters of cutting speed, cutting feed, and volume fraction of the reinforced particles. Output parameters were the thrust force and cutting torque. On completion of the experimental test, an ANN is used to validate the results obtained and also to predict the behavior of the system under any condition within its operating range. The predicted thrust force and cutting torque based on the ANN model were found to be in a very good agreement with the unexposed experimental data set. The modeling results confirm the feasibility of the ANN and its good correlation with the experimental results. The degrees of accuracy of the prediction were 93.24% and 94.17% for thrust force and cutting torque, respectively. It is concluded that ANN is an excellent analytical tool, which can be used for other machining processes, if it is well trained.
               ",autonomous vehicle
10.1016/j.neucom.2020.10.081,journal,Neurocomputing,sciencedirect,2021-03-14,sciencedirect,Deep face recognition: A survey,https://api.elsevier.com/content/article/pii/S0925231220316945,"
                  Deep learning applies multiple processing layers to learn representations of data with multiple levels of feature extraction. This emerging technique has reshaped the research landscape of face recognition (FR) since 2014, launched by the breakthroughs of DeepFace and DeepID. Since then, deep learning technique, characterized by the hierarchical architecture to stitch together pixels into invariant face representation, has dramatically improved the state-of-the-art performance and fostered successful real-world applications. In this survey, we provide a comprehensive review of the recent developments on deep FR, covering broad topics on algorithm designs, databases, protocols, and application scenes. First, we summarize different network architectures and loss functions proposed in the rapid evolution of the deep FR methods. Second, the related face processing methods are categorized into two classes: “one-to-many augmentation” and “many-to-one normalization”. Then, we summarize and compare the commonly used databases for both model training and evaluation. Third, we review miscellaneous scenes in deep FR, such as cross-factor, heterogenous, multiple-media and industrial scenes. Finally, the technical challenges and several promising directions are highlighted.
               ",autonomous vehicle
10.1016/j.patcog.2020.107561,journal,Pattern Recognition,sciencedirect,2020-12-31,sciencedirect,Sensor-based and vision-based human activity recognition: A comprehensive survey,https://api.elsevier.com/content/article/pii/S0031320320303642,"
                  Human activity recognition (HAR) technology that analyzes data acquired from various types of sensing devices, including vision sensors and embedded sensors, has motivated the development of various context-aware applications in emerging domains, e.g., the Internet of Things (IoT) and healthcare. Even though a considerable number of HAR surveys and review articles have been conducted previously, the major/overall HAR subject has been ignored, and these studies only focus on particular HAR topics. Therefore, a comprehensive review paper that covers major subjects in HAR is imperative. This survey analyzes the latest state-of-the-art research in HAR in recent years, introduces a classification of HAR methodologies, and shows advantages and weaknesses for methods in each category. Specifically, HAR methods are classified into two main groups, which are sensor-based HAR and vision-based HAR, based on the generated data type. After that, each group is divided into subgroups that perform different procedures, including the data collection, pre-processing methods, feature engineering, and the training process. Moreover, an extensive review regarding the utilization of deep learning in HAR is also conducted. Finally, this paper discusses various challenges in the current HAR topic and offers suggestions for future research.
               ",autonomous vehicle
10.1016/j.jallcom.2008.11.155,journal,Journal of Alloys and Compounds,sciencedirect,2009-06-10,sciencedirect,Artificial neural network modeling of the drilling process of self-lubricated aluminum/alumina/graphite hybrid composites synthesized by powder metallurgy technique,https://api.elsevier.com/content/article/pii/S0925838808021178,"
                  In recent years, the consumption of metal matrix composites (MMCs) materials in many engineering fields has increased enormously. Most industries are usually looking for replacement of ferrous components with lighter and high strength alloys like Al metal matrix composites. Despite the superior mechanical and thermal properties of particulate metal matrix composites (PMMCs), their poor machinability is the main drawback to their substitution to other metallic parts. Machining is a material removal process which is important for many stages prior to the application or assembling of the components. Accordingly, the need for accurate machining of composites has also increased tremendously. This study addresses the modeling of the machinability of self-lubricated aluminum/alumina/graphite hybrid composites synthesized by powder metallurgy (P/M). In the present work, a feed forward back propagation artificial neural network (ANN) system is used to investigate the influence of some parameters on the thrust force and cutting torque in the drilling processes. Experimental data collected were tested with artificial neural network technique. Multilayer perceptron model has been constructed with feed forward back propagation algorithm using the input parameters of cutting speed, cutting feed, and volume fraction of the reinforced particles. Output parameters were the thrust force and cutting torque. On completion of the experimental test, an ANN is used to validate the results obtained and also to predict the behavior of the system under any condition within its operating range. The predicted thrust force and cutting torque based on the ANN model were found to be in a very good agreement with the unexposed experimental data set. The modeling results confirm the feasibility of the ANN and its good correlation with the experimental results. The degrees of accuracy of the prediction were 93.24% and 94.17% for thrust force and cutting torque, respectively. It is concluded that ANN is an excellent analytical tool, which can be used for other machining processes, if it is well trained.
               ",autonomous vehicle
10.1016/S0066-4138(09)91013-8,journal,Annual Review in Automatic Programming,sciencedirect,1992-12-31,sciencedirect,A target-directed neurally controlled vehicle,https://api.elsevier.com/content/article/pii/S0066413809910138,"
                  An investigation of a neurally controlled vehicle in a computer-simulated parcours with dynamically changing obstacles is presented. The purpose was to judge the applicability of commercially available neural net shells in process control and automation. The chosen shell provides capabilities for associative memories by unsupervised learning, and for supervised learning by means of the back-propagation algorithm. This algorithm is furthermore enhanced by the functional link approach of Pao ([Pao89]). The vehicle and its environment are displayed graphically. The task for the vehicle is to find its way from a user-defined starting point to an ending point. The neural net is responsible for control of the alternating behaviors of target-orientation and obstacle avoidance. We use ten input neurons, nine representing sensors that deliver information about the distance from non-passable areas in any direction. The tenth sensor is responsible for locating the target in a compass-like way. Three output neurons determine one out of seven possible steering directions. The network was trained off-line, with patterns generated schematically by a program. The results are discussed and further refinements proposed.
               ",autonomous vehicle
10.1016/B978-0-08-100659-7.00007-5,journal,Machine Learning,sciencedirect,2018-12-31,sciencedirect,Chapter 7: Epilogue,https://api.elsevier.com/content/article/pii/B9780081006597000075,"
               
                  This chapters offers a perspective on the overall field of learning and reasoning, with emphasis on the scheme of life-long learning. The modeling of the environment by the unified notion of constraint is advocated especially for conquering the symbolic representations that are useful to carry out high level inference. The epilogue stresses the importance of assigning a purpose to the agents and of prospecting an artificial world which emphasizes their social interactions. It is claimed that the power of deep learning can be dramatically amplified when the agents live in a truly interactive environment. The importance of lively interactive learning protocols has opened the debate on the role of the benchmarks that are currently used in machine learning, since they are not conceived for dealing with life-long learning. Regardless of the importance of benchmarks, the epilogue advocates the importance of adopting evaluation schemes based on “grade-in-life” that can rely on crowdsourcing schemes.
            ",autonomous vehicle
10.1016/j.actaastro.2019.11.039,journal,Acta Astronautica,sciencedirect,2020-05-31,sciencedirect,Real-time optimal control for irregular asteroid landings using deep neural networks,https://api.elsevier.com/content/article/pii/S0094576520300151,"
                  To improve the autonomy and intelligence of asteroid landing control, a real-time optimal control approach is proposed using deep neural networks (DNN) to achieve precise and robust soft landings on asteroids with irregular gravitational fields. First, to reduce the time consumption of gravity calculation, DNNs are used to approximate the irregular gravitational fields of asteroids based on the samples calculated by a polyhedral method. Second, an approximate indirect method is presented to solve the time-optimal landing problems with high computational efficiency by taking advantage of the trained DNN-based gravity model and a homotopic technique. Then, five DNNs are developed to learn the functional relationship between the state and optimal actions obtained by the approximate indirect method, The resulting DNN-based landing controller can generate the optimal control instructions according to the flight state and achieve the real-time optimal control for asteroid landings. Finally, simulation results of the time-optimal landings for Eros are given to substantiate the effectiveness of these techniques and illustrate the real-time performance, control optimality, and robustness of the developed DNN-based optimal landing controller.
               ",autonomous vehicle
10.1016/j.neucom.2019.07.049,journal,Neurocomputing,sciencedirect,2019-12-22,sciencedirect,Multi-view laplacian least squares for human emotion recognition,https://api.elsevier.com/content/article/pii/S0925231219310240,"
                  Human emotion recognition is an emerging and important area in the field of human–computer interaction and artificial intelligence, which has been more and more related with multi-view learning methods. Subspace learning is an important direction of multi-view learning. However, most existing subspace learning methods could not make full use of both category discriminant information and local neighborhood information. As a typical subspace learning method, partial least squares (PLS) performs better and more robustly than many other subspace learning methods, because PLS is optimized with iteration method. However, PLS suffers from linear relationship assumption and two-view limitation. In this paper, a new nonlinear multi-view laplacian least squares (MvLLS) is proposed. MvLLS constructs a global laplacian weighted graph (GLWP) to introduce category discriminant information as well as protects the local neighborhood information. Optimized with iteration method, MvLLS is a multi-view extension of PLS. The proposed method has great extendibility and robustness. To meet the requirements of large-scale applications, weighted local preserving embedding (WLPE) is proposed as the out-of-sample extension of MvLLS, basing on the idea of maintaining the manifold structures of original space. Finally, the proposed method is verified on three multi-view emotion recognition tasks, the experiment results validate the effectiveness and robustness of MvLLS.
               ",autonomous vehicle
10.1016/j.apenergy.2021.117798,journal,Applied Energy,sciencedirect,2021-12-15,sciencedirect,"Review of low voltage load forecasting: Methods, applications, and recommendations",https://api.elsevier.com/content/article/pii/S0306261921011326,"
                  The increased digitalisation and monitoring of the energy system opens up numerous opportunities to decarbonise the energy system. Applications on low voltage, local networks, such as community energy markets and smart storage will facilitate decarbonisation, but they will require advanced control and management. Reliable forecasting will be a necessary component of many of these systems to anticipate key features and uncertainties. Despite this urgent need, there has not yet been an extensive investigation into the current state-of-the-art of low voltage level forecasts, other than at the smart meter level. This paper aims to provide a comprehensive overview of the landscape, current approaches, core applications, challenges and recommendations. Another aim of this paper is to facilitate the continued improvement and advancement in this area. To this end, the paper also surveys some of the most relevant and promising trends. It establishes an open, community-driven list of the known low voltage level open datasets to encourage further research and development.
               ",autonomous vehicle
10.1016/j.neucom.2011.05.025,journal,Neurocomputing,sciencedirect,2012-01-01,sciencedirect,A constructive algorithm to synthesize arbitrarily connected feedforward neural networks,https://api.elsevier.com/content/article/pii/S0925231211004061,"
                  In this work we present a constructive algorithm capable of producing arbitrarily connected feedforward neural network architectures for classification problems. Architecture and synaptic weights of the neural network should be defined by the learning procedure. The main purpose is to obtain a parsimonious neural network, in the form of a hybrid and dedicate linear/nonlinear classification model, which can guide to high levels of performance in terms of generalization. Though not being a global optimization algorithm, nor a population-based metaheuristics, the constructive approach has mechanisms to avoid premature convergence, by mixing growing and pruning processes, and also by implementing a relaxation strategy for the learning error. The synaptic weights of the neural networks produced by the constructive mechanism are adjusted by a quasi-Newton method, and the decision to grow or prune the current network is based on a mutual information criterion. A set of benchmark experiments, including artificial and real datasets, indicates that the new proposal presents a favorable performance when compared with alternative approaches in the literature, such as traditional MLP, mixture of heterogeneous experts, cascade correlation networks and an evolutionary programming system, in terms of both classification accuracy and parsimony of the obtained classifier.
               ",autonomous vehicle
10.1016/j.aei.2021.101404,journal,Advanced Engineering Informatics,sciencedirect,2021-10-31,sciencedirect,A survey of modeling for prognosis and health management of industrial equipment,https://api.elsevier.com/content/article/pii/S1474034621001567,"
                  Prognosis and health management plays an important role in the control of costs associated with operating large industrial equipment, such as wind turbines and aircraft. It is only fair that engineers and scientists have vastly researched modeling approaches to support decision making. Motivated by the growing availability of data and computational power as well as the advances in algorithms and methods, modeling frameworks often merge elements of physics, machine learning, and statistical learning. In this paper, we present a review on modeling in support of prognosis and health management of industrial equipment. This survey complements the existing prognosis and health management literature by discussing how modeling strategies are influenced by industry-specific aspects such as maintenance approaches (e.g., reactive, proactive, and predictive), implementation factors (e.g., industry, business model, purpose, development, and deployment), as well as supporting technologies (sensing, repair, and modeling itself). We use the onshore wind energy and civil aviation industries to illustrate how these aforementioned aspects can influence modeling and implementation of prognosis and health management. The literature review is broad and covers contributions over the past 40 years. We close the paper with few topics that can motive research going forward.
               ",autonomous vehicle
10.1016/B978-0-12-822844-9.00011-6,journal,Recent Trends in Computational Intelligence Enabled Research,sciencedirect,2021-12-31,sciencedirect,Chapter 2: Computational intelligence techniques for localization and clustering in wireless sensor networks,https://api.elsevier.com/content/article/pii/B9780128228449000116,"
               A wireless sensor network (WSN) are normally deployed in harsh environments to collect and deliver data to a remotely located base station. In a sensor network it is very important to know about the position of the sensor node (SN) and data collected by that node, as it has a significant impact on the overall performance of the WSN. Grouping SNs to form clusters has been adopted widely to overcome the scalability problem. It has been proved that for organizing a network into a connected hierarchy, clustering is an effective approach. In this chapter, we address the localization and clustering techniques in WSN, challenges/issues in providing localization and clustering for WSN, and the use of computational techniques for localization and clustering algorithms. We also outline the recent research works on the use of computational intelligence (CI) techniques and future challenges that need to be addressed in providing CI techniques for localization and clustering.
            ",autonomous vehicle
10.1016/j.engappai.2020.103770,journal,Engineering Applications of Artificial Intelligence,sciencedirect,2020-09-30,sciencedirect,"Boosting algorithms for network intrusion detection: A comparative evaluation of Real AdaBoost, Gentle AdaBoost and Modest AdaBoost",https://api.elsevier.com/content/article/pii/S0952197620301706,"
                  Computer networks have been experienced ever-increasing growth since they play a critical role in different aspects of human life. Regarding the vulnerabilities of computer networks, they should be monitored regularly to detect intrusions and attacks by using high-performance Intrusion Detection Systems (IDSs). IDSs try to differentiate between normal and abnormal behaviors to recognize intrusions. Due to the complex behavior of malicious entities, it is crucially important to adopt machine learning methods for intrusion detection with a fine performance and low time complexity. Boosting approach is considered as a way to deal with this challenge. In this paper, we prepare a clear summary of the latest progress in the context of intrusion detection methods, present a technical background on boosting, and demonstrate the ability of the three well-known boosting algorithms (Real Adaboost, Gentle Adaboost, and Modest Adaboost) as IDSs by using five IDS public benchmark datasets. The results show that the Modest AdaBoost has a higher error rate compared to Gentle and Real AdaBoost in IDSs. Besides, in the case of IDSs, Gentle and Real AdaBoost show the same performance as they have about 70% lower error rates compared to Modest Adaboost, however, Modest AdaBoost is about 7% faster than them. In addition, as IDSs need to retrain the model frequently, the results show that Modest AdaBoost has a much lower performance than Gentle and Real AdaBoost in case of error rate stability.
               ",autonomous vehicle
10.1016/B978-0-08-102575-8.00010-3,journal,Fatigue Life Prediction of Composites and Composite Structures,sciencedirect,2020-12-31,sciencedirect,10: Computational intelligence methods for the fatigue life modeling of composite materials,https://api.elsevier.com/content/article/pii/B9780081025758000103,"
               Novel computational methods such as artificial neural networks, adaptive neuro-fuzzy inference systems and genetic programming are used in this chapter for the modeling of the nonlinear behavior of composite laminates subjected to constant amplitude loading. The examined computational methods are stochastic nonlinear regression tools, and can therefore be used to model the fatigue behavior of any material, provided that sufficient data are available for training. They are material independent methods that simply follow the trend of the available data, in each case giving the best estimate of their behavior. Application on a wide range of experimental data gathered after fatigue testing glass/epoxy and glass/polyester laminates proved that their modeling ability compares favorably with, and is to some extent superior to, other modeling techniques.
            ",autonomous vehicle
10.1016/B978-0-12-420248-1.00001-5,journal,Artificial Intelligence in Behavioral and Mental Health Care,sciencedirect,2016-12-31,sciencedirect,Chapter 1: An Introduction to Artificial Intelligence in Behavioral and Mental Health Care,https://api.elsevier.com/content/article/pii/B9780124202481000015,"
               Artificial intelligence (AI) technologies and techniques have useful purposes in just about every domain of behavioral and mental health care including clinical decision-making, treatments, assessment, self-care, healthcare management, research and more. This introductory chapter provides an overview of AI and includes definitions of common terms and concepts to provide a foundation for what is discussed in subsequent chapters. Recent technological innovations are highlighted to demonstrate emerging capabilities and forthcoming opportunities. The benefits of the use of AI in mental health care are also discussed.
            ",autonomous vehicle
10.1016/B0-08-043076-7/00553-2,journal,International Encyclopedia of the Social & Behavioral Sciences,sciencedirect,2001-12-31,sciencedirect,Artificial Intelligence: Connectionist and Symbolic Approaches,https://api.elsevier.com/content/article/pii/B0080430767005532,"
               In this article, the two competing paradigms of artificial intelligence, connectionist and symbolic approaches, will be described. It is pointed out that no single existing paradigm can fully handle all the major AI problems. Each paradigm has its strengths and weaknesses. This situation indicates the need to integrate these two existing paradigms.
            ",autonomous vehicle
10.1016/0921-8890(95)00035-E,journal,Robotics and Autonomous Systems,sciencedirect,1995-11-30,sciencedirect,Artificial neural network for mobile robot topological localization,https://api.elsevier.com/content/article/pii/092188909500035E,"
                  This paper presents a neural network based approach to a mobile robot localization in front of a certain local object. The robot is equipped with ultrasonic range sensors mounted around the platform. We employ the Fuzzy-ARTMAP network for supervised learning of associations between vectors of sensor readouts and the robot's pose coordinates. In this approach, a world model in the form of a map, as well as its updating routine, become superflous for the considered problem solution. The system, trained on real world data of a door neighborhood region reveals satisfactory performance, sufficient for door-passing task purposes. The proposed method of a mobile robot positioning may be efficiently applied in environments containing natural, geometrical beacons.
               ",autonomous vehicle
10.1016/j.neucom.2017.08.040,journal,Neurocomputing,sciencedirect,2018-01-31,sciencedirect,A review on neural networks with random weights,https://api.elsevier.com/content/article/pii/S0925231217314613,"
                  In big data fields, with increasing computing capability, artificial neural networks have shown great strength in solving data classification and regression problems. The traditional training of neural networks depends generally on the error back propagation method to iteratively tune all the parameters. When the number of hidden layers increases, this kind of training has many problems such as slow convergence, time consuming, and local minima. To avoid these problems, neural networks with random weights (NNRW) are proposed in which the weights between the hidden layer and input layer are randomly selected and the weights between the output layer and hidden layer are obtained analytically. Researchers have shown that NNRW has much lower training complexity in comparison with the traditional training of feed-forward neural networks. This paper objectively reviews the advantages and disadvantages of NNRW model, tries to reveal the essence of NNRW, gives our comments and remarks on NNRW, and provides some useful guidelines for users to choose a mechanism to train a feed-forward neural network.
               ",autonomous vehicle
10.1016/j.jnca.2021.103093,journal,Journal of Network and Computer Applications,sciencedirect,2021-08-01,sciencedirect,"Emerging DDoS attack detection and mitigation strategies in software-defined networks: Taxonomy, challenges and future directions",https://api.elsevier.com/content/article/pii/S1084804521001156,"
                  Software-defined networking (SDN) is a network paradigm that decouples control and data planes from network devices and places them into separate entities. In SDN, the controller is responsible for controlling the logic of the entire network while network switches become forwarding elements that follow rules to dispatch flows. There are, however, several limitations in such a paradigm, as compared to conventional networking. For example, the controller is sensitive to a broad range of attacks, including distributed denial of service (DDoS) attacks. In this paper, we provide a systematic survey of existing DDoS detection and mitigation strategies in SDN. Based on the review of articles published between 2013 and May 2020, we provide a taxonomy of DDoS detection strategies (e.g., statistical, SDN architecture, and machine learning) and emerging approaches (e.g., network function virtualization, blockchain, honeynet, network slicing, and moving target defense). We also discuss existing challenges associated with SDN security and the implementation of security solutions, prior to identifying future research opportunities.
               ",autonomous vehicle
10.1016/B978-0-12-820203-6.00009-6,journal,Applications of Big Data in Healthcare,sciencedirect,2021-12-31,sciencedirect,11: Hybrid technique for heart diseases diagnosis based on convolution neural network and long short-term memory,https://api.elsevier.com/content/article/pii/B9780128202036000096,"
               Heart failure-related malfunctioning is the cause of the leading number of death worldwide since it is very difficult to determine the cause of malfunctioning of heart-based on symptoms. Further, the detection of this requires a lot of experience and knowledge as far as medical science is concerned. Therefore, in the presented study work, a technique has been suggested that predicts the cardiac malfunctioning. Nowadays, due to the advancements in data science, scientists, and medical professionals are largely interested in developing an automated cardiac malfunctioning prediction system as it can be highly accurate, efficient, cost-efficient, and very helpful in the early diagnosis. In this study work, a hybrid deep neural network using the dataset with 14 features as input and they are trained to utilize the convolution neural network (CNN) and long short-term memory (LSTM) hybrid algorithms to predict the presence or absence of disease in patients with the highest accuracy reaching 0.937 percent. The results of the study showed that the CNN−LSTM hybrid model had the best results in accuracy, recall, precession, F1 score, and AUC compared to other techniques.
            ",autonomous vehicle
10.1016/j.apm.2011.01.018,journal,Applied Mathematical Modelling,sciencedirect,2011-07-31,sciencedirect,A comparative assessment of classification methods for resonance frequency prediction of Langevin piezoelectric transducers,https://api.elsevier.com/content/article/pii/S0307904X1100031X,"A Langevin piezoelectric transducer is used as a physical element for transmitting and receiving sound waves. The operating frequency of a transducer determines the distance that the sound wave can travel, so it is important to measure it. Due to the fact the structure of a transducer is quite complicated, it is quite difficult to estimate the precise physical parameters for the simulation model. Therefore, it takes a long time to measure the resonance frequency in the laboratory and fix the parameters by trial and error methods. This study applies a learning method to estimate a transducer frequency instead by trial and error experiments. The learning methods applied and compared including artificial neural network, support vector machine, C4.5, neuro-fuzzy, and ega-fuzzification. Compared with the theoretical one-dimensional model (simple lump element model), the results indicate that a learning method is an efficient way to estimate the piezoelectric transducer resonance frequency. The mega-fuzzification method is the best compared with other methods in this study.",autonomous vehicle
10.1016/j.procs.2010.04.249,journal,Procedia Computer Science,sciencedirect,2010-05-31,sciencedirect,Computational intelligence based architecture for cognitive agents,https://api.elsevier.com/content/article/pii/S1877050910002504,"We discuss some limitations of reflexive agents to motivate the need to develop cognitive agents and propose a hierarchical, layered, architecture for cognitive agents. Our examples often involve the discussion of cognitive agents in highway traffic models. A cognitive agent is an agent capable of performing cognitive acts, i.e. a sequence of the following activities: “Perceiving” information in the environment and provided by other agents, “Reasoning” about this information using existing knowledge, “Judging” the obtained information using existing knowledge, “Responding” to other cognitive agents or to the external environment, as it may be required, and “Learning”, i.e. changing (and, hopefully augmenting) the existing knowledge if the newly acquired information allows it. We describe how computational intelligence techniques (e.g., fuzzy logic, neural networks, genetic algorithms, etc) allow mimicking to a certain extent the cognitive acts performed by human beings. The order with which the cognitive actions take place is important and so is the order with which the various computational intelligence techniques are applied. We believe that a hierarchical layered model should be defined for the generic cognitive agents in a style akin to the hierarchical OSI 7 layer model used in data communication. We outline in broad sense such a reference model.",autonomous vehicle
10.1016/j.techfore.2018.03.024,journal,Technological Forecasting and Social Change,sciencedirect,2020-04-30,sciencedirect,Big data analytics: Computational intelligence techniques and application areas,https://api.elsevier.com/content/article/pii/S0040162517318498,"
                  Big Data has significant impact in developing functional smart cities and supporting modern societies. In this paper, we investigate the importance of Big Data in modern life and economy, and discuss challenges arising from Big Data utilization. Different computational intelligence techniques have been considered as tools for Big Data analytics. We also explore the powerful combination of Big Data and Computational Intelligence (CI) and identify a number of areas, where novel applications in real world smart city problems can be developed by utilizing these powerful tools and techniques. We present a case study for intelligent transportation in the context of a smart city, and a novel data modelling methodology based on a biologically inspired universal generative modelling approach called Hierarchical Spatial-Temporal State Machine (HSTSM). We further discuss various implications of policy, protection, valuation and commercialization related to Big Data, its applications and deployment.
               ",autonomous vehicle
10.1016/j.neucom.2017.01.126,journal,Neurocomputing,sciencedirect,2018-02-07,sciencedirect,"Computational intelligence approaches for classification of medical data: State-of-the-art, future challenges and research directions",https://api.elsevier.com/content/article/pii/S0925231217315436,"
                  The explosive growth of data in volume, velocity and diversity that are produced by medical applications has contributed to abundance of big data. Current solutions for efficient data storage and management cannot fulfill the needs of heterogeneous data. Therefore, by applying computational intelligence (CI) approaches in medical data helps get better management, faster performance and higher level of accuracy in detection. This paper aims to investigate the state-of-the-art of computational intelligence approaches in medical data and to categorize the existing CI techniques, used in medical fields, as single and hybrid. In addition, the techniques and methodologies, their limitations and performances are presented in this study. The limitations are addressed as challenges to obtain a set of requirements for Computational Intelligence Medical Data (CIMD) in establishing an efficient CIMD architectural design. The results show that on the one hand Support Vector Machine (SVM) and Artificial Immune Recognition System (AIRS) as a single based computational intelligence approach were the best methods in medical applications. On the other hand, the hybridization of SVM with other methods such as SVM-Genetic Algorithm (SVM-GA), SVM-Artificial Immune System (SVM-AIS), SVM-AIRS and fuzzy support vector machine (FSVM) had great performances achieving better results in terms of accuracy, sensitivity and specificity.
               ",autonomous vehicle
10.1016/j.adhoc.2021.102581,journal,Ad Hoc Networks,sciencedirect,2021-10-01,sciencedirect,Robust Deep Identification using ECG and Multimodal Biometrics for Industrial Internet of Things,https://api.elsevier.com/content/article/pii/S1570870521001219,"
                  The use of electrocardiogram (ECG) data for personal identification in Industrial Internet of Things can achieve near-perfect accuracy in an ideal condition. However, real-life ECG data are often exposed to various types of noises and interferences. A reliable and enhanced identification method could be achieved by employing additional features from other biometric sources. This work, thus, proposes a novel robust and reliable identification technique grounded on multimodal biometrics, which utilizes deep learning to combine fingerprint, ECG and facial image data, particularly useful for identification and gender classification purposes. The multimodal approach allows the model to deal with a range of input domains removing the requirement of independent training on each modality, and inter-domain correlation can improve the model generalization capability on these tasks. In multitask learning, losses from one task help to regularize others, thus, leading to better overall performances. The proposed approach merges the embedding of multimodality by using feature-level and score level fusions. To the best of our understanding, the key concepts presented herein is a pioneering work combining multimodality, multitasking and different fusion methods. The proposed model achieves a better generalization on the benchmark dataset used while the feature-level fusion outperforms other fusion methods. The proposed model is validated on noisy and incomplete data with missing modalities and the analyses on the experimental results are provided.
               ",autonomous vehicle
10.1016/B978-0-12-821777-1.00002-1,journal,"Machine Learning, Big Data, and IoT for Medical Informatics",sciencedirect,2021-12-31,sciencedirect,Chapter 21: An ensemble classifier approach for thyroid disease diagnosis using the AdaBoostM algorithm,https://api.elsevier.com/content/article/pii/B9780128217771000021,"
               The use of information technology in medicine has been a reality for some time now, and the continuous progress that is taking place is reflected in the technologies applied to the care of people who become increasingly avant-garde. These technologies are used with the fundamental objective of improving the health and life expectancy of the world population. Ensemble learning methods provide more correct decision-making processes, at the expense of greater complexity and a loss of interpretability, compared to learning systems based on single hypotheses. The ensemble learning combines the predictions of hypothesis collections to obtain greater performance efficiency. In this chapter, we will explore how to use the ensemble methods for the diagnosis of thyroid disease. After analyzing the concepts behind the different Ensemble Learning methods, we will present a practical case in which we will use AdaBoostM algorithm for the diagnosis of thyroid disorders.
            ",autonomous vehicle
10.1016/j.dss.2021.113560,journal,Decision Support Systems,sciencedirect,2021-07-31,sciencedirect,"Reconciling business intelligence, analytics and decision support systems: More data, deeper insight",https://api.elsevier.com/content/article/pii/S0167923621000701,"
                  Business Intelligence and Analytics (BI&A) systems have demonstrated their potential to enhance decision making; however, the linkage between BI&A and decision support systems (DSS) has been contested by some, if not completely denied by others. In this research, we investigate the foundations of BI&A by using foundational literature on DSS to open the ‘black box’ of BI&A systems. We argue that BI&A is fundamentally a subfield of DSS that is seeking to convert more data into deeper insight, but it has lost its connection to DSS literature and, thereby, missed research opportunities. In this paper, we first define DSS and BI&A and then present a systematic review of foundational DSS literature to assess their leveraging in BI&A research. By classifying cited DSS articles and citing BI&A articles into four areas: conceptual framework, design & implementation, business value & organizational use, and cognition & decision making, potential research for BI&A is uncovered. We reconcile these two research streams by mapping BI&A frameworks to classical DSS components through interviews with practitioners. The result is formulated as a comparative, process-level architecture for converting data into insight. New research opportunities for BI&A are suggested motivated by foundational DSS literature.
               ",autonomous vehicle
10.1016/j.neucom.2017.12.049,journal,Neurocomputing,sciencedirect,2018-03-29,sciencedirect,Evolutionary convolutional neural networks: An application to handwriting recognition,https://api.elsevier.com/content/article/pii/S0925231217319112,"
                  Convolutional neural networks (CNNs) have been used over the past years to solve many different artificial intelligence (AI) problems, providing significant advances in some domains and leading to state-of-the-art results. However, the topologies of CNNs involve many different parameters, and in most cases, their design remains a manual process that involves effort and a significant amount of trial and error.
                  In this work, we have explored the application of neuroevolution to the automatic design of CNN topologies, introducing a common framework for this task and developing two novel solutions based on genetic algorithms and grammatical evolution. We have evaluated our proposal using the MNIST dataset for handwritten digit recognition, achieving a result that is highly competitive with the state-of-the-art without any kind of data augmentation or preprocessing. When misclassified samples are carefully observed, it is found that most of them involve handwritten digits that are difficult to recognize even by a human.
               ",autonomous vehicle
10.1016/j.drudis.2021.09.002,journal,Drug Discovery Today,sciencedirect,2021-09-16,sciencedirect,Towards Pharma 4.0 in clinical trials: A future-orientated perspective,https://api.elsevier.com/content/article/pii/S1359644621003913,"
                  Pharma 4.0, a technology ecosystem in drug development analogous to Industry 4.0 in healthcare, is transforming the traditional approach to drug discovery and development, aligning product quality with less time to market, and creating intelligent stakeholder networks through effective collaborations. The wide range of potential Pharma 4.0 networks have produced several conceptualizations, which have led to a lack of clarity and definition. The main emphasis of this paper is on the clinical trial stage of drug development in the Pharma 4.0 era. It highlights the merged computerized technologies that are currently used in clinical research, and proposes a framework for integrating Pharma 4.0 technologies. The impact of and barriers to employing the proposed framework are discussed, highlighting its potential and some future research applications.
               ",autonomous vehicle
10.1016/j.jhazmat.2021.126425,journal,Journal of Hazardous Materials,sciencedirect,2021-10-05,sciencedirect,Intelligent computational techniques in marine oil spill management: A critical review,https://api.elsevier.com/content/article/pii/S030438942101390X,"
                  Effective marine oil spill management (MOSM) is crucial to minimize the catastrophic impacts of oil spills. MOSM is a complex system affected by various factors, such as characteristics of spilled oil and environmental conditions. Oil spill detection, characterization, and monitoring; risk evaluation; response selection and process optimization; and waste management are the key components of MOSM demanding timely decision-making. Applying robust computational techniques based on real-time data (e.g., satellite and aerial observations) and historical records of oil spill incidents may considerably facilitate decision-making processes. Various soft-computing and artificial intelligence-based models and mathematical techniques have been used for the implementation of MOSM’s components. This study presents a review of literature published since 2010 on the application of computational techniques in MOSM. A statistical evaluation is performed concerning the temporal distribution of papers, publishers’ engagement, research subfields, countries of studies, and selected case studies. Key findings reported in the literature are summarized for two main practices in MOSM: spill detection, characterization, and monitoring; and spill management and response optimization. Potential gaps in applying computational techniques in MOSM have been identified, and a holistic computational-based framework has been suggested for effective MOSM.
               ",autonomous vehicle
10.1016/S0377-2217(97)00029-5,journal,European Journal of Operational Research,sciencedirect,1998-02-16,sciencedirect,Neural network as a simulation metamodel in economic analysis of risky projects,https://api.elsevier.com/content/article/pii/S0377221797000295,"
                  An artificial neural network (ANN) model for economic analysis of risky projects is presented in this paper. Outputs of conventional simulation models are used as neural network training inputs. The neural network model is then used to predict the potential returns from an investment project having stochastic parameters. The nondeterministic aspects of the project include the initial investment, the magnitude of the rate of return, and the investment period. Backpropagation method is used in the neural network modeling. Sigmoid and hyperbolic tangent functions are used in the learning aspect of the system. Analysis of the outputs of the neural network model indicates that more predictive capability can be achieved by coupling conventional simulation with neural network approaches. The trained network was able to predict simulation output based on the input values with very good accuracy for conditions not in its training set. This allowed an analysis of the future performance of the investment project without having to run additional expensive and time-consuming simulation experiments.
               ",autonomous vehicle
10.1016/j.neunet.2017.11.019,journal,Neural Networks,sciencedirect,2018-02-28,sciencedirect,A loop-based neural architecture for structured behavior encoding and decoding,https://api.elsevier.com/content/article/pii/S0893608017302824,"
                  We present a new type of artificial neural network that generalizes on anatomical and dynamical aspects of the mammal brain. Its main novelty lies in its topological structure which is built as an array of interacting elementary motifs shaped like loops. These loops come in various types and can implement functions such as gating, inhibitory or executive control, or encoding of task elements to name a few. Each loop features two sets of neurons and a control region, linked together by non-recurrent projections. The two neural sets do the bulk of the loop’s computations while the control unit specifies the timing and the conditions under which the computations implemented by the loop are to be performed. By functionally linking many such loops together, a neural network is obtained that may perform complex cognitive computations. To demonstrate the potential offered by such a system, we present two neural network simulations. The first illustrates the structure and dynamics of a single loop implementing a simple gating mechanism. The second simulation shows how connecting four loops in series can produce neural activity patterns that are sufficient to pass a simplified delayed-response task. We also show that this network reproduces electrophysiological measurements gathered in various regions of the brain of monkeys performing similar tasks. We also demonstrate connections between this type of neural network and recurrent or long short-term memory network models, and suggest ways to generalize them for future artificial intelligence research.
               ",autonomous vehicle
10.1016/j.comnet.2020.107364,journal,Computer Networks,sciencedirect,2020-10-09,sciencedirect,Multi-layered intrusion detection and prevention in the SDN/NFV enabled cloud of 5G networks using AI-based defense mechanisms,https://api.elsevier.com/content/article/pii/S1389128619310205,"
                  Software defined networking (SDN), network function virtualization (NFV), and cloud computing are receiving significant attention in 5G networks. However, this attention creates a new challenge for security provisioning in these integrated technologies. Research in the field of SDN, NFV, cloud computing, and 5G has recently focused on the intrusion detection and prevention system (IDPS). Existing IDPS solutions are inadequate, which could cause large resource wastage and several security threats. To alleviate security issues, timely detection of an attacker is important. Thus, in this paper, we propose a novel approach that is referred to as multilayered intrusion detection and prevention (ML-IDP) in an SDN/NFV-enabled cloud of 5G networks. The proposed approach defends against security attacks using artificial intelligence (AI). In this paper, we employed five layers: data acquisition layer, switches layer, domain controllers (DC) layer, smart controller (SC) layer, and virtualization layer (NFV infrastructure). User authentication is held in the first layer using the Four-Q-Curve algorithm. To address the flow table overloading attack in the switches layer, the game theory approach, which is executed in the IDP agent, is proposed. The involvement of the IDP agent is to completely avoid a flow table overloading attack by a deep reinforcement learning algorithm, and thus, it updates the current state of all switches. In the DC layer, packets are processed and classified into two classes (normal and suspicious) by a Shannon Entropy function. Normal packets are forwarded to the cloud via the SC. Suspicious packets are sent to the VNF using a growing multiple self-organization map (GM-SOM). The proposed ML-IDP system is evaluated using NS3.26 for different security attacks, including IP Spoofing, flow table overloading, DDoS, Control Plane Saturation, and host location hijacking. From the experiment results, we proved that the ML-IDP with AI-based defense mechanisms effectively detects and prevents attacks.
               ",autonomous vehicle
10.1016/j.ifacol.2021.08.017,journal,IFAC-PapersOnLine,sciencedirect,2021-12-31,sciencedirect,Variations in cycle-time when using knowledge-based tasks for humans and robots,https://api.elsevier.com/content/article/pii/S2405896321007217,"
                  Operator4.0 was coined in 2016 to create a research arena to understand how the physical, cognitive, and sensorial capabilities of an operator could be enhanced by automation. To create an interaction between operator and robots, there are important factors that needs to be defined. Two important factors are the task and function allocation. Without well-defined tasks it is hard to allocate the tasks between the robot and the human to create resource flexibility. Furthermore, it the tasks are knowledge-based rather than rule-based, the cycle time between operators can differ a lot. Two assumptions are discussed regarding knowledge-based tasks and automation. These are also tested in an experiment. Results show that it is a large variation of the cycle time for both humans (between 1,58 minutes up to 4,40 minutes) and robots (between 1,94 minutes up to 4,49 minutes) when it comes to knowledge-based and machine learning systems.
               ",autonomous vehicle
10.1016/j.newideapsych.2009.09.009,journal,New Ideas in Psychology,sciencedirect,2010-12-31,sciencedirect,Challenges for interactivist-constructivist robotics,https://api.elsevier.com/content/article/pii/S0732118X09000506,"
                  The interactivist-constructivist (IC) approach offers an attractive framework for the development of intelligent robots. However, we still lack genuinely intelligent robots, capable of representing the world, in the IC sense. Here we argue that the reason for this situation is the lack of learning mechanisms that would allow the components of the robotic controller to learn constructively while they direct the robot's action in accordance to its value system. We also suggest that spike-timing-dependent plasticity (STDP) may be such a learning mechanism that operates in the brain.
               ",autonomous vehicle
10.1016/j.asoc.2004.08.008,journal,Applied Soft Computing,sciencedirect,2005-07-31,sciencedirect,A new hybrid method using evolutionary algorithms to train Fuzzy Cognitive Maps,https://api.elsevier.com/content/article/pii/S1568494604001012,"
                  A novel hybrid method based on evolutionary computation techniques is presented in this paper for training Fuzzy Cognitive Maps. Fuzzy Cognitive Maps is a soft computing technique for modeling complex systems, which combines the synergistic theories of neural networks and fuzzy logic. The methodology of developing Fuzzy Cognitive Maps relies on human expert experience and knowledge, but still exhibits weaknesses in utilization of learning methods and algorithmic background. For this purpose, we investigate a coupling of differential evolution algorithm and unsupervised Hebbian learning algorithm, using both the global search capabilities of Evolutionary strategies and the effectiveness of the nonlinear Hebbian learning rule. The use of differential evolution algorithm is related to the concept of evolution of a number of individuals from generation to generation and that of nonlinear Hebbian rule to the concept of adaptation to the environment by learning. The hybrid algorithm is introduced, presented and applied successfully in real-world problems, from chemical industry and medicine. Experimental results suggest that the hybrid strategy is capable to train FCM effectively leading the system to desired states and determining an appropriate weight matrix for each specific problem.
               ",autonomous vehicle
10.1016/j.trc.2017.07.003,journal,Transportation Research Part C: Emerging Technologies,sciencedirect,2017-09-30,sciencedirect,Multi-agent immune networks to control interrupted flow at signalized intersections,https://api.elsevier.com/content/article/pii/S0968090X17301845,"
                  Urban traffic is subject to disturbances that cause long queues and extended waiting times at signalized intersections. Although Multi-Agent Systems (MAS) were considered to control traffic at signalized intersections in a distributed way, their generic conceptual framework and lack of built-in adaptation mechanisms prevent them from achieving specific disturbance management capabilities. The traffic signal control problem is still a challenging open-ended problem for which learning and adaptation mechanisms need to be developed to deal with disturbances in an intelligent way. In this article, we rely on concepts and mechanisms inspired by biological immunity to design a distributed, intelligent and adaptive traffic signal control system. We suggest a heterarchical multi-agent architecture, where each agent represents a traffic signal controller assigned to a signalized intersection. Each agent communicates and coordinates with neighboring agents, and achieves learning and adaptation to disturbances based on an artificial immune network. The suggested Immune Network Algorithm based Multi-Agent System (INAMAS) provides intelligent mechanisms that capture disturbance-related knowledge explicitly and take advantage of previous successes and failures in dealing with disturbances through an adaptation of the reinforcement principle. To demonstrate the efficiency of the suggested control architecture, we assess its performance against two control strategies from literature, namely fixed-time control and a distributed adaptation of the Longest Queue First – Maximal Weight Matching (LQF-MWM) algorithm. Agents are developed using SPADE platform and used to control a network of signalized intersections simulated with VISSIM, a state-of-the-art traffic simulation software. The results show that INAMAS is able to handle different traffic scenarios with competitive performance (in terms of vehicle queue lengths and waiting times), and that it is particularly more successful than the other controllers in dealing with extreme situations involving blocked approaches and high traffic volumes.
               ",autonomous vehicle
10.1016/B978-075067605-2/50008-9,journal,Fuzzy Logic for Embedded Systems Applications,sciencedirect,2004-12-31,sciencedirect,Chapter 6: Neural networks,https://api.elsevier.com/content/article/pii/B9780750676052500089,"
               This chapter introduces the fundamental ideas of neural networks. Models of biological neural networks are built around nodes (or processing units) that simulate the action of a neuron. Input and output links to these nodes simulate synapses. Weights are assigned to the input links to simulate the action of the neurotransmitters. An algorithm is then used to adjust the weights of the input links so that the neurons produce the desired output, thus simulating the process of learning. Such algorithms are therefore referred to as learning or training algorithms. Neural networks are trained, not programmed. Training is the process of adjusting the weights throughout the network so that it responds correctly to input patterns. The computation depends on parallel processing. This leads to fault tolerance and graceful degradation, as opposed to catastrophic failure. The memory is not localized, but rather distributed. Time is needed for the network to learn; a network with complex functions would take longer to learn. The learning time also depends on the architecture and learning algorithm used. The capacity of the network is, in general, dependent on the number of neurons used.
            ",autonomous vehicle
10.1016/j.imu.2021.100596,journal,Informatics in Medicine Unlocked,sciencedirect,2021-12-31,sciencedirect,"AI applications in robotics, diagnostic image analysis and precision medicine: Current limitations, future trends, guidelines on CAD systems for medicine",https://api.elsevier.com/content/article/pii/S2352914821000861,"Background AI in medicine has been recognized by both academia and industry in revolutionizing how healthcare services will be offered by providers and perceived by all stakeholders. Objectives We aim to review recent tendencies in building AI applications for medicine and foster its further development by outlining obstacles. Sub-objectives: (1) to highlight AI techniques that we have identified as key areas of AI-related research in healthcare; (2) to offer guidelines on building reliable AI-based CAD-systems for medicine; and (3) to reveal open research questions, challenges, and directions for future research. Methods To address the tasks, we performed a systematic review of the references on the main branches of AI applications for medical purposes. We focused primarily on limitations of the reviewed studies. Conclusions This study provides a summary of AI-related research in healthcare, it discusses the challenges and proposes open research questions for further research. Robotics has taken huge leaps in improving the healthcare services in a variety of medical sectors, including oncology and surgical interventions. In addition, robots are now replacing human assistants as they learn to become more sociable and reliable. However, there are challenges that must still be addressed to enable the use of medical robots in diagnostics and interventions. AI for medical imaging eliminates subjectivity in a visual diagnostic procedure and allows for the combining of medical imaging with clinical data, lifestyle risks and demographics. Disadvantages of AI solutions for radiology include both a lack of transparency and dedication to narrowed diagnostic questions. Designing an optimal automatic classifier should incorporate both expert knowledge on a disease and state-of-the-art computer vision techniques. AI in precision medicine and oncology allows for risk stratification due to genomics aberrations discovered on molecular testing. To summarize, AI cannot substitute a medical doctor. However, medicine may benefit from robotics, a CAD, and AI-based personalized approach.",autonomous vehicle
10.1016/j.comcom.2011.01.012,journal,Computer Communications,sciencedirect,2011-07-15,sciencedirect,Distributed denial of service attack detection using an ensemble of neural classifier,https://api.elsevier.com/content/article/pii/S0140366411000600,"
                  The vulnerabilities in the Communication (TCP/IP) protocol stack and the availability of more sophisticated attack tools breed in more and more network hackers to attack the network intentionally or unintentionally, leading to Distributed Denial of Service (DDoS) attack. The DDoS attacks could be detected using the existing machine learning techniques such as neural classifiers. These classifiers lack generalization capabilities which result in less performance leading to high false positives. This paper evaluates the performance of a comprehensive set of machine learning algorithms for selecting the base classifier using the publicly available KDD Cup dataset. Based on the outcome of the experiments, Resilient Back Propagation (RBP) was chosen as base classifier for our research. The improvement in performance of the RBP classifier is the focus of this paper. Our proposed classification algorithm, RBPBoost, is achieved by combining ensemble of classifier outputs and Neyman Pearson cost minimization strategy, for final classification decision. Publicly available datasets such as KDD Cup, DARPA 1999, DARPA 2000, and CONFICKER were used for the simulation experiments. RBPBoost was trained and tested with DARPA, CONFICKER, and our own lab datasets. Detection accuracy and Cost per sample were the two metrics evaluated to analyze the performance of the RBPBoost classification algorithm. From the simulation results, it is evident that RBPBoost algorithm achieves high detection accuracy (99.4%) with fewer false alarms and outperforms the existing ensemble algorithms. RBPBoost algorithm outperforms the existing algorithms with maximum gain of 6.6% and minimum gain of 0.8%.
               ",autonomous vehicle
10.1016/j.dss.2020.113490,journal,Decision Support Systems,sciencedirect,2021-05-31,sciencedirect,A strategic decision-making architecture toward hybrid teams for dynamic competitive problems,https://api.elsevier.com/content/article/pii/S0167923620302451,"Advances in artificial intelligence create new opportunities for computers to support humans as peers in hybrid teams in several complex problem-solving situations. This paper proposes a decision-making architecture for adaptively informing decisions in human-computer collaboration for large-scale competitive problems under dynamic environments. The proposed architecture integrates methods from sequence learning, model predictive control, and game theory. Computers in this architecture learn objectives and strategies from experimental data to support humans with strategic decisions while operational decisions are made by humans. The paper also presents data-driven methods for partitioning tasks among a team of computers in this architecture. The generalized methodology is illustrated on the real-time strategy game Starcraft II. The results from this application show that low-performing players can benefit from the game-theoretic decision support whereas this support can be overly conservative for high-performing players. The proposed approach provides safe though suboptimal suggestions particularly against an opponent with an unknown level of expertise. The results further show that problem solution with a team of computers based on non-intuitive task partitioning significantly improves the quality of decisions compared to an all-in-one solution with a single computer.",autonomous vehicle
10.1016/0004-3702(89)90049-0,journal,Artificial Intelligence,sciencedirect,1989-09-30,sciencedirect,Connectionist learning procedures,https://api.elsevier.com/content/article/pii/0004370289900490,"
                  A major goal of research on networks of neuron-like processing units is to discover efficient learning procedures that allow these networks to construct complex internal representations of their environment. The learning procedures must be capable of modifying the connection strengths in such a way that internal units which are not part of the input or output come to represent important features of the task domain. Several interesting gradient-descent procedures have recently been discovered. Each connection computes the derivative, with respect to the connection strength, of a global measure of the error in the performance of the network. The strength is then adjusted in the direction that decreases the error. These relatively simple, gradient-descent learning procedures work well for small tasks and the new challenge is to find ways of improving their convergence rate and their generalization abilities so that they can be applied to larger, more realistic tasks.
               ",autonomous vehicle
10.1016/j.jpdc.2020.07.008,journal,Journal of Parallel and Distributed Computing,sciencedirect,2020-12-31,sciencedirect,"Intelligently modeling, detecting, and scheduling elephant flows in software defined energy cloud: A survey",https://api.elsevier.com/content/article/pii/S0743731520303373,"
                  Elephant flows (elephants) refer to the sequences of packets that contribute only 10% of the total volume but consume over 90% of the network bandwidth. They often cause network congestion and should be efficiently managed. Present cloud data centers often involve host- and switch-based approaches to detect and schedule elephants, but suffer (1) each host and switch in the network needs to be customized, and (2) dynamic models and advanced policies are difficult to be applied. Software Defined Cloud (SDC) addresses these issues by enabling controller-based approaches. With the aid of Machine Learning (ML) technologies, SDC can achieve learning-based models, flexible deployment, and early detection and schedule of elephants for the optimization of network performance and energy usage in a dynamic and intelligent manner. On this purpose, this article emphases the significance of models describing elephants, surveys the mechanisms that may apply to model, detect, and schedule elephants for SDC to optimize the network performance and energy usage. To the best of our knowledge, this work is the first effort that reviews the techniques in all these related subtopics simultaneously in the context of energy cloud.
               ",autonomous vehicle
10.1016/B978-0-12-821126-7.00010-3,journal,Fundamentals of Optimization Techniques with Algorithms,sciencedirect,2020-12-31,sciencedirect,Chapter ten: Nature-inspired optimization,https://api.elsevier.com/content/article/pii/B9780128211267000103,"
               This chapter includes various nature-inspired optimization techniques, viz., genetic algorithm, neural network-based optimization, ant colony optimization (ACO), and particle swarm optimization (PSO). The genetic algorithm is based on the evolution theory given by Darwin, which is survival of the fittest. However, artificial neural network is inspired by natural neural networks found in mammals’ nervous system. ACO and PSO are branches of swarm intelligence, and these techniques are inspired by the behavior of swarms. A brief description of these methods along with the working algorithms is presented for easy illustration of various real-life problems.
            ",autonomous vehicle
10.1016/B978-0-12-822420-5.00009-X,journal,Introduction to Machine Olfaction Devices,sciencedirect,2022-12-31,sciencedirect,3: MOD data and data analysis,https://api.elsevier.com/content/article/pii/B978012822420500009X,"
               This chapter deals with data and data analysis via the outline of different methods, such as graphical analysis (GS), multivariate data analysis (MDA), and network analysis. GS is used to solve a problem in the form of analyzing data via graphing techniques, while MDA—which cover principal component analysis (PCA), feature selection (FS) and featured weighting (FW), cluster analysis, and self-organizing map (SOM)—covers the evaluation and collection of data to help in explaining relationships between different variables from these data. PCA is a method used to reduce a large set of data into a much smaller set, while keeping important and sufficient information needed from the original large data set. FS is applied in order to reduce the number of features, such as redundant items from a data set in order to improve the accuracy of a predicted model. FW is a method using a training set that estimates the influence of individual features on a final result. SOMs are data visualization techniques, and their functionalities are listed in bullet points. In addition, machine learning terms have been provided to encourage the reader to explore further this approach for the purpose of training the MOD using this method.
            ",autonomous vehicle
10.1016/j.jmapro.2021.07.014,journal,Journal of Manufacturing Processes,sciencedirect,2021-08-31,sciencedirect,Random decision forest based sustainable green machining using <ce:italic>Citrullus lanatus</ce:italic> extract as bio-cutting fluid,https://api.elsevier.com/content/article/pii/S1526612521005053,"
                  The presently employed synthetic cutting fluids in machining applications lead to skin irritations, hazardous aerosols, ingestion etc., to the machine tool operators. Accomplishing sustainable green machining through bio-cutting fluids, is the key motivating factor behind this work, even though the phenomenon of heat generation is inevitable in the machining process. In the present investigation, Citrullus lanatus extract is chosen as a potential candidate for bio-cutting fluid, since it offers multiple benefits such as increased tool life, cleaner production atmosphere, efficacious heat dissipation, healthy operating conditions, etc. Hybrid LM0-6SiCp-4Grp composite is selected for study since; it has potential applications in marine, aerospace, defence and automotive sectors. Experimentation is carried out with Citrullus lanatus extract as bio-cutting fluid in turning of hybrid LM0-6SiCp-4Grp composite under different processing conditions. The presence of green solution based on Citrullus lanatus extract curtails the rise of temperature significantly during turning operation. Higher magnitude of temperature decrease is observed when the percentage of the concentration of Citrullus lanatus extract is increased, irrespective of depth of turning. Bio-nature and cooling qualities of this green-cutting fluid can contribute its share for green machining and sustainable turning environment. The investigations are further progressed using machine learning algorithms based on the different categories of logical regressions implemented through the Python program. Random forest regression shows better results with 99.8% prediction accuracy compared to other machine learning approaches and the decision tree is constructed through the respective regression model. Decision tree has the ability to handle process parameters systematically and predicts the implications of Citrullus lanatus extract in decreasing the temperature at machine-tool interface during machining operation. This tree structured analysis provides a better inference that delivers flexibility in turning parameters and opens up wider options for operation. Thus, decision tree approach ensures optimized usage of production resources and increases the production capacity through judicial selection of cooling approaches associated with the turning process.
               ",autonomous vehicle
10.1016/j.matpr.2021.01.357,journal,Materials Today: Proceedings,sciencedirect,2021-12-31,sciencedirect,Data science applications for predictive maintenance and materials science in context to Industry 4.0,https://api.elsevier.com/content/article/pii/S221478532100448X,"
                  With the revolutionising of the industry to the next generations, machines have become more complicated. If they are not put to regular maintenance then there is more breakdown and disruption in the production line. These days, data science techniques have applications over almost every field and likewise are being applied to Industry 4.0. In this advanced setup, massive data is created and stored every second. Experts with expertise in advanced mathematical and computational skills are in demand to identify root causes of failures and quality deviations of a machine, contributing to minimising a loss in time and money. Moreover, new elements with tailored properties can be discovered with material theories and computational skills. The integration of data science with industry 4.0 will increase efficiency and will be helpful to predict the quality of material minimising the production line cost and time. Different research articles on industry 4.0, data science and predictive maintenance are identified and studied. This paper identifies five critical processes of data scientists for predictive maintenance and discussed briefly through a literature review. Data science uses various processes, scientific methods, and algorithms to extract knowledge from a large amount of data. It can collect a massive amount of industrial data, which is further used to improve the manufacturing systems' efficiency and reliability. It helps analyse the data and become essential for Industry 4.0.
               ",autonomous vehicle
10.1016/j.mlwa.2021.100031,journal,Machine Learning with Applications,sciencedirect,2021-06-15,sciencedirect,Review of classification algorithms with changing inter-class distances,https://api.elsevier.com/content/article/pii/S2666827021000128,"Machine learning algorithms are often faced with several data related problems. Real-world datasets come in various types and dimensions, each of which constitute some form of data related problems; moreover, they often contain irrelevant or noisy features. As a result of these, different data related problems require different techniques for the classification process. In this paper, some data related problems of interest are replicated in different synthetic datasets in order to investigate and evaluate the performance of a range of learning algorithms. Specifically, the data problems studied in this research are: datasets with varying inter class distances (classes are separated by different amounts); datasets with classes having different input relevance; datasets with classes defined by multiple features and by multiple underlying pattern; datasets with increasing number of noisy features; and datasets with varying amplitudes of noisy features. Also, datasets with combination of some of the problems were also synthesized. These datasets were then used to measure and validate the performance of a number of selected classification algorithms. The results of the experimental investigations show that the GNG had the best performance on datasets with varying inter class distances while DL performed best on the other datasets of different data problems.",autonomous vehicle
10.1016/B978-0-12-818576-6.00011-3,journal,Artificial Intelligence to Solve Pervasive Internet of Things Issues,sciencedirect,2021-12-31,sciencedirect,Chapter 11: Adaptive Complex Systems: Digital Twins,https://api.elsevier.com/content/article/pii/B9780128185766000113,"
               The design, control, and maintenance of complex systems are a challenge. Often it is difficult to understand the whole-system behavior because the knowledge of component behavior and interaction is uncertain. Such systems are often deployed into dynamic environments whose behavior is liable to change. This chapter reviews the features of complex systems and proposes an approach based on creating digital twins of systems that are capable of adaptation. We discuss technologies for digital twins and propose that the adaptation should be based on machine learning. We provide a simple tutorial example of agents with machine learning using our proposed technology and describe how we have used the technology to build a digital twin for supply chain networks.
            ",autonomous vehicle
10.1016/j.compind.2019.02.010,journal,Computers in Industry,sciencedirect,2019-06-30,sciencedirect,Unsupervised weld defect classification in radiographic images using multivariate generalized Gaussian mixture model with exact computation of mean and shape parameters,https://api.elsevier.com/content/article/pii/S0166361518305967,"
                  In industry, the welding inspection is considered as a mandatory stage in the process of quality assurance/quality control. This inspection should satisfy the requirements of the standards and codes governing the manufacturing process in order to prevent unfair harm to the industrial plant in construction. For this purpose, in this paper, a software specially conceived for computer-aided diagnosis in weld radiographic testing is presented, where a succession of operations of preprocessing, image segmentation, feature extraction and finally defects classification is carried out on radiographic images. The last operation which is the main contribution in this paper consists in an unsupervised classifier based on a finite mixture model using the multivariate generalized Gaussian distribution (MGGD). This classifier is newly applied on a dataset of weld defect radiographic images. The parameters of the nonzero-mean MGGD-based mixture model are estimated using the Expectation-Maximization algorithm where, exact computations of mean and shape parameters are originally provided. The weld defect database represent four weld defect types (crack, lack of penetration, porosity and solid inclusion) which are indexed by a shape geometric descriptor composed of geometric measures. An outstanding performance of the proposed mixture model, compared to the one using the multivariate Gaussian distribution, is shown, where the classification rate is improved by 3.2% for the whole database, to reach more than 96%. The efficiency of the proposed classifier is mainly due to the flexible fitting of the input data, thanks to the MGGD shape parameter.
               ",autonomous vehicle
10.1016/B978-0-323-85769-7.00008-2,journal,Cognitive Computing for Human-Robot Interaction,sciencedirect,2021-12-31,sciencedirect,Chapter 7: Cognitive computing in autonomous vehicles,https://api.elsevier.com/content/article/pii/B9780323857697000082,"
               Neuromorphic architecture is inspired by the human brain which mimics micro neurobiological architecture present in nervous systems, and the Von Neumann architecture model combines to form what we call cognitive computing. Cognition is a straightforward term that suggests that “to gain information and comprehend.” It is utilized in fields such as natural language processing, linguistics, pattern recognition, object detection, and decision-making processes. Cognitive computing has greater leverage over usual rule-based computing conspicuously within the field of human-robot interaction wherever there is a shut interaction between humans and robots. This chapter gives a brief introduction to cognitive computing and its various applications and advantages in autonomous vehicles (AVs). It takes into account hardware components and mathematical models required for the design of an AV. It focuses on different cognitive artificial intelligence techniques and algorithms that will help us to achieve closeness to human-level performance or what we call Level 5 autonomy in AVs having unlimited operational design domain. Achieving level 5 autonomy is an extremely difficult task because it requires almost perfect decision-making, object and event detection and response, localization even in uncertain conditions like cloudy weather, fog, extreme darkness, and rain, which act as a forestall to vision task and localization. That is where cognitive computing comes into the picture. Cognitive computation techniques enhance the accuracy of the model to achieve a human-like performance in decision-making or even in object detection. This chapter discusses general advancements in technology like the advent of memristors, which increases the number of neural synaptic cores and neurons that have led to increased safety and better accuracy in AVs. Yet even above all this, there is greater scope for development, as perfect level 5 autonomy is still not achieved. Furthermore, this chapter explores the further developments possible in this field of AVs and the effects it will cast on future generations.
            ",autonomous vehicle
10.1016/j.compstruct.2005.01.020,journal,Composite Structures,sciencedirect,2006-05-31,sciencedirect,Modeling the mechanical behavior of fiber-reinforced polymeric composite materials using artificial neural networks—A review,https://api.elsevier.com/content/article/pii/S0263822305000243,"
                  Artificial neural networks (ANN) have emerged as one of the useful artificial intelligence concepts used in the various engineering applications. Due to their massively parallel structure and ability to learn by example, ANN can deal with non-linear modeling for which an accurate analytical solution is difficult to obtain. ANN have already been used in medical applications, image and speech recognition, classification and control of dynamic systems, among others; but only recently have they been used in modeling the mechanical behavior of fiber-reinforced composite materials.
                  This work is an attempt to reflect on the work done in the mechanical modeling of fiber-reinforced composite materials using ANN during the last decade.
               ",autonomous vehicle
10.1016/j.procs.2015.08.234,journal,Procedia Computer Science,sciencedirect,2015-12-31,sciencedirect,Neural Network Techniques for Cancer Prediction: A Survey,https://api.elsevier.com/content/article/pii/S1877050915023613,"Cancer is a dreadful disease. Millions of people died every year because of this disease. It is very essential for medical practitioners to opt a proper treatment for cancer patients. Therefore cancer cells should be identified correctly. Neural networks are currently a burning research area in medical science, especially in the areas of cardiology, radiology, oncology, urology and etc. In this paper, we are surveying various neural network technologies for classification of cancer. The main aim of this survey in medical diagnostics is to guide researchers to develop most cost effective and user friendly systems, processes and approaches for clinicians.",autonomous vehicle
10.1016/j.inffus.2020.06.011,journal,Information Fusion,sciencedirect,2020-12-31,sciencedirect,A survey on empathetic dialogue systems,https://api.elsevier.com/content/article/pii/S1566253520303092,"
                  Dialogue systems have achieved growing success in many areas thanks to the rapid advances of machine learning techniques. In the quest for generating more human-like conversations, one of the major challenges is to learn to generate responses in a more empathetic manner. In this review article, we focus on the literature of empathetic dialogue systems, whose goal is to enhance the perception and expression of emotional states, personal preference, and knowledge. Accordingly, we identify three key features that underpin such systems: emotion-awareness, personality-awareness, and knowledge-accessibility. The main goal of this review is to serve as a comprehensive guide to research and development on empathetic dialogue systems and to suggest future directions in this domain.
               ",autonomous vehicle
10.1016/j.neucom.2021.10.039,journal,Neurocomputing,sciencedirect,2022-01-11,sciencedirect,Hierarchical multimodal transformer to summarize videos,https://api.elsevier.com/content/article/pii/S0925231221015253,"
                  Although video summarization has achieved tremendous success benefiting from Recurrent Neural Networks (RNN), RNN-based methods neglect the global dependencies and multi-hop relationships among video frames, which limits the performance. Transformer is an effective model to deal with this problem, and surpasses RNN-based methods in several sequence modeling tasks, such as machine translation, video captioning, etc. Motivated by the great success of transformer and the natural structure of video (frame-shot-video), a hierarchical transformer is developed for video summarization, which can capture the dependencies among frame and shots, and summarize the video by exploiting the scene information formed by shots. Furthermore, we argue that both the audio and visual information are essential for the video summarization task. To integrate the two kinds of information, they are encoded in a two-stream scheme, and a multimodal fusion mechanism is developed based on the hierarchical transformer. In this paper, the proposed method is denoted as Hierarchical Multimodal Transformer (HMT). Practically, extensive experiments show that HMT achieves (F-measure: 0.441, Kendall’s 
                        
                           τ
                        
                     : 0.079, Spearman’s 
                        
                           ρ
                        
                     : 0.080) and (F-measure: 0.601, Kendall’s 
                        
                           τ
                        
                     : 0.096, Spearman’s 
                        
                           ρ
                        
                     : 0.107) on SumMe and TVsum, respectively. It surpasses most of the traditional, RNN-based and attention-based video summarization methods.
               ",autonomous vehicle
10.1016/S0065-2458(08)60272-7,journal,Advances in Computers,sciencedirect,1993-12-31,sciencedirect,Artificial Neural Networks in Control Applications,https://api.elsevier.com/content/article/pii/S0065245808602727,"
                  In this chapter, a thumbnail sketch of the field of artificial neural networks is presented. This chapter discusses the relevance of ANN technology to practical control problems, with a slight emphasis toward problems arising in robotics. Models of ANNs are specified by three basic entities: models of the neurons themselves—that is, the node characteristics; models of synaptic interconnections and structures—that is, net topology and weights; and training or learning rules—that is, the method of adjusting the weights or the way the network interprets the information it is receiving. The nodes themselves can be characterized by analog (continuous) or digital (discrete) summing elements exhibiting either linear or nonlinear behavior. The back propagation method appears to have a dominant role in the modeling and identification areas. Control problems fall into two broad categories. In regulation and tracking problems, the objective is to follow a reference trajectory. Self-tuning regulators and model reference controllers belong to this class. In optimum control problems, the objective is to optimize some aspect of the system's behavior without recourse to any reference trajectory. When a detailed model of the plant being controlled is not available, adaptive control techniques can be used to solve either the tracking problem or the optimum control problem. The examples of Neural Controllers selected in the chapter are biased toward robotics. There are a number of other application areas, such as process control and optimization, where the potential of ANNs are being explored. With the advent of VLSI technology on the one hand and supercomputers on the other, it is now possible to routinely simulate large neural networks with relative ease. The time is now ripe to exploit the parallel, distributed nature of neural architectures.
               ",autonomous vehicle
10.1016/B978-0-12-819445-4.00015-1,journal,"Cognitive Informatics, Computer Modelling, and Cognitive Science",sciencedirect,2020-12-31,sciencedirect,Chapter 15: A special report on changing trends in preventive stroke/cardiovascular risk assessment via B-mode ultrasonography,https://api.elsevier.com/content/article/pii/B9780128194454000151,"
               Cardiovascular (CV) disease (CVD) and stroke risk assessment have been largely based on the success of traditional statistically derived risk calculators such as pooled cohort risk score or Framingham risk score. However, over the last decade, automated computational paradigms such as machine learning (ML) and deep learning (DL) techniques have penetrated into a variety of medical domains, including CVD/stroke risk assessment. This review is mainly focused on the changing trends in CVD/stroke risk assessment and its stratification from statistical-based models to ML-based paradigms using noninvasive carotid ultrasonography. In this review, ML-based strategies are categorized into two types: nonimage (or conventional ML based) and image based (or integrated ML based). The success of conventional (nonimage based) ML-based algorithms lies in the different data-driven patterns or features which are used to train the ML systems. Typically, these features are the patients’ demographics, serum biomarkers, and multiple clinical parameters. The integrated (image-based) ML-based algorithms integrate the features derived from the ultrasound scans of the arterial walls (such as morphological measurements) with conventional risk factors in ML frameworks. Even though the review covers ML-based system designs for carotid and coronary ultrasonography, the main focus of the review is on CVD/stroke risk scores based on carotid ultrasound. There are two key conclusions from this review: (1) fusion of image-based features with conventional CV risk factors can lead to better CVD/stroke risk stratification and (2) the ability to handle multiple sources of information in big data framework using artificial intelligence–based paradigms (such as ML and DL) are likely to be the future in preventive CVD/stroke risk assessment.
            ",autonomous vehicle
10.1016/j.cosrev.2020.100336,journal,Computer Science Review,sciencedirect,2021-02-28,sciencedirect,Comparative analysis on cross-modal information retrieval: A review,https://api.elsevier.com/content/article/pii/S1574013720304366,"
                  Human beings experience life through a spectrum of modes such as vision, taste, hearing, smell, and touch. These multiple modes are integrated for information processing in our brain using a complex network of neuron connections. Likewise for artificial intelligence to mimic the human way of learning and evolve into the next generation, it should elucidate multi-modal information fusion efficiently. Modality is a channel that conveys information about an object or an event such as image, text, video, and audio. A research problem is said to be multi-modal when it incorporates information from more than a single modality. Multi-modal systems involve one mode of data to be inquired for any (same or varying) modality outcome whereas cross-modal system strictly retrieves the information from a dissimilar modality. As the input–output queries belong to diverse modal families, their coherent comparison is still an open challenge with their primitive forms and subjective definition of content similarity. Numerous techniques have been proposed by researchers to handle this issue and to reduce the semantic gap of information retrieval among different modalities. This paper focuses on a comparative analysis of various research works in the field of cross-modal information retrieval. Comparative analysis of several cross-modal representations and the results of the state-of-the-art methods when applied on benchmark datasets have also been discussed. In the end, open issues are presented to enable the researchers to a better understanding of the present scenario and to identify future research directions.
               ",autonomous vehicle
10.1016/j.neucom.2011.04.028,journal,Neurocomputing,sciencedirect,2011-10-31,sciencedirect,TurSOM: A paradigm bridging Turing's unorganized machines and self-organizing maps demonstrating dual self-organization,https://api.elsevier.com/content/article/pii/S0925231211003122,"
                  Self-organization is a widely used technique in unsupervised learning and data analysis, largely exemplified by k-means clustering, self-organizing maps (SOM) and adaptive resonance theory.
                  In this paper we present a new algorithm: TurSOM, inspired by Turing's unorganized machines and Kohonen's SOM. Turing's unorganized machines are an early model of neural networks characterized by self-organizing connections, as opposed to self-organizing neurons in SOM.
                  TurSOM introduces three new mechanisms to facilitate both neuron and connection self-organization. These mechanisms are: a connection learning rate, connection reorganization, and a neuron responsibility radius.
                  TurSOM is implemented in a 1-dimensional network (i.e. chain of neurons) to exemplify the theoretical implications of these features. In this paper we demonstrate that TurSOM is superior to the classical SOM algorithm in several ways: (1) speed until convergence; (2) independent clusters; and (3) tangle-free networks.
               ",autonomous vehicle
10.1016/j.neunet.2020.01.016,journal,Neural Networks,sciencedirect,2020-05-31,sciencedirect,Robust min–max optimal control design for systems with uncertain models: A neural dynamic programming approach,https://api.elsevier.com/content/article/pii/S0893608020300186,"
                  The design of an artificial neural network (ANN) based sub-optimal controller to solve the finite-horizon optimization problem for a class of systems with uncertainties is the main outcome of this study. The optimization problem considers a convex performance index in the Bolza form. The dynamic uncertain restriction is considered as a linear system affected by modeling uncertainties, as well as by external bounded perturbations. The proposed controller implements a min–max approach based on the dynamic neural programming approximate solution. An ANN approximates the Value function to get the estimate of the Hamilton–Jacobi–Bellman (HJB) equation solution. The explicit adaptive law for the weights in the ANN is obtained from the approximation of the HJB solution. The stability analysis based on the Lyapunov theory yields to confirm that the approximate Value function serves as a Lyapunov function candidate and to conclude the practical stability of the equilibrium point. A simulation example illustrates the characteristics of the sub-optimal controller. The comparison of the performance indexes obtained with the application of different controllers evaluates the effect of perturbations and the sub-optimal solution.
               ",autonomous vehicle
10.1016/j.eswa.2020.114545,journal,Expert Systems with Applications,sciencedirect,2021-05-15,sciencedirect,Additive deep feature optimization for semantic image retrieval,https://api.elsevier.com/content/article/pii/S0957417420311891,"
                  Rapid increase in the distribution of multimedia content in recent times presents a challenging problem for content-based image retrieval systems. Image contents, such as position and shape of objects alongside contextual features such as background can be used to retrieve visually similar images. Variations in contrast, color, intensity and texture of contextually similar images make it an interesting research problem. A deep convolutional neural network-based model called MaxNet for content-based image retrieval is presented in this paper. The proposed system bypasses the reliance on handcrafted features and extracts deep features directly from the images, which are then used to retrieve contextually similar images from the database. The proposed MaxNet model is built by stacking the updated inception module in a hierarchical fashion. Features extracted from various pipelines in the inception module are aggregated after each inception maximizing the feature values. This novel aggregation step generates a model that is able to adapt to variety of datasets. Various types of aggregations are discussed in this study. Model overcomes the over-fitting problem by using a dropout layer after each inception block and just before the output layer. The system outputs softmax probabilities, which are stored in the feature database and are used to compute the similarity index to retrieve images similar to the query image. The MaxNet model is evaluated using four popular image retrieval datasets namely, Corel-1k, Corel-5k, Corel-10k and Caltech-101, where it outperforms state-of-the-art methods in key performance indicators.
               ",autonomous vehicle
10.1016/j.apenergy.2021.116932,journal,Applied Energy,sciencedirect,2021-06-15,sciencedirect,Adaptive energy management of a battery-supercapacitor energy storage system for electric vehicles based on flexible perception and neural network fitting,https://api.elsevier.com/content/article/pii/S0306261921004128,"
                  The hybrid energy storage system (HESS) composed of batteries and supercapacitors (SCs) is a dual energy storage technology that can compensate for the shortcomings of a single energy storage technology acting alone. The energy management of HESS splits the power and energy demands from the electric vehicle (EV) to the battery and SC and thus is vital to EV propulsion. This paper presents an online energy management strategy (EMS) that optimises the operating costs of battery-SC HESS and can be adaptive to real-time EV driving conditions. We analyse the optimal offline benchmarks to guide online EMS design and propose the adaptive online EMS with variable perception horizon based on both neural network and rule-based techniques. Compared with existing research, the proposed EMS features reduced complexity, flexible perception and intelligent rulemaking. Case study results show that the proposed variable perception horizon and neural network fitting can improve EMS optimality compared with the conventional methods in existing research. The proposed EMS can realise more than 97% cost optimisation efficacy of offline benchmarks. By the proposed EMS, this paper is expected to provide a practical and effective energy management approach for the battery-SC HESS to reduce costs in EV applications.
               ",autonomous vehicle
10.1016/j.comcom.2020.11.003,journal,Computer Communications,sciencedirect,2021-01-15,sciencedirect,Mobile network traffic pattern classification with incomplete a priori information,https://api.elsevier.com/content/article/pii/S0140366420319812,"
                  In complex networks systems like mobile edge infrastructures, real-time traffic classification according to application types is an enabling technique for network resource optimization and advanced security management. State-of-the-art schemes take advantage of machine learning techniques to train classification models based on behavioral characteristics of network traffic flows. Nonetheless, most existing studies assume complete a priori information of the application classes and formulate the task as a standalone multi-class classification problem. Such classification models cannot properly handle the unknown applications that are absent from the training set during the time of training. In this work, we propose a practical mobile network traffic classification scheme that builds robust classifiers based on incomplete a priori information. Specifically, the core idea is to extract the unknown patterns emerging in the network periodically to complement the initial labeled data set that only consists of a limited number of known applications. We propose two algorithms for the unknown pattern extraction step. One is based on iterative asymmetric binary classification and the other is based on constrained clustering. Empirical results based on a public data set show that the proposed scheme can effectively detect both known and unknown applications.
               ",autonomous vehicle
10.1016/j.anucene.2019.107261,journal,Annals of Nuclear Energy,sciencedirect,2020-06-01,sciencedirect,Secure embedded intelligence in nuclear systems: Framework and methods,https://api.elsevier.com/content/article/pii/S0306454919307716,"We present potential approaches for implementing secure embedded intelligence (SEI) and integrated state awareness (ISA) in advanced nuclear energy (NE) systems. A framework for deliberate design and operations of such systems is provided, and potential risks, challenges and impacts discussed. Activities that should be carried out in research, development, and demonstration (RD&D) opportunities are identified. Resulting implications for operation of ‘smart nuclear systems’, including remote operations, flexible operations, predictive maintenance, inherent digital security, and autonomous controls, are discussed.",autonomous vehicle
10.1016/j.neucom.2018.12.040,journal,Neurocomputing,sciencedirect,2019-03-07,sciencedirect,Video summarization via spatio-temporal deep architecture,https://api.elsevier.com/content/article/pii/S0925231218314966,"
                  Video summarization has unprecedented importance to help us overview current ever-growing amount of video collections. In this paper, we propose a novel dynamic video summarization model based on deep learning architecture. We are the first to solve the imbalanced class distribution problem in video summarization. The over-sampling algorithm is used to balance the class distribution on training data. The novel two-stream deep architecture with the cost-sensitive learning is proposed to handle the class imbalance problem in feature learning. In the spatial stream, RGB images are used to represent the appearance of video frames, and in the temporal stream, multi-frame motion vectors with deep learning framework is firstly introduced to represent and extract temporal information of the input video. The proposed method is evaluated on two standard video summarization datasets and a standard emotional dataset. Empirical validations for video summarization demonstrate that our model achieves performance improvement over the existing and state-of-the-art methods. Moreover, the proposed method is able to highlight the video content with the active level of arousal in affective computing task. In addition, the proposed frame-based model has another advantage. It can automatically preserve the connection between consecutive frames. Although the summary is constructed based on the frame level, the final summary is comprised of informative and continuous segments instead of individual separate frames.
               ",autonomous vehicle
10.1016/j.eng.2018.11.030,journal,Engineering,sciencedirect,2019-04-30,sciencedirect,Advances in Computer Vision-Based Civil Infrastructure Inspection and Monitoring,https://api.elsevier.com/content/article/pii/S2095809918308130,"Computer vision techniques, in conjunction with acquisition through remote cameras and unmanned aerial vehicles (UAVs), offer promising non-contact solutions to civil infrastructure condition assessment. The ultimate goal of such a system is to automatically and robustly convert the image or video data into actionable information. This paper provides an overview of recent advances in computer vision techniques as they apply to the problem of civil infrastructure condition assessment. In particular, relevant research in the fields of computer vision, machine learning, and structural engineering is presented. The work reviewed is classified into two types: inspection applications and monitoring applications. The inspection applications reviewed include identifying context such as structural components, characterizing local and global visible damage, and detecting changes from a reference image. The monitoring applications discussed include static measurement of strain and displacement, as well as dynamic measurement of displacement for modal analysis. Subsequently, some of the key challenges that persist toward the goal of automated vision-based civil infrastructure and monitoring are presented. The paper concludes with ongoing work aimed at addressing some of these stated challenges.",autonomous vehicle
10.1016/S0079-7421(02)80004-4,journal,Psychology of Learning and Motivation,sciencedirect,2002-12-31,sciencedirect,On the computational basis of learning and cognition: Arguments from LSA,https://api.elsevier.com/content/article/pii/S0079742102800044,"
                  This chapter discusses the computational basis of learning and cognition. To deal with a continuously changing environment, living things have three choices: (1) evolve unvarying processes that usually succeed, (2) evolve genetically fixed effector, perceptual, and computational functions that are contingent on the environment, and (3) learn adaptive functions during their lifetimes. The theme of this chapter is the relation between (2) and (3): the nature of evolutionarily determined computational processes that support learning. The principal goal of this chapter has been to suggest that high-dimensional vector space computations based on empirical associations among very large numbers of components could be a close model of a fundamental computational basis of most learning in both verbal and perceptual domains. More powerful representational effects can be brought about by linear inductive combinations of the elements of very large vocabularies than has often been realized. Success of one such model to demonstrate many natural properties of language commonly assumed to be essentially more complex, nonlinear, and/or unlearned, along with evidence and argument that similar computations may serve similar roles in object recognition, are taken to reaffirm the possibility that a single underlying associational mechanism lies behind many more special and complex appearing cognitive phenomena.
               ",autonomous vehicle
10.1016/j.eswa.2021.115060,journal,Expert Systems with Applications,sciencedirect,2021-10-01,sciencedirect,Robo-advisor using genetic algorithm and BERT sentiments from tweets for hybrid portfolio optimisation,https://api.elsevier.com/content/article/pii/S0957417421005017,"
                  Robo-advisors are increasingly popular, with machine learning algorithms taking centre stage for researchers. However, classical financial theories and techniques, such as Constant Rebalancing (CRB) and Modern Portfolio Theory (MPT), can still be relevant by combining them with social media sentiments. In this study, we propose two novel models, namely Sentimental All-Weather (SAW) and Sentimental MPT (SMPT), which capture the up-to-date market conditions through Twitter sentiments via Google’s Bidirectional Transformer (BERT) model. Genetic Algorithm was used to optimise the models for different objectives including maximising cumulative returns and minimising volatility. Trained on tweets and the United States stock data from August 2018 to end December 2019, and tested on an out-of-sample period from January 2020 to April 2020, our proposed models achieved superior performance in terms of common measures of portfolio performance including Sharpe ratio, cumulative returns, and value-at-risk, compared to the following benchmarks: buy-and-hold SPY index, MPT model, and CRB model for an All-Weather Portfolio.
               ",autonomous vehicle
10.1016/S1474-6670(17)33249-4,journal,IFAC Proceedings Volumes,sciencedirect,2001-09-30,sciencedirect,"Intelligent Techniques for Managing Complexity, Changes and Uncertainties in Manufacturing",https://api.elsevier.com/content/article/pii/S1474667017332494,"
                  The application of pattern recognition (PR) techniques, expert systems (ESs), artificial neural networks (ANNs), fuzzy systems (FSs) and nowadays hybrid artificial intelligence (AI) techniques in manufacturing can be regarded as consecutive elements of a process started two decades ago. On the one hand, the paper outlines the most important steps of this process and introduces some new results with special emphasis on hybrid AI and multistrategy machine learning (ML) approaches. On the other hand, agent-based (holonic) systems are highlighted as promising tools for managing complexity, changes and disturbances in production systems. Further integration of approaches is predicted.
               ",autonomous vehicle
10.3182/20090630-4-ES-2003.00085,journal,IFAC Proceedings Volumes,sciencedirect,2009-12-31,sciencedirect,Advanced Pattern Recognition method for the Monitoring of Dynamic Systems,https://api.elsevier.com/content/article/pii/S1474667016358281,"
                  The behavior of a dynamic system assumes different states over the time. In Pattern Recognition methods, each state is represented by a set of similar patterns forming restricted regions in the feature space, called classes. Recognizing the state, or class, of a new incoming pattern is performed using a membership function. In this paper, we propose to develop the supervised classification method Fuzzy Pattern Matching to be in addition a non supervised one. The goal is to monitor dynamic systems with a limited prior knowledge about their functioning. The detection of the occurrence of new states as well as the reinforcement of the estimation of their membership functions are performed online thanks to the combination of supervised and non supervised classification modes. No information in advance about the shape of classes or their number is required to achieve this detection and estimation reinforcement.
               ",autonomous vehicle
10.1016/j.procs.2020.09.092,journal,Procedia Computer Science,sciencedirect,2020-12-31,sciencedirect,Strategic Challenges for Platform-based Intelligent Assistants,https://api.elsevier.com/content/article/pii/S1877050920319906,"Artificial Intelligence-based Assistants AIAs are spreading quickly both in homes and offices. They already have left their original habitats of ""intelligent speakers"" providing easy access to music collections. The initiated a multitude of new devices and are already populating devices such as TV sets. Characteristic for the intelligent digital assistants is the formation of platforms around their core functionality. Thus, AIS capabilities of the assistants are used to offer new services and create new interfaces for business processes. There are positive network effects between the assistants and the services as well as within the services. Therefore, many companies see the need to get involved in the field of digital assistants but lack a framework to align their initiatives with their corporate strategies. In order to lay the foundation for a comprehensive method, we are therefore investigating intelligent digital assistants. Based on this analysis, we are developing a framework of strategic opportunities and challenges.",autonomous vehicle
10.1016/j.neuron.2021.05.021,journal,Neuron,sciencedirect,2021-07-21,sciencedirect,Promises and challenges of human computational ethology,https://api.elsevier.com/content/article/pii/S0896627321003743,"
                  The movements an organism makes provide insights into its internal states and motives. This principle is the foundation of the new field of computational ethology, which links rich automatic measurements of natural behaviors to motivational states and neural activity. Computational ethology has proven transformative for animal behavioral neuroscience. This success raises the question of whether rich automatic measurements of behavior can similarly drive progress in human neuroscience and psychology. New technologies for capturing and analyzing complex behaviors in real and virtual environments enable us to probe the human brain during naturalistic dynamic interactions with the environment that so far were beyond experimental investigation. Inspired by nonhuman computational ethology, we explore how these new tools can be used to test important questions in human neuroscience. We argue that application of this methodology will help human neuroscience and psychology extend limited behavioral measurements such as reaction time and accuracy, permit novel insights into how the human brain produces behavior, and ultimately reduce the growing measurement gap between human and animal neuroscience.
               ",autonomous vehicle
10.1016/j.neucom.2016.12.068,journal,Neurocomputing,sciencedirect,2017-08-09,sciencedirect,New trends in computational intelligence,https://api.elsevier.com/content/article/pii/S0925231217301121,,autonomous vehicle
10.1016/0020-0255(93)90049-R,journal,Information Sciences,sciencedirect,1993-05-31,sciencedirect,Generative learning structures and processes for generalized connectionist networks,https://api.elsevier.com/content/article/pii/002002559390049R,"
                  Massively parallel networks of relatively simple computing elements offer an attractive and versatile framework for exploring a variety of learning structures and processes for intelligent systems. This paper briefly summarizes some popular learning structures and processes used in such networks. It outlines a range of potentially more powerful alternatives for pattern-directed inductive learning in such systems. It motivates and develops a class of new learning algorithms for massively parallel networks of simple computing elements. We call this class of learning processes generative for they offer a set of mechanisms for constructive and adaptive determination of the network architecture—the number of processing elements and the connectivity among them—as a function of experience. Generative learning algorithms attempt to overcome some of the limitations of some approaches to learning in networks that rely on modification of weights on the links within an otherwise fixed network topology, for example, rather slow learning and the need for an a priori choice of network architecture. Several alternative designs as well as a range of control structures and processes that can be used to regulate the form and content of internal representations learned by such networks are examined. Empirical results from the study of some generative learning algorithms are briefly summarized, and several extensions and refinements of such algorithms and directions for future research are outlined.
               ",autonomous vehicle
10.1016/j.rser.2013.08.055,journal,Renewable and Sustainable Energy Reviews,sciencedirect,2014-05-31,sciencedirect,Solar radiation prediction using Artificial Neural Network techniques: A review,https://api.elsevier.com/content/article/pii/S1364032113005959,"
                  Solar radiation data plays an important role in solar energy research. These data are not available for location of interest due to absence of a meteorological station. Therefore, the solar radiation has to be predicted accurately for these locations using various solar radiation estimation models. The main objective of this study is to review Artificial Neural Network (ANN) based techniques in order to identify suitable methods available in the literature for solar radiation prediction and to identify research gaps. The study shows that Artificial Neural Network techniques predict solar radiation more accurately in comparison to conventional methods. The prediction accuracy of ANN models is found to be dependent on input parameter combinations, training algorithm and architecture configurations. Further research areas in ANN technique based methodologies are also identified in the present study.
               ",autonomous vehicle
10.1016/B978-0-12-823905-6.00005-2,journal,Advancements in Intelligent Gas Metal Arc Welding Systems,sciencedirect,2021-12-31,sciencedirect,Chapter 5: Advancement in intelligent GMAW,https://api.elsevier.com/content/article/pii/B9780128239056000052,"
               This chapter describes advancements in intelligent GMAW process. This chapter starts with the descriptions of developments in welding monitoring systems, explaining in details how welding is represented as a complex process, what is weld data sensing, and how the weld data can be managed. The second section focuses on intelligent control of GMAW, namely, the learning methods and definitions, seam tracking, penetration control, bead width, and tack welding control. This chapter further describes welding process contribution toward effective manufacturing, such as development of welding power sources and characteristics of smart power sources.
            ",autonomous vehicle
10.1016/j.neunet.2019.09.015,journal,Neural Networks,sciencedirect,2019-12-31,sciencedirect,Admiring the Great Mountain: A Celebration Special Issue in Honor of Stephen Grossberg’s 80th Birthday,https://api.elsevier.com/content/article/pii/S089360801930276X,"
                  This editorial summarizes selected key contributions of Prof. Stephen Grossberg and describes the papers in this 80th birthday special issue in his honor. His productivity, creativity, and vision would each be enough to mark a scientist of the first caliber. In combination, they have resulted in contributions that have changed the entire discipline of neural networks. Grossberg has been tremendously influential in engineering, dynamical systems, and artificial intelligence as well. Indeed, he has been one of the most important mentors and role models in my career, and has done so with extraordinary generosity and encouragement. All authors in this special issue have taken great pleasure in hereby commemorating his extraordinary career and contributions.
               ",autonomous vehicle
10.1016/B978-0-12-819972-5.00006-9,journal,Drones in Smart-Cities,sciencedirect,2020-12-31,sciencedirect,Chapter Six: AI simulations and programming environments for drones: an overview,https://api.elsevier.com/content/article/pii/B9780128199725000069,"
               Simulators are generally used in the field of robotics, medical applications to virtually understand a system. They help in economizing cost of the actual implementation of robotic systems. The main reason for the simulation of systems (such as surgical robots, etc.) with artificial intelligence (AI) algorithms is to enable them learn how to perform tasks without the aid of any human intervention.
               Artificial Intelligence is “Compulsory” as it plays a very vital role in the simulation of drones. Without it, deploying and maintaining the actual drone system will be extremely expensive. It will also be impossible to program them carry out specific tasks, especially those that endanger human life.
               We generally present an overview of AI Simulations and Programming Environments for Drones in our research. First, we briefly analyze the use of simulators with AI, particularly on drones otherwise known as quadcopter or unmanned aerial vehicles. Second, we compare simulation environments and programming languages used in the development of their system application.
            ",autonomous vehicle
10.1016/j.cogsys.2021.07.012,journal,Cognitive Systems Research,sciencedirect,2021-12-31,sciencedirect,Cognitive computing models for estimation of reference evapotranspiration: A review,https://api.elsevier.com/content/article/pii/S1389041721000620,"
                  Irrigation practices can be advanced by the aid of cognitive computing models. Repeated droughts, population expansion and the impact of global warming collectively impose rigorous restrictions over irrigation practices. Reference evapotranspiration (ET0) is a vital factor to predict the crop water requirements based on climate data. There are many techniques available for the prediction of ET0. An efficient ET0 prediction model plays an important role in irrigation system to increase water productivity. In the present study, a review has been carried out over cognitive computing models used for the estimation of ET0. Review exhibits that artificial neural network (ANN) approach outperforms support vector machine (SVM) and genetic programming (GP). Second order neural network (SONN) is the most promising approach among ANN models.
               ",autonomous vehicle
10.1016/j.patcog.2021.108264,journal,Pattern Recognition,sciencedirect,2022-02-28,sciencedirect,Deep momentum uncertainty hashing,https://api.elsevier.com/content/article/pii/S0031320321004441,"
                  Combinatorial optimization (CO) has been a hot research topic because of its theoretic and practical importance. As a classic CO problem, deep hashing aims to find an optimal code for each data from finite discrete possibilities, while the discrete nature brings a big challenge to the optimization process. Previous methods usually mitigate this challenge by binary approximation, substituting binary codes for real-values via activation functions or regularizations. However, such approximation leads to uncertainty between real-values and binary ones, degrading retrieval performance. In this paper, we propose a novel Deep Momentum Uncertainty Hashing (DMUH). It explicitly estimates the uncertainty during training and leverages the uncertainty information to guide the approximation process. Specifically, we model bit-level uncertainty via measuring the discrepancy between the output of a hashing network and that of a momentum-updated network. The discrepancy of each bit indicates the uncertainty of the hashing network to the approximate output of that bit. Meanwhile, the mean discrepancy of all bits in a hashing code can be regarded as image-level uncertainty. It embodies the uncertainty of the hashing network to the corresponding input image. The hashing bit and image with higher uncertainty are paid more attention during optimization. To the best of our knowledge, this is the first work to study the uncertainty in hashing bits. Extensive experiments are conducted on four datasets to verify the superiority of our method, including CIFAR-10, NUS-WIDE, MS-COCO, and a million-scale dataset Clothing1M. Our method achieves the best performance on all of the datasets and surpasses existing state-of-the-art methods by a large margin.
               ",autonomous vehicle
10.1016/j.adhoc.2019.101880,journal,Ad Hoc Networks,sciencedirect,2019-08-31,sciencedirect,A congestion control framework for delay- and disruption tolerant networks,https://api.elsevier.com/content/article/pii/S1570870518301288,"
                  Delay and Disruption Tolerant Networks (DTNs) are networks that experience frequent and long-lived connectivity disruptions. Unlike traditional networks, such as TCP/IP Internet, DTNs are often subject to high latency caused by very long propagation delays (e.g., interplanetary communication) and/or intermittent connectivity. In DTNs there is no guarantee of end-to-end connectivity between source and destination. Such distinct features pose a number of technical challenges in designing core network functions such as routing and congestion control mechanisms. Detecting and dealing with congestion in DTNs is an important problem since congestion can significantly deteriorate DTN performance. Most existing DTN congestion control mechanisms have been designed for a specific DTN application domain and have been shown to exhibit inadequate performance when used in different DTN scenarios and conditions.
                  In this paper, we introduce Smart-DTN-CC, a novel DTN congestion control framework that adjusts its operation automatically based on the dynamics of the underlying network and its nodes. Smart-DTN-CC is an adaptive and distributed congestion aware framework that mitigates congestion using reinforcement learning, a machine learning technique known to be well suited to problems where: (1) the environment, in this case the network, plays a crucial role; and (2) yet, no prior knowledge about the target environment can be assumed, i.e., the only way to acquire information about the environment is to interact with it through continuous online learning.
                  
                     Smart-DTN-CC nodes receive input from the environment (e.g., buffer occupancy, neighborhood membership, etc), and, based on that information, choose an action to take from a set of possible actions. Depending on the selected action’s effectiveness in controlling congestion, a reward will be given. Smart-DTN-CC’s goal is to maximize the overall reward which translates to minimizing congestion. To our knowledge, Smart-DTN-CC is the first DTN congestion control framework that has the ability to automatically and continuously adapt to the dynamics of the target environment. As demonstrated by our experimental evaluation, Smart-DTN-CC is able to consistently outperform existing DTN congestion control mechanisms under a wide range of network conditions and characteristics.
               ",autonomous vehicle
10.1016/B978-0-12-818576-6.00002-2,journal,Artificial Intelligence to Solve Pervasive Internet of Things Issues,sciencedirect,2021-12-31,sciencedirect,Chapter 2: Knowledge Representation and Reasoning in AI-Based Solutions and IoT Applications,https://api.elsevier.com/content/article/pii/B9780128185766000022,"
               Artificial intelligence (AI)-based solutions, knowledge representation and reasoning, and the Internet of Things applications have transformed how researchers and practitioners view the analytical and computational capabilities. The disruptive evolution of these technologies has encouraged researchers and practitioners to develop integrated AI-based analytical solutions needed for solving pervasive issues affecting computational applications. The capabilities include AI, knowledge Representation and Reasoning and Internet of Things. Such capabilities are designed to support AI-based solutions, knowledge representation and reasoning, and the Internet of Things (IoT) applications. These technology trends involve relevant computational areas, that is, intelligent devices, sensors, autonomous vehicles, robotics, virtual reality, augmented intelligence, and others. The study addresses and validates solutions on how researchers can solve issues that affect AI, knowledge representation and reasoning, and IoT applications.
            ",autonomous vehicle
10.1016/j.jnca.2021.103257,journal,Journal of Network and Computer Applications,sciencedirect,2021-11-13,sciencedirect,Recent advances in energy management for Green-IoT: An up-to-date and comprehensive survey,https://api.elsevier.com/content/article/pii/S1084804521002551,"
                  Internet-of-Things (IoT) refers to the massive network interconnection of objects often equipped with ubiquitous intelligence employed to provide smart services to end users. However, one of the substantial issues of IoT is the limited energy of IoT devices that are expected to run consistently for a long period of time without battery replacement. Moreover, in the wake of pervasive IoT, the number of IoT devices has exploded and lead to a tremendous rise in IoT networks carbon footprint. In this regard, Green-IoT and energy management of IoT emerged as challenging and attractive research topics for both academia and industry. In this paper, we conduct a comprehensive and an up-to-date survey on recent energy management techniques in IoT networks. We start by presenting the challenges of energy consumption in IoT networks. Then, we will present novel and well-known energy management approaches for IoT but focus on the most recent solutions proposed in each approach. Next, we will provide a comprehensive survey of the most recent energy management solutions for IoT ecosystem. We will also present recent trends and new research perspectives that can be exploited for energy conservation in IoT networks. Finally, we will give recommendations on how to exploit the techniques presented in our survey to achieve the IoT applications QoS requirements.
               ",autonomous vehicle
10.1016/S0885-2014(97)90023-X,journal,Cognitive Development,sciencedirect,1997-12-31,sciencedirect,"Rethinking innateness, learning, and constructivism: Connectionist perspectives on development",https://api.elsevier.com/content/article/pii/S088520149790023X,,autonomous vehicle
10.1016/j.knosys.2018.07.044,journal,Knowledge-Based Systems,sciencedirect,2018-12-01,sciencedirect,A unified knowledge compiler to provide support the scientific community,https://api.elsevier.com/content/article/pii/S0950705118303952,"
                  The scientific community represents an insatiable network with important needs related to the searching of information. The ever-broadening amount of domain-scientific on-line information that can be found requires increasingly sophisticated frameworks to manage it. Nevertheless, these frameworks are usually focused on specific useful functionalities and work more like expert systems than general purpose approaches. In order to ease the research process to the scientific community, the Unified Knowledge Compiler (UNIKO) framework is presented. This framework includes, among other functionalities, the recommendation of articles and authors related to a specific field of application, the evaluation of reputation scores of articles and authors, and the sentiment analysis of the texts of the articles. UNIKO is built as a hybrid framework based on Knowledge-Based Systems and Content-Based Recommendation Systems. In order to evaluate the performance of the system, several experiments have been done. The first experiment is developed to illustrate the reputation scoring task. The second one addresses the sentiment scores calculation based on a lexicon which is supported by a Convolutional Neural Network. The last experiment shows the recommendation tasks based on specific similarity measures and unsupervised learning.
               ",autonomous vehicle
10.1016/j.robot.2019.103312,journal,Robotics and Autonomous Systems,sciencedirect,2020-01-31,sciencedirect,Multimodal representation models for prediction and control from partial information,https://api.elsevier.com/content/article/pii/S0921889019301575,"
                  Similar to humans, robots benefit from interacting with their environment through a number of different sensor modalities, such as vision, touch, sound. However, learning from different sensor modalities is difficult, because the learning model must be able to handle diverse types of signals, and learn a coherent representation even when parts of the sensor inputs are missing. In this paper, a multimodal variational autoencoder is proposed to enable an iCub humanoid robot to learn representations of its sensorimotor capabilities from different sensor modalities. The proposed model is able to (1) reconstruct missing sensory modalities, (2) predict the sensorimotor state of self and the visual trajectories of other agents actions, and (3) control the agent to imitate an observed visual trajectory. Also, the proposed multimodal variational autoencoder can capture the kinematic redundancy of the robot motion through the learned probability distribution. Training multimodal models is not trivial due to the combinatorial complexity given by the possibility of missing modalities. We propose a strategy to train multimodal models, which successfully achieves improved performance of different reconstruction models. Finally, extensive experiments have been carried out using an iCub humanoid robot, showing high performance in multiple reconstruction, prediction and imitation tasks.
               ",autonomous vehicle
10.1016/j.patcog.2021.108084,journal,Pattern Recognition,sciencedirect,2021-12-31,sciencedirect,Deep robust multilevel semantic hashing for multi-label cross-modal retrieval,https://api.elsevier.com/content/article/pii/S0031320321002715,"
                  Hashing based cross-modal retrieval has recently made significant progress. But straightforward embedding data from different modalities involving rich semantics into a joint Hamming space will inevitably produce false codes due to the intrinsic modality discrepancy and noises. We present a novel deep Robust Multilevel Semantic Hashing (RMSH) for more accurate multi-label cross-modal retrieval. It seeks to preserve fine-grained similarity among data with rich semantics,i.e., multi-label, while explicitly require distances between dissimilar points to be larger than a specific value for strong robustness. For this, we give an effective bound of this value based on the information coding-theoretic analysis, and the above goals are embodied into a margin-adaptive triplet loss. Furthermore, we introduce pseudo-codes via fusing multiple hash codes to explore seldom-seen semantics, alleviating the sparsity problem of similarity information. Experiments on three benchmarks show the validity of the derived bounds, and our method achieves state-of-the-art performance.
               ",autonomous vehicle
10.1016/B978-0-12-810408-8.00005-5,journal,Deep Learning for Medical Image Analysis,sciencedirect,2017-12-31,sciencedirect,Chapter 3: Efficient Medical Image Parsing,https://api.elsevier.com/content/article/pii/B9780128104088000055,"
               
                  Fast and robust detection, segmentation and tracking of anatomical structures or pathologies support the entire clinical workflow enabling real-time guidance, quantification, and processing in the operating room. Most state-of-the-art solutions for parsing medical images are based on machine learning methods. While this enables the effective use of large annotated image databases, such techniques typically suffer from inherent limitations related to the efficiency in scanning high-dimensional parametric spaces and the learning of representative features for modeling the object appearance. In this context we present Marginal Space Deep Learning, a novel framework for volumetric image parsing which exploits both the strengths of efficient object parametrization in hierarchical marginal spaces and the representational power of state-of-the-art deep learning architectures. The system learns classifiers in clustered, high-probability regions of the parameter space capturing the appearance of the object under the considered pose transformations and shape variations, gradually increasing the dimensionality of the exploration space from translation (3D), translation–orientation (6D) to incorporating also the anisotropic scaling (9D) and shape variability (ND). During runtime the system uses the learned classifiers to exhaustively scan these spaces to select the most probable transformation parameters. As this implies a significant computational effort in the order of billions of scanning hypotheses we propose cascaded sparse adaptive neural networks, learning to focus the data sampling patterns of the networks on sparse, context-rich parts of the input, thereby considerably reducing the runtime and increasing the robustness of the system. While we show that this method significantly increases the performance of the state-of-the-art, we highlight its main limitation: the learning of the appearance model and the parameter scanning are completely decoupled as independent algorithmic steps. To address this we make a step toward human-like intelligent parsing, presenting an extension of the system that models the object appearance and the parameter search as a unified behavioral task for an artificial agent. As opposed to exhaustively scanning the parameter space, the system uses reinforcement learning to discover optimal navigation paths guiding the search to the optimal location. We show the initial performance of this approach on the detection of arbitrary landmarks in ultrasound, magnetic resonance, and computed tomography data, with considerable improvement over the state-of-the-art. Our future work is focused on extending this framework for generic image parsing.
            ",autonomous vehicle
10.1016/j.neucom.2013.07.055,journal,Neurocomputing,sciencedirect,2014-06-25,sciencedirect,Mobile robots׳ modular navigation controller using spiking neural networks,https://api.elsevier.com/content/article/pii/S0925231214000976,"
                  Autonomous navigation plays an important role in mobile robots. Artificial neural networks (ANNs) have been successfully used in nonlinear systems whose models are difficult to build. However, the third generation neural networks – Spiking neural networks (SNNs) – contain features that are more attractive than those of traditional neural networks (NNs). Because SNNs convey both temporal and spatial information, they are more suitable for mobile robots׳ controller design. In this paper, a modular navigation controller based on promising spiking neural networks for mobile robots is presented. The proposed behavior-based target-approaching navigation controller, in which the reactive architecture is used, is composed of three sub-controllers: the obstacle-avoidance SNN controller, the wall-following SNN controller and the goal-approaching controller. The proposed modular navigation controller does not require accurate mathematical models of the environment, and is suitable to unknown and unstructured environments. Simulation results show that the proposed transition conditions for sub-controllers are feasible. The navigation controller can control the mobile robot to reach a target successfully while avoiding obstacles and following the wall to get rid of the deadlock caused by local minimum.
               ",autonomous vehicle
10.1016/bs.host.2018.07.006,journal,Handbook of Statistics,sciencedirect,2018-12-31,sciencedirect,Chapter 9: Deep Neural Networks for Natural Language Processing,https://api.elsevier.com/content/article/pii/S016971611830021X,"
                  This chapter discusses the application of deep neural networks for natural language processing. First, we discuss word vector representation followed by feedforward neural networks. Next, training of deep neural network models and their optimization are discussed. Regularization for deep learning is discussed in detail. Application of deep neural approaches to language modeling is described. Lastly, convolutional neural networks and memory are described.
               ",autonomous vehicle
10.1016/j.asoc.2020.106417,journal,Applied Soft Computing,sciencedirect,2020-09-30,sciencedirect,Universal Functions Originator,https://api.elsevier.com/content/article/pii/S1568494620303574,"
                  Nowadays, couples of computing systems have been introduced to perform many applications, such as function approximation, pattern classification, categorization/clustering, forecasting/prediction, control, and optimization. Linear regression (LR) is commonly used for simple data where the relation between its coefficients is linear, while nonlinear regression (NLR) is used when that relation is nonlinear. Artificial neural networks (ANNs) and support vector machines (SVMs) are more efficient and they can be used for complex applications. However, each one of these approaches has its own strengths and weaknesses. This study introduces a new computing system called “universal functions originator (UFO)”. This system is a new symbolic regression (SR) technique that can generate mathematical models universally through two independent optimization algorithms. Different arithmetic operators can be entered into the search pool. Also, any analytic function can be dragged into that pool. UFO has been mathematically designed and practically tested with function approximation problems. However, UFO can also be used for the applications listed above, including anomaly detection, function complication, function simplification, dimension expansion, dimension reduction, and high-dimensional function visualization. This novel computing system shows an impressive performance with many promising uses and distinct capabilities. This study reveals the mechanism of UFO and solves some numerical problems via an advanced graphical user interface (GUI) designed just to validate the process of this computing system.
               ",autonomous vehicle
10.1016/B978-012443875-0/50032-X,journal,Knowledge-Based Systems,sciencedirect,2000-12-31,sciencedirect,31: Automatic Learning Approaches for Electric Power Systems,https://api.elsevier.com/content/article/pii/B978012443875050032X,"
               This chapter describes a methodology based on the combination of probabilistic reasoning, automatic learning, and Monte Carlo simulations, which has been used extensively for the study of electric power systems. Electric power systems are essentially large, complex nonlinear systems that have grown significantly in size and importance during the last 50 years. The very large size of electric power systems makes understanding them and the mastering of their reliability a quite complex problem. An interconnected system is operated by a number of independent companies that have to make decisions without knowing precisely the strategy of their neighbors. Thus, electric power systems are also a good example of distributed decision making under uncertainties. The methodology described in this chapter is a “computer experiment” type of method. This chapter describes the generic approach together with the principles of the main classes of automatic learning methods while also discussing a few real-life applications and some new research directions. This chapter concludes with a discussion of the usefulness of the proposed approach and its applicability to the study of complex systems in general.
            ",autonomous vehicle
10.1016/j.engappai.2021.104406,journal,Engineering Applications of Artificial Intelligence,sciencedirect,2021-09-30,sciencedirect,"An insight into crash avoidance and overtaking advice systems for Autonomous Vehicles: A review, challenges and solutions",https://api.elsevier.com/content/article/pii/S0952197621002542,"
                  Emergence of communication technologies made the automotive industries across the globe to embrace Advanced Driver Assistance Systems (ADAS) by considerable investments to ensure accident-free travel, reduction of pollution, fuel conservation. ADAS achieves its goals by integrating complex subsystems such as obstacle avoidance, overtaking advice, lane changing assistance, planning shortest routes, parking assistance, automatic gear shifting, etc., using the emerging technologies. This article emphasizes the road safety aspect of the ADAS by exploring Crash Avoidance and Overtaking Advice (CAOA) subsystems. Existing studies have a noticeable lack of connectivity between various aspects of CAOA subsystems. This review deeply explores and connects CAOA subsystems like road geometries, road debris, obstacle avoidance algorithms powered by Artificial Intelligence (AI), overtaking advice systems, perception challenges of human drivers in various light and weather conditions, driver inattention and misjudgments, vehicle blind-spots, vehicle parameter analysis, performance of vision sensors, in-vehicle computers, driver–vehicle interactions, Vehicle to Infrastructure (V2I) technologies. This article emphasizes the three primary performance metrics of the ADAS, namely accuracy, response time and robustness. Finally, this article discusses a typical functional architecture and gaps identified in existing studies. This article is structured to assist like-minded researchers, who work on CAOA systems for road safety.
               ",autonomous vehicle
10.1016/j.neucom.2012.04.012,journal,Neurocomputing,sciencedirect,2012-09-15,sciencedirect,Thalamic cooperation between the cerebellum and basal ganglia with a new tropism-based action-dependent heuristic dynamic programming method,https://api.elsevier.com/content/article/pii/S0925231212003487,"
                  In order to explore the possible cooperation mechanism between the cerebellum and basal ganglia in the central nervous system and to establish a more intelligent learning mechanism for robots, a new tropism-based ADHDP (action-dependent heuristic dynamic programming) learning mechanism involving the cortico-basal ganglia and cerebellar circuitry and the thalamic function is proposed. The cerebellum specializes in the actor part, while the basal ganglia are related to critic prediction. The thalamic function is considered as the tropism mechanism. Tropism value denoting the biological propensity is introduced to illustrate the degree of closing to the target. Although several motor control models have been proposed to explain the control and learning mechanism in the cerebellum and basal ganglia separately, it seems that the cooperation mechanism between them has not received much attention. In our proposed learning mechanism, the thalamic function and the cooperation between the cerebellum and basal ganglia are considered, and with a neurophysiological view, a striato-striatal lateral weight in the basal ganglia was added in the critic network. We present the detailed design architecture and explain how effective learning and optimization can be achieved with this novel tropism-based ADHDP architecture. Furthermore, we test its performance on the balance learning task of a two-wheeled self-balancing robot (TWSBR), which simulates the typical motor control and learning of the human body. In order to illustrate the effect of the thalamic function, some comparison researches about the balance learning problem have been done.
               ",autonomous vehicle
10.1016/j.procs.2017.08.343,journal,Procedia Computer Science,sciencedirect,2017-12-31,sciencedirect,Classification of Asthma Severity and Medication Using TensorFlow and Multilevel Databases,https://api.elsevier.com/content/article/pii/S1877050917317532,"Escalating cost of treating chronic diseases demand that they be, to the extent possible, self-managed by the patients. In self-management of disease an imperative is to predict, the possible future state of morbidity (at time, T¹), given the present precursor conditions (at time, Tº) and expected precursor condition (at time, T¹). This paper reports the results of a study to evaluate the potential use of using TensorFlow and Inpatient Databases at national level and hospital level for predicting the asthma severity. Methods of Deep Neural Networks (DNN) have been deployed in classification of morbidity conditions, as well as treatment options. The results indicate that training a DNN to predict asthma severity level or the imminence of an asthma attack is possible.",autonomous vehicle
10.1016/j.knosys.2021.107428,journal,Knowledge-Based Systems,sciencedirect,2021-11-14,sciencedirect,Dual-graph convolutional network based on band attention and sparse constraint for hyperspectral band selection,https://api.elsevier.com/content/article/pii/S0950705121006900,"
                  Band selection is a research hotspot in hyperspectral image processing. The continuity of the spectral bands causes the adjacent bands to be highly correlated, and correlation among long-range bands is possible with hundreds of spectral bands. Most existing deep learning methods fail to make full use of the inter-band correlation for band selection. In this paper, a novel dual-graph convolutional network based on band attention and a sparse constraint is proposed for band selection. The network consists of two branches. In the attention branch, band-based dual graphs are constructed to encode the contextual correlation of adjacent bands and the structural correlation of long-range bands into non-Euclidean space. Subsequently, the graph convolution-based band attention mechanism is devised to aggregate the band information in the band-based dual graphs and to generate the attention map for all bands. The band attention map is sparsely constrained and embedded as a mask into the trunk branch. In the trunk branch, sample-based dual graphs are constructed to represent the topological information of the samples in the spectral and spatial domains. Furthermore, a dense graph convolutional network is designed to extract and fuse the spatial–spectral and topological features from the shallow to deep layers for classification. A soft-shifting optimization strategy is implemented by defining a new loss from full bands and selected bands to solve the optimization problem caused by the sparse constraint. In this manner, band selection, feature extraction, and classification can be combined into an end-to-end trainable network. The experimental results on representative hyperspectral image datasets demonstrate the superiority of the proposed method over current state-of-the-art band selection methods.
               ",autonomous vehicle
10.1016/B978-1-78548-039-3.50003-1,journal,Intelligence in Energy,sciencedirect,2017-12-31,sciencedirect,3: Intelligence for Energy,https://api.elsevier.com/content/article/pii/B9781785480393500031,"
               The methods described in the previous chapter are useful in specific situations. Energy management is a complex problem involving people, policies, businesses, the global stock of materials and the planet. Each of these stakeholders has different goals, motivations and influences. Certainly, we can use Artificial Intelligence (AI) methods and tools to help solve problems related to managing the whole lifecycle of energy; however, without considering all these factors as a whole, our solution will be partial.
            ",autonomous vehicle
10.1016/0370-1573(91)90146-D,journal,Physics Reports,sciencedirect,1991-09-30,sciencedirect,Neural networks and applications tutorial,https://api.elsevier.com/content/article/pii/037015739190146D,"
                  The importance of neural networks has grown dramatically during this decade. While only a few years ago they were primarily of academic interest, now dozens of companies and many universities are investigating the potential use of these systems and products are beginning to appear.
                  The idea of building a machine whose architecture is inspired by that of the brain has roots which go far back in history. Nowadays, technological advances of computers and the availability of custom integrated circuits, permit simulations of hundreds or even thousands of neurons. In conjunction, the growing interest in learning machines, non-linear dynamics and parallel computation spurred renewed attention in artificial neural networks.
                  Many tentative applications have been proposed, including decision systems (associative memories, classifiers, data compressors and optimizers), or parametric models for signal processing purposes (system identification, automatic control, noise canceling, etc.). While they do not always outperform standard methods, neural network approaches are already used in some real world applications for pattern recognition and signal processing tasks.
                  The tutorial is divided into six lectures, that where presented at the Third Graduate Summer Course on Computational Physics (September 3–7, 1990) on Parallel Architectures and Applications, organized by the European Physical Society: (1) Introduction: machine learning and biological computation. (2) Adaptive artificial neurons (perceptron, ADALINE, sigmoid units, etc.): learning rules and implementations. (3) Neural network systems: architectures, learning algorithms. (4) Applications: pattern recognition, signal processing, etc. (5) Elements of learning theory: how to build networks which generalize. (6) A case study: a neural network for on-line recognition of handwritten alphanumeric characters.
               ",autonomous vehicle
10.1016/S1474-6670(17)50801-0,journal,IFAC Proceedings Volumes,sciencedirect,1992-06-30,sciencedirect,A Target-Directed Neurally Controlled Vehicle,https://api.elsevier.com/content/article/pii/S1474667017508010,"
                  An investigation of a neurally controlled vehicle in a computer-simulated parcours with dynamically changing obstacles is presented. The purpose was to judge the applicability of commercially available neural net shells in process control and automation. The chosen shell provides capabilities for associative memories by unsupervised learning, and for supervised learning by means of the back-propagation algorithm. This algorithm is furthermore enhanced by the functional link approach of Pao ([Pao89]). The vehicle and its environment are displayed graphically. The task for the vehicle is to find its way from a user-defined starting point to an ending point. The neural net is responsible for control of the alternating behaviors of target-orientation and obstacle avoidance. We use ten input neurons, nine representing sensors that deliver information about the distance from non-passable areas in any direction. The tenth sensor is responsible for locating the target in a compass-like way. Three output neurons determine one out of seven possible steering directions. The network was trained off-line, with patterns generated schematically by a program. The results are discussed and further refinements proposed.
               ",autonomous vehicle
10.1016/j.procs.2017.11.455,journal,Procedia Computer Science,sciencedirect,2017-12-31,sciencedirect,Support Vector Machine Optimized by Elephant Herding Algorithm for Erythemato-Squamous Diseases Detection,https://api.elsevier.com/content/article/pii/S1877050917327035,"Machine learning algorithms are used in numerous field and medicine is one of them. Automatic diagnosis or detection of different diseases based on list of symptoms can drastically improve and speedup diagnostics process. Determining diagnosis at earlier stages gives better healing results. In this paper a method for automatic erythemato-squamous diseases classification was proposed. Six erythemato-squamous diseases that are very hard to distinguish were classified by the optimized support vector machine. Recent swarm intelligent algorithm, elephant herding optimization algorithm was used to find optimal parameters for the support vector machine that was then used to determine the exact erythemato-squamous diseases. We compared accuracy of our proposed method to other approaches from literature using standard dataset and it obtained better results in all experiments.",autonomous vehicle
10.1016/j.procs.2020.06.027,journal,Procedia Computer Science,sciencedirect,2020-12-31,sciencedirect,Enhanced Image Restoration by GANs using Game Theory,https://api.elsevier.com/content/article/pii/S1877050920315313,"We propose a deep learning Generative Adversarial Network (GAN) which has two major components Generator Model in which our network is fed with input data and it attempts to figure out the joint probability distribution of the input data in order to generate more data points using the same distribution. We are using variational auto encoders, class-based generators and other generator models. Second model being Discriminator Model in which the network is a simple classifier model which classifies images produced by generator as either same as target image or different from target image. Throughout the process our aim is to get images from generator closest to target image’s probability distribution. This entails a competitive environment between the generator and discriminator. Game theory consists of two important concepts regarding this competition known as Minmax Algorithm and Nash Equilibrium Using the above two concepts in conjugation with the negative-f divergence loss, we restore images using GANs eliminating the prevalent issue of overfitting among contemporary networks and thereby enhancing the image restoration performance of our GAN network.",autonomous vehicle
10.1016/B978-0-12-820472-6.00048-7,journal,Reference Module in Biomedical Sciences,sciencedirect,2021-12-31,sciencedirect,Targeting GPCRs Via Multi-Platforms Arrays and AI,https://api.elsevier.com/content/article/pii/B9780128204726000487,"
               G Protein-Coupled Receptors (GPCRs) are the largest superfamily of proteins, able to perform a wide range of functions, depending on their specific sequence, three-dimensional (3D) structure, ligand-coupling, and subsequent signaling pathway. There is still a considerable number of questions that remain unanswered concerning their biological mechanism, as these are Membrane Proteins (MPs), particularly hard to experimentally characterize. Computational methodologies are privileged approaches and Artificial Intelligence (AI), in particular, is an attractive, innovative set of mathematical algorithms/methods that is capable of providing fundamental knowledge on this challenging subject.
               In this article, we covered over 60 cases where AI was successfully applied to boost the characterization of diverse GPCRs. Furthermore, we made a thorough review of existing Drug-Target Interaction (DTI) prediction methods associated with GPCRs and their respective ligands since these are major AI applications in the field. In all subsections, we explained the more relevant methods while exposing the most pertinent examples.
            ",autonomous vehicle
10.1016/B978-0-12-823504-1.00016-7,journal,Deep Learning Models for Medical Imaging,sciencedirect,2022-12-31,sciencedirect,Index,https://api.elsevier.com/content/article/pii/B9780128235041000167,Unknown,autonomous vehicle
10.1016/j.comcom.2020.07.035,journal,Computer Communications,sciencedirect,2020-09-01,sciencedirect,A taxonomy of AI techniques for 6G communication networks,https://api.elsevier.com/content/article/pii/S0140366420318478,"
                  With 6G flagship program launched by the University of Oulu, Finland, for full future adaptation of 6G by 2030, many institutes worldwide have started to explore various issues and challenges in 6G communication networks. 6G offers ultra high-reliable and massive ultra-low latency while opening the doors for many applications currently not viable by today’s 4G and 5G communication standards. The current 5G technology has security and privacy issues which makes its usage in limited applications. In such an environment, we believe that AI can offer efficient solutions for the aforementioned issues having low communication overhead cost. Keeping focus on all these issues, in this paper, we presented a comprehensive survey on AI-enabled 6G communication technology, which can be used in wide range of future applications. In this article, we explore how AI can be integrated into different applications such as object localization, UAV communication, surveillance, security and privacy preservation etc. Finally, we discussed a use case that shows the adoption of AI techniques in intelligent transport system.
               ",autonomous vehicle
10.1016/B978-0-12-823504-1.00016-7,journal,Deep Learning Models for Medical Imaging,sciencedirect,2022-12-31,sciencedirect,Index,https://api.elsevier.com/content/article/pii/B9780128235041000167,Unknown,autonomous vehicle
10.1016/j.infsof.2020.106296,journal,Information and Software Technology,sciencedirect,2020-07-31,sciencedirect,Testing and verification of neural-network-based safety-critical control software: A systematic literature review,https://api.elsevier.com/content/article/pii/S0950584920300471,"Context: Neural Network (NN) algorithms have been successfully adopted in a number of Safety-Critical Cyber-Physical Systems (SCCPSs). Testing and Verification (T&V) of NN-based control software in safety-critical domains are gaining interest and attention from both software engineering and safety engineering researchers and practitioners. Objective: With the increase in studies on the T&V of NN-based control software in safety-critical domains, it is important to systematically review the state-of-the-art T&V methodologies, to classify approaches and tools that are invented, and to identify challenges and gaps for future studies. Method: By searching the six most relevant digital libraries, we retrieved 950 papers on the T&V of NN-based Safety-Critical Control Software (SCCS). Then we filtered the papers based on the predefined inclusion and exclusion criteria and applied snowballing to identify new relevant papers. Results: To reach our result, we selected 83 primary papers published between 2011 and 2018, applied the thematic analysis approach for analyzing the data extracted from the selected papers, presented the classification of approaches, and identified challenges. Conclusion: The approaches were categorized into five high-order themes, namely, assuring robustness of NNs, improving the failure resilience of NNs, measuring and ensuring test completeness, assuring safety properties of NN-based control software, and improving the interpretability of NNs. From the industry perspective, improving the interpretability of NNs is a crucial need in safety-critical applications. We also investigated nine safety integrity properties within four major safety lifecycle phases to investigate the achievement level of T&V goals in IEC 61508-3. Results show that correctness, completeness, freedom from intrinsic faults, and fault tolerance have drawn most attention from the research community. However, little effort has been invested in achieving repeatability, and no reviewed study focused on precisely defined testing configuration or defense against common cause failure.",autonomous vehicle
10.1016/B978-0-12-817976-5.00001-2,journal,Data Science Applied to Sustainability Analysis,sciencedirect,2021-12-31,sciencedirect,Chapter 1: Overview of data science and sustainability analysis,https://api.elsevier.com/content/article/pii/B9780128179765000012,"
               Globally, challenges related to sustainability abound, including improving air and water quality, reducing food and water consumption, decreasing waste, enhancing energy efficiency and the share of renewable energy, and conserving ecologically valuable lands. One of the most pressing sustainability-related challenges is reducing greenhouse gas emissions that contribute to climate change while developing environmentally-sound adaptation strategies. Simultaneously, advancing the societal aspect of sustainability is critical, but challenging as large portions of the world’s population live below the International Poverty Line. Data science, including different statistical machine learning techniques, is a tool that will see increasing use in efforts to tackle sustainability challenges. Leveraging the growing volumes of data such as satellite imagery, continuous sensor data from industrial processes, social media data, and data from environmental sensors, requires such techniques. This book provides case studies and examples at the intersection of data science and sustainability in the areas of environmental quality and sustainability, energy and water, sustainable systems analysis, and society and policy.
            ",autonomous vehicle
10.1016/B978-0-08-051055-2.50029-8,journal,Machine Learning,sciencedirect,1990-12-31,sciencedirect,20: CONNECTIONIST LEARNING PROCEDURES,https://api.elsevier.com/content/article/pii/B9780080510552500298,"
               A major goal of research on networks of neuronlike processing units is to discover efficient learning procedures that allow these networks to construct complex internal representations of their environment. The learning procedures must be capable of modifying the connection strengths in such a way that internal units that are not part of the input or output come to represent important features of the task domain. Several interesting gradient-descent procedures have recently been discovered. Each connection computes the derivative, with respect to the connection strength, of a global measure of the error in the performance of the network. The strength is then adjusted in the direction that decreases the error. These relatively simple, gradient-descent learning procedures work well for small tasks, and the new challenge is to find ways of improving their convergence rate and their generalization abilities so that they can be applied to larger, more realistic tasks.
            ",autonomous vehicle
10.1016/B978-0-12-417049-0.00008-0,journal,Introduction to Mobile Robot Control,sciencedirect,2014-12-31,sciencedirect,8: Mobile Robot Control IV: Fuzzy and Neural Methods,https://api.elsevier.com/content/article/pii/B9780124170490000080,"
               Fuzzy logic systems (or, simply, fuzzy systems, FSs) and neural networks are universal approximators, that is, they can approximate any nonlinear function (mapping) with any desired accuracy, and have found wide application in the identification, planning, and model-free control of complex nonlinear systems, such as robotic systems and industrial processes. Fuzzy logic offers a linguistic (approximate) way of drawing conclusions from uncertain data, and neural networks offer the capability of learning and training with or without a teacher (supervisor).
               The objectives of this chapter are the following: (i) to provide a brief introduction to neural networks and fuzzy systems, (ii) to derive and discuss the general structure of fuzzy and neural robot controllers, (iii) to provide the details of mobile (nonholonomic) fuzzy tracker controller design, (iv) to fuzzy the model-based sliding mode controller and apply it to mobile robots, and (v) to solve the mobile adaptive tracking controller design problem using multilayer perceptrons and radial basis function neural networks.
            ",autonomous vehicle
10.1016/j.fuel.2021.120243,journal,Fuel,sciencedirect,2021-05-15,sciencedirect,"A complete review on biochar: Production, property, multifaceted applications, interaction mechanism and computational approach",https://api.elsevier.com/content/article/pii/S0016236121001198,"
                  Burning crop residues release large amounts of greenhouse gases, particulate matter, carbon monoxide, etc. which influence a lot of environmental issues that are hazardous to all living beings including humans. One of the useful methods of using crop residues is in the form of biochar obtained after employing thermo-chemical routes. Apart from the crop residues, the carbon rich contents obtained from forest, animal compost, waste plastics, etc., can be heated in oxygen starved atmosphere, that left char having enriched carbon and trace amount of minerals finds extensive applications in agriculture, especially to make soil more fertile, as a carbon sequestration agent, increases crops yields, etc. This review focuses on the synthetic strategies adopted to obtain biochar, and the numerous applications in various fields. Further, the interactions involved in between the host–guest molecules are explained using computational studies like Density Functional Theory, Artificial Neural Network analysis, Machine Learning methods, and Visual MINTEQ program.
               ",autonomous vehicle
10.1016/B978-0-12-822800-5.00009-3,journal,"Pathogenesis, Treatment and Prevention of Leishmaniasis",sciencedirect,2021-12-31,sciencedirect,Chapter 4: Recent advances on computational approach towards potential drug discovery against leishmaniasis,https://api.elsevier.com/content/article/pii/B9780128228005000093,"
               Leishmaniasis is a complex disease; it is classified under neglected diseases and it causes significant health problems in many developing countries. Even though some drugs are available to treat the lethal form of leishmaniasis, they are associated with toxicity and also there are reports of the emergence of drug-resistant strains. Therefore it is a prerequisite to search novel and effective drugs to manage leishmaniasis. Hence, various approaches have been used to develop an effective drug against leishmaniasis. In this regard, many computational approaches are being applied for accelerating and economizing drug discovery and development. Over the past few decades, computational drug development techniques such as molecular docking, virtual screening, pharmacophore modeling, and artificial intelligence, etc. have impacted the new directions of drug discovery. This chapter presents recent computer-aided approaches against leishmaniasis disease, which will be beneficial for researchers.
            ",autonomous vehicle
10.1016/j.artint.2005.04.006,journal,Artificial Intelligence,sciencedirect,2005-09-30,sciencedirect,Protocols from perceptual observations,https://api.elsevier.com/content/article/pii/S0004370205000986,"This paper presents a cognitive vision system capable of autonomously learning protocols from perceptual observations of dynamic scenes. The work is motivated by the aim of creating a synthetic agent that can observe a scene containing interactions between unknown objects and agents, and learn models of these sufficient to act in accordance with the implicit protocols present in the scene. Discrete concepts (utterances and object properties), and temporal protocols involving these concepts, are learned in an unsupervised manner from continuous sensor input alone. Crucial to this learning process are methods for spatio-temporal attention applied to the audio and visual sensor data. These identify subsets of the sensor data relating to discrete concepts. Clustering within continuous feature spaces is used to learn object property and utterance models from processed sensor data, forming a symbolic description. The progol Inductive Logic Programming system is subsequently used to learn symbolic models of the temporal protocols presented in the presence of noise and over-representation in the symbolic data input to it. The models learned are used to drive a synthetic agent that can interact with the world in a semi-natural way. The system has been evaluated in the domain of table-top game playing and has been shown to be successful at learning protocol behaviours in such real-world audio-visual environments.",autonomous vehicle
10.1016/j.compchemeng.2021.107529,journal,Computers & Chemical Engineering,sciencedirect,2021-12-31,sciencedirect,Data-centric process systems engineering: A push towards PSE 4.0,https://api.elsevier.com/content/article/pii/S0098135421003070,"
                  Process Systems Engineering (PSE) is now a mature field with a well-established body of knowledge, computational-oriented frameworks and methodologies designed and implemented for addressing chemical processes related problems spanning a wide range of scales in time and space. A common feature of many PSE approaches relies in their mostly deductive nature, based on a deep understanding of the underlying Chemical Engineering Science. Given the current data-intensive industrial and societal contexts, new sources of process or product information are now easily made available and should be exploited to complement and expand the classical PSE paradigm with inductive data-driven reasoning and knowledge discovery methodologies. In this article, based upon our over 25 years of research and teaching experience in the field, we discuss the scope and trends of this PSE evolution, refer to several relevant Data-Centric PSE approaches, and identify the main components, applications and future opportunities of this PSE 4.0 perspective.
               ",autonomous vehicle
10.1016/j.robot.2019.03.005,journal,Robotics and Autonomous Systems,sciencedirect,2019-08-31,sciencedirect,A Survey of Knowledge Representation in Service Robotics,https://api.elsevier.com/content/article/pii/S0921889018303506,"
                  Within the realm of service robotics, researchers have placed a great amount of effort into learning, understanding, and representing motions as manipulations for task execution by robots. The task of robot learning and problem-solving is very broad, as it integrates a variety of tasks such as object detection, activity recognition, task/motion planning, localization, knowledge representation and retrieval, and the intertwining of perception/vision and machine learning techniques. In this paper, we solely focus on knowledge representations and notably how knowledge is typically gathered, represented, and reproduced to solve problems as done by researchers in the past decades. In accordance with the definition of knowledge representations, we discuss the key distinction between such representations and useful learning models that have extensively been introduced and studied in recent years, such as machine learning, deep learning, probabilistic modeling, and semantic graphical structures. Along with an overview of such tools, we discuss the problems which have existed in robot learning and how they have been built and used as solutions, technologies or developments (if any) which have contributed to solving them. Finally, we discuss key principles that should be considered when designing an effective knowledge representation.
               ",autonomous vehicle
10.1016/j.cogsys.2018.04.008,journal,Cognitive Systems Research,sciencedirect,2018-10-31,sciencedirect,"On the quest to recreate intelligence. Review of <ce:italic>Exploring robotic minds: Actions, symbols, and consciousness as self-organizing dynamic phenomena</ce:italic>, Jun Tani. Oxford University Press (2017). 328 pp.",https://api.elsevier.com/content/article/pii/S1389041718301190,Unknown,autonomous vehicle
10.1016/j.oceaneng.2021.109380,journal,Ocean Engineering,sciencedirect,2021-09-01,sciencedirect,Collision-avoidance navigation systems for Maritime Autonomous Surface Ships: A state of the art survey,https://api.elsevier.com/content/article/pii/S0029801821007940,"
                  The rapid development of artificial intelligence significantly promotes collision-avoidance navigation of maritime autonomous surface ships (MASS), which in turn provides prominent services in maritime environments and enlarges the opportunity for coordinated and interconnected operations. Clearly, full autonomy of the collision-avoidance navigation for the MASS in complex environments still faces huge challenges and highly requires persistent innovations. First, we survey relevant guidance of the International Maritime Organization (IMO) and industry code of each country on MASS. Then, major advances in MASS industry R&D, and collision-avoidance navigation technologies, are thoroughly overviewed, from academic to industrial sides. Moreover, compositions of collision-avoidance navigation, brain-inspired cognitive navigation, and e-navigation technologies are analyzed to clarify the mechanism and principles efficiently systematically in typical maritime environments, whereby trends in maritime collision-avoidance navigation systems are highlighted. Finally, considering a general study of existing collision avoidance and action planning technologies, it is pointed out that collision-free navigation would significantly benefit the integration of MASS autonomy in various maritime scenarios.
               ",autonomous vehicle
10.1016/j.patcog.2019.05.015,journal,Pattern Recognition,sciencedirect,2019-10-31,sciencedirect,Bio-inspired digit recognition using reward-modulated spike-timing-dependent plasticity in deep convolutional networks,https://api.elsevier.com/content/article/pii/S0031320319301906,"
                  The primate visual system has inspired the development of deep artificial neural networks, which have revolutionized the computer vision domain. Yet these networks are much less energy-efficient than their biological counterparts, and they are typically trained with backpropagation, which is extremely data-hungry. To address these limitations, we used a deep convolutional spiking neural network (DCSNN) and a latency-coding scheme. We trained it using a combination of spike-timing-dependent plasticity (STDP) for the lower layers and reward-modulated STDP (R-STDP) for the higher ones. In short, with R-STDP a correct (resp. incorrect) decision leads to STDP (resp. anti-STDP). This approach led to an accuracy of 97.2% on MNIST, without requiring an external classifier. In addition, we demonstrated that R-STDP extracts features that are diagnostic for the task at hand, and discards the other ones, whereas STDP extracts any feature that repeats. Finally, our approach is biologically plausible, hardware friendly, and energy-efficient.
               ",autonomous vehicle
10.1016/j.inffus.2021.02.012,journal,Information Fusion,sciencedirect,2021-09-30,sciencedirect,A review of multimodal image matching: Methods and applications,https://api.elsevier.com/content/article/pii/S156625352100035X,"
                  Multimodal image matching, which refers to identifying and then corresponding the same or similar structure/content from two or more images that are of significant modalities or nonlinear appearance difference, is a fundamental and critical problem in a wide range of applications, including medical, remote sensing and computer vision. An increasing number and diversity of methods have been proposed over the past decades, particularly in this deep learning era, due to the challenges in eliminating modality variance and geometrical deformation that intrinsically exist in multimodal image matching. However, a comprehensive review and analysis of traditional and recent trainable methods and their applications in different research fields are lacking. To this end and in this survey, we first introduce two general frameworks, saying area- and feature-based, in terms of their core components, taxonomy, and procedure details. Second, we provide a comprehensive review of multimodal image matching methods from handcrafted to deep methods for each research field according to their imaging nature, including medical, remote sensing and computer vision. Extensive experimental comparisons of interest point detection, description and matching, and image registration are performed on various datasets containing common types of multimodal image pairs that we collected and annotated. Finally, we briefly introduce and analyze several typical applications to reveal the significance of multimodal image matching and provide insightful discussions and conclusions to these multimodal image matching approaches, and simultaneously deliver their future trends for researchers and engineers in related research areas to achieve further breakthroughs.
               ",autonomous vehicle
10.1016/j.neucom.2021.02.098,journal,Neurocomputing,sciencedirect,2021-10-21,sciencedirect,Topic analysis and development in knowledge graph research: A bibliometric review on three decades,https://api.elsevier.com/content/article/pii/S0925231221009528,"
                  Knowledge graph as a research topic is increasingly popular to represent structural relations between entities. Recent years have witnessed the release of various open-source and enterprise-supported knowledge graphs with dramatic growth in applying knowledge representation and reasoning into different areas like natural language processing and computer vision. This study aims to comprehensively explore the status and trends – particularly the thematic research structure – of knowledge graphs. Specifically, based on 386 research articles published from 1991 to 2020, we conducted analyses in terms of the (1) visualization of the trends of annual article and citation counts, (2) recognition of major institutions, countries/regions, and publication sources, (3) visualization of scientific collaborations of major institutions and countries/regions, and (4) detection of major research themes and their developmental tendencies. Interest in knowledge graph research has clearly increased from 1991 to 2020 and is continually expanding. China is the most prolific country in knowledge graph research. Moreover, countries/regions and institutions that have higher levels of international collaboration are more impactful. Several widely studied issues such as knowledge graph embedding, search and query based on knowledge graphs, and knowledge graphs for intangible cultural heritage are highlighted. Based on the results, we further summarize perspective directions and suggestions for researchers, practitioners, and project managers to facilitate future research on knowledge graphs.
               ",autonomous vehicle
10.1016/j.mcm.2013.02.002,journal,Mathematical and Computer Modelling,sciencedirect,2013-09-30,sciencedirect,Utilizing artificial neural networks and genetic algorithms to build an algo-trading model for intra-day foreign exchange speculation,https://api.elsevier.com/content/article/pii/S0895717713000290,"The Foreign Exchange Market is the biggest and one of the most liquid markets in the world. This market has always been one of the most challenging markets as far as short term prediction is concerned. Due to the chaotic, noisy, and non-stationary nature of the data, the majority of the research has been focused on daily, weekly, or even monthly prediction. The literature review revealed that there is a gap for intra-day market prediction. Identifying this gap, this paper introduces a prediction and decision making model based on Artificial Neural Networks (ANN) and Genetic Algorithms. The dataset utilized for this research comprises of 70 weeks of past currency rates of the 3 most traded currency pairs: GBP∖USD, EUR∖GBP, and EUR∖USD. The initial statistical tests confirmed with a significance of more than 95% that the daily FOREX currency rates time series are not randomly distributed. Another important result is that the proposed model achieved 72.5% prediction accuracy. Furthermore, implementing the optimal trading strategy, this model produced 23.3% Annualized Net Return.",autonomous vehicle
10.1016/j.cis.2017.04.015,journal,Advances in Colloid and Interface Science,sciencedirect,2017-07-31,sciencedirect,Applications of artificial neural networks for adsorption removal of dyes from aqueous solution: A review,https://api.elsevier.com/content/article/pii/S0001868616303335,"
                  Artificial neural networks (ANNs) have been widely applied for the prediction of dye adsorption during the last decade. In this paper, the applications of ANN methods, namely multilayer feedforward neural networks (MLFNN), support vector machine (SVM), and adaptive neuro fuzzy inference system (ANFIS) for adsorption of dyes are reviewed. The reported researches on adsorption of dyes are classified into four major categories, such as (i) MLFNN, (ii) ANFIS, (iii) SVM and (iv) hybrid with genetic algorithm (GA) and particle swarm optimization (PSO). Most of these papers are discussed. The further research needs in this field are suggested. These ANNs models are obtaining popularity as approaches, which can be successfully employed for the adsorption of dyes with acceptable accuracy.
               ",autonomous vehicle
10.1016/j.future.2020.07.025,journal,Future Generation Computer Systems,sciencedirect,2020-12-31,sciencedirect,A novel multi-view pedestrian detection database for collaborative Intelligent Transportation Systems,https://api.elsevier.com/content/article/pii/S0167739X20300340,"
                  Recent advances in machine-learning, especially in deep neural networks have significantly accelerated the development and deployment of transport-oriented intelligent designs with increasingly high efficiency. While these technologies are exceptionally promising toward revolutionizing our current mobility and reducing the number of road accidents, the way to safe Intelligent Transportation Systems (ITS) remains long. Since pedestrians are the most vulnerable road users, designing accurate pedestrian detection methods is a priority task. However, traditional monocular pedestrian detection methods are limited, especially in occlusion handling. Hence, a collaborative perception scheme in which vehicles no longer restrict their input data to their immediate embedded sensors and rather exploit data from remote sensors is necessary to achieve a more comprehensive environment perception. In this work, we propose a novel public dataset: Infrastructure to Vehicle Multi-View Pedestrian Detection Database (I2V-MVPD) that combines synchronized images from both a mobile camera embedded in a car and a static camera in the road infrastructure. We also propose a new multi-view pedestrian detection framework based on collaborative intelligence between vehicles and infrastructure. Our results show a significant improvement in detection performance over monocular detection.
               ",autonomous vehicle
10.1016/j.neucom.2016.09.140,journal,Neurocomputing,sciencedirect,2017-11-22,sciencedirect,Online phoneme recognition using multi-layer perceptron networks combined with recurrent non-linear autoregressive neural networks with exogenous inputs,https://api.elsevier.com/content/article/pii/S0925231217310184,"
                  Off-line pattern recognition in speech signals is a complex task. Yet, this task becomes harder when the recognition result is required online or in real-time. The present work proposes an online identification of the Portuguese language phonemes using a non-linear autoregressive model with exogenous inputs, commonly called NARX. The process first conditions the input speech signal, and extracts its frequency characteristics. Then it pre-classifies the extracted features into one of the ten possible groups of phonemes, as available in the Portuguese language. This pre-classification is done using a multilayer perceptron network (MLP) with a supervised learning. Subsequently, the MLP output vector, together with the vector that carries the input frequencies, feeds a NARX neural network by means of a temporal delay of four times and feed-backward recurrent links that encompass the results of all hidden layers of the network. As a result of this process, the proposed phoneme recognition process improves the accuracy of an online identification of the Portuguese spoken phonemes during a natural conversation. When the phoneme input signal is well conditioned and continuous over time, the proposed recognition process can provide the correct classification in real-time, with an acceptable accuracy rate.
               ",autonomous vehicle
10.1016/j.procir.2018.08.194,journal,Procedia CIRP,sciencedirect,2018-12-31,sciencedirect,"Dawn of new machining concepts:: Compensated, intelligent, bioinspired",https://api.elsevier.com/content/article/pii/S2212827118310436,"The impact of Industrie 4.0 onto machine tools is significant, despite the fact, that quite some of the novelties discussed within this new paradigm have their roots decades earlier. But especially the concerted action, which strives the development of sensors, controls, data processing together with connectivity, unprecedented data integration and the notion of cyber physical production systems open up new development lines towards manufacturing systems as enablers for the progress in manufacturing. Highly developed compensation concepts are developing into state depending AI-supported strategies. Maintenance becomes predictive, as learning of machines becomes global and model based. Further inspirations taken from biological systems are adopted for machining centres and drive a biological transformation of manufacturing machines. Machine intelligence becomes the basis for executing manufacturing processes, which requires a close integration of process intelligence (CAM-systems) and machine controls.",autonomous vehicle
10.1016/B978-1-4832-1448-1.50013-5,journal,Connectionist Models,sciencedirect,1991-12-31,sciencedirect,Exploring Adaptive Agency I: Theory and Methods for Simulating the Evolution of Learning,https://api.elsevier.com/content/article/pii/B9781483214481500135,"
               Psychology construed as the scientific study of adaptive agency can include not only modelling of specific psychological adaptations in particular species, but general exploration of the adaptive processes (including evolution, learning, and computation) that build, modify, and instantiate those adaptations. Connectionist theory has concentrated on understanding the adaptive processes of learning and computation, and has assumed general-purpose learning principles as the prime constructors of psychological adaptations. But connectionism has thereby ignored the central lesson of a century of learning theory in psychology: learning mechanisms must be understood in terms of their specific adaptive functions, just like other psychological adaptations. This paper introduces the notion of psychology as the study of adaptive agency, outlines a hierarchy of adaptive processes underlying adaptive agency, and reviews the history of learning theory and the emergence of ecological and evolutionary approaches to learning. We then develop a taxonomy of adaptive functions that learning mechanisms might serve, and outline a general simulation framework for exploring those adaptive functions. Finally, we present empirical results concerning the simulated evolution of associative learning.
            ",autonomous vehicle
10.1016/j.asoc.2018.05.023,journal,Applied Soft Computing,sciencedirect,2018-09-30,sciencedirect,Soft Computing based object detection and tracking approaches: State-of-the-Art survey,https://api.elsevier.com/content/article/pii/S1568494618302965,"
                  In recent years, analysis and interpretation of video sequences to detect and track objects of interest had become an active research field in computer vision and image processing. Detection and tracking includes extraction of moving object from frames and continuous tracking it thereafter forming persistent object trajectories over time. There are some really smart techniques proposed by researchers for efficient and robust detection or tracking of objects in videos. A comprehensive coverage of such innovative techniques for which solutions have been motivated by theories of soft computing approaches is proposed. The main objective of this research investigation is to study and highlight efforts of researchers who had conducted some brilliant work on soft computing based detection and tracking approaches in video sequence. The study is novel as it traces rise of soft computing methods in field of object detection and tracking in videos which has been neglected over the years. The survey is compilation of studies on neural network, deep learning, fuzzy logic, evolutionary algorithms, hybrid and recent innovative approaches that have been applied to field of detection and tracking. The paper also highlights benchmark datasets available to researchers for experimentation and validation of their own algorithms. Major research challenges in the field of detection and tracking along with some recommendations are also provided. The paper provides number of analyses to guide future directions of research and advocates for more applications of soft computing approaches for object detection and tracking approaches in videos. The paper is targeted at young researchers who will like to see it as platform for introduction to a mature and relatively complex field. The study will be helpful in appropriate use of an existing method for systematically designing a new approach or improving performance of existing approaches.
               ",autonomous vehicle
10.1016/j.eswa.2009.02.089,journal,Expert Systems with Applications,sciencedirect,2009-10-31,sciencedirect,Accumulated strain prediction of polypropylene modified marshall specimens in repeated creep test using artificial neural networks,https://api.elsevier.com/content/article/pii/S0957417409002346,"
                  This study presents an application of artificial neural networks (ANN) for the prediction of repeated creep test results for polypropylene (PP) modified asphalt mixtures. Polypropylene fibers are used to modify the bituminous binder in order to improve the physical and mechanical properties of the resulting asphaltic mixture. Marshall specimens, fabricated with M-03 type polypropylene fibers at optimum bitumen content were tested using universal testing machine (UTM-5P) in order to determine their rheological/creep behavior under repeated loading. Different load values and loading patterns have been applied to the previously prepared specimens at a predetermined temperature. It has been shown that the addition of polypropylene fibers results in improved Marshall stabilities and decrease in the flow values, providing the increase of the service life of samples under repeated creep testing. The proposed ANN model uses the physical properties of standard Marshall specimens such as polypropylene type, specimen height, unit weight, voids in mineral aggregate, voids filled with asphalt, air voids and repeated creep test properties such as rest period and pulse counts in order to predict the accumulated strain values obtained at the end of mechanical tests. Moreover parametric analyses have been carried out. The results of parametric analyses were used to evaluate the accumulated strain of the Marshall specimens subjected to repeated load creep tests in a quite well manner.
               ",autonomous vehicle
10.1016/j.procs.2020.04.239,journal,Procedia Computer Science,sciencedirect,2020-12-31,sciencedirect,A Cross-Layer Routing Metric with Link Prediction in Wireless Mesh Networks,https://api.elsevier.com/content/article/pii/S1877050920312308,"Wireless Mesh Networks (WMNs) is one of the encouraging technologies for future-generation wireless network with diverse applications as it is undergoing rapid progress. WMNs are multi-hop networks which are beneficial in situations where there is little or no network infrastructure. WMNs consist of mesh nodes which interact with each other to establish a network connection. Mesh nodes include mesh clients and mesh routers. Mesh clients can be stationary or move from one place to another, whereas mesh routers have minimum movability and act as a backbone for WMNs. Due to the mobility of mesh nodes, the fundamental problem in WMNs is estimating the link quality. The efficiency of network protocols depends on the accuracy of link quality. Therefore, there is a need for an intelligent mechanism to find solutions to link quality problems in the wireless network. To address this problem, we apply machine learning prediction techniques. We evaluate the performance of prediction techniques like multiple linear regression, support vector regression and Gaussian regression. The experiments were conducted on various scenarios with cross-layer routing metric PCL-IDA to predict the link quality, which is embedded in OLSR routing protocol. Results using NS-2 simulations reveal that the link quality parameters throughput, average delay and packet loss are better when multiple linear regression prediction technique is applied.",autonomous vehicle
10.1016/j.procs.2020.04.239,journal,Procedia Computer Science,sciencedirect,2020-12-31,sciencedirect,A Cross-Layer Routing Metric with Link Prediction in Wireless Mesh Networks,https://api.elsevier.com/content/article/pii/S1877050920312308,"Wireless Mesh Networks (WMNs) is one of the encouraging technologies for future-generation wireless network with diverse applications as it is undergoing rapid progress. WMNs are multi-hop networks which are beneficial in situations where there is little or no network infrastructure. WMNs consist of mesh nodes which interact with each other to establish a network connection. Mesh nodes include mesh clients and mesh routers. Mesh clients can be stationary or move from one place to another, whereas mesh routers have minimum movability and act as a backbone for WMNs. Due to the mobility of mesh nodes, the fundamental problem in WMNs is estimating the link quality. The efficiency of network protocols depends on the accuracy of link quality. Therefore, there is a need for an intelligent mechanism to find solutions to link quality problems in the wireless network. To address this problem, we apply machine learning prediction techniques. We evaluate the performance of prediction techniques like multiple linear regression, support vector regression and Gaussian regression. The experiments were conducted on various scenarios with cross-layer routing metric PCL-IDA to predict the link quality, which is embedded in OLSR routing protocol. Results using NS-2 simulations reveal that the link quality parameters throughput, average delay and packet loss are better when multiple linear regression prediction technique is applied.",autonomous vehicle
10.1016/S1474-6670(17)46067-8,journal,IFAC Proceedings Volumes,sciencedirect,1994-06-30,sciencedirect,Modelling and Monitoring of Milling through Neuro-Fuzzy Techniques,https://api.elsevier.com/content/article/pii/S1474667017460678,"
                  Real-time nature, uncertainty handling and learning ability are essential requirements for knowledge representation and processing techniques to be applied at lower levels of intelligent manufacturing systems. The paper demonstrates and compares the applicability of neural networks and neuro fuzzy techniques for monitoring of milling tools. Learning and classification performances of back propagation (BP) networks and the neuro-fuzzy approach using different learning techniques are compared. The possible role of such a hybrid solution in an intelligent manufacturing environment is investigated.
               ",autonomous vehicle
10.1016/0920-5489(94)90011-6,journal,Computer Standards & Interfaces,sciencedirect,1994-07-31,sciencedirect,"Computational intelligence standards: motivation, current activities and progress",https://api.elsevier.com/content/article/pii/0920548994900116,"
                  Computational Intelligence is an emerging technology of keen interest to the developers of computer standards and interfaces. Coherent communications among the diverse set of users of computational AI is necessary for the protection of all parties and can help further the serious development of artificial neural networks, fuzzy systems, evolutionary programming and virtual reality. Current activities of the IEEE Neural Networks Council Standards Committee encompass all these areas, emphasizing the development of glossaries and symbologies, performance measures and interface standards for these interrelated fields. Progress toward these goals is described in this paper.
               ",autonomous vehicle
10.1016/j.ress.2020.107032,journal,Reliability Engineering & System Safety,sciencedirect,2020-10-31,sciencedirect,Accident diagnosis algorithm with untrained accident identification during power-increasing operation,https://api.elsevier.com/content/article/pii/S0951832020305330,"
                  To ensure the safety of nuclear power plants (NPPs) from accidents or anomalies, regulatory bodies provide procedures that describe safety regulations that must be followed. However, even if well-designed procedures are provided to operators, diagnostic activity in an emergency scenario is classified as an extremely demanding task. Moreover, the diagnosis of accidents occurring under various operation modes, such as power increasing, is expected to be extremely difficult, owing to the diverse behaviors and availability of systems and components. With regard to such emergency response issues, artificial neural network-based methods are regarded as one of the most promising approaches, because of their noticeable achievements. However, regarding the application of neural networks, in the case of an untrained accident, there is no capability to answer “do not know.” This study aims to develop algorithms that can cover various NPP operation modes and deal with untrained accidents. To address the various NPP operation modes, the major changes that can affect the plant states are classified. Furthermore, to deal with untrained accidents, the applied diagnostic algorithms use long short-term memory and an autoencoder. Following this, this paper presents the implementation and test results of the accident diagnosis algorithms.
               ",autonomous vehicle
10.1016/j.artint.2019.07.003,journal,Artificial Intelligence,sciencedirect,2019-10-31,sciencedirect,Automatic generation of sentimental texts via mixture adversarial networks,https://api.elsevier.com/content/article/pii/S0004370218306088,"
                  Automatic generation of texts with different sentiment labels has wide use in artificial intelligence applications such as conversational agents. It is an important problem to be addressed for achieving emotional intelligence. In this paper, we propose two novel models, SentiGAN and C-SentiGAN, which have multiple generators and one multi-class discriminator, to address this problem. In our models, multiple generators are trained simultaneously, aiming at generating texts of different sentiment labels without supervision. We propose a penalty-based objective in generators to force each of them to generate diversified examples of a specific sentiment label. Moreover, the use of multiple generators and one multi-class discriminator can make each generator focus on generating its own texts of a specific sentiment label accurately. Experimental results on a variety of datasets demonstrate that our SentiGAN model consistently outperforms several state-of-the-art text generation models in the sentiment accuracy and quality of generated texts. In addition, experiments on conditional text generation tasks show that our C-SentiGAN model has good prospects for specific text generation tasks.
               ",autonomous vehicle
10.1016/j.actaastro.2021.03.029,journal,Acta Astronautica,sciencedirect,2021-07-31,sciencedirect,"On the guidance, navigation and control of in-orbit space robotic missions: A survey and prospective vision",https://api.elsevier.com/content/article/pii/S0094576521001429,"
                  In the first part, this article presents an overview of Guidance, Navigation and Control (GNC) methodologies developed for space manipulators to perform in-orbit robotic missions, including but not limited to, on-orbit servicing, satellite/station assembly, probing extra-terrestrial objects and space debris mitigation. Some space mission concepts are briefly mentioned, for which space robotics is discussed to be among the most practical and universal solutions. Common phases of an in-orbit robotic mission are identified as: close-range rendezvous, attitude synchronization, target identification, manipulator deployment, capture, and if needed, post-capture maneuvers. Prominent GNC methodologies that are either proposed for or applicable to each phase are extensively reviewed. In the current article, the emphasis is placed on the study of GNC methodologies utilized in attitude synchronization, manipulator deployment, and capture phases, specially the ones reported for use in the two free-floating and free-flying operating regimes of space manipulators. Kinematics and dynamics of space manipulator systems are formulated to help unifying the presentation of the main ideas behind different GNC methodologies. Using a unified notation, comparison tables and discussions provided in this paper, researchers can compare various GNC approaches and contribute to the next-generation GNC systems for space robots. In addition, this survey aids technology users to learn about in-orbit robotic missions and choose appropriate GNC technologies for specific applications. In the second part of this paper, two families of emerging control schemes based upon reinforcement learning and geometric mechanics are introduced as promising research directions in the GNC of space robotic systems. The benefits of implementing these techniques to the GNC of in-orbit robotic missions are discussed. An exclusive study of environmental disturbances affecting space manipulators and their threat to long-term autonomy concludes this article.
               ",autonomous vehicle
10.1016/B978-0-08-043981-5.50132-2,journal,Computational Mechanics–New Frontiers for the New Millennium,sciencedirect,2001-12-31,sciencedirect,Artificial Neural Networks in Biomedical Engineering: A Review,https://api.elsevier.com/content/article/pii/B9780080439815501322,"
               This paper presents a review of applications of artificial neural networks in biomedical engineering area. Artificial neural networks in general are explained; some limitations and some proven benefits of neural networks are discussed. Use of artificial neural network techniques in various biomedical engineering applications is summarised. A case study is used to demonstrate the efficacy of artificial neural networks in this area. The paper concludes with a discussion of future usage of artificial neural networks in the area of biomedical engineering.
            ",autonomous vehicle
10.1016/j.eswa.2014.08.058,journal,Expert Systems with Applications,sciencedirect,2015-02-15,sciencedirect,A neuro-fuzzy approach to self-management of virtual network resources,https://api.elsevier.com/content/article/pii/S0957417414005909,"
                  Network virtualisation promises to lead to better manageability of the future Internet by allowing for adaptable sharing of physical network resources among different virtual networks. However, the sharing of resources is not trivial as virtual nodes and links should first be mapped onto substrate nodes and links, and thereafter the allocated resources managed throughout the lifetime of the virtual network. In this paper, we design and evaluate reinforcement learning-based neuro-fuzzy algorithms that perform dynamic, decentralised and coordinated self-management of substrate network resources. The objective is to achieve better efficiency in the utilisation of substrate network resources while ensuring that the quality of service requirements of the virtual networks are not violated. The proposed algorithms are evaluated through comparisons with a Q-learning-based approach as well as two static resource allocation schemes.
               ",autonomous vehicle
10.1016/B978-0-12-817133-2.00015-X,journal,Artificial Intelligence in Precision Health,sciencedirect,2020-12-31,sciencedirect,Chapter 15: Clinical decision support systems to improve the diagnosis and management of respiratory diseases,https://api.elsevier.com/content/article/pii/B978012817133200015X,"
               The burden associated with respiratory diseases is increasing and will likely grow exponentially in the next generations. Therefore a great research effort has been directed to the development of improved methods for diagnosis and management of respiratory diseases. Advances in clinical decision support systems based on machine learning (ML) algorithms have opened a new realm of methods in this area. These methods are closely related to personalized medicine, providing support to medical decisions, practices, interventions, and technologies that are tailored to individual patients on the basis of their predicted response or risk of disease. In particular, pattern analysis of pulmonary function has attracted attention as an approach to early detection of respiratory diseases. This article presents a review of the development of clinical decision support systems using in pulmonary function analysis from a machine learning perspective. Initially, a brief description of the main ML algorithms is presented, as well as the main methods used for pulmonary function exams. Then, we discuss the previous studies concerning the use of ML methods in pulmonary function analysis in a historical order, emphasizing the state of the art in this field. Methods using respiratory data beyond pulmonary function were also reviewed, as well as recent works integrating telemonitoring of pulmonary function and ML algorithms to optimize control of disease trajectory and prevent exacerbations. This review showed that ML has been successfully used in the automated interpretation of pulmonary function tests. In several studies, the introduction of ML methods increased diagnostic accuracy, including the early diagnosis. Very promising results were also observed concerning the prevention of exacerbations. Several of these studies, however, are small-scale studies, so large-scale studies are still needed to validate current findings and to boost its adoption by the medical community. Finally, we conclude and examine important future directions for this research field, including big data analytics, interactive machine learning, and deep learning.
            ",autonomous vehicle
10.1016/j.neucom.2020.02.035,journal,Neurocomputing,sciencedirect,2020-07-20,sciencedirect,Sparse low rank factorization for deep neural network compression,https://api.elsevier.com/content/article/pii/S0925231220302253,"
                  Storing and processing millions of parameters in deep neural networks is highly challenging during the deployment of model in real-time application on resource constrained devices. Popular low-rank approximation approach singular value decomposition (SVD) is generally applied to the weights of fully connected layers where compact storage is achieved by keeping only the most prominent components of the decomposed matrices. Years of research on pruning-based neural network model compression revealed that the relative importance or contribution of each neuron in a layer highly vary among each other. Recently, synapses pruning has also demonstrated that having sparse matrices in network architecture achieve lower space and faster computation during inference time. We extend these arguments by proposing that the low-rank decomposition of weight matrices should also consider significance of both input as well as output neurons of a layer. Combining the ideas of sparsity and existence of unequal contributions of neurons towards achieving the target, we propose sparse low rank (SLR) method which sparsifies SVD matrices to achieve better compression rate by keeping lower rank for unimportant neurons. We demonstrate the effectiveness of our method in compressing famous convolutional neural networks based image recognition frameworks which are trained on popular datasets. Experimental results show that the proposed approach SLR outperforms vanilla truncated SVD and a pruning baseline, achieving better compression rates with minimal or no loss in the accuracy. Code of the proposed approach is avaialble at https://github.com/sridarah/slr.
               ",autonomous vehicle
10.1016/S0893-6080(16)30177-0,journal,Neural Networks,sciencedirect,2017-01-31,sciencedirect,Neural Networks,https://api.elsevier.com/content/article/pii/S0893608016301770,,autonomous vehicle
10.1016/j.inffus.2020.10.008,journal,Information Fusion,sciencedirect,2021-03-31,sciencedirect,Image retrieval from remote sensing big data: A survey,https://api.elsevier.com/content/article/pii/S1566253520303778,"
                  The blooming proliferation of aeronautics and astronautics platforms, together with the ever-increasing remote sensing imaging sensors on these platforms, has led to the formation of rapidly-growing earth observation data with the characteristics of large volume, large variety, large velocity, large veracity and large value, which raises awareness about the importance of large-scale image processing, fusion and mining. Unconsciously, we have entered an era of big earth data, also called remote sensing (RS) big data. Although RS big data provides great opportunities for a broad range of applications such as disaster rescue, global security, and so forth, it inevitably poses many additional processing challenges. As one of the most fundamental and important tasks in RS big data mining, image retrieval (i.e., image information mining) from RS big data has attracted continuous research interests in the last several decades. This paper mainly works for systematically reviewing the emerging achievements for image retrieval from RS big data. And then this paper further discusses the RS image retrieval based applications including fusion-oriented RS image processing, geo-localization and disaster rescue. To facilitate the quantitative evaluation of the RS image retrieval technique, this paper gives a list of publicly open datasets and evaluation metrics, and briefly recalls the mainstream methods on two representative benchmarks of RS image retrieval. Considering the latest advances from multiple domains including computer vision, machine learning and knowledge engineering, this paper points out some promising research directions towards RS big data mining. From this survey, engineers from industry may find skills to improve their RS image retrieval systems and researchers from academia may find ideas to conduct some innovative work.
               ",autonomous vehicle
10.1016/j.cosrev.2009.03.005,journal,Computer Science Review,sciencedirect,2009-08-31,sciencedirect,Reservoir computing approaches to recurrent neural network training,https://api.elsevier.com/content/article/pii/S1574013709000173,"
                  Echo State Networks and Liquid State Machines introduced a new paradigm in artificial recurrent neural network (RNN) training, where an RNN (the reservoir) is generated randomly and only a readout is trained. The paradigm, becoming known as reservoir computing, greatly facilitated the practical application of RNNs and outperformed classical fully trained RNNs in many tasks. It has lately become a vivid research field with numerous extensions of the basic idea, including reservoir adaptation, thus broadening the initial paradigm to using different methods for training the reservoir and the readout. This review systematically surveys both current ways of generating/adapting the reservoirs and training different types of readouts. It offers a natural conceptual classification of the techniques, which transcends boundaries of the current “brand-names” of reservoir methods, and thus aims to help in unifying the field and providing the reader with a detailed “map” of it.
               ",autonomous vehicle
10.1016/j.artmed.2008.07.002,journal,Artificial Intelligence in Medicine,sciencedirect,2008-10-31,sciencedirect,Artificial consciousness: A discipline between technological and theoretical obstacles,https://api.elsevier.com/content/article/pii/S0933365708000912,"
                  Artificial consciousness is still far from being an established discipline. We will try to outline some theoretical assumption that could help in dealing with phenomenal consciousness. What are the technological and theoretical obstacles that face the enthusiast scholars of artificial consciousness? After presenting an outline of the state of artificial consciousness, we will focus on the relevance of phenomenal consciousness. Artificial consciousness needs to tackle the issue of phenomenal consciousness in a physical world. Up to now, the only models that give some hope of succeeding are the various kinds of externalism.
               ",autonomous vehicle
10.1016/j.knosys.2021.107401,journal,Knowledge-Based Systems,sciencedirect,2021-11-14,sciencedirect,4G-VOS: Video Object Segmentation using guided context embedding,https://api.elsevier.com/content/article/pii/S0950705121006638,"
                  Video Object Segmentation (VOS) is a fundamental task required in many high-level real-world computer vision applications. VOS becomes challenging due to the presence of background distractors as well as to object appearance variations. Many existing VOS approaches use online model updates to capture the appearance variations which incurs high computational cost. Template matching and propagation-based VOS methods, although cost-effective, suffer from performance degradation under challenging scenarios such as occlusion and background clutter. In order to tackle these challenges, we propose a network architecture dubbed 4G-VOS to encode video context for improved VOS performance to tackle these challenges. To preserve long term semantic information, we propose a guided transfer embedding module. We employ a global instance matching module to generate similarity maps from the initial image and the mask. Besides, we use a generative directional appearance module to estimate and dynamically update the foreground/background class probabilities in a spherical embedding space. Moreover, during feature refinement, existing approaches may lose contextual information. Therefore, we propose a guided pooled decoder to exploit the global and local contextual information during feature refinement. The proposed framework is an end-to-end learning architecture that is trained in an offline fashion. Evaluations over three VOS benchmark datasets including DAVIS2016, DAVIS2017, and YouTube-VOS have demonstrated outstanding performance of the proposed algorithm compared to 40 existing state-of-the-art methods.
               ",autonomous vehicle
10.1016/j.vlsi.2021.08.004,journal,Integration,sciencedirect,2021-11-30,sciencedirect,FPGA-based implementation of classification techniques: A survey,https://api.elsevier.com/content/article/pii/S0167926021000894,"
                  Recently, a number of classification techniques have been introduced. However, processing large dataset in a reasonable time has become a major challenge. This made classification task more complex and expensive in calculation. Thus, the need for solutions to overcome these constraints such as field programmable gate arrays (FPGAs). In this paper, we give an overview of the various classification techniques. Then, we present the existing FPGA based implementation of these classification methods. After that, we investigate the confronted challenges and the optimizations strategies. Finally, we highlight the hardware accelerator architectures and tools for hardware design suggested to improve the FPGA implementation of classification methods.
               ",autonomous vehicle
10.1016/j.matpr.2020.11.115,journal,Materials Today: Proceedings,sciencedirect,2020-12-16,sciencedirect,Detection of plant leaf disease using digital image processing,https://api.elsevier.com/content/article/pii/S2214785320387216,"
                  Productivity in agriculture is highly economic dependent. This is one of the reasons why the diagnosis in agriculture of plant diseases is important because plant disease is very natural. If proper care is not taken in this area, the impact on plants and the quality, quantity or productivity of their products will be serious. For instance, in pine trees in the United States, a small leaf disease is a dangerous disease. It is helpful to diagnose plant disease with any automated procedure, as it decreases the widespread surveillance of farm sites, and detects the signs of diseases very early, that is when they arise on leaves of plant. A segmentation algorithm used to automatically detect or classify plant leaf diseases is presented in this article. This article. It also includes surveys of the various methods used to classify diseases to detect plant leaf diseases. A genetic algorithm is used to segment the picture which is important for the identification of disease in leaf disease.
               ",autonomous vehicle
10.1016/j.cogsys.2017.08.004,journal,Cognitive Systems Research,sciencedirect,2018-01-31,sciencedirect,Towards reasoning based representations: Deep Consistence Seeking Machine,https://api.elsevier.com/content/article/pii/S1389041717300256,"
                  Machine learning is making substantial progress in diverse applications. The success is mostly due to advances in deep learning. However, deep learning can make mistakes and its generalization abilities to new tasks are questionable. We ask when and how one can combine network outputs, when (i) details of the observations are evaluated by learned deep components and (ii) facts and rules are available. The Deep Consistence Seeking (DCS) machine seeks for consistent and deterministic event descriptions and improves the representation accordingly. The machine has an anomaly detection component that may trigger coherence seeking. Coherence seeking resolves conflicts between computational modules by preferring components with higher scores. We illustrate that context can help in correcting recognitions and in deriving training samples for self-training. We put these concepts into a general framework of cognition, by distinguishing creativity, rule extraction, verification, and symbol grounding. We demonstrate our approach in a driving scenario.
               ",autonomous vehicle
10.1016/0166-3615(91)90024-4,journal,Computers in Industry,sciencedirect,1991-11-30,sciencedirect,Neural networks—Their applications and perspectives in intelligent machining,https://api.elsevier.com/content/article/pii/0166361591900244,"
                  In intelligent manufacturing systems, unprecedented und unforeseen situations are expected to be solved, within certain limits, even on the basis of incomplete and inprecise information One has tried to achieve this goal since years. Gradually, it seems to be realizable through partial solutions, integrated in today's flexible manufacturing systems. These complexes are fairly complicated material and data processing systems, in which different sensors and actuators are thoroughly distributed. The most important requirements for the intelligent techniques to be applied in these systems are the abilities for integration of multiple sensor information, for real-time functioning, for effective knowledge representation and for learning or adaptivity. Artificial neural networks seem to be able to fulfil some of these requirements and to contribute to the implementation of partial solutions, which can lead to the realization of truly intelligent manufacturing systems. The paper gives a summary of known neural networks applications and perspectives in intelligent manufacturing. Special emphasis is given on intelligent machining, namely on the following fields: multisensor fusion and integration, learning of process models, adaptive control, monitoring, diagnostics and quality control. For the sake of perspicuity a short survey of different artificial neural network structures and learning algorithms is also given, together with frequent applications of neural network techniques in fields different from intelligent manufacturing.
               ",autonomous vehicle
10.1016/0893-6080(89)90035-X,journal,Neural Networks,sciencedirect,1989-12-31,sciencedirect,Neural network models for pattern recognition and associative memory,https://api.elsevier.com/content/article/pii/089360808990035X,"
                  This review outlines some fundamental neural network modules for associative memory, pattern recognition, and category learning. Included are discussions of the McCulloch-Pitts neuron, perceptrons, adaline and madaline, back propagation, the learning matrix, linear associative memory, embedding fields, instars and outstars, the avalanche, shunting competitive networks, competitive learning, computational mapping by instarl outstar families, adaptive resonance theory, the cognitron and neocognitron, and simulated annealing. Adaptive filter formalism provides a unified notation. Activation laws include additive and shunting equations. Learning laws include back-coupled error correction, Hebbian learning, and gated instar and outstar equations. Also included are discussions of real-time and off-line modeling, stable and unstable coding, supervised and unsupervised learning, and self-organization.
               ",autonomous vehicle
10.1016/B978-012443870-5.50045-9,journal,Fuzzy Theory Systems,sciencedirect,1999-12-31,sciencedirect,43: Implementation Techniques and Applications of Fuzzy Neural Network Systems,https://api.elsevier.com/content/article/pii/B9780124438705500459,"
               This chapter discusses several applications of neurofuzzy technology and their modeling. By mapping the fuzzy pattern classification process into a connectionist framework, various connectionist fuzzy classifiers for the problems of pattern recognition can be achieved. The first fuzzy neural network model presented in this chapter is based on a four-layer feedforward neural network structure for implementing a “weighted Euclidean distance” fuzzy classification procedure. They apply one-pass learning algorithms. The connectionist fuzzy classifier (CFC) model and its variations have been adopted for the problems of speech recognition, color image recognition, and information retrieval on the Internet. The purpose of developing this model is to combine the advantages of the back-propagation and PNN models. The second model presented in this chapter is for the applications of fuzzy expert systems and control engineering. The chapter presents NNFI and its reinforcement-learning version, the RFNN-DPS model. These are constructed with a three-layered network structure and consist of a few kinds of neurons: linguistic neurons, rule neurons, and credit-based exploration neurons. Finally, the chapter presents a third kind of application: function approximation and color space mapping. The presented fuzzy CMAC model can be successfully used in general function approximation and color reproduction.
            ",autonomous vehicle
10.1016/S0921-8890(99)00122-0,journal,Robotics and Autonomous Systems,sciencedirect,2000-06-30,sciencedirect,ARBIB: An autonomous robot based on inspirations from biology,https://api.elsevier.com/content/article/pii/S0921889099001220,"
                  Simple artificial creatures (‘animats’), which operate as autonomous, adaptive robots in the real world, can serve both as models of biology and as a radical alternative to conventional methods of designing intelligent systems. We describe the evolution and implementation of the autonomous robot ARBIB, which learns from and adapts to its environment. A primary goal was to test the notion that effective robot learning can be based on neural habituation and sensitization, so validating the suggestion of Hawkins and Kandel that (associative) classical and ‘higher-order’ conditioning might be based on an elaboration of these (non-associative) forms of learning. Accordingly, ARBIB’s ‘nervous system’ has a non-homogeneous population of spiking neurons, and learning is by modification of basic, pre-existing (‘hard-wired’) reflexes. By monitoring firing rates of specific neurons and synaptic weights between neural connections as ARBIB learns from its environment, we confirm that both classical and higher-order conditioning occur, leading to the emergence of interesting and ecologically valid behaviors.
               ",autonomous vehicle
10.1016/j.bdr.2015.04.002,journal,Big Data Research,sciencedirect,2015-12-31,sciencedirect,Multi-Label Regularized Generative Model for Semi-Supervised Collective Classification in Large-Scale Networks,https://api.elsevier.com/content/article/pii/S2214579615000301,"
                  The problem of collective classification (CC) for large-scale network data has received considerable attention in the last decade. Enabling CC usually increases accuracy when given a fully-labeled network with a large amount of labeled data. However, such labels can be difficult to obtain and learning a CC model with only a few such labels in large-scale sparsely labeled networks can lead to poor performance. In this paper, we show that leveraging the unlabeled portion of the data through semi-supervised collective classification (SSCC) is essential to achieving high performance. First, we describe a novel data-generating algorithm, called generative model with network regularization (GMNR), to exploit both labeled and unlabeled data in large-scale sparsely labeled networks. In GMNR, a network regularizer is constructed to encode the network structure information, and we apply the network regularizer to smooth the probability density functions of the generative model. Second, we extend our proposed GMNR algorithm to handle network data consisting of multi-label instances. This approach, called the multi-label regularized generative model (MRGM), includes an additional label regularizer to encode the label correlation, and we show how these smoothing regularizers can be incorporated into the objective function of the model to improve the performance of CC in multi-label setting. We then develop an optimization scheme to solve the objective function based on EM algorithm. Empirical results on several real-world network data classification tasks show that our proposed methods are better than the compared collective classification algorithms especially when labeled data is scarce.
               ",autonomous vehicle
10.1016/j.procir.2020.03.101,journal,Procedia CIRP,sciencedirect,2020-12-31,sciencedirect,Prescriptive Modelling System Design for an Armature Multi-coil Rewinding Cobot Machine,https://api.elsevier.com/content/article/pii/S2212827120308209,"Digital transformation has ushered in the digital economy, powered by digital intelligence and quantum computing. The various winding topologies in rotary machines result from multi-variant design specifications and connection types. Rewinding of rotary machines is a behaviour-based decision-making process conducted within the shop floor, as the procedure is dependent on multi-input multi-output variables. Due to high data variability in service remanufacturing of armature windings in rotary machines, data abstraction for intelligent automation and analytics leads to increased operational productivity and new insights into market dynamics. In this light, the aim of the paper is to illustrate the design of a prescriptive modelling system of a symmetrical multi-coil winding machine for armature winding. The proposed system is a hybrid least squares support vector machine and adaptive neuro-fuzzy inference system for optimizing and maintaining a copper fill factor at 90.7%. A mixed method research was utilized for qualitative and quantitative for the multivariate parameters. The results show that the system through in-slot repetitive orthocyclic winding process, with multi-spindle concentric layering improves the energy efficiency of the induction motors, which in turn lowers winding faults during the remanufacturing process. Streamlining operations through fog computing further enhances system latency and process reliability towards sustainable industrialization.",autonomous vehicle
10.1016/S0893-6080(15)00246-4,journal,Neural Networks,sciencedirect,2016-01-31,sciencedirect,Neural Networks,https://api.elsevier.com/content/article/pii/S0893608015002464,,autonomous vehicle
10.1016/S0893-6080(15)00246-4,journal,Neural Networks,sciencedirect,2016-01-31,sciencedirect,Neural Networks,https://api.elsevier.com/content/article/pii/S0893608015002464,,autonomous vehicle
10.1016/j.future.2019.06.004,journal,Future Generation Computer Systems,sciencedirect,2019-12-31,sciencedirect,Smart healthcare framework for ambient assisted living using IoMT and big data analytics techniques,https://api.elsevier.com/content/article/pii/S0167739X18321071,"
                  In the era of pervasive computing, human living has become smarter by the latest advancements in IoMT (Internet of Medical Things), wearable sensors and telecommunication technologies in order to deliver smart healthcare services. IoMT has the potential to revolutionize the healthcare industry. IoMT interconnects wearable sensors, patients, healthcare providers and caregivers via software and ICT (Information and Communication Technology). AAL (Ambient Assisted Living) enables integration of new technologies to be part of our daily life activities. In this paper, we have provided a novel smart healthcare framework for AAL to monitor the physical activities of elderly people using IoMT and intelligent machine learning algorithms for faster analysis, decision making and better treatment recommendations. Data is collected from multiple wearable sensors placed on subject’s left ankle, right arm, and chest, is transmitted through IoMT devices to the integrated cloud and data analytics layer. To process huge amounts of data in parallel, Hadoop MapReduce techniques are used. Multinomial Naïve Bayes classifier, which fits into the MapReduce paradigm, is utilized to recognize the motion experienced by different body parts and provides higher scalability and better performance with parallel processing when compared to serial processor. Our proposed framework predicts 12 physical activities with an overall accuracy of 97.1%. This can be considered as an optimal solution for recognizing physical activities to remotely monitor health conditions of elderly people.
               ",autonomous vehicle
10.1016/B978-0-08-102782-0.00017-4,journal,Memristive Devices for Brain-Inspired Computing,sciencedirect,2020-12-31,sciencedirect,Chapter 17: Synaptic realizations based on memristive devices,https://api.elsevier.com/content/article/pii/B9780081027820000174,"
               In the past 10 years, neuromorphic computing has emerged as a novel approach to tackle the challenges posed by the end of Moore’s law. Memristive devices are very promising due to their unique properties. They are highly compact, fast switching, power efficient, and can represent multiple states of memory (via a tunable resistance). As such they can find use as synaptic connections among neurons in biologically inspired hardware neural networks, and therefore could be a critical element for the development of hardware cognitive systems with capabilities such as those found in animal nervous systems.
               In this chapter, we present an overview on the current status of synaptic circuits based on memristive devices. We review various implementations including the single-memristor synapse, which employs resistive switching random access memory, phase-change memory and spin-transfer torque magnetic random access memory, hybrid structures combining complementary metal-oxide semiconductor transistors, and memristive devices and materials-based approaches aiming at reproducing biological learning rules by the physical properties of the device. Learning rules such as the spike-timing-dependent plasticity, the spike-rate-dependent plasticity and the short-term plasticity are described. We finally present some examples of learning circuits exploited in hardware neural networks, which make initial steps on a path toward memristive circuits capable of biorealistic brain-inspired cognitive computing.
            ",autonomous vehicle
10.1016/j.fsidi.2021.301217,journal,Forensic Science International: Digital Investigation,sciencedirect,2021-09-30,sciencedirect,On the need for AI to triage encrypted data containers in U.S. law enforcement applications,https://api.elsevier.com/content/article/pii/S2666281721001256,"
                  This paper takes an analogical approach to define the parameters by which artificial intelligence (AI) can be utilized to facilitate warrantless searches at U.S. ports of entry. The authors tailor their discussion to the prevention of child pornography (also referred to as child abuse or exploitation materials in the academic literature), and the traffic thereof. By making the legal case to utilize AI, particularly eXplainable AI (XAI), to search encrypted devices for attributes indicative of child pornography, the authors hope to encourage research in this field and develop better technology to help catch criminals without relinquishing privacy rights.
               ",autonomous vehicle
10.1016/0893-6080(96)00015-9,journal,Neural Networks,sciencedirect,1996-06-30,sciencedirect,Adaptive critic for sigma-pi networks,https://api.elsevier.com/content/article/pii/0893608096000159,"
                  This article presents an investigation which studied how training of sigma-pi networks with the associative reward-penalty (AR-P) regime may be enhanced by using two networks in parallel. The technique uses what has been termed an unsupervised “adaptive critic element” (ACE) to give critical advice to the supervised sigma-pi network. We utilise the conventions that the sigma-pi neuron model uses (i.e., quantisation of variables) to obtain an implementation we term the “quantised adaptive critic”, which is hardware realisable. The associative reward-penalty training regime either rewards, r = 1, the neural network by incrementing the weights of the net by a delta term times a learning rate, α, or penalises, r = 0, the neural network by decrementing the weights by an inverse delta term times the product of the learning rate and a penalty coefficient, α × λ
                     rp. Our initial research, utilising a “bounded” reward signal, r∗ ε {0,…;,1}, found that the critic provides advisory information to the sigma-pi net which augments its training efficiency. This led us to develop an extension to the adaptive critic and associative reward-penalty methodologies, utilising an “unbounded” reward signal, r∗ ε {−1,…;, 2}, which permits penalisation of a net even when the penalty coefficient, λ
                     rp, is set to zero, λ
                     rp = 0. One should note that with the standard associative reward-penalty methodology the net is normally only penalised if the penalty coefficient is non-zero (i.e., 0 < λ
                     rp ⩽ 1). One of the enigmas of associative reward-penalty (AR-P) training is that it broadcasts sparse information, in the form of an instantaneous binary reward signal, that is only dependent on the present output error. Here we put forward ACE and AR-P methodologies for sigma-pi nets, which are based on tracing the frequency of “stimuli” occurrence, and then using this to derive a prediction of the reinforcement. The predictions are then used to derive a reinforcement signal which uses temporal information. Hence one may use more precise information to enable more efficient training.
               ",autonomous vehicle
10.1016/0165-0114(95)00351-7,journal,Fuzzy Sets and Systems,sciencedirect,1997-01-23,sciencedirect,The neural network model RuleNet and its application to mobile robot navigation,https://api.elsevier.com/content/article/pii/0165011495003517,"
                  In this paper the neural network models RuleNet and its extension, Fuzzy RuleNet, are described in detail. RuleNet is a feedforward network model with a supervised learning algorithm, a dynamic architecture and discrete outputs. The main characteristics of RuleNet are its efficient learning and propagation algorithms and the possibility to translate symbolic knowledge into the network and vice versa without loss of information.
                  Fuzzy RuleNet is an extension to RuleNet with Fuzzy Logic. The main application area of this neuro-fuzzy model is fuzzy classification. An important characteristic of Fuzzy RuleNet is the possibility of knowledge transfer into and from the network without loss of information, therefore it can also be used for the generation of fuzzy systems (i.e. fuzzy rules and the corresponding membership functions).
                  RuleNet and Fuzzy RuleNet have been applied to a hierarchic behavior based navigation system for mobile robots. As a first step, a wall following behavior has been implemented utilizing these network models. The achieved results in the simulation environment as well as on a mobile robot experimental platform are very encouraging.
               ",autonomous vehicle
10.1016/B978-0-12-822249-2.09991-6,journal,Computational and Data-Driven Chemistry Using Artificial Intelligence,sciencedirect,2022-12-31,sciencedirect,Index,https://api.elsevier.com/content/article/pii/B9780128222492099916,Unknown,autonomous vehicle
10.1016/j.compeleceng.2016.11.005,journal,Computers & Electrical Engineering,sciencedirect,2017-01-31,sciencedirect,Random neural network based cognitive engines for adaptive modulation and coding in LTE downlink systems,https://api.elsevier.com/content/article/pii/S0045790616307066,"
                  This paper presents two random neural network (RNN) based context-aware decision making frameworks to improve adaptive modulation and coding (AMC) in long-term evolution (LTE) downlink systems. In the first framework, AMC is modelled as a traditional classification problem with the aim to maximize the probability of correct classification. The second framework seeks to optimize the throughput as opposed to simply maximizing the probability of the correct classification. To model the second framework, we developed a hybrid cognitive engine (CE) architecture by integrating an RNN based learning algorithm with genetic algorithm (GA) based reasoning. RNN inherent properties help CE to comply with the essential CE design requirement (i.e. concurrent long-term-learning, low computational complexity, and fast decision making). The performance of RNN is compared with artificial neural networks (ANN) and state-of-the-art effective exponential SINR mapping (EESM) algorithm. A comprehensive analysis of the proposed RNN based AMC scheme is presented by jointly incorporating the effect of different schedulers, feedback delays, and multi-antenna diversity on the throughput of an orthogonal frequency-division multiple access (OFDMA) system. The critical analysis of the first framework revealed that RNN based CE can achieve comparable results with faster adaptation, even in severe environment changes without the need of retraining compared to ANN. The analysis of the second approach demonstrated RNNs faster adaptation as compared to ANN and showed upto 253% gain in user throughput. RNN based CE efficiently exploited the channel quality information feedback delay to improve system throughput and helped cell-edge and cell-centre users to experience much better services in terms of achieved throughput as compared to EESM.
               ",autonomous vehicle
10.1016/j.entcom.2012.10.001,journal,Entertainment Computing,sciencedirect,2013-04-30,sciencedirect,Imitating human playing styles in Super Mario Bros,https://api.elsevier.com/content/article/pii/S1875952112000183,"
                  We describe and compare several methods for generating game character controllers that mimic the playing style of a particular human player, or of a population of human players, across video game levels. Similarity in playing style is measured through an evaluation framework, that compares the play trace of one or several human players with the punctuated play trace of an AI player. The methods that are compared are either hand-coded, direct (based on supervised learning) or indirect (based on maximising a similarity measure). We find that a method based on neuroevolution performs best both in terms of the instrumental similarity measure and in phenomenological evaluation by human spectators. A version of the classic platform game “Super Mario Bros” is used as the testbed game in this study but the methods are applicable to other games that are based on character movement in space.
               ",autonomous vehicle
10.1016/j.jnca.2005.08.005,journal,Journal of Network and Computer Applications,sciencedirect,2007-01-31,sciencedirect,Adaptive anomaly detection with evolving connectionist systems,https://api.elsevier.com/content/article/pii/S108480450500041X,"
                  Anomaly detection holds great potential for detecting previously unknown attacks. In order to be effective in a practical environment, anomaly detection systems have to be capable of online learning and handling concept drift. In this paper, a new adaptive anomaly detection framework, based on the use of unsupervised evolving connectionist systems, is proposed to address these issues. It is designed to adapt to normal behavior changes while still recognizing anomalies. The evolving connectionist systems learn a subject's behavior in an online, adaptive fashion through efficient local element tuning. Experiments with the KDD Cup 1999 network data and the Windows NT user profiling data show that our adaptive anomaly detection systems, based on Fuzzy Adaptive Resonance Theory (ART) and Evolving Fuzzy Neural Networks (EFuNN), can significantly reduce the false alarm rate while the attack detection rate remains high.
               ",autonomous vehicle
10.1016/j.procs.2016.07.409,journal,Procedia Computer Science,sciencedirect,2016-12-31,sciencedirect,Which Features Matter How Much When?,https://api.elsevier.com/content/article/pii/S1877050916316659,"How do brains learn which features matter how much, when and for what purposes? A specific feature may matter more or less for recognitions of different learned patterns, and in different contexts and attentional foci. Simple executable “neural circuits” built from biologically-inspired reusable memory pattern components in the NeurOS™ and NeuroBlocks™ technology 1 1 patents pending; see www.cognitivity.technology model and implement a range of learning and dynamic contextual/situational/attentional feature relevance. A pattern is a collection of weighted features, roughly analogous to a neuron or neuron assembly. New patterns are created for sufficiently novel feature combinations. Individual feature weights in best-matching existing patterns grow or diminish with repetition, yielding patterns that adjust to repeated experience. Arbitrarily complex classification meshes typical of human knowledge are easily assembled by varying a simple novelty parameter. Cascading pattern recognitions build up layers of concrete to abstract feature vocabularies. Names or labels are modeled as synonyms for experience patterns. Context can be modeled as yet another feature, derived from recent activity, to discriminate among otherwise similar patterns. Attention can be modeled as broad dynamic parameters modulating feature signal strengths.",autonomous vehicle
10.1016/B978-0-08-097086-8.43093-X,journal,International Encyclopedia of the Social & Behavioral Sciences,sciencedirect,2015-12-31,sciencedirect,Statistical Pattern Recognition,https://api.elsevier.com/content/article/pii/B978008097086843093X,"
               Statistical pattern recognition is concerned with the problem of designing machines that can detect and classify complex patterns in data. Statistical pattern recognition problems frequently arise in all fields of science and engineering. In particular, statistical pattern recognition methods are widely used by social and behavioral scientists in data analysis and in the course of modeling complex social, behavioral, and neural systems. This article reviews the major components of statistical pattern recognition systems and discusses methods for classification and learning. In addition, the importance of evaluating generalization performance using cross-validation measures and other out-of-sample measures is emphasized.
            ",autonomous vehicle
10.1016/j.neucom.2021.08.133,journal,Neurocomputing,sciencedirect,2021-11-20,sciencedirect,Aspect term extraction for opinion mining using a Hierarchical Self-Attention Network,https://api.elsevier.com/content/article/pii/S0925231221013448,"
                  Aspect identification is one of the important sub-tasks in opinion mining and this task can be considered as a token-level sequencing problem. Most recent approaches employ BERT based network to identify the aspect term, which is often complex, consumes a lot of memory, and needs more training time. In this paper, we propose a novel Hierarchical Self-Attention Network (HSAN) which performs well, needs lesser memory and training time. HSAN hierarchically applies a self-attention mechanism to first capture the importance of each word in the context of the overall meaning of the sentence and then it explores the internal dependency of the words in the same sentence to identify interdependent collocated words. A fusion of these two-attention mechanisms helps HSAN to predict multiple aspect terms effectively in the given sentence along with multi-token aspect terms. Our proposed network uses word embeddings, which is a combination of general-purpose embeddings and domain-specific embeddings. We evaluate the performance of HSAN on SemEval-2014 datasets, experimental results demonstrate the efficiency and effectiveness of our model.
               ",autonomous vehicle
10.1016/j.future.2021.08.009,journal,Future Generation Computer Systems,sciencedirect,2022-01-31,sciencedirect,STPD: Defending against <mml:math display=inline id=d1e1091 altimg=si25.svg><mml:msub><mml:mrow><mml:mi>ℓ</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math>-norm attacks with space transformation,https://api.elsevier.com/content/article/pii/S0167739X21003174,"
                  The human imperceptible adversarial examples crafted by 
                        
                           
                              ℓ
                           
                           
                              0
                           
                        
                     -norm attacks, which aims to minimize 
                        
                           
                              ℓ
                           
                           
                              0
                           
                        
                      distance from the original image, thereby misleading deep neural network classifiers into the wrong classification. Prior works of tackling 
                        
                           
                              ℓ
                           
                           
                              0
                           
                        
                      attacks can neither eliminate perturbed pixels nor improve the performance of the classifier in the recovered low-quality images. To address the issue, we propose a novel method, called space transformation pixel defender (STPD), to transform any image into a latent space to separate the perturbed pixels from the normal pixels. In particular, this strategy uses a set of one-class classifiers, including Isolation Forest and Elliptic Envelope, to locate the perturbed pixels from adversarial examples. The value of the neighboring normal pixels is then used to replace the perturbed pixels, which hold more than half of the votes from these one-class classifiers. We use our proposed strategy to successfully defend against well-known 
                        
                           
                              ℓ
                           
                           
                              0
                           
                        
                     -norm adversarial examples in the image classification settings. We show experimental results under the One-pixel Attack (OPA), the Jacobian-based Saliency Map Attack (JSMA), and the Carlini Wagner (CW) 
                        
                           
                              ℓ
                           
                           
                              0
                           
                        
                     -norm attack on CIFAR-10, COVID-CT, and ImageNet datasets. Our experimental results show that our approach can effectively defend against 
                        
                           
                              ℓ
                           
                           
                              0
                           
                        
                     -norm attacks compared with the most popular defense techniques.
               ",autonomous vehicle
10.1016/B978-008045405-4.00156-7,journal,Encyclopedia of Ecology,sciencedirect,2008-12-31,sciencedirect,Application of Ecological Informatics,https://api.elsevier.com/content/article/pii/B9780080454054001567,"
               Ecological systems are described as complex as well as in a nonequilibrium state while traditional models can deal rather with much simple systems or need a reductionism approach to cope with them. A definition of ‘ecological informatics’, a new discipline with tools to deal with such complexity, is presented as well as some of the methods from this field with the type of application where they have been successful. Two examples that the authors have developed to show the modeling and interpretation capabilities of those tools are described: ‘a decision support system’ for environmental management policies and an ‘IBM artificial neural network’ model of behavior and learning.
            ",autonomous vehicle
10.1016/j.compeleceng.2017.05.019,journal,Computers & Electrical Engineering,sciencedirect,2018-02-28,sciencedirect,A novel social network measurement and perception pattern based on a multi-agent and convolutional neural network,https://api.elsevier.com/content/article/pii/S0045790617313630,"
                  With the rapid advancement of the social network, the total interpersonal relationships among people constitute a social network in real life and the human is the node in this network. Against this background, this paper proposes a novel social network search and perception pattern based on a multi-agent and convolutional neural network. Our research can be regarded as a parallel integration of the multi-agent and CNN. In the CNN part, we adopt prior knowledge that differs from the ordinary convolution neural network and the convolution neural network unique neuron receptive field structure. In the multi-agent part, we combine the characteristics of individual and general-community agents; the establishment and revision of its faith intention is the result of internal thought conditions and interaction with external factors. We apply the proposed model to a social network search, and perception and connection awareness analysis, respectively. The experimental result proves that the proposed method achieves a satisfactory performance.
               ",autonomous vehicle
10.1016/j.critrevonc.2020.103068,journal,Critical Reviews in Oncology/Hematology,sciencedirect,2020-10-31,sciencedirect,Radiomics and “radi-…omics” in cancer immunotherapy: a guide for clinicians,https://api.elsevier.com/content/article/pii/S1040842820302043,"
                  In recent years the concept of precision medicine has become a popular topic particularly in medical oncology. Besides the identification of new molecular prognostic and predictive biomarkers and the development of new targeted and immunotherapeutic drugs, imaging has started to play a central role in this new era. Terms such as “radiomics”, “radiogenomics” or “radi…-omics” are becoming increasingly common in the literature and soon they will represent an integral part of clinical practice. The use of artificial intelligence, imaging and “-omics” data can be used to develop models able to predict, for example, the features of the tumor immune microenvironment through imaging, and to monitor the therapeutic response beyond the standard radiological criteria.
                  The aims of this narrative review are to provide a simplified guide for clinicians to these concepts, and to summarize the existing evidence on radiomics and “radi…-omics” in cancer immunotherapy.
               ",autonomous vehicle
10.1016/B978-0-12-820273-9.20001-8,journal,Machine Learning in Cardiovascular Medicine,sciencedirect,2021-12-31,sciencedirect,Index,https://api.elsevier.com/content/article/pii/B9780128202739200018,Unknown,autonomous vehicle
10.1016/B978-0-12-814411-4.00006-8,journal,Neural Circuit and Cognitive Development,sciencedirect,2020-12-31,sciencedirect,Chapter 6: Spike timing–dependent plasticity,https://api.elsevier.com/content/article/pii/B9780128144114000068,"
               Spike timing–dependent plasticity (STDP) is a form of long-term synaptic plasticity in which the precise order and timing of pre- and postsynaptic action potentials trigger long-term synaptic potentiation or depression. STDP exists in distinct Hebbian and anti-Hebbian forms, defined by whether presynaptic inputs that precede postsynaptic spikes are potentiated or depressed. STDP is found at many, but not all synapses, and is part of a broader learning rule in which spike timing, firing rate, and postsynaptic depolarization all contribute to induction of plasticity. STDP has powerful computational properties that can explain many aspects of activity-dependent circuit development and adult learning. Empirical evidence implicates STDP in several forms of sensory perceptual learning, experience-dependent circuit refinement, and sensorimotor learning in vivo. However, the overall prominence of STDP versus other forms of associative synaptic plasticity in vivo is still unsettled. This chapter reviews the properties, cellular mechanisms, and in vivo evidence for STDP.
            ",autonomous vehicle
10.1016/S0893-6080(14)00258-5,journal,Neural Networks,sciencedirect,2015-01-31,sciencedirect,Neural Networks,https://api.elsevier.com/content/article/pii/S0893608014002585,,autonomous vehicle
10.1016/j.neunet.2021.07.003,journal,Neural Networks,sciencedirect,2021-11-30,sciencedirect,Low-shot transfer with attention for highly imbalanced cursive character recognition,https://api.elsevier.com/content/article/pii/S0893608021002665,"
                  Recognition of ancient Korean–Chinese cursive character (Hanja) is a challenging problem mainly because of large number of classes, damaged cursive characters, various hand-writing styles, and similar confusable characters. They also suffer from lack of training data and class imbalance issues. To address these problems, we propose a unified Regularized Low-shot Attention Transfer with Imbalance 
                        τ
                     -Normalizing 
                        
                           (
                           R
                           E
                           L
                           A
                           T
                           I
                           N
                           )
                        
                      framework. This handles the problem with instance-poor classes using a novel low-shot regularizer that encourages the norm of the weight vectors for classes with few samples to be aligned to those of many-shot classes. To overcome the class imbalance problem, we incorporate a decoupled classifier to rectify the decision boundaries via classifier weight-scaling into the proposed low-shot regularizer framework. To address the limited training data issue, the proposed framework performs Jensen–Shannon divergence based data augmentation and incorporate an attention module that aligns the most attentive features of the pretrained network to a target network. We verify the proposed RELATIN framework using highly-imbalanced ancient cursive handwritten character datasets. The results suggest that (i) the extreme class imbalance has a detrimental effect on classification performance; (ii) the proposed low-shot regularizer aligns the norm of the classifier in favor of classes with few samples; (iii) weight-scaling of decoupled classifier for addressing class imbalance appeared to be dominant in all the other baseline conditions; (iv) further addition of the attention module attempts to select more representative features maps from base pretrained model; (v) the proposed 
                        
                           (
                           R
                           E
                           L
                           A
                           T
                           I
                           N
                           )
                        
                      framework results in superior representations to address extreme class imbalance issue.
               ",autonomous vehicle
10.1016/B978-0-08-100659-7.00001-4,journal,Machine Learning,sciencedirect,2018-12-31,sciencedirect,Chapter 1: The Big Picture,https://api.elsevier.com/content/article/pii/B9780081006597000014,"
               
                  This chapter gives a big picture of book. It provides motivation for the study of the discipline and introduces the intriguing topic of induction, by showing its puzzling nature, as well as its necessity in any task, which involves perceptual information. In order to stimulate a parallel learning of lab topics, the chapter contains a section on hands-on experience that focuses on the usage of advanced development tools for dealing with the MNIST classic benchmark of handwritten character recognition. Finally, a few machine learning challenges are discussed with the purpose of promoting the idea of constructing intelligent agents that live in their own environment.
            ",autonomous vehicle
10.1016/j.jisa.2019.02.007,journal,Journal of Information Security and Applications,sciencedirect,2019-06-30,sciencedirect,Fraud detection for E-commerce transactions by employing a prudential Multiple Consensus model,https://api.elsevier.com/content/article/pii/S2214212618304216,"
                  More and more financial transactions through different E-commerce platforms have appeared now-days within the big data era bringing plenty of opportunities but also challenges and risks of stealing information for potential frauds that need to be faced. This is due to the massive use of tools such as credit cards for electronic payments which are targeted by attackers to steal sensitive information and perform fraudulent operations. Although intelligent fraud detection systems have been developed to face the problem, they still suffer from some well-known problems due to the imbalance of the used data. Therefore this paper proposes a novel data intelligence technique based on a Prudential Multiple Consensus model which combines the effectiveness of several state-of-the-art classification algorithms by adopting a twofold criterion, probabilistic and majority based. The goal is to maximize the effectiveness of the model in detecting fraudulent transactions regardless the presence of any data imbalance. Our model has been validated with a set of experiments on a large real-world dataset characterized by a high degree of data imbalance and results show how the proposed model outperforms several state-of-the-art solutions, both in terms of ensemble models and classification approaches.
               ",autonomous vehicle
10.1016/j.apenergy.2020.116258,journal,Applied Energy,sciencedirect,2021-02-01,sciencedirect,Design and optimization of Stirling engines using soft computing methods: A review,https://api.elsevier.com/content/article/pii/S0306261920316482,"
                  The need for energy converters with high thermal efficiency is a central issue in the field of renewable energies. So far, different technologies have been introduced for converting renewable energies into mechanical work. Stirling engines with an optimal design can be an appropriate choice for this aim. In this paper, the applications of soft computing methods in optimization and design of the Stirling engines are discussed. Until now, four popular soft computing approaches such as genetic algorithm, particle swarm optimization, fuzzy logic, and artificial neural network have been extensively applied to design and optimize the Stirling engines. Addressing the conducted works in this field, reveals that these soft computing methods can effectively meet the main concerns of the researchers. The performance of the Stirling engines in terms of power and efficiency can be promoted by optimizing their parameters via the soft computing methods. Moreover, the soft computing methods can be further employed to optimize the Stirling engines based on other objectives such as desired operating frequency, desired strokes of power and displacer pistons, and optimal locations of closed-loop poles of the system. On the other hand, combining these soft computing methods results in hybrid intelligent techniques that serves to predict other complex characteristics of these engines including torque, heat transfer, and damping coefficients. The hybrid techniques usually contain the artificial neural networks (or fuzzy logic) incorporating the evolutionary (or swarm intelligence) algorithms for designing, optimizing, and predicting the engine specifications.
               ",autonomous vehicle
10.1016/j.physrep.2021.03.002,journal,Physics Reports,sciencedirect,2021-06-25,sciencedirect,"Physical principles of brain–computer interfaces and their applications for rehabilitation, robotics and control of human brain states",https://api.elsevier.com/content/article/pii/S0370157321001095,"
                  Brain–computer interfaces (BCIs) development is closely related to physics. In this paper, we review the physical principles of BCIs, and underlying novel approaches for registration, analysis, and control of brain activity. We analyze recent advances in BCI studies focusing on their applications for (i) controlling the movement of robots and exoskeletons, (ii) revealing and preventing brain pathologies, (iii) assessing and controlling psychophysiological states, and (iv) monitoring and controlling normal and pathological cognitive activity.
                  We consider the BCI as a hardware/software communication system that allows interaction of humans or animals with their surroundings without the involvement of peripheral nerves and muscles, using control signals generated from brain cerebral activity. Classifying BCIs into three main types (active, reactive and passive), we describe their functional models and neuroimaging methods, as well as novel techniques for signal enhancement and artifact recognition and avoidance, to improve BCI performance in real time. We also review different BCI applications, including communications, external device control, movement control, neuroprostheses, and assessment of human psychophysiological states.
                  Then, we describe the most common techniques for the analysis and classification of electroencephalographic (EEG) and magnetoencephalographic (MEG) data. Special attention is paid to modern technology based on machine learning and reservoir computing. We discuss main results on the creation and application of BCIs based on invasive and noninvasive EEG recordings. First, we consider neurointerfaces for controlling the movement of robots and exoskeletons. Second, we describe BCIs for diagnosis and control of pathological brain activity, in particular, epilepsy. We also discuss the results on the development of invasive BCIs for predicting and mitigating absence epileptic seizures. After that, we focus on passive neurointerfaces for assessing and controlling a person’s psychophysiological states and cognitive activity. Special attention is given to optogenetic brain interfaces using photostimulation to deliver intervention to specific cell types. We outline the basic principles of optogenetic neurocontrol and extracellular electrophysiology recording. We also describe the state-of-the-art of miniaturized closed-loop optogenetic devices to control normal and pathological brain activities.
                  Further, we discuss the new emerging technological trend in the BCI development which consists in using neurointerfaces to improve the interaction between people, so-called brain-to-brain interfaces (BBIs). Such interfaces can increase the efficiency of collaborative processes when working in a group. We propose a BBI which distributes a cognitive load among all team members working on a common task. This BBI allows sharing the workload among the participants according to their current cognitive performance, estimated from their electrical brain activity. The novel results of the brain-to-brain interaction are promising for the development of a new generation of communication systems based on the neurophysiological brain activity of interacting persons, where the BBI estimates physical conditions of each partner and adapts the assigned task accordingly.
                  Finally, we trace the main historical epochs in BCI development and applications and highlight possible future directions for this research area, including hybrid BCIs.
               ",autonomous vehicle
10.1016/j.matpr.2021.03.367,journal,Materials Today: Proceedings,sciencedirect,2021-04-28,sciencedirect,Prediction of rumour source identification through spam detection on social Networks- A survey,https://api.elsevier.com/content/article/pii/S2214785321024196,"
                  Online social networks (OSNs) are becoming part of everyone’s virtual world which helps people to connect and disseminate all kind of opinions, news, and other information in real-time. In this era of social networking, an increased amount of novel information being spread with different viewpoints in an unprecedented scale. However, along with useful information propagation, OSNs also serve as fertile land for false information or rumour propagation. This causes irreversible damage to society during emergency events as a negative effect and hinder them from the actual information. Normally, internet media like OSNs, gossip circulate quicker than average. Therefore, studying the rumour dynamics and controlling the rumours in OSNs is of critical research interest in recent times. To achieve this purpose, a robust and reliable centrality model need to be introduced to discover influential spreader by considering the interest of individual’s on neighbor. The opinion of an individual with neighbors is the primary source for this method along with their location in the network. Identifying the rumours during inception stages is difficult due to the lesser availability of data related to the rumour. In this paper a brief survey is done on different rumour source identification techniques.
               ",autonomous vehicle
10.1016/j.patrec.2021.09.008,journal,Pattern Recognition Letters,sciencedirect,2021-11-30,sciencedirect,Visual question answering: Which investigated applications?,https://api.elsevier.com/content/article/pii/S0167865521003147,"
                  Visual Question Answering (VQA) is an extremely stimulating and challenging research area where Computer Vision (CV) and Natural Language Processig (NLP) have recently met. In image captioning and video summarization, the semantic information is completely contained in still images or video dynamics, and it has only to be mined and expressed in a human-consistent way. Differently from this, in VQA semantic information in the same media must be compared with the semantics implied by a question expressed in natural language, doubling the artificial intelligence-related effort. Some recent surveys about VQA approaches have focused on methods underlying either the image-related processing or the verbal-related one, or on the way to consistently fuse the conveyed information. Possible applications are only suggested, and, in fact, most cited works rely on general-purpose datasets that are used to assess the building blocks of a VQA system. This paper rather considers the proposals that focus on real-world applications, possibly using as benchmarks suitable data bound to the application domain. The paper also reports about some recent challenges in VQA research.
               ",autonomous vehicle
10.1016/S0967-0661(96)90051-9,journal,Control Engineering Practice,sciencedirect,1996-06-30,sciencedirect,IFAC workshop on artificial intelligence in real-time control 1995,https://api.elsevier.com/content/article/pii/S0967066196900519,,autonomous vehicle
10.1016/0957-4174(95)00003-R,journal,Expert Systems with Applications,sciencedirect,1995-12-31,sciencedirect,Adjustments of error by neural networks for the shear stress carried by the stirrups of a beam-column joint,https://api.elsevier.com/content/article/pii/095741749500003R,"
                  The application of neural networks in the form of parameter predictions to assess the shear stress carried by the stirrups of reinforced concrete beam-column joint under axial load and biaxial bending has been considered. Computation algorithms in the form of numerical analysis were performed on the beam-column joint to simulate existing experimental data. The focus of this paper is to reconstruct existing experimental data by evaluating several parameters and establishing valid mathematical relationships based on neural networks that are in close agreement with existing relationships based on experimental results. Adjustments of error due to lateral stirrup spacing were carried out using neural network techniques to enhance the values of shear stress carried by the stirrups within the joint. The procedure demonstrates the capability of the back propagation algorithm with supervised learning to enhance the lateral spacing by reducing the percentage errors present in analysing experimental results.
               ",autonomous vehicle
10.1016/j.petrol.2020.107504,journal,Journal of Petroleum Science and Engineering,sciencedirect,2020-11-30,sciencedirect,Data-driven model for hydraulic fracturing design optimization: focus on building digital database and production forecast,https://api.elsevier.com/content/article/pii/S0920410520305751,"
                  Growing amount of hydraulic fracturing (HF) jobs in the recent two decades resulted in a significant amount of measured data available for development of predictive models via machine learning (ML). In multistage fractured completions, post-fracturing production analysis (e.g., from production logging tools) reveals evidence that different stages produce very non-uniformly, and up to 30% may not be producing at all due to a combination of geomechanics and fracturing design factors. Hence, there is a significant room for improvement of current design practices. We propose a data-driven model for fracturing design optimization, where the workflow is essentially split into two stages. As a result of the first stage, the present paper summarizes the efforts in the creation of a digital database of field data from several thousands of multistage HF jobs on vertical, inclined and near-horizontal wells from circa 20 different oilfields in Western Siberia, Russia. In terms of the number of points (fracturing jobs), the present database is a rare case of a representative dataset of about 5000 data points, compared to typical databases available in the literature, comprising tens or hundreds of points at best. Each point in the data base contains the vector of 92 input variables (the reservoir, well and the frac design parameters) and the vector of production data, which is characterized by 16 parameters, including the target, cumulative oil production. The focus is made on data gathering from various sources, data preprocessing and development of the architecture of the database as well as solving the production forecast problem via ML. Data preparation has been done using various ML techniques: the problem of missing values in the database is solved with collaborative filtering for data imputation; outliers are removed using visualization of cluster data structure by t-SNE algorithm. The production forecast problem is solved via CatBoost algorithm. Prediction capability of the model is measured with the coefficient of determination (
                        
                           
                              R
                              2
                           
                        
                     ) and reached 0.815. The inverse problem (selecting an optimum set of fracturing design parameters to maximize production) will be considered in the second part of the study to be published in another paper, along with a recommendation system for advising DESC and production stimulation engineers on an optimized fracturing design.
               ",autonomous vehicle
10.1016/B978-0-08-102584-0.00016-4,journal,Advances in Non-Volatile Memory and Storage Technology,sciencedirect,2019-12-31,sciencedirect,15: Emerging memory technologies for neuromorphic hardware,https://api.elsevier.com/content/article/pii/B9780081025840000164,"
               This chapter deals with the role that emerging nonvolatile resistive memory technologies (ReRAM) play in the implementation of optimized neuromorphic hardware as a highly promising solution for future ultralow-power cognitive systems.
               The chapter is organized as follows. We start with an introduction about status and main challenges of current artificial intelligent systems. The key reasons to distribute intelligence over the whole communication network are then discussed, underlining the need for low-power solutions based on specialized embedded hardware. We then show that emerging technologies (and in particular novel resistive memories), coupled with new brain-inspired paradigms, such as spike coding and spike-time-dependent plasticity, have extraordinary potential to provide intelligent features in hardware, approaching the way knowledge is created and processed in the human brain. After a brief survey of the main features and trends of resistive memories (in particular phase-change-, oxide-resistive-, and conductive-bridge-memories), we will introduce the advantages of using ReRAM as synapses to emulate plasticity in spiking neural networks. Several applications (as visual-pattern extraction, bio-signal sorting, etc.) are discussed. Finally, we conclude with our vision of the enabled future directions and the main challenges which should be tackled to exploit the full potential of brain-inspired technologies.
            ",autonomous vehicle
10.1016/B978-0-12-822049-8.00010-4,journal,Advanced Welding and Deforming,sciencedirect,2021-12-31,sciencedirect,Chapter 10: Modern optimization techniques for performance enhancement in welding,https://api.elsevier.com/content/article/pii/B9780128220498000104,"
               Computational optimization approaches, specifically the soft computing techniques became popular among researchers for optimization of different fabrication and manufacturing processes, including welding technologies. The major techniques of soft computation include fuzzy logic systems, artificial neural networks, and evolutionary computing. Evolutionary algorithms and neural networks are now commonly used to enhance the mechanical characteristics of welds, such as tensile strength, hardness, and fatigue properties of welds. Different analytical and soft computing techniques are used for the prediction and control of the different phase material, grain morphology, residual stress, and welding defects. In this chapter, applications of soft computing techniques as well as their fusions to solve optimization issues, are reviewed and reported. The optimization in welding mainly in context of control of welding quality, productivity, microstructure, and weld properties, is presented.
            ",autonomous vehicle
10.1016/B978-0-12-823817-2.00024-3,journal,Mobile Edge Artificial Intelligence,sciencedirect,2022-12-31,sciencedirect,Index,https://api.elsevier.com/content/article/pii/B9780128238172000243,Unknown,autonomous vehicle
10.1016/B978-0-323-67538-3.20001-8,journal,Artificial Intelligence and Deep Learning in Pathology,sciencedirect,2021-12-31,sciencedirect,Index,https://api.elsevier.com/content/article/pii/B9780323675383200018,Unknown,autonomous vehicle
10.1016/B978-0-323-89861-4.10000-4,journal,Computers in Earth and Environmental Sciences,sciencedirect,2022-12-31,sciencedirect,Index,https://api.elsevier.com/content/article/pii/B9780323898614100004,Unknown,autonomous vehicle
10.1016/j.eswa.2018.09.034,journal,Expert Systems with Applications,sciencedirect,2019-02-28,sciencedirect,Clinical text classification research trends: Systematic literature review and open issues,https://api.elsevier.com/content/article/pii/S0957417418306110,"
                  The pervasive use of electronic health databases has increased the accessibility of free-text clinical reports for supplementary use. Several text classification approaches, such as supervised machine learning (SML) or rule-based approaches, have been utilized to obtain beneficial information from free-text clinical reports. In recent years, many researchers have worked in the clinical text classification field and published their results in academic journals. However, to the best of our knowledge, no comprehensive systematic literature review (SLR) has recapitulated the existing primary studies on clinical text classification in the last five years. Thus, the current study aims to present SLR of academic articles on clinical text classification published from January 2013 to January 2018. Accordingly, we intend to maximize the procedural decision analysis in six aspects, namely, types of clinical reports, data sets and their characteristics, pre-processing and sampling techniques, feature engineering, machine learning algorithms, and performance metrics. To achieve our objective, 72 primary studies from 8 bibliographic databases were systematically selected and rigorously reviewed from the perspective of the six aspects. This review identified nine types of clinical reports, four types of data sets (i.e., homogeneous–homogenous, homogenous–heterogeneous, heterogeneous–homogenous, and heterogeneous–heterogeneous), two sampling techniques (i.e., over-sampling and under-sampling), and nine pre-processing techniques. Moreover, this review determined bag of words, bag of phrases, and bag of concepts features when represented by either term frequency or term frequency with inverse document frequency, thereby showing improved classification results. SML-based or rule-based approaches were generally employed to classify the clinical reports. To measure the performance of these classification approaches, we used precision, recall, F-measure, accuracy, AUC, and specificity in binary class problems. In multi-class problems, we primarily used micro or macro-averaging precision, recall, or F-measure. Lastly, open research issues and challenges are presented for future scholars who are interested in clinical text classification. This SLR will definitely be a beneficial resource for researchers engaged in clinical text classification.
               ",autonomous vehicle
10.1016/B978-0-12-821229-5.00018-5,journal,Machine Learning and the Internet of Medical Things in Healthcare,sciencedirect,2021-12-31,sciencedirect,Index,https://api.elsevier.com/content/article/pii/B9780128212295000185,Unknown,autonomous vehicle
10.1016/j.neucom.2013.03.047,journal,Neurocomputing,sciencedirect,2014-08-05,sciencedirect,Time series forecasting using a deep belief network with restricted Boltzmann machines,https://api.elsevier.com/content/article/pii/S0925231213007388,"
                  Multi-layer perceptron (MLP) and other artificial neural networks (ANNs) have been widely applied to time series forecasting since 1980s. However, for some problems such as initialization and local optima existing in applications, the improvement of ANNs is, and still will be the most interesting study for not only time series forecasting but also other intelligent computing fields. In this study, we propose a method for time series prediction using Hinton and Salakhutdinov׳s deep belief nets (DBN) which are probabilistic generative neural network composed by multiple layers of restricted Boltzmann machine (RBM). We use a 3-layer deep network of RBMs to capture the feature of input space of time series data, and after pretraining of RBMs using their energy functions, gradient descent training, i.e., back-propagation learning algorithm is used for fine-tuning connection weights between “visible layers” and “hidden layers” of RBMs. To decide the sizes of neural networks and the learning rates, Kennedy and Eberhart׳s particle swarm optimization (PSO) is adopted during the training processes. Furthermore, “trend removal”, a preprocessing to the original data, is also approached in the forecasting experiment using CATS benchmark data. Additionally, approximating and short-term prediction of chaotic time series such as Lorenz chaos and logistic map were also applied by the proposed method.
               ",autonomous vehicle
10.1016/j.eswax.2020.100025,journal,Expert Systems with Applications: X,sciencedirect,2020-04-30,sciencedirect,Knowledge-based problem solving in physical product development––A methodological review,https://api.elsevier.com/content/article/pii/S2590188520300044,"The manufacturing of products at low maturity levels (referred to as physical product development) requires knowledge intensive nonconformance problem solving, yet constituting a major difficulty in industry. Due to the exponential increase of failure cost during the product development process however, problems have to be effectively remedied as early as possible. Facing shortened innovation cycles, problem solving efficiency simultaneously constitutes a competitive factor. The purpose of this theoretical review is therefore the analysis of relevant approaches contributing to knowledge-based problem solving in physical product development, to synthesize a comprehensive construct as well as to derive novel conceptualizations. The latter demonstrably emerges from natural language processing, case ontologies and machine-/deep learning support, embedded in a distributed case-based reasoning architecture. Building on this, we likewise encourage researchers and professionals to propose new studies dedicated to the field of problem solving in physical product development.",autonomous vehicle
10.1016/j.ins.2013.03.059,journal,Information Sciences,sciencedirect,2014-01-10,sciencedirect,Ant intelligence inspired blind data detection for ultra-wideband radar sensors,https://api.elsevier.com/content/article/pii/S0020025513002752,"
                  Given the computational complexity and sophisticated implementation of traditionally parametric channel estimators, it has been gradually recognized that the existing data detection methodologies based on the finite impulse response (FIR) propagation channel modeling may become infeasible for ultra-wideband (UWB) radar sensors, especially in some large-scale distributed scenarios. By exploiting the implicit information involved in the received signals, in this investigation, we present a non-parametric UWB data detection scheme for the distributed radar sensor networks. A novel characteristic representation is suggested first. From a pattern classification point of view, a group of quantitative features are then extracted by making full use of the inherent property of UWB propagations. Thus, UWB data detection is formulated as a pattern classification problem in a multidimensional feature space. By thoroughly utilizing the self-similarity of the representative patterns, the ant swarm intelligence inspired clustering algorithm, with the new designed ant movement strategy, is adopted to perform unsupervised data detections. The developed scheme is independent of any a priori modeling information, which essentially avoids the expensive parametric estimators and thus enables practically feasible realizations. To alleviate the computational burden, the principle component analysis (PCA) is further employed to compress the feature space. The simulation results validate the new algorithm, which is superior to the other popular non-parametric data analysis schemes.
               ",autonomous vehicle
10.1016/j.jocs.2020.101281,journal,Journal of Computational Science,sciencedirect,2021-04-30,sciencedirect,Clustering of graphs using pseudo-guided random walk,https://api.elsevier.com/content/article/pii/S1877750320305779,"
                  Clustering is an unsupervised learning task that models data as coherent groups. Multiple approaches have been proposed in the past to cluster large volumes of data. Graphs provide a logical mapping of many real-world datasets rich enough to reflect various peculiarities of numerous domains. Apart from k-means, k-medoid, and other well-known clustering algorithms, utilization of random walk-based approaches to cluster data is a prominent area of data mining research. Markov clustering algorithm and limited random walk-based clustering are the prominent techniques that utilize the concept of random walk. The main goal of this work is to address the task of clustering graphs using an efficient random walk-based method. A novel walk approach in a graph is presented here that determines the weight of the edges and the degree of the nodes. This information is utilized by the pseudo-guidance model to guide the random walk procedure. This work introduces the friends-of-friends concept during the random walk process so that the edges’ weights are determined utilizing an inclusive criterion. This concept enables a random walk to be initiated from the highest degree node. The random walk continues until the walking agent cannot find any unvisited neighbor(s). The agent walks to its neighbors if it finds a weight of one or more, otherwise the agent’s stopping criteria is met. The nodes visited in this walk form a cluster. Once a walk comes to halt, the visited nodes are removed from the original graph and the next walk starts in the remaining graph. This process continues until all nodes of the graph are traversed. The focus of this work remains random walk-based clustering of graphs. The proposed approach is evaluated using 18 real-world benchmark datasets utilizing six cluster validity indices, namely Davies-Bouldin index (DBI), Dunn index (DI), Silhouette coefficient (SC), Calinski-Harabasz index (CHI), modularity index, and normalized cut. This proposal is compared with seven closely related approaches from the same domain, namely, limited random walk, pairwise clustering, personalized page rank clustering, GAKH (genetic algorithm krill herd) graph clustering, mixing time of random walks, density-based clustering of large probabilistic graphs, and Walktrap. Experiments suggest better performance of this work based on the evaluation metrics.
               ",autonomous vehicle
10.1016/j.websem.2021.100664,journal,Journal of Web Semantics,sciencedirect,2021-11-30,sciencedirect,SemML: Facilitating development of ML models for condition monitoring with semantics,https://api.elsevier.com/content/article/pii/S1570826821000391,"Monitoring of the state, performance, quality of operations and other parameters of equipment and production processes, which is typically referred to as condition monitoring, is an important common practice in many industries including manufacturing, oil and gas, chemical and process industry. In the age of Industry 4.0, where the aim is a deep degree of production automation, unprecedented amounts of data are generated by equipment and processes, and this enables adoption of Machine Learning (ML) approaches for condition monitoring. Development of such ML models is challenging. On the one hand, it requires collaborative work of experts from different areas, including data scientists, engineers, process experts, and managers with asymmetric backgrounds. On the other hand, there is high variety and diversity of data relevant for condition monitoring. Both factors hampers ML modelling for condition monitoring. In this work, we address these challenges by empowering ML-based condition monitoring with semantic technologies. To this end we propose a software system SemML that allows to reuse and generalise ML pipelines for conditions monitoring by relying on semantics. In particular, SemML has several novel components and relies on ontologies and ontology templates for ML task negotiation and for data and ML feature annotation. SemML also allows to instantiate parametrised ML pipelines by semantic annotation of industrial data. With SemML, users do not need to dive into data and ML scripts when new datasets of a studied application scenario arrive. They only need to annotate data and then ML models will be constructed through the combination of semantic reasoning and ML modules. We demonstrate the benefits of SemML on a Bosch use-case of electric resistance welding with very promising results.",autonomous vehicle
10.3182/20050703-6-CZ-1902.01510,journal,IFAC Proceedings Volumes,sciencedirect,2005-12-31,sciencedirect,AUTOMATIC GENERATION A NET OF MODELS FOR HIGH AND LOW LEVELS OF PRODUCTION CONTROL,https://api.elsevier.com/content/article/pii/S147466701637522X,"
                  The paper presents two applications of the novel, artificial neural network (ANN) and feature selection based combined, dynamic technique to automatically dissolve a large, complex system into a net of connected submodels. The first application is a solution for the lower level of customised mass-production systems, for increasing their productivity. The second one is a concept for the identification and reorganisation of manufacturing agents, based on simulation experience. The main idea behind, is to give the learning capability to agents already at their definition phase, and to maximise their foresight power.
               ",autonomous vehicle
10.1016/j.tics.2009.06.002,journal,Trends in Cognitive Sciences,sciencedirect,2009-09-30,sciencedirect,Does Cognitive Science Need Kernels?,https://api.elsevier.com/content/article/pii/S1364661309001430,"
                  Kernel methods are among the most successful tools in machine learning and are used in challenging data analysis problems in many disciplines. Here we provide examples where kernel methods have proven to be powerful tools for analyzing behavioral data, especially for identifying features in categorization experiments. We also demonstrate that kernel methods relate to perceptrons and exemplar models of categorization. Hence, we argue that kernel methods have neural and psychological plausibility, and theoretical results concerning their behavior are therefore potentially relevant for human category learning. In particular, we believe kernel methods have the potential to provide explanations ranging from the implementational via the algorithmic to the computational level.
               ",autonomous vehicle
10.1016/j.jcmg.2021.01.002,journal,JACC: Cardiovascular Imaging,sciencedirect,2021-02-28,sciencedirect,Building Trust in AI: Opportunities and Challenges for Cardiac Imaging,https://api.elsevier.com/content/article/pii/S1936878X21000528,,autonomous vehicle
10.1016/B978-0-323-85172-5.00018-6,journal,"Electronic Devices, Circuits, and Systems for Biomedical Applications",sciencedirect,2021-12-31,sciencedirect,Chapter 22: Health monitoring system,https://api.elsevier.com/content/article/pii/B9780323851725000186,"
               Electronics have become an essential part of biomedicine. The urge for real-time health monitoring and disease detection at an early stage has created a rapid growth of the market for smart sensors. Biosensors have investigated the prospects of point of care (POC) applications for better management of healthcare, and efforts are being made to make these more efficient. Integrating with micro-electro-mechanical systems (MEMS) and nano-electro-mechanical systems (NEMS) technology has enabled biosensors to be automated and more precise, with higher accuracy data sensing systems. The application of biosensors with POC has increased research related to nanotechnology, advanced functional sensing materials, miniaturized sensing system development, AI, and the internet of things (IoT). Breath analysis is one such form for which biosensors have been used. Diabetes, Parkinson disease, urinary tract infections, lung cancer, kidney disease, pancreas infection, etc., can be detected through breath analysis. This chapter deals with a smart sensor system for diagnosis of diseases, precisely chronologic at an early stage. The sensor system is developing for detecting volatile organic compounds. Sensor arrays are deployed to collect and process electromagnetic or acoustic signals. Health monitoring systems provides a better perception of the patient’s condition, allowing doctors to make the correct diagnosis in real time and enhance curative procedure. IoT integrated with machine learning and artificial intelligence plays a vital role here.
            ",autonomous vehicle
10.1016/j.neucom.2015.05.022,journal,Neurocomputing,sciencedirect,2015-11-30,sciencedirect,Gene selection for microarray data classification using a novel ant colony optimization,https://api.elsevier.com/content/article/pii/S0925231215006451,"
                  The high-dimensionality of microarray data with small number of samples has presented a difficult challenge for the microarray data classification task. The aim of gene selection is to reduce the dimensionality of microarray data in order to enhance the accuracy of the classification task. Existing gene selection methods generally use class labels of the data while due to availability of mislabels or unreliable labels of samples in the microarray data, unsupervised methods could be more essential to the gene selection process. In this paper, we propose an unsupervised gene selection method called MGSACO, which incorporates the ant colony optimization algorithm into the filter approach, by minimizing the redundancy between genes and maximizing the relevance of genes. Moreover, a new fitness function is applied in the proposed method which does not need any learning model to evaluate the subsets of selected genes. Thus, it is classified into the filter approach. The performance of the proposed method is extensively tested upon five publicly available microarray datasets, and it is compared to those of the seven well-known unsupervised and supervised gene selection methods in terms of classification error rates of the three frequently used classifiers including support vector machine, naïve Bayes, and decision tree. Experimental results show that MGSACO is significantly superior to the existing methods over different classifiers and datasets.
               ",autonomous vehicle
10.1016/j.procs.2018.10.481,journal,Procedia Computer Science,sciencedirect,2018-12-31,sciencedirect,A Sequence-to-Sequence based Approach For the double Transliteration of Tunisian Dialect,https://api.elsevier.com/content/article/pii/S1877050918321859,"Transliteration consists of automatically transforming a grapheme’s transcription from one writing system to another, while preserving its pronunciation. It is usually used in the context of machine translation and cross language information retrieval, mainly to deal with the issue of named entities and technical terms. In the case of some Arabic dialects, which are used on the social web in both Latin and Arabic scripts and which are still low-resource languages, transliteration is of great benefit for the automatic generation of various linguistic resources (parallel corpora and lexica), useful for their automatic processing. In this work, we focus on the Tunisian dialect transliteration. We propose a deep learning based Sequence-to-Sequence approach to perform a word-level transliteration of the user generated Tunisian dialect on the social web, in both Latin to Arabic and Arabic to Latin senses.",autonomous vehicle
10.1016/B978-0-12-820201-2.00002-7,journal,Artificial Intelligence in Cancer,sciencedirect,2020-12-31,sciencedirect,Chapter 2: The beginnings,https://api.elsevier.com/content/article/pii/B9780128202012000027,"
               The aim of this chapter is to present how the medical process begins when the first cancer suspicion rises, together with different state-of-the-art artificial intelligence methods that help the doctors diagnose faster and more accurately the disease. The doctor + artificial intelligence combo is presented. This chapter together with the statistical analysis practical issues learned in the first chapter establishes the grounds for artificial intelligence applied cancer research.
            ",autonomous vehicle
10.1016/j.pmcj.2009.04.001,journal,Pervasive and Mobile Computing,sciencedirect,2009-08-31,sciencedirect,"Ambient intelligence: Technologies, applications, and opportunities",https://api.elsevier.com/content/article/pii/S157411920900025X,"
                  Ambient intelligence is an emerging discipline that brings intelligence to our everyday environments and makes those environments sensitive to us. Ambient intelligence (AmI) research builds upon advances in sensors and sensor networks, pervasive computing, and artificial intelligence. Because these contributing fields have experienced tremendous growth in the last few years, AmI research has strengthened and expanded. Because AmI research is maturing, the resulting technologies promise to revolutionarize daily human life by making people’s surroundings flexible and adaptive.
                  In this paper, we provide a survey of the technologies that comprise ambient intelligence and of the applications that are dramatically affected by it. In particular, we specifically focus on the research that makes AmI technologies “intelligent”. We also highlight challenges and opportunities that AmI researchers will face in the coming years.
               ",autonomous vehicle
10.1016/j.procs.2016.08.127,journal,Procedia Computer Science,sciencedirect,2016-12-31,sciencedirect,CL-AntInc Algorithm for Clustering Binary Data Streams Using the Ants Behavior,https://api.elsevier.com/content/article/pii/S1877050916319287,"In this paper, we present a new approach using a non-hierarchical method in graph environment and the concept of artificial ants for both clustering and visualization using Tulip framework. This model can be presented to take into account data in blocks in an incremental way. It seems especially interesting to process binary data streaming. In this algorithm, we also suggest to apply swarm intelligence techniques for the incremental processing of this new challenging data type. The main novelty of this research work resides on the adaptation of CL-AntInc to perform clustering binary data streams and building growing graphs increasingly for this type of data. The proposed algorithm performance is evaluated using real world data sets extracted from Machine Learning Repository. Our algorithm is competitive when compared with other stream clustering methods.",autonomous vehicle
10.1016/j.istruc.2020.10.048,journal,Structures,sciencedirect,2020-12-31,sciencedirect,Application of ANN to the design of CFST columns,https://api.elsevier.com/content/article/pii/S2352012420306056,"
                  In this paper, artificial neural network (ANN) is used to predict the ultimate strength of rectangular and circular concrete-filled steel tubular (CFST) columns subjected to concentric and eccentric loading. Four comprehensive datasets are compiled and used for developing ANN-based predictive models. Empirical equations are also derived from the weights and biases of the ANNs to predict the ultimate strength of CFST columns. The proposed empirical equations can be used for both normal strength and high strength CFST columns with different section slenderness ratios (compact, non-compact and slender sections), and with different length-to-depth ratios (stub and slender columns). The test results are then compared with those predicted from the proposed empirical equations, American code, European code and Australian code. The comparison study shows that the ultimate strengths predicted from the proposed equations have the best agreement with the experimental results with the least mean square error (MSE). In addition, strength reduction factors for the proposed equations are derived using Monte Carlo simulation (MCS). The use of the proposed strength reduction factors will ensure that CFST columns designed by the developed ANN-based equations are safe because their reliability indices meet the target value of 3.0 required by American code or 3.8 required by European and Australian codes.
                  
               ",autonomous vehicle
10.1016/B978-0-444-70414-6.50011-X,journal,The Adaptive Brain II,sciencedirect,1987-12-31,sciencedirect,"Chapter 7: NEURAL DYNAMICS OF WORD RECOGNITION AND RECALL: ATTENTIONAL PRIMING, LEARNING, AND RESONANCE",https://api.elsevier.com/content/article/pii/B978044470414650011X,"
               Data and models about recognition and recall of words and nonwords are unified using a real-time network processing theory. Lexical decision and word frequency effect data are analysed in terms of the same theoretical concepts that have unified data about development of circular reactions, imitation of novel sounds, matching phonetic to articulatory requirements, serial and paired associate verbal learning, free recall, unitization, categorical perception, selective adaptation, auditory contrast, and word superiority effects. The theory, called adaptive resonance theory, arose from an analysis of how a language system self-organizes in real-time in response to its complex input environment. Such an approach emphasizes the moment-by-moment dynamical interactions that control language development, learning, and stability. Properties of language performance emerge from an analysis of the system constraints that govern stable language learning. Concepts such as logogens, verification, automatic activation, interactive activation, limited-capacity processing, conscious attention, serial search, processing stages, speed-accuracy trade-off, situational frequency, familiarity, and encoding specificity are revised and developed using this analysis. Concepts such as adaptive resonance, resonant equilibration of short term memory, bottom-up adaptive filtering, top-down adaptive template matching, competitive masking field, unitized list representation, temporal order information over item representations, attentional priming, attentional gain control, and list-item error trade-off are applied.
            ",autonomous vehicle
10.1016/j.jobe.2020.101967,journal,Journal of Building Engineering,sciencedirect,2021-03-31,sciencedirect,Towards developing a systematic knowledge trend for building energy consumption prediction,https://api.elsevier.com/content/article/pii/S2352710220335993,"
                  The rapid depletion of natural sources of energy, coupled with increasing global population has triggered the emergence of various techniques and strategies for building energy consumption prediction. According to information from existing body of knowledge, this paper systematically brings to fore the application areas of building energy consumption prediction (i.e. well-established and emerging), the relationships between these areas and the ways in which authors integrate the current spate of techniques. Based on direct implications of buildings on global energy consumption and CO2 emissions, this information makes it possible to identify trends, strengths and limitations in this context, thereby enabling the centralisation of activities required for future studies. This study follows several well-documented guides for conducting logical reviews of primary articles concerning main topics of building energy consumption prediction within popular online databases. The definition of articles’ search keywords as well as inclusion/exemption factors were governed by a combination of principles stipulated by Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) and Procedures for Performing Systematic Reviews (PPSR). In comparison to existing review articles in the studied field, the current study is novel in the sense that it provides a very holistic view to building energy consumption prediction, thereby minimising the need to consult multiple individualised studies that are limited to specific techniques, data sets, regions or types of buildings. Another unique feature of this study is its interrelationship network of articles which depicts a quick glance at some of the most influential studies as well as underrepresented areas, thereby aiding research planning, future directions and cross-disciplinary collaborations.
               ",autonomous vehicle
10.1016/j.comnet.2009.01.002,journal,Computer Networks,sciencedirect,2009-06-25,sciencedirect,Trends in the development of communication networks: Cognitive networks,https://api.elsevier.com/content/article/pii/S1389128609000085,"
                  One of the main challenges already faced by communication networks is the efficient management of increasing complexity. The recently proposed concept of cognitive network appears as a candidate that can address this issue. In this paper, we survey the existing research work on cognitive networks, as well as related and enabling techniques and technologies. We start with identifying the most recent research trends in communication networks and classifying them according to the approach taken towards the traditional layered architecture. In the analysis we focus on two related trends: cross-layer design and cognitive networks. We classify the cognitive networks related work in that mainly concerned with knowledge representation and that predominantly dealing with the cognition loop. We discuss the existing definitions of cognitive networks and, with respect to those, position our understanding of the concept. Next, we provide a summary of artificial intelligence techniques that are potentially suitable for the development of cognitive networks, and map them to the corresponding states of the cognition loop. We summarize and compare seven architectural proposals that comply with the requirements for a cognitive network. We discuss their relative merits and identify some future research challenges before we conclude with an overview of standardization efforts.
               ",autonomous vehicle
10.1016/j.micpro.2004.08.012,journal,Microprocessors and Microsystems,sciencedirect,2005-06-01,sciencedirect,An FPGA platform for on-line topology exploration of spiking neural networks,https://api.elsevier.com/content/article/pii/S0141933104001164,"
                  In this paper we present a platform for evolving spiking neural networks on FPGAs. Embedded intelligent applications require both high performance, so as to exhibit real-time behavior, and flexibility, to cope with the adaptivity requirements. While hardware solutions offer performance, and software solutions offer flexibility, reconfigurable computing arises between these two types of solutions providing a trade-off between flexibility and performance. Our platform is described as a combination of three parts: a hardware substrate, a computing engine, and an adaptation mechanism. We present, also, results about the performance and synthesis of the neural network implementation on an FPGA.
               ",autonomous vehicle
10.1016/B978-0-323-90472-8.00001-9,journal,Battery System Modeling,sciencedirect,2021-12-31,sciencedirect,Chapter 4: Battery state estimation methods,https://api.elsevier.com/content/article/pii/B9780323904728000019,"
               The battery state estimation is a very important task in its management system. The state of charge represents the battery’s remaining energy ratio after a period of use or a long period of disuse, which can reflect the battery life or the battery remaining use time. As for the battery operation, the state parameter reflects its working conditions. The estimation methods are described for the battery state estimation of different working conditions. Before the battery state estimation, the definition of its state parameters is conducted, including state of charge, state of energy, state of power, state of health, and remaining useful life. After that, the main state influencing factors are analyzed as well as algorithm fusion and comparison. The parameter measurement technology is then introduced into the balancing control theory analysis and temperature adjustment. For the estimation method analysis, the foundational methods are analyzed in advance, including open-circuit voltage and ampere hour integral. The smart algorithms are introduced such as extended Kalman filtering, support vector machine, neural network, and particle filtering.
            ",autonomous vehicle
10.1016/S0893-6080(11)00319-4,journal,Neural Networks,sciencedirect,2012-02-29,sciencedirect,Neural networks,https://api.elsevier.com/content/article/pii/S0893608011003194,,autonomous vehicle
10.1016/B978-0-08-041898-8.50017-7,journal,Artificial Intelligence in Real-Time Control 1992,sciencedirect,1993-12-31,sciencedirect,A TARGET-DIRECTED NEURALLY CONTROLLED VEHICLE,https://api.elsevier.com/content/article/pii/B9780080418988500177,"
               Keywords actuators, adaptive control, artificial intelligence, automation, computer applications, neural nets, optimal search techniques, robots, signal processing, simulation",autonomous vehicle
10.1016/B978-012443870-5.50051-4,journal,Fuzzy Theory Systems,sciencedirect,1999-12-31,sciencedirect,49: Techniques in Neural Fuzzy Systems and Their Applications in Processing Both Numerical and Linguistic Information,https://api.elsevier.com/content/article/pii/B9780124438705500514,"
               This chapter explores the approach to supervised learning of neural fuzzy systems that receive only linguistic teaching signals. The chapter proposes the basic structure of a five-layered feedforward network for the network realization of a fuzzy inference system. This connectionist structure can house fuzzy logic rules and membership functions and perform fuzzy inference. It uses α-level sets of fuzzy numbers to represent linguistic information. The inputs, outputs, and weights of the proposed network can be fuzzy numbers of any shape. As numerical values can be represented by fuzzy singletons, the proposed system can, in fact, process and learn hybrids of fuzzy numbers and numerical numbers. Based on interval arithmetic, a fuzzy supervised learning scheme is developed for the proposed system. This scheme generalizes the normal supervised learning techniques to the learning problems in which only linguistic teaching signals are available. The fuzzy supervised learning scheme can train the proposed network with desired fuzzy input-output pairs (or, equivalently, desired fuzzy if-then rules) represented by fuzzy numbers instead of numerical values. With supervised learning, the proposed system can be used for rule base concentration to reduce the number of rules in a fuzzy rule base. To illustrate the practical application of the proposed techniques, the chapter further develops a fuzzy language acquisition network. This network can catch the intended information from a sentence (command) spoken in a natural language with fuzzy terms. The intended information includes a meaningful semantic action and the fuzzy linguistic information of that action.
            ",autonomous vehicle
10.1016/j.neuron.2021.01.009,journal,Neuron,sciencedirect,2021-02-17,sciencedirect,Visualizing a joint future of neuroscience and neuromorphic engineering,https://api.elsevier.com/content/article/pii/S089662732100009X,"
                  Recent research resolves the challenging problem of building biophysically plausible spiking neural models that are also capable of complex information processing. This advance creates new opportunities in neuroscience and neuromorphic engineering, which we discussed at an online focus meeting.
               ",autonomous vehicle
10.1016/j.cosrev.2021.100402,journal,Computer Science Review,sciencedirect,2021-05-31,sciencedirect,Financial fraud detection applying data mining techniques: A comprehensive review from 2009 to 2019,https://api.elsevier.com/content/article/pii/S1574013721000423,"
                  This paper gives a comprehensive revision of the state-of-the-art research in detecting financial fraud from 2009 to 2019 inclusive and classifying them based on their types of fraud and data mining technology utilized in detecting financial fraud. The review result yielded a sample of 75 relevant articles (58 conference papers with 17 peer-reviewed journal articles) that are categorized into four main groups (bank fraud, insurance fraud, financial statement fraud, and cryptocurrency fraud). The study shows that 34 data mining techniques were used to identify fraud throughout various financial applications. The SVM is found to be one of the most widely used financial fraud detection techniques that carry about 23% of the overall study, followed by both Naïve Bayes and Random Forest, resulting in 15%. The results of our comprehensive review revealed that most data mining techniques are extensively implemented to bank fraud and insurance fraud with a total of 61 research studies out of 75 that constitute the largest portion equal to 81.33% of the overall number of papers. This review provides a good reference source in guiding the detection of financial fraud for both academic and practical industries with useful information on the most significant data mining techniques used and shows the list of countries that are exposed to financial fraud. Our review contributes by expanding the sample of the reviewed articles that were not included by previous research and presents a summary of the prominent works done by various researchers in the field of financial fraud.
               ",autonomous vehicle
10.1016/j.comnet.2021.107950,journal,Computer Networks,sciencedirect,2021-05-08,sciencedirect,NOMA and 5G emerging technologies: A survey on issues and solution techniques,https://api.elsevier.com/content/article/pii/S1389128621000888,"
                  Power Domain Non-Orthogonal Multiple Access (PD-NOMA) is a potential technology for the next generation of cellular networks. Compared to classical orthogonal multiple access (OMA) techniques, PD-NOMA leverages the distinct channel gains of users for multiplexing different signals in a single resource block (time, frequency, code) in power domain. This results in higher spectral efficiency, improved user fairness, better cell-edge throughput, increased reliability and connectivity and low-latency. The flexible combination of PD-NOMA with existing and emerging technologies such as heterogeneous networks (HetNets), multiple-input multiple-output (MIMO), massive MIMO, cooperative communication, cognitive radios (CRs), millimeter wave communication, simultaneous wireless information and power transfer (SWIPT), visible light communication (VLC), mobile edge computing (MEC), intelligent reflecting surfaces (IRS), unmanned aerial vehicles (UAVs), underwater communication etc., is expected to cause further enhancements in performance. Existing survey papers on NOMA mainly focus on its concept, comparison, issues and analysis without any categorization of different techniques to solve the issues related to it. This survey paper highlights the main issues and constraints of resource allocation, signaling, practical implementation and security aspects of NOMA and its integration with 5G and upcoming wireless technologies. Various solutions have been proposed in the literature that involve optimization, analytical, game theory, matching theory, graph theory and machine learning (ML) techniques. We present an in-depth analysis and comparison of these solutions with key insights emphasizing the feasibility and applicability for a qualitative analysis. We finally identify promising future research directions and challenges in the context of PD-NOMA’s application to the existing 5G and next generation wireless networks.
               ",autonomous vehicle
10.1016/j.neucom.2021.05.032,journal,Neurocomputing,sciencedirect,2021-10-07,sciencedirect,A coarse-to-fine capsule network for fine-grained image categorization,https://api.elsevier.com/content/article/pii/S0925231221007852,"
                  Fine-grained image categorization is challenging due to the subordinate categories within an entry-level category can only be distinguished by subtle discriminations. This necessitates localizing key (most discriminative) regions and extract domain-specific features alternately. Existing methods predominantly realize fine-grained categorization independently, while ignoring that representation learning and foreground localization can reinforce each other iteratively. Sharing the state-of-the-art performance of capsule encoding for abstract semantic representation, we formalize our pipeline as a coarse-to-fine capsule network (CTF-CapsNet). It consists of customized expert CapsNets arranged in each perception scale and region proposal networks (RPNs) between two adjacent scales. Their mutually motivated self-optimization can achieve increasingly specialized cross-utilization of object-level and component-level descriptions. The RPN zooms the areas to turn the attention to the most distinctive regions by concerning preceding informations learned by expert CapsNet for references, whilst a finer-scale model takes as feed an amplified attended patch from last scale. Overall, CTF-CapsNet is driven by three focal margin losses between label prediction and ground truth, and three regeneration losses between original input images/feature maps and reconstructed images. Experiments demonstrate that without any prior knowledge or strongly-supervised supports (e.g., bounding-box/part annotations), CTF-CapsNet can deliver competitive categorization performance among state-of-the-arts, i.e., testing accuracy achieves 89.57%, 88.63%, 90.51%, and 91.53% on our hand-crafted rice growth image set and three public benchmarks, i.e., CUB Birds, Stanford Dogs, and Stanford Cars, respectively.
               ",autonomous vehicle
10.1016/j.neunet.2009.03.014,journal,Neural Networks,sciencedirect,2009-04-30,sciencedirect,Introduction to the special issue on goal-directed neural systems,https://api.elsevier.com/content/article/pii/S0893608009000513,,autonomous vehicle
10.1016/j.engappai.2019.08.010,journal,Engineering Applications of Artificial Intelligence,sciencedirect,2019-10-31,sciencedirect,Heuristic design of fuzzy inference systems: A review of three decades of research,https://api.elsevier.com/content/article/pii/S0952197619301952,"
                  This paper provides an in-depth review of the optimal design of type-1 and type-2 fuzzy inference systems (FIS) using five well known computational frameworks: genetic-fuzzy systems (GFS), neuro-fuzzy systems (NFS), hierarchical fuzzy systems (HFS), evolving fuzzy systems (EFS), and multi-objective fuzzy systems (MFS), which is in view that some of them are linked to each other. The heuristic design of GFS uses evolutionary algorithms for optimizing both Mamdani-type and Takagi–Sugeno–Kang-type fuzzy systems. Whereas, the NFS combines the FIS with neural network learning systems to improve the approximation ability. An HFS combines two or more low-dimensional fuzzy logic units in a hierarchical design to overcome the curse of dimensionality. An EFS solves the data streaming issues by evolving the system incrementally, and an MFS solves the multi-objective trade-offs like the simultaneous maximization of both interpretability and accuracy. This paper ofers a synthesis of these dimensions and explores their potentials, challenges, and opportunities in FIS research. This review also examines the complex relations among these dimensions and the possibilities of combining one or more computational frameworks adding another dimension: deep fuzzy systems.
               ",autonomous vehicle
10.1016/0921-8890(95)00051-8,journal,Robotics and Autonomous Systems,sciencedirect,1995-12-31,sciencedirect,A view-based neurocomputational system for relational map-making and navigation in visual environments,https://api.elsevier.com/content/article/pii/0921889095000518,"
                  Artificial navigation systems stand to benefit greatly from learning maps of visual environments, but traditional map-making techniques are inadequate in several respects. This paper describes an adaptive, view-based, relational map-making system for navigating within a 3D environment defined by a spatially distributed set of visual landmarks. Inspired by an analogy to learning aspect graphs of 3D objects, the system comprises two neurocomputational architectures that emulate cognitive mapping in the rat hippocampus. The first architecture performs unsupervised place learning by combining the “What” with the “Where”, namely through conjunctions of landmark identity, pose, and egocentric gaze direction within a local, restricted sensory view of the environment. The second associatively learns action consequences by incorporating the “When”, namely through conjunctions of learned places and coarsely coded robot motions. Together, these networks form a map reminiscent of a partially observable Markov decision process, and consequently provide an ideal neural substrate for prediction, environment recognition, route planning, and exploration. Preliminary results from real-time implementations on a mobile robot called MAVIN (the Mobile Adaptive VIsual Navigator) demonstrate the potential for these capabilities.
               ",autonomous vehicle
10.1016/j.robot.2013.11.011,journal,Robotics and Autonomous Systems,sciencedirect,2014-04-30,sciencedirect,Autonomous tactile perception: A combined improved sensing and Bayesian nonparametric approach,https://api.elsevier.com/content/article/pii/S0921889013002285,"
                  In recent years, autonomous robots have increasingly been deployed in unknown environments and required to manipulate or categorize unknown objects. In order to cope with these unfamiliar situations, improvements must be made both in sensing technologies and in the capability to autonomously train perception models. In this paper, we explore this problem in the context of tactile surface identification and categorization. Using a highly-discriminant tactile probe based upon large bandwidth, triple axis accelerometer that is sensitive to surface texture and material properties, we demonstrate that unsupervised learning for surface identification with this tactile probe is feasible. To this end, we derived a Bayesian nonparametric approach based on Pitman–Yor processes to model power-law distributions, an extension of our previous work using Dirichlet processes Dallaire et al. (2011). When tested against a large collection of surfaces and without providing the actual number of surfaces, the tactile probe combined with our proposed approach demonstrated near-perfect recognition in many cases and achieved perfect recognition given the right conditions. We consider that our combined improvements demonstrate the feasibility of effective autonomous tactile perception systems.
               ",autonomous vehicle
10.1053/j.semnuclmed.2018.07.003,journal,Seminars in Nuclear Medicine,sciencedirect,2018-11-30,sciencedirect,Novel Quantitative PET Techniques for Clinical Decision Support in Oncology,https://api.elsevier.com/content/article/pii/S0001299818300539,"
                  Quantitative image analysis has deep roots in the usage of positron emission tomography (PET) in clinical and research settings to address a wide variety of diseases. It has been extensively employed to assess molecular and physiological biomarkers in vivo in healthy and disease states, in oncology, cardiology, neurology, and psychiatry. Quantitative PET allows relating the time-varying activity concentration in tissues/organs of interest and the basic functional parameters governing the biological processes being studied. Yet, quantitative PET is challenged by a number of degrading physical factors related to the physics of PET imaging, the limitations of the instrumentation used, and the physiological status of the patient. Moreover, there is no consensus on the most reliable and robust image-derived PET metric(s) that can be used with confidence in clinical oncology owing to the discrepancies between the conclusions reported in the literature. There is also increasing interest in the use of artificial intelligence based techniques, particularly machine learning and deep learning techniques in a variety of applications to extract quantitative features (radiomics) from PET including image segmentation and outcome prediction in clinical oncology. These novel techniques are revolutionizing clinical practice and are now offering unique capabilities to the clinical molecular imaging community and biomedical researchers at large. In this report, we summarize recent developments and future tendencies in quantitative PET imaging and present example applications in clinical decision support to illustrate its potential in the context of clinical oncology.
               ",autonomous vehicle
10.1016/B978-0-12-821777-1.09992-4,journal,"Machine Learning, Big Data, and IoT for Medical Informatics",sciencedirect,2021-12-31,sciencedirect,Index,https://api.elsevier.com/content/article/pii/B9780128217771099924,Unknown,autonomous vehicle
10.1016/B978-0-12-820203-6.00019-9,journal,Applications of Big Data in Healthcare,sciencedirect,2021-12-31,sciencedirect,Index,https://api.elsevier.com/content/article/pii/B9780128202036000199,Unknown,autonomous vehicle
10.1016/j.procs.2016.07.398,journal,Procedia Computer Science,sciencedirect,2016-12-31,sciencedirect,The HaveNWant Common Cortical Algorithm,https://api.elsevier.com/content/article/pii/S1877050916316544,"HaveNWant is a low-level cognitive schemata that processes bits of information bidirectionally. It is a common cortical algorithm whose Tinker-Toy® like parts can construct networks that react powerfully in embedded environments. They exhibit many of the animal learning abilities described by Piaget. Learning occurs when unknowns are detected, triggering the addition of new elements to the network. In this way, forward models can be built from experience, and many of these models can be linked together to form large distributed associative memories; HaveNWant's level of abstraction lies well above neurological models, focusing on functionality and avoiding biological constraints. It also lies above computer AND and OR gates, which operate unidirectionally. In HaveNWant, for every signal going one way, there is another signal coming back. HaveNWant atoms continually reconcile the information on their links, each imposing a particular constraint. Its networks aggregate many single links, to efficiently enforce large sophisticated relationships. HaveNWant operates below most AI architectures, which have algorithms that do not constrain bit-level computational locality. Although the examples given involve toy networks, we have a plan to extend the base algorithms by adding dynamically learned variables and noise tolerance, to produce robust behavior.",autonomous vehicle
10.1016/S0893-6080(13)00287-6,journal,Neural Networks,sciencedirect,2014-01-31,sciencedirect,Neural Networks,https://api.elsevier.com/content/article/pii/S0893608013002876,,autonomous vehicle
10.1016/j.ejor.2019.07.073,journal,European Journal of Operational Research,sciencedirect,2020-09-01,sciencedirect,Recent advances in selection hyper-heuristics,https://api.elsevier.com/content/article/pii/S0377221719306526,"Hyper-heuristics have emerged as a way to raise the level of generality of search techniques for computational search problems. This is in contrast to many approaches, which represent customised methods for a single problem domain or a narrow class of problem instances. The term hyper-heuristic was defined in the early 2000s as a heuristic to choose heuristics, but the idea of designing high-level heuristic methodologies can be traced back to the early 1960s. The current state-of-the-art in hyper-heuristic research comprises a set of methods that are broadly concerned with intelligently selecting or generating a suitable heuristic for a given situation. Hyper-heuristics can be considered as search methods that operate on lower-level heuristics or heuristic components, and can be categorised into two main classes: heuristic selection and heuristic generation. Here we will focus on the first of these two categories, selection hyper-heuristics. This paper gives a brief history of this emerging area, reviews contemporary selection hyper-heuristic literature, and discusses recent selection hyper-heuristic frameworks. In addition, the existing classification of selection hyper-heuristics is extended, in order to reflect the nature of the challenges faced in contemporary research. Unlike the survey on hyper-heuristics published in 2013, this paper focuses only on selection hyper-heuristics and presents critical discussion, current research trends and directions for future research.",autonomous vehicle
10.1016/j.ins.2014.06.028,journal,Information Sciences,sciencedirect,2015-02-10,sciencedirect,"Spiking neural network methodology for modelling, classification and understanding of EEG spatio-temporal data measuring cognitive processes",https://api.elsevier.com/content/article/pii/S0020025514006562,"
                  The paper offers a new methodology for modelling, recognition and understanding of electroencephalography (EEG) spatio-temporal data measuring complex cognitive brain processes during mental tasks. The key element is that mental tasks are performed through complex spatio-temporal brain processes and they can be better understood only if we model properly the spatio-/spectro temporal data that measures these processes. The proposed methodology is based on a recently proposed novel spiking neural network architecture, called NeuCube as a general framework for spatio-temporal brain data modelling. The methodology is demonstrated on benchmark cognitive EEG data. The new approach leads to a faster data processing, improved accuracy of the EEG data classification and improved understanding of this data and the cognitive processes that generated it. The paper concluded that the new methodology is worth exploring further on other spatio-temporal data, measuring complex cognitive brain processes, aiming at using this method for the development of the next generation of brain–computer interfaces and systems for early diagnosis of degenerative brain disease, such as Alzheimer’s Disease (AD), and for personalised neuro-rehabilitation systems.
               ",autonomous vehicle
10.1016/j.ins.2014.06.028,journal,Information Sciences,sciencedirect,2015-02-10,sciencedirect,"Spiking neural network methodology for modelling, classification and understanding of EEG spatio-temporal data measuring cognitive processes",https://api.elsevier.com/content/article/pii/S0020025514006562,"
                  The paper offers a new methodology for modelling, recognition and understanding of electroencephalography (EEG) spatio-temporal data measuring complex cognitive brain processes during mental tasks. The key element is that mental tasks are performed through complex spatio-temporal brain processes and they can be better understood only if we model properly the spatio-/spectro temporal data that measures these processes. The proposed methodology is based on a recently proposed novel spiking neural network architecture, called NeuCube as a general framework for spatio-temporal brain data modelling. The methodology is demonstrated on benchmark cognitive EEG data. The new approach leads to a faster data processing, improved accuracy of the EEG data classification and improved understanding of this data and the cognitive processes that generated it. The paper concluded that the new methodology is worth exploring further on other spatio-temporal data, measuring complex cognitive brain processes, aiming at using this method for the development of the next generation of brain–computer interfaces and systems for early diagnosis of degenerative brain disease, such as Alzheimer’s Disease (AD), and for personalised neuro-rehabilitation systems.
               ",autonomous vehicle
10.1016/B978-0-12-821633-0.09988-8,journal,"Demystifying Big Data, Machine Learning, and Deep Learning for Healthcare Analytics",sciencedirect,2021-12-31,sciencedirect,Index,https://api.elsevier.com/content/article/pii/B9780128216330099888,Unknown,autonomous vehicle
10.1016/B978-0-12-818576-6.00025-3,journal,Artificial Intelligence to Solve Pervasive Internet of Things Issues,sciencedirect,2021-12-31,sciencedirect,Index,https://api.elsevier.com/content/article/pii/B9780128185766000253,Unknown,autonomous vehicle
10.1016/j.ins.2019.09.082,journal,Information Sciences,sciencedirect,2020-02-29,sciencedirect,Parsimonious generalization of fuzzy thematic sets in taxonomies applied to the analysis of tendencies of research in data science,https://api.elsevier.com/content/article/pii/S0020025519309454,"
                  This paper proposes a novel method, referred to as ParGenFS, for finding a most specific generalization of a query set represented by a fuzzy set of topics assigned to leaves of the rooted tree of a taxonomy. The query set is generalized by “lifting” it to one or more “head subjects” in the higher ranks of the taxonomy. The head subjects should cover the query set, with the possible addition of some “gaps”, taxonomy nodes covered by the head subject but irrelevant to the query set. To decrease the numbers of gaps, we admit some “offshoots”, nodes belonging to the query set but not covered by a head subject. The method globally minimizes the total number of head subjects, gaps and offshoots, each suitably weighted. Our algorithm is applied to the structural analysis and description of a collection of 17,685 abstracts of research papers published in 17 Springer journals related to Data Science for the 20-year period 1998–2017. Our taxonomy of Data Science (TDS) is extracted from the Association for Computing Machinery Computing Classification System 2012 (ACM-CCS), a six-level hierarchical taxonomy manually developed by a team of ACM experts. The TDS also includes a number of additional leaves that we added to cater for recent developments not represented in the ACM-CCS taxonomy. We find fuzzy clusters of leaf topics over the text collection, using specially developed machinery. Three of the clusters are indeed thematic, relating to the Data Science sub-areas of (a) learning, (b) information retrieval, and (c) clustering. These three clusters are then lifted in the TDS using ParGenFS, which allows us to draw some conclusions about tendencies in developments in these areas.
               ",autonomous vehicle
10.1016/S1474-6670(17)57657-0,journal,IFAC Proceedings Volumes,sciencedirect,1996-07-31,sciencedirect,Connectionist Based Robot Control: An Overview,https://api.elsevier.com/content/article/pii/S1474667017576570,"
                  This paper focusses on the intersection of the area of robot control and connectionism approach, and represents an attempt to give a report of the basic principles and concepts of connectionism in robotics, with an outline of a number of recent algorithms used in learning control of manipulation robot. A major concern in this paper is the application of neural networks for learning of kinematic and dynamic relations used in robot motion control at the executive hierarchical level, as well as for the problems in sensor-based robot control
               ",autonomous vehicle
10.1016/B978-0-12-822249-2.00002-5,journal,Computational and Data-Driven Chemistry Using Artificial Intelligence,sciencedirect,2022-12-31,sciencedirect,Chapter 4: Approaches using AI in medicinal chemistry,https://api.elsevier.com/content/article/pii/B9780128222492000025,"
               The challenge of pharmacological effect prediction and its relation to analog design consists of the decision of which molecule to make next on the basis of the available data, medicinal chemistry knowledge, experience, and intuition. In the second half of the 1900s century, attempts were made to relate narcotics pharmacology to their physicochemical properties by specifically using distribution and partition coefficients. This was shortly followed by Paul Ehrlich's observation to attribute the pharmacological effect of a compound to a specific functional group. However, it was only in 1971 that this observation was called pharmacophore. About 30years after Ehrlich, Hammett related the effect of changes in structure on reaction mechanisms, specifically based on resonance interaction of an aromatic ring. This model was later extended by Taft by separating the inductive effects from the steric properties of substituents. This concept was formalized with the work of Hansch, Free, and Wilson which reasoned that the biological activity for a set of analogs could be described by the contributions that substituents or structural elements make to the activity of a parent structure. This led to the analytical description of general quantitative-structure–activity relationship studies (QSAR). If the question “What to synthesize next?” is answered then “How to synthesize it?” follows up. The prediction of chemical reactions starting from educts or the product and educing input reactants and reaction conditions is a fundamental scientific problem. As QSAR computer-aided synthesis planning (CASP) has a long history starting in the 1960s with LHASA a rule-based approach to retrosynthesis planning.
            ",autonomous vehicle
10.1016/S0893-6080(12)00303-6,journal,Neural Networks,sciencedirect,2013-01-31,sciencedirect,Neural networks,https://api.elsevier.com/content/article/pii/S0893608012003036,,autonomous vehicle
10.1016/j.patcog.2011.05.005,journal,Pattern Recognition,sciencedirect,2012-01-31,sciencedirect,A method for noise-robust context-aware pattern discovery and recognition from categorical sequences,https://api.elsevier.com/content/article/pii/S0031320311002044,"
                  An efficient method for weakly supervised pattern discovery and recognition from discrete categorical sequences is introduced. The method utilizes two parallel sources of data: categorical sequences carrying some temporal or spatial information and a set of labeled, but not exactly aligned, contextual events related to the sequences. From these inputs the method builds associative models able to describe systematically co-occurring structures in the input streams. The learned models, based on transitional probabilities of events observed at several different time lags, inherently segment and classify novel sequences into contextual categories. Learning and recognition processes are purely incremental and computationally cheap, making the approach suitable for on-line learning tasks. The capabilities of the algorithm are demonstrated in a keyword learning task from continuous infant-directed speech and a continuous speech recognition task operating at varying noise levels.
               ",autonomous vehicle
10.1016/j.iot.2020.100262,journal,Internet of Things,sciencedirect,2020-09-30,sciencedirect,"IoT-based enterprise resource planning: Challenges, open issues, applications, architecture, and future research directions",https://api.elsevier.com/content/article/pii/S2542660520300962,"
                  In today's highly competitive markets, organizations can create a competitive advantage through the successful implementation of Enterprise Resource Planning (ERP) systems. ERP works with different technologies, including the Internet of Things (IoT). IoT uses a unique Internet protocol to identify, control, and transfer data to individuals as well as databases. The data is collected through IoT, stored on the cloud, and extracted and managed in through ERP. In this study, we review the challenges, open issues, applications, and architecture of the IoT-based ERP. For this purpose, we review and analyze the latest IoT-related articles to present the unique features of the IoT and discuss its impact on ERP. The results show sensors and devices connected to the Internet can manage the stored data processed in the cloud through ERP without human intervention. We also discuss the challenges and opportunities in the relationship between ERP and the IoT risen by the introduction of the cloud.
               ",autonomous vehicle
10.1016/j.cogsys.2018.04.014,journal,Cognitive Systems Research,sciencedirect,2018-10-31,sciencedirect,A working memory model improves cognitive control in agents and robots,https://api.elsevier.com/content/article/pii/S1389041717300943,"
                  Cognition entails those mental processes enabling understanding the current situation through senses, experience, and thought, and supporting the acquisition of new knowledge. A fundamental contribution in cognition is offered by the working memory, that is a small, short-term memory containing and protecting from interference goal-relevant pieces of information. Grounding our work on biological and neuroscientific studies, we modeled and implemented working memory processes in a software model, IDRA-WM, that can simultaneously act as short-term memory and actions generator, thanks to the use of a reinforcement-driven mechanism for chunk selection. Moreover our system integrates the functions of the working memory with a basic action planner. We tested the model with robot relevant tasks to assess whether the proposed solution can learn to solve a problem on the basis of a delayed reward. The experimental results indicate that IDRA-WM is able to solve even those tasks that do not provide immediate reward after an action.
               ",autonomous vehicle
10.1016/B978-0-12-823014-5.20001-X,journal,Handbook of Deep Learning in Biomedical Engineering,sciencedirect,2021-12-31,sciencedirect,Index,https://api.elsevier.com/content/article/pii/B978012823014520001X,Unknown,autonomous vehicle
10.1016/j.isatra.2021.01.036,journal,ISA Transactions,sciencedirect,2021-10-31,sciencedirect,"A survey on attack detection, estimation and control of industrial cyber–physical systems",https://api.elsevier.com/content/article/pii/S001905782100046X,"
                  Cyber–physical systems (CPSs) are complex systems that involve technologies such as control, communication, and computing. Nowadays, CPSs have a wide range of applications in smart cities, smart grids, smart manufacturing and intelligent transportation. However, with integration of industrial control systems with modern communication technologies, CPSs would be inevitably exposed to increasing security threats, which could lead to severe degradation of the system performance and even destruction of CPSs. This paper presents a survey on recent advances on security issues of industrial cyber–physical systems (ICPSs). We specifically discuss two typical kinds of attacks, i.e., Denial-of-Service (DoS) attack and Deception attack, and present recent results in terms of attack detection, estimation, and control of ICPSs. Classifications of current studies are analyzed and summarized based on different system modeling and analysis methods. In addition, advantages and disadvantage of various methodologies are also discussed. Finally, the paper concludes with some potential future research directions on secure ICPSs.
               ",autonomous vehicle
10.1016/0921-8890(95)00013-6,journal,Robotics and Autonomous Systems,sciencedirect,1995-07-31,sciencedirect,Robot adaptivity,https://api.elsevier.com/content/article/pii/0921889095000136,"
                  Adaptivity is a crucial capability for survival in the biological world. Analogously, in order for robots to carry out their tasks autonomously in unstructured environments, it is essential that they be well-adapted to their surroundings. The most common way to endow robots with this capability is through the use of neural learning techniques. The main such techniques are reviewed in this paper, together with their use in both mobile robots and manipulator arms. Not only the advantages, but also the limitations of neural adaptivity are pointed out.
               ",autonomous vehicle
10.1016/j.jss.2020.110547,journal,Journal of Systems and Software,sciencedirect,2020-06-30,sciencedirect,Modeling programs hierarchically with stack-augmented LSTM,https://api.elsevier.com/content/article/pii/S0164121220300297,"
                  Programming language modeling has attracted extensive attention in recent years, and it plays an essential role in program processing fields. Statistical language models, which are initially designed for natural languages, have been generally used for modeling programming languages. However, different from natural languages, programming languages contain explicit and hierarchical structure that is hard to learn by traditional statistical language models. To address this challenge, we propose a novel Stack-Augmented LSTM neural network for programming language modeling. Adding a stack memory component into the LSTM network enables our model to capture the hierarchical information of programs through the PUSH and POP operations, which further allows our model capturing the long-term dependency in the programs. We evaluate the proposed model on three program analysis tasks, i.e., code completion, program classification, and code summarization. Evaluation results show that our proposed model outperforms baseline models in all the three tasks, indicating that by capturing the structural information of programs with a stack, our proposed model can represent programs more precisely.
               ",autonomous vehicle
10.1016/j.compchemeng.2020.107176,journal,Computers & Chemical Engineering,sciencedirect,2021-02-28,sciencedirect,Editorial,https://api.elsevier.com/content/article/pii/S0098135420312199,,autonomous vehicle
10.1016/j.ijhm.2019.01.003,journal,International Journal of Hospitality Management,sciencedirect,2019-07-31,sciencedirect,Market segmentation and travel choice prediction in Spa hotels through TripAdvisor’s online reviews,https://api.elsevier.com/content/article/pii/S0278431918302457,"
                  Customer segmentation via online reviews and ratings can assist different hotels, including spa hotels, to better inform marketing strategy development and ensure optimal marketing expenditures. However, traditional market segmentation approaches are ineffective in analysing social data on account of size, different dimensions and features of online review data. Machine learning approaches can assist in developing effective hybrid algorithms to overcome data-related complications associated with online reviews. Hence, the objective of this study is to develop a method for spa hotel segmentation and travel choice prediction by applying machine learning approaches. Method evaluation is conducted through a collection of datasets from travelers’ ratings and textual reviews of spa hotels on several features in TripAdvisor. Findings confirm that the proposed hybrid machine learning methods can be implemented as an incremental recommendation agent for spa hotel/resort segmentation through effectively utilizing ‘big data’ procured from online social media contexts.
               ",autonomous vehicle
10.1016/j.bdr.2020.100145,journal,Big Data Research,sciencedirect,2020-09-30,sciencedirect,Data-Driven Computational Social Science: A Survey,https://api.elsevier.com/content/article/pii/S2214579620300137,"
                  Social science concerns issues on individuals, relationships, and the whole society. The complexity of research topics in social science makes it the amalgamation of multiple disciplines, such as economics, political science, and sociology, etc. For centuries, scientists have conducted many studies to understand the mechanisms of the society. However, due to the limitations of traditional research methods, there exist many critical social issues to be explored. To solve those issues, computational social science emerges due to the rapid advancements of computation technologies and the profound studies on social science. With the aids of the advanced research techniques, various kinds of data from diverse areas can be acquired nowadays, and they can help us look into social problems with a new eye. As a result, utilizing various data to reveal issues derived from computational social science area has attracted more and more attentions. In this paper, to the best of our knowledge, we present a survey on data-driven computational social science for the first time which primarily focuses on reviewing application domains involving human dynamics. The state-of-the-art research on human dynamics is reviewed from three aspects: individuals, relationships, and collectives. Specifically, the research methodologies used to address research challenges in aforementioned application domains are summarized. In addition, some important open challenges with respect to both emerging research topics and research methods are discussed.
               ",autonomous vehicle
10.1016/j.tifs.2016.01.011,journal,Trends in Food Science & Technology,sciencedirect,2016-04-30,sciencedirect,"Data mining derived from food analyses using non-invasive/non-destructive analytical techniques; determination of food authenticity, quality & safety in tandem with computer science disciplines",https://api.elsevier.com/content/article/pii/S0924224415301424,"
                  Background
                  Food quality, safety and authenticity are important issues for consumers, governments, as well as the food industry. In the last decade, several researchers have attempted to go beyond traditional microbiological, DNA-based and other methods using rapid techniques. This broad term involves a variety of sensors such as hyperspectral and multispectral imaging, vibrational spectroscopy, as well as biomimetic receptors.
               
                  Scope and approach
                  The resulting data acquired from the above-mentioned sensors require the application of various case-specific data analysis methods for the purpose of simple understanding and visualization of the acquired high-dimensional dataset, but also for classification and prediction purposes.
               
                  Key findings and conclusions
                  It is evident that rapid techniques coupled with data analysis methods have given promising results in several food products with various sensors. Additionally there are several applications, new sensors and new algorithms that remain to be explored and validated in the future.
               ",autonomous vehicle
10.1016/j.jiec.2013.05.012,journal,Journal of Industrial and Engineering Chemistry,sciencedirect,2014-03-25,sciencedirect,Modeling and optimization of cross-flow ultrafiltration using hybrid neural network-genetic algorithm approach,https://api.elsevier.com/content/article/pii/S1226086X13002177,"
                  Precise modeling flux decline under various operating parameters in cross-flow ultrafiltration (UF) of oily wastewaters and afterward, employing an appropriate optimization algorithm in order to optimize operating parameters involved in the process model result in attaining desired permeate flux, is of fundamental great interest from an economical and technical point of view. Accordingly, this current research proposed a hybrid process modeling and optimization based on computational intelligence paradigms where the combination of artificial neural network (ANN) and genetic algorithm (GA) meets the challenge of specified-objective based on two steps: first the development of bio-inspired approach based on ANN, trained, validated and tested successfully with experimental data collected during the polyacrylonitrile (PAN) UF process to treat the oily wastewater of Tehran refinery in a laboratory scale in which the model received feed temperature (T), feed pH, trans-membrane pressure (TMP), cross-flow velocity (CFV), and filtration time as inputs; and gave permeate flux as an output. Subsequently, the 5-dimensional input space of the ANN model portraying process input variables was optimized by applying GA, with a view to realizing maximum or minimum process output variable. The results obtained validate the estimates of the ANN–GA technique with a good accuracy. Finally, the relative importance of the controllable operation factors on flux decline is determined by applying the various correlation statistic techniques. According to the result of the sensitivity analysis based on the correlation coefficient, the filtration time was the most significant one, followed by T, CFV, feed pH and TMP.
               ",autonomous vehicle
10.1016/j.cose.2020.102087,journal,Computers & Security,sciencedirect,2021-01-31,sciencedirect,"Android security assessment: A review, taxonomy and research gap study",https://api.elsevier.com/content/article/pii/S0167404820303606,"
                  Security threats are escalating exponentially posing a serious challenge to mobile platforms, specifically Android. In recent years the number of attacks has not only increased but each attack has become more damaging to the platform. Therefore, it is important to develop more stringent counter-measures to defend the mobile systems. Although in the last few years significant research progress is seen in the field of the detection and mitigation of Android security, yet numerous challenges and gaps still exist. This paper presents a comprehensive and sound taxonomy to review the state-of-the-art approaches used in Android security. We have highlighted the trends and patterns of different analysis approaches, identified the key aspects in terms of objectives, analysis techniques, code representations, tools and frameworks used, etc. and enumerated the research areas for future work. To carry out this study, the proper systematic literature review process is followed and the results of nearly 200 research publications have been comprehended based on different security aspects.
               ",autonomous vehicle
10.1016/S0885-2014(02)00119-3,journal,Cognitive Development,sciencedirect,2002-12-31,sciencedirect,Artificial Life and Piaget,https://api.elsevier.com/content/article/pii/S0885201402001193,"
                  Artificial Life is the study of all phenomena of the living world through their reproduction in artificial systems. We argue that Artificial Life models of evolution and development offer a new set of theoretical and methodological tools for investigating Piaget’s ideas. The concept of an Artificial Life Neural Network (ALNN) is first introduced, and contrasted with the study of other recent approaches to modeling development. We then illustrate how several key elements of Piaget’s theory of cognitive development (e.g., sensorimotor schemata, perception-action integration) can be investigated within the Artificial Life framework. We conclude by discussing possible new directions of Artificial Life research that will help to elaborate and extend Piaget’s developmental framework.
               ",autonomous vehicle
10.1016/j.cose.2021.102213,journal,Computers & Security,sciencedirect,2021-05-31,sciencedirect,ADS-B anomaly data detection model based on VAE-SVDD,https://api.elsevier.com/content/article/pii/S0167404821000377,"
                  As a key technology of the new generation air traffic surveillance system, ADS-B (Automatic Dependent Surveillance-Broadcast) is vulnerable to cyber security challenges because it lacks data integrity and authentication mechanism. For detecting ADS-B data attacks accurately, an anomaly detection model is proposed which fully considers temporal correlations and distribution characteristics of ADS-B data. First, VAE (Variational AutoEncoder) is used to reconstruct ADS-B data so that the reconstructed values can be obtained. Then, for the sake of solving the adaptive problem of anomaly detection threshold, the difference values between the reconstructed values and the actual values are put into SVDD (Support Vector Data Description) for training, and a hypersphere classifier that can detect ADS-B anomaly data is obtained. In addition, in order to prevent overfitting and underfitting, appropriate reconstructed values are selected which can reduce FPR (False Positive Rate) and FNR (False Negative Rate) of anomaly detection. Experiments show that the VAE-SVDD model can detect ADS-B anomaly data which is generated by attacks such as random position deviation and constant position deviation. Moreover, compared with other machine learning methods, this model is not only more adaptable, but also has a lower FPR and FNR.
               ",autonomous vehicle
10.1016/0925-2312(94)90033-7,journal,Neurocomputing,sciencedirect,1994-02-28,sciencedirect,Object-oriented backpropagation and its application to structural design,https://api.elsevier.com/content/article/pii/0925231294900337,"
                  A multilayer neural network development environment, called ANNDE, is presented for implementing effective learning algorithms for the domain of engineering design using the object-oriented programming paradigm. It consists of five primary components: learning domain, neural nets, library of learning strategies, learning process, and analysis process. These components have been implemented as five classes in two object-oriented programming languages C++ and G++. The library of learning strategies includes generalized delta rule with error backpropagation. Several examples are presented for learning in the domain of structural engineering.
               ",autonomous vehicle
10.1016/j.knosys.2020.106557,journal,Knowledge-Based Systems,sciencedirect,2021-01-09,sciencedirect,Business process variant analysis: Survey and classification,https://api.elsevier.com/content/article/pii/S0950705120306869,"It is common for business processes to exhibit a high degree of internal heterogeneity, in the sense that the executions of the process differ widely from each other due to contextual factors, human factors, or deliberate business decisions. For example, a quote-to-cash process in a multinational company is typically executed differently across different countries or even across different regions in the same country. Similarly, an insurance claims handling process might be executed differently across different claims handling centers or across multiple teams within the same claims handling center. A subset of executions of a business process that can be distinguished from others based on a given predicate (e.g. the executions of a process in a given country) is called a process variant. Understanding differences between process variants helps analysts and managers to make informed decisions as to how to standardize or otherwise improve a business process, for example by helping them find out what makes it that a given variant exhibits a higher performance than another one. Process variant analysis is a family of techniques to analyze event logs produced during the execution of a process, in order to identify and explain the differences between two or more process variants. A wide range of methods for process variant analysis have been proposed in the past decade. However, due to the interdisciplinary nature of this field, the proposed methods and the types of differences they can identify vary widely, and there is a lack of a unifying view of the field. To close this gap, this article presents a systematic literature review of methods for process variant analysis. The identified studies are classified according to their inputs, outputs, analysis purpose, underpinning algorithms, and extra-functional characteristics. The paper closes with a broad classification of approaches into three categories based on the paradigm they employ to compare multiple process variants.",autonomous vehicle
10.1016/j.ijinfomgt.2018.08.006,journal,International Journal of Information Management,sciencedirect,2019-04-30,sciencedirect,Real-time big data processing for anomaly detection: A Survey,https://api.elsevier.com/content/article/pii/S0268401218301658,"
                  The advent of connected devices and omnipresence of Internet have paved way for intruders to attack networks, which leads to cyber-attack, financial loss, information theft in healthcare, and cyber war. Hence, network security analytics has become an important area of concern and has gained intensive attention among researchers, off late, specifically in the domain of anomaly detection in network, which is considered crucial for network security. However, preliminary investigations have revealed that the existing approaches to detect anomalies in network are not effective enough, particularly to detect them in real time. The reason for the inefficacy of current approaches is mainly due the amassment of massive volumes of data though the connected devices. Therefore, it is crucial to propose a framework that effectively handles real time big data processing and detect anomalies in networks. In this regard, this paper attempts to address the issue of detecting anomalies in real time. Respectively, this paper has surveyed the state-of-the-art real-time big data processing technologies related to anomaly detection and the vital characteristics of associated machine learning algorithms. This paper begins with the explanation of essential contexts and taxonomy of real-time big data processing, anomalous detection, and machine learning algorithms, followed by the review of big data processing technologies. Finally, the identified research challenges of real-time big data processing in anomaly detection are discussed.
               ",autonomous vehicle
10.1016/B978-0-12-823337-5.00016-0,journal,Intelligence-Based Medicine,sciencedirect,2020-12-31,sciencedirect,Contents,https://api.elsevier.com/content/article/pii/B9780128233375000160,Unknown,autonomous vehicle
10.1016/j.jnca.2019.102498,journal,Journal of Network and Computer Applications,sciencedirect,2020-01-15,sciencedirect,A robust multimedia traffic SDN-Based management system using patterns and models of QoE estimation with BRNN,https://api.elsevier.com/content/article/pii/S1084804519303583,"
                  Nowadays, network infrastructures such as Software Defined Networks (SDN) achieve a huge computational power. This allows to add a high processing on the network nodes. In this paper, a multimedia traffic management system is presented. This system is based on estimation models of Quality of Experience (QoE) and also on the traffic patterns classification. In order to achieve this, a QoE estimation method has been modeled. This method allows for classifying the multimedia traffic from multimedia transmission patterns. In order to do this, the SDN controller gathers statistics from the network. The patterns used have been defined from a lineal combination of objective QoE measurements. The model has been defined by Bayesian regularized neural networks (BRNN). From this model, the system is able to classify several kind of traffic according to the quality perceived by the users. Then, a model has been developed to determine which video characteristics need to be changed to provide the user with the best possible quality in the critical moments of the transmission. The choice of these characteristics is based on the quality of service (QoS) parameters, such as delay, jitter, loss rate and bandwidth. Moreover, it is also based on subpatterns defined by clusters from the dataset and which represents network and video characteristics. When a critical network situation is given, the model selects, by using network parameters as entries, the subpattern with the most similar network condition. The minimum Euclidean distance between these entries and the network parameters of the subpatters is calculated to perform this selection. Both models work together to build a reliable multimedia traffic management system perfectly integrated into current network infrastructures, which is able to classify the traffic and solve critical situations changing the video characteristics, by using the SDN architecture.
               ",autonomous vehicle
10.1016/j.knosys.2020.106557,journal,Knowledge-Based Systems,sciencedirect,2021-01-09,sciencedirect,Business process variant analysis: Survey and classification,https://api.elsevier.com/content/article/pii/S0950705120306869,"It is common for business processes to exhibit a high degree of internal heterogeneity, in the sense that the executions of the process differ widely from each other due to contextual factors, human factors, or deliberate business decisions. For example, a quote-to-cash process in a multinational company is typically executed differently across different countries or even across different regions in the same country. Similarly, an insurance claims handling process might be executed differently across different claims handling centers or across multiple teams within the same claims handling center. A subset of executions of a business process that can be distinguished from others based on a given predicate (e.g. the executions of a process in a given country) is called a process variant. Understanding differences between process variants helps analysts and managers to make informed decisions as to how to standardize or otherwise improve a business process, for example by helping them find out what makes it that a given variant exhibits a higher performance than another one. Process variant analysis is a family of techniques to analyze event logs produced during the execution of a process, in order to identify and explain the differences between two or more process variants. A wide range of methods for process variant analysis have been proposed in the past decade. However, due to the interdisciplinary nature of this field, the proposed methods and the types of differences they can identify vary widely, and there is a lack of a unifying view of the field. To close this gap, this article presents a systematic literature review of methods for process variant analysis. The identified studies are classified according to their inputs, outputs, analysis purpose, underpinning algorithms, and extra-functional characteristics. The paper closes with a broad classification of approaches into three categories based on the paradigm they employ to compare multiple process variants.",autonomous vehicle
10.1016/j.procs.2021.04.167,journal,Procedia Computer Science,sciencedirect,2021-12-31,sciencedirect,Tokamak plasma models development for plasma magnetic control systems design by first principle equations and identification approach,https://api.elsevier.com/content/article/pii/S1877050921010036,"To design plasma magnetic control systems in modern tokamaks one needs models of the plasma. These models are linear parameter varying (LPV) because of relatively small variations of outputs of the feedback loops around scenarios and may be obtained by first-principle equations, identification approach or by their combinations. The challenge of obtaining models of the plasma in the tokamak becomes more complicated by the existence of plasma position and poloidal field control loops with diagnostics and actuators which are needed as inner cascades for plasma current and shape control in particular on the Globus-M2 tokamak (Ioffe Inst., St. Petersburg, RF). The plasma equilibrium is reconstructed on the base of magnetic measurements outside the plasma by Picard iterations, moving filaments or neural networks, and linear plasma models are developed around the equilibrium with the help of the Kirchhoff’s law and force balance. In order to ensure the operability of plasma current and shape feedback control systems, the identification approach (controlled plant model design on the base of experimental input-output signals) is planned to be used. The basic methods of the identification are supposed to be applied as follows: subspaces, wavelets, linear matrix inequalities (LMIs), adaptive state observers, and dynamical neural networks which are able to automatically adjust their states to an unknown plant (elements of artificial intelligence). The solutions of the identification problem will be compared with the models obtained by the first principles with the aim to get the sufficient accuracy of coincidence. The approaches to be developed of getting tokamak plasma models may be applied to any vertically elongated (D-shaped) operating tokamak such as Globus-M2 (RF), D-IIID, NSTX (US), JET, ST40 (GB), ASDEX Upgrade (Germany), TCV (Switzerland), EAST (China), Damavand (Iran) etc.",autonomous vehicle
10.1016/j.bica.2016.10.004,journal,Biologically Inspired Cognitive Architectures,sciencedirect,2016-10-31,sciencedirect,Desiderata for developmental cognitive architectures,https://api.elsevier.com/content/article/pii/S2212683X16300822,"
                  This paper complements Ron Sun’s influential Desiderata for Cognitive Architectures by focussing on the desirable attributes of a biologically-inspired cognitive architecture for an agent with a capacity for autonomous development. Ten desiderata are identified, dealing with value systems & motives, embodiment, sensorimotor contingencies, perception, attention, prospective action, memory, learning, internal simulation, and constitutive autonomy. These desiderata are motivated by studies in developmental psychology, cognitive neuroscience, and enactive cognitive science. All ten focus on the ultimate aspects of cognitive development — why a feature is necessary and what it enables — rather on than the proximate mechanisms by which it can be realized. As such, the desiderata are for the most part neutral regarding the paradigm of cognitive science — cognitivist or emergent — that is adopted when designing a cognitive architecture. Where some element of a desideratum is specific to a particular paradigm, this is noted.
               ",autonomous vehicle
10.1016/S0169-7161(96)14020-7,journal,Handbook of Statistics,sciencedirect,1996-12-31,sciencedirect,18 Financial applications of Artificial Neural Networks,https://api.elsevier.com/content/article/pii/S0169716196140207,"
                  Data-driven modeling approaches, such as Artificial Neural Networks (ANN), are becoming more and more popular in financial applications. ANNs are nonlinear nonparametric models. ANNs allow one to fully utilize the data and let the data determine the structure and parameters of a model without any restrictive parametric modeling assumptions. They are appealing in financial area because of the abundance of high quality financial data and the paucity of testable financial models. As the speed of computers increases and the cost of computing declines exponentially, this computer intensive method becomes attractive. This chapter introduces ANN and point out its relation to some familiar statistical models. Some practical ANN modeling methods are reviewed. The chapter also reviews empirical studies in several major fields of financial applications, including option pricing, forecasting o f foreign exchange rates, bankruptcy prediction, and stock market prediction.
               ",autonomous vehicle
10.1016/j.yasu.2020.03.002,journal,Advances in Surgery,sciencedirect,2020-09-30,sciencedirect,Video Assessment of Surgeons and Surgery,https://api.elsevier.com/content/article/pii/S0065341120300026,,autonomous vehicle
10.1016/j.imavis.2008.12.002,journal,Image and Vision Computing,sciencedirect,2009-10-02,sciencedirect,A linear-complexity reparameterisation strategy for the hierarchical bootstrapping of capabilities within perception–action architectures,https://api.elsevier.com/content/article/pii/S0262885608002552,"
                  Perception–action (PA) architectures are capable of solving a number of problems associated with artificial cognition, in particular, difficulties concerned with framing and symbol grounding. Existing PA algorithms tend to be ‘horizontal’ in the sense that learners maintain their prior percept–motor competences unchanged throughout learning. We here present a methodology for simultaneous ‘horizontal’ and ‘vertical’ perception–action learning in which there additionally exists the capability for incremental accumulation of novel percept–motor competences in a hierarchical fashion.
                  The proposed learning mechanism commences with a set of primitive ‘innate’ capabilities and progressively modifies itself via recursive generalising of parametric spaces within the linked perceptual and motor domains so as to represent environmental affordances in maximally-compact manner. Efficient reparameterising of the percept domain is here accomplished by the exploratory elimination of dimensional redundancy and environmental context.
                  Experimental results demonstrate that this approach exhibits an approximately linear increase in computational requirements when learning in a typical unconstrained environment, as compared with at least polynomially-increasing requirements for a classical perception–action system.
               ",autonomous vehicle
10.1016/j.newideapsych.2013.02.001,journal,New Ideas in Psychology,sciencedirect,2013-12-31,sciencedirect,Neural predictive mechanisms and their role in cognitive incrementalism,https://api.elsevier.com/content/article/pii/S0732118X13000287,"
                  Many neuroscientists view prediction as one of the core brain functions, especially on account of its support of fast movements in complex environments. This leads to the natural question whether predictive knowledge forms the cornerstone of our common-sense understanding of the world. However, there is little consensus as to the exact nature of predictive information and processes, or of the neural mechanisms that realize them. This paper compares procedural versus declarative notions of prediction, examines how the brain appears to carry out predictive functions, and discusses to what degree, and at what level, these neural mechanisms support cognitive incrementalism: the notion that high-level cognition stems from sensorimotor behavior.
               ",autonomous vehicle
10.1016/j.ijhm.2020.102629,journal,International Journal of Hospitality Management,sciencedirect,2020-09-30,sciencedirect,Linking AI quality performance and customer engagement: The moderating effect of AI preference,https://api.elsevier.com/content/article/pii/S027843192030181X,"
                  Drawing upon affordance theory, this study positions artificial intelligence (AI) as a commercial service in examining its influence on customer engagement in the hotel context. In particular, we seek to understand linkages between customer perceptions of AI service quality, AI customer satisfaction and engagement. Given the multiplicity of services offered by service organisations, customers’ preference for AI service is modelled as a moderator of customer perceptions and attitudes towards AI. Data was collected from a sample of hotel customers in Australia who had previously used AI tools or services. Our results reveal a significant chain effect between AI service indicators, service quality perceptions, AI satisfaction and customer engagement. AI preference has a significant moderation effect on information quality and satisfaction. These findings provide new insights into the consumer services literature and have important implications for marketing practitioners.
               ",autonomous vehicle
10.1016/j.robot.2021.103837,journal,Robotics and Autonomous Systems,sciencedirect,2021-11-30,sciencedirect,A survey on human-aware robot navigation,https://api.elsevier.com/content/article/pii/S0921889021001226,"
                  Intelligent systems are increasingly part of our everyday lives and have been integrated seamlessly to the point where it is difficult to imagine a world without them. Physical manifestations of those systems on the other hand, in the form of embodied agents or robots, have so far been used only for specific applications and are often limited to functional roles (e.g. in the industry, entertainment and military fields). Given the current growth and innovation in the research communities concerned with the topics of robot navigation, human–robot-interaction and human activity recognition, it seems like this might soon change. Robots are increasingly easy to obtain and use and the acceptance of them in general is growing. However, the design of a socially compliant robot that can function as a companion needs to take various areas of research into account. This paper is concerned with the navigation aspect of a socially-compliant robot and provides a survey of existing solutions for the relevant areas of research as well as an outlook on possible future directions.
               ",autonomous vehicle
10.1016/j.procs.2015.08.136,journal,Procedia Computer Science,sciencedirect,2015-12-31,sciencedirect,A Study on Deliberate Presumptions of Customer Payments with Reminder in the Absence of Face-to-face Contact Transactions,https://api.elsevier.com/content/article/pii/S1877050915022632,"This paper presents investigating the customer characteristics of reminder effects in the mail order industry, especially the bad debt customers. These kinds of investigations have not made intensively, performed only such as the shipping address, the recipient name, and the payment method so far and the conventional method for predicting such knowledge depends on the employee's working experiences. For these backgrounds, we observe the transaction data with the bad debt customer information gathered from a mail order company in Japan and characterized the customer with machine learning method. From the results of the analysis, potential fraudulent transactions are identified. Intensive research revealed that the classification of the deliberate customer and the careless customer with machine learning. This result will make use of the revenue expansion with the improvement of the bad debt collections.",autonomous vehicle
10.1016/j.procs.2015.07.320,journal,Procedia Computer Science,sciencedirect,2015-12-31,sciencedirect,Approaching Camera-based Real-World Navigation Using Object Recognition,https://api.elsevier.com/content/article/pii/S1877050915018232,"Traditional autonomous navigation systems for transportation use laser range scanners to con- struct 3D driving scenes in terms of open and occupied voxels. Active laser range scanners suffer from a series of failures, such as inability to detect wet road surfaces, dark surfaces and objects at large distances. In contrast, passive video cameras are immune from these failures but processing is challenging. High dimensionality of the input image requires efficient Big Data analytic methods for the system to perform in real-time. In this paper we argue that object recognition is essential for a navigation system to generalize learned landmarks to new driving scenes, which is a requirement for practical driving. To overcome this difficulty we present an online learning neural network for indoor navigation using only stereo cameras. The network can learn a Finite Automaton (FA) for the driving problem. Transition of the FA depends on several information sources: sensory input (stereo camera images) and motor input (i.e. object, action, GPS, and attention). Our agent simulates the transition of the FA by developing internal representation using the Developmental Network (DN) without handcrafting states or transi- tion rules. Although the proposed network is meant for both indoor and outdoor navigation, it has been only tested in indoor environments in current work. Our experiments demonstrate the agent learned to recognize landmarks and the corresponding actions (e.g. follow the GPS input, correct current direction, and avoid obstacles). Our future work includes training and learning in outdoor driving scenarios.",autonomous vehicle
10.1016/B978-0-08-051055-2.50033-X,journal,Machine Learning,sciencedirect,1990-12-31,sciencedirect,BIBLIOGRAPHY OF RECENT MACHINE LEARNING RESEARCH 1985–1989,https://api.elsevier.com/content/article/pii/B978008051055250033X,Unknown,autonomous vehicle
10.1016/0165-0114(93)90181-G,journal,Fuzzy Sets and Systems,sciencedirect,1993-05-25,sciencedirect,Fuzzy neural networks and neurocomputations,https://api.elsevier.com/content/article/pii/016501149390181G,"
                  The paper discusses relationships between conceptual and computational platforms of fuzzy sets and neurocomputations. Fuzzy sets provide an interesting and powerful scheme of knowledge representation with its clear-cut logical properties. Neural networks with their superb learning capabilities play a primordial role in numerical processing. The synergy between these two paradigms will be studied. Two new classes of basic logic-oriented processing units (fuzzy neurons) will be introduced. The first one embraces aggregative neurons realizing OR, AND and mixed OR/AND operations. The neurons of the reference character constituting the second category of the processing units pertain to a referential character of processing that includes manipulation on binary relations of matching, difference, inclusion and dominance. The proposed architecture of logic processors implements the paradigm of distributed processing with the aid of logic-driven neurons. Various application domains (fuzzy controllers, machine learning, decision-making and distributed modelling) will be studied.
               ",autonomous vehicle
10.1016/j.jksuci.2020.03.008,journal,Journal of King Saud University - Computer and Information Sciences,sciencedirect,2020-03-30,sciencedirect,Romanized Tunisian dialect transliteration using sequence labelling techniques,https://api.elsevier.com/content/article/pii/S1319157820303281,"In recent years, social web users in Arabic countries have been resorting to the dialects as a written language in their social exchanges. Arabic dialects derive from modern standard Arabic (MSA) and differ significantly from one country to another and one region to another. The use of these dialects has led to an increase of interest in the specificities of such informal languages and their automatic processing within the NLP community. In this work, we deal with the Tunisian dialect (TD) in particular. We address the issue of the automatic Latin to Arabic transliteration of TD language productions on the social web and propose an approach that models the transliteration as a sequence labeling task. At a word level, several techniques, based on machine and deep learning, have been tested for this study, using real word messages extracted from social networks. We experiment and compare three transliteration models: A Conditional Random Fields-based model (CRF), a Bidirectional Long Short-Term Memory based model (BLSTM), and a BLSTM based model with CRF decoding (BLSTM-CRF). The obtained results show that BLSTM-CRF, leads to the best performance, reaching 96.78% of correctly transliterated words. We also evaluate the BLSTM-CRF transliteration approach in context on a set of random TD messages extracted from the social web. We obtained a total error rate of 2.7%. 25% of which are context errors.",autonomous vehicle
10.1016/j.energy.2021.122367,journal,Energy,sciencedirect,2021-10-25,sciencedirect,"A Hybrid Optimized Model of Adaptive Neuro-Fuzzy Inference System, Recurrent Kalman Filter and Neuro-Wavelet for Wind Power Forecasting Driven by DFIG",https://api.elsevier.com/content/article/pii/S0360544221026165,"
                  Renewable energy resources are playing a compromising role in the new generation of sustainable energy and smart grid. Wind power is playing a crucial role these days to minimize the fossil fuel emissions. Their integration is depending on a highly accurate forecasting model due to its intermittency, nonlinearity, and fluctuation. This work presents a hybrid optimized model of Adaptive Neuro-Fuzzy Inference System (ANFIS), Recurrent Kalman Filter (RKF) and Neuro-Wavelet (WNN) for Wind Power Forecasting Driven by doubly fed induction generator (DFIG). The predictions of individual models and hybrid of ANFIS, RKF and WNN models for wind speed and power generated are compared with other published work results in the literature. Six different hybrid models are proposed (ANFIS+WNN+RKF, ANFIS+RKF+WNN, WNN+ANFIS+RKF, WNN+RKF+ANFIS, RKF+WNN+ANFIS, RKF+ANFIS+WNN). The results of this work indicate that all proposed hybrid models are performing well but the hybrid of ANFIS+RKF+WNN in sequence has the optimal performance compared to other models.
               ",autonomous vehicle
10.1016/S0920-5489(98)00069-5,journal,Computer Standards & Interfaces,sciencedirect,1999-02-15,sciencedirect,Formal neural network specification and its implications on standardization,https://api.elsevier.com/content/article/pii/S0920548998000695,"
                  This paper introduces a formal framework for describing and specifying neural networks and discusses several important issues with implications for neural network standardization. In particular, a neural network definition and two tools for graphical description and formal specification are introduced. Issues such as the theoretical impossibility of canonical description, or the need for complete specification (including global algorithms) are discussed. Several examples, making use of the developed tools, illustrate these discussions. In summary, this paper aims at contributing to the important endeavour of neural network standardization both practically and theoretically.
               ",autonomous vehicle
10.1016/B978-0-12-811306-6.00013-0,journal,Data Literacy,sciencedirect,2017-12-31,sciencedirect,Chapter 13: Correlation and Other Concepts You Should Know,https://api.elsevier.com/content/article/pii/B9780128113066000130,"
               This chapter provides an in-depth discussion on correlations—what they do and do not mean, how to derive them, and how to use them properly. We then discuss popular methods to analyze how one or more independent variables relate to the outcome(s) observed in an experiment. Regression methods, including linear regression, multiple regression, and logistic regression, are workhorses of data analysis. We then briefly survey other machine-learning methods including clustering, decision trees, support vector machines, and deep learning. The prospects and problems of handling Big Data are mentioned, and finally, we touch on methods that are designed to simplify high-dimensional data sets by reducing the number of dimensions to a small handful.
            ",autonomous vehicle
10.1016/j.csl.2010.04.002,journal,Computer Speech & Language,sciencedirect,2011-04-30,sciencedirect,A prototype for a conversational companion for reminiscing about images,https://api.elsevier.com/content/article/pii/S0885230810000331,"
                  This paper describes an initial prototype of the Companions project (www.companions-project.org): the Senior Companion (SC), designed to be a platform to display novel approaches to:
                        
                           (1)
                           The use of Information Extraction (IE) techniques to extract the content of incoming dialogue utterances after an ASR phase.
                        
                        
                           (2)
                           The conversion of the input to RDF form to allow the generation of new facts from existing ones, under the control of a Dialogue Manager (DM), that also has access to stored knowledge and knowledge accessed in real time from the web, all in RDF form.
                        
                        
                           (3)
                           A DM expressed as a stack and network virtual machine that models mixed initiative in dialogue control.
                        
                        
                           (4)
                           A tuned dialogue act detector based on corpus evidence.
                        
                     
                  
                  The prototype platform was evaluated, and we describe this; it is also designed to support more extensive forms of emotion detection carried by both speech and lexical content, as well as extended forms of machine learning. We describe preliminary studies and results for these, in particular a novel approach to enabling reinforcement learning for open dialogue systems through the detection of emotion in the speech signal and its deployment as a form of a learned DM, at a higher level than the DM virtual machine and able to direct the SC's responses to a more emotionally appropriate part of its repertoire.
               ",autonomous vehicle
10.1016/j.engstruct.2021.112311,journal,Engineering Structures,sciencedirect,2021-07-15,sciencedirect,Use of convolutional networks in the conceptual structural design of shear wall buildings layout,https://api.elsevier.com/content/article/pii/S0141029621004612,"
                  In the structural design of shear wall buildings, the initial process requires the interaction between the architecture and engineering teams to define the appropriate distribution of the walls, a stage typically carried out through a trial-and-error procedure, without any consideration of previous similar projects. In previous work, a database of 165 Chilean residential projects of reinforced shear wall concrete buildings was built, which fed a regressive neural network model to predict the wall’s engineering thickness and length values from an architectural 30-feature input vector, which accounts for geometric and topological properties, archiving remarkable results regarding the coefficient of determination (
                        
                           
                              
                                 R
                              
                              2
                           
                        
                     ). However, a regressive model of this nature does not incorporate a spatial detail or contextual information of each wall’s perimeter, and also, the prediction of other parameters such as the wall translation has a poor performance. For this reason, the present research proposes a framework based on convolutional neural network (CNN) models to generate the final engineering floor plan by combining two independent floor plan predictions, considering the architectural data as input. The first plan prediction is assembled using two regressive models that predict the wall engineering values of the thickness, the length, the wall translation on both axes from the architectural plan, and the floor bounding box width and aspect ratio. The second plan prediction is assembled using a model that generates a likely image of each wall’s engineering floor plan. Both independently predicted plans are combined to lead the final engineering floor plan, which allows predicting the wall’s rectangles design parameters and propose new structural elements not present in architecture, making the methodology an excellent candidate to accelerate the building wall layout’s early conceptual design.
               ",autonomous vehicle
10.1016/j.bdr.2021.100206,journal,Big Data Research,sciencedirect,2021-07-15,sciencedirect,A Survey on Data-driven Performance Tuning for Big Data Analytics Platforms,https://api.elsevier.com/content/article/pii/S221457962100023X,"
                  Many research works deal with big data platforms looking forward to data science and analytics. These are complex and usually distributed environments, composed of several systems and tools. As expected, there is a need for a closer look at performance issues.
                  In this work, we review performance tuning strategies in the big data environment. We focus on data-driven tuning techniques, discussing the use of database inspired approaches. Concerning big data and NoSQL stores, performance tuning issues are quite different from the so-called conventional systems. Many existing solutions are mostly ad-hoc activities that do not fit for multiple situations. But there are some categories of data-driven solutions that can be taken as guidelines and incorporated into general-purpose auto-tuning modules for big data systems.
                  We examine typical performance tuning actions, discussing available solutions to support some of the tuning process's primary activities. We also discuss recent implementations of data-driven performance tuning solutions for big data platforms. We propose an initial classification based on the domain state-of-the-art and present selected tuning actions for large-scale data processing systems. Finally, we organized existing works towards self-tuning big data systems based on this classification and presented general and system-specific tuning recommendations. We found that most of the literature pieces evaluate the use of tuning actions at the physical design perspective, and there is a lack of self-tuning machine-learning-based solutions for big data systems.
               ",autonomous vehicle
10.1016/B978-0-12-823337-5.00024-X,journal,Intelligence-Based Medicine,sciencedirect,2020-12-31,sciencedirect,Glossary,https://api.elsevier.com/content/article/pii/B978012823337500024X,Unknown,autonomous vehicle
10.1016/j.future.2021.04.005,journal,Future Generation Computer Systems,sciencedirect,2021-09-30,sciencedirect,An argumentation enabled decision making approach for Fall Activity Recognition in Social IoT based Ambient Assisted Living systems,https://api.elsevier.com/content/article/pii/S0167739X21001205,"
                  With the advancement in Information and Communication Technologies (ICTs), smart devices are becoming even more smart and intelligent with every passing day. Further, the evolution of speaking and hearing enabled devices in an IoT network is transforming the face of research in the Social IoT domain. However, the integration of argumentation enabled devices in Social IoT network has not been fully explored by researchers in the past. Therefore, this research work focuses on development of argument enabled Social IoT networks. In this paper, a fuzzy argument based classification scheme termed as Classification Enhanced with Fuzzy Argumentation (CleFAR) is proposed. The proposed scheme is deployed for classification of fall activities in fall prevention applications. A novel framework for fall prevention system using Fall Activity Recognition (FAR) is presented. The proposed system is designed for the purpose of fall activity recognition in smart home Ambient Assisted Living (AAL) systems. To experimentally evaluate the system’s performance, a smart home AAL environment is simulated and the inhabitant’s routine activity dataset is generated. The fall activities are simulated using wearable fall detection systems. The proposed scheme is trained and tested on generated datasets and its performance is compared with traditional classification algorithms such as Random Forest (RF), Support Vector Machines (SVM), Naive Bayes (NB), Decision Tree (DT) and Artificial Neural Networks (ANN) as well as existing argumentation based game theoretic Weighted Voting Scheme (WVS). Experimental results indicate that the proposed scheme outperforms the traditional classification schemes and WVS approach with prediction accuracy up to 91%. It turns out that the proposed approach achieves significant improvement over the existing schemes.
               ",autonomous vehicle
10.1016/j.inffus.2021.07.001,journal,Information Fusion,sciencedirect,2021-12-31,sciencedirect,"Advances in Data Preprocessing for Biomedical Data Fusion: An Overview of the Methods, Challenges, and Prospects",https://api.elsevier.com/content/article/pii/S1566253521001354,"
                  Due to the proliferation of biomedical imaging modalities, such as Photoacoustic Tomography, Computed Tomography (CT), Optical Microscopy and Tomography, etc., massive amounts of data are generated on a daily basis. While massive biomedical data sets yield more information about pathologies, they also present new challenges of how to fully explore the data. Data fusion methods are a step forward towards a better understanding of data by bringing multiple data observations together to increase the consistency of the information. However, data generation is merely the first step, and there are many other factors involved in the fusion process like noise, missing data, data scarcity, and high dimensionality. In this paper, an overview of the advances in data preprocessing in biomedical data fusion is provided, along with insights stemming from new developments in the field.
               ",autonomous vehicle
10.1016/j.engappai.2020.103868,journal,Engineering Applications of Artificial Intelligence,sciencedirect,2020-10-31,sciencedirect,Multi-modal recognition of worker activity for human-centered intelligent manufacturing,https://api.elsevier.com/content/article/pii/S0952197620302190,"
                  This study aims at sensing and understanding the worker’s activity in a human-centered intelligent manufacturing system. We propose a novel multi-modal approach for worker activity recognition by leveraging information from different sensors and in different modalities. Specifically, a smart armband and a visual camera are applied to capture Inertial Measurement Unit (IMU) signals and videos, respectively. For the IMU signals, we design two novel feature transform mechanisms, in both frequency and spatial domains, to assemble the captured IMU signals as images, which allow using convolutional neural networks to learn the most discriminative features. Along with the above two modalities, we propose two other modalities for the video data, i.e., at the video frame and video clip levels. Each of the four modalities returns a probability distribution on activity prediction. Then, these probability distributions are fused to output the worker activity classification result. A worker activity dataset is established, which at present contains 6 common activities in assembly tasks, i.e., grab a tool/part, hammer a nail, use a power-screwdriver, rest arms, turn a screwdriver, and use a wrench. The developed multi-modal approach is evaluated on this dataset and achieves recognition accuracies as high as 97% and 100% in the leave-one-out and half-half experiments, respectively.
               ",autonomous vehicle
10.1016/B978-0-08-041698-4.50010-X,journal,IFAC Proceedings Volumes,sciencedirect,1991-09-30,sciencedirect,Preprocessing inputs for adaptive critic control,https://api.elsevier.com/content/article/pii/B978008041698450010X,"
                  Preliminary results reported in this paper constitute the first step in an effort to explore the role of artificial neural networks (ANN) in maintaining the stability of dynamic systems. In particular, the goal is to understand the control mechanism necessary to maintain the postural stability of a musculoskeletal model of a human. In view of the speculation that the olive nucleus in the brainstem acts as an adaptive critic for the descending motor control, we chose to investigate the adaptive critic algorithm. The adaptive critic is a reinforcement learning technique which utilizes qualitative feedback, instead of quantitative feedback as in supervised learning; or no feedback at all as in self-organizing systems. The use of the adaptive critic to control a dynamic system, such as a cart-pole system, has been demonstrated by a number of investigators including Widrow, Gupta, and Maitra (1973); Barto, Sutton, and Anderson (1983); and Anderson (1989). Barto quantized the state variables before processing them in the adaptive critic network. Anderson's approach essentially replaces the quantizer with an ANN. In this paper, Anderson's method is modified by incorporating into the control loop what Klassen and Pao refer to as the Functional Link Outerproduct (FLO) in place of Anderson's ANN. The FLO expands the original input space by including higher order terms. This stratagem, while simplifying the structure, also improves the learning rate of the network. While Anderson's algorithm, when implemented as a controller, generalizes its control much better than Barto's algorithm, it takes much longer to train. The FLO, on the other hand, provides improved generalization over Barto's algorithm without the large increase in training time. The long-term goal of this project is to explore the utility of these ideas in controlling human posture.
               ",autonomous vehicle
10.1016/S0893-6080(20)30403-2,journal,Neural Networks,sciencedirect,2021-01-31,sciencedirect,List of Editorial Board Members,https://api.elsevier.com/content/article/pii/S0893608020304032,,autonomous vehicle
10.1016/j.neunet.2013.11.012,journal,Neural Networks,sciencedirect,2014-02-28,sciencedirect,Pointwise probability reinforcements for robust statistical inference,https://api.elsevier.com/content/article/pii/S0893608013002761,"
                  Statistical inference using machine learning techniques may be difficult with small datasets because of abnormally frequent data (AFDs). AFDs are observations that are much more frequent in the training sample that they should be, with respect to their theoretical probability, and include e.g. outliers. Estimates of parameters tend to be biased towards models which support such data. This paper proposes to introduce pointwise probability reinforcements (PPRs): the probability of each observation is reinforced by a PPR and a regularisation allows controlling the amount of reinforcement which compensates for AFDs. The proposed solution is very generic, since it can be used to robustify any statistical inference method which can be formulated as a likelihood maximisation. Experiments show that PPRs can be easily used to tackle regression, classification and projection: models are freed from the influence of outliers. Moreover, outliers can be filtered manually since an abnormality degree is obtained for each observation.
               ",autonomous vehicle
10.1016/S1474-6670(17)37567-5,journal,IFAC Proceedings Volumes,sciencedirect,1998-06-30,sciencedirect,Intelligent Control Methodologies For Control Strategy and Control Structure Selection,https://api.elsevier.com/content/article/pii/S1474667017375675,"
                  The purpose of this paper is to present a brief overlook and evaluation of the methods of artificial intelligence in process control that are relative to the selection of a control system and the choice of an adequate control policy of industrial processes. Consequently it discusses the selection of an adequate process control structure based on the intelligent control methodologies including neural networks, fuzzy and hybrid techniques, modeling and identification, and data analysis methods. For this goal we have elaborated a brief study and evaluation of process control methodologies for control strategy and control structure selection, and we have highlighted the needs to implement advanced adequate solutions to the existing problems.
               ",autonomous vehicle
10.1016/j.knosys.2021.107480,journal,Knowledge-Based Systems,sciencedirect,2021-11-28,sciencedirect,Increasingly Specialized Generative Adversarial Network for fine-grained visual categorization,https://api.elsevier.com/content/article/pii/S0950705121007425,"
                  Fine-grained visual categorization is challenging because the subordinate categories within an entry-level category can only be distinguished by subtle discriminations. This necessitates to localize key (most discriminative) regions and extract domain-specific features alternately, since implicit to fine-grained specialization is the existence of an entry-category visual shared among all classes. Existing methods predominantly implement fine-grained categorization independently, while neglecting that patch proposal and discrimination extraction are mutually correlated and can reinforce each other in an increasingly specialized manner. In this work, we concretize the above pipeline as an Increasing Specialized Generative Adversarial Network (IS-GAN), which recursively shapes a coarse-to-fine representation. It is a three-scale framework consisting of two highlights: a three-player expert GAN at each scale for feature extraction, and a Patch Proposal Network (PPN) between two adjacent scales for target positioning. To better anatomize pixel-to-pixel correlations at various octaves, the Gaussian pyramid and Laplacian pyramid descriptions are also integrated in each GAN. The PPN zooms the areas to shift the focus on the most representative regions by taking previous prediction of classifier as a reference, whilst a finer scale network receives an amplified attended region from previous scale. Overall, IS-GAN is driven by three focal losses from GANs and a converged object-level loss. Experiments demonstrate that IS-GAN can simultaneously (1) deliver competitive categorization performance among state-of-the-arts, i.e., validation accuracy achieves 92.23% and testing accuracy achieves 90.27%, and (2) recover fine-grained textures with high Peak Signal-to-Noise Ratios (PSNRs) (32.937) and Structural Similarities (SSIMs) (0.8607) from hand-crafted and public benchmarks.
               ",autonomous vehicle
10.1016/B978-155860759-0/50006-8,journal,Computational Intelligence,sciencedirect,2007-12-31,sciencedirect,chapter six: Neural network implementations,https://api.elsevier.com/content/article/pii/B9781558607590500068,"
               This chapter presents four neural network implementations: back-propagation neural networks, the learning vector quantizer (LVQ), Kohonen's self-organizing feature map networks, and evolutionary multilayer perceptron neural networks. The back-propagation source code for the neural network implementation is written to support the implementation of one or more hidden layers. The number of hidden layers and the number of processing element (PEs) in each layer can be specified in the run file. The classification of Iris data is included as a benchmark problem to be solved. The chapter discusses issues such as topology that are related to implementing neural networks on personal computers. All four of the neural networks implemented are layered networks. The back-propagation neural networks have more than two layers (at least one hidden layer), and the Kohonen networks have only two layers. The LVQ-I and self-organizing feature map networks consist of a two-layer feedforward topology, where the input layer is fully connected to the output layer.
            ",autonomous vehicle
10.1016/j.matpr.2020.12.1049,journal,Materials Today: Proceedings,sciencedirect,2021-03-04,sciencedirect,Review on gas turbine condition based diagnosis method,https://api.elsevier.com/content/article/pii/S2214785320407679,"For a long period, computerized models for the diagnosis and performance prediction of the gas turbine is developed. Usage of a strong, productive, and adaptable upkeep method improves essentially the unwavering quality and accessibility of a gas turbine and thus minimizes the unpredicted failures, downtime, and working costs. Early discovery of advancing deficiencies, whereas the plant is still working in a controllable locale, can offer assistance maintain a strategic distance from unusual occasion movement and diminish efficiency loss. Actualizing a legitimate predictive or condition-based maintenance (CBM) methodology makes difference maintenance engineers screen the gas turbine condition and spot up and coming faults. This will empower them to proactively adopt the appropriate maintenance choice which subsequently leads to improved system reliability and availability, decreased maintenance costs, and decreased the number of maintenance operations. This paper focuses to survey the published papers on gas turbine diagnosis strategy and to make summarize the reviewed papers. This paper describes different diagnostics methods and their advantages and disadvantages and finally, the future work is also included.",autonomous vehicle
10.1016/0167-2789(90)90181-N,journal,Physica D: Nonlinear Phenomena,sciencedirect,1990-09-02,sciencedirect,Adaptive stochastic cellular automata: Applications,https://api.elsevier.com/content/article/pii/016727899090181N,"
                  The stochastic learning cellular automata model has been applied to the problem of controlling unstable systems. Two example unstable systems studied are controlled by an adaptive stochastic cellular automata algorithm with an adaptive critic. The reinforcement learning algorithm and the architecture of the stochastic CA controller are presented. Learning to balance a single pole is discussed in detail. Balancing an inverted double pendulum highlights the power of the stochastic CA approach. The stochastic CA model is compared to conventional adaptive control and artificial neural network approaches.
               ",autonomous vehicle
10.1016/j.knosys.2015.04.007,journal,Knowledge-Based Systems,sciencedirect,2015-08-31,sciencedirect,Integration of graph clustering with ant colony optimization for feature selection,https://api.elsevier.com/content/article/pii/S0950705115001458,"
                  Feature selection is an important preprocessing step in machine learning and pattern recognition. The ultimate goal of feature selection is to select a feature subset from the original feature set to increase the performance of learning algorithms. In this paper a novel feature selection method based on the graph clustering approach and ant colony optimization is proposed for classification problems. The proposed method’s algorithm works in three steps. In the first step, the entire feature set is represented as a graph. In the second step, the features are divided into several clusters using a community detection algorithm and finally in the third step, a novel search strategy based on the ant colony optimization is developed to select the final subset of features. Moreover the selected subset of each ant is evaluated using a supervised filter based method called novel separability index. Thus the proposed method does not need any learning model and can be classified as a filter based feature selection method. The proposed method integrates the community detection algorithm with a modified ant colony based search process for the feature selection problem. Furthermore, the sizes of the constructed subsets of each ant and also size of the final feature subset are determined automatically. The performance of the proposed method has been compared to those of the state-of-the-art filter and wrapper based feature selection methods on ten benchmark classification problems. The results show that our method has produced consistently better classification accuracies.
               ",autonomous vehicle
10.1016/0950-7051(96)81920-4,journal,Knowledge-Based Systems,sciencedirect,1995-12-31,sciencedirect,Survey and critique of techniques for extracting rules from trained artificial neural networks,https://api.elsevier.com/content/article/pii/0950705196819204,"
                  It is becoming increasingly apparent that, without some form of explanation capability, the full potential of trained artificial neural networks (ANNs) may not be realised. This survey gives an overview of techniques developed to redress this situation. Specifically, the survey focuses on mechanisms, procedures, and algorithms designed to insert knowledge into ANNs (knowledge initialisation), extract rules from trained ANNs (rule extraction), and utilise ANNs to refine existing rule bases (rule refinement). The survey also introduces a new taxonomy for classifying the various techniques, discusses their modus operandi, and delineates criteria for evaluating their efficacy.
               ",autonomous vehicle
10.1016/S0924-6509(08)70037-0,journal,North-Holland Mathematical Library,sciencedirect,1993-12-31,sciencedirect,Self-Organizing Neural Networks for Stable Control of Autonomous Behavior in A Changing World,https://api.elsevier.com/content/article/pii/S0924650908700370,"
                  This chapter discusses how neural models function autonomously in a stable fashion despite unexpected changes in their environments. The content of these models consists of a small set of equations that describe processes such as activation of short term memory (STM) traces, associative learning by adaptive weights or long term memory (LTM) traces, and slow habituative gating or medium term memory (MTM) by chemical modulators and transmitters. Two of the core neural network models are often called the additive model and the shunting model. These models explain how STM and LTM traces interact during network processes of activation, associative learning, and recall. One of the most important contributions of neural network models has been to show how behavioral properties can arise as emergent properties because of network interactions. The chapter presents the adaptive resonance theory (ART) and ART systems learn prototypes. These prototypes can be used to encode individual exemplars.
               ",autonomous vehicle
10.1016/j.ijpe.2020.107868,journal,International Journal of Production Economics,sciencedirect,2021-01-31,sciencedirect,Big data algorithms and applications in intelligent transportation system: A review and bibliometric analysis,https://api.elsevier.com/content/article/pii/S0925527320302279,"
                  The volume and availability of data in the Intelligent Transportation System (ITS) result in the need for data-driven approaches. Big Data algorithms are applied to further enhance the intelligence of the applications in the transportation field. Applying Big Data algorithms has increasingly received attention in both the academic and industrial fields of ITS. Big Data algorithms in ITS have a wide range of applications including but not limited to signal recognition, object detection, traffic flow prediction, travel time planning, travel route planning and safety of vehicle and road. This survey aims to provide a bibliography, a comprehensive review of the application of ITS and a review of most recognized models with Big Data used in the context of ITS. 586 papers are reviewed over the period 1997–2019. This study provides a deep insight into applications of Big Data algorithms in ITS, revealing different areas of those applications and integrates models and applications. The result of the study identifies research gaps and direction for the future.
               ",autonomous vehicle
10.1016/j.procs.2014.08.254,journal,Procedia Computer Science,sciencedirect,2014-12-31,sciencedirect,A Study on Effect Evaluation of Payment Method Change in the Mail-order Industry,https://api.elsevier.com/content/article/pii/S1877050914012198,"This paper presents investigating the customer characteristics of payment method change in the mail order industry. This time we are focusing on the transactional activity of bad debt customers. These kinds of investigations have not made intensively, such as the shipping address, the recipient name, and the payment method so far and the conventional method for predicting such knowledge depends on the employees’ working experiences. For these backgrounds, we observed the transaction data with the bad debt customer information gathered from a mail order company and characterized the customer with machine learning. From the results of the analysis, we are succeeded in characterizing the potential customers. Intensive research revealed that the characteristics of customers who make fraud transactions. This result will make use of the revenue expansion with the improvement of the bad debt collections in the target industry.",autonomous vehicle
10.1016/j.neucom.2017.02.024,journal,Neurocomputing,sciencedirect,2017-06-07,sciencedirect,"Data-driven prognostics using a combination of constrained K-means clustering, fuzzy modeling and LOF-based score",https://api.elsevier.com/content/article/pii/S0925231217302941,"
                  Today, failure modes characterization and early detection is a key issue in complex assets. This is due to the negative impact of corrective operations and the conservative strategies usually put in practice, focused on preventive maintenance. In this paper anomaly detection issue is addressed in new monitoring sensor data by characterizing and modeling operational behaviors. The learning framework is performed on the basis of a machine learning approach that combines constrained K-means clustering for outlier detection and fuzzy modeling of distances to normality. A final score is also calculated over time, considering the membership degree to resulting fuzzy sets and a local outlier factor. Proposed solution is deployed in a CBM+ platform for online monitoring of the assets. In order to show the validity of the approach, experiments have been conducted on real operational faults in an auxiliary marine diesel engine. Experimental results show a fully comprehensive yet accurate prognostics approach, improving detection capabilities and knowledge management. The performance achieved is quite high (precision, sensitivity and specificity above 93% and 
                        
                           κ
                           =
                           0.93
                        
                     ), even more so given that a very small percentage of real faults are present in data.
               ",autonomous vehicle
10.1016/B978-012443880-4/50082-X,journal,Expert Systems,sciencedirect,2002-12-31,sciencedirect,38: Popfnns: Fuzzy Neural Techniques for Rule-Based Identification in Expert Systems,https://api.elsevier.com/content/article/pii/B978012443880450082X,"
               Fuzzy neural networks are hybrid intelligent systems that are constructed using both neural network and fuzzy logic techniques. They aim to emulate the human thinking process and can efficaciously serve as an extension of the creative and problem-solving abilities. They have the advantages of both neural networks and fuzzy systems, and alleviate the shortcomings of the respective techniques. Fuzzy neural networks offer a rich environment for producing intelligent applications that can enormously increase productivity and act as intelligent assistants. This chapter focuses on the design and implementation of fuzzy neural networks that use neural network techniques to realize fuzzy rule-based systems under different fuzzy inference models. It presents three novel fuzzy neural network structures and two rule identification algorithms. The fuzzy neural network structures include the pseudo outer-product-based fuzzy neural network using the truth-value restriction method; the pseudo outer-product-based fuzzy neural network using the approximate analogical reasoning schema together with the singleton fuzzifier; and the pseudo outer-product-based fuzzy neural network using the approximate analogical reasoning schema together with the nonsingleton fuzzifier. The two rule identification algorithms are the pseudo outer-product and the Lazy pseudo outer-product learning algorithms. These fuzzy neural networks have been applied efficiently in novel applications for pattern classifications of signatures and fingerprints and, they produce potential results.
            ",autonomous vehicle
10.1016/j.specom.2017.05.002,journal,Speech Communication,sciencedirect,2017-09-30,sciencedirect,A predictive coding framework for a developmental agent: Speech motor skill acquisition and speech production,https://api.elsevier.com/content/article/pii/S016763931630139X,"
                  Predictive coding has been hypothesized as a universal principle guiding the operation in different brain areas. In this paper, a predictive coding framework for a developmental agent with perception (audio), action (vocalization), and learning capabilities is proposed. The agent learns concurrently to plan optimally and the associations between sensory and motor parameters, by minimizing the sensory prediction error in an unsupervised manner. The proposed agent is solely driven by sensory prediction error and does not require reinforcement. It learns initially by self-exploration and later by imitation from the ambient environment.
                  Our goal is to investigate the process of speech motor skill acquisition and speech production in such an agent. Standard vocal exploration experiments show that it learns to generate speech-like sounds (acoustic babbling followed by proto-syllables and vowels) as well as the timing for motor command execution. Random goal exploration leads to the self-organization of developmental stages of vocal sequences in the agent due to increase in complexity of vocalization. The self-organization is invariant to certain acoustic feature representations. Self-exploration allows the agent to learn to imitate environmental sounds quickly. It learns to vocalize differently in different environments.
               ",autonomous vehicle
10.1016/j.inffus.2019.11.001,journal,Information Fusion,sciencedirect,2020-05-31,sciencedirect,Multi-sensor fusion for body sensor network in medical human–robot interaction scenario,https://api.elsevier.com/content/article/pii/S1566253519302027,"
                  With the development of sensor and communication technologies, body sensor networks(BSNs) have become an indispensable part of smart medical services by monitoring the real-time state of users. Due to introducing of smart medical robots, BSNs are not related to users, but also responsible for data acquisition and multi-sensor fusion in medical human–robot interaction scenarios. In this paper, a hybrid body sensor network architecture based on multi-sensor fusion(HBMF) is designed to support the most advanced smart medical services, which combines various sensor, communication, robot, and data processing technologies. The infrastructure and system functions are described in detail and compared with other architectures. Especially, A multi-sensor fusion method based on interpretable neural network(MFIN) for BSNs in medical human–robot interaction scenario is designed and analyzed to improve the performance of fusion decision-making. Compared with the current multi-sensor fusion methods, our design guarantees both the flexibility and reliability of the service in the medical human–robot interaction scenario.
               ",autonomous vehicle
10.1016/j.eswa.2019.112948,journal,Expert Systems with Applications,sciencedirect,2020-03-01,sciencedirect,A review: Knowledge reasoning over knowledge graph,https://api.elsevier.com/content/article/pii/S0957417419306669,"
                  Mining valuable hidden knowledge from large-scale data relies on the support of reasoning technology. Knowledge graphs, as a new type of knowledge representation, have gained much attention in natural language processing. Knowledge graphs can effectively organize and represent knowledge so that it can be efficiently utilized in advanced applications. Recently, reasoning over knowledge graphs has become a hot research topic, since it can obtain new knowledge and conclusions from existing data. Herein we review the basic concept and definitions of knowledge reasoning and the methods for reasoning over knowledge graphs. Specifically, we dissect the reasoning methods into three categories: rule-based reasoning, distributed representation-based reasoning and neural network-based reasoning. We also review the related applications of knowledge graph reasoning, such as knowledge graph completion, question answering, and recommender systems. Finally, we discuss the remaining challenges and research opportunities for knowledge graph reasoning.
               ",autonomous vehicle
10.1016/j.neucom.2018.07.091,journal,Neurocomputing,sciencedirect,2019-05-28,sciencedirect,Exploratory study on Class Imbalance and solutions for Network Traffic Classification,https://api.elsevier.com/content/article/pii/S092523121930164X,"
                  Network Traffic Classification is a fundamental component in network management, and the fast-paced advances in Machine Learning have motivated the application of learning techniques to identify network traffic. The intrinsic features of Internet networks lead to imbalanced class distributions when datasets are conformed, phenomena called Class Imbalance and that is attaching an increasing attention in many research fields. In spite of performance losses due to Class Imbalance, this issue has not been thoroughly studied in Network Traffic Classification and some previous works are limited to few solutions and/or assumed misleading methodological approaches. In this article, we deal with Class Imbalance in Network Traffic Classification, studying the presence of this phenomenon and analyzing a wide number of solutions in two different Internet environments: a lab network and a high-speed backbone. Namely, we experimented with 21 data-level algorithms, six ensemble methods and one cost-level approach. Throughout the experiments performed, we have applied the most recent methodological aspects for imbalanced problems, such as: DOB-SCV validation approach or the performance metrics assumed. And last but not least, the strategies to tune parameters and our algorithm implementations to adapt binary methods to multiclass problems are presented and shared with the research community, including two ensemble techniques used for the first time in Machine Learning to the best of our knowledge. Our experimental results reveal that some techniques mitigated Class Imbalance with interesting benefit for traffic classification models. More specifically, some algorithms reached increases greater than 8% in overall accuracy and greater than 4% in AUC-ROC for the most challenging network scenario.
               ",autonomous vehicle
10.1016/j.neucom.2015.03.117,journal,Neurocomputing,sciencedirect,2016-01-22,sciencedirect,A novel biologically inspired ELM-based network for image recognition,https://api.elsevier.com/content/article/pii/S0925231215011534,"
                  In this paper, a novel biologically inspired network for image recognition has been introduced. The Hierarchical model and X (HMAX) model and the extreme learning machine (ELM) are combined, to construct a five-layer feed-forward network: S1–C1–S2–C2–H. The previous four layers, originating from HMAX, provide robust feature representation of specific object, and the feature classification stage in the H layer is implemented with ELM. The HMAX model simulates the hierarchical processing mechanism in primate visual cortex, to calculate complex features representation. As a biological learning algorithm for generalized SLFNs, ELM learns much faster with good generalization performance, and performs well in classification applications. Four groups of experiments are performed on three datasets, and the results are compared with state-of-the-art techniques. Experimental results show that our proposed network has good performance with fast learning speed.
               ",autonomous vehicle
10.1016/j.apenergy.2021.117775,journal,Applied Energy,sciencedirect,2022-01-01,sciencedirect,Smart fusion of sensor data and human feedback for personalized energy-saving recommendations,https://api.elsevier.com/content/article/pii/S0306261921011120,"
                  Despite the variety of sensors that can be used in a smart home or office setup, for monitoring energy consumption and assisting users to save energy, their usefulness is limited when they are not properly integrated into the daily activities of humans. Energy-saving applications in such environments can benefit from the use of sensors and actuators when data are properly fused with previous knowledge about user habits and feedback about current user preferences. In this article, we present an online recommender system implemented in the EM3 platform, a platform for Consumer Engagement Toward Energy-Saving Behavior. The recommender system uniquely fuses sensors’ data with user habits and user feedback and provides personalized recommendations for energy efficiency at the right moment. The user response to the recommendations directly triggers actuators that perform energy-saving actions and is recorded and processed for refining future recommendations. The EM3 recommendation engine continuously evaluates the three inputs (i.e. sensor data, user habits, user feedback) and identifies the micro-moments that maximize the need for the recommended action and thus the recommendation acceptance. We evaluate the efficiency of the proposed recommender system, which is based on a stacked-LSTM for fusing multi-sensor data streams, in several scenarios, and the observed accuracy on predicting the right moment to send a recommendation to the user ranged from 93% to 97%.
               ",autonomous vehicle
10.1016/j.cose.2018.05.015,journal,Computers & Security,sciencedirect,2018-09-30,sciencedirect,Cyber-security: Identity deception detection on social media platforms,https://api.elsevier.com/content/article/pii/S0167404818306503,"
                  Social media platforms allow billions of individuals to share their thoughts, likes and dislikes in real-time, without any censorship. This freedom, however, comes at a cyber-security risk. Cyber threats are more difficult to detect in a cyber world where anonymity and false identities are ever-present. The speed at which these deceptive identities evolve calls for solutions to detect identity deception. Cyber-security threats caused by humans on social media platforms are widespread and warrant attention. This research posits a solution towards the intelligent detection of deceptive identities contrived by human individuals on social media platforms (SMPs). Firstly, this research evaluates machine learning models by using attributes such as the “profile image” found on SMPs. To improve on the results delivered by these models, past research findings from the field of psychology, such as that humans lie about their gender, are used. Newly engineered features such as “gender-derived-from-the-profile-image” are evaluated to grasp whether these features detect deception with greater accuracy. Furthermore, research results from detecting non-human (also known as bot) accounts are also leveraged to improve on the initial results. These machine learning results are lastly applied to a proposed model for the intelligent detection and interpretation of identity deception on SMPs. This paper shows that the cyber-security threat of identity deception can potentially be minimized, should the vulnerability in the current way of setting up user accounts on SMPs be re-engineered in the future.
               ",autonomous vehicle
10.1016/B978-0-12-819710-3.00008-9,journal,From Smart Grid to Internet of Energy,sciencedirect,2019-12-31,sciencedirect,"Chapter 8: Big data, privacy and security in smart grids",https://api.elsevier.com/content/article/pii/B9780128197103000089,"
               The smart grid applications are related with monitoring and control operations of conventional power grid. The integration of information and communication technologies (ICT) to existing power network has leveraged interaction of different generators, controllers, monitoring and measurement devices, and intelligent loads. The two-way communication infrastructure is comprised by numerous sensor networks that increased deployment of massive data from measurement nodes to monitoring centers. Big data is a widespread concept which become a trend for massive data streams that are transferred and processed in an ecosystem. The enormous amount of data are generated, transferred and stored to improve operating and management quality of smart grid. The big data analytics are performed to improve quality of service in terms of grid operators and consumers. The big data acquisition, processing, storing, and clustering stages are widely researched by a wide variety of specialist. In this chapter, the all these stages, big data acquisition technologies, machine learning methods used in big data analytics, privacy and security of big data infrastructure are introduced in detail. The privacy preserving methods, big data processing technologies and firmware infrastructures are presented in the context of this chapter.
            ",autonomous vehicle
10.1016/B978-0-12-822830-2.00016-7,journal,Unmanned Driving Systems for Smart Trains,sciencedirect,2021-12-31,sciencedirect,Index,https://api.elsevier.com/content/article/pii/B9780128228302000167,Unknown,autonomous vehicle
10.1016/B978-0-12-817444-9.00001-5,journal,Practical Guide for Biomedical Signals Analysis Using Machine Learning Techniques,sciencedirect,2019-12-31,sciencedirect,Chapter 1: Introduction and Background,https://api.elsevier.com/content/article/pii/B9780128174449000015,"
               Recently many achievements in the biomedical signal analysis and classification have been realized. Biomedical signals contain several information, which cannot be observed directly but hidden in the signal's structure. This hidden information can be converted into meaningful interpretations by utilizing different signal processing and machine learning techniques. Moreover, biomedical signals represent biological system properties to identify numerous pathological conditions. Therefore, the biomedical signal analysis using diverse signal processing and machine learning methods becomes a vital instrument to extract clinically significant information hidden in the signal. Biomedical signal analysis is used to develop automated diagnostic systems for decision support. Usually, biomedical signals are assessed manually or visually by expert physicians, but this might result in unreliable or inefficient diagnostic. Consequently, the aim of the automated biomedical signal analysis is to reduce the subjectivity of manual assessment of biomedical signals. The computer-aided biomedical signal analysis for assessing different signal characteristics help to make objective decision by improving accuracy. Additionally, in order to reduce subjectivity of assessment, biomedical signal processing is employed for the feature extraction and dimension reduction to support characterizing and understanding of the information exist in a biomedical signal.
            ",autonomous vehicle
10.1016/j.cogsys.2004.03.007,journal,Cognitive Systems Research,sciencedirect,2005-06-30,sciencedirect,Associative computer: a hybrid connectionistic production system,https://api.elsevier.com/content/article/pii/S1389041704000324,"
                  In this paper, we introduce a connectionistic hybrid production system, which relies on the distributed representation and the usage of associative memories. Benefits of the distributed representation include heuristics resulting from pictogram representation. By the usage of associative memory, the model can learn from experience. The behavior of the model is demonstrated by empirical experiments in block world. The hybrid model represents an effort to fill the gap between the statistical models like the artificial neural networks and symbolic models.
               ",autonomous vehicle
10.1016/j.epsr.2020.106788,journal,Electric Power Systems Research,sciencedirect,2020-12-31,sciencedirect,Big data analytics for future electricity grids,https://api.elsevier.com/content/article/pii/S0378779620305915,"
                  This paper provides a survey of big data analytics applications and associated implementation issues. The emphasis is placed on applications that are novel and have demonstrated value to the industry, as illustrated using field data and practical applications. The paper reflects on the lessons learned from initial implementations, as well as ideas that are yet to be explored. The various data science trends treated in the literature are outlined, while experiences from applying them in the electricity grid setting are emphasized to pave the way for future applications. The paper ends with opportunities and challenges, as well as implementation goals and strategies for achieving impactful outcomes.
               ",autonomous vehicle
10.1016/j.techfore.2021.120745,journal,Technological Forecasting and Social Change,sciencedirect,2021-06-30,sciencedirect,Optimizing early cancer diagnosis and detection using a temporal subtraction technique,https://api.elsevier.com/content/article/pii/S0040162521001773,"
                  To optimize the early diagnosis and detection of lung cancer, computer-aided diagnostic (CAD) systems have been a useful tool for analyzing medical images. The temporal subtraction technique, which is a CAD system, performs the subtraction operation between the current image and the previous image on the same patient, and supports observation by emphasizing the temporal changes. However, the temporal subtraction technique for 3D images, such as thoracic CT images, has not yet been established. There is a need to develop efficient and highly accurate 3D nonrigid registration techniques to reduce subtraction artifacts. This study aims to develop a 3D nonrigid registration technique to establish a 3D temporal subtraction technique. In particular, we focus on the Finite Element Method, which is versatile, applicable to a wide range of fields, and capable of handling any shape. Our new method was examined on 46 clinical cases with multidetector row computed tomography images. As a result, the proposed method improved by 6.93% (p = 3.0 × 10−6) compared to the conventional methods in terms of the rate of reduction of artifacts, and the effectiveness was verified. Therefore, this study contributes to the literature on early detection and treatment.
               ",autonomous vehicle
10.1016/B978-0-08-042234-3.50033-3,journal,Intelligent Components and Instruments for Control Applications 1994,sciencedirect,1994-12-31,sciencedirect,COMBINING NEURAL AND FUZZY TECHNIQUES IN MONITORING AND CONTROL OF MANUFACTURING PROCESSES,https://api.elsevier.com/content/article/pii/B9780080422343500333,"
               Key words. Intelligent manufacturing; manufacturing processes; monitoring; artificial intelligence; pattern recognition; learning systems; neural nets; fuzzy systems",autonomous vehicle
10.1016/j.measurement.2020.108747,journal,Measurement,sciencedirect,2021-02-28,sciencedirect,Physiological-signal-based emotion recognition: An odyssey from methodology to philosophy,https://api.elsevier.com/content/article/pii/S0263224120312471,"
                  Exploration on emotions continues from past to present. Nowadays, with the rapid advancement of intelligent technology, computer-aided emotion recognition using physiological signals has attracted increasing academic interests. Notwithstanding so, the research progress of this issue seemed to meet a bottleneck in the last decade without any substantial breakthrough. It naturally motivates us to thoroughly review and deeply rethink the current status and the future prospect of this issue. This work has provided a comprehensive overview and systematic taxonomy for physiological-signal-based emotion recognition. To do so, this work has made an unprecedented odyssey from signal properties through solution strategies to intrinsic principles. On the way, this paper has spared no effort to hunt the treasures about methodological essence and philosophical insight, draw the panorama of state-of-the-arts, and outlook the development of technologies. We expect all these discoveries and contributions can be beneficial for inspiring and guiding research innovations in the future.
               ",autonomous vehicle
10.1016/j.trit.2016.12.004,journal,CAAI Transactions on Intelligence Technology,sciencedirect,2016-10-31,sciencedirect,Recent Advances on Human-Computer Dialogue,https://api.elsevier.com/content/article/pii/S2468232216301081,"Human-Computer dialogue systems provide a natural language based interface between human and computers. They are widely demanded in network information services, intelligent accompanying robots, and so on. A Human-Computer dialogue system typically consists of three parts, namely Natural Language Understanding (NLU), Dialogue Management (DM) and Natural Language Generation (NLG). Each part has several different subtasks. Each subtask has been received lots of attentions, many improvements have been achieved on each subtask, respectively. But systems built in traditional pipeline way, where different subtasks are assembled sequently, suffered from some problems such as error accumulation and expanding, domain transferring. Therefore, researches on jointly modeling several subtasks in one part or cross different parts have been prompted greatly in recent years, especially the rapid developments on deep neural networks based joint models. There is even a few work aiming to integrate all subtasks of a dialogue system in a single model, namely end-to-end models. This paper introduces two basic frames of current dialogue systems and gives a brief survey on recent advances on variety subtasks at first, and then focuses on joint models for multiple subtasks of dialogues. We review several different joint models including integration of several subtasks inside NLU or NLG, jointly modeling cross NLG and DM, and jointly modeling through NLU, DM and NLG. Both advantages and problems of those joint models are discussed. We consider that the joint models, or end-to-end models, will be one important trend for developing Human-Computer dialogue systems.",autonomous vehicle
10.1016/j.techfore.2021.121177,journal,Technological Forecasting and Social Change,sciencedirect,2021-12-31,sciencedirect,Towards ESCO 4.0 – Is the European classification of skills in line with Industry 4.0? A text mining approach,https://api.elsevier.com/content/article/pii/S0040162521006107,"ESCO is a multilingual classification of Skills, Competences, Qualifications, and Occupations created by the European Commission to improve the supply of information on skills demand in the labour market. It is designed to assist individuals, employers, universities and training providers by giving them up to date and standardized information on skills. Rapid technological change means that ESCO needs to be updated in a timely manner. Evidence is presented here of how text-mining techniques can be applied to the analysis of data on emerging skill needs arising from Industry 4.0 to ensure that ESCO provides information which is current. The alignment between ESCO and Industry 4.0 technological trends is analysed. Using text mining techniques, information is extracted on Industry 4.0 technologies from: (i) two versions of ESCO (v1.0 - v1.1.); and (ii) from the 4.0 related scientific literature. These are then compared to identify potential data gaps in ESCO. The findings demonstrate that text mining applied on scientific literature to extract technology trends, can help policy makers to provide more up-to-date labour market intelligence.",autonomous vehicle
10.1016/j.compchemeng.2019.06.001,journal,Computers & Chemical Engineering,sciencedirect,2019-09-02,sciencedirect,Formation lithology classification using scalable gradient boosted decision trees,https://api.elsevier.com/content/article/pii/S0098135419302200,"
                  The classification of underground formation lithology is an important task in petroleum exploration and engineering since it forms the basis of geological research studies and reservoir parameter calculations. Hence, there have recently been increased efforts to automate lithology classification by incorporating various data science tools and principles. In this regard, efforts were made recently to evaluate machine learning methods to classify formation lithology by using data from the Daniudui gas field (DGF) and Hangjinqi gas field (HGF), both located in China. Although the boosted ensemble learners utilized in the studies performed well, there is still scope for improvement with respect to the prediction metrics. Additionally, the issue of scalability of some of these algorithms is also of concern. Hence, building upon the success of these algorithms in the previous studies, we tap into the state of the art of scalable ensemble decision tree algorithms, in our study. Specifically, we applied recently developed gradient boosted decision tree (GBDT) systems, namely, XGBoost, LightGBM and CatBoost, after combining well log data obtained from DGF and HGF. We compare their performance with random forests (RFs), AdaBoost and gradient boosting machines (GBMs) which serve as a baseline. We evaluated the algorithms using metrics such as the micro average, macro average and weighted average of precision (Pr), recall (Re) and F1-score (F1) on the test set after hyperparameter tuning. In our analysis, among the applied algorithms, we found that LightGBM possessed the highest metrics. Our work identifies LightGBM and CatBoost as good first-choice algorithms for the supervised classification of lithology when utilizing well log data.
               ",autonomous vehicle
10.1016/j.dsp.2021.103134,journal,Digital Signal Processing,sciencedirect,2021-06-15,sciencedirect,Towards goal-oriented semantic signal processing: Applications and future challenges,https://api.elsevier.com/content/article/pii/S1051200421001731,"
                  Advances in machine learning technology have enabled real-time extraction of semantic information in signals which can revolutionize signal processing techniques and improve their performance significantly for the next generation of applications. With the objective of a concrete representation and efficient processing of the semantic information, we propose and demonstrate a formal graph-based semantic language and a goal filtering method that enables goal-oriented signal processing. The proposed semantic signal processing framework can easily be tailored for specific applications and goals in a diverse range of signal processing applications. To illustrate its wide range of applicability, we investigate several use cases and provide details on how the proposed goal-oriented semantic signal processing framework can be customized. We also investigate and propose techniques for communications where sensor data is semantically processed and semantic information is exchanged across a sensor network.
               ",autonomous vehicle
10.1016/j.ins.2004.09.011,journal,Information Sciences,sciencedirect,2005-05-13,sciencedirect,On the design of intelligent robotic agents for assembly,https://api.elsevier.com/content/article/pii/S0020025504003159,"
                  Robotic agents can greatly be benefited from the integration of perceptual learning in order to monitor and adapt to changing environments. To be effective in complex unstructured environments, robots have to perceive the environment and adapt accordingly. In this paper it is discussed a biology inspired approach based on the adaptive resonance theory (ART) and implemented on an KUKA KR15 industrial robot during real-world operations (e.g. assembly operations). The approach intends to embed naturally the skill learning capability during manufacturing operations (i.e., within a flexible manufacturing system).
                  The integration of machine vision and force sensing has been useful to demonstrate the usefulness of the cognitive architecture to acquire knowledge and to effectively use it to improve its behaviour. Practical results are presented, showing that the robot is able to recognise a given component and to carry out the assembly. Adaptability is validated by using different component geometry during assemblies and also through skill learning which is shown by the robot’s dexterity.
               ",autonomous vehicle
10.1016/j.ipm.2019.102055,journal,Information Processing & Management,sciencedirect,2019-11-30,sciencedirect,The evolution of argumentation mining: From models to social media and emerging tools,https://api.elsevier.com/content/article/pii/S030645731930024X,"
                  Argumentation mining is a rising subject in the computational linguistics domain focusing on extracting structured arguments from natural text, often from unstructured or noisy text. The initial approaches on modeling arguments was aiming to identify a flawless argument on specific fields (Law, Scientific Papers) serving specific needs (completeness, effectiveness). With the emerge of Web 2.0 and the explosion in the use of social media both the diffusion of the data and the argument structure have changed. In this survey article, we bridge the gap between theoretical approaches of argumentation mining and pragmatic schemes that satisfy the needs of social media generated data, recognizing the need for adapting more flexible and expandable schemes, capable to adjust to the argumentation conditions that exist in social media. We review, compare, and classify existing approaches, techniques and tools, identifying the positive outcome of combining tasks and features, and eventually propose a conceptual architecture framework. The proposed theoretical framework is an argumentation mining scheme able to identify the distinct sub-tasks and capture the needs of social media text, revealing the need for adopting more flexible and extensible frameworks.
               ",autonomous vehicle
10.1016/B978-0-12-811153-6.00005-1,journal,eMaintenance,sciencedirect,2017-12-31,sciencedirect,Chapter 5: Diagnosis,https://api.elsevier.com/content/article/pii/B9780128111536000051,"
               Fault diagnosis is the process of tracing a fault by means of its symptoms, applying knowledge, and analyzing test results. Accurate diagnosis of faults in complex engineering systems requires acquiring the information through sensors, processing the information using advanced signal processing algorithms, and extracting required features for efficient classification or identification of faults. Identification of faults and subsequent remedial action can increase productivity and reduce maintenance costs in various industrial applications. Machine learning methods involving feature extraction, feature selection, and classification of faults offer a systematic approach to fault diagnosis and can be used in automated or unmanned environments. These are increasingly used in industrial sectors, such as manufacturing, automotive, marine, and aerospace, to maximize equipment uptime and minimize maintenance and operating costs.
            ",autonomous vehicle
10.1016/j.procs.2018.11.046,journal,Procedia Computer Science,sciencedirect,2018-12-31,sciencedirect,Metacognition for a Common Model of Cognition,https://api.elsevier.com/content/article/pii/S1877050918323329,"This paper provides a starting point for the development of metacognition in a common model of cognition. It identifies significant theoretical work on metacognition from multiple disciplines that the authors believe worthy of consideration. After first defining cognition and metacognition, we outline three general categories of metacognition, provide an initial list of its main components, consider the more difficult problem of consciousness, and present examples of prominent artificial systems that have implemented metacognitive components. Finally, we identify pressing design issues for the future.",autonomous vehicle
10.1016/j.engappai.2015.07.004,journal,Engineering Applications of Artificial Intelligence,sciencedirect,2015-10-31,sciencedirect,A cognitive robotic ecology approach to self-configuring and evolving AAL systems,https://api.elsevier.com/content/article/pii/S0952197615001517,"
                  Robotic ecologies are systems made out of several robotic devices, including mobile robots, wireless sensors and effectors embedded in everyday environments, where they cooperate to achieve complex tasks. This paper demonstrates how endowing robotic ecologies with information processing algorithms such as perception, learning, planning, and novelty detection can make these systems able to deliver modular, flexible, manageable and dependable Ambient Assisted Living (AAL) solutions. Specifically, we show how the integrated and self-organising cognitive solutions implemented within the EU project RUBICON (Robotic UBIquitous Cognitive Network) can reduce the need of costly pre-programming and maintenance of robotic ecologies. We illustrate how these solutions can be harnessed to (i) deliver a range of assistive services by coordinating the sensing & acting capabilities of heterogeneous devices, (ii) adapt and tune the overall behaviour of the ecology to the preferences and behaviour of its inhabitants, and also (iii) deal with novel events, due to the occurrence of new user׳s activities and changing user׳s habits.
               ",autonomous vehicle
10.1016/B978-0-323-85064-3.09998-2,journal,Image Processing for Automated Diagnosis of Cardiac Diseases,sciencedirect,2021-12-31,sciencedirect,Index,https://api.elsevier.com/content/article/pii/B9780323850643099982,Unknown,autonomous vehicle
10.1016/j.jss.2018.10.052,journal,Journal of Systems and Software,sciencedirect,2019-02-28,sciencedirect,An aggregated coupling measure for the analysis of object-oriented software systems,https://api.elsevier.com/content/article/pii/S0164121218302371,"
                  Coupling is a fundamental property of software systems which is strongly connected with the quality of software design and has high impact on program understanding. The coupling between software components influences software maintenance and evolution as well. In order to ease the maintenance and evolution processes it is essential to estimate the impact of changes made in the software system, coupling indicating such a possible impact. This paper introduces a new aggregated coupling measurement which captures both the structural and the conceptual characteristics of coupling between the software components. The proposed measure combines the textual information contained in the source code with the structural relationships between software components. We conduct several experiments which underline that the proposed aggregated coupling measure reveals new characteristics of coupling and is also effective for change impact analysis.
               ",autonomous vehicle
10.1016/B978-0-12-822260-7.20001-0,journal,Handbook of Computational Intelligence in Biomedical Engineering and Healthcare,sciencedirect,2021-12-31,sciencedirect,Index,https://api.elsevier.com/content/article/pii/B9780128222607200010,Unknown,autonomous vehicle
10.1016/B978-0-12-821259-2.00030-2,journal,Artificial Intelligence in Medicine,sciencedirect,2021-12-31,sciencedirect,Index,https://api.elsevier.com/content/article/pii/B9780128212592000302,Unknown,autonomous vehicle
10.1016/j.neucom.2019.12.020,journal,Neurocomputing,sciencedirect,2020-04-07,sciencedirect,Bi-Connect Net for salient object detection,https://api.elsevier.com/content/article/pii/S0925231219317229,"
                  As a challenging task for pixel-wise image analysis, salient object detection has made huge progress in recent years. However, there still exists a difficult problem: detection of distinguishing a salient and non-salient object in multiple objects under complex background (e.g. blur, translucent, light reflection, etc.). Our proposed method cast this difficulty as information dissolve problem in deep convolutional network, which is manifested as: first, the model cannot grab whole details of a salient object at training phrase; second, due to the isolation between layers and blocks, the valued information is blocked within a block, which leads to the difficulty in obtaining the position and the edge of salient objects simultaneously; third, the output of the network is a low-resolution saliency map, which cannot accurately express the edge of salient objects. To address information dissolve problems, we construct a Bi-Connect Net (BCN) composed of forward connection subnet and reverse side connection subnet. Besides, the proposed adaptive learning fusion method not only stress all blocks contribution but also combine multiple features with different scale, so that grab more details on the right salient location and precise edges at the same time. Extensive experiments show that our proposed Bi-Connect Net can outperform the state-of-the-art methods.
               ",autonomous vehicle
10.1016/j.eswa.2016.04.018,journal,Expert Systems with Applications,sciencedirect,2016-10-15,sciencedirect,Bio inspired computing – A review of algorithms and scope of applications,https://api.elsevier.com/content/article/pii/S095741741630183X,"
                  With the explosion of data generation, getting optimal solutions to data driven problems is increasingly becoming a challenge, if not impossible. It is increasingly being recognised that applications of intelligent bio-inspired algorithms are necessary for addressing highly complex problems to provide working solutions in time, especially with dynamic problem definitions, fluctuations in constraints, incomplete or imperfect information and limited computation capacity. More and more such intelligent algorithms are thus being explored for solving different complex problems. While some studies are exploring the application of these algorithms in a novel context, other studies are incrementally improving the algorithm itself. However, the fast growth in the domain makes researchers unaware of the progresses across different approaches and hence awareness across algorithms is increasingly reducing, due to which the literature on bio-inspired computing is skewed towards few algorithms only (like neural networks, genetic algorithms, particle swarm and ant colony optimization). To address this concern, we identify the popularly used algorithms within the domain of bio-inspired algorithms and discuss their principles, developments and scope of application. Specifically, we have discussed the neural networks, genetic algorithm, particle swarm, ant colony optimization, artificial bee colony, bacterial foraging, cuckoo search, firefly, leaping frog, bat algorithm, flower pollination and artificial plant optimization algorithm. Further objectives which could be addressed by these twelve algorithms have also be identified and discussed. This review would pave the path for future studies to choose algorithms based on fitment. We have also identified other bio-inspired algorithms, where there are a lot of scope in theory development and applications, due to the absence of significant literature.
               ",autonomous vehicle
10.1016/j.jmp.2008.12.007,journal,Journal of Mathematical Psychology,sciencedirect,2009-06-30,sciencedirect,Theoretical tools for understanding and aiding dynamic decision making,https://api.elsevier.com/content/article/pii/S002224960800117X,"
                  Dynamic decisions arise in many applications including military, medical, management, sports, and emergency situations. During the past 50 years, a variety of general and powerful tools have emerged for understanding, analyzing, and aiding humans faced with these decisions. These tools include expected and multi-attribute utility analyses, game theory, Bayesian inference and Bayes nets, decision trees and influence diagrams, stochastic optimal control theory, partially observable Markov decision processes, neural networks and reinforcement learning models, Markov logics, and rule-based cognitive architectures. What are all of these tools, how are they related, when are they most useful, and do these tools match the way humans make decisions? We address all of these questions within a broad overview that is written for an interdisciplinary audience. Each description of a tool introduces the principles upon which it is based, and also reviews empirical research designed to test whether humans actually use these principles to make decisions. We conclude with suggestions for future directions in research.
               ",autonomous vehicle
10.1016/j.neucom.2007.11.043,journal,Neurocomputing,sciencedirect,2008-08-31,sciencedirect,User and context adaptive neural networks for emotion recognition,https://api.elsevier.com/content/article/pii/S092523120800218X,"
                  Recognition of emotional states of users in human–computer interaction (HCI) has been shown to be highly dependent on individual human characteristics and way of behavior. Multimodality is a key issue in achieving more accurate results; however, fusing different modalities is a difficult issue in emotion analysis. Emotion recognition systems are generally either rule-based or extensively trained through emotionally colored HCI data sets. In either case, such systems need to take into account, i.e., adapt their knowledge to, the specific user or context of interaction. Neural networks fit well with the adaptation requirement, by collecting and analyzing data from specific environments. An effective approach is presented in this paper, which uses neural network architectures to both detect the need for adaptation of their knowledge, and adapt it through an efficient adaptation procedure. An experimental study with emotion datasets generate in the framework of the EC IST Humaine Network of Excellence.
               ",autonomous vehicle
10.1016/bs.mcps.2020.02.001,journal,Methods in Chemical Process Safety,sciencedirect,2020-12-31,sciencedirect,Chapter Three: Logic based methods for dynamic risk assessment,https://api.elsevier.com/content/article/pii/S2468651420300015,"
                  For the origin of logic based methods to assess process plant risks, we go back to the first symposium on Loss Prevention in the United Kingdom in 1971, where concepts in part developed for nuclear power plant risk assessment and extended to process plant have been presented. From there we follow the developments in the 1980s and the benchmarks in Europe in the 1990s revealing the large differences in outcomes when various teams work out the risks of a same plant. It showed the uncertainties intrinsic to the methodology of that time. The last 2 decades have seen various kinds of improvements but also awareness that other factors, such as human failure and organizational ones are important to include. The last subchapter is highlighting approaches partly based on machine learning and artificial intelligence that will make use of “big data,” even enabling dynamic operational risk assessment.
               ",autonomous vehicle
10.1016/j.jclepro.2021.127904,journal,Journal of Cleaner Production,sciencedirect,2021-09-10,sciencedirect,"Intelligent energy management: Evolving developments, current challenges, and research directions for sustainable future",https://api.elsevier.com/content/article/pii/S0959652621021223,"
                  In the last decade, there have been significant developments in the field of intelligent energy management systems (IEMSs), with various methods and new solutions proposed for managing the energy resources intelligently. An important issue related to finding the desired outcomes remains unexplored, i.e., how to determine key insights from the sparse academic literature in the age of digital publishing. To mitigate the issue, this study proposes a novel strategy to systematically survey the relevant studies by converting the sparse literature into visual presentations. We first apply a systematic approach called a PRISMA (Preferred Reporting Items for Systematic reviews and Meta-analyses) statement to provide the insights from the published literature of the past decade (2010–2020). Then, VOSviewer experiments are conducted to transform these sparse scholarly data into visual representations. In total, eighty-one papers published in high-impact journals are identified based on their scientific soundness and relevance, and a VOSviewer analysis is applied. The analysis revealed the existence of three research clusters focused on the following main thematic areas: the energy management in smart homes and smart grids (35 journal papers); the emerging concept of context-awareness (26 journal papers); and the role of privacy preservation in IEMSs (20 journal papers). This analysis uncovers the current state of IEMSs and explores existing issues, methods, findings, and gaps. Thus, future research directions have been recommended to fill the existing gaps. This systematic literature review is to assist both researchers and industry practitioners to understand the research gaps of previous studies.
               ",autonomous vehicle
10.1016/B0-08-043076-7/00619-7,journal,International Encyclopedia of the Social & Behavioral Sciences,sciencedirect,2001-12-31,sciencedirect,Statistical Pattern Recognition,https://api.elsevier.com/content/article/pii/B0080430767006197,"
               Statistical pattern recognition is concerned with the problem of designing machines that can classify complex patterns. Although statistical pattern recognition is typically viewed as a branch of Artificial Intelligence, such problems frequently arise in the social and behavioral sciences in the course of detecting complex structural relationships in large data sets. Statistical Pattern recognition problems also arise in the course of modeling complex social, behavioral, and neural systems. Most statistical pattern recognition systems consist of three major components. The first component is a feature selection and extraction stage where critical informational features about the data are identified for classification purposes. The second component is a probabilistic knowledge representation for representing the expected likelihood of particular features and the associated losses for making situation-specific decisions. The third component is a decision rule for making classification decisions which minimize an appropriate expected loss function. In many cases, a fourth component may also be required to estimate the probabilistic knowledge representation from ‘training data.’
            ",autonomous vehicle
10.1016/B978-0-08-097086-8.43021-7,journal,International Encyclopedia of the Social & Behavioral Sciences,sciencedirect,2015-12-31,sciencedirect,Cognitive Modeling: Connectionist Approaches,https://api.elsevier.com/content/article/pii/B9780080970868430217,"
               Connectionist approaches to cognitive modeling make use of large networks of simple computational units, which communicate by means of simple quantitative signals. Higher-level information processing emerges from the massively parallel interaction of these units by means of their connections, and a network may adapt its behavior by means of local changes in the strength of the connections. Connectionist approaches are related to neural networks and provide a distinct alternative to cognitive models inspired by the digital computer.
            ",autonomous vehicle
10.1016/j.neucom.2017.02.017,journal,Neurocomputing,sciencedirect,2017-05-24,sciencedirect,An enhanced fuzzy min–max neural network with ant colony optimization based-rule-extractor for decision making,https://api.elsevier.com/content/article/pii/S0925231217302874,"
                  This paper proposes an enhanced fuzzy min–max neural (EFMN) network model with ant colony optimization based-rule-extractor for grouping of the data-patterns and decision making by rule-list. There are many methods to extract the rules which are having less accuracy and with less performance. The earlier methods have drawbacks like they have not maintained the rule accuracy in terms of consistency. The proposed method has improved the accuracy, consistency and performance against existing methods. The number of rules obtained from this system is less in count and having higher rank. One of the strength of this method is that the rules obtained from the system are comprehensible, as they are in rule list format. This rule list is useful in decision making problem.
               ",autonomous vehicle
10.1016/0893-6080(95)00070-4,journal,Neural Networks,sciencedirect,1995-12-31,sciencedirect,pRAM nets for detection of small targets in sequences of infra-red images,https://api.elsevier.com/content/article/pii/0893608095000704,"
                  A probabilistic random access memory (pRAM) neural network is described for the classification of objects in a video sequence of FLIR (forward looking infra-red) images into two classes, target and clutter. The image sequences used for training and testing were gathered from real scenes. These sequences of frames were first passed through a hot-spot detection system which identified points in the image that have a high probability of corresponding to a target. Then feature extraction was done on the image patches surrounding these hot-spots using principal component analysis (PCA). These features served as input to a reinforcement learning pRAM net trained to produce values of (1 0) for targets and (0 1) for clutter. The experimental results have been promising, and on average, the network achieved a detection probability of 0.90 and 2–3 false alarms per flame in all training and test sets.
               ",autonomous vehicle
10.1016/j.neucom.2012.12.057,journal,Neurocomputing,sciencedirect,2014-02-11,sciencedirect,A multi-objective evolutionary algorithm-based ensemble optimizer for feature selection and classification with neural network models,https://api.elsevier.com/content/article/pii/S0925231213005353,"
                  In this paper, we propose a new multi-objective evolutionary algorithm-based ensemble optimizer coupled with neural network models for undertaking feature selection and classification problems. Specifically, the Modified micro Genetic Algorithm (MmGA) is used to form the ensemble optimizer. The aim of the MmGA-based ensemble optimizer is two-fold, i.e. to select a small number of input features for classification and to improve the classification performances of neural network models. To evaluate the effectiveness of the proposed system, a number of benchmark problems are first used, and the results are compared with those from other methods. The applicability of the proposed system to a human motion detection and classification task is then evaluated. The outcome positively demonstrates that the proposed MmGA-based ensemble optimizer is able to improve the classification performances of neural network models with a smaller number of input features.
               ",autonomous vehicle
10.1016/B978-0-12-818699-2.00016-0,journal,Hybrid Computational Intelligence,sciencedirect,2020-12-31,sciencedirect,Index,https://api.elsevier.com/content/article/pii/B9780128186992000160,Unknown,autonomous vehicle
10.1016/j.csl.2018.09.004,journal,Computer Speech & Language,sciencedirect,2019-05-31,sciencedirect,Overview of the sixth dialog system technology challenge: DSTC6,https://api.elsevier.com/content/article/pii/S0885230818300937,"
                  This paper describes the experimental setups and the evaluation results of the sixth Dialog System Technology Challenges (DSTC6) aiming to develop end-to-end dialogue systems. Neural network models have become a recent focus of investigation in dialogue technologies. Previous models required training data to be manually annotated with word meanings and dialogue states, but end-to-end neural network dialogue systems learn to directly output natural-language system responses without needing training data to be manually annotated. Thus, this approach allows us to scale up the size of training data and cover more dialog domains. In addition, dialogue systems require a meta-function to avoid deploying inappropriate responses generated by themselves. To challenge such issues, the DSTC6 consists of three tracks, (1). End-to-End Goal Oriented dialogue Learning to select system responses, (2). End-to-End Conversation Modeling to generate system responses using Natural Language Generation (NLG) and (3). Dialogue Breakdown Detection. Since each domain has different issues to be addressed to develop dialogue systems, we targeted restaurant retrieval dialogues to fill slot-value in Track 1, customer services on Twitter by combining goal-oriented dialogues and ChitChat in Track 2 and human-machine dialogue data for ChitChat in Track 3.
                  DSTC6 had 141 people declaring their interests and 23 teams submitted their final results. 18 scientific papers were presented in the wrap-up workshop. We find the blending end-to-end trainable models associated to meaningful prior knowledge performs the best for the restaurant retrieval for Track 1. Indeed, Hybrid Code Network and Memory Network have been the best models for this task. In Track 2, 78.5% of the system responses automatically generated by the best system were rated better than acceptable by humans and this achieves 89% of the number of the human responses rated in the same class. In Track3, the dialogue breakdown detection technologies performed as well as human agreements, in both data-sets of English and Japanese.
               ",autonomous vehicle
10.1016/B978-0-12-823519-5.00023-3,journal,Generative Adversarial Networks for Image-to-Image Translation,sciencedirect,2021-12-31,sciencedirect,Chapter 8: Visual similarity-based fashion recommendation system,https://api.elsevier.com/content/article/pii/B9780128235195000233,"
               Fashion recommendation systems are designed to retrieve a ranked list of catalog images similar to the target catalog image. These systems are crucial in helping customers move toward potential buying decisions from recommended products. Catching true correlation between similar product recommendations and user satisfaction will exponentially improve sale experience of companies. One of the major tasks in fashion recommendation systems is accurately finding similar images for a given product image.
               In this chapter, we outline a visual similarity-based fashion recommendation system that can be used in e-commerce especially for shoe and handbag recommendations. The system consists of several modules including a generative adversarial network (GAN) module for creating image representations (feature vectors) and an effective vector search library that acts as a database for querying similar images. We provide comparisons for our GAN-based recommendation results with pretrained convolutional neural networks (CNNs) on new catalog shoe and handbag images.
               Major contributions of this chapter can be summarized as follows: by utilizing GANs we are able to develop a robust and reliable fashion recommendation system, visual similarity-based recommendations are employed to create similar product search for e-commerce applications, and we also provide detailed comparisons for extracting high-quality, accurate, and efficient vector feature representation of product images using deep neural networks for fashion domain.
            ",autonomous vehicle
10.1016/j.jpdc.2020.05.004,journal,Journal of Parallel and Distributed Computing,sciencedirect,2020-09-30,sciencedirect,"Blockchain and AI amalgamation for energy cloud management: Challenges, solutions, and future directions",https://api.elsevier.com/content/article/pii/S074373152030277X,"
                  In the recent years, the Smart Grid (SG) system faces various challenges like the ever-increasing energy demand, the enormous growth of renewable energy sources (RES) with distributed energy generation (EG), the extensive Internet of Things (IoT) devices adaptation, the emerging security threats, and the foremost goal of sustaining the SG stability, efficiency and reliability. To cope up these issues there exists, the energy cloud management (ECM) system, which combines the infrastructure for energy, with intelligent energy usage and value-added services as per consumers demand. To achieve these, efficient demand-side forecasting and secure data transmission are the key factors. The energy management issues pose extreme gravity in finding sustainable solutions by using the blockchain (BC) and Artificial Intelligence (AI). AI-based techniques support various services such as energy load prediction, classification of the consumer, load management, and analysis where the BC provides data immutability and trust mechanism for secure energy management. Therefore, this paper reviews several existing AI-based approaches along with the advantages and challenges of integrating the BC technology and AI in the ECM system. We presented a decentralized AI-based ECM framework for energy management using BC and validate it using a case study. It is shown that how BC and AI can be used to mitigate ECM with security and privacy issues. Finally, we highlighted the open research issues and challenges of the BC-AI-based ECM system.
               ",autonomous vehicle
10.1016/j.plrev.2006.10.002,journal,Physics of Life Reviews,sciencedirect,2007-03-31,sciencedirect,Fundamentals of natural computing: an overview,https://api.elsevier.com/content/article/pii/S1571064506000315,"
                  Natural computing is a terminology introduced to encompass three classes of methods: (1) those that take inspiration from nature for the development of novel problem-solving techniques; (2) those that are based on the use of computers to synthesize natural phenomena; and (3) those that employ natural materials (e.g., molecules) to compute. The main fields of research that compose these three branches are the artificial neural networks, evolutionary algorithms, swarm intelligence, artificial immune systems, fractal geometry, artificial life, DNA computing, and quantum computing, among others. This paper provides an overview of the fundamentals of natural computing, particularly the fields listed above, emphasizing the biological motivation, some design principles, their scope of applications, current research trends and open problems. The presentation is concluded with a discussion about natural computing, and when it should be used.
               ",autonomous vehicle
10.1016/j.cogsys.2006.05.002,journal,Cognitive Systems Research,sciencedirect,2006-12-31,sciencedirect,Call for papers: 2007 International Joint Conference on Neural Networks (IJCNN 2007),https://api.elsevier.com/content/article/pii/S1389041706000283,,autonomous vehicle
10.1016/j.knosys.2021.107560,journal,Knowledge-Based Systems,sciencedirect,2021-12-25,sciencedirect,An ensemble filter-based heuristic approach for cancerous gene expression classification,https://api.elsevier.com/content/article/pii/S0950705121008224,"
                  Gene expression data of cancer has a huge feature set size, making its categorization a challenge for the existing classification methods. It contains redundancy, noise, and irrelevant genes. Therefore, feature selection/reduction plays a crucial role in the classification of such gene expression datasets. This work presents an ensemble of three filter methods, namely, Symmetrical Uncertainty (SU), chi square (
                        
                           
                              X
                           
                           
                              2
                           
                        
                     ), and Relief to reduce the feature dimensions by eliminating redundant and noisy genes. The present work designs a novel heuristic called Local Search-based Feature Selection (LSFS) that further reduces noise generated by the ensemble method. The resulting selected features are then optimized using a genetic algorithm. Afterwards, the optimal set of features is classified using three models; Support Vector Machine (SVM), k-NN (k-nearest neighbor), and Random Forest (RF) to find cancer relevant genes. Experiments are conducted using six benchmark datasets. The obtained results are compared with five state-of-the-art algorithms based on accuracy, sensitivity, specificity, F-measure, entropy, and precision. Additional experiments are carried out by manipulating the SVM kernel as a fitness value as well as using multiple distance measures and various values of k for k-NN. Prediction accuracy of the proposed system on the six benchmark datasets is 99%, 90%, 98%, 94%, 98%, and 99%. Significant outcomes obtained from experimental analysis indicate that the proposed approach improves classification of cancerous gene expression data and can be used as a practical tool for the analysis of gene expression data.
               ",autonomous vehicle
10.1016/j.neunet.2005.10.002,journal,Neural Networks,sciencedirect,2006-01-31,sciencedirect,"NEURAL NETWORKS: The Official Journal of the International Neural Network Society, European Neural Network Society, and Japanese Network Society",https://api.elsevier.com/content/article/pii/S0893608005002467,,autonomous vehicle
10.1016/B978-0-444-88740-5.50007-4,journal,Machine Intelligence and Pattern Recognition,sciencedirect,1991-12-31,sciencedirect,Links Between Artificial Neural Networks (ANN) and Statistical Pattern Recognition,https://api.elsevier.com/content/article/pii/B9780444887405500074,"
                  This chapter discusses the ways in which artificial neural networks (ANNs) differ from the well-known paradigms of statistical pattern recognition (SPR) and discusses whether there are any concepts in ANN for which no counterpart in SPR exists and vice-versa. The chapter explains the benefits that can come out of interaction between ANN and SPR researchers. It defines the advantages that ANN techniques have over SPR methods in dealing with real-world problems. For the most part, ANNs for pattern recognition are to SPR what poetry is to prose— technically a subset, based on the acceptance of certain design constraints of enormous practical utility, plus an additional source of inspiration that can be very helpful in the design process. Beyond that, the underlying concepts and paradigms are so close and the range of problems under study overlap to such a degree that greater mutual communication can minimize the duplication of effort and enrich both communities in many other ways. One area of substantive difference is that some parts of the ANN community treat pattern recognition as a subset of systems to perform dynamic system identification and control; this can lead to improvements in pattern recognition, in some cases, when the pattern to be recognized is essentially dynamic.
               ",autonomous vehicle
10.1016/j.techfore.2021.121193,journal,Technological Forecasting and Social Change,sciencedirect,2022-01-31,sciencedirect,The use of multi-criteria decision-making methods in business analytics: A comprehensive literature review,https://api.elsevier.com/content/article/pii/S0040162521006260,"
                  Business analytics (BA) systems are considered significant investments for enterprises because they have the potential to considerably improve firms’ performance. With the value offered by BA, companies are able to discover the hidden information in the data, improve decision-making processes, and support strategic planning. On the other hand, because there are multiple criteria and multiple alternatives involved in most decision-making situations, multi-criteria decision-making (MCDM) methods play an important role in BA practices. Providing inputs to the components of descriptive or predictive analytics or being used as a decision-making tool for evaluating the alternatives within prescriptive analytics exemplify the roles. Therefore, the use of hidden information discovered by business analytics and the need for utilizing the right MCDM method for optimal decision-making made these two concepts inseparable. In this paper, in order to review the use of MCDM methods in BA, the subject of BA is investigated from a taxonomical perspective (descriptive, predictive, and prescriptive), and its connection with MCDM techniques is revealed. Similarly, MCDM methods are studied using two main categories, multi-attribute decision making (MADM) and multi-objective decision making (MODM) methods. Furthermore, tabular and graphical analyses are also performed within the proposed review methodology. To the best of our knowledge, this review is the first attempt that holistically considers the use of MCDM methods in BA.
               ",autonomous vehicle
10.1016/j.comcom.2020.04.032,journal,Computer Communications,sciencedirect,2020-05-01,sciencedirect,An exploratory study of congestion control techniques in Wireless Sensor Networks,https://api.elsevier.com/content/article/pii/S0140366419320055,"
                  Congestion is one of the pervasive issues for Wireless Sensor Networks(WSNs) because of its bounded resources with respect to data processing, storage, transmission capacity and most importantly energy supply. A multitude of survey operations have been carried out over the previous decade to investigate and address multiple congestion-oriented problems and issues that are still unresolved. Various review articles published in the literature concentrated mainly on classical techniques for congestion control such as traffic-based, resource-based, and hybrid techniques, etc. In this survey article, an attempt has been made to present a systematic review of recent efforts assisted at refining the congestion control methodologies in WSNs by considering classical approaches as well as soft computing based approaches. It is a practicable idea to take a holistic view and study both approaches together. Finally, this article discusses design difficulties, various optimization models and future directions for the mechanism of congestion control in the domain of wireless sensor networks.
               ",autonomous vehicle
10.1016/S0165-1684(01)00185-2,journal,Signal Processing,sciencedirect,2002-04-30,sciencedirect,Neural methods for antenna array signal processing: a review,https://api.elsevier.com/content/article/pii/S0165168401001852,"
                  The neural method is a powerful nonlinear adaptive approach in various signal-processing scenarios. It is especially suitable for real-time application and hardware implementation. In this paper, we review its application in antenna array signal processing. This paper also serves as a tutorial to the neural method for antenna array signal processing.
               ",autonomous vehicle
10.1016/j.jmsy.2020.07.008,journal,Journal of Manufacturing Systems,sciencedirect,2020-07-31,sciencedirect,Towards multi-model approaches to predictive maintenance: A systematic literature survey on diagnostics and prognostics,https://api.elsevier.com/content/article/pii/S0278612520301187,"
                  The use of a modern technological system requires a good engineering approach, optimized operations, and proper maintenance in order to keep the system in an optimal state. Predictive maintenance focuses on the organization of maintenance actions according to the actual health state of the system, aiming at giving a precise indication of when a maintenance intervention will be necessary. Predictive maintenance is normally implemented by means of specialized computational systems that incorporate one of several models to fulfil diagnostics and prognostics tasks. As complexity of technological systems increases over time, single-model approaches hardly fulfil all functions and objectives for predictive maintenance systems. It is increasingly common to find research studies that combine different models in multi-model approaches to overcome complexity of predictive maintenance tasks, considering the advantages and disadvantages of each single model and trying to combine the best of them. These multi-model approaches have not been extensively addressed by previous review studies on predictive maintenance. Besides, many of the possible combinations for multi-model approaches remain unexplored in predictive maintenance applications; this offers a vast field of opportunities when architecting new predictive maintenance systems. This systematic survey aims at presenting the current trends in diagnostics and prognostics giving special attention to multi-model approaches and summarizing the current challenges and research opportunities.
               ",autonomous vehicle
10.1016/B978-0-12-821929-4.20001-2,journal,Machine Learning Guide for Oil and Gas Using Python,sciencedirect,2021-12-31,sciencedirect,Index,https://api.elsevier.com/content/article/pii/B9780128219294200012,Unknown,autonomous vehicle
10.1016/j.asoc.2018.06.032,journal,Applied Soft Computing,sciencedirect,2018-09-30,sciencedirect,A Mahalanobis distance based algorithm for assigning rank to the predicted fault prone software modules,https://api.elsevier.com/content/article/pii/S156849461830365X,"
                  This article proposes a methodology based on Artificial Neural Network(ANN) and type-2 fuzzy logic system (FLS) for detecting the fault prone software modules at early development phase. The present research concentrates on software metrics from requirement analysis and design phase of software life cycle. A new approach has been developed to sort out degree of fault proneness (DFP) of the software modules through type-2 FLS. ANN is used to prepare the rule base for inference engine. Furthermore, the proposed model has induced an order relation among the fault prone modules (FPMs) with the help of Mahalanobis distance (MD) metric. During software development process, a project manager needs to recognize the fault prone software modules with their DFP. Hence, the present study is of great importance to the project personnel to develop more cost-effective and reliable software. KC2 dataset of NASA has been applied for validating the model. Performance analysis clearly indicates the better prediction capability of the proposed model compared to some existing similar models.
               ",autonomous vehicle
10.1016/B978-0-12-821379-7.00011-4,journal,Practical Machine Learning for Data Analysis Using Python,sciencedirect,2020-12-31,sciencedirect,Index,https://api.elsevier.com/content/article/pii/B9780128213797000114,Unknown,autonomous vehicle
10.1016/j.ins.2017.08.005,journal,Information Sciences,sciencedirect,2017-12-31,sciencedirect,QUAT-DEM: Quaternion-DEMATEL based neural model for mutual coordination between UAVs,https://api.elsevier.com/content/article/pii/S0020025517308617,"
                  Networking between autonomously flying aerial vehicles requires efficient control and sustainable path for data transmission. Aerial nodes are extremely dynamic and provide a vast range of applications especially focusing on surveillance and data acquisition. However, these dynamic nodes are prone to unstable connectivity which hinders network operations, and also decreases the network lifetime. Such issues directly influence the control and relaying over aerial vehicles. Thus, it becomes essential to have an efficient strategy for controller selection and autonomous relaying. In this paper, a quaternion based neural model is proposed, which uses Decision Making Trial and Evaluation Laboratory Technique (DEMATEL) for the selection of network controllers and relays. The proposed algorithms are capable of selecting controller with a complexity O(n), and provide cooperative relaying with a complexity O(n × k). The effectiveness of the proposed model is demonstrated by using simulations. The results show that the proposed approach reduces delays by 25% during the selection of controller, and increases the transfer speed by 20% compared to existing approaches.
               ",autonomous vehicle
10.1016/B978-0-12-401678-1.00007-5,journal,Methods in Biomedical Informatics,sciencedirect,2014-12-31,sciencedirect,Chapter 7: Knowledge Discovery in Biomedical Data: Theory and Methods,https://api.elsevier.com/content/article/pii/B9780124016781000075,"
               As increasing amounts and types of biomedical data are collected, there is a corresponding need to identify patterns in these data to assist researchers, to monitor the health of individuals and populations, and for administrative purposes. The identification of these patterns is often referred to as “data mining,” or more broadly, “knowledge discovery in databases,” or KDD. KDD is an evolutionary process, with its own lifecycle and is a means to an end, not an end in and of itself. Numerous tools exist for this endeavor, many coming from statistics and machine learning. Statistical methods include descriptive statistics as well as multivariable models and statistical classifiers. Machine learning methods include decision tree induction, rule discovery algorithms, and naturally-inspired methods such as those used in the field of evolutionary computation. In addition, the importance of domain experts in the KDD process cannot be overestimated.
            ",autonomous vehicle
10.1016/j.compind.2018.03.025,journal,Computers in Industry,sciencedirect,2018-08-31,sciencedirect,A novel Big Data analytics and intelligent technique to predict driver's intent,https://api.elsevier.com/content/article/pii/S0166361517303640,"
                  Modern age offers a great potential for automatically predicting the driver's intent through the increasing miniaturization of computing technologies, rapid advancements in communication technologies and continuous connectivity of heterogeneous smart objects. Inside the cabin and engine of modern cars, dedicated computer systems need to possess the ability to exploit the wealth of information generated by heterogeneous data sources with different contextual and conceptual representations. Processing and utilizing this diverse and voluminous data, involves many challenges concerning the design of the computational technique used to perform this task. In this paper, we investigate the various data sources available in the car and the surrounding environment, which can be utilized as inputs in order to predict driver's intent and behavior. As part of investigating these potential data sources, we conducted experiments on e-calendars for a large number of employees, and have reviewed a number of available geo referencing systems. Through the results of a statistical analysis and by computing location recognition accuracy results, we explored in detail the potential utilization of calendar location data to detect the driver's intentions. In order to exploit the numerous diverse data inputs available in modern vehicles, we investigate the suitability of different Computational Intelligence (CI) techniques, and propose a novel fuzzy computational modelling methodology. Finally, we outline the impact of applying advanced CI and Big Data analytics techniques in modern vehicles on the driver and society in general, and discuss ethical and legal issues arising from the deployment of intelligent self-learning cars.
               ",autonomous vehicle
10.1016/j.ensm.2021.01.007,journal,Energy Storage Materials,sciencedirect,2021-04-30,sciencedirect,Designed high-performance lithium-ion battery electrodes using a novel hybrid model-data driven approach,https://api.elsevier.com/content/article/pii/S2405829721000088,"
                  Lithium-ion batteries (LIBs) have been widely recognized as the most promising energy storage technology due to their favorable power and energy densities for applications in electric vehicles (EVs) and other related functions. However, further improvements are needed which are underpinned by advances in conventional electrode designs. This paper reviews conventional and emerging electrode designs, including conventional LIB electrode modification techniques and electrode design for next-generation energy devices. Thick electrode designs with low tortuosity are the most conventional approach for energy density improvement. Chemistries such as lithium-sulfur, lithium-air and solid-state batteries show great potential, yet many challenges remain. Microscale structural modelling and macroscale functional modelling methods underpin much of the electrode design work and these efforts are summarized here. More importantly, this paper presents a novel framework for next-generation electrode design termed: Cyber Hierarchy And Interactional Network based Multiscale Electrode Design (CHAIN-MED), a hybrid solution combining model-based and data-driven techniques for optimal electrode design, which significantly shortens the development cycle. This review, therefore, provides novel insights into combining existing design approaches with multiscale models and machine learning techniques for next-generation LIB electrodes.
               ",autonomous vehicle
10.1016/j.petrol.2018.09.031,journal,Journal of Petroleum Science and Engineering,sciencedirect,2019-01-31,sciencedirect,"Novel statistical forecasting models for crude oil price, gas price, and interest rate based on meta-heuristic bat algorithm",https://api.elsevier.com/content/article/pii/S0920410518307861,"
                  Investment in the petroleum industry is usually faced with a high degree of risk due to uncertainty associated with economic factors. Typical factors include oil and gas price, interest rate, operational and capital expenditure. In addition, the investment risk increases as offshore exploration, drilling and production activities increase. Therefore, accurate prediction of economic factors is crucial in an upstream oil and gas sector in order to make better strategic decisions with minimized risk.
                  In the present study, four methods of the least square support vector machine (LSSVM), genetic programming (GP), artificial neural network (ANN), and auto-regressive integrated moving average (ARIMA) were initially used to forecast monthly oil price (MOP), daily gas price (DGP), and annual interest rate (AIR). Next, the meta-heuristic bat algorithm (BA) was applied in order to optimally combine the four mentioned forecasting methods in an integrated equation as a novel approach. All required historical data to forecast oil price, gas price and interest rate were collected from the Central Bank of the Islamic Republic of Iran.
                  Error analysis in terms of coefficient of determination (R2), average absolute relative error percentage (AAREP), root-mean square error (RMSE), and cumulative probability distribution versus absolute relative error percentage were used to compare the prediction performance of forecasting methods.
                  Error analysis proves that the BA optimized method is superior over all other forecasting methods in terms of highest R2 and lowest RMSE. After the BA optimized method, construction of LSSVM, ARIMA, ANN, and GP has better prediction ability, respectively. The results indicate that the BA optimized method reduces RMSE at least by 6.61% in MOP forecast; by 18.33% in DGP forecast; and by 23.13% in AIR forecast over all other forecasting methods.
               ",autonomous vehicle
10.1016/B978-0-323-85380-4.00021-X,journal,Intelligence Science,sciencedirect,2021-12-31,sciencedirect,Index,https://api.elsevier.com/content/article/pii/B978032385380400021X,Unknown,autonomous vehicle
10.1016/S0950-7051(98)00066-5,journal,Knowledge-Based Systems,sciencedirect,1998-11-23,sciencedirect,The omnipresence of case-based reasoning in science and application,https://api.elsevier.com/content/article/pii/S0950705198000665,"
                  A surprisingly large number of research disciplines have contributed towards the development of knowledge on lazy problem solving, which is characterized by its storage of ground cases and its demand-driven response to queries. Case-based reasoning (CBR) is an alternative, increasingly popular approach for designing expert systems that implements this approach. This paper lists pointers to some contributions in some related disciplines that offer insights for CBR research. We then outline a small number of Navy applications based on this approach that demonstrate its breadth of applicability. Finally, we list a few successful and failed attempts to apply CBR, and list some predictions on the future roles of CBR in applications.
               ",autonomous vehicle
10.1016/B978-0-323-90184-0.09992-6,journal,Deep Learning for Chest Radiographs,sciencedirect,2021-12-31,sciencedirect,Index,https://api.elsevier.com/content/article/pii/B9780323901840099926,Unknown,autonomous vehicle
10.1016/j.net.2018.03.010,journal,Nuclear Engineering and Technology,sciencedirect,2018-05-31,sciencedirect,An accident diagnosis algorithm using long short-term memory,https://api.elsevier.com/content/article/pii/S1738573318300950,"Accident diagnosis is one of the complex tasks for nuclear power plant (NPP) operators. In abnormal or emergency situations, the diagnostic activity of the NPP states is burdensome though necessary. Numerous computer-based methods and operator support systems have been suggested to address this problem. Among them, the recurrent neural network (RNN) has performed well at analyzing time series data. This study proposes an algorithm for accident diagnosis using long short-term memory (LSTM), which is a kind of RNN, which improves the limitation for time reflection. The algorithm consists of preprocessing, the LSTM network, and postprocessing. In the LSTM-based algorithm, preprocessed input variables are calculated to output the accident diagnosis results. The outputs are also postprocessed using softmax to determine the ranking of accident diagnosis results with probabilities. This algorithm was trained using a compact nuclear simulator for several accidents: a loss of coolant accident, a steam generator tube rupture, and a main steam line break. The trained algorithm was also tested to demonstrate the feasibility of diagnosing NPP accidents.",autonomous vehicle
10.1016/j.compchemeng.2020.106881,journal,Computers & Chemical Engineering,sciencedirect,2020-09-02,sciencedirect,"Considerations, challenges and opportunities when developing data-driven models for process manufacturing systems",https://api.elsevier.com/content/article/pii/S0098135419308373,"The increasing availability of data, due to the adoption of low-cost industrial internet of things technologies, coupled with increasing processing power from cloud computing, is fuelling increase use of data-driven models in manufacturing. Utilising case studies from the food and drink industry and waste management industry, the considerations and challenges faced when developing data-driven models for manufacturing systems are explored. Ensuring a high-quality set of model development data that accurately represents the manufacturing system is key to the successful development of a data-driven model. The cross-industry standard process for data mining (CRISP-DM) framework is used to provide a reference at to what stage process manufacturers will face unique considerations and challenges when developing a data-driven model. This paper then explores how data-driven models can be utilised to characterise process streams and support the implementation of the circular economy principals, process resilience and waste valorisation.",autonomous vehicle
10.1016/j.neunet.2013.01.011,journal,Neural Networks,sciencedirect,2013-07-31,sciencedirect,Investigating the computational power of spiking neurons with non-standard behaviors,https://api.elsevier.com/content/article/pii/S0893608013000154,"
                  Spiking neural networks have been called the third generation of neural networks. Their main difference with respect to the previous two generations is the use of realistic neuron models. Their computational power has been well studied with respect to threshold gates and sigmoidal neurons. However, biologically realistic models of spiking neurons can produce behaviors that can be computationally relevant, but their power has not been assessed in the same way. This paper studies the computational power of neurons with different behaviors based on the previous analyses conducted by Maass and Schmitt. The studied behaviors are rebound spiking, resonance and bursting. The results of the analysis are presented. A theoretical motivation for this study is presented and a discussion is done on the possible implications of the findings for using networks of spiking neurons for performing computations.
               ",autonomous vehicle
10.1016/j.rcim.2014.08.013,journal,Robotics and Computer-Integrated Manufacturing,sciencedirect,2015-06-30,sciencedirect,On-line knowledge acquisition and enhancement in robotic assembly tasks,https://api.elsevier.com/content/article/pii/S073658451400074X,"
                  Industrial robots are reliable machines for manufacturing tasks such as welding, panting, assembly, palletizing or kitting operations. They are traditionally programmed by an operator using a teach pendant in a point-to-point scheme with limited sensing capabilities such as industrial vision systems and force/torque sensing. The use of these sensing capabilities is associated to the particular robot controller, operative systems and programming language. Today, robots can react to environment changes specific to their task domain but are still unable to learn skills to effectively use their current knowledge. The need for such a skill in unstructured environments where knowledge can be acquired and enhanced is desirable so that robots can effectively interact in multimodal real-world scenarios.
                  In this article we present a multimodal assembly controller (MAC) approach to embed and effectively enhance knowledge into industrial robots working in multimodal manufacturing scenarios such as assembly during kitting operations with varying shapes and tolerances. During learning, the robot uses its vision and force capabilities resembling a human operator carrying out the same operation. The approach consists of using a MAC based on the Fuzzy ARTMAP artificial neural network in conjunction with a knowledge base. The robot starts the operation having limited initial knowledge about what task it has to accomplish. During the operation, the robot learns the skill for recognising assembly parts and how to assemble them. The skill acquisition is evaluated by counting the steps to complete the assembly, length of the followed assembly path and compliant behaviour. The performance improves with time so that the robot becomes an expert demonstrated by the assembly of a kit with different part geometries. The kit is unknown by the robot at the beginning of the operation; therefore, the kit type, location and orientation are unknown as well as the parts to be assembled since they are randomly fed by a conveyor belt.
               ",autonomous vehicle
10.1016/0303-2647(89)90024-5,journal,Biosystems,sciencedirect,1989-12-31,sciencedirect,Towards an artificial brain,https://api.elsevier.com/content/article/pii/0303264789900245,"
                  Three components of a brain model operating on neuromolecular computing principles are described. The first component comprises neurons whose input-output behavior is controlled by significant internal dynamics. Models of discrete enzymatic neurons, reaction-diffusion neurons operating on the basis of the cyclic nucleotide cascade, and neurons controlled by cytoskeletal dynamics are described. The second component of the model is an evolutionary learning algorithm which is used to mold the behavior of enzyme-driven neurons or small networks of these neurons for specific function, usually pattern recognition or target seeking tasks. The evolutionary learning algorithm may be interpreted either as representing the mechanism of variation and natural selection acting on a phylogenetic time scale, or as a conceivable outogenetic adaptation mechanism. The third component of the model is a memory manipulation scheme, called the reference neuron scheme. In principle it is capable of orchestrating a repertoire of enzyme-driven neurons for coherent function. The existing implementations, however, utilize simple neurons without internal dynamics. Spatial navigation and simple game playing (using tic-tac-toe) provide the task environments that have been used to study the properties of the reference neuron model. A memory-based evolutionary learning algorithm has been developed that can assign credit to the individual neurons in a network. It has been run on standard benchmark tasks, and appears to be quite effective both for conventional neural nets and for networks of discrete enzymatic neurons. The models have the character of artificial worlds in that they map the hierarchy of processes in the brain (at the molecular, neuronal, and network levels), provide a task environment, and use this relatively self-contained setup to develop and evaluate learning and adaptation algorithms.
               ",autonomous vehicle
10.1016/B978-0-323-85769-7.00024-0,journal,Cognitive Computing for Human-Robot Interaction,sciencedirect,2021-12-31,sciencedirect,Index,https://api.elsevier.com/content/article/pii/B9780323857697000240,Unknown,autonomous vehicle
10.1016/j.rser.2018.03.096,journal,Renewable and Sustainable Energy Reviews,sciencedirect,2018-08-31,sciencedirect,Analysis of different combinations of meteorological parameters in predicting the horizontal global solar radiation with ANN approach: A case study,https://api.elsevier.com/content/article/pii/S1364032118301953,"
                  Solar energy is a clean renewable energy source and availability of solar resources at a particular location depends on the local meteorological parameters. In the present study, prediction models using artificial neural networks (ANN) are developed by varying the meteorological parameters from one to six. A two year database of daily global solar radiation (GSR), daily minimum temperature (Tmin), daily maximum temperature (Tmax), difference of daily maximum and minimum temperature (DT), sunshine hours (S), theoretical sunshine hours (So) and extraterrestrial radiation (Ho) have been used to train the ANN. Six ANN models are developed (ANN-1 to ANN-6) with 32 possible combinations of inputs and are used to train the network to identify the best combination of inputs to estimate the monthly mean daily GSR accurately. All the models are validated and the performance of the models are analyzed by using the statistical tools.
                  Out of the six ANN models with all the possible combination of input variables, ANN-2 and ANN-3 have given best prediction with the combinations of [DT, Ho] and [DT, Ho, So] respectively. The statistical tool Relative Root Mean Square Error (RRMSE) showed the least value of 3.96% with [DT, Ho] inputs. The ANN-1 trained with calculated approximate sunshine hours (Sa) has also shown high prediction accuracy. Sunshine based model and temperature based model are validated with ANN-1, ANN-2 and ANN-3 architectures. Results showed that the developed ANN models outperform the considered empirical models. The combination of [So, Ho] has produced excellent estimation, which are theoretical parameters and does not require any measured meteorological parameters. Superior performance is observed with less number of inputs which are readily available for any location.
               ",autonomous vehicle
10.1016/j.measurement.2015.02.001,journal,Measurement,sciencedirect,2015-05-31,sciencedirect,Survey on Neuro-Fuzzy systems and their applications in technical diagnostics and measurement,https://api.elsevier.com/content/article/pii/S0263224115000585,"
                  Both fuzzy logic, as the basis of many inference systems, and Neural Networks, as a powerful computational model for classification and estimation, have been used in many application fields since their birth. These two techniques are somewhat supplementary to each other in a way that what one is lacking of the other can provide. This led to the creation of Neuro-Fuzzy systems which utilize fuzzy logic to construct a complex model by extending the capabilities of Artificial Neural Networks. Generally speaking all type of systems that integrate these two techniques can be called Neuro-Fuzzy systems. Key feature of these systems is that they use input–output patterns to adjust the fuzzy sets and rules inside the model. The paper reviews the principles of a Neuro-Fuzzy system and the key methods presented in this field, furthermore provides survey on their applications for technical diagnostics and measurement.
               ",autonomous vehicle
10.1016/j.engappai.2018.06.012,journal,Engineering Applications of Artificial Intelligence,sciencedirect,2018-09-30,sciencedirect,The model of an anomaly detector for HiLumi LHC magnets based on Recurrent Neural Networks and adaptive quantization,https://api.elsevier.com/content/article/pii/S095219761830143X,"This paper focuses on an examination of an applicability of Recurrent Neural Network models for detecting anomalous behavior of the CERN superconducting magnets. In order to conduct the experiments, the authors designed and implemented an adaptive signal quantization algorithm and a custom Gated Recurrent Unit-based detector and developed a method for the detector parameters selection. Three different datasets were used for testing the detector. Two artificially generated datasets were used to assess the raw performance of the system whereas the dataset intended for real-life experiments and model training was composed of the signals acquired from a new type of magnet, to be used during High-Luminosity Large Hadron Collider project. Several different setups of the developed anomaly detection system were evaluated and compared with state-of-the-art One Class Support Vector Machine (OC-SVM) reference model operating on the same data. The OC-SVM model was equipped with a rich set of feature extractors accounting for a range of the input signal properties. It was determined in the course of the experiments that the detector, along with its supporting design methodology, reaches F1 equal or very close to 1 for almost all test sets. Due to the profile of the data, the setup with the lowest maximum false anomaly length of the detector turned out to perform the best among all five tested configuration schemes of the detection system. The quantization parameters have the biggest impact on the overall performance of the detector with the best values of input/output grid equal to 16 and 8, respectively. The proposed solution of the detection significantly outperformed OC-SVM-based detector in most of the cases, with much more stable performance across all the datasets.",autonomous vehicle
10.1016/j.bica.2012.04.001,journal,Biologically Inspired Cognitive Architectures,sciencedirect,2012-07-31,sciencedirect,"Global Workspace Theory, its LIDA model and the underlying neuroscience",https://api.elsevier.com/content/article/pii/S2212683X12000060,"
                  A biologically inspired cognitive architecture must draw its insights from what is known from animal (including human) cognition. Such architectures should faithfully model the high-level modules and processes of cognitive neuroscience. Also, biologically inspired cognitive architectures are expected to contribute to the BICA “challenge of creating a real-life computational equivalent of the human mind”. One unified theory of cognition, Global Workspace Theory (GWT) has emerged as the most widely accepted, empirically supported theory of the role of consciousness in cognition. Recent experimental studies reveal rich cortical connectivity capable of supporting a large-scale dynamic network. We propose that brains in fact cyclically and dynamically form such a network according to GWT. The biologically inspired LIDA cognitive architecture implements GWT conceptually and computationally. Here we argue that the LIDA architecture’s breadth, flexible motivations using feelings, explicit attention mechanism, and continual, incremental and online learning in several modalities provide a significant first step in the direction of the BICA challenge. We also measure LIDA against the architectural features listed in the BICA Table of Implemented Cognitive Architectures. Applying recent brain connectivity results, we go on to elucidate the relationship between LIDA and the underlying and motivating neuroscience, using the language of non-linear dynamics. In particular, we claim that LIDA’s representations correspond to basins of attraction in the non-linear dynamics of neural activation patterns. In addition, we claim that the rhythms of LIDA’s cognitive cycle and of its internal cognitive elements have definite psychophysiological corollaries in the oscillatory patterns observed in the human brain.
               ",autonomous vehicle
10.1016/B978-0-12-815010-8.10000-6,journal,Ihorizon-Enabled Energy Management for Electrified Vehicles,sciencedirect,2019-12-31,sciencedirect,Appendix,https://api.elsevier.com/content/article/pii/B9780128150108100006,Unknown,autonomous vehicle
10.1016/B978-0-12-814391-9.10000-7,journal,Intelligent Data Mining and Fusion Systems in Agriculture,sciencedirect,2020-12-31,sciencedirect,Index,https://api.elsevier.com/content/article/pii/B9780128143919100007,Unknown,autonomous vehicle
10.3182/20100826-3-TR-4015.00062,journal,IFAC Proceedings Volumes,sciencedirect,2010-12-31,sciencedirect,Direct Policy Search Method in Fault Tolerant Autonomous Systems,https://api.elsevier.com/content/article/pii/S1474667015323880,"
                  This work is concerned with a new type of realtime reconfigurable control systems that is based on the use of a multi-agent system. To this end, two stages have been examined in the context of decision making; the fault detection and identification (FDI) stage and the reconfiguration stage (RC). The agent based FDI detects that a fault has occurred. It then further diagnoses the situation. The RC stage follows this by adapting or changing the control architecture to accommodate the fault. The agent based problem is to synchronize or integrate these two stages in the overall structure of a control system on real world applications. The multi-agent architecture proposed in this paper has several advantages in terms of modularity, reliability, ability to learn and achieve overall higher robustness over past “single software” methods. Specifically, this paper concentrates on one of the agents introduced, namely the Reconfiguration agent. Tests on a simulation of an example system have been carried out to demonstrate this new organisation of reconfigurable control systems.
               ",autonomous vehicle
10.1016/j.ifacol.2020.12.276,journal,IFAC-PapersOnLine,sciencedirect,2020-12-31,sciencedirect,Two-Stage Robot Controller Auto-Tuning Methodology for Trajectory Tracking Applications,https://api.elsevier.com/content/article/pii/S2405896320305541,"
                  Autonomy is increasingly demanded of industrial manipulators. Robots have to be capable of regulating their behavior to different operational conditions, without requiring high time/resource-consuming human intervention. Achieving an automated tuning of the control parameters of a manipulator is still a challenging task. This paper addresses the problem of automated tuning of the manipulator controller for trajectory tracking. A Bayesian optimization algorithm is proposed to tune firstly the low-level controller parameters (i.e., robot dynamics compensation), then the high-level controller parameters (i.e., the joint PID gains), providing a two-stage robot controller auto-tuning methodology. In both the optimization phases, the algorithm adapts the control parameters through a data-driven procedure, optimizing a user-defined trajectory tracking cost. Safety constraints ensuring, e.g., closed-loop stability and bounds on the maximum joint position errors, are also included. The performance of the proposed approach is demonstrated on a torque-controlled 7-degree-of-freedom FRANKA Emika robot manipulator. The 4 robot dynamics parameters (i.e., 4 link-mass parameters) are tuned in 40 iterations, while the robot control parameters (i.e., 21 PID gains) are tuned in 90 iterations. Comparable trajectory tracking-errors results with respect to the FRANKA Emika embedded position controller are achieved.
               ",autonomous vehicle
10.1016/j.promfg.2021.06.064,journal,Procedia Manufacturing,sciencedirect,2021-12-31,sciencedirect,Random forest regression for predicting an anomalous condition on a UR10 cobot end-effector from purposeful failure data,https://api.elsevier.com/content/article/pii/S2351978921000755,"Unexpected downtime from equipment failure has increased due to a production line’s mechanization to meet production throughput requirements. Manufacturing equipment requires accurate prediction models for determining future failure probability in maintenance scheduling. This paper explores using generated failure data under contrived failure scenarios in training a model for a robot with different combinations of data features. Failure data are generated by inducing an anomalous state in the robot arm. The anomalous state is created by attaching weights at the robot end-effector. A random forest regression model diagnoses the anomalous state and determines the anomalous state progression after gathering data. Three different regression models were trained to test accuracy based on different feature selections. The random forest regression predicted 92% of the robot joint operations through five-fold cross-validation, an anomaly in a robot joint 99% of the time, and the correct anomaly state-based on the confusion matrix, 85% of the time. In future research, the anomalous state will represent more targeted component failures on the system through purposeful permanent damage of the robots’ components. Future datasets generated will train other machine health algorithms for estimating component and system damage.",autonomous vehicle
10.1016/B978-0-12-819620-5.09990-2,journal,"Implementing Biomedical Innovations into Health, Education, and Practice",sciencedirect,2020-12-31,sciencedirect,Index,https://api.elsevier.com/content/article/pii/B9780128196205099902,Unknown,autonomous vehicle
10.1016/j.jconrel.2020.04.050,journal,Journal of Controlled Release,sciencedirect,2020-08-10,sciencedirect,Towards the quantum-enabled technologies for development of drugs or delivery systems,https://api.elsevier.com/content/article/pii/S0168365920302649,"
                  Enormous advances in technology and science have provided outstanding innovations including the development of quantum computers (QCs) capable of performing various tasks much more efficiently and quickly than the classical computers. Integrating and analyzing gigantic amounts of data, ultra-rapid calculations, solving intractable problems, secure communications, providing novel insights into the material design or biosystems, advanced simulations, rapid genome analysis and sequencing, early cancer detection, identifying novel drug applications, accelerated discovery of new molecules, targets, or theranostic agents and evaluation of their behaviors, and acquiring a deeper knowledge about the complex data patterns, formation of proteins, or mechanism of disease progression and evolution by QCs may indeed revolutionize conventional technologies and strategies. Application of quantum computing and machine learning for accelerated analysis of the biological or medical data, uncovering the mechanisms of chemical reactions or action of drug candidates, and creation of patient-specific treatment strategies using genomics data can result in the development of more effective and less toxic drugs or personalized therapy. This article highlights the importance of QCs in designing drugs and delivery systems, limitations, and possible solutions.
               ",autonomous vehicle
10.1016/B978-0-12-822226-3.20001-9,journal,Trends in Deep Learning Methodologies,sciencedirect,2021-12-31,sciencedirect,Index,https://api.elsevier.com/content/article/pii/B9780128222263200019,Unknown,autonomous vehicle
10.1016/j.robot.2019.103346,journal,Robotics and Autonomous Systems,sciencedirect,2020-02-29,sciencedirect,Waterline and obstacle detection in images from low-cost autonomous boats for environmental monitoring,https://api.elsevier.com/content/article/pii/S0921889019302775,"
                  Waterline detection from images taken by cameras mounted on low-cost autonomous surface vehicles (ASVs) is a key process for obtaining a fast obstacle detection. Achieving an accurate waterline prediction is difficult due to the instability of the ASV on which the camera is mounted and the presence of reflections, illumination changes, and waves. In this work, we present a method for waterline and obstacle detection designed for low-cost ASVs employed in environmental monitoring. The proposed approach is made of two steps: (1) a pixel-wise segmentation of the current image is used to generate a binary mask separating water and non-water regions, (2) the mask is analyzed to infer the position of the waterline, which in turn is used for detecting obstacles. Experiments were carried out on two publicly available datasets containing floating obstacles such as buoys, sailing and motor boats, and swans moving near the ASV. Quantitative results show the effectiveness of the proposed approach with 98.8% pixel-wise segmentation accuracy running at 10 frames per second on an embedded GPU board.
               ",autonomous vehicle
10.1016/S0893-6080(19)30379-X,journal,Neural Networks,sciencedirect,2020-01-31,sciencedirect,List of Editorial Board Members,https://api.elsevier.com/content/article/pii/S089360801930379X,,autonomous vehicle
10.1016/j.cam.2019.112560,journal,Journal of Computational and Applied Mathematics,sciencedirect,2020-05-01,sciencedirect,A novel intelligent option price forecasting and trading system by multiple kernel adaptive filters,https://api.elsevier.com/content/article/pii/S0377042719305655,"
                  Derivatives such as options are complex financial instruments. The risk in option trading leads to the demand of trading support systems for investors to control and hedge their risk. The nonlinearity and non-stationarity of option dynamics are the main challenge of option price forecasting. To address the problem, this study develops a multi-kernel adaptive filters (MKAF) for online option trading. MKAF is an improved version of the adaptive filter, which employs multiple kernels to enhance the richness of nonlinear feature representation. The MKAF is a fully adaptive online algorithm. The strength of MKAF is that the weights to the kernels are simultaneous optimally determined in filter coefficient updates. We do not need to design the weights separately. Therefore, MKAF is good at tracking nonstationary nonlinear option dynamics. Moreover, to reduce the computation time in updating the filter, and prevent overadaptation, the number of kernels is restricted by using coherence-based sparsification, which constructs a set of dictionary and uses a coherence threshold to restrict the dictionary size. This study compared the new method with traditional ones, we found the performance improvement is significant and robust. Especially, the cumulated trading profits are substantially increased.
               ",autonomous vehicle
10.1016/B978-0-12-820353-8.00012-8,journal,Intelligent Image and Video Compression,sciencedirect,2021-12-31,sciencedirect,Chapter 3: Signal processing and information theory fundamentals,https://api.elsevier.com/content/article/pii/B9780128203538000128,"
               
                  This chapter is not intended to be a complete course in discrete-time signal analysis or information theory – many excellent texts on this topic already exist covering both 1D [1] and 2D [2] signals. We assume here that the reader is familiar with the basics of sampling, linear systems analysis, digital filters, transforms, information theory, and statistical signal processing. Our purpose is to provide an overview of these important topics as they form the basis of many of the compression techniques described later in the book.
               We firstly review the sampling theorem and consider the cases of 2D (image) and 2D+time (video) sampling. This provides the discrete time samples that represent our image or video and that are processed during the various stages of compression and coding. In Section 3.2, we introduce the means of describing signals using statistical representations. We focus on second order statistics and we use these to characterize the redundancy contained in an image or video signal. It is this redundancy that is exploited during the compression process.
               Filters and transforms form the basic work horses of most compression systems and these are introduced in Section 3.3. The signal distortions introduced in most compression systems can be attributed to coefficient quantization after filtering or transformation; this important aspect is covered in Section 3.4. Prediction operators are also in common usage across many aspects of compression, from intra-prediction to motion estimation. The basic concepts of linear prediction are introduced in Section 3.5 and we pay particular attention to the mitigation of decoder drift. This architecture is important as it provides a framework for all current video compression standards.
               As a precursor to coverage of lossless compression and entropy coding in Chapter 7, we review the basics of information theory: self-information, entropy, symbols, and their statistics. This topic is key to all symbol encoding methods in use today. Finally, in Section 3.7 we provide an introduction to the important topic of machine learning (ML), which is having an increasing impact on both compression methods and quality assessment.
            ",autonomous vehicle
10.1016/j.eneco.2011.07.018,journal,Energy Economics,sciencedirect,2012-05-31,sciencedirect,Crude oil price forecasting: Experimental evidence from wavelet decomposition and neural network modeling,https://api.elsevier.com/content/article/pii/S0140988311001484,"
                  Oil price prediction has usually proved to be an intractable task due to the intrinsic complexity of oil market mechanism. In addition, the recent oil shock and its consequences relaunch the debate on understanding the behavior underlying the expected oil prices. Combining the dynamic properties of multilayer back propagation neural network and the recent Harr A trous wavelet decomposition, a Hybrid model HTW-MPNN is implemented to achieve prominent prediction of crude oil price. While recent studies focus on the determination of the best forecasting model by comparing various neural architectures or applying several decomposition techniques to the ANN, the new insight of this paper is to target the issue of the transfer function selection providing robust simulations on both in sample and out of sample basis. Based on the work of Yonaba, H., Anctil, F., and Fortin, V. (2010) “Comparing Sigmoid Transfer Functions for Neural Network Multistep Ahead Stream flow forecasting”. Journal of Hydrologic Engineering, April, 275–283, we use three variants of activation function namely sigmoid, bipolar sigmoid and hyperbolic tangent in order to test the model's flexibility. Furthermore, the forecasting robustness is checked through several levels of input–hidden nodes. Comparatively, results of HTW-MBPNN perform better than the conventional BPNN. Our conclusions add a major attribute to the previous studies corroborating the Occam razor's principle, especially when simulations are constructed through training and testing phases simultaneously. Finally, more eligible forecasting power is found according to the wavelet oil price signal which appears to be the closest to the real anticipations of future oil price fluctuations.
               ",autonomous vehicle
10.1016/0920-5489(94)90012-4,journal,Computer Standards & Interfaces,sciencedirect,1994-07-31,sciencedirect,Formal neural network specification and its implications on standardization,https://api.elsevier.com/content/article/pii/0920548994900124,"
                  This paper introduces a formal framework for describing and specifying neural networks and discusses several important issues with implications for neural network standardization. In particular, a neural network definition and two tools for graphical description and formal specification are introduced. Issues such as the theoretical impossibility of canonical description, or the need for complete specification (including global algorithms) are discussed. Several examples, making use of the developed tools, illustrate these discussions. In summary, this paper aims at contributing to the important endeavour of neural network standardization both practically and theoretically.
               ",autonomous vehicle
10.1016/j.asoc.2020.106300,journal,Applied Soft Computing,sciencedirect,2020-07-31,sciencedirect,Deep belief network and linear perceptron based cognitive computing for collaborative robots,https://api.elsevier.com/content/article/pii/S1568494620302404,"
                  Objective: This paper is to analyze the performance of the control system of collaborative robots based on cognitive computing technology. Methods: This study combines cognitive computing and deep belief network algorithms with collaborative robots to construct a cognitive computing system model based on deep belief networks, which is applied to the control system of collaborative robots. Further, the simulation is used to compare and analyze the algorithm performance of deep belief network (DBN), multilayer perceptron (MLP) and the cognitive computing system model of deep belief network and linear perceptron (DBNLP) proposed in this study. Results: The results show that compared with the DBN and MLP algorithms, the DBNLP algorithm model has a significantly lower error rate in the number of repetitions of the training set, the number of hidden neurons, and the number of network layers. And the number of task backlog, the number of resources to be allocated and the time consumption are less, as well as the accuracy is high. After comparing and analyzing the changes in the estimated value of Ex (expected value), En (entropy value) and He (hyper entropy value), it is found that the estimated value of the DBNLP algorithm model is closer to the true value than that of the DBN and MLP algorithms. Conclusion: The application of the DBNLP algorithm model to collaborative robots can significantly improve its accuracy and safety, providing an experimental basis for the performance improvement of later collaborative robots.
               ",autonomous vehicle
10.1016/j.neucom.2020.06.023,journal,Neurocomputing,sciencedirect,2020-10-21,sciencedirect,Dual reference age synthesis,https://api.elsevier.com/content/article/pii/S0925231220309942,"
                  Age synthesis methods typically take a single image as input and use a specific number to control the age of the generated image. In this paper, we propose a novel framework taking two images as inputs, named dual-reference age synthesis (DRAS), which approaches the task differently; instead of using “hard” age information, i.e. a fixed number, our model determines the target age in a “soft” way, by employing a second reference image. Specifically, the proposed framework consists of an identity agent, an age agent and a generative adversarial network. It takes two images as input – an identity reference and an age reference – and outputs a new image that shares corresponding features with each. Experimental results on two benchmark datasets (UTKFace and CACD) demonstrate the appealing performance and flexibility of the proposed framework.
               ",autonomous vehicle
10.1016/S0043-1354(97)00274-1,journal,Water Research,sciencedirect,1998-03-01,sciencedirect,Neural network predictors of average score per taxon and number of families at unpolluted river sites in Great Britain,https://api.elsevier.com/content/article/pii/S0043135497002741,"
                  Biological monitoring of river water quality in the United Kingdom and several other European and Commonwealth countries is based on the Biological Monitoring Working Party (BMWP) system. Central to the present day application of this system is the prediction of “unpolluted” average score per taxon (ASPT) and number of families present (NFAM). The paper outlines the need for such predictions and proceeds to develop predictors of ASPT and NFAM using neural networks. The basic principles of neural networks are outlined and a brief introduction to their structure and function is given via a typical example. Important preliminary considerations are fully discussed, such as model selection, training and testing procedures and the selection of relevant input variables. The results of impact analyses, designed to optimise the structures of the networks, are reported and discussed. In-depth analyses of the performance of the networks on independent test data and also relative to the industry's current model, RIVPACS III, are presented. The results of investigations into bias and error in the predicted values of ASPT and NFAM are discussed and related to some possible inadequacies in the database. It is concluded that: predictions of ASPT are significantly more reliable than those of NFAM; the neural networks performed marginally better than RIVPACS III; ASPT and NFAM can be predicted directly, without reference to site type or biological community, from a few key environmental variables; and there is scope for improved predictions if additional relevant environmental data are collected.
               ",autonomous vehicle
10.1016/j.eswa.2009.12.042,journal,Expert Systems with Applications,sciencedirect,2010-06-30,sciencedirect,Prediction of Marshall test results for polypropylene modified dense bituminous mixtures using neural networks,https://api.elsevier.com/content/article/pii/S0957417409010884,"
                  This study presents an application of neural networks (NN) for the prediction of Marshall test results for polypropylene (PP) modified asphalt mixtures. PP fibers are used to modify the bituminous binder in order to improve the physical and mechanical properties of the resulting asphaltic mixture. Marshall stability and flow tests were carried out on specimens fabricated with different type of PP fibers and also waste PP at optimum bitumen content. It has been shown that the addition of polypropylene fibers results in the improved Marshall stabilities and Marshall Quotient values, which is a kind of pseudo stiffness. The proposed NN model uses the physical properties of standard Marshall specimens such as PP type, PP percentage, bitumen percentage, specimen height, unit weight, voids in mineral aggregate, voids filled with asphalt and air voids in order to predict the Marshall stability, flow and Marshall Quotient values obtained at the end of mechanical tests. The explicit formulation of stability, flow and Marshall Quotient based on the proposed NN model is also obtained and presented for further use by researchers. Moreover parametric analyses have been carried out. The results of parametric analyses were used to evaluate mechanical properties of the Marshall specimens in a quite well manner.
               ",autonomous vehicle
10.1016/j.neucom.2020.11.064,journal,Neurocomputing,sciencedirect,2021-04-14,sciencedirect,Discrimination and correction of abnormal data for condition monitoring of drilling process,https://api.elsevier.com/content/article/pii/S0925231220318580,"
                  During the drilling process, the data quality influences the reliability of condition monitoring results. However, the actual drilling data may encounter various kinds of abnormal data, and different kinds of them need to be handled differently. The abnormal data caused by external factors such as sensor failure, storage errors, etc., should be corrected, while the abnormal data caused by drilling accidents should be protected. Therefore, not only the anomaly detection is needed, but the causes of anomalies should be further discriminated. This paper proposes a method for discrimination and correction of abnormal data in the drilling process. First, the local outlier factor anomaly detection algorithm is developed to detect all kinds of the abnormal data. Then, the dynamic time warping and fuzzy c-means are combined for the discrimination of causes of anomalies. Finally, the discriminated abnormal data caused by external factors are corrected with the k nearest neighbor interpolation. Simulation results involving actual data illustrate that the causes of anomalies can be discriminated effectively, and the monitoring results of monitoring models based on neural network improve after using the proposed method, which verify the necessity of anomaly discrimination.
               ",autonomous vehicle
10.1016/j.jksuci.2020.07.002,journal,Journal of King Saud University - Computer and Information Sciences,sciencedirect,2020-07-09,sciencedirect,Internet of things and data mining: An application oriented survey,https://api.elsevier.com/content/article/pii/S131915782030416X,"Advancement in the fields of electronic communication, data processing, and internet technologies enable easy access to and interaction with a variety of physical devices throughout the globe. Our whole world is enveloped by a blanket of innumerable smart devices equipped with the sensors and actuators. Extensive research on the Internet of things (IoT) with cloud technologies, make it possible to accumulate tremendous data created from this heterogeneous environment and transform it into precious knowledge by utilizing data mining technologies. Furthermore, this generated knowledge will play a key role in intelligent decision making, system performance boosting, and optimum management of resources and services. With this background, this paper presents a systematic and detailed review of various data mining techniques employed in the large and small scale IoT applications to formulate an intelligent environment. It also presents an overview of cloud-assisted IoT Big data mining system to better understand the importance of data mining for an IoT environment.",autonomous vehicle
10.1016/B978-1-55860-377-6.50050-5,journal,Machine Learning Proceedings 1995,sciencedirect,1995-12-31,sciencedirect,Case-Based Acquisition of Place Knowledge,https://api.elsevier.com/content/article/pii/B9781558603776500505,"
               In this paper we define the task of place learning and describe one approach to this problem. The framework represents distinct places using evidence grids, a probabilistic description of occupancy. Place recognition relies on case-based classification, augmented by a registration process to correct for translations. The learning mechanism is also similar to that in case- based systems, involving the simple storage of inferred evidence grids. Experimental studies with both physical and simulated robots suggest that this approach improves place recognition with experience, that it can handle significant sensor noise, and that it scales well to increasing numbers of places. Previous researchers have studied evidence grids and place learning, but they have not combined these two powerful concepts, nor have they used the experimental methods of machine learning to evaluate their methods' abilities.
            ",autonomous vehicle
10.1016/B978-0-12-818699-2.00004-4,journal,Hybrid Computational Intelligence,sciencedirect,2020-12-31,sciencedirect,Chapter 4: A computationally intelligent agent for detecting fake news using generative adversarial networks,https://api.elsevier.com/content/article/pii/B9780128186992000044,"
               The explosion of smartphones has led to major changes in the perception of news, including the use of social media to propagate fake news and system-generated content without proper validation. The existing solutions to this problem employ the use of technologies like machine learning. The identification of unverified articles is a classification problem where, given a document, the system classifies it as “fake” or “valid.” This process involves the collection of large amounts of text corpus of both valid and fake news articles. The issue with these existing systems is the validity of the data aggregated from different sources. This can lead to the problem of human bias in labeling the articles collected. As an initial step to reduce this bias, this project proposes a fake news detection framework, which uses a generative modeling technique. In this technique, a generator–discriminator (G–D) setup is employed. The G–D model is extended from the SeqGAN model. The generator generates new data instances, while the discriminator evaluates them for authenticity. When the models train competitively, the generator becomes better at creating synthetic samples and the discriminator gets better at identifying the synthetic samples. Thus, a data set is synthesized by merging the real articles with the articles generated by the generator, which is trained using the G–D setup. This data set is then used to train the agent (classifier) to identify the articles as “fake” or “valid.”
            ",autonomous vehicle
10.1016/B978-0-08-057121-8.50023-3,journal,Artificial Intelligence in Chemical Engineering,sciencedirect,1991-12-31,sciencedirect,CHAPTER 17: INTRODUCTION TO ARTIFICIAL NEURAL NETWORKS,https://api.elsevier.com/content/article/pii/B9780080571218500233,,autonomous vehicle
10.1016/0066-4138(94)90037-X,journal,Annual Review in Automatic Programming,sciencedirect,1994-12-31,sciencedirect,Neural network based adaptive control,https://api.elsevier.com/content/article/pii/006641389490037X,"
                  This paper presents differents ways of using artificial neural networks in adaptive control. A classification of architectures for control using neural networks is presented, showing the existing paralelism with Adaptive Control techniques.
               ",autonomous vehicle
10.1016/B978-0-12-083030-5.50008-4,journal,Neural Networks in Bioprocessing and Chemical Engineering,sciencedirect,1995-12-31,sciencedirect,2: Fundamental and Practical Aspects of Neural Computing,https://api.elsevier.com/content/article/pii/B9780120830305500084,"
               This chapter introduces neural networks. We first discuss what makes up a neural network, then move on to the fundamental and practical aspects of neural computing, and discuss aspects of network training (learning). We next illustrate how to develop a neural network using a commercial software package on a personal computer. Finally, we introduce a number of special neural networks that find significant applications in bioprocessing and chemical engineering.
            ",autonomous vehicle
10.1016/B978-0-12-822800-5.00020-2,journal,"Pathogenesis, Treatment and Prevention of Leishmaniasis",sciencedirect,2021-12-31,sciencedirect,Index,https://api.elsevier.com/content/article/pii/B9780128228005000202,Unknown,autonomous vehicle
10.1016/0893-6080(93)90019-S,journal,Neural Networks,sciencedirect,1993-12-31,sciencedirect,Centralized and decentralized neuro-adaptive robot controllers,https://api.elsevier.com/content/article/pii/089360809390019S,"
                  A new method for the direct control of robot manipulators using Artificial Neural Networks (ANNs) is presented in this paper. The control system consists of an inverse model of the robot dynamics which produces the forces/torques to be applied to the robot, given desired positions, velocities, and accelerations, and a neural controller generating a correcting signal. The weights of the neural network, which represent the controller parameters, are modified based on an error signal. The error signal represents the deviation between the desired and actual positions, velocities, and accelerations. The neural controller is computationally efficient in the sense that no parameter estimation is required to update the controller. The technique employs reinforcement learning for the updating of the network parameters. The neural-controller uses two modes of operation. The first mode is an off-line training session that most techniques employ. The second mode is the real-time tracking, which is very essential for the compensation of unforseeable shortcomings during on-line operations. The concept of the adaptive neural-controller is further extended to the decentralized case. Several neural-controllers are used to provide a better performance (a neural-controller per joint). The different cases are demonstrated by several examples to show the efficiency of the proposed algorithms.
               ",autonomous vehicle
10.1016/B978-0-12-815739-8.17001-4,journal,Machine Learning,sciencedirect,2020-12-31,sciencedirect,Glossary,https://api.elsevier.com/content/article/pii/B9780128157398170014,Unknown,autonomous vehicle
10.1016/j.bspc.2021.102770,journal,Biomedical Signal Processing and Control,sciencedirect,2021-07-31,sciencedirect,Robust segmentation of exudates from retinal surface using M-CapsNet via EM routing,https://api.elsevier.com/content/article/pii/S1746809421003670,"
                  Retinopathy is any damage to the retina of the eyes, which causes vision impairment and may lead to blindness. The initial manifestation of retinopathy is identified by the presence of exudates, microaneurysms on the retinal surface. So, the early detection of exudates prevents the further spread and simultaneously reduces the severity of the disease. However, automatic detection of exudates is a challenging task as the exudates vary from each other in terms of shape and size. This paper proposes a novel approach for the automatic segmentation of exudates using an encoder-decoder style network termed as “deep M-CapsNet using Expectation-Maximization (EM) Routing,” which reduces the memory allocation problems in semantic segmenting of objects. In M-CapsNet, every child capsule connects with every parent capsule at every location. Thus the predictions are forwarded to parent capsules using a shared kernel through a matrix capsule. Due to similar intensities between exudates and the optic disc, the M-CapsNet extracts the exudates along with the optic disc from the retinal surface. The optic disc is eliminated from the segmented output using regional and morphological features. This paper achieves an average accuracy of 94%, the specificity of 100%, the sensitivity of 100%, and the F1 score of 95% when tested over the images selected from publicly available datasets randomly. The experiment results demonstrate that M-CapsNet outperformed previous networks in detecting exudates.
               ",autonomous vehicle
10.1016/j.procs.2017.06.132,journal,Procedia Computer Science,sciencedirect,2017-12-31,sciencedirect,On the use of Networks in Biomedicine,https://api.elsevier.com/content/article/pii/S187705091731311X,"The concept of “neural network” emerges by electronic models inspired to the neural structure of human brain. Neural networks aim to solve problems currently out of computer's calculation capacity, trying to mimic the role of human brain. Recently, the number of biological based applications using neural networks is growing up. Biological networks represent correlations, extracted from sets of clinical data, diseases, mutations, and patients, and many other types of clinical or biological features. Biological networks are used to model both the state of a range of functionalities in a particular moment, and the space-time distribution of biological and clinical events. The study of biological networks, their analysis and modeling are important tasks in life sciences. Most biological networks are still far from being complete and they are often difficult to interpret due to the complexity of relationships and the peculiarities of the data. Starting from preliminary notions about neural networks, we focus on biological networks and discuss some well-known applications, like protein-protein interaction networks, gene regulatory networks (DNA-protein interaction networks), metabolic networks, signaling networks, neuronal network, phylogenetic trees and special networks. Finally, we consider the use of biological network inside a proposed model to map health related data.",autonomous vehicle
10.1016/j.engappai.2014.02.008,journal,Engineering Applications of Artificial Intelligence,sciencedirect,2014-06-30,sciencedirect,"Fuzzy systems, neural networks and neuro-fuzzy systems: A vision on their hardware implementation and platforms over two decades",https://api.elsevier.com/content/article/pii/S0952197614000384,"
                  In recent decades, and in order to develop applications covering several areas of knowledge, different researchers have been performing hardware implementations around paradigms such as fuzzy systems, neural networks or systems resulting from the hybridization of the previous two systems, known as neuro–fuzzy systems. Applications have been performed on different types of devices and/or platforms.
                  The point of view of this paper is focused on a hardware taxonomy (devices where the applications have been implemented) and highlights the characteristics of the different applications covering the aforementioned paradigms done over the last two decades, and the beginning of the current decade. Special mention is made up of reconfigurable devices.
               ",autonomous vehicle
10.1016/0370-1573(87)90096-2,journal,Physics Reports,sciencedirect,1987-12-31,sciencedirect,Artificial intelligence and large scale computation: A physics perspective,https://api.elsevier.com/content/article/pii/0370157387900962,"
                  We study the macroscopic behavior of computation and examine both emergent collective phenomena and dynamical aspects with an emphasis on software issues, which are at the core of large scale distributed computation and artificial intelligence systems. By considering large systems, we exhibit novel phenomena which cannot be foreseen from examination of their smaller counterparts. We review both the symbolic and connectionist views of artificial intelligence, provide a number of examples which display these phenomena, and resort to statistical mechanics, dynamical systems theory and the theory of random graphs to elicit the range of possible behaviors.
               ",autonomous vehicle
10.1016/S0140-6736(95)91746-2,journal,The Lancet,sciencedirect,1995-10-21,sciencedirect,Introduction to neural networks,https://api.elsevier.com/content/article/pii/S0140673695917462,,autonomous vehicle
10.1016/j.neucom.2021.08.087,journal,Neurocomputing,sciencedirect,2021-11-13,sciencedirect,CapsNet meets SIFT: A robust framework for distorted target categorization,https://api.elsevier.com/content/article/pii/S0925231221012844,"
                  Due to overexposure, jitter, motion, and other spatiotemporal-varying perturbations, the collected images always undergo various visual distortions (e.g., deformation, partially occluded signs, fisheye respective, affine or 3D projections, in-plane and out-of-plane rotation) during acquisition or transmission procedure. Deep neural networks (DNNs) perform poorly on such pristine images in terms of high-level abstract operations, e.g., object categorization and semantic segmentation. To conquer this legacy, a distortion-tolerant model denoted as CapsNetSIFT is proposed to enhance representability and detectability of target in distorted imagery. We modify and integrate capsule network (CapsNet) with scale invariant feature transform (SIFT) together, both of which boast innate invariance to spacial-scale transformations. Two key insights, the customized multi-dimensional CapsNet (MD-CapsNet) and vector matching SIFT (VM-SIFT), can cooperate together and reinforce each other: the former encodes and provides representative feature vectors for the later, whilst the later localizes space-scale invariant interval dimensions (instead of pixels) and establish correspondence between source standard images (high-quality training images) and distorted ones (testing images). Thus, the category of one source standard image owning the most associations is the ground-truth category. Evaluation results reveal that employing CapsNetSIFT for distorted target recognition (CUB-200–2011, Stanford Dogs, Stanford Cars, and our hand-crafted dataset), significantly improves the resistance against various simulated distortions, and outperforms state-of-the-arts with relatively higher training and testing accuracy (93.97% and 91.03%).
               ",autonomous vehicle
10.1016/j.neunet.2019.12.022,journal,Neural Networks,sciencedirect,2020-04-30,sciencedirect,Abstractive summarization of long texts by representing multiple compositionalities with temporal hierarchical pointer generator network,https://api.elsevier.com/content/article/pii/S0893608019304228,"
                  In order to tackle the problem of abstractive summarization of long multi-sentence texts, it is critical to construct an efficient model, which can learn and represent multiple compositionalities better. In this paper, we introduce a temporal hierarchical pointer generator network that can represent multiple compositionalities in order to handle longer sequences of texts with a deep structure. We demonstrate how a multilayer gated recurrent neural network organizes itself with the help of an adaptive timescale in order to represent the compositions. The temporal hierarchical network is implemented with a multiple timescale architecture where the timescale of each layer is also learned during the training process through error backpropagation through time. We evaluate our proposed model using an Introduction-Abstract summarization dataset from scientific articles and the CNN/Daily Mail summarization benchmark dataset. The results illustrate that, we successfully implement a summary generation system for long texts by using the multiple timescale with adaptation concept. We also show that we have improved the summary generation system with our proposed model on the benchmark dataset.
               ",autonomous vehicle
10.1016/B978-0-12-480575-0.50008-3,journal,Artificial Intelligence in Process Engineering,sciencedirect,1990-12-31,sciencedirect,4: Fault Detection and Diagnosis Using Artificial Neural Networks,https://api.elsevier.com/content/article/pii/B9780124805750500083,"
               Abstract
            Artificial Neural Networks (ANNs) have proven effective at solving problems in a wide variety of areas, such as image processing, speech recognition, and motor control. We describe here the application of ANNs to fault detection and diagnosis. Fault detection is a type of pattern recognition, and ANN classifiers appear to be a reasonable alternative to traditional classifiers.ANNs consist of a number of simple neuron like processing elements locally interacting through a set of unidirectional weighted connections. Knowledge is internally represented by the values of the weights and the topology of the connections. Learning involves modifying the connection weights. These networks can learn and adapt themselves to inputs from the actual processes, thus allowing representation of complex engineering systems, which are difficult to model either with traditional physical engineering relations or knowledge-based expert systems.Several factors motivate the use of artificial neural networks for fault diagnosis: their potential for high computation rates provided by massive parallelism, their capacity for a greater degree of robustness or fault tolerance due to the distributed representation, and their ability to adapt and continue learning to improve performance. Furthermore, they can accommodate the uncertain, nonstationary, and nonlinear nature of typical chemical processes.This chapter describes the features of neural networks that are desirable for knowledge representation for fault detection in chemical engineering processes. It explains the characteristics of neural networks, focusing on the nodes, the connections, and their learning capabilities. The chapter also describes a neural network design and simulation environment that can be used to study practical applications.To demonstrate the feasibility of applying artificial neural networks to steady-state processes, we describe fault detection and diagnosis in two examples of simple chemical operations. The compactness and ease with which fault discrimination occurred in both cases emphasize the elegance as well as the pragmatics of the ANN approach. Applied to fault diagnosis, ANNs obviate the need for explicit feature extraction. Once the network was trained, it provided rapid classification since the propagation through the network is essentially a series of simple matrix operations that may or may not be executed in parallel. The results demonstrate the effectiveness of artificial neural networks for real-time on-line discrimination among faults.",autonomous vehicle
10.1016/S1574-9576(06)80024-9,journal,Capturing Intelligence,sciencedirect,2006-12-31,sciencedirect,Chapter 22 Enhancing the power of the internet using fuzzy logic-based web intelligence: Beyond the semantic web,https://api.elsevier.com/content/article/pii/S1574957606800249,"
                  World Wide Web search engines have become the most heavily-used online services, with millions of searches performed each day. Their popularity is due, in part, to their ease of use. It is important to note that while the Semantic Web is dissimilar in many ways from the World Wide Web, the Semantic Web provides a common framework that allows data to be shared and reused across application, enterprise, and community boundaries through the World Wide Web. In this paper, we would like to go beyond the traditional semantic web which has been defined mostly as a mesh or distributed databases within the World Wide Web. For this reason, our view is that “Before one can use the power of semantic web, the relevant information has to be mined through the search mechanism and logical reasoning”. The central tasks for the most of the search engines can be summarized as (1) query or user information request — do what I mean and not what I say!, (2) model for the Internet, Web representation — web page collection, documents, text, images, music, etc., and (3) ranking or matching function — degree of relevance, recall, precision, similarity, etc. Design of any new intelligent search engine should be at least based on two main motivations: (1) the web environment is, for the most part, unstructured and imprecise. To deal with information in the web environment what is needed is a logic that supports modes of reasoning which are approximate rather than exact. While searches may retrieve thousands of hits, finding decision-relevant and query-relevant information in an imprecise environment is a challenging problem, which has to be addressed and (2) another, and less obvious, is deduction in an unstructured and imprecise environment given the huge stream of complex information. In this paper, we will first present the state of the search engines and Internet. Then we will focus on development of a framework for reasoning and deduction in the web. A web-based model to decision model for analysis of structured database will be presented. A framework to incorporate the information from web sites into the search engine will be presented as a model that will go beyond current semantic web idea. Another important and unique component of our system is compactification algorithm or Z-Compact. Z-Compact algorithm developed by L.A. Zadeh and it has been implemented for the first time as part of BISC-DSS for automatons multi-agents modeling as part of ONR project and has been extended to handle linguistic variables with deduction capability and currently is part of the BISC-DSS software and its has been applied in several applications.
               ",autonomous vehicle
10.1016/S0893-6080(19)30008-5,journal,Neural Networks,sciencedirect,2019-02-28,sciencedirect,Announcement,https://api.elsevier.com/content/article/pii/S0893608019300085,,autonomous vehicle
10.1016/B978-0-12-396502-8.00018-8,journal,Academic Press Library in Signal Processing,sciencedirect,2014-12-31,sciencedirect,Chapter 18: Introduction to Probabilistic Graphical Models,https://api.elsevier.com/content/article/pii/B9780123965028000188,"
                  Over the last decades, probabilistic graphical models have become the method of choice for representing uncertainty. They are used in many research areas such as computer vision, speech processing, time-series and sequential data modeling, cognitive science, bioinformatics, probabilistic robotics, signal processing, communications and error-correcting coding theory, and in the area of artificial intelligence.
                  This tutorial provides an introduction to probabilistic graphical models. We review three representations of probabilistic graphical models, namely, Markov networks or undirected graphical models, Bayesian networks or directed graphical models, and factor graphs. Then, we provide an overview about structure and parameter learning techniques. In particular, we discuss maximum likelihood and Bayesian learning, as well as generative and discriminative learning. Subsequently, we overview exact inference methods and briefly cover approximate inference techniques. Finally, we present typical applications for each of the three representations, namely, Bayesian networks for expert systems, dynamic Bayesian networks for speech processing, Markov random fields for image processing, and factor graphs for decoding error-correcting codes.
               ",autonomous vehicle
10.1016/j.jksuci.2019.02.001,journal,Journal of King Saud University - Computer and Information Sciences,sciencedirect,2021-05-31,sciencedirect,Airline ticket price and demand prediction: A survey,https://api.elsevier.com/content/article/pii/S131915781830884X,"Nowadays, airline ticket prices can vary dynamically and significantly for the same flight, even for nearby seats within the same cabin. Customers are seeking to get the lowest price while airlines are trying to keep their overall revenue as high as possible and maximize their profit. Airlines use various kinds of computational techniques to increase their revenue such as demand prediction and price discrimination. From the customer side, two kinds of models are proposed by different researchers to save money for customers: models that predict the optimal time to buy a ticket and models that predict the minimum ticket price. In this paper, we present a review of customer side and airlines side prediction models. Our review analysis shows that models on both sides rely on limited set of features such as historical ticket price data, ticket purchase date and departure date. Features extracted from external factors such as social media data and search engine query are not considered. Therefore, we introduce and discuss the concept of using social media data for ticket/demand prediction.",autonomous vehicle
10.1016/S0957-4158(03)00044-8,journal,Mechatronics,sciencedirect,2003-12-31,sciencedirect,Intelligent ship autopilots––A historical perspective,https://api.elsevier.com/content/article/pii/S0957415803000448,"
                  The paper presents a historical perspective of the development and use of the so-called intelligent paradigms of fuzzy logic and/or neural networks in ship autopilot designs. After a brief review of the development of the first PID autopilots, the paper describes how early work using fuzzy logic techniques to describe and model the actions of helmsmen led to the development of the first fuzzy autopilots and how parallel advances in self-organising and adaptive network fuzzy inference systems were utilised to refine the early fuzzy autopilot designs. The use of artificial neural networks in adaptive and model reference approaches is also highlighted as is their use in optimising the parameters of neurofuzzy designs. The paper concludes with some observations on stability issues and suggestions for future developments in intelligent ship autopilots.
               ",autonomous vehicle
10.1016/B978-0-12-824477-7.00007-9,journal,Foundations of Artificial Intelligence in Healthcare and Bioscience,sciencedirect,2021-12-31,sciencedirect,7: AI applications in prevalent diseases and disorders,https://api.elsevier.com/content/article/pii/B9780128244777000079,"
               Health care, biosciences and artificial intelligence (AI) represent a group of highly sophisticated and disruptive sciences and technologies. None, however, would mean much if they didn’t offer succor to the prevalent diseases and disorders that afflict humankind. Chapter 7 begins with the 2 biosciences of immunology and genetics that use AI to address chronic inflammation, autoimmune diseases, gene editing (CRISPR Cas9), gene replacement (CAR-T) and stem cell transplantation. The benefits discussed are incalculable as well as their value to all other prevailing diseases and disorders we face in health care. Some of those prevalent conditions and their AI applications are discussed throughout Chapter 7 and include cancer, cardio and cerebrovascular disease, diabetes, neurological disorders and the more common diseases of our bodily systems. Finally, AI’s role in prevalent issues like injury, infectious disease, chronic disease, mental health, depression, aging, exercise and nutrition are discussed.
            ",autonomous vehicle
10.1016/j.patcog.2015.03.020,journal,Pattern Recognition,sciencedirect,2015-09-30,sciencedirect,Relevance–redundancy feature selection based on ant colony optimization,https://api.elsevier.com/content/article/pii/S0031320315001211,"
                  The curse of dimensionality is a well-known problem in pattern recognition in which the number of patterns is smaller than the number of features in the datasets. Often, many of the features are irrelevant and redundant for the classification tasks. Therefore, the feature selection becomes an essential technique to reduce the dimensionality of the datasets. In this paper, unsupervised and multivariate filter-based feature selection methods are proposed by analyzing the relevance and redundancy of features. In the methods, the search space is represented as a graph and then the ant colony optimization is used to rank the features. Furthermore, a novel heuristic information measure is proposed to improve the accuracy of the methods by considering the similarity between subsets of features. The performance of the proposed methods was compared to the well-known univariate and multivariate methods using different classifiers. The results indicated that the proposed methods outperform the existing methods.
               ",autonomous vehicle
10.1016/j.aca.2008.02.025,journal,Analytica Chimica Acta,sciencedirect,2008-03-31,sciencedirect,Mining in chemometrics,https://api.elsevier.com/content/article/pii/S0003267008003450,"
                  Some of the increasingly spread data mining methods in chemometrics like exploratory data analysis, artificial neural networks, pattern recognition, and digital image processing with their highs and lows along with some of their representative applications are discussed. The development of more complex analytical instruments and the need to cope with larger experimental data sets have demanded for new approaches in data analysis, which have led to advanced methods in experimental design and data processing. Hypothesis-driven methods typified by inferential statistics have been gradually complemented or even replaced by data-driven model-free methods that seek for structure in data without reference to the experimental protocol or prior hypotheses. The emphasis is put on the ability of data mining methods to solve multivariate–multiresponse problems on the basis of experimental data and minimal statistical assumptions only, in contrast to classical methods, which require predefined priors to be tested against some null-hypothesis.
               ",autonomous vehicle
10.1016/j.coche.2021.100732,journal,Current Opinion in Chemical Engineering,sciencedirect,2022-03-31,sciencedirect,Computer-aided molecular design of solvents for chemical separation processes,https://api.elsevier.com/content/article/pii/S2211339821000642,"
                  Solvents are widely used in chemical industries, especially in various separation processes. As traditional trial-and-error solvent selection is time-consuming and expensive, model-based methods for solvent selection/design become important for efficient and sustainable chemical manufacturing. A lot of contributions have been made in this area in the past few decades. This article first reviews the prediction methods for solvent properties, including single molecular properties and mixture properties. Then, the solution strategies of solvent design problems are summarized, including generate-and-test, deterministic optimization, and stochastic optimization methods. Next, latest progresses of computer-aided solvent-process design in separation processes including liquid–liquid extraction, extractive distillation, gas absorption, and crystallization are reviewed. Finally, several remaining challenges and possible future directions for solvent design in separation processes are pointed out.
               ",autonomous vehicle
10.1016/B978-0-12-823337-5.00026-3,journal,Intelligence-Based Medicine,sciencedirect,2020-12-31,sciencedirect,Index,https://api.elsevier.com/content/article/pii/B9780128233375000263,Unknown,autonomous vehicle
10.1016/B978-0-12-801559-9.00014-4,journal,"Artificial Neural Network for Drug Design, Delivery and Disposition",sciencedirect,2016-12-31,sciencedirect,Chapter 14: ANN in Pharmaceutical Product and Process Development,https://api.elsevier.com/content/article/pii/B9780128015599000144,"
               Artificial neural networks (ANNs) are computer systems developed to mimic the operations of the human brain by mathematically modeling its neurophysiological structure. Neural networks have seen an explosion of interest over the last few years and are being successfully applied across an extraordinary range of problem domains of medicine, engineering, physics, biology, and pharmacy. From statistical perspectives, neural networks are interesting because of their potential use in prediction and classification problems. ANNs have been very useful in many aspects of pharmaceutical research, including analytical data analysis, pharmaceutical product and process optimization and manufacturing, pharmacokinetic and pharmacodynamic modeling, and in vitro-in vivo correlations. This chapter discusses the applications of ANNs in pharmaceutical product and process development that are useful in drug delivery research.
            ",autonomous vehicle
10.1016/j.jal.2014.11.006,journal,Journal of Applied Logic,sciencedirect,2015-06-30,sciencedirect,Information retrieval from hospital information system: Increasing effectivity using swarm intelligence,https://api.elsevier.com/content/article/pii/S1570868314000809,"This paper details the process of mining information from a hospital information system that has been designed approximately 15 years ago. The information is distributed within database tables in large textual attributes with a free structure. Information retrieval from these information is necessary for complementing cardiotocography signals with additional information that is to be implemented in a decision support system. The basic statistical overview (n-gram analysis) helped with the insight into data structure, however more sophisticated methods have to be used as human (and expert) processing of the whole data were out of consideration: over 620,000 text fields contained text reports in natural language with (many) typographical errors, duplicates, ambiguities, syntax errors and many (nonstandard) abbreviations. There was a strong need to efficiently determine the overall structure of the database and discover information that is important from the clinical point of view. We have used three different methods: k-means, self-organizing map and a self-organizing approach inspired by ant-colonies that performed clustering of the records. The records were visualized and revealed the most prominent information structure(s) that were consulted with medical experts and served for further mining from the database. The outcome of this task is a set of ordered or nominal attributes with a structural information that is available for rule discovery mining and automated processing for the research of asphyxia prediction during delivery. The proposed methodology has significantly reduced the processing time of loosely structured textual records for both IT and medical experts.",autonomous vehicle
10.1016/B978-0-12-822420-5.00018-0,journal,Introduction to Machine Olfaction Devices,sciencedirect,2021-12-31,sciencedirect,Index,https://api.elsevier.com/content/article/pii/B9780128224205000180,Unknown,autonomous vehicle
10.1016/B978-0-12-819438-6.00020-7,journal,Digital Media Steganography,sciencedirect,2020-12-31,sciencedirect,12: Digital media steganalysis,https://api.elsevier.com/content/article/pii/B9780128194386000207,"
               
                  Steganography is the process of hiding messages inside an object known as a carrier. The idea is establishing a covert communication channel where messages go unnoticed by observers having access to that channel. Steganalysis is dedicated to the detection of such hidden messages; these messages can be embedded in several different types of digital media, such as images, video, audio, plain text files, and covert channels. Traditional steganalysis schemes are divided into two stages; the first one involves manual extraction of high-end features, and the second one is classification using machine learning (ML) methods. Nowadays, deep learning (DL) can unify both traditional stages into an end-to-end scheme with promising results. In this chapter, we show the most relevant steganalysis techniques using statistical, ML, and DL methods on digital media.
            ",autonomous vehicle
10.1016/j.asoc.2014.01.026,journal,Applied Soft Computing,sciencedirect,2014-05-31,sciencedirect,A support vector machine model for intelligent selection of data representations,https://api.elsevier.com/content/article/pii/S1568494614000453,"
                  The design and implementation of efficient abstract data types are important issues for software developers. Selecting and creating the appropriate data structure for implementing an abstract data type is not a trivial problem for a software developer, as it is hard to anticipate all the usage scenarios of the deployed application. Moreover, it is not clear how to select a good implementation for an abstract data type when access patterns to it are highly variant, or even unpredictable. The problem of automatic data structure selection is a complex one because each particular data structure is usually more efficient for some operations and less efficient for others, that is why a static analysis for choosing the best representation can be inappropriate, as the performed operations cannot be statically predicted. Therefore, we propose a predictive model in which the software system learns to choose the appropriate data representation, at runtime, based on the effective data usage pattern. This paper describes a novel approach in using a support vector machine model in order to dynamically select the most suitable representation for an aggregate according to the software system's execution context. Computational experiments confirm a good performance of the proposed model and indicates the potential of our proposal. The advantages of our approach in comparison with similar existing approaches are also emphasized.
               ",autonomous vehicle
10.1016/j.eng.2021.04.021,journal,Engineering,sciencedirect,2021-09-30,sciencedirect,"Cyber–Physical Production Systems for Data-Driven, Decentralized, and Secure Manufacturing—A Perspective",https://api.elsevier.com/content/article/pii/S2095809921002988,"With the concepts of Industry 4.0 and smart manufacturing gaining popularity, there is a growing notion that conventional manufacturing will witness a transition toward a new paradigm, targeting innovation, automation, better response to customer needs, and intelligent systems. Within this context, this review focuses on the concept of cyber–physical production system (CPPS) and presents a holistic perspective on the role of the CPPS in three key and essential drivers of this transformation: data-driven manufacturing, decentralized manufacturing, and integrated blockchains for data security. The paper aims to connect these three aspects of smart manufacturing and proposes that through the application of data-driven modeling, CPPS will aid in transforming manufacturing to become more intuitive and automated. In turn, automated manufacturing will pave the way for the decentralization of manufacturing. Layering blockchain technologies on top of CPPS will ensure the reliability and security of data sharing and integration across decentralized systems. Each of these claims is supported by relevant case studies recently published in the literature and from the industry; a brief on existing challenges and the way forward is also provided.",autonomous vehicle
10.1016/B978-0-12-815739-8.20001-1,journal,Machine Learning,sciencedirect,2020-12-31,sciencedirect,Index,https://api.elsevier.com/content/article/pii/B9780128157398200011,Unknown,autonomous vehicle
10.1016/B978-0-323-90198-7.09994-8,journal,Principles and Labs for Deep Learning,sciencedirect,2021-12-31,sciencedirect,Index,https://api.elsevier.com/content/article/pii/B9780323901987099948,Unknown,autonomous vehicle
10.1016/bs.host.2016.07.004,journal,Handbook of Statistics,sciencedirect,2016-12-31,sciencedirect,"Chapter 1: Cognitive Computing: Concepts, Architectures, Systems, and Applications",https://api.elsevier.com/content/article/pii/S0169716116300451,"
                  Cognitive computing is an emerging field ushered in by the synergistic confluence of cognitive science, data science, and an array of computing technologies. Cognitive science theories provide frameworks to describe various models of human cognition including how information is represented and processed by the brain. Data science provides processes and systems to extract knowledge from both structured and unstructured data. Cognitive computing employs the computing discipline's theories, methods, and tools to model human cognition. The recent advances in data science and computing disciplines—neuromorphic processors, big data, predictive modeling, machine learning, natural language understanding, and cloud computing—are accelerating advances in cognitive science and cognitive computing.
                  The overarching goal of this chapter is to provide an interdisciplinary introduction to cognitive computing. The focus is on breadth to provide a unified view of the discipline. The chapter begins with an overview of cognitive science, data science, and cognitive computing. The principal technology enablers of cognitive computing are presented next. An overview of three major categories of cognitive architectures is presented, which is followed by a description of cognitive computing systems and their applications. Trends and future research directions in cognitive computing are discussed. The chapter concludes by listing various cognitive computing resources.
               ",autonomous vehicle
10.1016/j.eswa.2019.01.013,journal,Expert Systems with Applications,sciencedirect,2019-05-15,sciencedirect,A knowledge-based system for numerical design of experiments processes in mechanical engineering,https://api.elsevier.com/content/article/pii/S0957417419300120,"
                  This paper describes a specific knowledge-based system (KBS) to assist designers in configuring numerical design of experiments (NDoE) processes efficiently. NDoE processes are applied in product design to improve the quality of product, by taking into account variabilities and uncertainties. NDoE processes are defined by various and complex methodologies to achieve several objectives, as optimization, surrogate modeling or sensitivity analysis. On the other hand, NDoE processes may demand huge computing resources to execute hundreds simulations, and also advanced expert knowledge to set the best configuration amongst numerous possibilities. Designers aim to obtain most useful results with a minimal computational cost as soon as possible. Thus, the configuration step must be as fast as possible, and it must lead to an efficient combination of complex methods, algorithms and hyper-parameters, to obtain valuable information on the product. The proposed KBS and its inference engine, a bayesian network, is detailed and applied to a product developed by automotive industry. The KBS propose new efficient configurations to achieve designers' goal. This application shorten the configuration step of the NDoE process, and enables designers to use more complex methods. It also allows designers to capitalize knowledge and learn from each past NDoE process.
               ",autonomous vehicle
10.1016/B978-0-12-818438-7.00019-8,journal,Artificial Intelligence in Healthcare,sciencedirect,2020-12-31,sciencedirect,Index,https://api.elsevier.com/content/article/pii/B9780128184387000198,Unknown,autonomous vehicle
10.1016/B978-155860759-0/50005-6,journal,Computational Intelligence,sciencedirect,2007-12-31,sciencedirect,chapter five: Neural network concepts and paradigms,https://api.elsevier.com/content/article/pii/B9781558607590500056,"
               This chapter provides an overview of neural networks. Neural networks consist of processing elements and weighted connections. Each layer in a neural network consists of a collection of processing elements. Each processing element (PE) collects the values from all of its input connections, performs a predefined mathematical operation, and produces a single output value. The combination of processing elements and weighted connections creates a neural network topology. A convenient analogy is a directed graph, where the edges are analogous to the connection weights and the nodes are analogous to the processing elements. Neural networks cannot operate without data. Some neural networks use only single patterns and others use pattern pairs. The dimensionality of the input pattern is not necessarily the same as the output pattern. When a network uses only single patterns, it is defined as an autoassociative network, and when a network uses pattern pairs, it is heteroassociative.
            ",autonomous vehicle
10.1016/0301-0082(91)90008-O,journal,Progress in Neurobiology,sciencedirect,1991-12-31,sciencedirect,Generalization and specialization in artificial neural networks,https://api.elsevier.com/content/article/pii/030100829190008O,,autonomous vehicle
10.1016/B978-0-12-820604-1.09993-3,journal,Computational Intelligence and Its Applications in Healthcare,sciencedirect,2020-12-31,sciencedirect,Index,https://api.elsevier.com/content/article/pii/B9780128206041099933,Unknown,autonomous vehicle
10.1016/j.cie.2019.106099,journal,Computers & Industrial Engineering,sciencedirect,2019-11-30,sciencedirect,Understanding Big Data Analytics for Manufacturing Processes: Insights from Literature Review and Multiple Case Studies,https://api.elsevier.com/content/article/pii/S0360835219305686,"
                  Today, we are undoubtedly in the era of data. Big Data Analytics (BDA) is no longer a perspective for all level of the organization. This is of special interest in the manufacturing process with their high capital intensity, time constraints and given the huge amount of data already captured. However, there is a paucity in past literature on BDA to develop better understanding of the capabilities and strategic implications to extract value from BDA. In that vein, the central aim of this paper is to develop a novel model that summarizes the main capabilities of BDA in the context of manufacturing process. This is carried out by relying on the findings of a review of the ongoing research along with a multiple case studies within a leading phosphate derivatives manufacturer to point out the capabilities of BDA in manufacturing processes and outline recommendations to advance research in the field. The findings will help companies to understand the big data analytics capabilities and its potential implications for their manufacturing processes and support them seeking to design more effective BDA-enabler infrastructure.
               ",autonomous vehicle
10.1016/S0166-4115(97)80087-8,journal,Advances in Psychology,sciencedirect,1997-12-31,sciencedirect,Chapter 1: The necessity of Neural Networks,https://api.elsevier.com/content/article/pii/S0166411597800878,,autonomous vehicle
10.1016/j.scs.2020.102080,journal,Sustainable Cities and Society,sciencedirect,2020-05-31,sciencedirect,Providing secure and reliable communication for next generation networks in smart cities,https://api.elsevier.com/content/article/pii/S2210670720300676,"
                  Finding a framework that provides continuous, reliable, secure and sustainable diversified smart city services proves to be challenging in today’s traditional cloud centralized solutions. This article envisions a Mobile Edge Computing (MEC) solution that enables node collaboration among IoT devices to provide reliable and secure communication between devices and the fog layer on one hand, and the fog layer and the cloud layer on the other hand. The solution assumes that collaboration is determined based on nodes’ resource capabilities and cooperation willingness. Resource capabilities are defined using ontologies, while willingness to cooperate is described using a three-factor node criteria, namely: nature, attitude and awareness. A learning method is adopted to identify candidates for the service composition and delivery process. We show that the system does not require extensive training for services to be delivered correct and accurate. The proposed solution reduces the amount of unnecessary traffic flow to and from the edge, by relying on node-to-node communication protocols. Communication to the fog and cloud layers is used for more data and computing-extensive applications, hence, ensuring secure communication protocols to the cloud. Preliminary simulations are conducted to showcase the effectiveness of adapting the proposed framework to achieve smart city sustainability through service reliability and security. Results show that the proposed solution outperforms other semi-cooperative and non-cooperative service composition techniques in terms of efficient service delivery and composition delay, service hit ratio, and suspicious node identification.
               ",autonomous vehicle
10.1016/B978-0-12-817976-5.00036-X,journal,Data Science Applied to Sustainability Analysis,sciencedirect,2021-12-31,sciencedirect,Index,https://api.elsevier.com/content/article/pii/B978012817976500036X,Unknown,autonomous vehicle
10.1016/0004-3702(93)90048-G,journal,Artificial Intelligence,sciencedirect,1993-07-31,sciencedirect,"Introduction to the theory of neural computation: John A. Hertz, Anders S. Krogh and Richard G. Palmer",https://api.elsevier.com/content/article/pii/000437029390048G,,autonomous vehicle
10.1016/B978-0-12-819472-0.09993-7,journal,Trust in Human-Robot Interaction,sciencedirect,2021-12-31,sciencedirect,Index,https://api.elsevier.com/content/article/pii/B9780128194720099937,Unknown,autonomous vehicle
10.1016/B978-0-12-818503-2.00017-4,journal,Heat Transfer Engineering,sciencedirect,2021-12-31,sciencedirect,Index,https://api.elsevier.com/content/article/pii/B9780128185032000174,Unknown,autonomous vehicle
10.1016/j.future.2020.10.013,journal,Future Generation Computer Systems,sciencedirect,2021-02-28,sciencedirect,Finding rising stars through hot topics detection,https://api.elsevier.com/content/article/pii/S0167739X20329903,"
                  Topic modeling methods have usually been applied in the past to identify the research interests of researchers. Observing the scientific growth, the trending topics can be identified as Stable, Hot, or Cold. Finding rising stars (junior researchers, who are at the start of their career) from a bibliometric network is a challenging task, specifically if the researchers have an interest in multiple sub-domains or are working on diverse topics. Existing methods for finding rising stars explore the co-author networks or citation networks, and ignore the textual content, which may help in finding rising stars through hot topics detection over time. A publication contributing to a hot topic can be an indication that the author of that publication may be a rising star and can become an expert in that domain in the future. This study proposes the Hot Topics Rising Star Rank (HTRS-Rank) method for finding rising stars by detecting hot topics. HTRS-Rank finds the junior scholars, who contribute to hot topics at the start of their career and ranks them based on the presence of hot topics in their publications. AMiner five years dataset ranging from 2005–2009 is selected for experimentation. Top 10 researchers are considered to measure the association strength using rank correlation among HTRS-Rank and baseline methods. Experimental results show the efficiency of HTRS-Rank in comparison to the baseline methods. The proposed HTRS Rank (TF–IDF) provides low standard deviation for productivity, citations and sociality as compared to baseline methods for more social and highly cited authors. It is identified that HTRS-Rank (WordNet) emphasizes the semantic similarity of two sentences, whereas HTRS-Rank (TF–IDF) scheme emphasizes the uniqueness or importance of each term, therefore TF–IDF approach performs better than WordNet approach due to having higher correlation with StarRank and WMIRank.
               ",autonomous vehicle
10.1016/B978-012646490-0/50021-4,journal,Soft Computing and Intelligent Systems,sciencedirect,2000-12-31,sciencedirect,CHAPTER 18: Intelligent Control with Neural Networks,https://api.elsevier.com/content/article/pii/B9780126464900500214,,autonomous vehicle
10.1016/B978-0-12-813086-5.20001-7,journal,Biomedical Signal Analysis for Connected Healthcare,sciencedirect,2021-12-31,sciencedirect,Index,https://api.elsevier.com/content/article/pii/B9780128130865200017,Unknown,autonomous vehicle
10.1016/j.jngse.2019.05.011,journal,Journal of Natural Gas Science and Engineering,sciencedirect,2019-08-31,sciencedirect,Casing structural integrity and failure modes in a range of well types - A review,https://api.elsevier.com/content/article/pii/S1875510019301428,"
                  This paper focus on factors attributing to casing failure, their failure mechanism and the resulting failure mode. The casing is a critical component in a well and the main mechanical structural barrier element that provide conduits and avenue for oil and gas production over the well lifecycle and beyond. The casings are normally subjected to material degradation, varying local loads, induced stresses during stimulation, natural fractures, slip and shear during their installation and operation leading to different kinds of casing failure modes. The review paper also covers recent developments in casing integrity assessment techniques and their respective limitations.
                  The taxonomy of the major causes and cases of casing failure in different well types is covered. In addition, an overview of casing trend utilisation and failure mix by grades is provided. The trend of casing utilisation in different wells examined show deep-water and shale gas horizontal wells employing higher tensile grades (P110 & Q125) due to their characteristics. Additionally, this review presents casing failure mixed by grades, with P110 recording the highest failure cases owing to its stiffness, high application in injection wells, shale gas, deep-water and high temperature and high temperature (HPHT) - wells with high failure probability. A summary of existing tools used for the assessment of well integrity issues and their respective limitations is provided and conclusions drawn.
               ",autonomous vehicle
10.1016/j.mineng.2018.09.006,journal,Minerals Engineering,sciencedirect,2018-11-30,sciencedirect,Mechanism characteristic analysis and soft measuring method review for ball mill load based on mechanical vibration and acoustic signals in the grinding process,https://api.elsevier.com/content/article/pii/S0892687518304138,"
                  An operational optimization control for a mineral grinding process is limited by unmeasured load parameter inside a ball mill given its complex and unclear production mechanism. A mechanism characteristic analysis and soft measuring method for mill load parameter based on mechanical vibration and acoustic signals in the mineral grinding process is reviewed in this study. The modeling process based on the mechanical vibration and acoustic signals for the mill load parameters are summarized as a class of intelligent selective ensemble modeling problem. The applied soft measuring strategies for mill load parameter measurement are divided into three types, namely, off-line modeling, online modeling, and virtual sample generation, followed by a detailed discussion. Possible directions for mill load soft measurement techniques are provided for future research. These techniques include a vibration mechanism-based multi-component signal analysis, off-line intelligent ensemble soft measuring model based on simulation operational expert cognitive process, online updating strategy based on intelligently identified samples, and a mill load status intelligent recognition mode based on reinforcement learning strategy.
               ",autonomous vehicle
10.1016/j.procs.2018.10.411,journal,Procedia Computer Science,sciencedirect,2018-12-31,sciencedirect,Analysis of Computational Gene Prioritization Approaches,https://api.elsevier.com/content/article/pii/S1877050918321082,"Even though biological data analysis helps in understanding the chemical processes, handling them is difficult due to their abundant size, heterogeneous nature and access time overheads. Complex disorder diagnostics can be carried out effectively by recognizing the most conformant genes from a set of candidate genes which are having a higher association with the disorder. Traditional gene analysis methods such as gene mutation analysis, single nucleotide polymorphism (SNP) detection and other wet lab techniques are delimited by several factors such as high-cost clinical experiments, unpredictable time consumption and insufficient prior knowledge about genetic materials. They are replaced by efficient computational solutions due to extensive advantages like economical computational cost, appropriate testing and validation strategies, adequate prior information etc. This paper contains a thorough literature review about prevailing methods, tools and data sources primarily used for computational gene prioritization. The aggregation, analysis, interpretation and comparison of different gene prioritization strategies are done with a view to provide an insight into recent trends and traits persist in them. Different validation methods commonly used for gene prioritization are also analysed in this study.",autonomous vehicle
10.1016/B978-0-12-819043-2.00022-8,journal,Innovation in Health Informatics,sciencedirect,2020-12-31,sciencedirect,Index,https://api.elsevier.com/content/article/pii/B9780128190432000228,Unknown,autonomous vehicle
10.1016/S0893-6080(18)30021-2,journal,Neural Networks,sciencedirect,2018-02-28,sciencedirect,List of Editorial Board members,https://api.elsevier.com/content/article/pii/S0893608018300212,,autonomous vehicle
10.1016/B978-0-12-816718-2.00023-3,journal,Deep Learning and Parallel Computing Environment for Bioengineering Systems,sciencedirect,2019-12-31,sciencedirect,Index,https://api.elsevier.com/content/article/pii/B9780128167182000233,Unknown,autonomous vehicle
10.1016/S1018-3639(18)30675-5,journal,Journal of King Saud University - Engineering Sciences,sciencedirect,1997-12-31,sciencedirect,Prediction of Stress-strain Relationships for Reinforced Concrete Sections by Implementing Neural Network Techniques,https://api.elsevier.com/content/article/pii/S1018363918306755,The application of neural networks for predicting the stress-strain relationships of reinforced concrete sections is presented. Computation algorithms in the form of numerical analysis were performed on reinforced concrete sections to simulate existing experimental data. A systematic approach is provided by implementing neural networks in the form of prediction by backpropagation algorithms. The efficiency of neural network techniques is demonstrated by means of reconstructing previous experimental work and evaluating several parameters based on neural networks which are in agreement with experimental results. The procedure establishes valid mathematical relationships without relying on a particular algorithm and depends entirely on the manipulation of numerical data.,autonomous vehicle
10.1016/B978-0-08-042236-7.50006-7,journal,Artificial Intelligence in Real-Time Control 1994,sciencedirect,1995-12-31,sciencedirect,NEURAL NETWORK BASED ADAPTIVE CONTROL,https://api.elsevier.com/content/article/pii/B9780080422367500067,"
               Key Words— Adaptive control; Automatic control; Neural nets; Nonlinear control systems.",autonomous vehicle
10.1016/j.future.2020.11.019,journal,Future Generation Computer Systems,sciencedirect,2021-04-30,sciencedirect,Self-improving system integration: Mastering continuous change,https://api.elsevier.com/content/article/pii/S0167739X20330430,"
                  The research initiative “self-improving system integration” (SISSY) was established with the goal to master the ever-changing demands of system organisation in the presence of autonomous subsystems, evolving architectures, and highly-dynamic open environments. It aims to move integration-related decisions from design-time to run-time, implying a further shift of expertise and responsibility from human engineers to autonomous systems. This introduces a qualitative shift from existing self-adaptive and self-organising systems, moving from self-adaptation based on predefined variation types, towards more open contexts involving novel autonomous subsystems, collaborative behaviours, and emerging goals.
                  In this article, we revisit existing SISSY research efforts and establish a corresponding terminology focusing on how SISSY relates to the broad field of integration sciences. We then investigate SISSY-related research efforts and derive a taxonomy of SISSY technology. This is concluded by establishing a research road-map for developing operational self-improving self-integrating systems.
               ",autonomous vehicle
10.1016/j.cosrev.2020.100255,journal,Computer Science Review,sciencedirect,2020-08-31,sciencedirect,Context Aware Recommendation Systems: A review of the state of the art techniques,https://api.elsevier.com/content/article/pii/S1574013719301406,"
                  Recommendation systems are gaining increasing popularity in many application areas like e-commerce, movie and music recommendations, tourism, news, advertisement, stock markets, social networks etc. Conventional recommendation systems either use content based or collaborative filtering based approaches to model user preferences and give recommendations. These systems usually fail to consider evolving user preferences in different contextual situations. Context Aware Recommendation Systems take different contextual attributes into consideration and try to capture user preferences correctly. This survey focuses on the state-of-the art computational intelligence techniques trying to improve conventional design using contextual information. Further, these techniques are grouped into bio-inspired computing techniques and statistical computing techniques. The literature related to these techniques mentioning their ability to handle challenges faced by Context Aware Recommendation System are presented in this survey. The survey also talks about context inclusion strategies, classification of the contexts used in the literature reviewed, their impact on the problems faced by the recommendation systems, effective usage of these contexts, datasets used in the domain, future research scope in all the reviewed techniques and overall future research directions and challenges.
               ",autonomous vehicle
10.1016/j.neucom.2018.05.033,journal,Neurocomputing,sciencedirect,2018-10-15,sciencedirect,Detecting action tubes via spatial action estimation and temporal path inference,https://api.elsevier.com/content/article/pii/S092523121830585X,"
                  In this paper, we address the problem of action detection in unconstrained video clips. Our approach starts from action detection on object proposals at each frame, then aggregates the frame-level detection results belonging to the same actor across the whole video via linking, associating, and tracking to generate action tubes that are spatially compact and temporally continuous. To achieve the target, a novel action detection model with two-stream architecture is firstly proposed, which utilizes the fused feature from both appearance and motion cues and can be trained end-to-end. Then, the association of the action paths is formulated as a maximum set coverage problem with the results of action detection as a priori. We utilize an incremental search algorithm to obtain all the action proposals at one-pass operation with great efficiency, especially while dealing with the video of long duration or with multiple action instances. Finally, a tracking-by-detection scheme is designed to further refine the generated action paths. Extensive experiments on three validation datasets, UCF-Sports, UCF-101 and J-HMDB, show that the proposed approach advances state-of-the-art action detection performance in terms of both accuracy and proposal quality.
               ",autonomous vehicle
10.1016/j.cie.2020.107076,journal,Computers & Industrial Engineering,sciencedirect,2021-04-30,sciencedirect,A systematic literature review of supply chain decision making supported by the Internet of Things and Big Data Analytics,https://api.elsevier.com/content/article/pii/S0360835220307464,"The willingness to invest in Internet of Things (IoT) and Big Data Analytics (BDA) seems not to depend on supply nor demand of technological innovations. The required sensing and communication technologies have already matured and became affordable for most organizations. Businesses on the other hand require more operational data to address the dynamic and stochastic nature of supply chains. So why should we wait for the actual implementation of tracking and monitoring devices within the supply chain itself? This paper provides an objective overview of state-of-the-art IoT developments in today’s supply chain and logistics research. The main aim is to find examples of academic literature that explain how organizations can incorporate real-time data of physically operating objects into their decision making. A systematic literature review is conducted to gain insight into the IoT’s analytical capabilities, resulting into a list of 79 cross-disciplinary publications. Most researchers integrate the newly developed measuring devices with more traditional ICT infrastructures to either visualize the current way of operating, or to better predict the system’s future state. The resulting health/condition monitoring systems seem to benefit production environments in terms of dependability and quality, while logistics operations are becoming more flexible and faster due to the stronger emphasis on prescriptive analytics (e.g., association and clustering). Further research should extend the IoT’s perception layer with more context-aware devices to promote autonomous decision making, invest in wireless communication networks to stimulate distributed data processing, bridge the gap in between predictive and prescriptive analytics by enriching the spectrum of pattern recognition models used, and validate the benefits of the monitoring systems developed.",autonomous vehicle
10.1016/S0925-2312(97)90023-0,journal,Neurocomputing,sciencedirect,1997-11-30,sciencedirect,"ACNN'98 Ninth Australian Conference on Neural Networks : February 11–13, 1998, Brisbane, Australia",https://api.elsevier.com/content/article/pii/S0925231297900230,,autonomous vehicle
10.1016/j.bica.2013.11.003,journal,Biologically Inspired Cognitive Architectures,sciencedirect,2014-01-31,sciencedirect,Meta-reasoning for predictive error correction: Additional results from abstraction networks with empirical verification procedures,https://api.elsevier.com/content/article/pii/S2212683X13000960,"
                  In Jones and Goel (2012), we describe a meta-reasoning architecture that uses abstraction networks (ANs) and empirical verification procedures (EVPs) to ground self-diagnosis and self-repair of domain knowledge in perception. In particular, we showed that when a hierarchical classifier organized as an AN makes an incorrect prediction, then meta-reasoning can help diagnose and repair the semantics of the concepts in the network. Further, we demonstrated that if an EVP associated with each concept in the network can verify the semantics of that concept at diagnosis time, then the meta-reasoner can perform knowledge diagnosis and repair tractably. In this article, we report on three additional results on the use of perceptually grounded meta-reasoning for correcting prediction errors. Firstly, a new theoretical analysis indicates that the meta-reasoning diagnostic procedure is optimal and establishes the knowledge conditions under which the learning converges. Secondly, an empirical study indicates that the EVPs themselves can be adapted through refining the conceptual semantics. Thirdly, another empirical study shows that if EVPs cannot be defined for all concepts in a hierarchy, the computational technique degrades gracefully. While the theoretical analysis provides a deeper explanation of the sources of power in ANs, the two empirical studies demonstrate ways in which the strong assumptions made by ANs in their most basic form can be relaxed.
               ",autonomous vehicle
10.1016/j.procs.2020.03.391,journal,Procedia Computer Science,sciencedirect,2020-12-31,sciencedirect,Generative Model for NLP Applications based on Component Extraction,https://api.elsevier.com/content/article/pii/S1877050920308577,"People all around the world speak so many different languages, but a Computer System or any other Computerized Machine only understands a single language i.e. binary language (1s and 0s) This system or a process that converts human language to computer understandable language is known as Natural Language Processing (NLP), though various diversified models have suggested so far, yet the need for a generative predictive model which can optimize depending upon the nature of problem being addressed is still an area of research under work. The paper presents a Generative Model for NLP Applications based on significant components extracted from Case Studies. The generative model is a single platform for diversified areas of NLP that can address specific problems relating to read text, hear speech, interpret it, measure sentiment and determine which parts are important. This is achieved by process of elimination once the relevant components are identified. Single platform provides same model generating and reproducing optimized solutions and addressing different issues.",autonomous vehicle
10.1016/j.coisb.2017.04.008,journal,Current Opinion in Systems Biology,sciencedirect,2017-06-30,sciencedirect,Making our way through the world: Towards a functional understanding of the brain's spatial circuits,https://api.elsevier.com/content/article/pii/S2452310017300549,"
                  Many animals make return trips from a home base to gather food and supplies, mate, or survive the seasons. In a world of unreliable and ambiguous cues, localizing within familiar environments and mapping new environments – functions critical for making successful return trips – is a complex problem requiring memory, integration, and inference. We review some key features of the mammalian brain's navigation system and its computational challenges, as well as the task neuroscientists face in understanding how its components interact and function. We argue that synthesizing the wide body of neural phenomenology requires formalization of the navigation problem as one of sequential probabilistic inference, as done in the robotics field of simultaneous localization and mapping (SLAM).
               ",autonomous vehicle
10.1016/j.artint.2018.01.002,journal,Artificial Intelligence,sciencedirect,2018-05-31,sciencedirect,Autonomous agents modelling other agents: A comprehensive survey and open problems,https://api.elsevier.com/content/article/pii/S0004370218300249,"
                  Much research in artificial intelligence is concerned with the development of autonomous agents that can interact effectively with other agents. An important aspect of such agents is the ability to reason about the behaviours of other agents, by constructing models which make predictions about various properties of interest (such as actions, goals, beliefs) of the modelled agents. A variety of modelling approaches now exist which vary widely in their methodology and underlying assumptions, catering to the needs of the different sub-communities within which they were developed and reflecting the different practical uses for which they are intended. The purpose of the present article is to provide a comprehensive survey of the salient modelling methods which can be found in the literature. The article concludes with a discussion of open problems which may form the basis for fruitful future research.
               ",autonomous vehicle
10.1016/B978-0-12-818279-6.00026-8,journal,Thinking Machines,sciencedirect,2021-12-31,sciencedirect,Index,https://api.elsevier.com/content/article/pii/B9780128182796000268,Unknown,autonomous vehicle
10.1016/S0065-2458(08)60408-8,journal,Advances in Computers,sciencedirect,1993-12-31,sciencedirect,A Sociological History of the Neural Network Controversy,https://api.elsevier.com/content/article/pii/S0065245808604088,"
                  This chapter discusses the scientific controversies that have shaped neural network research from a sociological point of view. It looks at the controversy that surrounded Frank Rosenblatt's perceptron machine in the late 1950s and early 1960s. Rosenblatt was well aware of the main problems of his machine, and that he even insisted on them in his books and papers. Emphasis is given on one of the main problems of early neural network research, namely the issue of training multilayer systems. In the middle of the perceptron controversy, Minsky and Papert embarked on a project aimed at showing the limitations of Rosenblatt's perceptron beyond doubt. The chapter analyzes the main results of that project, and shows that Minsky and Papert, and neural network researchers interpreted those results rather differently. It discusses the processes through which this interpretative flexibility was closed and the effects that the crisis of early neural network research had upon the three most important neural network groups of the time, namely Widrow's group, Rosenblatt's group, and the group at SRI. The chapter also looks at the influence that factors like the emergence of symbolic artificial intelligence (AI) and computer technology had on the closure of the neural network controversy. After the closure of the perceptron controversy, symbol-processing remained the dominant approach to AI over the years, until the early 1980s. Some of the most important aspects of that changing context are reviewed and the history of back-propagation is discussed.
               ",autonomous vehicle
10.1016/j.knosys.2021.106970,journal,Knowledge-Based Systems,sciencedirect,2021-07-08,sciencedirect,"A review of multimodal human activity recognition with special emphasis on classification, applications, challenges and future directions",https://api.elsevier.com/content/article/pii/S0950705121002331,"
                  Human activity recognition (HAR) is one of the most important and challenging problems in the computer vision. It has critical application in wide variety of tasks including gaming, human–robot interaction, rehabilitation, sports, health monitoring, video surveillance, and robotics. HAR is challenging due to the complex posture made by the human and multiple people interaction. Various artifacts that commonly appears in the scene such as illuminations variations, clutter, occlusions, background diversity further adds the complexity to HAR. Sensors for multiple modalities could be used to overcome some of these inherent challenges. Such sensors could include an RGB-D camera, infrared sensors, thermal cameras, inertial sensors, etc. This article introduces a comprehensive review of different multimodal human activity recognition methods where different types of sensors being used along with their analytical approaches and fusion methods. Further, this article presents classification and discussion of existing work within seven rational aspects: (a) what are the applications of HAR; (b) what are the single and multi-modality sensing for HAR; (c) what are different vision based approaches for HAR; (d) what and how wearable sensors based system contributes to the HAR; (e) what are different multimodal HAR methods; (f) how a combination of vision and wearable inertial sensors based system contributes to the HAR; and (g) challenges and future directions in HAR. With a more and comprehensive understanding of multimodal human activity recognition, more research in this direction can be motivated and refined.
               ",autonomous vehicle
10.1016/j.neunet.2015.09.011,journal,Neural Networks,sciencedirect,2016-06-30,sciencedirect,Evolving spatio-temporal data machines based on the NeuCube neuromorphic framework: Design methodology and selected applications,https://api.elsevier.com/content/article/pii/S0893608015001860,"
                  The paper describes a new type of evolving connectionist systems (ECOS) called evolving spatio-temporal data machines based on neuromorphic, brain-like information processing principles (eSTDM). These are multi-modular computer systems designed to deal with large and fast spatio/spectro temporal data using spiking neural networks (SNN) as major processing modules. ECOS and eSTDM in particular can learn incrementally from data streams, can include ‘on the fly’ new input variables, new output class labels or regression outputs, can continuously adapt their structure and functionality, can be visualised and interpreted for new knowledge discovery and for a better understanding of the data and the processes that generated it. eSTDM can be used for early event prediction due to the ability of the SNN to spike early, before whole input vectors (they were trained on) are presented. A framework for building eSTDM called NeuCube along with a design methodology for building eSTDM using this is presented. The implementation of this framework in MATLAB, Java, and PyNN (Python) is presented. The latter facilitates the use of neuromorphic hardware platforms to run the eSTDM. Selected examples are given of eSTDM for pattern recognition and early event prediction on EEG data, fMRI data, multisensory seismic data, ecological data, climate data, audio-visual data. Future directions are discussed, including extension of the NeuCube framework for building neurogenetic eSTDM and also new applications of eSTDM.
               ",autonomous vehicle
10.1016/B978-0-323-90118-5.00016-3,journal,Machine Reading Comprehension,sciencedirect,2021-12-31,sciencedirect,Index,https://api.elsevier.com/content/article/pii/B9780323901185000163,Unknown,autonomous vehicle
10.1016/0005-1098(92)90053-I,journal,Automatica,sciencedirect,1992-11-30,sciencedirect,Neural networks for control systems—A survey,https://api.elsevier.com/content/article/pii/000510989290053I,"
                  This paper focuses on the promise of artificial neural networks in the realm of modelling, identification and control of nonlinear systems. The basic ideas and techniques of artificial neural networks are presented in language and notation familiar to control engineers. Applications of a variety of neural network architectures in control are surveyed. We explore the links between the fields of control science and neural networks in a unified presentation and identify key areas for future research.
               ",autonomous vehicle
10.1016/B978-0-12-420248-1.00009-X,journal,Artificial Intelligence in Behavioral and Mental Health Care,sciencedirect,2016-12-31,sciencedirect,Chapter 9: Public Health Surveillance: Predictive Analytics and Big Data,https://api.elsevier.com/content/article/pii/B978012420248100009X,"
               Recent advances in artificial intelligence are providing an unprecedented ability of medical research and clinical organizations to collect and analyze data that is broader in scope and more exacting in detail. New technologies are providing for revolutionary advances, improving our daily lives in health care. However, with any emerging technological advance, there is a potential for abuse of privacy and reduced quality of medical care. This chapter will discuss the process of collecting, analyzing, and applying data, using examples from clinical best practices, as well as the specific review of the Durkheim Project. Closing the discussion, will be a technical analysis of advancing analytic systems.
            ",autonomous vehicle
10.1016/B978-0-12-817216-2.00017-X,journal,Introduction to Algorithms for Data Mining and Machine Learning,sciencedirect,2019-12-31,sciencedirect,Index,https://api.elsevier.com/content/article/pii/B978012817216200017X,Unknown,autonomous vehicle
10.1016/B978-0-12-812364-5.00001-8,journal,Computational Phytochemistry,sciencedirect,2018-12-31,sciencedirect,Chapter 1: An Introduction to Computational Phytochemistry,https://api.elsevier.com/content/article/pii/B9780128123645000018,"
               This chapter provides an overview on ‘Computational Phytochemistry’, an emerging area of research, where computational tools, techniques and methods, artificial intelligence, and mathematical modelling are incorporated to resolve various issues in phytochemical research. In recent years, incorporation of computational techniques has been observed in screening plant materials, plant metabolomics, chemical fingerprinting, chemical taxonomy, biosynthesis and phylogenetic studies, prediction of pharmacological and toxicological properties (virtual screening or in silico studies), and automated structure determination of phytochemicals based on spectroscopic data. In most cases, the introduction of computer-aided approaches saves time and money associated with phytochemical research, ranging from bioactive compound discovery to identifying the metabolomes. In the coming years, ‘Computational Phytochemistry’ is set to transform the way we carry out phytochemical research today.
            ",autonomous vehicle
10.1016/j.pmcj.2006.12.001,journal,Pervasive and Mobile Computing,sciencedirect,2007-03-31,sciencedirect,How smart are our environments? An updated look at the state of the art,https://api.elsevier.com/content/article/pii/S1574119206000642,"
                  In this paper we take a look at the state of the art in smart environments research. The survey is motivated by the recent dramatic increase of activity in the field, and summarizes work in a variety of supporting disciplines. We also discuss the application of smart environments research to health monitoring and assistance, followed by ongoing challenges for continued research.
               ",autonomous vehicle
10.1016/B978-0-12-824536-1.20001-4,journal,Data Science for COVID-19,sciencedirect,2021-12-31,sciencedirect,Index,https://api.elsevier.com/content/article/pii/B9780128245361200014,Unknown,autonomous vehicle
10.1016/B978-0-08-101107-2.00042-7,journal,Handbook of Categorization in Cognitive Science,sciencedirect,2017-12-31,sciencedirect,Chapter 42: The Neurodynamics of Categorization: Critical Challenges and Proposed Solutions,https://api.elsevier.com/content/article/pii/B9780081011072000427,"
               Computational modeling is a useful tool for understanding human categorization. It allows for the testing of structural and functional properties of the cognitive process. In particular, connectionist networks are useful geometric models of categorization. Recurrent networks typically use Hebbian learning to convert the stimulus space into a feedback subspace sufficient to categorize new stimuli. In the present chapter, we review the evolution of some recurrent networks for modeling categorization by examining challenges they faced and proposed solutions. First, we examine the recurrent auto-associative memory (RAM) class of networks. Specifically, we examine the problems of divergence and noise and review some proposed solutions. Second, we review the progression of research on bidirectional heteroassociative memory (BAM) networks that are capable of both auto-associative and heteroassociative memories. Third, we introduce a hybrid model of feature-extracting bidirectional associative memory (FEBAM). By unifying properties from both BAM class networks and principal component analysis (PCA) networks, this hybrid presents a possible solution to limitations of previous models, such as the BAM, and is a potential candidate for effectively modeling the categorization process in humans.
            ",autonomous vehicle
10.1016/bs.pmch.2017.12.003,journal,Progress in Medicinal Chemistry,sciencedirect,2018-12-31,sciencedirect,Chapter Five: Big Data in Drug Discovery,https://api.elsevier.com/content/article/pii/S0079646817300243,"
                  Interpretation of Big Data in the drug discovery community should enhance project timelines and reduce clinical attrition through improved early decision making. The issues we encounter start with the sheer volume of data and how we first ingest it before building an infrastructure to house it to make use of the data in an efficient and productive way. There are many problems associated with the data itself including general reproducibility, but often, it is the context surrounding an experiment that is critical to success. Help, in the form of artificial intelligence (AI), is required to understand and translate the context. On the back of natural language processing pipelines, AI is also used to prospectively generate new hypotheses by linking data together. We explain Big Data from the context of biology, chemistry and clinical trials, showcasing some of the impressive public domain sources and initiatives now available for interrogation.
               ",autonomous vehicle
10.1016/B978-0-12-817736-5.09987-7,journal,Machine Learning for Subsurface Characterization,sciencedirect,2020-12-31,sciencedirect,Preface,https://api.elsevier.com/content/article/pii/B9780128177365099877,Unknown,autonomous vehicle
10.1016/j.jnca.2014.01.005,journal,Journal of Network and Computer Applications,sciencedirect,2014-05-31,sciencedirect,A comparative simulation study of TCP/AQM systems for evaluating the potential of neuron-based AQM schemes,https://api.elsevier.com/content/article/pii/S108480451400006X,"
                  In recent years, several neuron-based active queue management (AQM) schemes have been proposed. Such schemes exhibit important attributes including fast convergence with high accuracy to a desired queue length. This paper presents extensive comparative simulation results for four neural AQM schemes, namely, Neuron PID, AN-AQM, FAPIDNN, NRL, versus three traditional AQM schemes (ARED, REM and PI) together with a modified PI scheme named IAPI over a wide range of conditions and scenarios. For all schemes, we test their performance in various environments. Through extensive numerical comparisons, we demonstrate that the neuron-based schemes generally achieve faster convergence to queue length target, with smaller queue length jitter. We further demonstrate one order of magnitude reduction in the standard deviation of the end-to-end packet delay by the four neuron-based schemes over the other schemes, when the queueing delay is the dominant delay component. These advantages of neuronal schemes deserve recognition despite the fact that no proof of stability is available for such schemes.
               ",autonomous vehicle
10.1016/0925-2312(91)90018-7,journal,Neurocomputing,sciencedirect,1991-08-31,sciencedirect,Connectionist networks for constraint satisfaction,https://api.elsevier.com/content/article/pii/0925231291900187,"
                  Logic Constraint Satisfaction problems are in general NP-hard and a general deterministic polynomial time algorithm is not known. Since several Logic Constraint Problems can be reduced in polynomial time to the satisfaction of a Conjunctive Normal Form (CNF-SAT), this case is very important. We present here a technique to solve CNF-SAT by means of a class of simulated neural networks which are trained through a supervised procedure. The results of significant tests are described.
               ",autonomous vehicle
10.1016/B978-0-08-042236-7.50028-6,journal,Artificial Intelligence in Real-Time Control 1994,sciencedirect,1995-12-31,sciencedirect,NEURAL NETWORKS FOR ROBOT CONTROL,https://api.elsevier.com/content/article/pii/B9780080422367500286,"
               Key Words. Industrial Robots, Nonlinear Systems, Neural Nets, System identification, Adaptive Control",autonomous vehicle
10.1016/0066-4138(94)90059-0,journal,Annual Review in Automatic Programming,sciencedirect,1994-12-31,sciencedirect,Neural networks for robot control,https://api.elsevier.com/content/article/pii/0066413894900590,"
                  This paper describes the use of neural networks in diferent domains of robot control. Three robot control problems, relevant to a broad range of robotics applications, are analyzed, with a review of the state of the art and a description of current research by the authors, highlighting the advantages of the use of neural networks with respect to conventional techniques.
               ",autonomous vehicle
10.1016/j.neucom.2020.12.114,journal,Neurocomputing,sciencedirect,2021-05-07,sciencedirect,The theoretical research of generative adversarial networks: an overview,https://api.elsevier.com/content/article/pii/S0925231220320269,"
                  Generative adversarial networks (GAN) has received great attention and made great progress since its emergence in 2014. In this paper, we focus on the theoretical achievements of GAN and discuss them in detail for readers who wish to know more about GAN. Based on the number of the implemented network architectures, we category the improved methods into two groups: GAN variants, which are composed of two networks, to improve the performance by adding some regularization to the loss function; hybrid GANs, which are usually combined with other generative models to improve the training stability. For GAN variants, we discuss the theoretical results of the distribution divergence, training dynamics and various improved methods. For hybrid GANs, we introduce the improved methods of combining encoder, autoencoder or VAE. We also cover some other important issues, such as the quantify metrics of generated samples and the basic construction structure. In addition, we discuss the advantages of the GAN over other deep generative models, the future directions worthy of study, as well as the open issues that the community should further address.
               ",autonomous vehicle
10.1016/B978-0-12-817024-3.00008-8,journal,Smart Cities and Artificial Intelligence,sciencedirect,2020-12-31,sciencedirect,Chapter 8: Smart city functions,https://api.elsevier.com/content/article/pii/B9780128170243000088,"
               The Internet of Things (IoT), Artificial Intelligence (AI) and 5G are establishing the building blocks of the post human smart city. These high impact industrial and commercial applications are contextualized within the evolutionary development and hierarchy of smart city functions. By exploring their inter-relationship and co-dependencies of the systems functions and subsystems at structural, operational and strategic levels, we can begin to visualize the impact these technologies will have on the design, operations and DNA of smart cities. Building on a combination of the hardware infrastructure that enables smart cities and diverse forms of AI and their capabilities and applications, smart city functions — Smart Mobility, Smart Environment, Smart People, Smart Governance, Smart Economy and Smart Living — are connected and optimized as individual and collective functions within an integrated city operating system. This new alignment allows cities to function much better, reducing noise within system functionality while creating efficiencies. Edge computing is an example of system optimization through the reduction of energy consumption, more efficient data processing and overcoming latency and bandwidth issues. The facilitated convergence process, enabled by AI, contributes to the desired outcome. The true smart city is not just about technology, but convergence on a new holistic, biomimetic system architecture level that optimizes the health and wellbeing of the users, the city and the operating system itself.
            ",autonomous vehicle
10.1016/B978-0-12-820045-2.00007-6,journal,"The Era of Artificial Intelligence, Machine Learning, and Data Science in the Pharmaceutical Industry",sciencedirect,2021-12-31,sciencedirect,Chapter 6: Lead optimization,https://api.elsevier.com/content/article/pii/B9780128200452000076,"
               A lead compound has been identified and now must be optimized to become a candidate drug. Lead optimization refers to the process of designing and improving a preidentified lead compound, and involves manipulation of multiple parameters of the compound, relying on chemical modifications to the compound. During this process, synthetic changes are made to optimize the compounds ADMET properties (absorption, distribution, metabolism, excretion, and toxicity) as well as the compound’s activity, potency, and selectivity. In addition, the compound needs to remain novel to allow for it to be patented. These ADMET properties relate to ability of the compound to be taken up into the blood stream (absorption), how the compound is moved around the body to its desired site (distribution), how well the compounds are broken down once in the body (metabolism), how the compounds and any metabolites are removed from the body (excretion) and any negative effect the compound will have on the body (toxicity). However, these properties are not only considered during the lead optimization phase and are monitored throughout earlier drug development stages.
            ",autonomous vehicle
10.1016/j.comcom.2019.10.014,journal,Computer Communications,sciencedirect,2020-01-31,sciencedirect,"Path planning techniques for unmanned aerial vehicles: A review, solutions, and challenges",https://api.elsevier.com/content/article/pii/S0140366419308539,"
                  Path planning is one of the most important problems to be explored in unmanned aerial vehicles (UAVs) for finding an optimal path between source and destination. Although, in literature, a lot of research proposals exist on the path planning problems of UAVs but still issues of target location and identification persist keeping in view of the high mobility of UAVs. To solve these issues in UAVs path planning, optimal decisions need to be taken for various mission-critical operations performed by UAVs. These decisions require a map or graph of the mission environment so that UAVs are aware of their locations with respect to the map or graph. Keeping focus on the aforementioned points, this paper analyzes various UAVs path planning techniques used over the past many years. The aim of path planning techniques is not only to find an optimal and shortest path but also to provide the collision-free environment to the UAVs. It is important to have path planning techniques to compute a safe path in the shortest possible time to the final destination. In this paper, various path planning techniques for UAVs are classified into three broad categories, i.e., representative techniques, cooperative techniques, and non-cooperative techniques. With these techniques, coverage and connectivity of the UAVs network communication are discussed and analyzed. Based on each category of UAVs path planning, a critical analysis of the existing proposals has also been done. For better understanding, various comparison tables using parameters such as-path length, optimality, completeness, cost-efficiency, time efficiency, energy-efficiency, robustness and collision avoidance are also included in the text. In addition, a number of open research problems based on UAVs path planning and UAVs network communication are explored to provide deep insights to the readers.
               ",autonomous vehicle
10.1016/j.procs.2016.08.249,journal,Procedia Computer Science,sciencedirect,2016-12-31,sciencedirect,Activity Detection in Smart Home Environment,https://api.elsevier.com/content/article/pii/S1877050916320609,"Detection of human activities is a set of techniques that can be used in wide range of applications, including smart homes and healthcare. In this paper we focus on activity detection in a smart home environment, more specifically on detecting entrances to a room and exits from a room in a home or office space. This information can be used in applications that control HVAC (heating, ventilation, and air conditioning) and lighting systems, or in Ambient Assisted Living (AAL) applications which monitor the people's wellbeing. In our approach we use data from two simple sensors, passive infrared sensor (PIR) which monitors presence and hall effect sensor which monitors whether the door is opened or closed. This installation is non-intrusive and quite simple because the sensor node to which sensors are connected is battery powered, and no additional work to ensure power supply needs to be performed. Two approaches for activity detection are proposed, first based on a sliding window, and the other based on artificial neural network (ANN). The algorithms are tested on a dataset collected in our laboratory environment.",autonomous vehicle
10.1016/0952-1976(93)90026-T,journal,Engineering Applications of Artificial Intelligence,sciencedirect,1993-04-30,sciencedirect,Training backpropagation and CMAC neural networks for control of a SCARA robot,https://api.elsevier.com/content/article/pii/095219769390026T,"
                  The dynamic control of a robotic manipulator is accomplished by the computation and application of actuating torques required for the manipulator to follow desired trajectories. A considerable amount of work has been reported in the literature concerning the application of classical model-based and adaptive control techniques to the above problem. However, many of the available schemes suffer from the fact that they require an accurate model of the robot dynamics, including nonlinearities, which may be difficult to obtain beforehand.
                  In order to address the problem of adaptive control in unknown environments, it is possible to utilize artificial neural networks to learn the characteristics of the system rather than having to prespecify an explicit system model. In this paper, two artificial-neural-network-based strategies are implemented for the accurate trajectory tracking by a SCARA-type IBM 7540 robot. The performance of a backpropagation-based neural controller is compared with that of one based on a scheme similar to Albus' Cerebellar Model Articulation Controller (CMAC)1 [Albus J. S. Trans. ASME J. Dynamic Syst. Measur. Control, pp. 220–227 (1975)].
               ",autonomous vehicle
10.1016/j.comcom.2019.10.014,journal,Computer Communications,sciencedirect,2020-01-31,sciencedirect,"Path planning techniques for unmanned aerial vehicles: A review, solutions, and challenges",https://api.elsevier.com/content/article/pii/S0140366419308539,"
                  Path planning is one of the most important problems to be explored in unmanned aerial vehicles (UAVs) for finding an optimal path between source and destination. Although, in literature, a lot of research proposals exist on the path planning problems of UAVs but still issues of target location and identification persist keeping in view of the high mobility of UAVs. To solve these issues in UAVs path planning, optimal decisions need to be taken for various mission-critical operations performed by UAVs. These decisions require a map or graph of the mission environment so that UAVs are aware of their locations with respect to the map or graph. Keeping focus on the aforementioned points, this paper analyzes various UAVs path planning techniques used over the past many years. The aim of path planning techniques is not only to find an optimal and shortest path but also to provide the collision-free environment to the UAVs. It is important to have path planning techniques to compute a safe path in the shortest possible time to the final destination. In this paper, various path planning techniques for UAVs are classified into three broad categories, i.e., representative techniques, cooperative techniques, and non-cooperative techniques. With these techniques, coverage and connectivity of the UAVs network communication are discussed and analyzed. Based on each category of UAVs path planning, a critical analysis of the existing proposals has also been done. For better understanding, various comparison tables using parameters such as-path length, optimality, completeness, cost-efficiency, time efficiency, energy-efficiency, robustness and collision avoidance are also included in the text. In addition, a number of open research problems based on UAVs path planning and UAVs network communication are explored to provide deep insights to the readers.
               ",autonomous vehicle
10.1016/j.rser.2019.109596,journal,Renewable and Sustainable Energy Reviews,sciencedirect,2020-03-31,sciencedirect,Thorough state-of-the-art analysis of electric and hybrid vehicle powertrains: Topologies and integrated energy management strategies,https://api.elsevier.com/content/article/pii/S1364032119308044,"Hybrid and electric vehicles have been demonstrated as auspicious solutions for ensuring improvements in fuel saving and emission reductions. From the system design perspective, there are numerous indicators affecting the performance of such vehicles, in which the powertrain type, component configuration, and energy management strategy (EMS) play a key role. Achieving an energy-efficient powertrain requires tackling several conflicting control objectives such as the drivability, fuel economy, reduced emissions, and battery state of charge preservation, which make the EMS the most crucial aspect of powertrain system design. Accordingly, in the present study, various powertrain systems and topologies of (plug-in) hybrid electric vehicles and full-electric vehicles are assessed. In addition, EMSs as applied in the literature are systematically surveyed for a qualitative investigation, classification, and comparison of existing approaches in terms of the principles, advantages, and drawbacks through a comprehensive review. Furthermore, potential challenges considering the gaps in research are addressed, and directives paving the way toward further development of powertrains and EMSs in all respects are thoroughly provided.",autonomous vehicle
10.1016/j.ipm.2019.03.004,journal,Information Processing & Management,sciencedirect,2020-03-31,sciencedirect,"An overview of online fake news: Characterization, detection, and discussion",https://api.elsevier.com/content/article/pii/S0306457318306794,"
                  Over the recent years, the growth of online social media has greatly facilitated the way people communicate with each other. Users of online social media share information, connect with other people and stay informed about trending events. However, much recent information appearing on social media is dubious and, in some cases, intended to mislead. Such content is often called fake news. Large amounts of online fake news has the potential to cause serious problems in society. Many point to the 2016 U.S. presidential election campaign as having been influenced by fake news. Subsequent to this election, the term has entered the mainstream vernacular. Moreover it has drawn the attention of industry and academia, seeking to understand its origins, distribution and effects.
                  Of critical interest is the ability to detect when online content is untrue and intended to mislead. This is technically challenging for several reasons. Using social media tools, content is easily generated and quickly spread, leading to a large volume of content to analyse. Online information is very diverse, covering a large number of subjects, which contributes complexity to this task. The truth and intent of any statement often cannot be assessed by computers alone, so efforts must depend on collaboration between humans and technology. For instance, some content that is deemed by experts of being false and intended to mislead are available. While these sources are in limited supply, they can form a basis for such a shared effort.
                  In this survey, we present a comprehensive overview of the finding to date relating to fake news. We characterize the negative impact of online fake news, and the state-of-the-art in detection methods. Many of these rely on identifying features of the users, content, and context that indicate misinformation. We also study existing datasets that have been used for classifying fake news. Finally, we propose promising research directions for online fake news analysis.
               ",autonomous vehicle
10.1016/j.procir.2021.03.090,journal,Procedia CIRP,sciencedirect,2021-12-31,sciencedirect,Identification of a novel architecture for production planning and control in consideration of biomimetic algorithms,https://api.elsevier.com/content/article/pii/S2212827121003838,"Despite great progress in the academic study of production planning and control (PPC), current PPC systems still fail to fully meet industry’s needs. The approach presented here aims at a more functional and user-friendly PPC by conceptualising a novel idea that leverages the potentials of biomimetic algorithms. The proposed architecture consists of four core modules: order release module, digital shadow, event detection and processing, and decision-making module. Their interactions provide the functionality of the system while emphasising the workers’ key role in the socio-technical PPC system. Requirements for the implementation of the architecture as well as starting points for future research are identified along with the concept.",autonomous vehicle
10.1016/j.eswa.2020.113679,journal,Expert Systems with Applications,sciencedirect,2021-03-01,sciencedirect,Automatic text summarization: A comprehensive survey,https://api.elsevier.com/content/article/pii/S0957417420305030,"
                  Automatic Text Summarization (ATS) is becoming much more important because of the huge amount of textual content that grows exponentially on the Internet and the various archives of news articles, scientific papers, legal documents, etc. Manual text summarization consumes a lot of time, effort, cost, and even becomes impractical with the gigantic amount of textual content. Researchers have been trying to improve ATS techniques since the 1950s. ATS approaches are either extractive, abstractive, or hybrid. The extractive approach selects the most important sentences in the input document(s) then concatenates them to form the summary. The abstractive approach represents the input document(s) in an intermediate representation then generates the summary with sentences that are different than the original sentences. The hybrid approach combines both the extractive and abstractive approaches. Despite all the proposed methods, the generated summaries are still far away from the human-generated summaries. Most researches focus on the extractive approach. It is required to focus more on the abstractive and hybrid approaches. This research provides a comprehensive survey for the researchers by presenting the different aspects of ATS: approaches, methods, building blocks, techniques, datasets, evaluation methods, and future research directions.
               ",autonomous vehicle
10.1016/j.tcs.2008.02.011,journal,Theoretical Computer Science,sciencedirect,2008-08-20,sciencedirect,Theoretical advances in artificial immune systems,https://api.elsevier.com/content/article/pii/S0304397508001059,"Artificial immune systems (AIS) constitute a relatively new area of bio-inspired computing. Biological models of the natural immune system, in particular the theories of clonal selection, immune networks and negative selection, have provided the inspiration for AIS algorithms. Moreover, such algorithms have been successfully employed in a wide variety of different application areas. However, despite these practical successes, until recently there has been a dearth of theory to justify their use. In this paper, the existing theoretical work on AIS is reviewed. After the presentation of a simple example of each of the three main types of AIS algorithm (that is, clonal selection, immune network and negative selection algorithms respectively), details of the theoretical analysis for each of these types are given. Some of the future challenges in this area are also highlighted.",autonomous vehicle
10.1016/0165-0114(95)00196-4,journal,Fuzzy Sets and Systems,sciencedirect,1996-06-24,sciencedirect,Evolving fuzzy rule based controllers using genetic algorithms,https://api.elsevier.com/content/article/pii/0165011495001964,"
                  The synthesis of genetics-based machine learning and fuzzy logic is beginning to show promise as a potent tool in solving complex control problems in multi-variate non-linear systems. In this paper an overview of current research applying the genetic algorithm to fuzzy rule based control is presented. A novel approach to genetics-based machine learning of fuzzy controllers, called a Pittsburgh Fuzzy Classifier System # 1 (P-FCS1) is proposed. P-FCS1 is based on the Pittsburgh model of learning classifier systems and employs variable length rule-sets and simultaneously evolves fuzzy set membership functions and relations. A new crossover operator which respects the functional linkage between fuzzy rules with overlapping input fuzzy set membership functions is introduced. Experimental results using P-FCS 1 are reported and compared with other published results. Application of P-FCS1 to a distributed control problem (dynamic routing in computer networks) is also described and experimental results are presented.
               ",autonomous vehicle
10.1016/j.imavis.2018.09.017,journal,Image and Vision Computing,sciencedirect,2018-12-31,sciencedirect,Image annotation: Then and now,https://api.elsevier.com/content/article/pii/S0262885618301628,"
                  Automatic image annotation (AIA) plays a vital role in dealing with the exponentially growing digital images. Image annotation helps in effective retrieval, organization, classification, auto-illustration, etc. of the image. It started in early 1990. However, in the last three decades, there has been extensive research in AIA, and various new approaches have been advanced. In this article, we review more than 200 references related to image annotation proposed in the last three decades. This paper is an attempt to discuss predominant approaches, its constraints and ways to deal. Each segment of the article exhibits a discourse to expound the finding and future research directions and their hurdles. This paper also presents performance evaluation measures with relevant and influential image annotation database.
               ",autonomous vehicle
10.1016/j.bbagrm.2021.194753,journal,Biochimica et Biophysica Acta (BBA) - Gene Regulatory Mechanisms,sciencedirect,2021-12-31,sciencedirect,Lisen&Curate: A platform to facilitate gathering textual evidence for curation of regulation of transcription initiation in bacteria,https://api.elsevier.com/content/article/pii/S1874939921000717,"The number of published papers in biomedical research makes it rather impossible for a researcher to keep up to date. This is where manually curated databases contribute facilitating the access to knowledge. However, the structure required by databases strongly limits the type of valuable information that can be incorporated. Here, we present Lisen&Curate, a curation system that facilitates linking sentences or part of sentences (both considered sources) in articles with their corresponding curated objects, so that rich additional information of these objects is easily available to users. These sources are going to be offered both within RegulonDB and a new database, L-Regulon. To show the relevance of our work, two senior curators performed a curation of 31 articles on the regulation of transcription initiation of E. coli using Lisen&Curate. As a result, 194 objects were curated and 781 sources were recorded. We also found that these sources are useful to develop automatic approaches to detect objects in articles by observing word frequency patterns and by carrying out an open information extraction task. Sources may help to elaborate a controlled vocabulary of experimental methods. Finally, we discuss our ecosystem of interconnected applications, RegulonDB, L-Regulon, and Lisen&Curate, to facilitate the access to knowledge on regulation of transcription initiation in bacteria. We see our proposal as the starting point to change the way experimentalists connect a piece of knowledge with its evidence using RegulonDB.",autonomous vehicle
10.1016/bs.hna.2019.07.001,journal,Handbook of Numerical Analysis,sciencedirect,2019-12-31,sciencedirect,Chapter 15: Image and surface registration,https://api.elsevier.com/content/article/pii/S1570865919300146,"
                  Image registration is one of the most useful and practical applications of image analysis. Among its many tasks are the tracking of changes between data from different time points or motion correction. Moreover, superimposing complementary information across image modalities is needed, as new imaging modalities emerge in the field. This chapter presents a review of the fundamental ideas and models in the field. The goal is to reflect the current state of the art of image registration (IR) to motivate the readers to refine these models, and to enable the tackling of new challenges as they arise. After discussing the background (Section 2), we review main components of a variational model: a distance measure or data fidelity term (Section 3), regularization to ensure existence of solutions and constraints to further restrict the wanted transformation (Section 4). We also discuss diffeomorphic approaches which ensure local invertibility. We present the surface registration (SR) modelling in the same framework of variational models in Section 5 where the close relationship between IR and SR is also discussed, while we briefly discuss the numerical methods for IR and SR in Section 6. Finally in Section 7, we touch upon the main ideas in deep learning-based approaches for registration.
               ",autonomous vehicle
10.1016/j.langsci.2017.04.003,journal,Language Sciences,sciencedirect,2017-07-31,sciencedirect,"Language and other complex behaviors: Unifying characteristics, computational models, neural mechanisms",https://api.elsevier.com/content/article/pii/S0388000117300128,"
                  Similar to other complex behaviors, language is dynamic, social, multimodal, patterned, and purposive, its purpose being to promote desirable actions or thoughts in others and self (Edelman, 2017b). An analysis of the functional characteristics shared by complex sequential behaviors suggests that they all present a common overarching computational problem: dynamically controlled constrained navigation in concrete or abstract situation spaces. With this conceptual framework in mind, I compare and contrast computational models of language and evaluate their potential for explaining linguistic behavior and for elucidating the brain mechanisms that support it.
               ",autonomous vehicle
10.1016/j.ins.2006.08.001,journal,Information Sciences,sciencedirect,2007-03-01,sciencedirect,Computationally intelligent agents in economics and finance,https://api.elsevier.com/content/article/pii/S0020025506002301,"
                  This paper is an editorial guide for the second special issue on Computational Intelligence in economics and finance, which is a continuation of the special issue of Information Sciences, Vol. 170, No. 1. This second issue appears as a part of the outcome from the 3rd International Workshop on Computational Intelligence in Economics and Finance, which was held in Cary, North Carolina, September 26–30, 2003. This paper offers some main highlights of this event, with a particular emphasis on some of the observed progress made in this research field, and a brief introduction to the papers included in this special issue.
               ",autonomous vehicle
10.1016/B978-0-12-814761-0.00030-7,journal,Data Science,sciencedirect,2019-12-31,sciencedirect,Index,https://api.elsevier.com/content/article/pii/B9780128147610000307,Unknown,autonomous vehicle
10.1016/B978-0-12-812130-6.00004-4,journal,Intelligent Data Sensing and Processing for Health and Well-Being Applications,sciencedirect,2018-12-31,sciencedirect,Chapter 4: Tangible User Interfaces for Ambient Assisted Working,https://api.elsevier.com/content/article/pii/B9780128121306000044,"
               This chapter presents an overview of ambient intelligence, as well as the concepts for which it has served as a basis: ambient assisted living (AAL) and, particularly, ambient assisted working (AAW). This chapter also explains the characteristics of tangible user interfaces (TUIs), addressing their advantages as well as their limitations. The integration of TUIs in AAW is proposed as an alternative for interaction between users and interactive systems present in a work environment, describing some aspects that must be considered before this can happen, such as the integration of some kind of intelligence in TUIs to enable the identification of individual users (or type of user) and automatic adaptation to users.
            ",autonomous vehicle
10.3182/20050703-6-CZ-1902.01276,journal,IFAC Proceedings Volumes,sciencedirect,2005-12-31,sciencedirect,SURVEY OF INTELLIGENT CONTROL ALGORITHMS FOR HUMANOID ROBOTS,https://api.elsevier.com/content/article/pii/S1474667016372883,"
                  This paper focusses on the application of intelligent control techniques (neural networks, fuzzy logic and genetic algorithms) and their hybrid forms (neuro-fuzzy networks, neuro-genetic and fuzzy-genetic algorithms) in the area of humanoid robotic systems. Overall, this survey covers a broad selection of examples that will serve to demonstrate the advantages and disadvantages of the application of intelligent control techniques.
               ",autonomous vehicle
10.1016/B978-0-12-821187-8.00006-X,journal,Intelligent IoT Systems in Personalized Health Care,sciencedirect,2021-12-31,sciencedirect,Chapter Six: A study on security privacy issues and solutions in internet of medical things—A review,https://api.elsevier.com/content/article/pii/B978012821187800006X,"
               In recent times, the Internet of medical things (IoMT) has become an emerging technology in the field of modern communication systems. IoMT has a collection of physical things that embed with electronic devices, sensors, and actuators to connect via wireless communication without human interactions. These are primarily equipped with sensors (pressure sensor, temperature sensor, smoke sensor, etc.) that have less computing power to deploy in various real-time environments like retail, e-parking, e-healthcare, telecom, and finance. In the health-care domain within the style of fitness bands, wearables, wireless devices (heart rate monitor), etc. offer accurate services to the patients for treatment. Notably, these sorts of devices will provide remainders to the patient for appointments, etc. Modern health-care applications play a vital role in safety, health, and attention for billions of human lives from illness. Before this, technology interaction between doctors and patients was limited. There is no way to monitor the patient’s physical condition continuously by the specialist or hospitals. Body area network (BAN) is a novel tendency in health-care expertise that delivers distant gadgets to gather patient information. This collected information is stored in the BAN or shared with a particular hospital and doctor for treatment. Also, when the Patient’s confidential data is automatically stored in the public network, it increases the lack of privacy. But smart health-care designers do not consider the threats. Attackers can take the patient’s details from the public network for vulnerable activities. The applications involved in IoMT development need to be considered as complex issues before their launch, which needs to apply more safety and confidentiality issues. The main aim of this paper is to investigate the current safekeeping and secrecy issues and also surviving solutions to remote health-care applications. We reviewed various solution mechanisms for key generation and bioinspired optimization approaches to find novel solutions to complex issues on the web application in the health-care division.
            ",autonomous vehicle
10.1016/j.anucene.2020.107861,journal,Annals of Nuclear Energy,sciencedirect,2021-01-31,sciencedirect,Development and assessment of a nearly autonomous management and control system for advanced reactors,https://api.elsevier.com/content/article/pii/S0306454920305594,"
                  This paper develops a Nearly Autonomous Management and Control (NAMAC) system for advanced reactors. The development process of NAMAC is characterized by a three layer-layer architecture: knowledge base, the Digital Twin (DT) developmental layer, and the NAMAC operational layer. The DT is described as a knowledge acquisition system from the knowledge base for intended uses in the NAMAC system. A set of DTs with different functions is developed with acceptable performance and assembled according to the NAMAC operational workflow to furnish recommendations to operators. To demonstrate the capability of the NAMAC system, a case study is designed, where a baseline NAMAC is implemented for operating a simulator of the Experimental Breeder Reactor II during a single loss of flow accident. When NAMAC is operated in the training domain, it can provide reasonable recommendations that prevent the peak fuel centerline temperature from exceeding a safety criterion.
               ",autonomous vehicle
10.1016/j.knosys.2018.06.025,journal,Knowledge-Based Systems,sciencedirect,2018-11-01,sciencedirect,An improved feature selection algorithm based on graph clustering and ant colony optimization,https://api.elsevier.com/content/article/pii/S0950705118303344,"
                  Dimensionality reduction is an important preprocessing step to improve the performance of machine learning algorithms. Feature selection methods can efficiently speed up the learning process and improve the overall classification accuracy by reducing the computational complexity. Among the feature selection methods, multivariate methods are more effective in removing irrelevant and redundant features. An efficient multivariate feature selection method, optimization method, called ‘graph clustering based ant colony optimization (GCACO)’ has been recently introduced and shown to outperform other well-known feature selection methods. In the GCACO, features are divided into communities (clusters) in the entire feature space represented as a graph by an efficient community detection algorithm. An ACO-based search strategy is then used to select an optimal feature subset from the initial set of features. In this paper, a modified GCACO algorithm called MGCACO is presented to significantly improve the performance of the GCACO. Performance of the MGCACO algorithm was assessed by testing it on several standard benchmark datasets and sleep EEG data. The performance of the MGCACO was compared to those obtained using the original GCACO and other well-known filtering methods available in the literature. The MGCACO achieved superior performance over the GCACO and other univariate and multivariate algorithms with up to 10%. The MGCACO also exhibited higher efficiency in reducing the number of features all by keeping the classification accuracy maximum.
               ",autonomous vehicle
10.1016/j.autcon.2017.01.019,journal,Automation in Construction,sciencedirect,2017-06-30,sciencedirect,Recognition and evaluation of bridge cracks with modified active contour model and greedy search-based support vector machine,https://api.elsevier.com/content/article/pii/S0926580517300602,"
                  Concrete cracks are the most important representation for evaluating the bridge health condition and conducting to take appropriate actions to optimize expenditure on maintenance and rehabilitation. In this paper, we develop a fully-automatic machine learning based algorithm for extracting cracks from concrete bridge images, which combines a modified region-based active contour model for image segmentation and the linear support vector machine using greedy search strategy for noise elimination. In practice, the crack detection is a challenging problem because of (1) subtle difference between the cracks and the noises, (2) inconsistent intensity along the cracks, and (3) possible shadow regions with similar intensity to the cracks. To solve these problems, the proposed method consists of three steps. First, we build a high-precision image acquisition framework, which can automatically collect image sequences from the lower bridge slab and fuse the multiple sensor data for computing crack parameters. Second, we develop a modified region-based active contour model combined with the iterated Canny operator for the concrete image segmentation. Finally, we utilize the novel feature selection approach based on the linear support vector machine with a greedy search strategy for noise elimination. After that, we provide a crack width calculation method which combined the binary image with the gray scale image information. We evaluate the proposed method on a collection of 1200 real bridge images, which gathered from 10 existing bridges on various weathers, and the experimental results show that the proposed method achieves a better performance than several up-to-date algorithms.
               ",autonomous vehicle
10.1007/s40070-020-00116-7,journal,EURO Journal on Decision Processes,sciencedirect,2020-11-30,sciencedirect,What does it mean to provide decision support to a responsible and competent expert?: The case of diagnostic decision support systems,https://api.elsevier.com/content/article/pii/S2193943821001151,"Decision support consists in helping a decision-maker to improve his/her decisions. However, clients requesting decision support are often themselves experts and are often taken by third parties and/or the general public to be responsible for the decisions they make. This predicament raises complex challenges for decision analysts, who have to avoid infringing upon the expertise and responsibility of the decision-maker. The case of diagnosis decision support in healthcare contexts is particularly illustrative. To support clinicians in their work and minimize the risk of medical error, various decision support systems have been developed, as part of information systems that are now ubiquitous in healthcare contexts. To develop, in collaboration with the hospitals of Lyon, a diagnostic decision support system for day-to-day customary consultations, we propose in this paper a critical analysis of current approaches to diagnostic decision support, which mainly consist in providing them with guidelines or even full-fledged diagnosis recommendations. We highlight that the use of such decision support systems by physicians raises responsibility issues, but also that it is at odds with the needs and constraints of customary consultations. We argue that the historical choice to favor guidelines or recommendations to physicians implies a very specific vision of what it means to support physicians, and we argue that the flaws of this vision partially explain why current diagnostic decision support systems are not accepted by physicians in their application to customary situations. Based on this analysis, we propose that decision support to physicians for customary cases should be deployed in an “adjustive” approach, which consists in providing physicians with the data on patients they need, when they need them, during consultations. The rationale articulated in this article has a more general bearing than clinical decision support and bears lessons for decision support activities in other contexts where decision-makers are competent and responsible experts.",autonomous vehicle
10.1016/B978-0-12-815010-8.09991-9,journal,Ihorizon-Enabled Energy Management for Electrified Vehicles,sciencedirect,2019-12-31,sciencedirect,Index,https://api.elsevier.com/content/article/pii/B9780128150108099919,Unknown,autonomous vehicle
10.1016/j.sysarc.2021.102180,journal,Journal of Systems Architecture,sciencedirect,2021-09-30,sciencedirect,"A perspective on 6G: Requirement, technology, enablers, challenges and future road map",https://api.elsevier.com/content/article/pii/S1383762121001302,"
                  Mobile network operators are at the verge of distribution and allotment of existing mobile communications with 5G. It is a high time that we should be focused on the forthcoming sixth generation (6G) networking. Though, it is mandated that 6G would leverage best of the existing network functions and cover an extended geographical range, we need to first understand the importance of 6G. In this article, we discuss about the vision of 6G and elaborate how it should look like with relevant elaborations. Key objective of this paper is to present analysis of the crucial requirements to develop 6G network infrastructure. We present a list of important networking and communication technologies that shall indeed play the vital role to design 6G. We also present how key enablers of 6G shall highlight their significance to emerge 6G. The article also presents important use case scenarios which may be overserved in the 6G era. Lastly, we find open research challenges and discuss way outs. Discussion about future road map designing concludes this literature.
               ",autonomous vehicle
10.1016/j.eswa.2009.05.059,journal,Expert Systems with Applications,sciencedirect,2010-01-31,sciencedirect,A data driven ensemble classifier for credit scoring analysis,https://api.elsevier.com/content/article/pii/S0957417409004771,"
                  This study focuses on predicting whether a credit applicant can be categorized as good, bad or borderline from information initially supplied. This is essentially a classification task for credit scoring. Given its importance, many researchers have recently worked on an ensemble of classifiers. However, to the best of our knowledge, unrepresentative samples drastically reduce the accuracy of the deployment classifier. Few have attempted to preprocess the input samples into more homogeneous cluster groups and then fit the ensemble classifier accordingly. For this reason, we introduce the concept of class-wise classification as a preprocessing step in order to obtain an efficient ensemble classifier. This strategy would work better than a direct ensemble of classifiers without the preprocessing step. The proposed ensemble classifier is constructed by incorporating several data mining techniques, mainly involving optimal associate binning to discretize continuous values; neural network, support vector machine, and Bayesian network are used to augment the ensemble classifier. In particular, the Markov blanket concept of Bayesian network allows for a natural form of feature selection, which provides a basis for mining association rules. The learned knowledge is represented in multiple forms, including causal diagram and constrained association rules. The data driven nature of the proposed system distinguishes it from existing hybrid/ensemble credit scoring systems.
               ",autonomous vehicle
10.1016/j.jss.2020.110777,journal,Journal of Systems and Software,sciencedirect,2020-12-31,sciencedirect,Technical debt forecasting: An empirical study on open-source repositories,https://api.elsevier.com/content/article/pii/S0164121220301904,"
                  Technical debt (TD) is commonly used to indicate additional costs caused by quality compromises that can yield short-term benefits in the software development process, but may negatively affect the long-term quality of software products. Predicting the future value of TD could facilitate decision-making tasks regarding software maintenance and assist developers and project managers in taking proactive actions regarding TD repayment. However, no notable contributions exist in the field of TD forecasting, indicating that it is a scarcely investigated field. To this end, in the present paper, we empirically evaluate the ability of machine learning (ML) methods to model and predict TD evolution. More specifically, an extensive study is conducted, based on a dataset that we constructed by obtaining weekly snapshots of fifteen open source software projects over three years and using two popular static analysis tools to extract software-related metrics that can act as TD predictors. Subsequently, based on the identified TD predictors, a set of TD forecasting models are produced using popular ML algorithms and validated for various forecasting horizons. The results of our analysis indicate that linear Regularization models are able to fit and provide meaningful forecasts of TD evolution for shorter forecasting horizons, while the non-linear Random Forest regression performs better than the linear models for longer forecasting horizons. In most of the cases, the future TD value is captured with a sufficient level of accuracy. These models can be used to facilitate planning for software evolution budget and time allocation. The approach presented in this paper provides a basis for predictive TD analysis, suitable for projects with a relatively long history. To the best of our knowledge, this is the first study that investigates the feasibility of using ML models for forecasting TD.
               ",autonomous vehicle
10.1016/j.jvcir.2021.103218,journal,Journal of Visual Communication and Image Representation,sciencedirect,2021-08-31,sciencedirect,A survey on spatio-temporal framework for kinematic gait analysis in RGB videos,https://api.elsevier.com/content/article/pii/S1047320321001449,"
                  Human gait recognition from videos is one of the promising research topics for analyzing human walking behavior. Spatio-temporal features and kinematics interesting points (three dimensional skeleton points) are the two key metrics in the gait examination. In general, input to gait recognition methods is categorized into 3 groups namely; two dimensional video-based, depth image-based and three dimensional (3D) skeleton-based methods. This work aims to present a survey on spatio-temporal and kinematic gait characteristics based on visual and 3D skeletal traits in RGB videos. A detailed insight on the various benchmarked gait databases, gait recognition representations based on model-based, model-free approaches and classifiers are presented in this review. Also, this paper investigates the performance metrics, application areas and covariate factors that influence the gait recognition process. Finally, the paper outlines the future perspective of gait recognition system based on kinematic joint points.
               ",autonomous vehicle
10.1016/j.tust.2018.08.029,journal,Tunnelling and Underground Space Technology,sciencedirect,2018-11-30,sciencedirect,Evaluation method of rockburst: State-of-the-art literature review,https://api.elsevier.com/content/article/pii/S0886779817309094,"
                  The evaluation of rockburst is becoming increasingly important as mining activities reach greater depths below the ground surface. In the literature, rockburst assessment has been tackled by many researchers with various methods. However, there has not been a study that examines and compares different rockburst assessment methods. In this paper, rockburst classification and its varying definitions are briefly summarized. A comprehensive review of the research efforts since 1965 then follows. This includes empirical, numerical, statistical and intelligent classification methods. Of particular significance is that in all the above-mentioned techniques, the review highlights the source of data, timeline of study and the comparative performance of various techniques in terms of their prediction accuracy wherever available. The review also lists current achievements, limitations and some promising directions for future research.
               ",autonomous vehicle
10.1016/B978-0-12-819200-9.10000-6,journal,Measuring and Modeling Persons and Situations,sciencedirect,2021-12-31,sciencedirect,Index,https://api.elsevier.com/content/article/pii/B9780128192009100006,Unknown,autonomous vehicle
10.1016/B978-0-12-814245-5.00029-3,journal,Handbook of Robotic and Image-Guided Surgery,sciencedirect,2020-12-31,sciencedirect,29: Robotic and Image-Guided Knee Arthroscopy,https://api.elsevier.com/content/article/pii/B9780128142455000293,"
               Knee arthroscopy is a well-established minimally invasive procedure for the diagnosis and treatment of knee joint disorders and injuries, with more than 4 million cases and costing the global healthcare system over US$15 billion annually. The complexities associated with arthroscopic procedures dictate relatively longer learning curves for surgeons with the potential to not only cause unintended damage during surgery but may cause postsurgery complications. The advancement of robotics and imaging technologies can reduce the shortcomings, alleviating some of the health access and workforce stressors on the health system. In this chapter, we discuss several key platform technologies that form a complete system to assist in knee arthroscopy. The system consists of four components: (1) steerable robotic tools, (2) autonomous leg manipulators, (3) novel stereo cameras for intraknee perception, and (4) three-dimensional/four-dimensional ultrasound imaging for tissue and tools tracking. As a platform technology, the system is applicable in other minimally invasive surgeries like hip arthroscopy, intraabdominal surgery, and any surgical site that can be accessed with a continuum robot, imaged with either stereo vision, ultrasound, or a combination of both.
            ",autonomous vehicle
10.1016/j.bspc.2021.102852,journal,Biomedical Signal Processing and Control,sciencedirect,2021-08-31,sciencedirect,Fusion of medical images based on salient features extraction by PSO optimized fuzzy logic in NSST domain,https://api.elsevier.com/content/article/pii/S1746809421004493,"
                  The aim of multimodal medical image fusion is to extract information from different modal images into a single one so that the single fusion image contains the salient features of the source images to the maximum extent. Different medical imaging modalities such as CT and MRI are presented as different visual morphology, which often show different complementary salient features in clinical diagnosis. According to this characteristic, in order to fuse different salient features of multimodal medical images into a single image and provide a new dimension of information for clinical diagnosis to improve the diagnosis accuracy, an improved image fusion algorithm based on visual salience detection is proposed in this paper. The visual salience of two registered source images is calculated by the GBVS algorithm, and the low-frequency and high-frequency sub-bands are obtained by decomposing the source images in NSST domain. For the low-frequency sub-bands, fuzzy logic system is used to obtain the respective weights of the fused low-frequency sub-band with the local energy and GBVS graph as input. For the high-frequency sub-bands, the NSML values of each sub-band are calculated and compared to obtain the fused high-frequency sub-band. The final fused image is obtained by the inverse NSST transformation. In addition, PSO algorithm is used to optimize the membership function of fuzzy logic system for better adaption to medical images and feature extraction. By applying this multimodal medical image fusion method, the visual quality of the image is effectively improved and the salient features of tissues are preserved well. Experiments on fusion of grayscale, color and ultrasound multimodal medical images show that the proposed method has advantages on retention of image salient features. Compared with other models, the fused image obtained by the proposed method has higher objective indexes with clearer edge contour, higher overall contrast, and no ringing effect and artifacts.
               ",autonomous vehicle
10.1016/S0169-7161(16)30068-2,journal,Handbook of Statistics,sciencedirect,2016-12-31,sciencedirect,Index,https://api.elsevier.com/content/article/pii/S0169716116300682,,autonomous vehicle
10.1016/B978-0-12-822844-9.00031-1,journal,Recent Trends in Computational Intelligence Enabled Research,sciencedirect,2021-12-31,sciencedirect,Index,https://api.elsevier.com/content/article/pii/B9780128228449000311,Unknown,autonomous vehicle
10.1016/0921-8890(92)90054-3,journal,Robotics and Autonomous Systems,sciencedirect,1992-12-31,sciencedirect,Distributed adaptive control: The self-organization of structured behavior,https://api.elsevier.com/content/article/pii/0921889092900543,"
                  Starting with a neural model for classical conditioning we have developed a system for robot control that is completely self-organizing. Instead of relying on predefined control rules the system will develop adapted control by interacting with its environment. We have tested this model in navigation tasks. Our results demonstrate that the level at which control models are normally defined seems to emerge out of the neural level which implements our control architecture.
               ",autonomous vehicle
10.1016/S0950-7051(97)00006-3,journal,Knowledge-Based Systems,sciencedirect,1997-08-31,sciencedirect,An integrated knowledge-based system for urban planning decision support,https://api.elsevier.com/content/article/pii/S0950705197000063,"
                  More applications that integrate knowledge-based decision support systems and artificial neural networks are starting to appear, and interest in such integrated systems is growing rapidly. The paper presents an integrated system in which a knowledge-based decision support system is integrated with a multilayer artificial neural network for urban planning. By integrating decision support systems, knowledge-based systems and artificial neural networks, the system achieves improvements in the implementation of each as well as increases the scope of the application. This approach is very rewarding in its synergism of three technologies to solve complex problems. The paper discusses the structure of the integrated system, as well as providing an example of application.
               ",autonomous vehicle
10.1016/S0364-0213(99)80004-4,journal,Cognitive Science,sciencedirect,1996-03-31,sciencedirect,On the validity of simulating stagewise development by means of PDP networks: Application of catastrophe analysis and an experimental test of rule-like network performance,https://api.elsevier.com/content/article/pii/S0364021399800044,"
                  This article addresses the ability of Parallel Distributed Processing (PDP) networks to generate stagewise cognitive development in accordance with Piaget's theory of cognitive epigenesis. We carried out a replication study of the simulation experiments by McClelland (1989) and McClelland and Jenkins (1991) in which a PDP network learns to solve balance scale problems. In objective tests motivated from catastrophe theory, a mathematical theory of transitions in epigenetical systems, no evidence for stage transitions in network performance was found. It is concluded that PDP networks lack the ability to recover the positive outcomes of analogous catastrophe analyses of real cognitive developmental data. In an attempt to further characterize the learning behaviour of PDP networks, we carried out a second simulation study using the discrimination-shift paradigm. The results thus obtained indicate that PDP learning is compatible with the learning of stimulus-response relationships, not with the acquisition of mediating rules such as conceived in (neo-)Piagetian theory. In closing, we speculate about the feasibility of simulating stagewise development with alternative network architectures.
               ",autonomous vehicle
10.1016/S0921-8890(97)80707-5,journal,Robotics and Autonomous Systems,sciencedirect,1997-06-30,sciencedirect,Sensory—motor coordination: The metaphor and beyond,https://api.elsevier.com/content/article/pii/S0921889097807075,"
                  Any agent in the real world has to be able to make distinctions between different types of objects, i.e. it must have the competence of categorization. In mobile agents, there is a large variation in proximal sensory stimulation originating from the same object. Therefore, categorization behavior is hard to achieve, and the successes in the past in solving this problem have been limited. In this paper it is proposed that the problem of categorization in the real world is significantly simplified if it is viewed as one of sensory—motor coordination, rather than one of information processing happening “on the input side”. A series of models are presented to illustrate the approach. It is concluded that we should consider replacing the metaphor of information processing for intelligent systems by the one of sensory-motor coordination. However, the principle of sensory-motor coordination is more than a metaphor. It offers concrete mechanisms for putting agents to work in the real world. These ideas are illustrated with a series of experiments.
               ",autonomous vehicle
10.1016/B978-0-12-810480-4.00007-6,journal,Neurobiological Background of Exploration Geosciences,sciencedirect,2017-12-31,sciencedirect,Chapter 7: Brain-Based Technologies,https://api.elsevier.com/content/article/pii/B9780128104804000076,"
               In this chapter, I provide a summary description of computation approaches inspired to the functioning of the brain and to biology. These include artificial neural networks, fuzzy logic algorithms, and evolutionary computation algorithms. Moreover, I discuss the concept of brain-based technology (BBT). These consist of hardware, software, procedures, and workflows that are developed consistently with the human cognitive abilities and limitations. Optimized visualization tools represent a first example of BBT. Based on the functioning of human vision, they are aimed at maximizing color/shape perception and minimizing the interpretation pitfalls inherent in the cognitive process. Then, I introduce a new approach for multimodal and multisensory analysis of geophysical signals. This approach is aimed at expanding the realm of geophysical data analysis taking in account for simultaneous visual and audio perception. It combines the benefits of both imaging and sonification techniques with pattern recognition and automatic classification methods. I discuss the theoretical fundamentals of that approach and several examples based on real geophysical data. Finally, I discuss the concept of quantitative integration system (QUIS). This is an integration platform allowing optimized workflows for combining complementary data. QUIS represents a typical example of BBT. In fact, it is developed consistently with the main integration functions of human brain, characterized by high connectivity of advanced processing modules.
            ",autonomous vehicle
10.1016/B978-0-12-801238-3.11349-2,journal,Systems Medicine,sciencedirect,2021-12-31,sciencedirect,Models for Personalized Medicine,https://api.elsevier.com/content/article/pii/B9780128012383113492,"
               The customization of medicine to specific individuals promises clear improvements in disease treatment, but also faces substantial challenges, many of which have their roots in the complexity of the human body. This complexity cannot be grasped with intuition alone and is not appropriately captured by reductionist methods, which have been dominating biology and medicine for the past decades. Experimental and computational systems biology have the potential of generating adequate datasets and analyzing them in a manner that captures the complexity of health and disease systems in a personalized manner. This potential has not yet fully materialized, but examples and case studies provide a glimpse of the power these approaches are likely to have in the future.
            ",autonomous vehicle
10.1016/B978-0-12-816034-3.20001-1,journal,Biomedical Information Technology,sciencedirect,2020-12-31,sciencedirect,Index,https://api.elsevier.com/content/article/pii/B9780128160343200011,Unknown,autonomous vehicle
10.1016/0893-6080(94)90014-0,journal,Neural Networks,sciencedirect,1994-12-31,sciencedirect,"Introduction to the theory of neural computation: By John Hertz, Anders Krogh, and Richard G. Palmer, Addison-Wesley publishing company, 352 pages, ISBN 0-201-50395-6 (hardcover) and 0-201-51560-1 (paperback)",https://api.elsevier.com/content/article/pii/0893608094900140,,autonomous vehicle
10.1016/B978-012402772-5/50011-9,journal,Control in Power Electronics,sciencedirect,2002-12-31,sciencedirect,CHAPTER 10: Neural Networks and Fuzzy Logic Control in Power Electronics,https://api.elsevier.com/content/article/pii/B9780124027725500119,,autonomous vehicle
10.1016/B978-0-323-90231-1.20001-0,journal,Methods for Petroleum Well Optimization,sciencedirect,2022-12-31,sciencedirect,Index,https://api.elsevier.com/content/article/pii/B9780323902311200010,Unknown,autonomous vehicle
10.1016/S0167-9236(02)00106-9,journal,Decision Support Systems,sciencedirect,2003-05-31,sciencedirect,Complementing search engines with online web mining agents,https://api.elsevier.com/content/article/pii/S0167923602001069,"
                  While search engines have become the major decision support tools for the Internet, there is a growing disparity between the image of the World Wide Web stored in search engine repositories and the actual dynamic, distributed nature of Web data. We propose to attack this problem using an adaptive population of intelligent agents mining the Web online at query time. We discuss the benefits and shortcomings of using dynamic search strategies versus the traditional static methods in which search and retrieval are disjoint. This paper presents a public Web intelligence tool called MySpiders, a threaded multiagent system designed for information discovery. The performance of the system is evaluated by comparing its effectiveness in locating recent, relevant documents with that of search engines. We present results suggesting that augmenting search engines with adaptive populations of intelligent search agents can lead to a significant competitive advantage. We also discuss some of the challenges of evaluating such a system on current Web data, introduce three novel metrics for this purpose, and outline some of the lessons learned in the process.
               ",autonomous vehicle
10.1016/B978-0-12-824477-7.00021-3,journal,Foundations of Artificial Intelligence in Healthcare and Bioscience,sciencedirect,2021-12-31,sciencedirect,Glossary of terminology,https://api.elsevier.com/content/article/pii/B9780128244777000213,Unknown,autonomous vehicle
10.1016/j.rser.2021.111607,journal,Renewable and Sustainable Energy Reviews,sciencedirect,2021-11-30,sciencedirect,A survey on key techniques and development perspectives of equivalent consumption minimisation strategy for hybrid electric vehicles,https://api.elsevier.com/content/article/pii/S1364032121008832,"
                  Hybrid electric vehicles (HEVs), as a promising solution to mitigate environmental pollution and reduce fuel consumption, employ a combination of fuel and electric power as power supply for boosting the vehicle's fuel economy. Comparing to conventional internal combustion engine (ICE) driven vehicles, the additional propulsion power source in electrified powertrain systems of HEVs leads to the extra control degree of freedom. Thus, a well-designed energy management strategy (EMS) is indispensable to cope with the complexity of the power distribution existing in multiple power source system. Equivalent consumption minimisation strategy (ECMS) is one of the most promising EMS techniques due to its capability of achieving the real-time local optimal control. In ECMS, a key parameter – equivalent factor (EF) is usually employed to unify the ICE fuel consumption and the electric energy consumption into a single variable representing the equivalent fuel economy, thereby achieving the instantaneous fuel economy optimisation. This paper comprehensively surveys the state-of-the-art in ECMSs for PHEVs and HEVs. Firstly, the basic operation mechanism of ECMSs is discussed. Then, ECMSs are classified based on their dependence on either online computation or offline pre-computation. Moreover, the core technique of ECMSs – EF adaptation is elaborated in terms of their principles, key characteristics, advantages, and disadvantages. In addition, the key factors for the EF adaptation as well as the corresponding factor integration methods are analysed and summarised. Finally, future research trends and the gaps for the development of ECMSs are discussed.
               ",autonomous vehicle
10.1016/j.engappai.2020.104058,journal,Engineering Applications of Artificial Intelligence,sciencedirect,2021-02-28,sciencedirect,BeCAPTCHA: Behavioral bot detection using touchscreen and mobile sensors benchmarked on HuMIdb,https://api.elsevier.com/content/article/pii/S0952197620303274,"
                  In this paper we study the suitability of a new generation of CAPTCHA methods based on smartphone interactions. The heterogeneous flow of data generated during the interaction with the smartphones can be used to model human behavior when interacting with the technology and improve bot detection algorithms. For this, we propose BeCAPTCHA, a CAPTCHA method based on the analysis of the touchscreen information obtained during a single drag and drop task in combination with the accelerometer data. The goal of BeCAPTCHA is to determine whether the drag and drop task was realized by a human or a bot. We evaluate the method by generating fake samples synthesized with Generative Adversarial Neural Networks and handcrafted methods. Our results suggest the potential of mobile sensors to characterize the human behavior and develop a new generation of CAPTCHAs. The experiments are evaluated with HuMIdb
                        1
                      
                     
                        1
                        
                           https://github.com/BiDAlab/HuMIdb.
                      (Human Mobile Interaction database), a novel multimodal mobile database that comprises 14 mobile sensors acquired from 600 users. HuMIdb is freely available to the research community.
               ",autonomous vehicle
10.1016/j.future.2019.09.004,journal,Future Generation Computer Systems,sciencedirect,2020-10-31,sciencedirect,"Smart-troubleshooting connected devices: Concept, challenges and opportunities",https://api.elsevier.com/content/article/pii/S0167739X19306491,"
                  Today’s digital world and evolving technology has improved the quality of our lives but it has also come with a number of new threats. In the society of smart-cities and Industry 4.0, where many cyber–physical devices connect and exchange data through the Internet of Things, the need for addressing information security and solve system failures becomes inevitable. System failures can occur because of hardware failures, software bugs or interoperability issues. In this paper we introduce the industry-originated concept of “smart-troubleshooting” that is the set of activities and tools needed to gather failure information generated by heterogeneous connected devices, analyze them, and match them with troubleshooting instructions and software fixes. As a consequence of implementing smart-troubleshooting, the system would be able to self-heal and thus become more resilient. This paper aims to survey frameworks, methodologies and tools related to this new concept, and especially the ones needed to model, analyze and recover from failures in a (semi)automatic way. Smart-troubleshooting has a relation with event analysis to perform diagnostics and prognostics on devices manufactured by different suppliers in a distributed system. It also addresses management of appropriate product information specified in possibly unstructured formats to guide the troubleshooting workflow in identifying fault—causes and solutions. Relevant research is briefly surveyed in the paper in order to highlight current state-of-the-art, open issues, challenges to be tackled and future opportunities in this emerging industry paradigm.
               ",autonomous vehicle
10.1016/j.engstruct.2019.06.012,journal,Engineering Structures,sciencedirect,2019-09-15,sciencedirect,Predicting reinforcing bar development length using polynomial chaos expansions,https://api.elsevier.com/content/article/pii/S0141029618340768,"
                  The bond stress of a reinforcing bar in a cementitious matrix varies along the bar length and is difficult to quantify. Thus, design code provisions refer to the concept of bar development length and rely on statistical analysis of rebar-pull-out test results. In the present study, a novel data-driven predictive model based on Polynomial Chaos Expansions (PCE) was developed to predict the reinforcing bar development length using 534 experimental results of simple pull-out tests on short unit bar lengths. The predictive capability of PCE was compared to that of other data-driven models, namely the Response Surface Method (RSM) and Artificial Neural Networks (ANN). Moreover, predictions of the PCE, RSM and ANN were further compared with calculations of three commonly used design code formulas (i.e., ACI 318-14, ACI 408R-03, and Eurocode 2) and predictions of two existing empirical models (i.e. Model Code 2010 and Hwang et al. model). A parametric study was conducted to explore the sensitivity of the proposed model to influential input parameters. It was found that the Polynomial Chaos Expansions model offers a powerful predictive tool for reinforcing bar bond strength. The model was able to capture trends that differ from that of existing models that assume unrealistic uniform bond stress along the rebar. This flexible and data intensive model for predicting rebar bond stress and full embedment length could offer an intelligent platform for accommodating new bar materials, new test data, and calibrating existing design provisions to keep design codes relevant.
               ",autonomous vehicle
10.1016/j.renene.2020.07.145,journal,Renewable Energy,sciencedirect,2020-12-31,sciencedirect,A review of non-destructive testing on wind turbines blades,https://api.elsevier.com/content/article/pii/S0960148120312210,"
                  Wind energy, with an exponential growth in the last years, is nowadays one of the most important renewable energy sources. Modern wind turbines are bigger and complex to produce more energy. This industry requires to reduce its operating and maintenance costs and to increase its reliability, safety, maintainability and availability. Condition monitoring systems are beginning to be employed for this purpose. They must be reliable and cost-effective to reduce the long periods of downtimes and high maintenance costs, and to avoid catastrophic scenarios caused by undetected failures. This paper presents a survey about the most important and updated condition monitoring techniques based on non-destructive testing and methods applied to wind turbine blades. In addition, it analyses the future trends and challenges of structural health monitoring systems in wind turbine blades.
               ",autonomous vehicle
10.1016/j.cosrev.2018.11.002,journal,Computer Science Review,sciencedirect,2019-02-28,sciencedirect,A Survey on quantum computing technology,https://api.elsevier.com/content/article/pii/S1574013718301709,"
                  The power of quantum computing technologies is based on the fundamentals of quantum mechanics, such as quantum superposition, quantum entanglement, or the no-cloning theorem. Since these phenomena have no classical analogue, similar results cannot be achieved within the framework of traditional computing. The experimental insights of quantum computing technologies have already been demonstrated, and several studies are in progress. Here we review the most recent results of quantum computation technology and address the open problems of the field.
               ",autonomous vehicle
10.1016/j.jnca.2016.04.007,journal,Journal of Network and Computer Applications,sciencedirect,2016-06-30,sciencedirect,Fraud detection system: A survey,https://api.elsevier.com/content/article/pii/S1084804516300571,"
                  The increment of computer technology use and the continued growth of companies have enabled most financial transactions to be performed through the electronic commerce systems, such as using the credit card system, telecommunication system, healthcare insurance system, etc. Unfortunately, these systems are used by both legitimate users and fraudsters. In addition, fraudsters utilized different approaches to breach the electronic commerce systems. Fraud prevention systems (FPSs) are insufficient to provide adequate security to the electronic commerce systems. However, the collaboration of FDSs with FPSs might be effective to secure electronic commerce systems. Nevertheless, there are issues and challenges that hinder the performance of FDSs, such as concept drift, supports real time detection, skewed distribution, large amount of data etc. This survey paper aims to provide a systematic and comprehensive overview of these issues and challenges that obstruct the performance of FDSs. We have selected five electronic commerce systems; which are credit card, telecommunication, healthcare insurance, automobile insurance and online auction. The prevalent fraud types in those E-commerce systems are introduced closely. Further, state-of-the-art FDSs approaches in selected E-commerce systems are systematically introduced. Then a brief discussion on potential research trends in the near future and conclusion are presented.
               ",autonomous vehicle
10.1016/S0007-8506(07)60883-X,journal,CIRP Annals,sciencedirect,1997-12-31,sciencedirect,Intelligent Computing Methods for Manufacturing Systems,https://api.elsevier.com/content/article/pii/S000785060760883X,"
                  Intelligent computation is taken to include the development and application of artificial intelligence (Al) methods i.e. tools that exhibit characteristics associated with intelligence in human behaviour. Many approaches have been proposed to apply Al methods, techniques and paradigms to the solution of manufacturing problems. This paper discusses current trends in applications of intelligent computing tools to manufacturing and reviews the motivation and basis for the utilisation of these systems. The topics of the paper were confined to four main issues of manufacturing systems: design, planning, production and system level activities. A discussion about intelligent manufacturing systems from these four basic functional view points was introduced, the relevant intelligent computing methods and their use in manufacturing were surveyed, and a number of significant research issues and applications were illustrated. The main developments that were observed comprise the integration of Al methods into CAD, CAPP, etc.; the improvement of the performance of present Al techniques; the development of hybrid Al systems; the elaboration and application of new Al paradigms in manufacturing. Intelligent systems in the future are expected to be integrated, modular, and hybrid in nature, and they may well include all the techniques described in this paper and further more.
               ",autonomous vehicle
10.1016/j.neunet.2015.01.002,journal,Neural Networks,sciencedirect,2015-05-31,sciencedirect,Computational cognitive models of spatial memory in navigation space: A review,https://api.elsevier.com/content/article/pii/S0893608015000040,"Spatial memory refers to the part of the memory system that encodes, stores, recognizes and recalls spatial information about the environment and the agent’s orientation within it. Such information is required to be able to navigate to goal locations, and is vitally important for any embodied agent, or model thereof, for reaching goals in a spatially extended environment. In this paper, a number of computationally implemented cognitive models of spatial memory are reviewed and compared. Three categories of models are considered: symbolic models, neural network models, and models that are part of a systems-level cognitive architecture. Representative models from each category are described and compared in a number of dimensions along which simulation models can differ (level of modeling, types of representation, structural accuracy, generality and abstraction, environment complexity), including their possible mapping to the underlying neural substrate. Neural mappings are rarely explicated in the context of behaviorally validated models, but they could be useful to cognitive modeling research by providing a new approach for investigating a model’s plausibility. Finally, suggested experimental neuroscience methods are described for verifying the biological plausibility of computational cognitive models of spatial memory, and open questions for the field of spatial memory modeling are outlined.",autonomous vehicle
10.1016/j.cie.2021.107534,journal,Computers & Industrial Engineering,sciencedirect,2021-10-31,sciencedirect,Discussing resilience in the context of cyber physical systems,https://api.elsevier.com/content/article/pii/S0360835221004381,"
                  Cyber-Physical Systems (CPSs) are increasingly more complex and integrated into our everyday lives forming the basis of smart infrastructures, products, and services. Consequently, there is a greater need for their ability to perform their required functions under expected and unexpected adverse events. Moreover, the multitude of threats and their rapid evolution pushes the development of approaches that go beyond pure technical reliability, rather encompassing multi-dimensional performance of a socio-technical system. These dimensions call for the notion of resilience, to be used as a staging area for modelling system performance. While a large number of documents deal with this kind of problem for systems including CPSs, a comprehensive review on the topic is still lacking. The scope of this paper is to survey available literature for understanding to which extent CPSs contribute to system resilience, and to synthetize the approaches developed in this domain. More than 500 documents were reviewed through a protocol based on the PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-Analyses) review technique. This survey identifies main models and methods categorizing them on the basis of the hazards of interest and their effects on security, privacy, safety and business continuity. It also summarizes main conceptual frameworks and metrics used to assess and compare the resilience capabilities of a system including also CPSs. This cross-domain survey highlights the dominant techno-centric unit of analysis for available literature, still highlighting emerging trends towards more systemic representations of system threats, even socio-technically oriented, and respective modern investigation approaches.
               ",autonomous vehicle
10.1016/B978-0-12-824477-7.00014-6,journal,Foundations of Artificial Intelligence in Healthcare and Bioscience,sciencedirect,2021-12-31,sciencedirect,Contents,https://api.elsevier.com/content/article/pii/B9780128244777000146,Unknown,autonomous vehicle
10.1016/B978-0-12-820074-2.00029-0,journal,Local Electricity Markets,sciencedirect,2021-12-31,sciencedirect,Index,https://api.elsevier.com/content/article/pii/B9780128200742000290,Unknown,autonomous vehicle
10.1016/j.jnca.2019.102447,journal,Journal of Network and Computer Applications,sciencedirect,2020-01-01,sciencedirect,"Multimodal big data affective analytics: A comprehensive survey using text, audio, visual and physiological signals",https://api.elsevier.com/content/article/pii/S1084804519303078,"
                  Affective computing is an emerging multidisciplinary research field that is increasingly drawing the attention of researchers and practitioners in various fields, including artificial intelligence, natural language processing, cognitive and social sciences. Research in affective computing includes areas such as sentiment, emotion, and opinion modelling. The internet is an excellent source of data required for sentiment analysis, such as customer reviews of products, social media, forums, blogs, etc. Most of these data, called big data, are unstructured and unorganized. Hence there is a strong demand for developing suitable data processing techniques to process these rich and valuable data to produce useful information. Early surveys on sentiment and emotion recognition in the literature have been limited to discussions using text, audio, and visual modalities. So far, to the author's knowledge, a comprehensive survey combining physiological modalities with these other modalities for affective computing has yet to be reported. The objective of this paper is to fill the gap in this surveyed area. The usage of physiological modalities for affective computing brings several benefits in that the signals can be used in different environmental conditions, more robust systems can be constructed in combination with other modalities, and it has increased anti-spoofing characteristics. The paper includes extensive reviews on different frameworks and categories for state-of-the-art techniques, critical analysis of their performances, and discussions of their applications, trends and future directions to serve as guidelines for readers towards this emerging research area.
               ",autonomous vehicle
10.1016/B978-0-08-101107-2.00002-6,journal,Handbook of Categorization in Cognitive Science,sciencedirect,2017-12-31,sciencedirect,Chapter 2: To Cognize is to Categorize: Cognition is Categorization,https://api.elsevier.com/content/article/pii/B9780081011072000026,"
               We organisms are sensorimotor systems. Things in the world come into contact with our sensory surfaces, and we interact with them based on what that sensorimotor contact “affords.” All of our categories consist of ways in which we behave differently toward different kinds of things—things we do or do not eat, mate with, or flee from; or the things that we describe, through our language, as prime numbers, affordances, absolute discriminables, or truths. Categorization—doing the right thing with the right kind of thing—is largely what cognition is about, and for.
            ",autonomous vehicle
10.1016/j.robot.2008.06.012,journal,Robotics and Autonomous Systems,sciencedirect,2009-04-30,sciencedirect,Behavioral control through evolutionary neurocontrollers for autonomous mobile robot navigation,https://api.elsevier.com/content/article/pii/S0921889008000936,"
                  This paper deals with the study of scaling up behaviors in evolutive robotics (ER). Complex behaviors were obtained from simple ones. Each behavior is supported by an artificial neural network (ANN)-based controller or neurocontroller. Hence, a method for the generation of a hierarchy of neurocontrollers, resorting to the paradigm of Layered Evolution (LE), is developed and verified experimentally through computer simulations and tests in a Khepera
                        ®
                      micro-robot. Several behavioral modules are initially evolved using specialized neurocontrollers based on different ANN paradigms. The results show that simple behaviors coordination through LE is a feasible strategy that gives rise to emergent complex behaviors. These complex behaviors can then solve real-world problems efficiently. From a pure evolutionary perspective, however, the methodology presented is too much dependent on user’s prior knowledge about the problem to solve and also that evolution take place in a rigid, prescribed framework. Mobile robot’s navigation in an unknown environment is used as a test bed for the proposed scaling strategies.
               ",autonomous vehicle
10.1016/j.vehcom.2019.100214,journal,Vehicular Communications,sciencedirect,2020-06-30,sciencedirect,Cybersecurity challenges in vehicular communications,https://api.elsevier.com/content/article/pii/S221420961930261X,"As modern vehicles are capable to connect to an external infrastructure and Vehicle-to-Everything (V2X) communication technologies mature, the necessity to secure communications becomes apparent. There is a very real risk that today's vehicles are subjected to cyber-attacks that target vehicular communications. This paper proposes a three-layer framework (sensing, communication and control) through which automotive security threats can be better understood. The sensing layer is made up of vehicle dynamics and environmental sensors, which are vulnerable to eavesdropping, jamming, and spoofing attacks. The communication layer is comprised of both in-vehicle and V2X communications and is susceptible to eavesdropping, spoofing, man-in-the-middle, and sybil attacks. At the top of the hierarchy is the control layer, which enables autonomous vehicular functionality, including the automation of a vehicle's speed, braking, and steering. Attacks targeting the sensing and communication layers can propagate upward and affect the functionality and can compromise the security of the control layer. This paper provides the state-of-the-art review on attacks and threats relevant to the communication layer and presents countermeasures.",autonomous vehicle
10.1016/j.future.2015.07.012,journal,Future Generation Computer Systems,sciencedirect,2016-03-31,sciencedirect,Cybermatics: Cyber–physical–social–thinking hyperspace based science and technology,https://api.elsevier.com/content/article/pii/S0167739X15002356,"
                  The Internet of Things (IoT) is becoming an attractive system paradigm, in which physical perceptions, cyber interactions, social correlations, and even cognitive thinking can be intertwined in the ubiquitous things’ interconnections. It realizes a perfect integration of a new cyber–physical–social–thinking (CPST) hyperspace, which has profound implications for the future IoT. In this article, a novel concept Cybermatics is put forward as a broader vision of the IoT (called hyper IoT) to address science and technology issues in the heterogeneous CPST hyperspace. This article covers a broaden research field and presents a preliminary study focusing on its three main features (i.e., interconnection, intelligence, and greenness). Concretely, interconnected Cybermatics refers to the variants of Internet of anything, such as physical objects, cyber services, social people, and human thinking; intelligent Cybermatics considers the cyber–physical–social–thinking computing to provide algorithmic support for system infrastructures; green Cybermatics addresses energy issues to ensure efficient communications and networking. Finally, open challenging science and technology issues are discussed in the field of Cybermatics.
               ",autonomous vehicle
10.1016/j.compind.2019.01.010,journal,Computers in Industry,sciencedirect,2019-05-31,sciencedirect,Cognitive interaction with virtual assistants: From philosophical foundations to illustrative examples in aeronautics,https://api.elsevier.com/content/article/pii/S0166361518304445,"
                  Why do we perceive virtual assistants as something radically new? Our hypothesis is that today virtual assistants are raising an expectation for natural interaction with the human. Human interaction is by nature cognitive and collaborative. Human sciences help to flesh the ingredients of such cognitive interaction. Uttering a sentence is at the same time: producing sound; making a well-formed sentence; giving meaning; enriching a common background of beliefs and intentions; making something together. In this paper, we remind the basics of human cognitive communication as developed by human sciences, particularly philosophy of mind. We propose a definition of this way of interacting with computer as ‘cognitive interaction’, and we summarize the main characteristics of this interaction mode into a layered model. Finally we develop case studies to illustrate concretely the concepts. We analyze in light of our theoretical model three approaches of conversational systems in AI, to illustrate the different available options to implement the pragmatic dimension of cognitive interaction. We analyze first the seminal approach of Grosz and Sidner [20], and then we describe how the now classical approach of discourse structure developed by Asher and Lascarides [5] could capture the pragmatic dimension of interaction with an intelligent virtual assistant. Finally, we wonder whether a state-of-the-art chat bot framework actually implements the needed level of cognitive interaction. The contribution of this paper is: to remind and summarize essential ideas from other disciplines which are relevant to understand what should be the interaction with virtual assistants should be; to explain why the cooperation with virtual assistants is something special; to delineate the challenges we have to solve if we are to develop truly collaborative virtual assistants.
               ",autonomous vehicle
10.1016/S1568-4946(10)00181-X,journal,Applied Soft Computing,sciencedirect,2010-09-30,sciencedirect,Subject Index,https://api.elsevier.com/content/article/pii/S156849461000181X,,autonomous vehicle
10.1016/j.drudis.2020.01.020,journal,Drug Discovery Today,sciencedirect,2020-04-30,sciencedirect,Exploring chemical space using natural language processing methodologies for drug discovery,https://api.elsevier.com/content/article/pii/S1359644620300465,"
                  Text-based representations of chemicals and proteins can be thought of as unstructured languages codified by humans to describe domain-specific knowledge. Advances in natural language processing (NLP) methodologies in the processing of spoken languages accelerated the application of NLP to elucidate hidden knowledge in textual representations of these biochemical entities and then use it to construct models to predict molecular properties or to design novel molecules. This review outlines the impact made by these advances on drug discovery and aims to further the dialogue between medicinal chemists and computer scientists.
               ",autonomous vehicle
10.1016/B978-0-12-817356-5.09992-7,journal,Internet of Things in Biomedical Engineering,sciencedirect,2019-12-31,sciencedirect,Index,https://api.elsevier.com/content/article/pii/B9780128173565099927,Unknown,autonomous vehicle
10.1016/S0925-2312(09)00132-5,journal,Neurocomputing,sciencedirect,2009-06-30,sciencedirect,Announcements of Conferences,https://api.elsevier.com/content/article/pii/S0925231209001325,,autonomous vehicle
10.1016/0925-2312(91)90028-A,journal,Neurocomputing,sciencedirect,1991-07-31,sciencedirect,"Advanced neural computers: Edited by R. Eckmiller<ce:italic>Elsevier Science Publishers B.V. (North-Holland) 1990, ISBN 0 444 88400 9</ce:italic>",https://api.elsevier.com/content/article/pii/092523129190028A,,autonomous vehicle
10.1016/j.aiopen.2021.02.004,journal,AI Open,sciencedirect,2020-12-31,sciencedirect,Extracting Events and Their Relations from Texts: A Survey on Recent Research Progress and Challenges,https://api.elsevier.com/content/article/pii/S266665102100005X,"Event is a common but non-negligible knowledge type. How to identify events from texts, extract their arguments, even analyze the relations between different events are important for many applications. This paper summaries some constructed event-centric knowledge graphs and the recent typical approaches for event and event relation extraction, besides task description, widely used evaluation datasets, and challenges. Specifically, in the event extraction task, we mainly focus on three recent important research problems: 1) how to learn the textual semantic representations for events in sentence-level event extraction; 2) how to extract relations across sentences or in a document level; 3) how to acquire or augment labeled instances for model training. In event relation extraction, we focus on the extraction approaches for three typical event relation types, including coreference, causal and temporal relations, respectively. Finally, we give out our conclusion and potential research issues in the future.",autonomous vehicle
10.1016/j.neucom.2017.10.015,journal,Neurocomputing,sciencedirect,2018-01-31,sciencedirect,Bio-inspired architecture for static object segmentation in time varying background models from video sequences,https://api.elsevier.com/content/article/pii/S0925231217316661,"
                  Scene analysis is a complex task for a computer vision system, and therefore requires high level processing tasks. Although traditional image processing schemes have been used to implement these tasks, new alternatives based on bio-inspired model have been considered. This paper presents a novel bio-inspired model for static object segmentation from time varying backgrounds modeled from video sequences. The proposed model was developed by considering recent neurocomputational models. It is designed as a four layer system inspired to represent the behavior of lateral geniculate nucleus, and the visual cortex areas V1, V2, and V4. For this work three novel artificial neural network architectures to simulate the areas V1, V2 and V4 were designed. The proposed model performance was qualitatively and quantitatively compared with several segmentation algorithms. Findings of this comparison indicate that the model introduced outperforms the other segmentation methods considering speed and F measure in the segmentation of static objects from time varying background models from video sequences.
               ",autonomous vehicle
10.1016/j.compind.2021.103469,journal,Computers in Industry,sciencedirect,2021-09-30,sciencedirect,Digital twin paradigm: A systematic literature review,https://api.elsevier.com/content/article/pii/S0166361521000762,"
                  Manufacturing enterprises are facing the need to align themselves to the new information technologies (IT) and respond to the new challenges of variable market demand. One of the key enablers of this IT revolution toward Smart Manufacturing is the digital twin (DT). It embeds a “virtual” image of the reality constantly synchronized with the real operating scenario to provide sound information (knowledge model) to reality interpretation model to draw sound decisions. The paper aims at providing an up-to date picture of the main DT components, their features and interaction problems. The paper aims at clearly tracing the ongoing research and technical challenges in conceiving and building DTs as well, according to different application domains and related technologies. To this purpose, the main questions answered here are: ‘What is a Digital Twin?’; ‘Where is appropriate to use a Digital Twin?’; ‘When has a Digital Twin to be developed?’; ‘Why should a Digital Twin be used?’; ‘How to design and implement a Digital Twin?’; ‘What are the main challenges of implementing a Digital Twin?’. This study tries to answer to the previous questions funding on a wide systematic literature review of scientific research, tools, and technicalities in different application domains.
               ",autonomous vehicle
10.1533/9781845699796.1.139,journal,Fatigue Life Prediction of Composites and Composite Structures,sciencedirect,2010-12-31,sciencedirect,5: Novel computational methods for fatig life modeling of composite materials,https://api.elsevier.com/content/article/pii/B978184569525550005X,"
               
                  Novel computational methods such as artificial neural networks, adaptive neuro-fuzzy inference systems and genetic programming are used in this chapter for the modeling of the nonlinear behavior of composite laminates subjected to constant amplitude loading. The examined computational methods are stochastic nonlinear regression tools, and can therefore be used to model the fatigue behavior of any material, provided that sufficient data are available for training. They are material-independent methods that simply follow the trend of the available data, in each case giving the best estimate of their behavior. Application on a wide range of experimental data gathered after fatigue testing glass/epoxy and glass/polyester laminates proved that their modeling ability compares favorably with, and is to some extent superior to, other modeling techniques.
            ",autonomous vehicle
10.1016/B978-0-12-820472-6.00017-7,journal,Reference Module in Biomedical Sciences,sciencedirect,2021-12-31,sciencedirect,Systems Pharmacology: Enabling Multidimensional Therapeutics,https://api.elsevier.com/content/article/pii/B9780128204726000177,"
               Systems pharmacology is a recently developed scientific field concerning the appreciation of novel therapeutic networks that enables biomedical scientists to understand the actions of medicinal agents in a multidimensional mechanistic manner. A thorough appreciation of systems pharmacology requires the synergistic integration of multiple disciplines including, receptor biology, network theory, high-dimensionality data acquisition and advanced informatics deconvolution. Appreciating pharmacological signaling pathways at a systemic network level holds the promise that this practice can improve the efficiency of therapeutic development. This advancement is associated with the ability of systems pharmacology to generate a highly nuanced and quantitative appreciation of simultaneous medicinal signaling across multiple physiological domains. Implicit in this process is the potential benefit that multi-level systems medication, as opposed to agents with a limited therapeutic scope, can engender upon disease networks. In this article we shall outline the benefits of this data and biology convergence for both therapeutic discovery, refinement and precision targeting.
            ",autonomous vehicle
10.1016/j.neunet.2011.06.018,journal,Neural Networks,sciencedirect,2011-10-31,sciencedirect,Editorial for the special issue ICANN-2010,https://api.elsevier.com/content/article/pii/S089360801100178X,,autonomous vehicle
10.1016/j.ins.2020.06.058,journal,Information Sciences,sciencedirect,2021-01-04,sciencedirect,Callback2Vec: Callback-aware hierarchical embedding for mobile application,https://api.elsevier.com/content/article/pii/S0020025520306435,"
                  Although numerous embedding approaches have been proposed for code representation of mobile applications, insufficient attention has been paid to its essential running nature: event-driven. As a result, the contextual semantics of event-driven callbacks re hardly captured. Existing solutions either discard the information of event callbacks such as their sequences, or simply treat event callbacks as ordinary APIs. Both of the solutions deviate from the actual running behavior of the applications and thus suffer from critical information loss of the callback contexts. To address the problem, in this paper, a callback based hierarchical embedding approach Callback2Vec is proposed, in which ordinary APIs and callbacks are distinguished and tackled at different levels in a top-down fashion. As such, the contextual semantics of callbacks can be reasonably represented by the embedding vectors. In particular, a fine-grained callback-sequence-generation algorithm is devised to capture the running behavior of callbacks. To evaluate the representation capability of Callback2Vec, a systematic analysis targeting at the embedding results is conducted, whereby the conventional embedding characteristics are rigorously investigated and new implications are identified. Of significance, the proposed embedding approach has been validated to be capable of providing novel solutions for typical downstream applications, through comprehensive experiments with large scale public datasets.
               ",autonomous vehicle
10.1016/B978-0-12-801559-9.18001-9,journal,"Artificial Neural Network for Drug Design, Delivery and Disposition",sciencedirect,2016-12-31,sciencedirect,Index,https://api.elsevier.com/content/article/pii/B9780128015599180019,Unknown,autonomous vehicle
10.1016/j.bica.2018.07.005,journal,Biologically Inspired Cognitive Architectures,sciencedirect,2018-08-31,sciencedirect,Shifting and drifting attention while reading: A case study of nonlinear-dynamical attention allocation in the OpenCog cognitive architecture,https://api.elsevier.com/content/article/pii/S2212683X18300860,"
                  A simple experimental example of the general principle of “cognitive synergy” underlying the OpenCog AGI architecture is explored: An OpenCog system processing a series of articles that shifts from one topic (insects) to another (poisons), and using its nonlinear attention-allocation dynamics (based on the ECAN Economic Attention Networks framework) to spread attention back and forth between the nodes and links within OpenCog’s Atomspace knowledge store representing the words in the sentences, and other nodes and links containing related knowledge.
                  With this setup, we study how the ECAN system shifts the attentional focus of the system based on changes in topic – in terms of both the speed of attention switching, and the contextual similarity of the content of attentional focus to the sentences being processed at a given point in time. This also provides an avenue for exploring the effects of particular design choices within the ECAN system.
                  For instance, we find that in this particular example, if the parameters are set appropriately, ECAN indeed causes the system to assign particular importance to nodes and links related to the “insecticide” concept, when it is reading sentences about poisons in a situation where it has been primed by sentences about insects. This is an example of what we call “drifting” attention – the system’s attention moves to something suggested by its perceptions, even if not directly presented in them.
               ",autonomous vehicle
10.1016/j.bica.2018.07.005,journal,Biologically Inspired Cognitive Architectures,sciencedirect,2018-08-31,sciencedirect,Shifting and drifting attention while reading: A case study of nonlinear-dynamical attention allocation in the OpenCog cognitive architecture,https://api.elsevier.com/content/article/pii/S2212683X18300860,"
                  A simple experimental example of the general principle of “cognitive synergy” underlying the OpenCog AGI architecture is explored: An OpenCog system processing a series of articles that shifts from one topic (insects) to another (poisons), and using its nonlinear attention-allocation dynamics (based on the ECAN Economic Attention Networks framework) to spread attention back and forth between the nodes and links within OpenCog’s Atomspace knowledge store representing the words in the sentences, and other nodes and links containing related knowledge.
                  With this setup, we study how the ECAN system shifts the attentional focus of the system based on changes in topic – in terms of both the speed of attention switching, and the contextual similarity of the content of attentional focus to the sentences being processed at a given point in time. This also provides an avenue for exploring the effects of particular design choices within the ECAN system.
                  For instance, we find that in this particular example, if the parameters are set appropriately, ECAN indeed causes the system to assign particular importance to nodes and links related to the “insecticide” concept, when it is reading sentences about poisons in a situation where it has been primed by sentences about insects. This is an example of what we call “drifting” attention – the system’s attention moves to something suggested by its perceptions, even if not directly presented in them.
               ",autonomous vehicle
10.1016/j.ins.2020.04.019,journal,Information Sciences,sciencedirect,2020-10-31,sciencedirect,MAG-GAN: Massive attack generator via GAN,https://api.elsevier.com/content/article/pii/S0020025520303194,"
                  Adversarial attacks reveal the vulnerability of deep neural networks (DNNs). These attacks fool DNNs by adding small perturbations to normal examples. Currently, most attacks involve generating a single example or target a single deep model. To gain insight into adversarial attacks and develop a robust defense, this study focuses on a generic attack model applicable to most adversarial attacks. A novel mass-generator of adversarial examples with a strong attack ability and involving small perturbations is presented herein. The main contributions of this work include proposing a generic framework for adversarial attacks, designing comprehensive evaluation metrics for adversarial examples, and developing a novel method for mass-generating adversarial examples via a generative adversarial network (MAG-GAN). Finally, experiments were conducted to demonstrate the good performance of MAG-GAN compared with state-of-the-art attack methods. Once the model was trained, adversarial examples were mass-generated with a small perturbation and a strong attack ability. Furthermore, it was found that MAG-GAN model can be adopted as an efficient tool to reveal the vulnerability and improve the defense ability of existing DNNs. A promising result is that the target model mounted in MAG-GAN exhibited a good defense performance after game training, which is equivalent to adversarial training.
               ",autonomous vehicle
10.1016/B978-0-08-042016-5.50014-0,journal,Application of Artificial Intelligence in Process Control,sciencedirect,1992-12-31,sciencedirect,AN INTRODUCTION TO NEURAL NETWORKS,https://api.elsevier.com/content/article/pii/B9780080420165500140,,autonomous vehicle
10.1016/j.jobe.2020.101871,journal,Journal of Building Engineering,sciencedirect,2021-01-31,sciencedirect,Predicting chiller system performance using ARIMA-regression models,https://api.elsevier.com/content/article/pii/S235271022033504X,"
                  A proper selection of predictor variables would enhance the exploratory analysis of time series models while prompting practical strategies to optimize chiller system performance. This study explores essential operating variables to predict the time series of the coefficient of performance (COP) of a chiller system expressed as the cooling capacity output divided by the total electric power input of all components. Based on a huge set of historical operating data, hybrid ARIMA-regression models were developed by fitting 14 predictor variables other than the past COP-related terms. The most significant variable influencing predictability involves the part load ratio (PLR) and its order-3 lag terms lasting for 45 min. The system COP fluctuation is mainly governed by the PLR variation due to operating unnecessary chillers and non-pair up operation of system components. When chiller sequencing is properly implemented, the PLRs shift up with tempered variation. The paired component combinations lower the system electric power to maximize the system COP. The annual average system COP increases to 3.5538 from 3.3212 with a predicted electricity saving of 8.2955%. The novelty of this study is to highlight which variables and component operating statuses help improve the predictability of time series models while prioritizing practical strategies for performance improvement.
               ",autonomous vehicle
10.1016/S0925-2312(08)00493-1,journal,Neurocomputing,sciencedirect,2008-12-31,sciencedirect,Announcements of Conferences,https://api.elsevier.com/content/article/pii/S0925231208004931,,autonomous vehicle
10.1016/0022-2496(89)90014-X,journal,Journal of Mathematical Psychology,sciencedirect,1989-09-30,sciencedirect,Neural nets: From Hartley and Hebb to Hinton,https://api.elsevier.com/content/article/pii/002224968990014X,"
                  The history of neural nets is traced, beginning with the associationist ideas of Aristotle and the physiological speculations of Hartley and Hebb. Lashley's empirical work led him to conclude that memory traces are not localized. However, it was not until the development of the hologram by Gabor and its application to brain function by van Heerden that a possible theoretical basis for distributed information storage was proposed. Willshaw and his colleagues demonstrated that non-linear associative models were simpler, more efficient models, which retained the property of tolerance of degraded input. Since then there have been improvements in the realism of these models and attempts to integrate findings from the structure and function of the nervous system, notably the cerebellum and hippocampus.
               ",autonomous vehicle
10.1016/B978-0-08-100659-7.00021-X,journal,Machine Learning,sciencedirect,2018-12-31,sciencedirect,Index,https://api.elsevier.com/content/article/pii/B978008100659700021X,Unknown,autonomous vehicle
10.1016/j.pbiomolbio.2015.08.013,journal,Progress in Biophysics and Molecular Biology,sciencedirect,2015-12-31,sciencedirect,Naturalizing semiotics: The triadic sign of Charles Sanders Peirce as a systems property,https://api.elsevier.com/content/article/pii/S0079610715001261,"
                  The father of pragmatism, Charles Sanders Peirce, gave in 1903 the following definition of a sign: “A Sign, or Representamen, is a First which stands in such a genuine triadic relation to a Second, called its Object, as to be capable of determining a Third, called its Interpretant, to assume the same triadic relation to its Object in which it stands itself to the same Object. The triadic relation is genuine, that is its three members are bound together by it in a way that does not consist in any complexus of dyadic relations”. Despite its cult status and its pragmatic foundation, the Peircean sign has never revealed its true potential by being integrated into a formal system. In the present report, a reconstruction of the sign model is presented, which may at first appear somewhat obvious and superficial. However by use of the reconstructed model, the above statement and the majority of Peirce's other statements about the nature of signs fall into place. Instead of defining three links between Object (O), Representamen (R), and Interpretant (I), the sign is described as having a single three-dimensional link, specifying its location in a three dimensional (O,R,I) linkage space. To understand and explain sign function, the process of sign utilization (semiosis) has to be divided into two temporally separated phases, a sign-establishment phase where a three-dimensional link (Ψ(O,R,I)) is formed between three sign elements, and a later sign-interpretation phase where the established linkage is used for inferring significance to a novel phenomenon, if this satisfies the criteria for being a Representamen for the sign. Numerous statements from Peirce indicate that he used a two-staged semiosis paradigm although he did not state that explicitly.
                  The three-dimensional model was primarily constructed for use in biosemiotics, as an exploratory frame for mapping the evolutionary establishment of sign links, which logically must have preceded the fixation of any regulatory process in molecular biological systems. It became clear, however, that the model is able to clarify many of the difficult explanations offered by Peirce about his sign model. I make no claim that Peirce used a similar type of three-dimensional model, because he explicitly used the chemical atom as naturalization (natural scientific explanation) for his sign model, an interesting but problematic analogy. In order to discuss common versus specific semiotic scaffolds for molecular biosemiotics, biosemiotics and semiotics proper, I start with a generic definition of the three-dimensional sign system, using human semiosis as examples. After this, the major part of the paper, I define the specific biochemical and evolutionary scaffolds that is used for obtaining the evolutionary memory that is needed for sign establishment.
                  To exemplify semiosis according to the present model I present a typical situation where a Representamen (RE) and an object (OE) in the establishment phase are frequently encountered together by a sign interpreter. The process that links specific Representamens to specific Objects will first involve the recognition of the specific traits that distinguish the two sign elements. Subsequently the establishment process leads to the creation of a specific systems-state, called the Interpretant, which links the two traits in a way that allows retrieval of the information (a memory function). During a later interpretation phase, a hypothetical Object will be inferred by the interpreter when a Representamen (RI) harboring the required characteristics is encountered. This inference happens through a memory retrieval process, irrespective of the fact that relevant Objects of the sign may never be encountered after establishment. A simplified scheme for computer neural network algorithms is introduced as an example of such a system. Since the Peircean sign according to this definition is a systems property, there can be no sign without a sign interpreting systems or without some kind of memory function. A sign interpreter will thus harbor a semiotic scaffold that consists of at least an input sensor and an interpreting system coupled to a memory function. Further border conditions for semiotic scaffolds will be introduced.
                  Peirce published a comprehensive sign definition system, but he allowed only ten sign classes, selected from the twenty-seven sign classes that result from his three main subdivisions, each containing three classes. His allowed sign classes are here identified as those which do not infer more significance during interpretation than was warranted during establishment. The excluded sign classes are either undefinable in his system or are of such a nature that the objects during interpretation are inferred to be much more significant than what was warranted during establishment. Occult signs are of these forbidden free-wheeling types, and it is postulated that they were omitted because Peirce defined his sign classes for use in a novel sign based logical system, where such over-signification would be detrimental.
               ",autonomous vehicle
10.1016/0893-6080(88)90007-X,journal,Neural Networks,sciencedirect,1988-12-31,sciencedirect,Generalization of backpropagation with application to a recurrent gas market model,https://api.elsevier.com/content/article/pii/089360808890007X,"
                  Backpropagation is often viewed as a method for adapting artificial neural networks to classify patterns. Based on parts of the book by Rumelhart and colleagues, many authors equate backpropagation with the generalized delta rule applied to fully-connected feedforward networks. This paper will summarize a more general formulation of backpropagation, developed in 1974, which does more justice to the roots of the method in numerical analysis and statistics, and also does more justice to creative approaches expressed by neural modelers in the past year or two. It will discuss applications of backpropagation to forecasting over time (where errors have been halved by using methods other than least squares), to optimization, to sensitivity analysis, and to brain research.
                  This paper will go on to derive a generalization of backpropagation to recurrent systems (which input their own output), such as hybrids of perceptron-style networks and Grossberg/Hopfield networks. Unlike the proposal of Rumelhart, Hinton, and Williams, this generalization does not require the storage of intermediate iterations to deal with continuous recurrence. This generalization was applied in 1981 to a model of natural gas markets, where it located sources of forecast uncertainty related to the use of least squares to estimate the model parameters in the first place.
               ",autonomous vehicle
10.1016/B978-0-12-819445-4.00026-6,journal,"Cognitive Informatics, Computer Modelling, and Cognitive Science",sciencedirect,2020-12-31,sciencedirect,Index,https://api.elsevier.com/content/article/pii/B9780128194454000266,Unknown,autonomous vehicle
10.1016/j.bica.2012.11.001,journal,Biologically Inspired Cognitive Architectures,sciencedirect,2013-01-31,sciencedirect,Distributed recurrent self-organization for tracking the state of non-stationary partially observable dynamical systems,https://api.elsevier.com/content/article/pii/S2212683X12000576,"
                  In this paper, a distributed recurrent self-organizing architecture is presented. It can extract the current state of a dynamical system from the sequence of the recent observations provided by this system, even if they are ambiguous. The recurrent network is an adaptation of RecSOM to the context of the simulation of large scale distributed neural architectures, since it relies on a strictly local fine-grained computation. The experiments show the ability of the recurrent architecture to capture the states, but also exhibit some unexpected dynamical effects, like some instabilities of the learned mappings. The presented architecture addresses the cognitive ability to set up representations from sequences at a mesoscopic level. At that intermediate level, between cognition and neurons simulation, some complex dynamics is unveiled. It needs to be identified and understood in order to bridge the gap between neuronal activities and high level cognition.
               ",autonomous vehicle
10.1016/B978-0-12-811373-8.00003-3,journal,Security and Resilience in Intelligent Data-Centric Systems and Communication Networks,sciencedirect,2018-12-31,sciencedirect,Chapter 3: The Cyber Security Challenges in the IoT Era,https://api.elsevier.com/content/article/pii/B9780128113738000033,"
               Nowadays, the evolution of the world of the Internet of Thing is promising the explosion of a number of devices connected to the Internet. According to Cisco analysts (Evans, 2011), in 2015 there were more than 25 billion devices connected and with a projection of more than 50 billion devices connected by 2020. Also, the new business paradigms that the Internet of Things technologies enable are producing a super-fast increase of machine-to-machine communications. This is a real market breakthrough moment that opens up a lot opportunities for enterprises and, generally speaking, for the whole society. Inherently, it increases dramatically the security problems, which could frustrate a sizeable part of Internet of Things’ potential benefits that McKinsey valuates at approximately $4 trillion. Indeed, a recent survey by HP reports that the 70% of devices contain vulnerabilities. The aim of this chapter is to provide an overview of current trends about cyber security concerns and a glimpse of what the future of the Internet of Things will bring.
            ",autonomous vehicle
10.1016/j.neucom.2020.12.001,journal,Neurocomputing,sciencedirect,2021-03-28,sciencedirect,JDGAN: Enhancing generator on extremely limited data via joint distribution,https://api.elsevier.com/content/article/pii/S0925231220318828,"
                  Generative Adversarial Network (GAN) is a thriving generative model and considerable efforts have been made to enhance the generation capabilities via designing a different adversarial framework of GAN (e.g., the discriminator and the generator) or redesigning the penalty function. Although existing models have been demonstrated to be very effective, their generation capabilities have limitations. Existing GAN variants either result in identical generated instances or generate simulation data with low quality when the training data are diverse and extremely limited (a dataset consists of a set of classes but each class holds several or even one single sample) or extremely imbalanced (a category holds a set of samples and other categories hold one single sample). In this paper, we present an innovative approach to tackle this issue, which jointly employs joint distribution and reparameterization method to reparameterize the randomized space as a mixture model and learn the parameters of this mixture model along with that of GAN. In this way, we term our approach Joint Distribution GAN (JDGAN). In our work, we show that the JDGAN can not only generate high quality simulation data with diversity, but also increase the overlapping area between the generating distribution and the raw data distribution. We proceed to conduct extensive experiments, utilizing MNIST, CIFAR10 and Mass Spectrometry datasets, all using extremely limited amounts of data, to demonstrate the significant performance of JDGAN in both achieving the smallest Fréchet Inception Distance (FID) score and producing diverse generated data.
               ",autonomous vehicle
10.1016/j.pmatsci.2021.100795,journal,Progress in Materials Science,sciencedirect,2021-06-30,sciencedirect,Emerging metallic systems for additive manufacturing: <ce:italic>In-situ</ce:italic> alloying and multi-metal processing in laser powder bed fusion,https://api.elsevier.com/content/article/pii/S0079642521000190,"
                  While significant progress has been made in understanding laser powder bed fusion (L-PBF) as well as the fabrication of various materials using this technology, there is still limited adoption in the industry. One of the key obstacles identified is the lack of materials that can truly manufacture functional parts directly with L-PBF. This paper covers the emerging research on in-situ alloying and multi-metal processing. A comprehensive overview of the underlying scientific topics behind them is presented. The current state of research and progress from different perspectives (the materials and L-PBF processing parameters) are reviewed in order to provide a basis for follow-up research and development of these approaches. Defects, especially those associated with these two material processing routes, are also elucidated by discussing the mechanisms of their formation, including the main influencing factors, and the tendency for them to occur. Future research trends and potential topics are illustrated. The final part of this paper summarizes findings from this review and outlines the possibility of in-situ alloying and multi-metal processing using L-PBF.
               ",autonomous vehicle
10.1016/j.asoc.2009.02.003,journal,Applied Soft Computing,sciencedirect,2009-06-30,sciencedirect,Soft computing in medicine,https://api.elsevier.com/content/article/pii/S1568494609000246,"
                  Soft computing (SC) is not a new term; we have gotten used to reading and hearing about it daily. Nowadays, the term is used often in computer science and information technology. It is possible to define SC in different ways. Nonetheless, SC is a consortium of methodologies which works synergistically and provides, in one form or another, flexible information processing capability for handling real life ambiguous situations. Its aim is to exploit the tolerance for imprecision, uncertainty, approximate reasoning and partial truth in order to achieve tractability, robustness and low-cost solutions. SC includes fuzzy logic (FL), neural networks (NNs), and genetic algorithm (GA) methodologies. SC combines these methodologies as FL and NN (FL–NN), NN and GA (NN–GA) and FL and GA (FL–GA). Recent years have witnessed the phenomenal growth of bio-informatics and medical informatics by using computational techniques for interpretation and analysis of biological and medical data. Among the large number of computational techniques used, SC, which incorporates neural networks, evolutionary computation, and fuzzy systems, provides unmatched utility because of its demonstrated strength in handling imprecise information and providing novel solutions to hard problems.
                  The aim of this paper is to introduce briefly the various SC methodologies and to present various applications in medicine between the years 2000 and 2008. The scope is to demonstrate the possibilities of applying SC to medicine-related problems. The recent published knowledge about use of SC in medicine is researched in MEDLINE. This study detects which methodology or methodologies of SC are used frequently together to solve the special problems of medicine. According to MEDLINE database searches, the rates of preference of SC methodologies in medicine were found as 68% of FL–NN, 27% of NN–GA and 5% of FL–GA. So far, FL–NN methodology was significantly used in medicine. The rates of using FL–NN in clinical science, diagnostic science and basic science were found as %83, %71 and %48, respectively. On the other hand NN–GA and FL–GA methodologies were mostly preferred by basic science of medicine.
                  Another message emerging from this survey is that the number of papers which used NN–GA methodology has continuously risen until today. Also search results put the case clearly that FL–GA methodology has not applied well enough to medicine yet. Undeniable interest in studying SC methodologies in genetics, physiology, radiology, cardiology, and neurology disciplines proves that studying SC is very fruitful in these disciplines and it is expected that future researches in medicine will use SC more than it is used today to solve more complex problems.
               ",autonomous vehicle
10.1016/j.robot.2019.05.005,journal,Robotics and Autonomous Systems,sciencedirect,2019-09-30,sciencedirect,Context-based affordance segmentation from 2D images for robot actions,https://api.elsevier.com/content/article/pii/S0921889018309990,"
                  Affordances play a crucial role in robotics since they allow developing truly autonomous robots, which can freely explore and interact with the environment. Most of the existing approaches for analyzing affordances in a scene consider only one or few types of affordance, e.g., grasping points, object manipulation or locomotion. In many cases only whole objects are considered. In our study we include in total 12 affordances of object-related, manipulation and locomotion affordances, considering affordances of both objects and/or their parts. We design a system that can densely predict affordances given only a single 2D RGB image. For this, we propose a method that transfers object class labels to affordances. This enables us to train convolutional neural networks, a PSPNet-based network and a U-Net-style network, to directly predict affordances from an image using a selective binary cross entropy loss function. The method is able to handle (potentially multiple) affordances of objects and their parts in a pixel-wise manner even in the case of incomplete data. We perform qualitative as well as quantitative evaluations with simulated and real data including robot experiments. In general, we find that frequent affordances are recognized with a substantial fraction of correctly assigned pixels, while this is harder for infrequent affordances and small objects. In addition, we demonstrate that our method performs better than a recent competitive approach. As the proposed method operates on 2D images, it is easier to implement than competing 3D methods and it could therefore more easily provide useful affordance estimates for robotic actions as demonstrated experimentally.
               ",autonomous vehicle
10.1016/S0925-2312(06)00015-4,journal,Neurocomputing,sciencedirect,2006-03-31,sciencedirect,Announcements of Conferences,https://api.elsevier.com/content/article/pii/S0925231206000154,,autonomous vehicle
10.1016/j.fss.2005.05.037,journal,Fuzzy Sets and Systems,sciencedirect,2005-12-16,sciencedirect,Aggregation operators and models,https://api.elsevier.com/content/article/pii/S0165011405002873,"
                  This paper gives an overview of the field of aggregation operators. Current research lines are described focusing on those related with the process of building real applications.
               ",autonomous vehicle
10.1016/B978-0-12-386914-2.00015-7,journal,Handbook of Natural Gas Transmission and Processing,sciencedirect,2012-12-31,sciencedirect,Chapter 15: Process Modeling in the Natural Gas Processing Industry,https://api.elsevier.com/content/article/pii/B9780123869142000157,"Modeling is used in the natural gas industry for simulations of process and equipment design, analysis of system behavior, operator training, leak detection, and design of controllers. The level of model fidelity and rigor depends on the objective. This chapter discusses various modeling techniques, including empirical, rule-based, and first principles, which are commonly used for describing natural gas processes. The presented modeling techniques cover applications for data analysis, expert systems, control systems and dynamic simulations.
               Keywords: affine TS model, artificial intelligence, artificial neural network, autoregressive moving average model, capacity, causal model, committee of machines, control methodology, decision tree, defuzzification, Elman network, expert system, feedforward network, finite impulse response model, fuzzy inference algorithm, fuzzy logic, general function approximation, hedge, intelligent modeling, knowledge-based system, least-squares estimation, linear system, model-based reasoning, multilayer perceptron, neural nets, neural network, neurode, neuron, nonlinear system, processing element, radial basis function network, recurrent network, ridge regression, rule-based system, singleton model, space model, state-transition function, time series model, unit, Zadeh operator.",autonomous vehicle
10.1016/j.comnet.2021.107940,journal,Computer Networks,sciencedirect,2021-05-08,sciencedirect,"Communication technologies for Smart Water Grid applications: Overview, opportunities, and research directions",https://api.elsevier.com/content/article/pii/S1389128621000827,"
                  Due to the aging of current water infrastructure and the increased demand for water resources, water distribution systems encounter several problems such as pipes leaks and bursts, water contamination or pollution, and water optimization issues. To cope with these issues, access to water-related data and information related to the water grid is fundamental in order to build sophisticated strategies for water management. Integrating Information and Communication Technologies (ICT) into the current water infrastructure is one viable solution to access to water-related data. This new infrastructure that integrates ICT into the water distribution system is called Smart Water Grid (SWG). The information layer of the SWG requires well-suited communication networks. This paper reviews wireless communication technologies that can be exploited for the implementation of SWG applications. The paper mainly focuses on the exploration of Low Power Wide Area Networks (LPWANs) that promise to overcome problems confronted by communication technologies used currently in SWG applications. Some of the wireless technologies used in SWG applications suffer from high power consumption issues while others are limited in terms of communication range. For future SWG applications, in this work we recommend LPWAN technologies due to their low power consumption, long communication range and excellent radio penetration. A range of challenges and research directions regarding the recommended LPWANs is also discussed.
               ",autonomous vehicle
10.1016/j.jksuci.2017.06.001,journal,Journal of King Saud University - Computer and Information Sciences,sciencedirect,2018-10-31,sciencedirect,Big Data technologies: A survey,https://api.elsevier.com/content/article/pii/S1319157817300034,"Developing Big Data applications has become increasingly important in the last few years. In fact, several organizations from different sectors depend increasingly on knowledge extracted from huge volumes of data. However, in Big Data context, traditional data techniques and platforms are less efficient. They show a slow responsiveness and lack of scalability, performance and accuracy. To face the complex Big Data challenges, much work has been carried out. As a result, various types of distributions and technologies have been developed. This paper is a review that survey recent technologies developed for Big Data. It aims to help to select and adopt the right combination of different Big Data technologies according to their technological needs and specific applications’ requirements. It provides not only a global view of main Big Data technologies but also comparisons according to different system layers such as Data Storage Layer, Data Processing Layer, Data Querying Layer, Data Access Layer and Management Layer. It categorizes and discusses main technologies features, advantages, limits and usages.",autonomous vehicle
10.1016/S0925-2312(06)00066-X,journal,Neurocomputing,sciencedirect,2006-06-30,sciencedirect,Announcements of conferences,https://api.elsevier.com/content/article/pii/S092523120600066X,,autonomous vehicle
10.1533/9780857099440.1,journal,Machine Learning and Data Mining,sciencedirect,2007-12-31,sciencedirect,Chapter 1: Introduction,https://api.elsevier.com/content/article/pii/B9781904275213500010,,autonomous vehicle
10.1016/B978-0-12-824477-7.00023-7,journal,Foundations of Artificial Intelligence in Healthcare and Bioscience,sciencedirect,2021-12-31,sciencedirect,Index,https://api.elsevier.com/content/article/pii/B9780128244777000237,Unknown,autonomous vehicle
10.1016/S0925-2312(09)00037-X,journal,Neurocomputing,sciencedirect,2009-03-31,sciencedirect,Announcements of Conferences,https://api.elsevier.com/content/article/pii/S092523120900037X,,autonomous vehicle
10.1016/S0921-8890(99)00029-9,journal,Robotics and Autonomous Systems,sciencedirect,1999-07-31,sciencedirect,Integration of linguistic and numerical information for biped control,https://api.elsevier.com/content/article/pii/S0921889099000299,"
                  Bipedal locomotion is an important hallmark of human evolution. Despite of complex control systems, human locomotion is characterized by smooth, regular, and repeating movements. Therefore, there is the potential for applying human locomotion strategies and any knowledge available to the biped control. In order to make the most use of the information available, a linguistic-numerical integration-based biped control method is proposed in this paper. The numerical data from biped measuring instruments, and the linguistic rules obtained from intuitive walking knowledge and biomechanics study have been classified into four categories: direct rules, direct data, indirect rules, and indirect data. Based on inverse learning and data fusion theory, two simple and intuitive integration schemes are proposed to integrate linguistic and numerical information with various forms, such as direct and indirect. One is neurofuzzy-based integration, and another is fuzzy rules extraction-based integration. The simulation results show that the biped gait and joint control performance can be significantly improved by the prescribed synergy method-based neurofuzzy gait synthesis and fuzzy rules extraction-based joint control strategies using linguistic and numerical integrated information.
               ",autonomous vehicle
10.1016/B978-0-12-849896-5.00002-7,journal,Intelligent Coordinated Control of Complex Uncertain Systems for Power Distribution Network Reliability,sciencedirect,2016-12-31,sciencedirect,Chapter 2: Theoretical Basis for Intelligent Coordinated Control,https://api.elsevier.com/content/article/pii/B9780128498965000027,"
               This chapter mainly introduces the theoretical basis of intelligent coordinated control algorithm employed in the research, and summarizes its internal operation mechanism and merits and demerits of each intelligent control algorithm, laying a theoretical foundation for subsequent chap
            ",autonomous vehicle
10.1016/B978-0-12-815010-8.00002-8,journal,Ihorizon-Enabled Energy Management for Electrified Vehicles,sciencedirect,2019-12-31,sciencedirect,2: Integrated energy management for electrified vehicles,https://api.elsevier.com/content/article/pii/B9780128150108000028,"
               Electrified vehicles have evolved in the past years from a rare technology of limited applications into a consolidated reality. Nevertheless, the complexity of such powertrains has not been conveniently addressed by either the research community or by industry. Optimisation of fuel consumption requires a comprehensive analysis of the vehicle components and architectures, a holistic analysis of vehicle design and energy management and incorporation of the state-of-the-art of vehicle connectivity capabilities. This chapter reviews all possible vehicle electrification approaches and the latest developments in terms of heuristic and optimisation-based energy management. The advantages and disadvantages are highlighted along with opportunities for improvement within the context of the connected vehicle framework. The chapter closes with the future trends in electrified vehicle control and the proposal of a predictive framework robust to limited connectivity and learning capabilities for in-vehicle application, as an immediate solution for fuel consumption minimisation in ordinary electrified powertrains.
            ",autonomous vehicle
10.1016/j.jappgeo.2018.05.017,journal,Journal of Applied Geophysics,sciencedirect,2018-12-31,sciencedirect,Random noise attenuation by Wiener-ANFIS filtering,https://api.elsevier.com/content/article/pii/S0926985117300526,"
                  This paper introduces a method for background random noise attenuation in seismic reflection data giving priority to the preservation of coherent seismic events and automation of the algorithm. Since the statistical characteristics of random noise are different than those of coherent events, in the proposed method, after calculating Adaptive Wiener Filter (AWF), with different window sizes, the structure of the input data ware calculated by Fuzzy C-Mean Clustering (FCM). The sorted standard deviation of AWF values ware also used to determine the input data for training of Adaptive Neuro-Fuzzy Inference System (ANFIS). Trained network was generalized to all input data points and the output of ANFIS, alongside with data structure ware used to determine the optimized output by comparing noise level of all outputs. The proposed method was applied on both synthetic and real data sets and the results were compared to those of the conventional methods. The research findings revealed that the method was of a considerably higher performance in random noise attenuation as well as preserving the coherent events.
               ",autonomous vehicle
10.1016/j.ejor.2012.03.039,journal,European Journal of Operational Research,sciencedirect,2012-09-16,sciencedirect,Synergies between operations research and data mining: The emerging use of multi-objective approaches,https://api.elsevier.com/content/article/pii/S0377221712002494,"
                  Operations research and data mining already have a long-established common history. Indeed, with the growing size of databases and the amount of data available, data mining has become crucial in modern science and industry. Data mining problems raise interesting challenges for several research domains, and in particular for operations research, as very large search spaces of solutions need to be explored. Hence, many operations research methods have been proposed to deal with such challenging problems. But the relationships between these two domains are not limited to these natural applications of operations research approaches. The counterpart is also important to consider, since data mining approaches have also been applied to improve operations research techniques. The aim of this article is to highlight the interplay between these two research disciplines. A particular emphasis will be placed on the emerging theme of applying multi-objective approaches in this context.
               ",autonomous vehicle
10.1016/j.artint.2015.05.004,journal,Artificial Intelligence,sciencedirect,2017-03-31,sciencedirect,Auction optimization using regression trees and linear models as integer programs,https://api.elsevier.com/content/article/pii/S0004370215000788,"In a sequential auction with multiple bidding agents, the problem of determining the ordering of the items to sell in order to maximize the expected revenue is highly challenging. The challenge is largely due to the fact that the autonomy and private information of the agents heavily influence the outcome of the auction. The main contribution of this paper is two-fold. First, we demonstrate how to apply machine learning techniques to solve the optimal ordering problem in sequential auctions. We learn regression models from historical auctions, which are subsequently used to predict the expected value of orderings for new auctions. Given the learned models, we propose two types of optimization methods: a black-box best-first search approach, and a novel white-box approach that maps learned regression models to integer linear programs (ILP), which can then be solved by any ILP-solver. Although the studied auction design problem is hard, our proposed optimization methods obtain good orderings with high revenues. Our second main contribution is the insight that the internal structure of regression models can be efficiently evaluated inside an ILP solver for optimization purposes. To this end, we provide efficient encodings of regression trees and linear regression models as ILP constraints. This new way of using learned models for optimization is promising. As the experimental results show, it significantly outperforms the black-box best-first search in nearly all settings.",autonomous vehicle
10.1016/j.dcan.2021.03.006,journal,Digital Communications and Networks,sciencedirect,2021-03-26,sciencedirect,ED-SWE: Event detection based on scoring and word embedding in online social networks for the internet of people,https://api.elsevier.com/content/article/pii/S2352864821000171,"Online social media networks are gaining attention worldwide with an increasing number of people relying on them to connect, communicate and share their daily pertinent event-related information. Event detection is now increasingly leveraging online social networks for highlighting events happening around the world via the Internet of People. In this paper, a novel Event Detection model based on Scoring and Word Embedding (ED-SWE) is proposed for discovering key events from a large volume of data streams of tweets and for generating an event summary using key words and top-k tweets. The proposed ED-SWE model can distill high-quality tweets, reduce the negative impact of the advent of spam, and identify latent events in the data streams automatically. Moreover, a word embedding algorithm is used to learn a real-valued vector representation for a predefined fixed-sized vocabulary from a corpus of Twitter data. In order to further improve the performance of the Expectation-Maximization (EM) iteration algorithm, a novel initialization method based on the authority values of the tweets is also proposed in this paper to detect live events efficiently and precisely. Finally, a novel automatic identification method based on the cosine measure is used to automatically evaluate whether a given topic can form a live event. Experiments conducted on a real-world dataset to demonstrate that the ED-SWE model exhibits better efficiency and accuracy than several state-of-art event detection models.",autonomous vehicle
10.1016/j.conbuildmat.2014.04.103,journal,Construction and Building Materials,sciencedirect,2014-08-29,sciencedirect,Advanced structural health monitoring of concrete structures with the aid of acoustic emission,https://api.elsevier.com/content/article/pii/S0950061814004292,"
                  This article gives a comprehensive review of the acoustic emission (AE) technique for its applications in concrete structure health monitoring. Basic and established condition assessment methods for concrete structures are reviewed to configure a firm perception of AE application for enhanced performance and reliability. The AE approaches of focus are the parametric and signal analysis which can be used to develop damage evaluation criteria. Other than recent localization and source discrimination methods, applications of pivotal AE parameters such as b-value, Ib-value, AE energy, and hit are discussed herein, with highlights on the limitation of the individual parameter-based approaches when adopted on site. In addition, the introduction of new parameters such as sifted b-value, minimum b-value, and Q value is discussed as well, followed by a novel recent strategy for AE application in conjunction with tomography method to facilitate infrastructure assessment. Moreover, the key role of application of artificial intelligence methods towards damage mode identification has been highlighted.
               ",autonomous vehicle
10.1016/j.conb.2005.10.009,journal,Current Opinion in Neurobiology,sciencedirect,2005-12-31,sciencedirect,Computational motor control in humans and robots,https://api.elsevier.com/content/article/pii/S0959438805001583,"
                  Computational models can provide useful guidance in the design of behavioral and neurophysiological experiments and in the interpretation of complex, high dimensional biological data. Because many problems faced by the primate brain in the control of movement have parallels in robotic motor control, models and algorithms from robotics research provide useful inspiration, baseline performance, and sometimes direct analogs for neuroscience.
               ",autonomous vehicle
10.1016/B978-0-12-012749-8.50007-6,journal,Control and Dynamic Systems,sciencedirect,1991-12-31,sciencedirect,Neural Network Techniques in Manufacturing and Automation Systems,https://api.elsevier.com/content/article/pii/B9780120127498500076,,autonomous vehicle
10.1016/B978-0-12-817665-8.09997-5,journal,Hydraulic Fracturing in Unconventional Reservoirs,sciencedirect,2019-12-31,sciencedirect,Index,https://api.elsevier.com/content/article/pii/B9780128176658099975,Unknown,autonomous vehicle
10.1016/j.adhoc.2020.102277,journal,Ad Hoc Networks,sciencedirect,2020-11-01,sciencedirect,A survey on congestion detection and control in connected vehicles,https://api.elsevier.com/content/article/pii/S1570870520306387,"
                  The dynamic nature of vehicular ad hoc network (VANET) induced by frequent topology changes and node mobility, imposes critical challenges for vehicular communications. Aggravated by the high volume of information dissemination among vehicles over limited bandwidth, the topological dynamics of VANET causes congestion in the communication channel, which is the primary cause of problems such as message drop, delay, and degraded quality of service. To mitigate these problems, congestion detection, and control techniques are needed to be incorporated in a vehicular network. Congestion control approaches can be either open-loop or closed loop based on pre-congestion or post congestion strategies. We present a general architecture of vehicular communication in urban and highway environment as well as a state-of-the-art survey of recent congestion detection and control techniques. We also identify the drawbacks of existing approaches and classify them according to different hierarchical schemes. Through an extensive literature review, we recommend solution approaches and future directions for handling congestion in vehicular communications.
               ",autonomous vehicle
10.1533/9780857099440.backmatter,journal,Machine Learning and Data Mining,sciencedirect,2007-12-31,sciencedirect,Index,https://api.elsevier.com/content/article/pii/B9781904275213500216,Unknown,autonomous vehicle
10.1016/j.neures.2020.01.002,journal,Neuroscience Research,sciencedirect,2020-03-31,sciencedirect,Oscillotherapeutics – Time-targeted interventions in epilepsy and beyond,https://api.elsevier.com/content/article/pii/S016801022030002X,"
                  Oscillatory brain activities support many physiological functions from motor control to cognition. Disruptions of the normal oscillatory brain activities are commonly observed in neurological and psychiatric disorders including epilepsy, Parkinson’s disease, Alzheimer’s disease, schizophrenia, anxiety/trauma-related disorders, major depressive disorders, and drug addiction. Therefore, these disorders can be considered as common oscillation defects despite having distinct behavioral manifestations and genetic causes. Recent technical advances of neuronal activity recording and analysis have allowed us to study the pathological oscillations of each disorder as a possible biomarker of symptoms. Furthermore, recent advances in brain stimulation technologies enable time- and space-targeted interventions of the pathological oscillations of both neurological disorders and psychiatric disorders as possible targets for regulating their symptoms.
               ",autonomous vehicle
10.1016/B978-0-12-812594-6.00015-9,journal,Applied Biomechatronics using Mathematical Models,sciencedirect,2018-12-31,sciencedirect,Index,https://api.elsevier.com/content/article/pii/B9780128125946000159,Unknown,autonomous vehicle
10.1016/B978-012443880-4/50046-6,journal,Expert Systems,sciencedirect,2002-12-31,sciencedirect,2: Tools and applications,https://api.elsevier.com/content/article/pii/B9780124438804500466,"
               Over the last two decades, the knowledge engineer's toolbox has continued to develop and today, it constitutes a powerful set of tools for building expert systems to manage real-world problems across a wide range of application areas. Each tool offers unique features that make it well suited for certain types of problems. This chapter explores and reviews the most popular expert system development tools. It provides the history of the development of each tool. This is important because, it demonstrates how the artificial intelligence (AI) community responded to problems beyond the reach of existing methods. This chapter also deals with the working of these tools to provide a general sense of its operation. It considers the relevance of the tool by considering its strengths and weaknesses, and by looking at applications where it is typically employed. It provides valuable references that allow further probing of the tool's theory and applications to design a successful expert system.
            ",autonomous vehicle
10.1016/j.neunet.2021.08.021,journal,Neural Networks,sciencedirect,2021-12-31,sciencedirect,A conversational model for eliciting new chatting topics in open-domain conversation,https://api.elsevier.com/content/article/pii/S0893608021003269,"
                  In human conversations, the emergence of new topics is a key factor in enabling dialogues to last longer. Additional information brought by new topics can make the conversation more diverse and interesting. Chat-bots also need to be equipped with this ability to proactively elicit new chatting topics. However, previous studies have neglected the elicitation of new topics in open-domain conversations. At the same time, previous works have represented topics with word-level keywords or entities. However, a topic is open to multiple keywords and a keyword can reflect multiple potential topics. To move towards a fine-grained topic representation, we represent topic with topically related words. In this paper, we design a novel model, named CMTE, which focuses not only on coherence with context, but also brings up new chatting topics. In order to extract topic information from conversational utterances, a Topic Fetcher module is designed to fetch semantic-coherent topics with the help of topic model. To equip model with the ability to elicit new topics, a Topic Manager module is designed to associate the new topic with context. Finally, responses are generated by a well-designed fusion decoding mechanism to explicitly distinguish between topic words and general words. Experiment results show that our model is better than state of the art in automatic metrics and manual evaluations.
               ",autonomous vehicle
10.1016/S0925-2312(08)00165-3,journal,Neurocomputing,sciencedirect,2008-03-31,sciencedirect,Calendar of Meetings,https://api.elsevier.com/content/article/pii/S0925231208001653,,autonomous vehicle
10.1016/B978-1-78548-021-8.50003-X,journal,Bio-inspired Networking,sciencedirect,2015-12-31,sciencedirect,3: Nervous System,https://api.elsevier.com/content/article/pii/B978178548021850003X,"
               The Nervous System (NS) is responsible for receiving, processing, storing, and transmitting information from inside and outside of an animal’s organisms. The NS is a complex collection of nerves and specialized excitable cells known as neurons, which transmit signals between different parts of the body. The connections among neurons, and between the neurons and the body, are made by means of synapses. Putting it simply, a synapse is a small gap between two neurons. By transmitting signals between different parts of the body, the NS controls both the voluntary and involuntary body actions. It is present, with different levels of complexity, in most multicellular animals. Sponges are the only multicellular animal to lacks an NS, even though they have some homologous structures that play the role of the NS, for example, in locomotion.
            ",autonomous vehicle
10.1016/0303-2647(94)01451-C,journal,Biosystems,sciencedirect,1995-12-31,sciencedirect,Computing with dynamic attractors in neural networks,https://api.elsevier.com/content/article/pii/030326479401451C,"
                  In this paper we report on some new architectures for neural computation, motivated in part by biological considerations. One of our goals is to demonstrate that it is just as easy for a neural net to compute with arbitrary attractors — oscillatory or chaotic — as with the more usual asymptotically stable fixed points. The advantages (if any) of such architectures are currently being investigated; but it seems reasonable that the much richer dynamics of recurrent networks, so obvious in recordings of brain activity, must be useful for something. On the other hand, the constraints of computing with biological wet-ware may make chaotic dynamics unavoidable in complex nervous systems. We hypothesize also that the as yet unrivaled capabilities of the human brain derive from an ability to integrate both analog intuitive pattern recognition operations, and digital symbolic logical operations at the ground level of its hardware. To investigate these possibilities, we have constructed a parallel distributed processing architecture inspired by the structure and dynamics of cerebral cortex. The construction assumes that cortex is a set of coupled associative memories with dynamic attractors. It is guided also by a particular concept of the physical structure required of macroscopic computational systems in general for reliable computation in the presence of noise. Our challenge is to accomplish real tasks that brains can do, using ordinary differential equations, in networks that are as faithful as possible to the known anatomy and dynamics of cortex.
               ",autonomous vehicle
10.1016/B978-155860759-0/50013-5,journal,Computational Intelligence,sciencedirect,2007-12-31,sciencedirect,Index,https://api.elsevier.com/content/article/pii/B9781558607590500135,Unknown,autonomous vehicle
10.1016/j.artint.2009.01.001,journal,Artificial Intelligence,sciencedirect,2009-05-31,sciencedirect,Strengths and synergies of evolved and designed controllers: A study within collective robotics,https://api.elsevier.com/content/article/pii/S0004370209000022,"This paper analyses the strengths and weaknesses of self-organising approaches, such as evolutionary robotics, and direct design approaches, such as behaviour-based controllers, for the production of autonomous robots' controllers, and shows how the two approaches can be usefully combined. In particular, the paper proposes a method for encoding evolved neural-network based behaviours into motor schema-based controllers and then shows how these controllers can be modified and combined to produce robots capable of solving new tasks. The method has been validated in the context of a collective robotics scenario in which a group of physically assembled simulated autonomous robots are requested to produce different forms of coordinated behaviours (e.g., coordinated motion, walled-arena exiting, and light pursuing).",autonomous vehicle
10.1016/j.cjca.2018.04.021,journal,Canadian Journal of Cardiology,sciencedirect,2018-07-31,sciencedirect,Postoperative Remote Automated Monitoring: Need for and State of the Science,https://api.elsevier.com/content/article/pii/S0828282X18303192,"Worldwide, more than 230 million adults have major noncardiac surgery each year. Although surgery can improve quality and duration of life, it can also precipitate major complications. Moreover, a substantial proportion of deaths occur after discharge. Current systems for monitoring patients postoperatively, on surgical wards and after transition to home, are inadequate. On the surgical ward, vital signs evaluation usually occurs only every 4-8 hours. Reduced in-hospital ward monitoring, followed by no vital signs monitoring at home, leads to thousands of cases of undetected/delayed detection of hemodynamic compromise. In this article we review work to date on postoperative remote automated monitoring on surgical wards and strategy for advancing this field. Key considerations for overcoming current barriers to implementing remote automated monitoring in Canada are also presented.",autonomous vehicle
10.1016/j.neucom.2012.07.051,journal,Neurocomputing,sciencedirect,2013-11-23,sciencedirect,Controlling deterministic output variability in a feature extracting chaotic BAM,https://api.elsevier.com/content/article/pii/S0925231213002841,"
                  In this work, a chaotic feature extracting BAM that is capable of generating various behaviors is introduced. These behaviors arise from different attractors, ranging from a stored fixed point to a wandering chaotic region, including variations of all stored fixed points. Variations of stored patterns are generated by the network via the setting of the variability exhibited by every extracted feature. A control method is applied in order to move the network's trajectory into the desired regions and generate chaotic itinerancy, which is reported as a salient property of the brain system. This control is achieved by adjusting the free parameters of the feature extracting units' activation functions. Moreover, it is shown that the higher the number of units applied as feature extractors, the more local features are obtained, control of which leads to greater output uncertainty. However, the structure of this model is very simple and its complex behavior is a result of the interaction among feature units. These observations imply that the proposed model can be feasibly applied in information processing, such as searching in memory, pattern recognition in the presence of noise and variability, modeling episodic memory and decision making in a changing environment.
               ",autonomous vehicle
10.1016/S1877-0509(20)32261-4,journal,Procedia Computer Science,sciencedirect,2020-12-31,sciencedirect,Contents,https://api.elsevier.com/content/article/pii/S1877050920322614,,autonomous vehicle
10.1016/j.clsr.2021.105571,journal,Computer Law & Security Review,sciencedirect,2021-07-31,sciencedirect,DLA Piper - EU Update May 2021,https://api.elsevier.com/content/article/pii/S0267364921000443,,autonomous vehicle
10.1016/j.jnca.2020.102856,journal,Journal of Network and Computer Applications,sciencedirect,2021-01-15,sciencedirect,A comprehensive survey of load balancing techniques in software-defined network,https://api.elsevier.com/content/article/pii/S1084804520303222,"
                  A software-defined network (SDN) separates the network control plane from the data forwarding plane. SDN has shown significant benefits in many ways compared to conventional non-SDN networks. However, traffic distribution in SDN impacts efficiency and raises many other challenges. For instance, uneven load distribution in the SDN significantly impacts the network performance. Hence, several SDN load balancing (LB) techniques have been introduced to improve the efficiency of SDN. In this article, we provide a thematic taxonomy of LB in SDN, considering several parameters from the past technical studies such as the objectives of LB, data plane LB techniques, control plane LB techniques, other aspects of data plane/control plane LB as well as the performance metrics for LB techniques. Furthermore, useful insights on LB and a comparative analysis of various promising SDN LB techniques are also included in the survey. Finally, existing challenges and future direction on SDN LB techniques are highlighted.
               ",autonomous vehicle
10.1016/0925-2312(95)00097-6,journal,Neurocomputing,sciencedirect,1996-10-31,sciencedirect,Robust world-modelling and navigation in a real world,https://api.elsevier.com/content/article/pii/0925231295000976,"
                  This article will discuss a qualitative, topological and robust world-modelling technique with special regard to navigation tasks for mobile robots operating in unknown environments. As a central aspect, the reliability regarding error-tolerance and stability will be emphasized. Benefits and problems involved in exploration as well as in navigation tasks are discussed. The proposed method demands very low constraints for the kind and quality of the employed sensors as well as for the kinematic precision of the utilized mobile platform. Hard real-time constraints can be handled due to the low computational complexity.
                  The principal discussions are supported by real-world experiments with the mobile robot ‘ALICE’
                        1
                     
                     
                        1
                        The project ALICE is supported by the EU-project DG XII, F-5 (Teleman).
                     .
               ",autonomous vehicle
10.1016/B978-012506041-7/50011-5,journal,Evidence-Based Educational Methods,sciencedirect,2004-12-31,sciencedirect,Chapter 10: Adaptive Computerized Educational Systems: A Case Study,https://api.elsevier.com/content/article/pii/B9780125060417500115,"
               Adaptive instruction focuses on textual presentation and support services that adapt to meet the needs of the user in the best way possible; however, even within this meaning the term often describes at least two different instructional service strategies: strategies that are homeostatic and those that are truly adaptive in the same sense that control systems engineers use the term. Homeostatic characteristics common to home air-conditioning systems serve as a model for almost all modern “adaptive” instructional software systems. Upon closer inspection, it is a somewhat misguided use of the term “adaptive.” It is certainly not consistent with how cybernetic and systems researchers would describe the feedback-driven, disturbance-control dynamics for maintaining stability in homeostatic systems like the air-conditioning example. Truly adaptive systems also include the metaphorical ability to learn or adjust by self-modifying the goal or the desired state. That dramatically increases the long-term maintenance or even enhanced development of the system's integrity. The concept of managing the learning process suggests that there is a need to be sensitive to where the student is at all times in terms of the student's need for prompting, segmenting content, and reinforcing through testing results. Such principles guide educators to begin with the size of content segment best suited to an individual student.
            ",autonomous vehicle
10.1016/0925-2312(94)90035-3,journal,Neurocomputing,sciencedirect,1994-02-28,sciencedirect,Backpropagation networks for logic constraint solving,https://api.elsevier.com/content/article/pii/0925231294900353,"
                  Several different backpropagation networks are presented to solve the satisfaction of a conjuctive normal form of boolean clauses (CNF-SAT), a well-known and very important NP-hard problem. The CNF-SAT problem is crucial for solving any Constraint Satisfaction Problem (CSP). CSPs are very useful models for several practical real-life problems.
                  First, a very simple Constraint Satisfaction Problem, a resource allocation problem, is discussed to introduce a novel approach. Then the approach is generalized for the more difficult CNF-SAT. The technique is very well suited to a connectionist implementation.
                  The proposed backpropagation networks are compared, and the results of significant test are described. The paper shows that some of these networks can indeed be used to effectively solve the considered problem.
               ",autonomous vehicle
10.1016/B978-0-12-821326-1.00020-6,journal,An Industrial IoT Approach for Pharmaceutical Industry Growth,sciencedirect,2020-12-31,sciencedirect,Index,https://api.elsevier.com/content/article/pii/B9780128213261000206,Unknown,autonomous vehicle
10.1016/j.psep.2020.09.009,journal,Process Safety and Environmental Protection,sciencedirect,2021-03-31,sciencedirect,Recent Advances in Sensing and Assessment of Corrosion in Sewage Pipelines,https://api.elsevier.com/content/article/pii/S0957582020317262,"
                  Corrosion is known as the gradual destruction of materials, leading to structural integrity loss and deteriorates the surface function. Regarding sewage pipelines, corrosion is vital due to its substantial financial, health, and safety costs for society, and it is considered as one of the biggest problems facing water and wastewater infrastructure. Also, it is the primary cause of chemical property alteration, efficiency loss, life span reduction, etc. To overcome the resulting problems, various researches have been performed to understand not only the effective parameters leading to corrosion in sewer pipes but also monitoring the infrastructure conditions. Studies have depicted that developments in sensing systems to detect effective parameters in pipe corrosion such as temperature, H2S, and pH, have significantly reduced damage to the industrial equipment of sewage pipelines caused by corrosion. This paper presents a critical review of the effective factors resulting in sewer pipeline corrosion and discusses advanced sensing systems utilized for relevant monitoring. Also, microbiologically induced corrosion and effective factors are individually discussed. Moreover, various data analysis techniques adopted to evaluate outputs of the sensors for corrosion prediction have been explored. Finally, recommendations and future directions for improving sensing accuracy and robustness are detailed.
               ",autonomous vehicle
10.1016/B978-0-12-805095-8.09989-0,journal,The Five Technological Forces Disrupting Security,sciencedirect,2018-12-31,sciencedirect,Index,https://api.elsevier.com/content/article/pii/B9780128050958099890,Unknown,autonomous vehicle
10.1016/j.tsep.2019.03.002,journal,Thermal Science and Engineering Progress,sciencedirect,2019-05-31,sciencedirect,"Challenges in, and the development of, building energy saving techniques, illustrated with the example of an air source heat pump",https://api.elsevier.com/content/article/pii/S2451904918306140,"
                  Energy consumption of building takes a big proportion among all consumers, which promotes the research topics around building energy saving very hot. However, it is very hard and impossible to introduce all current trends in such a big field in an article. Nearly all branches and small crossing fields have been reviewed in the open literature. Considering the two aforementioned reasons, the research trends in building and energy saving are shortly reviewed here, taking the topic of air source heat pump as a typical example. Firstly, the background of building energy saving and its highlights over the past 50 years are divided and introduced, supported with published data of journal articles in Elsevier. It is expected to show the reader a whole map about this topic. This is followed by the reviews on building materials, consisting of thermal insulation material and phase change materials, and on building equipment, from building automation system to heating, ventilation and air-conditioning system, and then to detailed air parameter control strategy. Thirdly, it is reached to the air source heat pump, with trends around system and frosting/defrosting problem separately reviewed. As concluded, both of fundamental mechanism researches and application technology solutions are important. Research trends in this field are moving towards intelligent, multidimensional, and interdisciplinary.
               ",autonomous vehicle
10.1016/j.inffus.2021.09.017,journal,Information Fusion,sciencedirect,2022-02-28,sciencedirect,"On the use of information fusion techniques to improve information quality: Taxonomy, opportunities and challenges",https://api.elsevier.com/content/article/pii/S1566253521001925,"
                  The information fusion field has recently been attracting a lot of interest within the scientific community, as it provides, through the combination of different sources of heterogeneous information, a fuller and/or more precise understanding of the real world than can be gained considering the above sources separately. One of the fundamental aims of computer systems, and especially decision support systems, is to assure that the quality of the information they process is high. There are many different approaches for this purpose, including information fusion. Information fusion is currently one of the most promising methods. It is particularly useful under circumstances where quality might be compromised, for example, either intrinsically due to imperfect information (vagueness, uncertainty, …) or because of limited resources (energy, time, …). In response to this goal, a wide range of research has been undertaken over recent years. To date, the literature reviews in this field have focused on problem-specific issues and have been circumscribed to certain system types. Therefore, there is no holistic and systematic knowledge of the state of the art to help establish the steps to be taken in the future. In particular, aspects like what impact different information fusion methods have on information quality, how information quality is characterised, measured and evaluated in different application domains depending on the problem data type or whether fusion is designed as a flexible process capable of adapting to changing system circumstances and their intrinsically limited resources have not been addressed. This paper aims precisely to review the literature on research into the use of information fusion techniques specifically to improve information quality, analysing the above issues in order to identify a series of challenges and research directions, which are presented in this paper.
               ",autonomous vehicle
10.1016/B978-0-12-816176-0.00046-6,journal,Handbook of Medical Image Computing and Computer Assisted Intervention,sciencedirect,2020-12-31,sciencedirect,Index,https://api.elsevier.com/content/article/pii/B9780128161760000466,Unknown,autonomous vehicle
10.1016/j.ymssp.2010.11.018,journal,Mechanical Systems and Signal Processing,sciencedirect,2011-07-31,sciencedirect,Prognostic modelling options for remaining useful life estimation by industry,https://api.elsevier.com/content/article/pii/S0888327010004218,"
                  Over recent years a significant amount of research has been undertaken to develop prognostic models that can be used to predict the remaining useful life of engineering assets. Implementations by industry have only had limited success. By design, models are subject to specific assumptions and approximations, some of which are mathematical, while others relate to practical implementation issues such as the amount of data required to validate and verify a proposed model. Therefore, appropriate model selection for successful practical implementation requires not only a mathematical understanding of each model type, but also an appreciation of how a particular business intends to utilise a model and its outputs.
                  This paper discusses business issues that need to be considered when selecting an appropriate modelling approach for trial. It also presents classification tables and process flow diagrams to assist industry and research personnel select appropriate prognostic models for predicting the remaining useful life of engineering assets within their specific business environment. The paper then explores the strengths and weaknesses of the main prognostics model classes to establish what makes them better suited to certain applications than to others and summarises how each have been applied to engineering prognostics. Consequently, this paper should provide a starting point for young researchers first considering options for remaining useful life prediction. The models described in this paper are Knowledge-based (expert and fuzzy), Life expectancy (stochastic and statistical), Artificial Neural Networks, and Physical models.
               ",autonomous vehicle
10.1016/S0925-2312(06)00157-3,journal,Neurocomputing,sciencedirect,2006-08-31,sciencedirect,Call for papers: International Conference on Adaptive and Natural Computing Algorithms (ICANNGA '07),https://api.elsevier.com/content/article/pii/S0925231206001573,,autonomous vehicle
10.1016/S0925-2312(05)00273-0,journal,Neurocomputing,sciencedirect,2006-01-31,sciencedirect,Announcements of Conferences,https://api.elsevier.com/content/article/pii/S0925231205002730,,autonomous vehicle
10.1016/j.procs.2013.05.168,journal,Procedia Computer Science,sciencedirect,2013-12-31,sciencedirect,Immunological-based Approach for Accurate Fitting of 3D Noisy Data Points with Bézier Surfaces,https://api.elsevier.com/content/article/pii/S1877050913003116,"Free-form parametric surfaces are common tools nowadays in many applied fields, such as Computer-Aided Design & Manu- facturing (CAD/CAM), virtual reality, medical imaging, and many others. A typical problem in this setting is to fit surfaces to 3D noisy data points obtained through either laser scanning or other digitizing methods, so that the real data from a physical object are transformed back into a fully usable digital model. In this context, the present paper describes an immunological- based approach to perform this process accurately by using the classical free-form Bézier surfaces. Our method applies a powerful bio-inspired paradigm called Artificial Immune Systems (AIS), which is receiving increasing attention from the sci- entific community during the last few years because of its appealing computational features. The AIS can be understood as a computational methodology based upon metaphors of the biological immune system of humans and other mammals. As such, there is not one but several AIS algorithms. In this chapter we focus on the clonal selection algorithm (CSA), which explicitly takes into account the affinity maturation of the immune response. The paper describes how the CSA algorithm can be effectively applied to the accurate fitting of 3D noisy data points with Bézier surfaces. To this aim, the problem to be solved as well as the main steps of our solving method are described in detail. Some simple yet illustrative examples show the good performance of our approach. Our method is conceptually simple to understand, easy to implement, and very general, since no assumption is made on the set of data points or on the underlying function beyond its continuity. As a consequence, it can be successfully applied even under challenging situations, such as the absence of any kind of information regarding the underlying function of data.",autonomous vehicle
10.1016/S0933-3657(98)00073-6,journal,Artificial Intelligence in Medicine,sciencedirect,1999-06-30,sciencedirect,Book Review,https://api.elsevier.com/content/article/pii/S0933365798000736,,autonomous vehicle
10.1016/j.procs.2018.10.094,journal,Procedia Computer Science,sciencedirect,2018-12-31,sciencedirect,Predictive quality performance control in BPM: proposing a framework for predicting quality anomalies,https://api.elsevier.com/content/article/pii/S187705091831740X,"Business process management (BPM) literature suggests that more than 60% of quality improvement projects fail due to factors associated with the lack of predictive quality performance control and the failure of continuously searching for quality anomalies in quality performance over time. Quality anomalies are indications of extreme performance deviation from quality expectations and requirements. The findings suggest that quality performance control in BPM is the scientific method for producing quality anomaly knowledge and signalling opportunities for informed, systematic, and continuous performance improvement. A predictive framework is proposed based on the findings.",autonomous vehicle
10.1016/j.procs.2014.09.072,journal,Procedia Computer Science,sciencedirect,2014-12-31,sciencedirect,Subject Index,https://api.elsevier.com/content/article/pii/S1877050914013210,,autonomous vehicle
10.1016/B978-0-12-812970-8.09992-9,journal,"Mobility Patterns, Big Data and Transport Analytics",sciencedirect,2019-12-31,sciencedirect,Index,https://api.elsevier.com/content/article/pii/B9780128129708099929,Unknown,autonomous vehicle
10.1016/0895-7177(95)00221-9,journal,Mathematical and Computer Modelling,sciencedirect,1996-01-31,sciencedirect,Neurocontrol: A literature survey,https://api.elsevier.com/content/article/pii/0895717795002219,"This paper contains a literature review in Neural Control. The review includes seventyfive annotated citations in experimental and theoretical Neural Control applications. Additionally, another thirty-six citations are included. A brief introduction to general Neural Networks is included. Basic Neurocontrol topologies and training techniques are also reviewed.",autonomous vehicle
10.1016/S1568-4946(08)00084-7,journal,Applied Soft Computing,sciencedirect,2008-09-30,sciencedirect,Subject Index for Volume,https://api.elsevier.com/content/article/pii/S1568494608000847,,autonomous vehicle
10.1016/S1877-0509(18)31601-6,journal,Procedia Computer Science,sciencedirect,2018-12-31,sciencedirect,Table of Contents,https://api.elsevier.com/content/article/pii/S1877050918316016,,autonomous vehicle
10.1016/B978-0-12-804642-5.09992-8,journal,Intelligent Digital Oil and Gas Fields,sciencedirect,2018-12-31,sciencedirect,Index,https://api.elsevier.com/content/article/pii/B9780128046425099928,Unknown,autonomous vehicle
10.1016/j.patcog.2012.12.011,journal,Pattern Recognition,sciencedirect,2013-07-31,sciencedirect,Particle swarm classification: A survey and positioning,https://api.elsevier.com/content/article/pii/S0031320313000022,"
                  This paper offers a survey of recent work on particle swarm classification (PSC), a promising offshoot of particle swarm optimization (PSO), with the goal of positioning it in the overall classification domain. The richness of the related literature shows that this new classification approach may be an efficient alternative, in addition to existing paradigms. After describing the various PSC approaches found in the literature, the paper identifies and discusses two data-related problems that may affect PSC efficiency: high-dimensional datasets and mixed-attribute data. The solutions that have been proposed in the literature for each of these issues are described including recent improvements by a novel PSC algorithm developed by the authors. Subsequently, a positioning PSC for these problems with respect to other classification approaches is made. This is accomplished by using one proprietary and five well known benchmark datasets to determine the performances of PSC algorithm and comparing the obtained results with those reported for various other classification approaches. It is concluded that PSC can be efficiently applied to classification problems with large numbers of instances, both in continuous and mixed-attribute problem description spaces. Moreover, the obtained results show that PSC may not only be applied to more demanding problem domains, but it can also be a competitive alternative to well established classification techniques.
               ",autonomous vehicle
10.1016/S0925-2312(08)00263-4,journal,Neurocomputing,sciencedirect,2008-06-30,sciencedirect,Announcements of Conferences,https://api.elsevier.com/content/article/pii/S0925231208002634,,autonomous vehicle
10.1016/S0925-2312(08)00013-1,journal,Neurocomputing,sciencedirect,2008-01-31,sciencedirect,Announcements of Conferences,https://api.elsevier.com/content/article/pii/S0925231208000131,,autonomous vehicle
10.1016/j.cviu.2013.04.005,journal,Computer Vision and Image Understanding,sciencedirect,2013-08-31,sciencedirect,50 Years of object recognition: Directions forward,https://api.elsevier.com/content/article/pii/S107731421300091X,"
                  Object recognition systems constitute a deeply entrenched and omnipresent component of modern intelligent systems. Research on object recognition algorithms has led to advances in factory and office automation through the creation of optical character recognition systems, assembly-line industrial inspection systems, as well as chip defect identification systems. It has also led to significant advances in medical imaging, defence and biometrics. In this paper we discuss the evolution of computer-based object recognition systems over the last fifty years, and overview the successes and failures of proposed solutions to the problem. We survey the breadth of approaches adopted over the years in attempting to solve the problem, and highlight the important role that active and attentive approaches must play in any solution that bridges the semantic gap in the proposed object representations, while simultaneously leading to efficient learning and inference algorithms. From the earliest systems which dealt with the character recognition problem, to modern visually-guided agents that can purposively search entire rooms for objects, we argue that a common thread of all such systems is their fragility and their inability to generalize as well as the human visual system can. At the same time, however, we demonstrate that the performance of such systems in strictly controlled environments often vastly outperforms the capabilities of the human visual system. We conclude our survey by arguing that the next step in the evolution of object recognition algorithms will require radical and bold steps forward in terms of the object representations, as well as the learning and inference algorithms used.
               ",autonomous vehicle
10.1016/j.imavis.2005.08.009,journal,Image and Vision Computing,sciencedirect,2008-01-01,sciencedirect,Cognitive vision: The case for embodied perception,https://api.elsevier.com/content/article/pii/S0262885606000631,"
                  This paper considers arguments for the necessity of embodiment in cognitive vision systems. We begin by delineating the scope of cognitive vision, and follow this by a survey of the various approaches that can be taken to the realization of artificial cognitive vision systems, focussing on cognitive aspects. These range from the cognitivist symbolic representational paradigm, through connectionist systems and self-organizing dynamical systems, to the enactive cognition paradigm. We then consider various arguments for embodiment, beginning with paradigm-specific cases, and concluding with a paradigm-independent argument for embodied perception and cognition. We explore briefly different forms of embodiment and their relevance to the foregoing viewpoints. We highlight some of the key problems associated with embodied cognitive vision, including the phylogeny/ontogeny trade-off in artificial systems and the developmental limitations imposed by real-time environmental coupling. Finally, we conclude by considering some aspects of natural cognitive systems to see how they can provide insights to help in addressing these problems.
               ",autonomous vehicle
10.1016/B978-075067952-7/50008-8,journal,Cognitive Radio Technology,sciencedirect,2006-12-31,sciencedirect,Chapter 7: Cognitive Techniques: Physical and Link Layers,https://api.elsevier.com/content/article/pii/B9780750679527500088,"
               This chapter discusses the expectation of a fully functional cognitive radio, including the cognitive decision-making process using case-based theory and genetic algorithms (GAs), to solve the multi-objective optimization problem posed by such a radio. It focuses on the intelligent cross-layer optimization of physical (PHY) and link layers. It also defines optimization for a cognitive radio. The chapter presents a discussion of the cognitive radio as a mix of artificial intelligence (AI) and wireless communications. The chapter also addresses the PHY and MAC layers and considers which measurable radio settings and specifications (“knobs”) and which radio and channel performance measures (“meters”) fall into which layer. Multi-objective decision-making (MODM) theory to analyze the radio's performance and presents the analogy of GA to represent the methodology have been introduced.The tiered algorithm structure of the cognition loop, based on modeling, action, feedback, and knowledge representation has also been explored in detail.
            ",autonomous vehicle
10.1016/B978-0-12-820028-5.00008-4,journal,Smart Manufacturing,sciencedirect,2020-12-31,sciencedirect,Chapter 8: Smart manufacturing enabled by continuous monitoring and control of polymer characteristics,https://api.elsevier.com/content/article/pii/B9780128200285000084,"
               Smart manufacturing will have an enormous effect on the efficiency, economics, and quality of polymeric materials. Very few polymer manufacturing processes are fully optimized, and many follow legacy empirical processes known to yield products with acceptable properties. Optimization of these processes will lead to more efficient use of primarily fossil fuel–based energy and feed stocks, plant assets, and labor, as well as reduced emissions per ton of product; to greater worker and environmental safety by eliminating manual reactor sampling; and to an increase in product consistency and quality. This chapter focuses on the potential for smart polymer manufacturing that is currently developing, based on the automatic continuous online monitoring of polymerization (ACOMP) reaction platform. ACOMP continuously withdraws a minor volume of reactor contents from the process, dilutes, and conditions it as needed, producing a stream of dilute, analytical grade polymer solution, which can then flow through any desired train of detectors to obtain such characteristics as weight average molar mass (M
                  
                     w
                  ), reduced viscosity (RV), conversion rates, and copolymer composition.
            ",autonomous vehicle
10.1016/0893-6080(92)90014-A,journal,Neural Networks,sciencedirect,1992-12-31,sciencedirect,"Neural networks for control: Edited by T. Miller, R. S. Sutton, and P. J. Werbos, MIT Press, Cambridge, MA: 1990, hardcover $49.95, 524 pp., ISBN 0-262-13261-3",https://api.elsevier.com/content/article/pii/089360809290014A,,autonomous vehicle
10.1016/B978-0-12-816639-0.20001-X,journal,Smart Cities: Issues and Challenges,sciencedirect,2019-12-31,sciencedirect,Index,https://api.elsevier.com/content/article/pii/B978012816639020001X,Unknown,autonomous vehicle
10.1016/j.artmed.2013.07.002,journal,Artificial Intelligence in Medicine,sciencedirect,2013-11-30,sciencedirect,Phased searching with NEAT in a Time-Scaled Framework: Experiments on a computer-aided detection system for lung nodules,https://api.elsevier.com/content/article/pii/S0933365713000985,"
                  Objective
                  In the field of computer-aided detection (CAD) systems for lung nodules in computed tomography (CT) scans, many image features are presented and many artificial neural network (ANN) classifiers with various structural topologies are analyzed; frequently, the classifier topologies are selected by trial-and-error experiments. To avoid these trial and error approaches, we present a novel classifier that evolves ANNs using genetic algorithms, called “Phased Searching with NEAT in a Time or Generation-Scaled Framework”, integrating feature selection with the classification task.
               
                  Methods and materials
                  We analyzed our method's performance on 360 CT scans from the public Lung Image Database Consortium database. We compare our method's performance with other more-established classifiers, namely regular NEAT, Feature-Deselective NEAT (FD-NEAT), fixed-topology ANNs, and support vector machines (SVMs) using ten-fold cross-validation experiments of all 360 scans.
               
                  Results
                  The results show that the proposed “Phased Searching” method performs better and faster than regular NEAT, better than FD-NEAT, and achieves sensitivities at 3 and 4 false positives (FP) per scan that are comparable with the fixed-topology ANN and SVM classifiers, but with fewer input features. It achieves a detection sensitivity of 83.0±9.7% with an average of 4FP/scan, for nodules with a diameter greater than or equal to 3mm. It also evolves networks with shorter evolution times and with lower complexities than regular NEAT (p
                     =0.026 and p
                     <0.001, respectively). Analysis on the average and best network complexities evolved by regular NEAT and by our approach shows that our approach searches for good solutions in lower dimensional search spaces, and evolves networks without superfluous structure.
               
                  Conclusions
                  We have presented a novel approach that combines feature selection with the evolution of ANN topology and weights. Compared with the original threshold-based Phased Searching method of Green, our method requires fewer parameters and converges to the optimal network complexity required for the classification task at hand. The results of the ten-fold cross-validation experiments also show that our proposed CAD system for lung nodule detection performs well with respect to other methods in the literature.
               ",autonomous vehicle
10.1016/B978-0-12-804043-0.00025-8,journal,Neural Data Science,sciencedirect,2017-12-31,sciencedirect,Glossary (Including Additional Python and MATLAB Packages and Examples),https://api.elsevier.com/content/article/pii/B9780128040430000258,Unknown,autonomous vehicle
10.1016/j.engappai.2016.11.001,journal,Engineering Applications of Artificial Intelligence,sciencedirect,2017-02-28,sciencedirect,Improving relevance in a content pipeline via syntactic generalization,https://api.elsevier.com/content/article/pii/S0952197616302032,"
                  This is a report from the field on a linguistic-based relevance technology based on learning of parse trees for processing, classification and delivery of a stream of texts. We describe the content pipeline for eBay entertainment domain which employs this technology, and show that text processing relevance is the main bottleneck for its performance. A number of components of the content pipeline such as content mining, aggregation, deduplication, opinion mining, integrity enforcing need to rely on domain-independent efficient text classification, entity extraction and relevance assessment operations.
                  Text relevance assessment is based on the operation of syntactic generalization (SG) which finds a maximum common sub-tree for a pair of parse trees for sentences. Relevance of two portions of texts is then defined as a cardinality of this sub-tree. SG is intended to substitute keyword-based analysis for more accurate assessment of relevance which takes phrase-level and sentence-level information into account. In the partial case where short expression are commonly used terms such as Facebook likes, SG ascends to the level of categories and a reasoning technique is required to map these categories in the course of relevance assessment.
                  A number of content pipeline components employ web mining which needs SG to compare web search results. We describe how SG works in a number of components in the content pipeline including personalization and recommendation, and provide the evaluation results for eBay deployment. Content pipeline support is implemented as an open source contribution OpenNLP.Similarity and is available at https://github.com/bgalitsky/relevance-based-on-pars-trees.
               ",autonomous vehicle
10.1016/j.aej.2021.09.013,journal,Alexandria Engineering Journal,sciencedirect,2021-09-23,sciencedirect,Differential evolution: A recent review based on state-of-the-art works,https://api.elsevier.com/content/article/pii/S111001682100613X,"Differential evolution (DE) is a popular evolutionary algorithm inspired by Darwin’s theory of evolution and has been studied extensively to solve different areas of optimisation and engineering applications since its introduction by Storn in 1997. This study aims to review the massive progress of DE in the research community by analysing the 192 articles published on this subject from 1997 to 2021, particularly studies in the past five years. The methodology used to search for relevant DE papers and an overview of the original DE are firstly explained. Recent advances in the modifications proposed to enhance the effectiveness and efficiency of the original DE are reviewed by analysing the strengths and weaknesses of each published work, followed by the potential applications of these DE variants in solving different real-world engineering problems. In contrast to most existing DE review papers, additional analyses are performed in this survey by investigating the impacts of various parameter settings on given DE variants to identify their optimal values required for solving certain problem classes. The qualities of modifications incorporated into selected DE variants are also evaluated by measuring the performance gains achieved in terms of search accuracy and/or efficiency against the original DE. The additional surveys conducted in this study are anticipated to provide more insightful perspectives for both beginners and experts of DE research, enabling their better understanding about current research trends and new motivations to outline appropriate strategic planning for future development works.",autonomous vehicle
10.1016/j.jss.2017.08.038,journal,Journal of Systems and Software,sciencedirect,2017-12-31,sciencedirect,Self-organizing multi-agent systems for the control of complex systems,https://api.elsevier.com/content/article/pii/S0164121217301838,"
                  Because of the law of requisite variety, designing a controller for complex systems implies designing a complex system. In software engineering, usual top-down approaches become inadequate to design such systems. The Adaptive Multi-Agent Systems (AMAS) approach relies on the cooperative self-organization of autonomous micro-level agents to tackle macro-level complexity. This bottom-up approach provides adaptive, scalable, and robust systems. This paper presents a complex system controller that has been designed following this approach, and shows results obtained with the automatic tuning of a real internal combustion engine.
               ",autonomous vehicle
10.1016/j.cviu.2020.103034,journal,Computer Vision and Image Understanding,sciencedirect,2020-10-31,sciencedirect,Refining high-frequencies for sharper super-resolution and deblurring,https://api.elsevier.com/content/article/pii/S1077314220300874,"
                  A sub-problem of paramount importance in super-resolution is the generation of an upsampled image (or frame) that is ‘sharp’. In deblurring, the core problem itself is of removing the blur, and it is equivalent to the problem of generating a ‘sharper’ version of the given image. This sharpness in the generated image comes by accurately predicting the high-frequency details (commonly referred to as fine-details) such as object edges. Thus high-frequency prediction is a vital sub-problem in super-resolution and a core problem in deblurring. To generate a sharp upsampled or deblurred image, this paper proposes a multi-stage neural network architecture ‘HFR-Net’ that works on the principle of ‘explicit refinement and fusion of high-frequency details’. To implement this principle, HFR-Net is trained with a novel 2-phase progressive–retrogressive training method. In addition to the training method, this paper also introduces dual motion warping with attention. It is a technique that is specifically designed to handle videos that have different rates of motion. Results obtained from extensive experiments on multiple super-resolution and deblurring datasets reveal that the proposed approach gives better results than the current state-of-the-art techniques.
               ",autonomous vehicle
10.1016/B978-0-12-386979-1.00001-3,journal,Practical Text Mining and Statistical Analysis for Non-structured Text Data Applications,sciencedirect,2012-12-31,sciencedirect,Chapter 1: The History of Text Mining,https://api.elsevier.com/content/article/pii/B9780123869791000013,,autonomous vehicle
10.1016/j.eswa.2015.06.052,journal,Expert Systems with Applications,sciencedirect,2015-12-01,sciencedirect,A literature review of recommender systems in the television domain,https://api.elsevier.com/content/article/pii/S0957417415004546,"
                  Recommender Systems (RSs) are software tools and techniques providing suggestions of relevant items to users. These systems have received increasing attention from both academy and industry since the 1990s, due to a variety of practical applications as well as complex problems to solve. Since then, the number of research papers published has increased significantly in many application domains (books, documents, images, movies, music, shopping, TV programs, and others). One of these domains has our attention in this paper due to the massive proliferation of televisions (TVs) with computational and network capabilities and due to the large amount of TV content and TV-related content available on the Web. With the evolution of TVs and RSs, the diversity of recommender systems for TV has increased substantially. In this direction, it is worth mentioning that we consider “recommender systems for TV” as those that make recommendations of both TV-content and any content related to TV. Due to this diversity, more investigation is necessary because research on recommender systems for TV domain is still broader and less mature than in other research areas. Thus, this literature review (LR) seeks to classify, synthesize, and present studies according to different perspectives of RSs in the television domain. For that, we initially identified, from the scientific literature, 282 relevant papers published from 2003 to May, 2015. The papers were then categorized and discussed according to different research and development perspectives: recommended item types, approaches, algorithms, architectural models, output devices, user profiling and evaluation. The obtained results can be useful to reveal trends and opportunities for both researchers and practitioners in the area.
               ",autonomous vehicle
10.1016/j.inffus.2013.04.006,journal,Information Fusion,sciencedirect,2014-03-31,sciencedirect,A survey of multiple classifier systems as hybrid systems,https://api.elsevier.com/content/article/pii/S156625351300047X,"
                  A current focus of intense research in pattern classification is the combination of several classifier systems, which can be built following either the same or different models and/or datasets building approaches. These systems perform information fusion of classification decisions at different levels overcoming limitations of traditional approaches based on single classifiers. This paper presents an up-to-date survey on multiple classifier system (MCS) from the point of view of Hybrid Intelligent Systems. The article discusses major issues, such as diversity and decision fusion methods, providing a vision of the spectrum of applications that are currently being developed.
               ",autonomous vehicle
10.1016/B978-0-12-420248-1.00022-2,journal,Artificial Intelligence in Behavioral and Mental Health Care,sciencedirect,2016-12-31,sciencedirect,Index,https://api.elsevier.com/content/article/pii/B9780124202481000222,Unknown,autonomous vehicle
10.1016/S0925-2312(00)00308-8,journal,Neurocomputing,sciencedirect,2001-04-30,sciencedirect,A survey of hybrid ANN/HMM models for automatic speech recognition,https://api.elsevier.com/content/article/pii/S0925231200003088,"
                  In spite of the advances accomplished throughout the last decades, automatic speech recognition (ASR) is still a challenging and difficult task. In particular, recognition systems based on hidden Markov models (HMMs) are effective under many circumstances, but do suffer from some major limitations that limit applicability of ASR technology in real-world environments. Attempts were made to overcome these limitations with the adoption of artificial neural networks (ANN) as an alternative paradigm for ASR, but ANN were unsuccessful in dealing with long time-sequences of speech signals. Between the end of the 1980s and the beginning of the 1990s, some researchers began exploring a new research area, by combining HMMs and ANNs within a single, hybrid architecture. The goal in hybrid systems for ASR is to take advantage from the properties of both HMMs and ANNs, improving flexibility and recognition performance. A variety of different architectures and novel training algorithms have been proposed in literature. This paper reviews a number of significant hybrid models for ASR, putting together approaches and techniques from a highly specialistic and non-homogeneous literature. Efforts concentrate on describing and referencing architectures and algorithms, their advantages and limitations, as well as on categorizing them into broad classes. Early attempts to emulate HMMs by ANNs are first described. Then we focus on ANNs to estimate posterior probabilities of the states of an HMM and on “global” optimization, where a single, overall training criterion is defined over the HMM and the ANNs. Connectionist vector quantization for discrete HMMs, and other more recent approaches are also reviewed. It is pointed out that, in addition to their theoretical interest, hybrid systems have been allowing for tangible improvements in recognition performance over the standard HMMs in difficult and significant benchmark tasks.
               ",autonomous vehicle
10.1016/j.ins.2010.02.005,journal,Information Sciences,sciencedirect,2010-07-15,sciencedirect,On the potential contributions of hybrid intelligent approaches to Multicomponent Robotic System development,https://api.elsevier.com/content/article/pii/S0020025510000605,"
                  The area of cognitive or intelligent robotics is moving from the single monolithic robot control and behavior problem to that of controlling robots with multiple components or multiple robots operating together, and even collaborating, in dynamic and unstructured environments. This paper introduces the topic and provides a general overview of the current state of the field of Multicomponent Robotic Systems focusing on providing some insights into where Hybrid Intelligent Systems could provide key contributions to its advancement. Thus, the aim is to identify prospective research areas and to try to delimit the field from the point of view of the following essential problem: how to coordinate multiple robotic elements in order to perform useful tasks.
               ",autonomous vehicle
10.1016/j.neunet.2006.11.002,journal,Neural Networks,sciencedirect,2007-01-31,sciencedirect,Editorial Board,https://api.elsevier.com/content/article/pii/S0893608006002346,,autonomous vehicle
10.1016/j.ifacol.2018.08.416,journal,IFAC-PapersOnLine,sciencedirect,2018-12-31,sciencedirect,Performance Measurement in Production Internet: An Ecosystemic Perspective,https://api.elsevier.com/content/article/pii/S2405896318315428,"
                  Production Internet goes beyond the peer-to-peer networking, by embodying vertical and horizontal integration. It applies large scale coordination of operations, by merging the control of individual workflows and homeostatic governance. Implementing these functionalities requires adaptive and evolutionary mechanisms. For this purpose performance measurement can be applied. Firstly, the measures used to match vendors and clients or offerings and demands, are discussed, then those applied to aid the adaptive control of operations. Finally, the paper investigates such performance metrics that can support homeostasis and evolutionary mechanisms. The proposed use of performance measurement relies on predefined functional setup and reference architecture of Production Internet. As for the virtual subdomain, it considers both condition based decision making and reconfiguring the rules of operative control. What regards the real subdomain it presumes individual self-operated adaptation of the actors.
               ",autonomous vehicle
10.1016/B978-0-12-815630-8.20001-X,journal,Algorithmic Trading Methods,sciencedirect,2021-12-31,sciencedirect,Index,https://api.elsevier.com/content/article/pii/B978012815630820001X,Unknown,autonomous vehicle
10.1016/B978-0-12-813304-0.00008-6,journal,Safety Theory and Control Technology of High-Speed Train Operation,sciencedirect,2018-12-31,sciencedirect,Chapter 8: Real-Time Monitoring and Early Warning of a Train’s Running State and Operation Behavior,https://api.elsevier.com/content/article/pii/B9780128133040000086,"
               In the view of the whole system this chapter introduces the train state monitoring and early warning, which is based on big data and combines big data theory with human and signaling systems, comprehensively considering the coordination among the TCC, CBI, CTC, and other subsystems. We analyze the factors that can affect the train state of operation systematically, including the operation action of the signaling system and the operator, realizing the real-time and online monitoring and early warning of train running state then to ensure the safety of train operation.
            ",autonomous vehicle
10.1016/j.neucom.2018.07.061,journal,Neurocomputing,sciencedirect,2018-11-17,sciencedirect,SliceNet: A proficient model for real-time 3D shape-based recognition,https://api.elsevier.com/content/article/pii/S0925231218308968,"
                  The field of 3D object recognition has been dominated by 2D view-based methods mostly because of lower accuracy and larger computational load of 3D shape-based methods. Recognition with a 3D shape yields appreciable advantages e.g., making use of depth information and independence to ambient lighting, but we are still away from an eminent solution for 3D shape-based object recognition. In this paper first, a statistical method capable of modeling the input and output with random variables is used to investigate the reasons contributing to the inferior performance of the 3D convolution operation. The analysis suggests that the excessive size of the kernel causes the dramatic blowing up of the output variance of the 3D convolution operation and makes the output feature less discriminating. Then, based on the results of this analysis and inspired by the underlying principle of 3D shapes, SliceNet is proposed to learn 3D shape features using anisotropic 3D convolution. Specifically, the proposed method learns features from original 2D planar sketches comprising the 3D shape and has a significantly lower output variance. Experiments on ModelNet show that the recognition accuracy of the proposed SliceNet is comparable to well-established 2D view-based methods. Besides, the SliceNet also has a significantly smaller model size, simpler architecture, less training and inference time compared to 2D view-based and other 3D object recognition methods. An experiment with real-world data shows that the model trained on CAD files can be generalized to real-world objects without any re-training or fine-tuning.
               ",autonomous vehicle
10.1016/B0-12-227410-5/00317-3,journal,Encyclopedia of Physical Science and Technology,sciencedirect,2003-12-31,sciencedirect,Humanoid Robots,https://api.elsevier.com/content/article/pii/B0122274105003173,,autonomous vehicle
10.1016/B978-0-08-101107-2.00066-X,journal,Handbook of Categorization in Cognitive Science,sciencedirect,2017-12-31,sciencedirect,Index,https://api.elsevier.com/content/article/pii/B978008101107200066X,Unknown,autonomous vehicle
10.1016/j.asoc.2020.106827,journal,Applied Soft Computing,sciencedirect,2021-01-31,sciencedirect,A soft-computing framework for automated optimization of multiple product quality criteria with application to micro-fluidic chip production,https://api.elsevier.com/content/article/pii/S1568494620307651,"
                  We describe a general strategy for optimizing the quality of products of industrial batch processes that comprise multiple production stages. We focus on the particularities of applying this strategy in the field of micro-fluidic chip production. Our approach is based on three interacting components: (i) a new hybrid design of experiments (DoE) strategy that combines expert- and distribution-based space exploration with model-based uncertainty criteria to obtain a representative set of initial samples (i.e., settings of essential machining process parameters), (ii) construction of linear and non-linear predictive mappings from these samples to describe the relation between machining process parameters and resulting quality control (QC) values and (iii) incorporation of these mappings as surrogate fitness estimators into a multi-objective optimization process to discover settings that outperform those routinely used by operators. These optimized settings lead to final products with better quality and/or higher functionality for the clients. The optimization module employs a co-evolutionary strategy we developed that is able to deliver better Pareto non-dominated solutions than the renowned NSGA-II multi-objective solver. We applied our proposed high-level surrogate-based multi-objective strategy both in a single/late-stage optimization scenario and in a more challenging multi-stage scenario, yielding final optimization results that improved parameter settings and thus product quality compared to standard expert-based production process parameterizations.
               ",autonomous vehicle
10.1016/j.promfg.2015.07.614,journal,Procedia Manufacturing,sciencedirect,2015-12-31,sciencedirect,Ergonomic Criteria in the Investigation of Indirect Causes of Accidents,https://api.elsevier.com/content/article/pii/S2351978915006150,"Guidelines on ergonomic deficiencies may be derived from accident reports which commonly name employees as being directly at fault (direct causes of accident). The root causes of accidents found of significance for prevention purposes include the misalignment of workstations with the psychophysical capabilities of workers. Such causes lead to the engagement of ergonomists. Accident analysis tools and detailed specifications of direct accident causes have been used to develop a concept of a method for identifying ergonomic deficiencies. The accident investigation methods and tools used to classify accident causes by means of specified criteria make it possible to identify ergonomic deficiencies at each stage of assessment. The author has additionally described a concept of an expert system which supports the investigation of occupational accidents from the ergonomic standpoint. The accident investigation methods identified in this paper are TOL, Job Safety Analysis (JSA), “What if …”, FMEA, STEP, OARU (Occupational Accident Research Unit), FTA, the Ishikawa Diagram, the energy transfer method, “4xwhy”, MORT, KIK, WAIT (Work Accident Investigation Technique), as well as the ILCI and TRIPOD models. A concept has also been offered for using network methods to establish a hierarchy of ergonomic incompliances.",autonomous vehicle
10.1016/S0020-7373(07)80006-X,journal,International Journal of Man-Machine Studies,sciencedirect,1991-07-31,sciencedirect,A connectionist and symbolic hybrid for improving legal research,https://api.elsevier.com/content/article/pii/S002073730780006X,"
                  The task of legal research is complex, requiring an understanding of multiple interpretations of legal language, an awareness of the relationships between judicial decisions, and an ability to use analogical reasoning to find relevant documents. Attempts to automate parts of the process with computers have met with limited success. We describe a system called SCALIR which attempts to remedy these problems by combining connectionist and symbolic artificial intelligence approaches. This hybrid representational scheme gives SCALIR the ability to make both associative and deductive inferences. The system also provides an alternative to the traditional view of computer-assisted legal research by using a direct-manipulation style, interface, making searches into an interactive process, and by employing user feedback to improve its performance over time. In addition, we suggest a further application of SCALIR to a part of the analogy task, and argue that this approach is complementary with case-based reasoning techniques.
               ",autonomous vehicle
10.1016/j.neunet.2007.12.002,journal,Neural Networks,sciencedirect,2008-01-31,sciencedirect,Editorial Board,https://api.elsevier.com/content/article/pii/S0893608007002304,,autonomous vehicle
10.1016/B978-008044612-7/50056-1,journal,Handbook of Categorization in Cognitive Science,sciencedirect,2005-12-31,sciencedirect,Chapter 1: TO COGNIZE IS TO CATEGORIZE: COGNITION IS CATEGORIZATION,https://api.elsevier.com/content/article/pii/B9780080446127500561,"
               We organisms are sensorimotor systems. Things in the world come in contact with our sensory surfaces, and we interact with them based on what that sensorimotor contact “affords.” All of our categories consist of ways we behave differently toward different kinds of things – things we do or do not eat, mate with, or flee from; or the things that we describe, through our language, as prime numbers, affordances, absolute discriminables, or truths. That is all that cognition is for, and about.
               Pensar es olvidar diferencias, es generalizar, abstraer.
               En el abarrotado mundo de Funes no había sino detalles, casi inmediatos. Borges
               
                  (“Funes el memorioso”)
               
            ",autonomous vehicle
10.1016/j.eswa.2015.04.054,journal,Expert Systems with Applications,sciencedirect,2015-11-15,sciencedirect,Evolutionary fine-tuning of automated semantic annotation systems,https://api.elsevier.com/content/article/pii/S0957417415002961,"
                  Considering the ever-increasing speed at which new textual content is generated, an efficient and effective use of large text corpora requires automated natural language processing and text analysis tools. A subset of such tools, namely automated semantic annotation tools, are capable of interlinking syntactical forms of text with their underlying semantic concepts. The optimal performance of automated semantic annotation tools often depends on tuning the values of the tools’ adjustable parameters to the specificities of the annotation task, and particularly to the characteristics of the text to be annotated. Such characteristics include the text domain, terseness or verbosity level, text length, structure and style. Since the default configuration of annotation tools is not suitable for the large variety of input texts that different combinations of these attributes can produce, users often need to adjust the annotators’ tunable parameters in order to get the best results. However, the configuration of semantic annotators is presently a tedious and time consuming task as it is primarily based on a manual trial-and-error process. In this paper, we propose a Parameter Tuning Architecture (PTA) for automating the task of configuring parameter values of semantic annotation tools. We describe the core fitness functions of PTA that operate on the quality of the annotations produced, and offer a solution, based on a genetic algorithm, for searching the space of possible parameter values. Our experiments demonstrate that PTA enables effective configuration of parameter values of many semantic annotation tools.
               ",autonomous vehicle
10.1016/j.comcom.2020.06.030,journal,Computer Communications,sciencedirect,2020-07-01,sciencedirect,Towards trustworthy Internet of Things: A survey on Trust Management applications and schemes,https://api.elsevier.com/content/article/pii/S0140366419319073,"
                  Advancement in technology with the proliferation of new wireless communication protocols gave rise to the new era of ubiquitous computing, called Internet of Things (IoT). IoT facilitates connectivity between various heterogeneous physical devices through the internet to advantage users with intelligent and more advanced services. Effective utilization of these services demands a secure system where one can rely on the source of the information together with the received information. Trust Management (TM) is a crucial aspect of security that aims to maintain reliability in a system by ensuring the secure exchange of information. Using the concept of local and global perception about the reputation, TM measures the degree of trust on the system’s entities and endeavors to reduce risk and uncertainty in the system. For IoT, TM paves the way to accomplish various decision-making tasks, like reliable service composition, secure routing, device authentication, access control, etc. However, design and deployment of TM for IoT are hindered by the inherent characteristics of IoT systems that demand to be addressed.
                  In this paper, we identified various applications of TM and examined issues in the design and deployment of TM for IoT. A clear vision towards TM system, explaining the different phases involved in the process of managing the trust, is presented. Furthermore, an exhaustive survey on various TM schemes developed for IoT with their applicability and addressing issues is provided. The survey is conducted considering direct observations and indirect recommendations based distributed, semi-distributed, and centralized schemes along with the review on blockchain technology-based schemes for trust management in IoT. In addition to that, a comparative study of the existing schemes based on the various system measures like computation model, input attributes, evaluation tool, and performance metrics examining their strengths and weaknesses is given. Finally, the paper highlights open research challenges investigated by the survey to present future direction for the researchers.
               ",autonomous vehicle
10.1016/j.solener.2019.10.072,journal,Solar Energy,sciencedirect,2019-12-31,sciencedirect,100 Years of daylighting: A chronological review of daylight prediction and calculation methods,https://api.elsevier.com/content/article/pii/S0038092X19310692,"
                  Daylighting is a parallel universe to Architecture, where architects benefit greatly from daylight prediction techniques, which have witnessed a paradigm shift from simple methods to more sophisticated computational simulation tools. Still, such accumulating complexities made many designers disinclined to integrate what they consider difficult methods into their practices, even hindered the casual use of simulation tools, due to the lack of essential knowledge, among other complications. Herein, this research aims to provide a comprehensive review of over 100 years of growing fundamental directions to predict the amount of daylight inside buildings, with a particular focus on tracing sky models, weather datasets, building geometry and daylight calculation methods, which drove the progress of performance metrics and simulation tools, considering detailed descriptions of 50 prevalent simulation tools. This historical review is conducted with the architects’ nature in mind to underline existing knowledge gaps in the research domain and reveal future perspectives. Another implication of this research is to remove ambiguity of unfamiliar terms and technicalities, helping practitioners, especially young architects, of different backgrounds and expertise to grasp the essential daylight-related topics, guiding their decisions on suitable tools to use in building design.
               ",autonomous vehicle
10.1016/j.neucom.2012.03.021,journal,Neurocomputing,sciencedirect,2013-06-03,sciencedirect,Analysis of strategy in robot soccer game,https://api.elsevier.com/content/article/pii/S0925231212006686,"
                  Strategy is a kernel subsystem of robot soccer game. In our work, we present an approach to describe the strategy of the game, based on which we explain the morphology of strategy set. Loop strategies are likely to make robots be in a trap of executing repeated actions. We analyze the existence criterion of loop strategies, and then present some corollaries and theorems, by which the loop strategies and chain strategies can be found, also superfluous strategies and inconsistent strategies. We present a ranking model that indicates the weak node in strategy set. We also present a probability-based model which is the basis of evaluation of strategy. Additionally, we present a method to generate offensive strategy, and the statistic results of simulation game prove the validity of the method.
               ",autonomous vehicle
10.1016/0893-6080(88)90044-5,journal,Neural Networks,sciencedirect,1988-12-31,sciencedirect,Detailed daily schedule for the first annual meeting of the international neural network society,https://api.elsevier.com/content/article/pii/0893608088900445,,autonomous vehicle
10.1016/S0740-8188(02)00124-X,journal,Library & Information Science Research,sciencedirect,2002-12-31,sciencedirect,Automatic extraction of relationships between terms by means of Kohonen's algorithm,https://api.elsevier.com/content/article/pii/S074081880200124X,"
                  This article describes a method of finding the contextual relationships among different terms in a database. First, the vector model is used to represent the terms as vectors according to which documents they appear in. Second, these vectors are used as the input to a Kohonen network, which organizes them topologically. This organization, in turn, generates term clusters arranged on a grid, so that each term is not only related to the others in its own cluster but also to those of neighboring clusters.
               ",autonomous vehicle
10.1016/B978-0-12-819593-2.00004-2,journal,Emergence of Pharmaceutical Industry Growth with Industrial IoT Approach,sciencedirect,2020-12-31,sciencedirect,Chapter 4: Internet of Things–based pharmaceutics data analysis,https://api.elsevier.com/content/article/pii/B9780128195932000042,"
               Internet of Things (IoT) helps in creating innovation in pharma industry that will benefit manufacturers and as well as patients. The purpose of pharmaceutics is to improve patients’ lives so that they can live confidently and actively with the condition they suffer from. The IoT has already made tremendous changes in the pharma industry, and the technology is still in its infancy. This chapter aims to provide the various techniques related to IoT for pharmaceutical related data and further assist in the analysis of the data generated from pharmaceutical field. The various pharmaceutical concepts based on IoT are being investigated, and the clinical data is being investigated for the body movements, and an analysis is done based on that. The analysis of the proposed system shows that it is 4% better in reducing the error rate of the results.
            ",autonomous vehicle
10.1016/B978-0-08-051433-8.50006-9,journal,Practical Neural Network Recipies in C++,sciencedirect,1993-12-31,sciencedirect,1: Foundations,https://api.elsevier.com/content/article/pii/B9780080514338500069,"
               A brief history and overview of neural networks serves as a concise introduction for those readers having little or no experience in this area.
            ",autonomous vehicle
10.1016/B978-0-12-809633-8.20302-6,journal,Encyclopedia of Bioinformatics and Computational Biology,sciencedirect,2019-12-31,sciencedirect,Translational and Disease Bioinformatics,https://api.elsevier.com/content/article/pii/B9780128096338203026,"
               Translational bioinformatics is an emerging field which brings biological research into clinical significance in both patient care and drug discovery. It involves the development of computational algorithms to integrate and analyze the clinical as well as biological data to explore disease heterogeneity. The hunt for disease gene(s) with complete understanding of the intricate network of molecular processes involved in disease progression is crucial step for drug development. This article attempts to describe the data integration strategy of biological and clinical data to obtain the explicit goals. It also throws light on the major datasets and tools to understand the approaches of translational bioinformatics in alleviating diseases.
            ",autonomous vehicle
10.1016/B978-0-12-396502-8.00045-0,journal,Academic Press Library in Signal Processing,sciencedirect,2014-12-31,sciencedirect,Authors Biography,https://api.elsevier.com/content/article/pii/B9780123965028000450,,autonomous vehicle
10.1533/9781908818508.73,journal,Formulation Tools for Pharmaceutical Development,sciencedirect,2013-12-31,sciencedirect,4: Expert system for the development and formulation of push–pull osmotic pump tablets containing poorly water-soluble drugs,https://api.elsevier.com/content/article/pii/B9781907568992500049,"
               
                  The push–pull osmotic pump (PPOP) is a challenging technology in the drug delivery system (DDS) domain, and more and more institutes and companies are interested in this technology. A tool which could employ the technology and the experience of experts to design the formulations of PPOP would be helpful and convenient to reduce the cost and shorten the time of PPOP development. As yet, there is no expert system available for the formulation of controlled release dosage forms. This chapter describes such a tool, designed to employ the knowledge of human experts to help those who have no or little knowledge about PPOP and want to develop such products. Generally, an expert system contains three major components: a man–machine interface, a knowledge base, and an inference engine. Here, the knowledge base including database and rule base was built based on available experiences of professionals and experimental data. The prediction model of release behaviors was built using a back-propagation (BP) neural network. The formulation design model was established based on the prediction model of release behaviors, which was the nucleus of the inference engine. Finally, the expert system program was constructed by VB.NET associating with SQL Server.
            ",autonomous vehicle
10.1016/j.comcom.2020.01.004,journal,Computer Communications,sciencedirect,2020-02-01,sciencedirect,Energy aware edge computing: A survey,https://api.elsevier.com/content/article/pii/S014036641930831X,"
                  Edge computing is an emerging paradigm for the increasing computing and networking demands from end devices to smart things. Edge computing allows the computation to be offloaded from the cloud data centers to the network edge and edge nodes for lower latency, security and privacy preservation. Although energy efficiency in cloud data centers has been broadly investigated, energy efficiency in edge computing is largely left uninvestigated due to the complicated interactions between edge devices, edge servers, and cloud data centers. In order to achieve energy efficiency in edge computing, a systematic review on energy efficiency of edge devices, edge servers, and cloud data centers is required. In this paper, we survey the state-of-the-art research work on energy-aware edge computing, and identify related research challenges and directions, including architecture, operating system, middleware, applications services, and computation offloading.
               ",autonomous vehicle
10.1016/j.oceaneng.2018.07.004,journal,Ocean Engineering,sciencedirect,2018-09-15,sciencedirect,A study of the development of a condition-based maintenance system for an LNG FPSO,https://api.elsevier.com/content/article/pii/S0029801818312277,"
                  In general, the equipment failures or accidents of offshore plants during operation and maintenance (O&M) period cause catastrophic damage. Thus, it is necessary to undertake proactive maintenance in advance in order to avoid abnormal situations. Currently, owing to the emergence of information communication technologies (ICTs) and sensor technologies, it is possible to gather the health status data of important equipment and use this information for maintenance during the O&M period. It sheds light on condition-based maintenance (CBM) strategy. In this study, we introduce a case study on the development of a CBM system (CBMS) for an oil and gas offshore plant, i.e. liquefied natural gas floating production storage and offloading vessel (LNG FPSO). The study includes the introduction of the system architecture, main components, diagnostics and prognostics methods of the system, as well as a discussion of its implementation.
               ",autonomous vehicle
10.1016/S0007-8506(07)62994-1,journal,CIRP Annals,sciencedirect,2001-12-31,sciencedirect,Emergent Synthesis Methodologies for Manufacturing,https://api.elsevier.com/content/article/pii/S0007850607629941,"
                  This paper offers a concise overview of new manufacturing methodologies that are based on emergent synthesis. Starting with the conceptual questions concerning analysis, synthesis and emergence, it classifies the difficulties of synthesis problems with respect to the incompleteness of human knowledge on the environment and of the specification of purpose of the artifact. Then, it clarifies the importance of emergence and self-reference and their relations to solving synthesis problems. It reviews about 300 papers according to the problem classification and in terms of three phases of concept formation, theory and application. It also discusses the system-theoretical aspects of artifactual environment.
               ",autonomous vehicle
10.1016/j.trpro.2017.05.173,journal,Transportation Research Procedia,sciencedirect,2017-12-31,sciencedirect,Maritime vessel traffic modeling in the context of concept drift,https://api.elsevier.com/content/article/pii/S2352146517304660,"Maritime traffic modeling serves the purpose of extracting human-readable information and discovering knowledge in the otherwise illegible mass of traffic data. The goal of this study is to examine the presence and character of fluctuations in maritime traffic patterns. The main objective is to identify such fluctuations and capture them in terms of a concept drift, i.e., unforeseen shifts in statistical properties of the modeled target occurring over time. The empirical study is based on a collection of AIS vessel tracking data, spanning over a year. The scope of the study limits the AIS data area to the Baltic region (9-31°E, 53-66°N), which experiences some of the most dense maritime traffic in the world. The investigations employ a novel maritime traffic modeling method based on the potential fields concept, adapted for this study to facilitate the examination of concept drift. The concept drift is made apparent in course of the statistical and visual analysis of the experimental results. This study shows a number of particular cases, in which the maritime traffic is affected by concept drifts of varying extent and character. The visual representations of the traffic models make shifts in the traffic patterns apparent and comprehensible to human eye. Based on the experimental outcomes, the robustness of the modeling method against concept drift in traffic is discussed and improvements are proposed. The outcomes provide insights into regularly reoccurring drifts and irregularities within the traffic data itself that may serve to further optimize the modeling method, and – in turn – the performance of detection based on it.",autonomous vehicle
10.1016/j.engappai.2007.04.003,journal,Engineering Applications of Artificial Intelligence,sciencedirect,2008-03-31,sciencedirect,A symbol-based intelligent control system with self-exploration process,https://api.elsevier.com/content/article/pii/S0952197607000516,"
                  This paper presents a symbol-based intelligent control system (SyICS) with a self-exploration process. The SyICS is comprised of a symbolic controller, a percepter, and a self-adaptor, and is a rule-based control system with on-line parametric adaptation. The symbolic controller consists of a number of symbolic rules, such as IF–THEN rules, for controlling the plant. The percepter is a sensory mechanism to perceive the control efficiency. Once the sensory information is found to be improper, i.e., there is inefficient control, the self-adaptor will be activated; otherwise, the symbolic controller will keep on the controlling assignment. The self-adaptor is an adaptive mechanism to explore the new symbolic rules and update the knowledge base for on-line and real-time adaptation. The self-exploration process is applied for the self-adaptor, and the hybrid genetic algorithm with variable-length chromosome is presented to fulfill the self-exploration process. The advantages of the SyICS are: (1) the symbolic controller is intuitive and easy to implement, and (2) The mechanism of the on-line adaptation is adopted and performed by the efficient hybrid genetic algorithm. A robotic path planning application is used to demonstrate the SyICS approach by comparing it with other intelligent control methods. The simulation results show that the robotic paths of SyICS model are the most efficient for all cases based on the path's efficiency measure.
               ",autonomous vehicle
10.1016/j.jnca.2017.11.006,journal,Journal of Network and Computer Applications,sciencedirect,2018-01-15,sciencedirect,Analytics in/for cloud-an interdependence: A review,https://api.elsevier.com/content/article/pii/S1084804517303764,"
                  Cloud computing has brought a paradigmatic shift in providing data storage as well as computing resources. With the ever-increasing demand for cloud computing, the number of cloud providers is also increasing evidently, which poses challenges as well as opportunities for consumers and providers. From a consumer point of view, efficient selection of cloud resources at a minimum cost is a big challenge. On the other hand, a provider has to meet consumers’ requirements with sufficient profit in the fiercely competitive market. The relationship between cloud computing is truly symbiotic in the sense that cloud computing makes the practice of analytics more pervasive while analytics makes cloud computing more efficient and optimal in a lot of ways. In addressing these issues, analytics plays an important role. In this paper, we reviewed some important research articles, which focus on cloud computing from the viewpoint of analytics. Analytics and cloud computing are found to be quite interdependent. From analytics perspective, cloud computing makes available high-end computing resources even to an individual customer at an affordable price. We call this thread “Analytics in Cloud”. From the point of view of cloud computing, efficient management, allocation, and demand prediction can be performed using analytics. We call this thread “Analytics for Cloud”. This review paper is mainly based on these two threads of thought process. In this regard, we reviewed eighty-eight research articles published during 2003–2017 related to the formidable duo of cloud computing and analytics.
               ",autonomous vehicle
10.1016/j.istr.2007.09.001,journal,Information Security Technical Report,sciencedirect,2007-12-31,sciencedirect,Biologically-inspired Complex Adaptive Systems approaches to Network Intrusion Detection,https://api.elsevier.com/content/article/pii/S1363412707000416,"
                  The pervasiveness of the computing power has made it an inevitable commodity of the modern time. The inexorable technological advances clearly predict the continually increasing reliance of human life on the computing systems in the future. Intelligent portable devices are commonplace these days and information accessibility is ubiquitous. There is a network underlying any computer infrastructure. Complex Adaptive Systems (CAS) are a relatively new field with techniques inspired by Biology, Sociology and other fields. The field of CAS studies systems as a network of interdependent components. There has been a major breakthrough in the field of Network Intrusion Detection Systems (NIDS) in computer security through the adoption of a CAS perspective. This paper surveys some key work in this area with the primary focus being placed on biologically-inspired CAS approaches to NIDS.
               ",autonomous vehicle
10.1016/B978-0-12-810408-8.00032-8,journal,Deep Learning for Medical Image Analysis,sciencedirect,2017-12-31,sciencedirect,Index,https://api.elsevier.com/content/article/pii/B9780128104088000328,Unknown,autonomous vehicle
10.1016/j.sysarc.2015.07.007,journal,Journal of Systems Architecture,sciencedirect,2015-11-30,sciencedirect,Exploring ICMetrics to detect abnormal program behaviour on embedded devices,https://api.elsevier.com/content/article/pii/S1383762115000776,"
                  Execution of unknown or malicious software on an embedded system may trigger harmful system behaviour targeted at stealing sensitive data and/or causing damage to the system. It is thus considered a potential and significant threat to the security of embedded systems. Generally, the resource constrained nature of commercial off-the-shelf (COTS) embedded devices, such as embedded medical equipment, does not allow computationally expensive protection solutions to be deployed on these devices, rendering them vulnerable. A Self-Organising Map (SOM) based and Fuzzy C-means based approaches are proposed in this paper for detecting abnormal program behaviour to boost embedded system security. The presented technique extracts features derived from processor’s Program Counter (PC) and Cycles per Instruction (CPI), and then utilises the features to identify abnormal behaviour using the SOM. Results achieved in our experiment show that the proposed SOM based and Fuzzy C-means based methods can identify unknown program behaviours not included in the training set with 90.9% and 98.7% accuracy.
               ",autonomous vehicle
10.1016/B978-0-12-741245-0.50014-2,journal,Computational Vision,sciencedirect,1990-12-31,sciencedirect,8: Intelligent Systems,https://api.elsevier.com/content/article/pii/B9780127412450500142,"
               People only see what they are prepared to see.
            Ralph Waldo Emerson",autonomous vehicle
10.1016/j.abb.2015.07.018,journal,Archives of Biochemistry and Biophysics,sciencedirect,2016-01-01,sciencedirect,Metabolic biomarkers for chronic kidney disease,https://api.elsevier.com/content/article/pii/S0003986115300217,"
                  Chronic kidney disease (CKD) is an increasingly recognized burden for patients and health care systems with high (and growing) global incidence and prevalence, significant mortality, and disproportionately high treatment costs. Yet, the available diagnostic tools are either impractical in clinical routine or have serious shortcomings impeding a well-informed disease management although optimized treatment strategies with proven benefits for the patients have become available.
                  Advances in bioanalytical technologies have facilitated studies that identified genomic, proteomic, and metabolic biomarker candidates, and confirmed some of them in independent cohorts. This review summarizes the CKD-related markers discovered so far, and focuses on compounds and pathways, for which there is quantitative data, substantiating evidence from translational research, and a mechanistic understanding of the processes involved.
                  Also, multiparametric marker panels have been suggested that showed promising diagnostic and prognostic performance in initial analyses although the data basis from prospective trials is very limited. Large-scale studies, however, are underway and will provide the information for validating a set of parameters and discarding others.
                  Finally, the path from clinical research to a routine application is discussed, focusing on potential obstacles such as the use of mass spectrometry, and the feasibility of obtaining regulatory approval for targeted metabolomics assays.
               ",autonomous vehicle
10.1016/S0079-6123(08)60166-1,journal,Progress in Brain Research,sciencedirect,1965-12-31,sciencedirect,Pattern Recognition and Self-Organization using Wiener's Canonical Forms,https://api.elsevier.com/content/article/pii/S0079612308601661,"
                  Brick and Zames showed how Wiener's expansion procedure could be used to synthesize decision functions for stochastic inputs. In this chapter, it is accomplished by adopting Ashby's approach to intelligence. Thus creativity, planning, induction, and hypothesis generation are features, which will not specifically be a concern for the present model of the automaton. However, generalization of existing hypotheses arid selection, recognition and learning with respect to these are major factors in the present chapter. The model of the recognition system derived here is, in fact, a form of conditional probability computer, which operates on stochastic processes. Of major interest is the fact that this procedure appears to avoid the tendency toward exponential growth normally considered to be characteristic of conditional probability computers. The narrowing-down of the artificial intelligence problem to the recognition, learning, and prediction phases, however one proceeds to justify it, is not without precedence.
               ",autonomous vehicle
10.1016/B978-012088566-4/50011-8,journal,Cognitive Systems - Information Processing Meets Brain Science,sciencedirect,2006-12-31,sciencedirect,CHAPTER 6: Action,https://api.elsevier.com/content/article/pii/B9780120885664500118,"
               This chapter begins with the definition of action which is essentially any emitted or elicited behavior of an organism. Thus it includes both deliberate actions, chosen either explicitly or implicitly to achieve particular goals, and habits, reflexes or tropisms that are exercised more automatically in response to external stimuli and internal states. One might also consider internal actions involved in the control of ongoing processing, such as the allocation of sensory attention to a particular stimulus or region of sensory space. Furthermore, this chapter discusses the role played by modularity to understand the biological control of action and in building robots. At least three different sorts of modularity have been influential. The first form of modularity concerns hierarchical abstraction of state, goals and actions. A second sort of modularity concerns a separation between systems involved in apparently different functions, such as between those involved in defense and those involved in homeostatic regulation. A third sort of modularity divides control into specification, selection, execution and appraisal.
            ",autonomous vehicle
10.1016/0895-7177(94)00198-W,journal,Mathematical and Computer Modelling,sciencedirect,1995-01-31,sciencedirect,A comparison of adaptive critic and chemotaxis methods in adaptive control,https://api.elsevier.com/content/article/pii/089571779400198W,"Adaptive critic and chemotaxis algorithms are used to control a cart-pole system. Performance of these two methods are compared with earlier results obtained by using the functional link outerproduct method, as well as two other variations of the classical adaptive critic. This work is expected to shed light on biologically plausible adaptive control methods used to control and maintain the postural stability of the human musculo-skeletal system.",autonomous vehicle
10.1016/B978-0-12-464260-7.50017-1,journal,Neural and Brain Modeling,sciencedirect,1987-12-31,sciencedirect,9: Neural Networks,https://api.elsevier.com/content/article/pii/B9780124642607500171,,autonomous vehicle
10.1016/B978-0-12-820201-2.09991-8,journal,Artificial Intelligence in Cancer,sciencedirect,2020-12-31,sciencedirect,Index,https://api.elsevier.com/content/article/pii/B9780128202012099918,Unknown,autonomous vehicle
10.1016/B978-0-12-823696-3.00001-5,journal,Smartphone-Based Detection Devices,sciencedirect,2021-12-31,sciencedirect,13: Smartphone-based detection devices for the agri-food industry,https://api.elsevier.com/content/article/pii/B9780128236963000015,"
               Ensuring food safety at every level of an extremely complex agri-food processing network has always been a critical and challenging goal for the food scientists. In this queue, due to widespread accessibility, easy connectivity, and handy utility features, smartphone-based detection devices have embarked new era of rapid diagnostics in the field of medical sciences, environmental monitoring, and food safety analysis. The amalgamation of new generation sensors, like (paper-based sensors, fluorometric sensors, electrochemical sensors, nano-bio sensors, etc.) with a mobile phone offers the benefit of portable food analysis or ‘lab-on-smartphone’ platforms. Smartphone-based sensing approaches render momentous advantages like prompt testing, minimal cost, functional-readiness, records organization. Additionally, it can significantly trim down the requirement of high-end diagnostic tools and highly skilled human resources. Nowadays, mobile food scanners are widely implemented non-invasive analysis of food products for macro and micro components, detection of food pathogens, food contaminants, adulterants, food poisons, food allergens, color, temperature, moisture levels, etc. of various food products. These detection devices are generally based on mathematical models and hence, the major test of the technology lies in the robustness of these devices to contemplate the models in real-time cases. State of art suggests that smartphone-based detection devices have emerged as front-line tools in R & D and food industry in context to food safety. Therefore, this chapter presents an interesting and comprehensive outlook on the potential and challenges of smartphone-based detection devices for the agri-food processing chain.
            ",autonomous vehicle
10.1016/j.paerosci.2010.03.003,journal,Progress in Aerospace Sciences,sciencedirect,2010-10-31,sciencedirect,"A review of uncertainty in flight vehicle structural damage monitoring, diagnosis and control: Challenges and opportunities",https://api.elsevier.com/content/article/pii/S0376042110000254,"
                  This paper presents a comprehensive review of uncertainties involved in flight vehicle structural damage monitoring, diagnosis, prognosis and control. Uncertainties can cause infeasibilities, false diagnosis and very imprecise prognosis if not correctly taken into account. The purpose of this paper is to review existing methods that have been developed to address the problem of uncertainty in the area of damage sensing, diagnosis, prognosis and control in flight vehicles. The mathematical and statistical methods in analyzing uncertainty are first presented and compared. Then, the different sources and perspectives of uncertainties in the damage assessment process are presented and classified. Following this, diagnosis and prognosis methods are reviewed. Final review section covers the control of damaged structure under uncertainty. In each section and in the concluding remarks section the research challenges in the field of flight vehicle structural damage sensing, diagnosis and prognosis methods as well as control under uncertainty are identified and promising new ideas are discussed.
               ",autonomous vehicle
10.1016/B978-012646490-0/50031-7,journal,Soft Computing and Intelligent Systems,sciencedirect,2000-12-31,sciencedirect,INDEX,https://api.elsevier.com/content/article/pii/B9780126464900500317,Unknown,autonomous vehicle
10.1016/B978-0-12-817356-5.00002-4,journal,Internet of Things in Biomedical Engineering,sciencedirect,2019-12-31,sciencedirect,Chapter 2: Computer-Assisted Anthropology,https://api.elsevier.com/content/article/pii/B9780128173565000024,"
               Anthropology is the study of humans and human behavior (and also other primates). Various aspects of human life, including the objects made by humans (archeology), the biological aspects of humans (biological anthropology), how people in different parts of the world interact with each other and how their beliefs vary (sociocultural anthropology), and how humans communicate in different parts of the world (linguistic anthropology), are studied as a part of anthropology. This research is utilized in solving real-world problems in numerous fields, including healthcare, education, business, politics, the environment, interpreting history, etc.
               There has been a notable increase in the use of computers and related technologies for carrying out the required analysis in each of the four subfields of anthropology. Various data analytics and data visualization tools are used along with digital instruments. Many image-processing techniques along with computer graphics are employed in each of the four subfields. As currently used, the term digital anthropology overlaps with other terms such as virtual anthropology and cyber anthropology. Virtual anthropology involves the study of 3D digital models of anatomical structures of humans and nonhuman primates, as well as fossil specimens. Reconstruction of the fossils and the skeletons virtually is also extensively used. Virtual anthropology has a number of advantages, including the ability to study internal structures of the skeleton noninvasively and the availability of virtual objects due to the permanence of the data and the potential for data sharing. The use of digital imaging techniques, such as computed tomography (CT) or optical surface scanning, has contributed to numerous medical-related domains in archeology.
               As a part of sociocultural anthropology, people can be clustered based on their actions in social networks, online communities, and online games. This is a part of digital ethnography, which involves analyzing people on online social platforms, taking into consideration the conflicts of people within their communities and outside their communities and linguistic features and memetic dialects of people with respect to their cultures and geographic locations. Based on the huge amount of data which can be retrieved from online sites and taking into account the activities of different people in different situations online, conclusions can be drawn about the behavior of people.
               The chapter starts with an introduction to anthropology, followed by a description of each of the subfields. Then the computing technologies being employed in each of the fields of anthropology are described. Finally, digital ethnography and virtual anthropology are discussed.
            ",autonomous vehicle
10.1016/0303-2647(95)01589-2,journal,Biosystems,sciencedirect,1996-12-31,sciencedirect,The brain as a hermeneutic device,https://api.elsevier.com/content/article/pii/0303264795015892,"
                  An attempt has been made to reconcile the ‘device approach’ and the ‘philosophical approach’ to the brain. Systems exhibiting high structural and dynamic complexity may be candidates of being hermeneutic devices. The human brain, which is a structurally and dynamically complex device, not only perceives but also creates new reality: it is a hermeneutic device
               ",autonomous vehicle
10.1016/B978-0-12-816403-7.00018-0,journal,Trends in Personalized Nutrition,sciencedirect,2019-12-31,sciencedirect,Index,https://api.elsevier.com/content/article/pii/B9780128164037000180,Unknown,autonomous vehicle
10.1016/B978-0-12-805159-7.18001-5,journal,Learning and Memory: A Comprehensive Reference,sciencedirect,2017-12-31,sciencedirect,Index,https://api.elsevier.com/content/article/pii/B9780128051597180015,Unknown,autonomous vehicle
10.1016/B978-0-12-819724-0.09001-7,journal,Encyclopedia of Materials: Composites,sciencedirect,2021-12-31,sciencedirect,Subject Index,https://api.elsevier.com/content/article/pii/B9780128197240090017,Unknown,autonomous vehicle
10.1016/j.cirpj.2018.03.003,journal,CIRP Journal of Manufacturing Science and Technology,sciencedirect,2018-05-31,sciencedirect,Biologicalisation: Biological transformation in manufacturing,https://api.elsevier.com/content/article/pii/S1755581718300129,"
                  A new emerging frontier in the evolution of the digitalisation and the 4th industrial revolution (Industry 4.0) is considered to be that of “Biologicalisation in Manufacturing”. This has been defined by the authors to be “The use and integration of biological and bio-inspired principles, materials, functions, structures and resources for intelligent and sustainable manufacturing technologies and systems with the aim of achieving their full potential.” In this White Paper, detailed consideration is given to the meaning and implications of “Biologicalisation” from the perspective of the design, function and operation of products, manufacturing processes, manufacturing systems, supply chains and organisations. The drivers and influencing factors are also reviewed in detail and in the context of significant developments in materials science and engineering. The paper attempts to test the hypothesis of this topic as a breaking new frontier and to provide a vision for the development of manufacturing science and technology from the perspective of incorporating inspiration from biological systems. Seven recommendations are delivered aimed at policy makers, at funding agencies, at the manufacturing research community and at those industries involved in the development of next generation manufacturing technology and systems. It is concluded that it is valid to argue that Biologicalisation in Manufacturing truly represents a new and breaking frontier of digitalisation and Industry 4.0 and that the market potential is very strong. It is evident that extensive research and development is required in order to maximise on the benefits of a biological transformation.
               ",autonomous vehicle
10.1016/B978-008044612-7/50105-0,journal,Handbook of Categorization in Cognitive Science,sciencedirect,2005-12-31,sciencedirect,SUBJECT INDEX,https://api.elsevier.com/content/article/pii/B9780080446127501050,Unknown,autonomous vehicle
10.1016/j.eswa.2013.08.042,journal,Expert Systems with Applications,sciencedirect,2014-03-31,sciencedirect,Educational data mining: A survey and a data mining-based analysis of recent works,https://api.elsevier.com/content/article/pii/S0957417413006635,"
                  This review pursues a twofold goal, the first is to preserve and enhance the chronicles of recent educational data mining (EDM) advances development; the second is to organize, analyze, and discuss the content of the review based on the outcomes produced by a data mining (DM) approach. Thus, as result of the selection and analysis of 240 EDM works, an EDM work profile was compiled to describe 222 EDM approaches and 18 tools. A profile of the EDM works was organized as a raw data base, which was transformed into an ad-hoc data base suitable to be mined. As result of the execution of statistical and clustering processes, a set of educational functionalities was found, a realistic pattern of EDM approaches was discovered, and two patterns of value-instances to depict EDM approaches based on descriptive and predictive models were identified. One key finding is: most of the EDM approaches are ground on a basic set composed by three kinds of educational systems, disciplines, tasks, methods, and algorithms each. The review concludes with a snapshot of the surveyed EDM works, and provides an analysis of the EDM strengths, weakness, opportunities, and threats, whose factors represent, in a sense, future work to be fulfilled.
               ",autonomous vehicle
10.1016/j.cogsys.2010.07.006,journal,Cognitive Systems Research,sciencedirect,2011-06-30,sciencedirect,Rethinking cognitive architecture via graphical models,https://api.elsevier.com/content/article/pii/S1389041710000446,"
                  Cognitive architectures need to resolve the diversity dilemma – i.e., to blend diversity and uniformity – in order to couple functionality and efficiency with minimality, integrability, extensibility and maintainability. Building diverse architectures upon a uniform implementation level of graphical models is an intriguing approach because of the homogeneous manner in which such models produce state-of-the-art algorithms spanning symbol, probability and signal processing. To explore this approach a hybrid (discrete and continuous) mixed (Boolean and Bayesian) variant of the Soar architecture is being implemented via graphical models. Initial steps reported here, including a graphical implementation of production match and the beginnings of a mixed decision cycle incorporating a simple semantic memory, begin to show the potential of such an approach for cognitive architecture.
               ",autonomous vehicle
10.1016/B978-044452710-3/50007-0,journal,Quantitative Structure-Activity Relationships (QSAR) for Pesticide Regulatory Purposes,sciencedirect,2007-12-31,sciencedirect,Chapter 5: Hybrid systems,https://api.elsevier.com/content/article/pii/B9780444527103500070,,autonomous vehicle
10.1016/B978-0-12-804829-0.18001-0,journal,Encyclopedia of Biomedical Engineering,sciencedirect,2019-12-31,sciencedirect,Index,https://api.elsevier.com/content/article/pii/B9780128048290180010,Unknown,autonomous vehicle
10.1016/B978-012161964-0/50014-5,journal,Artificial Intelligence,sciencedirect,1996-12-31,sciencedirect,Index,https://api.elsevier.com/content/article/pii/B9780121619640500145,Unknown,autonomous vehicle
10.1016/B978-0-12-819113-2.00002-6,journal,Advanced Driver Intention Inference,sciencedirect,2020-12-31,sciencedirect,Chapter 2: State of the Art of Driver Lane Change Intention Inference,https://api.elsevier.com/content/article/pii/B9780128191132000026,"
               A lane change maneuver on the highway is an interactive task for human drivers. The driver has to process the multidisciplinary data based on the complex traffic context perception and vehicle control feedback. The intelligent vehicles and the advanced driver assistance systems (ADAS) need to have proper awareness of the traffic context as well as the driver to assist the driving tasks. Besides, it is needed for the ADAS to understand the driver potential intent correctly because it shares the control authority with the human driver. Inferring driver intention allows the ADAS to make proper assistance control to the driver. In this section, an overview of driver intention inference is proposed, and a particular focus is provided on the system design methodologies and classification. The lane change maneuver will be used as the main example for driver intention inference, as it is one of the most common and complex tasks during driving, which requires both longitudinal and lateral control actions. In this section, to have a general understanding of the driver's intention, a human intention mechanism is discussed in the beginning. Next, the driver intention is classified into different categories according to different criteria. The driver intention inference system is divided into different modules, which consists of traffic context awareness, driver state monitoring, and vehicle status measurement module. The relationship between these modules and the corresponding impacts on the driver intention inference is analyzed. The lane change intention inference system is reviewed from the input signals, algorithms, and evaluation aspects. Finally, the challenges and future works for driver intention inference are discussed.
            ",autonomous vehicle
10.1016/B978-0-12-819710-3.09996-8,journal,From Smart Grid to Internet of Energy,sciencedirect,2019-12-31,sciencedirect,Index,https://api.elsevier.com/content/article/pii/B9780128197103099968,Unknown,autonomous vehicle
10.1016/j.robot.2014.06.005,journal,Robotics and Autonomous Systems,sciencedirect,2014-12-31,sciencedirect,Ontology enhancing process for a situated and curiosity-driven robot,https://api.elsevier.com/content/article/pii/S0921889014001249,"
                  Nowadays, robots need to be able to interact with humans and objects in a flexible way and should be able to share the same knowledge (physical and social) of the human counterpart. Therefore, there is a need for a framework for expressing and sharing knowledge in a meaningful way by building the world model. In this paper, we propose a new framework for human–robot interaction using ontologies as powerful way of representing information which promote the sharing of meaningful knowledge between different objects. Furthermore, ontologies are powerful notions able to conceptualise the world in which the object such as Robot is situated. In this research, ontology is considered as improved solution to the grounding problem and enables interoperability between human and robot. The proposed system has been evaluated on a large number of test cases; results were very promising and support the implementation of the solution.
               ",autonomous vehicle
10.1016/S0169-023X(00)00049-5,journal,Data & Knowledge Engineering,sciencedirect,2001-03-31,sciencedirect,Information agent technology for the Internet: A survey,https://api.elsevier.com/content/article/pii/S0169023X00000495,"
                  The vast amount of heterogeneous information sources available on the Internet demands advanced solutions for acquiring, mediating, and maintaining relevant information for the common user. Intelligent information agents are autonomous computational software entities that are especially meant to (1) provide pro-active resource discovery, (2) resolve information impedance of information consumers and providers, and (3) offer value-added information services and products. These agents are supposed to cope with the difficulties associated with the information overload of the user, preferably just in time.
                  Based on a systematic classification of intelligent information agents, this paper presents an overview of the basic key enabling technologies needed to build such agents, and respective examples of information agent systems currently deployed on the Internet.
               ",autonomous vehicle
10.1016/S0166-4115(97)80115-X,journal,Advances in Psychology,sciencedirect,1997-12-31,sciencedirect,Index,https://api.elsevier.com/content/article/pii/S016641159780115X,,autonomous vehicle
10.1016/j.ress.2020.107007,journal,Reliability Engineering & System Safety,sciencedirect,2020-10-31,sciencedirect,Considering the human operator cognitive process for the interpretation of diagnostic outcomes related to component failures and cyber security attacks,https://api.elsevier.com/content/article/pii/S0951832020305081,"
                  In this work, we consider diagnostics of cyber attacks in Cyber-Physical Systems (CPSs), based on data analytics. For the first time to authors knowledge, the performance of such diagnosis is quantified considering the possible failure of the human operator cognitive process in interpreting and understanding the diagnosis support tool outcomes.
                  A Non-Parametric CUmulative SUM (NP-CUSUM) approach is used for data-driven diagnostic, and the cognitive process of the human operator who interprets its outputs is modeled by a Bayesian Belief Network (BBN). The overall framework is applied on the digital controller of the Advanced Lead-cooled Fast Reactor European Demonstrator (ALFRED).
               ",autonomous vehicle
10.1016/j.comcom.2016.07.012,journal,Computer Communications,sciencedirect,2016-11-15,sciencedirect,Cognitive radio for M2M and Internet of Things: A survey,https://api.elsevier.com/content/article/pii/S0140366416302699,"
                  Internet of things (IoT) paradigm poses new challenges to the communication technology as numerous heterogeneous objects will need to be connected. To address these issues new radio technologies and network architectures need to be designed to cater to several future devices having connectivity demands. For radio communications, the frequency spectrum allocation will have to be adapted for efficient spectrum utilization considering new bandwidth and application requirements. Novel research directions based on the use of opportunistic radio resource utilization such as those based on cognitive radio (CR) technology will have to be pursued for efficiency as well as reliability.
                  Cognitive Radio is a promising enabler communication technology for IoT. Its opportunistic communication paradigm is suited to communicating objects having event driven nature, that generate bursty traffic. Cognitive Radio can help overcome the problems of collision and excessive contention in the wireless access network that will arise due to the deployment of several objects connected to infrastructure through radio links. However, there are several issues that need to be addressed before cognitive radio technology can be used for Internet of things.
                  This paper surveys novel approaches and discusses research challenges related to the use of cognitive radio technology for Internet of things. In addition, the paper presents a general background on cognitive radio and Internet of Things with some potential applications. Our survey is different from existing surveys in that we focus on recent advances and ongoing research directions in cognitive radio in the context of Machine to Machine and Internet of Things. We review CR solutions that address generic problems of IoT including emerging challenges of autonomicity, scalability, energy efficiency, heterogeneity in terms of user equipment capabilities, complexity and environments, etc. The solutions are supported by our taxonomy of different CR approaches that are classified into two categories, flexible and efficient networking, and tackling heterogeneity. This paper intends to help new researchers entering the domain of CR and IoT by providing a comprehensive survey on recent advances.
               ",autonomous vehicle
10.1016/B978-012443880-4/50095-8,journal,Expert Systems,sciencedirect,2002-12-31,sciencedirect,Index,https://api.elsevier.com/content/article/pii/B9780124438804500958,Unknown,autonomous vehicle
10.1016/S0967-0661(96)90052-0,journal,Control Engineering Practice,sciencedirect,1996-06-30,sciencedirect,IFAC workshop on control education and technology transfer issues,https://api.elsevier.com/content/article/pii/S0967066196900520,,autonomous vehicle
10.1016/B978-0-12-811373-8.09991-2,journal,Security and Resilience in Intelligent Data-Centric Systems and Communication Networks,sciencedirect,2018-12-31,sciencedirect,Index,https://api.elsevier.com/content/article/pii/B9780128113738099912,Unknown,autonomous vehicle
10.1016/S0925-2312(96)90047-8,journal,Neurocomputing,sciencedirect,1996-07-15,sciencedirect,Announcements and calls for papers for conferences and workshops,https://api.elsevier.com/content/article/pii/S0925231296900478,,autonomous vehicle
10.1016/S1474-6670(17)49573-5,journal,IFAC Proceedings Volumes,sciencedirect,1992-09-30,sciencedirect,Neurofuzzy Controllers,https://api.elsevier.com/content/article/pii/S1474667017495735,"
                  Fuzzy modeling and control is a technique for handling qualitative information in a formal way. The greater simplicity of implementing fuzzy control systems may reduce design complexity and solve classes of previously intractable problems. Neural nets have come to mean architetures that have massively parallel interconnections of single, neuronlike processors. In control systems, they where first introduced to learn input-output mappings. This paper reviews the underlying ideas and applications of fuzzy and neural control systems. Neurofuzzy control and decision systems that are being developed are particularly emphasized. The key ideas behind these systems are outlined, and currently avaiable hardware and support tools described. Finally, it is suggested how neurofuzzy systems may be used to construct control systems with improved capatibilities.
               ",autonomous vehicle
10.1016/j.jcde.2014.12.006,journal,Journal of Computational Design and Engineering,sciencedirect,2015-04-30,sciencedirect,On condition based maintenance policy,https://api.elsevier.com/content/article/pii/S2288430014000141,"In the case of a high-valuable asset, the Operation and Maintenance (O&M) phase requires heavy charges and more efforts than the installation (construction) phase, because it has long usage life and any accident of an asset during this period causes catastrophic damage to an industry. Recently, with the advent of emerging Information Communication Technologies (ICTs), we can get the visibility of asset status information during its usage period. It gives us new challenging issues for improving the efficiency of asset operations. One issue is to implement the Condition-Based Maintenance (CBM) approach that makes a diagnosis of the asset status based on wire or wireless monitored data, predicts the assets abnormality, and executes suitable maintenance actions such as repair and replacement before serious problems happen. In this study, we have addressed several aspects of CBM approach: definition, related international standards, procedure, and techniques with the introduction of some relevant case studies that we have carried out.",autonomous vehicle
10.1016/S0933-3657(99)00047-0,journal,Artificial Intelligence in Medicine,sciencedirect,2000-05-01,sciencedirect,Evolutionary computation in medicine: an overview,https://api.elsevier.com/content/article/pii/S0933365799000470,"
                  The term evolutionary computation encompasses a host of methodologies inspired by natural evolution that are used to solve hard problems. This paper provides an overview of evolutionary computation as applied to problems in the medical domains. We begin by outlining the basic workings of six types of evolutionary algorithms: genetic algorithms, genetic programming, evolution strategies, evolutionary programming, classifier systems, and hybrid systems. We then describe how evolutionary algorithms are applied to solve medical problems, including diagnosis, prognosis, imaging, signal processing, planning, and scheduling. Finally, we provide an extensive bibliography, classified both according to the medical task addressed and according to the evolutionary technique used.
               ",autonomous vehicle
10.1016/S0925-2312(97)90010-2,journal,Neurocomputing,sciencedirect,1997-07-01,sciencedirect,Announcements and calls for papers for conferences and workshops,https://api.elsevier.com/content/article/pii/S0925231297900102,,autonomous vehicle
10.1016/B978-0-12-374535-4.00007-2,journal,Cognitive Radio Technology,sciencedirect,2009-12-31,sciencedirect,Chapter 7: Cognitive Techniques: Physical and Link Layers,https://api.elsevier.com/content/article/pii/B9780123745354000072,"
               This chapter focuses on intelligent cross-layer optimization of physical (PHY) and link layers. Physical and link layer parameters explains the fundamental sensing and control mechanism through which the reasoner understands what is going on in the spectrum and in the communication channel, and by which it performs changes to the control mechanisms to improve communication performance. We refer to these as knobs and meters. The knobs of a radio are any of the parameters that affect link performance and radio operation. Some of these are normally assumed to be design parameters, and others are usually assumed to be under real-time control of either the operator or the radio's realtime control processes. Performance is a measure of the system's operation based on the meter readings. In optimization theory, the meters represent utility or cost functions that must be maximized or minimized for optimum radio operation. The types of meters represent performance on different levels. On the physical layer, important performance measurements deal with bit fidelity.
            ",autonomous vehicle
10.1016/B978-075067446-1/50007-X,journal,Industrial Process Control,sciencedirect,2002-12-31,sciencedirect,6: Application Engineering of Control Systems,https://api.elsevier.com/content/article/pii/B978075067446150007X,"
               This chapter emphasizes on the configuration of control loops as proportional integral derivative (PID), cascade, override, feed forward, heat balancing, dead time compensation, logic, and batch control. Simulation, mathematical modeling, and optimization are useful tools that are used for applications such as training, analysis of control schemes, auto-tuning, advanced control, and progress in their application to control engineering. The inputs processed by control systems are limited by constraints such as complexity of control algorithms, inefficiency of control units to process a vast number of processes, inputs/outputs taking long time, and ever-increasing demand on control and instrumentation systems because of increasing complexity of chemical processes. An alternative approach with the following features is required: ability to interpret and use the vast quantities of process data, respond with high speed to process inputs, mapping capability/pattern recognition, adaptivity, learn and capture knowledge, and fault tolerant and robust. Three levels of behavior can hypothesize the human behavioral cognition and control: knowledge-based behavior, rule-based behavior, and skill-based behavior. Generic algorithms, which are biologically motivated, endeavor to find global optimum solutions. Qualitative reasoning is based on human methods of reasoning. Industrial process control includes data processing, parameter selection, training and validation, and to continually improve the performance of the neural network, the use of an expert system is recommended. Neural network methods rely on simple algebra and operator experience, and are computationally efficient.
            ",autonomous vehicle
10.1016/j.eswa.2018.01.025,journal,Expert Systems with Applications,sciencedirect,2018-06-01,sciencedirect,Amended fused TOPSIS-VIKOR for classification (ATOVIC) applied to some UCI data sets,https://api.elsevier.com/content/article/pii/S0957417418300319,"
                  Classification procedure is an important task of expert and intelligent systems. Developing new algorithms of classification which improve accuracy or true positive rates could have an influence on some life problems such as diagnosis prediction in medical domain. Multi-criteria decision making (MCDM) methods are expected to search the best alternative according to some criteria. Each criterion has a value relative to each alternative. There are only two sets: a set of criteria and a set of alternatives. This work merges MCDM methods TOPSIS and VIKOR and modifies them to be used for classification where the used sets are three: the classes, the objects and the attributes (features) describing the objects. Hence, ATOVIC, a new classification algorithm is proposed. In ATOVIC, criteria are replaced by features and alternatives are replaced by objects. The latter belong to corresponding classes. Two sets are employed one serves as reference and second serves as test. An object from test set will be classified to the relative class based on the reference set. ATOVIC is applied on a benchmark (UCI) CLEVELAND data set to predict heart disease. Following the complexity of the data set and its importance, ATOVIC application is done on different test sets of CLEVELAND using binary classification and multi-classification. Moreover, ATOVIC is applied to thyroid data set to detect hyperthyroidism and hypothyroidism diseases. The obtained results show the efficiency of ATOVIC in medical domain. In addition, ATOVIC is applied to three other data sets: chess, nursery and titanic, from UCI and KEEL websites. The obtained results are compared to those of some classifiers from literature. The experimental results demonstrate that ATOVIC method improves accuracy and true positive rates comparing to most classifiers considered from literature. Hence ATOVIC is promising for use in prediction or classification.
               ",autonomous vehicle
10.1016/j.dss.2014.01.004,journal,Decision Support Systems,sciencedirect,2014-03-31,sciencedirect,An intelligent situation awareness support system for safety-critical environments,https://api.elsevier.com/content/article/pii/S0167923614000050,"
                  Operators handling abnormal situations in safety-critical environments need to be supported from a cognitive perspective to reduce their workload, stress, and consequent error rate. Of the various cognitive activities, a correct understanding of the situation, i.e. situation awareness (SA), is a crucial factor in improving performance and reducing error. However, existing system safety researches focus mainly on technical issues and often neglect SA. This study presents an innovative cognition-driven decision support system called the situation awareness support system (SASS) to manage abnormal situations in safety-critical environments in which the effect of situational complexity on human decision-makers is a concern. To achieve this objective, a situational network modeling process and a situation assessment model that exploits the specific capabilities of dynamic Bayesian networks and risk indicators are first proposed. The SASS is then developed and consists of four major elements: 1) a situation data collection component that provides the current state of the observable variables based on online conditions and monitoring systems, 2) a situation assessment component based on dynamic Bayesian networks (DBN) to model the hazardous situations in a situational network and a fuzzy risk estimation method to generate the assessment result, 3) a situation recovery component that provides a basis for decision-making to reduce the risk level of situations to an acceptable level, and 4) a human-computer interface. The SASS is partially evaluated by a sensitivity analysis, which is carried out to validate DBN-based situational networks, and SA measurements are suggested for a full evaluation of the proposed system. The performance of the SASS is demonstrated by a case taken from US Chemical Safety Board reports, and the results demonstrate that the SASS provides a useful graphical, mathematically consistent system for dealing with incomplete and uncertain information to help operators maintain the risk of dynamic situations at an acceptable level.
               ",autonomous vehicle
10.1016/0004-3702(93)90050-L,journal,Artificial Intelligence,sciencedirect,1993-07-31,sciencedirect,"Connectionist models: Proceedings of the 1990 summer school: David Touretzky, Jeffrey Elman, Terrence Sejnowski and Geoffrey Hinton, eds.",https://api.elsevier.com/content/article/pii/000437029390050L,,autonomous vehicle
10.1016/j.aei.2015.01.008,journal,Advanced Engineering Informatics,sciencedirect,2015-04-30,sciencedirect,A review on computer vision based defect detection and condition assessment of concrete and asphalt civil infrastructure,https://api.elsevier.com/content/article/pii/S1474034615000208,"
                  To ensure the safety and the serviceability of civil infrastructure it is essential to visually inspect and assess its physical and functional condition. This review paper presents the current state of practice of assessing the visual condition of vertical and horizontal civil infrastructure; in particular of reinforced concrete bridges, precast concrete tunnels, underground concrete pipes, and asphalt pavements. Since the rate of creation and deployment of computer vision methods for civil engineering applications has been exponentially increasing, the main part of the paper presents a comprehensive synthesis of the state of the art in computer vision based defect detection and condition assessment related to concrete and asphalt civil infrastructure. Finally, the current achievements and limitations of existing methods as well as open research challenges are outlined to assist both the civil engineering and the computer science research community in setting an agenda for future research.
               ",autonomous vehicle
10.1016/j.camwa.2005.02.001,journal,Computers & Mathematics with Applications,sciencedirect,2005-05-31,sciencedirect,Book reports,https://api.elsevier.com/content/article/pii/S0898122105001045,,autonomous vehicle
10.1016/B978-0-12-083030-5.50014-X,journal,Neural Networks in Bioprocessing and Chemical Engineering,sciencedirect,1995-12-31,sciencedirect,Glossary,https://api.elsevier.com/content/article/pii/B978012083030550014X,Unknown,autonomous vehicle
10.1016/S0933-3657(00)00072-5,journal,Artificial Intelligence in Medicine,sciencedirect,2001-03-31,sciencedirect,A survey of fuzzy logic monitoring and control utilisation in medicine,https://api.elsevier.com/content/article/pii/S0933365700000725,"
                  Intelligent systems have appeared in many technical areas, such as consumer electronics, robotics and industrial control systems. Many of these intelligent systems are based on fuzzy control strategies which describe complex systems mathematical models in terms of linguistic rules. Since the 1980s new techniques have appeared from which fuzzy logic has been applied extensively in medical systems. The justification for such intelligent systems driven solutions is that biological systems are so complex that the development of computerised systems within such environments is not always a straightforward exercise. In practice, a precise model may not exist for biological systems or it may be too difficult to model. In most cases fuzzy logic is considered to be an ideal tool as human minds work from approximate data, extract meaningful information and produce crisp solutions. This paper surveys the utilisation of fuzzy logic control and monitoring in medical sciences with an analysis of its possible future penetration.
               ",autonomous vehicle
10.1016/B978-075067952-7/50019-2,journal,Cognitive Radio Technology,sciencedirect,2006-12-31,sciencedirect,Index,https://api.elsevier.com/content/article/pii/B9780750679527500192,Unknown,autonomous vehicle
10.1016/B978-012370509-9.09015-X,journal,Learning and Memory: A Comprehensive Reference,sciencedirect,2008-12-31,sciencedirect,Subject Index,https://api.elsevier.com/content/article/pii/B978012370509909015X,Unknown,autonomous vehicle
10.1016/j.compag.2015.09.011,journal,Computers and Electronics in Agriculture,sciencedirect,2015-10-31,sciencedirect,Discrete wavelets transform for improving greenness image segmentation in agricultural images,https://api.elsevier.com/content/article/pii/S0168169915002860,"
                  We propose a segmentation strategy for agricultural images in order to successfully distinguish between both soil and green parts, the last ones including weeds and crop plants, based on discrete wavelets transform. Vegetation indices have been commonly used for greenness image segmentation, but improvements are still possible. In agricultural images weeds and crops plants display high spatial variability with irregular and random distributions. Textures descriptors have the ability to capture this information, which conveniently combined with vegetation indices improve the greenness segmentation results. The proposed approach consists of the following steps: (a) greenness extraction based on vegetation indices; (b) application of the wavelets transform to the resulting image, allowing the extraction of spatial structures in three bands (horizontal, vertical and diagonal) containing detailed information; (c) use of texture descriptors to capture the spatial variability in the three bands; (d) combination of greenness and texture information, in the approximation coefficients of the wavelets transform, for enhancing plants (weeds and crops) identification; and (e) application of an image thresholding method for final image identification. The wavelets transform allows both capture of spatial texture and its fusion with the greenness information, making the main contribution of this paper. This approach is especially useful when the quality of imaging greenness is low. It has been favorably compared against existing strategies, obtaining better results, quantified by 4,5%.
               ",autonomous vehicle
10.1016/S0065-2458(08)60276-4,journal,Advances in Computers,sciencedirect,1993-12-31,sciencedirect,Subject Index,https://api.elsevier.com/content/article/pii/S0065245808602764,,autonomous vehicle
10.1016/B978-0-12-083030-5.50016-3,journal,Neural Networks in Bioprocessing and Chemical Engineering,sciencedirect,1995-12-31,sciencedirect,Index,https://api.elsevier.com/content/article/pii/B9780120830305500163,Unknown,autonomous vehicle
10.1533/9781855736375.2.103,journal,Food Process Modelling,sciencedirect,2001-12-31,sciencedirect,6: The power and pitfalls of inductive modelling,https://api.elsevier.com/content/article/pii/B9781855735651500134,,autonomous vehicle
10.1016/B978-0-12-818961-0.00036-3,journal,New Materials in Civil Engineering,sciencedirect,2020-12-31,sciencedirect,Index,https://api.elsevier.com/content/article/pii/B9780128189610000363,Unknown,autonomous vehicle
10.1016/S1572-4409(02)80013-X,journal,Process Metallurgy,sciencedirect,2002-12-31,sciencedirect,References,https://api.elsevier.com/content/article/pii/S157244090280013X,,autonomous vehicle
10.1016/S0166-4115(08)61767-7,journal,Advances in Psychology,sciencedirect,1987-12-31,sciencedirect,Preface,https://api.elsevier.com/content/article/pii/S0166411508617677,,autonomous vehicle
10.1016/B978-0-12-816502-7.00027-0,journal,Optical Fiber Telecommunications VII,sciencedirect,2020-12-31,sciencedirect,Index,https://api.elsevier.com/content/article/pii/B9780128165027000270,Unknown,autonomous vehicle
10.1016/S1474-6670(17)43719-0,journal,IFAC Proceedings Volumes,sciencedirect,1996-12-31,sciencedirect,AI in the Feedback Loop: A Survey of Alternative Approaches,https://api.elsevier.com/content/article/pii/S1474667017437190,"
                  An overview of different ways of using AI techniques in feedback control is presented. The paper gives special attention to fuzzy control and expert control.
               ",autonomous vehicle
10.1016/S1367-5788(97)00006-0,journal,Annual Reviews in Control,sciencedirect,1996-12-31,sciencedirect,AI in the feedback loop: a survey of alternative approaches,https://api.elsevier.com/content/article/pii/S1367578897000060,"
                  An overview of different ways of using AI techniques in feedback control is presented. The paper gives special attention to fuzzy control and expert control.
               ",autonomous vehicle
10.1016/j.autcon.2003.12.002,journal,Automation in Construction,sciencedirect,2004-05-31,sciencedirect,Multi-agent systems in construction–state of the art and prospects,https://api.elsevier.com/content/article/pii/S0926580503001262,"
                  This paper provides an overview of the research and development of multi-agent systems (MAS) in construction. It identifies the key issues in the development and deployment of agent-based systems, and indicates how these issues should be addressed in the construction domain. To do so, this paper first reviews the notions of MAS; discusses agent collaboration mechanisms, and highlights the advantages that MAS could provide to collaborative engineering activities. It then discusses the MAS models developed for different construction problems where agents interact in various ways to solve construction problems. Finally, a number of important issues in the application of agent-based approaches in construction (e.g., potential application areas, development method, and difficulties) are outlined and further analysed.
               ",autonomous vehicle
10.1016/j.jterra.2013.03.004,journal,Journal of Terramechanics,sciencedirect,2013-06-30,sciencedirect,A technical review on navigation systems of agricultural autonomous off-road vehicles,https://api.elsevier.com/content/article/pii/S0022489813000220,"
                  With the predicted increase in world population to over 10 billion, by the year 2050, growth in agricultural output needs to be continued. Considering this, autonomous vehicles application in precision agriculture is one of the main issues to be regarded noteworthy to improve the efficiency. In this research many papers on autonomous farm vehicles are reviewed from navigation systems viewpoint. All navigation systems are categorized in six classes: dead reckoning, image processing, statistical based developed algorithms, fuzzy logic control, neural network and genetic algorithm, and Kalman filter based. Researches in many agricultural operations from water monitoring to aerial crop scouting revealed that the centimeter level accuracy in all techniques is available and the velocity range for evaluated autonomous vehicles almost is smaller than 1m/s. Finally it would be concluded although many developments in agricultural automation using different techniques and algorithms are obtained especially in recent years, more works are required to acquire farmer’s consensus about autonomous vehicles. Additionally some issues such as safety, economy, implement standardization and technical service support in the entire world are merit to consideration.
               ",autonomous vehicle
10.1016/j.inffus.2020.05.009,journal,Information Fusion,sciencedirect,2020-11-30,sciencedirect,"The four dimensions of social network analysis: An overview of research methods, applications, and software tools",https://api.elsevier.com/content/article/pii/S1566253520302906,"
                  Social network based applications have experienced exponential growth in recent years. One of the reasons for this rise is that this application domain offers a particularly fertile place to test and develop the most advanced computational techniques to extract valuable information from the Web. The main contribution of this work is three-fold: (1) we provide an up-to-date literature review of the state of the art on social network analysis (SNA); (2) we propose a set of new metrics based on four essential features (or dimensions) in SNA; (3) finally, we provide a quantitative analysis of a set of popular SNA tools and frameworks. We have also performed a scientometric study to detect the most active research areas and application domains in this area. This work proposes the definition of four different dimensions, namely Pattern & Knowledge discovery, Information Fusion & Integration, Scalability, and Visualization, which are used to define a set of new metrics (termed degrees) in order to evaluate the different software tools and frameworks of SNA (a set of 20 SNA-software tools are analyzed and ranked following previous metrics). These dimensions, together with the defined degrees, allow evaluating and measure the maturity of social network technologies, looking for both a quantitative assessment of them, as to shed light to the challenges and future trends in this active area.
               ",autonomous vehicle
10.1016/S0921-8890(98)00077-3,journal,Robotics and Autonomous Systems,sciencedirect,1999-01-31,sciencedirect,Subject index to volumes 11–25,https://api.elsevier.com/content/article/pii/S0921889098000773,,autonomous vehicle
10.1016/j.knosys.2016.02.012,journal,Knowledge-Based Systems,sciencedirect,2016-08-01,sciencedirect,A multi-disciplinary review of knowledge acquisition methods: From human to autonomous eliciting agents,https://api.elsevier.com/content/article/pii/S0950705116000988,"
                  This paper offers a multi-disciplinary review of knowledge acquisition methods in human activity systems. The review captures the degree of involvement of various types of agencies in the knowledge acquisition process, and proposes a classification with three categories of methods: the human agent, the human-inspired agent, and the autonomous machine agent methods. In the first two categories, the acquisition of knowledge is seen as a cognitive task analysis exercise, while in the third category knowledge acquisition is treated as an autonomous knowledge-discovery endeavour. The motivation for this classification stems from the continuous change over time of the structure, meaning and purpose of human activity systems, which are seen as the factor that fuelled researchers’ and practitioners’ efforts in knowledge acquisition for more than a century.
                  We show through this review that the KA field is increasingly active due to the higher and higher pace of change in human activity, and conclude by discussing the emergence of a fourth category of knowledge acquisition methods, which are based on red-teaming and co-evolution.
               ",autonomous vehicle
10.1016/B978-0-08-051055-2.50036-5,journal,Machine Learning,sciencedirect,1990-12-31,sciencedirect,SUBJECT INDEX,https://api.elsevier.com/content/article/pii/B9780080510552500365,Unknown,autonomous vehicle
10.1016/B978-0-12-398296-4.00019-2,journal,"Metaheuristics in Water, Geotechnical and Transport Engineering",sciencedirect,2013-12-31,sciencedirect,19: The Hybrid Method and its Application to Smart Pavement Management,https://api.elsevier.com/content/article/pii/B9780123982964000192,"Quantification of the pavement deterioration trend plays a crucial role in determining optimum pavement maintenance strategies. Nowadays, pavement distress classification and quantification becomes more important, as the flourishing artificial intelligence (AI) methods and computational power increases as well. Recently, AI methods such as neural networks (NNs), fuzzy logic (FL), and expert system (ES) provide very good analytical tools for automatic detecting and classification of pavement distresses. The main goal of hybrid method (HM) is combining the advantages of different AI methods within a single system. HM and its application toward smart management in a pavement management system is a collection of research in the fields of pavement management, pavement quantification, and distress detection and classification. It covers topics such as the combination of NNs with ESs, NNs with FL systems, and HM for the image processing of distress. This chapter deals with applications of HM in automatic pavement distress detection and classification. Furthermore, it includes recent work on multiresolution methods for detection and isolation of pavement distress such as wavelet, ridgelet, and curvelet.KeywordsHybrid method, pavement distress, neural network, fuzzy logic, expert system",autonomous vehicle
10.1016/B978-0-12-811810-8.00003-8,journal,Wearable Technology in Medicine and Health Care,sciencedirect,2018-12-31,sciencedirect,"Chapter 3: Wearable Robotics for Upper-Limb Rehabilitation and Assistance: A Review of the State-of-the-Art, Challenges, and Future Research",https://api.elsevier.com/content/article/pii/B9780128118108000038,"
               A significant fraction of the world population is plagued by neuromuscular disorders which have no cure other than symptomatic management. An increasingly aging world population would inevitably lead to a further rise in these numbers. The management of the manifestations of many neuromuscular diseases ranging from Parkinson’s disease to stroke has been an active research topic in robotics since the 1960s. Pivotal advances in sensing, actuation, energy sources, and computing technologies have facilitated an increased penetration of wearable robotics into patient rehabilitation and assistance. Furthermore, breakthroughs in research areas including material sciences and new actuation schemes are further accelerating the development of wearable robotics. The purpose of this chapter is to provide a review of robotic devices for upper-limb rehabilitation and assistance. A discussion of the state-of-the-art in design, actuation, and intention-sensing technologies is presented. The chapter also outlines challenges hindering the clinical translation of these technologies and highlights future research opportunities.
            ",autonomous vehicle
10.1016/B978-0-12-632425-9.50013-8,journal,Handbook of VLSI Chip Design and Expert Systems,sciencedirect,1993-12-31,sciencedirect,Chapter 9: MODERN DESIGN METHODOLOGIES,https://api.elsevier.com/content/article/pii/B9780126324259500138,"
               This chapter discusses modern chip design methodologies. The graphics-display facility is one of the most attractive features of modern design systems. It renders immediate visibility of interim results of a design process in a way that allows the designer to interact with the system in a user-friendly and effective way. At the heart of an interactive graphics system is the visual display unit, which mostly takes the form of the familiar cathode-ray tube with a screen to display all the graphical information needed for monitoring the design. This visual information may contain a clarifying text, circuit schematics, timing characteristics, a mask artwork, or any other graphic information, which may be useful as the design evolves. Several graphics input devices are also available at present to allow the user to specify input information to the design system. One classic device is the alphanumeric keyboard that permits the user to enter the specification of a design description as well as a set of commands.
            ",autonomous vehicle
10.1016/j.rser.2021.111642,journal,Renewable and Sustainable Energy Reviews,sciencedirect,2021-12-31,sciencedirect,On the resilience of modern power systems: A comprehensive review from the cyber-physical perspective,https://api.elsevier.com/content/article/pii/S1364032121009175,"
                  The digital transformation of power systems into cyber-physical systems (CPSs) is the inevitable trend of modern power systems with the integration of large-scale renewable energy. The in-depth interdependence of cyber and physical spaces leads to more complicated external environments for such cyber-physical power systems (CPPSs) and brings great challenges to the resilience of CPPSs. A resilient CPS imposes strict requirements for its ability to cope with high-impact, low-probability cyber-physical disturbances. To better study the vulnerability and resilience of CPPSs, several representative blackouts from the past two decades are reviewed from the cyber-physical perspective. Inspired by general system theory, this study offers a framework with three key features of a CPPS and presents the three-layer interdependences from facilities to functions. The differences between CPPS resilience and conventional power system resilience are also emphasized. Thereafter, the study discusses the influence of cyber-physical disturbances from natural hazards, cyberattacks, and human-in-the-loop on the resilience of CPPSs. Accordingly, a survey of the state-of-the-art resilience-oriented techniques for CPPS in the face of natural hazards is organized based on quantitative metrics as well as planning and operation attributes. Regarding the resilience against cyberattacks, relevant cutting-edge research is reviewed in terms of prevention, detection, and mitigation strategies. Furthermore, from the cyber-physical-social perspective, the exploitation of social behaviors to inform the design of the physical system and the cyber system to ultimately enhance the resilience of CPPSs is also studied. Based on the findings from this research, the remaining challenges and the broad prospects of cyber-physical resilience enhancement techniques are also discussed.
               ",autonomous vehicle
10.1016/B978-0-08-042016-5.50033-4,journal,Application of Artificial Intelligence in Process Control,sciencedirect,1992-12-31,sciencedirect,Subject Index,https://api.elsevier.com/content/article/pii/B9780080420165500334,Unknown,autonomous vehicle
10.1016/B978-0-12-812594-6.00004-4,journal,Applied Biomechatronics using Mathematical Models,sciencedirect,2018-12-31,sciencedirect,"Chapter 4: Experiment design, data acquisition and signal processing",https://api.elsevier.com/content/article/pii/B9780128125946000044,"
               This chapter studies the gait variability (GV) from kinetics measurement of 3D-GRF of both limbs: vertical ground reaction force, medial-lateral force, and anterior-posterior force. We used typical signal processing algorithm to calculate GV, obtaining a special matrix gait cycle (MGC), where MGC is a matrix that contains all the gait cycles separated by stride times, to facilitate GV’s 3D-GRF charts, and other calculations were used to analyze the behaviors of signals obtained from the human body. Signal processing of joint angles was performed for kinematic measurement to calculate angular kinematic as absolutes and relative angles. Examples are given to obtain joint angles from x-y coordinates, and calculation for range of motion. Signal processing was performed by combinations of synchronized signals from different Bioinstruments. The general approaches for experimental design for measurement of the human body are kinematic, kinetic, and muscles potentials activities from surface electromyogram. Examples of research papers using the general approach for experiment design in Biomechanics are provided.
            ",autonomous vehicle
10.1533/9780857090195.backmatter,journal,Colour Measurement,sciencedirect,2010-12-31,sciencedirect,Index,https://api.elsevier.com/content/article/pii/B9781845695590500194,Unknown,autonomous vehicle
10.1016/j.compeleceng.2007.05.010,journal,Computers & Electrical Engineering,sciencedirect,2007-11-30,sciencedirect,Improving network security using genetic algorithm approach,https://api.elsevier.com/content/article/pii/S0045790607000584,"
                  With the expansion of Internet and its importance, the types and number of the attacks have also grown making intrusion detection an increasingly important technique. In this work we have realized a misuse detection system based on genetic algorithm (GA) approach. For evolving and testing new rules for intrusion detection the KDD99Cup training and testing dataset were used. To be able to process network data in real time, we have deployed principal component analysis (PCA) to extract the most important features of the data. In that way we were able to keep the high level of detection rates of attacks while speeding up the processing of the data.
               ",autonomous vehicle
10.1016/B978-0-12-816389-4.00009-8,journal,Coronary Calcium,sciencedirect,2019-12-31,sciencedirect,Chapter 9: Imaging vascular calcification: Where are we headed,https://api.elsevier.com/content/article/pii/B9780128163894000098,"
               Arterial calcium deposition occurs in response to a spectrum of inflammatory and degenerative processes. Hydroxyapatite crystals are the building blocks of vascular calcification and are laid down as an early feature. This microcalcification exhibits a distinct biological profile from established calcified plaque, which is often the marker of quiescent disease. Detecting microcalcification offers great promise because it is associated with intense biological activity within the vascular bed and increased plaque vulnerability.
               Established imaging techniques, such as computed tomography, cannot identify microcalcification because it occurs at an extremely small scale. 18F-sodium fluoride is a biological radiotracer that binds to exposed hydroxyapatite crystals and can be detected using positron emission tomography (PET). 18F-sodium fluoride PET identifies microcalcification associated with high-risk atherosclerotic plaques that would otherwise require invasive imaging to be detected.
               Anatomical imaging is already an essential tool to stratify the risk of future cardiovascular events caused by calcifying disease. Combining these imaging techniques with the molecular insights of 18F-sodium fluoride PET offers a comprehensive assessment of vascular calcification. Together, this combination of structural and molecular assessment represents the next major paradigm in cardiovascular imaging and risk stratification.
            ",autonomous vehicle
10.1016/0165-0114(95)90081-0,journal,Fuzzy Sets and Systems,sciencedirect,1995-06-09,sciencedirect,Recent Literature,https://api.elsevier.com/content/article/pii/0165011495900810,,autonomous vehicle
10.1016/j.asoc.2014.06.021,journal,Applied Soft Computing,sciencedirect,2014-10-31,sciencedirect,Improving a simulated soccer team's performance through a Memory-Based Collaborative Filtering approach,https://api.elsevier.com/content/article/pii/S1568494614002956,"
                  Collaborative filtering techniques have been used for some years, almost exclusively in Internet environments, helping users find items they are expected to like by using the user's past purchases to provide such recommendations. With this concept in mind, this research uses a collaborative filtering technique to automatically improve the performance of a simulated soccer team. Many studies have attempted to address this problem over the last years but none has shown meaningful improvements in the performance of the soccer team. Using a collaborative filtering technique based on nearest neighbors and the FC Portugal team as the test subject (in the context of the RoboCup 2D Simulation League), several simulations were run for matches against different teams with much better, better and worse performance than FC Portugal. The strategy used by FC Portugal was to combine 8 set-plays and 2 team formations. The simulation results revealed an improvement in performance between 32% and 384%. In the future, there are plans to expand this approach to other contexts, such as the 3D Simulation League.
               ",autonomous vehicle
10.1016/0020-0255(93)90028-K,journal,Information Sciences,sciencedirect,1993-08-01,sciencedirect,Uncertainty management in space station autonomous research: Pattern recognition perspective,https://api.elsevier.com/content/article/pii/002002559390028K,"
                  Various space station autonomous operations where research is being conducted to support unmanned missions are discussed. The problem of representation and management of uncertainties involved in image analysis and recognition tasks of those operations is particularly addressed. Various tools based on fuzzy set theory and probability theory that reflect different kinds of ambiguity, uncertainty, and information in an image are listed. Their usefulness in providing soft decision and quantitative indices for autonomous operations is described along with the uncertainty in membership function evaluation. The merits of incorporating fuzzy set theory in neural networks and in genetic algorithms for efficient handling of uncertainties are also addressed.
               ",autonomous vehicle
10.1016/S0168-0102(00)82943-0,journal,Neuroscience Research,sciencedirect,2000-12-31,sciencedirect,"Abstract of the joint meetings of the 23rd annual meeting of the Japan neuroscience society and the 10th annual meeting of the Japanese neural network society Yokohama, Japan, september 4–6, 2000 Plenary lecture",https://api.elsevier.com/content/article/pii/S0168010200829430,,autonomous vehicle
10.1016/S1364-6613(99)80001-1,journal,Trends in Cognitive Sciences,sciencedirect,1998-12-31,sciencedirect,Subject index,https://api.elsevier.com/content/article/pii/S1364661399800011,,autonomous vehicle
10.1016/B978-0-12-820028-5.00010-2,journal,Smart Manufacturing,sciencedirect,2020-12-31,sciencedirect,Chapter 10: Smart manufacturing in industrial gas production: A digital transformation,https://api.elsevier.com/content/article/pii/B9780128200285000102,"
               This chapter shows, from the experience of Air Liquide in its digital transformation through the development and implementation of its Smart Innovative Operations (SIO) program, that enterprise-wide industrial deployment of smart manufacturing solutions requires strong methodology, organization, and communication to provide the expected benefits in operational excellence (e.g., efficiency and reliability). This is true whether the smart manufacturing solution is fully internally developed, completely commercial-off-the-shelf (COTS), or a hybrid, as is the case for both SIO.Predict (anomaly detection) and SIO.Optim (mathematical programming optimization) as discussed in this chapter. Both SIO.Predict and SIO.Optim implement COTS software as a platform to both enable and be enabled by a strong internal organization within Air Liquide, including a variety of backgrounds, roles, and expertise. Such software platforms enable further innovation, including the implementation of the latest mathematical algorithms developed and demonstrated by industrial and academic researchers.
            ",autonomous vehicle
10.1016/S0898-1221(02)00272-9,journal,Computers & Mathematics with Applications,sciencedirect,2002-12-31,sciencedirect,Book reports,https://api.elsevier.com/content/article/pii/S0898122102002729,,autonomous vehicle
10.1016/B0-12-227240-4/00202-1,journal,Encyclopedia of Information Systems,sciencedirect,2004-12-31,sciencedirect,Subject Index,https://api.elsevier.com/content/article/pii/B0122272404002021,Unknown,autonomous vehicle
10.1016/j.neucom.2012.08.042,journal,Neurocomputing,sciencedirect,2013-08-19,sciencedirect,Do biological synapses perform probabilistic computations?,https://api.elsevier.com/content/article/pii/S0925231212007886,"
                  In this paper, the presynaptic rule, a classical model of synaptic reinforcement, is revisited. It is shown that this model is capable of reproducing recently discovered properties of biological synapses such as synaptic directionality, and metaplasticity of the long-term potentiation threshold. With slight modifications, the presynaptic model also reproduces metaplasticity of the long-term depression threshold and Artola, Bröcher and Singer’s experimental model. Two asymptotically equivalent approaches were adopted for this analysis, one with firing rates and another with conditional probabilities. Although both approximations are consistent with biological properties, the results obtained by the probabilistic approach are qualitatively closer to biological experimental results.
               ",autonomous vehicle
10.1016/j.camwa.2004.06.021,journal,Computers & Mathematics with Applications,sciencedirect,2004-06-30,sciencedirect,Book report,https://api.elsevier.com/content/article/pii/S0898122104839919,,autonomous vehicle
10.1016/B978-0-12-815585-1.00054-1,journal,Biotechnology Entrepreneurship,sciencedirect,2020-12-31,sciencedirect,Index,https://api.elsevier.com/content/article/pii/B9780128155851000541,Unknown,autonomous vehicle
10.1016/j.cis.2020.102252,journal,Advances in Colloid and Interface Science,sciencedirect,2020-10-31,sciencedirect,Pattern detection in colloidal assembly: A mosaic of analysis techniques,https://api.elsevier.com/content/article/pii/S0001868620302852,"Characterization of the morphology, identification of patterns and quantification of order encountered in colloidal assemblies is essential for several reasons. First of all, it is useful to compare different self-assembly methods and assess the influence of different process parameters on the final colloidal pattern. In addition, casting light on the structures formed by colloidal particles can help to get better insight into colloidal interactions and understand phase transitions. Finally, the growing interest in colloidal assemblies in materials science for practical applications going from optoelectronics to biosensing imposes a thorough characterization of the morphology of colloidal assemblies because of the intimate relationship between morphology and physical properties (e.g. optical and mechanical) of a material. Several image analysis techniques developed to investigate images (acquired via scanning electron microscopy, digital video microscopy and other imaging methods) provide variegated and complementary information on the colloidal structures under scrutiny. However, understanding how to use such image analysis tools to get information on the characteristics of the colloidal assemblies may represent a non-trivial task, because it requires the combination of approaches drawn from diverse disciplines such as image processing, computational geometry and computational topology and their application to a primarily physico-chemical process. Moreover, the lack of a systematic description of such analysis tools makes it difficult to select the ones more suitable for the features of the colloidal assembly under examination. In this review we provide a methodical and extensive description of real-space image analysis tools by explaining their principles and their application to the investigation of two-dimensional colloidal assemblies with different morphological characteristics.",autonomous vehicle
10.1016/j.cirp.2010.05.010,journal,CIRP Annals,sciencedirect,2010-12-31,sciencedirect,Advanced monitoring of machining operations,https://api.elsevier.com/content/article/pii/S0007850610001976,"
                  CIRP has had a long history of research and publication on the development and implementation of sensor monitoring of machining operations including tool condition monitoring, unmanned machining, process control and, more recently, advanced topics in machining monitoring, innovative signal processing, sensor fusion and related applications. This keynote follows a recent update of the literature on tool condition monitoring and documents the work of the cutting scientific technical committee in CIRP. The paper reviews the past contributions of CIRP in these areas and provides an up-to-date comprehensive survey of sensor technologies, signal processing, and decision making strategies for process monitoring. Application examples to industrial processes including reconfigurable sensor systems are reported. Future challenges and trends in sensor based machining operation monitoring are presented.
               ",autonomous vehicle
10.1016/j.ijdrr.2018.07.024,journal,International Journal of Disaster Risk Reduction,sciencedirect,2018-10-31,sciencedirect,"Risk, Reliability, Resilience (<mml:math altimg=si0003.gif overflow=scroll><mml:msup><mml:mrow><mml:mi mathvariant=normal>R</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:math>) and beyond in dam engineering: A state-of-the-art review",https://api.elsevier.com/content/article/pii/S2212420918306307,"
                  Dams are critical infra-structures whose their failure could leads to high economic and social consequences. For this reason, application of quantitative risk analysis has gained extensive attention in recent years. Dam safety management has become an indispensable part of all dam engineering projects worldwide. The concept of risk is heavily tied to probabilistic methods. From an engineering point of view, a clear definition of the terminologies involved in dam safety, and a comprehensive state-of-the-art review of the current literature are the starting points towards an effective risk-based approach. The first part of this paper provides a systematic review on the fundamental elements in uncertainty quantification. Then, different terminologies in risk-based dam safety are explored and their inter-connections are discussed. More than 350 papers are summarized, and several tables and conceptual plots are used for extra clarification. Since no such a paper is ever published, hopefully this can unify all the future activities and improves our understanding from probabilistic risk analysis.
               ",autonomous vehicle
10.1016/0165-0114(94)90295-X,journal,Fuzzy Sets and Systems,sciencedirect,1994-01-10,sciencedirect,Recent literature,https://api.elsevier.com/content/article/pii/016501149490295X,,autonomous vehicle
10.1016/j.ifacol.2016.07.692,journal,IFAC-PapersOnLine,sciencedirect,2016-12-31,sciencedirect,Towards proactive maintenance actions scheduling in the Semiconductor Industry (SI) using Bayesian approach,https://api.elsevier.com/content/article/pii/S2405896316309685,"
                  The Semiconductor Industry (SI) is one of the fastest growing manufacturing domains challenged by the high-mix low-volume production and short product life cycles. This results an increase in unscheduled equipment breakdowns that often result in decreasing and unstable production capacities. The success in the SI depends on our ability to quickly recover from unplanned events. It is reported (Abu-Samah et al, 2014) that misdiagnosis is one of the key reason for the extended failure durations. This is because of the fact that existing procedures to support maintenance decisions for an equipment recovery are often based on FMEA approach that represents static experts’ knowledge. In this paper, we present a methodology based on Bayesian network (BN) to advise technicians on the choice of maintenance procedure in case of unscheduled breakdowns. We argue that the sequence of patterns and alarms as generated by the equipment during production can be associated to the choice of maintenance procedure; therefore, BN is learned as a function of these alarms and warnings to predict the choice of maintenance procedure from unscheduled breakdown historical data. The set of warnings and alarms are grouped together in the proposed methodology using hybrid approach where these are first clustered based on distribution similarity followed by an experts’ intervention to fine tune initial clusters. The main contribution of the proposed methodology is to support technicians with advise on the choice of most likely effective maintenance procedure that will reduce unscheduled breakdown period and help in improving and stabilizing the production capacities. The proposed methodology is validated with a case study, from the world reputed semiconductor manufacturer, using historical data. The results show 49% of gain in terms of productive time from unscheduled breakdown periods.
               ",autonomous vehicle
10.1016/B978-0-08-057121-8.50026-9,journal,Artificial Intelligence in Chemical Engineering,sciencedirect,1991-12-31,sciencedirect,APPENDIX C: GLOSSARY OF TERMS,https://api.elsevier.com/content/article/pii/B9780080571218500269,Unknown,autonomous vehicle
10.1016/j.eswa.2014.01.011,journal,Expert Systems with Applications,sciencedirect,2014-07-31,sciencedirect,Knowledge discovery in medicine: Current issue and future trend,https://api.elsevier.com/content/article/pii/S0957417414000232,"
                  Data mining is a powerful method to extract knowledge from data. Raw data faces various challenges that make traditional method improper for knowledge extraction. Data mining is supposed to be able to handle various data types in all formats. Relevance of this paper is emphasized by the fact that data mining is an object of research in different areas. In this paper, we review previous works in the context of knowledge extraction from medical data. The main idea in this paper is to describe key papers and provide some guidelines to help medical practitioners. Medical data mining is a multidisciplinary field with contribution of medicine and data mining. Due to this fact, previous works should be classified to cover all users’ requirements from various fields. Because of this, we have studied papers with the aim of extracting knowledge from structural medical data published between 1999 and 2013. We clarify medical data mining and its main goals. Therefore, each paper is studied based on the six medical tasks: screening, diagnosis, treatment, prognosis, monitoring and management. In each task, five data mining approaches are considered: classification, regression, clustering, association and hybrid. At the end of each task, a brief summarization and discussion are stated. A standard framework according to CRISP-DM is additionally adapted to manage all activities. As a discussion, current issue and future trend are mentioned. The amount of the works published in this scope is substantial and it is impossible to discuss all of them on a single work. We hope this paper will make it possible to explore previous works and identify interesting areas for future research.
               ",autonomous vehicle
10.1016/0898-1221(93)90315-M,journal,Computers & Mathematics with Applications,sciencedirect,1993-04-30,sciencedirect,Book reports,https://api.elsevier.com/content/article/pii/089812219390315M,,autonomous vehicle
10.1016/B978-012373649-9.50035-1,journal,Computational Neuroscience in Epilepsy,sciencedirect,2008-12-31,sciencedirect,32: Computation Applied to Clinical Epilepsy and Antiepileptic Devices,https://api.elsevier.com/content/article/pii/B9780123736499500351,"
               Computational neuroscience research in epilepsy encompasses a broad range of scales in space and time. Some of the most promising work in this area focuses on biophysically accurate models of circuits and synapses in brain that give rise to seizures. More and more, computational neuroscientists are embracing opportunities to build anatomically accurate and clinically relevant models of functional networks in brain. Epilepsy is one of the most active areas in translational neuroengineering, with two early devices currently in pivotal clinical trials, and a number of others close behind. Understanding biophenomena such as epileptic seizures and translating research into therapeutic devices ultimately means iterating analysis (a whole broken into parts) and synthesis (parts unified into a whole). The overarching problem is to synthesize a model M that “compresses” all inputs I and paired outputs O observed in an experiment into a function that summarizes how I morphs into O. The function/model M could be a non linear regression, a seizure detector or predictor, a probability estimator, a ruleset, the vector field in the differential equations of motion of a dynamical network, etc. Analysis in this context could be a decomposition of data I or model M into parts that add up to the original (such as a Fourier series), or other projections not necessarily adding up such as arbitrary features. The M somehow captures a scientific target concept and “explains” the data. It also suggests how to ‘predict’ and “control” the underlying phenomenon.
            ",autonomous vehicle
10.1016/B978-0-12-374535-4.00029-1,journal,Cognitive Radio Technology,sciencedirect,2009-12-31,sciencedirect,Index,https://api.elsevier.com/content/article/pii/B9780123745354000291,Unknown,autonomous vehicle
10.1016/B978-0-12-812594-6.00005-6,journal,Applied Biomechatronics using Mathematical Models,sciencedirect,2018-12-31,sciencedirect,Chapter 5: Methods to develop mathematical models: traditional statistical analysis,https://api.elsevier.com/content/article/pii/B9780128125946000056,"
               Mathematical models for kinematics, kinetics, and muscles potentials activities from sEMG based on traditional statistical analysis are developed using different methods for data analysis, where each model is represented using a structure with a linear dynamic form, explicit and discrete, that can be verified as stochastic process and arising from empirical finding. In this chapter, Mathematical tools are studied with the objective of obtaining Mathematical Models from: traditional stochastic methods from probability and statistics as probability models, probability distributions, statistical inferences using statistical hypotheses testing parameters, z-tests, t-tests, paired t-tests, ANOVA. We apply them to: Linear equations, Regression methods, and Autoregressive equations. The different methods explained are applied to research Biomechanics examples to model and detect data behaviors, and this chapter is concluded with the development of a special software application of Mathematical Models for Analysis of Continuous Glucose Monitor (CGM) for Diabetic subjects. Note: Others Mathematical Models based on Domain/Conversion/Transform analysis, and Machine Learning Models Analysis are studied in the next chapters.
            ",autonomous vehicle
10.1016/j.knosys.2012.10.015,journal,Knowledge-Based Systems,sciencedirect,2013-02-28,sciencedirect,A unified approach to matching semantic data on the Web,https://api.elsevier.com/content/article/pii/S0950705112002961,"
                  In recent years, the Web has evolved from a global information space of linked documents to a space where data are linked as well. The Linking Open Data (LOD) project has enabled a large number of semantic datasets to be published on the Web. Due to the open and distributed nature of the Web, both the schema (ontology classes and properties) and instances of the published datasets may have heterogeneity problems. In this context, the matching of entities from different datasets is important for the integration of information from different data sources. Recently, much work has been conducted on ontology matching to resolve the schema heterogeneity problem in the semantic datasets. However, there is no unified framework for matching both schema entities and instances. This paper presents a unified matching approach to finding equivalent entities in ontologies and LOD datasets on the Web. The approach first combines multiple lexical matching strategies using a novel voting-based aggregation method; then it utilizes the structural information and the already found correspondences to discover additional ones. We evaluated our approach using datasets from both OAEI and LOD. The results show that the voting-based aggregation method provides highly accurate matching results, and that the structural propagation procedure effectively improves the recall of the results.
               ",autonomous vehicle
10.1016/B0-12-227210-2/00403-9,journal,Encyclopedia of the Human Brain,sciencedirect,2003-12-31,sciencedirect,Subject Index,https://api.elsevier.com/content/article/pii/B0122272102004039,Unknown,autonomous vehicle
10.1016/B978-0-12-490020-2.50010-3,journal,Connectionist Robot Motion Planning,sciencedirect,1990-12-31,sciencedirect,Chapter 3: How MURPHY Learns,https://api.elsevier.com/content/article/pii/B9780124900202500103,,autonomous vehicle
10.1016/S0065-2539(08)60550-8,journal,Advances in Electronics and Electron Physics,sciencedirect,1994-12-31,sciencedirect,Fuzzy Set Theoretic Tools for Image Analysis,https://api.elsevier.com/content/article/pii/S0065253908605508,"
                  This chapter describes various fuzzy set theoretic tools and explores their effectiveness in representing/describing various uncertainties that might arise in an image-recognition system and the ways these can be managed in making a decision. In the chapter, some examples of uncertainties that arise often in the process of recognizing a pattern are discussed, and it describes various fuzzy set theoretic tools for measuring information on grayness ambiguity and spatial ambiguity in an image. The concepts of bound functions and spectral fuzzy sets for handling uncertainties in membership functions are also discussed in the chapter. Their applications to low-level vision operations whose outputs are crucial and responsible for the overall performance of a vision system are presented in the chapter for demonstrating the effectiveness of these tools in managing uncertainties by providing both soft and hard decisions. Their usefulness in providing the quantitative indices for autonomous operations is also explained in the chapter. The chapter also describes the issues of feature/primitive extraction, knowledge acquisition and syntactic classification, and the features of Dempster-Shafer theory and rough set theory in this context. An application of the multivalued recognition system for detecting curved structures from remotely sensed image is also described in the chapter.
               ",autonomous vehicle
10.1016/j.asoc.2014.09.030,journal,Applied Soft Computing,sciencedirect,2015-01-31,sciencedirect,Elitist clonal selection algorithm for optimal choice of free knots in B-spline data fitting,https://api.elsevier.com/content/article/pii/S1568494614004839,"
                  Data fitting with B-splines is a challenging problem in reverse engineering for CAD/CAM, virtual reality, data visualization, and many other fields. It is well-known that the fitting improves greatly if knots are considered as free variables. This leads, however, to a very difficult multimodal and multivariate continuous nonlinear optimization problem, the so-called knot adjustment problem. In this context, the present paper introduces an adapted elitist clonal selection algorithm for automatic knot adjustment of B-spline curves. Given a set of noisy data points, our method determines the number and location of knots automatically in order to obtain an extremely accurate fitting of data. In addition, our method minimizes the number of parameters required for this task. Our approach performs very well and in a fully automatic way even for the cases of underlying functions requiring identical multiple knots, such as functions with discontinuities and cusps. To evaluate its performance, it has been applied to three challenging test functions, and results have been compared with those from other alternative methods based on AIS and genetic algorithms. Our experimental results show that our proposal outperforms previous approaches in terms of accuracy and flexibility. Some other issues such as the parameter tuning, the complexity of the algorithm, and the CPU runtime are also discussed.
               ",autonomous vehicle
10.1016/B978-0-12-741245-0.50019-1,journal,Computational Vision,sciencedirect,1990-12-31,sciencedirect,Subject Index,https://api.elsevier.com/content/article/pii/B9780127412450500191,Unknown,autonomous vehicle
10.1016/B978-0-08-092509-7.50012-9,journal,Neural Systems for Robotics,sciencedirect,1997-12-31,sciencedirect,8: A Dynamic Net for Robot Control,https://api.elsevier.com/content/article/pii/B9780080925097500129,,autonomous vehicle
10.1016/B978-0-12-823928-5.09990-4,journal,Medical Epigenetics,sciencedirect,2021-12-31,sciencedirect,Index,https://api.elsevier.com/content/article/pii/B9780128239285099904,Unknown,autonomous vehicle
10.1016/0926-860X(94)00262-2,journal,Applied Catalysis A: General,sciencedirect,1995-03-30,sciencedirect,"Preparation, characterization, and catalytic activities of silica-supported tantalum oxide for the vapor phase decomposition of methyl <ce:italic>tert</ce:italic>-butyl ether",https://api.elsevier.com/content/article/pii/0926860X94002622,"
                  Silica-supported tantalum oxide catalysts [Ta oxide/SiO2 (ALK)] were prepared by chemical reaction between tantalum alkoxide and surface hydroxyl groups of SiO2. The structure and physicochemical properties of Ta oxide/SiO2 (ALK) were compared with those of Ta oxide/SiO2 (IMP) prepared by impregnation on SiO2 with acidic TaCl5 solution, and of bulk hydrated tantalum oxide. Highly dispersed tantalum oxide species were formed on the surface by controlling the preparation conditions, such as type of alkoxy group of tantalum alkoxide, concentration of alkoxide in the solution, and impregnation temperature, as confirmed by Fourier transformation of the Ta LIII-EXAFS data. Tantalum oxide of Ta oxide/SiO2 (ALK) did not crystallize up to temperatures as high as 1500 K, while bulk hydrated tantalum oxide and tantalum oxide of Ta oxide/SiO2 (IMP) crystallized at 970 K. The acid strength of Ta oxide/SiO2 (ALK) was moderate, while hydrated tantalum oxide and Ta oxide/SiO2 (IMP) were strongly acidic. Ta oxide/SiO2 (ALK) displayed mainly a Lewis acid character. Ta oxide/SiO2 (ALK) was initially less active in the decomposition of methyl tert-butyl ether, but the selectivities for 2-methylpropene-1 and methanol were higher, as compared with hydrated tantalum oxide and Ta oxide/SiO2 (IMP). Furthermore, the activity and selectivity of Ta oxide/SiO2 (ALK) were preserved for ten hours and not inhibited upon addition of water.
               ",autonomous vehicle
10.1016/0926-860X(94)00234-7,journal,Applied Catalysis A: General,sciencedirect,1995-03-16,sciencedirect,Catalytic hydrotreatment of Illinois No. 6 Coalderived naphtha: Comparison of molybdenum nitride and molybdenum sulfide for heteroatom removal,https://api.elsevier.com/content/article/pii/0926860X94002347,"
                  The hydrotreatment of naphtha derived from Illinois No.6 coal was investigated using molybdenum sulfide and nitride catalysts. The two catalysts are compared on the basis of total catalyst weight. Molybdenum sulfide is more active than molybdenum nitride for hydrodesulfurization (HDS) of a coal-derived naphtha. The rates of hydrodeoxygenation (HDO) of the naphtha over both catalysts are comparable. For hydrodenitrogenation (HDN), the sulfide is more active than the nitride only at higher temperatures (>325°C). Based upon conversion data, the naphtha can be lumped into a reactive and a less reactive fraction with each following first-order kinetics for heteroatom removal. The HDS and HDN rates and activation energies of the less reactive lump are smaller for the nitride than for the sulfide catalyst.
               ",autonomous vehicle
10.1016/B978-0-12-372512-7.00012-2,journal,Heuristic Search,sciencedirect,2012-12-31,sciencedirect,Chapter 12: Adversary Search,https://api.elsevier.com/content/article/pii/B9780123725127000122,"This chapter studies heuristic search in game graphs. For two-player games, tree search from the current node is performed and endgame databases are built. The chapter discusses different refinement strategies to improve the exploration for forward search and suggests a symbolic classification algorithm.",autonomous vehicle
10.1016/B978-0-12-741245-0.50017-8,journal,Computational Vision,sciencedirect,1990-12-31,sciencedirect,References,https://api.elsevier.com/content/article/pii/B9780127412450500178,Unknown,autonomous vehicle
10.1016/B978-0-12-816403-7.00001-5,journal,Trends in Personalized Nutrition,sciencedirect,2019-12-31,sciencedirect,Chapter 1: An Introduction to Personalized Nutrition,https://api.elsevier.com/content/article/pii/B9780128164037000015,"
               Dietary factors are known to play a role in health and diseases, and there is convincing evidence that adopting a correct lifestyle (diet included) can be more effective than drug treatment in the prevention of diseases in susceptible subjects. Generalized dietary recommendations have a limited impact and often lead to modest improvements in food intake. Personalization of dietary recommendations, by taking into account specific characteristics of the recipients, may thus increase motivation toward dietary changes. In order for these changes to be effective, personalization can be based on biological evidence and/or personal behaviors, preferences, or objectives. This chapter provides an overview on strategies to deliver personalized nutrition and future perspectives in the field.
            ",autonomous vehicle
10.1016/B978-0-12-809633-8.09001-4,journal,Encyclopedia of Bioinformatics and Computational Biology,sciencedirect,2019-12-31,sciencedirect,Index,https://api.elsevier.com/content/article/pii/B9780128096338090014,Unknown,autonomous vehicle
10.1016/B978-044450208-7/50014-9,journal,Neuromimetic Semantics,sciencedirect,2004-12-31,sciencedirect,Index,https://api.elsevier.com/content/article/pii/B9780444502087500149,Unknown,autonomous vehicle
10.1016/B978-0-12-803206-0.00001-8,journal,Clinical Informatics Literacy,sciencedirect,2017-12-31,sciencedirect,1: Category Definitions,https://api.elsevier.com/content/article/pii/B9780128032060000018,,autonomous vehicle
10.1016/S0079-7421(08)60113-9,journal,Psychology of Learning and Motivation,sciencedirect,1989-12-31,sciencedirect,A Computational Approach to Hippocampal Function,https://api.elsevier.com/content/article/pii/S0079742108601139,"
                  This chapter describes the early, formative stages of a theory of hippocampal function. This theory has been stimulated by the psychological observations indicating a role for the hippocampus in short-term working memory and spatial behavior and develops mainly through the consideration of computational issues. These computational issues are related to the psychological viewpoint through physiological and anatomical observations. The hippocampus participates in the prediction of future representations based on past and present representations. All three classes of representations are derived from a multiplicity of sensory modalities, such as auditory, visual, and olfactory signals from neo- and piriform cortices. This fusion of sensory modalities requires recoding because of computational complexity problems. The CA1 region of the hippocampus is postulated to be a prediction-generating layer or tier. This region produces a prediction based on its input from hippocampal region CA3. The combined hippocampal dentate gyrus/CA3 (DG/CA3) system is postulated to be a preprocessor serving the CA1 prediction layer. The computational complexity problems arise from the combinatorial explosion of possible representations resulting when the hippocampus and supporting limbic structures mix representations from multiple sensory modalities.
               ",autonomous vehicle
10.1016/S0166-3615(03)00127-1,journal,Computers in Industry,sciencedirect,2003-12-31,sciencedirect,A soft computing approach for task contracting in multi-agent manufacturing control,https://api.elsevier.com/content/article/pii/S0166361503001271,"
                  This paper describes a new task-contracting schema for multi-agent manufacturing control based on soft computing. It aims to apply fuzzy techniques to implement a real-time multi-criteria task-contracting mechanism for part flow control in manufacturing floor. For comparison purposes, the paper also considers other recently proposed evolutionary strategies to adapt and optimize agents’ decision parameters to the changing conditions of the manufacturing floor. All the considered approaches are compared on a detailed simulation model of a hypothetical manufacturing system that was recently proposed in literature as benchmark for multi-agent control systems.
               ",autonomous vehicle
10.1016/0301-0082(84)90021-2,journal,Progress in Neurobiology,sciencedirect,1984-12-31,sciencedirect,Hebb synaptic plasticity,https://api.elsevier.com/content/article/pii/0301008284900212,,autonomous vehicle
10.1016/B978-0-08-057121-8.50028-2,journal,Artificial Intelligence in Chemical Engineering,sciencedirect,1991-12-31,sciencedirect,Subject Index,https://api.elsevier.com/content/article/pii/B9780080571218500282,Unknown,autonomous vehicle
10.1016/0165-0114(90)90093-L,journal,Fuzzy Sets and Systems,sciencedirect,1990-05-30,sciencedirect,Recent literature,https://api.elsevier.com/content/article/pii/016501149090093L,,autonomous vehicle
10.1016/j.robot.2016.12.006,journal,Robotics and Autonomous Systems,sciencedirect,2017-05-31,sciencedirect,Bio-inspired self-organising multi-robot pattern formation: A review,https://api.elsevier.com/content/article/pii/S0921889016300185,"
                  Self-organised emergent patterns can be widely seen in natural and man-made complex systems generated by interactions among local components without external or global control. This paper presents a survey of recent research advances in self-organising pattern formation in mobile multi-robot (or swarm robotic) systems. Relevant pattern formation methods are reviewed with a special focus on biologically-inspired self-organising approaches inspired from macroscopic collective behaviours or microscopic multicellular developing mechanisms. As the ultimate goal of this review is to provide insight into pattern formation using real robots, limitations and considerations on dealing with a large number of robots are discussed. In addition, guided self-organisation is also discussed as a design strategy where the swarm robotic system may be endowed with local rules for generating desired global patterns.
               ",autonomous vehicle
10.1016/S1474-6670(17)33260-3,journal,IFAC Proceedings Volumes,sciencedirect,2001-09-30,sciencedirect,Negotiation Approaches in B2B E-Commerce Applied to Supply Chain Management,https://api.elsevier.com/content/article/pii/S1474667017332603,"
                  A Negotiation Support System can be defined as an information system that helps a user to take decisions during a bargaining process with other agents. The relevance of such systems to manufacturing is increasing with the opportunities granted by Business-To-Business (B2B) electronic commerce over the Internet. In this context, it is necessary to provide an overview of the current state of the art across different disciplinary fields, with the specific aim of setting a research agenda for the future. The paper provides a classification of existing approaches and unsolved problems. For the sake of pragmatism, it has been chosen to organize such classification around a specific architecture for a Negotiation Support System which is being developed within a wider research project. The aim of the paper is to show how a system for supporting negotiation between different firms should be effective and which are the problematics and the research fields involved during the development.
               ",autonomous vehicle
10.1016/B978-0-12-814435-0.00017-1,journal,Internet of Things,sciencedirect,2019-12-31,sciencedirect,Chapter 5: Technology Fundamentals,https://api.elsevier.com/content/article/pii/B9780128144350000171,"
               This chapter presents an overview of technology fundamentals – the building blocks upon which the IoT rests. Here, we cover devices and gateways, personal, local and Wide Area Networking, Data Management, business processes, and cloud and analytics technologies. Devices form the physical basis of the Internet of Things and provide functions for sensing and actuating in the physical world. Local and Wide Area Networks provide these with the necessary infrastructure to connect to cloud services and associated applications. Data Management handles essential functions such as data acquisition, validation, and storage and makes sure that critical information is available at the right point, in a timely manner, and in the right form. Business processes refers to the series of steps to perform management, operational, and supporting activities for achieving specific mission objectives. XaaS is used as a general term to describe the functions provided as a service by cloud infrastructures, such as computational capacity, software, networking, and storage. Analytics are used to extract additional value from data generated by devices and enable new opportunities by using data from devices for multiple purposes, many of which may not have been imagined at the time of deployment. Knowledge Management Frameworks provide the ability to understand data-generated information and may leverage existing experiences within certain decision making contexts.
            ",autonomous vehicle
10.1016/j.echo.2021.03.013,journal,Journal of the American Society of Echocardiography,sciencedirect,2021-06-30,sciencedirect,ASE 2021 Original Science Presentations,https://api.elsevier.com/content/article/pii/S0894731721001796,,autonomous vehicle
10.3182/20050703-6-CZ-1902.00001,journal,IFAC Proceedings Volumes,sciencedirect,2005-12-31,sciencedirect,Front cover and table of contents,https://api.elsevier.com/content/article/pii/S147466701636013X,,autonomous vehicle
10.1016/S0065-2377(08)60108-8,journal,Advances in Chemical Engineering,sciencedirect,1999-12-31,sciencedirect,Process Data Analysis and Interpretation,https://api.elsevier.com/content/article/pii/S0065237708601088,"
                  This chapter proposes a perspective for integrating a wide-ranging array of technologies and managing complexity in comprehensive analysis and interpretation systems. This integrated perspective is the product of widely varying technology perspectives. To support increasingly sophisticated process management activities, the raw process data must be transformed into meaningful descriptions of process conditions. The primary purpose of pattern recognition is to determine class membership for a set of numeric input data. The performance of any given approach is ultimately driven by how well an appropriate discriminant can be defined to resolve the numeric data into a label of interest. In this context, the chapter takes a look at the alternatives to quantitative behavioral model approaches from the point of view of interpretation. These methods include a wide variety of linear and nonlinear modeling methods developed across a wide range of technical areas, including statistics, process simulation, control, and intelligent systems. The chapter discusses local and nonlocal data interpretation, Symbolic–Symbolic Interpretation, and scope of large scale operations. Three comprehensive examples illustrated in the chapter are detection of abnormal situations, data analysis of batch operation variability, and diagnosis of operating problems in a batch polymer reactor.
               ",autonomous vehicle
10.1016/B978-0-12-480575-0.50015-0,journal,Artificial Intelligence in Process Engineering,sciencedirect,1990-12-31,sciencedirect,Index,https://api.elsevier.com/content/article/pii/B9780124805750500150,Unknown,autonomous vehicle
10.1016/B978-0-12-803206-0.18001-0,journal,Clinical Informatics Literacy,sciencedirect,2017-12-31,sciencedirect,Index,https://api.elsevier.com/content/article/pii/B9780128032060180010,Unknown,autonomous vehicle
10.1016/B0-12-657410-3/09006-1,journal,Encyclopedia of Applied Psychology,sciencedirect,2004-12-31,sciencedirect,Subject Index,https://api.elsevier.com/content/article/pii/B0126574103090061,Unknown,autonomous vehicle
10.1016/B978-0-12-480575-0.50014-9,journal,Artificial Intelligence in Process Engineering,sciencedirect,1990-12-31,sciencedirect,10: An Adaptive Heuristic-Based System for Synthesis of Complex Separation Sequences,https://api.elsevier.com/content/article/pii/B9780124805750500149,"
               Abstract
            This chapter presents a framework for developing an intelligent computer software environment, specifically an adaptive knowledge-based system, for general separation scheme synthesis without heat integration. The adaptivity of the system manifests itself in the self-adjustment of the weights of the heuristics employed in the knowledge base. The separation problems under consideration are complex in the sense that each can have multiple input and output streams. Moreover, the separation schemes generated are non-sharp, in which some or all product streams may be impure, and any component is allowed to be distributed between two or more product streams.",autonomous vehicle
10.1016/B978-0-12-803766-9.00006-3,journal,Bioinspired Legged Locomotion,sciencedirect,2017-12-31,sciencedirect,Chapter 4: Control of Motion and Compliance,https://api.elsevier.com/content/article/pii/B9780128037669000063,"
               
                  This chapter reviews different methods for the control of legged locomotion with a special focus on bipedal locomotion. All locomotion systems are governed by complex nonlinear, hybrid dynamics, and are redundant, underactuated and often unstable, which makes their control a very challenging task.
               The chapter starts with a presentation of different concepts of stability and robustness of locomotion considering nominal walking situations as well as the reaction to larger external perturbations. Then, optimal control is discussed as a guiding principle of human and robot motion, and dynamic multibody system models as well as different optimization problem formulations for the generation, control and analysis of locomotion are shown. Constant or variable compliance plays an important role in biological and bio-inspired locomotion, but needs to be properly adapted in the design and control process which also can be addressed by optimal control. Next, impedance control in locomotion is discussed, looking at passive and active impedance and different approaches to emulated appropriate impedances for robots. The chapter also reviews control approaches for legged locomotion based on template models, i.e. very simple representations of the original locomotor system, with a focus using template models for the design of suitable controllers. The state of the art of passive dynamic walking robots as well as powered and almost passive dynamic robots is summarized and their achievements in terms of energy-efficiency, stability, robustness and versatility re discussed. Hybrid zero dynamics is presented as a control synthesis framework that reduces the complexity of whole-body dynamics control and allows to develop efficient controllers for dynamic walking and running motions. Finally, a control approach for locomotion based on the concept of central pattern generators is presented which helps to control locomotion of legged robots and gives insight into human movement control.
            ",autonomous vehicle
10.1016/S0065-2458(08)60717-2,journal,Advances in Computers,sciencedirect,1999-12-31,sciencedirect,Subject Index,https://api.elsevier.com/content/article/pii/S0065245808607172,,autonomous vehicle
10.1016/B978-0-08-102295-5.18001-1,journal,International Encyclopedia of Human Geography,sciencedirect,2020-12-31,sciencedirect,Index,https://api.elsevier.com/content/article/pii/B9780081022955180011,Unknown,autonomous vehicle
10.1016/j.neures.2008.05.002,journal,Neuroscience Research,sciencedirect,2008-12-31,sciencedirect,Abstracts of the 31st Annual Meeting of the Japan Neuroscience Society (Neuro 2008),https://api.elsevier.com/content/article/pii/S0168010208001478,,autonomous vehicle
10.1016/j.fueleneab.2021.02.002,journal,Fuel and Energy Abstracts,sciencedirect,2021-03-31,sciencedirect,Abstracts,https://api.elsevier.com/content/article/pii/S0140670121000023,,autonomous vehicle
10.1016/j.neuroimage.2006.04.176,journal,NeuroImage,sciencedirect,2006-12-31,sciencedirect,OHBM 2006 Program list of posters,https://api.elsevier.com/content/article/pii/S1053811906004575,,autonomous vehicle
10.1016/B978-0-12-818422-6.00094-0,journal,Principles of Tissue Engineering,sciencedirect,2020-12-31,sciencedirect,Index,https://api.elsevier.com/content/article/pii/B9780128184226000940,Unknown,autonomous vehicle
10.1016/S1936-878X(20)30581-7,journal,JACC: Cardiovascular Imaging,sciencedirect,2020-08-31,sciencedirect,Full Issue PDF,https://api.elsevier.com/content/article/pii/S1936878X20305817,,autonomous vehicle
10.1016/B978-0-12-418691-0.00026-5,journal,Structural Health Monitoring with Piezoelectric Wafer Active Sensors,sciencedirect,2014-12-31,sciencedirect,Index,https://api.elsevier.com/content/article/pii/B9780124186910000265,Unknown,autonomous vehicle
10.1016/0004-3702(86)90072-X,journal,Artificial Intelligence,sciencedirect,1986-09-30,sciencedirect,"Fusion, propagation, and structuring in belief networks",https://api.elsevier.com/content/article/pii/000437028690072X,"
                  Belief networks are directed acyclic graphs in which the nodes represent propositions (or variables), the arcs signify direct dependencies between the linked propositions, and the strengths of these dependencies are quantified by conditional probabilities. A network of this sort can be used to represent the generic knowledge of a domain expert, and it turns into a computational architecture if the links are used not merely for storing factual knowledge but also for directing and activating the data flow in the computations which manipulate this knowledge.
                  The first part of the paper deals with the task of fusing and propagating the impacts of new information through the networks in such a way that, when equilibrium is reached, each proposition will be assigned a measure of belief consistent with the axioms of probability theory. It is shown that if the network is singly connected (e.g. tree-structured), then probabilities can be updated by local propagation in an isomorphic network of parallel and autonomous processors and that the impact of new information can be imparted to all propositions in time proportional to the longest path in the network.
                  The second part of the paper deals with the problem of finding a tree-structured representation for a collection of probabilistically coupled propositions using auxiliary (dummy) variables, colloquially called “hidden causes.” It is shown that if such a tree-structured representation exists, then it is possible to uniquely uncover the topology of the tree by observing pairwise dependencies among the available propositions (i.e., the leaves of the tree). The entire tree structure, including the strengths of all internal relationships, can be reconstructed in time proportional to n log n, where n is the number of leaves.
               ",autonomous vehicle
10.3182/20120711-3-BE-2027.90001,journal,IFAC Proceedings Volumes,sciencedirect,2012-07-31,sciencedirect,Welcome and Introduction,https://api.elsevier.com/content/article/pii/S1474667015379167,,autonomous vehicle
10.1016/j.camwa.2005.04.001,journal,Computers & Mathematics with Applications,sciencedirect,2005-05-31,sciencedirect,Book reports,https://api.elsevier.com/content/article/pii/S0898122105001719,,autonomous vehicle
10.1016/B978-012088760-6.50017-9,journal,Structural Health Monitoring,sciencedirect,2008-12-31,sciencedirect,INDEX,https://api.elsevier.com/content/article/pii/B9780120887606500179,Unknown,autonomous vehicle
10.1016/B978-008045405-4.09015-7,journal,Encyclopedia of Ecology,sciencedirect,2008-12-31,sciencedirect,Subject Index,https://api.elsevier.com/content/article/pii/B9780080454054090157,Unknown,autonomous vehicle
10.1017/S2040470019000013,journal,Advances in Animal Biosciences,sciencedirect,2019-12-31,sciencedirect,Proceedings of the British Society of Animal Science,https://api.elsevier.com/content/article/pii/S2040470019000013,,autonomous vehicle
10.1016/B978-0-12-386456-7.09997-4,journal,Pathobiology of Human Disease,sciencedirect,2014-12-31,sciencedirect,Index,https://api.elsevier.com/content/article/pii/B9780123864567099974,Unknown,autonomous vehicle
10.1016/j.fueleneab.2018.10.002,journal,Fuel and Energy Abstracts,sciencedirect,2018-11-30,sciencedirect,Abstracts,https://api.elsevier.com/content/article/pii/S0140670118300365,,autonomous vehicle
10.1016/B0-12-227410-5/09009-8,journal,Encyclopedia of Physical Science and Technology,sciencedirect,2003-12-31,sciencedirect,Subject Index,https://api.elsevier.com/content/article/pii/B0122274105090098,Unknown,autonomous vehicle
10.1016/S0013-4694(97)80512-3,journal,Electroencephalography and Clinical Neurophysiology,sciencedirect,1997-07-31,sciencedirect,Abstracts P-34-7–P-65-16,https://api.elsevier.com/content/article/pii/S0013469497805123,,autonomous vehicle
10.1016/S0196-0644(05)80827-3,journal,Annals of Emergency Medicine,sciencedirect,1993-05-31,sciencedirect,Abstracts of the 23rd annual meeting of the society for academic emergency medicine,https://api.elsevier.com/content/article/pii/S0196064405808273,,autonomous vehicle
10.1016/B978-0-12-809597-3.09001-5,journal,Comprehensive Energy Systems,sciencedirect,2017-12-31,sciencedirect,Index,https://api.elsevier.com/content/article/pii/B9780128095973090015,Unknown,autonomous vehicle
