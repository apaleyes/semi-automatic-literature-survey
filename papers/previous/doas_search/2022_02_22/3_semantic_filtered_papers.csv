doi,type,publication,publisher,publication_date,database,title,url,abstract,domain,id,status
10.1109/ccnc49033.2022.9700515,to_check,2022 IEEE 19th Annual Consumer Communications & Networking Conference (CCNC),IEEE,2022-01-11 00:00:00,ieeexplore,cave-vr and unity game engine for visualizing city scale 3d meshes,https://ieeexplore.ieee.org/document/9700515/,"Modeling and simulation of large urban regions is beneficial for a range of applications including intelligent transportation, smart cities, infrastructure planning, and training artificial intelligence for autonomous navigation systems including ground vehicles and aerial drones. Immersive environments including virtual reality (VR), augmented reality (AR), mixed reality (MR or XR) can be used to explore city scale regions for planning, design, training and operations. Virtual environments are in the midst of rapid change as innovations in display tech-nologies, graphics processors and game engine software present new opportunities for incorporating modeling and simulation into engineering workflows. Game engine software like Unity with photorealistic rendering and realistic physics have plug-in support for a variety of virtual environments. In this paper, we explore the visualization of urban scale real world accurate meshes in virtual environments, including the Microsoft HoloLens head mounted display or the CAVE VR for multi-user interaction.",smart cities,1,not included
10.1007/978-3-030-85365-5_17,to_check,"Advances in Deep Learning, Artificial Intelligence and Robotics",Springer,2022-01-01 00:00:00,springer,robust model for rural education using deep learning and robotics,http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-85365-5_17,"Rural Education is important for the overall development of villages. National Achievement Survey (NAS) has surveyed and reported in many States of India consistent decline in the learning levels of students in mathematics, language and science from class III to class VIII studying in the government school system. Smart Villages is only possible if the literacy level and infrastructure improves considerably. This paper aims to perform a reality check of the situation by comparing different rural areas of various countries including India and study of related work done. The paper proposes a Robust Model for Rural Education by developing an intelligent humanoid robot using the Deep Learning approach, Human recognition, Object Recognition and Speech Recognition. The data set consists of Primary and Secondary Student data of around 10,000 Students (5 years) from 5 villages. The Proposed Model would be compared with existing models on the parameters of Learnability, Decision making, Flexibility and Cost-effectiveness. The implementation of this Model will help in decreasing the drop out rate, evaluate Students and give them a Learning platform based on their characteristics, increase adaptive and self paced learning. This Model can also be executed for Rural Adult Education and Skill building so that the Smart Village concept can become a reality.",multimedia,2,not included
http://arxiv.org/abs/2201.07312v1,to_check,arxiv,arxiv,2022-01-18 00:00:00,arxiv,model-driven cluster resource management for ai workloads in edge clouds,http://arxiv.org/abs/2201.07312v1,"Since emerging edge applications such as Internet of Things (IoT) analytics
and augmented reality have tight latency constraints, hardware AI accelerators
have been recently proposed to speed up deep neural network (DNN) inference run
by these applications. Resource-constrained edge servers and accelerators tend
to be multiplexed across multiple IoT applications, introducing the potential
for performance interference between latency-sensitive workloads. In this
paper, we design analytic models to capture the performance of DNN inference
workloads on shared edge accelerators, such as GPU and edgeTPU, under different
multiplexing and concurrency behaviors. After validating our models using
extensive experiments, we use them to design various cluster resource
management algorithms to intelligently manage multiple applications on edge
accelerators while respecting their latency constraints. We implement a
prototype of our system in Kubernetes and show that our system can host 2.3X
more DNN applications in heterogeneous multi-tenant edge clusters with no
latency violations when compared to traditional knapsack hosting algorithms.",multimedia,3,included
http://arxiv.org/abs/2201.02279v1,to_check,arxiv,arxiv,2022-01-06 00:00:00,arxiv,de-rendering 3d objects in the wild,http://arxiv.org/abs/2201.02279v1,"With increasing focus on augmented and virtual reality applications (XR)
comes the demand for algorithms that can lift objects from images and videos
into representations that are suitable for a wide variety of related 3D tasks.
Large-scale deployment of XR devices and applications means that we cannot
solely rely on supervised learning, as collecting and annotating data for the
unlimited variety of objects in the real world is infeasible. We present a
weakly supervised method that is able to decompose a single image of an object
into shape (depth and normals), material (albedo, reflectivity and shininess)
and global lighting parameters. For training, the method only relies on a rough
initial shape estimate of the training objects to bootstrap the learning
process. This shape supervision can come for example from a pretrained depth
network or - more generically - from a traditional structure-from-motion
pipeline. In our experiments, we show that the method can successfully
de-render 2D images into a decomposed 3D representation and generalizes to
unseen object categories. Since in-the-wild evaluation is difficult due to the
lack of ground truth data, we also introduce a photo-realistic synthetic test
set that allows for quantitative evaluation.",multimedia,4,not included
http://arxiv.org/abs/2201.01369v1,to_check,arxiv,arxiv,2022-01-04 00:00:00,arxiv,"using simulation optimization to improve zero-shot policy transfer of
  quadrotors",http://arxiv.org/abs/2201.01369v1,"In this work, we show that it is possible to train low-level control policies
with reinforcement learning entirely in simulation and, then, deploy them on a
quadrotor robot without using real-world data to fine-tune. To render zero-shot
policy transfers feasible, we apply simulation optimization to narrow the
reality gap. Our neural network-based policies use only onboard sensor data and
run entirely on the embedded drone hardware. In extensive real-world
experiments, we compare three different control structures ranging from
low-level pulse-width-modulated motor commands to high-level attitude control
based on nested proportional-integral-derivative controllers. Our experiments
show that low-level controllers trained with reinforcement learning require a
more accurate simulation than higher-level control policies.",robotics,5,included
10.1109/wacv51458.2022.00206,to_check,2022 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV),IEEE,2022-01-08 00:00:00,ieeexplore,plugging self-supervised monocular depth into unsupervised domain adaptation for semantic segmentation,https://ieeexplore.ieee.org/document/9707096/,"Although recent semantic segmentation methods have made remarkable progress, they still rely on large amounts of annotated training data, which are often infeasible to collect in the autonomous driving scenario. Previous works usually tackle this issue with Unsupervised Domain Adaptation (UDA), which entails training a network on synthetic images and applying the model to real ones while minimizing the discrepancy between the two domains. Yet, these techniques do not consider additional information that may be obtained from other tasks. Differently, we propose to exploit self-supervised monocular depth estimation to improve UDA for semantic segmentation. On one hand, we deploy depth to realize a plug-in component which can inject complementary geometric cues into any existing UDA method. We further rely on depth to generate a large and varied set of samples to Self-Train the final model. Our whole proposal allows for achieving state-of-the-art performance (58.8 mIoU) in the GTA5 → CS benchmark. Code is available at https://github.com/CVLAB-Unibo/d4-dbst.",autonomous vehicle,6,not included
http://arxiv.org/abs/2201.05115v1,to_check,arxiv,arxiv,2022-01-13 00:00:00,arxiv,functional anomaly detection: a benchmark study,http://arxiv.org/abs/2201.05115v1,"The increasing automation in many areas of the Industry expressly demands to
design efficient machine-learning solutions for the detection of abnormal
events. With the ubiquitous deployment of sensors monitoring nearly
continuously the health of complex infrastructures, anomaly detection can now
rely on measurements sampled at a very high frequency, providing a very rich
representation of the phenomenon under surveillance. In order to exploit fully
the information thus collected, the observations cannot be treated as
multivariate data anymore and a functional analysis approach is required. It is
the purpose of this paper to investigate the performance of recent techniques
for anomaly detection in the functional setup on real datasets. After an
overview of the state-of-the-art and a visual-descriptive study, a variety of
anomaly detection methods are compared. While taxonomies of abnormalities (e.g.
shape, location) in the functional setup are documented in the literature,
assigning a specific type to the identified anomalies appears to be a
challenging task. Thus, strengths and weaknesses of the existing approaches are
benchmarked in view of these highlighted types in a simulation study. Anomaly
detection methods are next evaluated on two datasets, related to the monitoring
of helicopters in flight and to the spectrometry of construction materials
namely. The benchmark analysis is concluded by recommendation guidance for
practitioners.",health,7,not included
10.1109/tcyb.2020.2964011,to_check,IEEE Transactions on Cybernetics,IEEE,2022-01-01 00:00:00,ieeexplore,hierarchical granular computing-based model and its reinforcement structural learning for construction of long-term prediction intervals,https://ieeexplore.ieee.org/document/8972350/,"As one of the most essential sources of energy, byproduct gas plays a pivotal role in the steel industry, for which the flow tendency is generally regarded as the guidance for planning and scheduling in real production. In order to obtain the numeric estimation along with its reliability, the construction of prediction intervals (PIs) is highly demanded by any practical applications as well as being long term for providing more information on future trends. Bearing this in mind, in this article, a hierarchical granular computing (HGrC)-based model is established for constructing long-term PIs, in which probabilistic modeling gives rise to a long horizon of numeric prediction, and the deployment of information granularities hierarchically extends the result to be interval-valued format. Considering that the structure of this model has a direct impact on its performance, Monte-Carlo search with a policy gradient technique is then applied for reinforcement structure learning. Compared with the existing methods, the size (length) of the granules in the proposed approach is unequal so that it becomes effective for not only periodic but also nonperiodic data. Furthermore, with the use of parallel strategy, the efficiency can be also guaranteed for real-world applications. The experimental results demonstrate that the proposed method is superior to other commonly encountered techniques, and the stability of the structure learning process behaves better when compared with other reinforcement learning approaches.",industry,8,not included
http://arxiv.org/abs/2201.06735v1,to_check,arxiv,arxiv,2022-01-18 00:00:00,arxiv,ai augmented digital metal component,http://arxiv.org/abs/2201.06735v1,"The aim of this work is to propose a new paradigm that imparts intelligence
to metal parts with the fusion of metal additive manufacturing and artificial
intelligence (AI). Our digital metal part classifies the status with real time
data processing with convolutional neural network (CNN). The training data for
the CNN is collected from a strain gauge embedded in metal parts by laser
powder bed fusion process. We implement this approach using additive
manufacturing, demonstrate a self-cognitive metal part recognizing partial
screw loosening, malfunctioning, and external impacting object. The results
indicate that metal part can recognize subtle change of multiple fixation state
under repetitive compression with 89.1% accuracy with test sets. The proposed
strategy showed promising potential in contributing to the hyper-connectivity
for next generation of digital metal based mechanical systems",industry,9,not included
10.1016/j.autcon.2021.104088,to_check,Automation in Construction,scopus,2022-02-01,sciencedirect,vision-based high-precision intelligent monitoring for shield tail clearance,https://api.elsevier.com/content/abstract/scopus_id/85120874971,"Real-time shield tail clearance measurement and monitoring is a key task during shield tunneling construction. The shield tail clearance measurement and monitoring technology development is still in its infancy, the current methods are mainly designed manually based on intuition. In order to fill the gap between the requirement of shield tail clearance measurement and monitoring and the limitations of the current methods, this paper systematically studies the existing mechanisms related to shield tail clearance measurement and monitoring, and develops a high-precision intelligent monitoring system for shield tail clearance. The proposed monitoring system includes four components: 1) two types of shield tail clearance calculation models, 2) the integrated hardware of the monitoring system which is composed of a data acquisition unit, a signal transmission unit and a control unit, 3) the region of interest (ROI) extraction method based on deep neural network, and the image processing algorithms for image enhancement and feature extraction, 4) the custom-developed software built on mature integrated development environment (IDE). After the calculation model of shield tail clearance is established, the system uses monitoring devices equipped with industrial cameras to obtain the on-site image, and then applies image processing technologies along with deep learning approach to extract the key features, which are brought into the model to calculate the values of shield tail clearance, finally displays these values and simulates the current tunneling attitude of the shield machine in real time. The experimental results show that the system proposed in this paper achieves the goal of high precision measuring and real-time monitoring of the shield tail clearance.",industry,10,included
10.1016/j.ssci.2021.105529,to_check,Safety Science,scopus,2022-02-01,sciencedirect,a novel decision support system for managing predictive maintenance strategies based on machine learning approaches,https://api.elsevier.com/content/abstract/scopus_id/85118705579,"Nowadays, the industrial environment is characterised by growing competitiveness, short response times, cost reduction and reliability of production to meet customer needs. Thus, the new industrial paradigm of Industry 4.0 has gained interest worldwide, leading many manufacturers to a significant digital transformation. Digital technologies have enabled a novel approach to decision-making processes based on data-driven strategies, where knowledge extraction relies on the analysis of a large amount of data from sensor-equipped factories. In this context, Predictive Maintenance (PdM) based on Machine Learning (ML) is one of the most prominent data-driven analytical approaches for monitoring industrial systems aiming to maximise reliability and efficiency. In fact, PdM aims not only to reduce equipment failure rates but also to minimise operating costs by maximising equipment life. When considering industrial applications, industries deal with different issues and constraints relating to process digitalisation. The main purpose of this study is to develop a new decision support system based on decision trees (DTs) that guides the decision-making process of PdM implementation, considering context-aware information, quality and maturity of collected data, severity, occurrence and detectability of potential failures (identified through FMECA analysis) and direct and indirect maintenance costs. The decision trees allow the study of different scenarios to identify the conditions under which a PdM policy, based on the ML algorithm, is economically profitable compared to corrective maintenance, considered to be the current scenario. The results show that the proposed methodology is a simple and easy way to implement tool to support the decision process by assessing the different levels of occurrence and severity of failures. For each level, savings and the potential costs have been evaluated at leaf nodes of the trees aimed at defining the most suitable maintenance strategy implementation. Finally, the proposed DTs are applied to a real industrial case to illustrate their applicability and robustness.",industry,11,included
10.1016/j.softx.2021.100956,to_check,SoftwareX,scopus,2022-01-01,sciencedirect,tx2_fcnn_node: an open-source ros compatible tool for monocular depth reconstruction,https://api.elsevier.com/content/abstract/scopus_id/85121968187,"We present tx2_fcnn_node – a Robot Operating System (ROS) compatible tool that is aimed at seamless integration of various monocular depth reconstruction neural networks to the robotic software based on ROS (which is a de-facto standard in the area of robotics). Our tool simplifies the process of deploying, evaluating, and comparing depth reconstruction neural networks both on real robots and in simulation. We complement our software with a set of the precompiled neural networks which can be used off the shelf, with some of them being able to demonstrate near real-time performance when running onboard compact embedded platforms, e.g. Nvidia Jetson TX2, that are often used nowadays both in academia and industry.",industry,12,included
10.1109/lra.2021.3116700,to_check,IEEE Robotics and Automation Letters,IEEE,2022-01-01 00:00:00,ieeexplore,sim2real learning of obstacle avoidance for robotic manipulators in uncertain environments,https://ieeexplore.ieee.org/document/9555228/,"Obstacle avoidance for robotic manipulators can be challenging when they operate in unstructured environments. This problem is probed with the sim-to-real (sim2real) deep reinforcement learning, such that a moving policy of the robotic arm is learnt in a simulator and then adapted to the real world. However, the problem of sim2real adaptation is notoriously difficult. To this end, this work proposes (1) a unified representation of obstacles and targets to capture the underlying dynamics of the environment while allowing generalization to unseen goals and (2) a flexible end-to-end model combining the unified representation with the deep reinforcement learning control module that can be trained by interacting with the environment. Such a representation is agnostic to the shape and appearance of the underlying objects, which simplifies and unifies the scene representation in both simulated and real worlds. We implement this idea with a vision-based actor-critic framework by devising a bounding box predictor module. The predictor estimates the 3D bounding boxes of obstacles and targets from the RGB-D input. The features extracted by the predictor are fed into the policy network, and all the modules are jointly trained. This makes the policy learn object-aware scene representation, which leads to a data-efficient learning of the obstacle avoidance policy. Our experiments in simulated environment and the real-world show that the end-to-end model of the unified representation achieves better sim2real adaption and scene generalization than state-of-the-art techniques.",multimedia,13,included
10.1109/access.2021.3139537,to_check,IEEE Access,IEEE,2000-01-01 00:00:00,ieeexplore,"automatic adaptation of open educational resources: an approach from a multilevel methodology based on students’ preferences, educational special needs, artificial intelligence and accessibility metadata",https://ieeexplore.ieee.org/document/9669174/,"The need for adaptive e-learning environments that respond to learning variability is now a fundamental requirement in education, as it helps to ensure that students learn and pass their courses within a set time frame. Although guidelines, techniques and methods have been established in recent years to contribute to the development of accessible and adaptable e-learning environments that promote digital inclusion, their implementation is challenging due to the lack of knowledge of an adequate way to do it and because it is considered more of a technological competence for scholars in the area. In this context, automated support for adapting material that responds to the correct use of accessibility metadata not only provides a way to improve the description of adapted educational resources, but also facilitates their search according to the needs and preferences of students, particularly those with disabilities. In this article, we carry out a multilevel methodological proposal for the automatic adaptation of open educational resources, in order to provide a tool that contributes to the accessibility and correct use of their metadata in e-learning environments. A research is conducted with students with disabilities to establish their real needs and preferences, highlighting the need to strengthen the adequate description and coherent alternative text in images, the correct subtitling in videos and the conversion of audio to text, data that are relevant to our proposal. The research conducted aims to contribute with an automated support tool in the generation of accessible educational resources that are correctly labeled for search and reuse. This research also aims to support researchers in artificial intelligence applications to address challenges and opportunities in the field of virtual education, in addition to providing an overview that could help those who generate educational resources and maintain their interest in making them accessible.",multimedia,14,not included
10.1007/978-3-030-92127-9_68,to_check,"11th International Conference on Theory and Application of Soft Computing, Computing with Words and Perceptions and Artificial Intelligence - ICSCCW-2021",Springer,2022-01-01 00:00:00,springer,application of digital twin theory for improvement of natural gas treatment unit,http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-92127-9_68,"This paper describes fundamental principles of Digital Twins theory and provides exact investigation results in application of Digital Twins theory in upstream branch of oil and gas industry, namely based on example of natural gas treatment plant’s performance increase. As one of key process units of natural gas treatment which allows to implement most powerful functions of digital twin gas sweetening unit with set of membranes is considered as object of investigation. On the base of membrane technology manipulated variables are defined as inputs to digital twin model. Some theoretical results as well as real references of model’s engine calculations are reflected in the paper. Details of technical dashboards to visualize calculated results of running model based on manipulated variables are presented including monthly key performance indicators report dashboard, process flow diagram dashboard and high-level management dashboard. Paper also demonstrates data flow between digital twin model and real process unit and also inside digital twin model.",multimedia,15,included
http://arxiv.org/abs/2201.10369v1,to_check,arxiv,arxiv,2022-01-25 00:00:00,arxiv,winograd convolution for deep neural networks: efficient point selection,http://arxiv.org/abs/2201.10369v1,"Convolutional neural networks (CNNs) have dramatically improved the accuracy
of tasks such as object recognition, image segmentation and interactive speech
systems. CNNs require large amounts of computing resources because
ofcomputationally intensive convolution layers. Fast convolution algorithms
such as Winograd convolution can greatly reduce the computational cost of these
layers at a cost of poor numeric properties, such that greater savings in
computation exponentially increase floating point errors.
  A defining feature of each Winograd convolution algorithm is a set of
real-value points where polynomials are sampled. The choice of points impacts
the numeric accuracy of the algorithm, but the optimal set of points for small
convolutions remains unknown. Existing work considers only small integers and
simple fractions as candidate points. In this work, we propose a novel approach
to point selection using points of the form {-1/c , -c, c, 1/c } using the full
range of real-valued numbers for c. We show that groups of this form cause
cancellations in the Winograd transform matrices that reduce numeric error. We
find empirically that the error for different values of c forms a rough curve
across the range of real-value numbers helping to localize the values of c that
reduce error and that lower errors can be achieved with non-obvious real-valued
evaluation points instead of integers or simple fractions. We study a range of
sizes for small convolutions and achieve reduction in error ranging from 2% to
around 59% for both 1D and 2D convolution. Furthermore, we identify patterns in
cases when we select a subset of our proposed points which will always lead to
a lower error. Finally we implement a complete Winograd convolution layer and
use it to run deep convolution neural networks on real datasets and show that
our proposed points reduce error, ranging from 22% to 63%.",multimedia,16,not included
http://arxiv.org/abs/2201.09550v1,to_check,arxiv,arxiv,2022-01-24 00:00:00,arxiv,crowd tracking and monitoring middleware via map-reduce,http://arxiv.org/abs/2201.09550v1,"This paper presents the design, implementation, and operation of a novel
distributed fault-tolerant middleware. It uses interconnected WSNs that
implement the Map-Reduce paradigm, consisting of several low-cost and low-power
mini-computers (Raspberry Pi). Specifically, we explain the steps for the
development of a novice, fault-tolerant Map-Reduce algorithm which achieves
high system availability, focusing on network connectivity. Finally, we
showcase the use of the proposed system based on simulated data for crowd
monitoring in a real case scenario, i.e., a historical building in Greece (M.
Hatzidakis' residence).The technical novelty of this article lies in presenting
a viable low-cost and low-power solution for crowd sensing without using
complex and resource-intensive AI structures or image and video recognition
techniques.",multimedia,17,included
10.1109/tifs.2021.3131026,to_check,IEEE Transactions on Information Forensics and Security,IEEE,2000-01-01 00:00:00,ieeexplore,poligraph: intrusion-tolerant and distributed fake news detection system,https://ieeexplore.ieee.org/document/9627681/,"We present Poligraph, an intrusion-tolerant and decentralized fake news detection system. Poligraph aims to address architectural, system, technical, and social challenges of building a practical, long-term fake news detection platform. We first conduct a case study for fake news detection at authors’ institute, showing that machine learning-based reviews are less accurate but timely, while human reviews, in particular, experts reviews, are more accurate but time-consuming. This justifies the need for combining both approaches. At the core of Poligraph is two-layer consensus allowing seamlessly combining machine learning techniques and human expert determination. We construct the two-layer consensus using Byzantine fault-tolerant (BFT) and asynchronous threshold common coin protocols. We prove the correctness of our system in terms of conventional definitions of security in distributed systems (agreement, total order, and liveness) as well as new review validity (capturing the accuracy of news reviews). We also provide theoretical foundations on parameter selection for our system. We implement Poligraph and evaluate its performance on Amazon EC2 using a variety of news from online publications and social media. We demonstrate Poligraph achieves throughput of more than 5,000 transactions per second and latency as low as 0.05 second. The throughput of Poligraph is only marginally (<inline-formula> <tex-math notation=""LaTeX"">${4\%}$ </tex-math></inline-formula>–<inline-formula> <tex-math notation=""LaTeX"">${7\%}$ </tex-math></inline-formula>) slower than that of an unreplicated, single-server implementation. In addition, we conduct a real-world case study for the review of fake and real news among both experts and non-experts, which validates the practicality of our approach.",science,18,included
http://arxiv.org/abs/2202.10335v1,to_check,arxiv,arxiv,2022-02-21 00:00:00,arxiv,explainability in machine learning: a pedagogical perspective,http://arxiv.org/abs/2202.10335v1,"Given the importance of integrating of explainability into machine learning,
at present, there are a lack of pedagogical resources exploring this.
Specifically, we have found a need for resources in explaining how one can
teach the advantages of explainability in machine learning. Often pedagogical
approaches in the field of machine learning focus on getting students prepared
to apply various models in the real world setting, but much less attention is
given to teaching students the various techniques one could employ to explain a
model's decision-making process. Furthermore, explainability can benefit from a
narrative structure that aids one in understanding which techniques are
governed by which questions about the data.
  We provide a pedagogical perspective on how to structure the learning process
to better impart knowledge to students and researchers in machine learning,
when and how to implement various explainability techniques as well as how to
interpret the results. We discuss a system of teaching explainability in
machine learning, by exploring the advantages and disadvantages of various
opaque and transparent machine learning models, as well as when to utilize
specific explainability techniques and the various frameworks used to structure
the tools for explainability. Among discussing concrete assignments, we will
also discuss ways to structure potential assignments to best help students
learn to use explainability as a tool alongside any given machine learning
application.
  Data science professionals completing the course will have a birds-eye view
of a rapidly developing area and will be confident to deploy machine learning
more widely. A preliminary analysis on the effectiveness of a recently
delivered course following the structure presented here is included as evidence
supporting our pedagogical approach.",science,19,not included
10.1016/j.comcom.2021.11.011,to_check,Computer Communications,scopus,2022-02-01,sciencedirect,ran energy efficiency and failure rate through ann traffic predictions processing,https://api.elsevier.com/content/abstract/scopus_id/85120657977,"In this paper, we focus on the application of ML tools to resource management in a portion of a Radio Access Network (RAN) and, in particular, to Base Station (BS) activation and deactivation, aiming at reducing energy consumption while providing enough capacity to satisfy the variable traffic demand generated by end users. In order to properly decide on BS (de)activation, traffic predictions are needed, and Artificial Neural Networks (ANN) are used for this purpose. Since critical BS (de)activation decisions are not taken in proximity of minima and maxima of the traffic patterns, high accuracy in the traffic estimation is not required at those times, but only close to the times when a decision is taken. This calls for careful processing of the ANN traffic predictions to increase the probability of correct decision. Numerical performance results in terms of energy saving and traffic lost due to incorrect BS deactivations are obtained by simulating algorithms for traffic predictions processing, using real traffic as input. Results suggest that good performance trade-offs can be achieved even in presence of non-negligible traffic prediction errors, if these forecasts are properly processed. The impact of forecast processing for dynamic resource allocation on the BS failure rate is also investigated. Results reveal that conservative approaches better prevent BSs from hardware failure. Nevertheless, the deployment of newer devices, designed for fast dynamic networks, allows the adoption of approaches which frequently activate and deactivate BSs, thus achieving higher energy saving.",science,20,not included
10.1109/tro.2021.3084374,to_check,IEEE Transactions on Robotics,IEEE,2022-02-01 00:00:00,ieeexplore,cat-like jumping and landing of legged robots in low gravity using deep reinforcement learning,https://ieeexplore.ieee.org/document/9453856/,"In this article, we show that learned policies can be applied to solve legged locomotion control tasks with extensive flight phases, such as those encountered in space exploration. Using an off-the-shelf deep reinforcement learning algorithm, we train a neural network to control a jumping quadruped robot while solely using its limbs for attitude control. We present tasks of increasing complexity leading to a combination of 3-D (re)orientation and landing locomotion behaviors of a quadruped robot traversing simulated low-gravity celestial bodies. We show that our approach easily generalizes across these tasks and successfully trains policies for each case. Using sim-to-real transfer, we deploy trained policies in the real world on the SpaceBok robot placed on an experimental testbed designed for 2-D microgravity experiments. The experimental results demonstrate that repetitive controlled jumping and landing with natural agility is possible.",robotics,21,included
10.1109/lra.2021.3129136,to_check,IEEE Robotics and Automation Letters,IEEE,2022-01-01 00:00:00,ieeexplore,ocrtoc: a cloud-based competition and benchmark for robotic grasping and manipulation,https://ieeexplore.ieee.org/document/9619915/,"In this paper, we propose a cloud-based benchmark for robotic grasping and manipulation, called the OCRTOC benchmark. The benchmark focuses on the object rearrangement problem, specifically table organization tasks. We provide a set of identical real robot setups and facilitate remote experiments of standardized table organization scenarios in varying difficulties. In this workflow, users upload their solutions to our remote server and their code is executed on the real robot setups and scored automatically. After each execution, the OCRTOC team resets the experimental setup manually. We also provide a simulation environment that researchers can use to develop and test their solutions. With the OCRTOC benchmark, we aim to lower the barrier of conducting reproducible research on robotic grasping and manipulation and accelerate progress in this field. Executing standardized scenarios on identical real robot setups allows us to quantify algorithm performances and achieve fair comparisons. Using this benchmark we held a competition in the 2020 International Conference on Intelligence Robots and Systems (IROS 2020). In total, 59 teams took part in this competition worldwide. We present the results and our observations of the 2020 competition, and discuss our adjustments and improvements for the upcoming OCRTOC 2021 competition. The homepage of the OCRTOC competition is <uri>www.ocrtoc.org</uri>, and the OCRTOC software package is available at <uri>https://github.com/OCRTOC/OCRTOC_software_package</uri>.",robotics,22,not included
10.1109/lra.2022.3143289,to_check,IEEE Robotics and Automation Letters,IEEE,2022-04-01 00:00:00,ieeexplore,visuotactile 6d pose estimation of an in-hand object using vision and tactile sensor data,https://ieeexplore.ieee.org/document/9682507/,"Knowledge of the 6D pose of an object can benefit in-hand object manipulation. Existing 6D pose estimation methods use vision data. In-hand 6D object pose estimation is challenging because of heavy occlusion produced by the robot’s grippers, which can have an adverse effect on methods that rely on vision data only. Many robots are equipped with tactile sensors at their fingertips that could be used to complement vision data. In this letter, we present a method that uses both tactile and vision data to estimate the pose of an object grasped in a robot’s hand.The main challenges of this research include 1) lack of standard representation for tactile sensor data, 2) fusion of sensor data from heterogeneous sources—vision and tactile, and 3) a need for large training datasets. To address these challenges, first, we propose use of point clouds to represent object surfaces that are in contact with the tactile sensor. Second, we present a network architecture based on pixel-wise dense fusion to fuse vision and tactile data to estimate the 6D pose of an object. Third, we extend NVIDIA’s Deep Learning Dataset Synthesizer to produce synthetic photo-realistic vision data and the corresponding tactile point clouds for 11 objects from the YCB Object and Model Set in Unreal Engine 4. We present results of simulated experiments suggesting that using tactile data in addition to vision data improves the 6D pose estimate of an in-hand object. We also present qualitative results of experiments in which we deploy our network on real physical robots showing successful transfer of a network trained on synthetic data to a real system.",robotics,23,included
http://arxiv.org/abs/2201.05753v1,to_check,arxiv,arxiv,2022-01-15 00:00:00,arxiv,"parameter identification and motion control for articulated rigid body
  robots using differentiable position-based dynamics",http://arxiv.org/abs/2201.05753v1,"Simulation modeling of robots, objects, and environments is the backbone for
all model-based control and learning. It is leveraged broadly across dynamic
programming and model-predictive control, as well as data generation for
imitation, transfer, and reinforcement learning. In addition to fidelity, key
features of models in these control and learning contexts are speed, stability,
and native differentiability. However, many popular simulation platforms for
robotics today lack at least one of the features above. More recently,
position-based dynamics (PBD) has become a very popular simulation tool for
modeling complex scenes of rigid and non-rigid object interactions, due to its
speed and stability, and is starting to gain significant interest in robotics
for its potential use in model-based control and learning. Thus, in this paper,
we present a mathematical formulation for coupling position-based dynamics
(PBD) simulation and optimal robot design, model-based motion control and
system identification. Our framework breaks down PBD definitions and
derivations for various types of joint-based articulated rigid bodies. We
present a back-propagation method with automatic differentiation, which can
integrate both positional and angular geometric constraints. Our framework can
critically provide the native gradient information and perform gradient-based
optimization tasks. We also propose articulated joint model representations and
simulation workflow for our differentiable framework. We demonstrate the
capability of the framework in efficient optimal robot design, accurate
trajectory torque estimation and supporting spring stiffness estimation, where
we achieve minor errors. We also implement impedance control in real robots to
demonstrate the potential of our differentiable framework in human-in-the-loop
applications.",robotics,24,included
