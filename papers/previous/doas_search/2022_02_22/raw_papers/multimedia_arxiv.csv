id,updated,published,title,summary,database
http://arxiv.org/abs/2202.08227v1,2022-02-16T18:12:14Z,2022-02-16T18:12:14Z,Ditto: Building Digital Twins of Articulated Objects from Interaction,"Digitizing physical objects into the virtual world has the potential to
unlock new research and applications in embodied AI and mixed reality. This
work focuses on recreating interactive digital twins of real-world articulated
objects, which can be directly imported into virtual environments. We introduce
Ditto to learn articulation model estimation and 3D geometry reconstruction of
an articulated object through interactive perception. Given a pair of visual
observations of an articulated object before and after interaction, Ditto
reconstructs part-level geometry and estimates the articulation model of the
object. We employ implicit neural representations for joint geometry and
articulation modeling. Our experiments show that Ditto effectively builds
digital twins of articulated objects in a category-agnostic way. We also apply
Ditto to real-world objects and deploy the recreated digital twins in physical
simulation. Code and additional results are available at
https://ut-austin-rpl.github.io/Ditto",arxiv
http://arxiv.org/abs/2202.06483v2,2022-02-15T01:54:22Z,2022-02-14T05:16:53Z,BiFSMN: Binary Neural Network for Keyword Spotting,"The deep neural networks, such as the Deep-FSMN, have been widely studied for
keyword spotting (KWS) applications. However, computational resources for these
networks are significantly constrained since they usually run on-call on edge
devices. In this paper, we present BiFSMN, an accurate and extreme-efficient
binary neural network for KWS. We first construct a High-frequency Enhancement
Distillation scheme for the binarization-aware training, which emphasizes the
high-frequency information from the full-precision network's representation
that is more crucial for the optimization of the binarized network. Then, to
allow the instant and adaptive accuracy-efficiency trade-offs at runtime, we
also propose a Thinnable Binarization Architecture to further liberate the
acceleration potential of the binarized network from the topology perspective.
Moreover, we implement a Fast Bitwise Computation Kernel for BiFSMN on ARMv8
devices which fully utilizes registers and increases instruction throughput to
push the limit of deployment efficiency. Extensive experiments show that BiFSMN
outperforms existing binarization methods by convincing margins on various
datasets and is even comparable with the full-precision counterpart (e.g., less
than 3% drop on Speech Commands V1-12). We highlight that benefiting from the
thinnable architecture and the optimized 1-bit implementation, BiFSMN can
achieve an impressive 22.3x speedup and 15.5x storage-saving on real-world edge
hardware.",arxiv
http://arxiv.org/abs/2202.05940v1,2022-02-12T00:18:13Z,2022-02-12T00:18:13Z,Automatic Curriculum Generation for Learning Adaptation in Networking,"As deep reinforcement learning (RL) showcases its strengths in networking and
systems, its pitfalls also come to the public's attention--when trained to
handle a wide range of network workloads and previously unseen deployment
environments, RL policies often manifest suboptimal performance and poor
generalizability.
  To tackle these problems, we present Genet, a new training framework for
learning better RL-based network adaptation algorithms. Genet is built on the
concept of curriculum learning, which has proved effective against similar
issues in other domains where RL is extensively employed. At a high level,
curriculum learning gradually presents more difficult environments to the
training, rather than choosing them randomly, so that the current RL model can
make meaningful progress in training. However, applying curriculum learning in
networking is challenging because it remains unknown how to measure the
""difficulty"" of a network environment.
  Instead of relying on handcrafted heuristics to determine the environment's
difficulty level, our insight is to utilize traditional rule-based (non-RL)
baselines: If the current RL model performs significantly worse in a network
environment than the baselines, then the model's potential to improve when
further trained in this environment is substantial. Therefore, Genet
automatically searches for the environments where the current model falls
significantly behind a traditional baseline scheme and iteratively promotes
these environments as the training progresses. Through evaluating Genet on
three use cases--adaptive video streaming, congestion control, and load
balancing, we show that Genet produces RL policies which outperform both
regularly trained RL policies and traditional baselines in each context, not
only under synthetic workloads but also in real environments.",arxiv
http://arxiv.org/abs/2202.05811v1,2022-02-11T18:21:18Z,2022-02-11T18:21:18Z,Overhead Image Factors for Underwater Sonar-based SLAM,"Simultaneous localization and mapping (SLAM) is a critical capability for any
autonomous underwater vehicle (AUV). However, robust, accurate state estimation
is still a work in progress when using low-cost sensors. We propose enhancing a
typical low-cost sensor package using widely available and often free prior
information; overhead imagery. Given an AUV's sonar image and a partially
overlapping, globally-referenced overhead image, we propose using a
convolutional neural network (CNN) to generate a synthetic overhead image
predicting the above-surface appearance of the sonar image contents. We then
use this synthetic overhead image to register our observations to the provided
global overhead image. Once registered, the transformation is introduced as a
factor into a pose SLAM factor graph. We use a state-of-the-art simulation
environment to perform validation over a series of benchmark trajectories and
quantitatively show the improved accuracy of robot state estimation using the
proposed approach. We also show qualitative outcomes from a real AUV field
deployment. Video attachment: https://youtu.be/_uWljtp58ks",arxiv
http://arxiv.org/abs/2202.04971v1,2022-02-10T12:03:00Z,2022-02-10T12:03:00Z,"ASRPU: A Programmable Accelerator for Low-Power Automatic Speech
  Recognition","The outstanding accuracy achieved by modern Automatic Speech Recognition
(ASR) systems is enabling them to quickly become a mainstream technology. ASR
is essential for many applications, such as speech-based assistants, dictation
systems and real-time language translation. However, highly accurate ASR
systems are computationally expensive, requiring on the order of billions of
arithmetic operations to decode each second of audio, which conflicts with a
growing interest in deploying ASR on edge devices. On these devices, hardware
acceleration is key for achieving acceptable performance. However, ASR is a
rich and fast-changing field, and thus, any overly specialized hardware
accelerator may quickly become obsolete.
  In this paper, we tackle those challenges by proposing ASRPU, a programmable
accelerator for on-edge ASR. ASRPU contains a pool of general-purpose cores
that execute small pieces of parallel code. Each of these programs computes one
part of the overall decoder (e.g. a layer in a neural network). The accelerator
automates some carefully chosen parts of the decoder to simplify the
programming without sacrificing generality. We provide an analysis of a modern
ASR system implemented on ASRPU and show that this architecture can achieve
real-time decoding with a very low power budget.",arxiv
http://arxiv.org/abs/2202.04834v1,2022-02-10T04:47:47Z,2022-02-10T04:47:47Z,"Geometric Digital Twinning of Industrial Facilities: Retrieval of
  Industrial Shapes","This paper devises, implements and benchmarks a novel shape retrieval method
that can accurately match individual labelled point clusters (instances) of
existing industrial facilities with their respective CAD models. It employs a
combination of image and point cloud deep learning networks to classify and
match instances to their geometrically similar CAD model. It extends our
previous research on geometric digital twin generation from point cloud data,
which currently is a tedious, manual process. Experiments with our joint
network reveal that it can reliably retrieve CAD models at 85.2\% accuracy. The
proposed research is a fundamental framework to enable the geometric Digital
Twin (gDT) pipeline and incorporate the real geometric configuration into the
Digital Twin.",arxiv
http://arxiv.org/abs/2202.02656v1,2022-02-05T23:27:46Z,2022-02-05T23:27:46Z,A survey of top-down approaches for human pose estimation,"Human pose estimation in two-dimensional images videos has been a hot topic
in the computer vision problem recently due to its vast benefits and potential
applications for improving human life, such as behaviors recognition, motion
capture and augmented reality, training robots, and movement tracking. Many
state-of-the-art methods implemented with Deep Learning have addressed several
challenges and brought tremendous remarkable results in the field of human pose
estimation. Approaches are classified into two kinds: the two-step framework
(top-down approach) and the part-based framework (bottom-up approach). While
the two-step framework first incorporates a person detector and then estimates
the pose within each box independently, detecting all body parts in the image
and associating parts belonging to distinct persons is conducted in the
part-based framework. This paper aims to provide newcomers with an extensive
review of deep learning methods-based 2D images for recognizing the pose of
people, which only focuses on top-down approaches since 2016. The discussion
through this paper presents significant detectors and estimators depending on
mathematical background, the challenges and limitations, benchmark datasets,
evaluation metrics, and comparison between methods.",arxiv
http://arxiv.org/abs/2202.02653v1,2022-02-05T22:49:26Z,2022-02-05T22:49:26Z,"Millisecond speed deep learning based proton dose calculation with Monte
  Carlo accuracy","Next generation online and real-time adaptive radiotherapy workflows require
precise particle transport simulations in sub-second times, which is unfeasible
with current analytical pencil beam algorithms (PBA) or stochastic Monte Carlo
(MC) methods. We present a data-driven millisecond speed dose calculation
algorithm (DoTA) accurately predicting the dose deposited by mono-energetic
proton pencil beams for arbitrary energies and patient geometries. Given the
forward-scattering nature of protons, we frame 3D particle transport as
modeling a sequence of 2D geometries in the beam's eye view. DoTA combines
convolutional neural networks extracting spatial features (e.g., tissue and
density contrasts) with a transformer self-attention backbone that routes
information between the sequence of geometry slices and a vector representing
the beam's energy, and is trained to predict low noise MC simulations of proton
beamlets using 80,000 different head and neck, lung, and prostate geometries.
Predicting beamlet doses in 5 ms with a very high gamma pass rate of 99.37%
(1%, 3 mm) compared to the ground truth MC calculations, DoTA significantly
improves upon analytical pencil beam algorithms both in precision and speed.
Offering MC accuracy 100 times faster than PBAs for pencil beams, our model
calculates full treatment plan doses in 10 to 15 s depending on the number of
beamlets, achieving a 99.70% (2%, 2 mm) gamma pass rate across 9 test patients.
Outperforming all previous analytical pencil beam and deep learning based
approaches, DoTA represents a new state of the art in data-driven dose
calculation and can directly compete with the speed of even commercial GPU MC
approaches. Providing the sub-second speed required for adaptive treatments,
straightforward implementations could offer similar benefits to other steps of
the radiotherapy workflow or other modalities such as helium or carbon
treatments.",arxiv
http://arxiv.org/abs/2202.02559v1,2022-02-05T14:12:01Z,2022-02-05T14:12:01Z,"Digital Twin of Wireless Systems: Overview, Taxonomy, Challenges, and
  Opportunities","Future wireless services must be focused on improving the quality of life by
enabling various applications, such as extended reality, brain-computer
interaction, and healthcare. These applications have diverse performance
requirements (e.g., user-defined quality of experience metrics, latency, and
reliability) that are challenging to be fulfilled by existing wireless systems.
To meet the diverse requirements of the emerging applications, the concept of a
digital twin has been recently proposed. A digital twin uses a virtual
representation along with security-related technologies (e.g., blockchain),
communication technologies (e.g., 6G), computing technologies (e.g., edge
computing), and machine learning, so as to enable the smart applications. In
this tutorial, we present a comprehensive overview on digital twins for
wireless systems. First, we present an overview of fundamental concepts (i.e.,
design aspects, high-level architecture, and frameworks) of digital twin of
wireless systems. Second, a comprehensive taxonomy is devised for both
different aspects. These aspects are twins for wireless and wireless for twins.
For the twins for wireless aspect, we consider parameters, such as twin objects
design, prototyping, deployment trends, physical devices design, interface
design, incentive mechanism, twins isolation, and decoupling. On the other
hand, for wireless for twins, parameters such as, twin objects access aspects,
security and privacy, and air interface design are considered. Finally, open
research challenges and opportunities are presented along with causes and
possible solutions.",arxiv
http://arxiv.org/abs/2201.12170v3,2022-02-16T07:33:20Z,2022-01-28T15:11:34Z,"Unsupervised Single-shot Depth Estimation using Perceptual
  Reconstruction","Real-time estimation of actual object depth is a module that is essential to
performing various autonomous system tasks such as 3D reconstruction, scene
understanding and condition assessment of machinery parts. During the last
decade of machine learning, extensive deployment of deep learning methods to
computer vision tasks has yielded approaches that succeed in achieving
realistic depth synthesis out of a simple RGB modality. While most of these
models are based on paired depth data or availability of video sequences and
stereo images, methods for single-view depth synthesis in a fully unsupervised
setting have hardly been explored. This study presents the most recent advances
in the field of generative neural networks, leveraging them to perform fully
unsupervised single-shot depth synthesis. Two generators for RGB-to-depth and
depth-to-RGB transfer are implemented and simultaneously optimized using the
Wasserstein-1 distance and a novel perceptual reconstruction term. To ensure
that the proposed method is plausible, we comprehensively evaluate the models
using industrial surface depth data as well as the Texas 3D Face Recognition
Database and the SURREAL dataset that records body depth. The success observed
in this study suggests the great potential for unsupervised single-shot depth
estimation in real-world applications.",arxiv
http://arxiv.org/abs/2201.10910v1,2022-01-26T12:58:05Z,2022-01-26T12:58:05Z,"A Bayesian Based Deep Unrolling Algorithm for Single-Photon Lidar
  Systems","Deploying 3D single-photon Lidar imaging in real world applications faces
multiple challenges including imaging in high noise environments. Several
algorithms have been proposed to address these issues based on statistical or
learning-based frameworks. Statistical methods provide rich information about
the inferred parameters but are limited by the assumed model correlation
structures, while deep learning methods show state-of-the-art performance but
limited inference guarantees, preventing their extended use in critical
applications. This paper unrolls a statistical Bayesian algorithm into a new
deep learning architecture for robust image reconstruction from single-photon
Lidar data, i.e., the algorithm's iterative steps are converted into neural
network layers. The resulting algorithm benefits from the advantages of both
statistical and learning based frameworks, providing best estimates with
improved network interpretability. Compared to existing learning-based
solutions, the proposed architecture requires a reduced number of trainable
parameters, is more robust to noise and mismodelling effects, and provides
richer information about the estimates including uncertainty measures. Results
on synthetic and real data show competitive results regarding the quality of
the inference and computational complexity when compared to state-of-the-art
algorithms.",arxiv
http://arxiv.org/abs/2201.10369v1,2022-01-25T15:00:54Z,2022-01-25T15:00:54Z,Winograd Convolution for Deep Neural Networks: Efficient Point Selection,"Convolutional neural networks (CNNs) have dramatically improved the accuracy
of tasks such as object recognition, image segmentation and interactive speech
systems. CNNs require large amounts of computing resources because
ofcomputationally intensive convolution layers. Fast convolution algorithms
such as Winograd convolution can greatly reduce the computational cost of these
layers at a cost of poor numeric properties, such that greater savings in
computation exponentially increase floating point errors.
  A defining feature of each Winograd convolution algorithm is a set of
real-value points where polynomials are sampled. The choice of points impacts
the numeric accuracy of the algorithm, but the optimal set of points for small
convolutions remains unknown. Existing work considers only small integers and
simple fractions as candidate points. In this work, we propose a novel approach
to point selection using points of the form {-1/c , -c, c, 1/c } using the full
range of real-valued numbers for c. We show that groups of this form cause
cancellations in the Winograd transform matrices that reduce numeric error. We
find empirically that the error for different values of c forms a rough curve
across the range of real-value numbers helping to localize the values of c that
reduce error and that lower errors can be achieved with non-obvious real-valued
evaluation points instead of integers or simple fractions. We study a range of
sizes for small convolutions and achieve reduction in error ranging from 2% to
around 59% for both 1D and 2D convolution. Furthermore, we identify patterns in
cases when we select a subset of our proposed points which will always lead to
a lower error. Finally we implement a complete Winograd convolution layer and
use it to run deep convolution neural networks on real datasets and show that
our proposed points reduce error, ranging from 22% to 63%.",arxiv
http://arxiv.org/abs/2201.09550v1,2022-01-24T09:48:46Z,2022-01-24T09:48:46Z,Crowd tracking and monitoring middleware via Map-Reduce,"This paper presents the design, implementation, and operation of a novel
distributed fault-tolerant middleware. It uses interconnected WSNs that
implement the Map-Reduce paradigm, consisting of several low-cost and low-power
mini-computers (Raspberry Pi). Specifically, we explain the steps for the
development of a novice, fault-tolerant Map-Reduce algorithm which achieves
high system availability, focusing on network connectivity. Finally, we
showcase the use of the proposed system based on simulated data for crowd
monitoring in a real case scenario, i.e., a historical building in Greece (M.
Hatzidakis' residence).The technical novelty of this article lies in presenting
a viable low-cost and low-power solution for crowd sensing without using
complex and resource-intensive AI structures or image and video recognition
techniques.",arxiv
http://arxiv.org/abs/2201.10947v1,2022-01-22T00:27:21Z,2022-01-22T00:27:21Z,"Enabling Deep Learning on Edge Devices through Filter Pruning and
  Knowledge Transfer","Deep learning models have introduced various intelligent applications to edge
devices, such as image classification, speech recognition, and augmented
reality. There is an increasing need of training such models on the devices in
order to deliver personalized, responsive, and private learning. To address
this need, this paper presents a new solution for deploying and training
state-of-the-art models on the resource-constrained devices. First, the paper
proposes a novel filter-pruning-based model compression method to create
lightweight trainable models from large models trained in the cloud, without
much loss of accuracy. Second, it proposes a novel knowledge transfer method to
enable the on-device model to update incrementally in real time or near real
time using incremental learning on new data and enable the on-device model to
learn the unseen categories with the help of the in-cloud model in an
unsupervised fashion. The results show that 1) our model compression method can
remove up to 99.36% parameters of WRN-28-10, while preserving a Top-1 accuracy
of over 90% on CIFAR-10; 2) our knowledge transfer method enables the
compressed models to achieve more than 90% accuracy on CIFAR-10 and retain good
accuracy on old categories; 3) it allows the compressed models to converge
within real time (three to six minutes) on the edge for incremental learning
tasks; 4) it enables the model to classify unseen categories of data (78.92%
Top-1 accuracy) that it is never trained with.",arxiv
http://arxiv.org/abs/2201.08619v1,2022-01-21T10:11:27Z,2022-01-21T10:11:27Z,"Dangerous Cloaking: Natural Trigger based Backdoor Attacks on Object
  Detectors in the Physical World","Deep learning models have been shown to be vulnerable to recent backdoor
attacks. A backdoored model behaves normally for inputs containing no
attacker-secretly-chosen trigger and maliciously for inputs with the trigger.
To date, backdoor attacks and countermeasures mainly focus on image
classification tasks. And most of them are implemented in the digital world
with digital triggers. Besides the classification tasks, object detection
systems are also considered as one of the basic foundations of computer vision
tasks. However, there is no investigation and understanding of the backdoor
vulnerability of the object detector, even in the digital world with digital
triggers. For the first time, this work demonstrates that existing object
detectors are inherently susceptible to physical backdoor attacks. We use a
natural T-shirt bought from a market as a trigger to enable the cloaking
effect--the person bounding-box disappears in front of the object detector. We
show that such a backdoor can be implanted from two exploitable attack
scenarios into the object detector, which is outsourced or fine-tuned through a
pretrained model. We have extensively evaluated three popular object detection
algorithms: anchor-based Yolo-V3, Yolo-V4, and anchor-free CenterNet. Building
upon 19 videos shot in real-world scenes, we confirm that the backdoor attack
is robust against various factors: movement, distance, angle, non-rigid
deformation, and lighting. Specifically, the attack success rate (ASR) in most
videos is 100% or close to it, while the clean data accuracy of the backdoored
model is the same as its clean counterpart. The latter implies that it is
infeasible to detect the backdoor behavior merely through a validation set. The
averaged ASR still remains sufficiently high to be 78% in the transfer learning
attack scenarios evaluated on CenterNet. See the demo video on
https://youtu.be/Q3HOF4OobbY.",arxiv
http://arxiv.org/abs/2201.08197v1,2022-01-20T14:23:50Z,2022-01-20T14:23:50Z,"Enhancement or Super-Resolution: Learning-based Adaptive Video Streaming
  with Client-Side Video Processing","The rapid development of multimedia and communication technology has resulted
in an urgent need for high-quality video streaming. However, robust video
streaming under fluctuating network conditions and heterogeneous client
computing capabilities remains a challenge. In this paper, we consider an
enhancement-enabled video streaming network under a time-varying wireless
network and limited computation capacity. ""Enhancement"" means that the client
can improve the quality of the downloaded video segments via image processing
modules. We aim to design a joint bitrate adaptation and client-side
enhancement algorithm toward maximizing the quality of experience (QoE). We
formulate the problem as a Markov decision process (MDP) and propose a deep
reinforcement learning (DRL)-based framework, named ENAVS. As video streaming
quality is mainly affected by video compression, we demonstrate that the video
enhancement algorithm outperforms the super-resolution algorithm in terms of
signal-to-noise ratio and frames per second, suggesting a better solution for
client processing in video streaming. Ultimately, we implement ENAVS and
demonstrate extensive testbed results under real-world bandwidth traces and
videos. The simulation shows that ENAVS is capable of delivering 5%-14% more
QoE under the same bandwidth and computing power conditions as conventional ABR
streaming.",arxiv
http://arxiv.org/abs/2201.08102v2,2022-01-21T16:10:14Z,2022-01-20T10:26:34Z,Safe Deep RL in 3D Environments using Human Feedback,"Agents should avoid unsafe behaviour during both training and deployment.
This typically requires a simulator and a procedural specification of unsafe
behaviour. Unfortunately, a simulator is not always available, and procedurally
specifying constraints can be difficult or impossible for many real-world
tasks. A recently introduced technique, ReQueST, aims to solve this problem by
learning a neural simulator of the environment from safe human trajectories,
then using the learned simulator to efficiently learn a reward model from human
feedback. However, it is yet unknown whether this approach is feasible in
complex 3D environments with feedback obtained from real humans - whether
sufficient pixel-based neural simulator quality can be achieved, and whether
the human data requirements are viable in terms of both quantity and quality.
In this paper we answer this question in the affirmative, using ReQueST to
train an agent to perform a 3D first-person object collection task using data
entirely from human contractors. We show that the resulting agent exhibits an
order of magnitude reduction in unsafe behaviour compared to standard
reinforcement learning.",arxiv
http://arxiv.org/abs/2201.07312v1,2022-01-18T21:01:51Z,2022-01-18T21:01:51Z,Model-driven Cluster Resource Management for AI Workloads in Edge Clouds,"Since emerging edge applications such as Internet of Things (IoT) analytics
and augmented reality have tight latency constraints, hardware AI accelerators
have been recently proposed to speed up deep neural network (DNN) inference run
by these applications. Resource-constrained edge servers and accelerators tend
to be multiplexed across multiple IoT applications, introducing the potential
for performance interference between latency-sensitive workloads. In this
paper, we design analytic models to capture the performance of DNN inference
workloads on shared edge accelerators, such as GPU and edgeTPU, under different
multiplexing and concurrency behaviors. After validating our models using
extensive experiments, we use them to design various cluster resource
management algorithms to intelligently manage multiple applications on edge
accelerators while respecting their latency constraints. We implement a
prototype of our system in Kubernetes and show that our system can host 2.3X
more DNN applications in heterogeneous multi-tenant edge clusters with no
latency violations when compared to traditional knapsack hosting algorithms.",arxiv
http://arxiv.org/abs/2201.07232v1,2022-01-18T17:17:36Z,2022-01-18T17:17:36Z,"Real-time X-ray Phase-contrast Imaging Using SPINNet -- A Speckle-based
  Phase-contrast Imaging Neural Network","X-ray phase-contrast imaging has become indispensable for visualizing samples
with low absorption contrast. In this regard, speckle-based techniques have
shown significant advantages in spatial resolution, phase sensitivity, and
implementation flexibility compared with traditional methods. However, their
computational cost has hindered their wider adoption. By exploiting the power
of deep learning, we developed a new speckle-based phase-contrast imaging
neural network (SPINNet) that boosts the phase retrieval speed by at least two
orders of magnitude compared to existing methods. To achieve this performance,
we combined SPINNet with a novel coded-mask-based technique, an enhanced
version of the speckle-based method. Using this scheme, we demonstrate a
simultaneous reconstruction of absorption and phase images on the order of 100
ms, where a traditional correlation-based analysis would take several minutes
even with a cluster. In addition to significant improvement in speed, our
experimental results show that the imaging resolution and phase retrieval
quality of SPINNet outperform existing single-shot speckle-based methods.
Furthermore, we successfully demonstrate its application in 3D X-ray
phase-contrast tomography. Our result shows that SPINNet could enable many
applications requiring high-resolution and fast data acquisition and
processing, such as in-situ and in-operando 2D and 3D phase-contrast imaging
and real-time at-wavelength metrology and wavefront sensing.",arxiv
http://arxiv.org/abs/2201.10978v1,2022-01-15T02:33:59Z,2022-01-15T02:33:59Z,Machine Learning for Food Review and Recommendation,"Food reviews and recommendations have always been important for online food
service websites. However, reviewing and recommending food is not simple as it
is likely to be overwhelmed by disparate contexts and meanings. In this paper,
we use different deep learning approaches to address the problems of sentiment
analysis, automatic review tag generation, and retrieval of food reviews. We
propose to develop a web-based food review system at Nanyang Technological
University (NTU) named NTU Food Hunter, which incorporates different deep
learning approaches that help users with food selection. First, we implement
the BERT and LSTM deep learning models into the system for sentiment analysis
of food reviews. Then, we develop a Part-of-Speech (POS) algorithm to
automatically identify and extract adjective-noun pairs from the review content
for review tag generation based on POS tagging and dependency parsing. Finally,
we also train a RankNet model for the re-ranking of the retrieval results to
improve the accuracy in our Solr-based food reviews search system. The
experimental results show that our proposed deep learning approaches are
promising for the applications of real-world problems.",arxiv
http://arxiv.org/abs/2201.06912v1,2022-01-14T17:41:26Z,2022-01-14T17:41:26Z,Digital Twin: From Concept to Practice,"Recent technological developments and advances in Artificial Intelligence
(AI) have enabled sophisticated capabilities to be a part of Digital Twin (DT),
virtually making it possible to introduce automation into all aspects of work
processes. Given these possibilities that DT can offer, practitioners are
facing increasingly difficult decisions regarding what capabilities to select
while deploying a DT in practice. The lack of research in this field has not
helped either. It has resulted in the rebranding and reuse of emerging
technological capabilities like prediction, simulation, AI, and Machine
Learning (ML) as necessary constituents of DT. Inappropriate selection of
capabilities in a DT can result in missed opportunities, strategic
misalignments, inflated expectations, and risk of it being rejected as just
hype by the practitioners. To alleviate this challenge, this paper proposes the
digitalization framework, designed and developed by following a Design Science
Research (DSR) methodology over a period of 18 months. The framework can help
practitioners select an appropriate level of sophistication in a DT by weighing
the pros and cons for each level, deciding evaluation criteria for the digital
twin system, and assessing the implications of the selected DT on the
organizational processes and strategies, and value creation. Three real-life
case studies illustrate the application and usefulness of the framework.",arxiv
http://arxiv.org/abs/2201.05184v1,2022-01-13T19:31:35Z,2022-01-13T19:31:35Z,"Achieving AI-enabled Robust End-to-End Quality of Experience over Radio
  Access Networks","Emerging applications such as Augmented Reality, the Internet of Vehicles and
Remote Surgery require both computing and networking functions working in
harmony. The End-to-end (E2E) quality of experience (QoE) for these
applications depends on the synchronous allocation of networking and computing
resources. However, the relationship between the resources and the E2E QoE
outcomes is typically stochastic and non-linear. In order to make efficient
resource allocation decisions, it is essential to model these relationships.
This article presents a novel machine-learning based approach to learn these
relationships and concurrently orchestrate both resources for this purpose. The
machine learning models further help make robust allocation decisions regarding
stochastic variations and simplify robust optimization to a conventional
constrained optimization. When resources are insufficient to accommodate all
application requirements, our framework supports executing some of the
applications with minimal degradation (graceful degradation) of E2E QoE. We
also show how we can implement the learning and optimization methods in a
distributed fashion by the Software-Defined Network (SDN) and Kubernetes
technologies. Our results show that deep learning-based modelling achieves E2E
QoE with approximately 99.8\% accuracy, and our robust joint-optimization
technique allocates resources efficiently when compared to existing
differential services alternatives.",arxiv
http://arxiv.org/abs/2201.04833v1,2022-01-13T08:33:53Z,2022-01-13T08:33:53Z,"SnapshotNet: Self-supervised Feature Learning for Point Cloud Data
  Segmentation Using Minimal Labeled Data","Manually annotating complex scene point cloud datasets is both costly and
error-prone. To reduce the reliance on labeled data, a new model called
SnapshotNet is proposed as a self-supervised feature learning approach, which
directly works on the unlabeled point cloud data of a complex 3D scene. The
SnapshotNet pipeline includes three stages. In the snapshot capturing stage,
snapshots, which are defined as local collections of points, are sampled from
the point cloud scene. A snapshot could be a view of a local 3D scan directly
captured from the real scene, or a virtual view of such from a large 3D point
cloud dataset. Snapshots could also be sampled at different sampling rates or
fields of view (FOVs), thus multi-FOV snapshots, to capture scale information
from the scene. In the feature learning stage, a new pre-text task called
multi-FOV contrasting is proposed to recognize whether two snapshots are from
the same object or not, within the same FOV or across different FOVs. Snapshots
go through two self-supervised learning steps: the contrastive learning step
with both part and scale contrasting, followed by a snapshot clustering step to
extract higher level semantic features. Then a weakly-supervised segmentation
stage is implemented by first training a standard SVM classifier on the learned
features with a small fraction of labeled snapshots. The trained SVM is used to
predict labels for input snapshots and predicted labels are converted into
point-wise label assignments for semantic segmentation of the entire scene
using a voting procedure. The experiments are conducted on the Semantic3D
dataset and the results have shown that the proposed method is capable of
learning effective features from snapshots of complex scene data without any
labels. Moreover, the proposed method has shown advantages when comparing to
the SOA method on weakly-supervised point cloud semantic segmentation.",arxiv
http://arxiv.org/abs/2201.04349v1,2022-01-12T07:49:46Z,2022-01-12T07:49:46Z,Video Intelligence as a component of a Global Security system,"This paper describes the evolution of our research from video analytics to a
global security system with focus on the video surveillance component. Indeed
video surveillance has evolved from a commodity security tool up to the most
efficient way of tracking perpetrators when terrorism hits our modern urban
centers. As number of cameras soars, one could expect the system to leverage
the huge amount of data carried through the video streams to provide fast
access to video evidences, actionable intelligence for monitoring real-time
events and enabling predictive capacities to assist operators in their
surveillance tasks. This research explores a hybrid platform for video
intelligence capture, automated data extraction, supervised Machine Learning
for intelligently assisted urban video surveillance; Extension to other
components of a global security system are discussed. Applying Knowledge
Management principles in this research helps with deep problem understanding
and facilitates the implementation of efficient information and experience
sharing decision support systems providing assistance to people on the field as
well as in operations centers. The originality of this work is also the
creation of ""common"" human-machine and machine to machine language and a
security ontology.",arxiv
http://arxiv.org/abs/2201.04195v1,2022-01-11T20:55:19Z,2022-01-11T20:55:19Z,Matching-based Service Offloading for Compute-less Driven IoT Networks,"With the advent of the Internet of Things (IoT) and 5G networks, edge
computing is offering new opportunities for business model and use cases
innovations. Service providers can now virtualize the cloud beyond the data
center to meet the latency, data sovereignty, reliability, and interoperability
requirements. Yet, many new applications (e.g., augmented reality, virtual
reality, artificial intelligence) are computation-intensive and
delay-sensitivity. These applications are invoked heavily with similar inputs
that could lead to the same output. Compute-less networks aim to implement a
network with a minimum amount of computation and communication. This can be
realized by offloading prevalent services to the edge and thus minimizing
communication in the core network and eliminating redundant computations using
the computation reuse concept. In this paper, we present matching-based
services offloading schemes for compute-less IoT networks. We adopt the
matching theory to match service offloading to the appropriate edge server(s).
Specifically, we design, WHISTLE, a vertical many-to-many offloading scheme
that aims to offload the most invoked and highly reusable services to the
appropriate edge servers. We further extend WHISTLE to provide horizontal
one-to-many computation reuse sharing among edge servers which leads to
bouncing less computation back to the cloud. We evaluate the efficiency and
effectiveness of WHISTLE with a real-world dataset. The obtained findings show
that WHISTLE is able to accelerate the tasks completion time by 20%, reduce the
computation up to 77%, and decrease the communication up to 71%. Theoretical
analyses also prove the stability of the designed schemes.",arxiv
http://arxiv.org/abs/2201.04014v2,2022-01-13T10:00:35Z,2022-01-11T15:53:53Z,Captcha Attack: Turning Captchas Against Humanity,"Nowadays, people generate and share massive content on online platforms
(e.g., social networks, blogs). In 2021, the 1.9 billion daily active Facebook
users posted around 150 thousand photos every minute. Content moderators
constantly monitor these online platforms to prevent the spreading of
inappropriate content (e.g., hate speech, nudity images). Based on deep
learning (DL) advances, Automatic Content Moderators (ACM) help human
moderators handle high data volume. Despite their advantages, attackers can
exploit weaknesses of DL components (e.g., preprocessing, model) to affect
their performance. Therefore, an attacker can leverage such techniques to
spread inappropriate content by evading ACM.
  In this work, we propose CAPtcha Attack (CAPA), an adversarial technique that
allows users to spread inappropriate text online by evading ACM controls. CAPA,
by generating custom textual CAPTCHAs, exploits ACM's careless design
implementations and internal procedures vulnerabilities. We test our attack on
real-world ACM, and the results confirm the ferocity of our simple yet
effective attack, reaching up to a 100% evasion success in most cases. At the
same time, we demonstrate the difficulties in designing CAPA mitigations,
opening new challenges in CAPTCHAs research area.",arxiv
http://arxiv.org/abs/2201.03808v1,2022-01-11T06:48:12Z,2022-01-11T06:48:12Z,MobileFaceSwap: A Lightweight Framework for Video Face Swapping,"Advanced face swapping methods have achieved appealing results. However, most
of these methods have many parameters and computations, which makes it
challenging to apply them in real-time applications or deploy them on edge
devices like mobile phones. In this work, we propose a lightweight
Identity-aware Dynamic Network (IDN) for subject-agnostic face swapping by
dynamically adjusting the model parameters according to the identity
information. In particular, we design an efficient Identity Injection Module
(IIM) by introducing two dynamic neural network techniques, including the
weights prediction and weights modulation. Once the IDN is updated, it can be
applied to swap faces given any target image or video. The presented IDN
contains only 0.50M parameters and needs 0.33G FLOPs per frame, making it
capable for real-time video face swapping on mobile phones. In addition, we
introduce a knowledge distillation-based method for stable training, and a loss
reweighting module is employed to obtain better synthesized results. Finally,
our method achieves comparable results with the teacher models and other
state-of-the-art methods.",arxiv
http://arxiv.org/abs/2201.03804v1,2022-01-11T06:32:12Z,2022-01-11T06:32:12Z,"CI-AVSR: A Cantonese Audio-Visual Speech Dataset for In-car Command
  Recognition","With the rise of deep learning and intelligent vehicle, the smart assistant
has become an essential in-car component to facilitate driving and provide
extra functionalities. In-car smart assistants should be able to process
general as well as car-related commands and perform corresponding actions,
which eases driving and improves safety. However, there is a data scarcity
issue for low resource languages, hindering the development of research and
applications. In this paper, we introduce a new dataset, Cantonese In-car
Audio-Visual Speech Recognition (CI-AVSR), for in-car command recognition in
the Cantonese language with both video and audio data. It consists of 4,984
samples (8.3 hours) of 200 in-car commands recorded by 30 native Cantonese
speakers. Furthermore, we augment our dataset using common in-car background
noises to simulate real environments, producing a dataset 10 times larger than
the collected one. We provide detailed statistics of both the clean and the
augmented versions of our dataset. Moreover, we implement two multimodal
baselines to demonstrate the validity of CI-AVSR. Experiment results show that
leveraging the visual signal improves the overall performance of the model.
Although our best model can achieve a considerable quality on the clean test
set, the speech recognition quality on the noisy data is still inferior and
remains as an extremely challenging task for real in-car speech recognition
systems. The dataset and code will be released at
https://github.com/HLTCHKUST/CI-AVSR.",arxiv
http://arxiv.org/abs/2201.03335v2,2022-01-24T02:56:19Z,2022-01-10T13:29:05Z,"DeepKE: A Deep Learning Based Knowledge Extraction Toolkit for Knowledge
  Base Population","We present a new open-source and extensible knowledge extraction toolkit,
called DeepKE (Deep learning based Knowledge Extraction), supporting standard
fully supervised, low-resource few-shot and document-level scenarios. DeepKE
implements various information extraction tasks, including named entity
recognition, relation extraction and attribute extraction. With a unified
framework, DeepKE allows developers and researchers to customize datasets and
models to extract information from unstructured texts according to their
requirements. Specifically, DeepKE not only provides various functional modules
and model implementation for different tasks and scenarios but also organizes
all components by consistent frameworks to maintain sufficient modularity and
extensibility. Besides, we present an online platform in
http://deepke.zjukg.cn/ for real-time extraction of various tasks. DeepKE has
been equipped with Google Colab tutorials and comprehensive documents for
beginners. We release the source code at https://github.com/zjunlp/DeepKE, with
a demo video.",arxiv
http://arxiv.org/abs/2201.02503v1,2022-01-07T15:42:50Z,2022-01-07T15:42:50Z,"A Review of Deep Learning Techniques for Markerless Human Motion on
  Synthetic Datasets","Markerless motion capture has become an active field of research in computer
vision in recent years. Its extensive applications are known in a great variety
of fields, including computer animation, human motion analysis, biomedical
research, virtual reality, and sports science. Estimating human posture has
recently gained increasing attention in the computer vision community, but due
to the depth of uncertainty and the lack of the synthetic datasets, it is a
challenging task. Various approaches have recently been proposed to solve this
problem, many of which are based on deep learning. They are primarily focused
on improving the performance of existing benchmarks with significant advances,
especially 2D images. Based on powerful deep learning techniques and recently
collected real-world datasets, we explored a model that can predict the
skeleton of an animation based solely on 2D images. Frames generated from
different real-world datasets with synthesized poses using different body
shapes from simple to complex. The implementation process uses DeepLabCut on
its own dataset to perform many necessary steps, then use the input frames to
train the model. The output is an animated skeleton for human movement. The
composite dataset and other results are the ""ground truth"" of the deep model.",arxiv
http://arxiv.org/abs/2201.02279v1,2022-01-06T23:50:09Z,2022-01-06T23:50:09Z,De-rendering 3D Objects in the Wild,"With increasing focus on augmented and virtual reality applications (XR)
comes the demand for algorithms that can lift objects from images and videos
into representations that are suitable for a wide variety of related 3D tasks.
Large-scale deployment of XR devices and applications means that we cannot
solely rely on supervised learning, as collecting and annotating data for the
unlimited variety of objects in the real world is infeasible. We present a
weakly supervised method that is able to decompose a single image of an object
into shape (depth and normals), material (albedo, reflectivity and shininess)
and global lighting parameters. For training, the method only relies on a rough
initial shape estimate of the training objects to bootstrap the learning
process. This shape supervision can come for example from a pretrained depth
network or - more generically - from a traditional structure-from-motion
pipeline. In our experiments, we show that the method can successfully
de-render 2D images into a decomposed 3D representation and generalizes to
unseen object categories. Since in-the-wild evaluation is difficult due to the
lack of ground truth data, we also introduce a photo-realistic synthetic test
set that allows for quantitative evaluation.",arxiv
http://arxiv.org/abs/2201.02028v1,2022-01-06T12:36:35Z,2022-01-06T12:36:35Z,"A Light in the Dark: Deep Learning Practices for Industrial Computer
  Vision","In recent years, large pre-trained deep neural networks (DNNs) have
revolutionized the field of computer vision (CV). Although these DNNs have been
shown to be very well suited for general image recognition tasks, application
in industry is often precluded for three reasons: 1) large pre-trained DNNs are
built on hundreds of millions of parameters, making deployment on many devices
impossible, 2) the underlying dataset for pre-training consists of general
objects, while industrial cases often consist of very specific objects, such as
structures on solar wafers, 3) potentially biased pre-trained DNNs raise legal
issues for companies. As a remedy, we study neural networks for CV that we
train from scratch. For this purpose, we use a real-world case from a solar
wafer manufacturer. We find that our neural networks achieve similar
performances as pre-trained DNNs, even though they consist of far fewer
parameters and do not rely on third-party datasets.",arxiv
http://arxiv.org/abs/2201.01943v1,2022-01-06T07:14:02Z,2022-01-06T07:14:02Z,"Machine Learning: Algorithms, Models, and Applications","Recent times are witnessing rapid development in machine learning algorithm
systems, especially in reinforcement learning, natural language processing,
computer and robot vision, image processing, speech, and emotional processing
and understanding. In tune with the increasing importance and relevance of
machine learning models, algorithms, and their applications, and with the
emergence of more innovative uses cases of deep learning and artificial
intelligence, the current volume presents a few innovative research works and
their applications in real world, such as stock trading, medical and healthcare
systems, and software automation. The chapters in the book illustrate how
machine learning and deep learning algorithms and models are designed,
optimized, and deployed. The volume will be useful for advanced graduate and
doctoral students, researchers, faculty members of universities, practicing
data scientists and data engineers, professionals, and consultants working on
the broad areas of machine learning, deep learning, and artificial
intelligence.",arxiv
http://arxiv.org/abs/2201.01144v2,2022-01-07T15:59:46Z,2022-01-04T14:13:06Z,Digital Twin Network: Opportunities and Challenges,"The proliferation of emergent network applications (e.g., AR/VR, telesurgery,
real-time communications) is increasing the difficulty of managing modern
communication networks. These applications typically have stringent
requirements (e.g., ultra-low deterministic latency), making it more difficult
for network operators to manage their network resources efficiently. In this
article, we propose the Digital Twin Network (DTN) as a key enabler for
efficient network management in modern networks. We describe the general
architecture of the DTN and argue that recent trends in Machine Learning (ML)
enable building a DTN that efficiently and accurately mimics real-world
networks. In addition, we explore the main ML technologies that enable
developing the components of the DTN architecture. Finally, we describe the
open challenges that the research community has to address in the upcoming
years in order to enable the deployment of the DTN in real-world scenarios.",arxiv
http://arxiv.org/abs/2201.00768v1,2022-01-03T17:17:11Z,2022-01-03T17:17:11Z,"Robust Natural Language Processing: Recent Advances, Challenges, and
  Future Directions","Recent natural language processing (NLP) techniques have accomplished high
performance on benchmark datasets, primarily due to the significant improvement
in the performance of deep learning. The advances in the research community
have led to great enhancements in state-of-the-art production systems for NLP
tasks, such as virtual assistants, speech recognition, and sentiment analysis.
However, such NLP systems still often fail when tested with adversarial
attacks. The initial lack of robustness exposed troubling gaps in current
models' language understanding capabilities, creating problems when NLP systems
are deployed in real life. In this paper, we present a structured overview of
NLP robustness research by summarizing the literature in a systemic way across
various dimensions. We then take a deep-dive into the various dimensions of
robustness, across techniques, metrics, embeddings, and benchmarks. Finally, we
argue that robustness should be multi-dimensional, provide insights into
current research, identify gaps in the literature to suggest directions worth
pursuing to address these gaps.",arxiv
http://arxiv.org/abs/2201.00309v1,2022-01-02T06:58:02Z,2022-01-02T06:58:02Z,"Optimizing Machine Learning Inference Queries with Correlative Proxy
  Models","We consider accelerating machine learning (ML) inference queries on
unstructured datasets. Expensive operators such as feature extractors and
classifiers are deployed as user-defined functions(UDFs), which are not
penetrable with classic query optimization techniques such as predicate
push-down. Recent optimization schemes (e.g., Probabilistic Predicates or PP)
assume independence among the query predicates, build a proxy model for each
predicate offline, and rewrite a new query by injecting these cheap proxy
models in the front of the expensive ML UDFs. In such a manner, unlikely inputs
that do not satisfy query predicates are filtered early to bypass the ML UDFs.
We show that enforcing the independence assumption in this context may result
in sub-optimal plans. In this paper, we propose CORE, a query optimizer that
better exploits the predicate correlations and accelerates ML inference
queries. Our solution builds the proxy models online for a new query and
leverages a branch-and-bound search process to reduce the building costs.
Results on three real-world text, image and video datasets show that CORE
improves the query throughput by up to 63% compared to PP and up to 80%
compared to running the queries as it is.",arxiv
