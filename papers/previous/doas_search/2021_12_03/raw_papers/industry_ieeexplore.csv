doi,title,publisher,content_type,abstract,html_url,publication_title,publication_date,database
10.1109/IS.2018.8710526,A Digital Twin-based Privacy Enhancement Mechanism for the Automotive Industry,IEEE,Conferences,"This paper discusses a Digital Twin demonstrator for privacy enhancement in the automotive industry. Here, the Digital Twin demonstrator is presented as a method for the design and implementation of privacy enhancement mechanisms, and is used to detect privacy concerns and minimize breaches and associated risks to which smart car drivers can be exposed through connected infotainment applications and services. The Digital Twin-based privacy enhancement demonstrator is designed to simulate variety of conditions that can occur in the smart car ecosystem. We firstly identify the core stakeholders (actors) in the smart car ecosystem, their roles and exposure to privacy vulnerabilities and associated risks. Secondly, we identify assets that consume and generate sensitive privacy data in smart cars, their functionalities, and relevant privacy concerns and risks. Thirdly, we design an infrastructure for collecting (i) real-time sensor data from smart cars and their assets, and (ii) environmental data, road and traffic data, generated through operational driving lifecycle. In order to ensure compliance of the collected data with privacy policies and regulations, e.g. with GDPR requirements for enforcement of the data subject's rights, we design methods for the Digital Twin-based privacy enhancement demonstrator that are based on behavioural analytics informed by GDPR. We also perform data anonymization to minimize privacy risks and enable actions such as sending an automatic informed consent to the stakeholders.",https://ieeexplore.ieee.org/document/8710526/,2018 International Conference on Intelligent Systems (IS),25-27 Sept. 2018,ieeexplore
10.1109/ICMCCE.2018.00050,A Smart Manufacturing Compliance Architecture of Electronic Batch Recording System (eBRS) for Life Sciences Industry,IEEE,Conferences,"The paradigm shift brought about by smart manufacturing or Industrie 4.0 has posed threefold challenges to electronic batch recording system (eBRS) in Life Sciences Industry: 1) the structure of the data should be informative and standard for interoperate using information models, 2) administration of synchronization between physical world and cyber world for smart decision making and optimization using cyber physical system (CPS) and 3) so-called digital manufacturing operations management (digital MOM) characterized by decentralization, comprehensive collaboration and servitization shall be implemented. Under the new situations of smart manufacturing or Industrie 4.0, the requirements from information models, CPS and digital MOM will become the most principal criteria to be considered for future eBRS/MES and other operations management information system in shop floor. To fulfill these demands, an approach combining ISA95/88 hybrid model with activities ontology and variant domain-driven design for SOA-based eBRS development has been presented. An eBRS software platform has been developed on the theoretical basis and applied to a specific application scenario of Lyophilized Injection Production for verifying its feasibility purpose.",https://ieeexplore.ieee.org/document/8537548/,"2018 3rd International Conference on Mechanical, Control and Computer Engineering (ICMCCE)",14-16 Sept. 2018,ieeexplore
10.1109/PacificVis.2018.00026,A Visual Analytics Approach for Equipment Condition Monitoring in Smart Factories of Process Industry,IEEE,Conferences,"Monitoring equipment conditions is of great value in manufacturing, which can not only reduce unplanned downtime by early detecting anomalies of equipment but also avoid unnecessary routine maintenance. With the coming era of Industry 4.0 (or industrial internet), more and more assets and machines in plants are equipped with various sensors and information systems, which brings an unprecedented opportunity to capture large-scale and fine-grained data for effective on-line equipment condition monitoring. However, due to the lack of systematic methods, analysts still find it challenging to carry out efficient analyses and extract valuable information from the mass volume of data collected, especially for process industry (e.g., a petrochemical plant) with complex manufacturing procedures. In this paper, we report the design and implementation of an interactive visual analytics system, which helps managers and operators at manufacturing sites leverage their domain knowledge and apply substantial human judgements to guide the automated analytical approaches, thus generating understandable and trustable results for real-world applications. Our system integrates advanced analytical algorithms (e.g., Gaussian mixture model with a Bayesian framework) and intuitive visualization designs to provide a comprehensive and adaptive semi-supervised solution to equipment condition monitoring. The example use cases based on a real-world manufacturing dataset and interviews with domain experts demonstrate the effectiveness of our system.",https://ieeexplore.ieee.org/document/8365986/,2018 IEEE Pacific Visualization Symposium (PacificVis),10-13 April 2018,ieeexplore
10.1109/CSSS.2011.5974714,A data prediction algorithm based on BP neural network in telecom industry,IEEE,Conferences,"The data mining technology is more and more widely used in the telecom industry. But telecom data set always includes instances with missing values. Besides, many data mining models are sensitive for the missing value and distortion. Estimating missing values becomes an inherent problem. To address the problem, A prediction method is proposed for the missing value based on the BP neural network and K-means algorithm. The algorithm first clusters the dataset, and selects a certain cluster for the instance with missing values, then we train the BP net using the cluster and get the architecture parameters. When a test instance has a missing value of a certain attribute, we regard the attribute as the aiming attribute and apply the instance to the network, it will produce a prediction value . This paper uses real datasets from the telecom industry as the test datasets. The result shows that the algorithm can be used to predict the missing value of telecom industry with good performance.",https://ieeexplore.ieee.org/document/5974714/,2011 International Conference on Computer Science and Service System (CSSS),27-29 June 2011,ieeexplore
10.1109/SNPDWinter52325.2021.00056,A study on Medical Device Security Status in Medical Convergence Industry,IEEE,Conferences,"Beyond informatization, the development of new technologies and the introduction of innovative technologies in the Fourth Industrial Revolution are the evolution of various industries. In the early information age, the introduction of various information systems has made it possible to manage and utilize large amounts of data in high quality, thus expanding the business of other industries faster and more reliably. However, in certain industries, various devices and equipments are dependent on the existing information system, and thus, time and cost difficulties are introduced to introduce a new system. Accordingly, existing security problems rather than new security flaws are coming to reality. The purpose of this study is to investigate and analyze the security status of existing medical devices according to the arrival of convergence environment in the medical industry.",https://ieeexplore.ieee.org/document/9403515/,"2021 21st ACIS International Winter Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing (SNPD-Winter)",28-30 Jan. 2021,ieeexplore
10.1109/ISPASS51385.2021.00014,AIBench Training: Balanced Industry-Standard AI Training Benchmarking,IEEE,Conferences,"Earlier-stage evaluations of a new AI architecture/system need affordable AI benchmarks. Only using a few AI component benchmarks like MLPerf alone in the other stages may lead to misleading conclusions. Moreover, the learning dynamics are not well understood, and the benchmarks' shelf-life is short. This paper proposes a balanced benchmarking methodology. We use real-world benchmarks to cover the factors space that impacts the learning dynamics to the most considerable extent. After performing an exhaustive survey on Internet service AI domains, we identify and implement nineteen representative AI tasks with state-of-the-art models. For repeatable performance ranking (RPR subset) and workload characterization (WC subset), we keep two subsets to a minimum for affordability. We contribute by far the most comprehensive AI training benchmark suite. The evaluations show: (1) AIBench Training (v1.1) outperforms MLPerf Training (v0.7) in terms of diversity and representativeness of model complexity, computational cost, convergent rate, computation, and memory access patterns, and hotspot functions; (2) Against the AIBench full benchmarks, its RPR subset shortens the benchmarking cost by 64%, while maintaining the primary workload characteristics; (3) The performance ranking shows the single-purpose AI accelerator like TPU with the optimized TensorFlow framework performs better than that of GPUs while losing the latter's general support for various AI models. The specification, source code, and performance numbers are available from the AIBench homepage https://www.benchcouncil.org/aibench-training/index.html.",https://ieeexplore.ieee.org/document/9408170/,2021 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS),28-30 March 2021,ieeexplore
10.1109/BigData.2014.7004408,Advanced planning and control of manufacturing processes in steel industry through big data analytics: Case study and architecture proposal,IEEE,Conferences,"Enterprises in today's globalized world are compelled to react on threats and opportunities in a highly flexible manner. Hence, companies that are able to analyze the current state of their business processes, forecast their most optimal progresses and with this proactively control them will have a decisive competitive advantage. Technological progress in sensor technology has boosted real-time situation awareness, especially in manufacturing operations. The paper at hands examines, based on a case study stemming from the steel manufacturing industry, which production-related data is collectable using state of the art sensors forming a basis for a detailed situation awareness and for deriving accurate forecasts. However, analyses of this data point out that dedicated big data analytics approaches are required to utilize the full potential out of it. By proposing an architecture for predictive process planning and control systems, the paper intends to form a working and discussion basis for further research and implementation efforts in big data analytics.",https://ieeexplore.ieee.org/document/7004408/,2014 IEEE International Conference on Big Data (Big Data),27-30 Oct. 2014,ieeexplore
10.1109/HICSS.2007.52,Agent-based Human-computer-interaction for Real-time Monitoring Systems in the Trucking Industry,IEEE,Conferences,"Auto ID systems can replace time-consuming, costly and error-prone processes of human data entry and produce detailed real time information. However, they would add value only to the extent that data is presented in a user-friendly manner. As model-based decision support is not always adequate, an agent-based approach is often chosen. Real life entities such as orders and trucks are represented by agents, which negotiate in order to solve planning problems. For the respective data representation at least two forms can be distinguished, focusing either on (1) resources (account-based) or (2) orders (order-centric). Applying cognitive fit theory we describe how the different interfaces affect decision making. The hypotheses would be tested in a laboratory experiment. The intended contribution should support that order-centric interfaces have higher user-friendliness and are especially beneficial to low-analytics and planners working under time pressure",https://ieeexplore.ieee.org/document/4076424/,2007 40th Annual Hawaii International Conference on System Sciences (HICSS'07),3-6 Jan. 2007,ieeexplore
10.1109/BDEIM52318.2020.00043,An Empirical Study on the Correlation between Operating Capital Management and Operating Performance-Taking the Communication Terminal and Accessories Industry as an example,IEEE,Conferences,"This article uses Stata16.0 software tools to analyze and research the 2015-2019 company annual reports of listed companies in the communication terminal and accessories industry collected in the RESSET financial research database, and summarizes the impact of short-term operating funds on operating performance in the company's supply, production and sales scenarios, provides reference suggestions and valuable experience for companies of different sizes in the communication terminal and accessories industry and other industries in the use of short-term operating funds, and promotes the sustainable and healthy development of enterprises.",https://ieeexplore.ieee.org/document/9407126/,2020 International Conference on Big Data Economy and Information Management (BDEIM),11-13 Dec. 2020,ieeexplore
10.1109/COASE.2018.8560557,An Industry 4.0 Cyber-Physical Framework for Micro Devices Assembly,IEEE,Conferences,"An advanced cyber manufacturing framework to support the collaborative assembly of micro devices is presented based on Industry 4.0 principles. The distributed cyber and physical components work together to plan, assemble and monitor micro assembly related tasks; micro assembly refers to the assembly of micron sized devices which cannot be manufactured by MEMS technologies. The collaborative framework proposed includes assembly planning and path planning modules, Virtual Reality based assembly simulation environments and physical assembly work cells. An ontology based approach was implemented to address semantic interoperability issues to support formation of temporary partnerships in a Virtual Enterprise context. The key to the design and implementation of this complex framework is an information centric process modeling approach which provides a data/information oriented basis for collaboration. A collaborative cyber physical test bed has been built to demonstrate feasibility of proposed framework and approach.",https://ieeexplore.ieee.org/document/8560557/,2018 IEEE 14th International Conference on Automation Science and Engineering (CASE),20-24 Aug. 2018,ieeexplore
10.1109/AIMSEC.2011.6010784,An improved B&amp;B technique applying to telecom industry,IEEE,Conferences,"The data mining technology is more and more widely used in the telecom industry. When we construct Bayesian Belief Network, the branch and bound technique based on the minimum description length principle (B&amp;B technique) is one of the classical algorithm. To telecom data, high dimensionality and huge volume set obstacle when constructing Bayesian Belief Network. But we utilize the inconspicuous correlation between telecom attributes, improve the process of B&amp;B technique and simplify the structure to solve complexity rooting from telecom data's feature. The algorithm first construct a dependence ordering, then a simplified B&amp;B technique suitable for telecom data is applied. We compare the result and complexity with the original B&amp;B technique. This paper uses real datasets from the telecom industry. The result shows that the new algorithm can construct the network almost the same as the original one, but with good performance.",https://ieeexplore.ieee.org/document/6010784/,"2011 2nd International Conference on Artificial Intelligence, Management Science and Electronic Commerce (AIMSEC)",8-10 Aug. 2011,ieeexplore
10.1109/SYSCON.2018.8369547,An interactive architecture for industrial scale prediction: Industry 4.0 adaptation of machine learning,IEEE,Conferences,"According to wiki definition, there are four design principles in Industry 4.0. These principles support companies in identifying and implementing Industry 4.0 scenarios, namely, Interoperability, Information transparency, Technical assistance, Decentralized decisions. In this paper we have discussed our work on an implementation of a machine learning based interactive architecture for industrial scale prediction for dynamic distribution of water resources across the continent, keeping the four corners of Industry 4.0 in place. We report the possibility of producing most probable high resolution estimation regarding the water balance in any region within Australia by implementation of an intelligent system that can integrate spatial-temporal data from various independent sensors and models, with the ground truth data produced by 250 practitioners from the irrigation industry across Australia. This architectural implementation on a cloud computing platform linked with a freely distributed mobile application, allowing interactive ground truthing of a machine learning model on a continental scale, shows accuracy of 90% with 85% sensitivity of correct surface soil moisture estimation with end users at its complete control. Along with high level of information transparency and interoperability, providing on-demand technical supports and motivating users by allowing them to customize and control their own local predictive models, show the successfulness of principles in Industry 4.0 in real environmental issues in the future adaptation in various industries starting from resource management to modern generation soft robotics.",https://ieeexplore.ieee.org/document/8369547/,2018 Annual IEEE International Systems Conference (SysCon),23-26 April 2018,ieeexplore
10.1109/CIT.2016.18,Analysis of Complex Data in Telecommunications Industry,IEEE,Conferences,"In this paper, we report an application of data analytics in a real world business case of the telecom industry. This work has been tied up with an IT company in India with a large data set of telecom customers. As part of data analytics, the first task was to perform cleansing of bad and missing data, transforming heterogeneous formats into a unified format, semantic analysis on the data (semantics of attributes and their relationships), and then perform unsupervised learning using BIRCH technique of clustering. We describe and discuss the approach and results of these tasks on a sample data in this work.",https://ieeexplore.ieee.org/document/7876322/,2016 IEEE International Conference on Computer and Information Technology (CIT),8-10 Dec. 2016,ieeexplore
10.1109/SHARK-ADI.2007.3,Architectural Knowlege Management Strategies: Approaches in Research and Industry,IEEE,Conferences,"The software architecture community has recently gained an increasing interest in managing architectural knowledge. However, up until now there have been no attempts to obtain an overview of the work in the field. In this paper we present a preliminary review on current approaches to architectural knowledge management. To this end, we compare approaches known from literature and encountered in industry with knowledge management theory. We found that in reports from research and practice there appears to be a preference to use the codification strategy. However, our observations of the software architecture industry show that organizations in general tend to use a personalization strategy unintentionally. This paper serves as a call for awareness of this gap between intention and reality, and questions the biased focus on intentional codification alone. We suggest to close this gap through focusing on hybrid approaches.",https://ieeexplore.ieee.org/document/4273342/,"Second Workshop on Sharing and Reusing Architectural Knowledge - Architecture, Rationale, and Design Intent (SHARK/ADI'07: ICSE Workshops 2007)",20-26 May 2007,ieeexplore
10.1109/PerComWorkshops51409.2021.9431009,Architecture and pervasive platform for machine learning services in Industry 4.0,IEEE,Conferences,"Pervasive computing promotes the integration of smart electronic devices in our living and working spaces in order to provide new, advanced services. Recently many prototype services based on machine learning techniques have been proposed in a number of domains like smart homes, smart buildings or smart plants. However, the number of applications effectively deployed in the real world is still limited. We believe that architectural principles and integrated frameworks are still missing today to successfully and repetitively support application developers and operators. In this paper, we present a novel architecture and a pervasive platform allowing the development of machine learning based applications in smart buildings.",https://ieeexplore.ieee.org/document/9431009/,2021 IEEE International Conference on Pervasive Computing and Communications Workshops and other Affiliated Events (PerCom Workshops),22-26 March 2021,ieeexplore
10.1109/HPCA51647.2021.00071,Ascend: a Scalable and Unified Architecture for Ubiquitous Deep Neural Network Computing : Industry Track Paper,IEEE,Conferences,"Deep neural networks (DNNs) have been successfully applied to a great variety of applications, ranging from small IoT devices to large scale services in a data center. In order to improve the efficiency of processing these DNN models, dedicated hardware accelerators are required for all these scenarios. Theoretically, there exists an optimized acceleration architecture for each application. However, considering the cost of chip design and corresponding tool-chain development, researchers need to trade off between efficiency and generality. In this work, we demonstrate that it is practical to use a unified architecture, called Ascend, to support those applications, ranging from IoT devices to data-center services. We provide a lot of design details to explain that the success of Ascend relies on contributions from different levels. First, heterogeneous computing units are employed to support various DNN models. And the datapath is adapted according to the requirement of computing and data access. Second, when scaling the Ascend architecture from a single core to a cluster containing thousands of cores, it involves design efforts, such as memory hierarchy and system level integration. Third, a multi-tier compiler, which provides flexible choices for developers, is the last critical piece. Experimental results show that using accelerators based on the Ascend architecture can achieve comparable or even better performance in different applications. In addition, various chips based on the Ascend architecture have been successfully commercialized. More than 100 million chips have been used in real products.",https://ieeexplore.ieee.org/document/9407221/,2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA),27 Feb.-3 March 2021,ieeexplore
10.1109/IV47402.2020.9304778,"Autonomous Driving Vehicle Control Auto-Calibration System: An Industry-Level, Data-Driven and Learning-based Vehicle Longitudinal Dynamic Calibrating Algorithm",IEEE,Conferences,"The control module is a crucial part for autonomous driving systems, a typical control algorithm often requires vehicle dynamics (such as longitudinal dynamics) as inputs, which, unfortunately are difficult to calibrate in real time. Further, it is also a challenge to reflect instantaneous changes in longitudinal dynamics (e.g. load changes) using a calibration table. As a result, control performance may deteriorate when load changes considerably (especially for small cargoes). In this paper, we will show how we build a data-driven longitudinal calibration procedure using machine learning techniques to adapt load changes in real time. We first generated offline calibration tables from human driving data. The offline table serves as an initial guess for later uses, and it only requires twenty minutes of data collection and processing. We then used an online learning algorithm to appropriately update the initial table (the offline table) based on real-time performance analysis. Experiments indicated (a) offline auto-calibration leads to a better control accuracy, compared with manual calibration; (b) online auto-calibration is capable to handle load changes and significantly reduce real time control error. This system has been deployed to more than one hundred Baidu self-driving vehicles (both hybrid and electronic vehicles) since April 2018. By January 2019, the system had been tested for more than 2,000 hours and over 10,000 kilometers (6,213 miles) and was still proven to be effective.",https://ieeexplore.ieee.org/document/9304778/,2020 IEEE Intelligent Vehicles Symposium (IV),19 Oct.-13 Nov. 2020,ieeexplore
10.1109/RTAS52030.2021.00041,Brief Industry Paper: Workload-Aware GPU Performance Estimation in the Airborne Embedded System,IEEE,Conferences,"New generation airborne embedded system has deployed Graphical Processing Units (GPUs) to raise processing capability to meet growing computational demands. Applications in the airborne embedded system have strict real-time constraints. Therefore, it is necessary to accurately predict timing behaviors of those applications. Many previous work propose GPU performance models to estimate the execution time of applications. However, most of those models do not consider the impact of co-execution on the GPU performance. In this paper, we propose a workload-aware GPU performance model to predict the execution time of applications executed concurrently on a single GPU. Experimental results illustrate that the proposed model can achieve a 5.1%-11.6% prediction error in a real airborne embedded hardware platform.",https://ieeexplore.ieee.org/document/9470486/,2021 IEEE 27th Real-Time and Embedded Technology and Applications Symposium (RTAS),18-21 May 2021,ieeexplore
10.1109/RTAS52030.2021.00048,Brief Industry Paper: optimizing Memory Efficiency of Graph Neural Networks on Edge Computing Platforms,IEEE,Conferences,"Graph neural networks (GNN) have achieved state-of-the-art performance on various industrial tasks. However, the poor efficiency of GNN inference and frequent Out-of-Memory (OOM) problem limit the successful application of GNN on edge computing platforms. To tackle these problems, a feature decomposition approach is proposed for memory efficiency optimization of GNN inference. The proposed approach could achieve outstanding optimization on various GNN models, covering a wide range of datasets, which speeds up the inference by up to 3×. Furthermore, the proposed feature decomposition could significantly reduce the peak memory usage (up to 5× in memory efficiency improvement) and mitigate OOM problems during GNN inference.",https://ieeexplore.ieee.org/document/9470441/,2021 IEEE 27th Real-Time and Embedded Technology and Applications Symposium (RTAS),18-21 May 2021,ieeexplore
10.1109/MED.2017.7984310,Cloud computing for big data analytics in the Process Control Industry,IEEE,Conferences,"The aim of this article is to present an example of a novel cloud computing infrastructure for big data analytics in the Process Control Industry. Latest innovations in the field of Process Analyzer Techniques (PAT), big data and wireless technologies have created a new environment in which almost all stages of the industrial process can be recorded and utilized, not only for safety, but also for real time optimization. Based on analysis of historical sensor data, machine learning based optimization models can be developed and deployed in real time closed control loops. However, still the local implementation of those systems requires a huge investment in hardware and software, as a direct result of the big data nature of sensors data being recorded continuously. The current technological advancements in cloud computing for big data processing, open new opportunities for the industry, while acting as an enabler for a significant reduction in costs, making the technology available to plants of all sizes. The main contribution of this article stems from the presentation for a fist time ever of a pilot cloud based architecture for the application of a data driven modeling and optimal control configuration for the field of Process Control. As it will be presented, these developments have been carried in close relationship with the process industry and pave a way for a generalized application of the cloud based approaches, towards the future of Industry 4.0.",https://ieeexplore.ieee.org/document/7984310/,2017 25th Mediterranean Conference on Control and Automation (MED),3-6 July 2017,ieeexplore
10.1109/WFCS.2019.8757952,Cloud-enabled Smart Data Collection in Shop Floor Environments for Industry 4.0,IEEE,Conferences,"Industry 4.0 transition is producing a remarkable change in the Smart Factories management. Modern companies can provide new services following products inside the shop floors along the entire production chain. To achieve the goal of servitization that the Industry 4.0 world requires, a modernization of current production chains is needed. This common demand comes mostly from manufacturing sector, where complex work machines collaborate with human workers. The data produced by the machines must be processed quickly, to allow the implementation of reactive services such as predictive maintenance, and remote control, always taking care of the safety of nearby people. This paper proposes a multi-layer architecture to monitor legacy production machines during their operations inside customers plants. The platform provides near real-time delivery of data collected from the machines with a high grade of customization according to customer needs. The performed tests show the scalability of the platform for a productive ecosystem with many machines at work, confirming its feasibility within different production facilities with different needs.",https://ieeexplore.ieee.org/document/8757952/,2019 15th IEEE International Workshop on Factory Communication Systems (WFCS),27-29 May 2019,ieeexplore
10.1109/WCNC45663.2020.9120761,Collaborative Learning Model for Cyberattack Detection Systems in IoT Industry 4.0,IEEE,Conferences,"Although the development of IoT Industry 4.0 has brought breakthrough achievements in many sectors, e.g., manufacturing, healthcare, and agriculture, it also raises many security issues to human beings due to a huge of emerging cybersecurity threats recently. In this paper, we propose a novel collaborative learning-based intrusion detection system which can be efficiently implemented in IoT Industry 4.0. In the system under consideration, we develop smart “filters” which can be deployed at the IoT gateways to promptly detect and prevent cyberattacks. In particular, each filter uses the collected data in its network to train its cyberattack detection model based on the deep learning algorithm. After that, the trained model will be shared with other IoT gateways to improve the accuracy in detecting intrusions in the whole system. In this way, not only the detection accuracy is improved, but our proposed system also can significantly reduce the information disclosure as well as network traffic in exchanging data among the IoT gateways. Through thorough simulations on real datasets, we show that the performance obtained by our proposed method can outperform those of the conventional machine learning methods.",https://ieeexplore.ieee.org/document/9120761/,2020 IEEE Wireless Communications and Networking Conference (WCNC),25-28 May 2020,ieeexplore
10.1109/EAEEIE.2013.6576535,Cooperation between industry and university based on the evaluation of the industrial research results in the academic environment,IEEE,Conferences,"Based on the European Structural Funds it was developed the Intelligent Mobile Box, Intelligent Panel Controller with intelligent adaptive controllers within the industrial research and experimental development in the company Kybernetes, s.r.o. Within the frame of the academic-industry cooperation, the intelligent adaptive controller was tested at the Department of Cybernetics and Artificial Intelligence, Technical university of Kosice, Slovakia. The tests of the mobile intelligent adaptive controller were performed on two levels of university study, on the Bachelor level on the exercises from the subject “Control of Technological Processes” and on the Engineering level the exercises from the subject “Intelligent Control Networks” and on one Diploma project. Goals of students of the Control of the technological processes course had two goals, firstly to connect the intelligent adaptive controller to pre-defined controlled system (real plant, real model or simulated model) and next to validate the control results. Students of the Diploma project on the Engineering level had more advanced goals. Tasks defined for engineering students were to connect the intelligent adaptive controller to non-defined controlled system, setup the adaptivity process of the controller regarding the learning error, parameterize the control system, observe and validate the control results. Both sides concluded this cooperation as very valuable. Main contributions for students were (U1) the challenge to apply studied theoretical knowledge on the real industrial controllers, (U2) experience with new research results and technologies deployed in industry and (U3) the implementation of the control and adaptive algorithms from abstract mathematical area to real PLC controller. On side of industry research company the main contributions were (C1) testing of designed algorithms and (C2) user feedback from students to make the application HMI interface more understandable a native.",https://ieeexplore.ieee.org/document/6576535/,2013 24th EAEEIE Annual Conference (EAEEIE 2013),30-31 May 2013,ieeexplore
,Corporate Social Responsibility Challenges and Risks of Industry 4.0 technologies: A review,VDE,Conferences,"The fourth industrial revolution arrived with many enabling technologies that would impact important sociological aspects in the industry. Some of the Industry 4.0 technologies are already running in different industrial application, and other are still as a paradigm state. The social, economic, and environmental acceptance of Industry 4.0 technologies is still under discussion, which open new opportunities to execute various analysis about the possible implications of the implementation of such technologies. This article refers to an exploratory analysis and identification of the different challenges and risks of this new Industry 4.0 paradigm and its related technologies. The technologies under review were Internet of Things, Artificial Intelligence, Cloud Computing, cybersecurity, bid data, blockchain, 5G, robotics, adding manufacturing, unmanned systems, autonomous vehicles, virtual reality, and augmented reality. As a result, different social challenges and risks were identified for each technology, starting from vulnerability, implementation cost, until social aspects such as education and unemployment caused by those new technologies. In conclusion, Industry 4.0 arrived with a lot of benefits to the industry business, but companies should not stop thinking about sustainable development.",https://ieeexplore.ieee.org/document/8835964/,"Smart SysTech 2019; European Conference on Smart Objects, Systems and Technologies",4-5 June 2019,ieeexplore
10.1109/AINS47559.2019.8968698,Cyber Security Risk Assessment on Industry 4.0 using ICS testbed with AI and Cloud,IEEE,Conferences,"Industry 4.0 is a new concept, thus risk assessment is necessary. Several risk assessment methods for Industrial Control System (ICS) and Industry 4.0 have been proposed, however, it is difficult to identify impacts on the physical world caused by cyber attacks against ICS since many of these are based on tabletop analysis or software simulations. Therefore, we focus on the risk assessment using actual machines (ICS testbed) which can help to solve the above problems. In Industry 4.0, autonomous judgment and execution are required for the cyber-physical system, it is based on information exchange using Artificial Intelligence (AI) and cloud technologies. In this research, we evaluate cyber risks through attacks against ICS with AI and cloud using ICS testbed. The proposed method can clarify cyber risks and impacts on the real world, and corresponding countermeasures.",https://ieeexplore.ieee.org/document/8968698/,"2019 IEEE Conference on Application, Information and Network Security (AINS)",19-21 Nov. 2019,ieeexplore
10.1109/COINS51742.2021.9524088,Cyber Security Risks of Technical Components in Industry 4.0,IEEE,Conferences,"Industry 4.0 is a new concept of automation data exchange in manufacturing. Industry 4.0 consists of various technical components such as Artificial Intelligence (AI), IoT, OPC UA, and cloud. There could be cyber risks when these technical components are used in Industrial Control System (ICS) without security consideration. Therefore cyber risk assessment, reconsideration of implementation, operation are necessary to adopt Industry 4.0. Problems of previous research for ICS risk assessments are some of them are theoretical and conceptual methods such as simulations. Moreover, they do not focus on Industry 4.0 components.The objective of this research is to prove the concrete cyber risks and introduces corresponding secure implementations for Industry 4.0 components such as Artificial Intelligence (AI), the Internet of Things (IoT), OPC Unified Architecture (OPC UA). The proposed method has benefits for clarifying impacts on the real world by cyber-attacks through penetration tests against ICS testbed with actual machines.",https://ieeexplore.ieee.org/document/9524088/,2021 IEEE International Conference on Omni-Layer Intelligent Systems (COINS),23-25 Aug. 2021,ieeexplore
10.1109/PTC.2001.964653,DPlan: a case study on the cooperation between university and industry,IEEE,Conferences,"The aim of this paper is to explain how the research interests of a university group have been matched with the real needs of a distribution company team working in the field. It describes a case study of technology transfer, from research, through prototyping, to the market. This paper also addresses the formulation and the main evolutionary computation techniques used to deal with the problem of optimal distribution planning. The integration of all systems currently in operation is a priority of EDP distribution. The strategy is to arrive at a situation where those systems become modules of an integration platform. The infrastructure for this integration platform is a geographic information system. In the paper we describe the integration of DPlan in this platform.",https://ieeexplore.ieee.org/document/964653/,2001 IEEE Porto Power Tech Proceedings (Cat. No.01EX502),10-13 Sept. 2001,ieeexplore
10.1109/CASE48305.2020.9216961,Decentralized coordination of autonomous AGVs for flexible factory automation in the context of Industry 4.0,IEEE,Conferences,"Future smart factories feature flexible systems that can dynamically reconFigure manufacturing systems via near real-time system monitoring and learning-based self-optimization. Automated guided vehicles (AGVs), as a critical method of transporting goods and material within a factory, is vital for flexible automation in a smart factory. However, there is an urgent gap in the ability to dynamically schedule and assign tasks for AGVs in a dynamic environment. In this research, we propose a decentralized AGV fleet architecture and task allocation method to enable dynamic allocation/reallocation of tasks in an AGV fleet. The developed algorithm can also reconFigure AGV task allocations to adapt to system changes, such as AGV failure and new AGVs joining the system. The system modeling, setup and algorithms are presented with a case study in a lab environment that demonstrates flexible collaboration between an AGV fleet and a robotic assembly cell.",https://ieeexplore.ieee.org/document/9216961/,2020 IEEE 16th International Conference on Automation Science and Engineering (CASE),20-21 Aug. 2020,ieeexplore
10.1109/IPFA49335.2020.9260582,"Detection and prevention of assembly defects, by machine learning algorithms, in semiconductor industry for automotive",IEEE,Conferences,"Years of experience within semiconductor manufacturing facilities have led to optimize processes to serve both quality and cost. The solution to achieve next generational levels requires a new approach: this one is fitting with implementation of advanced analytics and machine learning algorithms. Applied to manufacturing data which corresponds with a real big data context, these algorithms can provide insights and automate responses to detect, prevent and ultimately eliminate the most severe failure modes. The project described in this paper targets a wafer sawing process. Various challenges that are raised in such a project are of different natures. A first one is the need for a high level of technical expertise in the manufacturing process of focus: this is essential to define the meaningful dataset that represents comprehensively the desired output of the process. Another component is the data collection aspect: many data have to be collected, stored and parsed, and some small signals found will become the leading indicator to an upcoming process degradation and capability of capturing them is essential. Another key data is traceability of the processed material. Additionally, ensuring an informatic technology architecture to support collection, storage, parsing and computation of the datasets is a significant challenge. Lastly, project success is related to the data scientist expertise to build adequate machine learning algorithms. Optimization of the models can take several iterations with back and forth communication between data scientists and process technical experts. This paper describes issues revealed, some solutions found, and future expectations.",https://ieeexplore.ieee.org/document/9260582/,2020 IEEE International Symposium on the Physical and Failure Analysis of Integrated Circuits (IPFA),20-23 July 2020,ieeexplore
10.1109/EuCNC/6GSummit51104.2021.9482590,Empowering Industry 4.0 and Autonomous Drone Scouting use cases through 5G-DIVE Solution,IEEE,Conferences,"The 5G Edge Intelligence for Vertical Experimentation (5G-DIVE) project aims at demonstrating the technical merits and business value proposition of 5G technologies in two vertical pilots, namely the Industry 4.0 (I4.0) and Autonomous Drones Scout (ADS) pilots. This paper presents an overview of the overall 5G-DIVE solution and reports the results of the initial validation campaign of the selected use case, featuring 5G connectivity, distributed Edge computing, and artificial intelligence. The initial results for the I4.0 provide a baseline for next step validation campaign targeting a broader scale 5G implementation, while the ADS results provides promising results for enhancing the autonomous navigation in real-time.",https://ieeexplore.ieee.org/document/9482590/,2021 Joint European Conference on Networks and Communications & 6G Summit (EuCNC/6G Summit),8-11 June 2021,ieeexplore
10.1109/SMC.2019.8913901,Explainable Machine Learning in Industry 4.0: Evaluating Feature Importance in Anomaly Detection to Enable Root Cause Analysis,IEEE,Conferences,"In the past recent years, Machine Learning methodologies have been applied in countless application areas. In particular, they play a key role in enabling Industry 4.0. However, one of the main obstacles to the diffusion of Machine Learning-based applications is related to the lack of interpretability of most of these methods. In this work, we propose an approach for defining a `feature importance' in Anomaly Detection problems. Anomaly Detection is an important Machine Learning task that has an enormous applicability in industrial scenarios. Indeed, it is extremely relevant for the purpose of quality monitoring. Moreover, it is often the first step towards the design of a Machine Learning-based smart monitoring solution because Anomaly Detection can be implemented without the need of labelled data. The proposed feature importance evaluation approach is designed for Isolation Forest, one of the most commonly used algorithm for Anomaly Detection. The efficacy of the proposed method is tested on synthetic and real industrial datasets.",https://ieeexplore.ieee.org/document/8913901/,"2019 IEEE International Conference on Systems, Man and Cybernetics (SMC)",6-9 Oct. 2019,ieeexplore
10.1109/CTIT.2018.8649493,Game Theoretic Approach for Applying Artificial Intelligence in the Credit Industry,IEEE,Conferences,"The law of accelerating returns can be viewed as a concept that describes acceleration of technological progress. The idea is that tools are used for developing more advanced tools that are applied for creating even more advanced tools etc. A similar idea has been implemented in algorithms for advancing artificial intelligence. In this paper, the results of applying these algorithms in games are discussed. Nevertheless, real life tasks seem more complicated. The game theoretic approach can be applied for transition from theoretical and unrealistic games to more complex and practical tasks. Applications of the game theoretic approach to advance artificial intelligence in solving tasks in the credit industry are proposed.",https://ieeexplore.ieee.org/document/8649493/,2018 Fifth HCT Information Technology Trends (ITT),28-29 Nov. 2018,ieeexplore
10.1109/COMPSAC.2018.10204,Indoor Augmented Reality Using Deep Learning for Industry 4.0 Smart Factories,IEEE,Conferences,"This paper proposes to design, develop and implement a fast and markerless mobile augmented reality system to achieve the registration for, the visualization of, and the interaction with machines in indoor smart factories with Industry 4.0 vision. A lightweight deep-learning image detection module based on MobileNets running on mobile devices is used to detect/recognize different machines and different portions of machines. Internet of Things (IoT) networking allows machines and sensors in machines to report data, such as machine settings and machine states, to the cloud-side server. Thus, augmented information associated with a machine portion can be derived from the server and superimposed with the portion image shown on the device display. Furthermore, interaction methods based on touch gestures and distance calculation are also implemented. A prototype system is developed and tested in a mechanical workshop for the purpose of validation and evaluation. The system is shown to achieve high detection accuracy, intuitive visualization, and unique interaction modes.",https://ieeexplore.ieee.org/document/8377831/,2018 IEEE 42nd Annual Computer Software and Applications Conference (COMPSAC),23-27 July 2018,ieeexplore
10.23919/ECC.2003.7085250,Inferential sensor for the olive oil industry,IEEE,Conferences,"This paper shows an inferential sensor that has been developed to be used in the olive oil industry. This sensor has been designed to measure two variables that appear in the elaboration of olive oil in a mill which are very difficult to be measured on line by a physical sensor. The knowledge of these variables on line is crucial for the optimal operation of the process, since they provide the state of the plant, allowing the development of a control strategy that can improve the quality and yield of the product. This sensor measures variables that in other case should come form laboratory analysis with large processing delays or from very expensive and difficult to use on line analysers. The sensor has been devised based upon artificial Neural Networks (NN) and has been implemented as a routine running on a Programmable Logic Controller (PLC) and successfully tested on a real plant.",https://ieeexplore.ieee.org/document/7085250/,2003 European Control Conference (ECC),1-4 Sept. 2003,ieeexplore
10.23919/MIPRO48935.2020.9245232,Integrating Industry Seminars within a Software Engineering Module to Enhance Student Motivation,IEEE,Conferences,"Engineering students increasingly demand and require coverage of emerging technologies to prepare themselves for subsequent research and employment. Industry and professional bodies are also concerned that engineering education doesn't always prepare students adequately for the world of work. The software engineering postgraduate professional practice module at University College London is designed to provide real-world experience, before students commence their industry research projects. Industry speakers are invited from a range of organizations, including ThoughtWorks, IBM, Form3, Verne Global, and Fujitsu. Seminars include: DevOps, microservices, cloud-native architectures, machine learning and quantum technologies. Before each topic is covered, students are asked their understanding of the subject matter, via questionnaires. This information is shared with industry speakers to ensure the content of presentations is compatible with students' prior knowledge. It has proved valuable to allow time for discussions to facilitate professional networking, which particularly benefits female students. Students have indicated they highly value the real-world project examples delivered by industry experts. This suggests that integrating industry seminars can enhance engineering education and motivate students by covering leading-edge technologies and practices. However, this requires considerable time in coordinating and codeveloping seminars, and such initiatives need to be adequately resourced to be effective.",https://ieeexplore.ieee.org/document/9245232/,"2020 43rd International Convention on Information, Communication and Electronic Technology (MIPRO)",28 Sept.-2 Oct. 2020,ieeexplore
10.1109/RTSI.2019.8895598,Intelligent Embedded Load Detection at the Edge on Industry 4.0 Powertrains Applications,IEEE,Conferences,"In the context of Industry 4.0, there has been great focus in developing intelligent sensors. Deploying them, condition monitoring and predictive maintenance have become feasible solutions to minimize operating and maintenance costs while also increasing safety. A critical aspect is the applied load to the supervised machinery system. Vibration data can be used to determine the current condition, but this needs signal processing specially developed and adapted to the monitored machine part for feature extraction. Artificial intelligence (AI) can, on one hand, simplify the development of such special purpose processing and on another hand be used to monitor and classify machine conditions by learning features directly from data. By bringing the AI computation as close as possible to the sensor (Edge-AI), data bandwidth can be minimized, system scalability and responsiveness can be improved and real-time requirements can be fulfilled. This work describes how Edge-AI on a STM32-bit microcontroller can be implemented. Our experimental setup demonstrates how AI can be effectively used to detect and classify the load on a powertrain. In order to do this, we use a MEMS capacity accelerometer to sense vibrations of the system. Also, this work demonstrates how Deep Neural Networks (DNN) for signal classification are build and trained by using an open-source deep learning framework and how the code library for the microcontroller is automatically generated afterwards by using STM32Cube. AI toolchain. We compare the classification accuracy of a memory compressed DNN against an uncompressed DNN.",https://ieeexplore.ieee.org/document/8895598/,2019 IEEE 5th International forum on Research and Technology for Society and Industry (RTSI),9-12 Sept. 2019,ieeexplore
10.1109/AIM46487.2021.9517377,Introducing adaptive mechatronic designs in bulk handling industry,IEEE,Conferences,"The advances of mechatronic system design and system integration have shown improvements in functionality, performance and energy efficiency in many applications across industries, from autonomous ground vehicles and drones to conveyor belts. This trend has been adopted in some industries more than others. The design of equipment to handle granular or bulk material is commonly based on traditional approaches. Therefore, introducing mechatronic concepts in the design procedure can enable new possibilities, such as sensor integration and data analyses, adaptability and control. The efficiency of bulk material handling equipment in ports, agriculture and food processing is heavily influenced by the operational conditions. Typically, a piece of equipment is designed for defined operational conditions when the maximum performance can be achieved. In this work the concept of adaptability to varying operational conditions is explored by understanding the technologies implemented in other industries and the feasibility to be implemented in the bulk handling equipment design. Sensing technology, actuation and adaptability are systematically presented in this work to support the design process of the next generation of bulk handling equipment. This will pave the way for incorporating the technological trends in the design, such as: sustainability, “smartness”, Internet of Things, Industry 4.0, digital twin and machine learning. Adaptive mechatronic solutions will play a crucial role in generating and implementing innovative sustainable solutions for bulk handling equipment.",https://ieeexplore.ieee.org/document/9517377/,2021 IEEE/ASME International Conference on Advanced Intelligent Mechatronics (AIM),12-16 July 2021,ieeexplore
10.1109/ICDE51399.2021.00283,Learning to Optimize Industry-Scale Dynamic Pickup and Delivery Problems,IEEE,Conferences,"The Dynamic Pickup and Delivery Problem (DPDP) is aimed at dynamically scheduling vehicles among multiple sites in order to minimize the cost when delivery orders are not known a priori. Although DPDP plays an important role in modern logistics and supply chain management, state-of-the-art DPDP algorithms are still limited on their solution quality and efficiency. In practice, they fail to provide a scalable solution as the numbers of vehicles and sites become large. In this paper, we propose a data-driven approach, Spatial-Temporal Aided Double Deep Graph Network (ST-DDGN), to solve industry-scale DPDP. In our method, the delivery demands are first forecast using spatial-temporal prediction method, which guides the neural network to perceive spatial-temporal distribution of delivery demand when dispatching vehicles. Besides, the relationships of individuals such as vehicles are modelled by establishing a graph-based value function. ST-DDGN incorporates attention-based graph embedding with Double DQN (DDQN). As such, it can make the inference across vehicles more efficiently compared with traditional methods. Our method is entirely data driven and thus adaptive, i.e., the relational representation of adjacent vehicles can be learned and corrected by ST-DDGN from data periodically. We have conducted extensive experiments over real-world data to evaluate our solution. The results show that ST-DDGN reduces 11.27% number of the used vehicles and decreases 13.12% total transportation cost on average over the strong baselines, including the heuristic algorithm deployed in our UAT (User Acceptance Test) environment and a variety of vanilla DRL methods. We are due to fully deploy our solution into our online logistics system and it is estimated that millions of USD logistics cost can be saved per year.",https://ieeexplore.ieee.org/document/9458860/,2021 IEEE 37th International Conference on Data Engineering (ICDE),19-22 April 2021,ieeexplore
10.1109/SCC47175.2019.9116104,Modeling and management of human resources in the reconfiguration of production system in industry 4.0 by neural networks,IEEE,Conferences,"In Industry 4.0, the role of employees changes significantly. Real-time production line control transforms job content. Work processes affect working conditions. The implementation of a socio-technical approach to the organization of work gives workers the opportunity to adapt their skills. Indeed, production work will become more and more multi-factor, especially with regard to control and decision-making tasks. In this paper, a proposal for an intelligent system for modeling skills and human resource management in the production system chain through the use of two artificial neural networks. The first NN1 network allows for the identification of the human factor, as well as the second NN2 network is reserved for valuing the human skills needed in Industry 4.0.",https://ieeexplore.ieee.org/document/9116104/,"2019 International Conference on Signal, Control and Communication (SCC)",16-18 Dec. 2019,ieeexplore
10.1109/ISCAS.2019.8702575,Multi-View Fusion Neural Network with Application in the Manufacturing Industry,IEEE,Conferences,"In recent years the research community and industry have paid high attention to the field of machine learning, especially deep learning. Nowadays many real-world classification or rather prediction applications are implemented by neural network models. We propose a multi-view fusion neural network with application in the manufacturing industry. Image information of multiple cameras is fused and used by the proposed model to predict the state of a manufacturing machine. Experiments show that the overall classification performance is increased from a baseline of 92.7% to 99.5% by the fusion model.",https://ieeexplore.ieee.org/document/8702575/,2019 IEEE International Symposium on Circuits and Systems (ISCAS),26-29 May 2019,ieeexplore
10.23919/DATE48585.2020.9116407,Network Synthesis for Industry 4.0,IEEE,Conferences,"Today's factory machines are ever more connected with SCADA, MES, ERP applications as well as external systems for data analysis. Different types of network architectures must be used for this purpose. For instance, control applications at the lowest level are susceptible to delays and errors while data analysis with machine learning procedures requires to move a large amount of data without real-time constraints. Standard data formats, like Automation Markup Language (AML), have been established to document factory environment, machine placement and network deployment, however, no automatic technique is currently available in the context of Industry 4.0 to choose the best mix of network architectures according to spacial constraints, cost, and performance. We propose to fill this gap by formulating an optimization problem. First of all, spatial and communication requirements are extracted from the AML description. Then, the optimal interconnection of wired or wireless channels is obtained according to application objectives. Finally, this result is back-annotated to AML to be used in the life cycle of the production system. The proposed methodology is described through a small, but complete, smart production plant.",https://ieeexplore.ieee.org/document/9116407/,"2020 Design, Automation & Test in Europe Conference & Exhibition (DATE)",9-13 March 2020,ieeexplore
10.1109/UPEC.2015.7339883,Optimising the use of electrical energy within the waste water industry through improved utilization of process control and automation,IEEE,Conferences,"Active system control, real time control and active system management are some of the new concepts the water utilities are investigating as a way of improving asset energy usage. The assets under most scrutiny are those with high energy burdens such as pumps and blowers. Fuzzy logic, genetic algorithms and artificial neural networks are an increasing form of control employed within software systems. The increase in availability of faster processors permits engineers to run larger, more complex models with higher precision and reliability. By reviewing the already active forms of control variants, the current work aims to integrate a block wise hydraulic model with live process data, captured from field sensors, with a view to deploying a combined optimisation technique that in turn focuses on reducing demand side energy consumption. This paper presents an initial review of these systems and an incipient proposed approach.",https://ieeexplore.ieee.org/document/7339883/,2015 50th International Universities Power Engineering Conference (UPEC),1-4 Sept. 2015,ieeexplore
10.1109/ICSME.2017.41,Predicting and Evaluating Software Model Growth in the Automotive Industry,IEEE,Conferences,"The size of a software artifact influences the software quality and impacts the development process. In industry, when software size exceeds certain thresholds, memory errors accumulate and development tools might not be able to cope anymore, resulting in a lengthy program start up times, failing builds, or memory problems at unpredictable times. Thus, foreseeing critical growth in software modules meets a high demand in industrial practice. Predicting the time when the size grows to the level where maintenance is needed prevents unexpected efforts and helps to spot problematic artifacts before they become critical. Although the amount of prediction approaches in literature is vast, it is unclear how well they fit with prerequisites and expectations from practice. In this paper, we perform an industrial case study at an automotive manufacturer to explore applicability and usability of prediction approaches in practice. In a first step, we collect the most relevant prediction approaches from literature, including both, approaches using statistics and machine learning. Furthermore, we elicit expectations towards predictions from practitioners using a survey and stakeholder workshops. At the same time, we measure software size of 48 software artifacts by mining four years of revision history, resulting in 4,547 data points. In the last step, we assess the applicability of state-of-the-art prediction approaches using the collected data by systematically analyzing how well they fulfill the practitioners' expectations. Our main contribution is a comparison of commonly used prediction approaches in a real world industrial setting while considering stakeholder expectations. We show that the approaches provide significantly different results regarding prediction accuracy and that the statistical approaches fit our data best.",https://ieeexplore.ieee.org/document/8094464/,2017 IEEE International Conference on Software Maintenance and Evolution (ICSME),17-22 Sept. 2017,ieeexplore
10.1109/DSAA53316.2021.9564181,Predictive maintenance based on anomaly detection using deep learning for air production unit in the railway industry,IEEE,Conferences,"Predictive maintenance methods assist early detection of failures and errors in machinery before they reach critical stages. This study proposes a data-driven predictive maintenance framework for the air production unit (APU) system of a train of Metro do Porto by deep learning based on a sparse autoencoder (SAE) network that efficiently detects abnormal data and considerably reduces the false alarm rate. Several analog and digital sensors installed on the APU system allow the detection of behavioral changes and deviations from the normal pattern by analyzing the collected data. We implemented two versions of the SAE network in which we inputted analog sensors data and digital sensors data, and the experimental results show that the failures due to air leakage problems are predicted by analog sensors data while other types of failures are identified by digital sensors data. A low pass filter is applied to the output of the SAE network, and a sequence of abnormal data is used as an alarm for the APU system failure. Performance indicators of the SAE network with digital sensors data, in terms of F1 Score, Recall, and Precision, are respectively, about 33.6%, 42%, and 28% better than those of the SAE network with analog sensors data. For comparison purposes, we also implemented a variational autoencoder (VAE). The results show that SAE performance is better than that of VAE by 14%, 77%, and 37% respectively, for Recall, Precision and F1 Score.",https://ieeexplore.ieee.org/document/9564181/,2021 IEEE 8th International Conference on Data Science and Advanced Analytics (DSAA),6-9 Oct. 2021,ieeexplore
10.1109/ITSC45102.2020.9294450,Predictive maintenance leveraging machine learning for time-series forecasting in the maritime industry,IEEE,Conferences,"One of the key challenges in the maritime industry refers to minimizing the time a vessel cannot be utilized, which has multiple effects. The latter is addressed through maintenance approaches that however in many cases are not efficient in terms of cost and downtime. Predictive maintenance provides optimized maintenance scheduling offering extended vessel lifespan, coupled with reduced maintenance costs. As in several industries, including the maritime domain, an increasing amount of data is made available through the deployment and exploitation of data sources, such as on board sensors that provide real-time information. These data provide the required ground for analysis and thus support for various types of data-driven decision making. In the maritime domain, sensors are deployed on vessels to monitor their engines and data analysis tools are needed to assist engineers towards reduced operational risk through predictive maintenance solutions that are put in place. In this paper, we present an approach for anomaly detection on time-series data, utilizing machine learning on the vessels sensor data, in order to predict the condition of specific parts of the vessel's main engine and thus facilitate predictive maintenance. The novel characteristic of the proposed approach refers both to the inclusion of new innovative models to address the case of predictive maintenance in maritime and the combination of those different models, highlighting an improved result in terms of evaluation metrics.",https://ieeexplore.ieee.org/document/9294450/,2020 IEEE 23rd International Conference on Intelligent Transportation Systems (ITSC),20-23 Sept. 2020,ieeexplore
10.1109/AQTR49680.2020.9129934,Rapid Prototyping of IoT Applications for the Industry,IEEE,Conferences,"In this article a novel approach to rapid IoT application prototype design and development is presented using an existing experimental dataset and a functional model. Using the existing data, we populate our NoSQL Apache Cassandra database cluster with legacy data and generate similar data using a python code, considering the Mosquitto MQTT protocol implementation and Node-RED node.js development environment. Using Node-RED, we display the data already collected, and dynamically create new data that can be monitored in real-time in the provided dashboard. The possibilities and utility of this approach are explored in the article, and a simple prototype application for modeling the open access Combined Cycle Power Plant (CCPP) dataset provided by the UCI Machine Learning Repository is presented to prove the efficiency and rapidity of IoT application development. The presented system development approach can be used in industrial environment for rapid development of IIoT applications.",https://ieeexplore.ieee.org/document/9129934/,"2020 IEEE International Conference on Automation, Quality and Testing, Robotics (AQTR)",21-23 May 2020,ieeexplore
10.1109/IMITEC50163.2020.9334129,Real Time Customer Churn Scoring Model for the Telecommunications Industry,IEEE,Conferences,"There are two types of customers in the telecommunication industry; the pre-paid and the contract customers. In South Africa it is the pre-paid customers that keep telcos constantly worried because such customers do not have anything binding them to the company, they can leave and join a competitor at any time. To retain such customers, telcos need to customise suitable solutions especially for those customers that are agitating and can churn at any time. This needs customer churn prediction models that would take advantage of big data analytics and provide the telco industry with a real time solution. The purpose of this study was to develop a real time customer churn prediction model. The study used the CRISP-DM methodology and the three machine learning algorithms for implementation. Watson Studio software was used for the model prototype deployment. The study used the confusion matrix to unpack a number of performance measures. The results showed that all the models had some degree of misclassification, however the misclassification rate of the Logistic Regression was very minimal (2.2%) as differentiated from the Random Forest and the Decision Tree, which had misclassification rates of 20.8% and 21.7% respectively. The results further showed that both Random Forest and the Decision Tree had good accuracy rates of 78.3% and 79.2% respectively, although they were still not better than that of the Logistic Regression. Despite the two having good accuracy rates, they had the highest rates of misclassification of class events. The conclusion we drew from this was that, accuracy is not a dependable measure for determining model performance.",https://ieeexplore.ieee.org/document/9334129/,2020 2nd International Multidisciplinary Information Technology and Engineering Conference (IMITEC),25-27 Nov. 2020,ieeexplore
10.1109/FUZZY.2008.4630459,Real time Takagi-Sugeno fuzzy model based pattern recognition in the batch chemical industry,IEEE,Conferences,"This contribution describes the real time pressure check pattern recognition of an industrial batch dryer. The goal is to identify the start of the drying process and to calculate the time elapsed between two consequent batch starts (batch time) right after the batch has completed. The presented pattern recognition method implements a supervised learning approach based on Takagi-Sugeno fuzzy (TS) models. The decision maker design is based on plant data compressed by the PI algorithm (OSI Software, Inc). It is concluded that the developed classifier is able to perform real time classification and the compressed PI data can be used in order to design data analysis tools which are useful for chemical batch plant operation investigations.",https://ieeexplore.ieee.org/document/4630459/,2008 IEEE International Conference on Fuzzy Systems (IEEE World Congress on Computational Intelligence),1-6 June 2008,ieeexplore
10.1109/ICSIMA47653.2019.9057343,Real-Time Wireless Monitoring for Three Phase Motors in Industry: A Cost-Effective Solution using IoT,IEEE,Conferences,"In recent days modern environment industries are facing rapid flourishing for performance capabilities and their requirements for corporate clients and industrial sector. Internet of Things (IoT) is an innovative and rapidly growing field for automation and evaluation in networks, Artificial Intelligence, data sensing, data mining, and big data. These systems have a great tendency to monitor and control different process used in industries. IoT systems have been implemented and have applications in different industries due to their cost-effectiveness and flexibility In this paper we have developed a system which includes real-time monitoring of current reading of three-phase motor through a wireless network. With the help of this system, data can be saved and monitored and then transmitted to cloud storage. This system contains Arduino-UNO board, ACS-712 current sensor, ESP-8266 Wi-Fi module which sends information to an IoT API service THING-SPEAK that behave like a cloud for various sensors to monitor data. The proposed system was successfully deployed in Aisha Steel Mills, Karachi, Pakistan.",https://ieeexplore.ieee.org/document/9057343/,"2019 IEEE International Conference on Smart Instrumentation, Measurement and Application (ICSIMA)",27-29 Aug. 2019,ieeexplore
10.1109/ICMLC.2009.5212284,SVM optimized scheme based PSO in application of engineering industry process,IEEE,Conferences,"Aimed to the problem that it is hardship to get real-time and on-line measuring parameters in wood drying process, a novel PSO-SVM model that hybridized the particle swarm optimization (PSO) and support vector machines (SVM) to improve the nonlinearity caused by ambient temperature and other disturbance factors is presented. Support vector machines (SVM) based on statistical learning theory and structural risk minimization is proposed to deal with these problems. However, the model complexity and generalization performance of support vector machines (SVM) depend on a good setting of the three parameters (epsiv,c,gamma). In this paper, the particle swarm optimization is applied to optimize the parameters (epsiv,c,gamma) at the same time. Based on the proposed method, both PSO-SVM and SVM models are established and implemented to estimate lumber moisture content value in wood drying process. The result of comparative analysis is given. Experimental results show that solutions obtained by PSO-SVM training seem to be more robust and better generalization performance compared to SVM training.",https://ieeexplore.ieee.org/document/5212284/,2009 International Conference on Machine Learning and Cybernetics,12-15 July 2009,ieeexplore
10.1109/I-SMAC49090.2020.9243544,Scalable IoT Solution using Cloud Services – An Automobile Industry Use Case,IEEE,Conferences,"The role of IoT and related internet-based applications in otherwise mechanical devices to monitor, manage and enhance the performance of the same is quite widespread now. Almost all public cloud service providers provide scalable, fully managed and elastic IoT related services. The data flows from these services are essentially streaming and can be consumed for further use in various predictive, descriptive and visualization modules. The cloud platforms enable ingestion, transformation and usage of the data by providing streaming, machine learning and sharable visualization services. This ecosystem greatly reduces the time to create IoT based minimum viable product creation which in turn enhances the business value realization cycle. The effect of cycle time reduction to design, architect and develop IoT solutions leads to a rapid improvement of business lead time and makes it easier for businesses to gain from the data insights and plan the next course of action. In this paper, one such enterprise graded use case is explored, in which the Azure IoT platform in terms of the offerings and associated ecosystem of Azure Stream Analytics and Azure Machine learning services are explained. This paper covers design, architecture, development and deployment of the solution prepared and how the same is monitored once in production. Security is a very important aspect of the same and here the security architecture is being explored. A conclusion is presented with the scope of future enhancements using auto ML services in serverless platforms to enable real-time automated decision making augmented with human expertise and intelligence.",https://ieeexplore.ieee.org/document/9243544/,"2020 Fourth International Conference on I-SMAC (IoT in Social, Mobile, Analytics and Cloud) (I-SMAC)",7-9 Oct. 2020,ieeexplore
10.1109/IDAP.2018.8620887,Smart Grids and Industry 4.0,IEEE,Conferences,"Since the beginning of the Industrial Revolution, the concept of Industrial Revolution in the 4<sup>th</sup>/ Industry 4.0 which is a new turning point in the technologies used, is in every field as well as in the energy field which is considered as the key point of all revolutions. It is aimed that energy generation, transmission and distribution are more efficient and highly reliable together with the next generation software and hardware which is the result of this concept.It has been seen that existing networks can not be enough to achieve this goal.At this point, flexible, reliable, clean, sustainable and highly efficient electric energy due to smart grids come into play. However, due to the increase in demand for electricity due to increase in population and industrialization, the dependence of electric energy on production resources, the availability of renewable energy sources in a dispersed geographical structure and the loss and leakage are becoming more important in our country. The Cyber-Physical System, the Internet of Things, M2M (machine to machine), etc components are used in the energy field, especially in intelligent network systems, resulting in intensive use of technology at every stage from power generation to end-user distribution point. This includes better system operation, reduced costs, increased energy efficiency, reduced greenhouse effect, reduced downtime with reduced downtime, reduced loss/leakage rates, improved energy quality, better management of production and storage systems, intelligent meter reading and load management and real-time supply-demand management. Industry 4.0/4.Industrial Revolution and smart grids have many common aspects such as optimization, automation, efficient use and management of energy, intelligent production, everything internet. In this study, the development process of the Industrial 4.0 revolution in the world and in our country will be examined and the interaction with these process smart grids will be examined.",https://ieeexplore.ieee.org/document/8620887/,2018 International Conference on Artificial Intelligence and Data Processing (IDAP),28-30 Sept. 2018,ieeexplore
10.1109/WRC-SARA.2019.8931920,Software-defined Cloud Manufacturing in the Context of Industry 4.0,IEEE,Conferences,"In the practice of “Cloud Manufacturing (CMfg)” or “Industrial Internet”, there still exist key problems, including: 1) big data analytics and decision-making in the cloud could not meet the requirements of time-sensitive manufacturing applications, moreover uploading ZettaBytes of future device data to the cloud may cause serious network congestion, 2) the manufacturing system lacks openness and evolvability, thus restricting the rapid optimization and transformation of the system, 3) big data from the shop-floor IoT devices and the internet has not been effectively utilized to guide the optimization and upgrade of the manufacturing system. In view of these key practical problems, we propose an open evolutionary architecture of intelligent CMfg system with collaborative edge and cloud processing capability. Hierarchical gateways near shop-floor things are introduced to enable fast processing for time-sensitive applications. Big data in another dimension from the software defined perspective will be used to decide the efficient operations and highly dynamic upgrade of the system. From the software system view, we also propose a new mode - AI-Mfg-Ops (AI-enabled Cloud Manufacturing Operations) with a supporting framework, which can promote the fast operation and upgrading of CMfg systems with AI enabled monitoring-analysis-planning-execution close loop. This work can improve the universality of CMfg for real-time fast response and operation &amp; upgrading.",https://ieeexplore.ieee.org/document/8931920/,2019 WRC Symposium on Advanced Robotics and Automation (WRC SARA),21-22 Aug. 2019,ieeexplore
10.1109/ICRITO48877.2020.9198036,State of Art: Energy Efficient Protocols for Self-Powered Wireless Sensor Network in IIoT to Support Industry 4.0,IEEE,Conferences,"Up gradation of manufacturing systems in industries by means of implementing innovative manufacturing techniques that captures real time data, applies machine learning algorithms, makes entire system self-decisive and provides inter connectivity to the whole system is the prime focus of the Industry 4.0. It is aimed at bringing new industrial revolution with the help of internet of things technology due to its considerable influence in the industrial manufacturing process. Though impact of internet of things in industrial sector is huge, a practical implementation incorporates challenges in energy efficiency, self-powered sensor nodes and security. For processing of gathered data self-powered sensor nodes may sinks energy from ambient energy sources. A considerable amount of efforts has been put by researchers to address the challenges for development of energy efficient routing protocol for such nodes. The presented survey is aimed at analyzing the protocols for contribution towards the goals of industry 4.0 Energy efficient protocol will support the system to consume least energy for its operation. Henceforth combination of self-powered wireless sensor network and energy efficient protocol will be useful to power up many industrial IoT applications.",https://ieeexplore.ieee.org/document/9198036/,"2020 8th International Conference on Reliability, Infocom Technologies and Optimization (Trends and Future Directions) (ICRITO)",4-5 June 2020,ieeexplore
10.1109/SNPDWinter52325.2021.00068,The Case Study on Use of Bigdata and AI in Distribution Industry,IEEE,Conferences,"With the development of telecommunication technology, a commerce platform centered on mobile devices is developing. Global mobile commerce companies such as Amazon are trying to recommend optimized products to customers and provide optimized logistics services through real-time big data. As online-oriented commerce develops, rival local offline stores are making changes for survival. In Korea, the government is trying to find a balance with offline stores by controlling big data-oriented online commerce, as is the regulation that sought to protect the local market by limiting the business hours of Super Supermarket (SSM) to balance local offline stores and Supermarket. However, in the data-driven fourth industrial revolution, we will have to find ways to develop by utilizing data, which is the main raw material. This paper tried to investigate cases of applying and operating big data in the distribution industry and seek ways to promote focused on big-data sharing that can develop with offline stores by conducting a Delphi survey to experts.",https://ieeexplore.ieee.org/document/9403518/,"2021 21st ACIS International Winter Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing (SNPD-Winter)",28-30 Jan. 2021,ieeexplore
10.1109/MLISE54096.2021.00090,The Design and Implementation of Beidou Ground-based Augmentation System in Natural Resources Industry,IEEE,Conferences,"The Beidou Ground-based Augmentation System is an important part of the Beidou Navigation Satellite System (BDS). It can provide real-time centimeter-level positioning services, effectively solving the problem of low positioning accuracy and poor real-time performance of BDS in natural resource industry. This issue introduces the design and implementation of Beidou Ground-based Augmentation System in natural resources industry, and verifies the system's performance in the practical applications of mine law-enforcement monitoring.",https://ieeexplore.ieee.org/document/9611663/,2021 International Conference on Machine Learning and Intelligent Systems Engineering (MLISE),9-11 July 2021,ieeexplore
10.1109/icABCD49160.2020.9183853,The Impact of Smart Manufacturing Approach On The South African Manufacturing Industry,IEEE,Conferences,"SM is a technology-driven approach that mainly utilises machines to monitor the entire production of an organisation. The objective of SM in an organisation is to identify ways to automatize the manufacturing process while using data analytics to optimize the manufacturing performance. This research mitigates the impact of technology, in this case expressed as SM in South African industries. The research followed a quantitative approach whereby 42 respondents from low, medium low and high technology industries took part in the study. Data has been amassed from first-hand experience by mean of an adapted questionnaire constituted of three sections: The first section was about the general demographic information of the respondents. Section two investigates the respondent's awareness on SM. Finally, section three assessed the impact that SM had on the performance of the organisation. The findings of this study revealed that Smart Manufacturing has a positive impact in South African manufacturing organisations as it allows effective operations, fast response to customers demand, real time operations optimisation. Nevertheless, Smart Manufacturing is a new concept under the fourth industrial revolution in South Africa and will need time before being totally implemented in all organisations as it is costly.",https://ieeexplore.ieee.org/document/9183853/,"2020 International Conference on Artificial Intelligence, Big Data, Computing and Data Communication Systems (icABCD)",6-7 Aug. 2020,ieeexplore
10.1109/PICMET.2015.7273077,The effects of disruption on different types of tile manufacturing Industry-layouts: An empirical investigation on tile manufacturing industry,IEEE,Conferences,"Almost all manufacturing facilities need to use production scheduling systems to increase productivity and reduce production costs. Most manufacturing industries invest huge amount of money to manufacture and supply products on time in order to meet customers demand and objectives but due to unforeseen disruptions, these objectives are difficult to achieve. In Real-life, production operations are subject to a large number of unexpected disruptions that may invalidate an original schedule. This work considered effects of disruption on different types of industry-layouts that are seldom been used in manufacturing industries namely: fixed, product, production and process industry-layouts. Questionnaires were used for data collection from a number of companies in Cameroon and reliably theory, simulation software's were using to analyze the data. The study reveals that, disruptions varies per industry-layout and the leading sources of disruptions are machine breakdown, power failure, employee's absenteeism and material shortage. It is concluded that disruption of one type may not greatly affect productivity of a certain industry-layout whilst similar disruptions can have devastating effects on another type and also, the impacts of disruption are dependent on the Industry-layouts.",https://ieeexplore.ieee.org/document/7273077/,2015 Portland International Conference on Management of Engineering and Technology (PICMET),2-6 Aug. 2015,ieeexplore
10.1109/CCDC.2008.4598242,The input-output connection analysis of industry structure based on complex network theory,IEEE,Conferences,"In order to deeply reveal the structure characters of the real weighted complex networks, the clustering and correlation measures are given and compared from the view of topological and weighted network respectively. The structure of Chinese national economics in 2002 is set as an example to illustrate. The complex network model of national economic industry departments is established, the clustering and correlation characters of this industry relation network are analyzed by the software tool MATLAB with the data coming from the input-output data published by the national economic accounting department, therefore the input-output association structure characters of the national economic departments in our country are sufficiently explored.",https://ieeexplore.ieee.org/document/4598242/,2008 Chinese Control and Decision Conference,2-4 July 2008,ieeexplore
10.1109/AIRE.2014.6894851,Transferring research into the real world: How to improve RE with AI in the automotive industry,IEEE,Conferences,"For specifications, people use natural language. We show that processing natural language and combining this with intelligent deduction and reasoning with ontologies can possibly replace some manual processes associated with requirements engineering (RE). Our prior research shows that the software tools we developed can indeed solve problems in the RE process. This paper shows this does not only work in the software engineering domain, but also for embedded software in the automotive industry. We use artificial intelligence in the sense of combining semantic knowledge from ontologies and natural language processing. This enables computer systems to “understand” requirement texts and process these with “common sense”. Our specification improver RESI detects flaws in texts such as ambiguous words, incomplete process words, and erroneous quantifiers and determiners.",https://ieeexplore.ieee.org/document/6894851/,2014 IEEE 1st International Workshop on Artificial Intelligence for Requirements Engineering (AIRE),26-26 Aug. 2014,ieeexplore
10.1109/ICMAE.2017.8038685,Using artificial intelligence based expert system for selection of design subcontractors: A case study in aerospace industry,IEEE,Conferences,"As one of the top expectations for type certification of an aircraft, Aviation Authorities (AA) regulate design organization to establish Design Assurance System (DAS). DAS is composed of design, independent monitoring and airworthiness functions in which these functions are specialized for aerospace industry. Besides, Design Organization Approval (DOA) is a milestone to establish a rigid Design Assurance System. By this way, design organization assures aircraft development life cycle by complying with aviation regulations. To meet requirements of Design Organization Approval, Design Organization transfers its authority and technical signatories to its subcontractors to improve effectiveness of the system. So, performance of design subcontractors shall be traceable and measurable to match capability requirements of main contractor. Thus, subcontractor evaluation is a long and complicated process; survey implementation could be misleading in some cases. The purpose of this study is to propose a novel tool to measure performance of a design subcontractor according to necessities of Design Assurance System. Up to now, there is no tool to evaluate aviation design subcontractors. With this tool, contractor firm can evaluate multiple criteria in a single run. AHP is used to prioritize criteria relative to each other one-by-one. Then, for subcontractor selection and subcontractor monitoring, Artificial Neural Network (ANN) is applied to optimize decision making process. Annual Actual Data is applied in AHP model to assess current performance score of subcontractor. To have a long term judgment of this system, the model shall be applied to a design subcontractor for more than once on fixed periods such as quarterly, yearly etc.",https://ieeexplore.ieee.org/document/8038685/,2017 8th International Conference on Mechanical and Aerospace Engineering (ICMAE),22-25 July 2017,ieeexplore
10.1109/ISWCS.2019.8877305,Visible Light Positioning for Location-Based Services in Industry 4.0,IEEE,Conferences,"Industry 4.0 refers to the evolution in manufacturing from computerization to fully cyberphysical systems that exploit rich sensor data, adaptive real-time safety-critical control, and machine learning. An important aspect of this vision is the sensing and subsequent association of objects in the physical world with their cyber and virtual counterparts. In this paper we propose Visible Light Positioning (VLP) as an enabler for these Industry 4.0 applications. We also explore sensing techniques, including cameras (and depth sensors), and other light-based solutions for object positioning and detection along with their respective limitations. We then demonstrate an application of positioning for real time robot control in an interactive multiparty cyber-physical-virtual deployment. Lastly, based on our experience with this cyberphysical-virtual application, we propose Ray-Surface Positioning (RSP), a novel VLP technique, as a low cost positioning system for Industry 4.0.",https://ieeexplore.ieee.org/document/8877305/,2019 16th International Symposium on Wireless Communication Systems (ISWCS),27-30 Aug. 2019,ieeexplore
10.1109/BDCloud.2018.00136,"iSTEP, an Integrated Self-Tuning Engine for Predictive Maintenance in Industry 4.0",IEEE,Conferences,"The recent expansion of IoT-enabled (Internet of Things) devices in manufacturing contexts and their subsequent data-driven exploitation paved the way to the advent of the Industry 4.0, promoting a full integration of IT services, smart devices, and control systems with physical objects, their electronics and sensors. The real-time transmission and analysis of collected data from factories has the potential to create manufacturing intelligence, of which predictive maintenance is an expression. Hence the need to design new approaches able to manage not only the data volume, but also the variety and velocity, extracting actual value from the humongous amounts of collected data. To this aim, we present iSTEP, an integrated Self-Tuning Engine for Predictive maintenance, based on Big Data technologies and designed for Industry 4.0 applications. The proposed approach targets some of the most common needs of manufacturing enterprises: compatibility with both the on-premises and the in-the-cloud environments, exploitation of reliable and largely supported Big Data platforms, easy deployment through containerized software modules, virtually unlimited horizontal scalability, fault-tolerant self-reconfiguration, flexible yet friendly streaming-KPI computations, and above all, the integrated provisioning of self-tuning machine learning techniques for predictive maintenance. The current implementation of iSTEP exploits a distributed architecture based on Apache Kafka, Spark Streaming, MLlib, and Cassandra; iSTEP provides (i) a specific feature engineering block aimed at automatically extracting metrics from the production monitoring time series, which improves the predictive performance by 77% on average, and (ii) a self-tuning approach that dynamically selects the best prediction algorithm, which improves the predictive performance up to 60%. The iSTEP engine provides transparent predictive models, able to provide end users with insights into the knowledge learned, and it has been experimentally evaluated on a public unbalanced failure dataset, whose extensive results are discussed in the paper.",https://ieeexplore.ieee.org/document/8672266/,"2018 IEEE Intl Conf on Parallel & Distributed Processing with Applications, Ubiquitous Computing & Communications, Big Data & Cloud Computing, Social Computing & Networking, Sustainable Computing & Communications (ISPA/IUCC/BDCloud/SocialCom/SustainCom)",11-13 Dec. 2018,ieeexplore
10.1109/TLA.2021.9480156,A Data Governance Framework for Industry 4.0,IEEE,Journals,"The fourth industrial revolution, or Industry 4.0, represents a new stage of evolution in the organization, management and control of the value chain throughout the product or service life cycle. This digitization of the industrial environment is characterized by the connection of Information Technologies (IT) and Operations Technologies (OT) through cyber-physical systems and the Industrial IoT (IIoT). One of the main consequences of this integration is the increasing amount and variety of data generated in real time from different sources. In this environment of intensive generation of actionable information, data becomes a critical asset for Industry 4.0, at all stages of the value chain. However, in order to data become a competitive advantage for the company, it must be managed and governed like any other strategic asset, and therefore it is necessary to rely on a Data Governance system. Industry 4.0 requires a reformulation of governance since the data is a key element and the backbone of the processes of the organization. This paper proposes a Reference Framework for the implementation of Data Governance Systems for Industry 4.0. Previously, it contextualizes data governance for Industry 4.0 environments and identifies the requirements that this framework must address, which are conditioned by the specific features of Industry 4.0, among others, the intensive use of big data, cloud and edge computing, artificial intelligence and current regulations.",https://ieeexplore.ieee.org/document/9480156/,IEEE Latin America Transactions,Dec. 2021,ieeexplore
10.1109/ACCESS.2021.3121302,A Survey on Industry 4.0 for the Oil and Gas Industry: Upstream Sector,IEEE,Journals,"The market volatility in the oil and gas (O&amp;G) sector, the dwindling demand for oil due to the impact of COVID-19, and the push for alternative greener energy are driving the need for innovation and digitization in the O&amp;G industry. This has attracted research interest from academia and the industry in the application of industry 4.0 (I4.0) technologies in the O&amp;G sector. The application of some of these I4.0 technologies has been presented in the literature, but the domain still lacks a comprehensive survey of the application of I4.0 in the O&amp;G upstream sector. This paper investigates the state-of-the-art efforts directed toward I4.0 technologies in the O&amp;G upstream sector. To achieve this, first, an overview of the I4.0 is discussed followed by a systematic literature review from an integrative perspective for publications between 2012–2021 with 223 analyzed documents. The benefits and challenges of the adoption of I4.0 have been identified. Moreover, the paper adds value by proposing a framework for the implementation of I4.0 in the O&amp;G upstream sector. Finally, future directions and research opportunities such as framework, edge computing, quantum computing, communication technologies, standardization, and innovative areas related to the implementation of I4.0 in the upstream sector are presented. The findings from this review show that I4.0 technologies are currently being explored and deployed for various aspects of the upstream sector. However, some of the I4.0 technologies like additive manufacturing and virtual reality are least explored.",https://ieeexplore.ieee.org/document/9579415/,IEEE Access,2021,ieeexplore
10.1109/TII.2019.2962029,An Entropy-Based Approach to Real-Time Information Extraction for Industry 4.0,IEEE,Journals,"Industry 4.0 has drawn considerable attention from industry and academic research communities. The recent advances in Internet of Things (IoT), Big Data analytics, sensor technology, and artificial intelligence have led to the design and implementation of novel approaches to take full advantage of data-driven solutions applicable to Industry 4.0. With the availability of large datasets, it has become crucially important to identify the appropriate amount of relevant information, which would optimize the overall analysis of the corresponding systems. In this article, specific properties of dynamically evolving data systems are introduced and investigated, which provide framework to assess the appropriate amount of representative information.",https://ieeexplore.ieee.org/document/8941297/,IEEE Transactions on Industrial Informatics,Sept. 2020,ieeexplore
10.1109/TCPMT.2020.3047089,Automatic Industry PCB Board DIP Process Defect Detection System Based on Deep Ensemble Self-Adaption Method,IEEE,Journals,"A deep ensemble convolutional neural network (CNN) model to inspect printed circuit board (PCB) board dual in-line package (DIP) soldering defects with Hybrid-YOLOv2 (YOLOv2 as a foreground detector and ResNet-101 as a classifier) and Faster RCNN with ResNet-101 and Feature Pyramid Network (FPN) (FRRF) achieved a detection rate of 97.45% and a false alarm rate (FAR) of 20%-30% in the previous study [34]. However, applying the method to other production lines, environmental variations, such as lighting, orientations of the sample feeds, and mechanical deviations, led to the degradation in detection performance. This article proposes an effective self-adaption method that collects “exception data” like the samples with which the Artificial Intelligent (AI) model made mistakes from the automated optical inspection inference edge to the training server, retraining with exceptions on the server and deploying back to the edge. The proposed defect detection system has been verified with real tests that achieved a detection rate of 99.99% with an FAR 20%-30% and less than 15 s of inspection time on a resolution $7296 \times 6000$ PCB image. The proposed system has proven capable of shortening inspection and repair time for online operators, where a 33% efficiency boost from the three production lines of the collaborated factory has been reported [6]. The contribution of the proposed retraining mechanism is threefold: 1) because the retraining process directly learns from the exceptions, the model can quickly adapt to the characteristic of each production line, leading to a fast and reliable mass deployment; 2) the proposed retraining mechanism is a necessary self-service for conventional users as it incrementally improves the detection performance without professional guidance or fine-tuning; and 3) the semiautomatic exception data collection method helps to reduce the time-consuming manual labeling during the retraining process.",https://ieeexplore.ieee.org/document/9306873/,"IEEE Transactions on Components, Packaging and Manufacturing Technology",Feb. 2021,ieeexplore
10.1109/TII.2018.2807797,Deep Endoscope: Intelligent Duct Inspection for the Avionic Industry,IEEE,Journals,"We present the first autonomous endoscope for the visual inspection of very small ducts and cavities, up to a 6-mm diameter. The system has been designed, implemented, and tested in a challenging industrial scenario and in strict collaboration with an avionic industry partner. The inspected objects are metallic gearboxes eventually presenting different residuals (e.g., sand, machining swarfs, and metallic dust) inside the oil ducts. The automatic system is actuated by a robotic arm that moves the endoscope with a microcamera inside the gearbox duct, while a deep-learning-based spatio-temporal image analysis module detects, classifies, and localizes defects in real time. Feedback is given to the robotic arm in order to move or extract the endoscope given the detected anomalies. Evaluation provides a detection rate of nearly 98% given different tests with different types of residuals and duct structures.",https://ieeexplore.ieee.org/document/8295126/,IEEE Transactions on Industrial Informatics,April 2018,ieeexplore
10.1109/ACCESS.2020.2998723,"Digital Twin for the Oil and Gas Industry: Overview, Research Trends, Opportunities, and Challenges",IEEE,Journals,"With the emergence of industry 4.0, the oil and gas (O&amp;G) industry is now considering a range of digital technologies to enhance productivity, efficiency, and safety of their operations while minimizing capital and operating costs, health and environment risks, and variability in the O&amp;G project life cycles. The deployment of emerging technologies allows O&amp;G companies to construct digital twins (DT) of their assets. Considering DT adoption, the O&amp;G industry is still at an early stage with implementations limited to isolated and selective applications instead of industry-wide implementation, limiting the benefits from DT implementation. To gain the full potential of DT and related technological adoption, a comprehensive understanding of DT technology, the current status of O&amp;G-related DT research activities, and the opportunities and challenges associated with the deployment of DT in the O&amp;G industry are of paramount importance. In order to develop this understanding, this paper presents a literature review of DT within the context of the O&amp;G industry. The paper follows a systematic approach to select articles for the literature review. First, a keywords-based publication search was performed on the scientific databases such as Elsevier, IEEE Xplore, OnePetro, Scopus, and Springer. The filtered articles were then analyzed using online text analytic software (Voyant Tools) followed by a manual review of the abstract, introduction and conclusion sections to select the most relevant articles for our study. These articles and the industrial publications cited by them were thoroughly reviewed to present a comprehensive overview of DT technology and to identify current research status, opportunities and challenges of DT deployment in the O&amp;G industry. From this literature review, it was found that asset integrity monitoring, project planning, and life cycle management are the key application areas of digital twin in the O&amp;G industry while cyber security, lack of standardization, and uncertainty in scope and focus are the key challenges of DT deployment in the O&amp;G industry. When considering the geographical distribution for the DT related research in the O&amp;G industry, the United States (US) is the leading country, followed by Norway, United Kingdom (UK), Canada, China, Italy, Netherland, Brazil, Germany, and Saudi Arabia. The overall publication rate was less than ten articles (approximately) per year until 2017, and a significant increase occurred in 2018 and 2019. The number of journal publications was noticeably lower than the number of conference publications, and the majority of the publications presented theoretical concepts rather than the industrial implementations. Both these observations suggest that the DT implementation in the O&amp;G industry is still at an early stage.",https://ieeexplore.ieee.org/document/9104682/,IEEE Access,2020,ieeexplore
10.1109/ACCESS.2021.3103680,Human Centric Digital Transformation and Operator 4.0 for the Oil and Gas Industry,IEEE,Journals,"Working at an oil and gas facility, such as a drilling rig, production facility, processing facility, or storage facility, involves various challenges, including health and safety risks. It is possible to leverage emerging digital technologies such as smart sensors, wearable or mobile devices, big data analytics, cloud computing, extended reality technologies, robotic systems, and drones to mitigate the challenges faced by oil and gas workers. While these technologies are not new to the oil and gas industry, most of its existing digital transformation initiatives follow business or process-centric approaches, in which the critical driver of the technology adoption is the enhancement of production, efficiency, and revenue. As a result, they may not address the challenges faced by the workers. As oil and gas workers are among the essential assets in the oil and gas industry, it is vital to address the challenges faced by these workers. This paper proposes a human-centric digital transformational framework for the oil and gas industry to deploy existing digital technologies to enhance their workers' health, safety, and working conditions. The paper outlines the critical challenges faced by oilfield workers, introduces a system architecture to implements a human-centric digital transformation, discusses the opportunities of the proposed framework, and summarizes the key impediment for the proposed framework.",https://ieeexplore.ieee.org/document/9509417/,IEEE Access,2021,ieeexplore
10.1109/TNNLS.2011.2179309,Hybrid Neural Prediction and Optimized Adjustment for Coke Oven Gas System in Steel Industry,IEEE,Journals,"An energy system is the one of most important parts of the steel industry, and its reasonable operation exhibits a critical impact on manufacturing cost, energy security, and natural environment. With respect to the operation optimization problem for coke oven gas, a two-phase data-driven based forecasting and optimized adjusting method is proposed, where a Gaussian process-based echo states network is established to predict the gas real-time flow and the gasholder level in the prediction phase. Then, using the predicted gas flow and gasholder level, we develop a certain heuristic to quantify the user's optimal gas adjustment. The proposed operation measure has been verified to be effective by experimenting with the real-world on-line energy data sets coming from Shanghai Baosteel Corporation, Ltd., China. At present, the scheduling software developed with the proposed model and ensuing algorithms have been applied to the production practice of Baosteel. The application effects indicate that the software system can largely improve the real-time prediction accuracy of the gas units and provide with the optimized gas balance direction for the energy optimization.",https://ieeexplore.ieee.org/document/6126048/,IEEE Transactions on Neural Networks and Learning Systems,March 2012,ieeexplore
10.1109/JETCAS.2021.3097699,Machine-Learning-Based Microwave Sensing: A Case Study for the Food Industry,IEEE,Journals,"Despite the meticulous attention of food industries to prevent hazards in packaged goods, some contaminants may still elude the controls. Indeed, standard methods, like X-rays, metal detectors and near-infrared imaging, cannot detect low-density materials. Microwave sensing is an alternative method that, combined with machine learning classifiers, can tackle these deficiencies. In this paper we present a design methodology applied to a case study in the food sector. Specifically, we offer a complete flow from microwave dataset acquisition to deployment of the classifiers on real-time hardware and we show the effectiveness of this method in terms of detection accuracy. In the case study, we apply the machine-learning based microwave sensing approach to the case of food jars flowing at high speed on a conveyor belt. First, we collected a dataset from hazelnut-cocoa spread jars which were uncontaminated or contaminated with various intrusions, including low-density plastics. Then, we performed a design space exploration to choose the best MLPs as binary classifiers, which resulted to be exceptionally accurate. Finally, we selected the two most light-weight models for implementation on both an ARM-based CPU and an FPGA SoC, to cover a wide range of possible latency requirements, from loose to strict, to detect contaminants in real-time. The proposed design flow facilitates the design of the FPGA accelerator that might be required to meet the timing requirements by using a high-level approach, which might be suited for the microwave domain experts without specific digital hardware skills.",https://ieeexplore.ieee.org/document/9489295/,IEEE Journal on Emerging and Selected Topics in Circuits and Systems,Sept. 2021,ieeexplore
10.1109/KAMW.2008.4810712,2008 IEEE International Symposium on Knowledge Acquisition and Modeling Workshop proceedings,IEEE,Conferences,"The following concepts are discussed: advanced knowledge modeling languages and tools; knowledge capture through machine learning and knowledge discovery in data bases; specific knowledge modeling issues for CBR systems, cooperative KBS, training applications; knowledge acquisition from texts and WWW; evaluation of methods, techniques and tools for knowledge acquisition; knowledge engineering and software engineering; uncertainty and vagueness aspects of knowledge modeling; knowledge acquisition applications such as library, industry, commerce, government, education and so on; knowledge acquisition in intelligent system; inclusive learning; formal and informal learning; HCI for educational systems; transforming learning through technologies; new generations of educational technologies; Web 2.0 and social computing for learning; educational technologies for new generations; real-time assessment of learning and performance; mobile computing for learning and instruction; personalized educational systems; interdisciplinary programs for educational technologists; CSCL technologies; content authoring technologies; e-pedagogy and instructional design; knowledge management technologies in education; organizational management of e-learning in universities; e- testing and new test theories; data mining, text mining, and web mining in education.",https://ieeexplore.ieee.org/document/4810712/,2008 IEEE International Symposium on Knowledge Acquisition and Modeling Workshop,21-22 Dec. 2008,ieeexplore
10.1109/ITCA52113.2020.00084,5G Enabling Technologies in Rail,IEEE,Conferences,"Leveraging recent advances in IoT, blockchain, big data, artificial intelligence, and others, these state-of-art technologies still have difficulty in massive deployment and real fruition of working together in the industry. 5G brings new opportunities through enabling these technologies and thus leads to new developments. This paper introduces 5G and analyzes how 5G enable other technologies. Besides, it slices complicated railway scenarios into three aspects, and then discusses applications and innovations 5G and technologies can bring to rail.",https://ieeexplore.ieee.org/document/9422090/,2020 2nd International Conference on Information Technology and Computer Application (ITCA),18-20 Dec. 2020,ieeexplore
10.1109/ICITAET47105.2019.9170214,A Basic Permanent Magnets Array Interaction Project for Teaching Artificial Intelligence as a Complementary Model,IEEE,Conferences,"There are new algorithms such as artificial intelligence (AI) methodologies that have achieved accurate representation of experimental systems. On the other hand, undergraduate freshmen students must understand AI methodologies since the industry has developed several products based on those and some academic problems also can be solved using AI. If those students do not learn how to model real systems using AI, they will be losing the opportunity of applying this powerful tool for solving several real problems in their professional life. Since the AI model can be a representation for forecasting the performance of the real model, this model can help the design process and provide information during its operation. This paper proposes an engineering project to teach artificial intelligence algorithms using real systems that are non-linear. Since permanent magnets are used in several applications, they can be attractive for modeling those when they are interacting between them; hence, this paper shows the interaction among them when they are deployed as an electrical power source. Moreover, this source could be classified as a renewable energy source. The basic generation of electrical energy is based on changing the magnetic field. Although the operation principle is basic, the electrical source has a non-liner description that is extremely complex so AI could be applied to create a model that represents those non-linear relationships in a precise manner. The main goal of this work is to describe an undergraduate project that can be used for teaching how to model a real system using AI algorithms. The main characteristics and properties of the permanent magnets are studied for the comprehension of how magnets can be implemented. It is also examined the viability for the construction of an electric motor using only permanent magnets, based on the analysis of different designs and materials and finally an AI model is created.",https://ieeexplore.ieee.org/document/9170214/,2019 International Conference on Innovative Trends and Advances in Engineering and Technology (ICITAET),27-28 Dec. 2019,ieeexplore
10.1109/UIC-ATC.2017.8397649,A CNN based bagging learning approach to short-term load forecasting in smart grid,IEEE,Conferences,"Short-term load forecasting in smart grid is key to electricity dispatch scheduling, reliability analysis, and maintenance planning for the generators. In this paper, we present a convolutional neural networks (CNN) based bagging model for forecasting hourly loads. We employ CNN to train forecasting models on big load data sets. Then, we segment a real industry load data set into many subsets, fine-tune the forecasting models on these subsets to learn weak forecasting models, and assemble these weak forecasting models to conduct a bagging forecasting model, where the learning and assembling procedures are implemented on Spark. Specifically, all load samples in those data sets are reorganized as images with respect to similarities between relations of pixels in images and those of features in load samples. Experimental results indicate the effectiveness of the proposed method.",https://ieeexplore.ieee.org/document/8397649/,"2017 IEEE SmartWorld, Ubiquitous Intelligence & Computing, Advanced & Trusted Computed, Scalable Computing & Communications, Cloud & Big Data Computing, Internet of People and Smart City Innovation (SmartWorld/SCALCOM/UIC/ATC/CBDCom/IOP/SCI)",4-8 Aug. 2017,ieeexplore
10.1109/GIOTS.2018.8534533,A Car as a Semantic Web Thing: Motivation and Demonstration,IEEE,Conferences,"Car signal data is usually hard to access, understand and integrate for non automotive domain experts. In this paper, we use semantic technologies for enriching signal data in the automotive industry and access it through Web of Things interactions. This combination allows the access and integration of car data from the web. We built VSSo, a Vehicle Signal ontology based on SOSA/SSN Observations and Actuations, and generated WoT Actions, Events and Properties, enriched with domain metadata. We mapped VSSo to a Web of Things ontology and we developed a Web of Things protocol binding with LwM2M, and made an implementation in a real car. This implementation resulted in a first working prototype, and a number of future improvements required in order to be compliant with automotive standards.",https://ieeexplore.ieee.org/document/8534533/,2018 Global Internet of Things Summit (GIoTS),4-7 June 2018,ieeexplore
10.1109/PHM-Paris.2019.00052,A Common Service Middleware for Intelligent Complex Software System,IEEE,Conferences,"With the rapid development of the Internet of Things (IoT) and artificial intelligence (AI) technology, various intelligent complex software systems (i-CSS) are increasingly popular, becoming one of the most important software system development paradigms. Its inherent growth construction and adaptive evolution properties pose new challenges to existing software design and development methods. Especially, how to achieve growth construction by quickly reusing existing excellent software resources, and how to establish data flow across system boundaries around the business flow to achieve adaptive evolution based on data intelligence. Facing the above challenges, this paper proposes novel data-oriented analysis and design method (DOAD), microservice and container-based mashup development method (SCMD). On this basis, the paper implements i-CSS common service middleware to support the above methods in engineering. In a real cloud-based PHM system and the other three industry projects, the proposed methods and middleware are used for application verification, the results show that they can greatly reduce the complexity of i-CSS design and development, reduce the ability threshold of the i-CSS development team, improve the development efficiency of the development team, reduce the team development workload by 31.5% on average, and help the i-CSS team effectively cope with the challenges of growth construction and adaptive evolution.",https://ieeexplore.ieee.org/document/8756426/,2019 Prognostics and System Health Management Conference (PHM-Paris),2-5 May 2019,ieeexplore
10.1109/RE48521.2020.00029,A Deep Context-wise Method for Coreference Detection in Natural Language Requirements,IEEE,Conferences,"Requirements are usually written by different stakeholders with diverse backgrounds and skills and evolve continuously. Therefore inconsistency caused by specialized jargons and different domains, is inevitable. In particular, entity coreference in Requirement Engineering (RE) is that different linguistic expressions refer to the same real-world entity. It leads to misconception about technical terminologies, and impacts the readability and understandability of requirements negatively. Manual detection entity coreference is labor-intensive and time-consuming. In this paper, we propose a DEEP context-wise semantic method named DeepCoref to entity COREFerence detection. It consists of one fine-tuning BERT model for context representation and a Word2Vec-based network for entity representation. We use a multi-layer perception in the end to fuse and make a trade-off between two representations for obtaining a better representation of entities. The input of the network is requirement contextual text and related entities, and the output is the predictive label to infer whether two entities are coreferent. The evaluation on industry data shows that our approach significantly outperforms three baselines with average precision and recall of 96.10% and 96.06% respectively. We also compare DeepCoref with three variants to demonstrate the performance enhancement from different components.",https://ieeexplore.ieee.org/document/9218208/,2020 IEEE 28th International Requirements Engineering Conference (RE),31 Aug.-4 Sept. 2020,ieeexplore
10.1109/IS.2018.8710526,A Digital Twin-based Privacy Enhancement Mechanism for the Automotive Industry,IEEE,Conferences,"This paper discusses a Digital Twin demonstrator for privacy enhancement in the automotive industry. Here, the Digital Twin demonstrator is presented as a method for the design and implementation of privacy enhancement mechanisms, and is used to detect privacy concerns and minimize breaches and associated risks to which smart car drivers can be exposed through connected infotainment applications and services. The Digital Twin-based privacy enhancement demonstrator is designed to simulate variety of conditions that can occur in the smart car ecosystem. We firstly identify the core stakeholders (actors) in the smart car ecosystem, their roles and exposure to privacy vulnerabilities and associated risks. Secondly, we identify assets that consume and generate sensitive privacy data in smart cars, their functionalities, and relevant privacy concerns and risks. Thirdly, we design an infrastructure for collecting (i) real-time sensor data from smart cars and their assets, and (ii) environmental data, road and traffic data, generated through operational driving lifecycle. In order to ensure compliance of the collected data with privacy policies and regulations, e.g. with GDPR requirements for enforcement of the data subject's rights, we design methods for the Digital Twin-based privacy enhancement demonstrator that are based on behavioural analytics informed by GDPR. We also perform data anonymization to minimize privacy risks and enable actions such as sending an automatic informed consent to the stakeholders.",https://ieeexplore.ieee.org/document/8710526/,2018 International Conference on Intelligent Systems (IS),25-27 Sept. 2018,ieeexplore
10.1109/BigData47090.2019.9005598,A Dynamic Neural Network Model for Click-Through Rate Prediction in Real-Time Bidding,IEEE,Conferences,"Real-time bidding (RTB) that features perimpression-level real-time ad auctions has become a popular practice in today's digital advertising industry. In RTB, click-through rate (CTR) prediction is a fundamental problem to ensure the success of an ad campaign and boost revenue. In this paper, we present a dynamic CTR prediction model designed for the Samsung demand-side platform (DSP). From our production data, we identify two key technical challenges that have not been fully addressed by the existing solutions: the dynamic nature of RTB and user information scarcity. To address both challenges, we develop a Dynamic Neural Network model. Our model effectively captures the dynamic evolutions of both users and ads and integrates auxiliary data sources (e.g., installed apps) to better model users' preferences. We put forward a novel interaction layer that fuses both explicit user responses (e.g., clicks on ads) and auxiliary data sources to generate consolidated user preference representations. We evaluate our model using a large amount of data collected from the Samsung advertising platform and compare our method against several state-of-the-art methods that are likely suitable for real-world deployment. The evaluation results demonstrate the effectiveness of our method and the potential for production. In addition, we discuss how to address a few practical engineering challenges caused by big data toward making our model in readiness for deployment.",https://ieeexplore.ieee.org/document/9005598/,2019 IEEE International Conference on Big Data (Big Data),9-12 Dec. 2019,ieeexplore
10.1109/ICCCBDA51879.2021.9442522,A FPGA Deployment System Based on Convolutional Neural Network for Rolling Bearing Diagnosis,IEEE,Conferences,"Real-time fault diagnosis of rolling bearing is a challenging issue for industry. Although artificial intelligence-based technologies could be well used for fault diagnosis of rolling bearing, the factories may not take into account the deployment of diagnosis algorithms. To tackle the issue, this work proposes a flexible deployment system of diagnosis algorithm for rolling bearing, where the required Convolutional Neural Network (CNN) model is deployed on the Field Programmable Gate Array (FPGA) to identify the working conditions of rolling bearing using vibration signals. The experimental results show that the deployed system performs accurately and efficiently on the test set, while a real-time prediction of FPGA could be guaranteed, indicating its potential as a powerful auxiliary system of rotating machinery.",https://ieeexplore.ieee.org/document/9442522/,2021 IEEE 6th International Conference on Cloud Computing and Big Data Analytics (ICCCBDA),24-26 April 2021,ieeexplore
10.1109/CRV.2015.42,A Hidden Markov Model for Vehicle Detection and Counting,IEEE,Conferences,"To reduce roadway congestion and improve traffic safety, accurate traffic metrics, such as number of vehicles travelling through lane-ways, are required. Unfortunately most existing infrastructure, such as loop-detectors and many video detectors, do not feasibly provide accurate vehicle counts. Consequently, a novel method is proposed which models vehicle motion using hidden Markov models (HMM). The proposed method represents a specified small region of the roadway as 'empty', 'vehicle entering', 'vehicle inside', and 'vehicle exiting', and then applies a modified Viterbi algorithm to the HMM sequential estimation framework to initialize and track vehicles. Vehicle observations are obtained using an Adaboost trained Haar-like feature detector. When tested on 88 hours of video, from three distinct locations, the proposed method proved to be robust to changes in lighting conditions, moving shadows, and camera motion, and consistently out-performed Multiple Target Tracking (MTT) and Virtual Detection Line(VDL) implementations. The median vehicle count error of the proposed method is lower than MTT and VDL by 28%, and 70% respectively. As future work, this algorithm will be implemented to provide the traffic industry with improved automated vehicle counting, with the intent to eventually provide real-time counts.",https://ieeexplore.ieee.org/document/7158929/,2015 12th Conference on Computer and Robot Vision,3-5 June 2015,ieeexplore
10.1109/SiPS47522.2019.9020540,A Hybrid GPU + FPGA System Design for Autonomous Driving Cars,IEEE,Conferences,"Autonomous driving cars need highly complex hardware and software systems, which require high performance computing platforms in order to enable a real time AI-based perception and decision making pipeline. The industry has been exploring various in-vehicle accelerators such as GPUs, ASICs and FPGAs. Yet the autonomous driving platform design is far from mature when taking into account of system reliability, redundancy and higher level of autonomy. In this work, we propose a hybrid computing system design, which integrates a GPU as the primary computing system and a FPGA as a secondary system. This hybrid system architecture has multiple advantages: 1) The FPGA can be constantly running as a complementary system with very short latency, helping to detect main system failure and anomalous behavior, contributing to system functionality verification and reliability. 2) If the primary system fails (mostly from sensor or interconnection error), the FPGA will quickly detect the failure and run a safe-mode task with a subset of sensors. 3) The FPGA can be used as an independent computing system to run extra algorithm components to improve the overall system autonomy. For example, FPGA can handle driver monitoring tasks while GPU focuses on driving functions. Together they can boost the driving function from L2 (constantly requires driver's attention) to L3 (allows driver to mind off for 10 seconds). This paper defines how such a system works, discusses various use cases and potential design challenges, and shares some initial results and insights about how to make such a system deliver the maximum value for autonomous driving.",https://ieeexplore.ieee.org/document/9020540/,2019 IEEE International Workshop on Signal Processing Systems (SiPS),20-23 Oct. 2019,ieeexplore
10.1109/HPCC.and.EUC.2013.124,A Hypervisor for MIPS-Based Architecture Processors - A Case Study in Loongson Processors,IEEE,Conferences,"Loongson is a family of general purpose processors based on MIPS architecture designed and manufactured in Mainland China. With the maturity of Loongson CPUs, applications are widely available with the increasing development of software tools and hardware platforms by research teams in academia and industry. In recent years, products based on Loongson have been mainly used in education, personal computers and server systems. Meanwhile, it is not yet popularly used in industrial real-time control fields, so such products have large room and potential to further development and deployment. The M-Hyper visor discussed in this paper is a real-time hyper visor designed for MIPS architecture and implemented in Loongson2F processor. It is based on the management program of para-virtualization whilst multiple partitions are scheduled to execute according to their priorities. The design and implementation of M-Hyper visor is discussed, along with details as timer, interrupts, memory management, partition loading and scheduling, to enrich real-time virtualized applications for MIPS architecture. Evaluation results show the performance and viability of proposed design, being promising to new deployments.",https://ieeexplore.ieee.org/document/6832006/,2013 IEEE 10th International Conference on High Performance Computing and Communications & 2013 IEEE International Conference on Embedded and Ubiquitous Computing,13-15 Nov. 2013,ieeexplore
10.1109/IECON.2018.8592763,A Machine Learning Approach Applied to Energy Prediction in Job Shop Environments,IEEE,Conferences,"Energy efficiency has become a great challenge for manufacturing companies. Although it is possible to improve efficiency applying new and more efficient machines, decision makers tend to look for some less expensive alternatives. In this context, the adoption of more efficient strategies during the production planning can allow the reduction in energy consumption and associated emissions. Furthermore, the current reality of manufacturing companies, brought by Industry 4.0 concepts, requires more flexibility of production systems, thus, increasing complexity for machine rescheduling without compromising sustainable requirements. In this paper, we propose a method to predict total energy consumption in job shop systems applying machine learning techniques. Different schedules may result in different consumption rates. However, there is a nonlinear relationship between these targets. Therefore, an Artificial Neural Network (ANN) is applied for a quick estimation of total energy consumption. In order to validate the model, computational experiments, using digital manufacturing software tools, are performed on different job shop configurations to show the efficiency of the proposed model.",https://ieeexplore.ieee.org/document/8592763/,IECON 2018 - 44th Annual Conference of the IEEE Industrial Electronics Society,21-23 Oct. 2018,ieeexplore
10.23919/DATE48585.2020.9116539,A Machine Learning Based Write Policy for SSD Cache in Cloud Block Storage,IEEE,Conferences,"Nowadays, SSD cache plays an important role in cloud storage systems. The associated write policy, which enforces an admission control policy regarding filling data into the cache, has a significant impact on the performance of the cache system and the amount of write traffic to SSD caches. Based on our analysis on a typical cloud block storage system, approximately 47.09% writes are write-only, i.e., writes to the blocks which have not been read during a certain time window. Naively writing the write-only data to the SSD cache unnecessarily introduces a large number of harmful writes to the SSD cache without any contribution to cache performance. On the other hand, it is a challenging task to identify and filter out those write-only data in a real-time manner, especially in a cloud environment running changing and diverse workloads.In this paper, to alleviate the above cache problem, we propose an ML-WP, Machine Learning Based Write Policy, which reduces write traffic to SSDs by avoiding writing write-only data. The main challenge in this approach is to identify write-only data in a real-time manner. To realize ML-WP and achieve accurate write-only data identification, we use machine learning methods to classify data into two groups (i.e., write-only and normal data). Based on this classification, the write-only data is directly written to backend storage without being cached. Experimental results show that, compared with the industry widely deployed write-back policy, ML-WP decreases write traffic to SSD cache by 41.52%, while improving the hit ratio by 2.61% and reducing the average read latency by 37.52%.",https://ieeexplore.ieee.org/document/9116539/,"2020 Design, Automation & Test in Europe Conference & Exhibition (DATE)",9-13 March 2020,ieeexplore
10.1109/ISCAS.2018.8351785,A Machine Learning-Based Strategy for Efficient Resource Management of Video Encoding on Heterogeneous MPSoCs,IEEE,Conferences,"The design of new streaming systems is becoming a major area of research to deploy services targeted in the Internet-of-Things (IoT) era. In this context, the new High Efficiency Video Coding (HEVC) standard provides high efficiency and scalability of quality at the cost of increased computational complexity for edge nodes, which is a new challenge for the design of IoT systems. The usage of hardware acceleration in conjunction with general-purpose cores in Multiprocessor Systems-on-Chip (MP-SoCs) is a promising solution to create heterogeneous computing systems to manage the complexity of real-time streaming for high-end IoT systems, achieving higher throughput and power efficiency when compared to conventional processors alone. Furthermore, Machine Learning (ML) provides a promising solution to efficiently use this next-generation of heterogeneous MPSoC designs that the EDA industry is developing by dynamically optimizing system performance under diverse requirements such as frame resolution, search area, operating frequency and stream allocation. In this work, we propose an ML-based approach for stream allocation and Dynamic Voltage and Frequency Scaling (DVFS) management on a heterogeneous MPSoC composed of ARM cores and FPGA fabric containing hardware accelerators for the motion estimation of HEVC encoding. Our experiments on a Zynq7000 SoC outline 20% higher throughput when compared to the state-of-the-art streaming systems for next-generation IoT devices.",https://ieeexplore.ieee.org/document/8351785/,2018 IEEE International Symposium on Circuits and Systems (ISCAS),27-30 May 2018,ieeexplore
10.1109/CNS48642.2020.9162309,A Machine Learning-based Approach for Automated Vulnerability Remediation Analysis,IEEE,Conferences,"Security vulnerabilities in firmware/software pose an important threat ton power grid security, and thus electric utility companies should quickly decide how to remediate vulnerabilities after they are discovered. Making remediation decisions is a challenging task in the electric industry due to the many factors to consider, the balance to maintain between patching and service reliability, and the large amount of vulnerabilities to deal with. Unfortunately, remediation decisions are current manually made which take a long time. This increases security risks and incurs high cost of vulnerability management. In this paper, we propose a machine learning-based automation framework to automate remediation decision analysis for electric utilities. We apply it to an electric utility and conduct extensive experiments over two real operation datasets obtained from the utility. Results show the high effectiveness of the solution.",https://ieeexplore.ieee.org/document/9162309/,2020 IEEE Conference on Communications and Network Security (CNS),29 June-1 July 2020,ieeexplore
10.1109/ICCEAI52939.2021.00004,A Method for Designing and Analyzing Automotive Software Architecture: A Case Study for an Autonomous Electric Vehicle,IEEE,Conferences,"Software complexity is increased in automotive systems because many software functions are required for autonomous driving, electrified vehicles, and connected cars. In addition, autonomous driving requires centralized software that generally decreases evolvability with many connections. Thus, the automotive industry adopted the microservice architecture within the service-oriented architecture (SOA), which was already being used in distributed computing environments in the information and communication technology (ICT) industry. However, the software characteristics of an automotive system are different from those of an ICT system. Automotive software generally fulfills safety and real-time requirements that are not required in ICT software. Another challenge is integrating electric control units (ECUs) because software platforms supporting SOA require relatively high computational power and network bandwidth, which increases ECU cost. Thus, the deployment of software functions must be considered before integrating ECUs to find an optimal design solution for evolvability, dependability, real-time performance, cost, etc. However, many OEMs integrate ECUs based on deploying vehicular features without software architecture. It causes optimality problems during integrating ECUs. We propose component-based sensor-process-actuator architectural style for high-level architecture to handle quality attributes. Software architecture for an autonomous electrified vehicle will be presented with the proposed architectural style. The architecture is used to deploy software components and integrated ECUs with empirical quantitative analysis. Four design patterns for dependability with the architectural style will also be introduced.",https://ieeexplore.ieee.org/document/9544320/,2021 International Conference on Computer Engineering and Artificial Intelligence (ICCEAI),27-29 Aug. 2021,ieeexplore
10.1109/EDUCON46332.2021.9454147,"A Mixed Reality Approach Enriching the Agricultural Engineering Education Paradigm, against the COVID-19 Constraints",IEEE,Conferences,"Since the very early beginning of the mankind history, any great difficulty, like wars or diseases, had to be a challenge for progress and innovation, otherwise the game was lost. In this regard, the recent COVID-19 pandemic provides to the learners' and teachers' community a great opportunity to better adapt and enrich their educational practices. Initially, aiming to assist students of agricultural engineering to demystify the innovative technologies of their scientific area, a remotely programmed and controlled robotic arm platform for fruit-picking purposes is deployed. This is just the excuse behind which a colorful bouquet of modern and software and hardware components are glued together to provide the potential for supporting a wide range of modern engineering applications. In an era that the speed of the technological achievements makes difficult to categorize their impact in industry, society or education, the proposed approach can be classified as containing mainly mixed reality, mobile, blended and project-based learning characteristics. A first set of results indicate that the discussed platform can greatly assist the students to tackle the lack of physical presence in the laboratory/classroom providing a quite interesting alternative to full in-vitro educational practices.",https://ieeexplore.ieee.org/document/9454147/,2021 IEEE Global Engineering Education Conference (EDUCON),21-23 April 2021,ieeexplore
10.1109/COMPSAC48688.2020.0-202,A Modular Edge-/Cloud-Solution for Automated Error Detection of Industrial Hairpin Weldings using Convolutional Neural Networks,IEEE,Conferences,"The traction battery and the electric motor are the most important components of the electrified powertrain. To increase the energy efficiency of the electric motor, wound copper wires are being replaced by coated rectangular copper wires, so-called hairpins. Hence, to connect the hairpins conductively, they must be welded together. However, such new production processes are unknown compared with classic motor production. Therefore, this research aims to integrate Industry 4.0 techniques, such as cloud and edge computing, and advanced data analysis in the production process to better understand and optimize the manufacturing processes. Welding defects are classified with the help of a convolutional neural network (CNN) (predictive analysis) and, depending on the defect, a recommended course of action for reworking (prescriptive analysis) is given. However, the application of such complex algorithms as neural networks to large amounts of data requires huge computing resources. Therefore, a modular combination of an edge and cloud architecture is proposed in this paper. Furthermore, a pure cloud solution is compared with the edge solution.",https://ieeexplore.ieee.org/document/9202655/,"2020 IEEE 44th Annual Computers, Software, and Applications Conference (COMPSAC)",13-17 July 2020,ieeexplore
10.1109/ISIC.2008.4635950,A Multi-agent System for Integrated Control and Asset Management of Petroleum Production Facilities - Part 1: Prototype Design and Development,IEEE,Conferences,"This three-part paper thoroughly addresses the design and development of multi-agent system for asset management for the petroleum industry, which is crucial for profitable oil and gas facilities operations and maintenance. A research project was initiated to study the feasibility of an intelligent asset management system. Having proposed a conceptual model, architecture, and implementation plan for such a system in previous work [1], [2], [3], defined its autonomy, communications, and artificial intelligence (AI) requirements [4], [5], and initiated the preliminary design of a simple system prototype [6], we are extending the build of a system prototype and simulate it in real-time to validate its logical behavior in normal and abnormal process situations and analyze its performance.",https://ieeexplore.ieee.org/document/4635950/,2008 IEEE International Symposium on Intelligent Control,3-5 Sept. 2008,ieeexplore
10.1109/ISIC.2008.4635951,A Multi-agent System for Integrated Control and Asset Management of Petroleum Production Facilities - Part 2: Prototype Design Verification,IEEE,Conferences,"This three-part paper thoroughly addresses the design and development of multi-agent system for asset management for the petroleum industry, which is crucial for profitable oil and gas facilities operations and maintenance. A research project was initiated to study the feasibility of an intelligent asset management system. Having proposed a conceptual model, architecture, and implementation plan for such a system defined its autonomy, communications, and artificial intelligence (AI) requirements, and initiated the preliminary design of a simple system prototype, we are extending the build of a system prototype and simulate it in real-time to validate its logical behavior in normal and abnormal process situations and analyze its performance. The second-part paper addresses the ICAM system prototype design verification and its logical behavior during sensor faults in the plant.",https://ieeexplore.ieee.org/document/4635951/,2008 IEEE International Symposium on Intelligent Control,3-5 Sept. 2008,ieeexplore
10.1109/ISIC.2008.4635952,A Multi-agent System for Integrated Control and Asset Management of Petroleum Production Facilities - Part 3: Performance Analysis and System Limitations,IEEE,Conferences,"This three-part paper thoroughly addresses the design and development of multi-agent system for asset management for the petroleum industry, which is crucial for profitable oil and gas facilities operations and maintenance. A research project was initiated to study the feasibility of an intelligent asset management system. Having proposed a conceptual model, architecture, and implementation plan for such a system in previous work (J.H. Taylor and A.F. Sayda, 2005), (A.F. Sayda and J.H. Taylor, 2006), defined its autonomy, communications, and artificial intelligence (AI) requirements and initiated the preliminary design of a simple system prototype (J.H. Taylor and A.F. Sayda, 2008), we are extending the build of a system prototype and simulate it in real-time to validate its logical behavior in normal and abnormal process situations and analyze its performance. The third-part paper addresses the ICAM system prototype validation in terms of system performance analysis and system behavior during unexpected situations.",https://ieeexplore.ieee.org/document/4635952/,2008 IEEE International Symposium on Intelligent Control,3-5 Sept. 2008,ieeexplore
10.1109/IJCNN.2019.8852094,A Music Recommendation System Based on logistic regression and eXtreme Gradient Boosting,IEEE,Conferences,"With the rapid growth of music industry data, it is difficult for people to find their favorite songs in the music library. Therefore, people urgently need an efficient music recommendation system to help them retrieve music. Traditional collaborative filtering algorithms are applied to the field of music recommendation. However, collaborative filtering does not handle data sparse problems very well when new items are introduced. To solve this problem, some people use the logistic regression method as a classifier to predict the user's music preferences to recommend songs. Logistic regression is a linear model that does not handle complex non-linear data features. In this paper, we propose a hybrid LX recommendation algorithm by integrating logistic regression and eXtreme Gradient Boosting(xgboost). A series of experiments are conducted on a real music dataset to evaluate the effectiveness of our proposed LX model. Our results show that the error and AUC of our LX model are better than other methods.",https://ieeexplore.ieee.org/document/8852094/,2019 International Joint Conference on Neural Networks (IJCNN),14-19 July 2019,ieeexplore
10.1109/ISMCR47492.2019.8955729,A Novel Architecture for Condition Based Machinery Health Monitoring on Marine Vessels Using Deep Learning and Edge Computing,IEEE,Conferences,"Condition based machinery health monitoring on marine vessels involves collecting operational sensor data on the vessel using a robust data acquisition system and determining asset health using anomaly detection analytics. Automation and digitalization of marine vessels involve smart digital technologies such as the Internet of Things (IoT) to collect ships' health data and send it over to a central processing location where this data is analyzed. However, it is difficult to apply this to the shipping industry due to offshore data transmission bandwidth challenges. Deep Learning, a technology that can be used to conduct Machinery Health Monitoring (MHM) holds the key to solve the bandwidth problems. In this paper, we investigate the use of Convolutional Neural Networks (CNN) as a practical solution for deploying Smart Health Monitoring on Marine Vessels using the example of electric induction motors. We show a mechanism to develop data-driven deep learning model that can classify if the motor is in a healthy or faulty condition, and propose an architecture to deploy this model on the Marine vessel in real time on an edge computing hardware. While in operation, sensor data from the motor will be fed into the DL Model, and the resulting predictions will be presented in the Vessel Alarm Monitoring System.",https://ieeexplore.ieee.org/document/8955729/,2019 IEEE International Symposium on Measurement and Control in Robotics (ISMCR),19-21 Sept. 2019,ieeexplore
10.1109/ICESS.Symposia.2008.101,A Novel Embedded Maintenance Unit of Hydro Generator Excitation System,IEEE,Conferences,"Hydro generator excitation system is one of the most maintenance intensive subsystem in hydro power plants. A novel embedded maintenance unit of the excitation system is studied. A knowledge bank, which summarizes both the expert knowledge and the analytical knowledge on the failure modes and their manifestations as well as their effects, is established. An expert system is developed to optimize the control parameters, to detect and diagnose the equipment degradations or/and failures, and to give both control and maintenance remedy on base of the on-line state monitoring and performance evaluation of the regulating system. This expert system is integrated into the powerful microcomputer of the regulator and runs in parallel with the regular regulating tasks. A successful application in industry is presented.",https://ieeexplore.ieee.org/document/4627125/,2008 International Conference on Embedded Software and Systems Symposia,29-31 July 2008,ieeexplore
10.1109/ISDA.2008.252,A Novel Fitness Function in Genetic Algorithms to Optimize Neural Networks for Imbalanced Data Sets,IEEE,Conferences,"The imbalanced data sets are often encountered in business, industry and real life applications. In this paper, the novel fitness function in genetic algorithms to optimize neural networks is proposed for solving the classification problems in imbalanced data sets. Not only the parameters of neural networks but also the links-pruning between neurons are regarded as an optimization problem in this study. The fitness function consists of the mean square error, the classification error rate for each class, the distances between the examples and the boundary of classification. The artificial data set and the UCI data sets are used to verify the classifier we proposed. The experimental results showed that the classifier performs better than the conventional back-propagation neural network.",https://ieeexplore.ieee.org/document/4696407/,2008 Eighth International Conference on Intelligent Systems Design and Applications,26-28 Nov. 2008,ieeexplore
10.1109/ITNEC52019.2021.9586993,A Novel Method for Color Forged Image Detection,IEEE,Conferences,"Digital images permeate almost all areas of our lives, mainly in news media, scientific discoveries, medical imaging, and judicial evidence. However, in recent years, due to the wide application of deep learning in image processing technology, these technologies or software have not only brought convenience to people, but also made it easier for people to forge or tamper with digital images without leaving any traces. The authenticity of digital images has been affected. A serious threat to modify and threaten. These forged or tampered images will bring serious threats to judicial justice, social stability, and the medical industry, and cause huge negative effects. Therefore, this article proposes an authenticity detection algorithm for generating color forged images based on deep learning. The corresponding color channel features of the real and forged image datasets are extracted and FIsher encoded, respectively, and the encoded color channel features are used to train the SVM model. Experiments prove that our proposed method achieves better results in detecting image color tampering.",https://ieeexplore.ieee.org/document/9586993/,"2021 IEEE 5th Information Technology,Networking,Electronic and Automation Control Conference (ITNEC)",15-17 Oct. 2021,ieeexplore
10.1109/ICIEM51511.2021.9445281,A Novel approach of GUI Mapping with image based widget detection and classification,IEEE,Conferences,"Software testing is vital for the intellectual benefits of software reliability and quality. At present, Graphical user interfaces are the most common and widely used interfaces in the software industry. Furthermore, GUI Testing is an important approach to ensure the quality of software. Automated software testing is a GUI front end applications similar to APP, and WEB, etc and is a vastly time and resource-consuming task. Therefore, this will become even more complex in rapidly updated GUI applications such as Ex: Patches/Version updates of a mobile App, or the product and offer updates in marketing websites like Flipkart, Amazon, etc., in which the GUI components are continuously updated infinitely. Developing a test case scenario whenever a new GUI component is updated will affect the productivity of the application. In our experiment, we found a better way of improving GUI testing by consequently detecting and classifying GUI widgets using machine learning techniques. Additionally, we also found that detecting and classifying GUI objects in screenshots and reports with a position of the widgets (x, y coordinates) and type of the widgets, matches with trained samples, URL links, and screen links. Hence, we in this paper will analyze and devise an efficient automated testing strategy for Web Applications. This is a unique way of web Graphical user interface testing with a computer vision. This paper will also present the parameters used for object detection, classification, and evaluation with image processing using machine learning algorithms with better accuracy.",https://ieeexplore.ieee.org/document/9445281/,2021 2nd International Conference on Intelligent Engineering and Management (ICIEM),28-30 April 2021,ieeexplore
10.1109/RE51729.2021.00036,A Pipeline for Automating Labeling to Prediction in Classification of NFRs,IEEE,Conferences,"Non-Functional Requirements (NFRs) focus on the operational constraints of the software system. Early detection of NFRs enables their incorporation into the architectural design at an initial stage, a practice obviously preferable to expensive refactoring at a later stage. Automated identification and classification of NFRs has therefore seen numerous efforts using rule-based, machine learning and deep learning-based approaches. One of the major challenges for such an automation is the manual effort that needs to be invested into labeling of training data. This is a concern for large software vendors who typically work on a variety of applications in diverse domains. We address this challenge by designing a pipeline that facilitates classification of NFRs using only a limited amount (~ 20% of an available new dataset) of labeled data for training. We (1) employed Snorkel to automatically label a dataset comprising NFRs from various Software Requirement Specification documents, (2) trained several classifiers using it, and (3) reused these pre-trained classifiers using a Transfer Learning approach to classify NFRs in industry-specific datasets. From among the various language model classifiers, the best results have been obtained for a BERT based classifier fine-tuned to learn the linguistic intricacies of three different domain-specific datasets from real-life projects.",https://ieeexplore.ieee.org/document/9604524/,2021 IEEE 29th International Requirements Engineering Conference (RE),20-24 Sept. 2021,ieeexplore
10.1109/ITC-Egypt52936.2021.9513888,A Proposed end to end Telemedicine System based on embedded system and mobile application using CMOS wearable sensors,IEEE,Conferences,"Internet of things (IoT) and Embedded systems have extensive applications in healthcare markets. Integration of IoT with healthcare started with wearable smartwatches monitoring some signals and storing this data in the cloud. With 4G/5G and WiFi 6 networks. Healthcare data can be analyzed with Artificial Intelligence providing new era Internet of Medical Things (IoMT) that encompass an array of internet-capable medical devices that are in constant communication with each other or with the cloud; Internet of Healthcare Things (IoHT) that is the digital transformation of the healthcare industry. This article presents an end-to-end architecture with realization of three modules for key IoT aspects for healthcare and telemedicine. Results from a real implementation of application Platform for Data Processing including patient and doctor data base-based web site, MySQL data base, Android based mobile App, and PHP webserver.",https://ieeexplore.ieee.org/document/9513888/,2021 International Telecommunications Conference (ITC-Egypt),13-15 July 2021,ieeexplore
10.1109/AIVR50618.2020.00071,A QoE and Visual Attention Evaluation on the Influence of Spatial Audio in 360 Videos,IEEE,Conferences,"Recently, there has been growing interest from academia and industry on the application of immersive technologies across a range of domains. Once such technology, 360° video, can be captured using an omnidirectional multi-camera arrangement. These 360° videos can then be rendered via Virtual Reality (VR) Head Mounted Displays (HMD). Viewers then have the freedom to look around the scene in any direction they wish. Whereas a body of work exists that focused on modeling visual attention (VA) in VR, little research has considered the impact of the audio modality on VA in VR. It is well accepted that audio has an important role in VR experiences. High quality spatial audio offers listeners the opportunity to experience sound in all directions. One such technique, Ambisonics or 3D audio, offers a complete 360° soundscape. This paper reports the results of an empirical study that looked at understanding how (if at all) spatial audio influences visual attention in 360° videos. It also assessed the impact of spatial audio on the user's Quality of Experience (QoE) by capturing implicit, explicit, and objective metrics. The results suggest surprisingly similar explicit QoE ratings for both the spatial and non-spatial audio environments. The implicit metrics indicate that users integrated with the spatial environment more quickly than the non-spatial environment. Users who experienced the spatial audio environment had a higher maximum mean head pose pitch value and were found to be more focused towards the sound-emitting regions in the spatial audio environment experiences.",https://ieeexplore.ieee.org/document/9319084/,2020 IEEE International Conference on Artificial Intelligence and Virtual Reality (AIVR),14-18 Dec. 2020,ieeexplore
10.1109/CSCI49370.2019.00084,A Real-Time Based Intelligent System for Predicting Equipment Status,IEEE,Conferences,"In manufacturing industry, significant productivity losses arise due to equipment failures. Therefore, it is an important task to prevent the equipment from failure by monitoring each machine's sensor data in advance. However, most of the current developed systems have been only focused on monitoring the sensor data and have a difficulty in applying advanced algorithms to the real-time stream data. To address issues, we implemented an intelligent system that employs real-time streaming engine loaded with the machine learning libraries for predictive maintenance analysis. By applying a deep-learning based model to the real-time streaming data, we can provide not only trends of raw sensor data but also give an indicator representing an equipment's status in real-time. We anticipate that our system contributes to recognize the equipment's status by monitoring the indicator for productivity improvement in manufacturing industry in real-time.",https://ieeexplore.ieee.org/document/9071016/,2019 International Conference on Computational Science and Computational Intelligence (CSCI),5-7 Dec. 2019,ieeexplore
10.1109/COMPSAC.2019.10205,A Scalable Framework for Multilevel Streaming Data Analytics using Deep Learning,IEEE,Conferences,"The rapid growth of data in velocity, volume, value, variety, and veracity has enabled exciting new opportunities and presented big challenges for businesses of all types. Recently, there has been considerable interest in developing systems for processing continuous data streams with the increasing need for real-time analytics for decision support in the business, healthcare, manufacturing, and security. The analytics of streaming data usually relies on the output of offline analytics on static or archived data. However, businesses and organizations like our industry partner Gnowit, strive to provide their customers with real time market information and continuously look for a unified analytics framework that can integrate both streaming and offline analytics in a seamless fashion to extract knowledge from large volumes of hybrid streaming data. We present our study on designing a multilevel streaming text data analytics framework by comparing leading edge scalable open-source, distributed, and in-memory technologies. We demonstrate the functionality of the framework for a use case of multilevel text analytics using deep learning for language understanding and sentiment analysis including data indexing and query processing. Our framework combines Spark streaming for real time text processing, the Long Short Term Memory (LSTM) deep learning model for higher level sentiment analysis, and other tools for SQL-based analytical processing to provide a scalable solution for multilevel streaming text analytics.",https://ieeexplore.ieee.org/document/8754149/,2019 IEEE 43rd Annual Computer Software and Applications Conference (COMPSAC),15-19 Jul 2019,ieeexplore
10.1109/CISIS.2012.78,A Semantic Model for Utility Driven Discovery of Cloud Resources,IEEE,Conferences,"Cloud computing has reached a high level of acceptance, both in academia and in industry. The maturity of the technology, along with the considerable business opportunity that has been promised, is the main responsible for its success. Nevertheless, today only few, very big players dominate the commercial panorama and take over market shares. From that position, they impose rigid pricing policies and quite inflexible negotiation schemes. When the ongoing cloud standardization process will complete, and thus full interoperability among clouds will be accomplished, new players will come into play. A real competition among cloud providers will then start based on key factors like the capability of providing flexible services tailored to specific, fine-grained customers' requirements. In this market scenario a mechanism must be devised to support the matchmaking between what providers offer and what customers demand. In this work we define a semantic model to help customers and providers to characterize their demands/offers, and propose the use of semantic tools to perform the matchmaking in such a way to maximize both the provider's and the customer's satisfaction.",https://ieeexplore.ieee.org/document/6245782/,"2012 Sixth International Conference on Complex, Intelligent, and Software Intensive Systems",4-6 July 2012,ieeexplore
10.1109/DAS.2012.10,A Signature Verification Framework for Digital Pen Applications,IEEE,Conferences,"In this paper we present a framework for real-time online signature verification scenarios. The proposed framework is based on state-of-the-art feature extraction and Gaussian Mixture Model (GMM) classification. While our signature verification library is generally applicable to any input device using digital pens, we have implemented verification scenarios using the Anoto digital pen. As such our automated signature verification framework becomes an interesting commodity for industry, because the Anoto SDK is easy to apply and the GMM-based classification can be seamlessly integrated. The novelty of this work is the application of our framework that takes real-time online signature verification to every scenario where digital pens may potentially be used. In this paper we describe several scenarios where our framework has been applied, including signatures in financial contracts or ordering processes. We also propose a general approach to integrate the GMM-descriptions into electronic ID-cards in order to also store behavioral biometrics on these cards. In experiments we have measured the performance of the signature verification system when skilled forgeries were present. The interest shown by our partner financial institutions and the results of our initial evaluations indicate that our signature verification framework suits exactly the demands of our clients.",https://ieeexplore.ieee.org/document/6195406/,2012 10th IAPR International Workshop on Document Analysis Systems,27-29 March 2012,ieeexplore
10.1109/PCIC31437.2018.9080444,"A Smart Condition Monitoring System for HV Networks with Artificial Intelligence, Augmented Reality and Virtual Reality: Copyright Material IEEE, Paper No. PCIC-2018-37",IEEE,Conferences,"The authors present a conceptual design for a SMART asset monitoring solution for high voltage (HV) networks in the petrochemical industry. The paper discusses the potential for incorporating artificial intelligence (AI), augmented reality (AR) and virtual reality (VR) into an application of the Industrial Internet of Things (IIoT) for condition monitoring. The paper is a continuation of the work presented by the authors at the IEEE-PCIC 2017 conference in Calgary. The proposed asset management system analyses condition monitoring (CM) data and assesses the risk of failure data across complete HV networks. Knowledge of deteriorating asset condition provides the operator with an advanced, early warning of incipient mechanical and electrical faults. With knowledge of the severity and source of such faults, pinpointed preventative maintenance interventions can then made during planned maintenance outages. The complete HV network asset monitoring solution described includes permanent sensors and monitoring nodes deployed at strategic locations across the network. Processed data is passed via a local area network to local servers and then via secure data cloud transmission to a centralized monitoring server located at the CM headquarters. This central server operates a CM database that logs, displays, benchmarks and trends the condition data with comparison to a statistically-significant database of measurements. It is proposed in the IIoT solution proposed that this database will be downloadable to a smartphone/tablet for use by the field engineer. The monitoring technology will likely also incorporate a number of AI machine learning software modules for the de-noising of raw signals and the diagnosis of different types of defects within different types of HV plant items. The proposed SMART CM system includes an advanced graphical user interface (GUI) for viewing HV asset CM data along with operational and maintenance (O&amp;M) data. The GUI will also be able to display both condition criticality and operational criticality (on a color-coded range of 0-100%) for individual HV plant items on a digitized mimic of the HV network's single-line diagram (SLD). This could also be combined with geometric positioning data of assets across the facility (including HV cable routes and lengths) to provide a fully digitized SMART network diagram for use in the IIoT asset management solution. Asset management data, combined with the application of the developing techniques of AI, AR and VR, will greatly help the user to visualize the plant items in 3-D, their position within the network, their condition and operational criticality along with all related asset management information together on one dashboard screen, downloaded onto smartphone/tablet. The paper concludes with a case study showing the development of a specification for a SMART IIoT asset condition monitoring solution suitable for a large petrochemical refining facility.",https://ieeexplore.ieee.org/document/9080444/,2018 IEEE Petroleum and Chemical Industry Technical Conference (PCIC),24-26 Sept. 2018,ieeexplore
10.1109/ICMCCE.2018.00050,A Smart Manufacturing Compliance Architecture of Electronic Batch Recording System (eBRS) for Life Sciences Industry,IEEE,Conferences,"The paradigm shift brought about by smart manufacturing or Industrie 4.0 has posed threefold challenges to electronic batch recording system (eBRS) in Life Sciences Industry: 1) the structure of the data should be informative and standard for interoperate using information models, 2) administration of synchronization between physical world and cyber world for smart decision making and optimization using cyber physical system (CPS) and 3) so-called digital manufacturing operations management (digital MOM) characterized by decentralization, comprehensive collaboration and servitization shall be implemented. Under the new situations of smart manufacturing or Industrie 4.0, the requirements from information models, CPS and digital MOM will become the most principal criteria to be considered for future eBRS/MES and other operations management information system in shop floor. To fulfill these demands, an approach combining ISA95/88 hybrid model with activities ontology and variant domain-driven design for SOA-based eBRS development has been presented. An eBRS software platform has been developed on the theoretical basis and applied to a specific application scenario of Lyophilized Injection Production for verifying its feasibility purpose.",https://ieeexplore.ieee.org/document/8537548/,"2018 3rd International Conference on Mechanical, Control and Computer Engineering (ICMCCE)",14-16 Sept. 2018,ieeexplore
10.23919/CCC50068.2020.9188783,A Smartphone-Based Networked Control Platform: Design and Implementation,IEEE,Conferences,"The rapid development of embedded systems and IT makes electrical devices functional and tiny. This progress is especially reflected in the smartphone industry. In this paper, a novel applicable wireless control platform based on smartphones is proposed. The system is of high value due to the advantages of smartphones such as mobility, resourcefulness, tiny and etc. Mechanisms are designed in this paper to guarantee the real-timeness of the controller. To ease the controller implementation, code auto-generation technique is integrated to the platform, which is able to convert Simulink block diagrams to executable files for Android smartphones. In addition, protocols are designed to transfer data to remote workstations such that the control system can be supervised and monitored on-line. At last, the control platform is tested on a spacecraft simulator. The experimental results validate the effectiveness and reliability of the designed platform.",https://ieeexplore.ieee.org/document/9188783/,2020 39th Chinese Control Conference (CCC),27-29 July 2020,ieeexplore
10.1109/IOLTS52814.2021.9486704,A Suitability Analysis of Software Based Testing Strategies for the On-line Testing of Artificial Neural Networks Applications in Embedded Devices,IEEE,Conferences,"Electronic devices based on artificial intelligence solutions are pervading our everyday life. Nowadays, human decision processes are supported by real-time data gathered from intelligent systems. Artificial Neural Networks (ANNs) are one of the most used deep learning predictive models due to their outstanding computational capabilities. However, assessing their reliability is still an open issue faced by both the academic and industrial worlds, especially when ANNs are deployed on safety-critical systems, such as self-driving cars in the automotive world. In these systems, a strategy for identifying hardware faults is required by industry standards (e.g., ISO26262 for automotive, and DO254 for avionics). Among the existing in-field test strategies, the periodic scheduling of on-line Software Test Library (STL) is a wide strategy adopted; STL allows to reach an acceptable fault coverage without the need for additional hardware. However, when dealing with ANN-based applications, the execution of on-line tests interleaving the ANN inferences may jeopardise the strive for performance maximization. The paper presents a comprehensive analysis of six possible scenarios concerning the execution of on-line self-test programs in embedded devices running ANN-based applications. In the proposed scenarios, the impact of the STL execution on the ANN performance is analyzed; in particular, the execution times of an inference and the Fault Detection Time (FDT) of the STL are discussed and compared. Experimental analyses are provided by relying on: an open-source RISC-V platform running two different convolutional neural networks; a STL for RISC-V cores with a maximum achievable fault coverage of 90%.",https://ieeexplore.ieee.org/document/9486704/,2021 IEEE 27th International Symposium on On-Line Testing and Robust System Design (IOLTS),28-30 June 2021,ieeexplore
10.1109/ISSE46696.2019.8984442,A Systems Approach to Validating and Analyzing Improvements to Real-Time Image Classification Utilizing Machine Learning,IEEE,Conferences,"This paper delves into the generation and use of image classification models in a real-time environment utilizing machine learning. The ImageAI framework is used to generate a list of models from a set of training images and also for classifying new images using the generated models. Through this paper, previous research projects and industry programs are analyzed for design and operation. The basic implementation results in models that classify new images correctly the majority of the time with a high level of confidence. However, almost a quarter of the time the models classify images incorrectly. This paper attempts to improve the classification accuracy and improve the operational efficiency of the overall system as well.",https://ieeexplore.ieee.org/document/8984442/,2019 International Symposium on Systems Engineering (ISSE),1-3 Oct. 2019,ieeexplore
10.1109/TAAI.2013.73,A Tourist Navigation System in Which a Historical Character Guides to Related Spots by Hide-and-Seek,IEEE,Conferences,"Tourism is an important industry, and various initiatives for the discovery of sightseeing spots are being implemented. Most current sightseeing navigation systems, which are very efficient at enabling immediate acquisition of desired information, lack a sense of fun. Moreover, although some researches have been conducted on the creation of new encounters and discoveries in tourist areas, there is a problem in that the high degree of user freedom does not lead users to spots which tourist area authorities hope to promote. Thus, in this study we propose a system that stimulates rediscovery of sightseeing spots through hide-and-seek with CG characters using augmented reality technology. We intend to verify the anticipated effects of the proposed system in upcoming evaluation experiments.",https://ieeexplore.ieee.org/document/6783892/,2013 Conference on Technologies and Applications of Artificial Intelligence,6-8 Dec. 2013,ieeexplore
10.1109/ICIEVicIVPR52578.2021.9564229,A Vision-Based Lane Detection Approach for Autonomous Vehicles Using a Convolutional Neural Network Architecture,IEEE,Conferences,"Autonomous vehicles no longer belong to the realm of science fiction. They have become a prominent area of research in the last two decades because of the integration of Artificial Intelligence in the automobile industry. Apart from the development of various complex learning algorithms, the advancement of cameras, sensors, and geolocation technology as well as the escalation in the capacity of machines have played a crucial role in bringing this technology into reality. We have had significant breakthroughs in the development of autonomous cars within the last ten years. However, despite the success of multiple prototypes in navigating within the borders of a delimited area, researchers are yet to overcome several drawbacks before embodying them in the transport system; and one of those hurdles lies in the lane detection system of the cars. Therefore, in this article, we present an intelligent lane detection algorithm incorporating fully-connected Neural Networks with a secondary layer protection scheme to detect the borders of a lane. We achieved over 98% classification accuracy using the proposed lane detection model. We also implemented the model in a small prototype to take a look at its performance. Experimental results infer that the algorithm is capable of lane detection and ready for practical use.",https://ieeexplore.ieee.org/document/9564229/,"2021 Joint 10th International Conference on Informatics, Electronics & Vision (ICIEV) and 2021 5th International Conference on Imaging, Vision & Pattern Recognition (icIVPR)",16-20 Aug. 2021,ieeexplore
10.1109/PacificVis.2018.00026,A Visual Analytics Approach for Equipment Condition Monitoring in Smart Factories of Process Industry,IEEE,Conferences,"Monitoring equipment conditions is of great value in manufacturing, which can not only reduce unplanned downtime by early detecting anomalies of equipment but also avoid unnecessary routine maintenance. With the coming era of Industry 4.0 (or industrial internet), more and more assets and machines in plants are equipped with various sensors and information systems, which brings an unprecedented opportunity to capture large-scale and fine-grained data for effective on-line equipment condition monitoring. However, due to the lack of systematic methods, analysts still find it challenging to carry out efficient analyses and extract valuable information from the mass volume of data collected, especially for process industry (e.g., a petrochemical plant) with complex manufacturing procedures. In this paper, we report the design and implementation of an interactive visual analytics system, which helps managers and operators at manufacturing sites leverage their domain knowledge and apply substantial human judgements to guide the automated analytical approaches, thus generating understandable and trustable results for real-world applications. Our system integrates advanced analytical algorithms (e.g., Gaussian mixture model with a Bayesian framework) and intuitive visualization designs to provide a comprehensive and adaptive semi-supervised solution to equipment condition monitoring. The example use cases based on a real-world manufacturing dataset and interviews with domain experts demonstrate the effectiveness of our system.",https://ieeexplore.ieee.org/document/8365986/,2018 IEEE Pacific Visualization Symposium (PacificVis),10-13 April 2018,ieeexplore
10.1109/EDOC49727.2020.00017,A Zone Reference Model for Enterprise-Grade Data Lake Management,IEEE,Conferences,"Data lakes are on the rise as data platforms for any kind of analytics, from data exploration to machine learning. They achieve the required flexibility by storing heterogeneous data in their raw format, and by avoiding the need for pre-defined use cases. However, storing only raw data is inefficient, as for many applications, the same data processing has to be applied repeatedly. To foster the reuse of processing steps, literature proposes to store data in different degrees of processing in addition to their raw format. To this end, data lakes are typically structured in zones. There exists various zone models, but they are varied, vague, and no assessments are given. It is unclear which of these zone models is applicable in a practical data lake implementation in enterprises. In this work, we assess existing zone models using requirements derived from multiple representative data analytics use cases of a real-world industry case. We identify the shortcomings of existing work and develop a zone reference model for enterprise-grade data lake management in a detailed manner. We assess the reference model's applicability through a prototypical implementation for a real-world enterprise data lake use case. This assessment shows that the zone reference model meets the requirements relevant in practice and is ready for industry use.",https://ieeexplore.ieee.org/document/9233155/,2020 IEEE 24th International Enterprise Distributed Object Computing Conference (EDOC),5-8 Oct. 2020,ieeexplore
10.1109/SSCI.2017.8280935,A benchmark environment motivated by industrial control problems,IEEE,Conferences,"In the research area of reinforcement learning (RL), frequently novel and promising methods are developed and introduced to the RL community. However, although many researchers are keen to apply their methods on real-world problems, implementing such methods in real industry environments often is a frustrating and tedious process. Generally, academic research groups have only limited access to real industrial data and applications. For this reason, new methods are usually developed, evaluated and compared by using artificial software benchmarks. On one hand, these benchmarks are designed to provide interpretable RL training scenarios and detailed insight into the learning process of the method on hand. On the other hand, they usually do not share much similarity with industrial real-world applications. For this reason we used our industry experience to design a benchmark which bridges the gap between freely available, documented, and motivated artificial benchmarks and properties of real industrial problems. The resulting industrial benchmark (IB) has been made publicly available to the RL community by publishing its Java and Python code, including an OpenAI Gym wrapper, on Github. In this paper we motivate and describe in detail the IB's dynamics and identify prototypic experimental settings that capture common situations in real-world industry control problems.",https://ieeexplore.ieee.org/document/8280935/,2017 IEEE Symposium Series on Computational Intelligence (SSCI),27 Nov.-1 Dec. 2017,ieeexplore
10.1109/RTEST.2018.8397165,A classifier-based test oracle for embedded software,IEEE,Conferences,"Despite great advances in different software testing areas, one important challenge, achieving an automated test oracle, has been overlooked by academia and industry. Among various approaches for constructing a test oracle, machine learning techniques have been successful in recent years. However, there are some situations in which the existing machine learning based oracles have deficiencies. These situations include testing of applications with low observability, such as embedded software and multimedia software programs. There are also cases in testing embedded software in which explicit historical data in form of input-output relationships is not available, and situations in which the comparison between expected results and actual outputs is impossible or hard. Addressing these deficiencies, this paper proposes a new black box solution to construct automated oracles which can be applied to embedded software and other programs with low observability. To achieve this, we have employed an Artificial Neural Network (ANN) algorithm to build a model which merely requires program's input values as well as corresponding pass/fail outcome, as the training set. We have conducted extensive experiments on several benchmarks. The results manifest the applicability of the proposed approach to software systems with low observability as well as its higher accuracy in comparison to a well-known machine learning based method.",https://ieeexplore.ieee.org/document/8397165/,2018 Real-Time and Embedded Systems and Technologies (RTEST),9-10 May 2018,ieeexplore
10.1109/CSSS.2011.5974714,A data prediction algorithm based on BP neural network in telecom industry,IEEE,Conferences,"The data mining technology is more and more widely used in the telecom industry. But telecom data set always includes instances with missing values. Besides, many data mining models are sensitive for the missing value and distortion. Estimating missing values becomes an inherent problem. To address the problem, A prediction method is proposed for the missing value based on the BP neural network and K-means algorithm. The algorithm first clusters the dataset, and selects a certain cluster for the instance with missing values, then we train the BP net using the cluster and get the architecture parameters. When a test instance has a missing value of a certain attribute, we regard the attribute as the aiming attribute and apply the instance to the network, it will produce a prediction value . This paper uses real datasets from the telecom industry as the test datasets. The result shows that the algorithm can be used to predict the missing value of telecom industry with good performance.",https://ieeexplore.ieee.org/document/5974714/,2011 International Conference on Computer Science and Service System (CSSS),27-29 June 2011,ieeexplore
10.1109/INDIN.2012.6301363,A device configuration management tool for context-aware system,IEEE,Conferences,"Automation industry is moving towards more complex systems which are posing new challenges for operation from both machine and human perspectives. A group of challenges is related to management of the overwhelming information flow and to usability of the systems, and context-aware solutions have been recently introduced to the automation field in order to cope with challenges of this kind. The context awareness is seen as a solution which would allow to both the technological system and the human operator to infer the optimal decisions and to behave in the most effective way. In order to reach this capability, the external physical world and the system must be described in a way both interprétable for humans and machines. Ambition of this paper is to contribute to the context-aware (re)configuration of the system with a tool, which is designed to improve the efficiency of the configuration phase of a context-aware system. The tool provides a solution to configure and model the field devices of a system via automatically generated ontologies. This research is a part of a device management framework for a building automation use case, which is targeting to support controlling decisions of dwellers, technical support and social services.",https://ieeexplore.ieee.org/document/6301363/,IEEE 10th International Conference on Industrial Informatics,25-27 July 2012,ieeexplore
10.1109/ICE/ITMC49519.2020.9198625,A digital twin model of a pasteurization system for food beverages: tools and architecture,IEEE,Conferences,"Many enabling technologies of Industry 4.0 (Internet of Things “IoT”, Cloud systems, Big Data Analytics) contribute to the creation of what is the Digital Twin or virtual twin of a physical process, that is a mathematical model capable of describing the process, product or service in a precise way in order to carry out analyses and apply strategies. Digital Twin models integrate artificial intelligence, machine learning and analytics software with the data collected from the production plants to create digital simulation models that update when the parameters of the production processes or the working conditions change. This is a self-learning mechanism, which makes use of data collected from various sources (sensors that transmit operating conditions; experts, such as engineers with deep knowledge of the industrial domain; other similar machines or fleets of similar machines) and integrates also historical data relating to the past use of the machine. Starting from the virtual twin vision, simulation plays a key role within the Industry 4.0 transformation. Creating a virtual prototype has become necessary and strategic to raise the safety levels of the operators engaged in the maintenance phases, but above all the integration of the digital model with the IoT has become particularly effective, as the advent of software platforms offers the possibility of integrating real-time data with all the digital information that a company owns on a given process, ensuring the realization of the Digital Twin. In this context, this work aims at developing optimized solutions for application in a beverage pasteurization system using the Digital Twin approach, capable of creating a virtual modelling of the process and preventing high-risk events for operators.",https://ieeexplore.ieee.org/document/9198625/,"2020 IEEE International Conference on Engineering, Technology and Innovation (ICE/ITMC)",15-17 June 2020,ieeexplore
10.1109/ISSCS.2013.6651227,A face recognition system based on a Kinect sensor and Windows Azure cloud technology,IEEE,Conferences,"The aim of this paper is to build a system for human detection based on facial recognition. The state-of-the-art face recognition algorithms obtain high recognition rates base on demanding costs - computational, energy and memory. The use of these classical algorithms on an embedded system cannot achieve such performances due to the existing constrains: computational power and memory. Our objective is to develop a cheap, real time embedded system able to recognize faces without any compromise on system's accuracy. The system is designed for automotive industry, smart house application and security systems. To achieve superior performance (higher recognition rates) in real time, an optimum combination of new technologies was used for detection and classification of faces. The face detection system uses skeletal-tracking feature of Microsoft Kinect sensor. The face recognition, more precisely - the training of neural network, the most computing-intensive part of the software, is achieved based on the Windows Azures cloud technology.",https://ieeexplore.ieee.org/document/6651227/,"International Symposium on Signals, Circuits and Systems ISSCS2013",11-12 July 2013,ieeexplore
10.1109/IJCNN.1998.687153,A hybrid structure for adaptive fixed weight recurrent networks,IEEE,Conferences,"Due to the evolution of the underlying physical process, a correct model can transform into an erroneous one. We therefore propose a method which overcomes this problem by adapting the network along the way. Our method (clustered error injection) is based (a) on the ability of real-time recurrent learning networks to form clustered network structures and (b) on the error injection principle. The actual model error is fed back into the network as an input. This improves the model performance by adapting it to a changing environment. This technique is tested on two examples, a mathematical modelling problem and a real-life problem from the chemical process industry.",https://ieeexplore.ieee.org/document/687153/,1998 IEEE International Joint Conference on Neural Networks Proceedings. IEEE World Congress on Computational Intelligence (Cat. No.98CH36227),4-9 May 1998,ieeexplore
10.1109/ANSS.2005.8,A neural approach for fast simulation of flight mechanics,IEEE,Conferences,"Flight simulators have been part of aviation history since its beginning. With the development of modern aeronautics industry, flight simulators have gained an important place and the industry devoted to their manufacture has become significant. In the case of transportation aircraft, accurate mathematical models based on extensive experimental data have been developed by their manufacturers to optimize their aerodynamic and propulsive characteristics and to design efficient flight control systems. However, in the case of small general aviation aircraft this kind of knowledge is not commonly available and the design of accurate flight simulators can result in a tedious try and modify process until the simulator presents a qualitative behaviour close to the one of the real aircraft. This communication proposes through the use of neural networks a method to perform a direct estimation of the aerodynamic forces acting on aircraft. Artificial neural networks appear to be an appropriate numerical technique to achieve the mapping of these continuous relationships and detailed aerodynamics and thrust models should become no more mandatory to produce accurate flight simulation software.",https://ieeexplore.ieee.org/document/1401963/,38th Annual Simulation Symposium,4-6 April 2005,ieeexplore
10.1109/SSST.1991.138548,A neural network based histogramic procedure for fast image segmentation,IEEE,Conferences,"The determination of the dimension of a lumber board, the location and extent of surface defects on it, are essential in the construction of a visual inspection station for the lumber industry. The paper presents a neural network based histogramic procedure that performs on the image of a board and can be used to determine the board dimension, the location and extent of surface defects on it, in near real time. The method is based on segmentation of the image based on multiple threshold information derived from a multi-layered neural network. Such a scheme can be applied in general to image analysis and the implementation shows fast processing requiring very little control over the environment. The construction of the network and its training are also discussed.&lt;<ETX>&gt;</ETX>",https://ieeexplore.ieee.org/document/138548/,[1991 Proceedings] The Twenty-Third Southeastern Symposium on System Theory,10-12 March 1991,ieeexplore
10.1109/IAS.2005.1518384,A neural network based optimal wide area control scheme for a power system,IEEE,Conferences,"With deregulation of the power industry, many tie lines between control areas are driven to operate near their maximum capacity, especially those serving heavy load centers. Wide area control systems (WACSs) using wide-area or global signals can provide remote auxiliary control signals to local controllers such as automatic voltage regulators, power system stabilizers, etc to damp out inter-area oscillations. This paper presents the design and the DSP implementation of a nonlinear optimal wide area controller based on adaptive critic designs and neural networks for a power system on the real-time digital simulator (RTDS/spl reg/). The performance of the WACS as a power system stability agent is studied using the Kundur's two area power system example. The WACS provides better damping of power system oscillations under small and large disturbances even with the inclusion of local power system stabilizers.",https://ieeexplore.ieee.org/document/1518384/,"Fourtieth IAS Annual Meeting. Conference Record of the 2005 Industry Applications Conference, 2005.",2-6 Oct. 2005,ieeexplore
10.1109/WINCOM50532.2020.9272477,A new middleware for managing heterogeneous robot in ubiquitous environments,IEEE,Conferences,"Heterogeneity is one of the main issues for the deployment of the Industry 4.0. This is due to the diversity in the available robots and the IIoT devices. These equipments use different programming languages and communication protocols. To make the integration of such equipments easy, we propose TalkRoBots, a middleware that allows heterogeneous robots and IIoT devices to communicate together and exchange data in a transparent way. The middleware was experimented in a real scenario with different robots that demonstrate its efficiency.",https://ieeexplore.ieee.org/document/9272477/,2020 8th International Conference on Wireless Networks and Mobile Communications (WINCOM),27-29 Oct. 2020,ieeexplore
10.1109/ICITIIT49094.2020.9071531,A novel approach to detect IoT malware by system calls using Deep learning techniques,IEEE,Conferences,"Recently, IoT devices or smart objects widely utilized in all kinds of fields such as medical, defense, automobile industry etc. Due to its intelligence and popularity, attacker seeks its help to launch malicious attack in high speed at low cost. In this regard, Researchers have turned their interest towards improving the security for the Internet of things devices. In this model, the malware were detected based on their behavior in terms of system calls sequence arise during its execution. The system calls of IoT malware are gathered using Strace tool in Ubuntu. The generated malicious system calls are preprocessed by n-gram techniques to retrieve required features. The extracted system calls were classified into two class i.e normal and malicious sequence using Recurrent neural network(RNN). The efficiency of this deep learning is tested using various performance metrics. The real time IoT malware samples were collected from IOTPOT honeypot which emulates different CPU architecture of IoT devices.",https://ieeexplore.ieee.org/document/9071531/,2020 International Conference on Innovative Trends in Information Technology (ICITIIT),13-14 Feb. 2020,ieeexplore
10.1109/ROBOT.2004.1308024,A real-time monitoring and diagnosis system for manufacturing automation,IEEE,Conferences,"Condition monitoring and fault diagnosis in modern engineering practices is of great practical significance for improving the quality and productivity, preventing the machinery from damages. In general, this practice consists of two parts: extracting appropriate features from sensor signals and recognizing possible faulty patterns from the features. In order to cope with the complex manufacturing operations and develop a feasible system for real-time application, we proposed three approaches. By defining the marginal energy, a new feature representation emerged, while by real-time learning algorithms with support vector techniques and hidden Markov model representations, a modular software architecture and a new similarity measure were developed for comparison, monitoring, and diagnosis. A novel intelligent computer-based system has been developed and evaluated in over 30 factories and numerous metal stamping processes as an example of manufacturing operations. The real-time operation of this system demonstrated that the proposed system is able to detect abnormal conditions efficiently and effectively resulting in a low-cost, effective approach to real-time monitoring in manufacturing. The related technologies have been transferred to industry, presenting a tremendous impact in current automation practice in Asia and the world.",https://ieeexplore.ieee.org/document/1308024/,"IEEE International Conference on Robotics and Automation, 2004. Proceedings. ICRA '04. 2004",26 April-1 May 2004,ieeexplore
10.1109/MFI-2003.2003.1232590,A robust real time position and force (hybrid) control of a robot manipulator in presence of uncertainties,IEEE,Conferences,"We examine the living intelligent biological systems and model the computational system components. We consider the situation of a kind of ""blind-tracking"" with constant force/torque by a human hand. The problem involves hand kinematics, hand motor control, and an adaptive judgment method from the position and force/torque reflection of the uncertain hyper plane. In this study, these control levels were designed using neural networks and fuzzy logic technologies. The control levels are coordinated amongst themselves forming the distributed artificial intelligent (DAI) system. The conclusive characteristic of the proposed controller was a one-step-ahead feedback control. This DAI-based control systems was implemented in the RX-90 industrial robot. Certainly these types of control system will help an industry to be autonomous and increase the productivity as well.",https://ieeexplore.ieee.org/document/1232590/,"Proceedings of IEEE International Conference on Multisensor Fusion and Integration for Intelligent Systems, MFI2003.",1-1 Aug. 2003,ieeexplore
10.1109/ICISC.2018.8398982,A rule-based classification of short message service type,IEEE,Conferences,"Short message service (SMS) is one of the most popular means of communication due to its type of usages like one-time passwords(OTPs), banking transaction alerts, and other promotional messages. SMS is the most time-sensitive channel of communication that demands service providers to send any information globally without any delay in respective time zone. For successful implementation of this service, the Telecom Regulatory of India (TRAI) has recommended certain guidelines that must be followed for the delivery of short messages. The prevalent practice in the industry is to store rules or raw string in a database and to match with incoming SMSs in synchronous mode. However, there are various shortcomings in this traditional approach that have been also discussed in this paper. Further, to address these issues, this research work proposes a novel approach for automated classification of SMSs in real time by the SMS service using a rule-based system of database template matching. Further to validate the proposed approach, SMS database of Netcore Solutions Pvt Limited, India, has been used for training the proposed algorithm. Proposed rule-based classifier keeps learning and updating the training dataset as more SMSs are accumulated and processed through the server. Other advantages, in terms of performance metrics, of the classification using the proposed rule-based database template matching over the traditional database matching approach have been reported in this work as evaluation measures. Reported rule-based SMS classification algorithm shows highest average classification accuracy of 100% which is better as compared to the similar research works already available in the literature.",https://ieeexplore.ieee.org/document/8398982/,2018 2nd International Conference on Inventive Systems and Control (ICISC),19-20 Jan. 2018,ieeexplore
10.1109/ISIE.1998.707755,A self-organization neuro-fuzzy network applied to a seismic signal classification problem,IEEE,Conferences,"The marriage between neural networks and fuzzy logic systems can lead to improved systems in which the best of both worlds are combined, viz. learning capability as well as the ability to handle real-life ambiguities and uncertainties gracefully. In this contribution a heterogeneous neuro-fuzzy network is proposed as a solution for a problem relevant in the mining industry: the classification of seismic signals according to their generating sources. Different stages in the traditional fuzzy system are implemented in consecutive layers in the network, resulting in an architecture reminiscent of radial basis function networks. In contrast to some other neuro-fuzzy networks in which a rule-base is derived by the rule-elimination-algorithm, fuzzy rules are generated with the aid of a fuzzy adaptive resonance theory network. Besides leading to reduced training times, the proposed rule generation algorithm can result in a better understanding of the signals being classified.",https://ieeexplore.ieee.org/document/707755/,IEEE International Symposium on Industrial Electronics. Proceedings. ISIE'98 (Cat. No.98TH8357),7-10 July 1998,ieeexplore
10.1109/SNPDWinter52325.2021.00056,A study on Medical Device Security Status in Medical Convergence Industry,IEEE,Conferences,"Beyond informatization, the development of new technologies and the introduction of innovative technologies in the Fourth Industrial Revolution are the evolution of various industries. In the early information age, the introduction of various information systems has made it possible to manage and utilize large amounts of data in high quality, thus expanding the business of other industries faster and more reliably. However, in certain industries, various devices and equipments are dependent on the existing information system, and thus, time and cost difficulties are introduced to introduce a new system. Accordingly, existing security problems rather than new security flaws are coming to reality. The purpose of this study is to investigate and analyze the security status of existing medical devices according to the arrival of convergence environment in the medical industry.",https://ieeexplore.ieee.org/document/9403515/,"2021 21st ACIS International Winter Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing (SNPD-Winter)",28-30 Jan. 2021,ieeexplore
10.1109/CNSC.2014.6906645,A survey of water distribution system and new approach to intelligent water distribution system,IEEE,Conferences,"This paper reviews the challenges in water distribution systems (WDS), development and deployment of intelligent systems to manage WDS efficiently. Recognizing that intelligent systems has the potential to revolutionize management of precious natural resource by utilities, this paper provides a summary of the research work and practices for researchers and industry practitioners to ensure that the technology cultivates sustainable drinking water management. At present a lot technological developments ensures reduction of labor cost for various tasks in WDS. The technical sophistications of intelligent systems to ensure optimal use of precious resources have increased noticeably in recent decades. This paper addresses all concerned issues with WDS such as real time data collection, forecast of future water consumption, labor cost, recovery cost of water treatment and distribution, power supply requirement, operational time, water leakage, remote capturing of meter reading etc. We proposed a new intelligent water distribution system, which overcomes most of the problems in WDS.",https://ieeexplore.ieee.org/document/6906645/,2014 First International Conference on Networks & Soft Computing (ICNSC2014),19-20 Aug. 2014,ieeexplore
10.1109/INDIN.2008.4618263,A system scheme of 3D object reconstruction from single 2D graphics based on neural networks,IEEE,Conferences,"3D reconstruction has turned out to be a significant need in computer aided design (CAD) technology. With the rapid development of virtual reality and industry design, more and more 3D models are required in practical needs. So an artificial neural network based system scheme is presented in this paper, which can be used in 3D object reconstruction from single 2D image. BP neural network (BP NN) is applied in our system due to its non-linear mapping and self-adaptive ability. The 3D objects are represented by Open Inventor, which is a 3D software development kit based on OpenGL. Experimental results show that neural network is a promising approach for reconstruction and representation of 3D objects.",https://ieeexplore.ieee.org/document/4618263/,2008 6th IEEE International Conference on Industrial Informatics,13-16 July 2008,ieeexplore
10.1109/CLEI.2017.8226440,A visualization tool to detect refactoring opportunities in SOA applications,IEEE,Conferences,"Service-oriented computing (SOC) has been widely used by software industry for building distributed software applications that can be run in heterogeneous environments. It has also been required that these applications should be both high-quality and adaptable to market changes. However, a major problem in this type of applications is its growth; as the size and complexity of applications increase, the probability of duplicity of code increases. This problem could have a negative impact on quality attributes, such as performance, maintenance and evolution, among others. This paper presents a web tool called VizSOC to assist software developers in detecting refactoring opportunities in service-oriented applications. The tool receives WSDL (Web Service Description Language) documents, detects anti-patterns and suggests how to resolve them, and delivers a list of refactoring suggestions to start working on the refactoring process. To visualize the results in an orderly and comprehensible way, we use the Hierarchical Edge Bundles (HEB) visualization technique. The experimentation of the tool has been supported using two real-life case-studies, where we measured the amount of anti-patterns detected and the performance of clustering algorithms by using internal validity criteria. The results indicate that VizSOC is an effective aid to detect refactoring opportunities, and also allows developers to reduce effort along the detection process.",https://ieeexplore.ieee.org/document/8226440/,2017 XLIII Latin American Computer Conference (CLEI),4-8 Sept. 2017,ieeexplore
10.1109/COMITCon.2019.8862212,ACT Testbot and 4S Quality Metrics in XAAS Framework,IEEE,Conferences,"The purpose of this paper is to analyze all Cloud based Service Models, Continuous Integration, Deployment and Delivery process and propose an Automated Continuous Testing and testing as a service based TestBot and metrics dashboard which will be integrated with all existing automation, bug logging, build management, configuration and test management tools. Recently cloud is being used by organizations to save time, money and efforts required to setup and maintain infrastructure and platform. Continuous Integration and Delivery is in practice nowadays within Agile methodology to give capability of multiple software releases on daily basis and ensuring all the development, test and Production environments could be synched up quickly. In such an agile environment there is need to ramp up testing tools and processes so that overall regression testing including functional, performance and security testing could be done along with build deployments at real time. To support this phenomenon, we researched on Continuous Testing and worked with industry professionals who are involved in architecting, developing and testing the software products. A lot of research has been done towards automating software testing so that testing of software product could be done quickly and overall testing process could be optimized. As part of this paper we have proposed ACT TestBot tool, metrics dashboard and coined 4S quality metrics term to quantify quality of the software product. ACT testbot and metrics dashboard will be integrated with Continuous Integration tools, Bug reporting tools, test management tools and Data Analytics tools to trigger automation scripts, continuously analyze application logs, open defects automatically and generate metrics reports. Defect pattern report will be created to support root cause analysis and to take preventive action.",https://ieeexplore.ieee.org/document/8862212/,"2019 International Conference on Machine Learning, Big Data, Cloud and Parallel Computing (COMITCon)",14-16 Feb. 2019,ieeexplore
10.1109/ICCAE.2009.47,AI Based Framework for Dynamic Modeling of Software Maintenance Effort Estimation,IEEE,Conferences,"With the growth of IT and software industry there has been a shift in the software development paradigm from traditional techniques based to object-oriented and component-based development techniques. Due to this shift dynamic changes occur in the technology, environment and many other qualitative/quantitative factors, leading to increased maintenance effort. The challenge then lies inaccurately modeling and estimating the software maintenance effort under such dynamically emerging circumstances. This paper summarizes some of the recent advances in the field of dynamic software maintenance effort estimation. Various new static and dynamic factors and their roles in effort prediction are discussed, and based on them a dynamic modeling framework is proposed. Artificial intelligence is contemplated in this research, mainly functional networks based. Further, the proposed framework needs to be validated with real life project data.",https://ieeexplore.ieee.org/document/4804539/,2009 International Conference on Computer and Automation Engineering,8-10 March 2009,ieeexplore
10.1109/COMPSAC48688.2020.0-227,AI and ML-Driving and Exponentiating Sustainable and Quantifiable Digital Transformation.,IEEE,Conferences,"AI is a major transforming technology impacting every sector of life. AI is not a force to deprive humans and take over the control, rather a real enabler and lever for digital transformation. The former view aligns with Hollywood movies, and need to be undressed with the latter, which is realistic and becoming tangible over time as more organizations, and communities are leveraging AI's potential. Developing a practical understanding of AI, its capabilities, the challenges, and opportunities that it brings is fundamental to get the maximum out of its envisaged potential. The objective of this paper is to highlight how technology and industry have developed, discuss the role of AI in driving intelligent transformation concentrating on an applicable understanding of AI and related technologies. We introduce a new conceptual framework: AI's multi-dimensional role, to highlight its transformative power in multiple aspects. We also introduce an AI based Information and Model Governance Framework.",https://ieeexplore.ieee.org/document/9202502/,"2020 IEEE 44th Annual Computers, Software, and Applications Conference (COMPSAC)",13-17 July 2020,ieeexplore
10.1109/CloudCom.2019.00037,APEX: Adaptive Ext4 File System for Enhanced Data Recoverability in Edge Devices,IEEE,Conferences,"Recently Edge Computing paradigm has gained significant popularity both in industry and academia. With its increased usage in real-life scenarios, security, privacy and integrity of data in such environments have become critical. Malicious deletion of mission-critical data due to ransomware, trojans and viruses has been a huge menace and recovering such lost data is an active field of research. As most of Edge computing devices have compute and storage limitations, difficult constraints arise in providing an optimal scheme for data protection. These devices mostly use Linux/Unix based operating systems. Hence, this work focuses on extending the Ext4 file system to APEX (Adaptive Ext4): a file system based on novel on-the-fly learning model that provides an Adaptive Recover-ability Aware file allocation platform for efficient post-deletion data recovery and therefore maintaining data integrity. Our recovery model and its lightweight implementation allow significant improvement in recover-ability of lost data with lower compute, space, time, and cost overheads compared to other methods. We demonstrate the effectiveness of APEX through a case study of overwriting surveillance videos by CryPy malware on Raspberry-Pi based Edge deployment and show 678% and 32% higher recovery than Ext4 and current state-of-the-art File Systems. We also evaluate the overhead characteristics and experimentally show that they are lower than other related works.",https://ieeexplore.ieee.org/document/8968863/,2019 IEEE International Conference on Cloud Computing Technology and Science (CloudCom),11-13 Dec. 2019,ieeexplore
10.1109/AIVR46125.2019.00036,"AR Tracking with Hybrid, Agnostic And Browser Based Approach",IEEE,Conferences,"Mobile platform tools are desirable when it comes to practical augmented reality applications. With the convenience and portability that the form factor has to offer, it lays an ideal basic foundation for a feasible use case in industry and commercial applications. Here, we present a novel approach of using the monocular Simultaneous Localization and Mapping (SLAM) information provided by a Cross-Reality (XR) device to augment the linked 3D CAD models. The main objective is to use the tracking technology for an augmented and mixed reality experience by tracking a 3D model and superimposing its respective 3D CAD model data over the images we receive from the camera feed of the XR device without any scene preparation (e.g markers or feature maps). The intent is to conduct a visual analysis and evaluations based on the intrinsic and extrinsic of the model in the visualization system that instant3Dhub has to offer. To achieve this we make use of the Apple's ARKit to obtain the images, sensor data and SLAM heuristic of client XR device, remote marker-less model based 3D object tracking from monocular RGB image data and hybrid client server architecture. Our approach is agnostic of any SLAM system or Augmented Reality (AR) framework. We make use of the Apple's ARKit because of the its ease of use, affordability, stability and maturity as a platform and as an integrated system.",https://ieeexplore.ieee.org/document/8942252/,2019 IEEE International Conference on Artificial Intelligence and Virtual Reality (AIVR),9-11 Dec. 2019,ieeexplore
10.1109/ISCAS45731.2020.9180670,AVAC: A Machine Learning Based Adaptive RRAM Variability-Aware Controller for Edge Devices,IEEE,Conferences,"Recently, the Edge Computing paradigm has gained significant popularity both in industry and academia. Researchers now increasingly target to improve performance and reduce energy consumption of such devices. Some recent efforts focus on using emerging RRAM technologies for improving energy efficiency, thanks to their no leakage property and high integration density. As the complexity and dynamism of applications supported by such devices escalate, it has become difficult to maintain ideal performance by static RRAM controllers. Machine Learning provides a promising solution for this, and hence, this work focuses on extending such controllers to allow dynamic parameter updates. In this work we propose an Adaptive RRAM Variability-Aware Controller, AVAC, which periodically updates Wait Buffer and batch sizes using on-the-fly learning models and gradient ascent. AVAC allows Edge devices to adapt to different applications and their stages, to improve computation performance and reduce energy consumption. Simulations demonstrate that the proposed model can provide up to 29% increase in performance and 19% decrease in energy, compared to static controllers, using traces of real-life healthcare applications on a Raspberry-Pi based Edge deployment.",https://ieeexplore.ieee.org/document/9180670/,2020 IEEE International Symposium on Circuits and Systems (ISCAS),12-14 Oct 2020,ieeexplore
10.1109/IDT.2013.6727080,Accurate and efficient identification of worst-case execution time for multicore processors: A survey,IEEE,Conferences,"Parallel systems were for a long time confined to high-performance computing. However, with the increasing popularity of multicore processors, parallelization has also become important for other computing domains, such as desktops and embedded systems. Mission-critical embedded software, like that used in avionics and automotive industry, also needs to guarantee real time behavior. For that purpose, tools are needed to calculate the worst-case execution time (WCET) of tasks running on a processor, so that the real time system can make sure that real time guarantees are met. However, due to the shared resources present in a multicore system, this task is made much more difficult as compared to finding WCET for a single core processor. In this paper, we will discuss how recent research has tried to solve this problem and what the open research problems are.",https://ieeexplore.ieee.org/document/6727080/,2013 8th IEEE Design and Test Symposium,16-18 Dec. 2013,ieeexplore
10.1109/ICSC.2008.33,Activity Recognition Using a Web 3.0 Database,IEEE,Conferences,"Web 3.0 envisages software agents that know how to reason over activities, events, locations, people, companies, and their inter-relationships. Learning more about customers through behavioral and activity recognition is here today through currently available Semantic Technologies and is a showcase for how these technologies will evolve. The demonstration shows real world examples of activity recognition using a combination of industry standard RDF and OWL, reasoning with basic Geotemporal primitives and some well-known Social Network Analytics.",https://ieeexplore.ieee.org/document/4597233/,2008 IEEE International Conference on Semantic Computing,4-7 Aug. 2008,ieeexplore
10.1109/INDIN.2017.8104789,Addressing security challenges in industrial augmented reality systems,IEEE,Conferences,"In context of Industry 4.0 Augmented Reality (AR) is frequently mentioned as the upcoming interface technology for human-machine communication and collaboration. Many prototypes have already arisen in both the consumer market and in the industrial sector. According to numerous experts it will take only few years until AR will reach the maturity level to be deployed in productive applications. Especially for industrial usage it is required to assess security risks and challenges this new technology implicates. Thereby we focus on plant operators, Original Equipment Manufacturers (OEMs) and component vendors as stakeholders. Starting from several industrial AR use cases and the structure of contemporary AR applications, in this paper we identify security assets worthy of protection and derive the corresponding security goals. Afterwards we elaborate the threats industrial AR applications are exposed to and develop an edge computing architecture for future AR applications which encompasses various measures to reduce security risks for our stakeholders.",https://ieeexplore.ieee.org/document/8104789/,2017 IEEE 15th International Conference on Industrial Informatics (INDIN),24-26 July 2017,ieeexplore
10.1109/ICTer51097.2020.9325474,Adjusting The Hard Level on Game by a Prediction for Improving Attraction and Business Value,IEEE,Conferences,"The computer gaming industry has become more popular among the young generation over the years. It became a sport and According to past studies, people choose computer games over physical games (outdoor games) due to their busy lifestyles which crack social relationships and fear of injuries. There are around 2.2 billion gamers around the world. Out of the assessed 7.6 billion individuals living on earth, as of July 2018, that implies nearly 33% of individuals on this planet are gamers. Out of those 2.2 billion gamers, 1.2 billion of the players play PC game. An online survey was conducted to collect information from random game players and non-game players scattered throughout Sri Lanka to identify the problems faced by them when playing the games and why others do not play the game or are discouraged to play games. 354 participants participated in this survey. In which 127 of the participants were Female and 227 Male. The participation age range was identified as 15 to 49 Years. The research team was able to summarize that 80.04% of the participants play games while 19.06% don't play games. Out of the participants who play games, 9.08% are pro players while the rest of the 78% of players are intermediate players. We identified that the main problems faced by most of the players are as follows: Some missions are too easy and boring or players getting stuck in the middle of the level because the level is too hard or because the mission is too hard to complete. The Popularity of games can be varied according to the players' skill level. Tasks of a game can be boring if it's too easy for a skilled player. On the other hand, if it's too hard for a beginner level player, it may tend to give up the game. The other problem identified was the inability of game character customization as per the preference of 92.07% of gamers who participated in the survey. According to their responses, we were able to understand the problems that they faced while playing the game and what they expect in a game. Therefore, the research team proposes a game that changes the level of hardness/complexity of the level according to the players' mood automatically. In simple terms, the game will `adjust and adopt' based on the user's emotions. The next level will be automatically designed based on the player record of the previous level using an algorithm that takes the emotions as inputs. To achieve this, the gamer's heart pulse rate, facial expression, and speed of using the controls are captured and analysed to obtain the most accurate emotion level of the player using machine learning. The level of emotions is taken along with the parameters such as age, gender, and haptic feedback to adjust the hardness level of the level.It is important to identify the player skill level to understand the user's experience as a gamer. To determine a player's skill level in the background, a keylogger captures the pressed key and the time it takes to press the key. It is also necessary to identify the emotional state of the player to provide a unique user experience to the player and it can be done in two ways which are facial expression and heart-pulse. Facial expression detection results in identifying and predicting the basic emotions of an individual such as happiness, anger, and neutral which will be necessary for the adaptive game, and also the emotional range of a player is predicted. Additionally, age, gender, and face-shape are determined. The dataset is trained using machine learning to detect the facial expression and identify the emotion, age, gender, and face-shape of the individual. The Facial Expression of the player will be captured in real-time from the web camera of the computer and the emotion will be identified automatically as well as the age gender and face-shape. The other method is through Heart pulse, Heart pulse also can be used to detect the emotions of the player. There are 3 types of emotion levels Happy, Fear, Neutral. To do this, the heart rate sensor is used to capture the player's heart rate. According to the output received after training the dataset using machine learning algorithms and the skill level output is used to adjust the level of the game while the gamer is playing the game. To obtain a much more accurate emotional level, the heartbeat level, and the facial emotional level are analysed together. We have created a new model for analysing the above data using linear regression. Using the said model, the gender, age, the skill level of the player as well as the difficulty in the level of the game can be identified. The next level of the game will be designed using the data from the previous level and this process will continue until the player quits the game. A model is trained by using machine learning technology to predict character characteristics (moving speed, health regenerating) of the game character. The difficulty level of the player is taken as input for the training process. The output characteristics are given by an in-game module and adjust the characteristics of the game character until the player quits playing. The AI algorithm performs the process of reflecting the characteristics of the player to the in-game character. Age, gender, and face-shape are used as input data. The algorithm process input data to generate a unique set of variables. The variables manipulate the appearance of the in-game character until the player quits playing. Our game is developed within three months; therefore, it is difficult to predict the emotions or obtain a skill level like in an existing commercialized game because the demo game doesn't evoke any emotions or measure the skill level of the player. So, we have tested our application with the other similar existing games and we were able to obtain an accurate emotion and skill level. Even though the game is designed in a way to change the levels and character abilities based on the inputs obtained, we can't do so in other existing games. Therefore, the player records obtained from the games we tested were taken into a text record and analysed separately. Based on the analysed data we were able to obtain an accuracy level of 68%.In the future, the demo game will be further developed and tested with our system to obtain an emotion. The option of changing the game character based on the player's appearance is 75% successful, in which face-shape and gender were used. In the future, we will be using age as well. In the future a profile can be designed based on skill level, therefore making it possible to adjust the game based on a player's past record.This system provides a solution to problems faced by the game players such as the game being too easy and boring or hard and stressful. This can also prevent health issues that can be caused when a person gets too angry or violent and stress.",https://ieeexplore.ieee.org/document/9325474/,2020 20th International Conference on Advances in ICT for Emerging Regions (ICTer),4-7 Nov. 2020,ieeexplore
10.1109/CISCT.2019.8777408,Adoption of VR influencing AI on 3D objects,IEEE,Conferences,"The advent of new technologies has made it possible for new areas to be explored in the gaming industry. Virtual Reality has progressed at an exponential rate which has allowed it to be used in many industries. Virtual Reality allows users to be immersed in a highly detailed environment which allows for a more realistic experience. This paper outlines the development of a game on adoption of VR influencing AI on 3D objects, and the integration of virtual reality gaming Oculus and Leap Motion device in the gameplay, The Leap Motion is used to recognize hand gestures while Oculus will serve as the visual medium for the user. The game itself has been designed and developed on the Unity 3D gaming engine. This project endeavors to highlight the importance and diverse application of VR in various industries. The distinctive integration of Oculus, Leap and PC leads to a realistic gaming experience.",https://ieeexplore.ieee.org/document/8777408/,2019 International Conference on Information Science and Communication Technology (ICISCT),9-10 March 2019,ieeexplore
10.1109/SoutheastCon42311.2019.9113415,Advanced Signal Processing for Decision Making and Decision-Fusion Software Systems for Aircraft Structural Health Monitoring,IEEE,Conferences,"Tracking the structure health status of the aircraft fleet is one of the important factors for improving safety in the aviation industry. US Airforce Research Lab is working hard to develop technology to monitor the structural health status of aircraft before and after takeoff. This technology is called Structural Health Monitoring or Structural Health Management (SHM), and uses advanced signal processing techniques. The advantages of this include improving the technology and safety factors of the aircraft by increasing the robustness of the aircraft structure as well as reduce the cost of the labor maintenance. This research paper focuses on developing a software system that is capable of detecting a crack in the aircraft structure at an early stage. Also, the developed software will be used to estimate the crack lengths within 90% accuracy. The developed software uses the Matlab environment for all algorithms developed: calculating and finding the crack, and estimating the crack length. Artificial Intelligent (AI) techniques such as fuzzy logic and neural networks are used to support the decision regarding the length of the crack. Also, developed decision fusion frameworks are designed to increase the accurate percentage rate of the decision making results. The results obtained from this research are compared with the baseline developed by The Air Force Research Laboratory (AFRL). Finally, the results of this research along with the capabilities of using advanced signal processing integrated with AI, show that the calculations for the crack and the crack length produce very acceptable results using a real data gathered by AFRL. Also, the percentage average error performance analysis of the developed software's system algorithms is provided in the research paper.",https://ieeexplore.ieee.org/document/9113415/,2019 SoutheastCon,11-14 April 2019,ieeexplore
10.1109/BigData.2014.7004408,Advanced planning and control of manufacturing processes in steel industry through big data analytics: Case study and architecture proposal,IEEE,Conferences,"Enterprises in today's globalized world are compelled to react on threats and opportunities in a highly flexible manner. Hence, companies that are able to analyze the current state of their business processes, forecast their most optimal progresses and with this proactively control them will have a decisive competitive advantage. Technological progress in sensor technology has boosted real-time situation awareness, especially in manufacturing operations. The paper at hands examines, based on a case study stemming from the steel manufacturing industry, which production-related data is collectable using state of the art sensors forming a basis for a detailed situation awareness and for deriving accurate forecasts. However, analyses of this data point out that dedicated big data analytics approaches are required to utilize the full potential out of it. By proposing an architecture for predictive process planning and control systems, the paper intends to form a working and discussion basis for further research and implementation efforts in big data analytics.",https://ieeexplore.ieee.org/document/7004408/,2014 IEEE International Conference on Big Data (Big Data),27-30 Oct. 2014,ieeexplore
10.1109/EISIC.2017.21,Adversarial Machine Learning in Malware Detection: Arms Race between Evasion Attack and Defense,IEEE,Conferences,"Since malware has caused serious damages and evolving threats to computer and Internet users, its detection is of great interest to both anti-malware industry and researchers. In recent years, machine learning-based systems have been successfully deployed in malware detection, in which different kinds of classifiers are built based on the training samples using different feature representations. Unfortunately, as classifiers become more widely deployed, the incentive for defeating them increases. In this paper, we explore the adversarial machine learning in malware detection. In particular, on the basis of a learning-based classifier with the input of Windows Application Programming Interface (API) calls extracted from the Portable Executable (PE) files, we present an effective evasion attack model (named EvnAttack) by considering different contributions of the features to the classification problem. To be resilient against the evasion attack, we further propose a secure-learning paradigm for malware detection (named SecDefender), which not only adopts classifier retraining technique but also introduces the security regularization term which considers the evasion cost of feature manipulations by attackers to enhance the system security. Comprehensive experimental results on the real sample collections from Comodo Cloud Security Center demonstrate the effectiveness of our proposed methods.",https://ieeexplore.ieee.org/document/8240775/,2017 European Intelligence and Security Informatics Conference (EISIC),11-13 Sept. 2017,ieeexplore
10.1109/EEEI.2014.7005849,Adware detection and privacy control in mobile devices,IEEE,Conferences,"In this paper we propose a system and algorithms for detection of Adware in mobile devices that are based on machine learning algorithms and are capable of adapting to the ongoing transformation of Adware and Malware. The system is based on static and dynamic analysis of mobile applications, extraction of useful features and real-time classification. This classification is based on supervised machine learning algorithms with emphasis on fast, linear operations and efficient implementation on mobile devices. The system presented in this paper enables identification of relevant features that are salient in Adware and Malware, useful for further analysis by security researchers. The proposed system exhibits a detection rate of 97%. The system has been tested and verified on known, industry standard, datasets and is superior to state of the art solutions available in the market. These result have been verified by 3<sup>rd</sup> party evaluators.",https://ieeexplore.ieee.org/document/7005849/,2014 IEEE 28th Convention of Electrical & Electronics Engineers in Israel (IEEEI),3-5 Dec. 2014,ieeexplore
10.1109/CEWIT.2013.6713745,Agent-based planning and control for groupage traffic,IEEE,Conferences,"In this research and technology transfer project, the planning and control processes of the industrial partner Hellmann Worldwide Logistics GmbH &amp; Co. KG are analyzed. An agent-based approach is presented to model current processes and to exploit the identified optimization potential. The developed system directly connects the information flow and the material flow as well as their interdependencies in order to optimize the planning and control in groupage traffic. The software system maps current processes to agents as system components and improves the efficiency by intelligent objects. To handle the high complexity and dynamics of logistics autonomous intelligent agents plan and control the way of represented objects through the logistic network by themselves and induce a flexible and reactive system behavior. We evaluate the implemented dispatching application by simulating the groupage traffic processes using effectively transported orders and process data provided by our industrial partner. Moreover, we modeled real world infrastructures and considered also the dynamics by the simulation of unexpected events and process disturbances. The results show that the system significantly decreases daily cost by reducing the required number of transport providers and shifting conventional orders to next days, which need no immediate delivery. Thus the system increases the efficiency and meets the special challenges and requirements of groupage traffic. Moreover, the system supports freight carriers and dispatchers with adequate tour and routing proposals. Computed tours were successfully validated by human dispatchers. Due to the promising results, Hellmann is highly interested in transferring the prototype to an application that optimizes the daily operations in numerous distribution centers. Finally, provide further research perspectives, and emphasize the advantages of the developed system in Industry 4.0 applications.",https://ieeexplore.ieee.org/document/6713745/,2013 10th International Conference and Expo on Emerging Technologies for a Smarter World (CEWIT),21-22 Oct. 2013,ieeexplore
10.1109/NOMS.1998.655214,Alarm correlation engine (ACE),IEEE,Conferences,"Networks are growing in size and complexity, resulting in increased alarm volume and number of unfamiliar alarms. Often, there is no proportional increase in monitoring personnel and response time to faults suffers. GTE deployed Telephone Operations Network Integrated Control System (TONICS) in 1993 to support its network management operations. To stay competitive in the face of continued staff reductions, increase in network size, and monitoring complications related to deregulation of the telephone industry, GTE is introducing artificial intelligence techniques into TONICS. Alarm Correlation Engine (ACE), the system described in this paper, is part of the effort. ACE aids network management by correlating alarms on the basis of common cause to provide alarm compression, filtering, and suppression. In conjunction with its ability to carry out prescribed responses, it improves response times and increases productivity. ACE was developed with the following requirements: reliability, speed, versatility (handle alarms from different switches and networks), ease of knowledge engineering (field technicians must be able to construct, test, and modify correlation patterns), handle in real time multiple network problems, and finally, interface smoothly with GTE's TONICS system. ACE's strength lies in its domain specific correlation language which facilitates knowledge engineering and in its asynchronous processing core that enables integration into a real-time monitoring system.",https://ieeexplore.ieee.org/document/655214/,NOMS 98 1998 IEEE Network Operations and Management Symposium,15-20 Feb. 1998,ieeexplore
10.1109/DCOSS49796.2020.00046,An Agnostic Data-Driven Approach to Predict Stoppages of Industrial Packing Machine in Near,IEEE,Conferences,"As data awareness in manufacturing companies increases with the deployment of sensors and Internet of Things (IoT) devices, data-driven maintenance and prediction have become quite popular in the Industry 4.0 paradigm. Machine Learning (ML) has been recognised as a promising, efficient and reliable tool for fault detection use cases, as it allows to export important knowledge from monitored assets. Scientists deal with issues such as the small amount of data that indicate potential problems, or the imbalance which exists between the standard process data and the data inadequacy of the systems to make a high precision forecast. Currently, in this context, even large industries are not able to effectively predict abnormal behaviors in their tools, processes and equipment, when adopting strategies to anticipate crucial events. In this paper, we propose a methodology to enable prediction of a packing machine's stoppages in manufacturing process of a large industry, by using forecasting techniques based on univariate time series data. There are more than 100 reasons that cause the machine to stop, in a quite big production line length. However, we use a single signal, concerning the machines operational status to make our prediction, without considering other fault or warning signals, hence its characterization as ""agnostic"". A workflow is presented for cleaning and preprocessing the data, and for training and evaluating a predictive model. Two predictive models, namely ARIMA and Prophet, are applied and evaluated on real data from an advanced machining process used for packing. Training and evaluation tests indicate that the results of the applied methods perform well on a daily basis. Our work can be further extended and act as reference for future research activities that could lead to more robust and accurate prediction frameworks.",https://ieeexplore.ieee.org/document/9183540/,2020 16th International Conference on Distributed Computing in Sensor Systems (DCOSS),25-27 May 2020,ieeexplore
10.1109/ISEFS.2006.251169,An Approach to Real-time Color-based Object Tracking,IEEE,Conferences,"Object tracking is of great interest in different areas of industry, security and defense. Tracking moving objects based on color information is more robust than systems utilizing motion cues. In order to maintain the lock on the object as the surrounding conditions vary, the color model needs to be adapted in real-time. In this paper an on-line learning method for the color model is implemented using fuzzy adaptive resonance theory (ART). Fuzzy ART is a type of neural network that is trained based on competitive learning principle. The color model of the target region is regularly updated based on the vigilance criteria (which is a threshold) applied to the pixel color information. The target location in the next frame is predicted using evolving extended Takagi-Sugeno (exTS) model to improve the tracking performance. The results of applying exTS for prediction of the position of the moving target were compared with the usually used solution based on Kalman filter. The experiments with real footage demonstrate over a variety of scenarios the superiority of the exTS as a predictor comparing to the Kalman filter. Further investigation concentrates on using evolving clustering for realizing computationally efficient simultaneous tracking of different segments in the object",https://ieeexplore.ieee.org/document/4016733/,2006 International Symposium on Evolving Fuzzy Systems,7-9 Sept. 2006,ieeexplore
10.1109/ICCC.2018.00010,An Edge Based Smart Parking Solution Using Camera Networks and Deep Learning,IEEE,Conferences,"The smart parking industry continues to evolve as an increasing number of cities struggle with traffic congestion and inadequate parking availability. For urban dwellers, few things are more irritating than anxiously searching for a parking space. Research results show that as much as 30% of traffic is caused by drivers driving around looking for parking spaces in congested city areas. There has been considerable activity among researchers to develop smart technologies that can help drivers find a parking spot with greater ease, not only reducing traffic congestion but also the subsequent air pollution. Many existing solutions deploy sensors in every parking spot to address the automatic parking spot detection problems. However, the device and deployment costs are very high, especially for some large and old parking structures. A wide variety of other technological innovations are beginning to enable more adaptable systems-including license plate number detection, smart parking meter, and vision-based parking spot detection. In this paper, we propose to design a more adaptable and affordable smart parking system via distributed cameras, edge computing, data analytics, and advanced deep learning algorithms. Specifically, we deploy cameras with zoom-lens and motorized head to capture license plate numbers by tracking the vehicles when they enter or leave the parking lot; cameras with wide angle fish-eye lens will monitor the large parking lot via our custom designed deep neural network. We further optimize the algorithm and enable the real-time deep learning inference in an edge device. Through the intelligent algorithm, we can significantly reduce the cost of existing systems, while achieving a more adaptable solution. For example, our system can automatically detect when a car enters the parking space, the location of the parking spot, and precisely charge the parking fee and associate this with the license plate number.",https://ieeexplore.ieee.org/document/8457691/,2018 IEEE International Conference on Cognitive Computing (ICCC),2-7 July 2018,ieeexplore
10.1109/ICMLA.2019.00115,An Edge Computing Visual System for Vegetable Categorization,IEEE,Conferences,"In self-service supermarket and retail industry, efforts to reduce customer wait time using automatic grocery item identification are challenged by low recognition accuracy, long response time and substantial requirement for equipment. In this paper, we propose a novel edge computing system named EdgeVegfru for vegetable and fruit image classification. While existing work on Vegfru dataset shows excellent performance, few of them have been deployed in real-world applications. We adopt an edge computing paradigm, design, implement and evaluate the whole system on the Android devices. The proposed deep learning model and quantization algorithm reduce the model size and inference time significantly. Our system has shown out-standing accuracy within limited time and computation resources, compared with other machine learning methods(such as Support Vector Machine(SVM), Random Forest(RF)), thus providing the potential path for automatic recognition and pricing in self-service retail stores.",https://ieeexplore.ieee.org/document/8999203/,2019 18th IEEE International Conference On Machine Learning And Applications (ICMLA),16-19 Dec. 2019,ieeexplore
10.1109/I2CT45611.2019.9033957,An Efficient Approach to Fruit Classification and Grading using Deep Convolutional Neural Network,IEEE,Conferences,"In India, the agricultural industry has seen a boom in recent years, demanding an increased inclusion of automation in it. An important aspect of this agro-automation is grading and classification of agricultural produce. These labor intensive tasks can be automated by use of Computer Vision and Machine Learning. This paper focuses on developing a standalone system capable of classifying 3 types of fruit and taking apple as test case of grading. The fruit types include apple, orange, pear and lemon. Further, apples have been graded into four grades, Grade 1 being the best quality apple and Grade 4 consisting of the spoilt ones. Input is given in the form of fruit image. The involved methodology is dataset formation, preprocessing, software as well as hardware implementations and classification. Preprocessing consists of background removal and segmentation techniques in order to extract fruit area. Deep Convolutional Neural Network has been chosen for the real time implementation of system and applied on fruit 360 dataset. For that purpose, the Inception V3 model is trained using the transfer training approach, thus enabling it to distinguish fruit images. The results after experimentation show that the Top 5 accuracy on the dataset used is 90% and the Top 1 accuracy is 85% which targets accuracy limitation of previous attempts.",https://ieeexplore.ieee.org/document/9033957/,2019 IEEE 5th International Conference for Convergence in Technology (I2CT),29-31 March 2019,ieeexplore
10.1109/IDAP.2018.8620812,An Embedded Real-Time Object Detection and Measurement of its Size,IEEE,Conferences,"In these days, real-time object detection and dimensioning of objects is an important issue from many areas of industry. This is a vital topic of computer vision problems. This study presents an enhanced technique for detecting objects and computing their measurements in real time from video streams. We suggested an object measurement technique for real-time video by utilizing OpenCV libraries and includes the canny edge detection, dilation, and erosion algorithms. The suggested technique comprises of four stages: (1) identifying an object to be measured by using canny edge detection algorithm, (2) using morphological operators includes dilation and erosion algorithm to close gaps between edges, (3) find and sort contours, (4) measuring the dimensions of objects. In the implementation of the proposed technique, we designed a system that used OpenCV software library, Raspberry Pi 3 and Raspberry Camera. The proposed technique was nearly achieved 98% success in determines the size of the objects.",https://ieeexplore.ieee.org/document/8620812/,2018 International Conference on Artificial Intelligence and Data Processing (IDAP),28-30 Sept. 2018,ieeexplore
10.1109/BDEIM52318.2020.00043,An Empirical Study on the Correlation between Operating Capital Management and Operating Performance-Taking the Communication Terminal and Accessories Industry as an example,IEEE,Conferences,"This article uses Stata16.0 software tools to analyze and research the 2015-2019 company annual reports of listed companies in the communication terminal and accessories industry collected in the RESSET financial research database, and summarizes the impact of short-term operating funds on operating performance in the company's supply, production and sales scenarios, provides reference suggestions and valuable experience for companies of different sizes in the communication terminal and accessories industry and other industries in the use of short-term operating funds, and promotes the sustainable and healthy development of enterprises.",https://ieeexplore.ieee.org/document/9407126/,2020 International Conference on Big Data Economy and Information Management (BDEIM),11-13 Dec. 2020,ieeexplore
10.1109/ICSME.2017.71,An Experience Report on Applying Passive Learning in a Large-Scale Payment Company,IEEE,Conferences,"Passive learning techniques infer graph models on the behavior of a system from large trace logs. The research community has been dedicating great effort in making passive learning techniques more scalable and ready to use by industry. However, there is still a lack of empirical knowledge on the usefulness and applicability of such techniques in large scale real systems. To that aim, we conducted action research over nine months in a large payment company. Throughout this period, we iteratively applied passive learning techniques with the goal of revealing useful information to the development team. In each iteration, we discussed the findings and challenges to the expert developer of the company, and we improved our tools accordingly. In this paper, we present evidence that passive learning can indeed support development teams, a set of lessons we learned during our experience, a proposed guide to facilitate its adoption, and current research challenges.",https://ieeexplore.ieee.org/document/8094462/,2017 IEEE International Conference on Software Maintenance and Evolution (ICSME),17-22 Sept. 2017,ieeexplore
10.1109/ICAPAI49758.2021.9462061,An Industrial Assistance System with Manual Assembly Step Recognition in Virtual Reality,IEEE,Conferences,"In the era of Industry 4.0, worker assistance systems are becoming more and more important. In order to assist shop floor workers in manual assembly tasks, we implemented an assistance system in virtual reality. A deep neural network was trained to recognize the current work step in real-time during an assembly process, thus giving the assistance system context-awareness. We defined the problem of assembly step recognition as a multivariate time series classification using the poses of the workers head, both hands and all relevant tools and objects. With this definition, the VR environment's output can also be replaced with data from the real world. For our proof-of-concept assembly step recognition system, we created an assembly process consisting of six different work steps, five movable assembly parts and one tool. We showed that we can train an activity recognition model for assembly steps with only 10 assembly recordings. To achieve this, we used multiple data augmentation techniques and proposed a novel method of synthesizing new training data, which we call Path Joining. With only 10 training recordings, we attain a categorical classification accuracy of 81 percent and with 60 recordings we achieve an accuracy of 89 percent.",https://ieeexplore.ieee.org/document/9462061/,2021 International Conference on Applied Artificial Intelligence (ICAPAI),19-21 May 2021,ieeexplore
10.1109/ICDCS.2019.00139,An Industrial IoT Solution for Evaluating Workers' Performance Via Activity Recognition,IEEE,Conferences,"The Industrial Internet of Things (IIoT) is a key pillar of the Fourth Industrial Evolution or Industry 4.0. It aims to achieve direct information exchange between industrial machines, people, and processes. By tapping and analysing such data, IIoT can more importantly provide for significant improvements in productivity, product quality, and safety via proactive detection of problems in the performance and reliability of production machines, workers, and industrial processes. While the majority of existing IIoT research is currently focusing on the predictive maintenance of industrial machines (unplanned production stoppages lead to significant increases in costs and lost plant productivity), this paper focuses on monitoring and assessing worker productivity. This IIoT research is particularly important for large manufacturing plants where most production activities are performed by workers using tools and operating machines. With this aim, this paper introduces a novel industrial IoT solution for monitoring, evaluating, and improving worker and related plant productivity based on workers activity recognition using a distributed platform and wearable sensors. More specifically, this IIoT solution captures acceleration and gyroscopic data from wearable sensors in edge computers and analyses them in powerful processing servers in the cloud to provide a timely evaluation of the performance and productivity of each individual worker in the production line. These are achieved by classifying worker production activities and computing Key Performance Indicators (KPIs) from the captured sensor data. We present a real-world case study that utilises our IIoT solution in a large meat processing plant (MPP). We illustrate the design of the IIoT solution, describe the in-plant data collection during normal operation, and present the sensor data analysis and related KPI computation, as well as the outcomes and lessons learnt.",https://ieeexplore.ieee.org/document/8884821/,2019 IEEE 39th International Conference on Distributed Computing Systems (ICDCS),7-10 July 2019,ieeexplore
10.1109/AITEST52744.2021.00024,An Industrial Workbench for Test Scenario Identification for Autonomous Driving Software,IEEE,Conferences,"Testing of autonomous vehicles involves enormous challenges for the automotive industry. The number of real-world driving scenarios is extremely large, and choosing effective test scenarios is essential, as well as combining simulated and real world testing. We present an industrial workbench of tools and workflows to generate efficient and effective test scenarios for active safety and autonomous driving functions. The workbench is based on existing engineering tools, and helps smoothly integrate simulated testing, with real vehicle parameters and software. We aim to validate the workbench with real cases and further refine the input model parameters and distributions.",https://ieeexplore.ieee.org/document/9564354/,2021 IEEE International Conference on Artificial Intelligence Testing (AITest),23-26 Aug. 2021,ieeexplore
10.1109/COASE.2018.8560557,An Industry 4.0 Cyber-Physical Framework for Micro Devices Assembly,IEEE,Conferences,"An advanced cyber manufacturing framework to support the collaborative assembly of micro devices is presented based on Industry 4.0 principles. The distributed cyber and physical components work together to plan, assemble and monitor micro assembly related tasks; micro assembly refers to the assembly of micron sized devices which cannot be manufactured by MEMS technologies. The collaborative framework proposed includes assembly planning and path planning modules, Virtual Reality based assembly simulation environments and physical assembly work cells. An ontology based approach was implemented to address semantic interoperability issues to support formation of temporary partnerships in a Virtual Enterprise context. The key to the design and implementation of this complex framework is an information centric process modeling approach which provides a data/information oriented basis for collaboration. A collaborative cyber physical test bed has been built to demonstrate feasibility of proposed framework and approach.",https://ieeexplore.ieee.org/document/8560557/,2018 IEEE 14th International Conference on Automation Science and Engineering (CASE),20-24 Aug. 2018,ieeexplore
10.1109/ic-ETITE47903.2020.450,An IoT Based Smart Water Quality Monitoring System using Cloud,IEEE,Conferences,"The Internet of Things (IoT) is the network of physical devices, vehicles, home appliances, and other items embedded with electronics, software, sensors, actuators and connectivity which enables these things to connect and exchange data. The number of IoT devices has increased 31% year-over-year to 8.4 billion in 2017 and it is estimated that there will be 30 billion devices by 2020. Water pollution is a major environmental problem in India. The largest source of water pollution in India is untreated sewage. Other sources of pollution include agricultural runoff and unregulated small scale industry that results in polluting, most of the rivers, lakes and surface water in India. In this paper, An IoT Based Smart Water Quality Monitoring System using Cloud and Deep Learning is proposed to monitor the quality of the water in water-bodies. In conventional systems, the monitoring process involves the manual collection of sample water from various regions, followed by laboratory testing and analysis. This process is ineffective, as this process is arduous and time-consuming and it does not provide real-time results. The quality of water should be monitored continuously, to ensure the safe supply of water from any water bodies and water resources. Hence, the design and development of a low-cost system for real-time monitoring of water quality using the Internet of Things (IoT) is essential. Monitoring water quality in water bodies using Internet of Things (IoT) helps in combating environmental issues and improving the health and living standards of all living things. The proposed system monitors the quality of water relentlessly with the help of IoT devices, such as, NodeMCU. The in-built Wi-Fi module is attached in NodeMCU which enables internet connectivity transfers the measured data from sensors to the Cloud. The prototype is designed in such a way that it can monitor the number of pollutants in the water. Multiple sensors are used to measure various parameters to assess the quality of water from water bodies. The results are stored in the Cloud, deep learning techniques are used to predict whether the water suitable or not.",https://ieeexplore.ieee.org/document/9077792/,2020 International Conference on Emerging Trends in Information Technology and Engineering (ic-ETITE),24-25 Feb. 2020,ieeexplore
10.1109/ISSE46696.2019.8984462,An IoT Reconfigurable SoC Platform for Computer Vision Applications,IEEE,Conferences,"The field of Internet of Things (IoT) and smart sensors has expanded rapidly in various fields of research and industrial applications. The area of IoT robotics has become a critical component in the evolution of Industry 4.0 standard. In this paper, we developed an IoT based reconfigurable System on Chip (SoC) robot that is fast and efficient for computer vision applications. It can be deployed in other IoT robotics applications and achieve its intended function. A Terasic Hexapod Spider Robot (TSR) was used with its DE0-Nano SoC board to implement our IoT robotics system. The TSR was designed to provide a competent computer vision application to recognize different shapes using a machine learning classifier. The data processing for image detection was divided into two parts, the first part involves hardware implementation on the SoC board and to provide real-time interaction of the robot with the surrounding environment. The second part of implementation is based on the cloud processing technique, where further data analysis was performed. The image detection algorithm for the computer vision component was tested and successfully implemented to recognize shapes. The TSR moves or reacts based on the detected image. The Field Programmable Gate Array (FPGA) part is programmed to handle the movement of the robot and the Hard Processor System (HPS) handles the shape recognition, Wi-Fi connectivity, and Bluetooth communication. This design is implemented, tested and can be used in real-time applications in harsh environments where movements of other robots are restricted.",https://ieeexplore.ieee.org/document/8984462/,2019 International Symposium on Systems Engineering (ISSE),1-3 Oct. 2019,ieeexplore
10.1109/WCNCW.2019.8902853,An Open 5G NFV Platform for Smart City Applications Using Network Softwarization,IEEE,Conferences,"Advanced wireless communication network testbeds are now widely being deployed around European and cross-continental. This represents an interesting opportunity for vertical industry and academia to perform experimentation and validation before a real deployment. In this paper, we present 5GinFIRE as a suitably flexible platform towards open 5G (Network Function Virtualization (NFV) ecosystem and playground. On top of this platform, we designed and deployed a smart city safety system as a vertical use case, exploring 5G capabilities through a combination of NFV and machine learning to provide end-to-end communication and low latency smart city service. This safety system helps detecting criminals along the city and sending a notification to the security center. A Virtual Network Function (VNF) has been developed to enable video transcoding, face detection and recognition at the cloud or the edge of the network. The validation of the overall system is performed through the deployment of the use case indoor (Smart Internet Lab) and outdoor (Millennium Square Bristol). We show the VNF specification and present a quantitative analysis in terms of bandwidth, response time, processing time and transmission speed in terms of Quality of Experience (QoE).",https://ieeexplore.ieee.org/document/8902853/,2019 IEEE Wireless Communications and Networking Conference Workshop (WCNCW),15-18 April 2019,ieeexplore
10.1109/DSC47296.2019.8937650,An Optimized Positive-Unlabeled Learning Method for Detecting a Large Scale of Malware Variants,IEEE,Conferences,"Malicious softwares (Malware) are able to quickly evolve into many different variants and evade existing detection mechanisms, rendering the ineffectiveness of traditional signature-based malware detection systems. Many researchers have proposed advanced malware detection techniques by using Machine Learning. Although the machine learning based techniques perform well in detecting a wide range of malware variants, there still remain some problems when meeting the real scene in the industry. Since the volume of new malware variants grows fast and labelling data is expensive and takes a lot of labor, companies cannot label every one of those samples. They tend to label a small part of the malware samples and treat the rest of the unlabeled samples as benign samples in which the original malware samples are treated as mislabeled. This causes a bias of decision boundary which severely limits the accuracy. To address such a problem, in this paper, we propose a cost-sensitive boosting method to train an unbiased detection model with the malicious-unlabeled executables to improve the accuracy. Along with that, in order to detect malware variants efficiently, we propose a byte co-occurrence matrix as a representation of byte streams of executables to detect malware variants directly. Experimental results show that the machine learning methods optimized by our approach can achieve 80% to 90% accuracy while the original machine learning methods can only achieve 50% to 85% accuracy when the unlabeled data contain different rates of mislabeled positive data.",https://ieeexplore.ieee.org/document/8937650/,2019 IEEE Conference on Dependable and Secure Computing (DSC),18-20 Nov. 2019,ieeexplore
10.1109/ICPHYS.2018.8390779,An approach for implementing key performance indicators of a discrete manufacturing simulator based on the ISO 22400 standard,IEEE,Conferences,"Performance measurement tools and techniques have become very significant in today's industries for increasing the efficiency of their processes in order to face the competitive market. The first step towards performance measurement is the real-time monitoring and gathering of the data from the manufacturing system. Applying these performance measurement techniques on real-world industry in a way that is more general and efficient is the next challenge. This paper presents a methodology for implementing the key performance indicators defined in the ISO 22400 standard-Automation systems and integration, Key performance indicators (KPIs) for manufacturing operations management. The proposed methodology is implemented on a multi robot line simulator for measuring its performance at runtime. The approach implements a knowledge-based system within an ontology model which describes the environment, the system and the KPIs. In fact, the KPIs semantic descriptions are based on the data models presented in the Key Performance Indicators Markup Language (KPIML), which is an XML implementation of models developed by the Manufacturing Enterprise Solutions Association (MESA) international organization.",https://ieeexplore.ieee.org/document/8390779/,2018 IEEE Industrial Cyber-Physical Systems (ICPS),15-18 May 2018,ieeexplore
10.1109/AIMSEC.2011.6010784,An improved B&amp;B technique applying to telecom industry,IEEE,Conferences,"The data mining technology is more and more widely used in the telecom industry. When we construct Bayesian Belief Network, the branch and bound technique based on the minimum description length principle (B&amp;B technique) is one of the classical algorithm. To telecom data, high dimensionality and huge volume set obstacle when constructing Bayesian Belief Network. But we utilize the inconspicuous correlation between telecom attributes, improve the process of B&amp;B technique and simplify the structure to solve complexity rooting from telecom data's feature. The algorithm first construct a dependence ordering, then a simplified B&amp;B technique suitable for telecom data is applied. We compare the result and complexity with the original B&amp;B technique. This paper uses real datasets from the telecom industry. The result shows that the new algorithm can construct the network almost the same as the original one, but with good performance.",https://ieeexplore.ieee.org/document/6010784/,"2011 2nd International Conference on Artificial Intelligence, Management Science and Electronic Commerce (AIMSEC)",8-10 Aug. 2011,ieeexplore
10.1109/SYSCON.2018.8369547,An interactive architecture for industrial scale prediction: Industry 4.0 adaptation of machine learning,IEEE,Conferences,"According to wiki definition, there are four design principles in Industry 4.0. These principles support companies in identifying and implementing Industry 4.0 scenarios, namely, Interoperability, Information transparency, Technical assistance, Decentralized decisions. In this paper we have discussed our work on an implementation of a machine learning based interactive architecture for industrial scale prediction for dynamic distribution of water resources across the continent, keeping the four corners of Industry 4.0 in place. We report the possibility of producing most probable high resolution estimation regarding the water balance in any region within Australia by implementation of an intelligent system that can integrate spatial-temporal data from various independent sensors and models, with the ground truth data produced by 250 practitioners from the irrigation industry across Australia. This architectural implementation on a cloud computing platform linked with a freely distributed mobile application, allowing interactive ground truthing of a machine learning model on a continental scale, shows accuracy of 90% with 85% sensitivity of correct surface soil moisture estimation with end users at its complete control. Along with high level of information transparency and interoperability, providing on-demand technical supports and motivating users by allowing them to customize and control their own local predictive models, show the successfulness of principles in Industry 4.0 in real environmental issues in the future adaptation in various industries starting from resource management to modern generation soft robotics.",https://ieeexplore.ieee.org/document/8369547/,2018 Annual IEEE International Systems Conference (SysCon),23-26 April 2018,ieeexplore
10.1109/ROBOT.1992.220085,An optimal scheduling of pick place operations of a robot-vision-tracking system by using back-propagation and Hamming networks,IEEE,Conferences,"The authors present a neural network approach to solve the dynamic scheduling problem for pick-place operations of a robot-vision-tracking system. An optimal scheduling problem is formulated to minimize robot processing time without constraint violations. This is a real-time optimization problem which must be repeated for each group of objects. A scheme which uses neural networks to learn the mapping from object pattern space to optimal order space offline and to recall online what has been learned is presented. The idea was implemented in a real system to solve a problem in large commercial dishwashing operations. Experimental results have been shown that with four different objects, time savings of up to 21% are possible over first-come, first-served schemes currently used in industry.&lt;<ETX>&gt;</ETX>",https://ieeexplore.ieee.org/document/220085/,Proceedings 1992 IEEE International Conference on Robotics and Automation,12-14 May 1992,ieeexplore
10.1109/ESSCIRC.1992.5468456,Analog Electronic Neural Networks,IEEE,Conferences,"The interest in analog circuit techniques for implementing neural nets is undiminished, as is indicated by a large number of recent designs, coming from universities as well as from industry. One group of circuits are networks containing the ""multiply-accumulate"" neurons with a large interconnectivity. The main motivation for using analog circuit techniques is the fact that the multiply-accumulate operation can be implemented compactly, if only a moderate precision of the computation is required. Other types of networks are more algorithm-specific, hard-wired for one function, for example Kohonen networks, or neuromorphic designs implementing functions found in the visual or the auditory system. Most neural nets are built with standard CMOS technology, except for a few designs in CCD technology. A few analog neural net chips are now commercially available, and more and more reports of applications are appearing. This is a significant step in the development of analog neural nets, as now their usefulness is being put to the test in ""real-world"" applications.",https://ieeexplore.ieee.org/document/5468456/,ESSCIRC '92: Eighteenth European Solid-State Circuits conference,21-23 Sept. 1992,ieeexplore
10.1109/CIT.2016.18,Analysis of Complex Data in Telecommunications Industry,IEEE,Conferences,"In this paper, we report an application of data analytics in a real world business case of the telecom industry. This work has been tied up with an IT company in India with a large data set of telecom customers. As part of data analytics, the first task was to perform cleansing of bad and missing data, transforming heterogeneous formats into a unified format, semantic analysis on the data (semantics of attributes and their relationships), and then perform unsupervised learning using BIRCH technique of clustering. We describe and discuss the approach and results of these tasks on a sample data in this work.",https://ieeexplore.ieee.org/document/7876322/,2016 IEEE International Conference on Computer and Information Technology (CIT),8-10 Dec. 2016,ieeexplore
10.1109/CONISOFT50191.2020.00025,Analysis of automated estimation models using machine learning,IEEE,Conferences,"Plenty of practice based on software estimation has been developed in software industry. Algorithmic models represent the most formal approach that have provided the most reliable results. However, the use of informal practice is still prevalent just like the expert judgment which will not allow Software Engineering grow up.An important activity in big and small companies is to generate reliable estimation models. The development of these models is usually based on information obtained from past projects and requires a deep and precise analysis.This paper presents the application of the automated estimation-model generator system that uses machine learning techniques whit the objective of analysing the accuracy of these models comparing them to the traditional estimation methods using an international database and the internal database of a company.",https://ieeexplore.ieee.org/document/9307790/,2020 8th International Conference in Software Engineering Research and Innovation (CONISOFT),4-6 Nov. 2020,ieeexplore
10.1109/CICT.2013.6558219,Analysis of rice granules using image processing and neural network,IEEE,Conferences,"In food handling industry, grading of granular food materials is necessary because samples of material are subjected to adulteration. In the past, food products in the form of particles or granules were passed through sieves or other mechanical means for grading purposes. In this paper, analysis is performed on basmati rice granules; to evaluate the performance using image processing and Neural Network is implemented based on the features extracted from rice granules for classification grades of granules. Digital imaging is recognized as an efficient technique, to extract the features from rice granules in a non-contact manner. Images are acquired for rice using camera. Conversion to gray scale, Median smoothing, Adaptive thresholding, Canny edge detection, Sobel edge Detection, morphological operations, extraction of quantitative information are the checks that are performed on the acquired image using image processing technique through Open source Computer Vision (Open CV) which is a library of functions that aids image processing in real time. The morphological features acquired from the image are given to Neural Network. This work has been done to identify the relevant quality category for a given rice sample based on its parameters. The performance of image processing reduced the time of operation and improved the crop recognition greatly. Grading results obtained from Neural Network system shows greater accuracy when compared with the outputs from human experts.",https://ieeexplore.ieee.org/document/6558219/,2013 IEEE Conference on Information & Communication Technologies,11-12 April 2013,ieeexplore
10.1109/ICRITO51393.2021.9596185,Analyzing Psychological Gamers Profile through Progressive Gaming and Artificial Intelligence,IEEE,Conferences,"Game development has evolved to a point of being a method where it is being used to improve the day-to-day lifestyle of the person playing the game (henceforth referred to as a ‘player’). With these advancements, this paper represents the Integration of Artificial Intelligence (AI) in Game Development. AI has already been introduced in Game Development, ranging from Computer Generated Non-Player Characters (NPC), randomly generated maps or level, or even the difficulty of the level. In Gaming, AI is based on responsive and adaptive video game experiences. Using NPC to power interactive experience to simulate a human Game-player. One of the new Frontier of development and the method games are played, AI uses the behaviour of the player to control the game experience. Games refer to data that is made systematic using algorithms and helps to develop games with human interaction. As a developer, the experience needs to be delivered and AI makes the process easier. AI needs to store information to be able to reference which can respond to various stimuli. The mass of information required is the reason why AI has not been adapted into the Industry. Using Voice Intelligence is changing the inputs of games, and assist to increase intelligence genre games. Creating frameworks using design acknowledgment and reinforcement realizing is an important part of the future of Game development. By making the AI intelligent makes the games more real. Since Players pay attention to details, developers can use AI to help develop the game. In this paper, we will be focusing on the ways to identify the behaviour of the player and then using the information to develop the game further. By creating a profile, we can arrange and suit various reinforcements. Using the 16 personality tests, we can identify and rate the player based on the factor of their Mind, Energy, Nature, Tactics, and Identity.",https://ieeexplore.ieee.org/document/9596185/,"2021 9th International Conference on Reliability, Infocom Technologies and Optimization (Trends and Future Directions) (ICRITO)",3-4 Sept. 2021,ieeexplore
10.1109/SITA.2015.7358402,Analyzing the non-functional requirements to improve accuracy of software effort estimation through case based reasoning,IEEE,Conferences,"Producing accurate software effort estimation is essential for effective software project management that remains a considerable challenge to software engineering and software industry in general. Many methods have been proposed to increase the accuracy of estimating the software project size, effort, or cost. However, the primary focus has been on functional requirements FRs. We are convinced that the rigorous estimation requires a thorough knowledge of all the requirements of the software to be measured. Consequently, a clear identification of the FRs and NFRs as well as a strong understanding of the relationships existing between them is crucial to get measurements closer to reality. In this paper, we propose an early software size and effort estimation method based on a combination of COSMIC and case based reasoning that uses individual requirement measurements as a solution to improve the performance of CBR and to increase the precision of the estimations. This hybrid technique consists in adjusting the FRs measurements by the effect of NFRs with which they are connected. A new link requirements model is proposed in which the possible relationships existing between FRs and NFRs are expressed. This combination will help to efficiently include NFRs, and their relations with FRs, earlier in the measurement process and throughout the life cycle of the software development project.",https://ieeexplore.ieee.org/document/7358402/,2015 10th International Conference on Intelligent Systems: Theories and Applications (SITA),20-21 Oct. 2015,ieeexplore
10.1109/LCN52139.2021.9524931,Anomaly Detection for Discovering Performance Degradation in Cellular IoT Services,IEEE,Conferences,"Connected and automated vehicles (CAVs) are envisioned to revolutionize the transportation industry, enabling autonomous processes and real-time exchange of information among vehicles and infrastructure. To safely navigate the roadways, CAVs rely on sensor readings and data from the surrounding vehicles. Hence, a fault or anomaly arising from the hardware, software, or the network can lead into devastating consequences regarding safety. This study investigates potential performance degradation caused by anomalies, by analyzing real-life vehicles’ sensory and network-related data. The aim is to utilize unsupervised learning for anomaly detection, with a goal to describe the cause and effect of the detected anomalies from a performance perspective. The results show around 93% F1-score when detecting anomalies imposed by the cellular network and the vehicle’s sensors. Moreover, with approximately 90% F1-score we can detect anomalous predictions from a deployed network-related ML model predicting cellular throughput and describe the root-causes behind the detected anomalies.",https://ieeexplore.ieee.org/document/9524931/,2021 IEEE 46th Conference on Local Computer Networks (LCN),4-7 Oct. 2021,ieeexplore
10.1109/ICNSURV.2016.7486356,Anomaly detection in aircraft data using Recurrent Neural Networks (RNN),IEEE,Conferences,"Anomaly Detection in multivariate, time-series data collected from aircraft's Flight Data Recorder (FDR) or Flight Operational Quality Assurance (FOQA) data provide a powerful means for identifying events and trends that reduce safety margins. The industry standard “Exceedance Detection” algorithm uses a list of specified parameters and their thresholds to identify known deviations. In contrast, Machine Learning algorithms detect unknown unusual patterns in the data either through semi-supervised or unsupervised learning. The Multiple Kernel Anomaly Detection (MKAD) algorithm based on One-class SVM identified 6 of 11 canonical anomalies in a large dataset but is limited by the need for dimensionality reduction, poor sensitivity to short term anomalies, and inability to detect anomalies in latent features. This paper describes the application of Recurrent Neural Networks (RNN) with Long Term Short Term Memory (LTSM) and Gated Recurrent Units (GRU) architectures which can overcome the limitations described above. The RNN algorithms detected 9 out the 11 anomalies in the test dataset with Precision = 1, Recall = 0.818 and F1 score = 0.89. RNN architectures, designed for time-series data, are suited for implementation on the flight deck to provide real-time anomaly detection. The implications of these results are discussed.",https://ieeexplore.ieee.org/document/7486356/,2016 Integrated Communications Navigation and Surveillance (ICNS),19-21 April 2016,ieeexplore
10.23919/SOFTCOM.2019.8903672,Anomaly-based Intrusion Detection in Industrial Data with SVM and Random Forests,IEEE,Conferences,"Attacks on industrial enterprises are increasing in number as well as in effect. Since the introduction of industrial control systems in the 1970' s, industrial networks have been the target of malicious actors. More recently, the political and warfare-aspects of attacks on industrial and critical infrastructure are becoming more relevant. In contrast to classic home and office IT systems, industrial IT, so-called OT systems, have an effect on the physical world. Furthermore, industrial devices have long operation times, sometimes several decades. Updates and fixes are tedious and often not possible. The threats on industry with the legacy requirements of industrial environments creates the need for efficient intrusion detection that can be integrated into existing systems. In this work, the network data containing industrial operation is analysed with machine learning- and time series-based anomaly detection algorithms in order to discover the attacks introduced to the data. Two different data sets are used, one Modbus-based gas pipeline control traffic and one OPC UA-based batch processing traffic. In order to detect attacks, two machine learning-based algorithms are used, namely SVM and Random Forest. Both perform well, with Random Forest slightly outperforming SVM. Furthermore, extracting and selecting features as well as handling missing data is addressed in this work.",https://ieeexplore.ieee.org/document/8903672/,"2019 International Conference on Software, Telecommunications and Computer Networks (SoftCOM)",19-21 Sept. 2019,ieeexplore
10.1109/ICE2T.2017.8215979,Application framework for forest surveillance and data acquisition using unmanned aerial vehicle system,IEEE,Conferences,"An application framework is proposed in this paper that considers low cost surveillance mechanism and data acquisition in the forest. An application is developed as proof of concept with detailed design that can take advantage of unmanned urban vehicle to be directly configured and controlled in real-time. The advantages are numerous; it can be used for many purposes. For example, it can be used for observing critical and important area for intruder activities or to know the current state of any object of interest. We considered using machine learning and image processing and can be used for species of trees in the forest by color and size detection. A separate service running on separate remote server will be responsible for this. We have proposed a application framework particularly to be cheap and targeted to be easy to handle by non-technical persons and that it does not require large software system knowledge like Pix4D or DroneDeploy. This system will be useful for operations and research specially the forestry and palm oil plantation surveillance, and sustainable timber industry that specially needs carefully collected imageries and data from objects. Collection of raw data from sensor networks is also proposed in the system architecture.",https://ieeexplore.ieee.org/document/8215979/,2017 International Conference on Engineering Technology and Technopreneurship (ICE2T),18-20 Sept. 2017,ieeexplore
10.1109/Confluence51648.2021.9377160,Application of Artificial Intelligence in Human Resource Management Practices,IEEE,Conferences,"In the dynamic and competitive world, technology has changed the pace of all the industry. Artificial intelligence is a technology which enables the industry to grow at faster pace and efficiently finishing their work. This technology has entered into various departments such as finance department, human resource department, marketing, production etc. AI system has enabled the organization to enhance their existing performance and efficiently performing functions on a day-to-day basis. Currently, due to dynamic and competitive environment people working at different managerial level are working under pressure and understanding the need of artificial intelligence at workplace. Authors have used quantitative research to conduct the research and regression methods has been used to analyse the data. AI as a technology has a role in the different HR practices starting from talent acquisition and extending it to the assessing the performance of the people at work place. This research will study the relation of artificial intelligence and HR functions and different functions performed by HR department. The objective is to understand the factor like innovativeness and how use of HR operations. To conduct the study HR professionals from different IT companies were considered. Through the analysis the result indicated the positive linkage between different factors such ease of use and innovativeness which clearly indicates that AI has a influence on both the factors. This research paper will provide indepth knowledge about artificial intelligence which is at present a new revolution in industry and referred as industry 4.0.",https://ieeexplore.ieee.org/document/9377160/,"2021 11th International Conference on Cloud Computing, Data Science & Engineering (Confluence)",28-29 Jan. 2021,ieeexplore
10.1109/ICSME.2019.00057,Application of Philosophical Principles in Linux Kernel Customization,IEEE,Conferences,"Philosophical principles are very useful in customization of Linux kernel, e.g., the answer for the question: ""For the pointer to the start address of page table, is it a physical address or a virtual address?"" can be derived by one simple philosophical principle: the depth of recursion is limited. This is because if the pointer were a virtual address, there would be another new page table to store the translation information of this virtual address, but who was responsible for storing the translation information of the start address of this new page table? This would result an infinite recursion. So the pointer definitely is a physical address. In fact, the usefulness of philosophical principles comes from the reduction of searching space. And this reduction is very important in customization of Linux kernel, for it could cut down the size of the new code needed to be read. This is especially valuable when considering that Linux kernel is continuously updating and huge now. Another example to further demonstrate the reduction of searching space in customization is showed in the following: in customization of file system in kernel version 3.10, the question: ""Does the Linux kernel itself maintain the consistency between the buffer cache and the page cache?"". This is a hard problem in practice, for without any guidance of philosophical principle, a developer has to read all of the code in Linux kernel to get a precise answer. The tricky part of this question is that if the developer only read a part of the codes and doesn't find any mechanisms for maintenance of cache consistency, the conclusion of non-existence of such mechanisms still can not be drawn, for there's still a possibility that such mechanisms exist in the codes not explored. Besides, if the developer search internet to find the answer, assume that the developer is lucky enough, he/she finally finds one program example on a web page shows that the inconsistency may raise between buffer cache and page cache. He/she still can not get the conclusion that Linux kernel does not maintain such consistency, because that program example maybe is only valid in a specific scenario, e.g. in kernel version 2.26, not 3.10. But we can get a satisfied answer by using the philosophical principle: the cost of management process should be far less than the value created by being managed process. By this principle, it can be drawn that Linux kernel doesn't maintain the consistency between the buffer cache and page cache in kernel 3.10. This is because that the data in buffer cache and page cache is highly dependent on application logic, so if Linux kernel wanted to maintain such consistency, it would have to track all these applications, which cost was much higher than the benefits that these applications could produce. However, the successful application of philosophical principles depends on two factors: firstly, establishment of a mapping between concepts in Linux system and well-known concepts in human society. This is not a new idea, e.g. the word of ""cost"" is a concept first appeared in human society, not in computer science, but nowadays, developers establish a mapping between this concept and concepts in computer science. Although the idea is very old, it is still very effective. Since well-known concepts in human society are familiar to most developers and are what they have in common, the cost of applying philosophical principles is reduced. Besides this, already existing cause-effect relations among concepts in human society can be highly possible to be reused in philosophical deduction in Linux kernel. E.g, in the mapping we established, process is treated as a human and since in religion of human society, God creates humankind, it is natural to derive that there's one process that creates all other processes in Linux system with high probability. Secondly, a concrete model with many qualitative and quantitative details should be the basis of philosophical deduction. We build such model according to our past experiences and the construction of the model follows the philosophical principle: unfold the complexity only when it is necessary. E.g., in this model, for a specific detail, it is covered only when it is required in practice. This is to lower down the cost of modelling huge and continuously evolving Linux kernel. This model is very important, without it, philosophical deduction is impossible. But it is really a hard work, according to our experiences, it needs at least 6-years of work on Linux kernel for one developer to build it. Although philosophical principles are very useful in practice, there's a big gap on the recognition of philosophical principles between academic researchers and industry practioners. E.g., some academic researcher seriously doubts whether the mapping above, which mentioned God, is helpful. In fact, it is, for by this mapping, a developer will know that the existence of the process, which is the origin of all other processes, is highly possible and also that process maybe is not easily observed. This is true, for that process is the process which PID is zero and that process can not be observed by Linux command: ""ps -e"". That process is a very valuable point of customization, e.g., by modifying that process, all processes in the Linux will be affected. Why does this big gap exist? We believe there're at least three reasons: i. The bias on philosophical principles. This usually comes from the observation that some developers establish wrong mapping between the philosophical principles and the objects in real world. But is that true for those that has been verified many times in practice? ii. Wrong expectations. E.g., hope to get the precise answer when applying philosophical principles, instead of reducing the searching space. iii. Some academic researchers do not realize that a good philosophical principle usually is the result of a deep learning process of many years by human brain. Finally, we suggest that more efforts should be put on the studying of philosophical principles in program understanding and we believe that in the near future, the philosophical principles plus AI will be a trend in program understanding.",https://ieeexplore.ieee.org/document/8919057/,2019 IEEE International Conference on Software Maintenance and Evolution (ICSME),29 Sept.-4 Oct. 2019,ieeexplore
,Application of RapidMiner and R environments to dangerous seismic events prediction,IEEE,Conferences,"Underground coal mining is a branch of an industry which safety of operation is very dependent on the natural hazards. A proper seismic event prediction is a significant aspect of building classification models from the real data, which can affect the coal mining safety increase. In this paper four models, built in a well known data mining environments, are presented. The obtained models, depending on a given implementation of popular methods, occurred comparable to the best results from the competition.",https://ieeexplore.ieee.org/document/7733247/,2016 Federated Conference on Computer Science and Information Systems (FedCSIS),11-14 Sept. 2016,ieeexplore
10.1109/CMI.2016.7413757,Application of close loop expert system for heating control of rolling mill furnaces in a steel plant,IEEE,Conferences,"One of the critical impediment faced in hierarchical control in an process industry is unavailability of exact mathematical co-relations, which can precisely define the process behavior. These are primarily due to variable, complex and un-measurable factors and noises influencing the process behavior. However, such cases are most appropriate application areas of Expert Systems. In process industry, Expert Systems are one of the successful application areas of Artificial Intelligence, where expertise and knowledge of a Process Expert or a group of Experts are embedded as computer inference software and database. In a real time situation, these systems can take intelligent decisions as would have been taken by process Expert on a similar situation. Determination of exact Set Process Temperatures or thermal regime on different parts of Rolling Mill Furnaces like Annealing Furnace in a Steel Industry is an intriguing problem. However, this decision is very crucial as final mechanical and metallurgical quality of steel stock significantly depends on fixing and accurate control of these temperatures. But as a irony, no well defined mathematical co-relations are available, which can predict exact thermal regime to be followed to achieve desired quality and properties of steel coils/sheets under heating inside such furnaces. The aforesaid intriguing issue has been successfully resolved by development and implementation of Expert System guided heating control system through prediction and control of optimum furnace temperatures inside Annealing Furnaces at Cold Rolling Mill of Bokaro Steel Plant and Decarburization-Annealing Furnace at Silicon Steel Mill of Rourkela Steel Plant. In both the cases, concepts of hierarchical automation has been used, wherein Expert System comprising Level-II tier of automation predicts most appropriate thermal regime to obtain desired product quality for a given set of steel sheet. A seamlessly dovetailed PLC constitutes Level-I automation layer. PLC monitors and controls the plant as per advice from Expert System. Both the systems have enhanced plants efficiency by improving production, quality and energy conservation.",https://ieeexplore.ieee.org/document/7413757/,"2016 IEEE First International Conference on Control, Measurement and Instrumentation (CMI)",8-10 Jan. 2016,ieeexplore
10.1109/IJCNN.2001.939567,Application of neural networks in online oil content monitors,IEEE,Conferences,"Four major types of oils were examined to obtain both fluorescence and light scattering spectra as a function of oil concentrations. The large variations in fluorescence and scattering intensity with oil types and sub-types make it difficult to calibrate the analytical instrument using traditional methods. We implemented a multivariate, nonlinear calibration of instrumental response through an artificial neural network. We demonstrated that the combined use of fluorescence and scattering data significantly improves the quantitative prediction accuracy. The trained backpropagation neural network was used successfully to predict the concentrations of single oils and their mixtures, and appears well suited for the calibration of an online oil content monitor. The newly developed technique permits the online monitoring of oil concentrations in wastewater discharged from ships and oil refinery industry.",https://ieeexplore.ieee.org/document/939567/,IJCNN'01. International Joint Conference on Neural Networks. Proceedings (Cat. No.01CH37222),15-19 July 2001,ieeexplore
10.1109/CAIBDA53561.2021.00014,Applications of Smart Energy Integrated Service Platform in Optimization of Energy Utilization of Customers,IEEE,Conferences,"In the new era, comprehensive energy service has gradually become an important development direction to promote the high-quality development of energy industry and boost the development of real economy. The construction of smart integrated energy service platform is the internal demand for energy service enterprises to transform into integrated energy service providers, and is a solution to provide comprehensive energy support services for power users. The platform is divided into perception layer, acquisition layer, network layer, platform layer, application layer and display layer from bottom to top. Its main functions include energy regulation, energy efficiency analysis, smart operation, energy trading and smart dispatching. The construction of smart energy comprehensive service platform can provide customers with more high-quality, more convenient and more comprehensive energy value-added services, and constantly create more value for customers. Practice has proved that the implementation of smart energy integrated service platform can greatly reduce the monthly energy cost, monthly power load of the park and monthly coal consumption in park and enhance the indexes of monthly active users and customer satisfaction.",https://ieeexplore.ieee.org/document/9545981/,"2021 International Conference on Artificial Intelligence, Big Data and Algorithms (CAIBDA)",28-30 May 2021,ieeexplore
10.1109/SMICND.2003.1252463,Applications of bio-inspired systems as function approximators,IEEE,Conferences,"In this paper we would like to show how the bio-inspired systems topologies can be applied as function approximators (nonlinear regressors) in a wealth of practical applications. We selected one application in the financial industry and another in real estate. The goal is to discover nonlinear mappings between input variables and important outcomes. In the financial arena the outcome is to predict the value of the S&amp;P 500 (one of the most important Standard&amp;Poor's indexes), using several financial indicators, while in the real estate application the problem is to estimate the price of a house based on several indicators. We will see that bio-inspired systems provide a very powerful analysis tool. The software tool used was NeuroSolutions, from NeuroDimension Inc., a leading edge neural network development software.",https://ieeexplore.ieee.org/document/1252463/,2003 International Semiconductor Conference. CAS 2003 Proceedings (IEEE Cat. No.03TH8676),28 Sept.-2 Oct. 2003,ieeexplore
10.1109/IV.2003.1217948,Applied visual user interface technique in knowledge management,IEEE,Conferences,"Extracting actionable insight from large high-dimensional data sets, and its use for more effective decision-making, has become a pervasive problem across many application fields in both research and industry. The objective of our presentation is to report on some investigations of this problem covering both these areas. Taking as the problem domain the area of ""unsupervised learning"", we show that by tightly coupling statistical analysis technique with combinations of visualization components and techniques for interactivity, real-time analysis of multidimensional data can be efficiently made. We give particular attention to the ways in which dynamic visual representations can be used in these contexts to facilitate shared understanding. Our system is implemented and validated in the context of 3D medical imaging knowledge construction, knowledge management and geovisualisation.",https://ieeexplore.ieee.org/document/1217948/,"Proceedings on Seventh International Conference on Information Visualization, 2003. IV 2003.",18-18 July 2003,ieeexplore
10.1109/NetSoft48620.2020.9165317,Applying Machine Learning to End-to-end Slice SLA Decomposition,IEEE,Conferences,"5G is set to revolutionize the network service industry with unprecedented use-cases in industrial automation, augmented reality, virtual reality and many other domains. Network slicing is a key enabler to realize this concept, and comes with various SLA requirements in terms of latency, throughput, and reliability. Network slicing is typically performed in an end-to-end (e2e) manner across multiple domains, for example, in mobile networks, a slice can span access, transport and core networks. Thus, if an SLA requirement is specified for e2e services, we need to ensure that the total SLA budget is appropriately proportioned to each participating domain in an adaptive manner. Such an SLA decomposition can be extremely useful for network service operators as they can plan accordingly for actual deployment. In this paper we design and implement an SLA decomposition planner for network slicing using supervised machine learning algorithms. Traditional optimization based approaches cannot deal with the dynamic nature of such services. We design machine learning models for SLA decomposition, based on random forest, gradient boosting and neural network. We then evaluate each class of algorithms in terms of accuracy, sample complexity, and model explainability. Our experiments reveal that, in terms of these three requirements, the gradient boosting and neural network algorithms for SLA decomposition out-perform random forest algorithms, given emulated data sets.",https://ieeexplore.ieee.org/document/9165317/,2020 6th IEEE Conference on Network Softwarization (NetSoft),29 June-3 July 2020,ieeexplore
10.1109/SHARK-ADI.2007.3,Architectural Knowlege Management Strategies: Approaches in Research and Industry,IEEE,Conferences,"The software architecture community has recently gained an increasing interest in managing architectural knowledge. However, up until now there have been no attempts to obtain an overview of the work in the field. In this paper we present a preliminary review on current approaches to architectural knowledge management. To this end, we compare approaches known from literature and encountered in industry with knowledge management theory. We found that in reports from research and practice there appears to be a preference to use the codification strategy. However, our observations of the software architecture industry show that organizations in general tend to use a personalization strategy unintentionally. This paper serves as a call for awareness of this gap between intention and reality, and questions the biased focus on intentional codification alone. We suggest to close this gap through focusing on hybrid approaches.",https://ieeexplore.ieee.org/document/4273342/,"Second Workshop on Sharing and Reusing Architectural Knowledge - Architecture, Rationale, and Design Intent (SHARK/ADI'07: ICSE Workshops 2007)",20-26 May 2007,ieeexplore
10.23919/CISTI49556.2020.9141124,Artificial Intelligence Applied to Software Testing: A Literature Review,IEEE,Conferences,"In the last few years Artificial Intelligence (AI) algorithms and Machine Learning (ML) approaches have been successfully applied in real-world scenarios like commerce, industry and digital services, but they are not a widespread reality in Software Testing. Due to the complexity of software testing, most of the work of AI/ML applied to it is still academic. This paper briefly presents the state of the art in the field of software testing, applying ML approaches and AI algorithms. The progress analysis of the AI and ML methods used for this purpose during the last three years is based on the Scopus Elsevier, web of Science and Google Scholar databases. Algorithms used in software testing have been grouped by test types. The paper also tries to create relations between the main AI approaches and which type of tests they are applied to, in particular white-box, grey-box and black-box software testing types. We conclude that black-box testing is, by far, the preferred method of software testing, when AI is applied, and all three methods of ML (supervised, unsupervised and reinforcement) are commonly used in black-box testing being the “clustering” technique, Artificial Neural Networks and Genetic Algorithms applied to “fuzzing” and regression testing.",https://ieeexplore.ieee.org/document/9141124/,2020 15th Iberian Conference on Information Systems and Technologies (CISTI),24-27 June 2020,ieeexplore
10.1109/ICE/ITMC52061.2021.9570215,Artificial Intelligence as Enabler for Sustainable Development,IEEE,Conferences,"When the UN published the 17 Sustainable Development Goals (SDGs) in 2015, emerging technologies like Artificial Intelligence (AI) were not yet mature. However, through its deployment across industry sectors and verticals, issues related to sustainability, fairness, inclusiveness, efficiency, and usability of these technologies are now priorities for global consumers and producers. This paper discusses what needs to be considered by both policy makers and ‘managers’ in order to exploit the use of AI for SDG achievement. AI can act as a real and meaningful enabler to achieve sustainability goals; however, it may also have negative impacts. Therefore, a carefully balanced approach is required to ensure that Artificial Intelligence systems are employed to help solve sustainability issues without inadvertently affecting other goals.",https://ieeexplore.ieee.org/document/9570215/,"2021 IEEE International Conference on Engineering, Technology and Innovation (ICE/ITMC)",21-23 June 2021,ieeexplore
10.1109/ICAACCA51523.2021.9465297,Assessing Machine learning-based approaches for Silica concentration estimation in Iron Froth flotation,IEEE,Conferences,"In the mining industry, specifically in the flotation process, there is a challenge associated to noninvasive, real-time contaminant and impurities estimation. Achieving predictions on contaminant levels has a high impact on quality insurance and it can help technicians and engineers to make adjustments in advance to improve the quality of the final product, and thus profits. In this paper, exploratory research is performed on the problem of silica concentrate estimation for iron ore froth flotation using machine learning techniques, with the goal to identify algorithms that may be suitable for industry soft sensor development. For this purpose, a public, real process dataset is used and three different machine learning techniques are implemented: Random Forest (RF), Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU). The techniques were implemented, tested and compared in terms of their error percentage, mean absolute error, mean square error, and root mean square error. Obtained results show acceptable performance for LTSM and GRU implementations, being LSTM network the out-performer with errors below 9%.",https://ieeexplore.ieee.org/document/9465297/,2021 IEEE International Conference on Automation/XXIV Congress of the Chilean Association of Automatic Control (ICA-ACCA),22-26 March 2021,ieeexplore
10.1109/ICSPCC.2018.8567797,Attack Detection for Wireless Enterprise Network: a Machine Learning Approach,IEEE,Conferences,"An increasing number of enterprises are adopting wireless technology to deploy networks. However, wireless enterprise networks are more vulnerable than wired networks because of the broadcast feature. Thus, illegal attacks such as data theft and information forgery seriously threaten the property and information security of users and enterprises; these phenomena are attracting increasing attention from both academia and industry. Additionally, effectively detecting the attacks in the wireless enterprise networks is one of todays most important and challenging problems, especially in Wi-Fi networks, as attacks become increasingly covert and diverse. Fortunately, WiFi networks produce large amounts of data, providing copious big data for researchers. In this paper, using the Aegean Wi-Fi Intrusion Dataset (AWID), which is derived from the real-world Wi-Fi network, we introduce machine learning to detect network attacks. To significantly increase the training and convergence speeds, we deploy two-dimensional data cleaning and select 18 useful attributes from the original set of 154. Then, we introduce support vector machine (SVM) to detect attacks based on the cleaned dataset. The detection accuracy for flooding attacks, injection attacks, and normal data reached 89.18%, 87.34%, and 99.88% respectively. To the best of our knowledge, this is the first study to introduce a two-dimensional data cleaning method with an SVM to improve the detection accuracy for attacks. Finally, our detection results are comparable with the existing studies; however, our method operates with simpler data attributes with faster and more efficient training speed.",https://ieeexplore.ieee.org/document/8567797/,"2018 IEEE International Conference on Signal Processing, Communications and Computing (ICSPCC)",14-16 Sept. 2018,ieeexplore
10.1109/ICIT.2017.7915547,Augmented reality based on edge computing using the example of remote live support,IEEE,Conferences,"Augmented Reality (AR) introduces vast opportunities to the industry in terms of time and therefore cost reduction when utilized in various tasks. The biggest obstacle for a comprehensive deployment of mobile AR is that current devices still leave much to be desired concerning computational and graphical performance. To improve this situation in this paper we introduce an AR Edge Computing architecture with the aim to offload the demanding AR algorithms over the local network to a high-end PC considering the real-time requirements of AR. As an example use case we implemented an AR Remote Live Support application. Applications like this on the one hand are strongly demanded in the industry at present, on the other hand by now mostly do not implement a satisfying tracking algorithm lacking computational resources. In our work we lay the focus on both, the possibilities our architecture offers regarding improvements of tracking and the challenges it implies in respect of real-time. We found that offloading AR algorithms in real-time is possible with available WiFi making use of standard compression techniques like JPEG. However it can be improved by future radio solutions offering higher bandwidth to avoid additional latency contributed by the coding.",https://ieeexplore.ieee.org/document/7915547/,2017 IEEE International Conference on Industrial Technology (ICIT),22-25 March 2017,ieeexplore
10.1109/IJCNN.2016.7727278,Augmenting adaptation with retrospective model correction for non-stationary regression problems,IEEE,Conferences,"Existing adaptive predictive methods often use multiple adaptive mechanisms as part of their coping strategy in non-stationary environments. We address a scenario when selective deployment of these adaptive mechanisms is possible. In this case, deploying each adaptive mechanism results in different candidate models, and only one of these candidates is chosen to make predictions on the subsequent data. After observing the error of each of candidate, it is possible to revert the current model to the one which had the least error. We call this strategy retrospective model correction. In this work we aim to investigate the benefits of such approach. As a vehicle for the investigation we use an adaptive ensemble method for regression in batch learning mode which employs several adaptive mechanisms to react to changes in the data. Using real world data from the process industry we show empirically that the retrospective model correction is indeed beneficial for the predictive accuracy, especially for the weaker adaptive mechanisms.",https://ieeexplore.ieee.org/document/7727278/,2016 International Joint Conference on Neural Networks (IJCNN),24-29 July 2016,ieeexplore
10.1109/WSAI49636.2020.9143279,Automated Analysis of Seizure Behavior in Video: Methods and Challenges,IEEE,Conferences,"Automated analysis of seizure behavior in video using intelligent video analytics technology has significant applications in healthcare industry, since it can provide accurate and quantitative measurement of human seizure behavior for assisting diagnosis. This paper presents a brief survey on intelligent video analytics for automated seizure behavior analysis, including both conventional motion analysis based approaches and the state-of-the-art machine learning based approaches. Furthermore, a new automated video analytics framework is proposed in this paper, by exploiting the machine learning approach to build a seizure motion model and performing automatic detection of seizure events in the surveillance video in real time. This paper also discusses the preliminary experimental results and deployment of the proposed framework, as well as the future research challenges in this area.",https://ieeexplore.ieee.org/document/9143279/,2020 2nd World Symposium on Artificial Intelligence (WSAI),27-29 June 2020,ieeexplore
10.1109/NOMS.2018.8406234,Automated diagnostic of virtualized service performance degradation,IEEE,Conferences,"Service assurance for cloud applications is a challenging task and is an active area of research for academia and industry. One promising approach is to utilize machine learning for service quality prediction and fault detection so that suitable mitigation actions can be executed. In our previous work, we have shown how to predict service-level metrics in real-time just from operational data gathered at the server side. This gives the service provider early indications on whether the platform can support the current load demand. This paper provides the logical next step where we extend our work by proposing an automated detection and diagnostic capability for the performance faults manifesting themselves in cloud and datacenter environments. This is a crucial task to maintain the smooth operation of running services and minimizing downtime. We demonstrate the effectiveness of our approach which exploits the interpretative capabilities of Self- Organizing Maps (SOMs) to automatically detect and localize different performance faults for cloud services.",https://ieeexplore.ieee.org/document/8406234/,NOMS 2018 - 2018 IEEE/IFIP Network Operations and Management Symposium,23-27 April 2018,ieeexplore
10.1109/CICommS.2013.6582853,Automated network application classification: A competitive learning approach,IEEE,Conferences,"The design of a sustainable application level classification system has, over the past few years, been the subject of much research by academics and industry alike. The methodologies proposed rely predominantly on predefined signatures for each protocol, applied to each passing flow in order to classify them. These signatures are often static, resulting in inaccuracies during the classification process. This problem is compounded by delays in signature update releases. This paper presents an approach toward automated signature generation, mitigating classification problems experienced with existing systems. A hierarchical system is proposed, where signatures are developed and deployed in real-time. The ideas set forth in this research are evaluated by experimentation in a live network environment. Discriminators of both encrypted and plain-text application protocol samples were recorded and automatically annotated by a Hierarchical Self-Organizing Map (HSOM). The clusters identified by the HSOM were used in a supervised training process that correctly identified protocols with an almost perfect (99% percent) success rate.",https://ieeexplore.ieee.org/document/6582853/,2013 IEEE Symposium on Computational Intelligence for Communication Systems and Networks (CIComms),16-19 April 2013,ieeexplore
10.1109/IJCNN.2010.5596494,Automated texture classification of marble shades with real-time PLC neural network implementation,IEEE,Conferences,"The subjective evaluation of marbles based on their visual appearance could be replaced by an automated texture classification system, intending to achieve high classification accuracy and production effectiveness. The existing marble classification methods from a computational point of view are either too complex or very expensive. Nowadays some inspection systems in marble industry that automates the quality-control tasks and shade classification are too expensive and are compatible only with specific technological equipment. In this paper a new approach for classification of marble tiles with similar shades is proposed. It is based on simple image preprocessing, on training a MLP neural network (MLP NN) with marble histograms and implementation of the algorithm in a Programmable Logic Controller (PLC) for real-time execution. A method for training the MLP NN aiming optimization of MLP parameters and topology is proposed. The designed automated system uses only standard PLC modules and communication interfaces. The experimental test results when recognizing marble textures with added motion blur are represented and discussed. The performance of the modeling technique is assessed with different training and test sets. The classification accuracy results are compared to other results obtained by similar approaches.",https://ieeexplore.ieee.org/document/5596494/,The 2010 International Joint Conference on Neural Networks (IJCNN),18-23 July 2010,ieeexplore
10.1109/VTC2021-Spring51267.2021.9448686,Automatic Generation of Critical Test Cases for the Development of Highly Automated Driving Functions,IEEE,Conferences,"The development of highly automated driving functions is currently one of the key drivers for the automotive industry and research. In addition to the technical constraints in the implementation of these functions, a major challenge is the verification of functional safety. Conventional approaches aiming at statistical validation in the sense of real test drives are reaching their economic limits. On the other hand, there are simulation methods that allow a lot of freedom in test case design, but whose representativeness and relevance must be proven separately. In this paper an approach is presented that allows to generate critical concrete scenarios and test cases for automated driving functions by means of a reinforcement learning based optimization using here the example of an overtaking assistant. For this purpose, a Q-Learning approach is used that automates the parameter generation for the test cases. While pure combinatorics of the variable parameters leads to an unmanageable amount of test cases, the percentage of actually relevant critical test cases is very low. In this work we show how the share of critical and thus relevant test cases can be increased significantly by using the presented method compared to a purely combinatorial parameter variation.",https://ieeexplore.ieee.org/document/9448686/,2021 IEEE 93rd Vehicular Technology Conference (VTC2021-Spring),25-28 April 2021,ieeexplore
10.1109/VTCSpring.2019.8746507,Autonomous Driving without a Burden: View from Outside with Elevated LiDAR,IEEE,Conferences,"The current autonomous driving architecture places a heavy burden in signal processing for the graphics processing units (GPUs) in the car. This directly translates into battery drain and lower energy efficiency, crucial factors in electric vehicles. This is due to the high bit rate of the captured video and other sensing inputs, mainly due to Light Detection and Ranging (LiDAR) sensor at the top of the car which is an essential feature in autonomous vehicles. LiDAR is needed to obtain a high precision map for the vehicle AI to make relevant decisions. However, this is still a quite restricted view from the car. This is the same even in the case of cars without a LiDAR such as Tesla. The existing LiDARs and the cameras have limited horizontal and vertical fields of visions. In all cases it can be argued that precision is lower, given the smaller map generated. This also results in the accumulation of a large amount of data in the order of several TBs in a day, the storage of which becomes challenging. If we are to reduce the effort for the processing units inside the car, we need to uplink the data to edge or an appropriately placed cloud. However, the required data rates in the order of several Gbps are difficult to be met even with the advent of 5G. Therefore, we propose to have a coordinated set of LiDAR's outside at an elevation which can provide an integrated view with a much larger field of vision (FoV) to a centralized decision making body which then sends the required control actions to the vehicles with a lower bit rate in the downlink and with the required latency. The calculations we have based on industry standard equipment from several manufacturers show that this is not just a concept but a feasible system which can be implemented.The proposed system can play a supportive role with existing autonomous vehicle architecture and it is easily applicable in an urban area.",https://ieeexplore.ieee.org/document/8746507/,2019 IEEE 89th Vehicular Technology Conference (VTC2019-Spring),28 April-1 May 2019,ieeexplore
10.1109/ICDM50108.2020.00084,Autonomous Graph Mining Algorithm Search with Best Speed/Accuracy Trade-off,IEEE,Conferences,"Graph data is ubiquitous in academia and industry, from social networks to bioinformatics. The pervasiveness of graphs today has raised the demand for algorithms that can answer various questions: Which products would a user like to purchase given her order list? Which users are buying fake followers to increase their public reputation? Myriads of new graph mining algorithms are proposed every year to answer such questions - each with a distinct problem formulation, computational time, and memory footprint. This lack of unity makes it difficult for a practitioner to compare different algorithms and pick the most suitable one for a specific application. These challenges - even more severe for non-experts - create a gap in which state-of-the-art techniques developed in academic settings fail to be optimally deployed in real-world applications. To bridge this gap, we propose AutoGM, an automated system for graph mining algorithm development. We first define a unified framework UnifiedGM that integrates various message-passing based graph algorithms, ranging from conventional algorithms like PageRank to graph neural networks. Then UnifiedGM defines a search space in which five parameters are required to determine a graph algorithm. Under this search space, AutoGM explicitly optimizes for the optimal parameter set of UnifiedGM using Bayesian Optimization. AutoGM defines a novel budget-aware objective function for the optimization to incorporate a practical issue - finding the best speed-accuracy trade-off under a computation budget - into the graph algorithm generation problem. Experiments on real-world benchmark datasets demonstrate that AutoGM generates novel graph mining algorithms with the best speed/accuracy trade-off compared to existing models with heuristic parameters.",https://ieeexplore.ieee.org/document/9338289/,2020 IEEE International Conference on Data Mining (ICDM),17-20 Nov. 2020,ieeexplore
10.1109/EIT.2018.8500102,Behavioral Cloning for Lateral Motion Control of Autonomous Vehicles Using Deep Learning,IEEE,Conferences,"Current trend of the automotive industry combined with research by the major tech companies has proved that self-driving vehicles are the future. With successful demonstration of neural network based autonomous driving, NVIDIA has introduced a new paradigm for autonomous driving software. The biggest challenge for self-driving cars is autonomous lateral control. An end-to-end model seems very promising in providing a complete software stack for autonomous driving. Although this system is not ready to be provided as a feature in the market today, it is one of the many steps in the right direction to make self-driving cars a reality. The work described in this paper focusses on how an end-to-end model is implemented. The subtleties of training a successful end-to-end model are highlighted with the aim of providing an insight on deep learning and software required for neural network training. Detailed analyses of data acquisition and training systems are provided and installation procedures for all required tools and software discussed. TORCS is used for developing and testing the end-to-end model. Approximately ten hours of driving data was collected from two different tracks. Using four hours of data from a track, we trained a deep neural network to steer a car inside simulation. Even with such a small training set, the end-to-end model developed demonstrated capabilities to maintain lanes and complete laps in different tracks. For a multilane track, like the one used for training, the model demonstrated an autonomy of 96.62%. For single lane unknown tracks, the model steered the vehicle successfully for 89.02% of the time. The results indicate that end-to-end learning and behavioral cloning can be used to drive autonomously in new and unknown scenarios.",https://ieeexplore.ieee.org/document/8500102/,2018 IEEE International Conference on Electro/Information Technology (EIT),3-5 May 2018,ieeexplore
10.1109/NetSoft48620.2020.9165393,Benchmarking and Profiling 5G Verticals' Applications: An Industrial IoT Use Case,IEEE,Conferences,"The Industry 4.0 sector is evolving in a tremendous pace by introducing a set of industrial automation mechanisms tightly coupled with the exploitation of Internet of Things (IoT), 5G and Artificial Intelligence (AI) technologies. By combining such emerging technologies, interconnected sensors, instruments, and other industrial devices are networked together with industrial applications, formulating the Industrial IoT (IIoT) and aiming to improve the efficiency and reliability of the deployed applications and provide Quality of Service (QoS) guarantees. However, in a 5G era, efficient, reliable and highly performant applications' provision has to be combined with exploitation of capabilities offered by 5G networks. Optimal usage of the available resources has to be realised, while guaranteeing strict QoS requirements such as high data rates, ultra-low latency and jitter. The first step towards this direction is based on the accurate profiling of vertical industries' applications in terms of resources usage, capacity limits and reliability characteristics. To achieve so, in this paper we provide an integrated methodology and approach for benchmarking and profiling 5G vertical industries' applications. This approach covers the realisation of benchmarking experiments and the extraction of insights based on the analysis of the collected data. Such insights are considered the cornerstones for the development of AI models that can lead to optimal infrastructure usage along with assurance of high QoS provision. The detailed approach is applied in a real IIoT use case, leading to profiling of a set of 5G network functions.",https://ieeexplore.ieee.org/document/9165393/,2020 6th IEEE Conference on Network Softwarization (NetSoft),29 June-3 July 2020,ieeexplore
10.1109/ICAIIC51459.2021.9415189,Big Data Platform for Intelligence Industrial IoT Sensor Monitoring System Based on Edge Computing and AI,IEEE,Conferences,"The cutting edge of Industry 4.0 has driven everything to be converted to disruptive innovation and digitalized. This digital revolution is imprinted by modern and advanced technology that takes advantage of Big Data and Artificial Intelligence (AI) to nurture from automatic learning systems, smart city, smart energy, smart factory to the edge computing technology, and so on. To harness an appealing, noteworthy, and leading development in smart manufacturing industry, the modern industrial sciences and technologies such as Big Data, Artificial Intelligence, Internet of things, and Edge Computing have to be integrated cooperatively. Accordingly, a suggestion on the integration is presented in this paper. This proposed paper describes the design and implementation of big data platform for intelligence industrial internet of things sensor monitoring system and conveys a prediction of any upcoming errors beforehand. The architecture design is based on edge computing and artificial intelligence. To extend more precisely, industrial internet of things sensor here is about the condition monitoring sensor data - vibration, temperature, related humidity, and barometric pressure inside facility manufacturing factory.",https://ieeexplore.ieee.org/document/9415189/,2021 International Conference on Artificial Intelligence in Information and Communication (ICAIIC),13-16 April 2021,ieeexplore
10.1109/CCEM.2018.00025,Bodhisattva - Rapid Deployment of AI on Containers,IEEE,Conferences,"Cloud-based machine learning is becoming increasingly important in all verticals of the industry as all organizations want to leverage ML and AI to solve real-world problems of emerging markets. But, incorporating these services into business solutions is a goliath task, mainly due to the sheer effort necessary to go from development to deployment. We present a novel idea that enables users to easily specify, create, train and rapidly deploy machine learning models through a scalable Machine-Learning-as-a-Service (MLaaS) offering. The MLaaS is provided as an end-to-end microservice suite in a container-based PaaS environment for web applications on the cloud. Our implementation provides an intuitive web-based GUI for tenants to consume these services in a few quick steps. The utility of our service is demonstrated by training ML models for various use cases and comparing them on factors like time-to-deploy, resource usage and training metrics.",https://ieeexplore.ieee.org/document/8648632/,2018 IEEE International Conference on Cloud Computing in Emerging Markets (CCEM),23-24 Nov. 2018,ieeexplore
10.1109/SBESC49506.2019.9046091,Brazilian Mercosur License Plate Detection: a Deep Learning Approach Relying on Synthetic Imagery,IEEE,Conferences,"Automated license plate recognition (ALPR) technology is a powerful technology enabling more efficient and effective law enforcement, security, payment collection, and research. A common license plate standard was adopted by the member states of the Mercosur trading bloc (Argentina, Brazil, Paraguay and Uruguay) and consequently requires an upgrade to the ALPR software used by law enforcement and industry. Due to the scarcity of real license plate images, training state-of-the-art supervised detectors is unfeasible unless data augmentation techniques and synthetic training data are used. This paper presents an accurate and efficient automated Mercosur license plate detector using a Convolutional Neural Network (CNN) trained exclusively with synthetic imagery. In order to obtain the synthetic training data, Mercosur license plates were faithfully reproduced. Digital image processing techniques were employed to reduce the domain gap and a CNN with basic image manipulation was used to embed the artificial licensed plates in to realistic contexts. The trained model was then validated on real images captured from a parking lot and a publicly available traffic monitoring video stream. The results of experiments suggest detection accuracy of about 95% and an average running time of 40 milliseconds.",https://ieeexplore.ieee.org/document/9046091/,2019 IX Brazilian Symposium on Computing Systems Engineering (SBESC),19-22 Nov. 2019,ieeexplore
10.1109/BigData.2016.7840859,Building a research data science platform from industrial machines,IEEE,Conferences,"Data Science research has a long history in academia which spans from large-scale data management, to data mining and data analysis using technologies from database management systems (DBMS's). While traditional HPC offers tools on leveraging existing technologies with data processing needs, the large volume of data and the speed of data generation pose significant challenges. Using the Hadoop platform and tools built on top of it drew immense interest from academia after it gained success in industry. Georgia Institute of Technology received a donation of 200 compute nodes from Yahoo. Turning these industrial machines into a research Data Science Platform (DSP) poses unique challenges, such as: nontrivial hardware design decisions, configuration tool choices, node integration into existing HPC infrastructure, partitioning resource to meet different application needs, software stack choices, etc. We have 40 nodes up and running, 24 running as a Hadoop and Spark cluster, 12 running as a HBase and OpenTSDB cluster, the others running as service nodes. We successfully tested it against Spark Machine Learning algorithms using a 88GB image dataset, Spark DataFrame and GraphFrame with a Wikipedia dataset, and Hadoop MapReduce wordcount on a 300GB dataset. The OpenTSDB cluster is for real-time time series data ingestion and storage for sensor data. We are working on bringing up more nodes. We share our first-hand experience gained in our journey, which we believe will benefit and inspire other academic institutions.",https://ieeexplore.ieee.org/document/7840859/,2016 IEEE International Conference on Big Data (Big Data),5-8 Dec. 2016,ieeexplore
10.1109/TEMSCON-EUR52034.2021.9488593,Business Forum Panel: Perspectives and real-life application of AI in Croatia 20-May 2021 9:00-10:30 CET,IEEE,Conferences,"Organized by IEEE Croatia Section and Technology and Engineering Management Chapter Artificial Intelligence is one of the hottest topics at the moment. In recent years it has been attracting the equal attention of the academia, industry and policy makers. The topic itself is far from new, especially scientifically and engineering wise, but its true deployment raises many questions: technological, legal/ethical, economic. AI is without a doubt a technology for the future, but it is reasonable to ask what are its true perspective and application in a foreseeable future in Croatia and world wide. These and many other questions will be discussed on this round table with our distinguished guests.",https://ieeexplore.ieee.org/document/9488593/,2021 IEEE Technology & Engineering Management Conference - Europe (TEMSCON-EUR),17-20 May 2021,ieeexplore
10.1109/CBI.2013.34,Business and Information Systems Engineering -- In Quest for Research and Education Agenda in Europe,IEEE,Conferences,"The new complex digital and information services systems and industries are calling for new transdisciplinary approaches on how to achieve technical, social, and cultural knowledge and skills to serve future needs of the industry and society. The leading digital industry is in demand of engineer's with knowledge and capabilities to collaborate and move between highly complex digital business and technical systems domains. The new reality is under constant transformation, highly intangible and nonlinear interconnected system. Transformative digital business innovations, rapidly evolving business models, architectures enabled business model scalability and ultra large-scale of systems are the new characteristics of this software-dominant-logic. This paper aims at covering contemporary challenges of the interdisciplinary in business, service, software and systems engineering by analyzing different research ontologies and curriculum models. The paper analyses recent Computer Science (CS) and Information Systems (IS) curriculum developments and reflects through different ontologies and recent research. Paper uses Service Design and Engineering (SDE) as a didactic and curriculum model for future Information Systems Engineering (ISE) and Business and Information Systems Engineering (BISE). The new curriculums serve the needs of global information start-ups, businesses, governments, and societies. This paper aims at describing the ontological foundations and conflicts, the axiology of the new curriculum model and proposes an integrated multi-ontology as the foundation for BISE new curriculum.",https://ieeexplore.ieee.org/document/6642875/,2013 IEEE 15th Conference on Business Informatics,15-18 July 2013,ieeexplore
10.1109/ICPR48806.2021.9413181,CARRADA Dataset: Camera and Automotive Radar with Range- Angle- Doppler Annotations,IEEE,Conferences,"High quality perception is essential for autonomous driving (AD) systems. To reach the accuracy and robustness thatare required by such systems, several types of sensors must be combined. Currently, mostly cameras and laser scanners (lidar) are deployed to build a representation of the world around the vehicle. While radar sensors have been used fora long time in the automotive industry, they are still under-used for AD despite their appealing characteristics (notably, their ability to measure the relative speed of obstacles and to operate even in adverse weather conditions). To alarge extent, this situation is due to the relative lack of automotive datasets with real radar signals that are both raw and annotated. In this work, we introduce CARRADA, a dataset of synchronized camera and radar recordings with range-angle-Doppler annotations. We also present a semi-automatic annotation approach, which was used to annotate the dataset, and a radar semantic segmentation baseline, which we evaluate on several metrics. Both our code and dataset are available online.",https://ieeexplore.ieee.org/document/9413181/,2020 25th International Conference on Pattern Recognition (ICPR),10-15 Jan. 2021,ieeexplore
10.1109/I-SMAC49090.2020.9243440,CNN and Data Augmentation Based Fruit Classification Model,IEEE,Conferences,"Identification of different fruits is still an important and tedious task in the food processing industry. Workers not only have to identify the fruit but also have to make decisions to determine if the fruit is edible or not. Classification of fruits into edible and non-edible classes can be proved as a very important aspect in such industry. In our proposed system four fruits are classified namely, Banana, Papaya, Mango, and Guava into three stages raw, ripe, and over-ripe using Convolutional Neural Networks. In the model, a dataset of local fruits is used and studied their life cycle in different stages. In this, an accuracy of 97.74 % in 8 epochs with 0.9833 validation precision was achieved. The same model can be implemented using real-time images captured using cameras to identify the edibility of fruit which can prove to be helpful for everyone.",https://ieeexplore.ieee.org/document/9243440/,"2020 Fourth International Conference on I-SMAC (IoT in Social, Mobile, Analytics and Cloud) (I-SMAC)",7-9 Oct. 2020,ieeexplore
10.1109/ISVLSI49217.2020.00-12,CPSoSaware: Cross-Layer Cognitive Optimization Tools &amp; Methods for the Lifecycle Support of Dependable CPSoS,IEEE,Conferences,"Cyber-physical Systems of Systems (CPSoS) are large complex systems where physical elements interact with and are controlled by a large number of distributed and networked computing elements as well as human users. Their increasingly stringent demands on efficient use of resources, high service and product quality levels and, of course low cost and competitiveness on the world market introduce big challenges related to the design operation continuum of dependable connected CPSs. The CPSoSaware project aims at developing the models and software tools to allocate computational power/resources to the CPS end devices and autonomously determining what cyber-physical processes will be handled by the devices' heterogeneous components (CPUs, GPUs, FPGA fabric, software stacks). The project relies on Artificial Intelligence (AI) support to strengthen reliability, fault tolerance and security at system level and also to lead to CPS designs that work in a decentralized way, collaboratively, in an equilibrium, by sharing tasks and data with minimal central intervention. The CPSoSaware system will interact with the human users/operators through extended reality visual and touchable interfaces increasing situational awareness. The CPSoSaware system will be evaluated: i) in the automotive sector, in mixed traffic environments with semi-autonomous connected vehicles and ii) in the manufacturing industry where inspection and repair scenarios are employed using collaborative robots.",https://ieeexplore.ieee.org/document/9155036/,2020 IEEE Computer Society Annual Symposium on VLSI (ISVLSI),6-8 July 2020,ieeexplore
10.1109/AEMCSE51986.2021.00098,CSF: convolution sequential fusion model for session recommender system,IEEE,Conferences,"Capturing users’ specific preferences is a fundamental problem in a large-scale recommender system. Currently, Sequence-based session recommendation has become a hot research direction in the industry. However, the current approach is somewhat inadequate in capturing user’s long-term preferences. We propose a session model based on convolution sequential fusion (CSF) to capture users' specific preferences by fusing users' long-term behaviors and short-term sessions. Compared with existing sequential recommendations, we address two inherent weaknesses of session recommendations: 1) The interaction of long-term interest features is not sufficiently extracted. 2) The combination of long-term interest and short-term interest is too simplistic. The long-term interests of users are varied and complex. simultaneously, users' long-term interests are closely related to their short-term interests, so they should be better integrated. We propose to encode the behavior sequence with two corresponding components: the convolutional network for interactive extraction of users' long-term interests and the long-short term gated fusion module for better combination of long-short preferences Our entire model has been test on multiple real-world data sets, and the results demonstrate that our model is more effective than other recent models on multiple evaluation benchmarks.",https://ieeexplore.ieee.org/document/9513159/,"2021 4th International Conference on Advanced Electronic Materials, Computers and Software Engineering (AEMCSE)",26-28 March 2021,ieeexplore
10.1109/EUVIP.2018.8611701,Classification of Building Information Model (BIM) Structures with Deep Learning,IEEE,Conferences,"In this work we study an application of machine learning to the construction industry and we use classical and modern machine learning methods to categorize images of building designs into three classes: Apartment building, Industrial building or Other. No real images are used, but only images extracted from Building Information Model (BIM) software, as these are used by the construction industry to store building designs. For this task, we compared four different methods: the first is based on classical machine learning, where Histogram of Oriented Gradients (HOG) was used for feature extraction and a Support Vector Machine (SVM) for classification; the other three methods are based on deep learning, covering common pre-trained networks as well as one designed from scratch. To validate the accuracy of the models, a database of 240 images was used. The accuracy achieved is 57% for the HOG + SVM model, and above 89% for the neural networks.",https://ieeexplore.ieee.org/document/8611701/,2018 7th European Workshop on Visual Information Processing (EUVIP),26-28 Nov. 2018,ieeexplore
10.1109/I2MTC.2018.8409769,Classification of eye gestures using machine learning for use in embedded switch controller,IEEE,Conferences,"The classification of signals captured by sampling devices through analysis is a powerful tool with application spanning nearly every industry. Using such classification on real time signals to detect different events or anomalies provides a fast and reliable way of implementing monitoring or control systems. Machine learning classification models include support vector machines (SVM), K-nearest neighbors (KNN), decision trees, and many more. These algorithms separate labelled datasets based on features extracted from the inputs. In the assistive technology field, the use of eye tracking technology provides patients of quadriplegia, ALS, or other neurodegenerative disease with the ability to control speech devices using their eyes. To provide a low-cost alternative to the existing costly devices, an electrooculography (EOG) controller was utilized for obtaining signal data and classifying gestures. A dataset consisting of these gestures was collected over several trials and classified using an SVM, KNN, and decision tree's algorithm with a moving window buffer suitable for an embedded device with peak overall accuracies of 96.8%, 96.9%, and 95.4% respectively. The trained models are converted to C code and uploaded to an ATmega328p AVR microcontroller. Using a decision tree implementation, the intentional blink signal classification is successfully predicted with an accuracy of 97.33% accuracy and filters out unintentional blinks with 100% accuracy on the embedded device.",https://ieeexplore.ieee.org/document/8409769/,2018 IEEE International Instrumentation and Measurement Technology Conference (I2MTC),14-17 May 2018,ieeexplore
10.1109/CCNC49032.2021.9369589,Closed Loop Paging Optimization for Efficient Mobility Management,IEEE,Conferences,"The 4G/5G networks deploy conventional Tracking Area Update and multi-step paging procedures for mobility management. The paging procedure consumes significant amount of licensed spectrum resources. The signaling overhead is going to worsen further with the increasing use of small cells and higher user mobility speed. To address this challenge, the telecommunication industry is embracing closed loop approaches to predict user mobility patterns. Though the mobility pattern prediction is a known problem, most of the existing solutions apply it for enhancing the handover management and use academic dataset. Furthermore, limited research has been done on idle-state users. In this paper, we propose a Closed Loop Paging (CLOP) optimization solution using semi-supervised learning model to reduce paging overhead. We harness the real network dataset to analyze the location trail of users in the network to predict a subset of Base Stations for paging to locate idle-state users. Our empirical results demonstrate that Linear Support Vector Machine (L-SVM) classification method excels when compared to other supervised learning models. The L-SVM Classifier saves nearly 43% of the paging overhead with a marginal increase in connection establishment delay by around 7.3%.",https://ieeexplore.ieee.org/document/9369589/,2021 IEEE 18th Annual Consumer Communications & Networking Conference (CCNC),9-12 Jan. 2021,ieeexplore
10.1109/ACC.2006.1656462,Closed loop identification with Youla parameterization and neural nets,IEEE,Conferences,"System identification has become the fundamental pillar of the industry of the 21 century, since allows the integration of advanced model based control strategies with complex process; for diverse reasons, real process are in closed loop due to difference reasons making virtually impossible obtain a model using open loop techniques, in this work an innovative strategy to identify plants in closed loop using an indirect method based on Youla Kucera parameterization and neural nets is presented, stating 4 identifications subproblems with numerous potential applications on real problems",https://ieeexplore.ieee.org/document/1656462/,2006 American Control Conference,14-16 June 2006,ieeexplore
10.1109/COINS49042.2020.9191386,Closing the loop: Real-time Error Detection and Correction in automotive production using Edge-/Cloud-Architecture and a CNN,IEEE,Conferences,"One challenge faced by the automotive industry is the shift from combustion to electrically powered vehicles. This change strongly impacts on components such as the electric motor and the battery, and hence on production. In this context, the low level of expert knowledge is especially problematic. To meet these new challenges, this paper introduces a data-driven optimization of the production process by integrating a modular edge and cloud computing layer, and advanced data analysis. Defects are classified by a convolutional neural network (CNN) (predictive analytics) and corrected (depending on the defect type) by an automated rework (prescriptive analytics). The architecture of the CNN achieves an accuracy of 99.21% to predict the defect class. The automated rework process is selected through an implemented decision tree. The edge device communicates with a programmable logic controller (PLC) through a cyber physical interface. As an example of its practical application, the method is applied to hairpin welding of the stator of an electric motor with real production data.",https://ieeexplore.ieee.org/document/9191386/,2020 International Conference on Omni-layer Intelligent Systems (COINS),31 Aug.-2 Sept. 2020,ieeexplore
10.1109/MED.2017.7984310,Cloud computing for big data analytics in the Process Control Industry,IEEE,Conferences,"The aim of this article is to present an example of a novel cloud computing infrastructure for big data analytics in the Process Control Industry. Latest innovations in the field of Process Analyzer Techniques (PAT), big data and wireless technologies have created a new environment in which almost all stages of the industrial process can be recorded and utilized, not only for safety, but also for real time optimization. Based on analysis of historical sensor data, machine learning based optimization models can be developed and deployed in real time closed control loops. However, still the local implementation of those systems requires a huge investment in hardware and software, as a direct result of the big data nature of sensors data being recorded continuously. The current technological advancements in cloud computing for big data processing, open new opportunities for the industry, while acting as an enabler for a significant reduction in costs, making the technology available to plants of all sizes. The main contribution of this article stems from the presentation for a fist time ever of a pilot cloud based architecture for the application of a data driven modeling and optimal control configuration for the field of Process Control. As it will be presented, these developments have been carried in close relationship with the process industry and pave a way for a generalized application of the cloud based approaches, towards the future of Industry 4.0.",https://ieeexplore.ieee.org/document/7984310/,2017 25th Mediterranean Conference on Control and Automation (MED),3-6 July 2017,ieeexplore
10.1109/WFCS.2019.8757952,Cloud-enabled Smart Data Collection in Shop Floor Environments for Industry 4.0,IEEE,Conferences,"Industry 4.0 transition is producing a remarkable change in the Smart Factories management. Modern companies can provide new services following products inside the shop floors along the entire production chain. To achieve the goal of servitization that the Industry 4.0 world requires, a modernization of current production chains is needed. This common demand comes mostly from manufacturing sector, where complex work machines collaborate with human workers. The data produced by the machines must be processed quickly, to allow the implementation of reactive services such as predictive maintenance, and remote control, always taking care of the safety of nearby people. This paper proposes a multi-layer architecture to monitor legacy production machines during their operations inside customers plants. The platform provides near real-time delivery of data collected from the machines with a high grade of customization according to customer needs. The performed tests show the scalability of the platform for a productive ecosystem with many machines at work, confirming its feasibility within different production facilities with different needs.",https://ieeexplore.ieee.org/document/8757952/,2019 15th IEEE International Workshop on Factory Communication Systems (WFCS),27-29 May 2019,ieeexplore
10.1109/ICECCME52200.2021.9591113,Cobots for FinTech,IEEE,Conferences,"Embedded devices enabling payments transaction processing in Financial Services industry cannot have any margin for error. These devices need to be tested &amp; validated by replicating production like environment to the extent possible. This means literally handling payments related events like swiping a credit card, tapping a mobile phone or pressing buttons amongst many other things like in real world. Embedded Software development is time consuming as it involves multiple man-machine interactions and dependencies such as managing and handling embedded devices, operating devices (Push buttons, interpret display panels, read receipt printouts etc.) and sharing devices for collaboration within team. During the current pandemic, it was impossible for software teams to travel to office, share devices or even procure necessary devices on time for project related tasks. This caused delay to project delivery and increased Time to market. The paper describes how the team used Capgemini's flexible Robotics as a Service (RaaS) platform that helped during pandemic to automate feasible man-machine interactions using Robotic arms. The paper provides details of the work done by the team that involves internet of things (IoT), Artificial Intelligence (AI) to remotely handle and operate hardware and devices thereby completing embedded software development life cycles faster and well within budget while ensuring superior product quality and importantly ensuring team's health and safety. This is novel in Financial Services space.",https://ieeexplore.ieee.org/document/9591113/,"2021 International Conference on Electrical, Computer, Communications and Mechatronics Engineering (ICECCME)",7-8 Oct. 2021,ieeexplore
10.1109/WCNC45663.2020.9120761,Collaborative Learning Model for Cyberattack Detection Systems in IoT Industry 4.0,IEEE,Conferences,"Although the development of IoT Industry 4.0 has brought breakthrough achievements in many sectors, e.g., manufacturing, healthcare, and agriculture, it also raises many security issues to human beings due to a huge of emerging cybersecurity threats recently. In this paper, we propose a novel collaborative learning-based intrusion detection system which can be efficiently implemented in IoT Industry 4.0. In the system under consideration, we develop smart “filters” which can be deployed at the IoT gateways to promptly detect and prevent cyberattacks. In particular, each filter uses the collected data in its network to train its cyberattack detection model based on the deep learning algorithm. After that, the trained model will be shared with other IoT gateways to improve the accuracy in detecting intrusions in the whole system. In this way, not only the detection accuracy is improved, but our proposed system also can significantly reduce the information disclosure as well as network traffic in exchanging data among the IoT gateways. Through thorough simulations on real datasets, we show that the performance obtained by our proposed method can outperform those of the conventional machine learning methods.",https://ieeexplore.ieee.org/document/9120761/,2020 IEEE Wireless Communications and Networking Conference (WCNC),25-28 May 2020,ieeexplore
10.1109/ETFA.2017.8247730,Communication middleware technologies for industrial distributed control systems: A literature review,IEEE,Conferences,"Industry 4.0 is the German vision for the future of manufacturing, where smart factories use information and communication technologies to digitise their processes to achieve improved quality, lower costs, and increased efficiency. It is likely to bring a massive change to the way control systems function today. Future distributed control systems are expected to have an increased connectivity to the Internet, in order to capitalize on new offers and research findings related to digitalization, such as cloud, big data, and machine learning. A key technology in the realization of distributed control systems is middleware, which is usually described as a reusable software layer between operating system and distributed applications. Various middleware technologies have been proposed to facilitate communication in industrial control systems and hide the heterogeneity amongst the subsystems, such as OPC UA, DDS, and RT-CORBA. These technologies can significantly simplify the system design and integration of devices despite their heterogeneity. However, each of these technologies has its own characteristics that may work better for particular applications. Selection of the best middleware for a specific application is a critical issue for system designers. In this paper, we conduct a survey on available standard middleware technologies, including OPC UA, DDS, and RT-CORBA, and show new trends for different industrial domains.",https://ieeexplore.ieee.org/document/8247730/,2017 22nd IEEE International Conference on Emerging Technologies and Factory Automation (ETFA),12-15 Sept. 2017,ieeexplore
10.1109/ECAI.2018.8679004,Comparative Analysis of E-Learning Platforms on The Market,IEEE,Conferences,"The E-Learning concept came gradually in the public eye, having an ascendant exponential evolution, directly dependent on the technologies and methodologies involved. Its history is not a long one, stretched over two decades, the term being introduced in 1999 by Jay Cross. At the end of the 1990s, the term LMS, Learning Management System, was introduced, and e-Learning applications are widely used, both in education and in the private environment, as a support for continuous personal development. The emergence and extensive use of applications or program packages (eg Microsoft Office), dedicated dynamic content websites (eg Youtube) as well as access to electronic resources (eg Email, chat) led to the construction of communities within the context of online learning, facilitating communication, access and transfer of resources. After 2010, the E-Learning implications in the education system are recognized and heavily exploited. Access to huge treasures and resources, unlimited by geographical boundaries, a diverse range of ways to customize teaching and assessment techniques, an independent competitive market, ongoing development, and quality deliverables tailored to needs or purposes are just a few of the features of the current E-Learning domain. Educational programs attach great importance to both content, appearance and organization, and to the interaction between the teacher and the learner. The learner is provided with a variety of media resources, from slides, books and bibliographic materials grouped in one place to the real-time, to video-demonstrative teaching content in real time complete the learning experience and give it a practical note. This paper aims to present the characteristics that define an E-Learning platform and how they are assimilated by the educational systems, as well as to perform a comparative analysis of the existing E-learning platforms on the market, referring to the statistics on the E-learning industry in the world. The conclusions of the paper will attempt to outline some perspectives in this field.",https://ieeexplore.ieee.org/document/8679004/,"2018 10th International Conference on Electronics, Computers and Artificial Intelligence (ECAI)",28-30 June 2018,ieeexplore
10.1109/iSAI-NLP.2018.8692924,"Comparative Assessment of Indoor Positioning Technologies, Techniques, and Algorithms",IEEE,Conferences,"Indoor positioning systems (IPS) are used to locate the position of objects in indoor environments. Due to its many real-world applications, IPS has garnered interest from both academia and industry. Each IPS is made up of three major components: sensor technology, position-finding technique, and operating algorithm. The goal of this paper is to examine and independently compare different types of components with the aim to understand and make useful suggestions for improvement. This paper also presents the past and current trends in IPS and predict the future trends in approaches to the design and implementation of IPS.",https://ieeexplore.ieee.org/document/8692924/,2018 International Joint Symposium on Artificial Intelligence and Natural Language Processing (iSAI-NLP),15-17 Nov. 2018,ieeexplore
10.1109/FUZZY.1995.410037,"Computational intelligence and multimedia in education: NASA/RMS arm, control in diabetes, intelligent workstation, and management decision aide",IEEE,Conferences,"The use of computational intelligence and multimedia in the formal classroom and in continuing education is a powerful, many faceted tool. We summarize the system development principles important in educating engineers and scientists in the practical application of these tools. Intelligent systems, human interfaces, neural networks, genetic algorithms, evolutionary systems and virtual reality form a toolbox of opportunities. Examples are discussed in the light of turning fuzzy computing concepts into applications. Simulation skills needed for success in industry are emphasized. A prime example of such an educational tool is the system developed at the NASA Goddard Space Flight Center for full scale simulation in actual hardware of the Remote Manipulator System (RMS) arm used on the Shuttle. Other examples include the new Teledoc software expert system at the Diabetes Research Institute (DRI). The computer ""talks"" to callers to elicit information about glucose levels, stress, exercise, diet and various symptoms before adjusting insulin dosages, alerting patients to potential problems add their solutions-including calling the doctor. An intelligent workstation designed to filter information for busy executives, and a decision aide for managers are also discussed.&lt;<ETX>&gt;</ETX>",https://ieeexplore.ieee.org/document/410037/,Proceedings of 1995 IEEE International Conference on Fuzzy Systems.,20-24 March 1995,ieeexplore
10.1109/ICIT.2002.1189341,Computer based robot training in a virtual environment,IEEE,Conferences,"As more market segments are welcoming automation, the robotic field continues to expand. With the accepted breadth of viable industrial robotic applications increasing, the need for flexible robotic training also grows. In the area of simulation and offline programming there have been innovative developments to Computer Aided Robotics (CAR) Systems. New and notable releases have been introduced to the public, especially among the small, affordable, and easy to use systems. These CAR-Systems are mainly aimed at system integrators in general industry business fields to whom the complex, powerful software tools used by the automotive industry (and its suppliers) are oversized. In general, CAR-Systems are used to design robot cells and to create the offline programs necessary to reduce start-up time and to achieve a considerable degree of planning reliability. Another potential yet to be fully considered, is the use of such CAR-Systems as an inexpensive and user-friendly tool for robotics training. This paper will show the educational potential and possibility inherent in simulation and introduce a successful example of this new method of training. Finally, this presentation should be seen as an attempt to outline novel methods for future education in an industrial environment characterized by the increased occurrence and implementation of the virtual factory.",https://ieeexplore.ieee.org/document/1189341/,"2002 IEEE International Conference on Industrial Technology, 2002. IEEE ICIT '02.",11-14 Dec. 2002,ieeexplore
10.1109/AIMSEC.2011.6010807,Constructing dependence ordering for B&amp;B technique in learning Bayesian belief network,IEEE,Conferences,"The data mining technology is more and more widely used. How to construct a Bayesian belief network has been discussed in many different ways. As one of the classical algorithms, the branch and bound technique based on the minimum description length principle has been proposed by Joe Suzuki in 1998. But one of the most important premises of the B&amp;B Technique is that an attributes' dependence ordering has been prepared. To address the problem, a new method is proposed for attributes' dependence ordering. The algorithm first constructs a dependence tree using training dataset, then we use breadth first searching and get the dependence ordering. That results in a raw ordering. Then we order the nodes that are in the same layer of the dependence tree in order to make the result more accurate. This paper uses real datasets from the telecom industry as the test datasets. The result shows that the algorithm can construct the dependence ordering with good performance.",https://ieeexplore.ieee.org/document/6010807/,"2011 2nd International Conference on Artificial Intelligence, Management Science and Electronic Commerce (AIMSEC)",8-10 Aug. 2011,ieeexplore
10.1109/ASE.2019.00042,Continuous Incident Triage for Large-Scale Online Service Systems,IEEE,Conferences,"In recent years, online service systems have become increasingly popular. Incidents of these systems could cause significant economic loss and customer dissatisfaction. Incident triage, which is the process of assigning a new incident to the responsible team, is vitally important for quick recovery of the affected service. Our industry experience shows that in practice, incident triage is not conducted only once in the beginning, but is a continuous process, in which engineers from different teams have to discuss intensively among themselves about an incident, and continuously refine the incident-triage result until the correct assignment is reached. In particular, our empirical study on 8 real online service systems shows that the percentage of incidents that were reassigned ranges from 5.43% to 68.26% and the number of discussion items before achieving the correct assignment is up to 11.32 on average. To improve the existing incident triage process, in this paper, we propose DeepCT, a Deep learning based approach to automated Continuous incident Triage. DeepCT incorporates a novel GRU-based (Gated Recurrent Unit) model with an attention-based mask strategy and a revised loss function, which can incrementally learn knowledge from discussions and update incident-triage results. Using DeepCT, the correct incident assignment can be achieved with fewer discussions. We conducted an extensive evaluation of DeepCT on 14 large-scale online service systems in Microsoft. The results show that DeepCT is able to achieve more accurate and efficient incident triage, e.g., the average accuracy identifying the responsible team precisely is 0.641~0.729 with the number of discussion items increasing from 1 to 5. Also, DeepCT statistically significantly outperforms the state-of-the-art bug triage approach.",https://ieeexplore.ieee.org/document/8952483/,2019 34th IEEE/ACM International Conference on Automated Software Engineering (ASE),11-15 Nov. 2019,ieeexplore
10.1109/IGSC51522.2020.9291232,Conversion of an Unsupervised Anomaly Detection System to Spiking Neural Network for Car Hacking Identification,IEEE,Conferences,"Across industry, there is an increasing availability of streaming, time-varying data, where it is important to detect anomalous behavior. These data are found in an enormous number of sensor-based applications, in cybersecurity (where anomalous behavior could indicate an attack), and in finance. Spiking Neural Networks (SNNs) have come under the spotlight for machine learning applications due to the extreme energy efficiency of their implementation on neuromorphic processors like the Intel Loihi research chip. In this paper we explore the applicability of spiking neural networks for in vehicle cyberattack detection. We show exemplary results by converting an autoencoder model to spiking form. We present a learning model comparison that shows the proposed SNN autoencoder outperforms a One Class Support Vector Machine and an Isolation Forest. Furthermore, only a slight reduction in accuracy is observed when compared to a traditional autoencoder.",https://ieeexplore.ieee.org/document/9291232/,2020 11th International Green and Sustainable Computing Workshops (IGSC),19-22 Oct. 2020,ieeexplore
10.1109/EAEEIE.2013.6576535,Cooperation between industry and university based on the evaluation of the industrial research results in the academic environment,IEEE,Conferences,"Based on the European Structural Funds it was developed the Intelligent Mobile Box, Intelligent Panel Controller with intelligent adaptive controllers within the industrial research and experimental development in the company Kybernetes, s.r.o. Within the frame of the academic-industry cooperation, the intelligent adaptive controller was tested at the Department of Cybernetics and Artificial Intelligence, Technical university of Kosice, Slovakia. The tests of the mobile intelligent adaptive controller were performed on two levels of university study, on the Bachelor level on the exercises from the subject “Control of Technological Processes” and on the Engineering level the exercises from the subject “Intelligent Control Networks” and on one Diploma project. Goals of students of the Control of the technological processes course had two goals, firstly to connect the intelligent adaptive controller to pre-defined controlled system (real plant, real model or simulated model) and next to validate the control results. Students of the Diploma project on the Engineering level had more advanced goals. Tasks defined for engineering students were to connect the intelligent adaptive controller to non-defined controlled system, setup the adaptivity process of the controller regarding the learning error, parameterize the control system, observe and validate the control results. Both sides concluded this cooperation as very valuable. Main contributions for students were (U1) the challenge to apply studied theoretical knowledge on the real industrial controllers, (U2) experience with new research results and technologies deployed in industry and (U3) the implementation of the control and adaptive algorithms from abstract mathematical area to real PLC controller. On side of industry research company the main contributions were (C1) testing of designed algorithms and (C2) user feedback from students to make the application HMI interface more understandable a native.",https://ieeexplore.ieee.org/document/6576535/,2013 24th EAEEIE Annual Conference (EAEEIE 2013),30-31 May 2013,ieeexplore
10.1109/IEEM45057.2020.9309978,Creating Transparency in the Finished Vehicles Transportation Process Through the Implementation of a Real-Time Decision Support System,IEEE,Conferences,"The complexity of global distribution networks in the automotive industry and likewise the number of disruptions significantly increased throughout the last years. In order to monitor relevant processes and to optimize decision-making in case of disruptions, a concept for a decision support system (DSS) was introduced. For this purpose, the distribution process weaknesses of the German premium automotive company BMW were identified. The method used was a Failure Mode and Effect Analysis with operational managers and relevant process partners interviews. Based on the findings, performance indicators, thresholds, early warnings and options for action were specified. A big data platform supports the processing of the growing number of relevant data in real-time. In the long-term decision-making can be automated using machine learning algorithms. This paper proves that negative impacts of disruptions can be minimized, and the robustness of the process improved by anticipating and identifying deviations beforehand and in real-time. Hence, companies save money while strengthening customer satisfaction. The DSS can be seen as a necessary precursor of a digital twin.",https://ieeexplore.ieee.org/document/9309978/,2020 IEEE International Conference on Industrial Engineering and Engineering Management (IEEM),14-17 Dec. 2020,ieeexplore
10.1049/cp.2018.1731,Current trend in planning and scheduling of construction project using artificial in telligence,IET,Conferences,"During digitalization of Architecture, Engineering and Construction (AEC) industry, one of the influential technologies, Computer-Aided Design can present the state of these digital technologies that assist practitioners to achieve better work. Similar with CAD, digital project planning and schedule control approaches are still mainly relayed on expert experiences which leads to high expert cost in construction project. Researchers and construction-related project participants develops technologies related to Artificial Intelligence (AI) field to decrease the dependence level of expert in construction planning and schedule control. This paper focuses on 1) teasing out the historical trend in planning and scheduling of construction project of AI technologies; 2) using published construction projects and interview data collected by authors, introducing the current trend of implementation of AI technologies in construction planning and schedule control area. Then, this paper identified gaps of AI technologies adoption between academic perspective and real world in both construction scheduling and planning areas. This paper collected totally 1120 journal papers, by keywords as: Artificial Intelligence, Construction. Then the papers are selected following criterion that after 2000 and only related to relevant research fields. After paper selection, totally 383 papers are analysed to identify the historical trend.",https://ieeexplore.ieee.org/document/8826800/,"IET Doctoral Forum on Biomedical Engineering, Healthcare, Robotics and Artificial Intelligence 2018 (BRAIN 2018)",4-4 Nov. 2018,ieeexplore
10.1109/AINS47559.2019.8968698,Cyber Security Risk Assessment on Industry 4.0 using ICS testbed with AI and Cloud,IEEE,Conferences,"Industry 4.0 is a new concept, thus risk assessment is necessary. Several risk assessment methods for Industrial Control System (ICS) and Industry 4.0 have been proposed, however, it is difficult to identify impacts on the physical world caused by cyber attacks against ICS since many of these are based on tabletop analysis or software simulations. Therefore, we focus on the risk assessment using actual machines (ICS testbed) which can help to solve the above problems. In Industry 4.0, autonomous judgment and execution are required for the cyber-physical system, it is based on information exchange using Artificial Intelligence (AI) and cloud technologies. In this research, we evaluate cyber risks through attacks against ICS with AI and cloud using ICS testbed. The proposed method can clarify cyber risks and impacts on the real world, and corresponding countermeasures.",https://ieeexplore.ieee.org/document/8968698/,"2019 IEEE Conference on Application, Information and Network Security (AINS)",19-21 Nov. 2019,ieeexplore
10.1109/COINS51742.2021.9524088,Cyber Security Risks of Technical Components in Industry 4.0,IEEE,Conferences,"Industry 4.0 is a new concept of automation data exchange in manufacturing. Industry 4.0 consists of various technical components such as Artificial Intelligence (AI), IoT, OPC UA, and cloud. There could be cyber risks when these technical components are used in Industrial Control System (ICS) without security consideration. Therefore cyber risk assessment, reconsideration of implementation, operation are necessary to adopt Industry 4.0. Problems of previous research for ICS risk assessments are some of them are theoretical and conceptual methods such as simulations. Moreover, they do not focus on Industry 4.0 components.The objective of this research is to prove the concrete cyber risks and introduces corresponding secure implementations for Industry 4.0 components such as Artificial Intelligence (AI), the Internet of Things (IoT), OPC Unified Architecture (OPC UA). The proposed method has benefits for clarifying impacts on the real world by cyber-attacks through penetration tests against ICS testbed with actual machines.",https://ieeexplore.ieee.org/document/9524088/,2021 IEEE International Conference on Omni-Layer Intelligent Systems (COINS),23-25 Aug. 2021,ieeexplore
10.1109/ITHET.2012.6246060,Cybernetic Education Centre: Monitoring and control of learner's e-learning study in the field of Cybernetics and Automation by Coloured Petri Nets model,IEEE,Conferences,"This paper presents the Cybernetic Education Centre (CEDUC) as a hybrid e-learning and training centre for higher education of Cybernetics and Automation fields. If we consider Cybernetics we consider basically (1) controlled systems and (2) control systems. In case of controlled systems learner is focused on the process of analyze, identification, design of the mathematical model and simulation of the controlled system. Therefore this paper deals with controlled models in the laboratories of our department (a) real laboratory models, (b) simulation models and (c) virtual models which creates one integrated hybrid architecture what represents one of new ideas in the paper (Fig. 1). Learner of control systems is focused mainly on design of control parameters, design of control algorithms, design of hardware, software and communication architectures of control systems. Overall control system is represented by Distributed Control System (DCS) which serves for learners to verify designed control systems. The verification of the control systems is very important from safety point of view to prepare learners for real production conditions. Second new idea of the paper is implementation of the Coloured Petri Nets as automata to control access to the resources (not only typical study resources but also access to the components of hybrid DCS architecture) as well as to monitor the learner activities during the study. CEDUC is supported by intensive industry-university partnership. Conclusion of the paper summarizes the results of the study process of learners in DCS environment.",https://ieeexplore.ieee.org/document/6246060/,2012 International Conference on Information Technology Based Higher Education and Training (ITHET),21-23 June 2012,ieeexplore
10.1109/ECCE44975.2020.9235814,Data-Driven Nonparametric Li-Ion Battery Ageing Model Aiming At Learning From Real Operation Data: Holistic Validation With Ev Driving Profiles,IEEE,Conferences,"Conventional Li-ion battery ageing models require a significant amount of time and experimental resources to provide accurate predictions under realistic operating conditions. Furthermore, there is still an uncertainty on the validity of purely laboratory data-based ageing models for the accurate ageing prediction of battery systems deployed in field.At the same time, there is significant interest from industry in the introduction of new data collection telemetry technology. This implies the forthcoming availability of a significant amount of in-field battery operation data. In this context, the development of ageing models able to learn from in-field battery operation data is an interesting solution to mitigate the need for exhaustive laboratory testing, reduce the development cost of ageing models and at the same time ensure the validity of the model for prediction under real operating conditions.In this paper, a holistic data-driven ageing model developed under the Gaussian Process framework is validated with experimental battery ageing data. Both calendar and cycle ageing are considered, to predict the capacity loss within real EV driving scenarios. The model can learn from the driving data progressively observed, improving continuously its performances and providing more accurate and confident predictions.",https://ieeexplore.ieee.org/document/9235814/,2020 IEEE Energy Conversion Congress and Exposition (ECCE),11-15 Oct. 2020,ieeexplore
10.1109/IJCNN52387.2021.9534128,Data-driven Full-waveform Inversion Surrogate using Conditional Generative Adversarial Networks,IEEE,Conferences,"In the Oil and Gas industry, estimating a subsurface velocity field is an essential step in seismic processing, reservoir characterization, and hydrocarbon volume calculation. Full-waveform inversion (FWI) velocity modeling is an iterative advanced technique that provides an accurate and detailed velocity field model, although at a very high computational cost due to the physics-based numerical simulations required at each FWI iteration. In this study, we propose a method of generating velocity field models, as detailed as those obtained through FWI, using a conditional generative adversarial network (cGAN) with multiple inputs. The primary motivation of this approach is to circumvent the extremely high cost of full-waveform inversion velocity modeling. Real-world data were used to train and test the proposed network architecture, and three evaluation metrics (percent error, structural similarity index measure, and visual analysis) were adopted as quality criteria. Based on these metrics, the results evaluated upon the test set suggest that the GAN was able to accurately match real FWI generated outputs, enabling it to extract from input data the main geological structures and lateral velocity variations. Experimental results indicate that the proposed method, when deployed, has the potential to increase the speed of geophysical reservoir characterization processes, saving on time and computational resources.",https://ieeexplore.ieee.org/document/9534128/,2021 International Joint Conference on Neural Networks (IJCNN),18-22 July 2021,ieeexplore
10.1109/AERO.2018.8396547,Data-driven quality prognostics for automated riveting processes,IEEE,Conferences,"Technologies based in robotics and automatics are reshaping the aerospace industry. Aircraft manufacturers and top-tier suppliers now rely on robotics to perform most of its operational tasks. Over the years, a succession of implemented mobile robots has been developed with the mission of automating important industrial processes such as welding, material handling or assembly procedures. However, despite the progress achieved, a major limitation is that the process still requires human supervision and an extensive quality control process. An approach to address this limitation is to integrate machine learning methods within the quality control process. The idea is to develop algorithms that can direct manufacturing experts towards critical areas requiring human supervision and quality control. In this paper we present an application of machine learning to a concrete industrial problem involving the quality control of a riveting machine. The proposal consists of an intelligent predictive model that can be integrated within the existing real time sensing and pre-processing sub-systems at the equipment level. The framework makes use of several data-driven techniques for pre-processing and feature engineering, combined with the most accurate algorithms, validated through k-folds cross validation technique which also estimates prediction errors. The model is able to classify the manufacturing process of the machine as nominal or anomalous according to a real-world data set of design requirements and operational data. Several machine learning algorithms are compared such as linear regression, nearest neighbor, support vector machines, decision trees, random forests and extreme gradient boost. Results obtained from the case study suggest that the proposed model produces accurate predictions which meet industrial standards.",https://ieeexplore.ieee.org/document/8396547/,2018 IEEE Aerospace Conference,3-10 March 2018,ieeexplore
10.1109/IMCEC51613.2021.9482089,Data-driven scheduling for smart shop floor via reinforcement learning with model-based clustering algorithm,IEEE,Conferences,"Various information technologies provide the manufacturing system massive data, which can support the decision optimization of product lifecycle management. However, the lack of effective use for advanced technologies prevents manufacturing industry from being automated and intelligent. Therefore, this paper proposes the smart shop floor and implementation mechanism. Meanwhile, based on the clustering and reinforcement learning, the brain agent and scheduling agent are designed to determine the approriate rule according to the shop floor state information, thus realizing the data-driven real-time scheduling. Experimental results show that the smart shop floor can effectively deal with disturbance events and has better performance compared with composite dispatching rules.",https://ieeexplore.ieee.org/document/9482089/,"2021 IEEE 4th Advanced Information Management, Communicates, Electronic and Automation Control Conference (IMCEC)",18-20 June 2021,ieeexplore
10.1109/IJCNN.2019.8852303,Deep Capsule Network based Automatic Batch Code Identification Pipeline for a Real-life Industrial Application,IEEE,Conferences,"Automatic recognition of text, such as a batch code printed on a box placed on a moving conveyor belt, is still a challenging problem. This paper proposes an end-to-end character recognition technique while addressing the major challenges encountered in a real environment, such as motion blur in the acquired images, slanted or oriented characters, creased batch codes due to wear and tear of boxes, variations in label formats, and variations in printing styles. The major contribution of this work lies in development of three sequential modules: text localization using Connectionist Text Proposal Network(CTPN), character detection and character recognition using a modified version of the capsule network (CapsNet). In contrast to CapsNet, where only a standard single convolution is used, the proposed method uses a series of feature blocks, making it a deep CapsNet which is later proven to generate more comprehensive and better separable feature vectors over its counterpart. The feature generation module is further enhanced by setting a smaller kernel size than CapsNet. The proposed system is validated on a real-world box / packet dataset generated in a retail manufacturing industry. The proposed recognition network architecture is also validated on a standard public dataset (ICDAR 2013). The comparative results are presented with statistical analysis in the experimental results section.",https://ieeexplore.ieee.org/document/8852303/,2019 International Joint Conference on Neural Networks (IJCNN),14-19 July 2019,ieeexplore
10.1109/IJCNN.2019.8851923,Deep Domain Adaptation for Vulnerable Code Function Identification,IEEE,Conferences,"Due to the ubiquity of computer software, software vulnerability detection (SVD) has become crucial in the software industry and in the field of computer security. Two significant issues in SVD arise when using machine learning, namely: i) how to learn automatic features that can help improve the predictive performance of vulnerability detection and ii) how to overcome the scarcity of labeled vulnerabilities in projects that require the laborious labeling of code by software security experts. In this paper, we address these two crucial concerns by proposing a novel architecture which leverages deep domain adaptation with automatic feature learning for software vulnerability identification. Based on this architecture, we keep the principles and reapply the state-of-the-art deep domain adaptation methods to indicate that deep domain adaptation for SVD is plausible and promising. Moreover, we further propose a novel method named Semi-supervised Code Domain Adaptation Network (SCDAN) that can efficiently utilize and exploit information carried in unlabeled target data by considering them as the unlabeled portion in a semi-supervised learning context. The proposed SCDAN method enforces the clustering assumption, which is a key principle in semi-supervised learning. The experimental results using six real-world software project datasets show that our SCDAN method and the baselines using our architecture have better predictive performance by a wide margin compared with the Deep Code Network (VulDeePecker) method without domain adaptation. Also, the proposed SCDAN significantly outperforms the DIRT-T which to the best of our knowledge is currently the-state-of-the-art method in deep domain adaptation and other baselines.",https://ieeexplore.ieee.org/document/8851923/,2019 International Joint Conference on Neural Networks (IJCNN),14-19 July 2019,ieeexplore
10.1109/ICIAfS52090.2021.9605823,Deep Learning &amp; Computer Vision for IoT based Intelligent Driver Assistant System,IEEE,Conferences,"With the exponential increment of vehicles, roadside accidents also have been increasing rapidly where approximately 80% of these accidents were caused by human error. Therefore, the automobile industry and the government authorities are more focused on accident prevention by introducing improved road safety systems for the general public. Driver assistance system (DAS) is an intelligent road safety development where it senses the surrounding of a moving vehicle which will assist the driver to avoid the dangers and also warn the drivers of immediate dangers. With the current technological advancements, the automobile industry is equipped with the internet of things (IoT) based data transfer mechanisms, wherewith the concept of ‘connected car’ the passengers and the other interconnected vehicles with the internet can share data with back end applications. The data is consisting of the current location, the distance travelled by the vehicle, whether the vehicle requires urgent service. This study is mainly focused on the development of an intelligent driver assistance system based on computer vision and deep learning, which can prevent accidents with early detection of drowsiness, harmful objects and also by alerting the driver with the signboards and the road lines. The system is capable of passing the emergency messages to the driver as well as the other interconnected vehicles through the website by communicating the real-time road map generated within the system. The proposed system has been implemented and tested with multiple detection scenarios where machine learning has been employed to improve the accuracy of the results.",https://ieeexplore.ieee.org/document/9605823/,2021 10th International Conference on Information and Automation for Sustainability (ICIAfS),11-13 Aug. 2021,ieeexplore
10.1109/CASE48305.2020.9216881,Deep Learning for Early Damage Detection of Tailing Pipes Joints with a Robotic Device,IEEE,Conferences,"In the mining industry, it is usual to employ several kilometers of pipes to carry tailing from the plant to a dam. Only in the Salobo Mine, a copper operation in the Amazon forest from Vale S.A., there are more than three and a half kilometers of tailing pipes. Since the material passing through the tailing pipe causes an abrasion effect that could lead to failures, regular inspections are needed. However, given the risky environment to perform manual inspections, a teleoperated or autonomous robot is a crucial tool to keep track of the pipe health. In this work, we propose a deep-learning methodology to process the data stream of images from the robot, aiming to detect early failures directly on the onboard computer of the device in real-time. Multiple architectures of deep-learning image classification were evaluated to detect the anomalies. We validated the early damage detection accuracy and pinpointed the approximate location of the anomalies using the Class Activation Mapping of the networks. Then, we tested the runtime for the network architectures that obtained the best results on different hardware to analyze the need for a GPU onboard in the robot. Moreover, we also trained a Single Shot object Detector to find the boundaries of the pipe joints, which means that the anomaly classification is performed only when a joint is detected. Our results show that it is possible to build an automatic anomaly detection system in the software of the robot.",https://ieeexplore.ieee.org/document/9216881/,2020 IEEE 16th International Conference on Automation Science and Engineering (CASE),20-21 Aug. 2020,ieeexplore
10.1109/SCC.2017.51,Deep and Shallow Model for Insurance Churn Prediction Service,IEEE,Conferences,"Churn prediction is very important to the insurance industry. Therefore, there is a big value to investigate how to improve its performance. More importantly, a good model can be used by a common service provider and benefit many companies. State-of-the-art methods either use 1) shallow models such as logistic regression, with sophisticated feature engineering, or 2) deep models that learn features and classification models simultaneously. In terms of performance, shallow models can memorize better while deep models can generalize better but may under-generalize with insufficient data. Therefore, we propose a combined Deep &amp;, Shallow model (DSM) to take the strengths of both memorization and generalization in one model by jointly training shallow models and deep models. The experiment results show that for insurance churn prediction, joint training can significantly improve the performance and the DSM earns better performance than both shallow-only and deep-only models. In our real-life dataset, the DSM performs better than CNN, LSTM, Stochastic Gradient Descent, Linear Discriminant Analysis, Quadratic Discriminant Analysis, Gaussian Naive Bayes, AdaBoost, Random Forest, and Gradient Tree Boosting. In addition, the DSM can also be applied to other prediction services.",https://ieeexplore.ieee.org/document/8035004/,2017 IEEE International Conference on Services Computing (SCC),25-30 June 2017,ieeexplore
10.1109/ICCE.2018.8326053,Deep learning networks in CE,IEEE,Conferences,"I could probably offer a half-way answer to “what is the next challenge for CE Imaging?”. Reality is that I don't know for sure (there are many), but I know how to solve it: neural networks. The real question becomes how to enable low cost NN implementations and deployments in CE devices without the fear of losing years of work invested in training and optimizing the networks designed to solve specific problems (sound enhancement, better imaging, understanding of the surroundings, easier cooking, better coffee, etc.). This talk will detail the challenges and industry proven solutions for computer vision and computational imaging using a hybrid traditional imaging and deep learning approach. IP protection against intellectual theft once the solutions are deployed is also briefly discussed.",https://ieeexplore.ieee.org/document/8326053/,2018 IEEE International Conference on Consumer Electronics (ICCE),12-14 Jan. 2018,ieeexplore
10.1109/CBMS.2019.00040,Deep-Learning and HPC to Boost Biomedical Applications for Health (DeepHealth),IEEE,Conferences,"This document introduces the DeepHealth project: ""Deep-Learning and HPC to Boost Biomedical Applications for Health"". This project is funded by the European Commission under the H2020 framework program and aims to reduce the gap between the availability of mature enough AI-solutions and their deployment in real scenarios. Several existing software platforms provided by industrial partners will integrate state-of-the-art machine-learning algorithms and will be used for giving support to doctors in diagnosis, increasing their capabilities and efficiency. The DeepHealth consortium is composed by 21 partners from 9 European countries including hospitals, universities, large industry and SMEs.",https://ieeexplore.ieee.org/document/8787438/,2019 IEEE 32nd International Symposium on Computer-Based Medical Systems (CBMS),5-7 June 2019,ieeexplore
10.1109/SANER.2019.8668044,DeepCT: Tomographic Combinatorial Testing for Deep Learning Systems,IEEE,Conferences,"Deep learning (DL) has achieved remarkable progress over the past decade and has been widely applied to many industry domains. However, the robustness of DL systems recently becomes great concerns, where minor perturbation on the input might cause the DL malfunction. These robustness issues could potentially result in severe consequences when a DL system is deployed to safety-critical applications and hinder the real-world deployment of DL systems. Testing techniques enable the robustness evaluation and vulnerable issue detection of a DL system at an early stage. The main challenge of testing a DL system attributes to the high dimensionality of its inputs and large internal latent feature space, which makes testing each state almost impossible. For traditional software, combinatorial testing (CT) is an effective testing technique to balance the testing exploration effort and defect detection capabilities. In this paper, we perform an exploratory study of CT on DL systems. We propose a set of combinatorial testing criteria specialized for DL systems, as well as a CT coverage guided test generation technique. Our evaluation demonstrates that CT provides a promising avenue for testing DL systems.",https://ieeexplore.ieee.org/document/8668044/,"2019 IEEE 26th International Conference on Software Analysis, Evolution and Reengineering (SANER)",24-27 Feb. 2019,ieeexplore
10.1109/INFOCOM42981.2021.9488784,DeepLoRa: Learning Accurate Path Loss Model for Long Distance Links in LPWAN,IEEE,Conferences,"LoRa (Long Range) is an emerging wireless technology that enables long-distance communication and keeps low power consumption. Therefore, LoRa plays a more and more important role in Low-Power Wide-Area Networks (LPWANs), which easily extend many large-scale Internet of Things (IoT) applications in diverse scenarios (e.g., industry, agriculture, city). In lots of environments where various types of land-covers usually exist, it is challenging to precisely predict a LoRa link's path loss. As a result, how to deploy LoRa gateways to ensure reliable coverage and develop precise fingerprint-based localization becomes a difficult issue in practice. In this paper, we propose DeepLoRa, a deep learning-based approach to accurately estimate the path loss of long-distance links in complex environments. Specifically, DeepLoRa relies on remote sensing to automatically recognize land-cover types along a LoRa link. Then, DeepLoRa utilizes Bi-LSTM (Bidirectional Long Short Term Memory) to develop a land-cover aware path loss model. We implement DeepLoRa and use the data gathered from a real LoRaWAN deployment on campus to evaluate its performance extensively in terms of estimation accuracy and model transferability. The results show that DeepLoRa reduces the estimation error to less than 4 dB, which is 2× smaller than state-of-the-art models.",https://ieeexplore.ieee.org/document/9488784/,IEEE INFOCOM 2021 - IEEE Conference on Computer Communications,10-13 May 2021,ieeexplore
10.1109/DASC-PICom-DataCom-CyberSciTec.2017.200,DeepSim: Cluster Level Behavioral Simulation Model for Deep Learning,IEEE,Conferences,"We are witnessing an explosion of AI based use cases driving the computer industry, and especially datacenter and server architectures. As Intel faces fierce competition in this emerging technology space, it is important that architecture definitions and directions are driven with data from proper tools and methodologies, and insights are drawn from end-to-end holistic analysis at the datacenter levels. In this paper, we introduce DeepSim, a cluster-level behavioral simulation model for deep learning. DeepSim, which is based on the Intel CoFluent simulation framework, uses timed behavioral models to simulate complex interworking between compute nodes, networking, and storage at the datacenter level, providing a realistic performance model of a real-world image recognition applications based on the popular Deep Learning Framework Caffe. The end-to-end simulation data from DeepSim provides insight which can be used for architecture analysis driving future datacenter architecture directions. DeepSim enables scalable system design, deployment, and capacity planning through accurate performance insights. Results from preliminary scaling studies (e.g. node scaling and network scaling) and what-if analyses (e.g., Xeon with HBM and Xeon Phi with dual OPA) are presented in this paper. The simulation results are correlated well with empirical measurements, achieving an accuracy of 95%.",https://ieeexplore.ieee.org/document/8328544/,"2017 IEEE 15th Intl Conf on Dependable, Autonomic and Secure Computing, 15th Intl Conf on Pervasive Intelligence and Computing, 3rd Intl Conf on Big Data Intelligence and Computing and Cyber Science and Technology Congress(DASC/PiCom/DataCom/CyberSciTech)",6-10 Nov. 2017,ieeexplore
10.1109/INFCOMW.2017.8116515,Demo abstract: Telco localization techniques,IEEE,Conferences,"When a mobile device connects to a telecom network (e.g., GSM 2G, CDMA 3G and LTE 4G), the network generates measurement report (MR) data containing signal conditions to support communication services. Using MR data to predict the accurate position of a mobile device is an important problem in Telco industry, called Telco localization problem. Although the literatures have proposed various MR-based localization algorithms, it is unclear how such algorithms perform in terms of localization precision. We develop a tool named STLT, which supports comprehensive performance study of a broad category of the state-of-art localization algorithms. STLT provides flexible training/testing data division, deep parameter turning as well as three visualization modes to display localization results. Through the demonstration of STLT on three real MR datasets provided by one of the largest Telco operators in China, we have three interesting findings that could inspire and enhance future research in Telco location.",https://ieeexplore.ieee.org/document/8116515/,2017 IEEE Conference on Computer Communications Workshops (INFOCOM WKSHPS),1-4 May 2017,ieeexplore
10.1109/ICMLA.2019.00089,Denoising Internet Delay Measurements using Weak Supervision,IEEE,Conferences,"To understand the delay characteristics of the Internet, a myriad of measurement tools and techniques are proposed by the researchers in academia and industry. Datasets from such measurement tools are curated to facilitate analyses at a later time. Despite the benefits of these tools and datasets, the systematic interpretation of measurements in the face of measurement noise. Unfortunately, state-of-the-art denoising techniques are labor-intensive and ineffective. To tackle this problem, we develop NoMoNoise, an open-source framework for denoising latency measurements by leveraging the recent advancements in weak-supervised learning. NoMoNoise can generate measurement noise labels that could be integrated into the inference and control logic to remove and/or repair noisy measurements in an automated and rapid fashion. We evaluate the efficacy of NoMoNoise in a lab-based setting and a real-world setting by applying it on CAIDA's Ark dataset and show that NoMoNoise can remove noisy measurements effectively with high accuracy.",https://ieeexplore.ieee.org/document/8999330/,2019 18th IEEE International Conference On Machine Learning And Applications (ICMLA),16-19 Dec. 2019,ieeexplore
10.1109/HPEC.2019.8916576,Deploying AI Frameworks on Secure HPC Systems with Containers.,IEEE,Conferences,"The increasing interest in the usage of Artificial Intelligence (AI) techniques from the research community and industry to tackle “real world” problems, requires High Performance Computing (HPC) resources to efficiently compute and scale complex algorithms across thousands of nodes. Unfortunately, typical data scientists are not familiar with the unique requirements and characteristics of HPC environments. They usually develop their applications with high level scripting languages or frameworks such as TensorFlow and the installation processes often require connection to external systems to download open source software during the build. HPC environments, on the other hand, are often based on closed source applications that incorporate parallel and distributed computing API's such as MPI and OpenMP, while users have restricted administrator privileges, and face security restrictions such as not allowing access to external systems. In this paper we discuss the issues associated with the deployment of AI frameworks in a secure HPC environment and how we successfully deploy AI frameworks on SuperMUC-NG with Charliecloud.",https://ieeexplore.ieee.org/document/8916576/,2019 IEEE High Performance Extreme Computing Conference (HPEC),24-26 Sept. 2019,ieeexplore
10.1109/DSN.2019.00043,Deploying Intrusion-Tolerant SCADA for the Power Grid,IEEE,Conferences,"While there has been considerable research on making power grid Supervisory Control and Data Acquisition (SCADA) systems resilient to attacks, the problem of transitioning these technologies into deployed SCADA systems remains largely unaddressed. We describe our experience and lessons learned in deploying an intrusion-tolerant SCADA system in two realistic environments: a red team experiment in 2017 and a power plant test deployment in 2018. These experiences resulted in technical lessons related to developing an intrusion-tolerant system with a real deployable application, preparing a system for deployment in a hostile environment, and supporting protocol assumptions in that hostile environment. We also discuss some meta-lessons regarding the cultural aspects of transitioning academic research into practice in the power industry.",https://ieeexplore.ieee.org/document/8809554/,2019 49th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN),24-27 June 2019,ieeexplore
10.1109/ICBAIE49996.2020.00085,Design and Implementation of Intelligent Tour Guide System in Large Scenic Area Based on Fog Computing,IEEE,Conferences,"In recent years, the tourism industry has developed rapidly, and there are more and more large-scale tourist parks. There is an urgent need for intelligent guide systems that take tourists as the center and integrate multiple functions into one. Based on the basic requirements of smart tourism, this paper proposes an intelligent guide system based on fog computing. The overall architecture and system functions of the system are designed. The tasks undertaken by each level and the implementation methods of each function are analyzed in detail. This intelligent tour guide system can provide high-quality services such as location-based and real-scenery commentary, intelligent tour guides, program reminders, and consulting exchanges to bring tourists a more satisfactory travel experience.",https://ieeexplore.ieee.org/document/9196285/,"2020 International Conference on Big Data, Artificial Intelligence and Internet of Things Engineering (ICBAIE)",12-14 June 2020,ieeexplore
10.1109/AIID51893.2021.9456521,Design and Implementation of Online Monitoring System for Soil Salinity and Alkalinity in Yangtze River Delta Tideland,IEEE,Conferences,"Soil salinity and alkalinity is an important index concerned by planting industry. In order to meet the demand of long-term observation of soil salinity and alkalinity in precision agriculture and eco-environmental protection, and to solve the current pain points of long sampling period and high cost of soil salinity measurement, this paper designs and implements an online monitoring system for soil salinity alkalinity in tideland in the Yangtze River Delta for crop planting and soil remediation. This system uses solar power supply system and maintenance-free digital sensor, which can be arranged in monitoring area for a long time to collect soil temperature, humidity and salinity data. The collected data can be stored in SD card locally and transmitted to cloud server in real time through 4G network. Up to now, the system has been running stably for nearly two years under the condition of unattended and maintenance free. More than 30000 soil salinity data have been collected from 5 monitoring points, which can be used for long-term observation of the interaction between salinity and plant growth, so as to improve the soil and improve the quality of agriculture products.",https://ieeexplore.ieee.org/document/9456521/,2021 IEEE International Conference on Artificial Intelligence and Industrial Design (AIID),28-30 May 2021,ieeexplore
10.1109/AIID51893.2021.9456537,Design and Implementation of Regional Food Distribution Platform Based on Big Data,IEEE,Conferences,"In recent years, the rapid development of big data has made people's daily life very convenient, and at the same time, tasting all kinds of food has become an important activity for people to travel. Due to the vast territory of China, there are many types of cuisines with great differences, it is very important for travelers to understand the special regional cuisines in the area. This project aggregates regional food data based on big data, and provides tourists with efficient, stable and professional data retrieval and analysis services through a visual data interface, and provides intuitive, accurate, and real-time data support for the decision-making of finding characteristic regional food. This thesis first conducted a relevant understanding of big data and the overall situation of regional cuisine, analyzed the distribution of food in the sub-provincial city of Xi'an, explored the research methods and implementation methods of related projects at home and abroad, based on this, summarized the research of this project the goal. At the same time, the focus of this project is to analyze the price, score and popularity of regional food data analysis in my country, and to summarize, proofread and organize the data obtained before into standard and standardized data. Through the classification of basic information, the visual analysis and display of data is realized, and the key data urgently needed by decision makers are extracted from it. To analyze the needs of decision makers, establish corresponding strategies and measures to improve the quality of data services. The establishment of this system provides a useful supplement and improvement to the existing industry data analysis system. The system is mainly divided into six modules, which are data collection, data review, data summary, and visual data display modules. Among them, data collection includes crawling relevant data from the Internet and retrieving key data. The data audit function includes classifying the crawled data and reviewing its effectiveness. Data aggregation includes summarizing the filtered data in an excel table and passing Excel generates a visual chart that you want to know, and at the same time generates a word cloud by associating certain two related items in the data in the excel table. The visual data display module can more clearly show the results of data analysis to users, so that users can make better decisions. The visualization data uses AmChart Flash charts. The implementation of this system adopts Django Python Web framework, the development language chooses Python, the development tool adopted is PyCharm, and the database tool adopts MySQL.",https://ieeexplore.ieee.org/document/9456537/,2021 IEEE International Conference on Artificial Intelligence and Industrial Design (AIID),28-30 May 2021,ieeexplore
10.1109/ICMLC.2017.8108946,Design and implementation of smoke early warning system based on video analysis technology,IEEE,Conferences,"In order to obtain real-time accurate detection of smoke in the panoramic area, a visual -based smoke early warning system is designed. A 180-degree holographic and high speed camera is equipped as the video capture part of the system, and the video acquisition and target lock functions are implemented. The intelligent recognition function is separated into three steps. Firstly, the suspected smoke area is extracted by Vibe algorithm. Secondly, the Camshift method is used to track the suspected area of moving smoke. Thirdly, the smoke is detected by the proposed fusion dynamic features. The interface of system is developed by the software of QT. The designed system can not only realize sensitive area monitoring function in manual mode, but also the intelligent smoke early warning and detection function are achieved in automatic mode. The experimental results showed the designed system obtained the advantages of simple operation, rapid smoke alarm and the satisfied accuracy for industry requirement.",https://ieeexplore.ieee.org/document/8108946/,2017 International Conference on Machine Learning and Cybernetics (ICMLC),9-12 July 2017,ieeexplore
10.1109/SYNASC.2016.045,Detecting Malicious URLs: A Semi-Supervised Machine Learning System Approach,IEEE,Conferences,"As malware industry grows, so does the means of infecting a computer or device evolve. One of the most common infection vector is to use the Internet as an entry point. Not only that this method is easy to use, but due to the fact that URLs come in different forms and shapes, it is really difficult to distinguish a malicious URL from a benign one. Furthermore, every system that tries to classify or detect URLs must work on a real time stream and needs to provide a fast response for every URL that is submitted for analysis (in our context a fast response means less than 300-400 milliseconds/URL). From a malware creator point of view, it is really easy to change such URLs multiple times in one day. As a general observation, malicious URLs tend to have a short life (they appear, serve malicious content for several hours and then they are shut down usually by the ISP where they reside in). This paper aims to present a system that analyzes URLs in network traffic that is also capable of adjusting its detection models to adapt to new malicious content. Every correctly classified URL is reused as part of a new dataset that acts as the backbone for new detection models. The system also uses different clustering techniques in order to identify the lack of features on malicious URLs, thus creating a way to improve detection for this kind of threats.",https://ieeexplore.ieee.org/document/7829617/,2016 18th International Symposium on Symbolic and Numeric Algorithms for Scientific Computing (SYNASC),24-27 Sept. 2016,ieeexplore
10.1109/CSCI51800.2020.00019,Detection and Defense from False Data Injection Attacks In Aviation Cyber-Physical Systems Using Artificial Immune Systems,IEEE,Conferences,"In recent years, there has been a rapid expansion in the development of Cyber-Physical Systems (CPS), which allows the physical components and the cyber components of a system to be fully integrated and interacted with each other and with the physical world. The commercial aviation industry is shifting towards Aviation Cyber-Physical Systems (ACPS) framework because it allows real-time monitoring and diagnostics, real-time data analytics, and the use of Artificial Intelligent technologies in decision making. Inevitably, ACPS is not immune to cyber-attacks due to integrating a network system, which introduces serious security threats. False Data Injection (FDI) attack is widely used against CPS. It is a serious threat to the integrity of the connected physical components. In this paper, we propose a novel security algorithm for detecting FDI attacks in the communication network of ACPS using Artificial Immune System (AIS). The algorithm was developed based on the negative selection approach. The negative selection algorithm is used to detect malicious network packets and drop them. Then, a Nonlinear Autoregressive Exogenous (NARX) network is used to predict packets that dropped by the negative selection algorithm. The developed algorithm was implemented and tested on a networked control system of commercial aircraft as an Aviation Cyber-physical system.",https://ieeexplore.ieee.org/document/9457964/,2020 International Conference on Computational Science and Computational Intelligence (CSCI),16-18 Dec. 2020,ieeexplore
10.1109/ICSSA.2016.19,Detection of Man-in-the-Middle Attacks on Industrial Control Networks,IEEE,Conferences,"In this paper we present a method to detect Man-in-the-Middle attacks on industrial control systems. The approach uses anomaly detection by developing a model of normal behaviour of the industrial control system network. To come as close as possible to reality a simple industrial system, a conveyor belt with sensors and actuators, was set up with controllers widely used in industry. A machine learning approach based on the k-Nearest Neighbors algorithm with Bregman divergence was used to define a model of normal (valid) behaviour. Afterwards Man-in-the-Middle attacks were launched against the system and its behaviour during the attack was compared to the valid behaviour model. The results show that the approach taken was able to detect such attacks with satisfactory accuracy.",https://ieeexplore.ieee.org/document/7861654/,2016 International Conference on Software Security and Assurance (ICSSA),24-25 Aug. 2016,ieeexplore
10.1109/RAAI52226.2021.9508033,Development of Gasoline-Electric Hybrid Propulsion Surveillance and Reconnaissance VTOL UAV,IEEE,Conferences,"Vertical Take-Off and Landing (VTOL) Unmanned Aerial Vehicles (UAV) have been a high potential topic in the aerospace industry during the last decades due to its multirotor and fixed-wing nature of the aircraft. Besides, having the ability to rapidly deploy from a tight airstrip and gathering Intelligence, Surveillance, and Reconnaissance (ISR) information is the best way to be one step ahead of the enemy. In this paper, we present the implementation and development of gasoline-electric hybrid propulsion VTOL Unmanned Aerial vehicle respectively. The Hybrid propulsion VTOL UAV offers image and real-time video transmission to the ground station with fully autonomous control to get the best view of the enemy from the sky. The gasoline-electric hybrid propulsion system provides long flight endurance with efficient power consumption. The fundamentals of the multirotor and the conventional fixed-wing aircraft present the theoretical background of the aircraft. The accomplished design consists of high-performance multirotor motors with an efficient gasoline engine. Furthermore, the control system architecture, avionics, and power distribution system presented with addressing cost-effective trending design techniques. The performance of the system has been improved using commercially off-the-shelf (COTS) hardware.",https://ieeexplore.ieee.org/document/9508033/,"2021 IEEE International Conference on Robotics, Automation and Artificial Intelligence (RAAI)",21-23 April 2021,ieeexplore
10.1109/GCAT52182.2021.9587584,Development of System for Detection and Prevention of Cyber Attacks Using Artifıcial Intelligence Methods,IEEE,Conferences,"Artificial intelligence (AI) technologies have given the cyber security industry a huge leverage with the possibility of having significantly autonomous models that can detect and prevent cyberattacks – even though there still exist some degree of human interventions. AI technologies have been utilized in gathering data which can then be processed into information that are valuable in the prevention of cyberattacks. These AI-based cybersecurity frameworks have commendable scalability about them and are able to detect malicious activities within the cyberspace in a prompter and more efficient manner than conventional security architectures. However, our one or two completed studies did not provide a complete and clear analyses to apply different machine learning algorithms on different media systems. Because of the existing methods of attack and the dynamic nature of malware or other unwanted software (adware etc.) it is important to automatically and systematically create, update and approve malicious packages that can be available to the public. Some of Complex tests have shown that DNN performs maybe can better than conventional machine learning classification. Finally, we present a multiple, large and hybrid DNN torrent structure called Scale-Hybrid-IDS-AlertNet, which can be used to effectively monitor to detect and review the impact of network traffic and host-level events to warn directly or indirectly about cyber-attacks. Besides this, they are also highly adaptable and flexible, with commensurate efficiency and accuracy when it comes to the detection and prevention of cyberattacks.There has been a multiplicity of AI-based cyber security architectures in recent years, and each of these has been found to show varying degree of effectiveness. Deep Neural Networks, which tend to be more complex and even more efficient, have been the major focus of research studies in recent times. In light of the foregoing, the objective of this paper is to discuss the use of AI methods in fighting cyberattacks like malware and DDoS attacks, with attention on DNN-based models.",https://ieeexplore.ieee.org/document/9587584/,2021 2nd Global Conference for Advancement in Technology (GCAT),1-3 Oct. 2021,ieeexplore
10.1109/EDPC51184.2020.9388192,Development of a Cloud- and Edge-Architecture for adaptive model weight optimization of a CNN exemplified by optical detection of hairpin welding,IEEE,Conferences,"The beginning of a global reorientation towards an increasingly conscientious approach to nature and the human habitat has been accompanied by changes in industry and society. The automotive industry, where a transition from combustion to electrically powered vehicles is currently underway, is also concerned with this change. In addition to increasing the capacity of the battery, improving the efficiency of the electric motor is essential. To achieve these goals, however, new technologies such as hairpins for the stator are needed. An important process step involves the welding of two pairs of hairpins, which often leads to welding defects. Nevertheless, expert knowledge in this field is limited. Optical monitoring of the welding process with the help of a convolutional neural network (CNN) is a good approach. This approach can compensate for the low level of expert knowledge and detects and classifies welding defects directly in the production line. However, the disadvantage of optical monitoring is that production conditions and the surrounding environment change over time. This has an impact on optical detection and can negatively affect the accuracy of a CNN. For example, the camera perspective can change, which has a negative effect on optical quality monitoring. Therefore, this paper presents an approach for monitoring and evaluating the quality of a CNN in a cloud instance online. If a deteriorating quality is detected, the CNN in the cloud is re-trained by continuously collected data and then automatically deployed to the production line. This allows the CNN to adapt to the changing environmental conditions. The present approach is demonstrated and validated with real data of the stator production process. Compared with the current state-of-the-art, this control loop is highly automated and requires a minimum of human intervention.",https://ieeexplore.ieee.org/document/9388192/,2020 10th International Electric Drives Production Conference (EDPC),8-9 Dec. 2020,ieeexplore
10.1109/ICMLC.2010.5580600,Development of a distributed control system for PLC-based applications,IEEE,Conferences,"This paper presents experiments aims to built a platform to monitor and control PLC-based processes over PROFIBUS-DP network. The platform is built using industry-standard off-the-shelf PLCs. Integrated with each PLC are communication processors that can be used for connectivity to the network and to a DP modem. The communication processor module used in this work provides an industrial compatible protocol over PROFIBUS-DP. The mode of communication of the industrial automation control system base on the PROFIBUS agreement where variable frequency drives and motor control centers can incorporate bus technologies, and developed a variable-frequency speed base on PROFIBUS-DP. The system can real-time control and collect data from diversified equipment, and the data get across the PROFIBUS is carried to industrial control computer. After data analysis, system carry on the real-time monitoring to the scene, and realize the automation control. This system is very important to study bus, distributed system, the frequency conversion velocity and PLC. It is channel to research on automation control system.",https://ieeexplore.ieee.org/document/5580600/,2010 International Conference on Machine Learning and Cybernetics,11-14 July 2010,ieeexplore
10.1109/EMBC.2016.7591089,Development of a real time activity monitoring Android application utilizing SmartStep,IEEE,Conferences,"Footwear based activity monitoring systems are becoming popular in academic research as well as consumer industry segments. In our previous work, we had presented developmental aspects of an insole based activity and gait monitoring system-SmartStep, which is a socially acceptable, fully wireless and versatile insole. The present work describes the development of an Android application that captures the SmartStep data wirelessly over Bluetooth Low energy (BLE), computes features on the received data, runs activity classification algorithms and provides real time feedback. The development of activity classification methods was based on the the data from a human study involving 4 participants. Participants were asked to perform activities of sitting, standing, walking, and cycling while they wore SmartStep insole system. Multinomial Logistic Discrimination (MLD) was utilized in the development of machine learning model for activity prediction. The resulting classification model was implemented in an Android Smartphone. The Android application was benchmarked for power consumption and CPU loading. Leave one out cross validation resulted in average accuracy of 96.9% during model training phase. The Android application for real time activity classification was tested on a human subject wearing SmartStep resulting in testing accuracy of 95.4%.",https://ieeexplore.ieee.org/document/7591089/,2016 38th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC),16-20 Aug. 2016,ieeexplore
10.1109/CONECCT.2014.6740278,Development of biochip arryer and imaging system for making biochip,IEEE,Conferences,"A bio-chip arrayer system has been developed indigenously for producing bio-chips. The bio-chips are based on use of track etched membranes (TEM) as a novel solid support. TEM are extremely thin (10 micron) and highly microporous membranes. We have tested polycarbonate (PC) TEM by manual spotting of antibodies for the development of the antibody bio-chip for simultaneous estimation of the thyroid related hormones in sera of patients with thyroid disorders. However, manual spotting is a labor-intensive and error prone process. The arrayer has been developed using piezoelectric based pin head for non contact spotting of solution on the porous TEM. This is a computer controlled three axes robotic arm to automate and streamline different processes. It precisely and accurately dispenses few picolitre to nanolitre solution resulting in excellent spot reproducibility and gives uniform spot morphology. It uses real time pressure control and excellent circuit for voltage and pulse width for piezo-dispenser. It has been developed according to the flexible industry standard (25 × 75 mm) glass slide microarray format and can be used with any 3-D (membranes or gel pads) or 2-D (glass) substrates. Real time machine vision system has also been developed to monitor the dispenser performance. The system has been modeled using the neural networks to dispense the different viscous samples to maintain the same spot size. This system includes user friendly control software to define all critical parameters for printing operation. The performance, accuracy and repeatability of the system are evaluated by printing test patterns and quantifying various spot parameters using statistical methods.",https://ieeexplore.ieee.org/document/6740278/,"2014 IEEE International Conference on Electronics, Computing and Communication Technologies (CONECCT)",6-7 Jan. 2014,ieeexplore
10.1109/TELFOR48224.2019.8971360,Development of intelligent systems and application of gamification in artificial intelligent learning,IEEE,Conferences,"CToday, intelligent systems are used in many fields - medicine, agriculture, transport, telecommunications, industrial process management and control, finance, commerce, the computer game industry and many others. This paper describes a complete way to develop an intelligent software system for simulation and visualization of artificial intelligence algorithms. The system includes artificial intelligence algorithms from basic search strategies and game theory, inference algorithms and knowledge representation models, to advanced search techniques, machine and inductive learning. The application of these algorithms to real everyday problems and the application of this software system as an auxiliary tool for analysis of input data and inference in various fields are presented. The system is also applicable to education in an introductory artificial intelligence course at the university, so the last phase of the research involved the transition of the software system into a game-based tool and application of gamification.",https://ieeexplore.ieee.org/document/8971360/,2019 27th Telecommunications Forum (TELFOR),26-27 Nov. 2019,ieeexplore
10.1109/ICIP48927.2020.9367369,Development of simulator for efficient aquaculture of Sillago japonica using reinforcement learning,IEEE,Conferences,"Recently, the situation in the Japanese fishing industry has become critical, resulting in one of the most significant food issues in Japan. Aquaculture technology is expected to be a solution to this problem. Sillago japonica is a fish that inhabits shallow waters in parts of Asia, and large Sillago japonica is very expensive. Therefore, we believe that aquaculture of this fish would help revitalize the fishing industry in Japan. However, aquaculture requires considerable manual labor. Hence, we need to introduce new technologies for the aquaculture of Sillago japonica. Specifically, we have been developing two systems to improve efficiency and reduce costs of this aquaculture: an environment control system and an automatic feeding system. The former is to maintain favorable environment conditions for the fish in the aquaculture tank. The latter is for optimal feeding of the fish. In this paper, we describe the development of an automatic feeding system using artificial intelligence (AI). This system includes four processes: image input, image recognition, feeder control, and feeding action. We have adopted AI technologies to assist in the second and third processes. Although these two processes can be implemented together, it is easier for the AI to learn them as two separate processes. In particular, the second (image recognition) process uses supervised learning, and the third (feeder control) process uses reinforcement learning. However, it is impractical to train the AI in the third process in a real-world aquaculture environment that sustains many failures. Therefore, we have developed an aquaculture simulator to facilitate AI learning of the feeder control process. Additionally, we performed an experiment to validate our simulator using the number of feeders and the number of fish as a parameter.",https://ieeexplore.ieee.org/document/9367369/,2020 International Conference on Image Processing and Robotics (ICIP),6-8 March 2020,ieeexplore
10.1109/OCEANS-Genova.2015.7271691,DexROV: Enabling effective dexterous ROV operations in presence of communication latency,IEEE,Conferences,"Subsea interventions in the oil &amp; gas industry as well as in other domains such as archaeology or geological surveys are demanding and costly activities for which robotic solutions are often deployed in addition or in substitution to human divers - contributing to risks and costs cutting. The operation of ROVs (Remotely Operated Vehicles) nevertheless requires significant off-shore dedicated manpower to handle and operate the robotic platform and the supporting vessel. In order to reduce the footprint of operations, DexROV proposes to implement and evaluate novel operation paradigms with safer, more cost effective and time efficient ROV operations. As a keystone of the proposed approach, manned support will in a large extent be delocalized within an onshore ROV control center, possibly at a large distance from the actual operations, relying on satellite communications. The proposed scheme also makes provision for advanced dexterous manipulation and semi-autonomous capabilities, leveraging human expertise when deemed useful. The outcomes of the project will be integrated and evaluated in a series of tests and evaluation campaigns, culminating with a realistic deep sea (1,300 meters) trial in the Mediterranean sea.",https://ieeexplore.ieee.org/document/7271691/,OCEANS 2015 - Genova,18-21 May 2015,ieeexplore
10.1109/ICICT52872.2021.00047,Digital Knowledge Base for Industrial Augmented Reality Systems Based on Semantic Technologies,IEEE,Conferences,"Augmented Reality is a technology that offers enormous potential in the industry. Due to a lack of expertise, however, companies are facing various challenges in exploiting this potential. This includes the demand-oriented configuration of the AR system. Depending on the planned use case and company-specific influencing factors and general conditions, suitable AR devices are to be identified and functionalities need to be purposefully selected. The necessary knowledge is implicitly available in numerous sources and difficult to access for companies. In the scope of this work, therefore, a machine-readable knowledge base is developed, in which the knowledge relevant for the AR system configuration is consolidated and formalized. Four aspects of knowledge are considered: The AR system structure including technical and functional components; Potential influences through ambient factors; Experience and application knowledge; The specification of specific AR devices. The knowledge base is realized based on the Web Ontology Language (OWL) and thus enables a digital and partially automated processing of the knowledge using software tools and algorithms. In combination with Artificial Intelligence solutions, the developed knowledge base can be transferred to other systems in the future and provide a powerful tool for system configuration.",https://ieeexplore.ieee.org/document/9476944/,2021 4th International Conference on Information and Computer Technologies (ICICT),11-14 March 2021,ieeexplore
10.1109/IAI50351.2020.9262203,Digital Twin Enabled Smart Control Engineering as an Industrial AI: A New Framework and Case Study,IEEE,Conferences,"In Industry 4.0, the increasing complexity of industrial systems introduces unknown dynamics that affect the performance of manufacturing processes. Thus, Digital Twin appears as a breaking technology to develop virtual representations of any complex system design, analysis, and behavior prediction tasks to enhance the system understanding via enabling capabilities like real-time analytics, or Smart Control Engineering. In this paper, a novel framework is proposed for the design and implementation of Digital Twin applications to the development of Smart Control Engineering. The framework involve the steps of system documentation, Multidomain Simulation, Behavioral Matching, and real-time monitoring, which is applied to develop the Digital Twin for a real-time vision feedback temperature uniformity control. The obtained results show that Digital Twin is a fundamental part of the transformation into Industry 4.0.",https://ieeexplore.ieee.org/document/9262203/,2020 2nd International Conference on Industrial Artificial Intelligence (IAI),23-25 Oct. 2020,ieeexplore
10.1109/DESSERT.2018.8409186,Digitization of the economy of Ukraine: Strategic challenges and implementation technologies,IEEE,Conferences,"The main directions, challenges, threats of digitization of the national economy of Ukraine have been considered in the paper. The attention is focused on the found weaknesses and the imperfection of the strategy and the state policy of digitization of Ukraine's economy. The authors have proven the potential and new possibilities of solving public finance management problems with the usage of blockchain technology. It has been justified that activation of transformation processes in the real economy sector due to the introduction of Industry 4.0 concept is important for Ukraine. The paper reveals basic principles and technologies, the experience of the European Union, and characterizes Industry 4.0 view in Ukraine. The development of the latest financial technologies - FinTech - has been recognized as the driver of digital transformation of financial services. The types of FinTech innovations, the features of increasing competition between FinTech companies and traditional financial intermediaries, the tendencies of FinTech development in Ukraine have been characterized.",https://ieeexplore.ieee.org/document/8409186/,"2018 IEEE 9th International Conference on Dependable Systems, Services and Technologies (DESSERT)",24-27 May 2018,ieeexplore
10.1109/ICST.2019.00029,Directing a Search Towards Execution Properties with a Learned Fitness Function,IEEE,Conferences,"Search based software testing is a popular and successful approach both in academia and industry. SBST methods typically aim to increase coverage whereas searching for executions with specific properties is largely unresearched. Fitness functions for execution properties often possess search landscapes that are difficult or intractable. We demonstrate how machine learning techniques can convert a property that is not searchable, in this case crashes, into one that is. Through experimentation on 6000 C programs drawn from the Codeflaws repository, we demonstrate a strong, program independent correlation between crashing executions and library function call patterns within those executions as discovered by a neural net. We then exploit the correlation to produce a searchable fitness landscape to modify American Fuzzy Lop, a widely used fuzz testing tool. On a test set of previously unseen programs drawn from Codeflaws, a search strategy based on a crash targeting fitness function outperformed a baseline in 80.1% of cases. The experiments were then repeated on three real world programs: the VLC media player, and the libjpeg and mpg321 libraries. The correlation between library call traces and crashes generalises as indicated by ROC AUC scores of 0.91, 0.88 and 0.61. The produced search landscape however is not convenient due to plateaus. This is likely because these programs do not use standard C libraries as often as do those in Codeflaws. This limitation can be overcome by considering a more powerful observation domain and a broader training corpus in future work. Despite limited generalisability of the experimental setup, this research opens new possibilities in the intersection of machine learning, fitness functions, and search based testing in general.",https://ieeexplore.ieee.org/document/8730160/,"2019 12th IEEE Conference on Software Testing, Validation and Verification (ICST)",22-27 April 2019,ieeexplore
10.1109/CCECE.2019.8861718,Distributed Optimal Power Flow for Electric Power Systems with High Penetration of Distributed Energy Resources,IEEE,Conferences,"Optimization technology is developing to the point of becoming a cost-effective enabler of increased power transfer asset utilization. This paper presents a smart decomposition technique for the traditional optimal power flow (OPF) algorithm to allow distributed optimal power flow (DOPF) calculations without relying on a centralized controller. Hence, it develops a feasible distributed architectures for the electric power industry. The proposed method is implemented using Monte Carlo Tree Search based reinforcement learning (MCTS-RL) algorithm. This reduces computational complexity and allows to avoid difficulties associated with stochastic modeling often used to capture the random nature of distributed energy resources (DER) units and loads. The efficiency of the optimization process is improved when the DOPF reflects the fast response capability of the optimal solution. This contribution provides results for a real-time dispatchable resource and demonstrates the flexibility of RL to adapt to changes of system states, ultimately reducing the generation cost while maintaining the system security constraints.",https://ieeexplore.ieee.org/document/8861718/,2019 IEEE Canadian Conference of Electrical and Computer Engineering (CCECE),5-8 May 2019,ieeexplore
10.1109/IPDPS.2018.00068,Do Developers Understand IEEE Floating Point?,IEEE,Conferences,"Floating point arithmetic, as specified in the IEEE standard, is used extensively in programs for science and engineering. This use is expanding rapidly into other domains, for example with the growing application of machine learning everywhere. While floating point arithmetic often appears to be arithmetic using real numbers, or at least numbers in scientific notation, it actually has a wide range of gotchas. Compiler and hardware implementations of floating point inject additional surprises. This complexity is only increasing as different levels of precision are becoming more common and there are even proposals to automatically reduce program precision (reducing power/energy and increasing performance) when results are deemed """"good enough.'"""" Are software developers who depend on floating point aware of these issues? Do they understand how floating point can bite them? To find out, we conducted an anonymous study of different groups from academia, national labs, and industry. The participants in our sample did only slightly better than chance in correctly identifying key unusual behaviors of the floating point standard, and poorly understood which compiler and architectural optimizations were non-standard. These surprising results and others strongly suggest caution in the face of the expanding complexity and use of floating point arithmetic.",https://ieeexplore.ieee.org/document/8425212/,2018 IEEE International Parallel and Distributed Processing Symposium (IPDPS),21-25 May 2018,ieeexplore
10.1109/CSE.2014.273,Dolphin: Ultrasonic-Based Gesture Recognition on Smartphone Platform,IEEE,Conferences,"User experience of smart mobile devices can be improved in numerous scenarios with the assist of in-air gesture recognition. Most existing methods proposed by industry and academia are based on special sensors. On the contrary, a special sensor-independent in-air gesture recognition method named Dolphin is proposed in this paper which can be applied to off-the-shelf smart devices directly. The only sensors Dolphin needs are the loudspeaker and microphone embedded in the device. Dolphin emits a continuous 21 KHz tone by the loudspeaker and receive the gesture-reflecting ultrasonic wave by the microphone. The gesture performed is encoded into the reflected ultrasonic in the form of Doppler shift. By combining manual recognition and machine leaning methods, Dolphin extracts features from Doppler shift and recognizes a rich set of pre-defined gestures with high accuracy in real time. Parameter selection strategy and gesture recognition under several scenarios are discussed and evaluated in detail. Dolphin can be adapted to multiple devices and users by training using machine learning methods.",https://ieeexplore.ieee.org/document/7023784/,2014 IEEE 17th International Conference on Computational Science and Engineering,19-21 Dec. 2014,ieeexplore
10.1109/EHB50910.2020.9280165,Drivers’ Drowsiness Detection and Warning Systems for Critical Infrastructures,IEEE,Conferences,"Road traffic accidents, due to driver fatigue, tend to inflict high mortality rates comparing with accidents involving rested drivers. Currently there is an emerging automotive industry trend towards equipping vehicles with various driver-assistance technologies. Third parties also started producing complementary systems, including ones that can detect the driver's degree of fatigue, but this growing field requires further research and development. The main purpose of this paper is the development and implementation of a system capable to detecting and alert, in real-time, the driver's level of fatigue. A system like this is expected to make the driver aware of the assumed danger when his level of driving and taking decisions are reduced and is indicating a sleep break as the necessary approach. By monitoring the state of the human eyes, it is assumed that the signs of driver fatigue can be detected early enough to prevent a possible road accident, which could result in severe injuries or ultimately, in fatalities. Hence, in this work the authors are focused on the video monitoring of the driver face, especially on his eyes position in time, when open or closed, using a machine learning object detection algorithm, the Haar Cascade. Two pretrained Haar classifiers, a face cascade, and an eye cascade were imported from the OpenCV GitHub repository. The OpenCV library, as well as other required packages, were installed on a BeagleBone Black Wireless development board. The software implementation, in order to achieve the driver's drowsiness detection, was made through the Python software program. The proposed system manages to alert if the eyes of the driver are being kept closed for more than a certain amount of time by triggering a set of warning lights and sounds. The large-scale implementation of this type of system will drop the number of road accidents caused by the drivers' fatigue, thus saving countless lives and bringing a reduction of the socio-economic costs associated with these tragic events.",https://ieeexplore.ieee.org/document/9280165/,2020 International Conference on e-Health and Bioengineering (EHB),29-30 Oct. 2020,ieeexplore
10.1109/BigDataCongress.2017.44,Drowsy Driving Warning System Based on GS1 Standards with Machine Learning,IEEE,Conferences,"Drowsy driving is the primary cause of motor vehicle accidents and is a risk factor that leads to the loss of human life, remaining as a challenge for the global automotive industry. Recently, drowsy monitoring system has been actively studied for prediction system based machine learning. However, the challenges of automotive real-time constraints and flexibility should be taken into consideration against a large amount of heterogeneous data from vehicle network and other device. To solve this problem, we propose drowsy monitoring system based machine learning using GS1 standard. First, vehicle motion data is defined and modeled using the GS1 standard language for drowsy predict. Second, we propose an optimal algorithm selection and detail architecture for automotive real-time environments through machine learning algorithms (KNN, Naïve Bayes, Logistic Regression) and deep learning algorithms (RNN-LSTM). Finally, we describe system-wide integration and implementation through the open source hardware Raspberry Pi and the machine learning SW framework. We provide optimal LSTM architecture and implementation that takes into account the real-time environmental conditions and how to improve the readability and usability of the vehicle motion data. We also share the rapid prototyping methodology case of connected car systems without other sensor devices.",https://ieeexplore.ieee.org/document/8029337/,2017 IEEE International Congress on Big Data (BigData Congress),25-30 June 2017,ieeexplore
10.1109/DSAA53316.2021.9564245,Dynamic Graph Convolutional LSTM application for traffic flow estimation from error-prone measurements: results and transferability analysis,IEEE,Conferences,"The technological advances in the transportation and automotive industry led to the use of new types of sensing systems more cost-effective and adapted to large-scale dense deployment. Those sensing techniques allow continuously gathering traffic measurements times series in different geospatial locations. The accuracy of the obtained raw measurements is often hindered by different factors related to the sensing environment and the sensing process itself and thus fail to capture the short-term traffic variations crucial for real-time traffic monitoring. In this paper, we propose the DGC-LSTM model for area-wide traffic estimation from error-prone measurements time series. The backbone of the DGC-LSTM model is a graph convolutional Long Short Term Memory model with a dynamic adjacency matrix. The adjacency matrix is learned and optimized during the model training. The adjacency matrix values are estimated from the set of contextual features that impact the dynamicity of the dependencies in both the spatial and temporal dimensions. Experiments on a realistic synthetic labelled Bluetooth counts dataset is used for model evaluation. Lastly, we highlight the importance of transfer learning methods to improve the model applicability by ensuring model adaptation to the new deployment site while avoiding the extensive data-labelling effort.",https://ieeexplore.ieee.org/document/9564245/,2021 IEEE 8th International Conference on Data Science and Advanced Analytics (DSAA),6-9 Oct. 2021,ieeexplore
10.1109/ETFA46521.2020.9211946,Dynamic Process Planning using Digital Twins and Reinforcement Learning,IEEE,Conferences,"In order to enable changeable production of Industry 4.0 applications, a production system should respond to unpredictable changes quickly and adequately. This requires process planning to be performed based on the real time operating conditions and dynamic changes to be handled with cognitive skills. To meet this demand, we present a process planning approach using digital twins and reinforcement learning to derive near-optimal process plans. The digital twins enable access to real-time information about the production system. They also constitute the environment for training the agent of the reinforcement learning method. The environment works as a virtual plant, containing the attributes of the product and resources, and uses simulation models of the resources to calculate the reward for an action in terms of reinforcement learning. Reinforcement learning enables our approach to derive process plans via trial and error. Besides the virtual plant, our approach has a planner, which plays the role of the agent to derive near-optimal plans by trying different actions in the virtual plant, and observes the rewards. We apply the Q-learning algorithm to derive near optimal process plans. The evaluation results show that our approach is able to derive near-optimal process plans for different problem sizes. The evaluation also demonstrated the planner's ability to identify by itself which action to take in which situation. Consequently, no modeling of the preconditions and effects of the actions is necessary.",https://ieeexplore.ieee.org/document/9211946/,2020 25th IEEE International Conference on Emerging Technologies and Factory Automation (ETFA),8-11 Sept. 2020,ieeexplore
10.1109/CloudNet.2014.6968974,Dynamic allocation and efficient distribution of data among multiple clouds using network coding,IEEE,Conferences,"Distributed storage has attracted large interest lately from both industry and researchers as a flexible, cost-efficient, high performance, and potentially secure solution for geographically distributed data centers, edge caching or sharing storage among users. This paper studies the benefits of random linear network coding to exploit multiple commercially available cloud storage providers simultaneously with the possibility to constantly adapt to changing cloud performance in order to optimize data retrieval times. The main contribution of this paper is a new data distribution mechanisms that cleverly stores and moves data among different clouds in order to optimize performance. Furthermore, we investigate the trade-offs among storage space, reliability and data retrieval speed for our proposed scheme. By means of real-world implementation and measurements using well-known and publicly accessible cloud service providers, we can show close to 9x less network use for the adaptation compared to more conventional dense recoding approaches, while maintaining similar download time performance and the same reliability.",https://ieeexplore.ieee.org/document/6968974/,2014 IEEE 3rd International Conference on Cloud Networking (CloudNet),8-10 Oct. 2014,ieeexplore
10.1109/ICCCNT51525.2021.9580072,Early Detection of Disease in Rice Paddy: A Deep Learning based Convolution Neural Networks Approach,IEEE,Conferences,"The agriculture industry faces huge economic losses due to bacterial, viral or fungal infections in the crops due to which farmers lose 15 to 20% of their total profit every year. India is the second largest producer of rice and a leading exporter of the same in the global market. Thus, early detection of diseases in essential crops is a significant area of research in order to prevent further damage to them. The widespread development of Deep Learning makes it possible to achieve the goal of disease detection in crops. The novelty of this work is early detection of Brown spot disease in rice paddy using Convolution Neural Networks. The area of the disease affected was also found to optimize the usage of fertilizers. This work makes use of Image recognition and preprocessing algorithm based on real time data. Data preprocessing and feature extraction has been done using a self-designed image-processing tool. Tensor flow and Keras framework has been implemented on both training and testing data which was collected manually from rice fields. The proposed model achieved an accuracy of 97.32%.",https://ieeexplore.ieee.org/document/9580072/,2021 12th International Conference on Computing Communication and Networking Technologies (ICCCNT),6-8 July 2021,ieeexplore
10.1109/CSIT49958.2020.9321954,Eco-friendly Home Automation System Implemented Using Machine Learning Algorithms,IEEE,Conferences,"This paper presents the exemplary system of house automation implemented with the use of Industry 4.0 inventions. The proposed system tries to benefit from weather conditions to heat or cool the house without any electrical heaters or air conditioners. It is implemented with the aid of Machine Learning algorithms, the Internet of Things, and Cloud technology. The paper contains a technical and practical description of the system, the results of the real use, and proposed extensions that can improve the presented solution.",https://ieeexplore.ieee.org/document/9321954/,2020 IEEE 15th International Conference on Computer Sciences and Information Technologies (CSIT),23-26 Sept. 2020,ieeexplore
10.1109/CSCloud-EdgeCom49738.2020.00050,Edge Computing-based 3D Pose Estimation and Calibration for Robot Arms,IEEE,Conferences,"Industrial robots are widely used in current production lines, and complex pipeline processes, especially those with different assembly requirements, are designed for intelligent manufacturing in the era of industry 4.0. During the new crown epidemic, a large number of car companies used the production line to transform production of medical materials such as masks and protective clothing, which provided a strong guarantee for fighting the epidemic. In this scenario, a pipeline is often assembled from robotic arms from multiple suppliers. The traditional methods is complex and takes a lot of time. In this paper, we propose a novel deep learning based robot arm 3D pose estimation and calibration model with simple Kinect stereo cameras which can be deployed on light-weight edge computing systems. The light-weight deep CNN model can detection 5 predefined key points based on RGB-D data. In this way, when the assembly line composed of different robot arms needs to be reassembled, our model can quickly provide the robot's pose information without additional tuning processes. Testing in Webots with Rokae xb4 robot arm model shows that our model can quickly estimate the key point of the robot arm.",https://ieeexplore.ieee.org/document/9170983/,2020 7th IEEE International Conference on Cyber Security and Cloud Computing (CSCloud)/2020 6th IEEE International Conference on Edge Computing and Scalable Cloud (EdgeCom),1-3 Aug. 2020,ieeexplore
10.1109/ISAECT50560.2020.9523700,Edge-Cloud Architectures Using UAVs Dedicated To Industrial IoT Monitoring And Control Applications,IEEE,Conferences,"The deployment of new technologies to ease the control and management of a massive data volume and its uncertainty is a very significant challenge in the industry. Under the name ""Smart Factory"", the Industrial Internet of Things (IoT) aims to send data from systems that monitor and control the physical world to data processing systems for which cloud computing has proven to be an important tool to meet processing needs. unmanned aerial vehicles (UAVs) are now being introduced as part of IIoT and can perform important tasks. UAVs are now considered one of the best remote sensing techniques for collecting data over large areas. In the field of fog and edge computing, the IoT gateway connects various objects and sensors to the Internet. It function as a common interface for different networks and support different communication protocols. Edge intelligence is expected to replace Deep Learning (DL) computing in the cloud, providing a variety of distributed, low-latency and reliable intelligent services. In this paper, An unmanned aerial vehicle is automatically integrated into an industrial control system through an IoT gateway platform. Rather than sending photos from the UAV to the cloud for processing, an AI cloud trained model is deployed in the IoT gateway and used to process the taken photos. This model is designed to overcome the latency channels of the cloud computing architecture. The results show that the monitoring and tracking process using advanced computing in the IoT gateway is significantly faster than in the cloud.",https://ieeexplore.ieee.org/document/9523700/,2020 International Symposium on Advanced Electrical and Communication Technologies (ISAECT),25-27 Nov. 2020,ieeexplore
10.1109/AIIoT52608.2021.9454247,"Education System for Bangladesh Using Augmented Reality, Virtual Reality and Artificial Intelligence",IEEE,Conferences,"This paper presents an innovative application for students to study and understand their coursework without any external help from a private tutor. The system uses Augmented Reality (AR) to provide hands on experience for the students. The presented system also supports Virtual Reality (VR) that enriches this process and immerses the users into a fun and productive learning experience. Moreover, the system introduces an industry first Artificial intelligence (AI) based study guide that directs students towards necessary topics and advises them on what to improve on. All the core system features are implemented and are accessible via two mediums. First, a standalone mobile phone application. Second, a dedicated web portal.",https://ieeexplore.ieee.org/document/9454247/,2021 IEEE World AI IoT Congress (AIIoT),10-13 May 2021,ieeexplore
10.1109/ICEETS.2016.7583860,Efficiency optimization of induction motor drive using Artificial Neural Network,IEEE,Conferences,"Induction motors are the workhorse of industry, have good efficiency at rated load, but long duration usage of IM at partial load shows poor efficiency which leads to waste in energy and revenue as well. These motors are reliable, robust, high power/mass ratio and economic, hence replaced all other motors in the industry, so even minute increment in induction motor efficiency can have a major impact on consumption of electricity and saving of revenue, globally. This paper utilizes, a combination of two key concepts of efficiency optimization-loss model control (LMC) and search control (SC) for efficient operation of induction motors used in various industrial applications, in aforesaid load condition. At first, to estimate optimal I<sub>ds</sub> values for various load conditions, an optimal I<sub>ds</sub> expression in terms of machine parameters and load parameters, based on machine loss model in d-q frame along with classical optimization technique, is utilized. Secondly, an offline trained artificial neural network (ANN) controller is used to reproduce the optimal I<sub>ds</sub> values, in run-time load condition. This eliminates run-time computations and perturbation for optimal flux, as in conventional SC method. The (ANN) optimal controller is designed for optimal I<sub>ds</sub> as output, while providing load torque and speed information as inputs. The training is performed in MATLAB and good accuracy of the training model is seen. Dynamic and steady-state performances are compared for proposed optimal (optimal I<sub>ds</sub>) operations and conventional vector operations (constant I<sub>ds</sub>), with the help of a simulation model, developed in MATLAB. Excellent dynamic response in load transients as well as superior efficiency performance (1- 18%) at steady-state, for a wide range of speed and torque in simulation is attained. Assimilated with similar earlier work, the proposed methodology offers effortless implementation in real-time industrial facilities, ripple free operations, fast response and higher energy savings.",https://ieeexplore.ieee.org/document/7583860/,2016 International Conference on Energy Efficient Technologies for Sustainability (ICEETS),7-8 April 2016,ieeexplore
10.1109/ICSME46990.2020.00082,Efficient Bug Triage For Industrial Environments,IEEE,Conferences,"Bug triage is an important task for software maintenance, especially in the industrial environment, where timely bug fixing is critical for customer experience. This process is usually done manually and often takes significant time. In this paper, we propose a machine-learning-based solution to address the problem efficiently. We argue that in the industrial environment, it is more suitable to assign bugs to software components (then to responsible developers) than to developers directly. Because developers can change their roles in industry, they may not oversee the same software module as before. We also demonstrate experimentally that assigning bugs to components rather than developers leads to much higher accuracy. Our solution is based on text-projection features extracted from bug descriptions. We use a Deep Neural Network to train the classification model. The proposed solution achieves state-of-the-art performance based on extensive experiments using multiple data sets. Moreover, our solution is computationally efficient and runs in near real-time.",https://ieeexplore.ieee.org/document/9240673/,2020 IEEE International Conference on Software Maintenance and Evolution (ICSME),28 Sept.-2 Oct. 2020,ieeexplore
10.1109/eScience.2019.00047,Efficient Runtime Capture of Multiworkflow Data Using Provenance,IEEE,Conferences,"Computational Science and Engineering (CSE) projects are typically developed by multidisciplinary teams. Despite being part of the same project, each team manages its own workflows, using specific execution environments and data processing tools. Analyzing the data processed by all workflows globally is a core task in a CSE project. However, this analysis is hard because the data generated by these workflows are not integrated. In addition, since these workflows may take a long time to execute, data analysis needs to be done at runtime to reduce cost and time of the CSE project. A typical solution in scientific data analysis is to capture and relate the data in a provenance database while the workflows run, thus allowing for data analysis at runtime. However, the main problem is that such data capture competes with the running workflows, adding significant overhead to their execution. To mitigate this problem, we introduce in this paper a system called ProvLake, which adopts design principles for providing efficient distributed data capture from the workflows. While capturing the data, ProvLake logically integrates and ingests them into a provenance database ready for analyses at runtime. We validated ProvLake in a real use case in the O&amp;G industry encompassing four workflows that process 5 TB datasets for a deep learning classifier. Compared with Komadu, the closest solution that meets our goals, our approach enables runtime multiworkflow data analysis with much smaller overhead, such as 0.1%.",https://ieeexplore.ieee.org/document/9041720/,2019 15th International Conference on eScience (eScience),24-27 Sept. 2019,ieeexplore
10.1109/ICWS49710.2020.00067,Efficient Search for Moving Object Devices in Internet of Things Networks,IEEE,Conferences,"IoT search engines have attracted increasing attention from both academia and industry, since they are capable of crawling heterogeneous data sources in highly dynamic environment. To process tens of thousands of spatial-temporal-keyword queries per second, query efficiency and communication cost in IoT search engines become critical issues. To address these challenges, caching mechanisms in collaborative edge-cloud computing architecture, which can implement the caching paradigm in cloud for frequent n-hop neighboring activity regions, is proposed in this paper. Thereafter, frequent query results can be achieved quickly leveraging the spatial-temporal-keyword filtering index of n-hop neighbor regions through modeling keywords relevance and uncertain traveling time. Besides, we adopt STK-tree proposed previously to directly answer non-frequent queries. Extensive experiments on real-life dataset demonstrate that our method outperforms the state-of-the-art's techniques in terms of the reduction of the query time and the number of transmitted messages.",https://ieeexplore.ieee.org/document/9284140/,2020 IEEE International Conference on Web Services (ICWS),19-23 Oct. 2020,ieeexplore
10.1109/SC.2008.5213940,Efficient management of data center resources for Massively Multiplayer Online Games,IEEE,Conferences,"Today's massively multiplayer online games (MMOGs) can include millions of concurrent players spread across the world. To keep these highly-interactive virtual environments online, a MMOG operator may need to provision tens of thousands of computing resources from various data centers. Faced with large resource demand variability, and with misfit resource renting policies, the current industry practice is to maintain for each game tens of self-owned data centers. In this work we investigate the dynamic resource provisioning from external data centers for MMOG operation. We introduce a novel MMOG workload model that represents the dynamics of both the player population and the player interactions. We evaluate several algorithms, including a novel neural network predictor, for predicting the resource demand. Using trace-based simulation, we evaluate the impact of the data center policies on the resource provisioning efficiency; we show that dynamic provisioning can be much more efficient than its static alternative.",https://ieeexplore.ieee.org/document/5213940/,SC '08: Proceedings of the 2008 ACM/IEEE Conference on Supercomputing,15-21 Nov. 2008,ieeexplore
10.1109/CASE48305.2020.9249228,Efficiently Learning a Distributed Control Policy in Cyber-Physical Production Systems Via Simulation Optimization,IEEE,Conferences,"The manufacturing industry is becoming more dynamic than ever. The limitations of non-deterministic network delays and real-time requirements call for decentralized control. For such dynamic and complex systems, learning methods stand out as a transformational technology to have a more flexible control solution. Using simulation for learning enables the description of highly dynamic systems and provides samples without occupying a real facility. However, it requires prohibitively expensive computation. In this paper, we argue that simulation optimization is a powerful tool that can be applied to various simulation-based learning processes for tremendous effects. We proposed an efficient policy learning framework, ROSA (Reinforcement-learning enhanced by Optimal Simulation Allocation), with unprecedented integration of learning, control, and simulation optimization techniques, which can drastically improve the efficiency of policy learning in a cyber-physical system. A proof-of-concept is implemented on a conveyer-switch network, demonstrating how ROSA can be applied for efficient policy learning, with an emphasis on the industrial distributed control system.",https://ieeexplore.ieee.org/document/9249228/,2020 IEEE 16th International Conference on Automation Science and Engineering (CASE),20-21 Aug. 2020,ieeexplore
10.1109/VLSI-DAT.2019.8741637,Embedded Memories for Silicon-In-Package: Optimization of Memory Subsystem from IoT to Machine Learning,IEEE,Conferences,"Traditional memory subsystem consisting of SRAM, DRAM and SSD/HDD has served the needs of electronics industry for decades. With the rapid increase in Graphics, IoT and Machine Learning applications, several new memories have been innovated to optimize the memory hierarchy. Optane memory is deployed to close the performance/area gap between DRAM and SSD. Similarly, embedded DRAM inserted in products as L4 Cache to close the gap between SRAM and DRAM. Lately, MRAM and ReRAM are also brought to reality to target wide range of applications, covering embedded Non-Volatile Memory and Flash buffer at platform level. This talk will go through these innovate memories and how one needs to optimize the memory subsystem for the best application. There's no memory that fits all, but design and architecture opportunities exist for targeted applications.",https://ieeexplore.ieee.org/document/8741637/,"2019 International Symposium on VLSI Design, Automation and Test (VLSI-DAT)",22-25 April 2019,ieeexplore
10.1109/VLSI-TSA.2019.8804633,Embedded Memories for Silicon-ln-Package: Optimization of Memory Subsystem from loT to Machine Learning,IEEE,Conferences,"Traditional memory subsystem consisting of SRAM, DRAM and SSD/HDD has served the needs of electronics industry for decades. With the rapid increase in Graphics, loT and Machine Learning applications, several new memories have been innovated to optimize the memory hierarchy. Optane memory is deployed to close the performance/area gap between DRAM and SSD. Similarly, embedded DRAM inserted in products as L4 Cache to close the gap between SRAM and DRAM. Lately, MRAM and ReRAM are also brought to reality to target wide range ofapplications, covering embedded Non-Volatile Memory and Flash buffer at platform level. This talk will go through these innovate memories and how one needs to optimize the memory subsystem for the best application. There's no memory that fits all, but design and architecture opportunities exist for targeted applications.",https://ieeexplore.ieee.org/document/8804633/,"2019 International Symposium on VLSI Technology, Systems and Application (VLSI-TSA)",22-25 April 2019,ieeexplore
10.1109/BigData.2017.8258076,Empirical evaluations of active learning strategies in legal document review,IEEE,Conferences,"One type of machine learning, text classification, is now regularly applied in the legal matters involving voluminous document populations because it can reduce the time and expense associated with the review of those documents. One form of machine learning - Active Learning - has drawn attention from the legal community because it offers the potential to make the machine learning process even more effective. Active Learning, applied to legal documents, is considered a new technology in the legal domain and is continuously applied to all documents in a legal matter until an insignificant number of relevant documents are left for review. This implementation is slightly different than traditional implementations of Active Learning where the process stops once achieving acceptable model performance. The purpose of this paper is twofold: (i) to question whether Active Learning actually is a superior learning methodology and (ii) to highlight the ways that Active Learning can be most effectively applied to real legal industry data. Unlike other studies, our experiments were performed against large data sets taken from recent, real-world legal matters covering a variety of areas. We conclude that, although these experiments show the Active Learning strategy popularly used in legal document review can quickly identify informative training documents, it becomes less effective over time. In particular, our findings suggest this most popular form of Active Learning in the legal arena, where the highest-scoring documents are selected as training examples, is in fact not the most efficient approach in most instances. Ultimately, a different Active Learning strategy may be best suited to initiate the predictive modeling process but not to continue through the entire document review.",https://ieeexplore.ieee.org/document/8258076/,2017 IEEE International Conference on Big Data (Big Data),11-14 Dec. 2017,ieeexplore
10.1109/EuCNC/6GSummit51104.2021.9482590,Empowering Industry 4.0 and Autonomous Drone Scouting use cases through 5G-DIVE Solution,IEEE,Conferences,"The 5G Edge Intelligence for Vertical Experimentation (5G-DIVE) project aims at demonstrating the technical merits and business value proposition of 5G technologies in two vertical pilots, namely the Industry 4.0 (I4.0) and Autonomous Drones Scout (ADS) pilots. This paper presents an overview of the overall 5G-DIVE solution and reports the results of the initial validation campaign of the selected use case, featuring 5G connectivity, distributed Edge computing, and artificial intelligence. The initial results for the I4.0 provide a baseline for next step validation campaign targeting a broader scale 5G implementation, while the ADS results provides promising results for enhancing the autonomous navigation in real-time.",https://ieeexplore.ieee.org/document/9482590/,2021 Joint European Conference on Networks and Communications & 6G Summit (EuCNC/6G Summit),8-11 June 2021,ieeexplore
10.1109/VLSITechnology18217.2020.9265031,Empowering Next-Generation Applications through FLASH Innovation,IEEE,Conferences,"The flash industry has continuously produced game-changing innovations in density, latency, and form factors resulting in large cost-performance benefits. To address the wide spectrum of storage demands coming from phone/IoT devices, mobile compute, up to data centers, new flash architectures are essential to handle these next generation applications. Future technology must include not only new architectures and more layers in flash chip designs, but also a roadmap for QLC flash and beyond, new memories, new classes of SSDs, and new software technologies. They must all come together to enable and accelerate the next wave of applications including the real-time analytics, AI (Artificial Intelligence)/ML (Machine Learning), high-performance computing, IoT, and virtual and augmented reality.",https://ieeexplore.ieee.org/document/9265031/,2020 IEEE Symposium on VLSI Technology,16-19 June 2020,ieeexplore
10.1109/CCGRID.2017.22,Energy Model for Low-Power Cluster,IEEE,Conferences,"Energy efficiency in high performance computing (HPC) systems is a relevant issue nowadays, which is approached from multiple edges and components (network, I/O, resource management, etc). HPC industry turned its focus towards embedded and low-power computational infrastructures (of RISC architecture processors) to improve energy efficiency, therefore, we use an ARM-based cluster, known as millicluster, designed to achieve high energy efficiency with low power. We provide a model for energy consumption estimation based on experimental data, obtained of measurements performed during a benchmarking process that represents a real-world workload, such as scientific computing algorithms of artificial intelligence. The energy model enables power prediction of tasks in low-power nodes with high accuracy, and its implementation in a job scheduling algorithm of HPC, facilitates the optimization of energy consumption and performance metrics at the same time.",https://ieeexplore.ieee.org/document/7973809/,"2017 17th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing (CCGRID)",14-17 May 2017,ieeexplore
10.1109/ICE/ITMC49519.2020.9198492,Enhancing Cognition for Digital Twins,IEEE,Conferences,"In the era of Industry 4.0, Digital Twins (DTs) pave the way for the creation of the Cognitive Factory. By virtualizing and twinning information stemming from the real and the digital world, it is now possible to connect all parts of the production process by having virtual copies of physical elements interacting with each other in the digital and physical realms. However, this alone does not imply cognition. Cognition requires modelling not only the physical characteristics but also the behavior of production elements and processes. The latter can be founded upon data-driven models produced via Data Analytics and Machine Learning techniques, giving rise to the so-called Cognitive (Digital) Twin. To further enable the Cognitive Factory, a novel concept, dubbed as Enhanced Cognitive Twin (ECT), is proposed in this paper as a way to introduce advanced cognitive capabilities to the DT artefact that enable supporting decisions, with the end goal to enable DTs to react to inner or outer stimuli. The Enhanced Cognitive Twin can be deployed at different hierarchical levels of the production process, i.e., at sensor-, machine-, process-, employee- or even factory-level, aggregated to allow both horizontal and vertical interplay. The ECT notion is proposed in the context of process industries, where cognition is particularly important due to the continuous, non-linear, and varied nature of the respective production processes.",https://ieeexplore.ieee.org/document/9198492/,"2020 IEEE International Conference on Engineering, Technology and Innovation (ICE/ITMC)",15-17 June 2020,ieeexplore
10.1109/BigData.2018.8622583,Ensemble Machine Learning Systems for the Estimation of Steel Quality Control,IEEE,Conferences,"Recent advances in the steel industry have encountered challenges in soliciting decision making solutions for quality control of products based on data mining techniques. In this paper, we present a steel quality control prediction system encompassing with real-world data as well as comprehensive data analysis results. The core process is cautiously designed as a regression problem, which is then best handled by grouping various learning algorithms with their massive resource of historical production datasets. The characteristics of the currently most popular learning models used in regression problem analysis are as well investigated and compared. The performance indicates our steel quality control prediction system based on ensemble machine learning model can offer promising result whilst delivering high usability for local manufacturers to address the production problem by aid of development of machine learning techniques. Furthermore, real-world deployment of this system is demonstrated and discussed. Finally, future directions and the performance expectation are pointed out.",https://ieeexplore.ieee.org/document/8622583/,2018 IEEE International Conference on Big Data (Big Data),10-13 Dec. 2018,ieeexplore
10.1109/21CW48944.2021.9532522,Ergonomics of Human Machine Integration in Variable Autonomy,IEEE,Conferences,"“Human technologies are made by humans, for humans”. In recent days pairing people with the system is getting easier. The systems and tools we use are becoming increasingly intelligent and more interconnected with autonomous behavior giving birth to cyber physical systems. The advances in the miniaturization of computation makes our tool behave intelligent using Artificial Intelligence. This intelligence in the form of a software where the inputs are taken from entities of real-time systems. The ultimate goal of the future research should be to emulate the functions of human-human and human-autonomy teams directly and evaluate their joint performance and contributions. Armed with this approach and existing technologies we can uncover novel approaches in Industry 4.0 and this paper ends with the overview of human-machine autonomy and ergonomics at variable autonomy.",https://ieeexplore.ieee.org/document/9532522/,2021 IEEE Conference on Norbert Wiener in the 21st Century (21CW),22-25 July 2021,ieeexplore
10.1109/CCWC51732.2021.9375834,Evaluation of Machine Learning Architectures in Healthcare,IEEE,Conferences,"Machine Learning (ML) is now influencing every part of the industry. From detecting objects from an image to recommending different items while doing online shopping based on someone's recent browsing history. Now, the ML is touching the healthcare sector. It is now an area of interest for more doctors and scientists to implement different techniques and harvest the power of ML. Over the past few years, there is a race to implement Artificial Intelligence (AI) and ML in this sector. Multiple scholars have presented their approach. Currently, the proposed models are nowhere near the actual implementation of these models in the real world. However, these models are laying down the path to do so in the future. Here is a review of some of the papers discussing different techniques for targeting various diseases using AI/ML. Each paper introduces a method developed based on the separate datasets created from organically collected data.",https://ieeexplore.ieee.org/document/9375834/,2021 IEEE 11th Annual Computing and Communication Workshop and Conference (CCWC),27-30 Jan. 2021,ieeexplore
10.1109/MDM.2017.48,Experimental Study of Telco Localization Methods,IEEE,Conferences,"Telecommunication (Telco) localization is a technique to accurately locate mobile devices (MDs) using measurement report (MR) data, and has been widely used in Telco industry. Many techniques have been proposed, including measurement-based statistical algorithms, fingerprinting algorithms and different machine learning-based algorithms. However, it has not been well studied yet on how these algorithms perform on various Telco MR data sets. In this paper, we conduct a comprehensive experimental study of five state-of-art algorithms for Telco localization. Based on real data sets from two Telco networks, we study the localization performance of such algorithms. We find that a Random Forest-based machine learning algorithm performs best in most experiments due to high localization accuracy and insensitivity to data volume. The experimental result and observation in this paper may inspire and enhance future research in Telco localization.",https://ieeexplore.ieee.org/document/7962466/,2017 18th IEEE International Conference on Mobile Data Management (MDM),29 May-1 June 2017,ieeexplore
10.1109/WSC.1989.718775,Expert Simulation For On-line Scheduling,IEEE,Conferences,"In recent years, the automotive industry has realized the importance of speed of new products to market and has mounted efforts for improving it. The Expert System Scheduler (ESS) facilitates these efforts by enabling manufacturing plants to generate viable schedules under increasing constraints and demands for flexibility. The scheduler takes advantage of the Computer Integrated Manufacturing (CIM) environment by utilizing the real-time information from the factory for responsive scheduling. The Expert System Scheduler uses heuristics developed by an experiences factory scheduler. It uses simulation concepts and these heuristics to generate schedules. Forward and ""backward"" simulation are used at different stages of the schedule generation process. The system is used to control parts flow on the factory floor at one automated facility. This highly automated facility is a testbed for implementation of CIM concepts. The scheduler runs on a Texas Instruments (TI) Explorer II computer using software developed inhouse utilizing IntelliCorp's Knowledge Engineering Environment (KEE) shell and the LISP language. The scheduling computer is networked to the factory control computer, which actually controls the plant floor. The TI Explorer II acquires current plant floor information from the factory control system, generates a new schedule and sends it back within a short time. The configuration allows fast response to changes in requirements and plant floor conditions.",https://ieeexplore.ieee.org/document/718775/,1989 Winter Simulation Conference Proceedings,4-6 Dec. 1989,ieeexplore
10.1109/SMC.2019.8913901,Explainable Machine Learning in Industry 4.0: Evaluating Feature Importance in Anomaly Detection to Enable Root Cause Analysis,IEEE,Conferences,"In the past recent years, Machine Learning methodologies have been applied in countless application areas. In particular, they play a key role in enabling Industry 4.0. However, one of the main obstacles to the diffusion of Machine Learning-based applications is related to the lack of interpretability of most of these methods. In this work, we propose an approach for defining a `feature importance' in Anomaly Detection problems. Anomaly Detection is an important Machine Learning task that has an enormous applicability in industrial scenarios. Indeed, it is extremely relevant for the purpose of quality monitoring. Moreover, it is often the first step towards the design of a Machine Learning-based smart monitoring solution because Anomaly Detection can be implemented without the need of labelled data. The proposed feature importance evaluation approach is designed for Isolation Forest, one of the most commonly used algorithm for Anomaly Detection. The efficacy of the proposed method is tested on synthetic and real industrial datasets.",https://ieeexplore.ieee.org/document/8913901/,"2019 IEEE International Conference on Systems, Man and Cybernetics (SMC)",6-9 Oct. 2019,ieeexplore
10.1109/ICUAS48674.2020.9214045,Extensions of the open-source framework Aerostack 3.0 for the development of more interactive flights between UAVs,IEEE,Conferences,"The basis for properly verified R&amp;D works is to provide reliable prototyping tools at three most important stages: computer simulation, laboratory tests and real-world experiments. In the laboratory-limited conditions, particular importance is attributed to the first two stages, especially in the context of the safety development of autonomous flights of unmanned aerial vehicle (UAV) groups in various missions. The open-source framework Aerostack support those needs and its effectiveness has been proven in the International Micro Air Vehicle Indoor Competitions (IMAV 2013, 2016, 2017) and Mohammed Bin Zayed International Robotics Challenge (MBZIRC 2020). In the paper, the exemplary functionalities for the new version of Aerostack Version 3.0 Distribution Sirocco (Aerial robotics framework for the industry), extended additionally with a library of new behaviors, are presented. The mission of UAVs can be developed fast and effectively in order to conduct test flights with real drones in lab, before one will decide to fly autonomously outdoor. The representative results obtained for low-cost AR.Drone 2.0 UAV models in two missions, are presented. The first mission is autonomous patrolling the area by a pair of UAVs, the second - intercepting the intruder in guarded area by the guard UAV.",https://ieeexplore.ieee.org/document/9214045/,2020 International Conference on Unmanned Aircraft Systems (ICUAS),1-4 Sept. 2020,ieeexplore
10.1109/ICICT50521.2020.00032,Face Recognition Techniques using Statistical and Artificial Neural Network: A Comparative Study,IEEE,Conferences,"Face recognition is the process of identifying a person by their facial characteristics from a digital image or a video frame. Face recognition has extensive applications and there will be a massive development in future technologies. The main contribution of this research is to perform a comparative study between different statistical-based face recognition techniques, namely: Eigen-faces, Fisher-faces, and Local Binary Patterns Histograms (LBPH) to measure their effectiveness and efficiency using real-database images. These recognizers still used on top of commercial face recognition products. Additionally, this research is comprehensively comparing 17 face-recognition techniques adopted in research and industry that use artificial-neural network, criticize and categories them into an understandable category. Also, this research provides some directions and suggestions to overcome the direct and indirect issues for face recognition. It has found that there is no existing recognition method that the community of face recognition has agreed on and solves all the issues that face the recognition, such as different pose variation, illumination, blurry and low-resolution images. This study is important to the recognition communities, software companies, and government security officials. It has a direct impact on drawing clear path for new face recognition propositions. This study is one of the studies with respect to the size of its reviewed approaches and techniques.",https://ieeexplore.ieee.org/document/9092128/,2020 3rd International Conference on Information and Computer Technologies (ICICT),9-12 March 2020,ieeexplore
10.1109/ICS51289.2020.00088,Feature Selection for Malicious Traffic Detection with Machine Learning,IEEE,Conferences,"The network technology plays an important role in the emerging industry 4.0. Industrial control systems (ICS) are related to all aspects of human life and have become the target of cyber-attackers. Attacks on ICS may not only cause economic loss, but also damage equipment and hurt staff. The biggest challenges in establishing a secure network communication system is how to effectively detect and prevent malicious network behavior. A Network Intrusion Detection System (NIDS) can be deployed as a defense mechanism for cyberattacks. However, for industrial internet-of-things (IIoT) applications with limited computing resources, designing an effective NIDS is challenging. In this paper, we propose to use machine learning as the core technology to build a compact and effective NIDS for IIoT. The proposed method is validated by using the more recent UNSW-NB 15 dataset to improve the detection capability against new types of attacks in the real world. Furthermore, we demonstrate that the method is also valid for traditional KDD-CUP-99 dataset. Experimental results show that the proposed method achieves better performance than previous methods.",https://ieeexplore.ieee.org/document/9359069/,2020 International Computer Symposium (ICS),17-19 Dec. 2020,ieeexplore
10.1109/APSEC51365.2020.00047,Federated Learning Systems: Architecture Alternatives,IEEE,Conferences,"Machine Learning (ML) and Artificial Intelligence (AI) have increasingly gained attention in research and industry. Federated Learning, as an approach to distributed learning, shows its potential with the increasing number of devices on the edge and the development of computing power. However, most of the current Federated Learning systems apply a single-server centralized architecture, which may cause several critical problems, such as the single-point of failure as well as scaling and performance problems. In this paper, we propose and compare four architecture alternatives for a Federated Learning system, i.e. centralized, hierarchical, regional and decentralized architectures. We conduct the study by using two well-known data sets and measuring several system performance metrics for all four alternatives. Our results suggest scenarios and use cases which are suitable for each alternative. In addition, we investigate the trade-off between communication latency, model evolution time and the model classification performance, which is crucial to applying the results into real-world industrial systems.",https://ieeexplore.ieee.org/document/9359305/,2020 27th Asia-Pacific Software Engineering Conference (APSEC),1-4 Dec. 2020,ieeexplore
10.1109/ICCST50977.2020.00021,Food object recognition and intelligent billing system based on Cascade R-CNN,IEEE,Conferences,"With the development of information technology and artificial intelligence, using science and technology to change the low efficiency of the catering industry is a very effective means. The existing system of food identification and intelligent billing in the market includes artificial billing, RFID induction, photo recognition, etc. Based on the Cascade R-CNN algorithm and computer vision technology, this paper proposes an intelligent food identification and intelligent billing system. First, the database is created for algorithm training, then the mobile device is used to collect the food image, and the depth neural network is used to identify the food in the image. Finally, the price calculation result of each is found and returned to the user. In this paper, the basic principle and implementation method of the system are described in detail, and the experimental phenomenon is analyzed. The experimental results show that the system has good accuracy.",https://ieeexplore.ieee.org/document/9262826/,2020 International Conference on Culture-oriented Science & Technology (ICCST),28-31 Oct. 2020,ieeexplore
10.1109/R10-HTC.2018.8629835,Foody - Smart Restaurant Management and Ordering System,IEEE,Conferences,"Customers play a vital role in the contemporary food industry when determining the quality of the restaurant and its food. Restaurants give considerable attention to customers' feedback about their service, since the reputation of the business depends on it. Key factors of evaluating customer satisfaction are, being able to deliver the services effectively to lessen the time of consumption, as well as maintaining a high quality of service. In most cases of selecting a prominent restaurant, customers focus on their choice of favorite food in addition to available seating and space options. Long waiting times and serving the wrong order is a common mistake that happens in every restaurant that eventually leads to customer dissatisfaction. Objectives of this online application “Foody” is to address these deficiencies and provide efficient and accurate services to the customer, by providing unique menus to each customer considering their taste. This concept is implemented as a mobile application using latest IT concepts such as Business Intelligence, Data Mining, Predictive Analysis and Artificial Intelligence. This includes graphics and 3D modeling that provide existent physical information related to food such as colors, sizes and further user can view the ingredients of the meal as well as the available tables. In addition, the app shows the real-time map to the restaurant. Current table reservation status is indicated by the color change of the table. Unique food recommendation and it's order for each customer is generated by analyzing their social media information and the system notifies the customer the wait time by calculating it. Preparation of food and allocation is done subjectively. The expected outcome of the research is to develop a fully automated restaurant management system with the mentioned features as well as to avoid confusions between orders, provide better view of food and allow the customer to choose the menu according to their taste in a minimum time.",https://ieeexplore.ieee.org/document/8629835/,2018 IEEE Region 10 Humanitarian Technology Conference (R10-HTC),6-8 Dec. 2018,ieeexplore
10.1109/UIC-ATC.2017.8397484,Forecasting car rental demand based temporal and spatial travel patterns,IEEE,Conferences,"Recent years, shared mobility services have gained momentum across the world. Meanwhile, rental car industry has seen great developments in China and has reached a scale of economy. Knowing the rental behavior pattern and forecasting the demand become more important for rental businesses. To this end, in this paper, we aim to analyze the rental mobility pattern by examining multiple factors in a holistic manner. A special goal is to predict the demand of a given region. Specifically, we first analyze regular mobility based on real trips of rental cars. Then, we extract key features from multiple types of rental-related data, such as rental behavior profiles and geo-social information of regions. Next, based on these features, we develop a multi-task learning based regression approach for predicting rental cars' demand. This approach can effectively learn not only fundamental features but also relationships between regions by considering multiple factors. Finally, we conduct extensive experiments on real-world rental trip data collected in Beijing. The experimental results validate the effectiveness of the proposed approach for forecasting rental demand in the real world.",https://ieeexplore.ieee.org/document/8397484/,"2017 IEEE SmartWorld, Ubiquitous Intelligence & Computing, Advanced & Trusted Computed, Scalable Computing & Communications, Cloud & Big Data Computing, Internet of People and Smart City Innovation (SmartWorld/SCALCOM/UIC/ATC/CBDCom/IOP/SCI)",4-8 Aug. 2017,ieeexplore
10.1109/AIID51893.2021.9456549,Freight positioning technology of high speed railway carriage based on UWB,IEEE,Conferences,"With the rapid development of railway freight transportation industry, it is imperative for information and intelligent logistics management to replace the traditional logistics management technology. This paper proposes the design of high-speed railway material positioning system based on UWB technology. Firstly, the difficulties of UWB implementation in high-speed railway freight positioning are analyzed. In order to solve the characteristics of nonlinear and non Gaussian distribution of multi-path interference, this paper presents a particle filter ranging calibration algorithm for serious multipath interference in train compartment, which has serious multipath interference, processes 25 sets of ranging data with particle filter. Finally, by arranging 6 base stations in the high-speed railway carriage, the multi-base station weighted least square method is used to locate the information of the material location in the carriage. The feasibility of the system is verified by comparing with the real location.",https://ieeexplore.ieee.org/document/9456549/,2021 IEEE International Conference on Artificial Intelligence and Industrial Design (AIID),28-30 May 2021,ieeexplore
10.1109/CTIT.2018.8649493,Game Theoretic Approach for Applying Artificial Intelligence in the Credit Industry,IEEE,Conferences,"The law of accelerating returns can be viewed as a concept that describes acceleration of technological progress. The idea is that tools are used for developing more advanced tools that are applied for creating even more advanced tools etc. A similar idea has been implemented in algorithms for advancing artificial intelligence. In this paper, the results of applying these algorithms in games are discussed. Nevertheless, real life tasks seem more complicated. The game theoretic approach can be applied for transition from theoretical and unrealistic games to more complex and practical tasks. Applications of the game theoretic approach to advance artificial intelligence in solving tasks in the credit industry are proposed.",https://ieeexplore.ieee.org/document/8649493/,2018 Fifth HCT Information Technology Trends (ITT),28-29 Nov. 2018,ieeexplore
10.1109/WCMEIM52463.2020.00032,Garbage Classification and Recognition Based on SqueezeNet,IEEE,Conferences,"In this study, an intelligent garbage classification and recognition system was deployed on the industry integrated computer with the I3-7100U processor and 2G memory. Considering the unit prediction time and prediction accuracy, SqueezeNet was selected as the classification network training model among ResNet, InceptionV3, and SqueezeNet. The pretraining SqueezeNet network model on the ImageNet-1000 dataset was used for transfer learning, and the model predication accuracy was improved by using image enhancement and Adam optimizer. The comparison between the comprehensive test set and the real garbage image showed that the model predication accuracy reached 87.7% after training, and the prediction time in the industrial integrated machine was less than 2 seconds, which met the needs of practical applications.",https://ieeexplore.ieee.org/document/9409502/,2020 3rd World Conference on Mechanical Engineering and Intelligent Manufacturing (WCMEIM),4-6 Dec. 2020,ieeexplore
10.1109/ICNSC.2006.1673254,General Methodology for Action-Oriented Industrial Ecology Complex Systems Approach Applied to the Rotterdam Industrial Cluster,IEEE,Conferences,"A new approach for the understanding and shaping of the evolution of large scale socio-technical systems is presented. A proof-of-concept knowledge application has been developed, based on the industrial Rotterdam-Rijnmond case. The knowledge application includes the design of a model of industry-infrastructure evolution. Such networks are modeled via a system decomposition, formalization in an ontology and implementation of an agent based model. In simulation runs several network metrics are presented. The results provide insights in real world system behavior and show the validity and potential of the approach",https://ieeexplore.ieee.org/document/1673254/,"2006 IEEE International Conference on Networking, Sensing and Control",23-25 April 2006,ieeexplore
10.1109/AI4I.2018.8665690,Genetic Algorithm Based Parallelization Planning for Legacy Real-Time Embedded Programs,IEEE,Conferences,"Multicore platforms are pervasively deployed in many different sectors of industry. Hence, it is appealing to accelerate the execution through adapting the sequential programs to the underlying architecture to efficiently utilize the hardware resources, e.g., the multi-cores. However, the parallelization of legacy sequential programs remains a grand challenge due to the complexity of the program analysis and dynamics of the runtime environment. This paper focuses on parallelization planning in that the best parallelization candidates would be determined after the parallelism discovery in the target large sequential programs. In this endeavor, a genetic algorithm based method is deployed to help find an optimal solution considering different aspects from the task decomposition to solution evaluation while achieving the maximized speedup. We have experimented the proposed approach on industrial real time embedded application to reveal excellent speedup results.",https://ieeexplore.ieee.org/document/8665690/,2018 First International Conference on Artificial Intelligence for Industries (AI4I),26-28 Sept. 2018,ieeexplore
10.1109/ICCCI.2018.8441350,Genetic Approach based Bug Triage for Sequencing the Instance and Features,IEEE,Conferences,"In software industry analyzing bug by various tester and developer is a costly approach. So collecting these bug reports and triage is done manually which consume time with high rate of error. Here proposed work has focus on this triage of the bug reports by reducing the dataset size. In order to reduce cost of bug triage proper sequencing of the instance and feature selection is done. Here instance and feature selection are clustered by using list of words, keywords and bug id as fitness function parameters. Two stage learning genetic algorithm named as teacher learning based optimization was used for clustering. As genetic algorithms are unsupervised learning approach, so new set bug report triage is adopt by the proposed work. Experiment is done on real dataset of bug reports. Result shows that proposed work is better on precision value by 38.5% while execution time was reduce by 29.2% as compared with existing procedures.",https://ieeexplore.ieee.org/document/8441350/,2018 International Conference on Computer Communication and Informatics (ICCCI),4-6 Jan. 2018,ieeexplore
10.1109/CDC.2004.1428748,Grey-box modelling of a motorcycle shock absorber,IEEE,Conferences,"There is an increasing use of virtual prototyping tools in the motorcycle industry, aimed at reducing the development time of new models and speeding up performance optimization, by providing the designer with an in-laboratory virtual test track. Virtual prototyping software is essentially multi body simulation software that requires the availability of models of all the vehicle components. The choice of the model is then of paramount importance, since it heavily affects the accuracy and reliability of the simulation results. Conventional models (like linear models) are often inadequate to describe the behavior of complex nonlinear components, so that it is necessary to appeal to different modelling approaches. This is actually the case when dealing with motorcycle suspension systems, given that their most critical part, the shock absorber, exhibits nonlinear and time-variant behavior. In this paper, a grey-box model of a racing motorcycle mono tube shock absorber is proposed. It consists of a nonlinear parametric model and a black-box, neural network based model. The absorber model has been implemented in a numerical simulation environment, and it has been validated against experimental test data. The results of the validation show that the model is able to reproduce the real behavior of the shock absorber with an accuracy that matches or even beats that of other models previously presented in the literature. The interfacing of the proposed model to the ADAMS virtual prototyping environment is also discussed.",https://ieeexplore.ieee.org/document/1428748/,2004 43rd IEEE Conference on Decision and Control (CDC) (IEEE Cat. No.04CH37601),14-17 Dec. 2004,ieeexplore
10.1109/ICSCC51209.2021.9528256,Helmet Detection Using Faster Region-Based Convolutional Neural Networks and Single-Shot MultiBox Detector,IEEE,Conferences,"In a country like India, with excessive population density in all big cities, motorcycles have become dominant modes of transport. It is observed that most motorcyclists avoid wearing helmets despite it being an indispensable safety equipment, whose use can significantly reduce the risk of severe head and brain injuries during accidents. Due to violations of most of the traffic and safety rules, motorcycle accidents have been skyrocketing in the recent years. Hence, it’s the need of the hour to build an effective and scalable system capable of automatic helmet detection by analyzing the surveillance camera’s traffic videos. Although several theoretical deep learning-based models have been proposed to detect helmets for the traffic surveillance aspect, an optimal solution for the industry application is less discussed. This paper demonstrates a novel implementation of the Faster R-CNN and SSD framework for accurate helmet detection in real-time low-quality surveillance videos. The experimental results claim that there is a trade-off between accuracy and execution speed. We also present a comprehensive comparative analysis of the two algorithms and determine the best real-time use case scenarios for each of them.",https://ieeexplore.ieee.org/document/9528256/,2021 8th International Conference on Smart Computing and Communications (ICSCC),1-3 July 2021,ieeexplore
10.1109/ASPDAC.2011.5722294,High performance lithographic hotspot detection using hierarchically refined machine learning,IEEE,Conferences,"Under real and continuously improving manufacturing conditions, lithography hotspot detection faces several key challenges. First, real hotspots become less but harder to fix at post-layout stages; second, false alarm rate must be kept low to avoid excessive and expensive post-processing hotspot removal; third, full chip physical verification and optimization require fast turn-around time. To address these issues, we propose a high performance lithographic hotspot detection flow with ultra-fast speed and high fidelity. It consists of a novel set of hotspot signature definitions and a hierarchically refined detection flow with powerful machine learning kernels, ANN (artificial neural network) and SVM (support vector machine). We have implemented our algorithm with industry-strength engine under real manufacturing conditions in 45nm process, and showed that it significantly outperforms previous state-of-the-art algorithms in hotspot detection false alarm rate (2.4X to 2300X reduction) and simulation run-time (5X to 237X reduction), meanwhile archiving similar or slightly better hotspot detection accuracies. Such high performance lithographic hotspot detection under real manufacturing conditions is especially suitable for guiding lithography friendly physical design.",https://ieeexplore.ieee.org/document/5722294/,16th Asia and South Pacific Design Automation Conference (ASP-DAC 2011),25-28 Jan. 2011,ieeexplore
10.15439/2017F253,Human machine synergies in intra-logistics: Creating a hybrid network for research and technologies,IEEE,Conferences,"The purpose of the article is to outline the futuristic vision of Industry 4.0 in intra-logistics by creating a hybrid network for research and technologies thereby providing a detailed account on the research centre, available technologies and their possibilities for collaboration. Scientific challenges in the field of Industry 4.0 and intra-logistics are identified due to the new form of interaction between humans and machines. This kind of collaboration provides new possibilities of materials handling that can be developed with the support of real-time motion data tracking and virtual reality systems. These services will be provided by a new research centre for flexible human-machine cooperation networks in Dortmund. By the use of various reference and experiment systems various real-time scenarios can be emulated including digital twin simulation concepts. Big data emerges as an important paradigm in this research project where all systems are made flexible in terms of networking for all the systems to consume the data produced and also to combine all the data to arrive at new insights using concepts from machine learning and deep learning networks. This leads to the challenge of finding a common syntax for inter-operating systems. This paper describes the design and deployment strategies of research centre with the possibilities and the design insights for a futuristic Industry 4.0 material handling facility.",https://ieeexplore.ieee.org/document/8104684/,2017 Federated Conference on Computer Science and Information Systems (FedCSIS),3-6 Sept. 2017,ieeexplore
10.1109/SMC42975.2020.9283392,Human-in-the-Loop Error Precursor Detection using Language Translation Modeling of HMI States,IEEE,Conferences,"Situational Awareness (SA) is paramount to ensuring operational safety in Nuclear Power Plant (NPP) and Commercial aviation industry. An increase in Human-in-the-loop (HITL) error rate may be indicative of reduced operator SA while undermining safety. In this paper, natural language processing (NLP) is applied for modelling industrial Human Machine Interface (HMI) state transitions as a means to detect operator HITL error precursors in real-time. A custom seq2seq encode-decoder deep-learning model design is implemented and evaluated using real-plant scenario dataset obtained from a NPP Operator training simulator. Results support NLP HMI state model may be employed to detect HITL error precursor within the desired N time-steps prior to an accident event.",https://ieeexplore.ieee.org/document/9283392/,"2020 IEEE International Conference on Systems, Man, and Cybernetics (SMC)",11-14 Oct. 2020,ieeexplore
10.1109/ICISCE48695.2019.00071,Hydropower Generation Forecasting via Deep Neural Network,IEEE,Conferences,"With the advances of deep learning, its applications in our daily life have attracted considerable attention from both academics and industry. However, most of the existing works focus on computer vision and natural language processing, while few studies in the real industrial manufactures. The main reasons are that the data resources are difficult to obtain and the relationships between industrial data are too complex to be modeled. In this paper, we propose a deep neural network based approach for hydroelectric power generation prediction, which, to our knowledge, is the first attempt modeling power generation data with the combination of residual neural networks and recurrent neural networks. Furthermore, we consider different grains of the hydropower generation by dividing the data into four-levels, i.e. recent, daily, weekly, and time-series sequences, which can greatly improve the prediction performance. To this end, we employ a multi-information fusion method to fuse the four components (i.e. closeness, period, trend, long-period) predicted results, among which different component is learned with different weights to determine their influence on final hydropower prediction. Experiments conducting on the real hydropower generation prove the effectiveness of the proposed model, which significantly outperform the baselines. We hope this research will open a new perspective of improving data usage in the industry, especially in power generation areas.",https://ieeexplore.ieee.org/document/9107696/,2019 6th International Conference on Information Science and Control Engineering (ICISCE),20-22 Dec. 2019,ieeexplore
10.1109/PerComWorkshops48775.2020.9156250,ID Sequence Analysis for Intrusion Detection in the CAN bus using Long Short Term Memory Networks,IEEE,Conferences,"The number of computer controlled vehicles throughout the world is rising at a staggering speed. Even though this enhances the driving experience, it opens a new security hole in the automotive industry. To alleviate this issue, we are proposing an intrusion detection system (IDS) to the controller area network (CAN), which is the de facto communication standard of present-day vehicles. We implemented an IDS based on the analysis of ID sequences. The IDS uses a trained Long-Short Term Memory (LSTM) to predict an arbitration ID that will appear in the future by looking back to the last 20 packet arbitration IDs. The output from the LSTM network is a softmax probability of all the 42 arbitration IDs in our test car. The softmax probability is used in two approaches for IDS. In the first approach, a single arbitration ID is predicted by taking the class which has the highest softmax probability. This method only gave us an accuracy of 0.6. Applying this result in a real vehicle would give us a lot of false negatives, hence we devised a second approach that uses log loss as an anomaly signal. The evaluated log loss is compared with a predefined threshold to see if the result is in the anomaly boundary. Furthermore, We have tested our approach using insertion, drop and illegal ID attacks which greatly outperform the conventional method with practical F1 scores of 0.9, 0.84, and 1.0 respectively.",https://ieeexplore.ieee.org/document/9156250/,2020 IEEE International Conference on Pervasive Computing and Communications Workshops (PerCom Workshops),23-27 March 2020,ieeexplore
10.1109/ICETAS.2018.8629202,IOS Mobile Application for Food and Location Image Prediction using Convolutional Neural Networks,IEEE,Conferences,"Machine Learning is a popular research area in software industry alongside with big data, micro services, virtual reality, and augmented reality. With the recent developments in improving computing capacity, deep learning approaches such as Convolutional Neural Networks (CNN) has become the trendiest topic in machine learning for image recognition. In this paper, we have developed an IOS application for food image recognition using modified CNN models. In particular, we developed an IOS mobile application by converting the models to CoreML and then using them within the IOS application for food image recognition. After fine-tuning a pre-trained Google InceptionV3 model, we were able to achieve 82.03% Top-1 accuracy on the test set using a single crop per item. Using 10 crops per example and taking the most frequent predicted class(es), we were able to achieve 86.97% Top-1 Accuracy and 97.42% Top-5 Accuracy.",https://ieeexplore.ieee.org/document/8629202/,2018 IEEE 5th International Conference on Engineering Technologies and Applied Sciences (ICETAS),22-23 Nov. 2018,ieeexplore
10.1109/COMPSAC.2018.00073,IT Professional 20th Anniversary Panel: The New Realities of AI,IEEE,Conferences,"Summary form only given, as follows. A complete record of the panel discussion was not made available for publication as part of the conference proceedings. Artificial intelligence (AI) is now creating a lot of excitement and hype among professionals and across all kinds of business and industry, as well as among individuals. It is no longer just the theme of science fiction essays and movies. It is emerging as new, innovative approach for solving challenging problems that we encounter in practice, and as an enabler of disruptive innovations and smarter world. AI’s renaissance is driven by recent complementary developments, including major advances in the AI arena, realistic expectations, and recent success in its applications. AI is also raising some major concerns, real and perceived. Nevertheless, AI is trending to become a new normal and is increasingly being adopted in many applications despite its concerns and limitations. This panel will examine the new realities of AI and offer its perspectives and recommendations. It’ll deliberate on: Where is AI headed? What new applications and innovations will emerge, and how they might impact? What are the risks and concerns? How we can leverage AI for good, and address its major risks and concerns? How do we get prepared for the new AI age?",https://ieeexplore.ieee.org/document/8377698/,2018 IEEE 42nd Annual Computer Software and Applications Conference (COMPSAC),23-27 July 2018,ieeexplore
10.1109/VRAIS.1993.380764,Image generation implications for networked tactical training systems,IEEE,Conferences,"Image generation (IG) computers for networked simulation and training systems require additional capabilities beyond those IG's employs in traditional forms of realtime simulation. The specific characteristics associated with the networking and tactical training nature of ground and near-ground vehicle training applications are reviewed. The IG implications discussed include computational loading, scene management, advanced graphics techniques, databases, interoperability, interfaces, and required support calculations. It is recommended that an industry standard benchmark to specify, evaluate, and verify the performance of this type of image generator is established.&lt;<ETX>&gt;</ETX>",https://ieeexplore.ieee.org/document/380764/,Proceedings of IEEE Virtual Reality Annual International Symposium,18-22 Sept. 1993,ieeexplore
10.1109/CIIMA50553.2020.9290302,Implementación de SCADA a través del protocolo MQTT,IEEE,Conferences,"This document describes an implementation of a SCADA system powered by MQTT &amp; OPC-UA protocols and hosted within the Google Cloud Platform system. This combination allows to have integrated, scalable, secure and reliable industrial communications while allowing real-time data acquisition and sensor feed that can then be used in real-time OEE tracking or predictive maintenance models, to name some examples. This in line with the Industry 4.0 initiatives mainly fueled by data and machine learning autonomous systems.",https://ieeexplore.ieee.org/document/9290302/,2020 IX International Congress of Mechatronics Engineering and Automation (CIIMA),4-6 Nov. 2020,ieeexplore
10.1109/COASE.2007.4341740,Implementation Considerations of Various Virtual Metrology Algorithms,IEEE,Conferences,"In the semiconductor industry, run-to-run (R2R) control is an important technique to improve process capability and further enhance the production yield. As the dimension of electronic device shrinks increasingly, wafer-to-wafer (W2W) advanced process control (APC) becomes essential for critical stages. W2W APC needs to obtain the metrology value of each wafer; however, it will be highly time and cost consuming for obtaining actual metrology value of each wafer by physical measurement. Recently, an efficient and cost-effective approach denoted virtual metrology (VM) was proposed to substitute the actual metrology. To implement VM in W2W APC, both conjecture-accuracy and real-time requirements need to be considered. In this paper, various VM algorithms of back-propagation neural network (BPNN), simple recurrent neural network (SRNN) and multiple regression (MR) are evaluated to see whether they can meet the accuracy and real-time requirements of W2W APC or not. The fifth-generation TFT-LCD CVD process is used to test and verify the requirements. Test results show that both one-hidden-layered BPNN and SRNN VM algorithms can achieve acceptable conjecture accuracy and meet the real-time requirements of semiconductor and TFT-LCD W2W APC applications.",https://ieeexplore.ieee.org/document/4341740/,2007 IEEE International Conference on Automation Science and Engineering,22-25 Sept. 2007,ieeexplore
10.1109/ICC.2016.7511226,Implementation and evaluation of adaptive video streaming based on Markov decision process,IEEE,Conferences,"In HTTP-based adaptive streaming systems, media server simply stores video content segmented into a series of small chunks coded in different qualities and sizes. The decision for next chunk's quality level to achieve a high quality viewing experience is left to the client which is a challenging task, especially in mobile environment due to unexpected changes in network bandwidth. Using computer simulations, previous work has demonstrated that Markov decision process (MDP) is very effective for such decision making and that it can reduce video freezing or re-buffering events drastically compared to other methods of adaptation. However, to date there has been no practical implementation and evaluation of MDP-based DASH players. In this work, we extend a publicly available DASH player recently released by DASH industry forum to realise a real DASH player that implements MDP-based video adaptation. We implement two alternative MDP optimisation algorithms, value iteration and Q learning and evaluate their performances in real driving conditions under 300 minutes of video streaming. Our results show that value iteration and Q learning reduce video freezing by a factor of 8 and 11, respectively, compared to the default decision making algorithm implemented in the public DASH player.",https://ieeexplore.ieee.org/document/7511226/,2016 IEEE International Conference on Communications (ICC),22-27 May 2016,ieeexplore
10.1109/LESCPE.2001.941634,Implementation and performance analysis of very short term load forecaster based on the electronic dispatch project in ISO New England,IEEE,Conferences,"With deregulation in the electric power industry, an accurate very short-term load forecasting (VSTLF) function is increasingly important and becomes an integral component of the real-time (spot) market. VSTLF, dealing with the forecast horizon of one to several minutes ahead, is a relatively new division of the load forecasting which is of special importance in resource dispatch function and correction of area control error (ACE). This paper reports upon the implementation and performance analysis of very short term load forecast (VSTLF) in the electronic dispatch project in ISO New England.",https://ieeexplore.ieee.org/document/941634/,LESCOPE 01. 2001 Large Engineering Systems Conference on Power Engineering. Conference Proceedings. Theme: Powering Beyond 2001 (Cat. No.01ex490),11-13 July 2001,ieeexplore
10.1109/AIMS52415.2021.9466068,Implementation of Cloud Based Action Recognition Backend Platform,IEEE,Conferences,"The Internet of Things (IoT) growth are rapidly in various fields such as industry 4.0, smart cities, and smart homes. Implementation of IoT for electronic assistance had been researched to increase the longevity of human life. However, not all IoT implementation as human life assistance provides action recognition monitoring on multiple elderly people, provide information such as real-time action monitoring, and real-time streaming in a mobile application. Therefore, this research intends to create a system that can receive and provide information on each elderly people who registered. The Action Recognition Backend Platform will be working as cloud computing to receive and manage input data from Edge Computing Action Recognition. This platform integrated Deep Learning, Data Analytics, Big Data Warehouse that implemented Extract, Transform, and Load (ETL) methods, communication services with MQTT, and Kafka Streaming Processor. The test result showed that the edge computing action recognition got better model accuracy performance from our last model [1], which can predict with 50,7% accuracy in 0.5 confidence threshold. Moreover, the backend platform had been successfully implemented a simple IoT paradigm and got an average delivery time of MQTT communication at 204ms, for streaming data process took an average delay of 680ms.",https://ieeexplore.ieee.org/document/9466068/,2021 International Conference on Artificial Intelligence and Mechatronics Systems (AIMS),28-30 April 2021,ieeexplore
10.1109/ICPHYS.2019.8780271,Implementation of Industrial Cyber Physical System: Challenges and Solutions,IEEE,Conferences,"The Industry Internet of Things (IIoT) and the Industry Cyber-Physical System (ICPS) for real industry are becoming vitally necessary in the smart manufacturing environment. Very large number of intelligent sensors are being available generating an exploding amount of data. Several issues come with the big data in real industry, including the a grand-scale connected network construction with the data security and access protocol issues, data quality with considerable noise when gathered from industrial factories, efficient data storage, smart interconnection with cloud services, and real-time analytics requirements. This paper proposes an integrated CPS based architecture for smart manufacturing and provides the deployment details, addressing all the potential problems in an appropriate way. It has been successfully implemented in a real industry environment, and won the Best Industry Application of IoT at the BigInsights Data &amp; AI Innovation Awards.",https://ieeexplore.ieee.org/document/8780271/,2019 IEEE International Conference on Industrial Cyber Physical Systems (ICPS),6-9 May 2019,ieeexplore
10.1109/IES53407.2021.9594001,Implementation of SUMO Simulation for Comparison of CVRP,IEEE,Conferences,"With the rapid increase in human density, development, and mobility in urban areas, the need for logistics distribution systems is increasing which is an important part of connecting industries with their consumers. Thus, route planning is an important thing for the industry. Therefore, this paper proposes a comparison of several vehicle routing problems algorithms and test the routes that have been obtained on a simulation system based on real conditions. Our proposed algorithm consists of Mixed Integer Linear Programming (MILP), Clarke-Wright and Reinforcement Learning algorithm using Markov Decision Process. Digital maps, customer data and route planning results will be converted into a SUMO simulation. We compare the performance of the algorithm with parameters consisting of the number of routes, distance traveled, computation time and simulation time. The experimental results show that the MILP algorithm has the best performance with the most optimal route results, but other algorithms have a lower computation time.",https://ieeexplore.ieee.org/document/9594001/,2021 International Electronics Symposium (IES),29-30 Sept. 2021,ieeexplore
10.1109/ICCCWorkshops52231.2021.9538856,Implementation of an intelligent target detection system for edge node,IEEE,Conferences,"With the rapid development of the artificial intelligence industry, the application of object detection technology in real life is becoming increasingly widespread, such as intelligent monitoring, autonomous driving, and augmented reality. In this paper, object detection system is deployed on edge devices which is low complexity and low-cost device such as the Raspberry Pi. Implement Mobilenet-SSD based on the deep learning framework and deploy on edge devices, such as image acquisition, object detection and result display. The results show that the object detection technology can also be achieved on devices with scarce computing resources such as the Raspberry Pi and satisfies actual business requirements.",https://ieeexplore.ieee.org/document/9538856/,2021 IEEE/CIC International Conference on Communications in China (ICCC Workshops),28-30 July 2021,ieeexplore
10.1109/ICCE-TW46550.2019.8991771,Implementation of ransomware prediction system based on weighted-KNN and real-time isolation architecture on SDN Networks,IEEE,Conferences,"In May 2017, hackers used the ransomware WannaCry to launch large-scale attacks on 150 countries, affecting every industry. Therefore, detection and control of the ransomware virus has become an important issue for security experts in recent years. Recently, machine learning, deep learning, and artificial intelligence technologies have become increasingly mature. Many companies (such as Google) have introduced software-defined networking (SDN) to replace the original network architecture, traffic routing, and network configuration control management. Therefore, this paper proposes a ransomware prediction system based on weighted-K-Nearest-Neighbor. This system includes the detection and prediction of ransomware packet traffic and design and the implementation of a dynamic isolation system integrated SDN. The experimental results show that the precision of detecting normal flow and abnormal flow is 99.7 and 97.7, respectively.",https://ieeexplore.ieee.org/document/8991771/,2019 IEEE International Conference on Consumer Electronics - Taiwan (ICCE-TW),20-22 May 2019,ieeexplore
10.1109/RTC.2012.6418168,Implementation of the disruption predictor APODIS in JET real time network using the MARTe framework,IEEE,Conferences,"The evolution in the past years of Machine learning techniques, as well as the technological evolution of computer architectures and operating systems, are enabling new approaches for complex problems in different areas of industry and research, where a classical approach is nonviable due to lack of knowledge of the problem's nature. A typical example of this situation is the prediction of plasma disruptions in Tokamak devices. This paper shows the implementation of a real time disruption predictor. The predictor is based on a support vector machine (SVM). The implementation was done under the MARTe framework on a six core x86 architecture. The system is connected in JET's Real time Data Network (RTDN). Online results show a high degree of successful predictions and a low rate of false alarms thus, confirming its usefulness in a disruption mitigation scheme. The implementation shows a low computational load, which in an immediate future will be exploited to increase the prediction's temporal resolution.",https://ieeexplore.ieee.org/document/6418168/,2012 18th IEEE-NPSS Real Time Conference,9-15 June 2012,ieeexplore
10.1109/ICAML48257.2019.00042,Importance of Cloud Deployment Model and Security Issues of Software as a Service (SaaS) for Cloud Computing,IEEE,Conferences,"Cloud computing, now-a-days has become the jargon in the industry of IT. It is a model that gives worldwide access to shared pools of configurable resources over the internet. That means it run several applications or programs at the same time on more than one computer. The nature and usage of the cloud computing are developing rapidly both virtually and in reality. It is making things easier for the user of internet by many of its attractive features but these features, have not just only challenged the existing security system, but have also revealed new security issues. In this paper the deployment of cloud models are discussed . The cloud models are deployed as public cloud, private cloud and hybrid cloud. Each of these models have their own advantages and disadvantages. Cloud also provides some services like software as a service (SaaS), Platform as a service (PaaS), Infrastructure as a service (IaaS) . This paper is an insightful analysis of the features as well as the securities challenges of Software as a Service (SaaS) for cloud computing.",https://ieeexplore.ieee.org/document/8989204/,2019 International Conference on Applied Machine Learning (ICAML),25-26 May 2019,ieeexplore
10.1109/CISDA.2007.368136,Improved Missile Route Planning and Targeting using Game-Based Computational Intelligence,IEEE,Conferences,"This paper discusses a research project that employs computational intelligence (CI) to improve the ability of military planners to route sensors and weapons to effectively engage mobile targets. Future target motion is predicted through the use of multiple software agents employing goal oriented action planning (GOAP). Derived from the Stanford Research Institute Planning System (STRIPS), GOAP is a relatively new class of CI that is ideally suited to dynamic real-time environments such as military operations. The project is unusual in its adaptation of computer gaming industry technology for use in real-time, tactical military applications",https://ieeexplore.ieee.org/document/4219083/,2007 IEEE Symposium on Computational Intelligence in Security and Defense Applications,1-5 April 2007,ieeexplore
10.1109/ICIT52682.2021.9491636,Improvement of personal loans granting methods in banks using machine learning methods and approaches in Palestine,IEEE,Conferences,"For banking organizations, loan approval and risk assessment which is related is a very complex and significant process which is needs a high effort for relevant employee or manager to take a decision, because of manual or traditional methods that used in banks. The banking industry still needs a more precise method of predictive modeling for several problems. In general, for financial institutions and especially for banks forecasting credit defaulters is a hard challenge. The primary role of the current systems is to accept, or sending loan application to a specific level of approval to be studied and it is very difficult to foresee the probability of the borrower for paying the due dues amount without using methods to predict. Machine learning (ML) techniques and the algorithm that belongs to are a very amazing and promising technique in predicting for a large amount of data. Our research proposed to study three machine learning algorithms [1], Decision Tree (DT), Logistic Regression (LR), and Random Forest (RF), by using real data collected from Quds Bank with a variables that cover credit restriction and regulator instructions. The algorithm has been implemented to predict the loan approval of customers and the output tested in terms of the predicted accuracy.",https://ieeexplore.ieee.org/document/9491636/,2021 International Conference on Information Technology (ICIT),14-15 July 2021,ieeexplore
10.1109/IJCNN.2001.938518,Improving prediction of customer behavior in nonstationary environments,IEEE,Conferences,"Customer churn, switching from one service provider to another, costs the wireless telecommunications industry $4 billion each year in North America and Europe. To proactively build lasting relationships with customers, it is thus crucial to predict customer behavior. Machine learning has been applied to churn prediction, using historical data such as usage, billing, customer service, and demographics. However, because customer behavior is often nonstationary, training a model based on data extracted from a window of time in the past yields poor performance on the present. We propose two distinct approaches, using more historical data or new, unlabeled data, to improve the results for this real-world, large-scale, nonstationary problem. A new ensemble classification method, with combination weights learned from both labeled and unlabeled data, is also proposed, and it outperforms bagging and mixture of experts.",https://ieeexplore.ieee.org/document/938518/,IJCNN'01. International Joint Conference on Neural Networks. Proceedings (Cat. No.01CH37222),15-19 July 2001,ieeexplore
10.1109/COMPSAC.2018.10204,Indoor Augmented Reality Using Deep Learning for Industry 4.0 Smart Factories,IEEE,Conferences,"This paper proposes to design, develop and implement a fast and markerless mobile augmented reality system to achieve the registration for, the visualization of, and the interaction with machines in indoor smart factories with Industry 4.0 vision. A lightweight deep-learning image detection module based on MobileNets running on mobile devices is used to detect/recognize different machines and different portions of machines. Internet of Things (IoT) networking allows machines and sensors in machines to report data, such as machine settings and machine states, to the cloud-side server. Thus, augmented information associated with a machine portion can be derived from the server and superimposed with the portion image shown on the device display. Furthermore, interaction methods based on touch gestures and distance calculation are also implemented. A prototype system is developed and tested in a mechanical workshop for the purpose of validation and evaluation. The system is shown to achieve high detection accuracy, intuitive visualization, and unique interaction modes.",https://ieeexplore.ieee.org/document/8377831/,2018 IEEE 42nd Annual Computer Software and Applications Conference (COMPSAC),23-27 July 2018,ieeexplore
10.1109/GIOTS49054.2020.9119497,Industrial IoT and Digital Twins for a Smart Factory : An open source toolkit for application design and benchmarking,IEEE,Conferences,"The rapid evolution of digital technology and designed intelligence, such as the Internet of Things (IoT), Big data analytics, Artificial Intelligence (AI), Cyber Physical Systems (CPS), has been a catalyst for the 4th industrial revolution (known as industry 4.0). Among other, the two key state-of-the-art concepts in Industry 4.0, are Industrial IoT (IIoT) and digital twins. IIoT facilitates real-time data acquisition, processing and analytics over large amount of sensor data streams produced by sensors installed within a smart factory, while the `digital twin' concept aims to enable smart factories via the digital replication or representation of physical machines, processes, people in cyber-space. This paper explores the capability of present-state open-source platforms to collectively achieve digital twin capabilities, including IoT real-time data acquisition, virtual representation, analytics, and visualisation. The aim of this work is to `close the gap' between research and implementation, through a collective open source IoT and Digital Twin architecture. The performance of the open-source architecture in this work, is demonstrated in a use-case utilising industry `open data', and is bench-marked with universal testing tools.",https://ieeexplore.ieee.org/document/9119497/,2020 Global Internet of Things Summit (GIoTS),3-3 June 2020,ieeexplore
10.23919/ECC.2003.7085250,Inferential sensor for the olive oil industry,IEEE,Conferences,"This paper shows an inferential sensor that has been developed to be used in the olive oil industry. This sensor has been designed to measure two variables that appear in the elaboration of olive oil in a mill which are very difficult to be measured on line by a physical sensor. The knowledge of these variables on line is crucial for the optimal operation of the process, since they provide the state of the plant, allowing the development of a control strategy that can improve the quality and yield of the product. This sensor measures variables that in other case should come form laboratory analysis with large processing delays or from very expensive and difficult to use on line analysers. The sensor has been devised based upon artificial Neural Networks (NN) and has been implemented as a routine running on a Programmable Logic Controller (PLC) and successfully tested on a real plant.",https://ieeexplore.ieee.org/document/7085250/,2003 European Control Conference (ECC),1-4 Sept. 2003,ieeexplore
10.1109/IJCNN52387.2021.9533907,Information-theoretic Source Code Vulnerability Highlighting,IEEE,Conferences,"Software vulnerabilities are a crucial and serious concern in the software industry and computer security. A variety of methods have been proposed to detect vulnerabilities in real-world software. Recent methods based on deep learning approaches for automatic feature extraction have improved software vulnerability identification compared with machine learning approaches based on hand-crafted feature extraction. However, these methods can usually only detect software vulnerabilities at a function or program level, which is much less informative because, out of hundreds (thousands) of code statements in a program or function, only a few core statements contribute to a software vulnerability. This requires us to find a way to detect software vulnerabilities at a fine-grained level. In this paper, we propose a novel method based on the concept of mutual information that can help us to detect and isolate software vulnerabilities at a fine-grained level (i.e., several statements that are highly relevant to a software vulnerability that include the core vulnerable statements) in both unsupervised and semi-supervised contexts. We conduct comprehensive experiments on real-world software projects to demonstrate that our proposed method can detect vulnerabilities at a fine-grained level by identifying several statements that mostly contribute to the vulnerability detection decision.",https://ieeexplore.ieee.org/document/9533907/,2021 International Joint Conference on Neural Networks (IJCNN),18-22 July 2021,ieeexplore
10.1109/AIVR.2018.00054,"Integrating Biomechanical and Animation Motion Capture Methods in the Production of Participant Specific, Scaled Avatars",IEEE,Conferences,"3D motion capture of human movement in animation and biomechanics has developed in relatively separate and parallel domains. The two disciplines use different language, software, computational models and have different aims. As a result, in the life sciences, human movement is predominantly analyzed as non-visual biomechanical data. Whereas human movement visualization in animation typically lacks the accuracy outside of that required in the entertainment industry. This project draws from both disciplines to develop a novel approach in the creation of participant specific, motion capture skeletons which are retargeted onto participant specific, anatomically scaled, humanoid avatars. The customized motion capture marker placement, skeleton and character scaling used in this new approach aims to retain a high level of movement fidelity and minimize discrepancies between participant and avatar movement. This process has been used in the visualization of aesthetic movement such as dance and provides a step towards the generation of a digital double which can facilitate full body immersion into digital environments.",https://ieeexplore.ieee.org/document/8613673/,2018 IEEE International Conference on Artificial Intelligence and Virtual Reality (AIVR),10-12 Dec. 2018,ieeexplore
10.23919/MIPRO48935.2020.9245232,Integrating Industry Seminars within a Software Engineering Module to Enhance Student Motivation,IEEE,Conferences,"Engineering students increasingly demand and require coverage of emerging technologies to prepare themselves for subsequent research and employment. Industry and professional bodies are also concerned that engineering education doesn't always prepare students adequately for the world of work. The software engineering postgraduate professional practice module at University College London is designed to provide real-world experience, before students commence their industry research projects. Industry speakers are invited from a range of organizations, including ThoughtWorks, IBM, Form3, Verne Global, and Fujitsu. Seminars include: DevOps, microservices, cloud-native architectures, machine learning and quantum technologies. Before each topic is covered, students are asked their understanding of the subject matter, via questionnaires. This information is shared with industry speakers to ensure the content of presentations is compatible with students' prior knowledge. It has proved valuable to allow time for discussions to facilitate professional networking, which particularly benefits female students. Students have indicated they highly value the real-world project examples delivered by industry experts. This suggests that integrating industry seminars can enhance engineering education and motivate students by covering leading-edge technologies and practices. However, this requires considerable time in coordinating and codeveloping seminars, and such initiatives need to be adequately resourced to be effective.",https://ieeexplore.ieee.org/document/9245232/,"2020 43rd International Convention on Information, Communication and Electronic Technology (MIPRO)",28 Sept.-2 Oct. 2020,ieeexplore
10.1109/I-ESA.2009.60,Integrating Process and Ontology for Supply Chain Modelling,IEEE,Conferences,This paper introduces an ontology model developed to support supply chain process modelling. Supply chain provides the business context for achieving interoperability of enterprise systems. It is observed that the emphasis on ontology development for enterprise interoperability could result in information models that are not relevant to real business needs. This work explicitly defines the generic business processes relevant to supply chain operations and develops the ontology that was tested in the creation of the information model to support the information exchange needs three industry case studies. It demonstrated that prior identification of processes the ontology is supposed to support facilitates its development and also its subsequent validation. This paper introduces the overall ontology development approach together with some of the findings that summarizes our experiences in developing the ontology model to support supply chain process modelling.,https://ieeexplore.ieee.org/document/5260822/,2009 International Conference on Interoperability for Enterprise Software and Applications China,21-22 April 2009,ieeexplore
10.1109/FUZZY.2006.1681917,Intelligent Constant Current Control for Resistance Spot Welding,IEEE,Conferences,"Resistance spot welding is one of the primary means of joining sheet metal in the automotive industry and other industries. The demand for improved corrosion resistance has led the automotive industry to increasingly use zinc coated steel in auto body construction. One of the major concerns associated with welding coated steel is the mushrooming effect (the increase in the electrode diameter due to deposition of copper into the spot surface) resulting in reduced current density and undersized welds (cold welds). The most common approach to this problem is based on the use of simple unconditional incremental algorithms (steppers) for preprogrammed current scheduling. In this paper, an intelligent algorithm is proposed for adjusting the amount of current to compensate for the electrodes degradation. The algorithm works as a fuzzy logic controller using a set of engineering rules with fuzzy predicates that dynamically adapt the secondary current to the state of the weld process. The state is identified by indirectly estimating two of the main process characteristics - weld quality and expulsion rate. A soft sensor for indirect estimation of the weld quality employing a learning vector quantization (LVQ) type classifier is designed to provide a real time approximate assessment of the weld nugget diameter. Another soft sensing algorithm is applied to predict the impact of changes in current on the expulsion rate of the weld process. By maintaining the expulsion rate just below a minimal acceptable level, robust process control performance and satisfactory weld quality are achieved. The intelligent constant current control for resistance spot welding is implemented and validated on a medium frequency direct current (MFDC) constant current weld controller. Results demonstrate a substantial improvement of weld quality and reduction of process variability due to the proposed new control algorithm.",https://ieeexplore.ieee.org/document/1681917/,2006 IEEE International Conference on Fuzzy Systems,16-21 July 2006,ieeexplore
10.1109/RTSI.2019.8895598,Intelligent Embedded Load Detection at the Edge on Industry 4.0 Powertrains Applications,IEEE,Conferences,"In the context of Industry 4.0, there has been great focus in developing intelligent sensors. Deploying them, condition monitoring and predictive maintenance have become feasible solutions to minimize operating and maintenance costs while also increasing safety. A critical aspect is the applied load to the supervised machinery system. Vibration data can be used to determine the current condition, but this needs signal processing specially developed and adapted to the monitored machine part for feature extraction. Artificial intelligence (AI) can, on one hand, simplify the development of such special purpose processing and on another hand be used to monitor and classify machine conditions by learning features directly from data. By bringing the AI computation as close as possible to the sensor (Edge-AI), data bandwidth can be minimized, system scalability and responsiveness can be improved and real-time requirements can be fulfilled. This work describes how Edge-AI on a STM32-bit microcontroller can be implemented. Our experimental setup demonstrates how AI can be effectively used to detect and classify the load on a powertrain. In order to do this, we use a MEMS capacity accelerometer to sense vibrations of the system. Also, this work demonstrates how Deep Neural Networks (DNN) for signal classification are build and trained by using an open-source deep learning framework and how the code library for the microcontroller is automatically generated afterwards by using STM32Cube. AI toolchain. We compare the classification accuracy of a memory compressed DNN against an uncompressed DNN.",https://ieeexplore.ieee.org/document/8895598/,2019 IEEE 5th International forum on Research and Technology for Society and Industry (RTSI),9-12 Sept. 2019,ieeexplore
10.1109/ITST.2007.4295849,Intelligent Fleet Management System with Concurrent GPS &amp; GSM Real-Time Positioning Technology,IEEE,Conferences,"Fleet management system is a rapid growing industry. This system helps institutions to manage vehicle fleet efficiently and effectively through smart allocation of resources. In this project, an intelligent fleet management system which incorporates the power of concurrent Global Positioning System (GPS) and Global System for Mobile Communications (GSM) real-time positioning, front-end intelligent and web-based management software is proposed. In contrast to systems which depend solely on GPS positioning, the proposed system provides higher positioning accuracy and is capable to track the target at areas where GPS signals are weak or unavailable. The terminal is powered by Front-End Intelligent Technology (FEI), a comprehensive embedded technology that is equipped with necessary artificial intelligence to mimic human intelligence in decision-making for quicker response, better accuracy and less dependence on a backend server. With less dependency on the backend, large scale fleet management system can be implemented more effectively. The proposed system is successfully implemented and evaluated on twenty vehicles including buses and cars in Universiti Teknologi Malaysia (UTM). Results from the test-bed shown that user can monitor and track the real-time physical location and conditions of their vehicles via Internet or Short Message Service (SMS). The web-based fleet management software also helped the user to manage fleets more effectively.",https://ieeexplore.ieee.org/document/4295849/,2007 7th International Conference on ITS Telecommunications,6-8 June 2007,ieeexplore
10.1109/ARIS50834.2020.9205772,Intelligent Robot for Worker Safety Surveillance: Deep Learning Perception and Visual Navigation,IEEE,Conferences,"The fatal injury rate for the construction industry is higher than the average for all industries. Recently, researchers have shown an increased interest in occupational safety in the construction industry. However, all the current methods using conventional machine learning with stationary cameras suffer from some severe limitations, perceptual aliasing (e.g., different places/objects can appear identical), occlusion (e.g., place/object appearance changes between visits), seasonal / illumination changes, significant viewpoint changes, etc. This paper proposes a perception module using end-to-end deep-learning and visual SLAM (Simultaneous Localization and Mapping) for an effective and efficient object recognition and navigation using a differential-drive mobile robot. Various deep-learning frameworks and visual navigation strategies with evaluation metrics are implemented and validated for the selection of the best model. The deep-learning model's predictions are evaluated via the metrics (model speed, accuracy, complexity, precision, recall, P-R curve, F1 score). The YOLOv3 shows the best trade-off among all algorithms, 57.9% mean average precision (mAP), in real-world settings, and can process 45 frames per second (FPS) on NVIDIA Jetson TX2 which makes it suitable for real-time detection, as well as a right candidate for deploying the neural network on a mobile robot. The evaluation metrics used for the comparison of laser SLAM are Root Mean Square Error (RMSE). The Google Cartographer SLAM shows the lowest RMSE and acceptable processing time. The experimental results demonstrate that the perception module can meet the requirements of head protection criteria in Occupational Safety and Health Administration (OSHA) standards for construction. To be more precise, this module can effectively detect construction worker's non-hardhat-use in different construction site conditions and can facilitate improved safety inspection and supervision.",https://ieeexplore.ieee.org/document/9205772/,2020 International Conference on Advanced Robotics and Intelligent Systems (ARIS),19-21 Aug. 2020,ieeexplore
10.1109/ICIT.2006.372422,Intelligent Tuned PID Controllers for PMSM Drive - A Critical Analysis,IEEE,Conferences,"This paper presents a critical analysis of intelligent tuned PID controllers for the permanent magnet synchronous motor drive. The PID proportional-integral-derivative (PID) controller is the most popular controller in process industry. The PID algorithm is simple, reliable and robust for the control of first and second order processes and even high order processes with well damped modes. In this paper, PID control strategies, based on fuzzy logic, neural network and genetic algorithms are reviewed and implemented. A mathematical model of the drive is developed to study steady state and transient performance of current regulated voltage source inverter (VSI) fed PMSM in the field oriented mode under different conditions. The drive includes PID controller, vector control structure, the inverter and the machine. Different tuning algorithms and their effect on dynamic performance of PMSM drive in the real time frame are illustrated in terms of starting and speed reversal time, steady state error in speed, overshoot, performance parameters and speed and torque ripple. Good agreements between different methods has been observed and presented.",https://ieeexplore.ieee.org/document/4237744/,2006 IEEE International Conference on Industrial Technology,15-17 Dec. 2006,ieeexplore
10.1109/ISDA.2010.5687225,Intelligent online case-based planning agent model for real-time strategy games,IEEE,Conferences,"Research in learning and planning in real-time strategy (RTS) games is very interesting in several industries such as military industry, robotics, and most importantly game industry. A recent published work on online case-based planning in RTS Games does not include the capability of online learning from experience, so the knowledge certainty remains constant, which leads to inefficient decisions. In this paper, an intelligent agent model based on both online case-based planning (OLCBP) and reinforcement learning (RL) techniques is proposed. In addition, the proposed model has been evaluated using empirical simulation on Wargus (an open-source clone of the well known RTS game Warcraft 2). This evaluation shows that the proposed model increases the certainty of the case base by learning from experience, and hence the process of decision making for selecting more efficient, effective and successful plans.",https://ieeexplore.ieee.org/document/5687225/,2010 10th International Conference on Intelligent Systems Design and Applications,29 Nov.-1 Dec. 2010,ieeexplore
10.1109/MetroInd4.0IoT48571.2020.9138288,Introducing a cloud based architecture for the distributed analysis of Real-Time Ethernet traffic,IEEE,Conferences,"The use of industrial communication protocols based on Real-Time Ethernet (RTE) standards is completely replacing traditional industrial fieldbuses. As usual, when a technology becomes mature, the need of efficient diagnostic and maintenance tools quickly raises. Very often, following the paradigm of Industry 4.0, the most effective diagnostic systems are today based on distributed, cloud-centric, architectures and artificial intelligence. However, the distributed analysis of RTE systems is challenging, considering the plurality of protocols and the stringent cost constrains which are common in industry. In this paper, a new architecture for the distributed analysis of RTE networks is proposed, leveraging on distributed probes that send traffic samples to Matlab cloud for remote analysis. The paper also proposes a software conversion tool to adapt general PCAP files captured by popular sniffers (e.g. Wireshark) into MAT file for easier Matlab elaboration. Last, a test bench for characterization (in terms of transfer delays) of the first part of the chain for RTE traffic sampling is described. The results show that in less than 10 seconds it is possible to transfer chunks of RTE traffic data (captured on industrial networks with hundreds of real-time devices) directly to the cloud and to have them converted in Matlab format.",https://ieeexplore.ieee.org/document/9138288/,2020 IEEE International Workshop on Metrology for Industry 4.0 & IoT,3-5 June 2020,ieeexplore
10.1109/AIM46487.2021.9517377,Introducing adaptive mechatronic designs in bulk handling industry,IEEE,Conferences,"The advances of mechatronic system design and system integration have shown improvements in functionality, performance and energy efficiency in many applications across industries, from autonomous ground vehicles and drones to conveyor belts. This trend has been adopted in some industries more than others. The design of equipment to handle granular or bulk material is commonly based on traditional approaches. Therefore, introducing mechatronic concepts in the design procedure can enable new possibilities, such as sensor integration and data analyses, adaptability and control. The efficiency of bulk material handling equipment in ports, agriculture and food processing is heavily influenced by the operational conditions. Typically, a piece of equipment is designed for defined operational conditions when the maximum performance can be achieved. In this work the concept of adaptability to varying operational conditions is explored by understanding the technologies implemented in other industries and the feasibility to be implemented in the bulk handling equipment design. Sensing technology, actuation and adaptability are systematically presented in this work to support the design process of the next generation of bulk handling equipment. This will pave the way for incorporating the technological trends in the design, such as: sustainability, “smartness”, Internet of Things, Industry 4.0, digital twin and machine learning. Adaptive mechatronic solutions will play a crucial role in generating and implementing innovative sustainable solutions for bulk handling equipment.",https://ieeexplore.ieee.org/document/9517377/,2021 IEEE/ASME International Conference on Advanced Intelligent Mechatronics (AIM),12-16 July 2021,ieeexplore
10.1109/PerComWorkshops48775.2020.9156201,"Invited Talk: Software Engineering, AI and autonomous vehicles: Security assurance",IEEE,Conferences,"In this talk, I will first walk through some real industrial requirements and research challenges in autonomous vehicles. I will then talk about research works which can potentially solve these issues, mainly covering training, testing and anomaly detection for autonomous systems and driver behaviour detection. The talk will be broad but covering some state of the art interesting research questions and directions in autonomous vehicles safety and security assurance, and human vehicle interaction which shall be suitable both for researchers and industry practitioners for in-depth enquiry and collaboration.",https://ieeexplore.ieee.org/document/9156201/,2020 IEEE International Conference on Pervasive Computing and Communications Workshops (PerCom Workshops),23-27 March 2020,ieeexplore
10.1109/DSNW.2012.6264658,Invited talk: Challenges in Medical Cyber-Physical Systems,IEEE,Conferences,"Summary form only given. As computers and communication bandwidth become ever faster and cheaper, computing and communication capabilities are embedded in all types of objects and structures in the physical environment. Harnessing these capabilities to bridge the cyber-world with the physical world will allow the development of applications with great societal impact and economic benefit. At the heart of these applications are cyber-physical systems consisting of integrated computational and communication cores that interact with the physical world, with intelligence provided by embedded software. Cyber Physical Systems (CPS) are engineered systems that provide tight integration of and coordination between the cyber world of computing and communications and the physical world. CPS are to meet the needs of the new generation of engineered systems that are highly dependable, efficiently produced and certified, and capable of advanced performance in computation, communication, and control. CPS will transform how we interact with and control the physical world around us, as the Internet transformed how we interact and communicate with one another and revolutionized how and where we access information. One application domain of CPS is Medical Cyber-Physical Systems (MCPS), which are life-critical, context-aware, networked systems of medical devices. Medical device industry is undergoing a transformation, embracing the potential of embedded software and network connectivity. Instead of stand-alone devices that can be designed, certified, and used to treat patients independent of each other, distributed systems that simultaneously control multiple aspects of the patient's physiology are increasingly used in hospitals to provide high-quality continuous care for patients. The combination of embedded software controlling medical devices, networking capabilities, and complicated physiological dynamics of patient bodies makes MCPS complex. The need to design complex MCPS that are both safe and effective presents numerous challenges, including achieving high assurance in system software, interoperability, context-aware intelligence, autonomy, security and privacy, and device certification. In this talk, I will discuss these challenges in developing MCPS and present some of our work in addressing them, and several open research and development issues.",https://ieeexplore.ieee.org/document/6264658/,IEEE/IFIP International Conference on Dependable Systems and Networks Workshops (DSN 2012),25-28 June 2012,ieeexplore
10.1109/DASA51403.2020.9317177,"IoT Driven Resiliency with Artificial Intelligence, Machine Learning and Analytics for Digital Transformation",IEEE,Conferences,"A new manufacturing era “Industry 4.0” is emerging with two unique characteristics: intelligent manufacturing and integrated manufacturing. This pattern rationale with the progress of digital transformation, in which efficient manufacturing and production systems is being continuously pursued. Digital transformation initiatives generate large data sets due to massive integration of devices in internet of things (IoT) environment. This scenario demands the fastest insights to respond on time considering three key pillars: communication network evolution, digital business, and customer experience. IoT driven resiliency with traditional analytics has limited value without artificial intelligence (AI), and machine learning (ML). This study aims to explore this phenomenon of interest by conducting group discussion with software vendors. The results will helpful to utilize the power of AI and ML with analytics to leverage a large amount of data which would contribute to the success of digital transformation of organizations with real-time decision-making.",https://ieeexplore.ieee.org/document/9317177/,2020 International Conference on Decision Aid Sciences and Application (DASA),8-9 Nov. 2020,ieeexplore
10.1109/ICIAICT.2019.8784859,IoT-NDN: An IoT Architecture via Named Data Netwoking (NDN),IEEE,Conferences,"Internet of Things (IoT) systems have become the central part of future internet research. In the IoT, heterogeneous devices are connected to sense the environment or to observe individual tasks. Many research fields use the seamless IoT infrastructure to interact with the integrated devices and diverse services. Furthermore, IoT is a promising technology to increase the comfort and quality of life and opens new ways of interaction between people and things. Real life applications in healthcare sectors, home automation, industry, smart cities, monitoring scenarios, etc. benefit from the low-cost wireless technology in IoT on one hand. On the contrary, IoT system has many challenging features: many devices are resource-constrained with energy and memory, are highly heterogeneous and their applications continuously transmit transient information. Furthermore, Requesting, delivering and updating the information in IoT are challenging because of the resource limitation. Named Data Networking (NDN) is one of the latest and the most important Information-Centric Networking (ICN) approaches which uses named data to deliver data in the network. Based on hierarchically structured names, NDN matches the application pattern of IoT systems and uses its communication concept to optimize the power supply and distribute the data efficiently in the network. This paper discusses the main concepts of NDN including naming, routing, forwarding and caching in IoT infrastructure. To achieve an efficient system in different applications scenario in future IoT, an IoT architecture is proposed via NDN called IoT-NDN. The objective of this research work is the design and development of IoT-NDN for different fields in IoT systems. The deployment of IoT-NDN is challenging and requires proper design choices. The proposed solution and challenges of all mentioned issues are discussed in this paper.",https://ieeexplore.ieee.org/document/8784859/,"2019 IEEE International Conference on Industry 4.0, Artificial Intelligence, and Communications Technology (IAICT)",1-3 July 2019,ieeexplore
10.1109/FMEC49853.2020.9144776,IoT-WLAN Proximity Network for Potentiostats,IEEE,Conferences,"The implementation of potentiostats as portable and communicated devices has reached significant progress to benefit research, industry, and education. The Internet of Things (IoT) is a good opportunity to interconnect devices such as the potentiostats together with electronics, communication technologies, and chemistry into a single system. This work proposes a network for potentiostats using machine-to-machine (M2M) protocols, modifying its functioning mechanism in the broker to check the payload of the message that passes through it and synchronize the sensors depending on its content. Although one sensor can be synchronized directly to another, the broker decides which sensor to pair. This modification was made in the M2M protocol algorithm, both in the Broker and in the Client (sensor). In addition to this, the network uses an interconnection architecture of IoT smart networks of proximity with centralized management. The results of the tests carried out showed that the use of a modified M2M such as the one proposed in the architecture allows synchronization and comparison of the measurements of several sensors in real-time.",https://ieeexplore.ieee.org/document/9144776/,2020 Fifth International Conference on Fog and Mobile Edge Computing (FMEC),20-23 April 2020,ieeexplore
10.1109/IGSC48788.2019.8957164,IoT/CPS Ecosystem for Efficient Electricity Consumption : Invited Paper,IEEE,Conferences,"Modern society relies on smart systems like internet of things (IoT) and cyber physical systems (CPS) to monitor and control physical processes. The widespread deployment of IoT and CPSs result in fast growth of sensor data as physical processes are constantly monitored by billions of IP-enabled sensors (44 zettabytes by 2020). Hence, fog nodes are deployed to make network edge rich in computing resources to enable real-time data analytics using artificial intelligence/machine learning (AI/ML) for Big data generated from IoT and CPSs. This paper proposes IoT/CPS ecosystem for smart grid (SG) utilizing industry 4.0 concept to manage and control the loads using an intelligent predictive controller based on artificial neural network (ANN). The ANN is trained to predict the loads in certain districts based on previous smart meter readings installed at consumers and substations. This is a novel approach which integrates IoT/CPSs ecosystem into electric power system to deliver energy to consumers with high efficiency, reduce the cost, optimize the energy consumption, improve the reliability and enable real-time monitoring of power consumption.",https://ieeexplore.ieee.org/document/8957164/,2019 Tenth International Green and Sustainable Computing Conference (IGSC),21-24 Oct. 2019,ieeexplore
10.1109/HPCA.2010.5416648,Is hardware innovation over?,IEEE,Conferences,"My colleagues, promotion committees, research funding agencies and business people often wonder if there is need for any architecture research. There seems to be no room to dislodge Intel IA-32. Even the number of new Application-Specific Integrated Circuits (ASICs) seems to be declining each year, because of the ever-increasing development cost. This viewpoint ignores another reality which is that the future will be dominated by mobile devices such as smart phones and the infrastructure needed to support consumer services on these devices. This is already restructuring the IT industry. To the first-order, in the mobile world functionality is determined by what can be supported within a 3W power budget. The only way to reduce power by one to two orders of magnitude is via functionally specialized hardware blocks. A fundamental shift is needed in the current design flow of systems-on-a-chip (SoCs) to produce them in a less-risky and cost-effective manner. In this talk we will present, via examples, a method of designing systems that facilitates the synthesis of complex SoCs from reusable ¿IP¿ modules. The technical challenge is to provide a method for connecting modules in a parallel setting so that the functionality and the performance of the composite are predictable.",https://ieeexplore.ieee.org/document/5416648/,HPCA - 16 2010 The Sixteenth International Symposium on High-Performance Computer Architecture,9-14 Jan. 2010,ieeexplore
10.1109/SNPD.2017.8022721,Issues with conducting controlled on-line experiments for E-Commerce,IEEE,Conferences,"More and more on-line experiments have been done in E-Commerce in order to understand the behavior of users or customers and then apply the data analysis technique to provide business guidance. One of the techniques is A/B testing. However, there is not clear guidance on the sample size in order for us to have valuable, trustable discovery. The purpose of this work is to find out a way to group customers in the data sample in order to achieve an optimal difference between the buckets. Based on the analysis result of real data collected during joining an industry project, we think the problem is complex and the meaningful conclusions have to be drawn with caution from business experiments such as A/B testing, due to the vast variation in the data. Moreover, if we don't allocate enough samples in the treatment group, the experiment could be inconclusive even if the testing lasts for a longer enough time, such as one month.",https://ieeexplore.ieee.org/document/8022721/,"2017 18th IEEE/ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing (SNPD)",26-28 June 2017,ieeexplore
10.1109/ICCTA48790.2019.9478839,Keynote Speech II: Readiness for the Impact of Emerging Technologies,IEEE,Conferences,"Throughout history, technical innovation has always been the key driver of human progress. The use of new technologies brings positive changes to people and societies. It makes life easier, more pleasant, raises the standard of living, and improves human wellbeing, health and life expectancy. It also changes social frameworks, business dynamics, job types, and wealth distribution.The introduction of digital technology a few decades ago has spurred a series of transformations that have significantly changed the way we live, work, transact, and interact. Innovation continues to accelerate in a large set of technologies and applications. New hardware, software, and algorithmic tools allow us to process information much faster and in much larger volumes. Moreover, the digital world is becoming increasingly intertwined with the physical world of machines, to which it is bringing ubiquitous intelligence and a perpetual flow of information.These trends are driving us towards a very different future. That future has already started. A new wave of social, economic, and psychological changes is expected to abruptly affect almost everything we do. With change, many opportunities come along. Those who anticipate the course of the future, and prepare for it, will be ready to seize these opportunities and will come out winners. Those who chose to ignore the signs of change, will risk losing their livelihood and eventually hurting their families, businesses, and societies. Those who see the storm coming but react by standing still in panic, disgruntlement, and lamentation will be defenseless when the inevitable waves hit their shores.This presentation overviews the trends in technology and applications, including Artificial Intelligence, Big Data Analytics, Robotics, Internet of Things, Industry 4.0, etc. The impact that such advances are likely to have on the hightech as well as the low-tech job markets is outlined. Some actions and initiatives are proposed and discussed, with the purpose of triggering a larger debate on how individuals, businesses, academic institutions, and governments should prepare for the anticipated massive changes that are already beginning to affect our world.",https://ieeexplore.ieee.org/document/9478839/,2019 29th International Conference on Computer Theory and Applications (ICCTA),29-31 Oct. 2019,ieeexplore
10.1109/ReConFig48160.2019.8994747,Keynote2: Global-Scale FPGA-Accelerated Deep Learning Inference with Microsoft's Project Brainwave,IEEE,Conferences,"The computational challenges involved in accelerating deep learning have attracted the attention of computer architects across academia and industry. While many deep learning accelerators are currently theoretical or exist only in prototype form, Microsoft's Project Brainwave is in massive-scale production in data centers across the world. Project Brainwave runs on our Catapult networked FPGAs, which provide latency that is low enough to enable ""Real-time Al"" - deep learning inference that is fast enough for interactive services and achieves peak throughput at a batch size of 1. Project Brainwave powers the latest version of Microsoft's Bing search engine, which uses cutting-edge neural network models that are much larger than the neural networks used in typical benchmarks. In this talk, I'll discuss how Project Brainwave's FPGA-based and software components work together to accelerate both first-party workloads - like Bing search - and third-party applications using neural network models, like high energy physics and manufacturing quality control. I'll also talk about how FPGAs are the perfect platform for the fast-changing world of deep neural networks, since their reconfigurability allows us to update our accelerator in place to keep up with the state of the art.",https://ieeexplore.ieee.org/document/8994747/,2019 International Conference on ReConFigurable Computing and FPGAs (ReConFig),9-11 Dec. 2019,ieeexplore
10.1109/PerComWorkshops51409.2021.9431114,Keynote: Explainable-by-design Deep Learning,IEEE,Conferences,"MACHINE and AI justifiably attract the attention and interest not only of the wider scientific community and industry, but also society and policy makers. However, even the most powerful (in terms of accuracy) algorithms such as deep learning (DL) can give a wrong output, which may be fatal. Due to the opaque and cumbersome model structure used by DL, some authors started to talk about a dystopian “black box” society. Despite the success in this area, the way computers learn is still principally different from the way people acquire new knowledge, recognise objects and make decisions. People do not need a huge amount of annotated data. They learn by example, using similarities to previously acquired prototypes, not by using parametric analytical models. Current ML approaches are focused primarily on accuracy and overlook explainability, the semantic meaning of the internal model representation, reasoning and its link with the problem domain. They also overlook the efforts to collect and label training data and rely on assumptions about the data distribution that are often not satisfied. The ability to detect the unseen and unexpected and start learning this new class/es in real time with no or very little supervision is critically important and is something that no currently existing classifier can offer. The challenge is to fill this gap between high level of accuracy and the semantically meaningful solutions. The most efficient algorithms that have fuelled interest towards ML and AI recently are also computationally very hungry - they require specific hardware accelerators such as GPU, huge amounts of labeled data and time. They produce parametrised models with hundreds of millions of coefficients, which are also impossible to interpret or be manipulated by a human. Once trained, such models are inflexible to new knowledge. They cannot dynamically evolve their internal structure to start recognising new classes. They are good only for what they were originally trained for. They also lack robustness, formal guarantees about their behaviour and explanatory and normative transparency. This makes problematic use of such algorithms in high stake complex problems such as aviation, health, bailing from jail, etc. where the clear rationale for a particular decision is very important and the errors are very costly. All these challenges and identified gaps require a dramatic paradigm shift and a radical new approach. In this talk the speaker will present such a new approach towards the next generation of computationally lean ML and AI algorithms that can learn in real-time using normal CPUs on computers, laptops, smartphones or even be implemented on chip that will change dramatically the way these new technologies are being applied. It is explainable-by-design. It focuses on addressing the open research challenge of developing highly efficient, accurate ML algorithms and AI models that are transparent, interpretable, explainable and fair by design. Such systems are able to self-learn lifelong, and continuously improve without the need for complete retraining, can start learning from few training data samples, explore the data space, detect and learn from unseen data patterns, collaborate with humans or other such algorithms seamlessly.",https://ieeexplore.ieee.org/document/9431114/,2021 IEEE International Conference on Pervasive Computing and Communications Workshops and other Affiliated Events (PerCom Workshops),22-26 March 2021,ieeexplore
10.1109/COMPSAC.2008.188,Keynote: Towards An Ontological Foundation for Evolving Agent Communities,IEEE,Conferences,"Research in ontology management has reached a certain level of maturity, however, there is still little understanding of, and technological support for, the methodological and evolutionary aspects of ontologies as resources. Yet these are crucial in distributed and collaborative worlds such as the Semantic Web, where ontologies and their communities of use naturally and mutually co-evolve. Through a deep understanding of the real-time, community-driven, evolution of so-called ontologies, a semantic agent system can be made operationally relevant and sustainable over long periods of time. Such a paradigm shift in knowledge-intensive and community-driven systems would affect knowledge sharing and communication across diverse communities in business, industry, and society. In this paper, we give an overview of the practical and theoretical challenges and limitations, and based on that introduce an ontological and methodological foundation for community evolution processes.",https://ieeexplore.ieee.org/document/4591613/,2008 32nd Annual IEEE International Computer Software and Applications Conference,28 July-1 Aug. 2008,ieeexplore
10.1109/SMC.2017.8122711,Knowledge extracted from recurrent deep belief network for real time deterministic control,IEEE,Conferences,"Recently, the market on deep learning including not only software but also hardware is developing rapidly. Big data is collected through IoT devices and the industry world will analyze them to improve their manufacturing process. Deep Learning has the hierarchical network architecture to represent the complicated features of input patterns. Although deep learning can show the high capability of classification, prediction, and so on, the implementation on GPU devices are required. We may meet the trade-off between the higher precision by deep learning and the higher cost with GPU devices. We can success the knowledge extraction from the trained deep learning with high classification capability. The knowledge that can realize faster inference of pre-trained deep network is extracted as IF-THEN rules from the network signal flow given input data. Some experiment results with benchmark tests for time series data sets showed the effectiveness of our proposed method related to the computational speed.",https://ieeexplore.ieee.org/document/8122711/,"2017 IEEE International Conference on Systems, Man, and Cybernetics (SMC)",5-8 Oct. 2017,ieeexplore
10.1109/SC41405.2020.00025,Kraken: Memory-Efficient Continual Learning for Large-Scale Real-Time Recommendations,IEEE,Conferences,"Modern recommendation systems in industry often use deep learning (DL) models that achieve better model accuracy with more data and model parameters. However, current opensource DL frameworks, such as TensorFlow and PyTorch, show relatively low scalability on training recommendation models with terabytes of parameters. To efficiently learn large-scale recommendation models from data streams that generate hundreds of terabytes training data daily, we introduce a continual learning system called Kraken. Kraken contains a special parameter server implementation that dynamically adapts to the rapidly changing set of sparse features for the continual training and serving of recommendation models. Kraken provides a sparsity-aware training system that uses different learning optimizers for dense and sparse parameters to reduce memory overhead. Extensive experiments using real-world datasets confirm the effectiveness and scalability of Kraken. Kraken can benefit the accuracy of recommendation tasks with the same memory resources, or trisect the memory usage while keeping model performance.",https://ieeexplore.ieee.org/document/9355295/,"SC20: International Conference for High Performance Computing, Networking, Storage and Analysis",9-19 Nov. 2020,ieeexplore
10.1109/CCNC.2019.8651681,Lameness Detection as a Service: Application of Machine Learning to an Internet of Cattle,IEEE,Conferences,"Lameness is a big problem in the dairy industry, farmers are not yet able to adequately solve it because of the high initial setup costs and complex equipment in currently available solutions, and as a result, we propose an end-to-end IoT application that leverages advanced machine learning and data analytics techniques to identify lame dairy cattle. As part of a real world trial in Waterford, Ireland, 150 dairy cows were each fitted with a long range pedometer. The mobility data from the sensors attached to the front leg of each cow is aggregated at the fog node to form time series of behavioral activities (e.g., step count, lying time and swaps per hour). These are analyzed in the cloud and lameness anomalies are sent to farmer's mobile device using push notifications. The application and model automatically measure and can gather data continuously such that cows can be monitored daily. This means there is no need for herding the cows, furthermore the clustering technique employed proposes a new approach of having a different model for subsets of animals with similar activity levels as opposed to a one size fits all approach. It also ensures that the custom models dynamically adjust as weather and farm condition change as the application scales. The initial results indicate that we can predict lameness 3 days before it can be visually captured by the farmer with an overall accuracy of 87%. This means that the animal can either be isolated or treated immediately to avoid any further effects of lameness.",https://ieeexplore.ieee.org/document/8651681/,2019 16th IEEE Annual Consumer Communications & Networking Conference (CCNC),11-14 Jan. 2019,ieeexplore
10.1109/CarpathianCC.2017.7970404,Landing area recognition by image applied to an autonomous control landing of VTOL aircraft,IEEE,Conferences,"The pattern recognition aims to classify objects on different categories based on characteristics analysis. The usage of pattern recognition shows itself more and more frequent and widely used, covering different areas both in industry and research and development of new technologies. With that in mind, this work aims to compare two nonlinear classifiers, the Adaptive Boosting method and the Artificial Neural Network method, applied to the identification of a certain landmark, where the more profitable is inserted in a Vertical Take-Off and Landing (VTOL) aircraft real model to trigger the land action after a demanded mission in the trained pattern presence. It is used as sensing method, computer vision technique, from camera's acquired images the characteristics are extracted by a proceeding based on Viola-Jones technique. To optimize the classification, it is also used the Principal Component Analysis method to uncouple the amount of data in the training stage and optimize the results in both classifiers. To prove the efficiency of the classifier when the aircraft is flying, it is used to test a scenario where it is possible to simulate the landing action with different altitudes. The Adaptive Boosting method proved itself to be more advantageous due to its simple implementation and less computational processing effort, despite the slightly lower performance when it comes to classifying compared to the Artificial Neural Network. The Principal Component Analysis method also shows itself to be a good improvement when applied to both techniques, raising the success rate of the classifiers in all the tested cases. The results obtained in the simulation tests were considered satisfactory as the aircraft lands with great precision over the determined landmark after identifying the landing area used for training.",https://ieeexplore.ieee.org/document/7970404/,2017 18th International Carpathian Control Conference (ICCC),28-31 May 2017,ieeexplore
10.1109/ISGT.2016.7781159,Large-scale detection of non-technical losses in imbalanced data sets,IEEE,Conferences,"Non-technical losses (NTL) such as electricity theft cause significant harm to our economies, as in some countries they may range up to 40% of the total electricity distributed. Detecting NTLs requires costly on-site inspections. Accurate prediction of NTLs for customers using machine learning is therefore crucial. To date, related research largely ignore that the two classes of regular and non-regular customers are highly imbalanced, that NTL proportions may change and mostly consider small data sets, often not allowing to deploy the results in production. In this paper, we present a comprehensive approach to assess three NTL detection models for different NTL proportions in large real world data sets of 100Ks of customers: Boolean rules, fuzzy logic and Support Vector Machine. This work has resulted in appreciable results that are about to be deployed in a leading industry solution. We believe that the considerations and observations made in this contribution are necessary for future smart meter research in order to report their effectiveness on imbalanced and large real world data sets.",https://ieeexplore.ieee.org/document/7781159/,2016 IEEE Power & Energy Society Innovative Smart Grid Technologies Conference (ISGT),6-9 Sept. 2016,ieeexplore
10.1109/NICS.2018.8606862,Lead Engagement by Automated Real Estate Chatbot,IEEE,Conferences,"Recently, automated chatbot has been increasingly applied in real estate industry. Even though chatbots cannot fully replace the traditional relation between agents and home buyers, they can help to engage potential clients (or leads) in meaningful conversations, which is highly useful for lead capture. In this paper, we present an intelligent chatbot for this purpose. Various machine learning techniques, including multi-task deep learning technique for intent identification and frequent itemsets for conversation elaboration, have been employed in our system. Our chatbot has been deployed by CEO K35 GROUP JSC with daily updated data of real estate information at Hanoi and Ho Chi Minh cities, Vietnam.",https://ieeexplore.ieee.org/document/8606862/,2018 5th NAFOSTED Conference on Information and Computer Science (NICS),23-24 Nov. 2018,ieeexplore
10.1109/VPPC49601.2020.9330855,Learning Based Energy Management Strategy Offline Trainers Comparison for Plug-in Hybrid Electric Buses,IEEE,Conferences,"The automotive industry is facing a transformation towards the massive digitalization and data-acquisition of the vehicles operation. The exploitation of operational data opens up new opportunities in the energy efficiency improvement of the vehicles. In this regard, the combination of optimization techniques with neural networks and fuzzy systems in one unified framework, known as learning-based energy management strategies, have been identified as promising methods. These learning-based techniques combine the optimized operation with the IF-THEN human-type reasoning simplicity of a fuzzy system through neural-type of learning. Therefore, fuzzy-neural networks are the bridge that allows to learn offline from the optimal operation and design energy management strategy for real time implementation. In this regard, the main contribution of this paper lies on the comparison of a previously developed ANFIS approach with a simpler Neo-Fuzzy neuron based, with the aim to evaluate the tradeoff between accuracy and computational and structural efficiency. The proposed approach represents a fuzzy-neural structure with less parameters for training that is expected to facilitate its future real time application for energy management strategies for each bus from a fleet operating on a predefined route.",https://ieeexplore.ieee.org/document/9330855/,2020 IEEE Vehicle Power and Propulsion Conference (VPPC),18 Nov.-16 Dec. 2020,ieeexplore
10.1109/ICRA.2018.8460502,Learning Motion Predictors for Smart Wheelchair Using Autoregressive Sparse Gaussian Process,IEEE,Conferences,"Constructing a smart wheelchair on a commercially available powered wheelchair (PWC) platform avoids a host of seating, mechanical design and reliability issues but requires methods of predicting and controlling the motion of a device never intended for robotics. Analog joystick inputs are subject to black-box transformations which may produce intuitive and adaptable motion control for human operators, but complicate robotic control approaches; furthermore, installation of standard axle mounted odometers on a commercial PWC is difficult. In this work, we present an integrated hardware and software system for predicting the motion of a commercial PWC platform that does not require any physical or electronic modification of the chair beyond plugging into an industry standard auxiliary input port. This system uses an RGB-D camera and an Arduino interface board to capture motion data, including visual odometry and joystick signals, via ROS communication. Future motion is predicted using an autoregressive sparse Gaussian process model. We evaluate the proposed system on real-world short-term path prediction experiments. Experimental results demonstrate the system's efficacy when compared to a baseline neural network model.",https://ieeexplore.ieee.org/document/8460502/,2018 IEEE International Conference on Robotics and Automation (ICRA),21-25 May 2018,ieeexplore
10.1109/EDUCON.2018.8363365,Learning analytics for location-based serious games,IEEE,Conferences,"Pervasive gaming has experimented a huge commercial growth with location-based game successes such as Pokémon GO or Ingress. The serious game industry has an opportunity to take advantage of location-based mechanics to better connect games with the real world, creating more authentic immersive learning environments. Games such as historical tours, story-based exploration, laboratories, or flora explorations can greatly benefit from location-based mechanics. Location-based games usually include an augmented map to provide game context, which combines both traditional game-dependent elements such as avatars, and location-based elements such as areas or points of interest. For players, interacting with location-based elements may involve entering or exiting specific areas; or reaching a certain location and looking in a specific direction. To include standards-based learning analytics for location-based serious games (SGs), we have added support for player movement and location-based interactions to the pre-existing xAPI serious game profile. We have validated this approach through a case-study example that guided players through different sports-related facilities within a large outdoor area. This work have been carried out and it is available as part of the analytics infrastructure used in EU H2020 RAGE and BEACONING serious game projects.",https://ieeexplore.ieee.org/document/8363365/,2018 IEEE Global Engineering Education Conference (EDUCON),17-20 April 2018,ieeexplore
10.1109/CITS.2015.7297720,Learning automaton based context oriented middleware architecture for precision agriculture,IEEE,Conferences,"The continuous alteration of living landscape in earth due to the greatest impact of information technology has converted the whole earth into a small digital village. That means anyone could share information in any form with any other person residing anywhere in the earth. With the availability of information and communication technology like cloud computing, heterogeneous networking, crowd sensing, web services and data mining, anywhere and anytime information sharing is possible, but this will bring out lot of challenges like incompatibility in standards, data portability, data aggregation, data dissemination, differential context and communication overhead. The ICT has changed many aspects of human lifestyle, work places and living spaces. However, there is one sector namely agriculture which has been deprived of the real advantages of ICT. Hence, there exists a digital divide between farming industry and other industries. Farming industry consumes large amount of natural resources such as water, energy and fertilizers that could escalate issues such as global warming, soil degradation and depletion of ground water to the next level. To reduce the global warming effect, farming industry needs to be integrated with relevant technologies. This paper proposes ubiquitous context oriented middleware architecture for precision agriculture (LA-COMPa) to solve major issues such as waste of water, improper application of fertilizer, choice of wrong crops and season, poor yield and lack of marketing.",https://ieeexplore.ieee.org/document/7297720/,"2015 International Conference on Computer, Information and Telecommunication Systems (CITS)",15-17 July 2015,ieeexplore
10.1109/ICDE51399.2021.00283,Learning to Optimize Industry-Scale Dynamic Pickup and Delivery Problems,IEEE,Conferences,"The Dynamic Pickup and Delivery Problem (DPDP) is aimed at dynamically scheduling vehicles among multiple sites in order to minimize the cost when delivery orders are not known a priori. Although DPDP plays an important role in modern logistics and supply chain management, state-of-the-art DPDP algorithms are still limited on their solution quality and efficiency. In practice, they fail to provide a scalable solution as the numbers of vehicles and sites become large. In this paper, we propose a data-driven approach, Spatial-Temporal Aided Double Deep Graph Network (ST-DDGN), to solve industry-scale DPDP. In our method, the delivery demands are first forecast using spatial-temporal prediction method, which guides the neural network to perceive spatial-temporal distribution of delivery demand when dispatching vehicles. Besides, the relationships of individuals such as vehicles are modelled by establishing a graph-based value function. ST-DDGN incorporates attention-based graph embedding with Double DQN (DDQN). As such, it can make the inference across vehicles more efficiently compared with traditional methods. Our method is entirely data driven and thus adaptive, i.e., the relational representation of adjacent vehicles can be learned and corrected by ST-DDGN from data periodically. We have conducted extensive experiments over real-world data to evaluate our solution. The results show that ST-DDGN reduces 11.27% number of the used vehicles and decreases 13.12% total transportation cost on average over the strong baselines, including the heuristic algorithm deployed in our UAT (User Acceptance Test) environment and a variety of vanilla DRL methods. We are due to fully deploy our solution into our online logistics system and it is estimated that millions of USD logistics cost can be saved per year.",https://ieeexplore.ieee.org/document/9458860/,2021 IEEE 37th International Conference on Data Engineering (ICDE),19-22 April 2021,ieeexplore
10.1109/ISMAR-Adjunct54149.2021.00036,Learning to Perceive: Perceptual Resolution Enhancement for VR Display with Efficient Neural Network Processing,IEEE,Conferences,"Even though the Virtual Reality (VR) industry is experiencing a rapid growth with ever-expanding demands today, VR applications have yet to provide a fully immersive experience. The insufficient resolution of the VR head-mounted display (HMD) hinders the user from further immersion into the virtual world. In this work, we attempt to enhance the immersive experience by improving the perceptual resolution of VR HMDs. We employ an efficient neural-network-based approach with the proposed temporal integration loss function. By taking the temporal integration mechanism of the Human Visual System (HVS) into account, our network learns the perception process of the human eye, and temporally upsamples a sequence that in turn improves its perceived resolution. Specifically, we discuss a possible scenario where we deploy our approach on a VR system equipped with the eye-tracking technology, which could save up to 75% of the computational load. Compared with the state-of-the-art in terms of the inference time analysis and a user experiment, it shows that our approach runs around 1.89× faster and produces more favorable results.",https://ieeexplore.ieee.org/document/9585895/,2021 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct),4-8 Oct. 2021,ieeexplore
10.23919/DATE.2019.8714959,Learning to infer: RL-based search for DNN primitive selection on Heterogeneous Embedded Systems,IEEE,Conferences,"Deep Learning is increasingly being adopted by industry for computer vision applications running on embedded devices. While Convolutional Neural Networks' accuracy has achieved a mature and remarkable state, inference latency and throughput are a major concern especially when targeting low-cost and low-power embedded platforms. CNNs' inference latency may become a bottleneck for Deep Learning adoption by industry, as it is a crucial specification for many real-time processes. Furthermore, deployment of CNNs across heterogeneous platforms presents major compatibility issues due to vendor-specific technology and acceleration libraries.In this work, we present QS-DNN, a fully automatic search based on Reinforcement Learning which, combined with an inference engine optimizer, efficiently explores through the design space and empirically finds the optimal combinations of libraries and primitives to speed up the inference of CNNs on heterogeneous embedded devices. We show that, an optimized combination can achieve 45x speedup in inference latency on CPU compared to a dependency-free baseline and 2x on average on GPGPU compared to the best vendor library. Further, we demonstrate that, the quality of results and time ""to-solution"" is much better than with Random Search and achieves up to 15x better results for a short-time search.",https://ieeexplore.ieee.org/document/8714959/,"2019 Design, Automation & Test in Europe Conference & Exhibition (DATE)",25-29 March 2019,ieeexplore
10.1109/PDP50117.2020.00041,Lessons learned from comparing C-CUDA and Python-Numba for GPU-Computing,IEEE,Conferences,"Python as programming language is increasingly gaining importance, especially in data science, scientific, and parallel programming. It is faster and easier to learn than classical programming languages such as C. However, usability often comes at the cost of performance and applications written in Python are considered to be much slower than applications written in C or FORTRAN. Further, it does not allow the usage of GPUs-besides of pre-compiled libraries.However, the Numba package promises performance similar to C code for compute intensive parts of a Python application and it supports CUDA, which allows the use of GPUs inside a Python application.In this paper we compare the performance of Numba-CUDA and C -CUDA for different kinds of applications. For compute intensive benchmarks, the performance of the Numba version only reaches between 50% and 85% performance of the CCUDA version, despite the reduction operation, where the Numba version outperforms CUDA. Analyzing the PTX code and CUDA performance counters revealed that index-calculation is one limiting factor in Numba. Another problem is the type interference for single precision computations, as some values are computed in double precision. By optimizing this within the Numba package, the performance of Numba improves. However, C-CUDA applications still outperform the Numba versions. Further analysis with the CloverLeav Mini App shows that Numba performance further decreases for applications with multiple different compute kernels. The non-GPU part slows down these applications, due to the slow Python interpreter. This leads to a worse GPU utilization.Today Python is widely used in industry and academia and has been the first choice of coding languages among software programmers in the last years. Currently, according to the TIOBE index [5], it is the 3rd most popular programming language and the number one in IEEE Spectrum's fifth annual interactive ranking of the top programming languages [4]. One reason for this is that is easier to learn than classical programming languages like C. However, the other reason is the increasing popularity of Data Science, where Python is the most used language. A collection of libraries such as NumPy [22], and Matplotlib [1] or Scipy [8] provide a rich set of functions for scientific computing [16]. Packages like Dask [19], PyCompss [21] and MPI for Python [6] allow running Python applications on large, parallel machines, promising high performance. However, the performance of Python is considered slow compared to compiled languages such as C, C++, and FORTRAN, especially for heavy computations. In recent years, more and more tools have been developed to counter this prejudice. Numpy [22], for example, uses C-like arrays to store data and offers fast functions implemented in C to speed up calculations. The CuPy [14] package provides a similar set of functions, but these functions are implemented for GPUs using CUDA. The SciPy library is based on NumPy and provides a rich set on functionalities for scientific computing. Still, the high performance of these libraries is provided by the underling C-implementations. Internally, they use libraries like OpenBlas or IntelMKL to reach high performance and therefore, they are limited by the functions which are provided by theses libraries. Therefore, a performance problem always arises when the required functionality is not implemented within these libraries. In this case, the application falls back to the Python interpreter. Compared to ""bare metal"" code, interpreted code is slow. In addition, in Python it is not possible to use GPUs or other accelerators directly, as the Python interpreter cannot execute code on these machines. Therefore, the usage is only possible with precompiled libraries. To overcome this limitation, different approaches where developed to mix C, CUDA or OpenCL with Python. Cython [2] allows integrating C-code in Python applications to improve performance of critical sections. It also allows an easy development of wrappers for C-libraries. Similar, packages such as PyCuda and PyOpenCL [9] support wrappers for CUDA or OpenCL code within a Python script. Both approaches require the mixture of different programming languages.Numba [10] follows a different approach. Instead of merging C/CUDA code with Python, it allows the development of efficient applications for both, CPUs and GPUs in Python style. When a Python script using Numba is executed, marked functions are compiled just-in-time (JIT) using the LLVM framework. Using Python for GPU programming can mean a considerable simplification in the development of parallel applications.But often a simplification of comes at the expense of performance, and one expects a performance loss from Python compared to pure C code. In this paper, we want to understand the differences between native C-CUDA code and CUDA-code written in Python with Numba. We also want to share some basic tips how to improve the performance of applications written in Numba.We will first analyse a few micro benchmarks in detail. We are using these simple benchmarks, as it is easier to understand the differences with small code examples. We will use the collected information to derive some optimization for Numba. Finally, we evaluate and compare the performance of a more application like mini-app, written in C-CUDA and Numba accelerated Python. We will evaluate if our insights from the microbenchmarks to real applications.",https://ieeexplore.ieee.org/document/9092407/,"2020 28th Euromicro International Conference on Parallel, Distributed and Network-Based Processing (PDP)",11-13 March 2020,ieeexplore
10.1109/ICPP.2011.40,Location-Aware MapReduce in Virtual Cloud,IEEE,Conferences,"MapReduce is an important programming model for processing and generating large data sets in parallel. It is commonly applied in applications such as web indexing, data mining, machine learning, etc. As an open-source implementation of MapReduce, Hadoop is now widely used in industry. Virtualization, which is easy to configure and economical to use, shows great potential for cloud computing. With the increasing core number in a CPU and involving of virtualization technique, one physical machine can hosts more and more virtual machines, but I/O devices normally do not increase so rapidly. As MapReduce system is often used to running I/O intensive applications, decreasing of data redundancy and load unbalance, which increase I/O interference in virtual cloud, come to be serious problems. This paper builds a model and defines metrics to analyze the data allocation problem in virtual environment theoretically. And we design a location-aware file block allocation strategy that retains compatibility with the native Hadoop. Our model simulation and experiment in real system shows our new strategy can achieve better data redundancy and load balance to reduce I/O interference. Execution time of applications such as RandomWriter, Text Sort and Word Count are reduced by up to 33% and 10% on average.",https://ieeexplore.ieee.org/document/6047196/,2011 International Conference on Parallel Processing,13-16 Sept. 2011,ieeexplore
10.1109/SMC.2017.8123012,Long short term memory networks for short-term electric load forecasting,IEEE,Conferences,"Short-term electricity demand forecasting is critical to utility companies. It plays a key role in the operation of power industry. It becomes all the more important and critical with increasing penetration of renewable energy sources. Short-term load forecasting enables power companies to make informed business decisions in real-time. Demand patterns are extremely complex due to market deregulation and other environmental factors. Although there has been extensive research in the area of short-term electrical load forecasting, difficulties in implementation and lack of transparency in results has been cited as a main challenge. Deep neural architectures have recently shown their ability to mine complex underlying patterns in various domains. In our work, we present a deep recurrent neural architecture to unearth the complex patterns underlying the regional demand profiles without specific insights from the utilities. The model learns from historical data patterns. We show that deep recurrent neural network with long-short term memory architecture presents a robust methodology for accurate short term load forecasting with the ability to adapt and learn the underlying complex features over time. In most cases it matches the performance of the latest state-of-the-art techniques and even supercedes it in a few cases.",https://ieeexplore.ieee.org/document/8123012/,"2017 IEEE International Conference on Systems, Man, and Cybernetics (SMC)",5-8 Oct. 2017,ieeexplore
10.1109/ISSRE.2016.46,MHCP Model for Quality Evaluation for Software Structure Based on Software Complex Network,IEEE,Conferences,"Accidents caused by defective software systems have long been a nightmare. Though engineers utilize advanced techniques and rigorous quality control procedures, we still have to admit that the increasing complexity and expanding scale of software systems make it extremely difficult to guarantee high quality deliverables. Since large-scale software systems exhibit the characteristics of complex networks, applying the principles of complex networks to evaluate the quality of software systems has attracted attention from both academia and industry. Unfortunately, most current research studies focus only on one or a limited number of attributes of software structures which makes them ineffective in providing comprehensive and insightful quality evaluation for software structures. To overcome this problem, we propose an approach based on various software structural characteristics to evaluate software structures from modularity, hierarchy, complexity, and fault propagation points of view. A model based on these four aspects is proposed to better understand software structural quality. A prediction model is also proposed to provide insights on the nature of software evolution and its current status. Experiments using two software projects were performed against the thresholds obtained by evaluating more than 5,000 versions of open source projects. Our results suggest that the approach described in this paper can help us analyze real-world software projects for better quality evaluation.",https://ieeexplore.ieee.org/document/7774529/,2016 IEEE 27th International Symposium on Software Reliability Engineering (ISSRE),23-27 Oct. 2016,ieeexplore
10.1109/ISCA45697.2020.00045,MLPerf Inference Benchmark,IEEE,Conferences,"Machine-learning (ML) hardware and software system demand is burgeoning. Driven by ML applications, the number of different ML inference systems has exploded. Over 100 organizations are building ML inference chips, and the systems that incorporate existing models span at least three orders of magnitude in power consumption and five orders of magnitude in performance; they range from embedded devices to data-center solutions. Fueling the hardware are a dozen or more software frameworks and libraries. The myriad combinations of ML hardware and ML software make assessing ML-system performance in an architecture-neutral, representative, and reproducible manner challenging. There is a clear need for industry-wide standard ML benchmarking and evaluation criteria. MLPerf Inference answers that call. In this paper, we present our benchmarking method for evaluating ML inference systems. Driven by more than 30 organizations as well as more than 200 ML engineers and practitioners, MLPerf prescribes a set of rules and best practices to ensure comparability across systems with wildly differing architectures. The first call for submissions garnered more than 600 reproducible inference-performance measurements from 14 organizations, representing over 30 systems that showcase a wide range of capabilities. The submissions attest to the benchmark's flexibility and adaptability.",https://ieeexplore.ieee.org/document/9138989/,2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA),30 May-3 June 2020,ieeexplore
10.1109/INFOCOM.2018.8485910,MV-Sports: A Motion and Vision Sensor Integration-Based Sports Analysis System,IEEE,Conferences,"Recently, intelligent sports analytics is becoming a hot area in both industry and academia for coaching, practicing tactic and technical analysis. With the growing trend of bringing sports analytics to live broadcasting, sports robots and common playfield, a low cost system that is easy to deploy and performs real-time and accurate sports analytics is very desirable. However, existing systems, such as Hawk-Eye, cannot satisfy these requirements due to various factors. In this paper, we present MV-Sports, a cost-effective system for real-time sports analysis based on motion and vision sensor integration. Taking tennis as a case study, we aim to recognize player shot types and measure ball states. For fine-grained player action recognition, we leverage motion signal for fast action highlighting and propose a long short term memory (LSTM)-based framework to integrate MV data for training and classification. For ball state measurement, we compute the initial ball state via motion sensing and devise an extended kalman filter (EKF)-based approach to combine ball motion physics-based tracking and vision positioning-based tracking to get more accurate ball state. We implement MV-Sports on commercial off-the-shelf (COTS) devices and conduct real-world experiments to evaluate the performance of our system. The results show our approach can achieve accurate player action recognition and ball state measurement with sub-second latency.",https://ieeexplore.ieee.org/document/8485910/,IEEE INFOCOM 2018 - IEEE Conference on Computer Communications,16-19 April 2018,ieeexplore
10.1109/INCET49848.2020.9154161,Machine Learning Approach to Estimation of Internal Parameters of a Single Phase Transformer,IEEE,Conferences,"Transformer is the heart of power industry. So, it is necessary to track its performance every instant. Internal parameters of any transformer helps evaluate the performance and improve output waveform, raising efficiency[1]. They also state the condition of the internal windings[2]. It requires offloading of transformer to evaluate its internal parameters which is very costly since transformer needs to operate always to fully recover the investment. Many iterative analysis techniques have been proposed by many researchers in the past. Machine Learning is the current state of the art technology whose application is proposed in this paper. Various statistical methods are deployed in the training of such algorithms which comprises of Linear Regression, Stochastic Gradient Decent, Support Vector Machine and many more[3]. But, real time applications require minimum latency so Levenberg-Marquardt algorithm has been proposed for evaluation with higher accuracy.",https://ieeexplore.ieee.org/document/9154161/,2020 International Conference for Emerging Technology (INCET),5-7 June 2020,ieeexplore
10.1109/MECO49872.2020.9134073,Machine Learning for Network Routing,IEEE,Conferences,"Though currently a “hot topic”, over the past fifteen years [1][2], there has been significant work on the use of machine learning to design large scale computer-communication networks, motivated by the complexity of the systems that are being considered and the unpredictability of their workloads. A topic of great concern has been security [3] and novel techniques for detecting network attacks have been developed based on Machine Learning [8]. However the main challenge with Machine Learning methods in networks has concerned their compatibility with the Internet Protocol and with legacy systems, and a major step forward has come from the establishment of Software Defined Networks (SDN) [4] which delegate network routing to specific SDN routers [4]. SDN has become an industry standard for concentrating network management and routing decisions within specific SDN routers that download the selected paths periodically to network routers, which operate otherwise under the IP protocol. In this paper we describe our work on real-time control of Security and Privacy [7], Energy Consumption and QoS [6] of packet networks using Machine Learning based on the Cognitive Packet Network [9] principles and their application to the H2020 SerIoT Project [5].",https://ieeexplore.ieee.org/document/9134073/,2020 9th Mediterranean Conference on Embedded Computing (MECO),8-11 June 2020,ieeexplore
10.1109/SNPD.2019.8935703,Machine Learning in Failure Regions Detection and Parameters Analysis,IEEE,Conferences,"Testing automation is one of the challenges facing the software development industry, especially for large complex products. This paper proposes a mechanism called multi stage failure detector (MSFD) for automating black box testing using different machine learning algorithms. The input to MSFD is the tool's set of parameters and their value ranges. The parameter values are randomly sampled to produce a large number of parameter combinations that are fed into the software under test. Using neural networks, the resulting logs from the tool are classified into passing and failing logs and the failing logs are then clustered (using mean shift clustering) into different failure types. MSFD provides visualization of the failures along with the responsible parameters. Experiments on and results for two real-world complex software products are provided, showing the ability of MSFD to detect all failures and cluster them into the correct failure types, thus reducing the analysis time of failures, improving coverage, and increasing productivity.",https://ieeexplore.ieee.org/document/8935703/,"2019 20th IEEE/ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing (SNPD)",8-11 July 2019,ieeexplore
10.1109/SIMS.2018.8355297,Machine learning algorithms for estimating powder blend composition using near infrared spectroscopy,IEEE,Conferences,"This paper presents a NIRS based real time continuous monitoring of powder blend composition which has widespread applications such as the pharmaceutical industry. The paper extends the implementation of several machine learning methodologies applied to sensor data collected using an NIR spectrometer for a model powder blending process. Several techniques were examined for the development of chemometric models of the multi-sensor data, including Principal Component Analysis (PCA), Partial Least Squares Regression (PLSR), Support Vector Machines (SVM) and Artificial Neural Networks (ANN). The performances of each of the models were compared in terms of accuracy (MSE) in predicting blend composition. The results obtained show that machine learning-based approaches produce process models of similar accuracy and robustness compared to models developed by PLSR while requiring minimal pre-processing and also being more adaptable to new data. The paper also discusses the prospect of using Convolutional Neural Networks (CNN) for NIRS data analysis.",https://ieeexplore.ieee.org/document/8355297/,2018 2nd International Symposium on Small-scale Intelligent Manufacturing Systems (SIMS),16-18 April 2018,ieeexplore
10.1109/WCICSS.2016.7882607,Machine learning algorithms for process analytical technology,IEEE,Conferences,"Increased globalisation and competition are drivers for process analytical technologies (PAT) that enable seamless process control, greater flexibility and cost efficiency in the process industries. The paper will discuss process modelling and control for industrial applications with an emphasis on solutions enabling the real-time data analytics of sensor measurements that PAT demands. This research aims to introduce an integrated process control approach, embedding novel sensors for monitoring in real time the critical control parameters of key processes in the minerals, ceramics, non-ferrous metals, and chemical process industries. The paper presents a comparison of machine learning algorithms applied to sensor data collected for a polymerisation process. Several machine learning algorithms including Adaptive Neuro-Fuzzy Inference Systems, Neural Networks and Genetic Algorithms were implemented using MATLAB® Software and compared in terms of accuracy (MSE) and robustness in modelling process progression. The results obtained show that machine learning-based approaches produce significantly more accurate and robust process models compared to models developed manually while also being more adaptable to new data. The paper presents perspectives on the potential benefits of machine learning algorithms with a view to their future in the industrial process industry.",https://ieeexplore.ieee.org/document/7882607/,2016 World Congress on Industrial Control Systems Security (WCICSS),12-14 Dec. 2016,ieeexplore
10.1109/IntelliSys.2017.8324372,Machine learning and deep neural network — Artificial intelligence core for lab and real-world test and validation for ADAS and autonomous vehicles: AI for efficient and quality test and validation,IEEE,Conferences,"Autonomous vehicles are now the future of automobile industry. Human drivers can be completely taken out of the loop through the implementation of safe and intelligent autonomous vehicles. Although we can say that HW and SW development continues to play a large role in the automotive industry, test and validation of these systems is a must. The ability to test these vehicles thoroughly and efficiently will ensure their proper and flawless operation. When a large number of people with heterogeneous knowledge and skills try to develop an autonomous vehicle together, it is important to use a sensible engineering process. State of the art techniques for such development include Waterfall, Agile &amp; V-model, where test &amp; validation (T&amp;V) process is an integral part of such a development cycle. This paper will propose a new methodology using machine learning &amp; deep neural network (AI-core) for lab &amp; real-world T&amp;V for ADAS (Advanced driver assistance system) and autonomous vehicles. The methodology will initially connect T&amp;V of individual systems in each level of development and that of complete system efficiently, by using the proposed phase methodology, in which autonomous driving functions are grouped under categories, special T&amp;V processes are carried on simulation as well as in HIL systems. The complete transition towards AI in the field of T&amp;V will be a sequence of steps. Initially the AI-core is fed with available test scenarios, boundary conditions for the test cases and scenarios, and examples, the AI-core will conduct virtual tests on simulation environment using available test scenarios and further generates new test cases and scenarios for efficient and precise tests. These test cases and scenarios are meant to cover all available cases and concentrate on the area where bugs or failures occur. The complete surrounding environment in the simulation is also controlled by the AI-core which means that the system can attain endless/all-possible combinations of the surrounding environment which is necessary. Results of the tests are sorted and stored, critical and important tests are again repeated in the real-world environment using automated cars with other real subsystems to depict the surrounding environment, which are all controlled by the AI-core, and meanwhile the AI-core is always in the loop and learning from each and every executed test case and its results/outcomes. The main goal is to achieve efficient and high quality test and validation of systems for automated driving, which can save precious time in the development process. As a future scope of this methodology, we can step-up to make most parts of test and validation completely autonomous.",https://ieeexplore.ieee.org/document/8324372/,2017 Intelligent Systems Conference (IntelliSys),7-8 Sept. 2017,ieeexplore
10.1049/cp.2020.0049,Machine learning based anomaly-based intrusion detection system in a full digital substation,IET,Conferences,"The cyberattacks that occurred in recent years have raised concerns in critical infrastructures, including power system networks. Identifying ongoing attacks is essential to enable the energy industry to respond to adversaries. Many commercial products and research projects include machine learning based intrusion detection systems but there is still a need for understanding the data training requirements for those systems in order to successfully deploy them to protect power systems. This paper presents the development of an anomaly-based Intrusion Detection System (IDS) based on a machine learning methodology to create a whitelist. The system was implemented using GNU Octave. It was trained using traffic flow from real devices generated from a Virtual Site Acceptance Testing and Training (VSATT) platform where multi-vendor secondary devices were set up and communicated to each other. The system was then tested using different datasets which were also generated from the VSATT platform. Results show that the implemented IDS performed correctly under different case studies. The results also indicate that the learned traffic identifies GOOSE and MMS messages based on the normal behaviours from those protocols, but the presence of other messages might require manual inputs to be incorporated in the training dataset.",https://ieeexplore.ieee.org/document/9449350/,15th International Conference on Developments in Power System Protection (DPSP 2020),9-12 March 2020,ieeexplore
10.1109/CompComm.2017.8322600,Machine learning to detect anomalies in web log analysis,IEEE,Conferences,"As the information technology develops rapidly, Web servers are easily to be attacked because of their high value. Therefore, Web security has aroused great concern in both academia and industry. Anomaly detection plays a significant role in the field of Web security, and log messages recording detailed system runtime information has become an important data analysis object accordingly. Traditional log anomaly detection relies on programmers to manually inspect by keyword search and regular expression match. Although the programmers can use intrusion detection system to reduce their workload, yet the log system data is huge, attack types are various, and hacking skills are improving, which make the traditional detection not efficient enough. To improve the traditional detection technology, many of anomaly detection mechanisms have been proposed in recent years, especially the machine learning method. In this paper, an anomaly detection system for web log files has been proposed, which adopts a two-level machine learning algorithm. The decision tree model classifies normal and anomalous data sets. The normal data set is manually checked for the establishment of multiple HMMs. The experimental data comes from the real industrial environment where log files have been collected, which contain many true intrusion messages. After comparing with three types of machine learning algorithms used in anomaly detection, the experimental results on this data set suggest that this system achieves higher detection accuracy and can detect unknown anomaly data.",https://ieeexplore.ieee.org/document/8322600/,2017 3rd IEEE International Conference on Computer and Communications (ICCC),13-16 Dec. 2017,ieeexplore
10.1109/DISCOVER.2018.8673981,MalDetect: A Framework to detect Fast Flux Domains,IEEE,Conferences,"Performing passive or active attacks through malware-infected systems (bots) by hiding the identity of an attacker, referred to as fast flux is one of the common threat in the context of security. With the connectivity of millions of unsecured systems to networks, detecting fast-flux based attack is one of the major challenge to the industry. This paper presents, a framework which discriminates fast flux domains from Content Distribution Network (CDN) in real-time. The proposed framework embeds features such as, DNS query response, Geographical-location of IP addresses, network distinction and delay in our framework to detect the fast flux domains. Our model has been evaluated using five different machine learning algorithms and out of which, the Random Forest (RF) algorithm performed the best with an F1 score of 0.9915 and Matthews Correlation Coefficient of 0.9672. We also did experimentation on different feature sets individually to identify the best performing feature set in detecting the fast flux domains. We observed Geographical location-based feature set outperformed than the other feature set with a significant accuracy and precision.",https://ieeexplore.ieee.org/document/8673981/,"2018 IEEE Distributed Computing, VLSI, Electrical Circuits and Robotics (DISCOVER)",13-14 Aug. 2018,ieeexplore
10.1109/DTPI52967.2021.9540077,Mechanical Design Paradigm based on ACP Method in Parallel Manufacturing,IEEE,Conferences,"Parallel Manufacturing is a new manufacturing paradigm in industry, deeply integrating informalization, automation, and artificial intelligence. In this paper we propose a new mechanical design paradigm in Parallel Manufacturing based on ACP method. The key is to regard the design procedure based on artificial design and emulation method as two independent procedures, which can be modeled as a parallel system. The design procedure based on ACP method does not include a real system, which is an inventive extension of the traditional parallel system. This method can be implemented with social information by introducing the definition of SDV, SDM, and Intelligent Design Manager, making it highly adaptive for social manufacturing and Parallel Manufacturing.",https://ieeexplore.ieee.org/document/9540077/,2021 IEEE 1st International Conference on Digital Twins and Parallel Intelligence (DTPI),15 July-15 Aug. 2021,ieeexplore
10.1109/SNPD.2014.6888682,Microcredit risk assessment using crowdsourcing and social networks,IEEE,Conferences,"The task of automated risk assessment is attracting significant attention in the light of the recent microloan popularity growth. The industry requires a real time method for the timely processing of the extensive number of applicants for short-term small loans. Owing to the vast number of applications, manual verification is not a viable option. In cooperation with a microloan company in Azerbaijan, we have researched automated risk assessment using crowdsourcing. The principal concept behind this approach is the fact that a significant amount of information relating to a particular applicant can be retrieved from the social networks. The suggested approach can be divided into three parts: First, applicant information is collected on social networks such as LinkedIn and Facebook. This can only occur with the applicant's permission. Then, this data is processed using a program that extracts the relevant information segments. Finally, these information segments are evaluated using crowdsourcing. We attempted to evaluate the information segments using social networks. To that end, we automatically posted requests on the social networks regarding certain information segments and evaluated the community response by counting “likes” and “shares”. For example, we posted the status, “Do you think that a person who has worked at ABC Company is more likely to repay a loan? Please “like” this post if you agree.” From the results, we were able to estimate public opinion. Once evaluated, each information segment was then given a weight factor that was optimized using available loan-repay test data provided to us by a company. We then tested the proposed system on a set of 400 applicants. Using a second crowdsourcing approach, we were able to confirm that the resulting solution provided a 92.5% correct assessment, with 6.45% false positives and 11.11% false negatives, with an assessment duration of 24 hours.",https://ieeexplore.ieee.org/document/6888682/,"15th IEEE/ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing (SNPD)",30 June-2 July 2014,ieeexplore
10.1109/ECAI.2017.8166463,Mitigating DoS attacks in publish-subscribe IoT networks,IEEE,Conferences,"Internet of Things (IoT) is an emerging subject which enables multiple applications and requires robust security solutions. IoT architectures contribute to critical aspects of our society like transportation, health-care, industry, telecommunications and many others. Security is difficult to be implemented in the IoT context because the embedded devices are resource constrained and deployed in uncontrolled areas, thus being targeted by various security attacks. In scenarios like smart city, IoT networks are populated by both trusted and untrusted transient devices, thus mechanisms for mitigating complex security attacks must be implemented. Embedded networks with near real-time constraints are subject to denial-of-service attacks which can easily affect the applications availability by injecting malicious packets into the network. This paper proposes a lightweight security mechanism which addresses DoS attacks in IoT publish-subscribe applications.",https://ieeexplore.ieee.org/document/8166463/,"2017 9th International Conference on Electronics, Computers and Artificial Intelligence (ECAI)",29 June-1 July 2017,ieeexplore
10.1109/ISADS.1999.838444,Mobile agents in autonomous decentralized systems,IEEE,Conferences,"In the last years mobile agents have gained much attention in the industry and academic community. Many prototype systems and some products providing mobile agent capability have been developed and mobile agents have been proposed for being used in a set of application areas, such as electronic commerce, workflow systems, network management, among others. In the last years we also experienced an enormous development in the direction of network centric computing. Complex distributed environments emerge that enable and support the integration of systems and that allow new forms of automated cooperation. For an effective integration to take place, the autonomy of participants in the environment must be taken into consideration. Such environments represent then examples of autonomous decentralised systems (ADSs). In this paper the authors present their view on the convenience of applying mobile agents to ADSs. Firstly, some benefits of mobile agents that can be explored in applications are presented. Afterwards, a discussion of some issues that shall enable more effective use of mobile agents in real applications and that have impact on the definition of the scope of applicability of mobile agents to ADSs is presented.",https://ieeexplore.ieee.org/document/838444/,Proceedings. Fourth International Symposium on Autonomous Decentralized Systems. - Integration of Heterogeneous Systems -,23-23 March 1999,ieeexplore
10.1109/SCC.2018.00014,Modeling Sentiment Polarity in Support Ticket Data for Predicting Cloud Service Subscription Renewal,IEEE,Conferences,"In the cloud based service provisioning industry, one of the main challenges that providers face involves keeping existing tenants engaged while attracting new ones. To address this, providers need to gain insights about customer satisfaction. In that regards, support ticket data, understood as the main way of communication between both parties, can be mined to obtain an estimation of customer satisfaction by means of the polarity of the sentiment extracted from the report descriptions. To that end, in this work we propose a model which can learn a feature representation for sentiment polarity changes, from the sequence of tickets emitted by a given customer during the period associated with the service subscription term. Then, that resulting feature representation, combined with other handcrafted features related to contract and ticket data, is passed to a classifier which estimates the likelihood of service subscription renewal by the customer. Experiment results using real data from a service provider shows that learned representation of sentiment polarity changes from support ticket data in combination with other handcrafted features improves the accuracy in predicting subscription renewals. Moreover, our architecture is flexible enough incorporate and integrate several feature representations and give more expressive power to the prediction.",https://ieeexplore.ieee.org/document/8456400/,2018 IEEE International Conference on Services Computing (SCC),2-7 July 2018,ieeexplore
10.1109/ICAT.2006.85,Modeling and Application of Virtual Machine Tool,IEEE,Conferences,"The recent years of the 21th Century are associated with the advent of virtual reality technologies for modern industry and manufacturing engineering. Virtual Machine Tool Technology is given to design, test, control and machine parts in a virtual reality environment. This paper presents the methods to model and simulate the virtual machine tools in response to change in the machining requirements. Specifically, a set of module combination rules and a modeling method of the structure of machine tools using connectivity graph are developed. By this way virtual machine tool is implemented. The developed virtual machine tool can be efficiently used for industry training and machine leaning and operating.",https://ieeexplore.ieee.org/document/4089203/,16th International Conference on Artificial Reality and Telexistence--Workshops (ICAT'06),29 Nov.-1 Dec. 2006,ieeexplore
10.1109/ISIE45063.2020.9152407,Modeling and Predicting an Industrial Process Using a Neural Network and Automation Data,IEEE,Conferences,"Production optimization and prevention of faults and unplanned production halts are areas of particular interest in industry. Predictive analysis is commonly implemented with data analytics and machine learning techniques. Usually, the usage of such tools requires knowledge of the machine learning theory and the subject to be studied, e.g. a pumping process. This paper presents a case study on modeling of a pumping process using stored automation data. The model is trained to predict the performance percentage of the process with minimal background knowledge of the process and data analytics. The proposed model is built with IBM SPSS Modeler, a data analysis tool not usually used in real-time industrial predictive analysis as it is not often considered the best tool when working with time series data. The model is deployed in a cloud service to implement a real-time, visualized predictive analysis system. The case study shows that Modeler can be used for data analysis, modeling, and production purposes. Depending on the case, Modeler can provide an alternative tool compared with typical machine learning tools, as models built with Modeler can be deployed into a cloud service for production use. The findings indicate that industrial automation data are a valuable resource, and data analysis can be conducted on various platforms and tools.",https://ieeexplore.ieee.org/document/9152407/,2020 IEEE 29th International Symposium on Industrial Electronics (ISIE),17-19 June 2020,ieeexplore
10.1109/SCC47175.2019.9116104,Modeling and management of human resources in the reconfiguration of production system in industry 4.0 by neural networks,IEEE,Conferences,"In Industry 4.0, the role of employees changes significantly. Real-time production line control transforms job content. Work processes affect working conditions. The implementation of a socio-technical approach to the organization of work gives workers the opportunity to adapt their skills. Indeed, production work will become more and more multi-factor, especially with regard to control and decision-making tasks. In this paper, a proposal for an intelligent system for modeling skills and human resource management in the production system chain through the use of two artificial neural networks. The first NN1 network allows for the identification of the human factor, as well as the second NN2 network is reserved for valuing the human skills needed in Industry 4.0.",https://ieeexplore.ieee.org/document/9116104/,"2019 International Conference on Signal, Control and Communication (SCC)",16-18 Dec. 2019,ieeexplore
10.1109/ENABL.1996.555181,Modelling project coordination in a multi-agent framework,IEEE,Conferences,"In current engineering practice often traditional management structures and virtual organisations are combined. In addition to formal structures, informal, dynamic organisational structures emerge in which engineers are personally responsible for effective interaction. They decide when to exchange information, and with whom, when to question requirements, when to acknowledge conflicts, et cetera. In such virtual organisations, project coordination may become quite complicated. Communication and coordination in a real-life case of concurrent design in the aircraft industry has been modelled and specified within the modelling framework DESIRE, on the basis of Jennings' (1995) informal, multi-agent model of cooperative problem solving.",https://ieeexplore.ieee.org/document/555181/,Proceedings of WET ICE '96. IEEE 5th Workshop on Enabling Technologies; Infrastucture for Collaborative Enterprises,19-21 June 1996,ieeexplore
10.1109/ISCSCT.2008.104,Monitoring and Early-Warning of the Supply Chain by Using System Dynamics and Neural Networks,IEEE,Conferences,"Effective management of a supply chain requires the ability to detect unexpected variations at an early stage, which brings the possibility of taking preventive decisions to mitigate the variations. This paper proposes a methodology that monitors the dynamic trends of supply chain performance indicators, and gives early-warnings for potential risks. Initially, a supply chain model is built using system dynamics. Then, based on this model, a neural network which can be trained to adapt to the real supply chain is developed. Acting as the kernel of monitoring and early-warning module, the neural network can make online predictions of dynamic trend of indicators so that an enterprise would have enough time to respond to any unwanted situations. The architecture of monitoring and early-warning module is proposed and a case study of the manufacturing industry is presented to illustrate the methodology and architecture.",https://ieeexplore.ieee.org/document/4731437/,2008 International Symposium on Computer Science and Computational Technology,20-22 Dec. 2008,ieeexplore
10.1109/WF-IoT.2019.8767291,Mountain Pine Beetle Monitoring with IoT,IEEE,Conferences,"Outbreaks of forest pests cause large-scale damages, which lead to significant impact on the ecosystem as well as the forestry industry. Current methods of monitoring pest outbreaks involve field, aerial and remote sensing surveys. These methods only provide partial spatial coverage and can detect outbreaks only after they have substantially progressed across wide geographic areas. This paper presents an IoT system for real-time insect infestation detection using bioacoustic recognition via machine learning techniques. Specifically, we focus on detecting the Mountain Pine Beetle (MPB), which is the most destructive insect of mature pines in western North American forests. We present the design of the system and describe its various hardware and software components. Experimental results collected from a prototype implementation of the system are presented, which show that the system can detect MPB with 82% accuracy. We also demonstrate the applicability of our system in other noise monitoring applications, and report our experimental results on urban noise detection and classification.",https://ieeexplore.ieee.org/document/8767291/,2019 IEEE 5th World Forum on Internet of Things (WF-IoT),15-18 April 2019,ieeexplore
10.1109/ISCAS.2019.8702575,Multi-View Fusion Neural Network with Application in the Manufacturing Industry,IEEE,Conferences,"In recent years the research community and industry have paid high attention to the field of machine learning, especially deep learning. Nowadays many real-world classification or rather prediction applications are implemented by neural network models. We propose a multi-view fusion neural network with application in the manufacturing industry. Image information of multiple cameras is fused and used by the proposed model to predict the state of a manufacturing machine. Experiments show that the overall classification performance is increased from a baseline of 92.7% to 99.5% by the fusion model.",https://ieeexplore.ieee.org/document/8702575/,2019 IEEE International Symposium on Circuits and Systems (ISCAS),26-29 May 2019,ieeexplore
10.23919/DATE48585.2020.9116407,Network Synthesis for Industry 4.0,IEEE,Conferences,"Today's factory machines are ever more connected with SCADA, MES, ERP applications as well as external systems for data analysis. Different types of network architectures must be used for this purpose. For instance, control applications at the lowest level are susceptible to delays and errors while data analysis with machine learning procedures requires to move a large amount of data without real-time constraints. Standard data formats, like Automation Markup Language (AML), have been established to document factory environment, machine placement and network deployment, however, no automatic technique is currently available in the context of Industry 4.0 to choose the best mix of network architectures according to spacial constraints, cost, and performance. We propose to fill this gap by formulating an optimization problem. First of all, spatial and communication requirements are extracted from the AML description. Then, the optimal interconnection of wired or wireless channels is obtained according to application objectives. Finally, this result is back-annotated to AML to be used in the life cycle of the production system. The proposed methodology is described through a small, but complete, smart production plant.",https://ieeexplore.ieee.org/document/9116407/,"2020 Design, Automation & Test in Europe Conference & Exhibition (DATE)",9-13 March 2020,ieeexplore
10.1109/UralCon52005.2021.9559619,Neural Network for Real-Time Signal Processing: the Nonlinear Distortions Filtering,IEEE,Conferences,"Artificial neural networks, after their training and testing, allow, in the ""if then"" mode, to process signals in real time. This is relevant for signal processing tasks in electrical engineering and the electric power industry, especially for the analysis of nonlinear signal distortions, transients in electrical networks, during switching, emergency modes, and so on. The paper shows the neuro algorithm possibilities based on an elementary perceptron for filtering nonlinear distortions of industrial frequency signals of 50 Hz over a time interval of units of milliseconds. At the same time, the accuracy for the signals’ amplitude, phase, and frequency determining is units of percent. Examples of the fundamental frequency signal selection in the presence of harmonics, the determination of the signal parameters during transients, and the correction of the transformer saturation current are given. It is shown that real-time neural network processing can be carried out in a ""sliding window"" with the duration of units of milliseconds. The estimates made during the implementation of the neuro algorithm in a microprocessor device showed its application possibility in the electric power industry secondary equipment.",https://ieeexplore.ieee.org/document/9559619/,2021 International Ural Conference on Electrical Power Engineering (UralCon),24-26 Sept. 2021,ieeexplore
10.1109/IISA50023.2020.9284345,New Advanced Technology Methods for Energy Efficiency of Buildings,IEEE,Conferences,"Energy consumed by buildings represents a large part of the world’s total energy consumption with a total share of 40%. This is the reason why energy efficiency of buildings has become a very important scientific field. For the purpose of this paper a critical review of old and new methods of controlling the parts of a building’s automation and thus achieving energy savings are compared, analyzed and presented. The method of Fuzzy Cognitive Maps (FCM) and its significant impact on the improvement of the management of a building is being presented. FCMs is a new soft computing method which combine neural networks and Fuzzy Logic. They have been used with very promising results in many fields such as medicine, transportation, manufacturing agriculture, food industry and energy. In this paper the use of FCMs is exploited and specifically used in issues of energy efficiency of buildings. Obtained results, simulation and experimental, for case studies where FCMs were used in buildings, of residential and commercial use, in Southern Greece will be presented. Software tools based on the aforementioned applications will be briefly presented. In the near future these tools are going to be integrated in even more buildings thus giving us real data which can and will be used in future research for moving from high energy consumption to Net-Zero Energy Buildings (NZEB).",https://ieeexplore.ieee.org/document/9284345/,"2020 11th International Conference on Information, Intelligence, Systems and Applications (IISA",15-17 July 2020,ieeexplore
10.1109/AFRCON.1996.562998,New solutions in the control of the Hungarian power system,IEEE,Conferences,"In the frame of a reconstruction project on the Hungarian Power Control System a lot of intelligent control tools are investigated. Some are accepted, others are rejected. Since 1993 the first Dispatcher Training Simulator of the Hungarian Electric Power Industry has been working in the regional dispatcher center of the North Hungarian Electricity Distribution Company (EMASZ KDSZ). The simulator performs discrete, event-driven simulation of the network, the protection and the telemetry system. A new continuous approach to the power system restoration problem is proposed. The continuous restoration means an on-line function that supports all the dispatcher actions. It is active and alerts not only the restorative network state, but also in the normal state. This real time expert system is strongly based on the co-operation with the SCADA-EMS functions and the different off-line network calculations. The idea is proposed to be implemented in the Hungarian National Control Centre. In 1994-95 a common project ('A Neural Network Application to Power System's Signal Processing') was performed in the institute KFKI MSZKI. We sought for the appropriate neural network structure, which fits the best the problem of event recognition. The research was extended for a sequential pattern matching algorithm too. On the base of the results we are working on an event recognizer application.",https://ieeexplore.ieee.org/document/562998/,Proceedings of IEEE. AFRICON '96,27-27 Sept. 1996,ieeexplore
10.1109/ISIE45063.2020.9152527,Next generation control units simplifying industrial machine learning,IEEE,Conferences,"The increasing amount of sensor data creates new possibilities through data-driven projects in industry. The demands on the final solution architecture are different and typically include at least one controller (e.g. PLC). In this paper, we focus on these industrial controllers and identify their possible roles in smart factories: Collection, distribution and preprocessing of field data, execution of intelligence and functionalities to ease rapid prototyping approaches. Furthermore, we specify the capabilities leading to the fulfillment of these roles and present an application example of drive anomaly detection to demonstrate how to setup an industrial controller for machine learning projects.",https://ieeexplore.ieee.org/document/9152527/,2020 IEEE 29th International Symposium on Industrial Electronics (ISIE),17-19 June 2020,ieeexplore
10.1109/MED.2016.7535908,Nonlinear model predictive control hardware implementation with custom-precision floating point operations,IEEE,Conferences,"Model predictive control (MPC) based techniques have found many applications both in academia and in industry. Its reach, however, may not be compared to classical control techniques due to e.g. the difficulty of solving an optimization problem at each sampling interval with real-time requirements. Most of the efforts to make the application of MPC viable address this problem with more efficient solvers. This paper, in contrast, proposes a new approach for a real-time MPC solution by mapping an approximate off-line solution into an artificial neural network in a FPGA (Field Programmable Gate Array). We implemented a radial basis function artificial neural network on a low cost FPGA using custom precision floating point operations and tested the control on a single-link robotic manipulator. The amount of time used to calculate the control action at each time instant is in around one microsecond. The comparison between the offline and the approximate solution shows the soundness of the idea. We provide an analysis of hardware usage and execution time in order to achieve the best compromise considering the precision for a given application.",https://ieeexplore.ieee.org/document/7535908/,2016 24th Mediterranean Conference on Control and Automation (MED),21-24 June 2016,ieeexplore
10.1109/ICCCNT49239.2020.9225680,"Notice of Violation of IEEE Publication Principles: 6G Wireless Communication: Its Vision, Viability, Application, Requirement, Technologies, Encounters and Research",IEEE,Conferences,"The fast development of multiband ultrafast seamless network and super reliable data transmission system to support heavy traffic applications such as artificial intelligence, machine learning, deep learning, augmented reality, virtual reality, 3D media, Internet of Things, Enterprise Internet of Thing and the Internet of Nano-things that involves with the real time transfer of data, voice and video in terabytes per second (Tb/s), the current cellular network (5G Network is insufficient to meet the growth of usage of triple play services in fraction of time). To meet the expectation of heavy data users is a big challenge in today's generation. To handle the situation of drastic demand of data, the sixth generation of mobile technology (6G) should be deeply studied along with its potential in terms of bandwidth, low latency, channel capacity, channel modeling techniques, loss propagation models, energy spectrum efficiency, faster network connectivity and data security. In this paper the vision in terms intelligent computing and wireless massive connectivity, feasibility, requirement in terms of modifying the existing 5G network, technologies in terms of artificial intelligence, 3D networking, SM-MIMO and optical computing, challenges after deployment, research to promote good health for 6G and application of 6G in the field of industry, automation sector, health, and transport has been studied and presented.",https://ieeexplore.ieee.org/document/9225680/,"2020 11th International Conference on Computing, Communication and Networking Technologies (ICCCNT)",1-3 July 2020,ieeexplore
10.1109/SNPD.2016.7515945,Notice of Violation of IEEE Publication Principles: Adaptive Fuzzy PID speed control of DC belt conveyor system,IEEE,Conferences,Conveyor belt system is one of the most common transfer systems used in industry to transfer goods from one point to another in a limited distance. It is used in industries such as the electromechanical /mechanical assembly manufacturing to transfer work piece from one station to another or one process to another in food industries. The belt conveyor system discussed in this paper is driven by a DC motor and two speed controllers. The PID speed controller is designed to provide comparison to the main controller which is the Adaptive Fuzzy PID Speed controller. Both controllers are implemented in a real hardware where the algorithm will be written in PLC using SCL language. The experimental result shows that Adaptive Fuzzy PID controller performs better and adapted to the changes in load much faster than the conventional PID controller. This project has also proved that PLC is capable of performing high level control system tasks..,https://ieeexplore.ieee.org/document/7515945/,"2016 17th IEEE/ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing (SNPD)",30 May-1 June 2016,ieeexplore
10.1109/RTSI.2017.8065980,OPEB: Open physical environment benchmark for artificial intelligence,IEEE,Conferences,"Artificial Intelligence methods to solve continuous-control tasks have made significant progress in recent years. However, these algorithms have important limitations and still need significant improvement to be used in industry and real-world applications. This means that this area is still in an active research phase. To involve a large number of research groups, standard benchmarks are needed to evaluate and compare proposed algorithms. In this paper, we propose a physical environment benchmark framework to facilitate collaborative research in this area by enabling different research groups to integrate their designed benchmarks in a unified cloud-based repository and also share their actual implemented benchmarks via the cloud. We demonstrate the proposed framework using an actual implementation of the classical mountain-car example and present the results obtained using a Reinforcement Learning algorithm.",https://ieeexplore.ieee.org/document/8065980/,2017 IEEE 3rd International Forum on Research and Technologies for Society and Industry (RTSI),11-13 Sept. 2017,ieeexplore
10.1109/CCIS48116.2019.9073682,On Improving Text Generation Via Integrating Text Coherence,IEEE,Conferences,"Automatic text generation techniques with either extractive-based or generative-based methods are becoming increasingly popular and widely used in industry. In contrast to existing extractive-based text generation approaches that ignore the text coherence, our proposed approach integrates both keyword coverage and text coherence into our optimization framework. In this paper, we employ the semantics-based coherence and syntax-based coherence metrics to evaluate the text coherence. Extensive experiments on a real corpus demonstrate that our method outperforms baselines overall regrading ROUGE and Human evaluation metrics. Our model provides new insights on how to utilize coherence measures to arrange the sentences extracted by keyword covering method. The proposed method has been deployed on a real system to help generate coherent text.",https://ieeexplore.ieee.org/document/9073682/,2019 IEEE 6th International Conference on Cloud Computing and Intelligence Systems (CCIS),19-21 Dec. 2019,ieeexplore
10.1109/NCC52529.2021.9530062,On Traffic Classification in Enterprise Wireless Networks,IEEE,Conferences,"Enterprises today are quickly adopting intelligent, adaptive, and flexible wireless communication technologies in order to become compliant with Industry 4.0. One of the technological challenges related to this is to provide Quality of Services (QoS)-enabled network connectivity to the applications. Diverse QoS demands from the applications intimidate the underlying wireless networks to be agile and adaptive. Since the applications are diverse in nature, there must be a mechanism to learn the application types in near real-time so that the network can be provisioned accordingly. In this paper, we propose a Machine Learning (ML) based method to classify the application traffic. Our method is different from the existing port based and Deep Packet Inspection (DPI) based methods and uses statistical features of the network traffic related to the applications. We validate the performance of the proposed model in a lab based SDNized WiFi set-up. SDNization ensures that the proposed model can be deployed in practice.",https://ieeexplore.ieee.org/document/9530062/,2021 National Conference on Communications (NCC),27-30 July 2021,ieeexplore
10.1109/IJCNN.2015.7280779,On sequences of different adaptive mechanisms in non-stationary regression problems,IEEE,Conferences,"Existing adaptive predictive methods often use multiple adaptive mechanisms as part of their coping strategy in non-stationary environments. These mechanisms are usually deployed in a prescribed order which does not change. In this work we investigate and provide a comparative analysis of the effects of using a flexible order of adaptive mechanisms' deployment resulting in varying adaptation sequences. As a vehicle for this comparison, we use an adaptive ensemble method for regression in batch learning mode which employs several adaptive mechanisms to react to the changes in data. Using real world data from the process industry we demonstrate that such flexible deployment of available adaptive methods embedded in a cross-validatory framework can benefit the predictive accuracy over time.",https://ieeexplore.ieee.org/document/7280779/,2015 International Joint Conference on Neural Networks (IJCNN),12-17 July 2015,ieeexplore
10.1109/SITIS.2019.00053,On the Utility of Machine Learning for Service Capacity Management of Enterprise Applications,IEEE,Conferences,"The IT industry drives the digital transformation and, at the same time, is affected itself by related trends of automation and computerization. This paper examines the applicability of machine-learning techniques to the process of service capacity management for Commercial-off-the-shelf enterprise applications. We use real monitoring data from more than 18.000 SAP application and database instances which are running on more than 16.000 different servers in order to train performance models for standard business functions. A learning algorithm which is based on Boosted trees achieves sufficient accuracy to predict mean response times for ten frequently used transactions. To evaluate utility, models are successfully applied as part of a scenario-based demonstration in the fields of server sizing, load testing, and server consolidation with the objective to identify cost-effective designs. Results strongly emphasize the need to integrate monitoring data from uniform business applications in order to allow for novel and cost-effective capacity management service offerings.",https://ieeexplore.ieee.org/document/9067964/,2019 15th International Conference on Signal-Image Technology & Internet-Based Systems (SITIS),26-29 Nov. 2019,ieeexplore
10.1109/AUTEST.1989.81113,On the evaluation of real life test expert systems,IEEE,Conferences,"The authors consider the factors affecting the successful integration of expert systems (ESs) in testing and maintenance environments. The first factor considered has to do with the communication between the ES and the UUT (unit under test) expert and between the ES and the test technician. The authors discuss the importance of embedding in the expert system basic understanding of electronic terms and universal knowledge bases, i.e. knowledge which is not unique to a specific UUT. The second factor considered has to do with the communication between the ES and the ATE (automatic test equipment). It is claimed that artificial intelligence software will not penetrate the real-world test industry, unless it offers very smooth interfaces with test instrumentation and ATE. Several specific requirements are discussed. This work is based on extensive experience in introducing the AITEST expert system for electronic troubleshooting and service management to a wide variety of fields; including aerospace and military, automotive, computers and peripherals, communication and general instrumentation.&lt;<ETX>&gt;</ETX>",https://ieeexplore.ieee.org/document/81113/,IEEE Automatic Testing Conference.The Systems Readiness Technology Conference. Automatic Testing in the Next Decade and the 21st Century. Conference Record.,25-28 Sept. 1989,ieeexplore
10.23919/ChiCC.2018.8483784,On-line Detection and Analysis of Alloy Steel Elements Based on the LIBS Technology and Random Forest Regression,IEEE,Conferences,"The Laser Induced Breakdown Spectroscopy (LIBS) technology can be used to detect the elements in the alloy steel in real time. Quantitative analysis method of the traditional LIBS technology mainly has the calibration method and calibration free method, but there are two shortcomings: low prediction accuracy and over fitting. Random Forest Regression (RFR) algorithm can be used for classification and regression, can effectively avoid “overfitting” phenomenon. Therefore, in this paper, we combine the random forest regression algorithm with laser induced breakdown spectroscopy applied to the detection of the concentration of alloy steel elements in the metallurgy industry. At the same time, compared with partial least squares method based on the LIBS, the results show that the random forest algorithm combined with the LIBS technology has the higher prediction accuracy, lower root mean square error and better robustness.",https://ieeexplore.ieee.org/document/8483784/,2018 37th Chinese Control Conference (CCC),25-27 July 2018,ieeexplore
10.1109/ASCC.2015.7244597,Online sequential extreme learning machine algorithm based human activity recognition using inertial data,IEEE,Conferences,"Human activity recognition (HAR) is the basis for many real world applications concerning health care, sports and gaming industry. Different methodological perspectives have been proposed to perform HAR. One appealing methodology is to take an advantage of data that are collected from inertial sensors which are embedded in the individual's smartphone. These data contain rich amount of information about daily activities of the user. However, there is no straightforward analytical mapping between a performed activity and its corresponding data. Besides, online training for the classification in these types of applications is a concern. This paper aims at classifying human activities based on the inertial data collected from a user's smartphone. An Online Sequential Extreme Learning Machine (OSELM) method is implemented to train a single hidden layer feed-forward network (SLFN). Experimental results with an average accuracy of 82.05% are achieved.",https://ieeexplore.ieee.org/document/7244597/,2015 10th Asian Control Conference (ASCC),31 May-3 June 2015,ieeexplore
10.1109/INDIN.2004.1417325,OntoSmartResource: an industrial resource generation in semantic Web,IEEE,Conferences,"Semantic Web is a logical evolution of the existing Web. It was meant to serve for machines as today's Web does for humans. The term ""machines"" according to the existing semantic Web's vocabulary mostly means ""computers"". However industry needs such applications, which consider machines also as embedded computational entities within field devices, personal devices, microwave ovens, etc. In other words, now we should involve the real (industrial) world objects as resources into semantic Web. Still the main object of such a world will be a human, which becoming a resource (not just a user of resources) in the distributed environment. In this paper we introduce an extension of the semantic Web resources to a new generation of the enhanced smart semantic Web resources (OntoSmartResources). We consider following aspects as: agent-driven resource behavior mechanism, resource semantic description and maintenance, and some aspects of the human resources related to representation and adaptation",https://ieeexplore.ieee.org/document/1417325/,"2nd IEEE International Conference on Industrial Informatics, 2004. INDIN '04. 2004",24-26 June 2004,ieeexplore
10.1109/INDIN.2006.275808,"Ontology for Cognitics, Closed-Loop Agility Constraint, and Case Study - a Mobile Robot with Industrial-Grade Components",IEEE,Conferences,"The paper refers to intelligent industrial automation. The objective is to present key elements and methods for best practice, as well as some results obtained. The first part presents an ontology for automated cognition (cognitics), where, based on information and time, the main cognitive concepts, including those of complexity, knowledge, expertise, learning, intelligence abstraction, and concretization are rigorously defined, along with corresponding metrics and specific units. Among important conclusions at this point are the fact that reality is much too complex to be approached better than through much simplified models, in very restricted contexts. Another conclusion is the necessity to be focused on goal. Extensions are made here for group behavior. The second part briefly presents a basic law governing the choice of overall control architecture: achievable performance level of control system in terms of agility, relative to process dynamics, dictates the type of approaches which is suitable, in a spectrum which ranges from simple threshold-based switching, to classical closed-loop calculus (PID, state space multivariable systems, etc.), up to ""impossible"" cases where additional controllers must be considered, leading to cascaded, hierarchical control structures. For complex cases such as latter ones, new tools and methodologies must be designed, as is typical in O<sup>3</sup>NEIDA initiative, at least for software components. Finally, a large part of the paper presents a case study, a mobile robot, i.e. an embedded autonomous system with distributed, networked control, featuring industry-grade components, designed with the main goal of robust functionality. The case illustrates several of the concepts introduced earlier in the paper.",https://ieeexplore.ieee.org/document/4053562/,2006 4th IEEE International Conference on Industrial Informatics,16-18 Aug. 2006,ieeexplore
10.1109/FITME.2010.5654850,Ontology-based dynamic data gathering of geographic information services,IEEE,Conferences,"The geographic information service applications are getting more and more closely interconnected with the real-time industries and businesses recently. It becomes an urgent problem to gather business information from various industries directly. In this paper, we present an ontology-based dynamic data gathering method of geographic information services, which can extract instant information from MIS or similar systems of different commercial companies and organizations. We advanced the dynamic information ontology standard for industries to support the interoperability of the distributed and heterogeneous MIS and GIS databases. Then we collected information from various industries on-line by using Web Service technology, and integrated it with urban digital maps. In addition, the industry information is interconnected and aggregated through searching and data aggregation technologies. Based on these works, a multi-layers service framework is given and its detail functions are discussed.",https://ieeexplore.ieee.org/document/5654850/,2010 International Conference on Future Information Technology and Management Engineering,9-10 Oct. 2010,ieeexplore
10.1109/ICIECA.2005.1644370,Optimal approach for enhancement of large and small scale near-infrared and infrared imaging,IEEE,Conferences,"In a broad area of industry such as remote sensing and medical diagnosing, imaging enhancement technology takes a leading role, where energy distribution of the light source depends not only on image coordinate but also on wavelength. Both infrared (IR) and near-infrared (NIR) imaging techniques have a variety of applications in these fields. For instance, satellite images are taken via IR or NIR spectrometer and laser Doppler medical scanning is collaborated with NIR spectrometer. Matrix functions of any image correspond to brightness or energy at each image pixel. The actual decision making must rely on detailed investigation of images being obtained. Therefore, image processing should be taken into account so as to enhance the results from real world. Segmentation is an image analysis approach to clarify feature ambiguity and information noise, which divides an image into separate parts that correlate with the objects or areas of the particular object involved. This procedure can be conducted by clustering, which is a process of partitioning a set of pattern vectors into subsets. Being a simple unsupervised learning algorithm, k-means clustering algorithm has the potential to both simplify the computation and accelerate the convergence. In most cases optimization is closely related to clustering, which gives rise to the best way of problem solving. In this article, optimal approach is proposed to be implemented along with image segmentation. This methodology is to enhance both large scale and small scale IR and NIR image processing",https://ieeexplore.ieee.org/document/1644370/,2005 International Conference on Industrial Electronics and Control Applications,29 Nov.-2 Dec. 2005,ieeexplore
10.23919/SpliTech49282.2020.9243735,PADL: a Language for the Operationalization of Distributed Analytical Pipelines over Edge/Fog Computing Environments,IEEE,Conferences,"In this paper we introduce PADL, a language for modeling and deploying data-based analytical pipelines. The novelty of this language relies on its independence from both the infrastructure and the technologies used on it. Specifically, this descriptive language aims at embracing all the particularities and constraints of high-demanding deployment models, such as critical restrictions regarding latency, privacy and performance, by providing fully-compliant schemas for implementing data analytical workloads. The adoption of PADL provides means for the operationalization of these pipelines in a reproducible and resilient fashion. In addition, PADL is able to fully utilize the benefits of Edge and Fog computing layers. The feasibility of the language has been validated with an analytical pipeline deployed over an Edge computing environment to solve an Industry 4.0 use case. The promising results obtained therefrom pave the way towards the widespread adoption of our proposed language when deploying data analytical pipelines over real application scenarios.",https://ieeexplore.ieee.org/document/9243735/,2020 5th International Conference on Smart and Sustainable Technologies (SpliTech),23-26 Sept. 2020,ieeexplore
10.1109/TAAI.2015.7407079,PCBA demand forecasting using an evolving Takagi-Sugeno system,IEEE,Conferences,"This paper investigates the use of using an evolving fuzzy system for printed circuit board (PCBA) demand forecasting. The algorithm is based on the evolving Takagi-Sugeno (eTS) fuzzy system, which has the ability to incorporate new patterns by changing its internal structure in an on-line fashion. We argue that these capabilities could aid in forecasting dynamic demand patterns such as those experienced in the electronic manufacturing (EMS) industry. An eTS fuzzy system is implemented in the R statistical programming language and is tested on both synthetic and real-world data. To our knowledge, this is one of the first applications of an evolving fuzzy system to forecast product demand. The results indicate that the evolving fuzzy system outperforms competing approaches for the application considered.",https://ieeexplore.ieee.org/document/7407079/,2015 Conference on Technologies and Applications of Artificial Intelligence (TAAI),20-22 Nov. 2015,ieeexplore
10.1109/CIIMA50553.2020.9290289,PI tuning based on Bacterial Foraging Algorithm for flow control,IEEE,Conferences,"In the industrial field, the need has arisen to use more efficient and robust controllers using artificial intelligence techniques that optimize the operation of processes within the industry. In this way, the need arises to employ adaptive controllers such as the BFOA and implement it in real systems in which its functionality can be analyzed. This article presents the implementation and analysis in a fully instrumented functional prototype with industrial sensors. The work methodology is documented from the acquisition of the physical variables through the OPC client-server communication; the synchronization of the excitation of the input variable (variable speed drive) and obtaining the evolution of the flow in time; with the experimental data, the identification methodology by relative least squares was used to obtain the transfer function. Later, the BFOA algorithm was implemented to adjust the constants of a PI controller (Kp and Ki) and analyze the response through simulation using Matlab software, in which satisfactory results were observed based on the analysis of response to disturbances and as an end final part, the controller and the BFOA algorithm were implemented in a PLC-S7-1500 controller in SCL language, and the functionality was validated with the functional prototype, changing the flow setpoints at certain times, observing a behavior according to the simulations carried out. with a minimum overshoot of approximately 5 % and an establishment time of 20s.",https://ieeexplore.ieee.org/document/9290289/,2020 IX International Congress of Mechatronics Engineering and Automation (CIIMA),4-6 Nov. 2020,ieeexplore
10.1109/BigDataCongress.2019.00032,"PREMISES, a Scalable Data-Driven Service to Predict Alarms in Slowly-Degrading Multi-Cycle Industrial Processes",IEEE,Conferences,"In recent years, the number of industry-4.0-enabled manufacturing sites has been continuously growing, and both the quantity and variety of signals and data collected in plants are increasing at an unprecedented rate. At the same time, the demand of Big Data processing platforms and analytical tools tailored to manufacturing environments has become more and more prominent. Manufacturing companies are collecting huge amounts of information during the production process through a plethora of sensors and networks. To extract value and actionable knowledge from such precious repositories, suitable data-driven approaches are required. They are expected to improve the production processes by reducing maintenance costs, reliably predicting equipment failures, and avoiding quality degradation. To this aim, Machine Learning techniques tailored for predictive maintenance analysis have been adopted in PREMISES (PREdictive Maintenance service for Industrial procesSES), an innovative framework providing a scalable Big Data service able to predict alarming conditions in slowly-degrading processes characterized by cyclic procedures. PREMISES has been experimentally tested and validated on a real industrial use case, resulting efficient and effective in predicting alarms. The framework has been designed to address the main Big Data and industrial requirements, by being developed on a solid and scalable processing framework, Apache Spark, and supporting the deployment on modularized containers, specifically upon the Docker technology stack.",https://ieeexplore.ieee.org/document/8818217/,2019 IEEE International Congress on Big Data (BigDataCongress),8-13 July 2019,ieeexplore
10.1109/HOTI51249.2020.00011,"Panel: SmartNIC or DPU, Who Wins?",IEEE,Conferences," Summary form only given, as follows. The complete presentation was not made available for publication as part of the conference proceedings. SmartNICs and DPUs are surging in popularity to meet the new urgent requirements for media processing; neural networks, ML, and AI; and real-time automation, especially at the Network Edge. Yet they represent divergent visions of how to achieve the right combination of processing and throughput. In this discussion, we’ll highlight the fundamental differences between SmartNICs and DPUs and drill down into the hardware architectures that enable these massive compute engines, calling out the advantages and weaknesses of each approach. This panel brings together technology leaders from the six most relevant SmartNIC and DPU companies today to discuss their architecture and vision of the future. We have industry bedrocks AWS, Broadcom, NVIDIA, and Xilinx, as well as the newest kids on the block Fungible and Pensando. Eventually, it all comes down to software; we’ll touch on the underlying language and model each company chose, their path to providing solutions on their platforms, and their plan to enable thirdparty developers and perhaps even App stores. We'll close with 60 seconds from each company on the three things we should expect to see in the next 24 months.",https://ieeexplore.ieee.org/document/9188293/,2020 IEEE Symposium on High-Performance Interconnects (HOTI),19-21 Aug. 2020,ieeexplore
10.1109/BigData.2018.8622065,Parallel Large-Scale Neural Network Training For Online Advertising,IEEE,Conferences,"Neural networks have shown great successes in many fields. Due to the complexity of the training pipeline, however, using them in an industrial setting is challenging. In online advertising, the complexity arises from the immense size of the training data, and the dimensionality of the sparse feature space (both can be hundreds of billions). To tackle these challenges, we built TrainSparse (TS), a system that parallelizes the training of neural networks with a focus on efficiently handling large-scale sparse features. In this paper, we present the design and implementation of TS, and show the effectiveness of the system by applying it to predict the ad conversion rate (pCVR), one of the key problems in online advertising. We also compare several methods for dimensionality reduction on sparse features in the pCVR task. Experiments on real-world industry data show that TS achieves outstanding performance and scalability.",https://ieeexplore.ieee.org/document/8622065/,2018 IEEE International Conference on Big Data (Big Data),10-13 Dec. 2018,ieeexplore
10.1109/FPA.1994.636094,Perception systems implemented in analog VLSI for real-time applications,IEEE,Conferences,"We point out that analog VLSI can now be considered as the ideal medium to implement computational systems intended to carry out real time perceptive or even cognitive tasks that are not well handled by traditional computers. By exploiting the analog features of the transistors, only a few devices are needed to realise most of the elementary functions required to implement perceptive systems, resulting in very dense, sophisticated circuits and low power consumption. Elementary artificial retinas in silicon based on their biological counterparts have already been successfully used in industrial applications. Artificial cochleas and noses are also under development. This new enabling technology is of great interest over a wide range of industrial sectors, including robotics, automotive, surveillance and food industry.",https://ieeexplore.ieee.org/document/636094/,Proceedings of PerAc '94. From Perception to Action,7-9 Sept. 1994,ieeexplore
10.1109/CloudCom.2019.00066,Performance Analysis of Data Parallelism Technique in Machine Learning for Human Activity Recognition Using LSTM,IEEE,Conferences,"Human activity recognition (HAR), driven by large deep learning models, has received a lot of attention in recent years due to its high applicability in diverse application domains, manipulate time-series data to speculate on activities. Meanwhile, the cloud term ""as-a-service"" has essentially revolutionized the information technology industry market over the last ten years. These two trends somehow are incorporating to inspire a new model for the assistive living application: HAR as a service in the cloud. However, with frequently updates deep learning frameworks in open source communities as well as various new hardware features release, which make a significant software management challenge for deep learning model developers. To address this problem, container techniques are widely employed to facilitate the deep learning software development cycle. In addition, models and the available datasets are being larger and more complicated, and so, an expanding amount of computing resources is desired so that these models are trained in a feasible amount of time. This requires an emerging distributed training approach, called data parallelism, to achieve low resource utilization and faster execution in training time. Therefore, in this paper, we apply the data parallelism to build an assistive living HAR application using LSTM model, deploying in containers within a Kubernetes cluster to enable the real-time recognition as well as prediction of changes in human activity patterns. We then systematically measure the influence of this technique on the performance of the HAR application. Firstly, we evaluate our system performance with regard to CPU and GPU when deployed in containers and host environment, then analyze the outcomes to verify the difference in terms of the model learning performance. Through the experiments, we figure out that data parallelism strategy is efficient for improving model learning performance. In addition, this technique helps to increase the scaling efficiency in our system.",https://ieeexplore.ieee.org/document/8968845/,2019 IEEE International Conference on Cloud Computing Technology and Science (CloudCom),11-13 Dec. 2019,ieeexplore
10.1109/ICICT50816.2021.9358747,Performance Analysis of Frequency Variation System using Drives (VT240s and Axpert Eazy) for Industrial Application,IEEE,Conferences,"The proposed research work describes the general system management and real-time applicability of VFD i.e. Variable frequency drive. The project embodies the automation of 2 styles of VFD series namely VT240S and Axpert eazy. These VFD are going to be managed through PLC. Further, VFD is employed for control. The speed of the motor is often serially and domestically managed by VFD. In domestic management, there will be a switch affiliation within the drive and in serial management, the motor is managed through PLC. The motor will stop by each ways. For serial management, the programs should style in software package and it will be downloaded in PLC and by giving external wire affiliation like Modbus 485 from PLC to VFD, the operation is completed. Recently, the speed of the motor control is emerging as a big issue in industrial automation. Machine control with accurate result is required in industry applications for the design and development of the tools in various domains. Machine speed is used to control and vary the frequency parameter. So, the Variable Frequency Drive [VFD] that has been used to control the speed of the motor by varying the range of frequency is proposed to design a VFD machine system with number of tools and machine learning techniques in the paper.",https://ieeexplore.ieee.org/document/9358747/,2021 6th International Conference on Inventive Computation Technologies (ICICT),20-22 Jan. 2021,ieeexplore
10.1109/BigData.2018.8622389,Performance and Memory Trade-offs of Deep Learning Object Detection in Fast Streaming High-Definition Images,IEEE,Conferences,"Deep learning models are associated with various deployment challenges. Inference of such models is typically very compute-intensive and memory-intensive. In this paper, we investigate the performance of deep learning models for a computer vision application used in the automotive manufacturing industry. This application has demanding requirements that are characteristic of Big Data systems, including high volume and high velocity. The application has to process a very large set of high-definition images in real-time with appropriate accuracy requirements using a deep learning-based object detection model. Meeting the run time, accuracy, and resource requirements require a careful consideration of the choice of model, model parameters, hardware, and environmental support. In this paper, we investigate the trade-offs of the most popular deep neural network-based object detection models on four hardware platforms. We report the trade-offs of resource consumption, run time, and accuracy for a realistic real-time application environment.",https://ieeexplore.ieee.org/document/8622389/,2018 IEEE International Conference on Big Data (Big Data),10-13 Dec. 2018,ieeexplore
10.1109/METROI4.2019.8792860,Performance evaluation of full-cloud and edge-cloud architectures for Industrial IoT anomaly detection based on deep learning,IEEE,Conferences,"One of the most interesting application of data analysis to industry is the real-time detection of anomalies during production. Industrial IoT paradigm includes all the components to realize predictive systems, like the anomaly detection ones. In this case, the goal is to discover patterns, in a given dataset, that do not resemble the “normal” behavior, to identify faults, malfunctions or the effects of bad maintenance. The use of complex neural networks to implement deep learning algorithm for anomaly detection is very common. The position of the deep learning algorithm is one of the main problem: this kind of algorithm requires both high computational power and data transfer bandwidth, rising serious questions on the system scalability. Data elaboration in the edge domain (i.e. close to the machine) usually reduce data transfer but requires to instantiate expensive physical assets. Cloud computing is usually cheaper but Cloud data transfer is expensive. In this paper a test methodology for the comparison of the two architectures for anomaly detection system is proposed. A real use case is described in order to demonstrate the feasibility. The experimental results show that, by means of the proposed methodology, edge and Cloud solutions implementing deep learning algorithms for industrial applications can be easily evaluated. In details, for the considered use case (with Siemens controller and Microsoft Azure platform) the tradeoff between scalability, communication delay, and bandwidth usage, has been studied. The results show that the full-cloud architecture can outperform the edge-cloud architecture when Cloud computation power is scaled.",https://ieeexplore.ieee.org/document/8792860/,2019 II Workshop on Metrology for Industry 4.0 and IoT (MetroInd4.0&IoT),4-6 June 2019,ieeexplore
10.1109/SMARTCOMP.2017.7946997,PhD Forum: Deep Learning-Based Real-Time Malware Detection with Multi-Stage Analysis,IEEE,Conferences,"Protecting computer systems is a critical and ongoing problem, given that real-time malware detection is hard. The state-of-the-art for defense cannot keep pace with the increasing level of sophistication of malware. The industry, for instance, relies heavily on anti-virus technology for threat, which is effective for malware with known signatures, but not sustainable given the massive amount of malware samples released daily, as well as and its inefficacy in dealing with zero-day and polymorphic/metamorphic malware (practical detection rates range from 25% to 50%). Behavior-based approaches attempt to identify malware behaviors using instruction sequences, computation trace logic, and system (or API) call sequences. These solutions have been mostly based on conventional machine learning (ML) models with hand-craft features, such as K-nearest neighbor, SVM, and decision tree algorithms. However, current solutions based on ML suffer from high false-positive rates, mainly because of (i) the complexity and diversity of current software and malware, which are hard to capture during the learning phase of thealgorithms, (ii) sub-optimal feature extraction, and (iii) limited/outdated dataset. Since malware has been continuously evolving, existing protection mechanisms do not cope well with the increasedsophistication and complexity of these attacks, especially those performed by advanced persistent threats (APT), which are multi-module, stealthy, and target- focused. Furthermore, malware campaigns are not homogeneous--malware sophistication varies depending on the target, the type of service exploited as part of the attack (e.g., Internet Banking, relationship sites), the attack spreading source (e.g., phishing, drive-by downloads), and the location of the target. The accuracy of malware classification depends on gaining sufficient context information and extracting meaningful abstraction of behaviors. In problems about detecting malicious behavior based on sequence of system calls, longer sequences likely contain more information. However, classical ML- based detectors (i.e., Random Forest, Naive Bayes) often use short windows of system calls during the decision process and may not be able to extract enough features for accurate detection in a long term window. Thus, the main drawback of such approaches is to accomplish accurate detection, since it is difficult to analyze complex and longer sequences of malicious behaviors with limited window sizes, especially when malicious and benign behaviors are interposed. In contrast, Deep Learning models are capable of analyzing longer sequences of system calls and making better decisions through higher level information extraction and semantic knowledge learning. However, Deep Learning requires more computation time to estimate the probability of detection when the model needs to be retrained incrementally, a common requirement for malware detection when new variants and samples are frequently added to the training set. The trade-off is challenging: fast and not-so-accurate (classical ML methods) versus time-consuming and accurate detection (emerging Deep Learning methods). Our proposal is to leverage the best of the two worlds with Spectrum, a practical multi-stage malware- detection system operating in collaboration with the operating system (OS).",https://ieeexplore.ieee.org/document/7946997/,2017 IEEE International Conference on Smart Computing (SMARTCOMP),29-31 May 2017,ieeexplore
10.1109/ISI49825.2020.9280509,Phishcasting: Deep Learning for Time Series Forecasting of Phishing Attacks,IEEE,Conferences,"Phishing attacks remain pervasive and continue to be a source of significant monetary loss, identity theft, and malware. One of the challenges is that in most organizational settings, the detection paradigm is inherently about identifying and reacting to threats in real-time, as they are unfolding. As a way to complement these efforts with greater foresight, we introduce the idea of phishcasting — forecasting of phishing threat levels weeks or months into the future. Given that phishing attack volume time series data is noisy and devoid of traditional seasonal and cyclical trends, we extend the time series forecasting framework to utilize multiple time series, auxiliary information and alternate representations. We also introduce CoT-Net, a flexible, end-to-end CNN-LSTM based deep learning method for forecasting of complex phishing attack volume time series. CoT-Net uses time series embeddings to uncover correlations between organizational attack patterns within and across industry sectors. Using a publicly available test bed featuring multiple organizations’ attack volume over time, we find CoT-Net to outperform most state-of-the-art time series forecasting methods. By showing that phishcasting might be possible and practical, our work has important proactive implications for cybersecurity.",https://ieeexplore.ieee.org/document/9280509/,2020 IEEE International Conference on Intelligence and Security Informatics (ISI),9-10 Nov. 2020,ieeexplore
10.1109/ICTON.2019.8840155,Photonics-Supported 5G Test Facilities for Low Latency Applications,IEEE,Conferences,"Berlin has become one of the EU's strategic spot for the evolution of 5G networks due in part to several ongoing activities supporting the effort. So far, there has been a huge investment from 5G Berlin Innovation Cluster to deliver a unique infrastructure providing opportunities for research centres, SMEs, and start-ups to develop novel solutions and products [1]. In this regard, there has been a complementary activity from Berlin Centre for Digital Transformation to develop a three-node optical network, which connects three Fraunhofer institutes, HHI, FOKUS, and IPK in a metro ring structure [2]. Additionally, there is a recently funded German national project (OTB-5G+) to develop an Open Testbed Berlin for 5G and Beyond, which will deliver a three-node ROADM network. These activities along with some other ones, like the presence of a Berlin node for two of the EU 5G trial projects (i.e., 5Genesis [3] and 5Gvinni [4]), have made Berlin a strategic spot in the evolution of the EU 5G ecosystem. Fraunhofer HHI hosts several aspects of the above-mentioned infrastructure including the 3-node ROADM metro network testbed, which provide a great opportunity to perform real-field experiments with 5G-ready RAN infrastructure and edge compute capability for realizing low-latency end-to-end use-cases. This metro network is SDN controlled and hosts edge compute node capabilities. Additionally, it could offer NFV services and will eventually support network slicing. Regarding the RAN infrastructure, currently, there are LTE base stations installed, which operate at 2.6 GHz. They will be upgraded to 5G base stations operating at 3.7 GHz as soon as they become available. The access segment, which is based on a passive WDM network, is expected to be extended by sophisticated mm-wave and optical-wireless communication links as bridges demonstrating the network extensions where optical fibers cannot be deployed. This network will be an SDN-enabled allowing the inclusion of several street 5G access points to the infrastructure. The three-node ROADM test-bed described above and shown in Fig. 1 will also host several demonstrations, including the final demonstration of metro-haul project [5]. Two of the ROADM nodes will be realized as AMEN (i.e., Access-Metro Edge Node) and the last one is MCEN (i.e., Metro-Core Edge Node), both developed in the framework of Metro-Haul project. This network will be also connected to German metro ring. The German metro ring will provide connectivity to our partners' premises, including ADVA, NOKIA, and Infinera enabling field trials and optical transmission tests.Figure 1.A part of the 5G test facilities hosted in Berlin.In this talk, we will present in detail the facilities hosted in Berlin, covering its geographical expansion as well as the hosted optical and wireless technologies. We then explore several demonstrations planned to run over this infrastructure showcasing the benefits brought about by different technology pillars in the context of high capacity 5G test facilities in Berlin, including network slicing, network function virtualization, distributed computing over edge compute nodes to eventually address the execution of latency-critical and bandwidth- hungry applications in an intelligent, distributed, and resource-efficient way. We will conclude the talk by providing several ways for future collaboration through which research institutes and industry sector across Europe can get access to the infrastructure in order to perform collaborative field-trials, measurement campaign, autonomous networking solutions validation, etc.",https://ieeexplore.ieee.org/document/8840155/,2019 21st International Conference on Transparent Optical Networks (ICTON),9-13 July 2019,ieeexplore
10.1109/INFCOMW.2019.8845276,Poster Abstract: Deep Learning Workloads Scheduling with Reinforcement Learning on GPU Clusters,IEEE,Conferences,"With the recent widespread adoption of deep learning (DL) in academia and industry, more attention are attracted by DL platform, which can support research and development (R&amp;D) of AI firms, institutes and universities. Towards an off-the-shelf distributed GPU cluster, prior work propose prediction-based schedulers to allocate resources for diverse DL workloads. However, the prediction-based schedulers have disadvantages on prediction accuracy and offline-profiling costs. In this paper, we propose a learning-based scheduler, which models the scheduling problem as a reinforcement learning problem, achieving minimum average job completion time and maximum system utilization. The scheduler contains the designs of state space, action space, reward function and update scheme. Furthermore, we will evaluate our proposed scheduler implemented as a plugin of Tensorflow on real cluster and large-scale simulation.",https://ieeexplore.ieee.org/document/8845276/,IEEE INFOCOM 2019 - IEEE Conference on Computer Communications Workshops (INFOCOM WKSHPS),29 April-2 May 2019,ieeexplore
10.1109/SEC50012.2020.00019,Poster: Lambda architecture for robust condition based maintenance with simulated failure modes,IEEE,Conferences,"Condition based maintenance (CBM) is increasingly seen as a promising approach for addressing downtime issues which are a common occurrence in the manufacturing industry and are a major cause of lost productivity. However, it has been a challenge to develop a generic CBM solution that works for all assets since each asset has unique sources of noise. This mandates use of manual diagnostics to custom tailor a solution for each asset for accurate failure mode identification (FMI). This problem is further compounded by the scarcity of failure data. In this paper, we propose a lambda architecture for FMI of industrial assets that achieves low initial deployment cost while securing a reasonable classification accuracy. The lambda architecture consists of a light-compute edge node, such as Raspberry Pi, that processes high-speed vibration data in real-time to extract useful features and applies a deep-learning (DL) engine which is trained in a cloud platform, such as AWS. In addition, we also incorporate a failure modes' feature simulator so that DL models can adapt to different industrial assets without costly failure data collection. Finally, experimental results are provided using the bearings' failures dataset validating the proposed cost-effective CBM architecture with high accuracy and scalability.",https://ieeexplore.ieee.org/document/9355694/,2020 IEEE/ACM Symposium on Edge Computing (SEC),12-14 Nov. 2020,ieeexplore
10.1109/ICSME.2017.41,Predicting and Evaluating Software Model Growth in the Automotive Industry,IEEE,Conferences,"The size of a software artifact influences the software quality and impacts the development process. In industry, when software size exceeds certain thresholds, memory errors accumulate and development tools might not be able to cope anymore, resulting in a lengthy program start up times, failing builds, or memory problems at unpredictable times. Thus, foreseeing critical growth in software modules meets a high demand in industrial practice. Predicting the time when the size grows to the level where maintenance is needed prevents unexpected efforts and helps to spot problematic artifacts before they become critical. Although the amount of prediction approaches in literature is vast, it is unclear how well they fit with prerequisites and expectations from practice. In this paper, we perform an industrial case study at an automotive manufacturer to explore applicability and usability of prediction approaches in practice. In a first step, we collect the most relevant prediction approaches from literature, including both, approaches using statistics and machine learning. Furthermore, we elicit expectations towards predictions from practitioners using a survey and stakeholder workshops. At the same time, we measure software size of 48 software artifacts by mining four years of revision history, resulting in 4,547 data points. In the last step, we assess the applicability of state-of-the-art prediction approaches using the collected data by systematically analyzing how well they fulfill the practitioners' expectations. Our main contribution is a comparison of commonly used prediction approaches in a real world industrial setting while considering stakeholder expectations. We show that the approaches provide significantly different results regarding prediction accuracy and that the statistical approaches fit our data best.",https://ieeexplore.ieee.org/document/8094464/,2017 IEEE International Conference on Software Maintenance and Evolution (ICSME),17-22 Sept. 2017,ieeexplore
10.1109/ICTC.2016.7763364,Predicting churn in mobile free-to-play games,IEEE,Conferences,"In the mobile game industry, Free-to-Play games are dominantly released, and therefore player retention and purchases have become important issues. In this paper, we propose a game player model for predicting when players will leave a game. Firstly, we define player churn in the game and extract features that contain the properties of the player churn from the player logs. And then we tackle the problem of imbalanced datasets. Finally, we exploit classification algorithms from machine learning and evaluate the performance of the proposed prediction model using cross-validation. Experimental results show that the proposed model has high accuracy enough to predict churn for real-world application.",https://ieeexplore.ieee.org/document/7763364/,2016 International Conference on Information and Communication Technology Convergence (ICTC),19-21 Oct. 2016,ieeexplore
10.1109/ICECCE52056.2021.9514218,Predicting “Maintenance Priority” with AI,IEEE,Conferences,"In Tiipras oil refineries, an average of 100 thousand maintenance requests are created annually for more than 140 thousand pieces of equipment. These requests are prioritized manually by chief experts with over 25 years of experience and classified as urgent or planned. If maintenance requests that need to be solved urgently in the refining industry are mislabeled and delayed, they may cause process upsets leading to health &amp; safety hazards, environment problems or big asset damage. To minimize this risk, we think that supporting the decision mechanism with algorithms and cross checking/replacing human decisions by using today's AI technologies is the right approach that reduces the possibility of human error. In this study, our main goal is to automate maintenance prioritization process with supervised and unsupervised ML algorithms, deploy an AI system and achieve high accuracy. Our study was carried out basically in 4 main steps: • Exploratory Data Analysis • Clustering - Feature Addition - Feature Selection • Model Selection and Results • Additional Studies With this study, we aim to explain our AI study, share our experience with other partners that have similar needs and provide them an effective tool and systematic approach about management of transition from human to machine with a real industry case. We believe that the transfer of priority selection process from human to algorithms ensure consistent decisions, reduce costs and tolerate experience losses.",https://ieeexplore.ieee.org/document/9514218/,"2021 International Conference on Electrical, Communication, and Computer Engineering (ICECCE)",12-13 June 2021,ieeexplore
10.1109/ICRITO51393.2021.9596436,Prediction of Sensor Faults and Outliers in IoT Devices,IEEE,Conferences,"Internet of Things (IoT) is tremendously growing and interacting with the physical world in the era of Industry 4.0. In near future, billions of companies will have advanced communication technology and it will increase the growth of critical systems. The accuracy measurement of the functionality of a critical system is a challenging job. Fault Tolerance (FT) is a major concern to ensure the dependability, availability and reliability of critical systems. Faults should be predicted and controlled proactively to lessen failure impact on the critical systems. To predict these failures and use the relevant procedure to avoid it before it actually occurs, FT techniques are used. These techniques are implemented in critical systems to avoid failures as the security of systems is more important than the reliability of systems. It minimizes the effect of faults that are being investigated. FT techniques work on a concept that if the system is built differently then it should fail differently. If a redundant variant fails then atleast the other one should give a satisfactory result. This study exhibits an analysis of existing FT techniques like N-version programming, Recovery blocks and N-self-checking programming. A critical study of sensor faults and outliers prediction models in IoT is presented. A bibliometric analysis is also carried out on 716 Scopus indexed publications to analyze the current research trends in this domain.",https://ieeexplore.ieee.org/document/9596436/,"2021 9th International Conference on Reliability, Infocom Technologies and Optimization (Trends and Future Directions) (ICRITO)",3-4 Sept. 2021,ieeexplore
10.1109/ITSC45102.2020.9294450,Predictive maintenance leveraging machine learning for time-series forecasting in the maritime industry,IEEE,Conferences,"One of the key challenges in the maritime industry refers to minimizing the time a vessel cannot be utilized, which has multiple effects. The latter is addressed through maintenance approaches that however in many cases are not efficient in terms of cost and downtime. Predictive maintenance provides optimized maintenance scheduling offering extended vessel lifespan, coupled with reduced maintenance costs. As in several industries, including the maritime domain, an increasing amount of data is made available through the deployment and exploitation of data sources, such as on board sensors that provide real-time information. These data provide the required ground for analysis and thus support for various types of data-driven decision making. In the maritime domain, sensors are deployed on vessels to monitor their engines and data analysis tools are needed to assist engineers towards reduced operational risk through predictive maintenance solutions that are put in place. In this paper, we present an approach for anomaly detection on time-series data, utilizing machine learning on the vessels sensor data, in order to predict the condition of specific parts of the vessel's main engine and thus facilitate predictive maintenance. The novel characteristic of the proposed approach refers both to the inclusion of new innovative models to address the case of predictive maintenance in maritime and the combination of those different models, highlighting an improved result in terms of evaluation metrics.",https://ieeexplore.ieee.org/document/9294450/,2020 IEEE 23rd International Conference on Intelligent Transportation Systems (ITSC),20-23 Sept. 2020,ieeexplore
10.1109/ARITH.2019.00047,Privacy-Preserving Deep Learning via Additively Homomorphic Encryption,IEEE,Conferences,"We aim at creating a society where we can resolve various social challenges by incorporating the innovations of the fourth industrial revolution (e.g. IoT, big data, AI, robot, and the sharing economy) into every industry and social life. By doing so the society of the future will be one in which new values and services are created continuously, making people's lives more conformable and sustainable. This is Society 5.0, a super-smart society. Security and privacy are key issues to be addressed to realize Society 5.0. Privacy-preserving data analytics will play an important role. In this talk we show our recent works on privacy-preserving data analytics such as privacy-preserving logistic regression and privacy-preserving deep learning. Finally, we show our ongoing research project under JST CREST “AI”. In this project we are developing privacy-preserving financial data analytics systems that can detect fraud with high security and accuracy. To validate the systems, we will perform demonstration tests with several financial institutions and solve the problems necessary for their implementation in the real world.",https://ieeexplore.ieee.org/document/8877418/,2019 IEEE 26th Symposium on Computer Arithmetic (ARITH),10-12 June 2019,ieeexplore
10.1109/GloSIC.2018.8570124,Probabilistic Estimations of Increasing Expected Reliability and Safety for Intelligent Manufacturing,IEEE,Conferences,"In the near future the possibilities of the modern probabilistic models, artificial intelligence and machine learning methods can provide an intelligent support of making decisions by an operator in real time. An agile recovery of intelligent manufacturing integrity can be implemented owing to the development of industrial robotics. For intelligent manufacturing it means the expected reliability and safety may be in the near future at the expense of intelligent support of decision making and the agile recovery of integrity. To answer the question “How much essential may be this increasing?” here are proposed: general analytical approaches for a probabilistic estimation of the expected reliability and safety for every monitored element or the system of intelligent manufacturing on a level of probability distribution functions (PDF) of the time between the losses of system integrity; estimations of increasing the expected reliability and safety for intelligent manufacturing at the expense of the intelligent support of decision making and agile recovery of integrity; the comparisons of the estimations on a prognostic period up to 10 years using the identical model in applications to expected reliability and safety. The applications of the proposed approaches allow the customers, designers, developers, users and experts of Industry 4.0 intelligent manufacturing to be guided by the proposed probabilistic estimations for solving problems of reliability and safety in the system life cycle. The results are demonstrated by examples.",https://ieeexplore.ieee.org/document/8570124/,2018 Global Smart Industry Conference (GloSIC),13-15 Nov. 2018,ieeexplore
10.1109/IROS.2016.7759479,Probabilistic approaches for self-tuning path tracking controllers using prior knowledge of the terrain,IEEE,Conferences,"Nowadays, agricultural and mining industry applications require saving energy in mobile robotic tasks. This critical issue encouraged us to enhance the performance of path tracking controllers during manoeuvring over slippery and rough terrains. In this scenario, we propose probabilistic approaches under machine learning schemes in order to optimally self-tune the controller. The approaches are real time implemented and tested in a mining machinery skid steer loader Cat® 262C under gravel and muddy terrains (and their transitions). Finally, experimental results presented in this work show that the performance of the controller enhances up to 20% (average) without compromising saturations in the actuators.",https://ieeexplore.ieee.org/document/7759479/,2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),9-14 Oct. 2016,ieeexplore
10.1109/BigData.2018.8622491,Profiling Driver Behavior for Personalized Insurance Pricing and Maximal Profit,IEEE,Conferences,"Profiling driver behaviors and designing appropriate pricing models are essential for auto insurance companies to gain profits and attract customers (drivers). The existing approaches either rely on static demographic information like age, or model only coarse-grained driving behaviors. They are therefore ineffective to yield accurate risk predictions over time for appropriate pricing, resulting in profit decline or even financial loss. Moreover, existing pricing strategies seldom take profit maximization into consideration, especially under the enterprise constraints. The recent growth of vehicle telematics data (vehicle sensing data) brings new opportunities to auto insurance industry, because of its sheer size and fine-grained mobility for profiling drivers. But, how to fuse these sparse, inconsistent and heterogeneous data is still not well addressed. To tackle these problems, we propose a unified PPP (Profile-Price-Profit) framework, working on the real-world large-scale vehicle telematics data and insurance data. PPP profiles drivers' fine-grained behaviors by considering various driving features from the trajectory perspective. Then, to predict drivers' risk probabilities, PPP leverages the group-level insight and categorizes drivers' different temporal risk change patterns into groups by ensemble learning. Next, the pricing model in PPP incorporates both the demographic analysis and the mobility factors of driving risk and mileage, to generate personalized insurance price for supporting flexible premium periods. Finally, the maximal profit problem proves to be NP-Complete. Then, an efficient heuristic-based dynamic programming is proposed. Extensive experimental results demonstrated that, PPP effectively predicts the driver's risk and outperforms the current company's pricing strategy (in industry) and the state-of-the-art approach. PPP also achieves near the maximal profit (difference by only 3%) for the company, and lowers the total price for the drivers.",https://ieeexplore.ieee.org/document/8622491/,2018 IEEE International Conference on Big Data (Big Data),10-13 Dec. 2018,ieeexplore
10.1109/ISSREW.2016.23,Programming the Network: Application Software Faults in Software-Defined Networks,IEEE,Conferences,"Software-defined networking (SDN) is a key new paradigm emerging in the industry, in which networks can be dynamically reconfigured in real-time through software. SDN networks are also being used in conjunction with cloud computing to extend virtualization and elasticity to the network level and as a foundation for the Internet of Things (IoT). A key concept in SDN is the separation of the network control and data planes, together with an application plane that supports the programming of network applications in general-purpose languages such as Java and Python. These network applications can be developed by an enterprise, service provider or vendor, or purchased from third-parties through SDN application stores. While the programmability of SDN provides tremendous flexibility and adaptability to changing network conditions and demands, it also exposes networks to significant vulnerabilities through software faults in network applications, as well as in the control and data planes. In this paper, we demonstrate how faulty SDN applications can compromise other SDN applications or even crash an entire SDN network, and describe relationships between software faults in SDN applications and design faults in SDN controllers. We also show how machine-learning based anomaly detection and analytics can be used to identify SDN software faults and help guide real-time network response, through a proof-of-concept case study.",https://ieeexplore.ieee.org/document/7789391/,2016 IEEE International Symposium on Software Reliability Engineering Workshops (ISSREW),23-27 Oct. 2016,ieeexplore
10.1109/CSEET49119.2020.9206229,Project-Based Learning in a Machine Learning Course with Differentiated Industrial Projects for Various Computer Science Master Programs,IEEE,Conferences,"Graduating computer science students with skills sufficient for industrial needs is a priority in higher education teaching. Project-based approaches are promising to develop practical and social skills, needed to address real-world problems in teams. However, rapid technological transition makes an initial training of contemporary methods challenging. This affects the currently much-discussed machine learning domain as well. The study at hand describes a re-framed teaching approach for a machine learning course, offered to various computer science master programs. Project-based learning is introduced with differentiated projects provided by industrial partners that address the diverse study programs. Course attendees are supported with manuals, tools, and tutoring, passing through the Cross Industry Standard Process for Data Mining (CRISP-DM). Observations made during two iterations are reported, accompanied by a first empiric evaluation of student experiences.",https://ieeexplore.ieee.org/document/9206229/,2020 IEEE 32nd Conference on Software Engineering Education and Training (CSEE&T),9-12 Nov. 2020,ieeexplore
10.1109/IWOBI47054.2019.9114431,Proof of Concept: Using Reinforcement Learning agent as an adversary in Serious Games,IEEE,Conferences,"This article focuses on simple rehabilitation video-game called Flying with Friends. The rehabilitation industry has been experiencing a boom in recent years, coupled with the growing popularity of virtual reality technology, a drop in prices for these technologies and the expansion entertainment industry in the form of computer games. The goal of this experiment was to provide a proof that such systems are viable option when it comes to artificial intelligence systems in serious video-games, but not limited to only serious ones. The solution described in this article, in cooperation with experts, is going to be deployed in a real rehabilitation environment.",https://ieeexplore.ieee.org/document/9114431/,2019 IEEE International Work Conference on Bioinspired Intelligence (IWOBI),3-5 July 2019,ieeexplore
10.1109/PC.2017.7976254,Proposal of system for automatic weld evaluation,IEEE,Conferences,"The paper deals with the development of a system for automatic weld recognition using new information technologies based on cloud computing and single-board computer in the context of Industry 4.0. The proposed system is based on a visual system for weld recognition, and a neural network based on cloud computing for real-time weld evaluation, both implemented on a single-board low-cost computer. The proposed system was successfully verified on welding samples which correspond to a real welding process in the car production process. The system considerably contributes to the welds diagnostics in industrial processes of small- and medium-sized enterprises.",https://ieeexplore.ieee.org/document/7976254/,2017 21st International Conference on Process Control (PC),6-9 June 2017,ieeexplore
10.1109/ACC.2008.4587179,Prototype design of a multi-agent system for integrated control and asset management of petroleum production facilities,IEEE,Conferences,"This paper addresses a practical intelligent multi- agent system for asset management for the petroleum industry, which is crucial for profitable oil and gas facilities operations and maintenance. A research project was initiated to study the feasibility of an intelligent asset management system. Having proposed a conceptual model, architecture, and implementation plan for such a system in previous work and defined its autonomy, communications, and artificial intelligence (AI) requirements, we are proceeding to build a system prototype and simulate it in real time to validate its logical behavior in normal and abnormal process situations. We also conducted a thorough system performance analysis to detect any computational bottlenecks. Although the preliminary system prototype design has limitations, simulation results have demonstrated an effective system logical behavior and performance.",https://ieeexplore.ieee.org/document/4587179/,2008 American Control Conference,11-13 June 2008,ieeexplore
10.1109/WORKS49585.2019.00006,Provenance Data in the Machine Learning Lifecycle in Computational Science and Engineering,IEEE,Conferences,"Machine Learning (ML) has become essential in several industries. In Computational Science and Engineering (CSE), the complexity of the ML lifecycle comes from the large variety of data, scientists' expertise, tools, and workflows. If data are not tracked properly during the lifecycle, it becomes unfeasible to recreate a ML model from scratch or to explain to stackholders how it was created. The main limitation of provenance tracking solutions is that they cannot cope with provenance capture and integration of domain and ML data processed in the multiple workflows in the lifecycle, while keeping the provenance capture overhead low. To handle this problem, in this paper we contribute with a detailed characterization of provenance data in the ML lifecycle in CSE; a new provenance data representation, called PROV-ML, built on top of W3C PROV and ML Schema; and extensions to a system that tracks provenance from multiple workflows to address the characteristics of ML and CSE, and to allow for provenance queries with a standard vocabulary. We show a practical use in a real case in the O&amp;G industry, along with its evaluation using 239,616 CUDA cores in parallel.",https://ieeexplore.ieee.org/document/8943505/,2019 IEEE/ACM Workflows in Support of Large-Scale Science (WORKS),17-17 Nov. 2019,ieeexplore
10.1109/ICCChinaW.2018.8674519,R-CNN Object Detection Inference With Deep Learning Accelerator,IEEE,Conferences,"The explosively increasing demands of high-speed data applications have brought massive access requirements to various mobile devices. As integrating with artificial intelligence and neural network, the mobile device industry is often more concerned with faster inference with lower power consumption, bringing deep learning inference acceleration to the spotlight. In this paper, we perform a neural network inference merging R-CNN, an object detection model, into a deep learning accelerator architecture. It is a brand new implementation of neural network on embedded system hardware IP-cores of edge computation. On one hand, based on the embedded system environment, we implement the image pre-processing with region proposal algorithm and image post-processing with NMS method. On the other hand, we perform the feature computation with the deep learning accelerator through optimized software and hardware configurations. Through this method, we solve the problem of time-consuming in the computation of neural network layers and give a precise and real-time prediction of object detection. Our R-CNN inference achieves impressive results with 1.9 to 2.6 times higher performance compared with other inference processors.",https://ieeexplore.ieee.org/document/8674519/,2018 IEEE/CIC International Conference on Communications in China (ICCC Workshops),16-18 Aug. 2018,ieeexplore
10.1109/ICCICC46617.2019.9146036,RTPA-based Software Generation by AI Programming,IEEE,Conferences,"AI programming toward autonomous software generation is not only a highly demanded technology by the software industry, but also a hard challenge to the theories of software engineering and computational intelligence. A methodology and tool for autonomous program generation (APG) are recently developed based on Real-Time Process Algebra (RTPA). This paper demonstrates an experimental result of autonomous code generation on a digital clock system by the APG tool. The experimental results indicate a novel approach towards AI programming for machine-enabled software generation theories and technologies.",https://ieeexplore.ieee.org/document/9146036/,2019 IEEE 18th International Conference on Cognitive Informatics & Cognitive Computing (ICCI*CC),23-25 July 2019,ieeexplore
10.1109/CSCI51800.2020.00085,Radically Simplifying Game Engines: AI Emotions &amp; Game Self-Evolution,IEEE,Conferences,"Today, video games are a multi-billion-dollar industry, continuously evolving through the incorporation of new technologies and innovative design. However, current video game software content creation requires extensive and often-times ambiguous planning phases for developing aesthetics, online capabilities, and gameplay mechanics. Design elements can vary significantly relative to the expertise of artists, designers, budget, and overall game engine/software features and capabilities. Game development processes are often extensively long coding sessions, usually involving a highly iterative creative process, where user requirements are rarely provided. Therefore, we propose significantly simplifying game design and development with novel Artificial Cognition Architecture real-time scalability and dynamic emotion core. Rather than utilizing more static emotion state weighting emotion engines (e.g. ExAI), we leverage significant ACA research in successful implementation of analog neural learning bots with Maslowan objective function algorithms. We also leverage AI- based Artificial Psychology software which utilizes ACA's fine grained self-evolving emotion modeling in humanistic avatar patients for Psychologist training. An ACA common cognitive core provides the gaming industry with wider applications across video game genres. A modular, scalable, and cognitive emotion game architecture implements Non-Playable Character (NPC) learning and self-evolution. ACA models NPC's with fine grained emotions, providing interactive dynamic personality traits for a more realistic game environment and enables NPC self-evolution under the influence of both other NPC's and players. Furthermore, we explore current video game design engine architecture (e.g. Unity, Unreal Engine) and propose an ACA integration approach. We apply artificial cognition and emotion intelligence modeling to engender video games with more distinct, realistic consumer gaming experiences, while simultaneously minimizing software gaming development efforts and costs.",https://ieeexplore.ieee.org/document/9457899/,2020 International Conference on Computational Science and Computational Intelligence (CSCI),16-18 Dec. 2020,ieeexplore
10.1109/RSP.2018.8631995,Rapid Prototyping of Embedded Vision Systems: Embedding Computer Vision Applications into Low-Power Heterogeneous Architectures,IEEE,Conferences,"Embedded vision is a disruptive new technology in the vision industry. It is a revolutionary concept with far reaching implications, and it is opening up new applications and shaping the future of entire industries. It is applied in self-driving cars, autonomous vehicles in agriculture, digital dermascopes that help specialists make more accurate diagnoses, among many other unique and cutting-edge applications. The design of such systems gives rise to new challenges for embedded Software developers. Embedded vision applications are characterized by stringent performance constraints to guarantee real-time behaviours and, at the same time, energy constraints to save battery on the mobile platforms. In this paper, we address such challenges by proposing an overall view of the problem and by analysing current solutions. We present our last results on embedded vision design automation over two main aspects: the adoption of the model-based paradigm for the embedded vision rapid prototyping, and the application of heterogeneous programming languages to improve the system performance. The paper presents our recent results on the design of a localization and mapping application combined with image recognition based on deep learning optimized for an NVIDIA Jetson TX2.",https://ieeexplore.ieee.org/document/8631995/,2018 International Symposium on Rapid System Prototyping (RSP),4-5 Oct. 2018,ieeexplore
10.1109/VRAIS.1993.380800,Rationale and strategy for VR standards,IEEE,Conferences,"The author argues an integrated family of standards must be developed, for the sake of the virtual reality (VR) industry and its customers. He suggests that this will bring stability to the marketplace, thus increasing customer acceptance and encouraging developers to provide additional applications. Techniques to produce quality standards for computer graphics are being used to improve the standards currently under development. There are problems to be overcome, for example in getting adequate staffing to produce standards in a timely manner.&lt;<ETX>&gt;</ETX>",https://ieeexplore.ieee.org/document/380800/,Proceedings of IEEE Virtual Reality Annual International Symposium,18-22 Sept. 1993,ieeexplore
10.1109/INFOCOM.2019.8737649,ReLeS: A Neural Adaptive Multipath Scheduler based on Deep Reinforcement Learning,IEEE,Conferences,"The Multipath TCP (MPTCP) protocol, featured by its ability of capacity aggregation across multiple links and connectivity maintenance against single-path failure, has been attracting increasing attention from the industry and academy. Multipath packet scheduling is a unique and fundamental mechanism for the design and implementation of MPTCP, which is responsible for distributing the traffic over multiple subflows. The existing multipath schedulers are facing the challenges of network heterogeneities, comprehensive QoS goals, and dynamic environments, etc. To address these challenges, we propose ReLeS, a Reinforcement Learning based Scheduler for MPTCP. ReLeS uses modern deep reinforcement learning (DRL) techniques to learn a neural network to generate the control policy for packet scheduling. It adopts a comprehensive reward function that takes diverse QoS characteristics into consideration to optimize packet scheduling. To support real-time scheduling, we propose an asynchronous training algorithm that enables parallel execution of packet scheduling, data collecting, and neural network training. We implement ReLeS in the Linux kernel and evaluate it over both emulated and real network conditions. Extensive experiments show that ReLeS significantly outperforms the state-of-the-art schedulers.",https://ieeexplore.ieee.org/document/8737649/,IEEE INFOCOM 2019 - IEEE Conference on Computer Communications,29 April-2 May 2019,ieeexplore
10.1109/FTDCS.2007.29,Reaching Semantic Interoperability through Semantic Association of Domain Standards,IEEE,Conferences,"The vision of semantic interoperability has led much research on ontology matching. Research in this area primarily focuses on discovering similarity between entities of ontologies. The performance of proposed approaches relies on the existence of such similarity relationship and sufficient data for inferring it. However, in reality, many distributed systems do not have such presumptions. This paper addresses this challenge by associating the entities through affinity semantic (to what degree they are related in their application context). Through the analysis of a motivating example in building construction industry, this paper formally defines semantic association based on multiple-perspective domain standards. This paper hypothesizes that the establishment and the use of such standards can practically serve as a framework for reaching semantic interoperability between autonomous information systems. This paper also shows that such framework has the potential to make revolutionary impacts on workflow automation, information retrieval, and ontology matching research areas",https://ieeexplore.ieee.org/document/4144609/,11th IEEE International Workshop on Future Trends of Distributed Computing Systems (FTDCS'07),21-23 March 2007,ieeexplore
10.1109/IMITEC50163.2020.9334129,Real Time Customer Churn Scoring Model for the Telecommunications Industry,IEEE,Conferences,"There are two types of customers in the telecommunication industry; the pre-paid and the contract customers. In South Africa it is the pre-paid customers that keep telcos constantly worried because such customers do not have anything binding them to the company, they can leave and join a competitor at any time. To retain such customers, telcos need to customise suitable solutions especially for those customers that are agitating and can churn at any time. This needs customer churn prediction models that would take advantage of big data analytics and provide the telco industry with a real time solution. The purpose of this study was to develop a real time customer churn prediction model. The study used the CRISP-DM methodology and the three machine learning algorithms for implementation. Watson Studio software was used for the model prototype deployment. The study used the confusion matrix to unpack a number of performance measures. The results showed that all the models had some degree of misclassification, however the misclassification rate of the Logistic Regression was very minimal (2.2%) as differentiated from the Random Forest and the Decision Tree, which had misclassification rates of 20.8% and 21.7% respectively. The results further showed that both Random Forest and the Decision Tree had good accuracy rates of 78.3% and 79.2% respectively, although they were still not better than that of the Logistic Regression. Despite the two having good accuracy rates, they had the highest rates of misclassification of class events. The conclusion we drew from this was that, accuracy is not a dependable measure for determining model performance.",https://ieeexplore.ieee.org/document/9334129/,2020 2nd International Multidisciplinary Information Technology and Engineering Conference (IMITEC),25-27 Nov. 2020,ieeexplore
10.1109/IJCNN.1993.714095,Real World Computing program - overview,IEEE,Conferences,"In 1992 the Real World Computing (RWC) program was launched by MITI (Ministry of International Trade and Industry, Japan). This is a Japanese new national project on information processing as the successor to the Fifth Generation Computer project which formally ended in 1992. The primary objective of the new program is to lay the theoretical foundation and to pursue the technological realization of human-like flexible information processing as a new paradigm of information processing towards the highly information-based society of the 21st century. Contrary to that the Fifth Generation project pursued the aspect of logical (symbol-based) information processing of humans, the RWC program is rather pursuing another aspect of intuitive (pattern-based) information processing as a new framework and foundation of information processing, and also aiming at unifying both aspects in a bottom-up manner.",https://ieeexplore.ieee.org/document/714095/,"Proceedings of 1993 International Conference on Neural Networks (IJCNN-93-Nagoya, Japan)",25-29 Oct. 1993,ieeexplore
10.1109/ICCICT.2018.8325873,Real time control of induction motor using neural network,IEEE,Conferences,"Induction Machine being simple in operation and highly reliable equipment with low cost involving minimal maintenance requirements it has become the most popular equipment in industry. With the development of power electronic technology, low cost DSP, micro-controllers and parameter estimation techniques the induction motor an attractive component for the future high performance drives induction motors have many applications in the industries. The PWM technique which drives a Voltage Source Inverter (VSI) in order to apply v/f control is used to control a 3 ph induction motor. In industrial applications the most widely used controllers are PI controllers because of their simple structure and their capability of delivering good performance over a large band of operating condition. PI and ANN controllers have been designed and developed using MATLAB/SIMULINK. Prototype model is developed to validate the effectiveness of the PI and ANN control of induction motor drive using dSPACE DS1104 controller. The performance of the SVPWM based induction motor in open loop and closed loop is presented with simulation. Artificial Neural Network and Conventional PI controllers have been practically implemented using SVPWM based VSI fed induction motor in open loop mode. Hardware set up has been developed using Inverter and dSPACE controller. The real time performance of ANN based induction motor is presented by validating simulation results with the hardware results.",https://ieeexplore.ieee.org/document/8325873/,2018 International Conference on Communication information and Computing Technology (ICCICT),2-3 Feb. 2018,ieeexplore
10.1109/CBI.2013.63,Real-Time Collaboration and Experience Reuse for Cloud-Based Workflow Management Systems,IEEE,Conferences,"In this paper we explore the concept of a real-time workflow collaboration platform. The work presents how a cloud-based Workflow Management System (WfMS) combines the technologic features which are offered by the cloud computing paradigm with a developed resource model for collaboration and reuse of experiential knowledge in workflows. It is based on a prototypical generic software system for integrated process and knowledge management and addresses the concept for collaborative workflow modeling. The concept for the reuse of workflows combines the ability of the resource model to share workflows with Case-Based Reasoning, a specific field of Artificial Intelligence. In particular, a sample workflow of a process in the financial industry is discussed. By enabling collaborative workflow modeling and by providing expert knowledge to large group of users, we aim at the improvement of the quality of workflows. Consequently, workload can be reduced, thus facilitating the work of all process stakeholders.",https://ieeexplore.ieee.org/document/6642905/,2013 IEEE 15th Conference on Business Informatics,15-18 July 2013,ieeexplore
10.1109/INDUSCON51756.2021.9529474,Real-Time Downhole Geosteering Data Processing Using Deep Neural Networks On FPGA,IEEE,Conferences,"The success of machine learning has spread the deployment of Deep neural Networks (DNNs) in numerous industrial applications. As an essential technique in today’s oilfield industry, geosteering requires performing DNN inference on the hardware devices that operates under the severe down-hole environments. However, it can produce massive power dissipation and cause long delays to execute the computation-intensive DNN inference on the current hardware platforms, e.g., CPU and GPU. In this paper, we propose an FPGA-based hardware design to efficiently conduct the DNN inference for geosteering tasks in downhole environments. At first, a comprehensive analysis is presented to choose the optimal computation mapping method for the target DNN model. A detailed description of the customized hardware implementation is then proposed to accomplish a complete DNN inference on the FPGA board. The experimental results shows that the proposed design achieves 7× (1.4×) improvement on performance and 82× (1.3×) reduction on power consumption compared with CPU(GPU).",https://ieeexplore.ieee.org/document/9529474/,2021 14th IEEE International Conference on Industry Applications (INDUSCON),15-18 Aug. 2021,ieeexplore
10.1109/SEAA.2019.00035,Real-Time Estimated Time of Arrival Prediction System using Historical Surveillance Data,IEEE,Conferences,"Prediction of Estimated Times of Arrival (ETA) is a challenging problem for the aviation industry. Flights recurrently deviate from their scheduled time of arrival, which has negative downstream consequences that affect the efficiency of operations. Therefore, accurate and up-to-date ETA estimations prior to its landing can help in optimizing the actions to be taken by the different air transportation agents whenever schedule deviations are incurred, and thus reduce the economic, logistic and environmental impact that they cause. This presentation exposes an infrastructure for high-fidelity computation of accurate ETA in real-time, based on a data-driven approach that leverages the use of recorded aircraft trajectories. This infrastructure is composed of different elements: (1) a live ADS-B tracks gathering system embedded in a lambda-architecture cluster, with capabilities for real-time distribution and data lake storage (2) an ETA prediction machine learning model, employing the actual 4D aircraft position as input; and (3) a hybrid cloud architecture to support real-time visualization and distributions of ETA predictions. The proposed infrastructure has been successfully validated in a real environment (Transforming Transport, an European Commission funded project). This infrastructure enables real-time computation and distribution of accurate ETA for any arrival operation of interest. Results supported the envisioned benefits of getting such accurate ETA, which basically turn into a reduction of associated costs for airport authorities and airlines.",https://ieeexplore.ieee.org/document/8906713/,2019 45th Euromicro Conference on Software Engineering and Advanced Applications (SEAA),28-30 Aug. 2019,ieeexplore
10.1109/ICECCE52056.2021.9514169,Real-Time Fraud Prediction On Streaming Customer-Behaviour Data,IEEE,Conferences,"The field of detection and analysis of fraud situations continues to be an essential topic in the telecom industry. This study proposes a methodology that can predict abnormal behavior situations on real-time streaming customer behavior data in the telecommunication domain. The prototype implementation of the proposed method is designed, developed, and applied to the telecommunications dataset. We perform performance tests for the method's prediction success and scalability metrics on the designed prototype application. The results indicate that the proposed method proves to be a promising approach in the telecommunication sector.",https://ieeexplore.ieee.org/document/9514169/,"2021 International Conference on Electrical, Communication, and Computer Engineering (ICECCE)",12-13 June 2021,ieeexplore
10.1109/08IAS.2008.164,Real-Time Implementation of Intelligent Modeling and Control Techniques on a PLC Platform,IEEE,Conferences,"Programmable logic controllers (PLCs) have been used for many decades for standard control in industrial and factory environments. Over the years, PLCs have become computational efficient and powerful, and a robust platform with applications beyond the standard control and factory automation. Due to the new advanced PLC's features and computational power, they are ideal platforms for exploring advanced modeling and control methods, including computational intelligence based techniques such as neural networks, particle swarm optimization (PSO) and many others. Some of these techniques require fast floating-point calculations that are now possible in real-time on the PLC. This paper focuses on the Allen-Bradley ControlLogix brand of PLCs, due to their high performance and extensive use in industry. The design and implementation of a neurocontroller consisting of two neural networks, one for modeling and the other for control, and the training of these neural networks with particle swarm optimization is presented in this paper on a single PLC. The neurocontroller in this study is a power system stabilizer (PSS) that is used for power system oscillation damping. The PLC is interfaced to a power system simulated on the real time digital simulator. Real time results are presented showing that the PLC is a suitable hardware platform for implementing advanced modeling and control techniques for industrial applications.",https://ieeexplore.ieee.org/document/4658952/,2008 IEEE Industry Applications Society Annual Meeting,5-9 Oct. 2008,ieeexplore
10.1109/MobServ.2016.24,Real-Time Privacy Preserving Crowd Estimation Based on Sensor Data,IEEE,Conferences,"As one of the popular topics to ensure public safety, crowd estimation has attracted lots of attentions from both industry and academia. Most of traditional crowd estimation approaches rely on sophisticated computer vision algorithms to estimate crowd based on camera data, therefore suffering from privacy issues and high deployment and data processing cost. In this paper we present a sensor fusion based approach to real-time crowd estimation based on privacy-conscious and inexpensive sensors. The approach has been implemented and verified first by a small scale deployment at our lab, and then tested based on a 3-month trial at a shopping mall in Singapore. A deep analysis has been carried out based on the data sets collected from the trial, showing promising results: (1) the data from CO2, sound pressure and infrared sensors are influential in estimating crowd levels for indoor environments, (2) Random Forest and C4.5 are identified as the more suitable supervised learning models, (3) an accuracy of 95% can be achieved by our crowd estimation system in a real scenario. In contrast to the state of the art, our approach is privacy preserving and can provide comparable estimation accuracy with lower deployment and processing cost and better applicability for large scale setups. It can be used either as an alternative solution when user privacy must be enforced or as a complementary solution to camera-based crowd estimation when privacy is less concerned because of pubic safety.",https://ieeexplore.ieee.org/document/7787060/,2016 IEEE International Conference on Mobile Services (MS),27 June-2 July 2016,ieeexplore
10.1109/ICIS.2018.8466525,Real-Time Tracing Of A Weld Line Using Artificial Neural Networks,IEEE,Conferences,"Robotic Manipulators are becoming increasingly popular nowadays with applications in almost every industry and production line. It is difficult but essential to create a common algorithm for the different types of manipulators present in todays market so that automation can be achieved at a faster rate. This paper aims to present a real time implementation of a method to control a Tal Brabo! Robotic manipulator to move along a given weld line in order to be utilized in factories for increasing production capacity and decreasing production time. The controller used here is provided by Trio, whose ActiveX component is interfaced to MATLAB. Images were captured to identify weld lines in every possible alignment to find points of interest and the neural network was trained in order to follow a given weld line once the work-piece was placed on the work-table.",https://ieeexplore.ieee.org/document/8466525/,2018 IEEE/ACIS 17th International Conference on Computer and Information Science (ICIS),6-8 June 2018,ieeexplore
10.1109/AI4I46381.2019.00022,Real-World Conversational AI for Hotel Bookings,IEEE,Conferences,"In this paper, we present a real-world conversational AI system to search for and book hotels through text messaging. Our architecture consists of a frame-based dialogue management system, which calls machine learning models for intent classification, named entity recognition, and information retrieval subtasks. Our chatbot has been deployed on a commercial scale, handling tens of thousands of hotel searches every day. We describe the various opportunities and challenges of developing a chatbot in the travel industry.",https://ieeexplore.ieee.org/document/9027787/,2019 Second International Conference on Artificial Intelligence for Industries (AI4I),25-27 Sept. 2019,ieeexplore
10.1109/ICMLC.2012.6359610,Real-time dynamic vehicle detection on resource-limited mobile platform,IEEE,Conferences,"Given the rapid expansion of car ownership worldwide, vehicle safety is an increasingly critical issue in the automobile industry. The reduced cost of cameras and optical devices has made it economically feasible to deploy front-mounted intelligent systems for visual-based event detection. Prior to vehicle event detection, detecting vehicles robustly in real time is challenging, especially conducting detection process in images captured by a dynamic camera. Therefore, in this paper, a robust vehicle detector is developed. Our contribution is three-fold. Road modeling is first proposed to confine detection area for maintaining low computation complexity and reducing false alarms as well. Haar-like features and eigencolors are then employed for the vehicle detector. To tackle the occlusion problem, chamfer distance is used to estimate the probability of each individual vehicle. AdaBoost algorithm is used to select critical features from a combined high dimensional feature set. Experiments on an extensive dataset show that our proposed system can effectively detect vehicles under different lighting and traffic conditions, and thus demonstrates its feasibility in real-world environments.",https://ieeexplore.ieee.org/document/6359610/,2012 International Conference on Machine Learning and Cybernetics,15-17 July 2012,ieeexplore
10.1109/SACI51354.2021.9465544,Real-time locating system and digital twin in Lean 4.0,IEEE,Conferences,"Digital twin plays a key role in the current development of smart manufacturing systems. Through simulation in the cyber world, real phenomena in the physical world can be predicted and optimized before the final implementation. The usage of the digital twin is enhanced along with the uprising of Industry 4.0, in which data availability supports the further insight of system status, helping the operation managers understand their system and perform resources adjustment more easily. Based on this digitization mature, Lean 4.0, a new concept elaborated from Lean manufacturing, has been interested recently. There are several technologies constituted digital twin that provide a favourable condition for Lean 4.0, such as augmented reality, cloud computing. In this paper, the integration of the Real-time Locating System (RTLS) into digital twin is proposed, which facilitates the performance of Lean 4.0 in manufacturing operation. Not only gain effective control over the facility's assets, but this integration also enhances the resources utilization, cut down operational wastes, thus brings a better turnover for industrial systems. A case study of successful implementation is shown, which proved the possible advantages of this approach.",https://ieeexplore.ieee.org/document/9465544/,2021 IEEE 15th International Symposium on Applied Computational Intelligence and Informatics (SACI),19-21 May 2021,ieeexplore
10.1109/ICSensT.2017.8304431,Real-time monitoring of powder blend composition using near infrared spectroscopy,IEEE,Conferences,"Near Infrared Spectroscopy (NIRS) is a very powerful utility in a Process Analytical Technology (PAT) system because it can be used to monitor a multitude of process parameters non-invasively, non-destructively in real time and in hazardous environments. A catch to the versatility of NIRS is the requirement for Multi-Variate Data Analysis (MVDA) to calibrate the measurement of the parameter of interest. This paper presents a NIRS based real time continuous monitoring of powder blend composition which has widespread applications such as the pharmaceutical industry. The proposed system design enables reduction of optical path length so that the sensors can be successfully installed into powder conveyance systems. Sensor signal processing techniques were developed in this work to improve accuracy while minimizing pre-processing steps. The paper presents the implementation of several parameter estimation methodologies applied to sensor data collected using MATLAB® software for a model powder blending process. Several techniques were examined for the development of chemometric models of the multi-sensor data, including Principal Component Analysis (PCA), Partial Least Squares Regression (PLSR), Support Vector Machines (SVM) and Artificial Neural Networks (ANN). The performances of each of the models were compared in terms of accuracy (MSE) in predicting blend composition. The results obtained show that machine learning-based approaches produce process models of similar accuracy and robustness compared to models developed by PLSR while requiring minimal pre-processing and also being more adaptable to new data.",https://ieeexplore.ieee.org/document/8304431/,2017 Eleventh International Conference on Sensing Technology (ICST),4-6 Dec. 2017,ieeexplore
10.1109/ICCCN52240.2021.9522281,Realization of an Intrusion Detection use-case in ONAP with Acumos,IEEE,Conferences,"With Software-Defined Networking and Machine Learning/Artificial Intelligence (ML/AI) reaching new paradigms in their corresponding fields, both academia and industry have exhibited interests in discovering unique aspects of intelligent and autonomous communication networks. Transforming such intentions and interests to reality involves software development and deployment, which has its own story of significant evolution. There has been a notable shift in the strategies and approaches to software development. Today, the divergence of tools and technologies as per demand is so substantial that adapting a software application from one environment to another could involve tedious redesign and redevelopment. This implies enormous effort in migrating existing applications and research works to a modern industrial setup. Additionally, the struggles with sustainability maintenance of such applications could be painful. Concerning ML/AI, the capabilities to train, deploy, retrain, and re-deploy AI models as quickly as possible will be crucial for AI-driven network systems. An end-to-end workflow using unified open-source frameworks is the need of the hour to facilitate the integration of ML/AI models into the modern software-driven virtualized communication networks. Hence, in our paper, we present such a prototype by demonstrating the journey of a sample SVM classifier from being a python script to be deployed as a micro-service using ONAP and Acumos. While illustrating various features of Acumos and ONAP, this paper intends to make readers familiar with an end-to-end workflow taking advantage of the integration of both open-source platforms.",https://ieeexplore.ieee.org/document/9522281/,2021 International Conference on Computer Communications and Networks (ICCCN),19-22 July 2021,ieeexplore
10.1109/CIG.2019.8848119,Realtime Adaptive Virtual Reality for Pain Reduction,IEEE,Conferences,"Recent years have seen digital game mediums taking conventional amusement, entertainment and leisure industries by storm. They have revolutionized the system to the extent that the industry cannot now even dream to do without this overwhelming reality. The same game mediums that have capitalized on intrinsic leisure aspects have simultaneously focused with equal vigor on other equally, if not more, important collateral objectives. This paper builds on this concept and discusses a work in progress currently being carried out at the University of Malta. It proposes the use of games as a means of distraction therapy for individuals undergoing painful clinical treatment procedures. The creation of an adaptive Virtual Reality (VR) game within an Artificial Intelligence framework will without doubt be of a significantly greater benefit to the community than mere entertainment applications.",https://ieeexplore.ieee.org/document/8848119/,2019 IEEE Conference on Games (CoG),20-23 Aug. 2019,ieeexplore
10.1109/ISAMSR53229.2021.9567891,Recent and Future Innovative Artificial Intelligence Services and Fields,IEEE,Conferences,"Recent innovative Artificial Intelligence (AI) solutions accelerate digital transformations in different fields. It is important to highlight and explore this innovative AI service in different domains so that digital transformation can be planned, designed, and implemented for maximum society benefits. This paper investigates the different fields of AI services that can be utilized towards achieving highly beneficial digital transformations in different societal domains. This includes marketing, finance and banking, healthcare industry, emotion, and creative AI, as well as recent AI fields as explainable and responsible AI. Exploring and understanding the innovative AI services in these domains widen researcher capabilities to achieve effective and highly beneficial digital transformations.",https://ieeexplore.ieee.org/document/9567891/,"2021 4th International Symposium on Agents, Multi-Agent Systems and Robotics (ISAMSR)",6-8 Sept. 2021,ieeexplore
10.1109/ICPAIR.2011.5976889,Recognition of bolt and nut using artificial neural network,IEEE,Conferences,"This paper focuses on the recognition system of bolt and nut in real time for application in various industries, particularly the automotive industry. The objective of this study is to develop the image processing algorithm to get the normalized cropping images which would be suitable inputs for the learning process using Backpropagation Neural Network. Testing is done using a real-time visual recognition system. The Matlab software version 7.6 is used to integrate all algorithms, whereas the stepper motor differentiates the final result of bolt and nut in separate places. The result shows that the system can detect moving object accurately on the belt conveyor at a speed of 9 cm/sec. with an accuracy 92%.",https://ieeexplore.ieee.org/document/5976889/,2011 International Conference on Pattern Analysis and Intelligence Robotics,28-29 June 2011,ieeexplore
10.1109/RAMS.2002.981623,Reliability improvement of airport ground transportation vehicles using neural networks to anticipate system failure,IEEE,Conferences,"This paper describes a joint industry/university collaboration to develop a prototype system to provide real time monitoring of an airport ground transportation vehicle with the objectives of improving availability and minimizing field failures by estimating the proper timing for condition-based maintenance. Hardware for the vehicle was designed, developed and tested to monitor door characteristics (voltage and current through the motor that opens and closes the doors and door movement time and position), to quickly predict degraded performance, and to anticipate failures. A combined statistical and neural network approach was implemented. The neural network ""learns"" the differences among door sets and can be tuned quite easily through this learning. Signals are processed in real time and combined with previous monitoring data to estimate, using the neural network, the condition of the door set relative to maintenance needs. The prototype system was installed on several vehicle door sets at the Pittsburgh International Airport and successfully tested for several months under simulated and actual operating conditions. Preliminary results indicate that improved operational reliability and availability can be achieved.",https://ieeexplore.ieee.org/document/981623/,Annual Reliability and Maintainability Symposium. 2002 Proceedings (Cat. No.02CH37318),28-31 Jan. 2002,ieeexplore
10.1109/WINCOM.2018.8629594,"Reliable and cost-effective communication at high seas, for a safe operation of autonomous ship",IEEE,Conferences,"Nowadays, the high automation of ships has resulted in an ever -increasing need for real time data exchange to monitor ships, cargo, and ensure safety and security on board. This evolution in ship technology and management, must be supported by reliable, and cost-effective communication carriers. The satellite communication services can deliver vital real time capability to keep the ship continuously connected to the maritime players while it is trading worldwide. At high seas, the only communication service that can meet the ship need in term of data exchange is the mobile satellite communication. The reliability of these communication services will contribute significantly in the success of the future implementation of the autonomous ship in the maritime industry. This paper consists of the presentation of an overview of the current satellite services and its comparison in term of reliability, cost-effectiveness, and its capability to meet the autonomous ship communication need for its safe operation. This study results in a proposal of the suitable satellite communication services that can support the implementation of autonomous ship.",https://ieeexplore.ieee.org/document/8629594/,2018 6th International Conference on Wireless Networks and Mobile Communications (WINCOM),16-19 Oct. 2018,ieeexplore
10.1109/DIS.2006.63,Remote Programming of Multirobot Systems within the UPC-UJI Telelaboratories: System Architecture and Agent-Based Multirobot Control,IEEE,Conferences,"One of the areas that needs more improvement within the e-learning environments via Internet (in fact they suppose a very big effort to be accomplished) is allowing students to access and practice real experiments is a real laboratory, instead of using simulations in Marin, R. et al. (2003). Real laboratories allow students to acquire methods, skills and experience related to real equipment, in a manner that is very close to the way they are being used in industry. The purpose of the project is the study, development and implementation of an e-learning environment to allow undergraduate students to practice subjects related to Robotics and Artificial Intelligence. The system, which is now at a preliminary stage, will allow the remote experimentation with real robotic devices (i.e. robots, cameras, etc.). It will enable the student to learn in a collaborative manner (remote participation with other students) where it will be possible to combine the on-site activities (performed ""in-situ"" within the real lab during the normal practical sessions), with the ""online"" one (performed remotely from home via the Internet). Moreover, the remote experiments within the e-laboratory to control the real robots can be performed by both, students and even scientist. This project is under development and it is carried out jointly by two Universities (UPC and UJI). In this article we present the system architecture and the way students and researchers have been able to perform a remote programming of multirobot systems via Web",https://ieeexplore.ieee.org/document/1633445/,IEEE Workshop on Distributed Intelligent Systems: Collective Intelligence and Its Applications (DIS'06),15-16 June 2006,ieeexplore
10.1109/DTPI52967.2021.9540104,Research and practice of lightweight digital twin speeding up the implementation of flexible manufacturing systems,IEEE,Conferences,"Parallel manufacturing in Industry 5.0 requires digital twin to digitize physical systems, building virtual models to open up channels connecting physical systems, information systems, and social systems, and transforming the physical models of the existing production environment to achieve two-way feedback of virtual and real is the current research direction. This paper proposes the modeling idea of lightweight digital twin, extracts core dimensions and performs digital virtual simulation, so as to quickly realize the complete process of two-way feedback, and realize a set of chess flexible parallel manufacturing production lines as a practice for the design of complete lightweight digital twin.",https://ieeexplore.ieee.org/document/9540104/,2021 IEEE 1st International Conference on Digital Twins and Parallel Intelligence (DTPI),15 July-15 Aug. 2021,ieeexplore
10.1109/ICMLC.2006.258822,Research of SDSS Based on Spatial DW and Comgis,IEEE,Conferences,"With the continuously expanded means of obtaining spatial data, many real life applications would substantially benefit from introducing spatial representations of the data. More and more researchers pay attention to SDSS (spatial decision support system). Spatial data warehouse provides an efficient analysis environment for both spatial data and non-spatial data, and ComGis can simplify the processing of spatial data in complex system. By combining these two technologies, a framework of spatial DSS is proposed. Finally, a prototype used in field of administration for industry and commerce is introduced",https://ieeexplore.ieee.org/document/4028306/,2006 International Conference on Machine Learning and Cybernetics,13-16 Aug. 2006,ieeexplore
10.1109/ICET51757.2021.9450991,Research on Artificial Intelligence Industrial Big Data Platform for Industrial Internet Applications,IEEE,Conferences,"With the development of industry, enterprises have put forward higher requirements for real-time data collection and analysis. According to this, a distributed industrial big data platform for Industrial Internet applications is designed, which can effectively support the industrial field big data collection and storage. The system contains a distributed database, real-time database and offline big data platform to implement the collection and storage of all data in the system. Thus, the industrial big data platform architecture and related data hierarchical processing flow are proposed, which can effectively meet the requirements of the multi-source data and real-time processing of smart factory. The proposed data platform technical architecture will have important reference value for the realization of smart manufacturing and smart factory.",https://ieeexplore.ieee.org/document/9450991/,2021 IEEE 4th International Conference on Electronics Technology (ICET),7-10 May 2021,ieeexplore
10.1109/DSAA.2016.32,Reserve Price Optimization at Scale,IEEE,Conferences,"Online advertising is a multi-billion dollar industry largely responsible for keeping most online content free and content creators (""publishers"") in business. In one aspect of advertising sales, impressions are auctioned off in second price auctions on an auction-by-auction basis through what is known as real-time bidding (RTB). An important mechanism through which publishers can influence how much revenue they earn is reserve pricing in RTB auctions. The optimal reserve price problem is well studied in both applied and academic literatures. However, few solutions are suited to RTB, where billions of auctions for ad space on millions of different sites and Internet users are conducted each day among bidders with heterogenous valuations. In particular, existing solutions are not robust to violations of assumptions common in auction theory and do not scale to processing terabytes of data each hour, a high dimensional feature space, and a fast changing demand landscape. In this paper, we describe a scalable, online, real-time, incrementally updated reserve price optimizer for RTB that is currently implemented as part of the AppNexus Publisher Suite. Our solution applies an online learning approach, maximizing a custom cost function suited to reserve price optimization. We demonstrate the scalability and feasibility with the results from the reserve price optimizer deployed in a production environment. In the production deployed optimizer, the average revenue lift was 34.4% with 95% confidence intervals (33.2%, 35.6%) from more than 8 billion auctions over 46 days, a substantial increase over non-optimized and often manually set rule based reserve prices.",https://ieeexplore.ieee.org/document/7796939/,2016 IEEE International Conference on Data Science and Advanced Analytics (DSAA),17-19 Oct. 2016,ieeexplore
10.1109/ICE/ITMC52061.2021.9570221,Resilient Manufacturing Systems enabled by AI support to AR equipped operator,IEEE,Conferences,"Supply chains and manufacturing systems robustness and resilience are, for many years, but especially nowadays, key features requested to ensure reliable and efficient production processes. Two domains are crucial to achieve such purpose: the former is fast and comprehensive monitoring, efficient and reliable condition detection and effective and explicable support for decision making. The latter refers to the intervention by operators, able to better identify problems and to put in place effective operations aimed at fixing it or, better, to prevent such circumstances. This paper presents an integrated approach encompassing a sophisticated IoT and AI-based approach to monitor and detect critical situations, fully integrated with an AR (Augmented Reality) system supporting operators in the field to take informed actions in bi-directional continuous connection. Activities in the context of EC funded project Qu4lity developed in Politecnico di Milano Industry 4.0 Lab, a test environment implementing the proposed approach and demonstrating in an automated production line the effectiveness of the approach, significantly improving performances. Analysis of performance indicators demonstrates the soundness of the proposed solution and implementation methodology to make the overall production process more resilient, efficient and with product defects reduction.",https://ieeexplore.ieee.org/document/9570221/,"2021 IEEE International Conference on Engineering, Technology and Innovation (ICE/ITMC)",21-23 June 2021,ieeexplore
10.1109/ELMAR.2019.8918680,Review on Text Detection Methods on Scene Images,IEEE,Conferences,"Recently, a lot of effort has been put into developing text detection methods on natural scene images in academic research and industry. In general, text detection refers to localizing all text instances in an image which can be further processed with an Optical Character Recognition (OCR) software in order to obtain machine-readable characters. The amount of published methods is constantly growing which makes it very challenging to be up-to-date with all approaches and state-of-the-art methods. Review papers become outdated in a less than a year from being published. Deep learning, a fast-growing field by itself, has become a mainstream approach in developing text detection methods. In this paper we present the up-to-date state-of-the-art methods in this challenging field. The methods are compared by their accuracy and real-time performance. We also present the most popular evaluation datasets for scene text detection.",https://ieeexplore.ieee.org/document/8918680/,2019 International Symposium ELMAR,23-25 Sept. 2019,ieeexplore
10.1109/ICECCE49384.2020.9179241,Role of Ubiquitous Computing and Mobile WSN Technologies and Implementation,IEEE,Conferences,"Computing capabilities such as real time data, unlimited connection, data from sensors, environmental analysis, automated decisions (machine learning) are demanded by many areas like industry for example decision making, machine learning, by research and military, for example GPS, sensor data collection. The possibility to make these features compatible with each domain that demands them is known as ubiquitous computing. Ubiquitous computing includes network topologies such as wireless sensor networks (WSN) which can help further improving the existing communication, for example the Internet. Also, ubiquitous computing is included in the Internet of Things (IoT) applications. In this article, it is discussed the mobility of WSN and its advantages and innovations, which make possible implementations for smart home and office. Knowing the growing number of mobile users, we place the mobile phone as the key factor of the future ubiquitous wireless networks. With secure computing, communicating, and storage capacities of mobile devices, they can be taken advantage of in terms of architecture in the sense of scalability, energy efficiency, packet delay, etc. Our work targets to present a structure from a ubiquitous computing point of view for researchers who have an interest in ubiquitous computing and want to research on the analysis, to implement a novel method structure for the ubiquitous computing system in military sectors. Also, this paper presents security and privacy issues in ubiquitous sensor networks (USN).",https://ieeexplore.ieee.org/document/9179241/,"2020 International Conference on Electrical, Communication, and Computer Engineering (ICECCE)",12-13 June 2020,ieeexplore
10.1109/SPAC53836.2021.9539961,SDN-based Industrial Internet Security Gateway,IEEE,Conferences,"Industrial Internet is widely used in the production field. As the openness of networks increases, industrial networks facing increasing security risks. Information and communication technologies are now available for most industrial manufacturing. This industry-oriented evolution has driven the emergence of cloud systems, the Internet of Things (IoT), Big Data, and Industry 4.0. However, new technologies are always accompanied by security vulnerabilities, which often expose unpredictable risks. Industrial safety has become one of the most essential and challenging requirements. In this article, we highlight the serious challenges facing Industry 4.0, introduce industrial security issues and present the current awareness of security within the industry. In this paper, we propose solutions for the anomaly detection and defense of the industrial Internet based on the demand characteristics of network security, the main types of intrusions and their vulnerability characteristics. The main work is as follows: This paper first analyzes the basic network security issues, including the network security needs, the security threats and the solutions. Secondly, the security requirements of the industrial Internet are analyzed with the characteristics of industrial sites. Then, the threats and attacks on the network are analyzed, i.e., system-related threats and process-related threats; finally, the current research status is introduced from the perspective of network protection, and the research angle of this paper, i.e., network anomaly detection and network defense, is proposed in conjunction with relevant standards. This paper proposes a software-defined network (SDN)-based industrial Internet security gateway for the security protection of the industrial Internet. Since there are some known types of attacks in the industrial network, in order to fully exploit the effective information, we combine the ExtratreesClassifier to enhance the detection rate of anomaly detection. In order to verify the effectiveness of the algorithm, this paper simulates an industrial network attack, using the acquired training data for testing. The test data are industrial network traffic datasets, and the experimental results show that the algorithm is suitable for anomaly detection in industrial networks.",https://ieeexplore.ieee.org/document/9539961/,"2021 International Conference on Security, Pattern Analysis, and Cybernetics（SPAC)",18-20 June 2021,ieeexplore
10.1109/BigDataSecurity-HPSC-IDS49724.2020.00050,SLA as a mechanism to manage risks related to chatbot services.,IEEE,Conferences,Intelligent Chatbot services become one of the mainstream applications in user help and many other areas. Apart from bringing numerous benefits to users these services may bring additional risks to the companies that employ them. The study starts with the review of the scale of chatbot industry and common use cases by focusing on their applications &amp; industry tendencies. Review of functionality and architecture of typical chatbot services shows the potential risks associated with chatbots. Analysis of such risks in the paper helped to build a checklist that security managers can use to assess risks prior to chatbot implementation. The proposed checklist was tested by reviewing a number of Service Level Agreements (SLA) of real chatbot providers.,https://ieeexplore.ieee.org/document/9123051/,"2020 IEEE 6th Intl Conference on Big Data Security on Cloud (BigDataSecurity), IEEE Intl Conference on High Performance and Smart Computing, (HPSC) and IEEE Intl Conference on Intelligent Data and Security (IDS)",25-27 May 2020,ieeexplore
10.1109/WF-IoT.2016.7845468,SQenloT: Semantic query engine for industrial Internet-of-Things gateways,IEEE,Conferences,"The Advent of Internet-of-Things (IoT) paradigm has brought exciting opportunities to solve many real-world problems. IoT in industries is poised to play an important role not only to increase productivity and efficiency but also to improve customer experiences. Two main challenges that are of particular interest to industry include: handling device heterogeneity and getting contextual information to make informed decisions. These challenges can be addressed by IoT along with proven technologies like the Semantic Web. In this paper, we present our work, SQenIoT: a Semantic Query Engine for Industrial IoT. SQenIoT resides on a commercial product and offers query capabilities to retrieve information regarding the connected things in a given facility. We also propose a things query language, targeted for resource-constrained gateways and non-technical personnel such as facility managers. Two other contributions include multi-level ontologies and mechanisms for semantic tagging in our commercial products. The implementation details of SQenIoT and its performance results are also presented.",https://ieeexplore.ieee.org/document/7845468/,2016 IEEE 3rd World Forum on Internet of Things (WF-IoT),12-14 Dec. 2016,ieeexplore
10.1109/FCCM48280.2020.00067,Scalable Full Hardware Logic Architecture for Gradient Boosted Tree Training,IEEE,Conferences,"Gradient Boosted Tree is most effective and standard machine learning algorithm in many fields especially with various type of tabular dataset. Besides, recent industry field and robotics field require high-speed, power efficient and real-time training with enormous data. FPGA is effective device which enable custom domain specific approach to give acceleration as well as power efficiency. We introduce a scalable full hardware implementation of Gradient Boosted Tree training with high performance and flexibility of hyper parameterization. Experimental work shows that our hardware implementation achieved 11-33 times faster than state-of-art GPU acceleration even with small gates and low power FPGA device.",https://ieeexplore.ieee.org/document/9114741/,2020 IEEE 28th Annual International Symposium on Field-Programmable Custom Computing Machines (FCCM),3-6 May 2020,ieeexplore
10.1109/IS3C50286.2020.00134,Screw defect detection system based on AI image recognition technology,IEEE,Conferences,"In the past ten years, smart manufacturing has been widely discussed and gradually introduced into various manufacturing fields. Since Germany proposed the concept of “Industry 4.0” in 2011, it has been spreading and feverish all over the world. For Industry 4.0, information digitization, intelligent defect detection and database platform management are their main core technologies. Aiming at a large screw industry manufacturing field in central and southern Taiwan, this paper proposes a screw defect detection system based on AI image recognition technology to detect damage to the nut during the “molding” process in the screw production process, and it is determined whether the inspected screw passes the inspection. The recognition result is given as shown in Figure 1. This paper uses 500 non-defective screw samples and 20 defective screw samples provided by the screw factory. The above samples collected real-time images through the sampling structure designed in this article, and we adopt Microsoft Corporation's ML.NET suite to model AI images, and uses the following four deep learning models: ResNetV2 50, ResNetV2 101, InceptionV3, MoblieNetV2 for learning; in the process of learning, this article divides the data set into three types of data sets (one is the unknown set that is not used for training but mixed with correct and defective samples, and the other is used for post-training verification of mixed samples with correct and defective samples. The third is a training set for training a mixture of correct and defective samples) This arrangement is used for subsequent verification models; after training, a PC-based screw defect detection system is implemented as shown in Figure 2; finally, with Detect screw defects in the form of instant photography. After the experiment, in 1,000 repeated tests, the success rate of defect detection reached 97%, while the false positive rate was only 2%.",https://ieeexplore.ieee.org/document/9394116/,"2020 International Symposium on Computer, Consumer and Control (IS3C)",13-16 Nov. 2020,ieeexplore
10.1109/CNS48642.2020.9162320,SecureAIS - Securing Pairwise Vessels Communications,IEEE,Conferences,"Modern vessels increasingly rely on the Automatic Identification System (AIS) digital technology to wirelessly broadcast identification and position information to neighboring vessels and ports. AIS is a time-slotted protocol that also provides unicast messages-usually employed to manage self-separation and to exchange safety information. However, AIS does not provide any security feature. The messages are exchanged in clear-text, and they are not authenticated, being vulnerable to several attacks, including forging and replay. Despite the existing contributions in the literature propose some strategies to overcome the exposed weaknesses, some are insecure, others do not comply with the standard, while the remaining few ones would introduce an unacceptable overhead-that is, they would require a relevant number of AIS-time slots, because of the limited payload available for each time-slot. In this paper, we propose SecureAIS, a key agreement scheme that allows any pair of vessels in reach of an AIS radio to agree on a shared session key of the desired length, by requiring just a fraction of the AIS time-slots of competing solutions. Further, the scheme is fully standard compliant and does not require any modification to the available hardware. The proposed scheme integrates the Elliptic Curve Qu-Vanstone (ECQV) implicit certification scheme and the Elliptic Curve Diffie Hellman (ECDH) key agreement technique, requiring moderate computational overhead while enjoying an optimal usage of the available bandwidth. When configured with the highest security level of 256 bits, SecureAIS allows two AIS transceivers to agree on a shared session key in 20 time-slots, against the 96 time-slots required by the competing solution based on traditional X.509 certificates. The proposed solution has been implemented and tested in a real scenario, while its security has been formally evaluated through the ProVerif tool. Finally, the source code of our Proof-of-concept using GNURadio and Ettus Research X310 has been also released as open-source to pave the way to further research by both Industry and Academia in maritime communication security.",https://ieeexplore.ieee.org/document/9162320/,2020 IEEE Conference on Communications and Network Security (CNS),29 June-1 July 2020,ieeexplore
10.1109/iSAI-NLP48611.2019.9045404,Semantic Enhancement and Multi-level Label Embedding for Chinese News Headline Classification,IEEE,Conferences,"News headline classification is a specific example of short text classification, which aims to extract semantic information from the short text and classify it accurately. It can provide a fast classification method for data of various kinds of news media, thus arousing the common concern of academia and industry. Most short text classification methods are based on the semantic expansion of external knowledge, which is unable to expansion dynamically in real time and make full use of label information. To overcome these problems, we propose a novel method which consists of three parts: semantic enhancement, multi-dimensional feature fusion network and multi-level label embedding. Firstly, the word-level semantic information are embedded into the character encoding from pre-train model to enhance semantic features. Secondly, both of Bi-GRU and multi-scale CNN are used to extract sequence and local features of text to enhance the semantic representation of the sentence. Furthermore, the multi-level label embedding is used to filter textual vector and assist classification in the word and sentence level respectively. Experimental results on NLPCC 2017 Chinese news headline classification task show that our model achieves 84.74% of accuracy and 84.75% of F1, improves over the best baseline model by 1.5% and 1.6%, respectively, and reaches the state-of-the-art performance.",https://ieeexplore.ieee.org/document/9045404/,2019 14th International Joint Symposium on Artificial Intelligence and Natural Language Processing (iSAI-NLP),30 Oct.-1 Nov. 2019,ieeexplore
10.1109/ETFA.2014.7005195,Semantic repository for case-based reasoning in CBM services,IEEE,Conferences,"Condition-based maintenance (CBM) has been implemented in industry to arrange the maintenance work as efficiently as possible. Case-based reasoning (CBR) can be used to automate part of the CBM decision process. However, in complex situations the final decisions have to be made by domain maintenance experts based on information gathered from several sources. This paper presents an approach for utilizing Semantic Web technologies and CBR in a knowledge base system supporting CBM services. The case knowledge base (CKB) is built over a semantic repository with an inference engine supporting ontology based information integration and data access using SPARQL queries. The knowledge base model developed for the system contains CBR task ontology and domain ontology for industrial control valves. Feasibility of the prototype CKB system was evaluated in experiments with real industrial case data.",https://ieeexplore.ieee.org/document/7005195/,Proceedings of the 2014 IEEE Emerging Technology and Factory Automation (ETFA),16-19 Sept. 2014,ieeexplore
10.1109/ICCICC46617.2019.9146103,Sequence Learning for Images Recognition in Videos with Differential Neural Networks,IEEE,Conferences,"Sequence learning from real-time videos is one of the hard challenges to current machine learning technologies and classic neural networks. Since existing supervised learning technologies are heavily dependent on intensive data and prior training, new methodologies for learning temporal sequences by unsupervised learning theories and technologies are yet to be developed. This paper presents the design and implementation of a novel Differential Neural Network (∇NN) for unsupervised sequence learning. The methodology is developed with a set of fundamental theories and enabling technologies for solving the problems of visual object recognition, motion detection, and visual semantic analysis in video sequence. A set of experiments on ∇NN for sequence learning is demonstrated. This work has not only led to a theoretical breakthrough to novel machine sequence learning, but also applicable to a wide range of challenging problems in computational intelligence and the AI industry.",https://ieeexplore.ieee.org/document/9146103/,2019 IEEE 18th International Conference on Cognitive Informatics & Cognitive Computing (ICCI*CC),23-25 July 2019,ieeexplore
10.1109/CCGrid.2015.41,Service Clustering for Autonomic Clouds Using Random Forest,IEEE,Conferences,"Managing and optimising cloud services is one of the main challenges faced by industry and academia. A possible solution is resorting to self-management, as fostered by autonomic computing. However, the abstraction layer provided by cloud computing obfuscates several details of the provided services, which, in turn, hinders the effectiveness of autonomic managers. Data-driven approaches, particularly those relying on service clustering based on machine learning techniques, can assist the autonomic management and support decisions concerning, for example, the scheduling and deployment of services. One aspect that complicates this approach is that the information provided by the monitoring contains both continuous (e.g. CPU load) and categorical (e.g. VM instance type) data. Current approaches treat this problem in a heuristic fashion. This paper, instead, proposes an approach, which uses all kinds of data and learns in a data-driven fashion the similarities and resource usage patterns among the services. In particular, we use an unsupervised formulation of the Random Forest algorithm to calculate similarities and provide them as input to a clustering algorithm. For the sake of efficiency and meeting the dynamism requirement of autonomic clouds, our methodology consists of two steps: (i) off-line clustering and (ii) on-line prediction. Using datasets from real-world clouds, we demonstrate the superiority of our solution with respect to others and validate the accuracy of the on-line prediction. Moreover, to show the applicability of our approach, we devise a service scheduler that uses the notion of similarity among services and evaluate it in a cloud test-bed.",https://ieeexplore.ieee.org/document/7152517/,"2015 15th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing",4-7 May 2015,ieeexplore
10.1109/EEEIC.2019.8783979,Short-term load forecasting for Jordan’s Power System Using Neural Network based Different,IEEE,Conferences,"In recent times, there is a burst of interest in load forecasting. It is considered as a backbone activity in the electric power industry. Load forecasting influence decision both from an engineering perspective as well as a financial perspective. The key success in load forecasting is a partnership with the industry and how much the suitable load forecasting method has been widely used by the industry. These methods should be a simple solution to the real world problem. Although some level of sophistication is necessary to achieve high accuracy, so the successful methods typically find a good balance between simplicity and accuracy. Hence neural networks (NN) has received wide concern because of its understandable model, simple and direct implementation and satisfactory performance. In this paper, a short-term load forecasting is demonstrated based on neural networks (NN) adaption in Jordan's power system. Updated parameters are op-timized using different techniques; particle swarm optimization (PSO), genetic algorithm (GA) and elephant herding optimization (EHO). In addition, the error is calculated before and after optimization techniques. As the initial experimental result, the analysis shows that the prediction obtained using metaheuristic optimization based NNs is competitively suitable for the load forecasting. Whereas, two-layer NN gives better prediction with the least error. Therefore, it is clear from the above studies that two-layer NN is the best for load forecasting. Finally, this work is implemented using neural network toolbox and optimization Matlab codes in MathWorks for developing a new and foreseeable future forecasting model.",https://ieeexplore.ieee.org/document/8783979/,2019 IEEE International Conference on Environment and Electrical Engineering and 2019 IEEE Industrial and Commercial Power Systems Europe (EEEIC / I&CPS Europe),11-14 June 2019,ieeexplore
10.1109/ICCEA53728.2021.00060,ShuffleNet2MC: A method of light weight fault diagnosis,IEEE,Conferences,"Bearing fault diagnosis plays an important role in the field of modern industry. Although convolution neural network achieves good results, large amount of parameters costs a lot of calculation, which brings challenges to the deployment of fault diagnosis tasks in low computational power equipments. To solve the problems, an novel CNN model ShuffleNet2MC based on improved Shufflenetv2 network is proposed. Firstly, Depthwise convolution and Channel Shuffle are used to reduce the computational cost while ensuring the accuracy of diagnosis computation; Secondly, mixed convolution is used to extract the features of different resolutions through multi-scale and multi-channel method, which improves the accuracy of the model; Finally, K-means quantization is applied to the model, which greatly reduces the GFLOPS of the model and further improves the performance of the model while ensuring that the accuracy is basically unchanged. A large number of experiments on the bearing fault dataset of Western Reserve University show that: The times of floating point operation and classification accuracy of ShufflenetV2 are 0.001GFLOPS and 97.9% respectively in the task of fault diagnosis. Compared with other models, it not only reduces the model parameters and compresses the model, but also gets better classification accuracy.",https://ieeexplore.ieee.org/document/9581143/,2021 International Conference on Computer Engineering and Application (ICCEA),25-27 June 2021,ieeexplore
10.1109/ICIEM51511.2021.9445285,Simulation based vehicle movement tracking using Kalman Filter algorithm for Autonomous vehicles,IEEE,Conferences,"In the domain of Software automotive industry, one of the most widely used algorithms for performing analysis of driving operations is the Kalman filter algorithm. In today's world of advanced machine learning, the Kalman filter remains an important tool to fuse measurements from several sensors to estimate the real time state of robotics systems such as a self-driving vehicle. Kalman filter is able to update an estimate of evolving nature of continuously changing states of the common filters to take a probabilistic estimate. The driving scenario results are updated in real time using 2-steps update and correction method. In this paper, we have described the process of Kalman filter and its variant to estimate about the detection of moving object in a given traffic scenario using advance toolboxes of MATLAB. Results have been shown for multiple changing parameters.",https://ieeexplore.ieee.org/document/9445285/,2021 2nd International Conference on Intelligent Engineering and Management (ICIEM),28-30 April 2021,ieeexplore
10.1109/IDAP.2018.8620887,Smart Grids and Industry 4.0,IEEE,Conferences,"Since the beginning of the Industrial Revolution, the concept of Industrial Revolution in the 4<sup>th</sup>/ Industry 4.0 which is a new turning point in the technologies used, is in every field as well as in the energy field which is considered as the key point of all revolutions. It is aimed that energy generation, transmission and distribution are more efficient and highly reliable together with the next generation software and hardware which is the result of this concept.It has been seen that existing networks can not be enough to achieve this goal.At this point, flexible, reliable, clean, sustainable and highly efficient electric energy due to smart grids come into play. However, due to the increase in demand for electricity due to increase in population and industrialization, the dependence of electric energy on production resources, the availability of renewable energy sources in a dispersed geographical structure and the loss and leakage are becoming more important in our country. The Cyber-Physical System, the Internet of Things, M2M (machine to machine), etc components are used in the energy field, especially in intelligent network systems, resulting in intensive use of technology at every stage from power generation to end-user distribution point. This includes better system operation, reduced costs, increased energy efficiency, reduced greenhouse effect, reduced downtime with reduced downtime, reduced loss/leakage rates, improved energy quality, better management of production and storage systems, intelligent meter reading and load management and real-time supply-demand management. Industry 4.0/4.Industrial Revolution and smart grids have many common aspects such as optimization, automation, efficient use and management of energy, intelligent production, everything internet. In this study, the development process of the Industrial 4.0 revolution in the world and in our country will be examined and the interaction with these process smart grids will be examined.",https://ieeexplore.ieee.org/document/8620887/,2018 International Conference on Artificial Intelligence and Data Processing (IDAP),28-30 Sept. 2018,ieeexplore
10.1109/CAC.2017.8243743,Soft-sensor software design of dissolved oxygen in aquaculture,IEEE,Conferences,"A software is designed for water quality monitoring in the aquaculture industry. It is mainly for dissolved oxygen soft sensing. Dissolved oxygen is an important dependent factor of water quality and has effect on fish growth. At present, most of the dissolved oxygen sensors are expensive for aquaculture farmers, so they won't use them for real-time detection to perform control and optimal operation. To deal with this problem, a soft-sensor software was designed and developed based on a data-driven model which is proposed by partial least squares (PLS) and neural networks. The software included three parts which were control software, monitoring software and model calculation software. An industrial case study demonstrated the feasibility and efficiency of the proposed soft-sensor software, and it was simple to use, real-time and generic. It was also the foundation for the control and optimization of dissolved oxygen in the aquaculture process.",https://ieeexplore.ieee.org/document/8243743/,2017 Chinese Automation Congress (CAC),20-22 Oct. 2017,ieeexplore
10.1109/WICT.2012.6409060,Software effort prediction using unsupervised learning (clustering) and functional link artificial neural networks,IEEE,Conferences,"Software cost estimation continues to be an area of concern for managing of software development industry. We use unsupervised learning (e.g., clustering algorithms) combined with functional link artificial neural networks for software effort prediction. The unsupervised learning (clustering) indigenously divide the input space into the required number of partitions thus eliminating the need of ad-hoc selection of number of clusters. Functional link artificial neural networks (FLANNs), on the other hand is a powerful computational model. Chebyshev polynomial has been used in the FLANN as a choice for functional expansion to exhaustively study the performance. Three real life datasets related to software cost estimation have been considered for empirical evaluation of this proposed method. The experimental results show that our method could significantly improve prediction accuracy of conventional FLANN and has the potential to become an effective method for software cost estimation.",https://ieeexplore.ieee.org/document/6409060/,2012 World Congress on Information and Communication Technologies,30 Oct.-2 Nov. 2012,ieeexplore
10.1109/ICSI.1992.217250,Software engineering environment development (SEED): an integration project of ROC,IEEE,Conferences,"The SEED development environment composes a software developer's service center, workstation environment (user site), and a communications network. Applications emphasis is on business software written in Cobol, scientific and engineering software written in Fortran 77, and system software written in C. Some AI or real-time and military-usable languages such as Lisp, Prolog, and Ada are also provided for software developers. Instead of having a heavy economic reliance on manufacturing, Taiwan must achieve even greater success in the services and information industries. In these industries, there is no more important component than software. It is no exaggeration to state that the continued elevation of the living standard in Taiwan depends on the growth and near term future domestic dominance and international competitiveness of its information industry. This growth demands sincere and focused efforts on the software industry. The government, related institutes, education institutions, and private industries will continue to plan and work together to make Taiwan software industry world-class in size and technology.&lt;<ETX>&gt;</ETX>",https://ieeexplore.ieee.org/document/217250/,Proceedings of the Second International Conference on Systems Integration,15-18 June 1992,ieeexplore
10.1109/VLSID.2018.26,Special Session: Design of Energy-Efficient and Reliable VLSI Systems: A Data-Driven Perspective,IEEE,Conferences,"Summary form only given, as follows. The complete presentation was not made available for publication as part of the conference proceedings. The amount of data generated and collected across computing platforms every day is not only enormous, but growing at an exponential rate. Advanced data analytics and machine learning techniques have become increasingly essential to analyze and extract meaning from such “Big Data”. These techniques can be very useful to detect patterns and trends to improve the operational behavior of computing systems, but they also introduce a number of outstanding challenges: (1) How can we design and deploy data analytics mechanisms to improve energy-efficiency and reliability in IoT and mobile devices, without introducing significant software overheads? (2) How to leverage emerging technologies (e.g.,3D integration) to design energy-efficient and reliable manycore systems for big data computing? (3) How to use machine learning and data mining techniques for effective design space exploration of computing systems, and enable adaptive control to improve energy-efficiency? (4) How can data analytics detect anomalies and increase robustness in the network backbone of emerging large-scale networking systems? To address these outstanding challenges, out-of-the-box approaches need to be explored. In this special session, we will discuss these outstanding problems and describe far-reaching solutions applicable across the interconnected ecosystem of IoT and mobile devices, manycore chips, datacenters, and networks. The special session brings together speakers with unique insights on applying data analytics and machine learning to real-world problems to achieve the most sought after features on multi-scale computing platforms, viz. intelligent data mining, energyefficiency, and robustness. By integrating data analytics and machine learning algorithms, statistical modeling, embedded hardware and software design, and cloud computing content, this session will engage a broad section of Embedded and VLSI Design conference attendees. This special session is targeted towards university researchers/professors, students, industry professionals, and embedded/VLSI system designers. This session will attract newcomers who want to learn how to apply data analytics to solve problems in computing systems, as well as experienced researchers looking for exciting new directions in embedded systems, VLSI design, EDA algorithms, and multi-scale computing.",https://ieeexplore.ieee.org/document/8326889/,2018 31st International Conference on VLSI Design and 2018 17th International Conference on Embedded Systems (VLSID),6-10 Jan. 2018,ieeexplore
10.1109/ACMI53878.2021.9528185,Speed Bump &amp; Pothole Detection with Single Shot MultiBox Detector Algorithm &amp; Speed Control for Autonomous Vehicle,IEEE,Conferences,"The development of self-driving cars has always been an extensive research field for the automobile industry. To make a capable self-driving car, many challenges need to be resolved. Detection of the road condition is one of them. This paper focuses on a particular part-detection of speed bumps and potholes using a camera and analyzing the video feed with the help of artificial intelligence. To solve this problem a popular and lightweight algorithm, SSD (Single Shot Multibox Detector) is used. This is an optimal choice because of being lightweight and also accurate enough to run on mobile devices and to use in real-life situations. For detecting speed bumps and potholes, a dataset has been created based on the road structure of Bangladesh as the main priority of this system is to work on the local environment. Raspberry Pi has been used as the main processing unit because of being small but powerful. A warning system has been implemented so that it can warn the onboard driver about the upcoming pothole or speed bump. This system can also send a signal to the speed controller unit of the car to reduce the speed on detection to avoid accidents or damages to the car. The speed control unit is a microcontroller-based system that uses an ATmega328 microcontroller and L298 motor driver. This paper summarizes the combination of an artificial intelligence-based detection system injunction with a microcontroller-based speed control system in a cost-effective way that can be used in building self-driving cars.",https://ieeexplore.ieee.org/document/9528185/,"2021 International Conference on Automation, Control and Mechatronics for Industry 4.0 (ACMI)",8-9 July 2021,ieeexplore
10.1109/ISCA45697.2020.00038,SpinalFlow: An Architecture and Dataflow Tailored for Spiking Neural Networks,IEEE,Conferences,"Spiking neural networks (SNNs) are expected to be part of the future AI portfolio, with heavy investment from industry and government, e.g., IBM TrueNorth, Intel Loihi. While Artificial Neural Network (ANN) architectures have taken large strides, few works have targeted SNN hardware efficiency. Our analysis of SNN baselines shows that at modest spike rates, SNN implementations exhibit significantly lower efficiency than accelerators for ANNs. This is primarily because SNN dataflows must consider neuron potentials for several ticks, introducing a new data structure and a new dimension to the reuse pattern. We introduce a novel SNN architecture, SpinalFlow, that processes a compressed, time-stamped, sorted sequence of input spikes. It adopts an ordering of computations such that the outputs of a network layer are also compressed, time-stamped, and sorted. All relevant computations for a neuron are performed in consecutive steps to eliminate neuron potential storage overheads. Thus, with better data reuse, we advance the energy efficiency of SNN accelerators by an order of magnitude. Even though the temporal aspect in SNNs prevents the exploitation of some reuse patterns that are more easily exploited in ANNs, at 4-bit input resolution and 90% input sparsity, SpinalFlow reduces average energy by 1.8×, compared to a 4-bit Eyeriss baseline. These improvements are seen for a range of networks and sparsity/resolution levels; SpinalFlow consumes 5× less energy and 5.4× less time than an 8-bit version of Eyeriss. We thus show that, depending on the level of observed sparsity, SNN architectures can be competitive with ANN architectures in terms of latency and energy for inference, thus lowering the barrier for practical deployment in scenarios demanding real-time learning.",https://ieeexplore.ieee.org/document/9138926/,2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA),30 May-3 June 2020,ieeexplore
10.1109/ASRU.2009.5372951,"Spoken dialogue systems: Challenges, and opportunities for research",IEEE,Conferences,"Summary form only given. Research into spoken dialog systems has yielded some interesting results recently, such as statistical models for improved robustness, and machine learning for optimal control, among others. What are the basic ideas behind these techniques? What opportunities do they exploit? Are they ready to be deployed in real systems? What remains to be done? This talk aims to tackle these questions. First, the task of dialog management and its challenges will be reviewed, including the effects of ASR errors; the curse of history; the lack of a single optimization metric; and the theory of mind problem. Next, current solutions to these problems will be addressed, focusing on learnings from real deployed systems, particularly in industry. Though relatively pervasive, current practices in industry still yield systems with important flaws. Recently, the research community has attempted to advance the state-of-the-art with techniques such as statistical models, machine learning, simulation, and incremental processing. This talk will present the basic ideas of some of these techniques, and examine their prospects for success in real applications - in light of both pragmatic commercial constraints, and also more fundamental properties of dialog. Finally, opportunities for further progress will be suggested.",https://ieeexplore.ieee.org/document/5372951/,2009 IEEE Workshop on Automatic Speech Recognition & Understanding,13 Nov.-17 Dec. 2009,ieeexplore
10.1109/IT48810.2020.9070503,State Detection of Rotary Actuators Using Wavelet Transform and Neural Networks,IEEE,Conferences,"Rotary actuators are among the most commonly used machines in the industry and the algorithm for detecting the level of wear they are subjected to can prevent significant amount of unnecessary maintenance expenses. This paper proposes a new algorithm which can detect the state of the rotating machine using acoustic signals recorded in its vicinity. The algorithm uses a combination of wavelet transform and neural networks and is computationally inexpensive, so it can be implemented on a simple microcontroller. The testing has been done on real acoustic signals recorded in thermal power plant Kostolac in Serbia.",https://ieeexplore.ieee.org/document/9070503/,2020 24th International Conference on Information Technology (IT),18-22 Feb. 2020,ieeexplore
10.1109/ICRITO48877.2020.9198036,State of Art: Energy Efficient Protocols for Self-Powered Wireless Sensor Network in IIoT to Support Industry 4.0,IEEE,Conferences,"Up gradation of manufacturing systems in industries by means of implementing innovative manufacturing techniques that captures real time data, applies machine learning algorithms, makes entire system self-decisive and provides inter connectivity to the whole system is the prime focus of the Industry 4.0. It is aimed at bringing new industrial revolution with the help of internet of things technology due to its considerable influence in the industrial manufacturing process. Though impact of internet of things in industrial sector is huge, a practical implementation incorporates challenges in energy efficiency, self-powered sensor nodes and security. For processing of gathered data self-powered sensor nodes may sinks energy from ambient energy sources. A considerable amount of efforts has been put by researchers to address the challenges for development of energy efficient routing protocol for such nodes. The presented survey is aimed at analyzing the protocols for contribution towards the goals of industry 4.0 Energy efficient protocol will support the system to consume least energy for its operation. Henceforth combination of self-powered wireless sensor network and energy efficient protocol will be useful to power up many industrial IoT applications.",https://ieeexplore.ieee.org/document/9198036/,"2020 8th International Conference on Reliability, Infocom Technologies and Optimization (Trends and Future Directions) (ICRITO)",4-5 June 2020,ieeexplore
10.1109/IPDPS.2019.00113,Stochastic Gradient Descent on Modern Hardware: Multi-core CPU or GPU? Synchronous or Asynchronous?,IEEE,Conferences,"There is an increased interest in building data analytics frameworks with advanced algebraic capabilities both in industry and academia. Many of these frameworks, e.g., TensorFlow, implement their compute-intensive primitives in two flavors-as multi-thread routines for multi-core CPUs and as highly-parallel kernels executed on GPU. Stochastic gradient descent (SGD) is the most popular optimization method for model training implemented extensively on modern data analytics platforms. While the data-intensive properties of SGD are well-known, there is an intense debate on which of the many SGD variants is better in practice. In this paper, we perform a comprehensive experimental study of parallel SGD for training machine learning models. We consider the impact of three factors - computing architecture (multi-core CPU or GPU), synchronous or asynchronous model updates, and data sparsity - on three measures-hardware efficiency, statistical efficiency, and time to convergence. We draw several interesting findings from our experiments with logistic regression (LR), support vector machines (SVM), and deep neural nets (MLP) on five real datasets. As expected, GPU always outperforms parallel CPU for synchronous SGD. The gap is, however, only 2-5X for simple models, and below 7X even for fully-connected deep nets. For asynchronous SGD, CPU is undoubtedly the optimal solution, outperforming GPU in time to convergence even when the GPU has a speedup of 10X or more. The choice between synchronous GPU and asynchronous CPU is not straightforward and depends on the task and the characteristics of the data. Thus, CPU should not be easily discarded for machine learning workloads. We hope that our insights provide a useful guide for applying parallel SGD in practice and - more importantly - choosing the appropriate computing architecture",https://ieeexplore.ieee.org/document/8820776/,2019 IEEE International Parallel and Distributed Processing Symposium (IPDPS),20-24 May 2019,ieeexplore
10.1109/ITSC.2019.8917416,Strengthening the Case for a Bayesian Approach to Car-following Model Calibration and Validation using Probabilistic Programming,IEEE,Conferences,"Compute and memory constraints have historically prevented traffic simulation software users from fully utilizing the predictive models underlying them. When calibrating car-following models, particularly, accommodations have included 1) using sensitivity analysis to limit the number of parameters to be calibrated, and 2) identifying only one set of parameter values using data collected from multiple car-following instances across multiple drivers. Shortcuts are further motivated by insufficient data set sizes, for which a driver may have too few instances to fully account for the variation in their driving behavior. In this paper, we demonstrate that recent technological advances can enable transportation researchers and engineers to overcome these constraints and produce calibration results that 1) outperform industry standard approaches, and 2) allow for a unique set of parameters to be estimated for each driver in a data set, even given a small amount of data. We propose a novel calibration procedure for car-following models based on Bayesian machine learning and probabilistic programming, and apply it to real-world data from a naturalistic driving study. We also discuss how this combination of mathematical and software tools can offer additional benefits such as more informative model validation and the incorporation of true-to-data uncertainty into simulation traces.",https://ieeexplore.ieee.org/document/8917416/,2019 IEEE Intelligent Transportation Systems Conference (ITSC),27-30 Oct. 2019,ieeexplore
10.1109/WiCom.2008.1203,Study on the Mobile Water Conservancy Service Platform Based on Ontology,IEEE,Conferences,"Traditional application water conservancy software takes the full use of modern information technology and resources as a premise, some researches have been achieved on technical problems, such as collection, transmission and processing of water conservancy information. But there are problems as single mode and rough real-time of information issue, inflexibility and weak extension ability in software design. Based on the ontology theory, framework, component, WAP, Portal, we developed a mobile water conservancy service platform, constructed mobile applications of flood control on the platform, improved the situation of pattern, efficiency of information issue, and reusability of software. Platform provides reliable sustain of information collection, processing in events of water conservancy emergency and disasters, the application prospect and practical value of mobile service platform in water conservancy industry.",https://ieeexplore.ieee.org/document/4679111/,"2008 4th International Conference on Wireless Communications, Networking and Mobile Computing",12-14 Oct. 2008,ieeexplore
10.1109/ICE/ITMC49519.2020.9198430,Supporting SMEs in the Lake Constance Region in the Implementation of Cyber-Physical-Systems: Framework and Demonstrator,IEEE,Conferences,"With the emergence of the recent Industry 4.0 movement, data integration is now also being driven along the production line, made possible primarily by the use of established concepts of intelligent supply chains, such as the digital avatars. Digital avatars - sometimes also called Digital Twins or more broadly Cyber-Physical Systems (CPS) - are already successfully used in holistic systems for intelligent transport ecosystems, similar to the use of Big Data and artificial intelligence technologies interwoven with modern production and supply chains. The goal of this paper is to describe how data from interwoven, autonomous and intelligent supply chains can be integrated into the diverse data ecosystems of the Industry 4.0, influenced by a multitude of data exchange formats and varied data schemas. In this paper, we describe how a framework for supporting SMEs was established in the Lake Constance region and describe a demonstrator sprung from the framework. The demonstrator project's goal is to exhibit and compare two different approaches towards optimisation of manufacturing lines. The first approach is based upon static optimisation of production demand, i.e. exact or heuristic algorithms are used to plan and optimise the assignment of orders to individual machines. In the second scenario, we use real-time situational awareness - implemented as digital avatar - to assign local intelligence to jobs and raw materials in order to compare the results to the traditional planning methods of scenario one. The results are generated using event-discrete simulation and are compared to common (heuristic) job scheduling algorithms.",https://ieeexplore.ieee.org/document/9198430/,"2020 IEEE International Conference on Engineering, Technology and Innovation (ICE/ITMC)",15-17 June 2020,ieeexplore
10.1109/ICCES45898.2019.9002132,Symptom Based Health Prediction using Data Mining,IEEE,Conferences,"The general day to day health of a person is vital for the efficient functioning of the human body. Taking certain prominent symptoms and their diseases to build a Machine learning model to predict common diseases based on real symptoms is the objective of this research. With the dataset of the most commonly exhibited diseases, we built a relation to predicting the possible disease based on the input of symptoms. The proposed model utilizes the capability of different Machine learning algorithms combined with text processing to achieve accurate prediction. Text processing has been implemented using Tokenization and, is combined with various algorithms to test the similarities and the outputs. In health industry, it provides several benefits such as pre-emptive detection of diseases, faster diagnosis, medical history for review of patients etc.",https://ieeexplore.ieee.org/document/9002132/,2019 International Conference on Communication and Electronics Systems (ICCES),17-19 July 2019,ieeexplore
10.1109/ICIST.2018.8426166,Table Transformation Rule Learner,IEEE,Conferences,"As we know, table data is a popular data form in industry and scientific research fields. However, sometimes the original table data could not meet updating requirements in real applications, so we need to convert them into required form. In this paper we propose an approach to learn the transformation rules that convert original table data to target form. Based on Inductive Logic Programming(ILP), we design a learning system called Table Transformation Rule Learner (TTRL). It uses specific predicates and background knowledge for this task to generate table transformation rules. We implement a unique heuristic function (HF) in TTRL to accelerate searching process for rule generation, and we use semi-supervised learning (SSL) in order to obtain more information especially from small set of sample data. We also address the problem like over-generalization which may occur when having only positive training examples in ILP learning process. We test our program in several kinds of table data, and the result shows that the transformation rules can be learned correctly. Moreover, our designed searching strategy can greatly reduce the time cost of searching rules.",https://ieeexplore.ieee.org/document/8426166/,2018 Eighth International Conference on Information Science and Technology (ICIST),30 June-6 July 2018,ieeexplore
10.1109/AIMS.2013.4,Table of contents,IEEE,Conferences,"The following topics are dealt with: artificial intelligence; neural networks and fuzzy systems; evolutionary computation; bioinformatics and bioengineering; data and semantic mining; games, VR and visualization; intelligent systems and applications; systems intelligence; control intelligence; e-science and e-systems; robotics, cybernetics, engineering, and manufacturing; operations research; discrete-event and real-time systems; image, speech and signal processing; industry, business, management, human factors and social issues; energy, power, transport, logistics, harbour, shipping and marine simulation; parallel, distributed, and software architectures and systems; mobile-ad hoc wireless networks, Mobicast, sensor placement, and target tracking; performance engineering of computer and communications systems; and circuits and devices.",https://ieeexplore.ieee.org/document/6959881/,"2013 1st International Conference on Artificial Intelligence, Modelling and Simulation",3-5 Dec. 2013,ieeexplore
10.1109/ICITCS.2016.7740300,Table of contents,IEEE,Conferences,The following topics are dealt with: power-aware data structure; memory allocation techniques; EPS-based motion recognition systems; secure distance bounding protocol; TH-UWB; cooperative spectrum sensing scheme; femtocell network; video quality; unequal loss protection; Wi-Fi based broadcasting system; example-based retrieval system; human motion data; astronaut virtual training system; layout familiarization training; stereo image correction; 3D optical microscope; UHF RFID; BLE; stereo-based tag association; medical image segmentation; sensitive adaptive thresholding; interactive event recognition; semantic video understanding; Bgslibrary algorithms; traffic surveillance video; CUDA-based acceleration techniques; image filtering; image-based ship detection; AR navigation; augmented reality; vehicle information; head-up display: LiDAR data; classifier performance; people detection; wavelet transform; max-min energy-efficiency optimization; wireless powered communication network; harvest-then-transmit protocol; ARM64bit Server; WeChat text messages service flow traffic classification; machine learning technique; load balancing; WSN; novel Markov decision process based routing algorithm; repulsion-propulsion firefly algorithm; sentence based mathematical problem solving approach; ontology modeling; sentiment analysis; HARN algorithm; real-time road surface condition determination algorithm; automatic weather system; kinematic constraint method; human gesture recognition; weighted dynamic time warping; IT demand governance; business goal structuring notation; software requirement specification; AOP-based approach;decision support system; proactive flood control; context-aware user interface field classification; common vocabulary set; maritime equipment; e-navigation services; genetic algorithm; strategic information systems planning; speech enhancement; ES information; phase-error based filters; holistic service orchestration; distributed micro data center; hierarchical cluster network; wellness sports industry; secure agent based architecture; resource allocation; cloud computing; distributed multi-platform context-aware user interface; parallel prime number labeling; XML data; MapReduce; metadata extension; data presentations; aspect-oriented user interfaces design integration; Angular 2 framework; energy impact; Web user interface technology; mobile devices; JIT compilation-based unified SQL query optimization system; partial materialization; data integration; SQL-on-Hadoop engines; z-transform based encryption algorithm; FARIS; fast and memory-efficient URL filter; domain specific machine; synchronized blind audio watermarking; multilevel DWT; windowed vector modulation; packet length covert channel capacity estimation; flexible authentication protocol; WBAN; SMS-based mobile botnet detection module; full-duplex jamming attack; active eavesdropping; Big Data security analysis; complex security requirements patterns; SSH attacks; SSL/TLS nonintrusive proxy; JSON data; neural stegoclassifier; polymorphic malware detection; linguistic based steganography; lexical substitution; syntactical transformation; holistic-based feature extraction; error correcting code biometric template protection technique; network based IMSI catcher detection; Internet of Things environment; light-weight API-call safety checking; automotive control software; SmartDriver; project management software; model-based testing; exploratory testing; automated ECG beat classification system and convolutional neural networks.,https://ieeexplore.ieee.org/document/7740300/,2016 6th International Conference on IT Convergence and Security (ICITCS),26-26 Sept. 2016,ieeexplore
10.1109/ICSE-SEET.2019.00014,Teaching Internet of Things (IoT) Literacy: A Systems Engineering Approach,IEEE,Conferences,"The Internet of Things (IoT) invades our world with billions of smart, interconnected devices, all programmed to make our lives easier. For educators, teaching such a vast and dynamic field is both a necessity and a challenge. IoT-relevant topics such as programming, hardware, networking and artificial intelligence are already covered in core computing curricula. Does this mean that fresh graduates are well prepared to tackle complex IoT problems? Unfortunately, nothing could be further from the truth. The problem is that IoT devices are complex systems, where software, hardware, and humans interact with each other. From this interaction, unique behavior and hazardous situations can emerge that might easily stay undetected, unless systems are analyzed as a whole. This paper presents two differently flavored courses that teach IoT using a holistic, system-centric approach. The first is a broad introduction to Pervasive Computing, focused on the intelligence of ""Things"". The second is an advanced course that zooms on the process of testing a software-intensive system. The key characteristics of our approach are : (1) teaching only the bare essentials (topics needed for end-to-end engineering of a smart system), (2) a strong, hands-on project component, using microcontroller-based miniature systems, inspired by real-life, and (3) a rich partnership with industry and academic idea incubators. Positive student evaluations gathered during the last five years demonstrate that such an approach brings engagement, self-confidence and realism in IoT classrooms. We believe that this success can be replicated in other courses, by shifting the focus on different IoT-relevant aspects.",https://ieeexplore.ieee.org/document/8802112/,2019 IEEE/ACM 41st International Conference on Software Engineering: Software Engineering Education and Training (ICSE-SEET),25-31 May 2019,ieeexplore
10.1109/ICRTAC.2018.8679129,TensorFlow Based Website Click through Rate (CTR) Prediction Using Heat maps,IEEE,Conferences,"Web Heat Maps are used to identify the click patterns and activities visited by the users of the website. Using heat maps, one can make a manual decision based on the user's click activity. This paper proposes a framework using TensorFlow to identify and detect the users click activity in real time. Tensor Flow also suggest or take business decisions predicted through users clicks. This paper models Tensor Flow's machine learning library to take automated decisions like placement of suitable products, placement of advertisements and others based on the highest clicks recorded by the users. The results predict that the future businesses like e-commerce, fashion and retail industry can benefit more if this framework is deployed in such applications.",https://ieeexplore.ieee.org/document/8679129/,2018 International Conference on Recent Trends in Advance Computing (ICRTAC),10-11 Sept. 2018,ieeexplore
10.1109/ICSE-Companion.2019.00131,Testing Untestable Neural Machine Translation: An Industrial Case,IEEE,Conferences,"Neural Machine Translation (NMT) has shown great advantages and is becoming increasingly popular. However, in practice, NMT often produces unexpected translation failures in its translations. While reference-based black-box system testing has been a common practice for NMT quality assurance during development, an increasingly critical industrial practice, named in-vivo testing, exposes unseen types or instances of translation failures when real users are using a deployed industrial NMT system. To fill the gap of lacking test oracles for in-vivo testing of NMT systems, we propose a new methodology for automatically identifying translation failures without reference translations. Our evaluation conducted on real-world datasets shows that our methodology effectively detects several targeted types of translation failures. Our experiences on deploying our methodology in both production and development environments of WeChat (a messenger app with over one billion monthly active users) demonstrate high effectiveness of our methodology along with high industry impact.",https://ieeexplore.ieee.org/document/8802818/,2019 IEEE/ACM 41st International Conference on Software Engineering: Companion Proceedings (ICSE-Companion),25-31 May 2019,ieeexplore
10.1109/SNPDWinter52325.2021.00068,The Case Study on Use of Bigdata and AI in Distribution Industry,IEEE,Conferences,"With the development of telecommunication technology, a commerce platform centered on mobile devices is developing. Global mobile commerce companies such as Amazon are trying to recommend optimized products to customers and provide optimized logistics services through real-time big data. As online-oriented commerce develops, rival local offline stores are making changes for survival. In Korea, the government is trying to find a balance with offline stores by controlling big data-oriented online commerce, as is the regulation that sought to protect the local market by limiting the business hours of Super Supermarket (SSM) to balance local offline stores and Supermarket. However, in the data-driven fourth industrial revolution, we will have to find ways to develop by utilizing data, which is the main raw material. This paper tried to investigate cases of applying and operating big data in the distribution industry and seek ways to promote focused on big-data sharing that can develop with offline stores by conducting a Delphi survey to experts.",https://ieeexplore.ieee.org/document/9403518/,"2021 21st ACIS International Winter Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing (SNPD-Winter)",28-30 Jan. 2021,ieeexplore
10.1109/MLISE54096.2021.00090,The Design and Implementation of Beidou Ground-based Augmentation System in Natural Resources Industry,IEEE,Conferences,"The Beidou Ground-based Augmentation System is an important part of the Beidou Navigation Satellite System (BDS). It can provide real-time centimeter-level positioning services, effectively solving the problem of low positioning accuracy and poor real-time performance of BDS in natural resource industry. This issue introduces the design and implementation of Beidou Ground-based Augmentation System in natural resources industry, and verifies the system's performance in the practical applications of mine law-enforcement monitoring.",https://ieeexplore.ieee.org/document/9611663/,2021 International Conference on Machine Learning and Intelligent Systems Engineering (MLISE),9-11 July 2021,ieeexplore
10.1109/ICCSE51940.2021.9569551,The Integration Development of Artificial Intelligence and Education,IEEE,Conferences,"With the rapid progress and development of modern information science and technology, artificial intelligence technology has become more and more extensive in many fields. How to incorporate artificial intelligence into education has become a hot topic of the whole society. In this paper, analysis of artificial intelligence used to extract application potential and value of intelligent correction, real-time monitoring, education fairness and campus safety. But there are also challenges in personality education, safety ethics, teaching efficiency, etc. In order to make artificial intelligence better serve the education industry, it is necessary to increase the infrastructure construction and environment configuration of artificial intelligence equipment. And then improving the education practitioners’ awareness and correct cognition of the relationship between intelligent machine safety ethics and artificial intelligence.",https://ieeexplore.ieee.org/document/9569551/,2021 16th International Conference on Computer Science & Education (ICCSE),17-21 Aug. 2021,ieeexplore
10.1109/ICC40277.2020.9148964,The Scalability Analysis of Machine Learning Based Models in Road Traffic Flow Prediction,IEEE,Conferences,"Nowadays, traffic flow prediction, as a vital part of the Intelligent Transportation System (ITS), has attracted considerable attention from both academia and industry. Many prediction methods have been proposed and can be categorized into parametric methods and non-parametric methods. Nonparametric methods, especially Machine Learning (ML)-based methods, compared to parametric methods, need less prior knowledge about the relationship among different traffic patterns and can better fit the non-linear features of traffic data. However, we notice that, due to the complex structure, ML-models require a higher cost of implementation regarding time consumption of training and predicting. Therefore, in this paper, we evaluate not only the accuracy but also the efficiency and scalability of some state-of-the-art ML-models, which is the key to apply a prediction model into the real world. Furthermore, we design an off-line optimization method, Desensitization, to improve the scalability of a given model.",https://ieeexplore.ieee.org/document/9148964/,ICC 2020 - 2020 IEEE International Conference on Communications (ICC),7-11 June 2020,ieeexplore
10.1109/MIPRO.2016.7522210,The challenge of cellular cooperative ITS services based on 5G communications technology,IEEE,Conferences,"We live at a time when the automotive industry is going through a technological revolution with the development of vehicles changing into autonomous moving objects having the properties of artificial intelligence. Additionally, cellular communications networks introduce new technologies and concepts: SDN (Software-defined networking), NFV (Network Functions Virtualization). These advanced software-defined communications networks virtualize network functions, allowing a new way of configuration, control and management. Increasing the speed and automation are key requirements to support the most demanding services such as mobile payment of contextual services, as well as the introduction of new “Machine” users. SDN also assists in the implementation of new infrastructure for the dynamic services that are based on the concepts of IoT, Big Data and Everything-as-a-Service. Using NFV enables the Internet of Things services that provide a new way of connecting people, processes, data and devices. It is precisely thes5G communications networkse requirements that are essential for the introduction of C-ITS systems, i.e., the integration of passengers, drivers, vehicles and transport infrastructure, as well as information, statistics, predictive traffic analytics, all this in real-time. Due to this trend of development of cellular communications networks in the next 5G communications networks, automotive and ITS traffic systems are becoming the most important market for business expansion of telecom operators. Such revolutionary technological changes, together with the integration of various industries entails a series of challenges. The aim of this paper is to define important information, challenges and opportunities for the telecom industry to provide mobility for people and goods.",https://ieeexplore.ieee.org/document/7522210/,"2016 39th International Convention on Information and Communication Technology, Electronics and Microelectronics (MIPRO)",30 May-3 June 2016,ieeexplore
10.1109/IMTC.2001.928200,The development of an artificial neural network embedded automated inspection quality management system,IEEE,Conferences,This paper describes in detail the development of an innovative artificial neural network embedded automated inspection scheme for the manufacturing industry employing digital image processing techniques. Such a system is capable of performing real-time image processing tasks and identifies the size and location of the finished components on manufactured products as well as the flaws and scratches on surface of products during the manufacturing process. The proposed artificial neural network embedded quality management system provides a user-friendly user interface that has been implemented and tested on a case study from a printed circuit board manufacture. The experimental results have demonstrated the functionality and superiority of the developed artificial neural network embedded inspection system.,https://ieeexplore.ieee.org/document/928200/,IMTC 2001. Proceedings of the 18th IEEE Instrumentation and Measurement Technology Conference. Rediscovering Measurement in the Age of Informatics (Cat. No.01CH 37188),21-23 May 2001,ieeexplore
10.1109/PICMET.2015.7273077,The effects of disruption on different types of tile manufacturing Industry-layouts: An empirical investigation on tile manufacturing industry,IEEE,Conferences,"Almost all manufacturing facilities need to use production scheduling systems to increase productivity and reduce production costs. Most manufacturing industries invest huge amount of money to manufacture and supply products on time in order to meet customers demand and objectives but due to unforeseen disruptions, these objectives are difficult to achieve. In Real-life, production operations are subject to a large number of unexpected disruptions that may invalidate an original schedule. This work considered effects of disruption on different types of industry-layouts that are seldom been used in manufacturing industries namely: fixed, product, production and process industry-layouts. Questionnaires were used for data collection from a number of companies in Cameroon and reliably theory, simulation software's were using to analyze the data. The study reveals that, disruptions varies per industry-layout and the leading sources of disruptions are machine breakdown, power failure, employee's absenteeism and material shortage. It is concluded that disruption of one type may not greatly affect productivity of a certain industry-layout whilst similar disruptions can have devastating effects on another type and also, the impacts of disruption are dependent on the Industry-layouts.",https://ieeexplore.ieee.org/document/7273077/,2015 Portland International Conference on Management of Engineering and Technology (PICMET),2-6 Aug. 2015,ieeexplore
10.1109/CCDC.2008.4598242,The input-output connection analysis of industry structure based on complex network theory,IEEE,Conferences,"In order to deeply reveal the structure characters of the real weighted complex networks, the clustering and correlation measures are given and compared from the view of topological and weighted network respectively. The structure of Chinese national economics in 2002 is set as an example to illustrate. The complex network model of national economic industry departments is established, the clustering and correlation characters of this industry relation network are analyzed by the software tool MATLAB with the data coming from the input-output data published by the national economic accounting department, therefore the input-output association structure characters of the national economic departments in our country are sufficiently explored.",https://ieeexplore.ieee.org/document/4598242/,2008 Chinese Control and Decision Conference,2-4 July 2008,ieeexplore
10.1109/AIAWS.1991.236599,The strategic significance of expert systems,IEEE,Conferences,"Expert or knowledge-based systems are being deployed in virtually every industry because of their power to bring strategic competitive advantage to the firms who successfully employ them. The strategic significance of expert systems lies in their ability to improve the effectiveness and efficiency of the decision-making process and, in the long run, improved decision-making leads to growth and increased profitability. This is especially true of organizations, such as brokerage firms, whose 'life's blood' is fruitful, consistent, and reliable advice. Expert system technology is advancing at a great pace as new methods for improved reasoning and automated learning are emerging from research to reality. If firms in the business of offering advice do not continually upgrade their expert systems with the latest conceptual tools, they will find it difficult to grow, even survive, in the 'knowledge age'.&lt;<ETX>&gt;</ETX>",https://ieeexplore.ieee.org/document/236599/,Proceedings First International Conference on Artificial Intelligence Applications on Wall Street,9-11 Oct. 1991,ieeexplore
10.1109/ICCCIS51004.2021.9397068,To Detect the Distributed Denial-of-Service Attacks in SDN using Machine Learning Algorithms,IEEE,Conferences,"The reason for Software Defined Network (SDN) to gain importance in both the academics and industry as a new emerging way of network management, is its architecture which decouples the data plane (forwarding devices) and the control plane (controller) making it possible to upgrade and update into newer versions. SDN is a new or novel way of `programmable networks' which has led to growth of innovative technologies and scenarios in terms of network virtualization, flexibility, enhanced growth control, dynamic network policy, reduced operational cost. Despite, these advantages; it is also one of main reasons for cyber threats. Amongst them the most vulnerable is the DDoS attacks. DDoS attack in SDN is quite a threat to the security in SDN network. It attacks at the network layer or application layer of the infrastructure. It can cause problems as simple as inability to refresh a particular page to as severe as failure of an entire server. In this paper, DDoS is taken into consideration with SDN and proposed a IDS which studied for detection of the attackers in the real time incoming traffic. Machine Learning algorithms such as Naïve Bayes, KNN, K-Means clustering, and Linear Regression are used to form the module 1 of the IDS (the Signature IDS) and Module 2 form for uses three way handshake to identify the exact host which is an intruder. On finding the intruder it is being placed in the Access Control List (ACL). Also, analysis of efficiency of different machine learning algorithm is performed to understand the effectiveness.",https://ieeexplore.ieee.org/document/9397068/,"2021 International Conference on Computing, Communication, and Intelligent Systems (ICCCIS)",19-20 Feb. 2021,ieeexplore
10.1109/ISIC.2007.4450938,Toward A Practical Multi-agent System for Integrated Control and Asset Management of Petroleum Production Facilities,IEEE,Conferences,"This paper addresses a practical intelligent multi-agent system for asset management for the petroleum industry, which is crucial for profitable oil and gas facilities operations and maintenance. A research project was initiated to study the feasibility of an intelligent asset management system. Having proposed a conceptual model, architecture, and implementation plan for such a system in previous work [1], [2], [3], we define the autonomy, communications, and artificial intelligence (AI) requirements of the component agents of such a system. We also discuss the software implementation of such agents. Furthermore, we describe a simple system prototype, and conduct a real time simulation experiment to analyze the prototype performance. Simulation results reveal that MATLAB can be used to build high performance real-time multi-agent systems, which can be used for many applications.",https://ieeexplore.ieee.org/document/4450938/,2007 IEEE 22nd International Symposium on Intelligent Control,1-3 Oct. 2007,ieeexplore
10.1109/ICSESS.2016.7883200,Toward an intelligent solution for perishable food cold chain management,IEEE,Conferences,"With the continuous development of Internet of Things (IoT), cloud computing, and artificial intelligence, many cold chain logistics providers have updated their IT solutions to enhance the quality control of perishable food products in the cold chain industry. Based on RFID, WSN, GPS, and cloud computing, a typical cloud-based IoT platform empowers a logistics company to monitor the real-time status of supervised food products and give decision support intelligently in a cost-effective way. Furthermore, complex problems such as load planning and route planning can be solved by using the artificial intelligence. Therefore, in this paper we attempt to propose and present an intelligent solution from the perspective of key enabling technologies and system framework.",https://ieeexplore.ieee.org/document/7883200/,2016 7th IEEE International Conference on Software Engineering and Service Science (ICSESS),26-28 Aug. 2016,ieeexplore
10.1109/MELCON.2018.8379089,Toward energy saving and environmental protection by implementation of autonomous ship,IEEE,Conferences,"The energy saving and environmental protection remain the main pillars of the maritime industry's sustainability enhancement. This sustainability will become a reality by implementation of autonomous ship (AS), which is considered as an alternative and may carry the potential to increase the profitability of shipping company. The elimination of crew, the introduction of innovative technology and adoption of new ship design on board of this kind of ship will participate positively in energy saving and environmental protection. For an in-situ study, a hundred of conventional ships (CS) have been visited, their energy consumption and environmental impact are assessed taking into consideration the trading area, navigation scenarios, power capacity and number of crew members. In this paper, we assess and quantify the positive impact on energy saving and environmental pollution prevention resulting from the implementation of AS. For this the facilities related to crew living are enumerated and the impact of their elimination on energy saving and environmental pollution prevention is presented. Other ship design concepts are proposed to enhance the energy-saving and environmental protection are proposed. A benchmarking of CS and AS in term of energy saving and environmental pollution prevention is presented.",https://ieeexplore.ieee.org/document/8379089/,2018 19th IEEE Mediterranean Electrotechnical Conference (MELECON),2-7 May 2018,ieeexplore
10.1145/3377811.3380368,Towards Characterizing Adversarial Defects of Deep Learning Software from the Lens of Uncertainty,IEEE,Conferences,"Over the past decade, deep learning (DL) has been successfully applied to many industrial domain-specific tasks. However, the current state-of-the-art DL software still suffers from quality issues, which raises great concern especially in the context of safety- and security-critical scenarios. Adversarial examples (AEs) represent a typical and important type of defects needed to be urgently addressed, on which a DL software makes incorrect decisions. Such defects occur through either intentional attack or physical-world noise perceived by input sensors, potentially hindering further industry deployment. The intrinsic uncertainty nature of deep learning decisions can be a fundamental reason for its incorrect behavior. Although some testing, adversarial attack and defense techniques have been recently proposed, it still lacks a systematic study to uncover the relationship between AEs and DL uncertainty. In this paper, we conduct a large-scale study towards bridging this gap. We first investigate the capability of multiple uncertainty metrics in differentiating benign examples (BEs) and AEs, which enables to characterize the uncertainty patterns of input data. Then, we identify and categorize the uncertainty patterns of BEs and AEs, and find that while BEs and AEs generated by existing methods do follow common uncertainty patterns, some other uncertainty patterns are largely missed. Based on this, we propose an automated testing technique to generate multiple types of uncommon AEs and BEs that are largely missed by existing techniques. Our further evaluation reveals that the uncommon data generated by ourmethod is hard to be defended by the existing defense techniques with the average defense success rate reduced by 35%. Our results call for attention and necessity to generate more diverse data for evaluating quality assurance solutions of DL software.",https://ieeexplore.ieee.org/document/9284139/,2020 IEEE/ACM 42nd International Conference on Software Engineering (ICSE),5-11 Oct. 2020,ieeexplore
10.1109/ETFA46521.2020.9212097,Towards Real-time Process Monitoring and Machine Learning for Manufacturing Composite Structures,IEEE,Conferences,"Components made from carbon fiber reinforced plastics (CFRP) offer attractive stability properties for the automotive or aerospace industry despite their light weight. To automate CFRP production, resin transfer molding (RTM) based on thermoset plastics is commonly applied. However, this manufacturing process has its shortcomings in quality and costs. The project CosiMo aims for a highly automated and cost-attractive manufacturing process using cheaper thermoplastic materials. In a thermoplastic RTM (T-RTM) process, the polymerization of ε-caprolactam to polyamide 6 is investigated using an intelligent mold tooling. Multiple sensor types integrated into the mold allow for tracking of process-relevant variables, such as material flow and polymerization state. In addition to monitoring the T-RTM process, a digital twin visualizes progress and makes predictions about issues and countermeasures based on machine learning.",https://ieeexplore.ieee.org/document/9212097/,2020 25th IEEE International Conference on Emerging Technologies and Factory Automation (ETFA),8-11 Sept. 2020,ieeexplore
10.1109/GreenTech46478.2020.9289758,"Towards Smart e-Infrastructures, A Community Driven Approach Based on Real Datasets",IEEE,Conferences,"e-Infrastructures have powered the successful penetration of e-services across domains, and form the backbone of the modern computing landscape. e-Infrastructure is a broad term used for large, medium and small scale computing environments. The increasing sophistication and complexity of applications have led to even small-scale data centers consisting of thousands of interconnects. However, efficient utilization of resources in data centers remains a challenging task, mainly due to the complexity of managing physical nodes, network equipment, cooling systems, electricity, etc. This results in a very strong carbon footprint of this industry. In recent years, efforts based on machine learning approaches have shown promising results towards reducing energy consumption of data centers. Yet, practical solutions that can help data center operators in offering energy efficient services are lacking. This problem is more visible in the context of medium and small scale data center operators (the long tail of e-infrastructure providers). Additionally, a disconnect between solution providers (machine learning experts) and data center operators has been observed. This article presents a community-driven open source software framework that allows community members to develop better understanding of various aspects of resource utilization. The framework leverages machine learning models for forecasting and optimizing various parameters of data center operations, enabling improved efficiency, quality of service and lower energy consumption. Also, the proposed framework does not require datasets to be shared, which alleviates the extra effort of organizing, describing and anonymizing data in an appropriate format.",https://ieeexplore.ieee.org/document/9289758/,2020 IEEE Green Technologies Conference(GreenTech),1-3 April 2020,ieeexplore
10.1109/ICTAI.2009.106,Towards a Technology Platform for Building Corporate Radar Applications that Mine the Web for Business Insight,IEEE,Conferences,"In this paper, we give a progress report on an ongoing effort at Accenture to develop a technology platform for building a wide range of corporate radar applications,which can turn the Web into a systematic source of business insight. Our goal is to share the platform we have developed and the lessons we have learned, so others can leverage this knowledge when building similar applications. We give an overview of this platform, which integrates a combination of established AI technologies - i.e. semantic models, natural language processing, and inference engines - in a novel way. We then illustrate the kinds of corporate radars that can be built with our platform through two applications we developed at Accenture: the technology lifecycle tracker, which assesses the maturity of technologies from the wireless industry, and the technology trend tracker, which measures hype versus reality for emerging technology trends such as cloud computing, software-as-a-service, and more. Finally, we discuss our experiences in using this platform to build these applications and the lessons learned.",https://ieeexplore.ieee.org/document/5363824/,2009 21st IEEE International Conference on Tools with Artificial Intelligence,2-4 Nov. 2009,ieeexplore
10.1109/ICIT.2016.7474875,Towards a fully automated 3D printability checker,IEEE,Conferences,"3D printing has become one of the most popular evolutionary techniques with diverse application, even as normal people's hobby. As printing target, enormous 3D virtual models from game industry and virtual reality flood the internet, shared in various online forums, such as thingiverse. But which of them can really be printed? In this paper we propose the 3D Printability Checker, which can be used to automatically answer this non-trivial question. One of the major novelties of this paper is the process of dependable software engineering we use to build this Printability Checker. Firstly, we prove that this question is decidable with a given 3D object and a list of printer profiles. Secondly, we design and implement such a checker. Finally, we show our experimental results and use them further for a machine learning approach to improve our system in an automatic way. The generic framework provides a useful basis of automatic self-improvement of the software by combining current techniques in the area of formal method, geometry modelling and machine learning.",https://ieeexplore.ieee.org/document/7474875/,2016 IEEE International Conference on Industrial Technology (ICIT),14-17 March 2016,ieeexplore
10.1109/ISEF.2017.8090700,Towards an IOT-enabled intelligent energy management system,IEEE,Conferences,"Vehicles and billion other physical devices are today enriched with electronics, sensors and software that enable data collection and exchange. At the same time, Internet-of-Things initiates a new era where the world will change deeply and decisively in many ways. In this context, both industry and research community attempts to merge engineering and artificial intelligence. This direction considers objects, vehicles, devices and even buildings, as the driving force for autonomous IoT that enables intelligent management for crucial issues. To this end, this article proposes a novel approach that combines theoretical and scientific knowledge, related to IoT and Artificial Intelligence to real-world needs as they are reported by an engineering perspective. The article presents the first steps towards a theoretical model that will lead to an intelligent platform which will enable efficient energy management in real world paradigms, moving from simulations to real-life applications.",https://ieeexplore.ieee.org/document/8090700/,"2017 18th International Symposium on Electromagnetic Fields in Mechatronics, Electrical and Electronic Engineering (ISEF) Book of Abstracts",14-16 Sept. 2017,ieeexplore
10.1109/JCSSE53117.2021.9493822,Towards robust Machine Learning models for grape ripeness assessment,IEEE,Conferences,"Artificial intelligence methods need to be more transparent for wider acceptance by the industry. In particular deep neural networks (DNN) are not explainable, due to the complex processes the input undergo. The present work addresses model explainability for wine grapes quality assessment through 1D-CNN, using regression activation maps (RAM) to show the contribution score of each wavelength for the prediction of sugar content. This way we identify the relevant regions related to this enological parameter. The results obtained indicate that the proposed approach can successfully highlight important spectral regions related to sugars absorption, improving the current state of the art, and opening way to dimensionality reduction methods and further model interpretation.",https://ieeexplore.ieee.org/document/9493822/,2021 18th International Joint Conference on Computer Science and Software Engineering (JCSSE),30 June-2 July 2021,ieeexplore
10.1109/AIRE.2014.6894851,Transferring research into the real world: How to improve RE with AI in the automotive industry,IEEE,Conferences,"For specifications, people use natural language. We show that processing natural language and combining this with intelligent deduction and reasoning with ontologies can possibly replace some manual processes associated with requirements engineering (RE). Our prior research shows that the software tools we developed can indeed solve problems in the RE process. This paper shows this does not only work in the software engineering domain, but also for embedded software in the automotive industry. We use artificial intelligence in the sense of combining semantic knowledge from ontologies and natural language processing. This enables computer systems to “understand” requirement texts and process these with “common sense”. Our specification improver RESI detects flaws in texts such as ambiguous words, incomplete process words, and erroneous quantifiers and determiners.",https://ieeexplore.ieee.org/document/6894851/,2014 IEEE 1st International Workshop on Artificial Intelligence for Requirements Engineering (AIRE),26-26 Aug. 2014,ieeexplore
10.1109/TDC.2002.1177707,Transformer monitoring moving forward from monitoring to diagnostics,IEEE,Conferences,"The technologies employed for monitoring transformers have been evolving over the last 10 or more years to the point where they are now commonly accepted, and have been demonstrated to provide useful data on the key parameters and components of critical power transformers. A major aspect of these technologies has been the accumulation of copious amounts of data, and the associated problem, of what to do with it all. With time and resources in short supply to do a proper analysis of this data, to turn it into useful transformer information, there needs to be a new set of technologies and techniques implemented. The advent of new methods of data modeling and interpretation using statistical analysis, rules based, and artificial intelligence systems is now moving from the research stage to practical field implementation. The industry has a very real need to move from ""just monitoring"" equipment to the point of being able to have the knowledge of the operating condition of the equipment and when things begin to go wrong, diagnose the problem to provide a recommended course of action.",https://ieeexplore.ieee.org/document/1177707/,IEEE/PES Transmission and Distribution Conference and Exhibition,6-10 Oct. 2002,ieeexplore
10.1109/TDC.2001.971373,Transformer monitoring-moving forward from monitoring to diagnostics,IEEE,Conferences,"The technologies employed for monitoring transformers have been evolving over the last 10 or more years to the point where they are now commonly accepted, and have been demonstrated to provide useful data on the key parameters and components of critical power transformers. A major aspect of these technologies has been the accumulation of copious amounts of data, and the associated problem, of what to do with it all. With time and resources in short supply to do a proper analysis of this data, to turn it into useful transformer information, there needs to be a new set of technologies and techniques implemented. The advent of new methods of data modeling and interpretation using statistical analysis, rule based, and artificial intelligence systems is now moving from the research stage to practical field implementation. The industry has a very real need to move from ""just monitoring"" equipment to the point of being able to have the knowledge of the operating condition of the equipment and when things begin to go wrong, diagnose the problem to provide a recommended course of action.",https://ieeexplore.ieee.org/document/971373/,2001 IEEE/PES Transmission and Distribution Conference and Exposition. Developing New Perspectives (Cat. No.01CH37294),2-2 Nov. 2001,ieeexplore
10.1109/ICCAIRO.2017.61,Tuning Software Based on Genetic Algorithm Applied to Industrial PID Loops,IEEE,Conferences,"Time-delay processes are frequently found in industry and the most common representations are first order plus delay time (FOPDT) and integrator plus delay time (IPDT) transfer functions. The identification of time delay systems is a challenging task and usually, the performance of its control is not optimal. This work presents a software based on a real coded genetic algorithm to identify the system, using open or close loop information, and to tune the PID controller using several methods. Results on simulation and real industrial loops are presented.",https://ieeexplore.ieee.org/document/8253004/,"2017 International Conference on Control, Artificial Intelligence, Robotics & Optimization (ICCAIRO)",20-22 May 2017,ieeexplore
10.1109/ISMAR.2015.73,Tutorial 4: AR Implementations in Informal Learning,IEEE,Conferences,"A variety of cases uses of AR in informal learning environments. The cases uses are drawn from a variety of different contexts. There will be examples of AR use in education, tourism, event organizing, and others. This is mainly geared to people creating learning environments in any industry a foundation to start implementation AR. The featured case use will be how AR was used at TEDxKyoto to engage participants. There will also be several student projects that use AR presented and available for demo.",https://ieeexplore.ieee.org/document/7328050/,2015 IEEE International Symposium on Mixed and Augmented Reality,29 Sept.-3 Oct. 2015,ieeexplore
10.1109/CICA.2009.4982774,Tutorial CICA-T Computing with intelligence for identification and control of nonlinear systems,IEEE,Conferences,"System characterization and identification are fundamental problems in systems theory and play a major role in the design of controllers. System identification and nonlinear control has been proposed and implemented using intelligent systems such as neural networks, fuzzy logic, reinforcement learning, artificial immune system and many others using inverse models, direct/indirect adaptive, or cloning a linear controller. Adaptive Critic Designs (ACDs) are neural networks capable of optimization over time under conditions of noise and uncertainty. The ACD technique develops optimal control laws using two networks - critic and action. There are merits for each approach adopted will be presented. The primary aim of this tutorial is to provide control and system engineers/researchers from industry/academia, new to the field of computational intelligence with the fundamentals required to benefit from and contribute to the rapidly growing field of computational intelligence and its real world applications, including identification and control of power and energy systems, unmanned vehicle navigation, signal and image processing, and evolvable and adaptive hardware systems.",https://ieeexplore.ieee.org/document/4982774/,2009 IEEE Symposium on Computational Intelligence in Control and Automation,30 March-2 April 2009,ieeexplore
10.1109/TAAI.2015.7407137,Tutorial IV computational intelligence for data analytics,IEEE,Conferences,"Humankind has been collecting data since the recording started, but in the last decade with the considerable advances in computing and storage technologies, advancements of cloud computing, development of ubiquitous connectivity and the internet of things, there has been explosion in the size and variety of collected data. Nevertheless, one can be data-rich and knowledge-poor, and this is where the data analytics and the development and application of machine learning models become necessity for gaining insight of complex processes to prove scientific theories and discoveries, support decision making and enhance strategic planning in different areas of the economy, finance, industry, healthcare, etc. Recently, there is an influx of polymorphic, unstructured and multimodal data - social media, images, audio, video, etc., which is complicating further the data processing and knowledge extraction process. But even the traditional structured datasets present problems that need to be addressed and overcome in the early stages of data pre-processing, feature extraction and feature selection. This is because they usually contain variety of data formats, e.g., categorical, continuous, ordinal, and frequently missing data (usually result of sensors faults, human errors, collection, transportation, or storage problems). The most popular approaches in dealing with missing data generally fall in three groups: Deletion methods; Single imputation methods; and Model-based methods. In this tutorial I will talk about the third group methods, which are considered to be the most popular, 'modem' model-based approaches. Particularly, Multiple imputation (MI) method will be introduced and discussed in addition to the K-Nearest Neighbour Imputation (KNN-I) and Bagged Tree Imputation (BTI). Subsequently, MI, KNN-I and BTl will be applied in a case study for pre-processing a real world radar signal large dataset (more than 30 000 samples). The dataset comprises intercepted and collected pulse train characteristics, which typically include signal frequencies, type of modulation, scan period, pulse repetition intervals, etc., and usually consist of mixture of continuous, discrete and categorical data, and also frequently include missing values. Missing values are imminent part of real world datasets and radar datasets make no exception of that. Then will briefly talk about supervised and unsupervised learning and the use of three supervised approaches: Neural Networks (NN); Random Forests (RF); and Support Vector Machines (SVM) for solving radar signal classification and source identification problem. Results from applying the NN, RF and SVM (using R and Matlab) on complete data subset (without missing data) and the full dataset with substituted (up to 60%) missing data with MI, KNN-I and BTl will be critically analysed and discussed. Finally, I'll talk about the opportunities and challenges in applying computational intelligence and machine learning techniques to Big Data and the available software for Big Data.",https://ieeexplore.ieee.org/document/7407137/,2015 Conference on Technologies and Applications of Artificial Intelligence (TAAI),20-22 Nov. 2015,ieeexplore
10.1109/VTCFall.2018.8690788,Ultra-Low Power IoT Traffic Monitoring System,IEEE,Conferences,"Given the sizable anticipated proliferation of Internet of Things (IoT) devices, Forrester Research forecasts that the fleet management and transportation industry sectors will enjoy more growth than others. This may come as no surprise, since infrastructure (e.g., roadways, bridges, airports) is a prime candidate for sensor integration to provide real-time measurements and to support intelligent decisions. The predicted increase of deployed devices makes it difficult to calculate the amount of energy required for these functions. Current estimates suggest that 2 to 4% of worldwide carbon emissions can be attributed to the information and communication industry. This paper presents novel algorithms designed to optimize power consumption of an intelligent vehicle counter and classifier sensors. Each was based on an event-driven methodology wherein a control block orchestrates the work of different components and subsystems. System duty-cycle is reduced through several techniques, and a reinforcement learning algorithm is introduced to control the system power policy, according to the traffic environment. Battery life for a sensor supported by a 2300 mAh battery was extended from 48-hour, adopted all-on policy to more than 400 days when leveraging the algorithms and techniques presented in this work.",https://ieeexplore.ieee.org/document/8690788/,2018 IEEE 88th Vehicular Technology Conference (VTC-Fall),27-30 Aug. 2018,ieeexplore
10.1109/CSTIC52283.2021.9461259,Universal Semiconductor ATPG Solutions for ATE Platform under the Trend of AI and ADAS,IEEE,Conferences,"This article introduces a universal semiconductor Automatic Test Pattern Generation (ATPG) solution for Automated Test Equipment (ATE) platform. With the increasing trend of Artificial Intelligence (AI) and Advanced Driving Assistance System (ADAS) the communication between semiconductor devices requires advanced protocols such as Mobile Industry Processor Interface (MIPI) and Point-to-point (P2P) protocols. A designer-based solution is developed to provide a one-click software approach to create test vectors for common protocols and customized protocols. As a result, the silicon debug cycle can be massively reduced, comparing with converting waveform files generated from traditional Electronic Design Automation (EDA) tools. This solution can dramatically reduce the workload of test engineers and enable IC designers to participate in the debugging process of the device directly with an intuitive way. Such workflow can rise the efficiency of semiconductor test process and further decrease the Time to Market (TTM) of new product. This solution is designed as a comparable tool towards traditional EDA tools and will be another choice for ATPG solution. Up to now, this solution can generate test vectors for advanced protocols like MIPI D-PHY/C-PHY as well as basic protocols such as Inter-Integrated Circuit (I2C) and Serial Peripheral Interface (SPI) and complete evaluation on real device.",https://ieeexplore.ieee.org/document/9461259/,2021 China Semiconductor Technology International Conference (CSTIC),14-15 March 2021,ieeexplore
10.1109/ICTC.2017.8190968,Unmanned aerial vehicle surveillance system (UAVSS) for forest surveillance and data acquisition,IEEE,Conferences,"An application framework is proposed in this paper that considers low cost surveillance mechanism and data acquisition in the forest. An application is developed as proof of concept with detailed design that can take advantage of unmanned urban vehicle to be directly configured and controlled in real-time. The advantages are numerous; it can be used for many purposes. For example, it can be used for observing critical and important area for intruder activities or to know the current state of any object of interest. We considered using machine learning and image processing and can be used for species of trees in the forest by color and size detection. A separate service running on separate remote server will be responsible for this. We have proposed a application framework particularly to be cheap and easy to handle by non-technical persons and that it does not require large software system knowledge like Pix4D or DroneDeploy. This system will be useful for operations and research specially the forestry and palm oil plantation surveillance, and sustainable timber industry that specially needs carefully collected imageries and data from objects. Collection of raw data from sensor networks is also proposed in the system architecture.",https://ieeexplore.ieee.org/document/8190968/,2017 International Conference on Information and Communication Technology Convergence (ICTC),18-20 Oct. 2017,ieeexplore
10.1109/BigData.2016.7840903,Unravelling the Myth of big data and artificial intelligence in sustainable natural resource development,IEEE,Conferences,"Natural Resources businesses span one or more of the following verticals: Explore &amp; Discover, Develop, Extract &amp; Transport, Market &amp; Trade. Natural Gas (NG) is a key component of the Natural Resources industry, which contributes both directly and indirectly to sustain and support the day to day energy needs of humans. Various forums drive key initiatives to maintain the sustainability of natural resources for decades to come. The recent evolution of advanced Information Technology (IT) fields, termed as `Big Data and Artificial Intelligence (BD&amp;AI),' fortify these initiatives by providing anytime, anywhere access of data (for example in remote offshore assets or facilities) from a vast corpus base, in addition to enabling natural human interaction. In this paper we intend to explore the key primitives of BD&amp;AI and cite the method of real-world implementation in the Natural Gas industry to further enable sustainable development.",https://ieeexplore.ieee.org/document/7840903/,2016 IEEE International Conference on Big Data (Big Data),5-8 Dec. 2016,ieeexplore
10.1109/I2MTC43012.2020.9128596,Unsupervised Flight Phase Recognition with Flight Data Clustering based on GMM,IEEE,Conferences,"Currently, with the rapid development of the aviation industry, researchers are paying more attention to the improvement of aviation safety. Aviation safety mainly includes flight safety, aviation ground safety, and air defense safety. In terms of flight safety, the analysis of large amounts of flight data has gradually become a useful tool for timely detection of potential dangers at various stages of flight. As a result, flight data analysis has been one of the hot topics in aviation. However, due to the complexity of the aircraft operating conditions throughout the aircraft, if the data is analyzed at the entire flight phase, it is very difficult and time consuming to identify the problematic fight phase. Therefore, flight phase recognition for civil aircraft is implemented in this study. A flight phase recognition method based on Gaussian Mixture Model (GMM) is proposed in this work, which is the important foundation for timely detecting the abnormal event and improving the system safety and reliability. Firstly, the FDR data are preprocessed by spline interpolation and normalization, and then a GMM-based flight phase clustering is realized. In addition, a set of evaluation method is developed to evaluate the quality of flight phase recognition result. Finally, the effectiveness of the method is verified by using real FDR data from NASA's open database.",https://ieeexplore.ieee.org/document/9128596/,2020 IEEE International Instrumentation and Measurement Technology Conference (I2MTC),25-28 May 2020,ieeexplore
10.1109/RoEduNet51892.2020.9324883,Usage of Asymetric Small Binning to Compute Histogram of Oriented Gradients for Edge Computing Image Sensors,IEEE,Conferences,"In case of multiple imaging sensors used in different networks (home security, surveillance, automotive, industrial), there is a challenge to perform object detection algorithms in real time, even on the cloud, for a large number of sensors. This is why there is an intensive effort in the industry to move object detection processing on the edge, with the benefits of reducing the bandwidth needs and allowing for scalability in large networks. In this paper we present a hardware friendly optimization technique to compute Histogram of Oriented Gradients (HOG) on the edge, by approximating the HOG orientation as a multitude of small bins. The technique is implemented in RTL for FPGA or ASIC and serves as the first step in a standard object detection algorithm (using Histogram of Oriented Gradients as feature extractor and Support Vector Machine as the detection algorithm). We verified the results of the proposed optimizations for errors by comparison to a reference method and the overall object detection algorithm for robustness.",https://ieeexplore.ieee.org/document/9324883/,2020 19th RoEduNet Conference: Networking in Education and Research (RoEduNet),11-12 Dec. 2020,ieeexplore
10.1109/iCECE.2010.1483,Use of Accurate GPS Timing Based on Radial Basis Probabilistic Neural Network in Electric Systems,IEEE,Conferences,"GPS based time reference provides inexpensive but highly-accurate timing and synchronization capability and meets requirements in power system fault location, monitoring, and control. In the present era of restructuring and modernization of electric power utilities, the applications of GIS/GPS technology in power industry are growing and covering several technical and management activities. Because of GPS receivers error sources are time variant, it is necessary to remove the GPS measurement noise. In this paper a Radial Basis Probabilistic Neural Network (RBPNN) has been applied to GPS receivers timing data for modeling of their timing error sources. This method estimates GPS receivers timing errors from their previous values. The efficiency of this algorithm is illustrated by experiments. For real-time restitution of GPS timing accuracy, the proposed method is implemented on designed hardwares. The experimental tests results on collected real data emphasizes that GPS timing RMS errors can be reduced from 300 nsec and 200 nsec to less than 160 nsec and 41 nsec, with and without SA, respectively.",https://ieeexplore.ieee.org/document/5630774/,2010 International Conference on Electrical and Control Engineering,25-27 June 2010,ieeexplore
10.1109/ICST.2017.20,Using Semantic Similarity in Crawling-Based Web Application Testing,IEEE,Conferences,"To automatically test web applications, crawling-based techniques are usually adopted to mine the behavior models, explore the state spaces or detect the violated invariants of the applications. However, their broad use is limited by the required manual configurations for input value selection, GUI state comparison and clickable detection. In existing crawlers, the configurations are usually string-matching based rules looking for tags or attributes of DOM elements, and often application-specific. Moreover, in input topic identification, it can be difficult to determine which rule suggests a better match when several rules match an input field to more than one topic. This paper presents a natural-language approach based on semantic similarity to address the above issues. The proposed approach represents DOM elements as vectors in a vector space formed by the words used in the elements. The topics of encountered input fields during crawling can then be inferred by their similarities with ones in a labeled corpus. Semantic similarity can also be applied to suggest if a GUI state is newly discovered and a DOM element is clickable under an unsupervised learning paradigm. We evaluated the proposed approach in input topic identification with 100 real-world forms and GUI state comparison with real data from industry. Our evaluation shows that the proposed approach has comparable or better performance to the conventional techniques. Experiments in input topic identification also show that the accuracy of the rule-based approach can be improved by up to 22% when integrated with our approach.",https://ieeexplore.ieee.org/document/7927970/,"2017 IEEE International Conference on Software Testing, Verification and Validation (ICST)",13-17 March 2017,ieeexplore
10.1109/ICMAE.2017.8038685,Using artificial intelligence based expert system for selection of design subcontractors: A case study in aerospace industry,IEEE,Conferences,"As one of the top expectations for type certification of an aircraft, Aviation Authorities (AA) regulate design organization to establish Design Assurance System (DAS). DAS is composed of design, independent monitoring and airworthiness functions in which these functions are specialized for aerospace industry. Besides, Design Organization Approval (DOA) is a milestone to establish a rigid Design Assurance System. By this way, design organization assures aircraft development life cycle by complying with aviation regulations. To meet requirements of Design Organization Approval, Design Organization transfers its authority and technical signatories to its subcontractors to improve effectiveness of the system. So, performance of design subcontractors shall be traceable and measurable to match capability requirements of main contractor. Thus, subcontractor evaluation is a long and complicated process; survey implementation could be misleading in some cases. The purpose of this study is to propose a novel tool to measure performance of a design subcontractor according to necessities of Design Assurance System. Up to now, there is no tool to evaluate aviation design subcontractors. With this tool, contractor firm can evaluate multiple criteria in a single run. AHP is used to prioritize criteria relative to each other one-by-one. Then, for subcontractor selection and subcontractor monitoring, Artificial Neural Network (ANN) is applied to optimize decision making process. Annual Actual Data is applied in AHP model to assess current performance score of subcontractor. To have a long term judgment of this system, the model shall be applied to a design subcontractor for more than once on fixed periods such as quarterly, yearly etc.",https://ieeexplore.ieee.org/document/8038685/,2017 8th International Conference on Mechanical and Aerospace Engineering (ICMAE),22-25 July 2017,ieeexplore
10.1109/FSKD.2015.7382048,Using hidden markov model for dynamic malware analysis: First impressions,IEEE,Conferences,"Malware developers are coming up with new techniques to escape malware detection. Furthermore, with the common availability of malware construction kits and metamorphic virus generators, creation of obfuscated malware has become a child's play. This has made the task of anti-malware industry a challenging one, who need to analyze tens of thousands of new malware samples everyday in order to provide defense against the malware threat. The silver lining is that most of the malware generated by such means is different only syntactically, and hence techniques employing dynamic analysis and behavior modeling can be effectively used for classifying malware. In this paper we have proposed a malware classification scheme based on Hidden Markov Models using system calls as observed symbols. Our approach combines the powerful statistical pattern analysis capability of Hidden Markov Models with the proven capacity of system calls as discriminating dynamic features for countering malware obfuscation. Testing the proposed technique on system call logs of real malware shows that it has the potential of effectively classifying unknown malware into known classes.",https://ieeexplore.ieee.org/document/7382048/,2015 12th International Conference on Fuzzy Systems and Knowledge Discovery (FSKD),15-17 Aug. 2015,ieeexplore
10.1109/ISMAR.2019.00-22,VR Props: An End-to-End Pipeline for Transporting Real Objects Into Virtual and Augmented Environments,IEEE,Conferences,"Improvements in both software and hardware, as well as an increase in consumer suitable equipment, have resulted in great advances in the fields of virtual and augmented reality. Typically, systems use controllers or hand gestures to interact with virtual objects. However, these motions are often unnatural and diminish the immersion of the experience. Moreover, these approaches offer limited tactile feedback. There does not currently exist a platform to bring an arbitrary physical object into the virtual world without additional peripherals or the use of expensive motion capture systems. Such a system could be used for immersive experiences within the entertainment industry as well as being applied to VR or AR training experiences, in the fields of health and engineering. We propose an end-to-end pipeline for creating an interactive virtual prop from rigid and non-rigid physical objects. This includes a novel method for tracking the deformations of rigid and non-rigid objects at interactive rates using a single RGBD camera. We scan our physical object and process the point cloud to produce a triangular mesh. A range of possible deformations can be obtained by using a finite element method simulation and these are reduced to a low dimensional basis using principal component analysis. Machine learning approaches, in particular neural networks, have become key tools in computer vision and have been used on a range of tasks. Moreover, there has been an increased trend in training networks on synthetic data. To this end, we use a convolutional neural network, trained on synthetic data, to track the movement and potential deformations of an object in unlabelled RGB images from a single RGBD camera. We demonstrate our results for several objects with different sizes and appearances.",https://ieeexplore.ieee.org/document/8943647/,2019 IEEE International Symposium on Mixed and Augmented Reality (ISMAR),14-18 Oct. 2019,ieeexplore
10.1109/CCECE47787.2020.9255806,Vehicle Damage Classification and Fraudulent Image Detection Including Moiré Effect Using Deep Learning,IEEE,Conferences,"Image-based vehicle insurance processing and loan management has large scope for automation in automotive industry. In this paper we consider the problem of car damage classification, where categories include medium damage, huge damage and no damage. Based on deep learning techniques, MobileNet model is proposed with transfer learning for classification. Moreover, moving towards automation also comes with diverse hurdles; users can upload fake images like screenshots or taking pictures from computer screens, etc. To tackle this problem a hybrid approach is proposed to provide only authentic images to algorithm for damage classification as input. In this regard, moiré effect detection and metadata analysis is performed to detect fraudulent images. For damage classification 95% and for moiré effect detection 99% accuracy is achieved.",https://ieeexplore.ieee.org/document/9255806/,2020 IEEE Canadian Conference on Electrical and Computer Engineering (CCECE),30 Aug.-2 Sept. 2020,ieeexplore
10.1109/CVIDL51233.2020.00-85,Vehicle automatic driving system based on embedded and machine learning,IEEE,Conferences,"With the rapid development of Chinnes highway transportation industry, the problem of road traffic safety has become increasingly prominent. Highway passenger transport accidents are generally fatal and fatal accidents. Traffic accidents not only cause enormous economic losses to transport enterprises, but also have a very bad social impact on local highway transport management departments, which has even become a new social instability factor. This is mainly because there are many problems in autopilot technology, such as low recognition accuracy, poor real-time performance, weak anti-interference ability and so on. However, embedded technology and machine learning can solve these problems well, so autopilot technology will become the mainstream in the future. Firstly, this paper analyses the importance of autopilot technology. Then this paper analyses the machine learning target recognition, vehicle automatic driving system model and vehicle automatic driving system flow. Finally, this paper designs the function of autopilot system.",https://ieeexplore.ieee.org/document/9270523/,"2020 International Conference on Computer Vision, Image and Deep Learning (CVIDL)",10-12 July 2020,ieeexplore
10.1109/SERVICES.2010.125,Virtual Services in Cloud Computing,IEEE,Conferences,"Cloud computing has aroused wide research interests and has been accepted by industry. Services are playing the essential role in cloud computing as cloud computing refers to “both the applications delivered as services over the Internet and the hardware and systems software in the datacenters that provide those services”. Therefore, service-oriented architecture should play an important role in cloud computing. In addition, one of the characteristics of cloud computing is to make services available on demand. Given a group of services, different demands may involve different set of services and in different order. This is related to services reuse and composition. However, existing methods only solve service composition in a binary manner: Either a solution exists or no solutions at all. In this paper, we propose the concept of virtual services, which physically do not exist, but are conceptually treated in the same way as physical services. Virtual services are useful when traditional service compositions fail. Virtual services can connect the existing physical services and enable the composition process to succeed. The specifications of virtual services will provide valuable information about how to develop real services to meet the requirements of the given demand. In this paper, we present algorithms that can help identify the virtual services in the case of composition failure and provide the specification of virtual services for further analysis and development.",https://ieeexplore.ieee.org/document/5575563/,2010 6th World Congress on Services,5-10 July 2010,ieeexplore
10.1109/ITST.2017.7972192,Virtual assistants and self-driving cars,IEEE,Conferences,"Self-driving cars are technologically a reality and in the next decade they are expected to reach the highest level of automation. While there is general agreement that an advanced human-autonomous vehicle (HAV) interaction is key to achieve the benefits of self-driving cars, it is less clear what role artificial intelligence (AI) should play in this context. While the scientific community is debating on the role and intersections of AI, autonomous vehicles and related issues, above all ethics, the automotive industry is already presenting AI-based products and services that may influence, in a direction or in another, our technological and societal futures. This paper focuses on virtual assistants, the personification of the car intelligence incorporating, among others, an algorithmic “brain”, a synthetic human “voice” and powerful sensor-based “senses”. Should virtual assistants just assist humans or replace them whenever necessary? Should their scope of action be limited to safety-related driving tasks or to any activity performed in the car or controlled from the car? Although at a very early stage of commercial development, the paper will review the state-of-the-art of in-car virtual assistants underlining their role and functions in the connected and automated driving ecosystem. By drawing from earlier reflections on automation, robots and intelligent agents, it will then identify a series of issues to be addressed by the scientific community, policy-makers and the automotive industry stakeholders.",https://ieeexplore.ieee.org/document/7972192/,2017 15th International Conference on ITS Telecommunications (ITST),29-31 May 2017,ieeexplore
10.1109/ITHET.2012.6246058,Virtual industrial training: Joining innovative interfaces with plant modeling,IEEE,Conferences,"Training in industry is one of the most critical and expensive tasks to be faced by the management. Furthermore, in some cases, it is dangerous or even impossible to directly train operators on the real plants where security and safety problems may arise, making it very difficult to start training programs at low cost. For these reasons, the field of training in industry is rapidly developing using software or hardware solutions coming mainly from the following research areas: i) Human-Computer interaction, i.e., the use of complex and interactive human-machine interfaces, ii) plant simulators, i.e., software systems which are delivered with the plant itself to test and to learn complex tasks and processes, iii) Intelligent Training Systems, i.e., the availability of intelligent and personalized training systems where a virtual tutor guides users through a personalized learning path. In this paper we present the overall architecture of a system for industrial training, embedded into an Intelligent Tutoring System that can provide more effective and personalized training and learning in a context where working directly on real plants can be difficult and very expensive. In particular we present a simulator for training operators in using power plants, based on a multimedia and on interactive interface. This system is particularly suitable to be used for training in industrial electric and oil plants. Moreover, the system allows operators for collaborative problem solving. Currently the system is under delivery to an Italian Electric industry.",https://ieeexplore.ieee.org/document/6246058/,2012 International Conference on Information Technology Based Higher Education and Training (ITHET),21-23 June 2012,ieeexplore
10.1109/WOSSPA.2011.5931460,Viscosity and gas/oil ratio curves estimation using advances to neural networks,IEEE,Conferences,"In oil and gas industry, prior prediction of certain properties is needed ahead of exploration and facility design. Viscosity and gas/oil ratio (GOR), are among those properties described through curves with their values varying over a specific range of reservoir pressures. However, the usual prediction approach could result into curves that are not consistent, exhibiting scattered behaviour as compared to the real curves. In this paper two advances to artificial neural networks are implemented to solve the problem. These are Support Vector Regressors and Functional Networks. Statistical error measures have been used and showed the high performance of the proposed techniques. Moreover, the predicted curves are consistent with the actual curves.",https://ieeexplore.ieee.org/document/5931460/,"International Workshop on Systems, Signal Processing and their Applications, WOSSPA",9-11 May 2011,ieeexplore
10.1109/ISWCS.2019.8877305,Visible Light Positioning for Location-Based Services in Industry 4.0,IEEE,Conferences,"Industry 4.0 refers to the evolution in manufacturing from computerization to fully cyberphysical systems that exploit rich sensor data, adaptive real-time safety-critical control, and machine learning. An important aspect of this vision is the sensing and subsequent association of objects in the physical world with their cyber and virtual counterparts. In this paper we propose Visible Light Positioning (VLP) as an enabler for these Industry 4.0 applications. We also explore sensing techniques, including cameras (and depth sensors), and other light-based solutions for object positioning and detection along with their respective limitations. We then demonstrate an application of positioning for real time robot control in an interactive multiparty cyber-physical-virtual deployment. Lastly, based on our experience with this cyberphysical-virtual application, we propose Ray-Surface Positioning (RSP), a novel VLP technique, as a low cost positioning system for Industry 4.0.",https://ieeexplore.ieee.org/document/8877305/,2019 16th International Symposium on Wireless Communication Systems (ISWCS),27-30 Aug. 2019,ieeexplore
10.1109/ICSTCC.2019.8885611,Visual Analytics Framework for Condition Monitoring in Cyber-Physical Systems,IEEE,Conferences,"One of the biggest challenges facing the factory of the future today is to reduce the time-to-market access and increase through the improvement of competitiveness and efficiency. In order to achieve this target, data analytics in Industrial Cyber-Physical System becomes a feasible option. In this paper, a visual analytics framework for condition monitoring of the machine tool is presented with the aim to manage events and alarms at factory level. The framework is assessed in a particular use case that consists in a multi-threaded cloud-based solution for the global analysis of the behaviour of variables acquired from PLC, CNC and robot manipulator. A human-machine interface is also designed for the real-time visualization of the key performance indicators according to the user's criteria. This tool implemented is a great solution for condition monitoring and decision-making process based on data analytics from simple statistics to complex machine learning methods. The results achieved are part of the vision and implementation of the industrial test bed of “Industry and Society 5.0” platform.",https://ieeexplore.ieee.org/document/8885611/,"2019 23rd International Conference on System Theory, Control and Computing (ICSTCC)",9-11 Oct. 2019,ieeexplore
10.1109/CASE48305.2020.9216781,Weak Scratch Detection of Optical Components Using Attention Fusion Network,IEEE,Conferences,"Scratches on the optical surface can directly affect the reliability of the optical system. Machine vision-based methods have been widely applied in various industrial surface defect inspection scenarios. Since weak scratches imaging in the dark field has an ambiguous edge and low contrast, which brings difficulty in automatic defect detection. Recently, many existing visual inspection methods based on deep learning cannot effectively inspect weak scratches due to the lack of attention-aware features. To address the problems arising from industry-specific characteristics, this paper proposes “Attention Fusion Network;”, a convolutional neural network using attention mechanism built by hard and soft attention modules to generate attention-aware features. The hard attention module is implemented by integrating the brightness adjustment operation in the network, and the soft attention module is composed of scale attention and channel attention. The proposed model is trained on a real-world industrial scratch dataset and compared with other defect inspection methods. The proposed method can achieve the best performance to detect the weak scratch inspection of optical components compared to the traditional scratch detection methods and other deep learning-based methods.",https://ieeexplore.ieee.org/document/9216781/,2020 IEEE 16th International Conference on Automation Science and Engineering (CASE),20-21 Aug. 2020,ieeexplore
10.1109/SenSysML50931.2020.00007,Welcome Message from SenSys-ML'20 Chairs,IEEE,Conferences,"We are very excited to welcome you to the second ACM/IEEE International Workshop on Machine Learning on Edge in Sensor Systems (SenSys-ML 2020). SenSys-ML’20 will be held in conjunction with the CPS-IoT Week 2020 and focuses on work that combines sensor signals from the physical world with machine learning, particularly in ways that are distributed to the device or use edge and fog computing. The development and deployment of ML at the very edge remains a technological challenge constrained by computing, memory, energy, network bandwidth, and data privacy and security limitations. This is especially true for battery-operated devices and always-on use cases and applications. In recent years this has gained attention from both academia and industry and many TinyML initiatives have been started focusing both in hardware and software advancements. This workshop will provide a forum for sensing, networking and machine learning researchers to present and share their latest research on building machine learning-enabled sensor systems. Sensys-ML focuses on providing extensive feedback on Work-In-Progress papers involving machine learning (TinyML/ UltraML) on sensor systems. Many papers were submitted from multiple countries, and four papers were selected for publication. This year our articles had themes including techniques for collecting low-resolution images from thermal cameras for human activity recognition, VAE-based approach for privacy preservation of sensor data, deep model compression techniques, and novel GRU based shallow Neural Networks. We see advancement and contributions made by research work will have a significant impact on real-world applications and future research directions.",https://ieeexplore.ieee.org/document/9111711/,2020 IEEE Second Workshop on Machine Learning on Edge in Sensor Systems (SenSys-ML),21-21 April 2020,ieeexplore
10.1109/YAC.2018.8406429,Wheel classification using convolutional neural networks,IEEE,Conferences,"With the fast development of automobile wheel industry, many methods of wheel classification have been proposed. The appearance of wheel changes a lot during the process of production, but few of these conventional methods is designed to handle the many challenges of wheel classification for different appearances. This paper studies on wheel classification for different appearances during the process of production, and proposes a wheel classification method using convolutional neural network. Firstly, we implement circle detection, gray value analysis and size normalization on wheel images and collect ten common types of wheel to build a dataset. Based on the features of the wheel images, a convolutional neural network is proposed. Then data augmentation is implemented on the dataset and local response normalization layer is added in the convolutional neural network. The experiment results show that the presented method has a better performance both in classification accuracy and real-time requirement than conventional methods and achieves 95.6% accuracy on wheel classification in the dataset.",https://ieeexplore.ieee.org/document/8406429/,2018 33rd Youth Academic Annual Conference of Chinese Association of Automation (YAC),18-20 May 2018,ieeexplore
10.1109/INFCOM.2009.5062082,"Wi-Sh: A Simple, Robust Credit Based Wi-Fi Community Network",IEEE,Conferences,"Wireless community networks, where users share wireless bandwidth is attracting tremendous interest from academia and industry. Companies such as FON have been successful in attracting large communities of users. However, solutions such as FON either require users to buy specialized FON routers or firmware modifications to existing routers. In this paper we propose a solution which requires no such sophisticated hardware. An alternative is to provide a solution which requires users to download a client software on to their PCs. While the solution appears simple it raises several issues of incentivizing users to share their bandwidth and also issues of preventing users from cheating behaviors which give them an unfair advantage. In this paper, we propose a system and solution which (i) requires only software downloads on PCs, (ii) is robust to tampering of the software, and intermittent monitoring of an access point by the owner, (iii) a credit based mechanism whereby users earn credits for sharing bandwidth and punishment and pricing mechanism whereby users are charged at a higher price whenever they are caught misbehaving. By making simple but plausible assumptions about user behavior, we show via analysis and extensive simulations that the system converges to a Pareto optimal Nash equilibrium. We further validate our system model, by running trace driven simulations on real world data. We believe that the solution provided by Wi-Sh is an attractive and more credible alternative to solutions such as FON.",https://ieeexplore.ieee.org/document/5062082/,IEEE INFOCOM 2009,19-25 April 2009,ieeexplore
10.1109/RTSS.2018.00029,Work-in-Progress: Making Machine Learning Real-Time Predictable,IEEE,Conferences,"Machine learning (ML) on edge computing devices is becoming popular in the industry as a means to make control systems more intelligent and autonomous. The new trend is to utilize embedded edge devices, as they boast higher computational power and larger memories than before, to perform ML tasks that had previously been limited to cloud-hosted deployments. In this work, we assess the real-time predictability and consider data privacy concerns by comparing traditional cloud services with edge-based ones for certain data analytics tasks. We identify the subset of ML problems appropriate for edge devices by investigating if they result in real-time predictable services for a set of widely used ML libraries. We specifically enhance the Caffe library to make it more suitable for real-time predictability. We then deploy ML models with high accuracy scores on an embedded system, exposing it to industry sensor data from the field, to demonstrates its efficacy and suitability for real-time processing.",https://ieeexplore.ieee.org/document/8603205/,2018 IEEE Real-Time Systems Symposium (RTSS),11-14 Dec. 2018,ieeexplore
10.1109/ASE.2019.00077,Wuji: Automatic Online Combat Game Testing Using Evolutionary Deep Reinforcement Learning,IEEE,Conferences,"Game testing has been long recognized as a notoriously challenging task, which mainly relies on manual playing and scripting based testing in game industry. Even until recently, automated game testing still remains to be largely untouched niche. A key challenge is that game testing often requires to play the game as a sequential decision process. A bug may only be triggered until completing certain difficult intermediate tasks, which requires a certain level of intelligence. The recent success of deep reinforcement learning (DRL) sheds light on advancing automated game testing, without human competitive intelligent support. However, the existing DRLs mostly focus on winning the game rather than game testing. To bridge the gap, in this paper, we first perform an in-depth analysis of 1349 real bugs from four real-world commercial game products. Based on this, we propose four oracles to support automated game testing, and further propose Wuji, an on-the-fly game testing framework, which leverages evolutionary algorithms, DRL and multi-objective optimization to perform automatic game testing. Wuji balances between winning the game and exploring the space of the game. Winning the game allows the agent to make progress in the game, while space exploration increases the possibility of discovering bugs. We conduct a large-scale evaluation on a simple game and two popular commercial games. The results demonstrate the effectiveness of Wuji in exploring space and detecting bugs. Moreover, Wuji found 3 previously unknown bugs, which have been confirmed by the developers, in the commercial games.",https://ieeexplore.ieee.org/document/8952543/,2019 34th IEEE/ACM International Conference on Automated Software Engineering (ASE),11-15 Nov. 2019,ieeexplore
10.1109/FPL53798.2021.00075,XANDAR: X-by-Construction Design framework for Engineering Autonomous &amp; Distributed Real-time Embedded Software Systems,IEEE,Conferences,"The next generation of networked embedded systems (ES) necessitates rapid prototyping and high performance while maintaining key qualities like trustworthiness and safety. However, development of safety-critical ES suffers from complex software (SW) toolchains and engineering processes. Moreover, the current trend in autonomous systems, which relies on Machine Learning (ML) and AI applications when combined with fail-operational requirements renders the Verification and Validation (V&amp;V) of these new systems a challenging endeavor. Prime examples are Advanced Driver-Assistance Systems (ADAS) that are prone to various safety/security vulnerabilities. The XANDAR project aims at developing a mature SW toolchain (from requirements analysis to the actual code integration on target including V&amp;V) fulfilling the needs of industry for rapid prototyping of interoperable and autonomous ES. Starting from a model-based system architecture, XANDAR will leverage automatic model synthesis and software parallelization techniques to achieve specific non-functional requirements setting the foundation for a novel (real-time, safety-, and security)-by-Construction paradigm.",https://ieeexplore.ieee.org/document/9556412/,2021 31st International Conference on Field-Programmable Logic and Applications (FPL),30 Aug.-3 Sept. 2021,ieeexplore
10.1109/UKSim.2012.123,[Cover art],IEEE,Conferences,The following topics are dealt with: neural networks; evolutionary computation; adaptive dynamic programming; re-enforcement learning; bio-informatics; bio-engineering; computational finance; economics; semantic mining; data mining; virtual reality; data visualization; intelligent systems; soft computing; hybrid computing; e-science; e-systems; robotics; cybernetics; manufacturing; engineering; operations research; discrete event systems; real time systems; image processing; speech processing; signal processing; industry; business; social issues; human factors; marine simulation; power systems; logistics;parallel systems; distributed systems; software architectures;Internet modelling; semantic Web; ontologies; mobile ad hoc wireless networks; Mobicast; sensor placement; target tracking; circuits; sensors and devices.,https://ieeexplore.ieee.org/document/6205540/,2012 UKSim 14th International Conference on Computer Modelling and Simulation,28-30 March 2012,ieeexplore
10.1109/RAIT.2016.7507858,[Title page],IEEE,Conferences,The following topics are dealt with: wireless communication; signal processing; artificial intelligence; knowledge discovery; soft computing; cryptography; network security; image processing; video processing; pattern recognition; machine learning; VLSI design; embedded systems; real time systems; geosciences; mining industry; software engineering; information retrieval; e-govemance; information communication technology; and medical computing.,https://ieeexplore.ieee.org/document/7507858/,2016 3rd International Conference on Recent Advances in Information Technology (RAIT),3-5 March 2016,ieeexplore
10.1109/ICITSI.2016.7858181,[Title page],IEEE,Conferences,"The following topics are dealt with: SDLC SPASI v. 4.0. business process; information extraction; statistics indicator tables; rule generalizations; ontology; conventional learning system; ICT-based learning; job training system; time-series data; RAID; software-based accelerator; virtualization environment; enterprise architecture government organization; TOGAF ADM; SONA; e- library; modified quantitative models for performance measurement system method; business process improvement; district government innovation service case study; government organization; m-government implementation evaluation; trusted Big Data; official statistics study case; data profiling; data quality improvement; secure internet access; copyright protection; color images; transform domain; luminance component; information network architecture; local government; software as a service; expert system; meningitis disease; certainty factor method; digital asset management system; broadcasting organizations; e-portofolio definition; system security requirement identification; electronic payment system; Internet-based long distance education; operational model data governance; requirement engineering; open government information network development; process capability assessment; information security management; information security governance; national cyber physical systems; e-learning readiness; remote control system; serial communications mobile; microcontroller; knowledge sharing; indonesia higher educational institutions; cultural heritage metadata; geo linked open data; NUSANTARA: knowledge management system; adaptive personalized learning system; interactive learning media; RPP ICT; government human capital management; knowledge management tools utilization; knowledge management readiness; analytic hierarchy process; government institutions; usability testing; scrum methodology; assistant information system; automatic arowana raiser; pSPEA2; strength Pareto evolutionary algorithm 2; early diagnosis expert system deficiency; digital forensic; user acceptance; human resource information system; automated plasmodium detection; malaria diagnosis; thin blood smear image; 3D virtual game; MOODLE; SLOODLE; open simulator case study; color-based segmentation; feature detection; ball post; goal post; mobile soccer robot game field; smart farming; real time q-log-based feature normalization; distant speech recognition; Monte Carlo localization; robot operating system; finite element method; 3D DC resistivity modeling; multi GPU; breast cancer lesions; adaptive thresholding; morphological operation; gamification framework; online training; collaborative working system; classification breast cancer ultrasound images; posterior features; three-wheeled omnidirectional robot controller; public services satisfaction; sentiment analysis; color blind test quantification; RGB primary color cluster; ERP modules requirement; micro, small and medium enterprise fashion industry; small culinary enterprises; business system requirement; small craft companies ; power analysis attack; DES and IT value model.",https://ieeexplore.ieee.org/document/7858181/,2016 International Conference on Information Technology Systems and Innovation (ICITSI),24-27 Oct. 2016,ieeexplore
10.1109/BDCloud.2018.00136,"iSTEP, an Integrated Self-Tuning Engine for Predictive Maintenance in Industry 4.0",IEEE,Conferences,"The recent expansion of IoT-enabled (Internet of Things) devices in manufacturing contexts and their subsequent data-driven exploitation paved the way to the advent of the Industry 4.0, promoting a full integration of IT services, smart devices, and control systems with physical objects, their electronics and sensors. The real-time transmission and analysis of collected data from factories has the potential to create manufacturing intelligence, of which predictive maintenance is an expression. Hence the need to design new approaches able to manage not only the data volume, but also the variety and velocity, extracting actual value from the humongous amounts of collected data. To this aim, we present iSTEP, an integrated Self-Tuning Engine for Predictive maintenance, based on Big Data technologies and designed for Industry 4.0 applications. The proposed approach targets some of the most common needs of manufacturing enterprises: compatibility with both the on-premises and the in-the-cloud environments, exploitation of reliable and largely supported Big Data platforms, easy deployment through containerized software modules, virtually unlimited horizontal scalability, fault-tolerant self-reconfiguration, flexible yet friendly streaming-KPI computations, and above all, the integrated provisioning of self-tuning machine learning techniques for predictive maintenance. The current implementation of iSTEP exploits a distributed architecture based on Apache Kafka, Spark Streaming, MLlib, and Cassandra; iSTEP provides (i) a specific feature engineering block aimed at automatically extracting metrics from the production monitoring time series, which improves the predictive performance by 77% on average, and (ii) a self-tuning approach that dynamically selects the best prediction algorithm, which improves the predictive performance up to 60%. The iSTEP engine provides transparent predictive models, able to provide end users with insights into the knowledge learned, and it has been experimentally evaluated on a public unbalanced failure dataset, whose extensive results are discussed in the paper.",https://ieeexplore.ieee.org/document/8672266/,"2018 IEEE Intl Conf on Parallel & Distributed Processing with Applications, Ubiquitous Computing & Communications, Big Data & Cloud Computing, Social Computing & Networking, Sustainable Computing & Communications (ISPA/IUCC/BDCloud/SocialCom/SustainCom)",11-13 Dec. 2018,ieeexplore
10.1109/TLA.2021.9480156,A Data Governance Framework for Industry 4.0,IEEE,Journals,"The fourth industrial revolution, or Industry 4.0, represents a new stage of evolution in the organization, management and control of the value chain throughout the product or service life cycle. This digitization of the industrial environment is characterized by the connection of Information Technologies (IT) and Operations Technologies (OT) through cyber-physical systems and the Industrial IoT (IIoT). One of the main consequences of this integration is the increasing amount and variety of data generated in real time from different sources. In this environment of intensive generation of actionable information, data becomes a critical asset for Industry 4.0, at all stages of the value chain. However, in order to data become a competitive advantage for the company, it must be managed and governed like any other strategic asset, and therefore it is necessary to rely on a Data Governance system. Industry 4.0 requires a reformulation of governance since the data is a key element and the backbone of the processes of the organization. This paper proposes a Reference Framework for the implementation of Data Governance Systems for Industry 4.0. Previously, it contextualizes data governance for Industry 4.0 environments and identifies the requirements that this framework must address, which are conditioned by the specific features of Industry 4.0, among others, the intensive use of big data, cloud and edge computing, artificial intelligence and current regulations.",https://ieeexplore.ieee.org/document/9480156/,IEEE Latin America Transactions,Dec. 2021,ieeexplore
10.1109/ACCESS.2020.3011477,A Data Security Enhanced Access Control Mechanism in Mobile Edge Computing,IEEE,Journals,"Mobile edge computing, with characteristics of position awareness, mobile support, low latency, decentralization, and distribution, has received widespread attention from industry and academia, and has been applied to application areas such as intelligent transportation, smart city, and real-time big data analysis. However, it also brings the new security threats, especially data security threats during data access that leads to unauthorized/unauthorized access, alteration and disclosure of data, affecting the confidentiality and integrity of the data. Therefore, access control, as an important method to ensure the security of user data during data access, began to be applied to mobile edge computing. However, the existing access control has the disadvantages of coarse-grain, poor flexibility and accuracy, lack of internal attack considerations, etc., which cannot meet the needs of data security in practical applications of mobile edge computing. In this paper, a data security enhanced Fine-Grained Access Control mechanism (FGAC) is proposed to ensure data security during data access in mobile edge computing. In FGAC, a dynamic fine-grained trusted user grouping scheme based on attributes and metagraphs theory was first designed. Secondly, the scheme was combined with the traditional role-based access control mechanism to assign roles to users based on user group credibility. And then, based on attribute matching the user authentication further verifies whether the user is allowed to perform the access operations to achieve fine-grained data protection. Experimental results show that FGAC can effectively identify malicious users and make group adjustments, while achieving fine-grained access control and assure the data security during the data access process in mobile edge computing.",https://ieeexplore.ieee.org/document/9146646/,IEEE Access,2020,ieeexplore
10.1109/TPWRS.2019.2922333,A Data-Driven Framework for Assessing Cold Load Pick-Up Demand in Service Restoration,IEEE,Journals,"Cold load pick-up (CLPU) has been a critical concern to utilities. Researchers and industry practitioners have underlined the impact of CLPU on distribution system design and service restoration. The recent large-scale deployment of smart meters has provided the industry with a huge amount of data that are highly granular, both temporally and spatially. In this paper, a data-driven framework is proposed for assessing CLPU demand of residential customers using smart meter data. The proposed framework consists of two interconnected layers: 1) At the feeder level, a nonlinear autoregression model is applied to estimate the diversified demand during the system restoration and calculate the CLPU demand ratio. 2) At the customer level, Gaussian mixture models and probabilistic reasoning are used to quantify the CLPU demand increase. The proposed methodology has been verified using real smart meter data and outage cases.",https://ieeexplore.ieee.org/document/8735925/,IEEE Transactions on Power Systems,Nov. 2019,ieeexplore
10.1109/ACCESS.2019.2927082,A Decade of Internet of Things: Analysis in the Light of Healthcare Applications,IEEE,Journals,"Impressive growth in the number of wearable health monitoring devices has affected global health industry as they provide rapid and intricate details related to physical examinations, such as discomfort, heart rate, and blood glucose level, which enable doctors to efficiently diagnose sensitive heart troubles. The Internet of Medical Things (IoMT) is a phenomenon wherein computer networks and medical equipment are connected through the Internet to provide real-time interaction between physicians and patients. In this article, we present a comprehensive view of the IoMT and its related Machine Learning (ML)-based developed frameworks designed, or being utilized, in the last decade, i.e., from 2010 to 2019. The presented techniques are designed for monitoring limbs, controlling rural healthcare, identifying e-health applications, monitoring health through mobile apps, classifying heart sounds, detecting stress in drivers, monitoring cardiac diseases, making the decision to predict heart attacks, recognizing human activities, and classifying breast cancer. The aim is to provide a clear picture of the existing IoMT environment so that the analysis may pave the way for the diagnosis of critical disorders such as cancer, heart attack, and blood pressure among others. In the end, we also provide some unresolved challenges that are confronted in the deployment of the secure IoMT-based healthcare systems.",https://ieeexplore.ieee.org/document/8755982/,IEEE Access,2019,ieeexplore
10.1109/TII.2019.2915846,A Global Manufacturing Big Data Ecosystem for Fault Detection in Predictive Maintenance,IEEE,Journals,"Artificial intelligence, big data, machine learning, cloud computing, and Internet of Things (IoT) are terms which have driven the fourth industrial revolution. The digital revolution has transformed the manufacturing industry into smart manufacturing through the development of intelligent systems. In this paper, a big data ecosystem is presented for the implementation of fault detection and diagnosis in predictive maintenance with real industrial big data gathered directly from large-scale global manufacturing plants, aiming to provide a complete architecture which could be used in industrial IoT-based smart manufacturing in an industrial 4.0 system. The proposed architecture overcomes multiple challenges including big data ingestion, integration, transformation, storage, analytics, and visualization in a real-time environment using various technologies such as the data lake, NoSQL database, Apache Spark, Apache Drill, Apache Hive, OPC Collector, and other techniques. Transformation protocols, authentication, and data encryption methods are also utilized to address data and network security issues. A MapReduce-based distributed PCA model is designed for fault detection and diagnosis. In a large-scale manufacturing system, not all kinds of failure data are accessible, and the absence of labels precludes all the supervised methods in the predictive phase. Furthermore, the proposed framework takes advantage of some of the characteristics of PCA such as its ease of implementation on Spark, its simple algorithmic structure, and its real-time processing ability. All these elements are essential for smart manufacturing in the evolution to Industry 4.0. The proposed detection system has been implemented into the real-time industrial production system in a cooperated company, running for several years, and the results successfully provide an alarm warning several days before the fault happens. A test case involving several outages in 2014 is reported and analyzed in detail during the experiment section.",https://ieeexplore.ieee.org/document/8710319/,IEEE Transactions on Industrial Informatics,Jan. 2020,ieeexplore
10.1109/TIM.2020.3047194,A High-Precision Diagnosis Method for Damp Status of OIP Bushing,IEEE,Journals,"An accurate assessment of the damp status of oil-impregnated paper (OIP) bushings is crucial for the power industry to make informed decisions on the maintenance and replacement schedule of bushings. This article proposes a hybrid of the convolutional neural network (CNN) and the hidden Markov model (HMM) for estimating the damp status (i.e., moisture level and moisture source) of bushings especially when nonuniform moisture distribution exhibits in the bushing. First, simulation models of moisture diffusion and frequency-domain spectroscopy (FDS) of the OIP bushing were constructed using the finite element modeling (FEM) approach. Then, CNN was employed to extract informative features from FDS results of the OIP bushing, which is sensitive to both concentrations and sources of moisture. Finally, HMMs were further utilized as a strong stability tool to recognize the damp status of OIP bushings. The proposed method was implemented to identify the bushing damp status using both simulation data and real-life measurements. Identification results demonstrate that the proposed method has high accuracy in determining the moisture level and moisture source of the OIP bushing insulation.",https://ieeexplore.ieee.org/document/9306924/,IEEE Transactions on Instrumentation and Measurement,2021,ieeexplore
10.1109/ACCESS.2018.2859440,A Hybrid User Experience Evaluation Method for Mobile Games,IEEE,Journals,"With the development of the mobile phone industry, mobile applications market becomes thriving. However, the immature information technology and unfriendly interface bring negative user experience (UX) to the mobile users and thus affect the service life of mobile applications, especially for the most concerned entertaining applications, mobile games. As a result, the evaluating UX and finding crucial factors of UX become a challenge. Over the last decades, numerous researches have tried to deal with this issue, but none of them has clearly identified the relations among positive-negative UX, sufficient human characteristics and specific events of applications. This paper proposes a subjective-objective evaluation method. Subjective UX of mobile games is sufficiently obtained and objective UX is verified through the electrocardiogram signals and heart rate variability. In order to reveal distinct relations among UX factors, sufficient user characteristics and categories of game events, an improved RIPPER algorithm is proposed by this paper to obtain the relations. Experiments are performed with 300 testers who played mobile parkour games for at least five minutes. The accuracy and efficiency of the proposed method have been verified through real experiments and objective measures. In addition, this paper provides an effective sampling method and a data analysis algorithm to obtain crucial UX factors for mobile applications.",https://ieeexplore.ieee.org/document/8423056/,IEEE Access,2018,ieeexplore
10.1109/JSEN.2021.3113908,A Lightweight Framework for Human Activity Recognition on Wearable Devices,IEEE,Journals,"Human Activity Recognition (HAR) is the automatic detection and understanding of human motion behavior based on data extracted from video camera, ambient sensors or wearable sensors which particularly has recently attracted increased attention from both researchers and industry. However, for running practical HAR systems on wearable devices, there are some requirements such as design and development of small, lightweight, powerful, and low-cost smart sensors. In this context, data must be continuously collected, and edge computing is a viable solution, which is an energy-efficient technique, offering real-time response and privacy requirements for HAR applications. Rather than sending data to the cloud, edge computing is a local process that minimizes the data transmission time and responds with low latency. Recently, HAR system designers have adopted deep learning techniques inspired by their outstanding performance in many application areas and achieved relevant gain in activity recognition performance, however, these techniques were not demonstrated suitable for running on resource constrained devices. Thus, designing energy-efficient deep learning models is critical for realizing efficient HAR for mobile applications. In this work, we present a lightweight framework for the deployment of low-power but accurate HAR systems for these devices. We also implement and run the system in a microcontroller and analyze computational cost and energy consumption, and how different system configurations and deep learning model complexity influence on that.",https://ieeexplore.ieee.org/document/9541192/,IEEE Sensors Journal,"1 Nov.1, 2021",ieeexplore
10.1109/ACCESS.2019.2945337,A Linearization Model of Turbofan Engine for Intelligent Analysis Towards Industrial Internet of Things,IEEE,Journals,"Big data processing technologies, e.g., multi-sensor data fusion and cloud computing are being widely used in research, development, manufacturing, health monitoring and maintenance of aero-engines, driven by the ever-rapid development of intelligent manufacturing and Industrial Internet of Things (IIoT). This has promoted rapid development of the aircraft engine industry, increasing the aircraft engine safety, reliability and intelligence. At present, the aero-engine data computing and processing platform used in the industrial Internet of things is not complete, and the numerical calculation and control of aero-engine are inseparable from the linear model, while the existing aero-engine model linearization method is not accurate enough to quickly calculate the dynamic process parameters of the engine. Therefore, in this paper, we propose a linear model of turbofan engine for intelligent analysis in IIoT, with the aim to provide a new perspective for the analysis of engine dynamics. The construction of the proposed model includes three steps: First, a nonlinear mathematical model of a turbofan engine is established by adopting the component modeling approach. Then, numerous parameters of the turbofan engine components and their operating data are obtained by simulating various working conditions. Finally, based on the simulated data for the engine under these conditions, the model at the points during the dynamic process is linearized, such that a dynamic real-time linearized model of turbofan engine is obtained. Simulation results show that the proposed model can accurately depict the dynamic process of the turbofan engine and provide a valuable reference for designing the aero-engine control system and supporting intelligent analysis in IIoT.",https://ieeexplore.ieee.org/document/8856194/,IEEE Access,2019,ieeexplore
10.1109/ACCESS.2020.2996214,A Machine Learning Security Framework for Iot Systems,IEEE,Journals,"Internet of Things security is attracting a growing attention from both academic and industry communities. Indeed, IoT devices are prone to various security attacks varying from Denial of Service (DoS) to network intrusion and data leakage. This paper presents a novel machine learning (ML) based security framework that automatically copes with the expanding security aspects related to IoT domain. This framework leverages both Software Defined Networking (SDN) and Network Function Virtualization (NFV) enablers for mitigating different threats. This AI framework combines monitoring agent and AI-based reaction agent that use ML-Models divided into network patterns analysis, along with anomaly-based intrusion detection in IoT systems. The framework exploits the supervised learning, distributed data mining system and neural network for achieving its goals. Experiments results demonstrate the efficiency of the proposed scheme. In particular, the distribution of the attacks using the data mining approach is highly successful in detecting the attacks with high performance and low cost. Regarding our anomaly-based intrusion detection system (IDS) for IoT, we have evaluated the experiment in a real Smart building scenario using one-class SVM. The detection accuracy of anomalies achieved 99.71%. A feasibility study is conducted to identify the current potential solutions to be adopted and to promote the research towards the open challenges.",https://ieeexplore.ieee.org/document/9097876/,IEEE Access,2020,ieeexplore
10.1109/TR.2019.2895462,A Novel Class-Imbalance Learning Approach for Both Within-Project and Cross-Project Defect Prediction,IEEE,Journals,"Software defect prediction (SDP) is an available way to enhance test efficiency and guarantee software reliability. However, there are more clean instances than defective instances in real software projects, and this results in severe class distribution skews and gets the poor performance of classifiers. So solving the class-imbalance problem in SDP has attracted growing attention from industry and academia in software engineering. In this paper, we propose a novel class-imbalance learning approach for both within-project and cross-project class-imbalance problem. We utilize the thought of stratification embedded in nearest neighbor (STr-NN) to produce evolving training datasets with balanced data. For within-project, we directly employ the STr-NN approach for defect prediction. For cross-project, we first introduce transfer component analysis to mitigate the distribution differences between source and target dataset, and then employ the STr-NN approach on the transferred data. We conduct experiments on PROMISE and NASA datasets using ensemble learning based on weight vote. Experimental results indicate that our approach has higher area under curve (AUC), Recall and comparable probability of a false alarm (pf), and F-measure than some existing methods for the class-imbalance problem.",https://ieeexplore.ieee.org/document/8648214/,IEEE Transactions on Reliability,March 2020,ieeexplore
10.1109/ACCESS.2020.3046654,A Novel Context-Aware Mobile Application Recommendation Approach Based on Users Behavior Trajectories,IEEE,Journals,"With the rapid development of mobile internet technology, mobile applications (apps) have been rapidly popularized. To facilitate users' choice of apps, app recommendation is becoming a research hotspot in academia and industry. Although traditional app recommendation approaches have achieved certain results, these methods only mechanically consider the user's current context information, ignoring the impact of the user's previous related context on the user's current selection of apps. We believe this has hindered the further improvement of the recommendation effect. Based on this fact, this paper proposes a novel context-aware mobile application recommendation approach based on user behavior trajectories. We named this approach CMARA, which is the initials acronym of the proposed approach. Specifically, 1) CMARA integrates the heterogeneous information of the target users such as the user's app, time, and location, into users behavior trajectories to model the users' app usage preferences; 2) CMARA constructs the context Voronoi diagram using the users' contextual point and leverages the context Voronoi diagram to build a novel user similarity model; 3) CMARA uses the target user's current contextual information to generate an app recommendation list that meets the user's preferences. Through experiments on large-scale real-world data, we verified the effectiveness of CMARA.",https://ieeexplore.ieee.org/document/9303368/,IEEE Access,2021,ieeexplore
10.1109/ACCESS.2020.3000960,A Novel PPA Method for Fluid Pipeline Leak Detection Based on OPELM and Bidirectional LSTM,IEEE,Journals,"Pipeline leak detection has attracted great research interests for years in the energy industry. Continuous pressure monitoring is one of the most straightforward approaches in leak detection which utilizes pressure point analysis (PPA) algorithms to exploit the transient pressure characteristics and identify leak events. However, a critical issue that jeopardizes the deployment of PPA based methods is the high false alarm rate. In this paper, a novel PPA based leak detection method is proposed which can accurately detect the leak events and dramatically decrease the number of false alarms compared to existing methods. Firstly, the proposed method takes advantage of the good approximation ability and fast learning speed of optimally-pruned extreme learning machine (OPELM) to produce a preliminary leak detection result. Then, the strong memorizing ability of bidirectional long-short term memory (BiLSTM) network is exploited to identify the true positive from the preliminary detection result, hence significantly decrease the number of false alarms. Furthermore, a feature extraction mechanism is also proposed to obtain both the dynamic and static characteristics from raw pressure wave. Experiments and verifications are performed on different real world data sets obtained from pipeline leak tests. It shows that the proposed method can achieve higher detection accuracy with significantly less false alarms. It enhances the practicality of pressure monitoring based leak detection schemes.",https://ieeexplore.ieee.org/document/9110829/,IEEE Access,2020,ieeexplore
10.1109/ACCESS.2020.2990190,"A Novel Simulated-Annealing Based Electric Bus System Design, Simulation, and Analysis for Dehradun Smart City",IEEE,Journals,"Smart transportation network development with environmental issues into consideration has brought Industry 4.0 based solutions on priority. In this direction, battery-powered electric bus systems have been considered widely for ensuring flexibility, operation cost, and lesser pollutants emission. Industry 4.0 provides automation through a cyber-physical system (CPS), the interconnection of bus system entities with industrial internet-of-things (IIoT), remote information availability through cloud computing and scientific disciplines (human-computer interaction, artificial intelligence, machine learning etc.) integration. In this work, a discrete event-based simulation-optimization approach is integrated that take care of bus energy consumption according to real-time city's passenger needs and on-road friction levels. The proposed simulation optimization methodology utilizes multi-objective with dependent and independent variables for optimizing the overall system performance. In simulation optimization, objective functions are designed to tackle battery consumption, Internet-of-Thing (IoT) network performance, cloud operations efficiency and smart scientific discipline integration. Simulation parameters are based on a real-time bus system which is further analyzed, filtered and adapted as per the needs of the system. In another analysis, supercharger's capacities are varied to evaluate the performance of the proposed system and identify the low cost and efficient smart transportation system. Simulation results show different scenarios for variations in the number of buses, charging stations, bus-depots, mobile charging facilities, and bus-schedules. Simulation results show that the average passenger's waiting time in the waiting is (after ticket booking) varies between 0.2 minutes to 0.7 minutes in real-time traffic conditions. In similar traffic conditions, total passenger's time in system (ticket booking to travel) varies between 41.6 minutes (for 24 hours) to 45.5 minutes (for 1 year). In the simulation, priorities are given to those dependent and independent variables which save the battery consumption and elongate the utilization of buses. Lastly, it is also observed that the proposed system is suitable for resource-constraint devices because Gate Equivalent (GE) calculation shows that the proposed system can be implemented between 1986 GEs (communicational cost without confidentiality and authentication) and 7939 GEs (computational cost with HMAC for authentication in data storage). This ensures varies security primitivs such as confidentiality, availability and authentication.",https://ieeexplore.ieee.org/document/9078106/,IEEE Access,2020,ieeexplore
10.1109/TSG.2011.2159819,A Resilient Real-Time System Design for a Secure and Reconfigurable Power Grid,IEEE,Journals,"Energy infrastructure is a critical underpinning of modern society that any compromise or sabotage of its secure and reliable operation has an enormous impact on people's daily lives and the national economy. The massive northeastern power blackout of August 2003 and the most recent Florida blackout have both revealed serious defects in both system-level management and device-level designs of the power grid in handling attacks. At the system level, the control area operators lack the capability to 1) obtain real-time status information of the vastly distributed equipment; 2) respond rapidly enough once events start to unravel; and 3) perform coordinated actions autonomously across the region. At the device level, the traditional hardware lacks the capability to 1) provide reliable frequency and voltage control according to system demands and 2) rapidly reconfigure the system to a secure state through switches and power-electronics based devices. These blackouts were a wake-up call for both the industry and academia to consider new techniques and system architecture design that can help assure the security and reliability of the power grid. In this paper, we present a hardware-in-the-loop reconfigurable system design with embedded intelligence and resilient coordination schemes at both local and system levels that would tackle the vulnerabilities of the grid. The new system design consists of five key components: 1) a location-centric hybrid system architecture that facilitates not only distributed processing but also coordination among geographically close devices; 2) the insertion of intelligence into power electronic devices at the lower level of the power grid to enable a more direct reconfiguration of the physical makeup of the grid; 3) the development of a robust collaboration algorithm among neighboring devices to handle possible faulty, missing, or incomplete information; 4) the design of distributed algorithms to better understand the local state of the power grid; and 5) the adoption of a control-theoretic real-time adaptation strategy to guarantee the availability of large distributed systems. Preliminary evaluation results showing the advantages of each component are provided. A phased implementation plan is also suggested at the end of the discussion.",https://ieeexplore.ieee.org/document/6003812/,IEEE Transactions on Smart Grid,Dec. 2011,ieeexplore
10.1109/ACCESS.2020.3001277,"A Survey of Multi-Access Edge Computing in 5G and Beyond: Fundamentals, Technology Integration, and State-of-the-Art",IEEE,Journals,"Driven by the emergence of new compute-intensive applications and the vision of the Internet of Things (IoT), it is foreseen that the emerging 5G network will face an unprecedented increase in traffic volume and computation demands. However, end users mostly have limited storage capacities and finite processing capabilities, thus how to run compute-intensive applications on resource-constrained users has recently become a natural concern. Mobile edge computing (MEC), a key technology in the emerging fifth generation (5G) network, can optimize mobile resources by hosting compute-intensive applications, process large data before sending to the cloud, provide the cloud-computing capabilities within the radio access network (RAN) in close proximity to mobile users, and offer context-aware services with the help of RAN information. Therefore, MEC enables a wide variety of applications, where the real-time response is strictly required, e.g., driverless vehicles, augmented reality, robotics, and immerse media. Indeed, the paradigm shift from 4G to 5G could become a reality with the advent of new technological concepts. The successful realization of MEC in the 5G network is still in its infancy and demands for constant efforts from both academic and industry communities. In this survey, we first provide a holistic overview of MEC technology and its potential use cases and applications. Then, we outline up-to-date researches on the integration of MEC with the new technologies that will be deployed in 5G and beyond. We also summarize testbeds and experimental evaluations, and open source activities, for edge computing. We further summarize lessons learned from state-of-the-art research works as well as discuss challenges and potential future directions for MEC research.",https://ieeexplore.ieee.org/document/9113305/,IEEE Access,2020,ieeexplore
10.1109/ACCESS.2020.2974035,A Survey on Behavioral Pattern Mining From Sensor Data in Internet of Things,IEEE,Journals,"The deployment of large-scale wireless sensor networks (WSNs) for the Internet of Things (IoT) applications is increasing day-by-day, especially with the emergence of smart city services. The sensor data streams generated from these applications are largely dynamic, heterogeneous, and often geographically distributed over large areas. For high-value use in business, industry and services, these data streams must be mined to extract insightful knowledge, such as about monitoring (e.g., discovering certain behaviors over a deployed area) or network diagnostics (e.g., predicting faulty sensor nodes). However, due to the inherent constraints of sensor networks and application requirements, traditional data mining techniques cannot be directly used to mine IoT data streams efficiently and accurately in real-time. In the last decade, a number of works have been reported in the literature proposing behavioral pattern mining algorithms for sensor networks. This paper presents the technical challenges that need to be considered for mining sensor data. It then provides a thorough review of the mining techniques proposed in the recent literature to mine behavioral patterns from sensor data in IoT, and their characteristics and differences are highlighted and compared. We also propose a behavioral pattern mining framework for IoT and discuss possible future research directions in this area.",https://ieeexplore.ieee.org/document/8999541/,IEEE Access,2020,ieeexplore
10.1109/ACCESS.2021.3121302,A Survey on Industry 4.0 for the Oil and Gas Industry: Upstream Sector,IEEE,Journals,"The market volatility in the oil and gas (O&amp;G) sector, the dwindling demand for oil due to the impact of COVID-19, and the push for alternative greener energy are driving the need for innovation and digitization in the O&amp;G industry. This has attracted research interest from academia and the industry in the application of industry 4.0 (I4.0) technologies in the O&amp;G sector. The application of some of these I4.0 technologies has been presented in the literature, but the domain still lacks a comprehensive survey of the application of I4.0 in the O&amp;G upstream sector. This paper investigates the state-of-the-art efforts directed toward I4.0 technologies in the O&amp;G upstream sector. To achieve this, first, an overview of the I4.0 is discussed followed by a systematic literature review from an integrative perspective for publications between 2012–2021 with 223 analyzed documents. The benefits and challenges of the adoption of I4.0 have been identified. Moreover, the paper adds value by proposing a framework for the implementation of I4.0 in the O&amp;G upstream sector. Finally, future directions and research opportunities such as framework, edge computing, quantum computing, communication technologies, standardization, and innovative areas related to the implementation of I4.0 in the upstream sector are presented. The findings from this review show that I4.0 technologies are currently being explored and deployed for various aspects of the upstream sector. However, some of the I4.0 technologies like additive manufacturing and virtual reality are least explored.",https://ieeexplore.ieee.org/document/9579415/,IEEE Access,2021,ieeexplore
10.1109/ACCESS.2017.2678990,Adaptive Scheme for Caching YouTube Content in a Cellular Network: Machine Learning Approach,IEEE,Journals,"Content caching at base stations is a promising solution to address the large demands for mobile data services over cellular networks. Content caching is a challenging problem as it requires predicting the future popularity of the content and the operating characteristics of the cellular networks. In this paper, we focus on constructing an algorithm that improves the users' quality of experience (QoE) and reduces network traffic. The algorithm accounts for users' behavior and properties of the cellular network (e.g. cache size, bandwidth, and load). The constructed content and network aware adaptive caching scheme uses an extreme-learning machine neural network to estimate the popularity of content, and mixed-integer linear programming to compute where to place the content and select the physical cache sizes in the network. The proposed caching scheme simultaneously performs efficient cache deployment and content caching. Additionally, a simultaneous perturbation stochastic approximation method is developed to reduce the number of neurons in the extreme-learning machine method while ensuring a sufficient predictive performance is maintained. Using real-world data from YouTube and a NS-3 simulator, we demonstrate how the caching scheme improves the QoE of users and network performance compared with industry standard caching schemes.",https://ieeexplore.ieee.org/document/7873292/,IEEE Access,2017,ieeexplore
10.1109/ACCESS.2021.3111229,AdaptiveSystems: An Integrated Framework for Adaptive Systems Design and Development Using MPS JetBrains Domain-Specific Modeling Environment,IEEE,Journals,"This paper contains the design and development of an adaptive systems (<italic>AdaptiveSystems</italic> Domain-Specific Language - DSL) framework to assist language developers and data scientists in their attempt to apply Artificial Intelligence (AI) algorithms in several application domains. Big-data processing and AI algorithms are at the heart of autonomics research groups among industry and academia. Major advances in the field have traditionally focused on algorithmic research and increasing the performance of the developed algorithms. However, it has been recently recognized by the AI community that the applicability of these algorithms and their consideration in context is of paramount importance for their adoption. Current approaches to address AI in context lie in two areas: adaptive systems research that mainly focuses on implementing adaptivity mechanisms (technical perspective) and AI in context research that focuses on business aspects (business perspective). There is currently no approach that combines all aspects required from business considerations to an appropriate level of abstraction. In this paper, we attempt to address the problem of designing adaptive systems and therefore providing AI in context by utilising DSL technology. We propose a new DSL (<italic>AdaptiveSystems</italic>) and a methodology to apply this to the creation of a DSL for specific application domains such as <italic>AdaptiveVLE (Adaptive Virtual Learning Environment)</italic> DSL. The language developer will be able to instantiate the <italic>AdaptiveSystems</italic> DSL to any application domain by using the guidelines in this paper with an integrated path from design to implementation. The domain expert will then be able to use the developed DSL (e.g. <italic>AdaptiveVLE</italic> DSL) to design and develop their application. Future work will include extension and experimentation of the applicability of this work to more application domains within British Telecom (BT) and other areas such as health care, finance, etc.",https://ieeexplore.ieee.org/document/9531598/,IEEE Access,2021,ieeexplore
10.1109/TPAMI.2010.36,Age Synthesis and Estimation via Faces: A Survey,IEEE,Journals,"Human age, as an important personal trait, can be directly inferred by distinct patterns emerging from the facial appearance. Derived from rapid advances in computer graphics and machine vision, computer-based age synthesis and estimation via faces have become particularly prevalent topics recently because of their explosively emerging real-world applications, such as forensic art, electronic customer relationship management, security control and surveillance monitoring, biometrics, entertainment, and cosmetology. Age synthesis is defined to rerender a face image aesthetically with natural aging and rejuvenating effects on the individual face. Age estimation is defined to label a face image automatically with the exact age (year) or the age group (year range) of the individual face. Because of their particularity and complexity, both problems are attractive yet challenging to computer-based application system designers. Large efforts from both academia and industry have been devoted in the last a few decades. In this paper, we survey the complete state-of-the-art techniques in the face image-based age synthesis and estimation topics. Existing models, popular algorithms, system performances, technical difficulties, popular face aging databases, evaluation protocols, and promising future directions are also provided with systematic discussions.",https://ieeexplore.ieee.org/document/5406526/,IEEE Transactions on Pattern Analysis and Machine Intelligence,Nov. 2010,ieeexplore
10.1109/ACCESS.2020.2991474,An Adaptive Method for Inspecting Illumination of Color Intensity in Transparent Polyethylene Terephthalate Preforms,IEEE,Journals,"Machine vision systems are applied in industry to control the quality of production while optimizing efficiency. A machine vision and AI-based inspection of color intensity in transparent Polyethylene Terephthalate (PET) preforms is especially sensitive to backgrounds and lighting, therefore, much attention is given to its illumination conditions. The paper examines the adverse factors affecting the quality of image recognition and presents an adaptive method for reducing the influence of changing illumination conditions in the color inspection process of transparent PET preforms. The method is based on predicting measured color intensity correction parameters according to illumination conditions. To test this adaptive method, a hardware and software system for image capture and processing was developed. This system is capable of inspecting large quantities of preforms in real time using a neural network with a modified gradient descent and momentum algorithm. The experiment showed that correction of the measured color intensity value reduced the standard deviation caused by variable and uneven illumination by 61.51%, demonstrating that machine vision color intensity evaluation is a robust and adaptive solution under illuminated conditions for detecting abnormalities in machine-based PET inspection procedures.",https://ieeexplore.ieee.org/document/9082606/,IEEE Access,2020,ieeexplore
10.1109/TITS.2020.2980855,An Efficient and Scalable Simulation Model for Autonomous Vehicles With Economical Hardware,IEEE,Journals,"Autonomous vehicles rely on sophisticated hardware and software technologies for acquiring holistic awareness of their immediate surroundings. Deep learning methods have effectively equipped modern self-driving cars with high levels of such awareness. However, their application requires high-end computational hardware, which makes utilization infeasible for the legacy vehicles that constitute most of today's automotive industry. Hence, it becomes inherently challenging to achieve high performance while at the same time maintaining adequate computational complexity. In this paper, a monocular vision and scalar sensor-based model car is designed and implemented to accomplish autonomous driving on a specified track by employing a lightweight deep learning model. It can identify various traffic signs based on a vision sensor as well as avoid obstacles by using an ultrasonic sensor. The developed car utilizes a single Raspberry Pi as its computational unit. In addition, our work investigates the behavior of economical hardware used to deploy deep learning models. In particular, we herein propose a novel, computationally efficient, and cost-effective approach. The designed system can serve as a platform to facilitate the development of economical technologies for autonomous vehicles that can be used as part of intelligent transportation or advanced driver assistance systems. The experimental results indicate that this model can achieve real-time response on a resource-constrained device without significant overheads, thus making it a suitable candidate for autonomous driving in current intelligent transportation systems.",https://ieeexplore.ieee.org/document/9094331/,IEEE Transactions on Intelligent Transportation Systems,March 2021,ieeexplore
10.1109/TII.2019.2962029,An Entropy-Based Approach to Real-Time Information Extraction for Industry 4.0,IEEE,Journals,"Industry 4.0 has drawn considerable attention from industry and academic research communities. The recent advances in Internet of Things (IoT), Big Data analytics, sensor technology, and artificial intelligence have led to the design and implementation of novel approaches to take full advantage of data-driven solutions applicable to Industry 4.0. With the availability of large datasets, it has become crucially important to identify the appropriate amount of relevant information, which would optimize the overall analysis of the corresponding systems. In this article, specific properties of dynamically evolving data systems are introduced and investigated, which provide framework to assess the appropriate amount of representative information.",https://ieeexplore.ieee.org/document/8941297/,IEEE Transactions on Industrial Informatics,Sept. 2020,ieeexplore
10.1109/TSMC.1977.4309725,An Interactive Program Fo Conversational Elicitation of Decision Structures,IEEE,Journals,"An interactive computer program has been designed and implemented that elicits a decision tree from a decisionmaker in an English-like conversational mode. It emulates a decision analyst who guides the decisionmaker in structuring and organizing his knowledge about a particular problem domain. The objectives of the research were: 1) to provide the decision analysis industry with a practical automated tool for eliciting decision structures where manual elicitation techniques are either infeasible or uneconomical, 2) to cast the decision analyst's behavior into a formal framework in order to examine the principles governing the elicitation procedure and gain a deeper understanding of the analysis process itself, and 3) to provide experimental psychologists with an automated research tool for coding subjects' perception of problem situations into a standard and formal representation. The approach centers on the realization that the process of conducting an elicitation dialogue is structurally identical to conducting a heuristic search on game trees, as is commonly practiced in artificial intelligence programs. Heuristic search techniques, when applied to tree elicitation, permit real-time rollback and sensitivity analysis as the tree is being formulated. Thus it is possible to concentrate effort on expanding those parts of the tree which are crucial for the resolution of the solution plan. The program requires the decisionmaker to provide provisional values at each intermediate stage in the tree construction, which estimate the promise of future opportunities open to him from that stage.",https://ieeexplore.ieee.org/document/4309725/,"IEEE Transactions on Systems, Man, and Cybernetics",May 1977,ieeexplore
10.1109/TCIAIG.2012.2212194,AntBot: Ant Colonies for Video Games,IEEE,Journals,"The video game industry is an emerging market which continues to expand. From its early beginning, developers have focused mainly on sound and graphical applications, paying less attention to developing game bots or other kinds of nonplayer characters (NPCs). However, recent advances in artificial intelligence offer the possibility of developing game bots which are dynamically adjustable to several difficulty levels as well as variable game environments. Previous works reveal a lack of swarm intelligence approaches to develop these kinds of agents. Considering the potential of particle swarm optimization due to its emerging properties and self-adaptation to dynamic environments, further investigation into this field must be undertaken. This research focuses on developing a generic framework based on swarm intelligence, and in particular on ant colony optimization, such as it allows general implementation of real-time bots that work over dynamic game environments. The framework has been adapted to allow the implementation of intelligent agents for the classical game Ms. Pac-Man. These were trialed at the Ms. Pac-Man competitions held during the 2011 International Congress on Evolutionary Computation.",https://ieeexplore.ieee.org/document/6262464/,IEEE Transactions on Computational Intelligence and AI in Games,Dec. 2012,ieeexplore
10.1109/TMECH.2013.2253116,Approaching Servoclass Tracking Performance by a Proportional Valve-Controlled System,IEEE,Journals,"An industry-grade proportional valve is much cheaper and rugged than a servovalve. A feedforward controller for a proportional valved system has been developed here to achieve tracking controls beyond 1 Hz that are usually attained by servovalves. For compensating the higher nonlinearities, feedforward controllers have been designed offline by proposing appropriate static models for friction and valve flow and executing the supporting experiments. These controllers have been implemented with real-time PID feedback of only the piston displacement. Excellent tracking performance has been obtained up to 2 Hz that has deteriorated with an increase in cylinder friction.",https://ieeexplore.ieee.org/document/6494307/,IEEE/ASME Transactions on Mechatronics,Aug. 2013,ieeexplore
10.1109/JAS.2020.1003021,Artificial intelligence applications in the development of autonomous vehicles: a survey,IEEE,Journals,"The advancement of artificial intelligence (AI) has truly stimulated the development and deployment of autonomous vehicles (AVs) in the transportation industry. Fueled by big data from various sensing devices and advanced computing resources, AI has become an essential component of AVs for perceiving the surrounding environment and making appropriate decision in motion. To achieve goal of full automation (i.e., self-driving), it is important to know how AI works in AV systems. Existing research have made great efforts in investigating different aspects of applying AI in AV development. However, few studies have offered the research community a thorough examination of current practices in implementing AI in AVs. Thus, this paper aims to shorten the gap by providing a comprehensive survey of key studies in this research avenue. Specifically, it intends to analyze their use of AIs in supporting the primary applications in AVs: 1) perception; 2) localization and mapping; and 3) decision making. It investigates the current practices to understand how AI can be used and what are the challenges and issues associated with their implementation. Based on the exploration of current practices and technology advances, this paper further provides insights into potential opportunities regarding the use of AI in conjunction with other emerging technologies: 1) high definition maps, big data, and high performance computing; 2) augmented reality (AR)/virtual reality (VR) enhanced simulation platform; and 3) 5G communication for connected AVs. This paper is expected to offer a quick reference for researchers interested in understanding the use of AI in AV research.",https://ieeexplore.ieee.org/document/9016391/,IEEE/CAA Journal of Automatica Sinica,March 2020,ieeexplore
10.1109/TII.2018.2816971,Automatic Selection of Optimal Parameters Based on Simple Soft-Computing Methods: A Case Study of Micromilling Processes,IEEE,Journals,"Nowadays, the application of novel soft-computing methods to new industrial processes is often limited by the actual capacity of the industry to assimilate state-of-the-art computational methods. The selection of optimal parameters for efficient operation is very challenging in microscale manufacturing processes, because of intrinsic nonlinear behavior and reduced dimensions. In this paper, a decision-making system for selecting optimal parameters in micromilling operations is designed and implemented using simple and efficient soft-computing techniques. The procedure primarily consists of four steps: an experimental characterization; the modeling of cutting force and surface roughness by means of a multilayer perceptron; multiobjective optimization using the cross-entropy method, taking into account productivity and surface quality; and a decision-making procedure for selecting the most appropriate parameters using a fuzzy inference system. Finally, two different alloys for micromilling processes are considered, in order to evaluate the proposed system: a titanium-based alloy and a tungsten-copper alloy. The experimental study demonstrated the effectiveness of the proposed solution for automated decision-making, based on simple soft-computing methods, and its successful application to a real-life industrial challenge.",https://ieeexplore.ieee.org/document/8325494/,IEEE Transactions on Industrial Informatics,Feb. 2019,ieeexplore
10.1109/TSMCC.2004.843236,Automatic system for quality-based classification of marble textures,IEEE,Journals,"In this paper, we present an automatic system and algorithms for the classification of marble slabs into different groups in real time in production line, according to slabs quality. The application of the system is aimed at the marble industry, in order to automate and improve the manual classification process of marble slabs carried out at present. The system consists of a mechatronic prototype, which houses all the required physical components for the acquisition of marble slabs images in suitable light conditions, and computational algorithms, which are used to analyze the color texture of the marble surfaces and classify them into their corresponding group. In order to evaluate the color representation influence on the image analysis, four color spaces have been tested: RGB, XYZ, YIQ, and K-L. After the texture analysis performed with the sum and difference histograms algorithm, a feature extraction process has been implemented with principal component analysis. Finally, a multilayer perceptron neural network trained with the backpropagation algorithm with adaptive learning rate is used to classify the marble slabs in three categories, according to their quality. The results (successful classification rate of 98.9%) show very high performance compared with the traditional (manual) system.",https://ieeexplore.ieee.org/document/1522532/,"IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)",Nov. 2005,ieeexplore
10.1109/ACCESS.2020.2993010,Bearing Intelligent Fault Diagnosis in the Industrial Internet of Things Context: A Lightweight Convolutional Neural Network,IEEE,Journals,"The advancement of Industry 4.0 and Industrial Internet of Things (IIoT) has laid more emphasis on reducing the parameter amount and storage space of the model in addition to the automatic and accurate fault diagnosis. In this case, this paper proposes a lightweight convolutional neural network (LCNN) method for intelligent fault diagnosis of bearing, which can largely satisfy the need of less parameter amount and storage space as well as high accuracy. First, depthiwise separable convolution is adopted, and a LCNN structure is constructed through an inverse residual structure and a linear bottleneck layer operation. Secondly, a novel decomposed Hierarchical Search Space is introduced to automatically explore the optimal LCNN for bearing fault diagnosis in the context of the IIoT. In the meantime, the real-time monitoring and fault diagnosis of the model are also deployed. In order to verify the validity of the designed model, Case Western Reserve University Bearing fault dataset and MFPT bearing fault dataset are adopted. The results demonstrate the great advantages of the model. The LCNN model can automatically learn and select the appropriate features, highly improving the fault diagnosis accuracy. Meanwhile, the computational and storage costs of the model are largely reduced, which contributes to its being applied to the mechanical system in the IIoT context.",https://ieeexplore.ieee.org/document/9088980/,IEEE Access,2020,ieeexplore
10.1109/ACCESS.2021.3106797,CNC Machine Tool Fault Diagnosis Integrated Rescheduling Approach Supported by Digital Twin-Driven Interaction and Cooperation Framework,IEEE,Journals,"The problems of CNC machine tool (CNCMT) fault diagnosis and production rescheduling have attracted continuous attention because of their great significance to the manufacturing industry. Digital twin is a supporting technology for achieving smart manufacturing and provides a new paradigm for solving these problems. This paper explores a digital twin-driven interaction and cooperation framework and proposes the architecture and implementation mechanism to enable the sharing of data, knowledge, and resource, to realize the fusion of physical space and cyber space, and to improve the accuracy of fault diagnosis. Under this framework, aiming at the influence of CNCMT failure on the initial production planning, a self-adaptation rescheduling method based on Monte Carlo Tree Search (MCTS) algorithm is proposed to provide support for developing more efficient production planning. Finally, the effectiveness of the proposed framework is validated by experimental study. The framework and integrated rescheduling approach can provide guidance for enterprises in implementing CNCMT maintenance and production scheduling to meet high accuracy and reliability requirements.",https://ieeexplore.ieee.org/document/9520390/,IEEE Access,2021,ieeexplore
10.1109/ACCESS.2016.2619719,Comparing Oversampling Techniques to Handle the Class Imbalance Problem: A Customer Churn Prediction Case Study,IEEE,Journals,"Customer retention is a major issue for various service-based organizations particularly telecom industry, wherein predictive models for observing the behavior of customers are one of the great instruments in customer retention process and inferring the future behavior of the customers. However, the performances of predictive models are greatly affected when the real-world data set is highly imbalanced. A data set is called imbalanced if the samples size from one class is very much smaller or larger than the other classes. The most commonly used technique is over/under sampling for handling the class-imbalance problem (CIP) in various domains. In this paper, we survey six well-known sampling techniques and compare the performances of these key techniques, i.e., mega-trend diffusion function (MTDF), synthetic minority oversampling technique, adaptive synthetic sampling approach, couples top-N reverse k-nearest neighbor, majority weighted minority oversampling technique, and immune centroids oversampling technique. Moreover, this paper also reveals the evaluation of four rules-generation algorithms (the learning from example module, version 2 (LEM2), covering, exhaustive, and genetic algorithms) using publicly available data sets. The empirical results demonstrate that the overall predictive performance of MTDF and rules-generation based on genetic algorithms performed the best as compared with the rest of the evaluated oversampling methods and rule-generation algorithms.",https://ieeexplore.ieee.org/document/7707454/,IEEE Access,2016,ieeexplore
10.1109/TDEI.2020.009070,Condition Monitoring Based on Partial Discharge Diagnostics Using Machine Learning Methods: A Comprehensive State-of-the-Art Review,IEEE,Journals,"This paper presents a state-of-the-art review on machine learning (ML) based intelligent diagnostics that have been applied for partial discharge (PD) detection, localization, and pattern recognition. ML techniques, particularly those developed in the last five years, are examined and classified as conventional ML or deep learning (DL). Important features of each method, such as types of input signal, sampling rate, core methodology, and accuracy, are summarized and compared in detail. Advantages and disadvantages of different ML algorithms are discussed. Moreover, technical roadblocks preventing intelligent PD diagnostics from being applied to industry are identified, such as insufficient/imbalanced dataset, data inconsistency, and difficulties in cost-effective real-time deployment. Finally, potential solutions are proposed, and future research directions are suggested.",https://ieeexplore.ieee.org/document/9293208/,IEEE Transactions on Dielectrics and Electrical Insulation,December 2020,ieeexplore
10.1109/TASE.2020.3010536,Condition-Driven Data Analytics and Monitoring for Wide-Range Nonstationary and Transient Continuous Processes,IEEE,Journals,"Frequent and wide changes in operation conditions are quite common in real process industry, resulting in typical wide-range nonstationary and transient characteristics along time direction. The considerable challenge is, thus, how to solve the conflict between the learning model accuracy and change complexity for analysis and monitoring of nonstationary and transient continuous processes. In this work, a novel condition-driven data analytics method is developed to handle this problem. A condition-driven data reorganization strategy is designed which can neatly restore the time-wise nonstationary and transient process into different condition slices, revealing similar process characteristics within the same condition slice. Process analytics can then be conducted for the new analysis unit. On the one hand, coarse-grained automatic condition-mode division is implemented with slow feature analysis to track the changing operation characteristics along condition dimension. On the other hand, fine-grained distribution evaluation is performed for each condition mode with Gaussian mixture model. Bayesian inference-based distance (BID) monitoring indices are defined which can clearly indicate the fault effects and distinguish different operation scenarios with meaningful physical interpretation. A case study on a real industrial process shows the feasibility of the proposed method which, thus, can be generalized to other continuous processes with typical wide-range nonstationary and transient characteristics along time direction. <italic>Note to Practitioners</italic>—Industrial processes in general have nonstationary characteristics which are ubiquitous in real world data, often reflected by a time-variant mean, a time-variant autocovariance, or both resulting from various factors. The focus of this study is to develop a universal analytics and monitoring method for wide-range nonstationary and transient continuous processes. Condition-driven concept takes the place of time-driven thought. For the first time, it is recognized that there are similar process characteristics within the same condition slice and changes in the process correlations may relate to its condition modes. Besides, the proposed method can provide enhanced physical interpretation for the monitoring results with concurrent analysis of the static and dynamic information which carry different information, analogous to the concepts of “position” and “velocity” in physics, respectively. The static information can tell the current operation condition, while the dynamic information can clarify whether the process status is switching between different steady states. It is noted that the condition-driven concept is universal and can be extended to other applications for industrial manufacturing applications.",https://ieeexplore.ieee.org/document/9158352/,IEEE Transactions on Automation Science and Engineering,Oct. 2021,ieeexplore
10.1109/ACCESS.2021.3108624,Convolutional Neural Network Based Approval Prediction of Enhancement Reports,IEEE,Journals,"For a given software enhancement report, identifying its possible approval status could help software developers by suggesting feature enhancements to compete in the software industry. An automatic solution for the approval prediction of enhancements could assist all the participants in resolving enhancements. The key challenges are the preprocessing of noisy textual information and the state-of-the-art feature models to combine the syntactical and semantic word information available in the given text. To this end, we propose a deep learning based approach for the approval prediction of enhancement reports that incorporates the users' sentiments involved in the text. First, we preprocess the textual information of all enhancement reports to avoid noise. Second, we compute the sentiment of each enhancement report using Senti4SD. Third, we combine the bag-of-words (BOW) representation and traditional word2vec based representation to learn the novel deep representation (a recurrent neural network (RNN) with attention based representation) of preprocessed text. Using an attention mechanism enables the model to remember the context over a long sequence of words in an enhancement report. Fourth, based on sentiment and deep representation, we train a deep learning based classifier for the approval prediction of enhancement reports. Finally, we reuse the 40, 000 enhancement reports from 10 real software applications to evaluate the proposed approach. The cross-application evaluation suggests that the proposed approach is accurate and outperforms the state-of-the-art. The results of the proposed approach improve the precision from 86.52% to 90.56%, recall from 66.45% to 80.10%, and f-measure from 78.12% to 85.01%.",https://ieeexplore.ieee.org/document/9524733/,IEEE Access,2021,ieeexplore
10.1109/TII.2019.2942800,Cyber Vulnerability Intelligence for Internet of Things Binary,IEEE,Journals,"Internet of Things (IoT) integrates a variety of software (e.g., autonomous vehicles and military systems) in order to enable the advanced and intelligent services. These software increase the potential of cyber-attacks because an adversary can launch an attack using system vulnerabilities. Existing software vulnerability analysis methods used to be relying on human experts crafted features, which usually miss many vulnerabilities. It is important to develop an automatic vulnerability analysis system to improve the countermeasures. However, source code is not always available (e.g., most IoT related industry software are closed source). Therefore, vulnerability detection on binary code is a demanding task. This article addresses the automatic binary-level software vulnerability detection problem by proposing a deep learning-based approach. The proposed approach consists of two phases: binary function extraction, and model building. First, we extract binary functions from the cleaned binary instructions obtained by using IDA Pro. Then, we employ the attention mechanism on top of a bidirectional long short-term memory for building the predictive model. To show the effectiveness of the proposed approach, we have collected datasets from several different sources. We have compared our proposed approach with a series of baselines including source code-based techniques and binary code-based techniques. We have also applied the proposed approach to real-world IoT related software such as VLC media player and LibTIFF project that used on Autonomous Vehicles. Experimental results show that our proposed approach betters the baselines and is able to detect more vulnerabilities.",https://ieeexplore.ieee.org/document/8892533/,IEEE Transactions on Industrial Informatics,March 2020,ieeexplore
10.1109/TII.2020.3028612,DWFCAT: Dual Watermarking Framework for Industrial Image Authentication and Tamper Localization,IEEE,Journals,"The image data received through various sensors are of significant importance in Industry 4.0. Unfortunately, these data are highly vulnerable to various malicious attacks during its transit to the destination. Although the use of pervasive edge computing (PEC) with the Internet of Things (IoT) has solved various issues, such as latency, proximity, and real-time processing, but the security and authentication of data between the nodes is still a significant concern in PEC-based industrial-IoT scenarios. In this article, we present “DWFCAT,” a dual watermarking framework for content authentication and tamper localization for industrial images. The robust and fragile watermarks along with overhead bits related to the cover image for tamper localization are embedded in different planes of the cover image. We have used discrete cosine transform coefficients and exploited their energy compaction property for robust watermark embedding. We make use of a four-point neighborhood to predict the value of a predefined pixel and use it for embedding the fragile watermark bits in the spatial domain. Chaotic and deoxyribonucleic acid encryption is used to encrypt the robust watermark before embedding to enhance its security. The results indicate that DWFCAT can withstand a range of hybrid signal processing and geometric attacks, such as Gaussian noise, salt and pepper, joint photographic experts group (JPEG) compression, rotation, low-pass filtering, resizing, cropping, sharpening, and histogram equalization. The experimental results prove that the DWFCAT is highly efficient compared with the various state-of-the-art approaches for authentication and tamper localization of industrial images.",https://ieeexplore.ieee.org/document/9214433/,IEEE Transactions on Industrial Informatics,July 2021,ieeexplore
10.1109/JSEN.2008.926923,Data Processing Method Applying Principal Component Analysis and Spectral Angle Mapper for Imaging Spectroscopic Sensors,IEEE,Journals,"A data processing method to classify hyperspectral images from an imaging spectroscopic sensor is evaluated. Each image contains the whole diffuse reflectance spectra of the analyzed material for all the spatial positions along a specific line of vision. The implemented linear algorithm comes to solve real time constrains typical of industrial systems. This processing method is composed of two blocks: data compression is performed by means of principal component analysis (PCA) and the spectral interpretation algorithm for classification is the spectral angle mapper (SAM). This strategy, applying PCA and SAM, has been successfully tested for online raw material sorting in the tobacco industry, where the desired raw material (tobacco leaves) should be discriminated from other unwanted spurious materials, such as plastic, cardboard, leather, feathers, candy paper, etc. Hyperspectral images are recorded by a sensor consisting of a monochromatic camera and a passive prism-grating-prism device. Performance results are compared with a spectral interpretation algorithm based on artificial neural networks (ANN).",https://ieeexplore.ieee.org/document/4567472/,IEEE Sensors Journal,July 2008,ieeexplore
10.1109/ACCESS.2021.3051583,Data-Driven Condition Monitoring of Mining Mobile Machinery in Non-Stationary Operations Using Wireless Accelerometer Sensor Modules,IEEE,Journals,"This paper presents the development of an easy-to-deploy and smart monitoring IoT system that utilizes vibration measurement devices to assess real-time condition of bulldozers, power shovels and backhoes, in non-stationary operations in the mining industry. According to operating experience data and the type of mining machine, total loss failure rates per machine fleet can reach up to 30%. Vibration analysis techniques are commonly used for condition monitoring and early detection of unforeseen failures to generate predictive maintenance plans for heavy machinery. However, this maintenance strategy is intensively used only for stationary machines and/or mobile machinery in stationary operations. Today, there is a lack of proper solutions to detect and prevent critical failures for non-stationary machinery. This paper shows a cost-effective solution proposal for implementing a vibration sensor network with wireless communication and machine learning data-driven capabilities for condition monitoring of non-stationary heavy machinery in mining operations. During the machine operation, 3-axis accelerations were measured using two sensors deployed across the machine. The machine accelerations (amplitudes and frequencies) are measured in two different frequency spectrums to improve each sensing location's time resolution. Multiple machine learning algorithms use this machine data to assess conditions according to manufacturer recommendations and operational benchmarks Proposed data-driven machine learning models classify the machine condition in states according to the ISO 2372 standards for vibration severity: Good, Acceptable, Unsatisfactory, or Unacceptable. After performing field tests with bulldozers and backhoes from different manufacturers, the machine learning algorithms are able to classify machine health status with an accuracy between 85% - 95%. Moreover, the system allows early detection of “Unacceptable” states between 120 to 170 hours prior to critical failure. These results demonstrate that the proposed system will collect relevant data to generate predictive maintenance plans and avoid unplanned downtimes.",https://ieeexplore.ieee.org/document/9324826/,IEEE Access,2021,ieeexplore
10.1109/ACCESS.2020.2998940,Data-Driven Estimation of Heavy-Truck Residual Value at the Buy-Back,IEEE,Journals,"In a context of deep transformation of the entire automotive industry, starting from pervasive and native connectivity, commercial vehicles (heavy, light, and buses) are generating and transmitting much more data than passenger cars, with a much higher expected value, motivated by the higher costs of the vehicles and their added-value related businesses, such as logistics, freight, and transportation management. This paper presents a data-driven and unsupervised methodology to provide a descriptive model assessing the residual value estimates of heavy trucks subject to buy-back. A huge amount of telematics data characterizing the actual usage of commercial vehicles is jointly analyzed with different external conditions (e.g., altimetry), affecting the truck's performance to estimate the devaluation of the vehicle at the buy-back. The proposed approach has been evaluated on a large set of real-world heavy trucks to demonstrate its effectiveness in correctly assessing the real status of wear and residual value at the end of leasing contracts, to provide a few and quantitative insights through an informative, interactive and user-friendly dashboard to make a proper decision on the next business strategies to be adopted. The proposed solution has already been deployed by a private company within its data analytics services since (1) an interpretable descriptive model of the main factors/parameters and corresponding weights affecting the residual value is provided and (2) the experimental results confirmed the promising outcomes of the proposed data-driven methodology.",https://ieeexplore.ieee.org/document/9104708/,IEEE Access,2020,ieeexplore
10.1109/ACCESS.2021.3101284,"Data-Driven Remaining Useful Life Estimation for Milling Process: Sensors, Algorithms, Datasets, and Future Directions",IEEE,Journals,"An increase in unplanned downtime of machines disrupts and degrades the industrial business, which results in substantial credibility damage and monetary loss. The cutting tool is a critical asset of the milling machine; the failure of the cutting tool causes a loss in industrial productivity due to unplanned downtime. In such cases, a proper predictive maintenance strategy by real-time health monitoring of cutting tools becomes essential. Accurately predicting the useful life of equipment plays a vital role in the predictive maintenance arena of industry 4.0. Many active research efforts have been done to estimate tool life in varied directions. However, the consolidated study of the implemented techniques and future pathways is still missing. So, the purpose of this paper is to provide a systematic and comprehensive literature survey on the data-driven approach of Remaining Useful Life (RUL) estimation of cutting tools during the milling process. The authors have summarized different monitoring techniques, feature extraction methods, decision-making models, and available sensors currently used in the data-driven model. The authors have also presented publicly available datasets related to milling under various operating conditions to compare the accuracy of the prediction model for tool wear estimation. Finally, the article concluded with the challenges, limitations, recent advancements in RUL prognostics techniques using Artificial Intelligence (AI), and future research scope to explore more in this area.",https://ieeexplore.ieee.org/document/9502093/,IEEE Access,2021,ieeexplore
10.1109/TCSS.2020.2987846,Deep Correlation Mining Based on Hierarchical Hybrid Networks for Heterogeneous Big Data Recommendations,IEEE,Journals,"The advancement of several significant technologies, such as artificial intelligence, cyber intelligence, and machine learning, has made big data penetrate not only into the industry and academic field but also our daily life along with a variety of cyber-enabled applications. In this article, we focus on a deep correlation mining method in heterogeneous big data environments. A hierarchical hybrid network (HHN) model is constructed to describe multitype relationships among different entities, and a series of measures are defined to quantify the internal correlations within one specific layer or external correlations between different layers. An intelligent router based on deep reinforcement learning framework is designed to generate optimal actions to route across the HHN. An improved random walk with the restart-based algorithm is then developed with the intelligent router, based on the hierarchical influence across network associated with multiple correlations. An intelligent recommendation mechanism is finally designed and applied to support users' collaboration works in scholarly big data environments. Experiments based on DBLP and ResearchGate data show the practicability and usefulness of our model and method.",https://ieeexplore.ieee.org/document/9090327/,IEEE Transactions on Computational Social Systems,Feb. 2021,ieeexplore
10.1109/TII.2018.2807797,Deep Endoscope: Intelligent Duct Inspection for the Avionic Industry,IEEE,Journals,"We present the first autonomous endoscope for the visual inspection of very small ducts and cavities, up to a 6-mm diameter. The system has been designed, implemented, and tested in a challenging industrial scenario and in strict collaboration with an avionic industry partner. The inspected objects are metallic gearboxes eventually presenting different residuals (e.g., sand, machining swarfs, and metallic dust) inside the oil ducts. The automatic system is actuated by a robotic arm that moves the endoscope with a microcamera inside the gearbox duct, while a deep-learning-based spatio-temporal image analysis module detects, classifies, and localizes defects in real time. Feedback is given to the robotic arm in order to move or extract the endoscope given the detected anomalies. Evaluation provides a detection rate of nearly 98% given different tests with different types of residuals and duct structures.",https://ieeexplore.ieee.org/document/8295126/,IEEE Transactions on Industrial Informatics,April 2018,ieeexplore
10.1109/ACCESS.2019.2924030,Demand Response Management for Industrial Facilities: A Deep Reinforcement Learning Approach,IEEE,Journals,"As a major consumer of energy, the industrial sector must assume the responsibility for improving energy efficiency and reducing carbon emissions. However, most existing studies on industrial energy management are suffering from modeling complex industrial processes. To address this issue, a model-free demand response (DR) scheme for industrial facilities was developed. In practical terms, we first formulated the Markov decision process (MDP) for industrial DR, which presents the composition of the state, action, and reward function in detail. Then, we designed an actor-critic-based deep reinforcement learning algorithm to determine the optimal energy management policy, where both the actor (Policy) and the critic (Value function) are implemented by the deep neural network. We then confirmed the validity of our scheme by applying it to a real-world industry. Our algorithm identified an optimal energy consumption schedule, reducing energy costs without compromising production.",https://ieeexplore.ieee.org/document/8742652/,IEEE Access,2019,ieeexplore
10.1109/ACCESS.2020.2980736,Design and Evaluation of a Reliable Low-Cost Atmospheric Pollution Station in Urban Environment,IEEE,Journals,"The pollution of the air constitutes an environmental risk to health, crops, animals, forests and water. There are several policies for reducing air pollution regarding industry, energy, transportation, and agriculture. Unfortunately, there is limited monitoring of the air quality in cities and rural areas for supervising the accomplishment of these policies. Reliable monitoring of air pollutants is, typically, based on expensive fixed stations, which constitutes a barrier to tackle. This research presents the design, implementation and evaluation of a small, low-cost, station for monitoring atmospheric pollution. The prototype registers ozone (O<sub>3</sub>) and carbon monoxide (CO) using inexpensive sensors. To assure high reliability of the measurements obtained by the sensors installed in this station, it is proposed a calibration procedure based on the selection of the best performance analysis of the following machine learning techniques: multiple linear regression, artificial neural networks, and random forest. Additionally, a decision rule is implemented to select an optimal combination of sensors for the estimation models, while the sample timestamp is considered as a temporal heuristic at the input of the system, assuming similarities in the daily environmental dynamics. In order to test the station in a realistic scenario, the calibration and evaluation sets were taken in two different time frames of one and two months, respectively. The overall process was implemented with reference data coming from a certified air quality fixed station in the city of Cuenca - Ecuador. Experimental results showed that the real-time reports of ozone provided by the prototype are quite similar to the fixed station during the evaluation period, with a resulting correlation of up to r = 0.92 and r = 0.91 in the calibration and evaluation set, respectively. However, signal drift and aging in CO<sub>x</sub> sensors diminished the accuracy of carbon monoxide calibration models, resulting in lower correlation (r ≤ 0.76) with the evaluation set.",https://ieeexplore.ieee.org/document/9035500/,IEEE Access,2020,ieeexplore
10.1109/ACCESS.2020.2991225,Design and Performance Evaluation of an AI-Based W-Band Suspicious Object Detection System for Moving Persons in the IoT Paradigm,IEEE,Journals,"The threat of terrorism has spread all over the world, and the situation has become grave. Suspicious object detection in the Internet of Things (IoT) is an effective way to respond to global terrorist attacks. The traditional solution requires performing security checks one by one at the entrance of each gate, resulting in bottlenecks and crowding. In the IoT paradigm, it is necessary to be able to perform suspicious object detection on moving people. Artificial intelligence (AI) and millimeter-wave imaging are advanced technologies in the global security field. However, suspicious object detection for moving persons in the IoT, which requires the integration of many different imaging technologies, is still a challenge in both academia and industry. Furthermore, increasing the recognition rate of suspicious objects and controlling network congestion are two main issues for such a suspicious object detection system. In this paper, an AI-based W-band suspicious object detection system for moving persons in the IoT paradigm is designed and implemented. In this system, we establish a suspicious object database to support AI technology for improving the probability of identifying suspicious objects. Moreover, we propose an efficient transmission mechanism to reduce system network congestion since a massive amount of data will be generated by 4K cameras during real-time monitoring. The evaluation results indicate that the advantages and efficiency of the proposed scheme are significant.",https://ieeexplore.ieee.org/document/9081933/,IEEE Access,2020,ieeexplore
10.1109/ACCESS.2020.2998723,"Digital Twin for the Oil and Gas Industry: Overview, Research Trends, Opportunities, and Challenges",IEEE,Journals,"With the emergence of industry 4.0, the oil and gas (O&amp;G) industry is now considering a range of digital technologies to enhance productivity, efficiency, and safety of their operations while minimizing capital and operating costs, health and environment risks, and variability in the O&amp;G project life cycles. The deployment of emerging technologies allows O&amp;G companies to construct digital twins (DT) of their assets. Considering DT adoption, the O&amp;G industry is still at an early stage with implementations limited to isolated and selective applications instead of industry-wide implementation, limiting the benefits from DT implementation. To gain the full potential of DT and related technological adoption, a comprehensive understanding of DT technology, the current status of O&amp;G-related DT research activities, and the opportunities and challenges associated with the deployment of DT in the O&amp;G industry are of paramount importance. In order to develop this understanding, this paper presents a literature review of DT within the context of the O&amp;G industry. The paper follows a systematic approach to select articles for the literature review. First, a keywords-based publication search was performed on the scientific databases such as Elsevier, IEEE Xplore, OnePetro, Scopus, and Springer. The filtered articles were then analyzed using online text analytic software (Voyant Tools) followed by a manual review of the abstract, introduction and conclusion sections to select the most relevant articles for our study. These articles and the industrial publications cited by them were thoroughly reviewed to present a comprehensive overview of DT technology and to identify current research status, opportunities and challenges of DT deployment in the O&amp;G industry. From this literature review, it was found that asset integrity monitoring, project planning, and life cycle management are the key application areas of digital twin in the O&amp;G industry while cyber security, lack of standardization, and uncertainty in scope and focus are the key challenges of DT deployment in the O&amp;G industry. When considering the geographical distribution for the DT related research in the O&amp;G industry, the United States (US) is the leading country, followed by Norway, United Kingdom (UK), Canada, China, Italy, Netherland, Brazil, Germany, and Saudi Arabia. The overall publication rate was less than ten articles (approximately) per year until 2017, and a significant increase occurred in 2018 and 2019. The number of journal publications was noticeably lower than the number of conference publications, and the majority of the publications presented theoretical concepts rather than the industrial implementations. Both these observations suggest that the DT implementation in the O&amp;G industry is still at an early stage.",https://ieeexplore.ieee.org/document/9104682/,IEEE Access,2020,ieeexplore
10.1109/JPROC.2020.2998530,"Digital Twin in the IoT Context: A Survey on Technical Features, Scenarios, and Architectural Models",IEEE,Journals,"Digital twin (DT) is an emerging concept that is gaining attention in various industries. It refers to the ability to clone a physical object (PO) into a software counterpart. The softwarized object, termed logical object, reflects all the important properties and characteristics of the original object within a specific application context. To fully determine the expected properties of the DT, this article surveys the state-of-the-art starting from the original definition within the manufacturing industry. It takes into account related proposals emerging in other fields, namely augmented and virtual reality (e.g., avatars), multiagent systems, and virtualization. This survey thereby allows for the identification of an extensive set of DT features that point to the “softwarization” of POs. To properly consolidate a shared DT definition, a set of foundational properties is identified and proposed as a common ground outlining the essential characteristics (must-haves) of a DT. Once the DT definition has been consolidated, its technical and business value is discussed in terms of applicability and opportunities. Four application scenarios illustrate how the DT concept can be used and how some industries are applying it. The scenarios also lead to a generic DT architectural model. This analysis is then complemented by the identification of software architecture models and guidelines in order to present a general functional framework for the DT. This article, eventually, analyses a set of possible evolution paths for the DT considering its possible usage as a major enabler for the softwarization process.",https://ieeexplore.ieee.org/document/9120192/,Proceedings of the IEEE,Oct. 2020,ieeexplore
10.1109/ACCESS.2021.3120843,Digital Twins From Smart Manufacturing to Smart Cities: A Survey,IEEE,Journals,"Digital twins are quickly becoming a popular tool in several domains, taking advantage of recent advancements in the Internet of Things, Machine Learning and Big Data, while being used by both the industry sector and the research community. In this paper, we review the current research landscape as regards digital twins in the field of smart cities, while also attempting to draw parallels with the application of digital twins in Industry 4.0. Although digital twins have received considerable attention in the Industrial Internet of Things domain, their utilization in smart cities has not been as popular thus far. We discuss here the open challenges in the field and argue that digital twins in smart cities should be treated differently and be considered as cyber-physical “systems of systems”, due to the vastly different system size, complexity and requirements, when compared to other recent applications of digital twins. We also argue that researchers should utilize established tools and methods of the smart city community, such as co-creation, to better handle the specificities of this domain in practice.",https://ieeexplore.ieee.org/document/9576739/,IEEE Access,2021,ieeexplore
10.1109/TSE.2013.2295827,Effects of Developer Experience on Learning and Applying Unit Test-Driven Development,IEEE,Journals,"Unit test-driven development (UTDD) is a software development practice where unit test cases are specified iteratively and incrementally before production code. In the last years, researchers have conducted several studies within academia and industry on the effectiveness of this software development practice. They have investigated its utility as compared to other development techniques, focusing mainly on code quality and productivity. This quasi-experiment analyzes the influence of the developers' experience level on the ability to learn and apply UTDD. The ability to apply UTDD is measured in terms of process conformance and development time. From the research point of view, our goal is to evaluate how difficult is learning UTDD by professionals without any prior experience in this technique. From the industrial point of view, the goal is to evaluate the possibility of using this software development practice as an effective solution to take into account in real projects. Our results suggest that skilled developers are able to quickly learn the UTDD concepts and, after practicing them for a short while, become as effective in performing small programming tasks as compared to more traditional test-last development techniques. Junior programmers differ only in their ability to discover the best design, and this translates into a performance penalty since they need to revise their design choices more frequently than senior programmers.",https://ieeexplore.ieee.org/document/6690135/,IEEE Transactions on Software Engineering,April 2014,ieeexplore
10.1109/ACCESS.2020.2998581,Efficiency Improvement of Function Point-Based Software Size Estimation With Deep Learning Model,IEEE,Journals,"Software cost estimation is crucial to software management, which has received considerable attention from both industry and academia. Software size is an important metric that forms the cornerstone of software cost estimation. The function point has been proven to be a useful software size unit for size estimation and has been successfully implemented in many countries. However, in current practice, the rule of function point size method is complicated and performed manually. Consequently, it is costly in both time and resources spent to apply these methods, especially in the scenario of large-scale software development in the industry. In this paper, a deep learning-based named entity recognition (NER) model was designed in place of manual function point recognition. In particular, a BiLSTM-CRF model was trained on previously labeled requirements in the industry to classify the function point type of new requirements in the same domain. The proposed method was verified on 29 real projects provided by our industry partner. A comparative experiment was designed for the quantitative evaluation of efficiency improvement of the proposed NER model aided function point estimation. The result suggests that, for the NER model, the precision and F1 of the BiLSTM-CRF-based function point analysis on test samples achieved 94.5% and 80.3%, respectively. Moreover, the improvement in the efficiency of the software size estimation process achieved an average of 38.6%, which is a significant enhancement for the function point-based software size estimation.",https://ieeexplore.ieee.org/document/9103508/,IEEE Access,2021,ieeexplore
10.1109/ACCESS.2020.3033725,Efficient Deep Learning Bot Detection in Games Using Time Windows and Long Short-Term Memory (LSTM),IEEE,Journals,"Bots in video games has been gaining the interest of industry as well as academia as a problem that has been enabled by the recent advances in deep learning and reinforcement learning. In turn several studies have attempted to establish bot detectors in various video games. In this article, we introduce a bot detection model that can implemented in real-time and provide feedback on whether a player that is being observed is a bot or human. The model uses a limited feature set and amount of time of observation in order to be small and generalize easily to other domains. We trained and tested our model in a series of replays for Starcraft: Brood War and have yielded a higher accuracy than past studies and a fraction of detection time.",https://ieeexplore.ieee.org/document/9239256/,IEEE Access,2020,ieeexplore
10.1109/TII.2021.3049405,Enabling Secure Authentication in Industrial IoT With Transfer Learning Empowered Blockchain,IEEE,Journals,"Industrial Internet of Things (IIoT) is ushering in huge development opportunities in the era of Industry 4.0. However, there are significant data security and privacy challenges during automatic and real-time data collection, monitoring for industrial applications in IIoT. Data security and privacy in IIoT applications are closely related to the reliability of users, which is determined by user authentication that have been widely used as an effective approach. However, the existing user authentication mechanisms in IIoT suffer from single factor authentication and poor adaptability with the rapid growth of the number of users and the diversity of user categories. To solve the aforementioned issues, this article proposes a novel Authentication mechanism based on Transfer Learning empowered Blockchain, coined ATLB. In ATLB, blockchains are applied to achieve the privacy preservation for industrial applications. In addition, by introducing the transfer learning based authentication mechanism, trustworthy blockchains are built such that the privacy preservation for industrial applications is further enhanced. Specifically, ATLB first employs a guiding deep deterministic policy gradient algorithm to train the user authentication model of a specific region, which is then transferred locally for foreign user authentication or cross-regionally for another region's user authentication such that the model training time is significantly reduced. Experimental results show that the proposed ATLB not only provides accurate authentications for IIoT applications but also achieves high throughput and low latency.",https://ieeexplore.ieee.org/document/9314211/,IEEE Transactions on Industrial Informatics,Nov. 2021,ieeexplore
10.1109/ACCESS.2021.3101397,Energy-Efficient Edge-Fog-Cloud Architecture for IoT-Based Smart Agriculture Environment,IEEE,Journals,"The current agriculture systems compete to take advantage of industry advanced technologies, including the internet of things (IoT), cloud/fog/edge computing, artificial intelligence, and agricultural robots to monitor, track, analyze and process various functions and services in real-time. Additionally, these technologies can make the agricultural processes smarter and more cost-efficient by using automated systems and eliminating any human interventions, hence enhancing agricultural production to meet future expectations. Although the current agriculture systems that adopt the traditional cloud-based architecture have provided powerful computing infrastructure to distributed IoT sensors. However, the cost of energy consumption associated with transferring heterogeneous data over the multiple network tiers to process, analyze and store the sensor's information in the cloud has created a huge load on information and communication infrastructure. Besides, the energy consumed by cloud data centers has an environmental impact associated with using non-clean fuels, which usually release carbon emissions (CO<sub>2</sub>) to produce electricity. Thus, to tackle these issues, we propose a new integrated edge-fog-cloud architectural paradigm that promises to enhance the energy-efficient of smart agriculture systems and corresponding carbon emissions. This architecture allows data collection from several sensors to process and analyze the agriculture data that require real-time operation (e.g., weather temperature, soil moisture, soil acidity, irrigation, etc.) in several layers (edge, fog, and cloud). Thus, the real-time processing could be held by the edge and fog layers to reduce the load on the cloud layer, which will help to enhance the overall energy consumption and process the agriculture applications/services efficiently. Mathematical modeling is conducted using mixed-integer linear programming (MILP) for a smart agriculture environment, where the proposed architecture is implemented, and results are analyzed and compared to the traditional implementation. According to the results of thousands of agriculture sensors, the proposed architecture outperforms the traditional cloud-based architecture in terms of reducing the overall energy consumption by 36% and the carbon emissions by 43%. In addition to these achievements, the results show that our proposed architecture can reduce network traffic by up to 86%, which can reduce network congestion. Finally, we develop a heuristic algorithm to validate and mimic the presented approach, and it shows comparable results to the MILP model.",https://ieeexplore.ieee.org/document/9502114/,IEEE Access,2021,ieeexplore
10.1109/ACCESS.2020.3047343,Establishing Trust in Online Advertising With Signed Transactions,IEEE,Journals,"Programmatic advertising operates one of the most sophisticated and efficient service platforms on the Internet. However, the complexity of this ecosystem is a direct cause of one of the most important problems in online advertising, the lack of transparency. This lack of transparency enables subsequent problems such as advertising fraud, which causes billions of dollars in losses. In this paper we propose Ads.chain, a technological solution to the lack-of-transparency problem in programmatic advertising. Ads.chain extends the current effort of the Internet Advertising Bureau (IAB) in providing traceability in online advertising through the Ads.txt and Ads.cert solutions, addressing the limitations of these techniques. Ads.chain is (to the best of the authors' knowledge) the first solution that provides end-to-end cryptographic traceability at the ad transaction level. It is a communication protocol that can be seamlessly embedded into ad-tags and the OpenRTB protocol, the de-facto standards for communications in online advertising, allowing an incremental adoption by the industry. We have implemented Ads.chain and made the code publicly available. We assess the performance of Ads.chain through a thorough analysis in a lab environment that emulates a real ad delivery process at real-life throughputs. The obtained results show that Ads.chain can be implemented with limited impact on the hardware resources and marginal delay increments at the publishers lower than 0.20 milliseconds per ad space on webpages and 2.6 milliseconds at the programmatic advertising platforms. These results confirm that Ads.chain's impact on the user experience and the overall operation of the programmatic ad delivery process can be considered negligible.",https://ieeexplore.ieee.org/document/9306812/,IEEE Access,2021,ieeexplore
10.1109/TII.2020.2988208,Fault Description Based Attribute Transfer for Zero-Sample Industrial Fault Diagnosis,IEEE,Journals,"In this article, a challenging fault diagnosis task is studied, in which no samples of the target faults are available for the model training. This scenario has hardly been studied in industrial research. But it is a common problem that massive fault samples are not available for the target faults, which limits the successes of conventional data-driven approaches in practical application. Here, we introduce the idea of zero-shot learning into the industry field, and tackle the zero-sample fault diagnosis task by proposing the fault description based attribute transfer method. Specifically, the method learns to determine the fault categories using the human-defined fault descriptions instead of the collected fault samples.The defined description consists of arbitrary attributes of the faults, including the fault positions, the consequences of the fault, and even the cause of the fault, etc. For the attribute knowledge of target faults, they can be prelearned and transferred from some readily available faults occurred in the same process. Afterwards, the target faults can be diagnosed based on the defined fault descriptions without the need for any additional data based training. Besides, the supervised principle component analysis is adopted in our method to extract the attribute related features to offer an effective attribute learning. We analyze and interpret the feasibility of the fault description based method theoretically. Also, the zero-sample fault diagnosis experiments are designed and conducted on the benchmark Tennessee-Eastman process and the real thermal power plant process to validate the effectiveness. The results show that it is indeed possible to diagnose target faults without their samples.",https://ieeexplore.ieee.org/document/9072621/,IEEE Transactions on Industrial Informatics,March 2021,ieeexplore
10.1109/TII.2019.2937876,Federated Tensor Mining for Secure Industrial Internet of Things,IEEE,Journals,"In a vertical industry alliance, Internet of Things (IoT) deployed in different smart factories are similar. For example, most automobile manufacturers have the similar assembly lines and IoT surveillance systems. It is common to observe the industrial knowledge using deep learning and data mining methods based on the IoT data. However, some knowledge is not easy to be mined from only one factory's data because the samples are still few. If multiple factories within an alliance can gather their data together, more knowledge could be mined. However, the key concern of these factories is the data security. Existing matrix-based methods can guarantee the data security inside a factory but do not allow the data sharing among factories, and thus their mining performance is poor due to lack of correlation. To address this concern, in this article we propose the novel federated tensor mining (FTM) framework to federate multisource data together for tensor-based mining while guaranteeing the security. The key contribution of FTM is that every factory only needs to share its ciphertext data for security issue, and these ciphertexts are adequate for tensor-based knowledge mining due to its homomorphic attribution. Real-data-driven simulations demonstrate that FTM not only mines the same knowledge compared with the plaintext mining, but also is enabled to defend the attacks from distributed eavesdroppers and centralized hackers. In our typical experiment, compared with the matrix-based privacy-preserving compressive sensing (PPCS), FTM increases up to 24% on mining accuracy.",https://ieeexplore.ieee.org/document/8815886/,IEEE Transactions on Industrial Informatics,March 2020,ieeexplore
10.1109/ACCESS.2021.3080237,"Fog-Centric IoT Based Framework for Healthcare Monitoring, Management and Early Warning System",IEEE,Journals,"Internet of things (IoT) and machine learning based systems incorporating smart wearable technology are rapidly evolving to monitor and manage healthcare and physical activities. This paper is focused on the proposition of a fog-centric wireless, real-time, smart wearable and IoT-based framework for ubiquitous health and fitness analysis in a smart gym environment. The proposed framework aims to aid in the health and fitness industry based on body vitals, body movement and health related data. The framework is expected to assist athletes, trainers and physicians with the interpretation of multiple physical signs and raise alerts in case of any health hazard. We proposed a method to collect and analyze exercise specific data which can be used to measure exercise intensity and its benefit to athlete's health and serve as recommendation system for upcoming athletes. We determined the validity of the proposed framework by giving a six weeks workout plan with six days a week for workout activity targeting all muscles followed by one day for recovery. We recorded the electrocardiogram, heart rate, heart rate variability, breath rate, and determined athlete's movement using a 3D-acceleration. The collected data in the research is used in two modules. A Health zone module implemented on body vitals data which categorizes athlete's health state into various categories. Hzone module is responsible for health hazards identification and alarming. Outstandingly, the Hzone module is able to identify an athlete's physical state with 97% accuracy. A gym activity recognition (GAR) module is implemented to recognize workout activity in real-time using body movements and body vitals data. The purpose of the GAR module is to collect and analyze exercise specific data. The GAR module achieved an accuracy of above 89% on athlete independent model based on muscle group.",https://ieeexplore.ieee.org/document/9430526/,IEEE Access,2021,ieeexplore
10.1109/TDSC.2018.2800048,GaitLock: Protect Virtual and Augmented Reality Headsets Using Gait,IEEE,Journals,"With the fast penetration of commercial Virtual Reality (VR) and Augmented Reality (AR) systems into our daily life, the security issues of those devices have attracted significant interests from both academia and industry. Modern VR/AR systems typically use head-mounted devices (i.e., headsets) to interact with users, and often store private user data, e.g., social network accounts, online transactions or even payment information. This poses significant security threats, since in practice the headset can be potentially obtained and accessed by unauthenticated parties, e.g., identity thieves, and thus cause catastrophic breach. In this paper, we propose a novel GaitLock system, which can reliably authenticate users using their gait signatures. Our system doesn't require extra hardware, e.g., fingerprint sensors or retina scanners, but only uses the on-board inertial measurement units (IMUs) equipped in almost all mainstream VR/AR headsets to authenticate the legitimate users from intruders, by simply asking them to walk a few steps. To achieve that, we propose a new gait recognition model Dynamic-SRC, which combines the strength of Dynamic Time Warping (DTW) and Sparse Representation Classifier (SRC), to extract unique gait patterns from the inertial signals during walking. We implement GaitLock on Google Glass (a typical AR headset), and extensive experiments show that GaitLock outperforms the state-of-the-art systems significantly in recognition accuracy (&gt; 98 percent success in 5 steps), and is able to run in-situ on the resource-constrained VR/AR headsets without incurring high energy cost.",https://ieeexplore.ieee.org/document/8276563/,IEEE Transactions on Dependable and Secure Computing,1 May-June 2019,ieeexplore
10.1109/ACCESS.2021.3101647,Gas Path Fault Diagnosis of Gas Turbine Engine Based on Knowledge Data-Driven Artificial Intelligence Algorithm,IEEE,Journals,"As the core power for the aviation industry, shipbuilding industry, and power station industry, it is essential to ensure that the gas turbines operate safely, reliably, greenly and efficiently. Learn from the advantages and disadvantages of the thermodynamic model based and data-driven artificial intelligence based gas-path diagnosis methods, a newfangled gas turbine gas-path diagnosis approach on the basis of knowledge data-driven artificial intelligence is proposed. That is a hybrid method of deep learning and gas path analysis. First, gas turbine thermodynamic model of the object to be diagnosed is constructed by adaptation modeling strategy. And the engine thermodynamic model is taken as the basal model to simulate various gas path faults. Secondly, a large number of knowledge data corresponding to component health parameters and gas turbine boundary condition parameters &amp; gas-path measurable parameters are simulated by setting different component health parameter values and different boundary conditions based on this basal model. And next, define the vector composed of the boundary condition parameters &amp; the gas path measurable parameters in the knowledge database as the input vector, and the component health parameter vector as the output vector, and a deep learning model for regression modeling of this knowledge database is designed. At last, along with the gas turbine engine runs, the trained model outputs component health parameters in real time after trained deep learning model is deployed to the corresponding gas turbine power plant. The simulation experiment results show that, accurate and quantified health parameters of each gas path component can be obtained by the proposed method in this paper, and the overall root mean square error does not exceed 0.033%, and the maximum relative error does not exceed 0.36%, which illustrates the proposed method has great application potential.",https://ieeexplore.ieee.org/document/9502707/,IEEE Access,2021,ieeexplore
10.1109/TII.2020.3034627,Guest Editorial: Special Section on Advanced Signal Processing and AI Technologies for Industrial Big Data,IEEE,Journals,The papers in this special section focus on advanced signal processing and artificial intelligence (AI) technologies for industrial Big Data (IBD) powered by Industry 4.0. Modern industry has evolved from the traditional manufacturing industry to digital and intelligent industry. Huge amount of complex real-time data are generated from the thousands of industrial sensors in physical and man-made environments. Industrial big data (IBD) afford us an unprecedented opportunity to obtain an in-depth understanding of Internet of Things and facilitate data-driven approaches for industrial optimization and scheduling. The papers in this section collect the latest ideas and research on advanced signal processing and artificial intelligence (AI) technologies for IBD.,https://ieeexplore.ieee.org/document/9361683/,IEEE Transactions on Industrial Informatics,May 2021,ieeexplore
10.1109/ACCESS.2021.3103680,Human Centric Digital Transformation and Operator 4.0 for the Oil and Gas Industry,IEEE,Journals,"Working at an oil and gas facility, such as a drilling rig, production facility, processing facility, or storage facility, involves various challenges, including health and safety risks. It is possible to leverage emerging digital technologies such as smart sensors, wearable or mobile devices, big data analytics, cloud computing, extended reality technologies, robotic systems, and drones to mitigate the challenges faced by oil and gas workers. While these technologies are not new to the oil and gas industry, most of its existing digital transformation initiatives follow business or process-centric approaches, in which the critical driver of the technology adoption is the enhancement of production, efficiency, and revenue. As a result, they may not address the challenges faced by the workers. As oil and gas workers are among the essential assets in the oil and gas industry, it is vital to address the challenges faced by these workers. This paper proposes a human-centric digital transformational framework for the oil and gas industry to deploy existing digital technologies to enhance their workers' health, safety, and working conditions. The paper outlines the critical challenges faced by oilfield workers, introduces a system architecture to implements a human-centric digital transformation, discusses the opportunities of the proposed framework, and summarizes the key impediment for the proposed framework.",https://ieeexplore.ieee.org/document/9509417/,IEEE Access,2021,ieeexplore
10.1109/TNNLS.2011.2179309,Hybrid Neural Prediction and Optimized Adjustment for Coke Oven Gas System in Steel Industry,IEEE,Journals,"An energy system is the one of most important parts of the steel industry, and its reasonable operation exhibits a critical impact on manufacturing cost, energy security, and natural environment. With respect to the operation optimization problem for coke oven gas, a two-phase data-driven based forecasting and optimized adjusting method is proposed, where a Gaussian process-based echo states network is established to predict the gas real-time flow and the gasholder level in the prediction phase. Then, using the predicted gas flow and gasholder level, we develop a certain heuristic to quantify the user's optimal gas adjustment. The proposed operation measure has been verified to be effective by experimenting with the real-world on-line energy data sets coming from Shanghai Baosteel Corporation, Ltd., China. At present, the scheduling software developed with the proposed model and ensuing algorithms have been applied to the production practice of Baosteel. The application effects indicate that the software system can largely improve the real-time prediction accuracy of the gas units and provide with the optimized gas balance direction for the energy optimization.",https://ieeexplore.ieee.org/document/6126048/,IEEE Transactions on Neural Networks and Learning Systems,March 2012,ieeexplore
10.1109/TVT.2021.3099129,Hybrid Policy Learning for Energy-Latency Tradeoff in MEC-Assisted VR Video Service,IEEE,Journals,"Virtual reality (VR) is promising to fundamentally transform a broad spectrum of industry sectors and the way humans interact with virtual content. However, despite unprecedented progress, current networking and computing infrastructures are incompetent to unlock VR's full potential. In this paper, we consider delivering the wireless multi-tile VR video service over a mobile edge computing (MEC) network. The primary goal is to minimize the system latency/energy consumption and to arrive at a tradeoff thereof. To this end, we first cast the time-varying view popularity as a model-free Markov chain to effectively capture its dynamic characteristics. After jointly assessing the caching and computing capacities on both the MEC server and the VR playback device, a hybrid policy is then implemented to coordinate the dynamic caching replacement and the deterministic offloading, so as to fully utilize the system resources. The underlying multi-objective problem is reformulated as a partially observable Markov decision process, and a deep deterministic policy gradient algorithm is proposed to iteratively learn its solution, where a long short-term memory neural network is embedded to continuously predict the dynamics of the unobservable popularity. Simulation results demonstrate the superiority of the proposed scheme in achieving a trade-off between the energy efficiency and the latency reduction over the baseline methods.",https://ieeexplore.ieee.org/document/9495190/,IEEE Transactions on Vehicular Technology,Sept. 2021,ieeexplore
10.1109/ACCESS.2017.2783118,IEEE Access Special Section Editorial: Health Informatics for the Developing World,IEEE,Journals,"We live in a world with growing disparity in the quality of life available to people in the developed and developing countries. Healthcare in the developing world is fraught with numerous problems such as the lack of health infrastructure, and human resources, which results in very limited health coverage. The field of health informatics has made great strides in recent years towards improving public health systems in the developing world by augmenting them with state-of-the-art information and communication technologies (ICT). Through real-world deployment of these technologies, there is real hope that the health industry in the developing world will progress from its current, largely dysfunctional state to one that is more effective, personalized, and cost effective. Health informatics can usher a new era of personalized health analytics, with the potential to transform healthcare in the developing world. In conjunction with mHealth and eHealth, many other important health informatics trends—such as artificial intelligence (AI), machine learning (ML), big data, crowdsourcing, cloud computing—are also emerging. Exponentially growing heterogeneous data, with the help of big data analytics, has the potential to provide descriptive, predictive, and prescriptive health insights as well as enable new applications such as telemedicine and remote diagnostics and surgery. Such systems could enhance the overall process of monitoring, diagnosis, and prognosis of diseases.",https://ieeexplore.ieee.org/document/8262687/,IEEE Access,2017,ieeexplore
10.1109/ACCESS.2020.3004145,IETCR: An Information Entropy Based Test Case Reduction Strategy for Mutation-Based Fault Localization,IEEE,Journals,"Mutation-based fault localization (MBFL) is a recently proposed technique with the advantage of high fault localization accuracy. However, such a mutation analysis based technique is difficult to be accepted by industry due to its huge computational cost on mutation analysis. There are three ways to improve MBFL's efficiency, which are reducing the number of mutants, optimizing the mutants' execution process, and reducing the number of test cases. The former two ways have been mainly studied and shown promising results, but for the latter way, the related studies are limited since this kind of method will reduce the precision of MBFL. In this paper, we mainly focus on the latter way and propose an information entropy based test case reduction (IETCR) strategy for MBFL. In particular, we first calculate the entropy change of test cases and select a proportion of them according to their value. Then we use a reduced test suite to execute mutants. To show the effectiveness of the IETCR strategy, we choose six real-world programs with 112 faulty versions. In terms of mutation reduction rate, we find MBFL with the IETCR strategy can reduce 56.3%~88.3% cost while keeping almost the same fault localization accuracy when compared to the original MBFL without test case reduction. Moreover, we use Wilcoxon signed-rank test for statistical analysis, which shows that there is no statistically significant difference between MBFL with IETCR strategy and the original MBFL.",https://ieeexplore.ieee.org/document/9122504/,IEEE Access,2020,ieeexplore
10.1147/JRD.2016.2630478,IT troubleshooting with drift analysis in the DevOps era,IBM,Journals,"Over the past few years, DevOps practices have led to many changes in the software industry. The need for agility has resulted in continuous development and deployment of frequent small updates in IT production systems. However, the ever-changing applications and their IT operations environments challenge existing IT troubleshooting approaches, which generally depend on prebuilt domain knowledge and ignore the frequent changes in the DevOps era. Moreover, the complexity and diversity of application architectures exacerbate the challenges. In this paper, we propose an unsupervised learning based drift analysis tool named CHASER to detect and analyze abnormal changes (referred to as “drifts,” which include configuration errors, processes hanging, etc.), with learned change models and patterns in real time as well as in the root cause analysis. First, we categorize the changes into two distinct groups (static and dynamic state changes) and periodically collect the finer grained changes. Then, we extract the time-series and structural features from these changes and apply statistical and machine learning algorithms to learn models and patterns from historical data. Furthermore, we apply these models and patterns to detect drifts in real time and infer possible root causes of reported errors based on a multidimensional correlation approach to improve the precision. Through experiments and case studies, we demonstrate the capability of CHASER.",https://ieeexplore.ieee.org/document/7877287/,IBM Journal of Research and Development,1 Jan.-Feb. 2017,ieeexplore
10.1109/ACCESS.2018.2809681,Industrial Big Data Analytics for Prediction of Remaining Useful Life Based on Deep Learning,IEEE,Journals,"Due to the recent development of cyber-physical systems, big data, cloud computing, and industrial wireless networks, a new era of industrial big data is introduced. Deep learning, which brought a revolutionary change in computer vision, natural language processing, and a variety of other applications, has significant potential for solutions providing in sophisticated industrial applications. In this paper, a concept of device electrocardiogram (DECG) is presented, and an algorithm based on deep denoising autoencoder (DDA) and regression operation is proposed for the prediction of the remaining useful life of industrial equipment. First, the concept of electrocardiogram is explained. Then, a problem statement based on manufacturing scenario is presented. Subsequently, the architecture of the proposed algorithm called integrated DDA and the algorithm workflow are provided. Moreover, DECG is compared with traditional factory information system, and the feasibility and effectiveness of the proposed algorithm are validated experimentally. The proposed concept and algorithm combine typical industrial scenario and advance artificial intelligence, which has great potential to accelerate the implementation of industry 4.0.",https://ieeexplore.ieee.org/document/8302913/,IEEE Access,2018,ieeexplore
10.1109/TITS.2020.2997832,Intelligent Edge Computing in Internet of Vehicles: A Joint Computation Offloading and Caching Solution,IEEE,Journals,"Recently, Internet of Vehicles (IoV) has become one of the most active research fields in both academic and industry, which exploits resources of vehicles and Road Side Units (RSUs) to execute various vehicular applications. Due to the increasing number of vehicles and the asymmetrical distribution of traffic flows, it is essential for the network operator to design intelligent offloading strategies to improve network performance and provide high-quality services for users. However, the lack of global information and the time-variety of IoVs make it challenging to perform effective offloading and caching decisions under long-term energy constraints of RSUs. Since Artificial Intelligence (AI) and machine learning can greatly enhance the intelligence and the performance of IoVs, we push AI inspired computing, caching and communication resources to the proximity of smart vehicles, which jointly enable RSU peer offloading, vehicle-to-RSU offloading and content caching in the IoV framework. A Mix Integer Non-Linear Programming (MINLP) problem is formulated to minimize total network delay, consisting of communication delay, computation delay, network congestion delay and content downloading delay of all users. Then, we develop an online multi-decision making scheme (named OMEN) by leveraging Lyapunov optimization method to solve the formulated problem, and prove that OMEN achieves near-optimal performance. Leveraging strong cognition of AI, we put forward an imitation learning enabled branch-and-bound solution in edge intelligent IoVs to speed up the problem solving process with few training samples. Experimental results based on real-world traffic data demonstrate that our proposed method outperforms other methods from various aspects.",https://ieeexplore.ieee.org/document/9109630/,IEEE Transactions on Intelligent Transportation Systems,April 2021,ieeexplore
10.1109/ACCESS.2019.2909114,Intelligent Transportation and Control Systems Using Data Mining and Machine Learning Techniques: A Comprehensive Study,IEEE,Journals,"Traffic congestion is becoming the issues of the entire globe. This study aims to explore and review the data mining and machine learning technologies adopted in research and industry to attempt to overcome the direct and indirect traffic issues on humanity and societies. The study's methodology is to comprehensively review around 165 studies, criticize, and categorize all these studies into a chronological and understandable category. The study is focusing on the traffic management approaches that were depended on data mining and machine learning technologies to detect and predict the traffic only. This study has found that there is no standard traffic management approach that the community of traffic management has agreed on. This study is important to the traffic research communities, traffic software companies, and traffic government officials. It has a direct impact on drawing a clear path for new traffic management propositions. This study is one of the largest studies with respect to the size of its reviewed articles that were focused on data mining and machine learning. Additionally, this study will draw general attention to a new traffic management proposition approach.",https://ieeexplore.ieee.org/document/8681028/,IEEE Access,2019,ieeexplore
10.1109/ACCESS.2018.2877175,"Internet of Too Many Things in Smart Transport: The Problem, The Side Effects and The Solution",IEEE,Journals,"The Internet of Things (IoT) involves embedding electronics, software, sensors, and actuators into physical devices, such as vehicles, buildings, and a wide range of smart devices. Network connectivity allows IoT devices to collect and exchange data. The prevalence of IoT devices has increased rapidly in last five years, driven by cheaper electronics and a desire to monitor and control the physical world. We introduce the concept of IoT flood to describe the increased use of IoT devices. Just like the data deluge, the IoT flood has potential benefits and risks. This paper focuses on the hidden side effects of the increased usage of IoT, such as energy consumption, physical pollution, radiation, and health issues. We indicate that an evaluation system with carefully designed metrics reflecting the impact of IoT flood with input from academic, industry, and government is required. We propose some practical measures that can reduce the IoT flood, such as common platforms and data sharing to reduce the side effects. This paper demonstrates the IoT flood problem and potential solutions by examining the intelligent transport system domain where IoT is being deployed to solve problems related to time efficiency and energy consumption through smart mobility.",https://ieeexplore.ieee.org/document/8506609/,IEEE Access,2018,ieeexplore
10.1109/ACCESS.2019.2912012,Knowledge Based Recommender System for Academia Using Machine Learning: A Case Study on Higher Education Landscape of Pakistan,IEEE,Journals,"Allocation of courses and research students based on faculty's subject specialization and area of interest has always remained a challenging task for university administration due to the presence of academics' cross-domain interests, stale faculty resumes at university portals and changing the skill set demands from the industry. Collaborative filtering and content-based recommender systems have already been in use by the industry for recommending things, such as movies, news, restaurants, and shopping items to the users, and however, no one has utilized these off-the-shelf models for enhancing the student experience and improving the quality of higher education in academia. This paper presents a case study showcasing the use of probabilistic topic models for generating recommendations to users in academia through appropriate course allocation and supervisor assignment. The proposed system coined as ScholarLite harnesses the power of machine learning to extract research themes from faculty members' past publications, mines research interests from their resumes, and combines it with their educational background to generate recommendations for course teaching, research supervision, and industry-academia collaboration. We have shown the recommendation results on real-world data gathered from the higher education commission of the country and demonstrated that the proposed techniques are scalable across various programs offered by the universities and could be deployed in a small budget by universities for automating course and supervisor allocation procedures. The experiments confirm our performance expectation by showing good relevance and objectivity in results, thus making this decision management system more appealing for large-scale deployment and use by academia.",https://ieeexplore.ieee.org/document/8693719/,IEEE Access,2019,ieeexplore
10.1109/ACCESS.2021.3087493,Learning a Diagnostic Strategy on Medical Data With Deep Reinforcement Learning,IEEE,Journals,"In recent years, Artificial Intelligence based disease diagnosis has drawn considerable attention both in academia and industry. In medical scenarios, a well-trained classifier can effectively detect a disease with sufficient features associating with medical tests. However, such features are not always readily available due to the high cost of time and money associating with medical tests. To address this, this study identifies the diagnostic strategy learning problem and proposes a novel framework consisting of three components to learn a diagnostic strategy with limited features. First, as we often encounter incomplete medical records of the patients, a sequence encoder is designed to encode any set of information in various sizes into fixed-length vectors. Second, taking the output of the encoder as the input, a feature selector based on reinforcement learning techniques is proposed to learn the best feature sequence for diagnosis. Finally, with the best feature sequence, an oracle classifier is used to give the final diagnosis. To evaluate the performance of the proposed method, experiments are conducted on nine real medical datasets. The results suggest that the proposed method is effective for providing personalized diagnostic strategies and makes better diagnoses with fewer features compared with existing methods.",https://ieeexplore.ieee.org/document/9448279/,IEEE Access,2021,ieeexplore
10.1109/TIV.2020.3002505,Learning to Drive by Imitation: An Overview of Deep Behavior Cloning Methods,IEEE,Journals,"There is currently a huge interest around autonomous vehicles from both industry and academia. This is mainly due to recent advances in machine learning and deep learning, allowing the development of promising methods for autonomous driving. The gap toward full autonomy is incrementally being reduced with essentially three main existing approaches. First, Modular systems that combine a pipeline of methods with each solving one specific sub-task of driving. Second, Direct Perception techniques that directly estimate affordances (car orientation, distances between lane borders, etc) used to compute control commands through a simple logic. Finally, end-to-end frameworks that automatically map raw sensor data to actuation values. The objective of this paper is to review some recent works focusing on end-to-end deep learning models for lane stable driving, as well as some publicly available real world datasets and open-source simulators that enable the development and evaluation of such methods.",https://ieeexplore.ieee.org/document/9117169/,IEEE Transactions on Intelligent Vehicles,June 2021,ieeexplore
10.1109/ACCESS.2020.3008289,Learning-Based IoT Data Aggregation for Disaster Scenarios,IEEE,Journals,"Industrial Internet of Everything (IIoE), as the deep integration of industry 6.0, the Internet of Things (IoT) and 6G mobile communication technology, pave the way for intelligent industry, enabling industrial optimization and automation. To ensure the high quality of services (QoS) in IIoE, tremendous real-time information generated by the pervasive smart things needs to be aggregated and processed quickly and reliably. However, a large-scale disaster could damage the entire communication network and cut off data aggregation such that Qos is compromised. In this paper, an Intelligent NIB based Data Aggregation Strategy, named (IDAS), is proposed for after disaster scenarios in IIoE. Specifically, IDAS first applies both iterative cubature kalman filter and radial basis function neural network to predict the data collection rates of survived infrastructures. Then, an energy efficient task distribution mechanism is design. Next, a deep reinforcement learning method is developed for the car-carrying NIB route design to perform corresponding task. Eventually, all data are aggregated toward the rescue headquarter by NIB deployment based on Fermat tree constructions. The theoretical analysis and simulations indicate that IDAS is not only energy efficient for after disaster scenarios but requires the least NIB consumption while compared with contemporary strategies.",https://ieeexplore.ieee.org/document/9137637/,IEEE Access,2020,ieeexplore
10.1109/JSEN.2021.3087537,Long-Distance Pipeline Safety Early Warning: A Distributed Optical Fiber Sensing Semi-Supervised Learning Method,IEEE,Journals,"Pipeline safety early warning (PSEW) systems based on distributed optical fiber sensors are used to recognize and locate third-party events that may damage long-distance energy transportation pipelines and are essential to ensure pipeline safety and energy supply. However, the deployment of PSEW systems in real sites is hindered by the high experimental cost of collecting large real-site data sets for model building and the small percentage of labeled data (typically less than 0.5%). Besides, the optical fiber sensors are sensitive to hardware and the environment, ensuring challenges to directly migrate the old PSEW system for a new deployment. In this study, a novel semi-supervised learning model is proposed to monitor the safety of pipelines in real-time. Concretely, the sparse stacked autoencoder trained with unlabeled data is used to extract more robust features, and the fully-connected network trained with a small amount of labeled data is used for location and identification. Encouraging empirical results on the real-world long-distance energy pipelines of the PipeChina confirm that our method achieves better recognition and localization performance in comparison to the baseline with less labeled data. Further, the model size and recognition latency are reduced by 18.9× and 7.9× of the baseline, respectively. Also, the decoded features have better visualization than the input. This work reduces the cost of PSEW system deployments, improves its performance and portability, and will contribute to the widespread use of PSEW systems in the industry.",https://ieeexplore.ieee.org/document/9448237/,IEEE Sensors Journal,"1 Sept.1, 2021",ieeexplore
10.1109/ACCESS.2019.2942390,"Machine Learning for 5G/B5G Mobile and Wireless Communications: Potential, Limitations, and Future Directions",IEEE,Journals,"Driven by the demand to accommodate today's growing mobile traffic, 5G is designed to be a key enabler and a leading infrastructure provider in the information and communication technology industry by supporting a variety of forthcoming services with diverse requirements. Considering the ever-increasing complexity of the network, and the emergence of novel use cases such as autonomous cars, industrial automation, virtual reality, e-health, and several intelligent applications, machine learning (ML) is expected to be essential to assist in making the 5G vision conceivable. This paper focuses on the potential solutions for 5G from an ML-perspective. First, we establish the fundamental concepts of supervised, unsupervised, and reinforcement learning, taking a look at what has been done so far in the adoption of ML in the context of mobile and wireless communication, organizing the literature in terms of the types of learning. We then discuss the promising approaches for how ML can contribute to supporting each target 5G network requirement, emphasizing its specific use cases and evaluating the impact and limitations they have on the operation of the network. Lastly, this paper investigates the potential features of Beyond 5G (B5G), providing future research directions for how ML can contribute to realizing B5G. This article is intended to stimulate discussion on the role that ML can play to overcome the limitations for a wide deployment of autonomous 5G/B5G mobile and wireless communications.",https://ieeexplore.ieee.org/document/8844682/,IEEE Access,2019,ieeexplore
10.1109/ACCESS.2017.2754507,Model-Based Development of Knowledge-Driven Self-Reconfigurable Machine Control Systems,IEEE,Journals,"To accommodate the trend toward mass customization launched by intelligent manufacturing in the era of Industry 4.0, this paper proposes the combination of model-driven engineering and knowledgedriven engineering during the development process of self-reconfigurable machine control systems. The complete tool chain for model development, execution, and reconfiguration is established. For the design phase, a machine-control-domain-specific modeling language and the supporting design environment are developed. With regard to the execution stage, a runtime framework compliant with the IEC 61499 standard is proposed. On the ground of the modeling environment and the reconfigurable run-time framework, a self-adaptive control module is developed to establish the close-loop self-reconfiguration infrastructure. The ontological representation of knowledge base toward this end is described, along with extendable SQWRL rules specified to automatically initiate the reconfiguration process in the cases of external user demands and internal faults. A prototype motion control kernel in the low-level layer of machine control system architecture is developed with the proposed modeling language and is then deployed to the runtime framework. Two case studies on self-reconfiguration of the proof-of-concept motion control kernel are demonstrated, which prove the feasibility of our proposal.",https://ieeexplore.ieee.org/document/8047091/,IEEE Access,2017,ieeexplore
10.1109/TCSS.2019.2918285,Multi-Modality Behavioral Influence Analysis for Personalized Recommendations in Health Social Media Environment,IEEE,Journals,"Recently, health social media have engaged more and more people to share their personal feelings, opinions, and experience in the context of health informatics, which has drawn increasing attention from both academia and industry. In this paper, we focus on the behavioral influence analysis based on heterogeneous health data generated in social media environments. An integrated deep neural network (DNN)-based learning model is designed to analyze and describe the latent behavioral influence hidden across multiple modalities, in which a convolutional neural network (CNN)-based framework is used to extract the time-series features within a certain social context. The learned features based on cross-modality influence analysis are then trained in a SoftMax classifier, which can result in a restructured representation of high-level features for online physician rating and classification in a data-driven way. Finally, two algorithms within two representative application scenarios are developed to provide patients with personalized recommendations in health social media environments. Experiments using the real world data demonstrate the effectiveness of our proposed model and method.",https://ieeexplore.ieee.org/document/8861193/,IEEE Transactions on Computational Social Systems,Oct. 2019,ieeexplore
10.1109/ACCESS.2020.2976803,Multicriteria Based Decision Making of DevOps Data Quality Assessment Challenges Using Fuzzy TOPSIS,IEEE,Journals,"In current era, DevOps gain much interaction in software industry as it provides the flexible development environment. To meet the continuous development and operations, DevOps mainly focus, to integrate the data from heterogeneous source. While DevOps adoption, the quality assessment of data integrated from heterogeneous environment, is important and challenging at the same time. This study aims to identify the critical factors that could negatively impact the data quality assessment process in DevOps. We have used the systematic literature review (SLR) approach and identify a total of 13 critical challenging factors. The finding of SLR are further validated with industry experts via questionnaire survey. Finally, we have applied the Fuzzy TOPSIS approach to prioritize the investigated challenging factors with respect to their significance of DevOps data quality assessment process. The results show that analyzing data in real time, visualization of data and missing information and other invalid data are the highest ranked challenging factors which need to be addressed on priority basis, to successfully measure the quality of heterogeneous data in DevOps. We believe that the finding of this study will assist the practitioner to consider the most significant factors for measuring the quality of heterogeneous data in DevOps.",https://ieeexplore.ieee.org/document/9015992/,IEEE Access,2020,ieeexplore
10.1109/TLA.2021.9477280,Multilayer Extreme Learning Machine as Equalizer in OFDM-based Radio-over-fiber Systems,IEEE,Journals,"Mobile/wireless networks aim to support diverse services with numerous and sophisticated requirements, such as energy efficiency, spectral efficiency, negligible latency, robustness against time and frequency selective channels, low hardware complexity, among others. From the central station to the base stations, radio-over-fiber orthogonal frequency division multiplexing (RoF-OFDM) schemes with direct-detection are then implemented. Unfortunately, laser phase noise, chromatic fiber dispersion, and carrier frequency offset impair the orthogonality of the subcarriers; hence, deteriorating the performance of the RoF-OFDM system. In order to take all the processing tasks to the cognitive level (the last goal in the telecommunication industry), various extreme learning machines (ELMs), composed by only a single hidden layer, have been recently adopted as equalizers. The reason behind this trend comes from the lower computational complexity, higher detection accuracy, and minimum human intervention of the ELM algorithms. In this article, we introduce a multilayer ELM-based receiver for RoF schemes transmitting phase-correlated OFDM signals affected by phase and frequency errors. Results report that by appropriately setting the hyper-parameters of the multilayer ELMs, the ELM with 3 hidden layers outperforms most of the ELMs reported in the literature (the ELM with 2 hidden layers, original ELM, regularized ELM, and 2 fully-independent ELMs defined in the real domain), as well as the benchmark pilot-assisted equalizer in terms of bit error rate. Nevertheless, this benefit comes with excessive computational cost. Finally, we show that the fully-complex ELM is still the best equalizer taking into account several key metrics.",https://ieeexplore.ieee.org/document/9477280/,IEEE Latin America Transactions,Oct. 2021,ieeexplore
10.1109/JIOT.2019.2960099,Multiuser Physical Layer Authentication in Internet of Things With Data Augmentation,IEEE,Journals,"Unlike most of the upper layer authentication mechanisms, the physical (PHY) layer authentication takes advantages of channel impulse response from wireless propagation to identify transmitted packages with low-resource consumption, and machine learning methods are effective ways to improve its implementation. However, the training of the machine-learning-based PHY-layer authentication requires a large number of training samples, which makes the training process time consuming and computationally resource intensive. In this article, we propose a data augmented multiuser PHY-layer authentication scheme to enhance the security of mobile-edge computing system, an emergent architecture in the Internet of Things (IoT). Three data augmentation algorithms are proposed to speed up the establishment of the authentication model and improve the authentication success rate. By combining the deep neural network with data augmentation methods, the performance of the proposed multiuser PHY-layer authentication scheme is improved and the training speed is accelerated, even with fewer training samples. Extensive simulations are conducted under the real industry IoT environment and the figures illustrate the effectiveness of our approach.",https://ieeexplore.ieee.org/document/8935162/,IEEE Internet of Things Journal,March 2020,ieeexplore
10.1109/ACCESS.2021.3112996,Neural Style Transfer: A Critical Review,IEEE,Journals,"Neural Style Transfer (NST) is a class of software algorithms that allows us to transform scenes, change/edit the environment of a media with the help of a Neural Network. NST finds use in image and video editing software allowing image stylization based on a general model, unlike traditional methods. This made NST a trending topic in the entertainment industry as professional editors/media producers create media faster and offer the general public recreational use. In this paper, the current progress in Neural Style Transfer with all related aspects such as still images and videos is presented critically. The authors looked at the different architectures used and compared their advantages and limitations. Multiple literature reviews focus on either the Neural Style Transfer (of images) or cover Generative Adversarial Networks (GANs) that generate video. As per the authors’ knowledge, this is the only research article that looks at image and video style transfer, particularly mobile devices with high potential usage. This article also reviewed the challenges faced in applyingvideo neural style transfer in real-time on mobile devices and presents research gaps with future research directions. NST, a fascinating deep learning application, has considerable research and application potential in the coming years.",https://ieeexplore.ieee.org/document/9539183/,IEEE Access,2021,ieeexplore
10.1109/TASE.2012.2198057,Neural-Network-Based Optimal Control for a Class of Unknown Discrete-Time Nonlinear Systems Using Globalized Dual Heuristic Programming,IEEE,Journals,"In this paper, a neuro-optimal control scheme for a class of unknown discrete-time nonlinear systems with discount factor in the cost function is developed. The iterative adaptive dynamic programming algorithm using globalized dual heuristic programming technique is introduced to obtain the optimal controller with convergence analysis in terms of cost function and control law. In order to carry out the iterative algorithm, a neural network is constructed first to identify the unknown controlled system. Then, based on the learned system model, two other neural networks are employed as parametric structures to facilitate the implementation of the iterative algorithm, which aims at approximating at each iteration the cost function and its derivatives and the control law, respectively. Finally, a simulation example is provided to verify the effectiveness of the proposed optimal control approach. Note to Practitioners-The increasing complexity of the real-world industry processes inevitably leads to the occurrence of nonlinearity and high dimensions, and their mathematical models are often difficult to build. How to design the optimal controller for nonlinear systems without the requirement of knowing the explicit model has become one of the main foci of control practitioners. However, this problem cannot be handled by only relying on the traditional dynamic programming technique because of the ""curse of dimensionality"". To make things worse, the backward direction of solving process of dynamic programming precludes its wide application in practice. Therefore, in this paper, the iterative adaptive dynamic programming algorithm is proposed to deal with the optimal control problem for a class of unknown nonlinear systems forward-in-time. Moreover, the detailed implementation of the iterative ADP algorithm through the globalized dual heuristic programming technique is also presented by using neural networks. Finally, the effectiveness of the control strategy is illustrated via simulation study.",https://ieeexplore.ieee.org/document/6203617/,IEEE Transactions on Automation Science and Engineering,July 2012,ieeexplore
10.1109/ACCESS.2019.2958284,On the Generation of Anomaly Detection Datasets in Industrial Control Systems,IEEE,Journals,"In recent decades, Industrial Control Systems (ICS) have been affected by heterogeneous cyberattacks that have a huge impact on the physical world and the people's safety. Nowadays, the techniques achieving the best performance in the detection of cyber anomalies are based on Machine Learning and, more recently, Deep Learning. Due to the incipient stage of cybersecurity research in ICS, the availability of datasets enabling the evaluation of anomaly detection techniques is insufficient. In this paper, we propose a methodology to generate reliable anomaly detection datasets in ICS that consists of four steps: attacks selection, attacks deployment, traffic capture and features computation. The proposed methodology has been used to generate the Electra Dataset, whose main goal is the evaluation of cybersecurity techniques in an electric traction substation used in the railway industry. Using the Electra dataset, we train several Machine Learning and Deep Learning models to detect anomalies in ICS and the performed experiments show that the models have high precision and, therefore, demonstrate the suitability of our dataset for use in production systems.",https://ieeexplore.ieee.org/document/8926471/,IEEE Access,2019,ieeexplore
10.1364/JOCN.403205,Open whitebox architecture for smart integration of optical networking and data center technology [Invited],IEEE,Journals,"In this paper, we identify challenges in developing future optical network infrastructure for new services based on technologies such as 5G, virtual reality, and artificial intelligence, and we suggest approaches to handling these challenges that include a business model, architecture, and diversity. Through activities in multiservice agreement and de facto standard organizations, we have shown how the hardware abstraction layer interfaces of optical transceivers are implemented for multivendor and heterogeneous environments, coherent digital signal processor interoperability, and optical transport whiteboxes. We have driven the effort to define the transponder abstraction interface with partners. The feasibility of such implementation was verified through demonstrations and trials. In addition, we are constructing an open-transport platform by combining existing open-source software and implementing software components that automate and enhance operations. An open architecture maintains a healthy ecosystem for industry and allows for a flexible, operator-driven network.",https://ieeexplore.ieee.org/document/9275288/,Journal of Optical Communications and Networking,January 2021,ieeexplore
10.1109/ACCESS.2020.3035880,Prioritization Based Taxonomy of DevOps Challenges Using Fuzzy AHP Analysis,IEEE,Journals,"The DevOps (development and operations) is a collaborative software development environment which offers the continues development and deployment of quality software project within short time. The DevOps practices are not yet mature enough, and the software organizations hesitate to adopt it. This study aims: 1) to explore the DevOps challenges by conducting systematic literature review (SLR) and to get the insight of industry experts via questionnaire survey study and 2) to prioritize the investigated challenges using fuzzy analytical hierarchy process (FAHP). The study findings provide the set of critical challenges faced by the software organizations while adopting DevOps and a prioritization-based taxonomy of the DevOps challenges. The application of FAHP is novel in this research area as it assists in addressing the vagueness of practitioners concerning the influencing factors of DevOps. We believe that the finding of this study will serve as a body of knowledge for real world practitioners and researchers to revise and develop the new strategies for the successful implementation of DevOps practices in the software industry.",https://ieeexplore.ieee.org/document/9247948/,IEEE Access,2020,ieeexplore
10.1109/TC.2020.3000051,Priority Assignment on Partitioned Multiprocessor Systems With Shared Resources,IEEE,Journals,"Driven by industry demand, there is an increasing need to develop real-time multiprocessor systems which contain shared resources. The Multiprocessor Stack Resource Policy (MSRP) and Multiprocessor resource sharing Protocol (MrsP) are two major protocols that manage access to shared resources. Both of them can be applied to Fixed-Priority Preemptive Scheduling (FPPS), which is enforced by most commercial real-time systems regulations, and which requires task priorities to be assigned before deployment. Along with MSRP and MrsP, there exist two forms of schedulability tests that bound the worst-case blocking time due to resource accesses: the traditional ones being more widely adopted and the more recently developed holistic ones which deliver tighter analysis. On uniprocessor systems, there are several well-established optimal priority assignment algorithms. Unfortunately, on multiprocessor systems with shared resources, the issue of priority assignment has not been adequately understood. In this article, we investigate three mainstream priority assignment algorithms-Deadline Monotonic Priority Ordering (DMPO), Audsley's Optimal Priority Assignment (OPA), and Robust Priority Assignment (RPA), in the context of partitioned multiprocessor systems with shared resources. Our contributions are multifold: First, we prove that DMPO is optimal with the traditional schedulability tests. Second, two counter examples are given as evidence that DMPO is not optimal with the tighter holistic schedulability tests. Third, we then analyze the pessimism arising from the adoption of OPA and RPA with the holistic tests. Lastly, we propose a Slack-based Priority Ordering (SPO) algorithm that minimises such pessimism, and has polynomial time complexity. Comprehensive experiments show that SPO outperforms (i.e., results in a larger number of schedulable systems) DMPO, OPA, and RPA in general with the holistic schedulability tests, by up to 15 percent. With the theoretical contributions, this paper is a useful guide to priority assignment in real-time partitioned multiprocessor systems with shared resources.",https://ieeexplore.ieee.org/document/9109645/,IEEE Transactions on Computers,1 July 2021,ieeexplore
10.1109/ACCESS.2020.2979015,Quantum GIS Based Descriptive and Predictive Data Analysis for Effective Planning of Waste Management,IEEE,Journals,"Waste has a direct impact on human health and the surrounding environment. Apart from the health aspect, many industries' growth is effected by waste material such as the food industry. Waste management authorities are interested in reducing the cost of waste management operations and searching for sustainable waste management solutions. For effective planning of waste management, reliable data analysis is required to produce results that can facilitate the planning process. Data mining and machine learning-based data analysis over the waste data can produce a more detailed, and in-time waste information generation, which can lead to effectively manage the waste amount of specific area. In this paper, a descriptive data analysis approach, along with predictive analysis, is used to produce in-time waste information. The performance of the proposed approach is evaluated using a real waste dataset of Jeju Island, South Korea. Waste bins are virtualized on its actual location on the Jeju map in Quantum Geographic Information Systems(QGIS) software. The performance results of the predictive analysis models are evaluated in terms of Mean Absolute Error(MAE), Root Mean Square Error (RMSE), and Mean Absolute Percentage Error(MAPE). Performance results indicate that predictive analysis models are reliable for the effective planning and optimization of waste management operations.",https://ieeexplore.ieee.org/document/9026977/,IEEE Access,2020,ieeexplore
10.1109/TII.2019.2958606,Real-Time Anomaly Detection of NoSQL Systems Based on Resource Usage Monitoring,IEEE,Journals,"Today, the emergence of the industry revolution systems such as Industry 4.0, Internet of Things, and big data frameworks poses new challenges in terms of storage and processing of real-time data. As systems scale in humongous sizes, a crucial task is to administer the variety of different subsystems and applications to ensure high performance. This is directly related with the identification and elimination of system failures and errors, while the system runs. In particular, database systems may experience abnormalities related with decreased throughput or increased resource usage, that in turn affects system performance. In this article, we focus on not only SQL (NoSQL) database systems that are ideal for storing sensor data in the concept of Industry 4.0. This typically includes a variety of applications and workloads that are difficult to online monitor, thus making anomaly detection a challenging task. Creating a robust platform to serve such infrastructures with minimum hardware or software failures is a key challenge. In this article, we propose RADAR, an anomaly detection system that works on real time. RADAR is a data-driven decision-making system for NoSQL systems, by providing process information extraction during resource monitoring and by associating resource usage with the top processes, to identify anomalous cases. In this article, we focus on anomalies such as hardware failures or software bugs that could lead to abnormal application runs, without necessarily stopping system functionality, e.g., due to a system crash, but by affecting its performance, e.g., decreased database system throughput. Although different patterns may occur through time, we focus on periodic running workloads (e.g., monitoring daily usage) that are very common for NoSQL systems, and Internet of Things scenarios where data streams are forwarded to the Cloud for storage and processing. We apply various machine learning algorithms such as autoregressive integrated moving average (ARIMA), seasonal ARIMA, and long-short-term memory recurrent neural networks. We experimentally analyze our solution to demonstrate the benefits of supporting online erroneous state identification and characterization for modern applications.",https://ieeexplore.ieee.org/document/8930068/,IEEE Transactions on Industrial Informatics,Sept. 2020,ieeexplore
10.1109/TSG.2017.2783894,Real-Time Energy Management in Microgrids With Reduced Battery Capacity Requirements,IEEE,Journals,"Energy storage units hold promise to transform the electric power industry, since they can supply power to end customers during peak demand times, and operate as customers upon a power surplus. This paper studies online energy management with renewable energy resources and energy storage units. For the problem at hand, the popular approaches rely on stochastic dual (sub)gradient (SDG) iterations for a chosen stepsize μ, which generally require battery capacity O(1/μ) to guarantee an O(μ)-optimal solution. With the goal of achieving optimal energy cost with considerably reduced battery capacity requirements, an online learning-aided management (OLAM) scheme is introduced for energy management, which incorporates statistical learning advances into real-time energy management. To facilitate real-time implementation of the proposed scheme, the alternating direction method of multipliers method is also leveraged to solve the involved subproblems in a distributed fashion. It is analytically established that OLAM incurs an O(μ) optimality gap, while only requiring battery capacity O(log<sup>2</sup>(μ)/√μ). Simulations on the IEEE power grid benchmark corroborate that OLAM incurs similar average cost relative to that of SDG, while requiring markedly lower battery capacity.",https://ieeexplore.ieee.org/document/8214260/,IEEE Transactions on Smart Grid,March 2019,ieeexplore
10.1109/TITS.2019.2906038,Real-Time Sensor Anomaly Detection and Identification in Automated Vehicles,IEEE,Journals,"Connected and automated vehicles (CAVs) are expected to revolutionize the transportation industry, mainly through allowing for a real-time and seamless exchange of information between vehicles and roadside infrastructure. Although connectivity and automation are projected to bring about a vast number of benefits, they can give rise to new challenges in terms of safety, security, and privacy. To navigate roadways, CAVs need to heavily rely on their sensor readings and the information received from other vehicles and roadside units. Hence, anomalous sensor readings caused by either malicious cyber attacks or faulty vehicle sensors can result in disruptive consequences and possibly lead to fatal crashes. As a result, before the mass implementation of CAVs, it is important to develop methodologies that can detect anomalies and identify their sources seamlessly and in real time. In this paper, we develop an anomaly detection approach through combining a deep learning method, namely convolutional neural network (CNN), with a well-established anomaly detection method, and Kalman filtering with a χ<sup>2</sup>-detector, to detect and identify anomalous behavior in CAVs. Our numerical experiments demonstrate that the developed approach can detect anomalies and identify their sources with high accuracy, sensitivity, and F1 score. In addition, this developed approach outperforms the anomaly detection and identification capabilities of both CNNs and Kalman filtering with a χ<sup>2</sup>-detector method alone. It is envisioned that this research will contribute to the development of safer and more resilient CAV systems that implement a holistic view toward intelligent transportation system (ITS) concepts.",https://ieeexplore.ieee.org/document/8684317/,IEEE Transactions on Intelligent Transportation Systems,March 2020,ieeexplore
10.1109/JSEN.2021.3075535,Recent Advancements in the Development of Sensors for the Structural Health Monitoring (SHM) at High-Temperature Environment: A Review,IEEE,Journals,"With Industry 4.0 becoming increasingly pervasive, the importance and usage of sensors has increased several folds. Industry 4.0 refers to a new phase in the industrial revolution that mainly focuses on interconnectivity, automation, machine learning, and real-time data. Real-time structural health monitoring (SHM) of components in the industrial process is one of the crucial and important component of Industry 4.0. SHM of components exposed to high-temperature (<inline-formula> <tex-math notation=""LaTeX"">$\sim 650^{\circ }\text{C}$ </tex-math></inline-formula>) is becoming increasingly important nowadays. However, harsh and high temperature environments impose a great challenge towards their implementation. This review is an attempt to demonstrate the development, application, limitations and recent advancement of the existing sensors used for SHM. Some sensors such as eddy current (EC) sensors and fiber Bragg grating (FBG) sensors have been discussed in detail. A phenomenological study of the electromagnetic sensor for the SHM of engineering components that are exposed to high temperature has been addressed. State-of-the-art fabrication methodologies such as low temperature co-fired ceramic (LTCC) technology for such type of sensors for high temperature SHM applications have been elucidated. Future challenges and opportunities for SHM applications of high temperature sensors have been highlighted.",https://ieeexplore.ieee.org/document/9415648/,IEEE Sensors Journal,"15 July15, 2021",ieeexplore
10.1109/ACCESS.2020.3009298,Recent Developments of the Internet of Things in Agriculture: A Survey,IEEE,Journals,"A rise in the population has immensely increased the pressure on the agriculture sector. With the advent of technology, this decade is witnessing a shift from conventional approaches to the most advanced ones. The Internet of Things (IoT) has transformed both the quality and quantity of the agriculture sector. Hybridization of species along with the real-time monitoring of the farms paved a way for resource optimization. Scientists, research institutions, academicians, and most nations across the globe are moving towards the practice and execution of collaborative projects to explore the horizon of this field for serving mankind. The tech industry is racing to provide more optimal solutions. Inclusion of IoT, along with cloud computing, big data analytics, and wireless sensor networks can provide sufficient scope to predict, process, and analyze the situations and improve the activities in the real-time scenario. The concept of heterogeneity and interoperability of the devices by providing flexible, scalable, and durable methods, models are also opening new domains in this field. Therefore, this paper contributes towards the recent IoT technologies in the agriculture sector, along with the development of hardware and software systems. The public and private sector projects and startup's started all over the globe to provide smart and sustainable solutions in precision agriculture are also discussed. The current scenario, applications, research potential, limitations, and future aspects are briefly discussed. Based on the concepts of IoT a precision farming framework is also proposed in this article.",https://ieeexplore.ieee.org/document/9139962/,IEEE Access,2020,ieeexplore
10.1109/ACCESS.2020.2976513,Resource-Constrained Machine Learning for ADAS: A Systematic Review,IEEE,Journals,"The advent of machine learning (ML) methods for the industry has opened new possibilities in the automotive domain, especially for Advanced Driver Assistance Systems (ADAS). These methods mainly focus on specific problems ranging from traffic sign and light recognition to pedestrian detection. In most cases, the computational resources and power budget found in ADAS systems are constrained while most machine learning methods are computationally intensive. The usual solution consists in adapting the ML models to comply with the memory and real-time (RT) requirements for inference. Some models are easily adapted to resource-constrained hardware, such as Support Vector Machines, while others, like Neural Networks, need more complex processes to fit into the desired hardware. The ADAS hardware (HW platforms) are diverse, from complex MPSoC CPUs down to classical MCUs, DPSs and application-specific FPGAs and ASICs or specific GPU platforms (such as the NVIDIA families Tegra or Jetson). Therefore, there is a tradeoff between the complexity of the ML model implemented and the selected platform that impacts the performance metrics: function results, energy consumption and speed (latency and throughput). In this paper, a survey in the form of systematic review is conducted to analyze the scope of the published research works that embed ML models into resource-constrained implementations for ADAS applications and what are the achievements regarding the ML performance, energy and speed trade-off.",https://ieeexplore.ieee.org/document/9016213/,IEEE Access,2020,ieeexplore
10.1109/ACCESS.2020.3045563,SDN Enhanced Resource Orchestration of Containerized Edge Applications for Industrial IoT,IEEE,Journals,"With the rise of the Industrial Internet of Things (IIoT), there is an intense pressure on resource and performance optimization leveraging on existing technologies, such as Software Defined Networking (SDN), edge computing, and container orchestration. Industry 4.0 emphasizes the importance of lean and efficient operations for sustainable manufacturing. Achieving this goal would require engineers to consider all layers of the system, from hardware to software, and optimizing for resource efficiency at all levels. This emphasizes the need for container-based virtualization tools such as Docker and Kubernetes, offering Platform as a Service (PaaS), while simultaneously leveraging on edge technologies to reduce related latencies. For network management, SDN is poised to offer a cost-effective and dynamic scalability solution by customizing packet handling for various edge applications and services. In this paper, we investigate the energy and latency trade-offs involved in combining these technologies for industrial applications. As a use case, we emulate a 3D-drone-based monitoring system aimed at providing real-time visual monitoring of industrial automation. We compare a native implementation to a containerized implementation where video processing is orchestrated while streaming is handled by an external UE representing the IIoT device. We compare these two scenarios for energy utilization, latency, and responsiveness. Our test results show that only roughly 16 percent of the total power consumption happens on the mobile node when orchestrated. Virtualization adds up about 4.5 percent of the total power consumption while the latency difference between the two approaches becomes negligible after the streaming session is initialized.",https://ieeexplore.ieee.org/document/9296769/,IEEE Access,2020,ieeexplore
10.1109/ACCESS.2020.3027357,SPADE 3: Supporting the New Generation of Multi-Agent Systems,IEEE,Journals,"Although intelligent agent-based systems have existed for several years, the progression in terms of real applications or their integration in the industry have not yet reached the expected levels. During the last two decades, many agent platforms have appeared with the aim of simplifying the development of multi-agent systems. Some of these platforms have been designed for general purposes, while others have been oriented towards specific domains. However, the lack of standards and the complexity associated with supporting such systems, among other difficulties, have hampered their generalised use. This article looks in depth at the current situation of existing agent platforms, trying to analyse their current shortcomings and their expected needs in the near future. The goal of the paper is to identify possible lines of work and some of the most crucial aspects to be considered in order to popularize the application of agent technology as a dynamic and flexible solution to current problems. Moreover, the paper presents SPADE 3, a new version of the SPADE middleware, which has been totally redesigned in order to conform to the identified challenges. Finally, a case study is proposed to illustrate how SPADE 3 is able to fulfill these challenges.",https://ieeexplore.ieee.org/document/9207929/,IEEE Access,2020,ieeexplore
10.1109/TLA.2012.6142495,Second Life: A New Approach In Professional Education In The Study Of Work Safety.,IEEE,Journals,"The ICT (Information and Communication Technologies) are present with great intensity in education and has also presented highlighting the professional education with tools for data analysis, graphics, simulation, virtual reality and augmented reality. The knowledge gained with ICTs are used in practical activities in industrial operations. The student examines machine elements through simulations, yet during the practices with machines and equipment, errors occur from simple forgetfulness of use of PPE (Personal Protective Equipment), lack of parameters in the machinery, not removal of the ornaments the body, can somehow put at risk the life of the students in these settings wrong or manipulation of machinery and equipment that can cause accidents. Our proposal is to use a virtual reality tool specifically the SL (Second Life), having a friendly interface, programming features and usability features immersive, so we developed the simulation and representation of machine failures that could occur on the day of an industry. During the use of simulators in SL, the student learns from those mistakes in the virtual world, serving as a warning so it does not perform the same mistakes in the real world, where unsafe acts can danger your life up. The proposal also gives the possibility to use a machine for machining of smaller proportions built specifically for educational activities, plus devices that facilitate the use by PWD's (People with disabilities), connected to the virtual world.",https://ieeexplore.ieee.org/document/6142495/,IEEE Latin America Transactions,Jan. 2012,ieeexplore
10.1109/59.589783,Security boundary visualization for systems operation,IEEE,Journals,"This paper presents a security assessment approach for operational planning studies that provides the operator with accurate boundary visualization in terms of easily monitored precontingency information. The approach is modeled after traditional security assessment procedures which result in use of a nomogram for characterizing the security boundaries; these procedures are common among many North American utilities today. Therefore, the approach builds on what is already familiar in the industry, but it takes advantage of computer automation and neural networks for generating and understanding large databases. The appeal of the approach is threefold: it provides increased accuracy in boundary representation, it reduces the labor traditionally required in generating security boundaries, and the resulting boundaries, encoded in fast, flexible C subroutines, can be integrated into energy management system software to provide the operator with compact, understandable boundary illustration in real time. These improvements are of particular interest in securely operating transmission systems close to their limits so as to fully utilize existing facilities.",https://ieeexplore.ieee.org/document/589783/,IEEE Transactions on Power Systems,May 1997,ieeexplore
10.1109/ACCESS.2016.2619898,Segmentation of Factories on Electricity Consumption Behaviors Using Load Profile Data,IEEE,Journals,"In recent years, the new achievements in the field of technology and data science allowed to gather detailed and well-structured information about electricity consumption behaviors of industrial enterprises. Such type of information can find numerous applications in the power distribution industry. The utilities often use the data from contracts to assign each industrial customer a class label according to this type defined in predetermined industry segmentation. Such type of fixed-chart segmentation is not able to satisfy the needs of modern enterprises for the flexible and dynamic determination of production modes. In this paper, we address this problem by proposing a new method for the segmentation of various types of factories based on their electricity consumption patterns represented in load profile data. It exploits the evolution-based characteristics of smart meter data of multiple types of factories to remove irrelevant features. We use data visualization to estimate the number of clusters and apply the well-known k-means algorithm on filtered data to generate segmentation. Experimental results on real load profile data collected with smart meters from manufacturing industries in Guangdong province of China have shown that the new clustering approach produced the meaningful segmentation of factories that reflect production operations.",https://ieeexplore.ieee.org/document/7752771/,IEEE Access,2016,ieeexplore
10.1109/ACCESS.2020.2994933,Sensor-Driven Learning of Time-Dependent Parameters for Prescriptive Analytics,IEEE,Journals,"Big data analytics is rapidly emerging as a key Internet of Things (IoT) initiative aiming at providing meaningful insights and supporting optimal decision making under time constraints. In this direction, prescriptive analytics has just started to emerge. Prescriptive analytics moves beyond descriptive and predictive analytics aiming at providing adaptive, automated, constrained, time-dependent and optimal decisions. The use of time-dependent parameters in prescriptive analytics models provide a more reliable and realistic representation of the complex and dynamic environment and the associated decision making process; however, their estimation poses significant challenges due to the uncertainty derived from inaccurate user input, noisy data, and non-stationarity of real-world data streams. Since feedback and learning mechanisms for tracking the prescriptive analytics are crucial enablers for self-configuration and self-optimization, this paper proposes an approach for sensor-driven learning of time-dependent parameters for prescriptive analytics models deployed in streaming computational environments. The proposed approach was validated in an Industry 4.0 use case, while it was further evaluated through extensive simulation experiments. The proposed approach overcomes challenges related to uncertainty derived from user's input, non-stationary data and sensor noise and provides estimates of time-dependent parameters that lead to more reliable prescriptions.",https://ieeexplore.ieee.org/document/9094172/,IEEE Access,2020,ieeexplore
10.1109/JSEN.2020.3029459,"Sensor-Fault Detection, Isolation and Accommodation for Digital Twins via Modular Data-Driven Architecture",IEEE,Journals,"Sensor technologies empower Industry 4.0 by enabling integration of in-field and real-time raw data into digital twins. However, sensors might be unreliable due to inherent issues and/or environmental conditions. This article aims at detecting anomalies in measurements from sensors, identifying the faulty ones and accommodating them with appropriate estimated data, thus paving the way to reliable digital twins. More specifically, we propose a general machine-learning-based architecture for sensor validation built upon a series of neural-network estimators and a classifier. Estimators correspond to virtual sensors of all unreliable sensors (to reconstruct normal behaviour and replace the isolated faulty sensor within the system), whereas the classifier is used for detection and isolation tasks. A comprehensive statistical analysis on three different real-world data-sets is conducted and the performance of the proposed architecture validated under hard and soft synthetically-generated faults.",https://ieeexplore.ieee.org/document/9216114/,IEEE Sensors Journal,"15 Feb.15, 2021",ieeexplore
10.1109/TITS.2020.2980864,Solving the Security Problem of Intelligent Transportation System With Deep Learning,IEEE,Journals,"Objective: the objective of this study is to study deep learning to solve the safety problems of intelligent transportation system. Method: the intelligent transportation system is improved by using the deep learning algorithm, and the improved system is simulated, and the data transmission performance, accuracy prediction performance and path change strategy of the system are statistically analyzed. Results: in the analysis of the data transmission performance of the system, the probability of successful propagation is found to be 100%. When the value of λ is 0.01~0.05, it is the closest to the actual result and the data delay is the smallest. In the analysis of the accuracy prediction of the system, it is found that the system of this study has the best accuracy prediction performance with the increase of the number of iterations compared with other models in different categories. After further analyzing the path induction strategy of the system, it is found that the route guidance strategy of this study can effectively restrain the spread of congestion and achieve the effect of timely evacuation of traffic congestion in the face of congested road sections. Conclusion: it is found that the improvement of the intelligent transportation system by using deep learning can significantly reduce the data transmission delay of the system, improve the prediction accuracy, and effectively change the path in the face of congestion to suppress the congestion spread. Although there are some shortcomings in the experiment, it still provides experimental reference for the development of the transportation industry in the later stage.",https://ieeexplore.ieee.org/document/9043888/,IEEE Transactions on Intelligent Transportation Systems,July 2021,ieeexplore
10.1109/TII.2019.2919268,Stochastic Configuration Networks Based Adaptive Storage Replica Management for Power Big Data Processing,IEEE,Journals,"In the power industry, processing business big data from geographically distributed locations, such as online line-loss analysis, has emerged as an important application. How to achieve highly efficient big data storage to meet the requirements of low latency processing applications is quite challenging. In this paper, we propose a novel adaptive power storage replica management system, named PARMS, based on stochastic configuration networks (SCNs), in which the network traffic and the data center (DC) geodistribution are taken into consideration to improve data real-time processing. First, as a fast learning model with less computation burden and sound prediction performance, the SCN model is employed to estimate the traffic state of power data networks. Then, a series of data replica management algorithms is proposed to lower the effects of limited bandwidths and a fixed underlying infrastructure. Finally, the proposed PARMS is implemented using data-parallel computing frameworks (DCFs) for the power industry. Experiments are carried out in an electric power corporation of 230 million users, China Southern power grid, and the results show that our proposed solution can deal with power big data storage efficiently and the job completion times across geodistributed DCs are reduced by 12.19% on average.",https://ieeexplore.ieee.org/document/8723083/,IEEE Transactions on Industrial Informatics,Jan. 2020,ieeexplore
10.1109/TNSRE.2014.2305111,The Extraction of Neural Information from the Surface EMG for the Control of Upper-Limb Prostheses: Emerging Avenues and Challenges,IEEE,Journals,"Despite not recording directly from neural cells, the surface electromyogram (EMG) signal contains information on the neural drive to muscles, i.e, the spike trains of motor neurons. Using this property, myoelectric control consists of the recording of EMG signals for extracting control signals to command external devices, such as hand prostheses. In commercial control systems, the intensity of muscle activity is extracted from the EMG and used for single degrees of freedom activation (direct control). Over the past 60 years, academic research has progressed to more sophisticated approaches but, surprisingly, none of these academic achievements has been implemented in commercial systems so far. We provide an overview of both commercial and academic myoelectric control systems and we analyze their performance with respect to the characteristics of the ideal myocontroller. Classic and relatively novel academic methods are described, including techniques for simultaneous and proportional control of multiple degrees of freedom and the use of individual motor neuron spike trains for direct control. The conclusion is that the gap between industry and academia is due to the relatively small functional improvement in daily situations that academic systems offer, despite the promising laboratory results, at the expense of a substantial reduction in robustness. None of the systems so far proposed in the literature fulfills all the important criteria needed for widespread acceptance by the patients, i.e. intuitive, closed-loop, adaptive, and robust real-time (&lt;;200 ms delay) control, minimal number of recording electrodes with low sensitivity to repositioning, minimal training, limited complexity and low consumption. Nonetheless, in recent years, important efforts have been invested in matching these criteria, with relevant steps forwards.",https://ieeexplore.ieee.org/document/6737308/,IEEE Transactions on Neural Systems and Rehabilitation Engineering,July 2014,ieeexplore
10.1109/PROC.1984.12819,The anticipated impact of supercomputers on finite-element analysis,IEEE,Journals,"The supercomputers of the 1980's have already impacted large-scale computation. This paper discusses the status and anticipated impact of supercomputers on finite-element analysis which is the primary tool for structural analysis and is also very useful in other areas of engineering analysis. The initial impact has been the significant reduction in turnaround time for large problems and the corresponding opportunity to solve heretofore unsolvable problems. In these cases, emphasis has been placed on employing already-proven computing software which was modifed to take advantage of vector processing and other forms of parallel operations. This trend is expected to continue because the established usage base of commercially available programs is not likely to be quickly dislodged. The near term will see the further use of design optimization, broader use of nonlinear mechanics, and a closer link between designers and analysts because of improved computer turnaround. The economy of scale suggests that solution techniques will be performed not only faster but cheaper than is possible with scalar processors which will further encourage the analysis of larger, more complex structures. The supercomputers of the future are expected to offer additional challenges to today's application systems. A primary factor in this will be the effective use of multiprocessors. Additional influence is expected as Artificial Intelligence matures to the point where Expert Systems become a reality for selected engineering and scientific disciplines. In order to effectively compete, today's software companies must address the possibility of significant changes in the architecture and methodology currently embodied in today's systems. Improved packaging, most likely in the form of pre- and postprocessors, will be necessary to provide industry- or technology-specific systems solutions.",https://ieeexplore.ieee.org/document/1457087/,Proceedings of the IEEE,Jan. 1984,ieeexplore
10.1109/ACCESS.2021.3083499,Towards Secured Online Monitoring for Digitalized GIS Against Cyber-Attacks Based on IoT and Machine Learning,IEEE,Journals,"Recently, the Internet of Things (IoT) has an important role in the growth and development of digitalized electric power stations while offering ambitious opportunities, specifically real-time monitoring and cybersecurity. In this regard, this paper introduces a novel IoT architecture for the online monitoring of the gas-insulated switchgear (GIS) status instead of the traditional observation methods. The proposed IoT architecture is derived from the concept of the cyber-physic system (CPS) in Industry 4.0. However, the cyber-attacks and the classification of the GIS insulation defects represent the main challenges against the implementation of IoT topology for the online monitoring and tracking of the GIS status. For this purpose, advanced machine learning techniques are utilized to detect cyber-attacks to conduct the paradigm and verification. Different test scenarios on various defects in GIS are performed to demonstrate the effectiveness of the proposed IoT architecture. Partial discharge pulse sequence features are extracted for each defect to represent the inputs for IoT architecture. The results confirm that the proposed IoT architecture based on the machine learning technique, that is the extreme gradient boosting (XGBoost), can visualize all defects in the GIS with different alarms, besides showing the cyber-attacks on the networks effectively. Furthermore, the defects of GIS and the fake data due to the cyber-attacks are recognized and presented on the dashboard of the proposed IoT platform with high accuracy and more clarified visualization to enhance the decision-making about the GIS status.",https://ieeexplore.ieee.org/document/9440436/,IEEE Access,2021,ieeexplore
10.1109/TCIAIG.2016.2528499,Using Behavior Objects to Manage Complexity in Virtual Worlds,IEEE,Journals,"The quality of high-level AI of nonplayer characters (NPCs) in commercial open-world games (OWGs) has been increasing during the past years. However, due to constraints specific to the game industry, this increase has been slow and it has been driven by larger budgets rather than adoption of new complex AI techniques. Most of the contemporary AI is still expressed as hard-coded scripts. The complexity and manageability of the script codebase is one of the key limiting factors for further AI improvements. In this paper, we address this issue. We present behavior objects (BO)-a general approach to development of NPC behaviors for large OWGs. BOs are inspired by object-oriented programming and extend the concept of smart objects. Our approach promotes encapsulation of data and code for multiple related behaviors in one place, hiding internal details and embedding intelligence in the environment. BOs are a natural abstraction of five different techniques that we have implemented to manage AI complexity in an upcoming AAA OWG. We report the details of the implementations in the context of behavior trees and the lessons learned during development. Our study should serve as an inspiration for AI architecture designers from both the academia and the industry.",https://ieeexplore.ieee.org/document/7406730/,IEEE Transactions on Computational Intelligence and AI in Games,June 2017,ieeexplore
10.1109/JIOT.2019.2946389,Using Collaborative Edge-Cloud Cache for Search in Internet of Things,IEEE,Journals,"With the Internet of Things (IoT) becoming the infrastructure to support domain applications, IoT search engines have attracted increasing attention from users, industry, and research community, since they are capable of crawling heterogeneous data sources in a highly dynamic environment. IoT search engines have to be able to process tens of thousands of spatial-time-keyword queries per second, making query throughput a critical issue. To achieve this heavy workload, caching mechanisms in collaborative edge-cloud computing architecture, which can implement the caching paradigm in cloud for frequent n -hop neighbor activity regions, is first proposed in this article. With our design, the frequent query result can be gained quickly from the spatial-time-keyword filtering index of n -hop neighbor regions by modeling keywords relevance and uncertain traveling time. In addition, we use STK-tree proposed previously to directly answer nonfrequent queries. Extensive experiments on real-life and synthetic data sets demonstrate that our proposed method outperforms the state-of-the-art approaches with respect to query time and message number.",https://ieeexplore.ieee.org/document/8863936/,IEEE Internet of Things Journal,Feb. 2020,ieeexplore
10.1109/ACCESS.2021.3082934,Visual Product Tracking System Using Siamese Neural Networks,IEEE,Journals,"Management of unstructured production data is a key challenge for Industry 4.0. Effective product tracking endorses data integration and productivity improvements throughout the manufacturing processes. Radio-frequency identification (RFID) tags are used in many tracking cases, but in some manufacturing environments, those cannot be used as they might get damaged or removed during processing. In this paper, we propose an alternative visual product tracking system. The physical system uses two cameras placed at the two ends of the tracked process(es). Product pairs are then matched with a Siamese neural network operating on the product images and trained offline on the problem at hand with labeled data. The proposed system can track products solely based on their visual appearance and without any physical interference with the products or production processes. Unlike other existing image-based methods, the proposed system is invariant to major positional and visual changes in the products. As a proof-of-concept, we tested the proposed system with real plywood factory data and were able to track the products with 98.5 % accuracy in a realistic test scenario. The implementation of the proposed method and the Veneer21 dataset are publicly available at https://github.com/TuomasJalonen/visual-product-tracking-system.",https://ieeexplore.ieee.org/document/9439511/,IEEE Access,2021,ieeexplore
10.1109/TASE.2020.2997718,Weighted Double-Low-Rank Decomposition With Application to Fabric Defect Detection,IEEE,Journals,"Recently, many methods based on low-rank representation have been proposed for fabric defect detection. Most of them relax the low-rank decomposition problem to a nuclear norm minimization (NNM) problem to pursue the convexity of the objective function. When solving the standard NNM problem, matrix singular values have to be treated equally. This, however, would be impractical in the scenario of fabric defect detection as the matrix singular values have clear physical meanings, and thus, they should be treated differently. In this article, we propose a weighted double-low-rank decomposition method (WDLRD) to treat the matrix singular values differently by assigning different weights. Thus, the most important/distinguishing characteristics of a fabric image can be preserved. Another difference between WDLRD and the other existing low-rank-based methods is that WDLRD considers a defective fabric image being decomposed to two low-rank matrices, i.e., low-rank defect-free matrix and low-rank defect matrix, as the defect-free and defective regions are usually composed of homogeneous objects that have a high correlation. Besides, WDLRD is more robust for defect detection in various situations by adding a noise term to avoid noise or other interference on the fabric surface. In addition, a defect prior is incorporated into the objective function of WDLRD to guide locating the defective regions. The proposed optimization problem can be easily solved by an iterative algorithm based on augmented Lagrange multipliers. Experimental results on TILDA, periodically patterned fabric, and Textile &amp; Apparel Artificial Intelligence databases show that the proposed WDLRD obtains better performance than state-of-the-art methods in locating the defective regions on fabric images. <italic>Note to Practitioners</italic>—This article is motivated by the problem that the performance of fabric defect detection in the textile industry is poor. It is necessary to develop an effective method to improve the defect detection accuracy and reduce overall manufacturing cost. Existing automatic defect detection approaches usually contain two stages: first, capture fabric images from the weaving machine and then use a defect detection algorithm in a host computer to conduct a real-time inspection and give an alarm if defects occur. This article focuses on locating defects for given defective images after the procedure of binary classification (which determines an image as defective or defect free). The article proposes an objective function to mathematically interpret the optimization problem between fabric images and predictive defects. The optimal solution can be obtained by employing the alternating direction method of multipliers (ADMMs). The proposed method is described as a new defect detection algorithm. Extensive experiments were conducted to evaluate the algorithm, and the experimental results indicate that the proposed method is superior to many existing fabric defect detection methods. Preliminary experiments suggest that this method is feasible but has not yet been really used in production. In future research, we will collect more fabric images from the textile industry and develop large-scale databases for verifying the proposed method for real-life applications.",https://ieeexplore.ieee.org/document/9123438/,IEEE Transactions on Automation Science and Engineering,July 2021,ieeexplore
10.1109/TAI.2021.3057027,ZJU-Leaper: A Benchmark Dataset for Fabric Defect Detection and a Comparative Study,IEEE,Journals,"Fabric inspection plays an important role in the process of quality control in textile manufacturing. There is a growing demand in the textile industry to leverage computer vision technology for more efficient quality control in the hope that it will replace the traditional labor-intensive inspection by naked eyes. However, there is an underlying viewpoint in most existing fabric datasets that automatic defect detection is a traditional image classification problem, thus more samples help better, which lacks enough consideration about the problem itself and real application environments. After deep communication with users, we find these facts that an assembly line usually has only a few fixed texture fabrics for a long period, users prefer fast deployment and easily upgradable model to a general model and long-time tuning, and users hope the process of collecting samples, annotating, and deployment affects assembly lines as little as possible. This implies that defect detection is different from popular deep learning problems. Multiple-stage models and fast training become more attractive since users are able to train and deploy models by themselves according to the real conditions of samples that can be obtained. Based on this analysis, we propose a new fabric dataset “ZJU-Leaper”. It provides a series of task settings in accordance with the progressive strategy dealing with the problem, from only normal samples to many defective samples with precise annotations, to facilitate real-world application. To avoid misleading information and inconsistency issues associated with the prior evaluation metrics, we propose a new evaluation protocol by experimental analysis of task-specific indexes, which can tell a truthful comparison between different inspection methods. We also offer some novel solutions to address the new challenges of our dataset, as part of the baseline experiments. It is our hope that ZJU-Leaper can accelerate the research of automated visual inspection and empower the practitioners with more opportunities for manufacturing automation in the textile industry. <p><italic>Impact Statement</italic>—Automatic defect inspection is very important in quality control of the fabric industry by helping manufacturers to identify production problems early, hence improving product quality and production efficiency. Meanwhile, it is able to reduce the high labor cost of manual inspection and boost the productivity of the textile industry. To develop effective mathematical inspection algorithms, the fabric dataset serves as an indispensable component to present a practical application environment and enable fair evaluation for algorithms. This paper proposes a new dataset, called “ZJU-Leaper” designed from a viewpoint of multiple-stage models and fast training, containing threefold novelty: 1) the data collection and organization consider the actual requirements and special characteristics of assembly lines in textile factories; 2) it has several designed task settings in order to meet the different levels of requirements in the practical inspection task; 3) it provides a reasonable evaluation protocol for comprehensive comparisons between different inspection algorithms. The preliminary experiments show that some existing algorithms still cannot reach the satisfying performance by this benchmark, which implies more effort should be made to develop new methods for the real use of automatic defect inspection.</p>",https://ieeexplore.ieee.org/document/9346038/,IEEE Transactions on Artificial Intelligence,Dec. 2020,ieeexplore
10.1109/COASE.2018.8560557,An Industry 4.0 Cyber-Physical Framework for Micro Devices Assembly,IEEE,Conferences,"An advanced cyber manufacturing framework to support the collaborative assembly of micro devices is presented based on Industry 4.0 principles. The distributed cyber and physical components work together to plan, assemble and monitor micro assembly related tasks; micro assembly refers to the assembly of micron sized devices which cannot be manufactured by MEMS technologies. The collaborative framework proposed includes assembly planning and path planning modules, Virtual Reality based assembly simulation environments and physical assembly work cells. An ontology based approach was implemented to address semantic interoperability issues to support formation of temporary partnerships in a Virtual Enterprise context. The key to the design and implementation of this complex framework is an information centric process modeling approach which provides a data/information oriented basis for collaboration. A collaborative cyber physical test bed has been built to demonstrate feasibility of proposed framework and approach.",https://ieeexplore.ieee.org/document/8560557/,2018 IEEE 14th International Conference on Automation Science and Engineering (CASE),20-24 Aug. 2018,ieeexplore
10.1109/SYSCON.2018.8369547,An interactive architecture for industrial scale prediction: Industry 4.0 adaptation of machine learning,IEEE,Conferences,"According to wiki definition, there are four design principles in Industry 4.0. These principles support companies in identifying and implementing Industry 4.0 scenarios, namely, Interoperability, Information transparency, Technical assistance, Decentralized decisions. In this paper we have discussed our work on an implementation of a machine learning based interactive architecture for industrial scale prediction for dynamic distribution of water resources across the continent, keeping the four corners of Industry 4.0 in place. We report the possibility of producing most probable high resolution estimation regarding the water balance in any region within Australia by implementation of an intelligent system that can integrate spatial-temporal data from various independent sensors and models, with the ground truth data produced by 250 practitioners from the irrigation industry across Australia. This architectural implementation on a cloud computing platform linked with a freely distributed mobile application, allowing interactive ground truthing of a machine learning model on a continental scale, shows accuracy of 90% with 85% sensitivity of correct surface soil moisture estimation with end users at its complete control. Along with high level of information transparency and interoperability, providing on-demand technical supports and motivating users by allowing them to customize and control their own local predictive models, show the successfulness of principles in Industry 4.0 in real environmental issues in the future adaptation in various industries starting from resource management to modern generation soft robotics.",https://ieeexplore.ieee.org/document/8369547/,2018 Annual IEEE International Systems Conference (SysCon),23-26 April 2018,ieeexplore
10.1109/PerComWorkshops51409.2021.9431009,Architecture and pervasive platform for machine learning services in Industry 4.0,IEEE,Conferences,"Pervasive computing promotes the integration of smart electronic devices in our living and working spaces in order to provide new, advanced services. Recently many prototype services based on machine learning techniques have been proposed in a number of domains like smart homes, smart buildings or smart plants. However, the number of applications effectively deployed in the real world is still limited. We believe that architectural principles and integrated frameworks are still missing today to successfully and repetitively support application developers and operators. In this paper, we present a novel architecture and a pervasive platform allowing the development of machine learning based applications in smart buildings.",https://ieeexplore.ieee.org/document/9431009/,2021 IEEE International Conference on Pervasive Computing and Communications Workshops and other Affiliated Events (PerCom Workshops),22-26 March 2021,ieeexplore
10.1109/WFCS.2019.8757952,Cloud-enabled Smart Data Collection in Shop Floor Environments for Industry 4.0,IEEE,Conferences,"Industry 4.0 transition is producing a remarkable change in the Smart Factories management. Modern companies can provide new services following products inside the shop floors along the entire production chain. To achieve the goal of servitization that the Industry 4.0 world requires, a modernization of current production chains is needed. This common demand comes mostly from manufacturing sector, where complex work machines collaborate with human workers. The data produced by the machines must be processed quickly, to allow the implementation of reactive services such as predictive maintenance, and remote control, always taking care of the safety of nearby people. This paper proposes a multi-layer architecture to monitor legacy production machines during their operations inside customers plants. The platform provides near real-time delivery of data collected from the machines with a high grade of customization according to customer needs. The performed tests show the scalability of the platform for a productive ecosystem with many machines at work, confirming its feasibility within different production facilities with different needs.",https://ieeexplore.ieee.org/document/8757952/,2019 15th IEEE International Workshop on Factory Communication Systems (WFCS),27-29 May 2019,ieeexplore
10.1109/WCNC45663.2020.9120761,Collaborative Learning Model for Cyberattack Detection Systems in IoT Industry 4.0,IEEE,Conferences,"Although the development of IoT Industry 4.0 has brought breakthrough achievements in many sectors, e.g., manufacturing, healthcare, and agriculture, it also raises many security issues to human beings due to a huge of emerging cybersecurity threats recently. In this paper, we propose a novel collaborative learning-based intrusion detection system which can be efficiently implemented in IoT Industry 4.0. In the system under consideration, we develop smart “filters” which can be deployed at the IoT gateways to promptly detect and prevent cyberattacks. In particular, each filter uses the collected data in its network to train its cyberattack detection model based on the deep learning algorithm. After that, the trained model will be shared with other IoT gateways to improve the accuracy in detecting intrusions in the whole system. In this way, not only the detection accuracy is improved, but our proposed system also can significantly reduce the information disclosure as well as network traffic in exchanging data among the IoT gateways. Through thorough simulations on real datasets, we show that the performance obtained by our proposed method can outperform those of the conventional machine learning methods.",https://ieeexplore.ieee.org/document/9120761/,2020 IEEE Wireless Communications and Networking Conference (WCNC),25-28 May 2020,ieeexplore
,Corporate Social Responsibility Challenges and Risks of Industry 4.0 technologies: A review,VDE,Conferences,"The fourth industrial revolution arrived with many enabling technologies that would impact important sociological aspects in the industry. Some of the Industry 4.0 technologies are already running in different industrial application, and other are still as a paradigm state. The social, economic, and environmental acceptance of Industry 4.0 technologies is still under discussion, which open new opportunities to execute various analysis about the possible implications of the implementation of such technologies. This article refers to an exploratory analysis and identification of the different challenges and risks of this new Industry 4.0 paradigm and its related technologies. The technologies under review were Internet of Things, Artificial Intelligence, Cloud Computing, cybersecurity, bid data, blockchain, 5G, robotics, adding manufacturing, unmanned systems, autonomous vehicles, virtual reality, and augmented reality. As a result, different social challenges and risks were identified for each technology, starting from vulnerability, implementation cost, until social aspects such as education and unemployment caused by those new technologies. In conclusion, Industry 4.0 arrived with a lot of benefits to the industry business, but companies should not stop thinking about sustainable development.",https://ieeexplore.ieee.org/document/8835964/,"Smart SysTech 2019; European Conference on Smart Objects, Systems and Technologies",4-5 June 2019,ieeexplore
10.1109/AINS47559.2019.8968698,Cyber Security Risk Assessment on Industry 4.0 using ICS testbed with AI and Cloud,IEEE,Conferences,"Industry 4.0 is a new concept, thus risk assessment is necessary. Several risk assessment methods for Industrial Control System (ICS) and Industry 4.0 have been proposed, however, it is difficult to identify impacts on the physical world caused by cyber attacks against ICS since many of these are based on tabletop analysis or software simulations. Therefore, we focus on the risk assessment using actual machines (ICS testbed) which can help to solve the above problems. In Industry 4.0, autonomous judgment and execution are required for the cyber-physical system, it is based on information exchange using Artificial Intelligence (AI) and cloud technologies. In this research, we evaluate cyber risks through attacks against ICS with AI and cloud using ICS testbed. The proposed method can clarify cyber risks and impacts on the real world, and corresponding countermeasures.",https://ieeexplore.ieee.org/document/8968698/,"2019 IEEE Conference on Application, Information and Network Security (AINS)",19-21 Nov. 2019,ieeexplore
10.1109/COINS51742.2021.9524088,Cyber Security Risks of Technical Components in Industry 4.0,IEEE,Conferences,"Industry 4.0 is a new concept of automation data exchange in manufacturing. Industry 4.0 consists of various technical components such as Artificial Intelligence (AI), IoT, OPC UA, and cloud. There could be cyber risks when these technical components are used in Industrial Control System (ICS) without security consideration. Therefore cyber risk assessment, reconsideration of implementation, operation are necessary to adopt Industry 4.0. Problems of previous research for ICS risk assessments are some of them are theoretical and conceptual methods such as simulations. Moreover, they do not focus on Industry 4.0 components.The objective of this research is to prove the concrete cyber risks and introduces corresponding secure implementations for Industry 4.0 components such as Artificial Intelligence (AI), the Internet of Things (IoT), OPC Unified Architecture (OPC UA). The proposed method has benefits for clarifying impacts on the real world by cyber-attacks through penetration tests against ICS testbed with actual machines.",https://ieeexplore.ieee.org/document/9524088/,2021 IEEE International Conference on Omni-Layer Intelligent Systems (COINS),23-25 Aug. 2021,ieeexplore
10.1109/CASE48305.2020.9216961,Decentralized coordination of autonomous AGVs for flexible factory automation in the context of Industry 4.0,IEEE,Conferences,"Future smart factories feature flexible systems that can dynamically reconFigure manufacturing systems via near real-time system monitoring and learning-based self-optimization. Automated guided vehicles (AGVs), as a critical method of transporting goods and material within a factory, is vital for flexible automation in a smart factory. However, there is an urgent gap in the ability to dynamically schedule and assign tasks for AGVs in a dynamic environment. In this research, we propose a decentralized AGV fleet architecture and task allocation method to enable dynamic allocation/reallocation of tasks in an AGV fleet. The developed algorithm can also reconFigure AGV task allocations to adapt to system changes, such as AGV failure and new AGVs joining the system. The system modeling, setup and algorithms are presented with a case study in a lab environment that demonstrates flexible collaboration between an AGV fleet and a robotic assembly cell.",https://ieeexplore.ieee.org/document/9216961/,2020 IEEE 16th International Conference on Automation Science and Engineering (CASE),20-21 Aug. 2020,ieeexplore
10.1109/EuCNC/6GSummit51104.2021.9482590,Empowering Industry 4.0 and Autonomous Drone Scouting use cases through 5G-DIVE Solution,IEEE,Conferences,"The 5G Edge Intelligence for Vertical Experimentation (5G-DIVE) project aims at demonstrating the technical merits and business value proposition of 5G technologies in two vertical pilots, namely the Industry 4.0 (I4.0) and Autonomous Drones Scout (ADS) pilots. This paper presents an overview of the overall 5G-DIVE solution and reports the results of the initial validation campaign of the selected use case, featuring 5G connectivity, distributed Edge computing, and artificial intelligence. The initial results for the I4.0 provide a baseline for next step validation campaign targeting a broader scale 5G implementation, while the ADS results provides promising results for enhancing the autonomous navigation in real-time.",https://ieeexplore.ieee.org/document/9482590/,2021 Joint European Conference on Networks and Communications & 6G Summit (EuCNC/6G Summit),8-11 June 2021,ieeexplore
10.1109/SMC.2019.8913901,Explainable Machine Learning in Industry 4.0: Evaluating Feature Importance in Anomaly Detection to Enable Root Cause Analysis,IEEE,Conferences,"In the past recent years, Machine Learning methodologies have been applied in countless application areas. In particular, they play a key role in enabling Industry 4.0. However, one of the main obstacles to the diffusion of Machine Learning-based applications is related to the lack of interpretability of most of these methods. In this work, we propose an approach for defining a `feature importance' in Anomaly Detection problems. Anomaly Detection is an important Machine Learning task that has an enormous applicability in industrial scenarios. Indeed, it is extremely relevant for the purpose of quality monitoring. Moreover, it is often the first step towards the design of a Machine Learning-based smart monitoring solution because Anomaly Detection can be implemented without the need of labelled data. The proposed feature importance evaluation approach is designed for Isolation Forest, one of the most commonly used algorithm for Anomaly Detection. The efficacy of the proposed method is tested on synthetic and real industrial datasets.",https://ieeexplore.ieee.org/document/8913901/,"2019 IEEE International Conference on Systems, Man and Cybernetics (SMC)",6-9 Oct. 2019,ieeexplore
10.1109/COMPSAC.2018.10204,Indoor Augmented Reality Using Deep Learning for Industry 4.0 Smart Factories,IEEE,Conferences,"This paper proposes to design, develop and implement a fast and markerless mobile augmented reality system to achieve the registration for, the visualization of, and the interaction with machines in indoor smart factories with Industry 4.0 vision. A lightweight deep-learning image detection module based on MobileNets running on mobile devices is used to detect/recognize different machines and different portions of machines. Internet of Things (IoT) networking allows machines and sensors in machines to report data, such as machine settings and machine states, to the cloud-side server. Thus, augmented information associated with a machine portion can be derived from the server and superimposed with the portion image shown on the device display. Furthermore, interaction methods based on touch gestures and distance calculation are also implemented. A prototype system is developed and tested in a mechanical workshop for the purpose of validation and evaluation. The system is shown to achieve high detection accuracy, intuitive visualization, and unique interaction modes.",https://ieeexplore.ieee.org/document/8377831/,2018 IEEE 42nd Annual Computer Software and Applications Conference (COMPSAC),23-27 July 2018,ieeexplore
10.1109/RTSI.2019.8895598,Intelligent Embedded Load Detection at the Edge on Industry 4.0 Powertrains Applications,IEEE,Conferences,"In the context of Industry 4.0, there has been great focus in developing intelligent sensors. Deploying them, condition monitoring and predictive maintenance have become feasible solutions to minimize operating and maintenance costs while also increasing safety. A critical aspect is the applied load to the supervised machinery system. Vibration data can be used to determine the current condition, but this needs signal processing specially developed and adapted to the monitored machine part for feature extraction. Artificial intelligence (AI) can, on one hand, simplify the development of such special purpose processing and on another hand be used to monitor and classify machine conditions by learning features directly from data. By bringing the AI computation as close as possible to the sensor (Edge-AI), data bandwidth can be minimized, system scalability and responsiveness can be improved and real-time requirements can be fulfilled. This work describes how Edge-AI on a STM32-bit microcontroller can be implemented. Our experimental setup demonstrates how AI can be effectively used to detect and classify the load on a powertrain. In order to do this, we use a MEMS capacity accelerometer to sense vibrations of the system. Also, this work demonstrates how Deep Neural Networks (DNN) for signal classification are build and trained by using an open-source deep learning framework and how the code library for the microcontroller is automatically generated afterwards by using STM32Cube. AI toolchain. We compare the classification accuracy of a memory compressed DNN against an uncompressed DNN.",https://ieeexplore.ieee.org/document/8895598/,2019 IEEE 5th International forum on Research and Technology for Society and Industry (RTSI),9-12 Sept. 2019,ieeexplore
10.1109/SCC47175.2019.9116104,Modeling and management of human resources in the reconfiguration of production system in industry 4.0 by neural networks,IEEE,Conferences,"In Industry 4.0, the role of employees changes significantly. Real-time production line control transforms job content. Work processes affect working conditions. The implementation of a socio-technical approach to the organization of work gives workers the opportunity to adapt their skills. Indeed, production work will become more and more multi-factor, especially with regard to control and decision-making tasks. In this paper, a proposal for an intelligent system for modeling skills and human resource management in the production system chain through the use of two artificial neural networks. The first NN1 network allows for the identification of the human factor, as well as the second NN2 network is reserved for valuing the human skills needed in Industry 4.0.",https://ieeexplore.ieee.org/document/9116104/,"2019 International Conference on Signal, Control and Communication (SCC)",16-18 Dec. 2019,ieeexplore
10.23919/DATE48585.2020.9116407,Network Synthesis for Industry 4.0,IEEE,Conferences,"Today's factory machines are ever more connected with SCADA, MES, ERP applications as well as external systems for data analysis. Different types of network architectures must be used for this purpose. For instance, control applications at the lowest level are susceptible to delays and errors while data analysis with machine learning procedures requires to move a large amount of data without real-time constraints. Standard data formats, like Automation Markup Language (AML), have been established to document factory environment, machine placement and network deployment, however, no automatic technique is currently available in the context of Industry 4.0 to choose the best mix of network architectures according to spacial constraints, cost, and performance. We propose to fill this gap by formulating an optimization problem. First of all, spatial and communication requirements are extracted from the AML description. Then, the optimal interconnection of wired or wireless channels is obtained according to application objectives. Finally, this result is back-annotated to AML to be used in the life cycle of the production system. The proposed methodology is described through a small, but complete, smart production plant.",https://ieeexplore.ieee.org/document/9116407/,"2020 Design, Automation & Test in Europe Conference & Exhibition (DATE)",9-13 March 2020,ieeexplore
10.1109/IDAP.2018.8620887,Smart Grids and Industry 4.0,IEEE,Conferences,"Since the beginning of the Industrial Revolution, the concept of Industrial Revolution in the 4<sup>th</sup>/ Industry 4.0 which is a new turning point in the technologies used, is in every field as well as in the energy field which is considered as the key point of all revolutions. It is aimed that energy generation, transmission and distribution are more efficient and highly reliable together with the next generation software and hardware which is the result of this concept.It has been seen that existing networks can not be enough to achieve this goal.At this point, flexible, reliable, clean, sustainable and highly efficient electric energy due to smart grids come into play. However, due to the increase in demand for electricity due to increase in population and industrialization, the dependence of electric energy on production resources, the availability of renewable energy sources in a dispersed geographical structure and the loss and leakage are becoming more important in our country. The Cyber-Physical System, the Internet of Things, M2M (machine to machine), etc components are used in the energy field, especially in intelligent network systems, resulting in intensive use of technology at every stage from power generation to end-user distribution point. This includes better system operation, reduced costs, increased energy efficiency, reduced greenhouse effect, reduced downtime with reduced downtime, reduced loss/leakage rates, improved energy quality, better management of production and storage systems, intelligent meter reading and load management and real-time supply-demand management. Industry 4.0/4.Industrial Revolution and smart grids have many common aspects such as optimization, automation, efficient use and management of energy, intelligent production, everything internet. In this study, the development process of the Industrial 4.0 revolution in the world and in our country will be examined and the interaction with these process smart grids will be examined.",https://ieeexplore.ieee.org/document/8620887/,2018 International Conference on Artificial Intelligence and Data Processing (IDAP),28-30 Sept. 2018,ieeexplore
10.1109/WRC-SARA.2019.8931920,Software-defined Cloud Manufacturing in the Context of Industry 4.0,IEEE,Conferences,"In the practice of “Cloud Manufacturing (CMfg)” or “Industrial Internet”, there still exist key problems, including: 1) big data analytics and decision-making in the cloud could not meet the requirements of time-sensitive manufacturing applications, moreover uploading ZettaBytes of future device data to the cloud may cause serious network congestion, 2) the manufacturing system lacks openness and evolvability, thus restricting the rapid optimization and transformation of the system, 3) big data from the shop-floor IoT devices and the internet has not been effectively utilized to guide the optimization and upgrade of the manufacturing system. In view of these key practical problems, we propose an open evolutionary architecture of intelligent CMfg system with collaborative edge and cloud processing capability. Hierarchical gateways near shop-floor things are introduced to enable fast processing for time-sensitive applications. Big data in another dimension from the software defined perspective will be used to decide the efficient operations and highly dynamic upgrade of the system. From the software system view, we also propose a new mode - AI-Mfg-Ops (AI-enabled Cloud Manufacturing Operations) with a supporting framework, which can promote the fast operation and upgrading of CMfg systems with AI enabled monitoring-analysis-planning-execution close loop. This work can improve the universality of CMfg for real-time fast response and operation &amp; upgrading.",https://ieeexplore.ieee.org/document/8931920/,2019 WRC Symposium on Advanced Robotics and Automation (WRC SARA),21-22 Aug. 2019,ieeexplore
10.1109/ICRITO48877.2020.9198036,State of Art: Energy Efficient Protocols for Self-Powered Wireless Sensor Network in IIoT to Support Industry 4.0,IEEE,Conferences,"Up gradation of manufacturing systems in industries by means of implementing innovative manufacturing techniques that captures real time data, applies machine learning algorithms, makes entire system self-decisive and provides inter connectivity to the whole system is the prime focus of the Industry 4.0. It is aimed at bringing new industrial revolution with the help of internet of things technology due to its considerable influence in the industrial manufacturing process. Though impact of internet of things in industrial sector is huge, a practical implementation incorporates challenges in energy efficiency, self-powered sensor nodes and security. For processing of gathered data self-powered sensor nodes may sinks energy from ambient energy sources. A considerable amount of efforts has been put by researchers to address the challenges for development of energy efficient routing protocol for such nodes. The presented survey is aimed at analyzing the protocols for contribution towards the goals of industry 4.0 Energy efficient protocol will support the system to consume least energy for its operation. Henceforth combination of self-powered wireless sensor network and energy efficient protocol will be useful to power up many industrial IoT applications.",https://ieeexplore.ieee.org/document/9198036/,"2020 8th International Conference on Reliability, Infocom Technologies and Optimization (Trends and Future Directions) (ICRITO)",4-5 June 2020,ieeexplore
10.1109/ISWCS.2019.8877305,Visible Light Positioning for Location-Based Services in Industry 4.0,IEEE,Conferences,"Industry 4.0 refers to the evolution in manufacturing from computerization to fully cyberphysical systems that exploit rich sensor data, adaptive real-time safety-critical control, and machine learning. An important aspect of this vision is the sensing and subsequent association of objects in the physical world with their cyber and virtual counterparts. In this paper we propose Visible Light Positioning (VLP) as an enabler for these Industry 4.0 applications. We also explore sensing techniques, including cameras (and depth sensors), and other light-based solutions for object positioning and detection along with their respective limitations. We then demonstrate an application of positioning for real time robot control in an interactive multiparty cyber-physical-virtual deployment. Lastly, based on our experience with this cyberphysical-virtual application, we propose Ray-Surface Positioning (RSP), a novel VLP technique, as a low cost positioning system for Industry 4.0.",https://ieeexplore.ieee.org/document/8877305/,2019 16th International Symposium on Wireless Communication Systems (ISWCS),27-30 Aug. 2019,ieeexplore
10.1109/BDCloud.2018.00136,"iSTEP, an Integrated Self-Tuning Engine for Predictive Maintenance in Industry 4.0",IEEE,Conferences,"The recent expansion of IoT-enabled (Internet of Things) devices in manufacturing contexts and their subsequent data-driven exploitation paved the way to the advent of the Industry 4.0, promoting a full integration of IT services, smart devices, and control systems with physical objects, their electronics and sensors. The real-time transmission and analysis of collected data from factories has the potential to create manufacturing intelligence, of which predictive maintenance is an expression. Hence the need to design new approaches able to manage not only the data volume, but also the variety and velocity, extracting actual value from the humongous amounts of collected data. To this aim, we present iSTEP, an integrated Self-Tuning Engine for Predictive maintenance, based on Big Data technologies and designed for Industry 4.0 applications. The proposed approach targets some of the most common needs of manufacturing enterprises: compatibility with both the on-premises and the in-the-cloud environments, exploitation of reliable and largely supported Big Data platforms, easy deployment through containerized software modules, virtually unlimited horizontal scalability, fault-tolerant self-reconfiguration, flexible yet friendly streaming-KPI computations, and above all, the integrated provisioning of self-tuning machine learning techniques for predictive maintenance. The current implementation of iSTEP exploits a distributed architecture based on Apache Kafka, Spark Streaming, MLlib, and Cassandra; iSTEP provides (i) a specific feature engineering block aimed at automatically extracting metrics from the production monitoring time series, which improves the predictive performance by 77% on average, and (ii) a self-tuning approach that dynamically selects the best prediction algorithm, which improves the predictive performance up to 60%. The iSTEP engine provides transparent predictive models, able to provide end users with insights into the knowledge learned, and it has been experimentally evaluated on a public unbalanced failure dataset, whose extensive results are discussed in the paper.",https://ieeexplore.ieee.org/document/8672266/,"2018 IEEE Intl Conf on Parallel & Distributed Processing with Applications, Ubiquitous Computing & Communications, Big Data & Cloud Computing, Social Computing & Networking, Sustainable Computing & Communications (ISPA/IUCC/BDCloud/SocialCom/SustainCom)",11-13 Dec. 2018,ieeexplore
10.1109/TLA.2021.9480156,A Data Governance Framework for Industry 4.0,IEEE,Journals,"The fourth industrial revolution, or Industry 4.0, represents a new stage of evolution in the organization, management and control of the value chain throughout the product or service life cycle. This digitization of the industrial environment is characterized by the connection of Information Technologies (IT) and Operations Technologies (OT) through cyber-physical systems and the Industrial IoT (IIoT). One of the main consequences of this integration is the increasing amount and variety of data generated in real time from different sources. In this environment of intensive generation of actionable information, data becomes a critical asset for Industry 4.0, at all stages of the value chain. However, in order to data become a competitive advantage for the company, it must be managed and governed like any other strategic asset, and therefore it is necessary to rely on a Data Governance system. Industry 4.0 requires a reformulation of governance since the data is a key element and the backbone of the processes of the organization. This paper proposes a Reference Framework for the implementation of Data Governance Systems for Industry 4.0. Previously, it contextualizes data governance for Industry 4.0 environments and identifies the requirements that this framework must address, which are conditioned by the specific features of Industry 4.0, among others, the intensive use of big data, cloud and edge computing, artificial intelligence and current regulations.",https://ieeexplore.ieee.org/document/9480156/,IEEE Latin America Transactions,Dec. 2021,ieeexplore
10.1109/ACCESS.2021.3121302,A Survey on Industry 4.0 for the Oil and Gas Industry: Upstream Sector,IEEE,Journals,"The market volatility in the oil and gas (O&amp;G) sector, the dwindling demand for oil due to the impact of COVID-19, and the push for alternative greener energy are driving the need for innovation and digitization in the O&amp;G industry. This has attracted research interest from academia and the industry in the application of industry 4.0 (I4.0) technologies in the O&amp;G sector. The application of some of these I4.0 technologies has been presented in the literature, but the domain still lacks a comprehensive survey of the application of I4.0 in the O&amp;G upstream sector. This paper investigates the state-of-the-art efforts directed toward I4.0 technologies in the O&amp;G upstream sector. To achieve this, first, an overview of the I4.0 is discussed followed by a systematic literature review from an integrative perspective for publications between 2012–2021 with 223 analyzed documents. The benefits and challenges of the adoption of I4.0 have been identified. Moreover, the paper adds value by proposing a framework for the implementation of I4.0 in the O&amp;G upstream sector. Finally, future directions and research opportunities such as framework, edge computing, quantum computing, communication technologies, standardization, and innovative areas related to the implementation of I4.0 in the upstream sector are presented. The findings from this review show that I4.0 technologies are currently being explored and deployed for various aspects of the upstream sector. However, some of the I4.0 technologies like additive manufacturing and virtual reality are least explored.",https://ieeexplore.ieee.org/document/9579415/,IEEE Access,2021,ieeexplore
10.1109/TII.2019.2962029,An Entropy-Based Approach to Real-Time Information Extraction for Industry 4.0,IEEE,Journals,"Industry 4.0 has drawn considerable attention from industry and academic research communities. The recent advances in Internet of Things (IoT), Big Data analytics, sensor technology, and artificial intelligence have led to the design and implementation of novel approaches to take full advantage of data-driven solutions applicable to Industry 4.0. With the availability of large datasets, it has become crucially important to identify the appropriate amount of relevant information, which would optimize the overall analysis of the corresponding systems. In this article, specific properties of dynamically evolving data systems are introduced and investigated, which provide framework to assess the appropriate amount of representative information.",https://ieeexplore.ieee.org/document/8941297/,IEEE Transactions on Industrial Informatics,Sept. 2020,ieeexplore
10.1109/IECON.2018.8592763,A Machine Learning Approach Applied to Energy Prediction in Job Shop Environments,IEEE,Conferences,"Energy efficiency has become a great challenge for manufacturing companies. Although it is possible to improve efficiency applying new and more efficient machines, decision makers tend to look for some less expensive alternatives. In this context, the adoption of more efficient strategies during the production planning can allow the reduction in energy consumption and associated emissions. Furthermore, the current reality of manufacturing companies, brought by Industry 4.0 concepts, requires more flexibility of production systems, thus, increasing complexity for machine rescheduling without compromising sustainable requirements. In this paper, we propose a method to predict total energy consumption in job shop systems applying machine learning techniques. Different schedules may result in different consumption rates. However, there is a nonlinear relationship between these targets. Therefore, an Artificial Neural Network (ANN) is applied for a quick estimation of total energy consumption. In order to validate the model, computational experiments, using digital manufacturing software tools, are performed on different job shop configurations to show the efficiency of the proposed model.",https://ieeexplore.ieee.org/document/8592763/,IECON 2018 - 44th Annual Conference of the IEEE Industrial Electronics Society,21-23 Oct. 2018,ieeexplore
10.1109/COMPSAC48688.2020.0-202,A Modular Edge-/Cloud-Solution for Automated Error Detection of Industrial Hairpin Weldings using Convolutional Neural Networks,IEEE,Conferences,"The traction battery and the electric motor are the most important components of the electrified powertrain. To increase the energy efficiency of the electric motor, wound copper wires are being replaced by coated rectangular copper wires, so-called hairpins. Hence, to connect the hairpins conductively, they must be welded together. However, such new production processes are unknown compared with classic motor production. Therefore, this research aims to integrate Industry 4.0 techniques, such as cloud and edge computing, and advanced data analysis in the production process to better understand and optimize the manufacturing processes. Welding defects are classified with the help of a convolutional neural network (CNN) (predictive analysis) and, depending on the defect, a recommended course of action for reworking (prescriptive analysis) is given. However, the application of such complex algorithms as neural networks to large amounts of data requires huge computing resources. Therefore, a modular combination of an edge and cloud architecture is proposed in this paper. Furthermore, a pure cloud solution is compared with the edge solution.",https://ieeexplore.ieee.org/document/9202655/,"2020 IEEE 44th Annual Computers, Software, and Applications Conference (COMPSAC)",13-17 July 2020,ieeexplore
10.1109/PacificVis.2018.00026,A Visual Analytics Approach for Equipment Condition Monitoring in Smart Factories of Process Industry,IEEE,Conferences,"Monitoring equipment conditions is of great value in manufacturing, which can not only reduce unplanned downtime by early detecting anomalies of equipment but also avoid unnecessary routine maintenance. With the coming era of Industry 4.0 (or industrial internet), more and more assets and machines in plants are equipped with various sensors and information systems, which brings an unprecedented opportunity to capture large-scale and fine-grained data for effective on-line equipment condition monitoring. However, due to the lack of systematic methods, analysts still find it challenging to carry out efficient analyses and extract valuable information from the mass volume of data collected, especially for process industry (e.g., a petrochemical plant) with complex manufacturing procedures. In this paper, we report the design and implementation of an interactive visual analytics system, which helps managers and operators at manufacturing sites leverage their domain knowledge and apply substantial human judgements to guide the automated analytical approaches, thus generating understandable and trustable results for real-world applications. Our system integrates advanced analytical algorithms (e.g., Gaussian mixture model with a Bayesian framework) and intuitive visualization designs to provide a comprehensive and adaptive semi-supervised solution to equipment condition monitoring. The example use cases based on a real-world manufacturing dataset and interviews with domain experts demonstrate the effectiveness of our system.",https://ieeexplore.ieee.org/document/8365986/,2018 IEEE Pacific Visualization Symposium (PacificVis),10-13 April 2018,ieeexplore
10.1109/ICE/ITMC49519.2020.9198625,A digital twin model of a pasteurization system for food beverages: tools and architecture,IEEE,Conferences,"Many enabling technologies of Industry 4.0 (Internet of Things “IoT”, Cloud systems, Big Data Analytics) contribute to the creation of what is the Digital Twin or virtual twin of a physical process, that is a mathematical model capable of describing the process, product or service in a precise way in order to carry out analyses and apply strategies. Digital Twin models integrate artificial intelligence, machine learning and analytics software with the data collected from the production plants to create digital simulation models that update when the parameters of the production processes or the working conditions change. This is a self-learning mechanism, which makes use of data collected from various sources (sensors that transmit operating conditions; experts, such as engineers with deep knowledge of the industrial domain; other similar machines or fleets of similar machines) and integrates also historical data relating to the past use of the machine. Starting from the virtual twin vision, simulation plays a key role within the Industry 4.0 transformation. Creating a virtual prototype has become necessary and strategic to raise the safety levels of the operators engaged in the maintenance phases, but above all the integration of the digital model with the IoT has become particularly effective, as the advent of software platforms offers the possibility of integrating real-time data with all the digital information that a company owns on a given process, ensuring the realization of the Digital Twin. In this context, this work aims at developing optimized solutions for application in a beverage pasteurization system using the Digital Twin approach, capable of creating a virtual modelling of the process and preventing high-risk events for operators.",https://ieeexplore.ieee.org/document/9198625/,"2020 IEEE International Conference on Engineering, Technology and Innovation (ICE/ITMC)",15-17 June 2020,ieeexplore
10.1109/WINCOM50532.2020.9272477,A new middleware for managing heterogeneous robot in ubiquitous environments,IEEE,Conferences,"Heterogeneity is one of the main issues for the deployment of the Industry 4.0. This is due to the diversity in the available robots and the IIoT devices. These equipments use different programming languages and communication protocols. To make the integration of such equipments easy, we propose TalkRoBots, a middleware that allows heterogeneous robots and IIoT devices to communicate together and exchange data in a transparent way. The middleware was experimented in a real scenario with different robots that demonstrate its efficiency.",https://ieeexplore.ieee.org/document/9272477/,2020 8th International Conference on Wireless Networks and Mobile Communications (WINCOM),27-29 Oct. 2020,ieeexplore
10.1109/INDIN.2017.8104789,Addressing security challenges in industrial augmented reality systems,IEEE,Conferences,"In context of Industry 4.0 Augmented Reality (AR) is frequently mentioned as the upcoming interface technology for human-machine communication and collaboration. Many prototypes have already arisen in both the consumer market and in the industrial sector. According to numerous experts it will take only few years until AR will reach the maturity level to be deployed in productive applications. Especially for industrial usage it is required to assess security risks and challenges this new technology implicates. Thereby we focus on plant operators, Original Equipment Manufacturers (OEMs) and component vendors as stakeholders. Starting from several industrial AR use cases and the structure of contemporary AR applications, in this paper we identify security assets worthy of protection and derive the corresponding security goals. Afterwards we elaborate the threats industrial AR applications are exposed to and develop an edge computing architecture for future AR applications which encompasses various measures to reduce security risks for our stakeholders.",https://ieeexplore.ieee.org/document/8104789/,2017 IEEE 15th International Conference on Industrial Informatics (INDIN),24-26 July 2017,ieeexplore
10.1109/CEWIT.2013.6713745,Agent-based planning and control for groupage traffic,IEEE,Conferences,"In this research and technology transfer project, the planning and control processes of the industrial partner Hellmann Worldwide Logistics GmbH &amp; Co. KG are analyzed. An agent-based approach is presented to model current processes and to exploit the identified optimization potential. The developed system directly connects the information flow and the material flow as well as their interdependencies in order to optimize the planning and control in groupage traffic. The software system maps current processes to agents as system components and improves the efficiency by intelligent objects. To handle the high complexity and dynamics of logistics autonomous intelligent agents plan and control the way of represented objects through the logistic network by themselves and induce a flexible and reactive system behavior. We evaluate the implemented dispatching application by simulating the groupage traffic processes using effectively transported orders and process data provided by our industrial partner. Moreover, we modeled real world infrastructures and considered also the dynamics by the simulation of unexpected events and process disturbances. The results show that the system significantly decreases daily cost by reducing the required number of transport providers and shifting conventional orders to next days, which need no immediate delivery. Thus the system increases the efficiency and meets the special challenges and requirements of groupage traffic. Moreover, the system supports freight carriers and dispatchers with adequate tour and routing proposals. Computed tours were successfully validated by human dispatchers. Due to the promising results, Hellmann is highly interested in transferring the prototype to an application that optimizes the daily operations in numerous distribution centers. Finally, provide further research perspectives, and emphasize the advantages of the developed system in Industry 4.0 applications.",https://ieeexplore.ieee.org/document/6713745/,2013 10th International Conference and Expo on Emerging Technologies for a Smarter World (CEWIT),21-22 Oct. 2013,ieeexplore
10.1109/DCOSS49796.2020.00046,An Agnostic Data-Driven Approach to Predict Stoppages of Industrial Packing Machine in Near,IEEE,Conferences,"As data awareness in manufacturing companies increases with the deployment of sensors and Internet of Things (IoT) devices, data-driven maintenance and prediction have become quite popular in the Industry 4.0 paradigm. Machine Learning (ML) has been recognised as a promising, efficient and reliable tool for fault detection use cases, as it allows to export important knowledge from monitored assets. Scientists deal with issues such as the small amount of data that indicate potential problems, or the imbalance which exists between the standard process data and the data inadequacy of the systems to make a high precision forecast. Currently, in this context, even large industries are not able to effectively predict abnormal behaviors in their tools, processes and equipment, when adopting strategies to anticipate crucial events. In this paper, we propose a methodology to enable prediction of a packing machine's stoppages in manufacturing process of a large industry, by using forecasting techniques based on univariate time series data. There are more than 100 reasons that cause the machine to stop, in a quite big production line length. However, we use a single signal, concerning the machines operational status to make our prediction, without considering other fault or warning signals, hence its characterization as ""agnostic"". A workflow is presented for cleaning and preprocessing the data, and for training and evaluating a predictive model. Two predictive models, namely ARIMA and Prophet, are applied and evaluated on real data from an advanced machining process used for packing. Training and evaluation tests indicate that the results of the applied methods perform well on a daily basis. Our work can be further extended and act as reference for future research activities that could lead to more robust and accurate prediction frameworks.",https://ieeexplore.ieee.org/document/9183540/,2020 16th International Conference on Distributed Computing in Sensor Systems (DCOSS),25-27 May 2020,ieeexplore
10.1109/ICAPAI49758.2021.9462061,An Industrial Assistance System with Manual Assembly Step Recognition in Virtual Reality,IEEE,Conferences,"In the era of Industry 4.0, worker assistance systems are becoming more and more important. In order to assist shop floor workers in manual assembly tasks, we implemented an assistance system in virtual reality. A deep neural network was trained to recognize the current work step in real-time during an assembly process, thus giving the assistance system context-awareness. We defined the problem of assembly step recognition as a multivariate time series classification using the poses of the workers head, both hands and all relevant tools and objects. With this definition, the VR environment's output can also be replaced with data from the real world. For our proof-of-concept assembly step recognition system, we created an assembly process consisting of six different work steps, five movable assembly parts and one tool. We showed that we can train an activity recognition model for assembly steps with only 10 assembly recordings. To achieve this, we used multiple data augmentation techniques and proposed a novel method of synthesizing new training data, which we call Path Joining. With only 10 training recordings, we attain a categorical classification accuracy of 81 percent and with 60 recordings we achieve an accuracy of 89 percent.",https://ieeexplore.ieee.org/document/9462061/,2021 International Conference on Applied Artificial Intelligence (ICAPAI),19-21 May 2021,ieeexplore
10.1109/ICDCS.2019.00139,An Industrial IoT Solution for Evaluating Workers' Performance Via Activity Recognition,IEEE,Conferences,"The Industrial Internet of Things (IIoT) is a key pillar of the Fourth Industrial Evolution or Industry 4.0. It aims to achieve direct information exchange between industrial machines, people, and processes. By tapping and analysing such data, IIoT can more importantly provide for significant improvements in productivity, product quality, and safety via proactive detection of problems in the performance and reliability of production machines, workers, and industrial processes. While the majority of existing IIoT research is currently focusing on the predictive maintenance of industrial machines (unplanned production stoppages lead to significant increases in costs and lost plant productivity), this paper focuses on monitoring and assessing worker productivity. This IIoT research is particularly important for large manufacturing plants where most production activities are performed by workers using tools and operating machines. With this aim, this paper introduces a novel industrial IoT solution for monitoring, evaluating, and improving worker and related plant productivity based on workers activity recognition using a distributed platform and wearable sensors. More specifically, this IIoT solution captures acceleration and gyroscopic data from wearable sensors in edge computers and analyses them in powerful processing servers in the cloud to provide a timely evaluation of the performance and productivity of each individual worker in the production line. These are achieved by classifying worker production activities and computing Key Performance Indicators (KPIs) from the captured sensor data. We present a real-world case study that utilises our IIoT solution in a large meat processing plant (MPP). We illustrate the design of the IIoT solution, describe the in-plant data collection during normal operation, and present the sensor data analysis and related KPI computation, as well as the outcomes and lessons learnt.",https://ieeexplore.ieee.org/document/8884821/,2019 IEEE 39th International Conference on Distributed Computing Systems (ICDCS),7-10 July 2019,ieeexplore
10.1109/COASE.2018.8560557,An Industry 4.0 Cyber-Physical Framework for Micro Devices Assembly,IEEE,Conferences,"An advanced cyber manufacturing framework to support the collaborative assembly of micro devices is presented based on Industry 4.0 principles. The distributed cyber and physical components work together to plan, assemble and monitor micro assembly related tasks; micro assembly refers to the assembly of micron sized devices which cannot be manufactured by MEMS technologies. The collaborative framework proposed includes assembly planning and path planning modules, Virtual Reality based assembly simulation environments and physical assembly work cells. An ontology based approach was implemented to address semantic interoperability issues to support formation of temporary partnerships in a Virtual Enterprise context. The key to the design and implementation of this complex framework is an information centric process modeling approach which provides a data/information oriented basis for collaboration. A collaborative cyber physical test bed has been built to demonstrate feasibility of proposed framework and approach.",https://ieeexplore.ieee.org/document/8560557/,2018 IEEE 14th International Conference on Automation Science and Engineering (CASE),20-24 Aug. 2018,ieeexplore
10.1109/ISSE46696.2019.8984462,An IoT Reconfigurable SoC Platform for Computer Vision Applications,IEEE,Conferences,"The field of Internet of Things (IoT) and smart sensors has expanded rapidly in various fields of research and industrial applications. The area of IoT robotics has become a critical component in the evolution of Industry 4.0 standard. In this paper, we developed an IoT based reconfigurable System on Chip (SoC) robot that is fast and efficient for computer vision applications. It can be deployed in other IoT robotics applications and achieve its intended function. A Terasic Hexapod Spider Robot (TSR) was used with its DE0-Nano SoC board to implement our IoT robotics system. The TSR was designed to provide a competent computer vision application to recognize different shapes using a machine learning classifier. The data processing for image detection was divided into two parts, the first part involves hardware implementation on the SoC board and to provide real-time interaction of the robot with the surrounding environment. The second part of implementation is based on the cloud processing technique, where further data analysis was performed. The image detection algorithm for the computer vision component was tested and successfully implemented to recognize shapes. The TSR moves or reacts based on the detected image. The Field Programmable Gate Array (FPGA) part is programmed to handle the movement of the robot and the Hard Processor System (HPS) handles the shape recognition, Wi-Fi connectivity, and Bluetooth communication. This design is implemented, tested and can be used in real-time applications in harsh environments where movements of other robots are restricted.",https://ieeexplore.ieee.org/document/8984462/,2019 International Symposium on Systems Engineering (ISSE),1-3 Oct. 2019,ieeexplore
10.1109/SYSCON.2018.8369547,An interactive architecture for industrial scale prediction: Industry 4.0 adaptation of machine learning,IEEE,Conferences,"According to wiki definition, there are four design principles in Industry 4.0. These principles support companies in identifying and implementing Industry 4.0 scenarios, namely, Interoperability, Information transparency, Technical assistance, Decentralized decisions. In this paper we have discussed our work on an implementation of a machine learning based interactive architecture for industrial scale prediction for dynamic distribution of water resources across the continent, keeping the four corners of Industry 4.0 in place. We report the possibility of producing most probable high resolution estimation regarding the water balance in any region within Australia by implementation of an intelligent system that can integrate spatial-temporal data from various independent sensors and models, with the ground truth data produced by 250 practitioners from the irrigation industry across Australia. This architectural implementation on a cloud computing platform linked with a freely distributed mobile application, allowing interactive ground truthing of a machine learning model on a continental scale, shows accuracy of 90% with 85% sensitivity of correct surface soil moisture estimation with end users at its complete control. Along with high level of information transparency and interoperability, providing on-demand technical supports and motivating users by allowing them to customize and control their own local predictive models, show the successfulness of principles in Industry 4.0 in real environmental issues in the future adaptation in various industries starting from resource management to modern generation soft robotics.",https://ieeexplore.ieee.org/document/8369547/,2018 Annual IEEE International Systems Conference (SysCon),23-26 April 2018,ieeexplore
10.1109/Confluence51648.2021.9377160,Application of Artificial Intelligence in Human Resource Management Practices,IEEE,Conferences,"In the dynamic and competitive world, technology has changed the pace of all the industry. Artificial intelligence is a technology which enables the industry to grow at faster pace and efficiently finishing their work. This technology has entered into various departments such as finance department, human resource department, marketing, production etc. AI system has enabled the organization to enhance their existing performance and efficiently performing functions on a day-to-day basis. Currently, due to dynamic and competitive environment people working at different managerial level are working under pressure and understanding the need of artificial intelligence at workplace. Authors have used quantitative research to conduct the research and regression methods has been used to analyse the data. AI as a technology has a role in the different HR practices starting from talent acquisition and extending it to the assessing the performance of the people at work place. This research will study the relation of artificial intelligence and HR functions and different functions performed by HR department. The objective is to understand the factor like innovativeness and how use of HR operations. To conduct the study HR professionals from different IT companies were considered. Through the analysis the result indicated the positive linkage between different factors such ease of use and innovativeness which clearly indicates that AI has a influence on both the factors. This research paper will provide indepth knowledge about artificial intelligence which is at present a new revolution in industry and referred as industry 4.0.",https://ieeexplore.ieee.org/document/9377160/,"2021 11th International Conference on Cloud Computing, Data Science & Engineering (Confluence)",28-29 Jan. 2021,ieeexplore
10.1109/NetSoft48620.2020.9165393,Benchmarking and Profiling 5G Verticals' Applications: An Industrial IoT Use Case,IEEE,Conferences,"The Industry 4.0 sector is evolving in a tremendous pace by introducing a set of industrial automation mechanisms tightly coupled with the exploitation of Internet of Things (IoT), 5G and Artificial Intelligence (AI) technologies. By combining such emerging technologies, interconnected sensors, instruments, and other industrial devices are networked together with industrial applications, formulating the Industrial IoT (IIoT) and aiming to improve the efficiency and reliability of the deployed applications and provide Quality of Service (QoS) guarantees. However, in a 5G era, efficient, reliable and highly performant applications' provision has to be combined with exploitation of capabilities offered by 5G networks. Optimal usage of the available resources has to be realised, while guaranteeing strict QoS requirements such as high data rates, ultra-low latency and jitter. The first step towards this direction is based on the accurate profiling of vertical industries' applications in terms of resources usage, capacity limits and reliability characteristics. To achieve so, in this paper we provide an integrated methodology and approach for benchmarking and profiling 5G vertical industries' applications. This approach covers the realisation of benchmarking experiments and the extraction of insights based on the analysis of the collected data. Such insights are considered the cornerstones for the development of AI models that can lead to optimal infrastructure usage along with assurance of high QoS provision. The detailed approach is applied in a real IIoT use case, leading to profiling of a set of 5G network functions.",https://ieeexplore.ieee.org/document/9165393/,2020 6th IEEE Conference on Network Softwarization (NetSoft),29 June-3 July 2020,ieeexplore
10.1109/ICAIIC51459.2021.9415189,Big Data Platform for Intelligence Industrial IoT Sensor Monitoring System Based on Edge Computing and AI,IEEE,Conferences,"The cutting edge of Industry 4.0 has driven everything to be converted to disruptive innovation and digitalized. This digital revolution is imprinted by modern and advanced technology that takes advantage of Big Data and Artificial Intelligence (AI) to nurture from automatic learning systems, smart city, smart energy, smart factory to the edge computing technology, and so on. To harness an appealing, noteworthy, and leading development in smart manufacturing industry, the modern industrial sciences and technologies such as Big Data, Artificial Intelligence, Internet of things, and Edge Computing have to be integrated cooperatively. Accordingly, a suggestion on the integration is presented in this paper. This proposed paper describes the design and implementation of big data platform for intelligence industrial internet of things sensor monitoring system and conveys a prediction of any upcoming errors beforehand. The architecture design is based on edge computing and artificial intelligence. To extend more precisely, industrial internet of things sensor here is about the condition monitoring sensor data - vibration, temperature, related humidity, and barometric pressure inside facility manufacturing factory.",https://ieeexplore.ieee.org/document/9415189/,2021 International Conference on Artificial Intelligence in Information and Communication (ICAIIC),13-16 April 2021,ieeexplore
10.1109/MED.2017.7984310,Cloud computing for big data analytics in the Process Control Industry,IEEE,Conferences,"The aim of this article is to present an example of a novel cloud computing infrastructure for big data analytics in the Process Control Industry. Latest innovations in the field of Process Analyzer Techniques (PAT), big data and wireless technologies have created a new environment in which almost all stages of the industrial process can be recorded and utilized, not only for safety, but also for real time optimization. Based on analysis of historical sensor data, machine learning based optimization models can be developed and deployed in real time closed control loops. However, still the local implementation of those systems requires a huge investment in hardware and software, as a direct result of the big data nature of sensors data being recorded continuously. The current technological advancements in cloud computing for big data processing, open new opportunities for the industry, while acting as an enabler for a significant reduction in costs, making the technology available to plants of all sizes. The main contribution of this article stems from the presentation for a fist time ever of a pilot cloud based architecture for the application of a data driven modeling and optimal control configuration for the field of Process Control. As it will be presented, these developments have been carried in close relationship with the process industry and pave a way for a generalized application of the cloud based approaches, towards the future of Industry 4.0.",https://ieeexplore.ieee.org/document/7984310/,2017 25th Mediterranean Conference on Control and Automation (MED),3-6 July 2017,ieeexplore
10.1109/WFCS.2019.8757952,Cloud-enabled Smart Data Collection in Shop Floor Environments for Industry 4.0,IEEE,Conferences,"Industry 4.0 transition is producing a remarkable change in the Smart Factories management. Modern companies can provide new services following products inside the shop floors along the entire production chain. To achieve the goal of servitization that the Industry 4.0 world requires, a modernization of current production chains is needed. This common demand comes mostly from manufacturing sector, where complex work machines collaborate with human workers. The data produced by the machines must be processed quickly, to allow the implementation of reactive services such as predictive maintenance, and remote control, always taking care of the safety of nearby people. This paper proposes a multi-layer architecture to monitor legacy production machines during their operations inside customers plants. The platform provides near real-time delivery of data collected from the machines with a high grade of customization according to customer needs. The performed tests show the scalability of the platform for a productive ecosystem with many machines at work, confirming its feasibility within different production facilities with different needs.",https://ieeexplore.ieee.org/document/8757952/,2019 15th IEEE International Workshop on Factory Communication Systems (WFCS),27-29 May 2019,ieeexplore
10.1109/WCNC45663.2020.9120761,Collaborative Learning Model for Cyberattack Detection Systems in IoT Industry 4.0,IEEE,Conferences,"Although the development of IoT Industry 4.0 has brought breakthrough achievements in many sectors, e.g., manufacturing, healthcare, and agriculture, it also raises many security issues to human beings due to a huge of emerging cybersecurity threats recently. In this paper, we propose a novel collaborative learning-based intrusion detection system which can be efficiently implemented in IoT Industry 4.0. In the system under consideration, we develop smart “filters” which can be deployed at the IoT gateways to promptly detect and prevent cyberattacks. In particular, each filter uses the collected data in its network to train its cyberattack detection model based on the deep learning algorithm. After that, the trained model will be shared with other IoT gateways to improve the accuracy in detecting intrusions in the whole system. In this way, not only the detection accuracy is improved, but our proposed system also can significantly reduce the information disclosure as well as network traffic in exchanging data among the IoT gateways. Through thorough simulations on real datasets, we show that the performance obtained by our proposed method can outperform those of the conventional machine learning methods.",https://ieeexplore.ieee.org/document/9120761/,2020 IEEE Wireless Communications and Networking Conference (WCNC),25-28 May 2020,ieeexplore
10.1109/ETFA.2017.8247730,Communication middleware technologies for industrial distributed control systems: A literature review,IEEE,Conferences,"Industry 4.0 is the German vision for the future of manufacturing, where smart factories use information and communication technologies to digitise their processes to achieve improved quality, lower costs, and increased efficiency. It is likely to bring a massive change to the way control systems function today. Future distributed control systems are expected to have an increased connectivity to the Internet, in order to capitalize on new offers and research findings related to digitalization, such as cloud, big data, and machine learning. A key technology in the realization of distributed control systems is middleware, which is usually described as a reusable software layer between operating system and distributed applications. Various middleware technologies have been proposed to facilitate communication in industrial control systems and hide the heterogeneity amongst the subsystems, such as OPC UA, DDS, and RT-CORBA. These technologies can significantly simplify the system design and integration of devices despite their heterogeneity. However, each of these technologies has its own characteristics that may work better for particular applications. Selection of the best middleware for a specific application is a critical issue for system designers. In this paper, we conduct a survey on available standard middleware technologies, including OPC UA, DDS, and RT-CORBA, and show new trends for different industrial domains.",https://ieeexplore.ieee.org/document/8247730/,2017 22nd IEEE International Conference on Emerging Technologies and Factory Automation (ETFA),12-15 Sept. 2017,ieeexplore
,Corporate Social Responsibility Challenges and Risks of Industry 4.0 technologies: A review,VDE,Conferences,"The fourth industrial revolution arrived with many enabling technologies that would impact important sociological aspects in the industry. Some of the Industry 4.0 technologies are already running in different industrial application, and other are still as a paradigm state. The social, economic, and environmental acceptance of Industry 4.0 technologies is still under discussion, which open new opportunities to execute various analysis about the possible implications of the implementation of such technologies. This article refers to an exploratory analysis and identification of the different challenges and risks of this new Industry 4.0 paradigm and its related technologies. The technologies under review were Internet of Things, Artificial Intelligence, Cloud Computing, cybersecurity, bid data, blockchain, 5G, robotics, adding manufacturing, unmanned systems, autonomous vehicles, virtual reality, and augmented reality. As a result, different social challenges and risks were identified for each technology, starting from vulnerability, implementation cost, until social aspects such as education and unemployment caused by those new technologies. In conclusion, Industry 4.0 arrived with a lot of benefits to the industry business, but companies should not stop thinking about sustainable development.",https://ieeexplore.ieee.org/document/8835964/,"Smart SysTech 2019; European Conference on Smart Objects, Systems and Technologies",4-5 June 2019,ieeexplore
10.1109/AINS47559.2019.8968698,Cyber Security Risk Assessment on Industry 4.0 using ICS testbed with AI and Cloud,IEEE,Conferences,"Industry 4.0 is a new concept, thus risk assessment is necessary. Several risk assessment methods for Industrial Control System (ICS) and Industry 4.0 have been proposed, however, it is difficult to identify impacts on the physical world caused by cyber attacks against ICS since many of these are based on tabletop analysis or software simulations. Therefore, we focus on the risk assessment using actual machines (ICS testbed) which can help to solve the above problems. In Industry 4.0, autonomous judgment and execution are required for the cyber-physical system, it is based on information exchange using Artificial Intelligence (AI) and cloud technologies. In this research, we evaluate cyber risks through attacks against ICS with AI and cloud using ICS testbed. The proposed method can clarify cyber risks and impacts on the real world, and corresponding countermeasures.",https://ieeexplore.ieee.org/document/8968698/,"2019 IEEE Conference on Application, Information and Network Security (AINS)",19-21 Nov. 2019,ieeexplore
10.1109/COINS51742.2021.9524088,Cyber Security Risks of Technical Components in Industry 4.0,IEEE,Conferences,"Industry 4.0 is a new concept of automation data exchange in manufacturing. Industry 4.0 consists of various technical components such as Artificial Intelligence (AI), IoT, OPC UA, and cloud. There could be cyber risks when these technical components are used in Industrial Control System (ICS) without security consideration. Therefore cyber risk assessment, reconsideration of implementation, operation are necessary to adopt Industry 4.0. Problems of previous research for ICS risk assessments are some of them are theoretical and conceptual methods such as simulations. Moreover, they do not focus on Industry 4.0 components.The objective of this research is to prove the concrete cyber risks and introduces corresponding secure implementations for Industry 4.0 components such as Artificial Intelligence (AI), the Internet of Things (IoT), OPC Unified Architecture (OPC UA). The proposed method has benefits for clarifying impacts on the real world by cyber-attacks through penetration tests against ICS testbed with actual machines.",https://ieeexplore.ieee.org/document/9524088/,2021 IEEE International Conference on Omni-Layer Intelligent Systems (COINS),23-25 Aug. 2021,ieeexplore
10.1109/IAI50351.2020.9262203,Digital Twin Enabled Smart Control Engineering as an Industrial AI: A New Framework and Case Study,IEEE,Conferences,"In Industry 4.0, the increasing complexity of industrial systems introduces unknown dynamics that affect the performance of manufacturing processes. Thus, Digital Twin appears as a breaking technology to develop virtual representations of any complex system design, analysis, and behavior prediction tasks to enhance the system understanding via enabling capabilities like real-time analytics, or Smart Control Engineering. In this paper, a novel framework is proposed for the design and implementation of Digital Twin applications to the development of Smart Control Engineering. The framework involve the steps of system documentation, Multidomain Simulation, Behavioral Matching, and real-time monitoring, which is applied to develop the Digital Twin for a real-time vision feedback temperature uniformity control. The obtained results show that Digital Twin is a fundamental part of the transformation into Industry 4.0.",https://ieeexplore.ieee.org/document/9262203/,2020 2nd International Conference on Industrial Artificial Intelligence (IAI),23-25 Oct. 2020,ieeexplore
10.1109/DESSERT.2018.8409186,Digitization of the economy of Ukraine: Strategic challenges and implementation technologies,IEEE,Conferences,"The main directions, challenges, threats of digitization of the national economy of Ukraine have been considered in the paper. The attention is focused on the found weaknesses and the imperfection of the strategy and the state policy of digitization of Ukraine's economy. The authors have proven the potential and new possibilities of solving public finance management problems with the usage of blockchain technology. It has been justified that activation of transformation processes in the real economy sector due to the introduction of Industry 4.0 concept is important for Ukraine. The paper reveals basic principles and technologies, the experience of the European Union, and characterizes Industry 4.0 view in Ukraine. The development of the latest financial technologies - FinTech - has been recognized as the driver of digital transformation of financial services. The types of FinTech innovations, the features of increasing competition between FinTech companies and traditional financial intermediaries, the tendencies of FinTech development in Ukraine have been characterized.",https://ieeexplore.ieee.org/document/8409186/,"2018 IEEE 9th International Conference on Dependable Systems, Services and Technologies (DESSERT)",24-27 May 2018,ieeexplore
10.1109/ETFA46521.2020.9211946,Dynamic Process Planning using Digital Twins and Reinforcement Learning,IEEE,Conferences,"In order to enable changeable production of Industry 4.0 applications, a production system should respond to unpredictable changes quickly and adequately. This requires process planning to be performed based on the real time operating conditions and dynamic changes to be handled with cognitive skills. To meet this demand, we present a process planning approach using digital twins and reinforcement learning to derive near-optimal process plans. The digital twins enable access to real-time information about the production system. They also constitute the environment for training the agent of the reinforcement learning method. The environment works as a virtual plant, containing the attributes of the product and resources, and uses simulation models of the resources to calculate the reward for an action in terms of reinforcement learning. Reinforcement learning enables our approach to derive process plans via trial and error. Besides the virtual plant, our approach has a planner, which plays the role of the agent to derive near-optimal plans by trying different actions in the virtual plant, and observes the rewards. We apply the Q-learning algorithm to derive near optimal process plans. The evaluation results show that our approach is able to derive near-optimal process plans for different problem sizes. The evaluation also demonstrated the planner's ability to identify by itself which action to take in which situation. Consequently, no modeling of the preconditions and effects of the actions is necessary.",https://ieeexplore.ieee.org/document/9211946/,2020 25th IEEE International Conference on Emerging Technologies and Factory Automation (ETFA),8-11 Sept. 2020,ieeexplore
10.1109/CSIT49958.2020.9321954,Eco-friendly Home Automation System Implemented Using Machine Learning Algorithms,IEEE,Conferences,"This paper presents the exemplary system of house automation implemented with the use of Industry 4.0 inventions. The proposed system tries to benefit from weather conditions to heat or cool the house without any electrical heaters or air conditioners. It is implemented with the aid of Machine Learning algorithms, the Internet of Things, and Cloud technology. The paper contains a technical and practical description of the system, the results of the real use, and proposed extensions that can improve the presented solution.",https://ieeexplore.ieee.org/document/9321954/,2020 IEEE 15th International Conference on Computer Sciences and Information Technologies (CSIT),23-26 Sept. 2020,ieeexplore
10.1109/CSCloud-EdgeCom49738.2020.00050,Edge Computing-based 3D Pose Estimation and Calibration for Robot Arms,IEEE,Conferences,"Industrial robots are widely used in current production lines, and complex pipeline processes, especially those with different assembly requirements, are designed for intelligent manufacturing in the era of industry 4.0. During the new crown epidemic, a large number of car companies used the production line to transform production of medical materials such as masks and protective clothing, which provided a strong guarantee for fighting the epidemic. In this scenario, a pipeline is often assembled from robotic arms from multiple suppliers. The traditional methods is complex and takes a lot of time. In this paper, we propose a novel deep learning based robot arm 3D pose estimation and calibration model with simple Kinect stereo cameras which can be deployed on light-weight edge computing systems. The light-weight deep CNN model can detection 5 predefined key points based on RGB-D data. In this way, when the assembly line composed of different robot arms needs to be reassembled, our model can quickly provide the robot's pose information without additional tuning processes. Testing in Webots with Rokae xb4 robot arm model shows that our model can quickly estimate the key point of the robot arm.",https://ieeexplore.ieee.org/document/9170983/,2020 7th IEEE International Conference on Cyber Security and Cloud Computing (CSCloud)/2020 6th IEEE International Conference on Edge Computing and Scalable Cloud (EdgeCom),1-3 Aug. 2020,ieeexplore
10.1109/EuCNC/6GSummit51104.2021.9482590,Empowering Industry 4.0 and Autonomous Drone Scouting use cases through 5G-DIVE Solution,IEEE,Conferences,"The 5G Edge Intelligence for Vertical Experimentation (5G-DIVE) project aims at demonstrating the technical merits and business value proposition of 5G technologies in two vertical pilots, namely the Industry 4.0 (I4.0) and Autonomous Drones Scout (ADS) pilots. This paper presents an overview of the overall 5G-DIVE solution and reports the results of the initial validation campaign of the selected use case, featuring 5G connectivity, distributed Edge computing, and artificial intelligence. The initial results for the I4.0 provide a baseline for next step validation campaign targeting a broader scale 5G implementation, while the ADS results provides promising results for enhancing the autonomous navigation in real-time.",https://ieeexplore.ieee.org/document/9482590/,2021 Joint European Conference on Networks and Communications & 6G Summit (EuCNC/6G Summit),8-11 June 2021,ieeexplore
10.1109/ICE/ITMC49519.2020.9198492,Enhancing Cognition for Digital Twins,IEEE,Conferences,"In the era of Industry 4.0, Digital Twins (DTs) pave the way for the creation of the Cognitive Factory. By virtualizing and twinning information stemming from the real and the digital world, it is now possible to connect all parts of the production process by having virtual copies of physical elements interacting with each other in the digital and physical realms. However, this alone does not imply cognition. Cognition requires modelling not only the physical characteristics but also the behavior of production elements and processes. The latter can be founded upon data-driven models produced via Data Analytics and Machine Learning techniques, giving rise to the so-called Cognitive (Digital) Twin. To further enable the Cognitive Factory, a novel concept, dubbed as Enhanced Cognitive Twin (ECT), is proposed in this paper as a way to introduce advanced cognitive capabilities to the DT artefact that enable supporting decisions, with the end goal to enable DTs to react to inner or outer stimuli. The Enhanced Cognitive Twin can be deployed at different hierarchical levels of the production process, i.e., at sensor-, machine-, process-, employee- or even factory-level, aggregated to allow both horizontal and vertical interplay. The ECT notion is proposed in the context of process industries, where cognition is particularly important due to the continuous, non-linear, and varied nature of the respective production processes.",https://ieeexplore.ieee.org/document/9198492/,"2020 IEEE International Conference on Engineering, Technology and Innovation (ICE/ITMC)",15-17 June 2020,ieeexplore
10.1109/21CW48944.2021.9532522,Ergonomics of Human Machine Integration in Variable Autonomy,IEEE,Conferences,"“Human technologies are made by humans, for humans”. In recent days pairing people with the system is getting easier. The systems and tools we use are becoming increasingly intelligent and more interconnected with autonomous behavior giving birth to cyber physical systems. The advances in the miniaturization of computation makes our tool behave intelligent using Artificial Intelligence. This intelligence in the form of a software where the inputs are taken from entities of real-time systems. The ultimate goal of the future research should be to emulate the functions of human-human and human-autonomy teams directly and evaluate their joint performance and contributions. Armed with this approach and existing technologies we can uncover novel approaches in Industry 4.0 and this paper ends with the overview of human-machine autonomy and ergonomics at variable autonomy.",https://ieeexplore.ieee.org/document/9532522/,2021 IEEE Conference on Norbert Wiener in the 21st Century (21CW),22-25 July 2021,ieeexplore
10.1109/SMC.2019.8913901,Explainable Machine Learning in Industry 4.0: Evaluating Feature Importance in Anomaly Detection to Enable Root Cause Analysis,IEEE,Conferences,"In the past recent years, Machine Learning methodologies have been applied in countless application areas. In particular, they play a key role in enabling Industry 4.0. However, one of the main obstacles to the diffusion of Machine Learning-based applications is related to the lack of interpretability of most of these methods. In this work, we propose an approach for defining a `feature importance' in Anomaly Detection problems. Anomaly Detection is an important Machine Learning task that has an enormous applicability in industrial scenarios. Indeed, it is extremely relevant for the purpose of quality monitoring. Moreover, it is often the first step towards the design of a Machine Learning-based smart monitoring solution because Anomaly Detection can be implemented without the need of labelled data. The proposed feature importance evaluation approach is designed for Isolation Forest, one of the most commonly used algorithm for Anomaly Detection. The efficacy of the proposed method is tested on synthetic and real industrial datasets.",https://ieeexplore.ieee.org/document/8913901/,"2019 IEEE International Conference on Systems, Man and Cybernetics (SMC)",6-9 Oct. 2019,ieeexplore
10.1109/ICS51289.2020.00088,Feature Selection for Malicious Traffic Detection with Machine Learning,IEEE,Conferences,"The network technology plays an important role in the emerging industry 4.0. Industrial control systems (ICS) are related to all aspects of human life and have become the target of cyber-attackers. Attacks on ICS may not only cause economic loss, but also damage equipment and hurt staff. The biggest challenges in establishing a secure network communication system is how to effectively detect and prevent malicious network behavior. A Network Intrusion Detection System (NIDS) can be deployed as a defense mechanism for cyberattacks. However, for industrial internet-of-things (IIoT) applications with limited computing resources, designing an effective NIDS is challenging. In this paper, we propose to use machine learning as the core technology to build a compact and effective NIDS for IIoT. The proposed method is validated by using the more recent UNSW-NB 15 dataset to improve the detection capability against new types of attacks in the real world. Furthermore, we demonstrate that the method is also valid for traditional KDD-CUP-99 dataset. Experimental results show that the proposed method achieves better performance than previous methods.",https://ieeexplore.ieee.org/document/9359069/,2020 International Computer Symposium (ICS),17-19 Dec. 2020,ieeexplore
10.15439/2017F253,Human machine synergies in intra-logistics: Creating a hybrid network for research and technologies,IEEE,Conferences,"The purpose of the article is to outline the futuristic vision of Industry 4.0 in intra-logistics by creating a hybrid network for research and technologies thereby providing a detailed account on the research centre, available technologies and their possibilities for collaboration. Scientific challenges in the field of Industry 4.0 and intra-logistics are identified due to the new form of interaction between humans and machines. This kind of collaboration provides new possibilities of materials handling that can be developed with the support of real-time motion data tracking and virtual reality systems. These services will be provided by a new research centre for flexible human-machine cooperation networks in Dortmund. By the use of various reference and experiment systems various real-time scenarios can be emulated including digital twin simulation concepts. Big data emerges as an important paradigm in this research project where all systems are made flexible in terms of networking for all the systems to consume the data produced and also to combine all the data to arrive at new insights using concepts from machine learning and deep learning networks. This leads to the challenge of finding a common syntax for inter-operating systems. This paper describes the design and deployment strategies of research centre with the possibilities and the design insights for a futuristic Industry 4.0 material handling facility.",https://ieeexplore.ieee.org/document/8104684/,2017 Federated Conference on Computer Science and Information Systems (FedCSIS),3-6 Sept. 2017,ieeexplore
10.1109/CIIMA50553.2020.9290302,Implementación de SCADA a través del protocolo MQTT,IEEE,Conferences,"This document describes an implementation of a SCADA system powered by MQTT &amp; OPC-UA protocols and hosted within the Google Cloud Platform system. This combination allows to have integrated, scalable, secure and reliable industrial communications while allowing real-time data acquisition and sensor feed that can then be used in real-time OEE tracking or predictive maintenance models, to name some examples. This in line with the Industry 4.0 initiatives mainly fueled by data and machine learning autonomous systems.",https://ieeexplore.ieee.org/document/9290302/,2020 IX International Congress of Mechatronics Engineering and Automation (CIIMA),4-6 Nov. 2020,ieeexplore
10.1109/AIMS52415.2021.9466068,Implementation of Cloud Based Action Recognition Backend Platform,IEEE,Conferences,"The Internet of Things (IoT) growth are rapidly in various fields such as industry 4.0, smart cities, and smart homes. Implementation of IoT for electronic assistance had been researched to increase the longevity of human life. However, not all IoT implementation as human life assistance provides action recognition monitoring on multiple elderly people, provide information such as real-time action monitoring, and real-time streaming in a mobile application. Therefore, this research intends to create a system that can receive and provide information on each elderly people who registered. The Action Recognition Backend Platform will be working as cloud computing to receive and manage input data from Edge Computing Action Recognition. This platform integrated Deep Learning, Data Analytics, Big Data Warehouse that implemented Extract, Transform, and Load (ETL) methods, communication services with MQTT, and Kafka Streaming Processor. The test result showed that the edge computing action recognition got better model accuracy performance from our last model [1], which can predict with 50,7% accuracy in 0.5 confidence threshold. Moreover, the backend platform had been successfully implemented a simple IoT paradigm and got an average delivery time of MQTT communication at 204ms, for streaming data process took an average delay of 680ms.",https://ieeexplore.ieee.org/document/9466068/,2021 International Conference on Artificial Intelligence and Mechatronics Systems (AIMS),28-30 April 2021,ieeexplore
10.1109/IJCNN.2001.938518,Improving prediction of customer behavior in nonstationary environments,IEEE,Conferences,"Customer churn, switching from one service provider to another, costs the wireless telecommunications industry $4 billion each year in North America and Europe. To proactively build lasting relationships with customers, it is thus crucial to predict customer behavior. Machine learning has been applied to churn prediction, using historical data such as usage, billing, customer service, and demographics. However, because customer behavior is often nonstationary, training a model based on data extracted from a window of time in the past yields poor performance on the present. We propose two distinct approaches, using more historical data or new, unlabeled data, to improve the results for this real-world, large-scale, nonstationary problem. A new ensemble classification method, with combination weights learned from both labeled and unlabeled data, is also proposed, and it outperforms bagging and mixture of experts.",https://ieeexplore.ieee.org/document/938518/,IJCNN'01. International Joint Conference on Neural Networks. Proceedings (Cat. No.01CH37222),15-19 July 2001,ieeexplore
10.1109/COMPSAC.2018.10204,Indoor Augmented Reality Using Deep Learning for Industry 4.0 Smart Factories,IEEE,Conferences,"This paper proposes to design, develop and implement a fast and markerless mobile augmented reality system to achieve the registration for, the visualization of, and the interaction with machines in indoor smart factories with Industry 4.0 vision. A lightweight deep-learning image detection module based on MobileNets running on mobile devices is used to detect/recognize different machines and different portions of machines. Internet of Things (IoT) networking allows machines and sensors in machines to report data, such as machine settings and machine states, to the cloud-side server. Thus, augmented information associated with a machine portion can be derived from the server and superimposed with the portion image shown on the device display. Furthermore, interaction methods based on touch gestures and distance calculation are also implemented. A prototype system is developed and tested in a mechanical workshop for the purpose of validation and evaluation. The system is shown to achieve high detection accuracy, intuitive visualization, and unique interaction modes.",https://ieeexplore.ieee.org/document/8377831/,2018 IEEE 42nd Annual Computer Software and Applications Conference (COMPSAC),23-27 July 2018,ieeexplore
10.1109/GIOTS49054.2020.9119497,Industrial IoT and Digital Twins for a Smart Factory : An open source toolkit for application design and benchmarking,IEEE,Conferences,"The rapid evolution of digital technology and designed intelligence, such as the Internet of Things (IoT), Big data analytics, Artificial Intelligence (AI), Cyber Physical Systems (CPS), has been a catalyst for the 4th industrial revolution (known as industry 4.0). Among other, the two key state-of-the-art concepts in Industry 4.0, are Industrial IoT (IIoT) and digital twins. IIoT facilitates real-time data acquisition, processing and analytics over large amount of sensor data streams produced by sensors installed within a smart factory, while the `digital twin' concept aims to enable smart factories via the digital replication or representation of physical machines, processes, people in cyber-space. This paper explores the capability of present-state open-source platforms to collectively achieve digital twin capabilities, including IoT real-time data acquisition, virtual representation, analytics, and visualisation. The aim of this work is to `close the gap' between research and implementation, through a collective open source IoT and Digital Twin architecture. The performance of the open-source architecture in this work, is demonstrated in a use-case utilising industry `open data', and is bench-marked with universal testing tools.",https://ieeexplore.ieee.org/document/9119497/,2020 Global Internet of Things Summit (GIoTS),3-3 June 2020,ieeexplore
10.1109/RTSI.2019.8895598,Intelligent Embedded Load Detection at the Edge on Industry 4.0 Powertrains Applications,IEEE,Conferences,"In the context of Industry 4.0, there has been great focus in developing intelligent sensors. Deploying them, condition monitoring and predictive maintenance have become feasible solutions to minimize operating and maintenance costs while also increasing safety. A critical aspect is the applied load to the supervised machinery system. Vibration data can be used to determine the current condition, but this needs signal processing specially developed and adapted to the monitored machine part for feature extraction. Artificial intelligence (AI) can, on one hand, simplify the development of such special purpose processing and on another hand be used to monitor and classify machine conditions by learning features directly from data. By bringing the AI computation as close as possible to the sensor (Edge-AI), data bandwidth can be minimized, system scalability and responsiveness can be improved and real-time requirements can be fulfilled. This work describes how Edge-AI on a STM32-bit microcontroller can be implemented. Our experimental setup demonstrates how AI can be effectively used to detect and classify the load on a powertrain. In order to do this, we use a MEMS capacity accelerometer to sense vibrations of the system. Also, this work demonstrates how Deep Neural Networks (DNN) for signal classification are build and trained by using an open-source deep learning framework and how the code library for the microcontroller is automatically generated afterwards by using STM32Cube. AI toolchain. We compare the classification accuracy of a memory compressed DNN against an uncompressed DNN.",https://ieeexplore.ieee.org/document/8895598/,2019 IEEE 5th International forum on Research and Technology for Society and Industry (RTSI),9-12 Sept. 2019,ieeexplore
10.1109/MetroInd4.0IoT48571.2020.9138288,Introducing a cloud based architecture for the distributed analysis of Real-Time Ethernet traffic,IEEE,Conferences,"The use of industrial communication protocols based on Real-Time Ethernet (RTE) standards is completely replacing traditional industrial fieldbuses. As usual, when a technology becomes mature, the need of efficient diagnostic and maintenance tools quickly raises. Very often, following the paradigm of Industry 4.0, the most effective diagnostic systems are today based on distributed, cloud-centric, architectures and artificial intelligence. However, the distributed analysis of RTE systems is challenging, considering the plurality of protocols and the stringent cost constrains which are common in industry. In this paper, a new architecture for the distributed analysis of RTE networks is proposed, leveraging on distributed probes that send traffic samples to Matlab cloud for remote analysis. The paper also proposes a software conversion tool to adapt general PCAP files captured by popular sniffers (e.g. Wireshark) into MAT file for easier Matlab elaboration. Last, a test bench for characterization (in terms of transfer delays) of the first part of the chain for RTE traffic sampling is described. The results show that in less than 10 seconds it is possible to transfer chunks of RTE traffic data (captured on industrial networks with hundreds of real-time devices) directly to the cloud and to have them converted in Matlab format.",https://ieeexplore.ieee.org/document/9138288/,2020 IEEE International Workshop on Metrology for Industry 4.0 & IoT,3-5 June 2020,ieeexplore
10.1109/AIM46487.2021.9517377,Introducing adaptive mechatronic designs in bulk handling industry,IEEE,Conferences,"The advances of mechatronic system design and system integration have shown improvements in functionality, performance and energy efficiency in many applications across industries, from autonomous ground vehicles and drones to conveyor belts. This trend has been adopted in some industries more than others. The design of equipment to handle granular or bulk material is commonly based on traditional approaches. Therefore, introducing mechatronic concepts in the design procedure can enable new possibilities, such as sensor integration and data analyses, adaptability and control. The efficiency of bulk material handling equipment in ports, agriculture and food processing is heavily influenced by the operational conditions. Typically, a piece of equipment is designed for defined operational conditions when the maximum performance can be achieved. In this work the concept of adaptability to varying operational conditions is explored by understanding the technologies implemented in other industries and the feasibility to be implemented in the bulk handling equipment design. Sensing technology, actuation and adaptability are systematically presented in this work to support the design process of the next generation of bulk handling equipment. This will pave the way for incorporating the technological trends in the design, such as: sustainability, “smartness”, Internet of Things, Industry 4.0, digital twin and machine learning. Adaptive mechatronic solutions will play a crucial role in generating and implementing innovative sustainable solutions for bulk handling equipment.",https://ieeexplore.ieee.org/document/9517377/,2021 IEEE/ASME International Conference on Advanced Intelligent Mechatronics (AIM),12-16 July 2021,ieeexplore
10.1109/DASA51403.2020.9317177,"IoT Driven Resiliency with Artificial Intelligence, Machine Learning and Analytics for Digital Transformation",IEEE,Conferences,"A new manufacturing era “Industry 4.0” is emerging with two unique characteristics: intelligent manufacturing and integrated manufacturing. This pattern rationale with the progress of digital transformation, in which efficient manufacturing and production systems is being continuously pursued. Digital transformation initiatives generate large data sets due to massive integration of devices in internet of things (IoT) environment. This scenario demands the fastest insights to respond on time considering three key pillars: communication network evolution, digital business, and customer experience. IoT driven resiliency with traditional analytics has limited value without artificial intelligence (AI), and machine learning (ML). This study aims to explore this phenomenon of interest by conducting group discussion with software vendors. The results will helpful to utilize the power of AI and ML with analytics to leverage a large amount of data which would contribute to the success of digital transformation of organizations with real-time decision-making.",https://ieeexplore.ieee.org/document/9317177/,2020 International Conference on Decision Aid Sciences and Application (DASA),8-9 Nov. 2020,ieeexplore
10.1109/IGSC48788.2019.8957164,IoT/CPS Ecosystem for Efficient Electricity Consumption : Invited Paper,IEEE,Conferences,"Modern society relies on smart systems like internet of things (IoT) and cyber physical systems (CPS) to monitor and control physical processes. The widespread deployment of IoT and CPSs result in fast growth of sensor data as physical processes are constantly monitored by billions of IP-enabled sensors (44 zettabytes by 2020). Hence, fog nodes are deployed to make network edge rich in computing resources to enable real-time data analytics using artificial intelligence/machine learning (AI/ML) for Big data generated from IoT and CPSs. This paper proposes IoT/CPS ecosystem for smart grid (SG) utilizing industry 4.0 concept to manage and control the loads using an intelligent predictive controller based on artificial neural network (ANN). The ANN is trained to predict the loads in certain districts based on previous smart meter readings installed at consumers and substations. This is a novel approach which integrates IoT/CPSs ecosystem into electric power system to deliver energy to consumers with high efficiency, reduce the cost, optimize the energy consumption, improve the reliability and enable real-time monitoring of power consumption.",https://ieeexplore.ieee.org/document/8957164/,2019 Tenth International Green and Sustainable Computing Conference (IGSC),21-24 Oct. 2019,ieeexplore
10.1109/ICCTA48790.2019.9478839,Keynote Speech II: Readiness for the Impact of Emerging Technologies,IEEE,Conferences,"Throughout history, technical innovation has always been the key driver of human progress. The use of new technologies brings positive changes to people and societies. It makes life easier, more pleasant, raises the standard of living, and improves human wellbeing, health and life expectancy. It also changes social frameworks, business dynamics, job types, and wealth distribution.The introduction of digital technology a few decades ago has spurred a series of transformations that have significantly changed the way we live, work, transact, and interact. Innovation continues to accelerate in a large set of technologies and applications. New hardware, software, and algorithmic tools allow us to process information much faster and in much larger volumes. Moreover, the digital world is becoming increasingly intertwined with the physical world of machines, to which it is bringing ubiquitous intelligence and a perpetual flow of information.These trends are driving us towards a very different future. That future has already started. A new wave of social, economic, and psychological changes is expected to abruptly affect almost everything we do. With change, many opportunities come along. Those who anticipate the course of the future, and prepare for it, will be ready to seize these opportunities and will come out winners. Those who chose to ignore the signs of change, will risk losing their livelihood and eventually hurting their families, businesses, and societies. Those who see the storm coming but react by standing still in panic, disgruntlement, and lamentation will be defenseless when the inevitable waves hit their shores.This presentation overviews the trends in technology and applications, including Artificial Intelligence, Big Data Analytics, Robotics, Internet of Things, Industry 4.0, etc. The impact that such advances are likely to have on the hightech as well as the low-tech job markets is outlined. Some actions and initiatives are proposed and discussed, with the purpose of triggering a larger debate on how individuals, businesses, academic institutions, and governments should prepare for the anticipated massive changes that are already beginning to affect our world.",https://ieeexplore.ieee.org/document/9478839/,2019 29th International Conference on Computer Theory and Applications (ICCTA),29-31 Oct. 2019,ieeexplore
10.1109/SCC47175.2019.9116104,Modeling and management of human resources in the reconfiguration of production system in industry 4.0 by neural networks,IEEE,Conferences,"In Industry 4.0, the role of employees changes significantly. Real-time production line control transforms job content. Work processes affect working conditions. The implementation of a socio-technical approach to the organization of work gives workers the opportunity to adapt their skills. Indeed, production work will become more and more multi-factor, especially with regard to control and decision-making tasks. In this paper, a proposal for an intelligent system for modeling skills and human resource management in the production system chain through the use of two artificial neural networks. The first NN1 network allows for the identification of the human factor, as well as the second NN2 network is reserved for valuing the human skills needed in Industry 4.0.",https://ieeexplore.ieee.org/document/9116104/,"2019 International Conference on Signal, Control and Communication (SCC)",16-18 Dec. 2019,ieeexplore
10.23919/DATE48585.2020.9116407,Network Synthesis for Industry 4.0,IEEE,Conferences,"Today's factory machines are ever more connected with SCADA, MES, ERP applications as well as external systems for data analysis. Different types of network architectures must be used for this purpose. For instance, control applications at the lowest level are susceptible to delays and errors while data analysis with machine learning procedures requires to move a large amount of data without real-time constraints. Standard data formats, like Automation Markup Language (AML), have been established to document factory environment, machine placement and network deployment, however, no automatic technique is currently available in the context of Industry 4.0 to choose the best mix of network architectures according to spacial constraints, cost, and performance. We propose to fill this gap by formulating an optimization problem. First of all, spatial and communication requirements are extracted from the AML description. Then, the optimal interconnection of wired or wireless channels is obtained according to application objectives. Finally, this result is back-annotated to AML to be used in the life cycle of the production system. The proposed methodology is described through a small, but complete, smart production plant.",https://ieeexplore.ieee.org/document/9116407/,"2020 Design, Automation & Test in Europe Conference & Exhibition (DATE)",9-13 March 2020,ieeexplore
10.1109/NCC52529.2021.9530062,On Traffic Classification in Enterprise Wireless Networks,IEEE,Conferences,"Enterprises today are quickly adopting intelligent, adaptive, and flexible wireless communication technologies in order to become compliant with Industry 4.0. One of the technological challenges related to this is to provide Quality of Services (QoS)-enabled network connectivity to the applications. Diverse QoS demands from the applications intimidate the underlying wireless networks to be agile and adaptive. Since the applications are diverse in nature, there must be a mechanism to learn the application types in near real-time so that the network can be provisioned accordingly. In this paper, we propose a Machine Learning (ML) based method to classify the application traffic. Our method is different from the existing port based and Deep Packet Inspection (DPI) based methods and uses statistical features of the network traffic related to the applications. We validate the performance of the proposed model in a lab based SDNized WiFi set-up. SDNization ensures that the proposed model can be deployed in practice.",https://ieeexplore.ieee.org/document/9530062/,2021 National Conference on Communications (NCC),27-30 July 2021,ieeexplore
10.23919/SpliTech49282.2020.9243735,PADL: a Language for the Operationalization of Distributed Analytical Pipelines over Edge/Fog Computing Environments,IEEE,Conferences,"In this paper we introduce PADL, a language for modeling and deploying data-based analytical pipelines. The novelty of this language relies on its independence from both the infrastructure and the technologies used on it. Specifically, this descriptive language aims at embracing all the particularities and constraints of high-demanding deployment models, such as critical restrictions regarding latency, privacy and performance, by providing fully-compliant schemas for implementing data analytical workloads. The adoption of PADL provides means for the operationalization of these pipelines in a reproducible and resilient fashion. In addition, PADL is able to fully utilize the benefits of Edge and Fog computing layers. The feasibility of the language has been validated with an analytical pipeline deployed over an Edge computing environment to solve an Industry 4.0 use case. The promising results obtained therefrom pave the way towards the widespread adoption of our proposed language when deploying data analytical pipelines over real application scenarios.",https://ieeexplore.ieee.org/document/9243735/,2020 5th International Conference on Smart and Sustainable Technologies (SpliTech),23-26 Sept. 2020,ieeexplore
10.1109/BigDataCongress.2019.00032,"PREMISES, a Scalable Data-Driven Service to Predict Alarms in Slowly-Degrading Multi-Cycle Industrial Processes",IEEE,Conferences,"In recent years, the number of industry-4.0-enabled manufacturing sites has been continuously growing, and both the quantity and variety of signals and data collected in plants are increasing at an unprecedented rate. At the same time, the demand of Big Data processing platforms and analytical tools tailored to manufacturing environments has become more and more prominent. Manufacturing companies are collecting huge amounts of information during the production process through a plethora of sensors and networks. To extract value and actionable knowledge from such precious repositories, suitable data-driven approaches are required. They are expected to improve the production processes by reducing maintenance costs, reliably predicting equipment failures, and avoiding quality degradation. To this aim, Machine Learning techniques tailored for predictive maintenance analysis have been adopted in PREMISES (PREdictive Maintenance service for Industrial procesSES), an innovative framework providing a scalable Big Data service able to predict alarming conditions in slowly-degrading processes characterized by cyclic procedures. PREMISES has been experimentally tested and validated on a real industrial use case, resulting efficient and effective in predicting alarms. The framework has been designed to address the main Big Data and industrial requirements, by being developed on a solid and scalable processing framework, Apache Spark, and supporting the deployment on modularized containers, specifically upon the Docker technology stack.",https://ieeexplore.ieee.org/document/8818217/,2019 IEEE International Congress on Big Data (BigDataCongress),8-13 July 2019,ieeexplore
10.1109/ICRITO51393.2021.9596436,Prediction of Sensor Faults and Outliers in IoT Devices,IEEE,Conferences,"Internet of Things (IoT) is tremendously growing and interacting with the physical world in the era of Industry 4.0. In near future, billions of companies will have advanced communication technology and it will increase the growth of critical systems. The accuracy measurement of the functionality of a critical system is a challenging job. Fault Tolerance (FT) is a major concern to ensure the dependability, availability and reliability of critical systems. Faults should be predicted and controlled proactively to lessen failure impact on the critical systems. To predict these failures and use the relevant procedure to avoid it before it actually occurs, FT techniques are used. These techniques are implemented in critical systems to avoid failures as the security of systems is more important than the reliability of systems. It minimizes the effect of faults that are being investigated. FT techniques work on a concept that if the system is built differently then it should fail differently. If a redundant variant fails then atleast the other one should give a satisfactory result. This study exhibits an analysis of existing FT techniques like N-version programming, Recovery blocks and N-self-checking programming. A critical study of sensor faults and outliers prediction models in IoT is presented. A bibliometric analysis is also carried out on 716 Scopus indexed publications to analyze the current research trends in this domain.",https://ieeexplore.ieee.org/document/9596436/,"2021 9th International Conference on Reliability, Infocom Technologies and Optimization (Trends and Future Directions) (ICRITO)",3-4 Sept. 2021,ieeexplore
10.1109/GloSIC.2018.8570124,Probabilistic Estimations of Increasing Expected Reliability and Safety for Intelligent Manufacturing,IEEE,Conferences,"In the near future the possibilities of the modern probabilistic models, artificial intelligence and machine learning methods can provide an intelligent support of making decisions by an operator in real time. An agile recovery of intelligent manufacturing integrity can be implemented owing to the development of industrial robotics. For intelligent manufacturing it means the expected reliability and safety may be in the near future at the expense of intelligent support of decision making and the agile recovery of integrity. To answer the question “How much essential may be this increasing?” here are proposed: general analytical approaches for a probabilistic estimation of the expected reliability and safety for every monitored element or the system of intelligent manufacturing on a level of probability distribution functions (PDF) of the time between the losses of system integrity; estimations of increasing the expected reliability and safety for intelligent manufacturing at the expense of the intelligent support of decision making and agile recovery of integrity; the comparisons of the estimations on a prognostic period up to 10 years using the identical model in applications to expected reliability and safety. The applications of the proposed approaches allow the customers, designers, developers, users and experts of Industry 4.0 intelligent manufacturing to be guided by the proposed probabilistic estimations for solving problems of reliability and safety in the system life cycle. The results are demonstrated by examples.",https://ieeexplore.ieee.org/document/8570124/,2018 Global Smart Industry Conference (GloSIC),13-15 Nov. 2018,ieeexplore
10.1109/PC.2017.7976254,Proposal of system for automatic weld evaluation,IEEE,Conferences,"The paper deals with the development of a system for automatic weld recognition using new information technologies based on cloud computing and single-board computer in the context of Industry 4.0. The proposed system is based on a visual system for weld recognition, and a neural network based on cloud computing for real-time weld evaluation, both implemented on a single-board low-cost computer. The proposed system was successfully verified on welding samples which correspond to a real welding process in the car production process. The system considerably contributes to the welds diagnostics in industrial processes of small- and medium-sized enterprises.",https://ieeexplore.ieee.org/document/7976254/,2017 21st International Conference on Process Control (PC),6-9 June 2017,ieeexplore
10.1109/SACI51354.2021.9465544,Real-time locating system and digital twin in Lean 4.0,IEEE,Conferences,"Digital twin plays a key role in the current development of smart manufacturing systems. Through simulation in the cyber world, real phenomena in the physical world can be predicted and optimized before the final implementation. The usage of the digital twin is enhanced along with the uprising of Industry 4.0, in which data availability supports the further insight of system status, helping the operation managers understand their system and perform resources adjustment more easily. Based on this digitization mature, Lean 4.0, a new concept elaborated from Lean manufacturing, has been interested recently. There are several technologies constituted digital twin that provide a favourable condition for Lean 4.0, such as augmented reality, cloud computing. In this paper, the integration of the Real-time Locating System (RTLS) into digital twin is proposed, which facilitates the performance of Lean 4.0 in manufacturing operation. Not only gain effective control over the facility's assets, but this integration also enhances the resources utilization, cut down operational wastes, thus brings a better turnover for industrial systems. A case study of successful implementation is shown, which proved the possible advantages of this approach.",https://ieeexplore.ieee.org/document/9465544/,2021 IEEE 15th International Symposium on Applied Computational Intelligence and Informatics (SACI),19-21 May 2021,ieeexplore
10.1109/ICE/ITMC52061.2021.9570221,Resilient Manufacturing Systems enabled by AI support to AR equipped operator,IEEE,Conferences,"Supply chains and manufacturing systems robustness and resilience are, for many years, but especially nowadays, key features requested to ensure reliable and efficient production processes. Two domains are crucial to achieve such purpose: the former is fast and comprehensive monitoring, efficient and reliable condition detection and effective and explicable support for decision making. The latter refers to the intervention by operators, able to better identify problems and to put in place effective operations aimed at fixing it or, better, to prevent such circumstances. This paper presents an integrated approach encompassing a sophisticated IoT and AI-based approach to monitor and detect critical situations, fully integrated with an AR (Augmented Reality) system supporting operators in the field to take informed actions in bi-directional continuous connection. Activities in the context of EC funded project Qu4lity developed in Politecnico di Milano Industry 4.0 Lab, a test environment implementing the proposed approach and demonstrating in an automated production line the effectiveness of the approach, significantly improving performances. Analysis of performance indicators demonstrates the soundness of the proposed solution and implementation methodology to make the overall production process more resilient, efficient and with product defects reduction.",https://ieeexplore.ieee.org/document/9570221/,"2021 IEEE International Conference on Engineering, Technology and Innovation (ICE/ITMC)",21-23 June 2021,ieeexplore
10.1109/SPAC53836.2021.9539961,SDN-based Industrial Internet Security Gateway,IEEE,Conferences,"Industrial Internet is widely used in the production field. As the openness of networks increases, industrial networks facing increasing security risks. Information and communication technologies are now available for most industrial manufacturing. This industry-oriented evolution has driven the emergence of cloud systems, the Internet of Things (IoT), Big Data, and Industry 4.0. However, new technologies are always accompanied by security vulnerabilities, which often expose unpredictable risks. Industrial safety has become one of the most essential and challenging requirements. In this article, we highlight the serious challenges facing Industry 4.0, introduce industrial security issues and present the current awareness of security within the industry. In this paper, we propose solutions for the anomaly detection and defense of the industrial Internet based on the demand characteristics of network security, the main types of intrusions and their vulnerability characteristics. The main work is as follows: This paper first analyzes the basic network security issues, including the network security needs, the security threats and the solutions. Secondly, the security requirements of the industrial Internet are analyzed with the characteristics of industrial sites. Then, the threats and attacks on the network are analyzed, i.e., system-related threats and process-related threats; finally, the current research status is introduced from the perspective of network protection, and the research angle of this paper, i.e., network anomaly detection and network defense, is proposed in conjunction with relevant standards. This paper proposes a software-defined network (SDN)-based industrial Internet security gateway for the security protection of the industrial Internet. Since there are some known types of attacks in the industrial network, in order to fully exploit the effective information, we combine the ExtratreesClassifier to enhance the detection rate of anomaly detection. In order to verify the effectiveness of the algorithm, this paper simulates an industrial network attack, using the acquired training data for testing. The test data are industrial network traffic datasets, and the experimental results show that the algorithm is suitable for anomaly detection in industrial networks.",https://ieeexplore.ieee.org/document/9539961/,"2021 International Conference on Security, Pattern Analysis, and Cybernetics（SPAC)",18-20 June 2021,ieeexplore
10.1109/IS3C50286.2020.00134,Screw defect detection system based on AI image recognition technology,IEEE,Conferences,"In the past ten years, smart manufacturing has been widely discussed and gradually introduced into various manufacturing fields. Since Germany proposed the concept of “Industry 4.0” in 2011, it has been spreading and feverish all over the world. For Industry 4.0, information digitization, intelligent defect detection and database platform management are their main core technologies. Aiming at a large screw industry manufacturing field in central and southern Taiwan, this paper proposes a screw defect detection system based on AI image recognition technology to detect damage to the nut during the “molding” process in the screw production process, and it is determined whether the inspected screw passes the inspection. The recognition result is given as shown in Figure 1. This paper uses 500 non-defective screw samples and 20 defective screw samples provided by the screw factory. The above samples collected real-time images through the sampling structure designed in this article, and we adopt Microsoft Corporation's ML.NET suite to model AI images, and uses the following four deep learning models: ResNetV2 50, ResNetV2 101, InceptionV3, MoblieNetV2 for learning; in the process of learning, this article divides the data set into three types of data sets (one is the unknown set that is not used for training but mixed with correct and defective samples, and the other is used for post-training verification of mixed samples with correct and defective samples. The third is a training set for training a mixture of correct and defective samples) This arrangement is used for subsequent verification models; after training, a PC-based screw defect detection system is implemented as shown in Figure 2; finally, with Detect screw defects in the form of instant photography. After the experiment, in 1,000 repeated tests, the success rate of defect detection reached 97%, while the false positive rate was only 2%.",https://ieeexplore.ieee.org/document/9394116/,"2020 International Symposium on Computer, Consumer and Control (IS3C)",13-16 Nov. 2020,ieeexplore
10.1109/IDAP.2018.8620887,Smart Grids and Industry 4.0,IEEE,Conferences,"Since the beginning of the Industrial Revolution, the concept of Industrial Revolution in the 4<sup>th</sup>/ Industry 4.0 which is a new turning point in the technologies used, is in every field as well as in the energy field which is considered as the key point of all revolutions. It is aimed that energy generation, transmission and distribution are more efficient and highly reliable together with the next generation software and hardware which is the result of this concept.It has been seen that existing networks can not be enough to achieve this goal.At this point, flexible, reliable, clean, sustainable and highly efficient electric energy due to smart grids come into play. However, due to the increase in demand for electricity due to increase in population and industrialization, the dependence of electric energy on production resources, the availability of renewable energy sources in a dispersed geographical structure and the loss and leakage are becoming more important in our country. The Cyber-Physical System, the Internet of Things, M2M (machine to machine), etc components are used in the energy field, especially in intelligent network systems, resulting in intensive use of technology at every stage from power generation to end-user distribution point. This includes better system operation, reduced costs, increased energy efficiency, reduced greenhouse effect, reduced downtime with reduced downtime, reduced loss/leakage rates, improved energy quality, better management of production and storage systems, intelligent meter reading and load management and real-time supply-demand management. Industry 4.0/4.Industrial Revolution and smart grids have many common aspects such as optimization, automation, efficient use and management of energy, intelligent production, everything internet. In this study, the development process of the Industrial 4.0 revolution in the world and in our country will be examined and the interaction with these process smart grids will be examined.",https://ieeexplore.ieee.org/document/8620887/,2018 International Conference on Artificial Intelligence and Data Processing (IDAP),28-30 Sept. 2018,ieeexplore
10.1109/ICRITO48877.2020.9198036,State of Art: Energy Efficient Protocols for Self-Powered Wireless Sensor Network in IIoT to Support Industry 4.0,IEEE,Conferences,"Up gradation of manufacturing systems in industries by means of implementing innovative manufacturing techniques that captures real time data, applies machine learning algorithms, makes entire system self-decisive and provides inter connectivity to the whole system is the prime focus of the Industry 4.0. It is aimed at bringing new industrial revolution with the help of internet of things technology due to its considerable influence in the industrial manufacturing process. Though impact of internet of things in industrial sector is huge, a practical implementation incorporates challenges in energy efficiency, self-powered sensor nodes and security. For processing of gathered data self-powered sensor nodes may sinks energy from ambient energy sources. A considerable amount of efforts has been put by researchers to address the challenges for development of energy efficient routing protocol for such nodes. The presented survey is aimed at analyzing the protocols for contribution towards the goals of industry 4.0 Energy efficient protocol will support the system to consume least energy for its operation. Henceforth combination of self-powered wireless sensor network and energy efficient protocol will be useful to power up many industrial IoT applications.",https://ieeexplore.ieee.org/document/9198036/,"2020 8th International Conference on Reliability, Infocom Technologies and Optimization (Trends and Future Directions) (ICRITO)",4-5 June 2020,ieeexplore
10.1109/ICE/ITMC49519.2020.9198430,Supporting SMEs in the Lake Constance Region in the Implementation of Cyber-Physical-Systems: Framework and Demonstrator,IEEE,Conferences,"With the emergence of the recent Industry 4.0 movement, data integration is now also being driven along the production line, made possible primarily by the use of established concepts of intelligent supply chains, such as the digital avatars. Digital avatars - sometimes also called Digital Twins or more broadly Cyber-Physical Systems (CPS) - are already successfully used in holistic systems for intelligent transport ecosystems, similar to the use of Big Data and artificial intelligence technologies interwoven with modern production and supply chains. The goal of this paper is to describe how data from interwoven, autonomous and intelligent supply chains can be integrated into the diverse data ecosystems of the Industry 4.0, influenced by a multitude of data exchange formats and varied data schemas. In this paper, we describe how a framework for supporting SMEs was established in the Lake Constance region and describe a demonstrator sprung from the framework. The demonstrator project's goal is to exhibit and compare two different approaches towards optimisation of manufacturing lines. The first approach is based upon static optimisation of production demand, i.e. exact or heuristic algorithms are used to plan and optimise the assignment of orders to individual machines. In the second scenario, we use real-time situational awareness - implemented as digital avatar - to assign local intelligence to jobs and raw materials in order to compare the results to the traditional planning methods of scenario one. The results are generated using event-discrete simulation and are compared to common (heuristic) job scheduling algorithms.",https://ieeexplore.ieee.org/document/9198430/,"2020 IEEE International Conference on Engineering, Technology and Innovation (ICE/ITMC)",15-17 June 2020,ieeexplore
10.1109/ISWCS.2019.8877305,Visible Light Positioning for Location-Based Services in Industry 4.0,IEEE,Conferences,"Industry 4.0 refers to the evolution in manufacturing from computerization to fully cyberphysical systems that exploit rich sensor data, adaptive real-time safety-critical control, and machine learning. An important aspect of this vision is the sensing and subsequent association of objects in the physical world with their cyber and virtual counterparts. In this paper we propose Visible Light Positioning (VLP) as an enabler for these Industry 4.0 applications. We also explore sensing techniques, including cameras (and depth sensors), and other light-based solutions for object positioning and detection along with their respective limitations. We then demonstrate an application of positioning for real time robot control in an interactive multiparty cyber-physical-virtual deployment. Lastly, based on our experience with this cyberphysical-virtual application, we propose Ray-Surface Positioning (RSP), a novel VLP technique, as a low cost positioning system for Industry 4.0.",https://ieeexplore.ieee.org/document/8877305/,2019 16th International Symposium on Wireless Communication Systems (ISWCS),27-30 Aug. 2019,ieeexplore
10.1109/BDCloud.2018.00136,"iSTEP, an Integrated Self-Tuning Engine for Predictive Maintenance in Industry 4.0",IEEE,Conferences,"The recent expansion of IoT-enabled (Internet of Things) devices in manufacturing contexts and their subsequent data-driven exploitation paved the way to the advent of the Industry 4.0, promoting a full integration of IT services, smart devices, and control systems with physical objects, their electronics and sensors. The real-time transmission and analysis of collected data from factories has the potential to create manufacturing intelligence, of which predictive maintenance is an expression. Hence the need to design new approaches able to manage not only the data volume, but also the variety and velocity, extracting actual value from the humongous amounts of collected data. To this aim, we present iSTEP, an integrated Self-Tuning Engine for Predictive maintenance, based on Big Data technologies and designed for Industry 4.0 applications. The proposed approach targets some of the most common needs of manufacturing enterprises: compatibility with both the on-premises and the in-the-cloud environments, exploitation of reliable and largely supported Big Data platforms, easy deployment through containerized software modules, virtually unlimited horizontal scalability, fault-tolerant self-reconfiguration, flexible yet friendly streaming-KPI computations, and above all, the integrated provisioning of self-tuning machine learning techniques for predictive maintenance. The current implementation of iSTEP exploits a distributed architecture based on Apache Kafka, Spark Streaming, MLlib, and Cassandra; iSTEP provides (i) a specific feature engineering block aimed at automatically extracting metrics from the production monitoring time series, which improves the predictive performance by 77% on average, and (ii) a self-tuning approach that dynamically selects the best prediction algorithm, which improves the predictive performance up to 60%. The iSTEP engine provides transparent predictive models, able to provide end users with insights into the knowledge learned, and it has been experimentally evaluated on a public unbalanced failure dataset, whose extensive results are discussed in the paper.",https://ieeexplore.ieee.org/document/8672266/,"2018 IEEE Intl Conf on Parallel & Distributed Processing with Applications, Ubiquitous Computing & Communications, Big Data & Cloud Computing, Social Computing & Networking, Sustainable Computing & Communications (ISPA/IUCC/BDCloud/SocialCom/SustainCom)",11-13 Dec. 2018,ieeexplore
10.1109/TLA.2021.9480156,A Data Governance Framework for Industry 4.0,IEEE,Journals,"The fourth industrial revolution, or Industry 4.0, represents a new stage of evolution in the organization, management and control of the value chain throughout the product or service life cycle. This digitization of the industrial environment is characterized by the connection of Information Technologies (IT) and Operations Technologies (OT) through cyber-physical systems and the Industrial IoT (IIoT). One of the main consequences of this integration is the increasing amount and variety of data generated in real time from different sources. In this environment of intensive generation of actionable information, data becomes a critical asset for Industry 4.0, at all stages of the value chain. However, in order to data become a competitive advantage for the company, it must be managed and governed like any other strategic asset, and therefore it is necessary to rely on a Data Governance system. Industry 4.0 requires a reformulation of governance since the data is a key element and the backbone of the processes of the organization. This paper proposes a Reference Framework for the implementation of Data Governance Systems for Industry 4.0. Previously, it contextualizes data governance for Industry 4.0 environments and identifies the requirements that this framework must address, which are conditioned by the specific features of Industry 4.0, among others, the intensive use of big data, cloud and edge computing, artificial intelligence and current regulations.",https://ieeexplore.ieee.org/document/9480156/,IEEE Latin America Transactions,Dec. 2021,ieeexplore
10.1109/TII.2019.2915846,A Global Manufacturing Big Data Ecosystem for Fault Detection in Predictive Maintenance,IEEE,Journals,"Artificial intelligence, big data, machine learning, cloud computing, and Internet of Things (IoT) are terms which have driven the fourth industrial revolution. The digital revolution has transformed the manufacturing industry into smart manufacturing through the development of intelligent systems. In this paper, a big data ecosystem is presented for the implementation of fault detection and diagnosis in predictive maintenance with real industrial big data gathered directly from large-scale global manufacturing plants, aiming to provide a complete architecture which could be used in industrial IoT-based smart manufacturing in an industrial 4.0 system. The proposed architecture overcomes multiple challenges including big data ingestion, integration, transformation, storage, analytics, and visualization in a real-time environment using various technologies such as the data lake, NoSQL database, Apache Spark, Apache Drill, Apache Hive, OPC Collector, and other techniques. Transformation protocols, authentication, and data encryption methods are also utilized to address data and network security issues. A MapReduce-based distributed PCA model is designed for fault detection and diagnosis. In a large-scale manufacturing system, not all kinds of failure data are accessible, and the absence of labels precludes all the supervised methods in the predictive phase. Furthermore, the proposed framework takes advantage of some of the characteristics of PCA such as its ease of implementation on Spark, its simple algorithmic structure, and its real-time processing ability. All these elements are essential for smart manufacturing in the evolution to Industry 4.0. The proposed detection system has been implemented into the real-time industrial production system in a cooperated company, running for several years, and the results successfully provide an alarm warning several days before the fault happens. A test case involving several outages in 2014 is reported and analyzed in detail during the experiment section.",https://ieeexplore.ieee.org/document/8710319/,IEEE Transactions on Industrial Informatics,Jan. 2020,ieeexplore
10.1109/ACCESS.2020.2990190,"A Novel Simulated-Annealing Based Electric Bus System Design, Simulation, and Analysis for Dehradun Smart City",IEEE,Journals,"Smart transportation network development with environmental issues into consideration has brought Industry 4.0 based solutions on priority. In this direction, battery-powered electric bus systems have been considered widely for ensuring flexibility, operation cost, and lesser pollutants emission. Industry 4.0 provides automation through a cyber-physical system (CPS), the interconnection of bus system entities with industrial internet-of-things (IIoT), remote information availability through cloud computing and scientific disciplines (human-computer interaction, artificial intelligence, machine learning etc.) integration. In this work, a discrete event-based simulation-optimization approach is integrated that take care of bus energy consumption according to real-time city's passenger needs and on-road friction levels. The proposed simulation optimization methodology utilizes multi-objective with dependent and independent variables for optimizing the overall system performance. In simulation optimization, objective functions are designed to tackle battery consumption, Internet-of-Thing (IoT) network performance, cloud operations efficiency and smart scientific discipline integration. Simulation parameters are based on a real-time bus system which is further analyzed, filtered and adapted as per the needs of the system. In another analysis, supercharger's capacities are varied to evaluate the performance of the proposed system and identify the low cost and efficient smart transportation system. Simulation results show different scenarios for variations in the number of buses, charging stations, bus-depots, mobile charging facilities, and bus-schedules. Simulation results show that the average passenger's waiting time in the waiting is (after ticket booking) varies between 0.2 minutes to 0.7 minutes in real-time traffic conditions. In similar traffic conditions, total passenger's time in system (ticket booking to travel) varies between 41.6 minutes (for 24 hours) to 45.5 minutes (for 1 year). In the simulation, priorities are given to those dependent and independent variables which save the battery consumption and elongate the utilization of buses. Lastly, it is also observed that the proposed system is suitable for resource-constraint devices because Gate Equivalent (GE) calculation shows that the proposed system can be implemented between 1986 GEs (communicational cost without confidentiality and authentication) and 7939 GEs (computational cost with HMAC for authentication in data storage). This ensures varies security primitivs such as confidentiality, availability and authentication.",https://ieeexplore.ieee.org/document/9078106/,IEEE Access,2020,ieeexplore
10.1109/ACCESS.2021.3121302,A Survey on Industry 4.0 for the Oil and Gas Industry: Upstream Sector,IEEE,Journals,"The market volatility in the oil and gas (O&amp;G) sector, the dwindling demand for oil due to the impact of COVID-19, and the push for alternative greener energy are driving the need for innovation and digitization in the O&amp;G industry. This has attracted research interest from academia and the industry in the application of industry 4.0 (I4.0) technologies in the O&amp;G sector. The application of some of these I4.0 technologies has been presented in the literature, but the domain still lacks a comprehensive survey of the application of I4.0 in the O&amp;G upstream sector. This paper investigates the state-of-the-art efforts directed toward I4.0 technologies in the O&amp;G upstream sector. To achieve this, first, an overview of the I4.0 is discussed followed by a systematic literature review from an integrative perspective for publications between 2012–2021 with 223 analyzed documents. The benefits and challenges of the adoption of I4.0 have been identified. Moreover, the paper adds value by proposing a framework for the implementation of I4.0 in the O&amp;G upstream sector. Finally, future directions and research opportunities such as framework, edge computing, quantum computing, communication technologies, standardization, and innovative areas related to the implementation of I4.0 in the upstream sector are presented. The findings from this review show that I4.0 technologies are currently being explored and deployed for various aspects of the upstream sector. However, some of the I4.0 technologies like additive manufacturing and virtual reality are least explored.",https://ieeexplore.ieee.org/document/9579415/,IEEE Access,2021,ieeexplore
10.1109/TII.2019.2962029,An Entropy-Based Approach to Real-Time Information Extraction for Industry 4.0,IEEE,Journals,"Industry 4.0 has drawn considerable attention from industry and academic research communities. The recent advances in Internet of Things (IoT), Big Data analytics, sensor technology, and artificial intelligence have led to the design and implementation of novel approaches to take full advantage of data-driven solutions applicable to Industry 4.0. With the availability of large datasets, it has become crucially important to identify the appropriate amount of relevant information, which would optimize the overall analysis of the corresponding systems. In this article, specific properties of dynamically evolving data systems are introduced and investigated, which provide framework to assess the appropriate amount of representative information.",https://ieeexplore.ieee.org/document/8941297/,IEEE Transactions on Industrial Informatics,Sept. 2020,ieeexplore
10.1109/ACCESS.2020.2993010,Bearing Intelligent Fault Diagnosis in the Industrial Internet of Things Context: A Lightweight Convolutional Neural Network,IEEE,Journals,"The advancement of Industry 4.0 and Industrial Internet of Things (IIoT) has laid more emphasis on reducing the parameter amount and storage space of the model in addition to the automatic and accurate fault diagnosis. In this case, this paper proposes a lightweight convolutional neural network (LCNN) method for intelligent fault diagnosis of bearing, which can largely satisfy the need of less parameter amount and storage space as well as high accuracy. First, depthiwise separable convolution is adopted, and a LCNN structure is constructed through an inverse residual structure and a linear bottleneck layer operation. Secondly, a novel decomposed Hierarchical Search Space is introduced to automatically explore the optimal LCNN for bearing fault diagnosis in the context of the IIoT. In the meantime, the real-time monitoring and fault diagnosis of the model are also deployed. In order to verify the validity of the designed model, Case Western Reserve University Bearing fault dataset and MFPT bearing fault dataset are adopted. The results demonstrate the great advantages of the model. The LCNN model can automatically learn and select the appropriate features, highly improving the fault diagnosis accuracy. Meanwhile, the computational and storage costs of the model are largely reduced, which contributes to its being applied to the mechanical system in the IIoT context.",https://ieeexplore.ieee.org/document/9088980/,IEEE Access,2020,ieeexplore
10.1109/TII.2020.3028612,DWFCAT: Dual Watermarking Framework for Industrial Image Authentication and Tamper Localization,IEEE,Journals,"The image data received through various sensors are of significant importance in Industry 4.0. Unfortunately, these data are highly vulnerable to various malicious attacks during its transit to the destination. Although the use of pervasive edge computing (PEC) with the Internet of Things (IoT) has solved various issues, such as latency, proximity, and real-time processing, but the security and authentication of data between the nodes is still a significant concern in PEC-based industrial-IoT scenarios. In this article, we present “DWFCAT,” a dual watermarking framework for content authentication and tamper localization for industrial images. The robust and fragile watermarks along with overhead bits related to the cover image for tamper localization are embedded in different planes of the cover image. We have used discrete cosine transform coefficients and exploited their energy compaction property for robust watermark embedding. We make use of a four-point neighborhood to predict the value of a predefined pixel and use it for embedding the fragile watermark bits in the spatial domain. Chaotic and deoxyribonucleic acid encryption is used to encrypt the robust watermark before embedding to enhance its security. The results indicate that DWFCAT can withstand a range of hybrid signal processing and geometric attacks, such as Gaussian noise, salt and pepper, joint photographic experts group (JPEG) compression, rotation, low-pass filtering, resizing, cropping, sharpening, and histogram equalization. The experimental results prove that the DWFCAT is highly efficient compared with the various state-of-the-art approaches for authentication and tamper localization of industrial images.",https://ieeexplore.ieee.org/document/9214433/,IEEE Transactions on Industrial Informatics,July 2021,ieeexplore
10.1109/ACCESS.2021.3101284,"Data-Driven Remaining Useful Life Estimation for Milling Process: Sensors, Algorithms, Datasets, and Future Directions",IEEE,Journals,"An increase in unplanned downtime of machines disrupts and degrades the industrial business, which results in substantial credibility damage and monetary loss. The cutting tool is a critical asset of the milling machine; the failure of the cutting tool causes a loss in industrial productivity due to unplanned downtime. In such cases, a proper predictive maintenance strategy by real-time health monitoring of cutting tools becomes essential. Accurately predicting the useful life of equipment plays a vital role in the predictive maintenance arena of industry 4.0. Many active research efforts have been done to estimate tool life in varied directions. However, the consolidated study of the implemented techniques and future pathways is still missing. So, the purpose of this paper is to provide a systematic and comprehensive literature survey on the data-driven approach of Remaining Useful Life (RUL) estimation of cutting tools during the milling process. The authors have summarized different monitoring techniques, feature extraction methods, decision-making models, and available sensors currently used in the data-driven model. The authors have also presented publicly available datasets related to milling under various operating conditions to compare the accuracy of the prediction model for tool wear estimation. Finally, the article concluded with the challenges, limitations, recent advancements in RUL prognostics techniques using Artificial Intelligence (AI), and future research scope to explore more in this area.",https://ieeexplore.ieee.org/document/9502093/,IEEE Access,2021,ieeexplore
10.1109/ACCESS.2020.2998723,"Digital Twin for the Oil and Gas Industry: Overview, Research Trends, Opportunities, and Challenges",IEEE,Journals,"With the emergence of industry 4.0, the oil and gas (O&amp;G) industry is now considering a range of digital technologies to enhance productivity, efficiency, and safety of their operations while minimizing capital and operating costs, health and environment risks, and variability in the O&amp;G project life cycles. The deployment of emerging technologies allows O&amp;G companies to construct digital twins (DT) of their assets. Considering DT adoption, the O&amp;G industry is still at an early stage with implementations limited to isolated and selective applications instead of industry-wide implementation, limiting the benefits from DT implementation. To gain the full potential of DT and related technological adoption, a comprehensive understanding of DT technology, the current status of O&amp;G-related DT research activities, and the opportunities and challenges associated with the deployment of DT in the O&amp;G industry are of paramount importance. In order to develop this understanding, this paper presents a literature review of DT within the context of the O&amp;G industry. The paper follows a systematic approach to select articles for the literature review. First, a keywords-based publication search was performed on the scientific databases such as Elsevier, IEEE Xplore, OnePetro, Scopus, and Springer. The filtered articles were then analyzed using online text analytic software (Voyant Tools) followed by a manual review of the abstract, introduction and conclusion sections to select the most relevant articles for our study. These articles and the industrial publications cited by them were thoroughly reviewed to present a comprehensive overview of DT technology and to identify current research status, opportunities and challenges of DT deployment in the O&amp;G industry. From this literature review, it was found that asset integrity monitoring, project planning, and life cycle management are the key application areas of digital twin in the O&amp;G industry while cyber security, lack of standardization, and uncertainty in scope and focus are the key challenges of DT deployment in the O&amp;G industry. When considering the geographical distribution for the DT related research in the O&amp;G industry, the United States (US) is the leading country, followed by Norway, United Kingdom (UK), Canada, China, Italy, Netherland, Brazil, Germany, and Saudi Arabia. The overall publication rate was less than ten articles (approximately) per year until 2017, and a significant increase occurred in 2018 and 2019. The number of journal publications was noticeably lower than the number of conference publications, and the majority of the publications presented theoretical concepts rather than the industrial implementations. Both these observations suggest that the DT implementation in the O&amp;G industry is still at an early stage.",https://ieeexplore.ieee.org/document/9104682/,IEEE Access,2020,ieeexplore
10.1109/ACCESS.2021.3120843,Digital Twins From Smart Manufacturing to Smart Cities: A Survey,IEEE,Journals,"Digital twins are quickly becoming a popular tool in several domains, taking advantage of recent advancements in the Internet of Things, Machine Learning and Big Data, while being used by both the industry sector and the research community. In this paper, we review the current research landscape as regards digital twins in the field of smart cities, while also attempting to draw parallels with the application of digital twins in Industry 4.0. Although digital twins have received considerable attention in the Industrial Internet of Things domain, their utilization in smart cities has not been as popular thus far. We discuss here the open challenges in the field and argue that digital twins in smart cities should be treated differently and be considered as cyber-physical “systems of systems”, due to the vastly different system size, complexity and requirements, when compared to other recent applications of digital twins. We also argue that researchers should utilize established tools and methods of the smart city community, such as co-creation, to better handle the specificities of this domain in practice.",https://ieeexplore.ieee.org/document/9576739/,IEEE Access,2021,ieeexplore
10.1109/TII.2021.3049405,Enabling Secure Authentication in Industrial IoT With Transfer Learning Empowered Blockchain,IEEE,Journals,"Industrial Internet of Things (IIoT) is ushering in huge development opportunities in the era of Industry 4.0. However, there are significant data security and privacy challenges during automatic and real-time data collection, monitoring for industrial applications in IIoT. Data security and privacy in IIoT applications are closely related to the reliability of users, which is determined by user authentication that have been widely used as an effective approach. However, the existing user authentication mechanisms in IIoT suffer from single factor authentication and poor adaptability with the rapid growth of the number of users and the diversity of user categories. To solve the aforementioned issues, this article proposes a novel Authentication mechanism based on Transfer Learning empowered Blockchain, coined ATLB. In ATLB, blockchains are applied to achieve the privacy preservation for industrial applications. In addition, by introducing the transfer learning based authentication mechanism, trustworthy blockchains are built such that the privacy preservation for industrial applications is further enhanced. Specifically, ATLB first employs a guiding deep deterministic policy gradient algorithm to train the user authentication model of a specific region, which is then transferred locally for foreign user authentication or cross-regionally for another region's user authentication such that the model training time is significantly reduced. Experimental results show that the proposed ATLB not only provides accurate authentications for IIoT applications but also achieves high throughput and low latency.",https://ieeexplore.ieee.org/document/9314211/,IEEE Transactions on Industrial Informatics,Nov. 2021,ieeexplore
10.1109/TII.2020.3034627,Guest Editorial: Special Section on Advanced Signal Processing and AI Technologies for Industrial Big Data,IEEE,Journals,The papers in this special section focus on advanced signal processing and artificial intelligence (AI) technologies for industrial Big Data (IBD) powered by Industry 4.0. Modern industry has evolved from the traditional manufacturing industry to digital and intelligent industry. Huge amount of complex real-time data are generated from the thousands of industrial sensors in physical and man-made environments. Industrial big data (IBD) afford us an unprecedented opportunity to obtain an in-depth understanding of Internet of Things and facilitate data-driven approaches for industrial optimization and scheduling. The papers in this section collect the latest ideas and research on advanced signal processing and artificial intelligence (AI) technologies for IBD.,https://ieeexplore.ieee.org/document/9361683/,IEEE Transactions on Industrial Informatics,May 2021,ieeexplore
10.1109/ACCESS.2018.2809681,Industrial Big Data Analytics for Prediction of Remaining Useful Life Based on Deep Learning,IEEE,Journals,"Due to the recent development of cyber-physical systems, big data, cloud computing, and industrial wireless networks, a new era of industrial big data is introduced. Deep learning, which brought a revolutionary change in computer vision, natural language processing, and a variety of other applications, has significant potential for solutions providing in sophisticated industrial applications. In this paper, a concept of device electrocardiogram (DECG) is presented, and an algorithm based on deep denoising autoencoder (DDA) and regression operation is proposed for the prediction of the remaining useful life of industrial equipment. First, the concept of electrocardiogram is explained. Then, a problem statement based on manufacturing scenario is presented. Subsequently, the architecture of the proposed algorithm called integrated DDA and the algorithm workflow are provided. Moreover, DECG is compared with traditional factory information system, and the feasibility and effectiveness of the proposed algorithm are validated experimentally. The proposed concept and algorithm combine typical industrial scenario and advance artificial intelligence, which has great potential to accelerate the implementation of industry 4.0.",https://ieeexplore.ieee.org/document/8302913/,IEEE Access,2018,ieeexplore
10.1109/ACCESS.2017.2754507,Model-Based Development of Knowledge-Driven Self-Reconfigurable Machine Control Systems,IEEE,Journals,"To accommodate the trend toward mass customization launched by intelligent manufacturing in the era of Industry 4.0, this paper proposes the combination of model-driven engineering and knowledgedriven engineering during the development process of self-reconfigurable machine control systems. The complete tool chain for model development, execution, and reconfiguration is established. For the design phase, a machine-control-domain-specific modeling language and the supporting design environment are developed. With regard to the execution stage, a runtime framework compliant with the IEC 61499 standard is proposed. On the ground of the modeling environment and the reconfigurable run-time framework, a self-adaptive control module is developed to establish the close-loop self-reconfiguration infrastructure. The ontological representation of knowledge base toward this end is described, along with extendable SQWRL rules specified to automatically initiate the reconfiguration process in the cases of external user demands and internal faults. A prototype motion control kernel in the low-level layer of machine control system architecture is developed with the proposed modeling language and is then deployed to the runtime framework. Two case studies on self-reconfiguration of the proof-of-concept motion control kernel are demonstrated, which prove the feasibility of our proposal.",https://ieeexplore.ieee.org/document/8047091/,IEEE Access,2017,ieeexplore
10.1109/TII.2019.2958606,Real-Time Anomaly Detection of NoSQL Systems Based on Resource Usage Monitoring,IEEE,Journals,"Today, the emergence of the industry revolution systems such as Industry 4.0, Internet of Things, and big data frameworks poses new challenges in terms of storage and processing of real-time data. As systems scale in humongous sizes, a crucial task is to administer the variety of different subsystems and applications to ensure high performance. This is directly related with the identification and elimination of system failures and errors, while the system runs. In particular, database systems may experience abnormalities related with decreased throughput or increased resource usage, that in turn affects system performance. In this article, we focus on not only SQL (NoSQL) database systems that are ideal for storing sensor data in the concept of Industry 4.0. This typically includes a variety of applications and workloads that are difficult to online monitor, thus making anomaly detection a challenging task. Creating a robust platform to serve such infrastructures with minimum hardware or software failures is a key challenge. In this article, we propose RADAR, an anomaly detection system that works on real time. RADAR is a data-driven decision-making system for NoSQL systems, by providing process information extraction during resource monitoring and by associating resource usage with the top processes, to identify anomalous cases. In this article, we focus on anomalies such as hardware failures or software bugs that could lead to abnormal application runs, without necessarily stopping system functionality, e.g., due to a system crash, but by affecting its performance, e.g., decreased database system throughput. Although different patterns may occur through time, we focus on periodic running workloads (e.g., monitoring daily usage) that are very common for NoSQL systems, and Internet of Things scenarios where data streams are forwarded to the Cloud for storage and processing. We apply various machine learning algorithms such as autoregressive integrated moving average (ARIMA), seasonal ARIMA, and long-short-term memory recurrent neural networks. We experimentally analyze our solution to demonstrate the benefits of supporting online erroneous state identification and characterization for modern applications.",https://ieeexplore.ieee.org/document/8930068/,IEEE Transactions on Industrial Informatics,Sept. 2020,ieeexplore
10.1109/JSEN.2021.3075535,Recent Advancements in the Development of Sensors for the Structural Health Monitoring (SHM) at High-Temperature Environment: A Review,IEEE,Journals,"With Industry 4.0 becoming increasingly pervasive, the importance and usage of sensors has increased several folds. Industry 4.0 refers to a new phase in the industrial revolution that mainly focuses on interconnectivity, automation, machine learning, and real-time data. Real-time structural health monitoring (SHM) of components in the industrial process is one of the crucial and important component of Industry 4.0. SHM of components exposed to high-temperature (<inline-formula> <tex-math notation=""LaTeX"">$\sim 650^{\circ }\text{C}$ </tex-math></inline-formula>) is becoming increasingly important nowadays. However, harsh and high temperature environments impose a great challenge towards their implementation. This review is an attempt to demonstrate the development, application, limitations and recent advancement of the existing sensors used for SHM. Some sensors such as eddy current (EC) sensors and fiber Bragg grating (FBG) sensors have been discussed in detail. A phenomenological study of the electromagnetic sensor for the SHM of engineering components that are exposed to high temperature has been addressed. State-of-the-art fabrication methodologies such as low temperature co-fired ceramic (LTCC) technology for such type of sensors for high temperature SHM applications have been elucidated. Future challenges and opportunities for SHM applications of high temperature sensors have been highlighted.",https://ieeexplore.ieee.org/document/9415648/,IEEE Sensors Journal,"15 July15, 2021",ieeexplore
10.1109/ACCESS.2020.3045563,SDN Enhanced Resource Orchestration of Containerized Edge Applications for Industrial IoT,IEEE,Journals,"With the rise of the Industrial Internet of Things (IIoT), there is an intense pressure on resource and performance optimization leveraging on existing technologies, such as Software Defined Networking (SDN), edge computing, and container orchestration. Industry 4.0 emphasizes the importance of lean and efficient operations for sustainable manufacturing. Achieving this goal would require engineers to consider all layers of the system, from hardware to software, and optimizing for resource efficiency at all levels. This emphasizes the need for container-based virtualization tools such as Docker and Kubernetes, offering Platform as a Service (PaaS), while simultaneously leveraging on edge technologies to reduce related latencies. For network management, SDN is poised to offer a cost-effective and dynamic scalability solution by customizing packet handling for various edge applications and services. In this paper, we investigate the energy and latency trade-offs involved in combining these technologies for industrial applications. As a use case, we emulate a 3D-drone-based monitoring system aimed at providing real-time visual monitoring of industrial automation. We compare a native implementation to a containerized implementation where video processing is orchestrated while streaming is handled by an external UE representing the IIoT device. We compare these two scenarios for energy utilization, latency, and responsiveness. Our test results show that only roughly 16 percent of the total power consumption happens on the mobile node when orchestrated. Virtualization adds up about 4.5 percent of the total power consumption while the latency difference between the two approaches becomes negligible after the streaming session is initialized.",https://ieeexplore.ieee.org/document/9296769/,IEEE Access,2020,ieeexplore
10.1109/ACCESS.2020.2994933,Sensor-Driven Learning of Time-Dependent Parameters for Prescriptive Analytics,IEEE,Journals,"Big data analytics is rapidly emerging as a key Internet of Things (IoT) initiative aiming at providing meaningful insights and supporting optimal decision making under time constraints. In this direction, prescriptive analytics has just started to emerge. Prescriptive analytics moves beyond descriptive and predictive analytics aiming at providing adaptive, automated, constrained, time-dependent and optimal decisions. The use of time-dependent parameters in prescriptive analytics models provide a more reliable and realistic representation of the complex and dynamic environment and the associated decision making process; however, their estimation poses significant challenges due to the uncertainty derived from inaccurate user input, noisy data, and non-stationarity of real-world data streams. Since feedback and learning mechanisms for tracking the prescriptive analytics are crucial enablers for self-configuration and self-optimization, this paper proposes an approach for sensor-driven learning of time-dependent parameters for prescriptive analytics models deployed in streaming computational environments. The proposed approach was validated in an Industry 4.0 use case, while it was further evaluated through extensive simulation experiments. The proposed approach overcomes challenges related to uncertainty derived from user's input, non-stationary data and sensor noise and provides estimates of time-dependent parameters that lead to more reliable prescriptions.",https://ieeexplore.ieee.org/document/9094172/,IEEE Access,2020,ieeexplore
10.1109/JSEN.2020.3029459,"Sensor-Fault Detection, Isolation and Accommodation for Digital Twins via Modular Data-Driven Architecture",IEEE,Journals,"Sensor technologies empower Industry 4.0 by enabling integration of in-field and real-time raw data into digital twins. However, sensors might be unreliable due to inherent issues and/or environmental conditions. This article aims at detecting anomalies in measurements from sensors, identifying the faulty ones and accommodating them with appropriate estimated data, thus paving the way to reliable digital twins. More specifically, we propose a general machine-learning-based architecture for sensor validation built upon a series of neural-network estimators and a classifier. Estimators correspond to virtual sensors of all unreliable sensors (to reconstruct normal behaviour and replace the isolated faulty sensor within the system), whereas the classifier is used for detection and isolation tasks. A comprehensive statistical analysis on three different real-world data-sets is conducted and the performance of the proposed architecture validated under hard and soft synthetically-generated faults.",https://ieeexplore.ieee.org/document/9216114/,IEEE Sensors Journal,"15 Feb.15, 2021",ieeexplore
10.1109/ACCESS.2021.3083499,Towards Secured Online Monitoring for Digitalized GIS Against Cyber-Attacks Based on IoT and Machine Learning,IEEE,Journals,"Recently, the Internet of Things (IoT) has an important role in the growth and development of digitalized electric power stations while offering ambitious opportunities, specifically real-time monitoring and cybersecurity. In this regard, this paper introduces a novel IoT architecture for the online monitoring of the gas-insulated switchgear (GIS) status instead of the traditional observation methods. The proposed IoT architecture is derived from the concept of the cyber-physic system (CPS) in Industry 4.0. However, the cyber-attacks and the classification of the GIS insulation defects represent the main challenges against the implementation of IoT topology for the online monitoring and tracking of the GIS status. For this purpose, advanced machine learning techniques are utilized to detect cyber-attacks to conduct the paradigm and verification. Different test scenarios on various defects in GIS are performed to demonstrate the effectiveness of the proposed IoT architecture. Partial discharge pulse sequence features are extracted for each defect to represent the inputs for IoT architecture. The results confirm that the proposed IoT architecture based on the machine learning technique, that is the extreme gradient boosting (XGBoost), can visualize all defects in the GIS with different alarms, besides showing the cyber-attacks on the networks effectively. Furthermore, the defects of GIS and the fake data due to the cyber-attacks are recognized and presented on the dashboard of the proposed IoT platform with high accuracy and more clarified visualization to enhance the decision-making about the GIS status.",https://ieeexplore.ieee.org/document/9440436/,IEEE Access,2021,ieeexplore
10.1109/ACCESS.2021.3082934,Visual Product Tracking System Using Siamese Neural Networks,IEEE,Journals,"Management of unstructured production data is a key challenge for Industry 4.0. Effective product tracking endorses data integration and productivity improvements throughout the manufacturing processes. Radio-frequency identification (RFID) tags are used in many tracking cases, but in some manufacturing environments, those cannot be used as they might get damaged or removed during processing. In this paper, we propose an alternative visual product tracking system. The physical system uses two cameras placed at the two ends of the tracked process(es). Product pairs are then matched with a Siamese neural network operating on the product images and trained offline on the problem at hand with labeled data. The proposed system can track products solely based on their visual appearance and without any physical interference with the products or production processes. Unlike other existing image-based methods, the proposed system is invariant to major positional and visual changes in the products. As a proof-of-concept, we tested the proposed system with real plywood factory data and were able to track the products with 98.5 % accuracy in a realistic test scenario. The implementation of the proposed method and the Veneer21 dataset are publicly available at https://github.com/TuomasJalonen/visual-product-tracking-system.",https://ieeexplore.ieee.org/document/9439511/,IEEE Access,2021,ieeexplore
10.1109/ANSS.2005.8,A neural approach for fast simulation of flight mechanics,IEEE,Conferences,"Flight simulators have been part of aviation history since its beginning. With the development of modern aeronautics industry, flight simulators have gained an important place and the industry devoted to their manufacture has become significant. In the case of transportation aircraft, accurate mathematical models based on extensive experimental data have been developed by their manufacturers to optimize their aerodynamic and propulsive characteristics and to design efficient flight control systems. However, in the case of small general aviation aircraft this kind of knowledge is not commonly available and the design of accurate flight simulators can result in a tedious try and modify process until the simulator presents a qualitative behaviour close to the one of the real aircraft. This communication proposes through the use of neural networks a method to perform a direct estimation of the aerodynamic forces acting on aircraft. Artificial neural networks appear to be an appropriate numerical technique to achieve the mapping of these continuous relationships and detailed aerodynamics and thrust models should become no more mandatory to produce accurate flight simulation software.",https://ieeexplore.ieee.org/document/1401963/,38th Annual Simulation Symposium,4-6 April 2005,ieeexplore
10.1109/ICBAIE52039.2021.9389899,An Open-Source Programming Language-Based Interactive Device: Popular Science of the Five Cereals for Children,IEEE,Conferences,"In this paper, Arduino is used in combination with switch, lighting and sound facilities to realize human-computer interaction. Arduino is an open-source electronics platform that integrates hardware and software. Given its versatility, expressiveness and operability, designers and artists are free to turn their ideas into reality on this platform, using programming languages to create high-quality interactive installations. In response to children's difficulty telling the difference between the five cereals, this paper seeks to educate children on crops in the form of human-computer interaction through the design and manufacture of a popular science-themed interactive device. Through the external hardware configuration (pressure induction, lamp and sound), children are guided to knowing and grasping the growth stages of common plants. In this way, children's perceptions of nature and crops can be constructed through a combination of technology and art.",https://ieeexplore.ieee.org/document/9389899/,"2021 IEEE 2nd International Conference on Big Data, Artificial Intelligence and Internet of Things Engineering (ICBAIE)",26-28 March 2021,ieeexplore
10.23919/IConAC.2019.8895095,Ant Colony Optimization Algorithm for Industrial Robot Programming in a Digital Twin,IEEE,Conferences,"Advanced manufacturing that is adaptable to constantly changing product designs often requires dynamic changes on the factory floor to enable manufacture. The integration of robotic manufacture with machine learning approaches offers the possibility to enable such dynamic changes on the factory floor. While ensuring safety and the possibility of losses of components and waste of material are against their usage. Furthermore, developments in design of virtual environments makes it possible to perform simulations in a virtual environment, to enable human-in-the-loop production of parts correctly the first time like never before. Such powerful simulation and control software provides the means to design a digital twin of manufacturing environment in which trials are completed at almost at no cost. In this paper, ant colony optimization is used to program an industrial robot to avoid obstacles and find its way to pick and place objects during an assembly task in an environment containing obstacles that must be avoided. The optimization is completed in a digital twin environment first and movements transferred to the real robot after human inspection. It is shown that the proposed methodology can find the optimal solution, in addition to avoiding collisions, for an assembly task with minimum human intervention.",https://ieeexplore.ieee.org/document/8895095/,2019 25th International Conference on Automation and Computing (ICAC),5-7 Sept. 2019,ieeexplore
10.1109/SMC.2013.819,Automated Sound Signalling Device Quality Assurance Tool for Embedded Industrial Control Applications,IEEE,Conferences,This paper presents a novel system for automatic detection and recognition of faulty audio signaling devices as part of an automated industrial manufacturing process. The system uses historical data labeled by human experts in detecting faulty signaling devices to train an artificial neural network based classifier for modeling their decision making process. The neural network is implemented on a real time embedded micro controller which can be more efficiently incorporated into an automated production line eliminating the need for a manual inspection within the manufacturing process. We present real world experiments based on data pertaining to the production and manufacture of audio signaling components used in car instrument clusters. Our results show that the proposed expert system is able to successfully classify faulty audio signaling devices to a high degree of accuracy. The results can be generalized to other signaling devices where an output signal is represented by a complex and changing frequency spectrum even with significant environmental noise.,https://ieeexplore.ieee.org/document/6722574/,"2013 IEEE International Conference on Systems, Man, and Cybernetics",13-16 Oct. 2013,ieeexplore
10.1109/ICIAICT.2019.8784838,Automated Testing Framework for Geographical Distributed Testing Environment,IEEE,Conferences,"Automated testing is critical for the complex products development and manufacture. In generally, connecting multiple test environments in geographical distributed environment to share resources is a way to reduce cost. However, it conflicts with the principle of “keep test environment clean”. Moreover, high latency between geographical different networks may impact origin automated test activities in reality. In this paper, we propose an automated testing framework for geographical distributed testing environment to solve the problems of sharing resources. In the proposed framework, improved Test Harness (TH) and integrated Test as a Service (TaaS) are proposed to assure automated testing flow and guarantee test integrity. The proposed framework may not impact origin automated test activities, which could reduce the cost of test activities in really. In addition, a basic implementation of Test Harness and TaaS Console in the proposed framework are presented in this paper.",https://ieeexplore.ieee.org/document/8784838/,"2019 IEEE International Conference on Industry 4.0, Artificial Intelligence, and Communications Technology (IAICT)",1-3 July 2019,ieeexplore
10.1109/ICAMIMIA47173.2019.9223365,Autonomous Car Simulation Using Evolutionary Neural Network Algorithm,IEEE,Conferences,"Automation with artificial intelligence (AI) has widely implemented in robotics, transportation and manufacture. AI has become a powerful technology that change human life and help human more flexible doing something. In this paper, it will show a result of simulation from an autonomous car using the evolutionary neural network algorithm which combines genetic algorithm and neural network. The purpose of the simulation is to test the model that we develop to know the right direction based on the track, so the evolutionary neural network that implemented to the autonomous car be able to deliver the best solution before it implements in the real machine or car technology. Genetic algorithm combines with a neural network to reach an evolution condition. The evolution process is achieved through crossover, mutation and selection process, so the algorithm will give the best result from the iteration of the experiment. The result of our experiment shows that evolutionary neural network algorithm give the best result within 3 layer architecture, with iteration average is 14.5 reach finish point (check point) 3 in the track simulation. Based on the simulation, our car model can find out the right direction.",https://ieeexplore.ieee.org/document/9223365/,"2019 International Conference on Advanced Mechatronics, Intelligent Manufacture and Industrial Automation (ICAMIMIA)",9-10 Oct. 2019,ieeexplore
10.1109/ICMLA.2019.00320,Coarse Annotation Refinement for Segmentation of Dot-Matrix Batchcodes,IEEE,Conferences,"Deep Convolutional Neural Networks (CNN) have been extensively applied in various computer vision tasks. Although such approaches have demonstrated exceptionally high performance in various open challenges, adapting them to more specialised tasks can be non-trivial. In this paper we discuss our design and implementation of a batchcode detection system capable of accurate segmentation of batchcode regions within images of consumer products. A batchcode is a unique identifier printed on the packaging of many products that encodes useful information such as date and location of manufacture. Detection of batchcodes in images of products is a useful step in many processes, including quality control, supply chain tracking and counterfeit detection. Beginning with a unique dataset of product images and a set of crowdsourced coarse annotations that roughly correspond to the locations of batchcodes, we demonstrate that such annotations are insufficient for training a reliable model, and subsequently describe a novel label refinement process, which we call the Maximally Stable Global Region (MSGR) method, that we use to generate accurate ground-truth data suitable for training a robust neural network. We also show that detection accuracy can be further improved by applying MSGR to the output of the neural network. We evaluate our approach using a manually labelled test dataset of images of shampoo bottles, and demonstrate the efficacy of the proposed method for accurate real-time batchcode detection.",https://ieeexplore.ieee.org/document/8999037/,2019 18th IEEE International Conference On Machine Learning And Applications (ICMLA),16-19 Dec. 2019,ieeexplore
10.1109/DESSERT50317.2020.9125038,Combination of Digital Twin and Artificial Intelligence in Manufacturing Using Industrial IoT,IEEE,Conferences,"The paper focuses on Digital Twin (DT) in Manufacturing using Artificial Intelligence (AI) and Industrial IoT. According to the concept, the manufacturing includes three main units: equipment, personnel and processes. All data from these units are inherited to manufacture model (DT) and decision support system with the use of AI. DT data technology allows finding the required knowledge that can be interpreted and used to support the process of decision-making in the management of the enterprise. AI applications open up a broad spectrum of opportunities in manufacturing to add value by optimizing processes and generating new business models. The Landscape was described by a formal model to assure the possibility to analyze the state and development of landscape in detail considering DT and other technologies. DT and IIoT implementation for the simulation of real enterprise manufacturing were considered.",https://ieeexplore.ieee.org/document/9125038/,"2020 IEEE 11th International Conference on Dependable Systems, Services and Technologies (DESSERT)",14-18 May 2020,ieeexplore
10.1109/PERCOMW.2018.8480343,Development of Energy-efficient Sensor Networks by Minimizing Sensors Numbers with a Machine Learning Model,IEEE,Conferences,"With the increasing demand to construct sensor networks for a smart IoT (Internet of Things) world, numerous sensors with sensing and communication capabilities are expected to be deployed in the future. Thanks to the development of hardware manufacture technology, relatively small IoT smart sensors are now commercially available and cost-effective. However, the total power required by operating these sensors is expected to be enormous, due to their large number and frequent activity. Removing “unneeded sensors” is the most direct way to reduce the power consumption of sensor networks. Here, “unneeded sensors” refers to those that can be placed in sleep mode, or even be removed from the network topology entirely, without serious impact on the overall networks data processing performance. In this paper, we report the development of an energy-efficient sensor network by using a machine learning model to determine the actual necessity of all the sensors in a sensor network. Machine learning model is introduced to identify unneeded sensors by comparing the data from neighboring sensors to that from the potentially unneeded ones. For identifying unneeded sensors, different strategies with different computational complexity are also proposed. Numerical experiments conducted in two real indoor environments verify that our proposed scheme can reduce the total number of active sensors by around 1/3, while maintaining more than 90% of the original high monitoring performance of the sensor network.",https://ieeexplore.ieee.org/document/8480343/,2018 IEEE International Conference on Pervasive Computing and Communications Workshops (PerCom Workshops),19-23 March 2018,ieeexplore
10.1109/PES.2007.386098,Dynamic Voltage Restorer with Neural Network Controlled Voltage Disturbance Detector and Real-time Digital Voltage Control,IEEE,Conferences,"This paper describes the high power DVR (dynamic voltage restorer) with the new voltage disturbance detection method and the real-time digital PWM voltage control. The new voltage disturbance detector was implemented by using the delta rule of the neural network control. Through the proposed method, we can instantaneously track the peak value of each phase voltage under the severe unbalance voltage conditions. Compared to the conventional synchronous reference frame method, the proposed one shows the minimum time delay to determine the instance of the voltage sag or voltage swell event. Also real-time digital PWM voltage control technique was adopted, where the inverter output filter capacitance voltage, the filter reactor current and the load current are sampled to calculate the inverter PWM command for the next sampling interval. By using digital control, the disturbance voltage can be compensated to the reference voltage level within two sampling intervals. The proposed disturbance detector and the voltage compensator were applied to the high power DVR (440 V/1000 kVA) that was developed for the application of semiconductor manufacture plant. The performances of the proposed DVR control were verified through computer simulation and experimental results. Finally, conclusions are given.",https://ieeexplore.ieee.org/document/4275864/,2007 IEEE Power Engineering Society General Meeting,24-28 June 2007,ieeexplore
10.1109/ICPST.2006.321897,Medium Voltage Dynamic Voltage Restorer with Neural Network Controlled Voltage Disturbance Detector,IEEE,Conferences,"This paper describes the medium voltage high power DVR (dynamic voltage restorer) with the new voltage disturbance detection method and the real-time digital voltage control. The new voltage disturbance detector was implemented by using the delta rule of the neural network control. Through the proposed method, we can instantaneously obtain the peak value of each phase voltage even under the single phase input voltage conditions. Compared to the conventional synchronous reference frame method, the proposed one shows the minimum time delay to determine the instance of the voltage sag or voltage swell event. Also real-time digital voltage control technique was adopted, where the inverter output filter capacitance voltage, the filter reactor current and the load current are sampled to calculate the inverter PWM command for the next sampling interval. By using digital control, the disturbance voltage can be compensated to the reference voltage level within two sampling intervals. The proposed disturbance detector and the voltage compensator were applied to the medium voltage high power DVR (22.9 kV/4 MVA) that was developed for the application of semiconductor manufacture plant. The performances of the proposed DVR control were verified through computer simulation and experimental results. Finally, conclusions are given.",https://ieeexplore.ieee.org/document/4116056/,2006 International Conference on Power System Technology,22-26 Oct. 2006,ieeexplore
10.1109/AIMS52415.2021.9466014,Multi-Pole Road Sign Detection Based on Faster Region-based Convolutional Neural Network (Faster R-CNN),IEEE,Conferences,"Building an approach system that is able to serve various types of traffic signs is a challenge. The important stages in handling an object are finding objects, dividing them into several categories, and marking objects with bounding boxes. However, in reality, monitoring traffic signs objects is quite difficult because it is based on various factors such as; other closed objects, driving times, or traffic sign conditions. This study aims to measure the level of precision in monitoring traffic signs (detection speed of 4-6 frames per second) from video recording (single camera) using the Faster Region based Convolutional Neural Network (Faster R-CNN) algorithm. The traffic sign detection system uses the Faster R-CNN algorithm with Inception v2 model which is implemented in the TensorFlow API framework. The Faster R-CNN consists of 2 different modules. The first module is a deep convolutional neural network which functions to build the area to be detected, which is called the Regional Proposal Network (RPN), and the second module is the Fast R-CNN detector which functions to use the previously proposed area. This system is one unit, a detection network based on the results of the manufacture and testing of a traffic sign detection system based on the Faster R-CNN method, so it can be shown that there is no difference in the results of detection of traffic signs in day and night conditions. Where the precision testing for detection of traffic signs during the day and at night is 100%.",https://ieeexplore.ieee.org/document/9466014/,2021 International Conference on Artificial Intelligence and Mechatronics Systems (AIMS),28-30 April 2021,ieeexplore
10.1109/CIIMA50553.2020.9290291,Open Source Multichannel EMG Armband design,IEEE,Conferences,"This research paper presents the design and implementation of an open source multichannel EMG armband for hand gesture recognition. Initially a brief introduction about electromyography and similar researches is presented. Then, the general structure of the system is explained, after this it can be found the detailed description of the circuits used to acquire the signal, how the acquisition system was configured and how the communication between the EMG device and the computer was handled. Lastly, it is concluded that the device has an immense potential for EMG signal analysis and is cheap, easy to use and manufacture, however, it can be improved through the use of high precision chips like the ADC ADS1298, what gives the device greater acquisition capacities.",https://ieeexplore.ieee.org/document/9290291/,2020 IX International Congress of Mechatronics Engineering and Automation (CIIMA),4-6 Nov. 2020,ieeexplore
10.1109/ICAIE50891.2020.00111,Supplemental Cultivation Plan of Innovation Quality for the Undergraduates of Building Environment and Energy Engineering in the Applied Technology Universities,IEEE,Conferences,"One of the most fundamental qualities for science research and technology development is the innovation quality, which covers many aspects such as consciousness, method and spirit. The innovation quality developed in the undergraduate period would be the most basic foundation in their over forty-years working life. A supplemental cultivation plan with a routine consisting of hardware, programming, simulation and optimization was presented in this paper for the undergraduates of Building Environment and Energy Engineering (BEEE). The hardware manufacture of digital devices was trained to promote the freshmen manipulative capability, while software programming with general-purpose languages was introduced to enhance the comprehension of data analysis in the 2nd academic year. During the period when some professional courses were taught, the building simulation modular could be introduced to enhance the understanding for the building thermal process and its corresponding Heating, Ventilating, Air-conditioning and Refrigeration (HVAC &amp; R) system. A building management system (BMS) would be developed step by step and interactive with virtual building platform. That BMS and its virtual cases could be employed as the testbed for optimization of HVAC &amp; R system, such as energy consumption prediction, fault detection and diagnosis. Python and EnergyPlus were used in the custom training program oriented to the employment direction for the system operation and maintenance. Group achievements and individual cases show that the presented supplemental cultivation plan played a very important role in the cultivation of BEEE graduates' innovative quality. The effect for the graduated students need pay more attention to investigate the long-term effects, and more volunteers will participate in this plan to find out their better future.",https://ieeexplore.ieee.org/document/9262525/,2020 International Conference on Artificial Intelligence and Education (ICAIE),26-28 June 2020,ieeexplore
10.1109/IMTC.2001.928200,The development of an artificial neural network embedded automated inspection quality management system,IEEE,Conferences,This paper describes in detail the development of an innovative artificial neural network embedded automated inspection scheme for the manufacturing industry employing digital image processing techniques. Such a system is capable of performing real-time image processing tasks and identifies the size and location of the finished components on manufactured products as well as the flaws and scratches on surface of products during the manufacturing process. The proposed artificial neural network embedded quality management system provides a user-friendly user interface that has been implemented and tested on a case study from a printed circuit board manufacture. The experimental results have demonstrated the functionality and superiority of the developed artificial neural network embedded inspection system.,https://ieeexplore.ieee.org/document/928200/,IMTC 2001. Proceedings of the 18th IEEE Instrumentation and Measurement Technology Conference. Rediscovering Measurement in the Age of Informatics (Cat. No.01CH 37188),21-23 May 2001,ieeexplore
10.1109/PICMET.2015.7273077,The effects of disruption on different types of tile manufacturing Industry-layouts: An empirical investigation on tile manufacturing industry,IEEE,Conferences,"Almost all manufacturing facilities need to use production scheduling systems to increase productivity and reduce production costs. Most manufacturing industries invest huge amount of money to manufacture and supply products on time in order to meet customers demand and objectives but due to unforeseen disruptions, these objectives are difficult to achieve. In Real-life, production operations are subject to a large number of unexpected disruptions that may invalidate an original schedule. This work considered effects of disruption on different types of industry-layouts that are seldom been used in manufacturing industries namely: fixed, product, production and process industry-layouts. Questionnaires were used for data collection from a number of companies in Cameroon and reliably theory, simulation software's were using to analyze the data. The study reveals that, disruptions varies per industry-layout and the leading sources of disruptions are machine breakdown, power failure, employee's absenteeism and material shortage. It is concluded that disruption of one type may not greatly affect productivity of a certain industry-layout whilst similar disruptions can have devastating effects on another type and also, the impacts of disruption are dependent on the Industry-layouts.",https://ieeexplore.ieee.org/document/7273077/,2015 Portland International Conference on Management of Engineering and Technology (PICMET),2-6 Aug. 2015,ieeexplore
10.1109/VLSID.2018.20,Tutorial T2A: Safe Autonomous Systems: Real-Time Error Detection and Correction in Safety-Critical Signal Processing and Control Algorithms,IEEE,Conferences,"While the last two decades have seen revolutions in computing and communications systems, the next few decades will see a revolution in the use of every-day robotics and artificial intelligence in broad societal applications. Examples of such systems include sensor networks, the smart power grid, self-driven cars and autonomous drones. Such systems are driven by signal processing, control and learning algorithms that process sensor data, actuate control functions and learn about the environment in which these systems operate. The trustworthiness and safety of such systems is of paramount importance and has significant impact on the commercial viability of the underlying technology. As a consequence, anomalies in system operation due to computation errors in on-board processors, degradation and failure of embedded sensors, actuators and electro-mechanical subsystems and unforeseen changes in their operation environment need to detected with minimum latency. Such anomalies also need to be mitigated in ways that ensure the safety of such systems under all possible failure scenarios. Many future systems will be selflearning in the field. It is necessary to ensure that such learning does not compromise the safety of all human personnel involved in the operation of such systems. To enable safe operation of such systems, the underlying hardware needs to be tuned in the field to maximize performance, reliability and error-resilience while minimizing power consumption. To enable such dynamic adaptation, device operating conditions and the onset of soft errors are sensed using post-manufacture and real-time checking mechanisms. These mechanisms rely on the use of built-in sensors and/or low-overhead function encoding techniques to detect anomalies in system functions. A key capability is that of being able to deduce multiple performance parameters of the system-under-test using compact optimized stimulus using learning algorithms. The sensors and function encodings assess the loss in performance of the relevant systems due to workload uncertainties, manufacturing process imperfections, soft errors and hardware malfunction and failures induced by electromechanical degradation. These are then mitigated through the use of algorithm-through-circuit level compensation techniques based on pre-deployment simulation and post-deployment self-learning. These techniques continuously trade off performance vs. power of the individual software and hardware modules in such a way as to deliver the end-to-end desired application level Quality of Service (QoS), while minimizing energy/power consumption and maximizing reliability and safety. Applications to signal processing, and control algorithms for example autonomous systems will be discussed.",https://ieeexplore.ieee.org/document/8326883/,2018 31st International Conference on VLSI Design and 2018 17th International Conference on Embedded Systems (VLSID),6-10 Jan. 2018,ieeexplore
10.1109/TADVP.2004.828824,Intelligent SOP manufacturing,IEEE,Journals,"Microsystems packaging is fundamentally dependent on the manufacture of microelectronic, photonic, radio frequency (RF), and MEMS devices. The system-on-package (SOP) approach has been identified as a key strategy for integrating these strategic packaging technologies. Because of rising costs, the challenge before SOP manufacturers is to offset capital investment with greater automation and technological innovation in the fabrication process. To reduce manufacturing cost, several important subtasks have emerged, including increasing fabrication yield, reducing product cycle time, maintaining consistent levels of product quality and performance, and improving the reliability of processing equipment. Because of the large number of steps involved, maintaining product quality in an SOP manufacturing facility requires the control of hundreds of process variables. The interdependent issues of high yield, high quality, and low cycle time are addressed by the ongoing development of several critical capabilities in state-of-the-art computer-integrated manufacturing (CIM) systems: in situ process monitoring, process/equipment modeling, real-time process control, and equipment diagnosis. Recently, the use of computational intelligence in various manufacturing applications has increased, and the SOP manufacturing arena is no exception to this trend. Artificial neural networks, genetic algorithms (GAs), and other techniques have emerged as powerful tools for assisting CIM systems in performing various process monitoring, modeling, and control functions. This paper reviews current research in these areas, as well as the potential for deployment of these capabilities in state-of-the-art SOP manufacturing facilities.",https://ieeexplore.ieee.org/document/1331523/,IEEE Transactions on Advanced Packaging,May 2004,ieeexplore
10.1109/TASE.2017.2783342,MASD: A Multimodal Assembly Skill Decoding System for Robot Programming by Demonstration,IEEE,Journals,"Programming by demonstration (PBD) transforms the robot programming from the code level to automated interface between robot and human, promoting the flexibility of robotized automation. In this paper, we focus on programming the industrial robot for assembly tasks by parsing the human demonstration into a series of assembly skills and compiling the skill to the robot executables. To achieve this goal, an identification system using multimodal information to recognize the assembly skill, called MASD, is proposed including: 1) an initial learning stage using a hierarchical model to recognize the action by considering the features from action-object effect, gesture, and trajectory and 2) a retrospective thinking stage using a segmentation method to cut the continuous demonstrations into multiple assembly skills optimally. Using MASD, the demonstration of assembly tasks can be explained with high accuracy in real time, driving a hypothesis that a PBD system on the top of MASD can be extended to more realistic assembly tasks beyond pure positional moving and picking. In experiments, the skill identification module is used to recognize the five kinds of assembly skills in demonstrations of both single and multiple assembly skills, and outperforms the comparative action identification methods. Besides integrated with the MASD, the PBD system can generate the program based on the demonstration and successfully enable an ABB industrial robotic arm simulator to assemble a flashlight and a switch, verifying the initial hypothesis. Note to Practitioners-In the conventional robotized automation, the key role of the robot mainly owes to its capacity for repeating a wide variety of tasks with high speed and accuracy in long term, with a cost of days to months of programming for deployment. On the other hand, the new trend of customization brings the new characteristics: production in short cycle and small volume. This irreversible momentum urges the robot to switch from task to task efficiently. The biggest bottleneck here is the tedious programming, which also has high prerequisites for most practitioners in manufacturing. This situation motivates the development of a PBD system that can understand the assembly skills performed by the human experts in the demonstration and accordingly generate the program for robot's execution of the taught task. In this paper, we present a skill decoding system to parse the observational raw demonstration into symbolic sequences, which is the crucial bridge to enable the automatic programming. The system achieves high performance in recognition and is tailored for the PBD in assembly tasks by considering both advantages and disadvantages in the background of assembly, such as controllable environment and limited computational resources. It is particularly useful for assembly tasks with modularized actions based on a set of standard parts. At the perspective of industrial application, the PBD upon the proposed system is a promising solution to improve the flexibility of manufacture, which is expected to be true in midterm but an important step toward this goal.",https://ieeexplore.ieee.org/document/8263146/,IEEE Transactions on Automation Science and Engineering,Oct. 2018,ieeexplore
10.1109/WSC48552.2020.9383897,A Case Study of Digital Twin for a Manufacturing Process Involving Human Interactions,IEEE,Conferences,"Current algorithms, computations, and solutions that predict how humans will engage in smart manufacturing are insufficient for real-time activities. In this paper, a digital-twin implementation of a manual, manufacturing process is presented. This work (1) combines simulation with data from the physical world and (2) uses reinforcement learning to improve decision making on the shop floor. An adaptive simulation-based, digital twin is developed for a real manufacturing case. The digital twin demonstrates the improvement in predicting overall production output and solutions to existing problems.",https://ieeexplore.ieee.org/document/9383897/,2020 Winter Simulation Conference (WSC),14-18 Dec. 2020,ieeexplore
10.1109/ICMCCE.2018.00050,A Smart Manufacturing Compliance Architecture of Electronic Batch Recording System (eBRS) for Life Sciences Industry,IEEE,Conferences,"The paradigm shift brought about by smart manufacturing or Industrie 4.0 has posed threefold challenges to electronic batch recording system (eBRS) in Life Sciences Industry: 1) the structure of the data should be informative and standard for interoperate using information models, 2) administration of synchronization between physical world and cyber world for smart decision making and optimization using cyber physical system (CPS) and 3) so-called digital manufacturing operations management (digital MOM) characterized by decentralization, comprehensive collaboration and servitization shall be implemented. Under the new situations of smart manufacturing or Industrie 4.0, the requirements from information models, CPS and digital MOM will become the most principal criteria to be considered for future eBRS/MES and other operations management information system in shop floor. To fulfill these demands, an approach combining ISA95/88 hybrid model with activities ontology and variant domain-driven design for SOA-based eBRS development has been presented. An eBRS software platform has been developed on the theoretical basis and applied to a specific application scenario of Lyophilized Injection Production for verifying its feasibility purpose.",https://ieeexplore.ieee.org/document/8537548/,"2018 3rd International Conference on Mechanical, Control and Computer Engineering (ICMCCE)",14-16 Sept. 2018,ieeexplore
10.1049/cp:19940661,A cognitive engineering approach with AI techniques to reactive scheduling in the supervision of dynamic manufacturing processes,IET,Conferences,"Recent advances in artificial intelligence (AI) techniques have greatly increased the potential for applying them to engineering intelligent real-time control and scheduling systems as joint man-machine cognitive systems. Rapid reactivity is becoming an increasingly important competitive factor for today's manufacturing companies in planning and controlling their shop floor processes. The construction and revision of shop floor schedules in such a way to keep them in line with rapidly changing events can significantly improve the manufacturing systems' performance in dynamic environments with many uncertainties in processes. Reactive scheduling as one major function of an intelligent supervisory control system for dynamic manufacturing processes is presented in the paper from functional-decisional, AI-based cognitive engineering and related performance aspects.&lt;<ETX>&gt;</ETX>",https://ieeexplore.ieee.org/document/332000/,"Second International Conference on Intelligent Systems Engineering, 1994",5-9 Sept. 1994,ieeexplore
10.1109/BigData.2017.8258116,A data-driven approach for improving sustainability assessment in advanced manufacturing,IEEE,Conferences,"Sustainability assessment (SA) has been one of the prime contributors to advanced manufacturing analysis, and it traditionally involves life cycle assessment (LCA) techniques for retrospective and prospective evaluations. One big challenge to reach a reliable sustainability assessment comes from the inadequate understandings of the underlying activities related to each of the product lifecycle stages based on expert knowledge. Data-driven modeling, on the other hand, is an emerging approach that takes advantage of machine-learning methods in building models that would complement or replace the knowledge-based models capturing physical behaviors. Incorporating suitable data analytics models to utilize real-time product and process data could significantly improve LCA techniques. To address the complexity and uncertainty involved in multilevel SA decision-making activities, this paper proposes a modular LCA framework to accommodate a hybrid modeling paradigm that includes knowledge-based and data-driven models. We identify and emphasize on two important challenges: (1) Generalizing knowledge-based and data-driven models into analytics models so that they can be uniformly deployed and interchanged, and (2) Modularizing the LCA decision logics and model structures so that the LCA decision process can be streamlined and easily maintained. The issues related to the decomposition, standardization, deployment and execution of analytics models are discussed in this paper. Three well-adopted standards - STEP (Standard for the Exchange of Product model data), DMN (Decision Model and Notation), and PMML (Predictive Model Markup Language) are employed to capture the product-related data/information, the decision logic decomposition of analytics models, and the structure decomposition of analytics models, respectively. The feasibility and benefits of the proposed modular, hybrid sustainability assessment methodology have been illustrated with an injection molding case study, incorporating an overall modular Scorecard-based LCA architecture with a Bayesian Network predictive model.",https://ieeexplore.ieee.org/document/8258116/,2017 IEEE International Conference on Big Data (Big Data),11-14 Dec. 2017,ieeexplore
10.1109/ROBOT.1998.677426,A distributed planning network for manufacturing systems management,IEEE,Conferences,"The paper describes a planning network (PN) which is to assist the better harmonization and higher quality and production volume of small and medium-size enterprises (SME), if they are joining their efforts by being active members of the PN. To be an active member (node) of the PN means to accept the production and quality rules of the PN and to offer a given amount of the SME's planning and production capacity to the PN. The PN means a real-time online network with three basic functions. The local planning module (LPM), the coordination unit (CU) and the evaluation module (EM) are the three software units that covers the functions. This PN is a kind of virtual, or extended enterprise, however it is different because of the long-term cooperation of the partners, instead of working together only for 1-2 given orders. The intelligence of the PN is supported by expert systems in the CU and EM.",https://ieeexplore.ieee.org/document/677426/,Proceedings. 1998 IEEE International Conference on Robotics and Automation (Cat. No.98CH36146),20-20 May 1998,ieeexplore
10.1109/ROBOT.1988.12239,A knowledge-based system linking simulation to real-time control for manufacturing cells,IEEE,Conferences,A description is given of the authors' early work (1987) on the implementation of an experimental knowledge-based discrete-event simulation system. The system is being developed as an extension of the MUSE artificial intelligence (AI) toolkit to allow the reuse of modules of the simulation code to carry out real-time control. The operation of the system is illustrated by applying it to the problem of the control and scheduling of modular flexible machining cells.&lt;<ETX>&gt;</ETX>,https://ieeexplore.ieee.org/document/12239/,Proceedings. 1988 IEEE International Conference on Robotics and Automation,24-29 April 1988,ieeexplore
10.1109/ROBOT.1999.772469,A multi-contract net protocol for dynamic scheduling in flexible manufacturing systems (FMS),IEEE,Conferences,"Deals with a multi-agent architecture and a negotiation protocol for the dynamic scheduling of flexible manufacturing systems. The originality of the multi-agent architecture resides in the existence of task agents and resource agents. The multi-contract net protocol proposed, is an innovation with regard to the contract-net protocol due to the way that it makes it possible to negotiate several tasks concurrently, in real time and with more optimal results taking into account uncertainty and conflict situations in the scheduling of operations. The paper, also, stresses the efficiency and the optimality of the distributed implementation.",https://ieeexplore.ieee.org/document/772469/,Proceedings 1999 IEEE International Conference on Robotics and Automation (Cat. No.99CH36288C),10-15 May 1999,ieeexplore
10.1109/ROBOT.2004.1308024,A real-time monitoring and diagnosis system for manufacturing automation,IEEE,Conferences,"Condition monitoring and fault diagnosis in modern engineering practices is of great practical significance for improving the quality and productivity, preventing the machinery from damages. In general, this practice consists of two parts: extracting appropriate features from sensor signals and recognizing possible faulty patterns from the features. In order to cope with the complex manufacturing operations and develop a feasible system for real-time application, we proposed three approaches. By defining the marginal energy, a new feature representation emerged, while by real-time learning algorithms with support vector techniques and hidden Markov model representations, a modular software architecture and a new similarity measure were developed for comparison, monitoring, and diagnosis. A novel intelligent computer-based system has been developed and evaluated in over 30 factories and numerous metal stamping processes as an example of manufacturing operations. The real-time operation of this system demonstrated that the proposed system is able to detect abnormal conditions efficiently and effectively resulting in a low-cost, effective approach to real-time monitoring in manufacturing. The related technologies have been transferred to industry, presenting a tremendous impact in current automation practice in Asia and the world.",https://ieeexplore.ieee.org/document/1308024/,"IEEE International Conference on Robotics and Automation, 2004. Proceedings. ICRA '04. 2004",26 April-1 May 2004,ieeexplore
10.1109/ITAIC.2011.6030279,A solution of dynamic manufacturing resource aggregation in CPS,IEEE,Conferences,"Market diversification and economic globalization bring various uncertainties to the completion of manufacturing task. It requires manufacturing resource aggregation (MRA) could adjust to dynamic changing factors (DCFs) that happen during the life cycle of MRA. The emerging Cyber Physical Systems (CPS), in which the real-time states of physical resources are fully sensed, enable to identify the DCFs in time. This supports to realize the real dynamic MRA. Take CPS as background, the paper focuses on dynamic MRA which includes two key issues, manufacturing resource virtualization model (MRVM) and dynamic service composition. In order to give a complete manufacturing resource view, MRVM which contains both static and dynamic information is built. Based on the analysis towards life cycle of MRA, the paper proposes a dynamic MRA architecture. And as the core parts of it, the implementation process of manufacturing resource service composition and a multi-level dynamic adjustment strategy are described in detail. At last, we make a conclusion.",https://ieeexplore.ieee.org/document/6030279/,2011 6th IEEE Joint International Information Technology and Artificial Intelligence Conference,20-22 Aug. 2011,ieeexplore
10.1109/ICPHYS.2018.8390759,Accented visualization by augmented reality for smart manufacturing aplications,IEEE,Conferences,"Effective application of Augmented Reality user interfaces is one of the challenging trends of industrial cyber-physical systems development and implementation. Despite reach functionality of modern AR devices (like goggles, head mounted displays or tablets) the problem of their comfortable and productive use is not completely solved yet. To cover this gap, it is proposed in this paper to implement a new paradigm of “accented visualization” that allows adapting additional data presented by AR device according to the user's current interest, attention and focus. To provide such context driven functionality there was developed intelligent software based on eye tracking and capturing the user's focus in ontology as a knowledge base. Probation and testing of the proposed approach present 89 % of the solution efficiency.",https://ieeexplore.ieee.org/document/8390759/,2018 IEEE Industrial Cyber-Physical Systems (ICPS),15-18 May 2018,ieeexplore
10.1109/BigData.2014.7004408,Advanced planning and control of manufacturing processes in steel industry through big data analytics: Case study and architecture proposal,IEEE,Conferences,"Enterprises in today's globalized world are compelled to react on threats and opportunities in a highly flexible manner. Hence, companies that are able to analyze the current state of their business processes, forecast their most optimal progresses and with this proactively control them will have a decisive competitive advantage. Technological progress in sensor technology has boosted real-time situation awareness, especially in manufacturing operations. The paper at hands examines, based on a case study stemming from the steel manufacturing industry, which production-related data is collectable using state of the art sensors forming a basis for a detailed situation awareness and for deriving accurate forecasts. However, analyses of this data point out that dedicated big data analytics approaches are required to utilize the full potential out of it. By proposing an architecture for predictive process planning and control systems, the paper intends to form a working and discussion basis for further research and implementation efforts in big data analytics.",https://ieeexplore.ieee.org/document/7004408/,2014 IEEE International Conference on Big Data (Big Data),27-30 Oct. 2014,ieeexplore
10.1109/CSCWD.2012.6221798,An agent-based dynamic scheduling approach for flexible manufacturing systems,IEEE,Conferences,"The paper presents a dynamic scheduling approach for flexible manufacturing systems (FMS). The scheduling approach is implemented based on the negotiation and collaboration between agents in a multi-agent system (MAS) which represents the FMS. Through the collaboration between the agents in the MAS, the system exhibits the behavior that response to the disruption caused by dynamic events arise randomly in FMS such as jobs arrive over time and machines breakdown in real time and globally. The scheduling and controlling process is done on-line, without interrupting the system's operation and without user intervention. An experiment is conducted to evaluate the efficiency of the scheduling strategies exhibited by the proposed agent-based scheduling approach. The results demonstrate the superiority of the suggested scheduling approach as well as its capacity to cope with a fast changing environment.",https://ieeexplore.ieee.org/document/6221798/,Proceedings of the 2012 IEEE 16th International Conference on Computer Supported Cooperative Work in Design (CSCWD),23-25 May 2012,ieeexplore
10.1109/ICPHYS.2018.8390779,An approach for implementing key performance indicators of a discrete manufacturing simulator based on the ISO 22400 standard,IEEE,Conferences,"Performance measurement tools and techniques have become very significant in today's industries for increasing the efficiency of their processes in order to face the competitive market. The first step towards performance measurement is the real-time monitoring and gathering of the data from the manufacturing system. Applying these performance measurement techniques on real-world industry in a way that is more general and efficient is the next challenge. This paper presents a methodology for implementing the key performance indicators defined in the ISO 22400 standard-Automation systems and integration, Key performance indicators (KPIs) for manufacturing operations management. The proposed methodology is implemented on a multi robot line simulator for measuring its performance at runtime. The approach implements a knowledge-based system within an ontology model which describes the environment, the system and the KPIs. In fact, the KPIs semantic descriptions are based on the data models presented in the Key Performance Indicators Markup Language (KPIML), which is an XML implementation of models developed by the Manufacturing Enterprise Solutions Association (MESA) international organization.",https://ieeexplore.ieee.org/document/8390779/,2018 IEEE Industrial Cyber-Physical Systems (ICPS),15-18 May 2018,ieeexplore
10.1109/IS.2012.6335231,An intelligent fuzzy-Petri reasoning supervisory control for FMS manufacturing plants,IEEE,Conferences,"The organization level of complex automated robotic and manufacturing systems, whose dynamics is discrete-event dominated, is revisited and some innovative results towards enhancing machine intelligence derived. These comprise a fuzzy-Petri-net reasoning emulator implementing the fuzzy-Petri rule-based decision-making with an appropriate knowledge base support. This fuzzy-Petri reformulation follows the original concept of entropy-based, intelligent machines of G.N. Saridis. Its generic feature in the sense of an improved fuzzy-Petri reasoning based organizing coordination controller is crucial. A software tool emulating the fuzzy-Petri-net reasoning is discussed. Its version employing knowledge-base with 10 rules takes only a few milliseconds on a standard PC to be simulated. Thus it is believed a feasible yet efficient real-time reasoning for supervisory control functions has been developed. Some computer simulations results for FMS applications are given that illustrate these novel developments.",https://ieeexplore.ieee.org/document/6335231/,2012 6th IEEE International Conference Intelligent Systems,6-8 Sept. 2012,ieeexplore
10.1109/INDIN.2005.1560474,An intelligent immune agent management system for facilitating e-diagnosis in manufacturing,IEEE,Conferences,"This paper focuses on designing and implementing an agent framework for the purposes of electronic prognostics of critical tools and machinery in the plant. It deals with dual communication i.e. one originating from the sensors and relayed to various reception centers and the other originating from an intelligent prognostics engine (IPE) being fed back to the control mechanism (manual/automated) of the machinery. The paper presents the feasibility of deploying agents in embedded and supervisory controllers for collating, managing and communicating information. The conceptual design and prototype model establishes the efficacy of the agent framework in diagnostics as well as prognostics. The framework is developed using Java agent development framework (JADE). This paper concentrates on development of peer agent communication, communication ontologies; agent behaviors based on messaging performatives, integration issues and graphical user interfaces for diverse agents and human interfaces. It also evolves a conceptual agent framework for manufacturing environment. The work can potentially improve reliability: reduce equipment downtime by using intelligence and contextual information from sensors and events. It facilitates the timely availability of the critical data for real-time monitoring and corrective signals, as well as a transparent and rapid configuration mechanism for intelligent monitoring.",https://ieeexplore.ieee.org/document/1560474/,"INDIN '05. 2005 3rd IEEE International Conference on Industrial Informatics, 2005.",10-12 Aug. 2005,ieeexplore
10.1109/CASE48305.2020.9216855,Anomaly detection and prediction in discrete manufacturing based on cooperative LSTM networks,IEEE,Conferences,"Manufacturing processes are characterized by their temporal and spatial distributed nonlinear physics. Analytical models are not available and numerical models do not incorporate abnormal process effects that are not known to the engineer. These unknown anomalies cause reduced process stability and fluctuant product quality. To tackle the problem, numerous approaches for anomaly detection based on neural networks have been developed over the years. Long short-term memory (LSTM) networks have also been investigated intensively for prediction purposes. Current approaches lack in the capability of constructing prediction models for both process and anomaly behavior. Furthermore, they do not deliver a solution for short-term as well as long-term anomalies. Hence, the current paper presents a novel detection and prediction procedure based on a LSTM architecture to cooperatively predict process outputs and anomalies by using two separate but interacting models. The anomaly detector is realized as stacked LSTM auto-encoder and the cooperative prediction models are based on sequence-to-sequence networks with gated recurrent units for short-term and LSTM for long-term effects. The approach is evaluated within a real industrial environment by means of a production plant for hot forging at a German automotive supplier for metal components.",https://ieeexplore.ieee.org/document/9216855/,2020 IEEE 16th International Conference on Automation Science and Engineering (CASE),20-21 Aug. 2020,ieeexplore
10.1109/TENCON.2004.1414983,Certain studies on sample time for a predictive fuzzy logic controller through real time implementation of phenol-formaldehyde manufacturing,IEEE,Conferences,"In polymer manufacturing industries, the automation and control of chemical process incorporating techniques of fuzzy control neural networks, and expert systems had lead to a more secured and stable operation. A sudden and unpredictable heat is often produced by the nonlinear exothermal reaction when phenol and formaldehyde are mixed together. Therefore, the polymerization process has to be controlled with a high level of precision in order to avoid temperature run-away. This paper proposes a design methodology for a sensor based process control system. The duration of ON and OFF time of certain relays are the parameters to be controlled in order to keep the exothermic reaction under control The universe of discourse for the output of the FLC system is the sample time that assigned to the relays where maximum time for heater or valve can be turned on before the next action is applied This paper discusses a detailed real time implementation of the exothermal process control using Matlab-fuzzy logic toolbox. An enhanced predictive FLC structure is developed and compared to a predictive FLC control structure. The obtained practical results thus ensure that the predictive FLC can be enhanced by modifying the rules and the membership Junctions of the universe of discourse, which is proved to be better in controlling the reaction temperature.",https://ieeexplore.ieee.org/document/1414983/,2004 IEEE Region 10 Conference TENCON 2004.,24-24 Nov. 2004,ieeexplore
10.1109/DESSERT50317.2020.9125038,Combination of Digital Twin and Artificial Intelligence in Manufacturing Using Industrial IoT,IEEE,Conferences,"The paper focuses on Digital Twin (DT) in Manufacturing using Artificial Intelligence (AI) and Industrial IoT. According to the concept, the manufacturing includes three main units: equipment, personnel and processes. All data from these units are inherited to manufacture model (DT) and decision support system with the use of AI. DT data technology allows finding the required knowledge that can be interpreted and used to support the process of decision-making in the management of the enterprise. AI applications open up a broad spectrum of opportunities in manufacturing to add value by optimizing processes and generating new business models. The Landscape was described by a formal model to assure the possibility to analyze the state and development of landscape in detail considering DT and other technologies. DT and IIoT implementation for the simulation of real enterprise manufacturing were considered.",https://ieeexplore.ieee.org/document/9125038/,"2020 IEEE 11th International Conference on Dependable Systems, Services and Technologies (DESSERT)",14-18 May 2020,ieeexplore
10.1109/INDIN41052.2019.8972310,Data-driven modeling of semi-batch manufacturing: a rubber compounding test case,IEEE,Conferences,"The continuously growing amount of available data from manufacturing processes supports the development of data-driven models. The typical target application of these models is optimal control and continuous quality management within an objective of zero-defect manufacturing. However, data obtained from batch processes are characterized by its high dimensionality that exceeds the computational capabilities of online applications and data-driven model's reliability must be guaranteed for proper industrial implementation. We explore two approaches to reduce problem's size: feature extraction and feature selection; several multivariate regression methods are also compared regarding it precision and robustness. We base our analysis on an industrial rubber compounding process where natural rubber is blended in a semi-batch mixer with several additives, then it is further mixed up using cylinders and it is conditioned in bands for storing. For this process, real production data is collected and stored in the manufacturing execution system of the company. The objective of the analysis is to predict mechanical properties of the rubber at the end of the processes. Based on the provided data, several data-driven models are built and tested. From the comparison among them it is concluded: models based on feature extraction and artificial neural networks yield the highest accuracy, while feature-selected models provide better physical interpretability and increased robustness regarding industrial deployment.",https://ieeexplore.ieee.org/document/8972310/,2019 IEEE 17th International Conference on Industrial Informatics (INDIN),22-25 July 2019,ieeexplore
10.1109/CACRE52464.2021.9501291,Give Me a Wrench!: Finding Tools for Human Partners in Human-Robot Collaborative Manufacturing Contexts,IEEE,Conferences,"Manufacturing processes can be optimized by enabling human-robot collaboration. A relevant goal in this area is to create a collaborative solution in which robots can provide assisting actions to humans, thereby, reducing menial labor as well as increasing productivity. The solution is based on implementing efficient hand-over of mechanical tools from robots to humans. Hand-over tasks are inevitable in human-robot collaborative manufacturing contexts. These tasks need three-step mechanism: object identification, object grasping, and the actual hand-over. This paper presents an approach for robots to find tools for human partners in human-robot collaboration via deep learning. This is achieved using the object detection system YOLOv3 for identification of commonly used mechanical tools. By training on a custom dataset of 800 images of mechanical tools created for the study, the tool recognition is implemented in realworld human-robot hand-over tasks. Experimental results show that the proposed approach achieves a high accuracy for identification of tools in real-world human-robot collaboration. Future work of this study is also discussed.",https://ieeexplore.ieee.org/document/9501291/,"2021 6th International Conference on Automation, Control and Robotics Engineering (CACRE)",15-17 July 2021,ieeexplore
10.1109/IOLTS50870.2020.9159704,High-level Modeling of Manufacturing Faults in Deep Neural Network Accelerators,IEEE,Conferences,"The advent of data-driven real-time applications requires the implementation of Deep Neural Networks (DNNs) on Machine Learning accelerators. Google's Tensor Processing Unit (TPU) is one such neural network accelerator that uses systolic array-based matrix multiplication hardware for computation in its crux. Manufacturing faults at any state element of the matrix multiplication unit can cause unexpected errors in these inference networks. In this paper, we propose a formal model of permanent faults and their propagation in a TPU using the Discrete-Time Markov Chain (DTMC) formalism. The proposed model is analyzed using the probabilistic model checking technique to reason about the likelihood of faulty outputs. The obtained quantitative results show that the classification accuracy is sensitive to the type of permanent faults as well as their location, bit position and the number of layers in the neural network. The conclusions from our theoretical model have been validated using experiments on a digit recognition-based DNN.",https://ieeexplore.ieee.org/document/9159704/,2020 IEEE 26th International Symposium on On-Line Testing and Robust System Design (IOLTS),13-15 July 2020,ieeexplore
10.1109/ICSMC.2000.886346,Holonic self-organization of multi-agent systems by fuzzy modeling with application to intelligent manufacturing,IEEE,Conferences,"Holonic manufacturing aims to design standardized, modular manufacturing systems made of interchangeable parts, to enable flexibility, online reconfigurability and self-organizing capabilities for the production systems. Recent advances in distributed artificial intelligence and networking technologies have proven that theoretical multi-agent systems (MAS) concepts are very suitable for the real life implementation of holonic concepts. Building on our recent results in the design and implementation of holonic reconfigurable architectures, the paper introduces a novel approach to the online self-organization of distributed systems. By using fuzzy set and uncertainty theoretical concepts, we construct a mathematical foundation for modeling MAS, where appropriate holonic structures are identified for each particular application. This approach opens new possibilities for the design of any distributed system that needs self-organization as an intrinsic property.",https://ieeexplore.ieee.org/document/886346/,"Smc 2000 conference proceedings. 2000 ieee international conference on systems, man and cybernetics. 'cybernetics evolving to systems, humans, organizations, and their complex interactions' (cat. no.0",8-11 Oct. 2000,ieeexplore
10.1109/FUZZY.1997.619468,Hybrid manufacturing line supervision and diagnosis by means of fuzzy rules connected with a causal graph,IEEE,Conferences,"The method proposed here consists in generating reactive knowledge represented by fuzzy rules from a given causal graph model. Causal reasoning forms a practical support for model-based diagnosis. Also the fuzzy logic allows us to generate the corrective actions for a qualitative model-based system (supervision). These two kinds of reasoning have been integrated in a computer system for a real world application. A global model based on a causal graph of the process is used for the diagnostic, but local fuzzy reasoning blocks are used for supervision.",https://ieeexplore.ieee.org/document/619468/,Proceedings of 6th International Fuzzy Systems Conference,5-5 July 1997,ieeexplore
10.23919/ACC45564.2020.9147268,Inferential Methods for Additive Manufacturing Feedback,IEEE,Conferences,"Adaptive manufacturing has revolutionized desktop prototyping and the production of physical models for non-load bearing or stress inducing applications. Many extrusion-based printers are available for purchase by entrepreneurial enthusiasts or businesses with manufacturing space limitations. These low-cost printers allow for quick prototyping but are not designed or intended for high quality production or high-cycle production, requiring extensive user tuning and upkeep to maintain the printer in usable condition. In a quest to apply modern deep learning and reinforcement learning based models, this work focuses on the development of control systems and infrastructure needed to resolve many of these intrinsic limitations of desktop 3D printers. A series of real-time agents were designed and deployed to actively monitor the printing of every layer and make continuous corrections in the printing parameters and G-code commands to reduce the variance in the tensile strength of homogeneous parts printed in a large batch.",https://ieeexplore.ieee.org/document/9147268/,2020 American Control Conference (ACC),1-3 July 2020,ieeexplore
10.1109/CACS50047.2020.9289771,Intelligent Manufacturing Monitoring and Surface Roughness Prediction System – A Case Study of Aluminum Parts Milling,IEEE,Conferences,"The aim of this study is to create an economical automatic machining system to predict surface roughness during processing, which is an important quality criterion. Complex network accelerators and software acceleration are used to achieve real-time calculations. When the expected results are not obtained, the turning tool is changed or processing is halted. The system can maximize the processing efficiency. In this study, a deep neural network is used to predict the roughness of the plane, and sensors are installed at different positions to study the effects of different positions and numbers on accuracy. The accuracy obtained is 92.3%.",https://ieeexplore.ieee.org/document/9289771/,2020 International Automatic Control Conference (CACS),4-7 Nov. 2020,ieeexplore
10.1109/ICAT.2013.6684074,Intelligent system for inspection and selection of parts in a manufacturing cell,IEEE,Conferences,"This paper addresses the design and implementation of an artificial vision system implemented in a manufacturing cell. The vision system recognizes and selects in an intelligent manner the manufactured parts through a feedforward artificial neural network and the decisions are completely based on the part's color and its geometry. A simple digital camera is used as an image acquisition device. This image is then processed by an artificial neural network, which is able to identify the part's color. Then a Programmable Logic Controller (PLC) drives an electropneumatic system, in order to store the identified part into a corresponding repository. An interface based on power electronic devices and a Data Acquisition Card (DAQ) system implements the communication between the PLC and the computer. The proposed system is completely implemented and tested in a real Flexible Manufacturing System (FMS) of FESTO© showing good results.",https://ieeexplore.ieee.org/document/6684074/,"2013 XXIV International Conference on Information, Communication and Automation Technologies (ICAT)",30 Oct.-1 Nov. 2013,ieeexplore
10.1109/COASE.2016.7743572,Learning-based dynamic scheduling of semiconductor manufacturing system,IEEE,Conferences,"A learning-based scheduling framework for semiconductor manufacturing system is studied in this paper. This framework obtains a dynamic scheduling model by applying machine learning algorithm based on optimal data samples, through which an approximate optimal scheduling strategy under a certain production state can be acquired on time. And then an implementation of a dynamic scheduling model based on extreme learning machine (ELM) is proposed. In order to improve efficiency, a hybrid feature selection and classification algorithm is suggested, which combines filter feature selection method and wrapper feature selection method. Finally, the proposed dynamic scheduling model is tested in a real semiconductor manufacturing system to compare and analysis between the algorithm performance and production performance. The result indicates that the learning-based scheduling method is superior to single scheduling rules and it also meets the requirements of real-time manufacturing scheduling.",https://ieeexplore.ieee.org/document/7743572/,2016 IEEE International Conference on Automation Science and Engineering (CASE),21-25 Aug. 2016,ieeexplore
10.1109/SNPD.2012.137,MAST: From a Toy to Real-Life Manufacturing Control,IEEE,Conferences,"Paper reports on the evolution of agent-based simulation and control system called MAST. The system was designed originally for agent-based simulation of product routing but over the years matured into a generic purpose manufacturing simulation and control tool featuring real-time connectivity to legacy PLCs, ontology-based dynamic scheduling, advanced diagnostics, etc. Paper describes MAST architecture, behavior of agents and capabilities for dynamic reconfiguration. In addition, two examples of application of MAST to real problems from manufacturing domain are given. The latest trend of exploitation of semantic technologies in industrial agents is discussed.",https://ieeexplore.ieee.org/document/6299316/,"2012 13th ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing",8-10 Aug. 2012,ieeexplore
10.1109/ICECCE49384.2020.9179391,Machine Learning Based Approach to Process Characterization for Smart Devices in 3D Industrial Manufacturing,IEEE,Conferences,"A key differentiator between the additive manufacturing and the traditional injection molding is the precision-manufacturing. An error-free print job significantly guarantees good part quality with minimum wastage of the material and energy. In practice, however, achieving error-free production is quite challenging and this emphasizes the need to learn what behavior of the machine leads to an erroneous job. Knowing the health of the printer or the print job helps quantify print job performance, as well as build system alerts to take reactive actions. Intending to run the model dynamically while the machine is printing, time-series based deep learning models like LSTM are most suitable. This paper presents a machine learning based anomaly detection approach to discover patterns in sensors' measurements in the streaming mode in MultiJet Fusion 3d printer developed at HP Inc. A hybrid architecture of LSTM and Auto-encoder has been proposed to learn the printer behavior generate an alarm in the event of an anomaly. The results of both LSTM and LSTM-Autoencoder models have also been discussed by taking real-life examples of 3D printing jobs.",https://ieeexplore.ieee.org/document/9179391/,"2020 International Conference on Electrical, Communication, and Computer Engineering (ICECCE)",12-13 June 2020,ieeexplore
10.1109/DTPI52967.2021.9540077,Mechanical Design Paradigm based on ACP Method in Parallel Manufacturing,IEEE,Conferences,"Parallel Manufacturing is a new manufacturing paradigm in industry, deeply integrating informalization, automation, and artificial intelligence. In this paper we propose a new mechanical design paradigm in Parallel Manufacturing based on ACP method. The key is to regard the design procedure based on artificial design and emulation method as two independent procedures, which can be modeled as a parallel system. The design procedure based on ACP method does not include a real system, which is an inventive extension of the traditional parallel system. This method can be implemented with social information by introducing the definition of SDV, SDM, and Intelligent Design Manager, making it highly adaptive for social manufacturing and Parallel Manufacturing.",https://ieeexplore.ieee.org/document/9540077/,2021 IEEE 1st International Conference on Digital Twins and Parallel Intelligence (DTPI),15 July-15 Aug. 2021,ieeexplore
10.1109/ISCAS.2019.8702575,Multi-View Fusion Neural Network with Application in the Manufacturing Industry,IEEE,Conferences,"In recent years the research community and industry have paid high attention to the field of machine learning, especially deep learning. Nowadays many real-world classification or rather prediction applications are implemented by neural network models. We propose a multi-view fusion neural network with application in the manufacturing industry. Image information of multiple cameras is fused and used by the proposed model to predict the state of a manufacturing machine. Experiments show that the overall classification performance is increased from a baseline of 92.7% to 99.5% by the fusion model.",https://ieeexplore.ieee.org/document/8702575/,2019 IEEE International Symposium on Circuits and Systems (ISCAS),26-29 May 2019,ieeexplore
10.1109/AQTR.2014.6857897,Multi-agent system for heterarchical product-driven manufacturing,IEEE,Conferences,"Product-driven manufacturing has gained a lot of traction recently among practitioners as it has the potential to take flexibility and agility of the manufacturing system to a new level compared to hierarchical control models. The advances in embedded technology have created the premises for the emergence of truly intelligent products that are capable not only of identification and information storage, but also of complex behavior and local decision making. In this context, this paper proposes a multi-agent control system that aims to solve the new challenges introduced by the shift to product-driven manufacturing, specifically addressing the special needs for information flow between shop floor entities and the MES system. The paper presents the pilot implementation, using the JADE multi-agent platform, a backtracking scheduler, an artificial neural network (ANN) for local decision making and the experimental results outlining the agent processing requirements during the product lifecycle.",https://ieeexplore.ieee.org/document/6857897/,"2014 IEEE International Conference on Automation, Quality and Testing, Robotics",22-24 May 2014,ieeexplore
10.1109/CASE48305.2020.9216979,Online Computation Performance Analysis for Distributed Machine Learning Pipelines in Fog Manufacturing,IEEE,Conferences,"Smart manufacturing enables real-time data streaming from interconnected manufacturing processes to improve manufacturing quality, throughput, flexibility, and cost reduction via computation services. In these computation services, machine learning pipelines integrate various types of computation method options to match the contextualized, on-demand computation needs for the maximum prediction accuracy or the best model structure interpretation. On the other hand, there is a pressing need to integrate Fog computing in manufacturing, which will reduce communication time latency and dependency on connections, improve responsiveness and reliability of the computation services, and maintain data privacy. However, there is a knowledge gap in using machine learning pipelines in Fog manufacturing. Existing offloading strategies are not effective, due to the lack of accurate prediction model for the performance of computation services before the execution of those heterogeneous computation tasks. In this paper, machine learning pipelines are implemented in Fog manufacturing. The computation performance of each sub-step of pipelines is predicted and analyzed via linear regression models and random forest regression models. A Fog manufacturing testbed is adopted to validate the performance of the employed models. The results show that the models can adequately predict the performance of computation services, which can be further integrated into Fog manufacturing to better support offloading strategies for machine learning pipelines.",https://ieeexplore.ieee.org/document/9216979/,2020 IEEE 16th International Conference on Automation Science and Engineering (CASE),20-21 Aug. 2020,ieeexplore
10.1109/CSO.2009.254,Online Scheduling of Flexible Flow Shop Manufacturing,IEEE,Conferences,"The flexible flow shop refers to such a manufacturing environment in which jobs are to be processed through serial stages, with one or multiple machines available at each stage. It is usually a complex task when specific objective is demanded such as minimum cost, minimum time, etc. Static scheduling of such problems has been much researched, however, little efforts have been made on realtime scheduling when the release time of each job is unknown. In this paper, some online scheduling methods are presented to deal with the tough problem of realtime Just-In-Time manufacturing. In addition to applicable dispatching rules, agent-based approaches are also proposed featuring feedback, learning, and realtime prediction. The simulation result reveals that the presented distributed learning approach, especially when combined with realtime prediction, delivers a high performance.",https://ieeexplore.ieee.org/document/5193655/,2009 International Joint Conference on Computational Sciences and Optimization,24-26 April 2009,ieeexplore
10.1109/INM.2015.7140329,Ontology integration for advanced manufacturing collaboration in cloud platforms,IEEE,Conferences,"Advances in the field of cloud computing and networking have led to rapid development and market growth in areas such as online retail, gaming and healthcare. In the field of advanced manufacturing however, the impact has been significantly lesser than expected due to limitations in cloud platforms for fostering community engagement. To address this problem, we study a new cloud-based architecture that provides Platform-asa-Service (PaaS) management capabilities to the manufacturing community for delivering Software-as-a-Service (SaaS) “Apps” to their customers. Our architecture aims at supporting an “App Marketplace” that thrives on agile development, organic collaboration and scalable sales of next generation manufacturing Apps requiring high-performance simulation and modeling. Towards realizing the vision of the above architecture, our paper involves investigation and implementation of an Ontology Service that interoperates with other common web services related to resource brokering and accounting. Our Ontology Service uses principles of mapping and merging to translate a manufacturing App's collaboration requirements to suitable resource specifications on public cloud platforms. Integrated resultant ontology can be queried to provision the required resource parameters such as amount of memory/storage, number of processing units, and network protocol configurations needed for deployment of an App. We validate the effectiveness of our Ontology Service using the Protégé framework in a pilot testbed of a real-world “WheelSim” App in the NSF GENI Cloud platform. Our ontology integration results show benefits to an App developer in terms of: optimal user experience, lower design time and lower cost/simulation.",https://ieeexplore.ieee.org/document/7140329/,2015 IFIP/IEEE International Symposium on Integrated Network Management (IM),11-15 May 2015,ieeexplore
10.1109/GloSIC.2018.8570124,Probabilistic Estimations of Increasing Expected Reliability and Safety for Intelligent Manufacturing,IEEE,Conferences,"In the near future the possibilities of the modern probabilistic models, artificial intelligence and machine learning methods can provide an intelligent support of making decisions by an operator in real time. An agile recovery of intelligent manufacturing integrity can be implemented owing to the development of industrial robotics. For intelligent manufacturing it means the expected reliability and safety may be in the near future at the expense of intelligent support of decision making and the agile recovery of integrity. To answer the question “How much essential may be this increasing?” here are proposed: general analytical approaches for a probabilistic estimation of the expected reliability and safety for every monitored element or the system of intelligent manufacturing on a level of probability distribution functions (PDF) of the time between the losses of system integrity; estimations of increasing the expected reliability and safety for intelligent manufacturing at the expense of the intelligent support of decision making and agile recovery of integrity; the comparisons of the estimations on a prognostic period up to 10 years using the identical model in applications to expected reliability and safety. The applications of the proposed approaches allow the customers, designers, developers, users and experts of Industry 4.0 intelligent manufacturing to be guided by the proposed probabilistic estimations for solving problems of reliability and safety in the system life cycle. The results are demonstrated by examples.",https://ieeexplore.ieee.org/document/8570124/,2018 Global Smart Industry Conference (GloSIC),13-15 Nov. 2018,ieeexplore
10.1109/IEMT.1995.526119,Real-time diagnosis of semiconductor manufacturing equipment using neural networks,IEEE,Conferences,"This paper presents a tool for the real-time diagnosis of integrated circuit fabrication equipment. The approach focuses on integrating neural networks into a knowledge-based expert system. The system employs evidential reasoning to identify malfunctions by combining evidence originating from equipment maintenance history, on-line sensor data, and in-line past-process measurements. Neural networks are used in the maintenance phase of diagnosis to approximate the functional form of the failure history distribution of each component. Predicted failure rates are then converted to belief levels. For on-line diagnosis in the case of previously unencountered faults, a CUSUM control chart is implemented on real sensor data to detect very small process shifts and their trends. For the known fault case, hypothesis resting on the statistical mean and variance of the sensor data is performed to search for similar data patterns and assign belief levels. Finally, neural process models of process figures of merit (such as etch uniformity) derived from prior experimentation are used to analyze the in-line measurements, and identify the most suitable candidate among faulty input parameters (such as gas flow) to explain process shifts. A working prototype for this hybrid diagnostic system is being implemented on the Plasma Therm 700 series reactive ion etcher located in the Georgia Tech Microelectronic Research Center.",https://ieeexplore.ieee.org/document/526119/,Seventeenth IEEE/CPMT International Electronics Manufacturing Technology Symposium. 'Manufacturing Technologies - Present and Future',2-4 Oct. 1995,ieeexplore
10.1109/RTOSS.1994.292553,Real-time platforms and environments for time constrained flexible manufacturing,IEEE,Conferences,"The Spring Kernel and associated algorithms, languages, and tools provide system support for static or dynamic real-time applications that require predictable operation. Spring currently consists of two major parts: (1) the development environment, where application and target systems are described, preprocessed and downloaded, and (2) the run-time environment, where the operating system, the Spring Kernel, creates and ensures predictable executions of application tasks. We have integrated our real-time systems technology with component technologies from robotics, computer vision, and real-time artificial intelligence, to develop a test platform for flexible manufacturing. The results being produced are generic so that they should be in many other real-time applications such as air traffic control and chemical plants. We describe this platform, identify new features developed, and comment on some lessons learned to date from this experiment.&lt;<ETX>&gt;</ETX>",https://ieeexplore.ieee.org/document/292553/,Proceedings of 11th IEEE Workshop on Real-Time Operating Systems and Software,18-19 May 1994,ieeexplore
10.1109/DEXA.2001.953126,Reconfiguring real-time holonic manufacturing systems,IEEE,Conferences,We describe a general approach for dynamic and intelligent reconfiguration of real-time distributed control systems that utilises the IEC 61499 function block model. This work is central to the development of distributed intelligent control systems that are inherently adaptable and dynamically reconfigurable. The approach that is used takes advantage of distributed artificial intelligence at the planning and control levels to achieve significantly shorter up-front commissioning times as well as significantly more responsiveness to change. This approach is based on object-oriented and agent-based methods and aims at overcoming the difficulties associated with managing real-time reconfiguration of a holonic manufacturing system.,https://ieeexplore.ieee.org/document/953126/,12th International Workshop on Database and Expert Systems Applications,3-7 Sept. 2001,ieeexplore
10.1109/IAAI51705.2020.9332828,Remote Monitoring System of Mechanical Manufacturing Equipment based on Digital Image Processing,IEEE,Conferences,"Precision instrument, machinery manufacturing equipment needs to use a monitoring system to monitor production details in real time during its production and processing. However, the traditional remote monitoring system has a weak ability to process monitoring images, resulting in a low peak signal-to-noise ratio of the system. The surveillance image is blurry. Therefore, based on digital image processing, a new remote monitoring system for machinery manufacturing equipment is designed. In hardware, the front-end of voltage signal acquisition is redesigned, and the communication circuit is optimized. In software, digital image processing technology is used to sharpen the image fuzzy contour, correct the image texture, and enhance the fuzzy details of the monitoring image; the remote positioning and monitoring function of the system is set to realize the remote monitoring of mechanical manufacturing equipment. Experimental results: Compared with the monitoring system under the traditional design, the peak signal-to-noise ratio of the remote monitoring system designed this time is much higher than that of the traditional monitoring system. Visible digital image processing technology can help the surveillance system to enhance the fuzzy details of the image.",https://ieeexplore.ieee.org/document/9332828/,2020 IEEE International Conference on Industrial Application of Artificial Intelligence (IAAI),25-27 Dec. 2020,ieeexplore
10.1109/DTPI52967.2021.9540104,Research and practice of lightweight digital twin speeding up the implementation of flexible manufacturing systems,IEEE,Conferences,"Parallel manufacturing in Industry 5.0 requires digital twin to digitize physical systems, building virtual models to open up channels connecting physical systems, information systems, and social systems, and transforming the physical models of the existing production environment to achieve two-way feedback of virtual and real is the current research direction. This paper proposes the modeling idea of lightweight digital twin, extracts core dimensions and performs digital virtual simulation, so as to quickly realize the complete process of two-way feedback, and realize a set of chess flexible parallel manufacturing production lines as a practice for the design of complete lightweight digital twin.",https://ieeexplore.ieee.org/document/9540104/,2021 IEEE 1st International Conference on Digital Twins and Parallel Intelligence (DTPI),15 July-15 Aug. 2021,ieeexplore
10.1109/ICAICA52286.2021.9498172,Research on Edge Computing of Automatic Control System of Unattended Intelligent Manufacturing Equipment,IEEE,Conferences,"In order to solve the problem that unattended substations are difficult to monitor in real time due to the large number of points, this paper proposes an intelligent classification edge computing algorithm for real-time monitoring of inbound personnel. The paper uses the continuous-time Markov algorithm to complete the intelligent classification of the personnel entering the station: record the name and time of the entry for the known personnel, and perform the alarm function and other prescribed actions for the stranger. Experimental results in practical applications show that adjusting the hyperparameters of the algorithm will achieve different sensitivity and recognition rates. After fine- tuning the hyperparameters, the accuracy of the algorithm reaches about 90%. The monitoring platform developed based on this algorithm has been deployed on smart terminals, relying on edge computing technology to realize automatic identification of unattended substation personnel entering the station, and achieved expected results in production practice.",https://ieeexplore.ieee.org/document/9498172/,2021 IEEE International Conference on Artificial Intelligence and Computer Applications (ICAICA),28-30 June 2021,ieeexplore
10.1109/ICE/ITMC52061.2021.9570221,Resilient Manufacturing Systems enabled by AI support to AR equipped operator,IEEE,Conferences,"Supply chains and manufacturing systems robustness and resilience are, for many years, but especially nowadays, key features requested to ensure reliable and efficient production processes. Two domains are crucial to achieve such purpose: the former is fast and comprehensive monitoring, efficient and reliable condition detection and effective and explicable support for decision making. The latter refers to the intervention by operators, able to better identify problems and to put in place effective operations aimed at fixing it or, better, to prevent such circumstances. This paper presents an integrated approach encompassing a sophisticated IoT and AI-based approach to monitor and detect critical situations, fully integrated with an AR (Augmented Reality) system supporting operators in the field to take informed actions in bi-directional continuous connection. Activities in the context of EC funded project Qu4lity developed in Politecnico di Milano Industry 4.0 Lab, a test environment implementing the proposed approach and demonstrating in an automated production line the effectiveness of the approach, significantly improving performances. Analysis of performance indicators demonstrates the soundness of the proposed solution and implementation methodology to make the overall production process more resilient, efficient and with product defects reduction.",https://ieeexplore.ieee.org/document/9570221/,"2021 IEEE International Conference on Engineering, Technology and Innovation (ICE/ITMC)",21-23 June 2021,ieeexplore
10.1109/IJCNN.1993.714078,Simulation of manufacturing models and its learning with artificial neural networks,IEEE,Conferences,"Introduces some new faces of simulation for intelligent manufacturing systems. Not only the productive parameters are treated from a multi-level point of view but the economic indicators were calculated with global optimistic features. The general marketing aspects are considered as critical conditions. To solve several decision making problems of a complex manufacturing organization, the authors extract meaningful variables to observe by data analysis and construct an artificial neural net. The authors try to take some advantages of non-symbolic processing such that parallel treatment and its implementation guide toward a real-time system, learning capabilities also can be applied independently to problems.",https://ieeexplore.ieee.org/document/714078/,"Proceedings of 1993 International Conference on Neural Networks (IJCNN-93-Nagoya, Japan)",25-29 Oct. 1993,ieeexplore
10.1109/WRC-SARA.2019.8931920,Software-defined Cloud Manufacturing in the Context of Industry 4.0,IEEE,Conferences,"In the practice of “Cloud Manufacturing (CMfg)” or “Industrial Internet”, there still exist key problems, including: 1) big data analytics and decision-making in the cloud could not meet the requirements of time-sensitive manufacturing applications, moreover uploading ZettaBytes of future device data to the cloud may cause serious network congestion, 2) the manufacturing system lacks openness and evolvability, thus restricting the rapid optimization and transformation of the system, 3) big data from the shop-floor IoT devices and the internet has not been effectively utilized to guide the optimization and upgrade of the manufacturing system. In view of these key practical problems, we propose an open evolutionary architecture of intelligent CMfg system with collaborative edge and cloud processing capability. Hierarchical gateways near shop-floor things are introduced to enable fast processing for time-sensitive applications. Big data in another dimension from the software defined perspective will be used to decide the efficient operations and highly dynamic upgrade of the system. From the software system view, we also propose a new mode - AI-Mfg-Ops (AI-enabled Cloud Manufacturing Operations) with a supporting framework, which can promote the fast operation and upgrading of CMfg systems with AI enabled monitoring-analysis-planning-execution close loop. This work can improve the universality of CMfg for real-time fast response and operation &amp; upgrading.",https://ieeexplore.ieee.org/document/8931920/,2019 WRC Symposium on Advanced Robotics and Automation (WRC SARA),21-22 Aug. 2019,ieeexplore
10.1109/CSCWD.2005.194338,Study of ASP service lifecycle management technologies for networked manufacturing system,IEEE,Conferences,"Nowadays, networked manufacturing system based on ASP (application service provider) has become one of the hotspots of research and application. How to manage large amount of ASP services, and how to provide better service quality, has become very important. The paper mainly studies the ASP service management technologies. We present the concept of service lifecycle management (SLM), defines ASP service, and set up a state model for service lifecycle. Then, we give several key technologies' solutions for implementation of service management. Finally the application of these technologies in a real networked manufacturing system is introduced.",https://ieeexplore.ieee.org/document/1504245/,"Proceedings of the Ninth International Conference on Computer Supported Cooperative Work in Design, 2005.",24-26 May 2005,ieeexplore
10.1109/icABCD49160.2020.9183853,The Impact of Smart Manufacturing Approach On The South African Manufacturing Industry,IEEE,Conferences,"SM is a technology-driven approach that mainly utilises machines to monitor the entire production of an organisation. The objective of SM in an organisation is to identify ways to automatize the manufacturing process while using data analytics to optimize the manufacturing performance. This research mitigates the impact of technology, in this case expressed as SM in South African industries. The research followed a quantitative approach whereby 42 respondents from low, medium low and high technology industries took part in the study. Data has been amassed from first-hand experience by mean of an adapted questionnaire constituted of three sections: The first section was about the general demographic information of the respondents. Section two investigates the respondent's awareness on SM. Finally, section three assessed the impact that SM had on the performance of the organisation. The findings of this study revealed that Smart Manufacturing has a positive impact in South African manufacturing organisations as it allows effective operations, fast response to customers demand, real time operations optimisation. Nevertheless, Smart Manufacturing is a new concept under the fourth industrial revolution in South Africa and will need time before being totally implemented in all organisations as it is costly.",https://ieeexplore.ieee.org/document/9183853/,"2020 International Conference on Artificial Intelligence, Big Data, Computing and Data Communication Systems (icABCD)",6-7 Aug. 2020,ieeexplore
10.1109/ICIT.2002.1189350,The application of a reinforcement learning agent to a multi-product manufacturing facility,IEEE,Conferences,"An intelligent agent-based scheduling system, consisting of a reinforcement learning agent and a simulation model has been developed and tested on a classic scheduling problem. The production facility studied is a multiproduct serial line subject to stochastic failure. The agent goal is to minimise total production costs, through selection of job sequence and batch size. To explore state space the agent used reinforcement learning. By applying an independent inventory control policy for each product, the agent successfully identified optimal operating policies for a real production facility.",https://ieeexplore.ieee.org/document/1189350/,"2002 IEEE International Conference on Industrial Technology, 2002. IEEE ICIT '02.",11-14 Dec. 2002,ieeexplore
10.1109/PICMET.2015.7273077,The effects of disruption on different types of tile manufacturing Industry-layouts: An empirical investigation on tile manufacturing industry,IEEE,Conferences,"Almost all manufacturing facilities need to use production scheduling systems to increase productivity and reduce production costs. Most manufacturing industries invest huge amount of money to manufacture and supply products on time in order to meet customers demand and objectives but due to unforeseen disruptions, these objectives are difficult to achieve. In Real-life, production operations are subject to a large number of unexpected disruptions that may invalidate an original schedule. This work considered effects of disruption on different types of industry-layouts that are seldom been used in manufacturing industries namely: fixed, product, production and process industry-layouts. Questionnaires were used for data collection from a number of companies in Cameroon and reliably theory, simulation software's were using to analyze the data. The study reveals that, disruptions varies per industry-layout and the leading sources of disruptions are machine breakdown, power failure, employee's absenteeism and material shortage. It is concluded that disruption of one type may not greatly affect productivity of a certain industry-layout whilst similar disruptions can have devastating effects on another type and also, the impacts of disruption are dependent on the Industry-layouts.",https://ieeexplore.ieee.org/document/7273077/,2015 Portland International Conference on Management of Engineering and Technology (PICMET),2-6 Aug. 2015,ieeexplore
10.1109/IEMC.1998.727776,The importance of artificial intelligence-expert systems in computer integrated manufacturing,IEEE,Conferences,"In order to maintain their competitiveness, companies feel compelled to adopt productivity increasing measures. Yet, they cannot relinquish the flexibility their production cycles need in order to improve their response, and thus, their positioning in the market. To achieve this, companies must combine these two seemingly opposed principles. Thanks to new technological advances, this combination is already a working reality in some companies. It is made possible today by the implementation of computer integrated manufacturing (CIM) and artificial intelligence (AI) techniques, fundamentally by means of expert systems (ES) and robotics. Depending on how these (AI/CIM) techniques contribute to automation, their immediate effects are an increase in productivity and cost reductions. Yet also, the system's flexibility allows for easier adaptation and, as a result, an increased ability to generate value, in other words, competitiveness is improved. The authors have analyzed three studies to identify the possible benefits or advantages, as well as the inconveniences, that this type of technique may bring to companies, specifically in the production field. Although the scope of the studies and their approach differ from one to the other, their joint contribution can be of unquestionable value in order to understand a little better the importance of ES within the production system.",https://ieeexplore.ieee.org/document/727776/,IEMC '98 Proceedings. International Conference on Engineering and Technology Management. Pioneering New Technologies: Management Issues and Challenges in the Third Millennium (Cat. No.98CH36266),11-13 Oct. 1998,ieeexplore
10.1109/AQTR.2018.8402748,Time series forecasting for dynamic scheduling of manufacturing processes,IEEE,Conferences,"Manufacturing control systems evolved in the recent decades from pre-programmed rigid systems to adaptable, data driven, cloud based implementations, capable to respond to environment changes and new requirements in real time. A byproduct of this transformation is represented by large amounts of structured and semi-structured information, both historical and real-time data that is made available on various layers of the system. This accumulation of information brings the opportunity to move from the rule based decision making algorithms used traditionally by these control systems towards more intelligent approaches, driven by modern deep learning mechanisms. This paper proposes a time series forecasting model using recursive neural networks (RNN) for operation scheduling and sequencing in a virtual shop floor environment. The time series aspect of the RNN is novel in manufacturing domain, in the sense that the new best prediction produced considers the previous decisions and outcomes. The proposed implementation explains how the RNN can be mapped to the specifics of a manufacturing control system and introduces a bidding mechanism to allow dynamic evaluation of individual forecasts. The pilot implementation, initial experiments on sample data sets and results presented show how using recursive neural networks can optimize resource utilization and energy consumption.",https://ieeexplore.ieee.org/document/8402748/,"2018 IEEE International Conference on Automation, Quality and Testing, Robotics (AQTR)",24-26 May 2018,ieeexplore
10.1109/ETFA46521.2020.9212097,Towards Real-time Process Monitoring and Machine Learning for Manufacturing Composite Structures,IEEE,Conferences,"Components made from carbon fiber reinforced plastics (CFRP) offer attractive stability properties for the automotive or aerospace industry despite their light weight. To automate CFRP production, resin transfer molding (RTM) based on thermoset plastics is commonly applied. However, this manufacturing process has its shortcomings in quality and costs. The project CosiMo aims for a highly automated and cost-attractive manufacturing process using cheaper thermoplastic materials. In a thermoplastic RTM (T-RTM) process, the polymerization of ε-caprolactam to polyamide 6 is investigated using an intelligent mold tooling. Multiple sensor types integrated into the mold allow for tracking of process-relevant variables, such as material flow and polymerization state. In addition to monitoring the T-RTM process, a digital twin visualizes progress and makes predictions about issues and countermeasures based on machine learning.",https://ieeexplore.ieee.org/document/9212097/,2020 25th IEEE International Conference on Emerging Technologies and Factory Automation (ETFA),8-11 Sept. 2020,ieeexplore
10.1109/DEST.2009.5276766,Transforming SME manufacturing plants into evolvable systems through agents,IEEE,Conferences,"Manufacturing plants need to be flexible and evolvable, which might be achieved by stepping up the level of abstraction at which they are designed. In this work a methodology that starts by creating ontologies of the plants that represent information flows is proposed. Agent-based technology is then used to provide systems with the ability to evolve. The proposal is designed to suit the needs of SME' plants, whose operation cannot be disturbed while progressively undergo a transformation into more advanced ones. The methodology has been based on a partial implementation on a real case that, in its turn, has served to validate the whole procedure.",https://ieeexplore.ieee.org/document/5276766/,2009 3rd IEEE International Conference on Digital Ecosystems and Technologies,1-3 June 2009,ieeexplore
10.1109/ICRA.2019.8794123,Visual Guidance and Automatic Control for Robotic Personalized Stent Graft Manufacturing,IEEE,Conferences,"Personalized stent graft is designed to treat Abdominal Aortic Aneurysms (AAA). Due to the individual difference in arterial structures, stent graft has to be custom made for each AAA patient. Robotic platforms for autonomous personalized stent graft manufacturing have been proposed in recently which rely upon stereo vision systems for coordinating multiple robots for fabricating customized stent grafts. This paper proposes a novel hybrid vision system for real-time visual-sevoing for personalized stent-graft manufacturing. To coordinate the robotic arms, this system is based on projecting a dynamic stereo microscope coordinate system onto a static wide angle view stereo webcam coordinate system. The multiple stereo camera configuration enables accurate localization of the needle in 3D during the sewing process. The scale-invariant feature transform (SIFT) method and color filtering are implemented for stereo matching and feature identifications for object localization. To maintain the clear view of the sewing process, a visual-servoing system is developed for guiding the stereo microscopes for tracking the needle movements. The deep deterministic policy gradient (DDPG) reinforcement learning algorithm is developed for real-time intelligent robotic control. Experimental results have shown that the robotic arm can learn to reach the desired targets autonomously.",https://ieeexplore.ieee.org/document/8794123/,2019 International Conference on Robotics and Automation (ICRA),20-24 May 2019,ieeexplore
10.1109/ECC.2015.7330620,Web tension regulation with partially known periodic disturbances in roll-to-roll manufacturing systems,IEEE,Conferences,"This paper deals with the problem of synthesizing feedforward control to aid the regulation of web tension in the presence of partially known exogenous inputs. The problem appears in many engineering applications including Roll-to-Roll (R2R) manufacturing systems where the governing equation for tension is nonlinear. Currently known methods for the nonlinear output regulation problem either require the solution of a constrained partial differential equation or the preview information of the signal to be tracked. In this paper, we consider the problem of regulating web tension while rejecting periodic disturbances and use a novel approach to synthesize feedforward control as the solution of a system of differential-algebraic equations, which is considerably less complex and suitable for practical implementation. We assume that the disturbance signal is the output of a known exogenous system with unknown initial conditions. A parameter identification scheme to estimate the unknown initial conditions is developed. The proposed technique is successfully applied to web tension regulation in a large R2R machine which contains real-time hardware and software that is used in industrial practice. Extensive experiments were conducted to evaluate the proposed scheme under various experimental conditions, including different web speeds and materials. We will discuss a representative sample of the results with the proposed nonlinear tension regulator and provide a comparison with a well-tuned industrial PI control scheme to highlight the benefits of using the proposed scheme.",https://ieeexplore.ieee.org/document/7330620/,2015 European Control Conference (ECC),15-17 July 2015,ieeexplore
10.1109/TII.2019.2915846,A Global Manufacturing Big Data Ecosystem for Fault Detection in Predictive Maintenance,IEEE,Journals,"Artificial intelligence, big data, machine learning, cloud computing, and Internet of Things (IoT) are terms which have driven the fourth industrial revolution. The digital revolution has transformed the manufacturing industry into smart manufacturing through the development of intelligent systems. In this paper, a big data ecosystem is presented for the implementation of fault detection and diagnosis in predictive maintenance with real industrial big data gathered directly from large-scale global manufacturing plants, aiming to provide a complete architecture which could be used in industrial IoT-based smart manufacturing in an industrial 4.0 system. The proposed architecture overcomes multiple challenges including big data ingestion, integration, transformation, storage, analytics, and visualization in a real-time environment using various technologies such as the data lake, NoSQL database, Apache Spark, Apache Drill, Apache Hive, OPC Collector, and other techniques. Transformation protocols, authentication, and data encryption methods are also utilized to address data and network security issues. A MapReduce-based distributed PCA model is designed for fault detection and diagnosis. In a large-scale manufacturing system, not all kinds of failure data are accessible, and the absence of labels precludes all the supervised methods in the predictive phase. Furthermore, the proposed framework takes advantage of some of the characteristics of PCA such as its ease of implementation on Spark, its simple algorithmic structure, and its real-time processing ability. All these elements are essential for smart manufacturing in the evolution to Industry 4.0. The proposed detection system has been implemented into the real-time industrial production system in a cooperated company, running for several years, and the results successfully provide an alarm warning several days before the fault happens. A test case involving several outages in 2014 is reported and analyzed in detail during the experiment section.",https://ieeexplore.ieee.org/document/8710319/,IEEE Transactions on Industrial Informatics,Jan. 2020,ieeexplore
10.1109/JIOT.2019.2940131,A Two-Stage Transfer Learning-Based Deep Learning Approach for Production Progress Prediction in IoT-Enabled Manufacturing,IEEE,Journals,"In make-to-order manufacturing enterprises, accurate production progress (PP) prediction is an important basis for dynamic production process optimization and on-time delivery of orders. The implementation of Internet of Things (IoT) makes it possible to take real-time production state as an important factor affecting PP. In the IoT-enabled workshop, a two-stage transfer learning-based prediction method using both historical production data and real-time state data is proposed to solve the problem of low-prediction accuracy and poor generalization performance caused by insufficient data of target order. The deep autoencoder (DAE) model with transfer learning is designed to extract the generalized features of target order in the first stage, which uses bootstrap sampling to avoid over fitting. The deep belief network (DBN) model with transfer learning is constructed to fit the nonlinear relation for PP prediction in the second stage. A real case from an IoT enabled machining workshop is taken to validate the performance of the proposed method over the other methods such as DBN, deep neural network.",https://ieeexplore.ieee.org/document/8827506/,IEEE Internet of Things Journal,Dec. 2019,ieeexplore
10.1109/TASE.2006.886833,An Intelligent Online Monitoring and Diagnostic System for Manufacturing Automation,IEEE,Journals,"Condition monitoring and fault diagnosis in modern manufacturing automation is of great practical significance. It improves quality and productivity, and prevents damage to machinery. In general, this practice consists of two parts: 1)extracting appropriate features from sensor signals and 2)recognizing possible faulty patterns from the features. Through introducing the concept of marginal energy in signal processing, a new feature representation is developed in this paper. In order to cope with the complex manufacturing operations, three approaches are proposed to develop a feasible system for online applications. This paper develops intelligent learning algorithms using hidden Markov models and the newly developed support vector techniques to model manufacturing operations. The algorithms have been coded in modular architecture and hierarchical architecture for the recognition of multiple faulty conditions. We define a novel similarity measure criterion for the comparison of signal patterns which will be incorporated into a novel condition monitoring system. The sensor-based intelligent system has been implemented in stamping operations as an example. We demonstrate that the proposed method is substantially more effective than the previous approaches. Its unique features benefit various real-world manufacturing automation engineering, and it has great potential for shop floor applications.",https://ieeexplore.ieee.org/document/4358068/,IEEE Transactions on Automation Science and Engineering,Jan. 2008,ieeexplore
10.1109/TASE.2020.3044107,An Online Policy for Energy-Efficient State Control of Manufacturing Equipment,IEEE,Journals,"Machine state control is one of the most promising energy-efficient measures for machining processes. A proper control reduces the energy consumed during idle periods by switching off/on the machines. A critical barrier for practical implementation is related to the knowledge of part arrival process that is affected by uncertainty. The stochastic processes involved in the system are usually assumed to be known. However, real production environments are subject to several sources of randomness that are difficult to model a priori. This work provides an online time-based algorithm that is able to control the machine state. Through a method for the estimation of the stochastic process, the algorithm provides the optimal control parameters based on a collected set of observations. A new policy is formulated to manage the control over time such that changes in the control parameters are applied only under certain conditions. Potential benefits are discussed using realistic numerical cases. Note to Practitioners-This article analyzes the control problem of switching off/on a machine tool for energy saving during machine idle periods. A control policy based on time information is investigated when the machine requires a startup time to resume the service after being switched off. The proposed policy works online while acquiring information from the real system. An algorithm is described for identifying and applying the optimal control parameters. The results of this research will be useful for a practical implementation of a switching policy for energy saving. This implementation requires the estimation of the power adsorbed by the machine in four different states and, therefore, it reduces the implementation effort for practitioners.",https://ieeexplore.ieee.org/document/9308932/,IEEE Transactions on Automation Science and Engineering,April 2021,ieeexplore
10.1109/LRA.2017.2737046,Baxter's Homunculus: Virtual Reality Spaces for Teleoperation in Manufacturing,IEEE,Journals,"We demonstrate a low-cost telerobotic system that leverages commercial virtual reality (VR) technology and integrates it with existing robotics control infrastructure. The system runs on a commercial gaming engine using off-the-shelf VR hardware and can be deployed on multiple network architectures. The system is based on the homunculus model of mind wherein we embed the user in a VR control room. The control room allows for multiple sensor displays, and dynamic mapping between the user and robot. This dynamic mapping allows for selective engagement between the user and the robot. We compared our system with state-of-the-art automation algorithms and standard VR-based telepresence systems by performing a user study. The study showed that new users were faster and more accurate than the automation or a direct telepresence system. We also demonstrate that our system can be used for pick and place, assembly, and manufacturing tasks.",https://ieeexplore.ieee.org/document/8003431/,IEEE Robotics and Automation Letters,Jan. 2018,ieeexplore
10.1109/ACCESS.2020.2977846,Big Data Driven Edge-Cloud Collaboration Architecture for Cloud Manufacturing: A Software Defined Perspective,IEEE,Journals,"In the practice of cloud manufacturing, there still exist some major challenges, including: 1) cloud based big data analytics and decision-making cannot meet the requirements of many latency-sensitive applications on shop floors; 2) existing manufacturing systems lack enough reconfigurability, openness and evolvability to deal with shop-floor disturbances and market changes; and 3) big data from shop-floors and the Internet has not been effectively utilized to guide the optimization and upgrade of manufacturing systems. This paper proposes an open evolutionary architecture of the intelligent cloud manufacturing system with collaborative edge and cloud processing. Hierarchical gateways connecting and managing shop-floor things at the “edge” side are introduced to support latency-sensitive applications for real-time responses. Big data processed both at the gateways and in the cloud will be used to guide continuous improvement and evolution of edge-cloud systems for better performance. As software tools are becoming dominant as the “brain” of manufacturing control and decision-making, this paper also proposes a new mode - “AI-Mfg-Ops” (AI enabled Manufacturing Operations) with a supporting software defined framework, which can promote fast operation and upgrading of cloud manufacturing systems with smart monitoring-analysis-planning-execution in a closed loop. This research can contribute to the rapid response and efficient operation of cloud manufacturing systems.",https://ieeexplore.ieee.org/document/9020166/,IEEE Access,2020,ieeexplore
10.1109/TCSS.2021.3052231,Cognitive Analytics of Social Media Services for Edge Resource Pre-Allocation in Industrial Manufacturing,IEEE,Journals,"With the development of industrial intelligence, the resource requests of various social media services in smart cities are expanding rapidly. For hosting services, the edge computing (EC) platform for its low-latency resource provisioning is fully explored. However, the mapping between edge servers (ESs) and services affects the service latency. Meanwhile, the real-time dynamic distribution of resource requirements also impairs the load balance. Therefore, how to optimize the load balance of ESs while meeting the latency-critical requests remains challenging. To deal with the above challenge, in this article, we propose a resource pre-allocation (RPA) method for the social media services with cognitive analytics. Technically, the deep spatiotemporal residual network (ST-ResNet) is employed to complete the cognitive analytics of resource requests. Then based on the analysis results, the optimal resource allocation (ORA) scheme is designed with multiobjective optimization. Finally, the performance of RPA is evaluated by a real-world resource request data set.",https://ieeexplore.ieee.org/document/9340550/,IEEE Transactions on Computational Social Systems,April 2021,ieeexplore
10.1109/ACCESS.2021.3120843,Digital Twins From Smart Manufacturing to Smart Cities: A Survey,IEEE,Journals,"Digital twins are quickly becoming a popular tool in several domains, taking advantage of recent advancements in the Internet of Things, Machine Learning and Big Data, while being used by both the industry sector and the research community. In this paper, we review the current research landscape as regards digital twins in the field of smart cities, while also attempting to draw parallels with the application of digital twins in Industry 4.0. Although digital twins have received considerable attention in the Industrial Internet of Things domain, their utilization in smart cities has not been as popular thus far. We discuss here the open challenges in the field and argue that digital twins in smart cities should be treated differently and be considered as cyber-physical “systems of systems”, due to the vastly different system size, complexity and requirements, when compared to other recent applications of digital twins. We also argue that researchers should utilize established tools and methods of the smart city community, such as co-creation, to better handle the specificities of this domain in practice.",https://ieeexplore.ieee.org/document/9576739/,IEEE Access,2021,ieeexplore
10.1109/ACCESS.2021.3120126,Energy-Aware Flowshop Scheduling: A Case for AI-Driven Sustainable Manufacturing,IEEE,Journals,"A fully verifiable and deployable framework for optimizing schedules in a batch-based production system is proposed. The scheduler is designed to control and optimize the flow of batches of material into a network of identical and non-identical parallel and series machines that produce a high variation of complex hard metal products. The proposed multi-objective batch-based flowshop scheduling optimization (MOBS-NET) deploys a fully connected deep neural network (FCDNN) with respect to three performance criteria of energy, cost and makespan. The problem is NP-hard and considers minimizing the energy consumed per unit of product, operations cost, and the makespan. The output of the method has been validated and verified as optimal operational planning and scheduling meeting the business operational objectives. Real-time and look ahead discrete event simulation of the production process provides the feedback and assurance of the robustness and practicality of the optimum schedules prior to implementation.",https://ieeexplore.ieee.org/document/9570368/,IEEE Access,2021,ieeexplore
10.1109/70.63270,Hybrid hierarchical scheduling and control systems in manufacturing,IEEE,Journals,"Some experiments on the integration of algorithmic techniques with knowledge-based ones are discussed. Two case studies are presented: an FMS cell and a press shop. It was found that the algorithmic procedures developed for production scheduling resulted in limiting the ability to cope with the complexity of the real manufacturing world. The scheduling problem, seen as a constraint satisfaction problem, can be approached with rule-based techniques. Nevertheless, algorithmic techniques are found to be valuable for their efficiency and ability to deal with aggregated data. This ability is fundamental for an efficient implementation of hierarchical control systems in general and in the manufacturing context in particular. This suggests that the integration of rule-based techniques with algorithmic ones can increase the efficiency of searching in the space of possible solutions. The ability to deal with aggregated data can have little value when detailed real-time operation scheduling is needed. In this case, simple dispatching rules are often used, and sophisticated operations research methods are not used. In such a dynamic situation, a purely-rule based approach may be more suitable.&lt;<ETX>&gt;</ETX>",https://ieeexplore.ieee.org/document/63270/,IEEE Transactions on Robotics and Automation,Dec. 1990,ieeexplore
10.1109/TADVP.2004.828824,Intelligent SOP manufacturing,IEEE,Journals,"Microsystems packaging is fundamentally dependent on the manufacture of microelectronic, photonic, radio frequency (RF), and MEMS devices. The system-on-package (SOP) approach has been identified as a key strategy for integrating these strategic packaging technologies. Because of rising costs, the challenge before SOP manufacturers is to offset capital investment with greater automation and technological innovation in the fabrication process. To reduce manufacturing cost, several important subtasks have emerged, including increasing fabrication yield, reducing product cycle time, maintaining consistent levels of product quality and performance, and improving the reliability of processing equipment. Because of the large number of steps involved, maintaining product quality in an SOP manufacturing facility requires the control of hundreds of process variables. The interdependent issues of high yield, high quality, and low cycle time are addressed by the ongoing development of several critical capabilities in state-of-the-art computer-integrated manufacturing (CIM) systems: in situ process monitoring, process/equipment modeling, real-time process control, and equipment diagnosis. Recently, the use of computational intelligence in various manufacturing applications has increased, and the SOP manufacturing arena is no exception to this trend. Artificial neural networks, genetic algorithms (GAs), and other techniques have emerged as powerful tools for assisting CIM systems in performing various process monitoring, modeling, and control functions. This paper reviews current research in these areas, as well as the potential for deployment of these capabilities in state-of-the-art SOP manufacturing facilities.",https://ieeexplore.ieee.org/document/1331523/,IEEE Transactions on Advanced Packaging,May 2004,ieeexplore
10.1109/TASE.2020.3044620,Knowledge-Based Automation for Smart Manufacturing Systems,IEEE,Journals,"Smart manufacturing is targeted as the next generation of manufacturing by many national and international strategic development. The increasingly rich production data, the integration and extensive application of information technology, and the intelligent data processing and system modeling methods have collectively enabled smart manufacturing. Building upon them, manufacturing system modeling, knowledge acquisition, design, and real-time control are the key components [item 1) in the Appendix], [item 2) in the Appendix]. It is still one of the really huge challenges to gather data, transform them into information, and derive knowledge out of this information, especially given the requirement of knowledge that can be trusted as manufacturing systems may harm humans and the environment if they come to the wrong conclusion. Despite the learning and derivation of knowledge, it could be modeled beforehand and taken, for example, as an environmental model for online decisions like in deliberative agent-based systems. Nevertheless, for decisions during operation, real-time requirements, dependability, and security issues are to be guaranteed. Finally, for acceptance and trust, humans need to “understand” the reasons behind automated decisions; therefore, explainability or at least a white box description is an issue of such knowledge-based systems.",https://ieeexplore.ieee.org/document/9316429/,IEEE Transactions on Automation Science and Engineering,Jan. 2021,ieeexplore
10.1109/TMECH.2014.2366033,Output Regulation of Nonlinear Systems With Application to Roll-to-Roll Manufacturing Systems,IEEE,Journals,"This paper deals with the problem of synthesizing feedforward control to aid the regulation of output of a nonlinear system in the presence of partially known exogenous inputs. The problem appears in many engineering applications including Roll-to-Roll (R2R) manufacturing systems. Currently known methods for this problem either require the solution of a constrained partial differential equation or the preview information of the signal to be tracked. The novelty of this paper lies in synthesizing feedforward control as the solution of a system of differential-algebraic equations, which is considerably less complex and suitable for practical implementation. In this paper, we consider the problem of regulating the output while rejecting the disturbances and apply it to R2R manufacturing systems. The problem of tracking reference signals can also be handled with the suggested technique. We assume that the disturbance signal is the output of a known exogenous system with unknown initial conditions. A parameter identification scheme to estimate the unknown initial conditions is developed. The proposed technique is successfully applied to control of web tension in a large R2R machine which mimics most of the features of industrial R2R machines and contains real-time hardware and software that is used in industrial practice. Extensive experiments were conducted to evaluate the proposed scheme for web tension control under various experimental conditions, including different web speeds and materials. We will present and discuss the representative experimental results with the proposed technique and provide a comparison with an industrial PI control scheme to highlight the benefits of using the proposed scheme.",https://ieeexplore.ieee.org/document/6963413/,IEEE/ASME Transactions on Mechatronics,June 2015,ieeexplore
10.1109/3476.585143,Real-time diagnosis of semiconductor manufacturing equipment using a hybrid neural network expert system,IEEE,Journals,"This paper presents a tool for the real-time diagnosis of integrated circuit fabrication equipment. The approach focuses on integrating neural networks into an expert system. The system employs evidential reasoning to identify malfunctions by combining evidence originating from equipment maintenance history, on-line sensor data, and in-line post-process measurements. Neural networks are used in the maintenance phase of diagnosis to approximate the functional form of the failure history distribution of each component. Predicted failure rates are then converted to belief levels. For on-line diagnosis in the ease of previously unencountered faults, a CUSUM control chart is implemented on real sensor data to detect very small process shifts and their trends. For the known fault case, continuous hypothesis testing on the statistical mean and variance of the sensor data is performed to search for similar data patterns and assign belief levels. Finally, neural process models of process figures of merit (such as etch uniformity) derived from prior experimentation are used to analyze the in-line measurements, and identify the most suitable candidate among faulty input parameters (such as gas flow) to explain process shifts. A working prototype for this hybrid diagnostic system has been implemented on the Plasma Therm 700 series reactive ion etcher located in the Georgia Tech Microelectronics Research Center.",https://ieeexplore.ieee.org/document/585143/,"IEEE Transactions on Components, Packaging, and Manufacturing Technology: Part C",Jan. 1997,ieeexplore
10.1109/TSMCC.2013.2265234,Self-Organized P2P Approach to Manufacturing Service Discovery for Cross-Enterprise Collaboration,IEEE,Journals,"The combination of service-oriented architecture (SOA) and peer-to-peer (P2P) architecture plays a promising role in distributed manufacturing environments in that the peer service can be used to facilitate the integration and discovery of distributed manufacturing resources and achieve communication and collaboration across distributed virtual enterprises. However, the large size, dynamic nature, and heterogeneous expression of distributed manufacturing resources bring forth a serious challenge in scalability and efficiency. This paper presents a self-organized P2P framework that supports scalable and efficient manufacturing service (MS) discovery for cross-enterprise collaboration by forming and maintaining autonomous enterprise peer groups (PG). Each enterprise exhibits as a peer that provides some sharable MSs that are represented comprehensively and formally with a generalized ontology. Each enterprise PG dynamically clusters a set of enterprise peers offering semantically similar MSs, and elects the most reputed peer through multicriteria trust evaluation as its core (i.e., super peer, SP). Then, a MS request can be first routed to the suitable SP and further to its leaf peer in a systematic way, thus supporting efficient service discovery. A prototype system is implemented on JXTA for real application and validated through an experimental case study.",https://ieeexplore.ieee.org/document/6600968/,"IEEE Transactions on Systems, Man, and Cybernetics: Systems",March 2014,ieeexplore
10.1109/ACCESS.2020.3032601,Sliding Mode Observer Based Multi-Layer Metal Plates Core Temperature On-Line Estimation for Semiconductor Intelligence Manufacturing,IEEE,Journals,"This paper focuses on the technological development of core temperature estimation algorithm of multi-layer metal plates. It applies new control technology in the field of real time monitoring of semiconductor producing processes. To achieve real time core temperature estimation in semiconductor equipment processes, this paper will follow the order of: system description, model derivation, parameter identification, and design of a robust core temperature observer of multi-layer metal plates, which will then be cross validated with experimental data. In the metal heating system model proposed in this paper, external cooling is considered as an unknown interference. Since the system contains an unknown interference, the sliding mode observer (SMO) will use the equivalent conversion technique to tackle with the uncertainty. In order to improve the degree of freedom and flexibility in the design of the gain matrix, and considering the effects of parameter identification uncertainty, this paper introduces the multi-objective linear matrix inequality (LMI) in the design of the sliding mode observer to suppress the impacts of the non-matching uncertainty on the system and to reduce the gain matrix which also satisfies the convergence requirement of the designer. In terms of algorithm implementation, the parameters of the thermally processed multilayer plate model are first identified through the minimum difference filter (MDF), the data is then filtered offline, and lastly, the model parameter is derived using the least square (LS) method. The filtering technology can greatly improve the accuracy of parameter identification and improve the SMO estimation precision. Finally, the experimental data of the actual semiconductor machine and the estimation from the proposed method verifies the usefulness of the SMO in core temperature estimation of multi-layer metal plate heating systems.",https://ieeexplore.ieee.org/document/9233428/,IEEE Access,2020,ieeexplore
10.1109/TSMCC.2010.2059012,Toward Self-Reconfiguration of Manufacturing Systems Using Automation Agents,IEEE,Journals,"The reconfiguration of control software is regarded as an important ability to enhance the effectiveness and efficiency in future manufacturing systems. Agent technology is considered as a promising approach to provide reconfiguration abilities, but existing work has been focused mainly on the reconfiguration of higher layers concerned with production scheduling and planning. In this paper, we present an automation agent architecture for controlling physical components that integrates “on the fly” reconfiguration abilities on the low-level layer. Our approach is combined with an ontological representation of the low-level functionality at the high-level control layer, which is able to reason and initiate reconfiguration processes to modify the low-level control (LLC). As current control systems are mostly based on standards and principles that do not support reconfiguration, leading to rigid control software architectures, we base our approach on the promising Standard IEC 61499 for the LLC, extended by an innovative reconfiguration infrastructure. We demonstrate this approach with a case study of a reconfiguration process that modifies the LLC functionality provided by the automation agent of a physical component. Thereby, we obtain the ability to support numerous different LLC configurations without increasing the LLC's complexity. By applying our automation agent architecture, we enhance not only the flexibility of each component's control software, but also achieve the precondition for reconfiguring the entire manufacturing system.",https://ieeexplore.ieee.org/document/5557828/,"IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)",Jan. 2011,ieeexplore
10.1109/70.508435,Virtual-reality-based point-and-direct robotic inspection in manufacturing,IEEE,Journals,"This paper explores a flexible manufacturing paradigm in which robot grasping is interactively specified and skeletal images are efficiently used in combination to allow rapidly setting up surface flaw identification tasks in small-quantity/large-variety manufacturing. Two complementary technologies are combined to make implementation of inspection as rapid as possible. First, a novel material handling approach is described for robotic picking and placing of parts onto an inspection table using virtual tools. This allows an operator to point and give directives to set up robotic inspection tasks. Second, since specification may be approximate using this method, a fast and flexible means of identifying images of perfect and flawed parts is explored that avoids rotational or translational restrictions on workpiece placement. This is accomplished by using skeleton pixel counts as neural network inputs. The total system, including material handling and skeleton-based inspection, features flexibility during manufacturing set-up, and reduces the process time and memory requirements for workpiece inspection.",https://ieeexplore.ieee.org/document/508435/,IEEE Transactions on Robotics and Automation,Aug. 1996,ieeexplore
10.1109/IESM45758.2019.8948171,A Bi-objective Model for Dual-resource Constrained Job Shop Scheduling with the Consideration of Energy Consumption,IEEE,Conferences,"Dual resource constrained job shop problem (DR-CJSP) is an extension of the job shop scheduling problem such that each job has two available processing resources like machines and workers, each of which is of a non-identical number of operations. In many real manufacturing systems, machines often process in different processing speeds for some special demands. Note that the processing time of each job not only depends on the assigned worker but also on deployed machine. This undoubtedly leads to non-deterministic job processing time, thus resulting in different electricity consumption. We investigate the bi-objective DRCJSP problem with multi-processing speed for minimizing makespan and total electricity consumption in this work. We establish a bi-objective integer programming model and devise an epsilon constraint algorithm together with a non-dominated sorting genetic algorithm II (NSGA-II). The epsilon constraint method produces exact solutions for small job instances while NSGA-II can efficiently solve large job instances. Numerical experiments validate the proposed model and algorithms.",https://ieeexplore.ieee.org/document/8948171/,2019 International Conference on Industrial Engineering and Systems Management (IESM),25-27 Sept. 2019,ieeexplore
10.1109/FUZZ45933.2021.9494540,A Big Bang-Big Crunch Type-2 Fuzzy Logic System for Explainable Predictive Maintenance,IEEE,Conferences,"The role of maintenance in modern manufacturing systems is becoming a more significant contributor to organizational benefit. World-class enterprises are pushing forward with “predict-and prevent” maintenance instead of embracing the drawbacks of reactive maintenance (or a “fail-and fix” approach). The advancement towards Artificial Intelligence (AI), Internet of Things (IoT) and cloud computing has led to a shift in maintenance paradigms with the rising interest in Machine Learning (ML) and in particular deep learning. However, opaque box AI models are complex and difficult to understand and explain to the lay user. This limits the use of these models in predictive maintenance where it is crucial to understand and analyze the model before deployment and it is imperative to understand the logic behind any given decision. This paper introduces a Type-2 Fuzzy Logic System (FLS) optimized by the Big-Bang Big-Crunch algorithm that allows maximizing the interpretability of a model as well as its prediction accuracy for the faults which may occur in future. We tested the proposed type-2 FLS model on water pumps where data was collected in real-time by our proprietary hardware deployed at Aquatronic Group Management Plc. The observations indicate that the proposed system provides a highly interpretable and accurate model for predicting the faults in equipment for building services, process and water industries. The system predictions are used to understand why a particular fault may occur, leading to improved and better-informed service visits for the customers thus reducing the disruptions faced due to equipment failures.",https://ieeexplore.ieee.org/document/9494540/,2021 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE),11-14 July 2021,ieeexplore
10.1109/RTCSA.2018.00012,A Case Study of Cyber-Physical System Design: Autonomous Pick-and-Place Robot,IEEE,Conferences,"Although modern robots in warehousing systems can perform adequately in a goods-to-person model using hand-designed algorithms that are specialized to a particular environment, developing a robotic system that is capable of handling new products at an inexpensive cost remains a challenge. A conspicuous example of this challenge is seen in Amazon's use of autonomous robots to fetch customers' orders in their massive warehouses. To encourage advance in this technology, Amazon organized the competition, Amazon Picking Challenge that asked participants to develop their own hardware and software for the general task of picking a designated set of products from inventory shelves and then placing them at a target location (called a pick-and-place task). Current technology for pick-and-place tasks is still insufficient to meet the demand for low-cost automation. Handling awkward or oddly shaped object must still depend on hand-programming or specialized robotic systems, making manufacturing automation less flexible and expensive. In this paper, we shall present the design and implementation of a software system that is a step in advancing the technology toward full automation at reasonable costs. Our system integrates a set of state-of-the-art techniques in computer vision, deep-learning, trajectory optimization, visual servoing to create a library of skills that can be composed to perform a variety of robotic tasks. We demonstrate the capability of our system for performing autonomous pick-and-place tasks with an implementation using Hoppy, an industrial robotic arm in an environment similar to the Amazon Picking Challenge.",https://ieeexplore.ieee.org/document/8607230/,2018 IEEE 24th International Conference on Embedded and Real-Time Computing Systems and Applications (RTCSA),28-31 Aug. 2018,ieeexplore
10.1109/WSC48552.2020.9383897,A Case Study of Digital Twin for a Manufacturing Process Involving Human Interactions,IEEE,Conferences,"Current algorithms, computations, and solutions that predict how humans will engage in smart manufacturing are insufficient for real-time activities. In this paper, a digital-twin implementation of a manual, manufacturing process is presented. This work (1) combines simulation with data from the physical world and (2) uses reinforcement learning to improve decision making on the shop floor. An adaptive simulation-based, digital twin is developed for a real manufacturing case. The digital twin demonstrates the improvement in predicting overall production output and solutions to existing problems.",https://ieeexplore.ieee.org/document/9383897/,2020 Winter Simulation Conference (WSC),14-18 Dec. 2020,ieeexplore
10.1109/CISIS.2008.82,A Computing Model for Marketable Quality and Profitability of Corporations: A Case Study Evaluation Using a New Sources Data,IEEE,Conferences,"In this paper, we introduce and evaluate a computing model for marketable quality and profitability of corporations. We discuss the model prediction of the turning and transition period based on a new source data. By applying the real data of some leading manufacturing corporations in Japan we analyze the model accuracy. The analysis results show the proposed model give a good approximation and prediction of the turning and transition period of Japanese economy. By using the proposed model, we can obtain a boundary between sellers' and buyers' market.",https://ieeexplore.ieee.org/document/4606679/,"2008 International Conference on Complex, Intelligent and Software Intensive Systems",4-7 March 2008,ieeexplore
10.1109/RAMS48097.2021.9605729,A Data Mining Approach for Forecasting Machine Related Disruptions,IEEE,Conferences,"SUMMARY &amp; CONCLUSIONSProduction disruptions in high-tech mass production companies producing many parts every single minute will lead to considerable economic impact and affect manufacturing efficiency. The root causes of disruptions are classified into three categories: human-related, machine-related, and material related. Using different management, hiring, and training strategies, companies are generally successful in reducing human-related and material related disruptions. However, machine-related disruptions (MRDs) are still occurring even in companies employing a solid maintenance program.The MRDs pose random pauses of various durations in a production. Forecasting the characteristics of such pauses (downtimes) can assist in real-time manufacturing process adjustment and real-time rescheduling of a production. This study aims at utilizing available recorded MRDs for forecasting time to forthcoming MRD and its duration. Our general approach is to evaluate the performance of different data mining-based learning techniques for predicting both the duration and time to forthcoming MRDs and determining the outperforming approach. We consider a smart factory located in the northern part of Toronto great area active in the field of thermoplastic injection molding of various components. We use the historical data on the MRDs recorded from 2013 to 2019 to conduct the investigation. In this study, a set of different classifiers, including rule-based, function-based, tree-based, and lazy, are implemented for forecasting each of the duration and time to forthcoming MRD. The part ID, machine ID, mold age, and the ordinal number of the forthcoming MRD are considered as the input attributes of the developed data mining-based classifiers.In order to determine how effectively the data mining-based methods perform, we calculate different performance criteria, including the Mean Absolute Error (MAE), Root Mean Square Error (RMSE), Bias, Correlation Coefficient (CC), and R<sup>2</sup>. The overall accuracy rate for some tree-based algorithms is significant (CC exceeded 0.92 and MAE&lt;0.06). It shows the capabilities of data mining-based approaches in forecasting the durations and times to MRDs. The obtained trained models are accurate enough to be coupled with stochastic optimization algorithms for real-time manufacturing process adjustment and rescheduling when an MDR takes place.",https://ieeexplore.ieee.org/document/9605729/,2021 Annual Reliability and Maintainability Symposium (RAMS),24-27 May 2021,ieeexplore
10.1109/CAMAD50429.2020.9209305,A Joint Decentralized Federated Learning and Communications Framework for Industrial Networks,IEEE,Conferences,"Industrial wireless networks are pushing towards distributed architectures moving beyond traditional server-client transactions. Paired with this trend, new synergies are emerging among sensing, communications and Machine Learning (ML) co-design, where resources need to be distributed across different wireless field devices, acting as both data producers and learners. Considering this landscape, Federated Learning (FL) solutions are suitable for training a ML model in distributed systems. In particular, decentralized FL policies target scenarios where learning operations must be implemented collaboratively, without relying on the server, and by exchanging model parameters updates rather than training data over capacity-constrained radio links. This paper proposes a real-time framework for the analysis of decentralized FL systems running on top of industrial wireless networks rooted in the popular Time Slotted Channel Hopping (TSCH) radio interface of the IEEE 802.15.4e standard. The proposed framework is suitable for neural networks trained via distributed Stochastic Gradient Descent (SGD), it quantifies the effects of model pruning, sparsification and quantization, as well as physical and link layer constraints, on FL convergence time and learning loss. The goal is to set the fundamentals for comprehensive methods and procedures supporting decentralized FL pre-deployment design. The proposed tool can be thus used to optimize the deployment of the wireless network and the ML model before its actual installation. It has been verified based on real data targeting smart robotic-assisted manufacturing.",https://ieeexplore.ieee.org/document/9209305/,2020 IEEE 25th International Workshop on Computer Aided Modeling and Design of Communication Links and Networks (CAMAD),14-16 Sept. 2020,ieeexplore
10.1109/IECON.2018.8592763,A Machine Learning Approach Applied to Energy Prediction in Job Shop Environments,IEEE,Conferences,"Energy efficiency has become a great challenge for manufacturing companies. Although it is possible to improve efficiency applying new and more efficient machines, decision makers tend to look for some less expensive alternatives. In this context, the adoption of more efficient strategies during the production planning can allow the reduction in energy consumption and associated emissions. Furthermore, the current reality of manufacturing companies, brought by Industry 4.0 concepts, requires more flexibility of production systems, thus, increasing complexity for machine rescheduling without compromising sustainable requirements. In this paper, we propose a method to predict total energy consumption in job shop systems applying machine learning techniques. Different schedules may result in different consumption rates. However, there is a nonlinear relationship between these targets. Therefore, an Artificial Neural Network (ANN) is applied for a quick estimation of total energy consumption. In order to validate the model, computational experiments, using digital manufacturing software tools, are performed on different job shop configurations to show the efficiency of the proposed model.",https://ieeexplore.ieee.org/document/8592763/,IECON 2018 - 44th Annual Conference of the IEEE Industrial Electronics Society,21-23 Oct. 2018,ieeexplore
10.1109/COMPSAC48688.2020.0-202,A Modular Edge-/Cloud-Solution for Automated Error Detection of Industrial Hairpin Weldings using Convolutional Neural Networks,IEEE,Conferences,"The traction battery and the electric motor are the most important components of the electrified powertrain. To increase the energy efficiency of the electric motor, wound copper wires are being replaced by coated rectangular copper wires, so-called hairpins. Hence, to connect the hairpins conductively, they must be welded together. However, such new production processes are unknown compared with classic motor production. Therefore, this research aims to integrate Industry 4.0 techniques, such as cloud and edge computing, and advanced data analysis in the production process to better understand and optimize the manufacturing processes. Welding defects are classified with the help of a convolutional neural network (CNN) (predictive analysis) and, depending on the defect, a recommended course of action for reworking (prescriptive analysis) is given. However, the application of such complex algorithms as neural networks to large amounts of data requires huge computing resources. Therefore, a modular combination of an edge and cloud architecture is proposed in this paper. Furthermore, a pure cloud solution is compared with the edge solution.",https://ieeexplore.ieee.org/document/9202655/,"2020 IEEE 44th Annual Computers, Software, and Applications Conference (COMPSAC)",13-17 July 2020,ieeexplore
10.1109/CSCI49370.2019.00084,A Real-Time Based Intelligent System for Predicting Equipment Status,IEEE,Conferences,"In manufacturing industry, significant productivity losses arise due to equipment failures. Therefore, it is an important task to prevent the equipment from failure by monitoring each machine's sensor data in advance. However, most of the current developed systems have been only focused on monitoring the sensor data and have a difficulty in applying advanced algorithms to the real-time stream data. To address issues, we implemented an intelligent system that employs real-time streaming engine loaded with the machine learning libraries for predictive maintenance analysis. By applying a deep-learning based model to the real-time streaming data, we can provide not only trends of raw sensor data but also give an indicator representing an equipment's status in real-time. We anticipate that our system contributes to recognize the equipment's status by monitoring the indicator for productivity improvement in manufacturing industry in real-time.",https://ieeexplore.ieee.org/document/9071016/,2019 International Conference on Computational Science and Computational Intelligence (CSCI),5-7 Dec. 2019,ieeexplore
10.1109/COMPSAC.2019.10205,A Scalable Framework for Multilevel Streaming Data Analytics using Deep Learning,IEEE,Conferences,"The rapid growth of data in velocity, volume, value, variety, and veracity has enabled exciting new opportunities and presented big challenges for businesses of all types. Recently, there has been considerable interest in developing systems for processing continuous data streams with the increasing need for real-time analytics for decision support in the business, healthcare, manufacturing, and security. The analytics of streaming data usually relies on the output of offline analytics on static or archived data. However, businesses and organizations like our industry partner Gnowit, strive to provide their customers with real time market information and continuously look for a unified analytics framework that can integrate both streaming and offline analytics in a seamless fashion to extract knowledge from large volumes of hybrid streaming data. We present our study on designing a multilevel streaming text data analytics framework by comparing leading edge scalable open-source, distributed, and in-memory technologies. We demonstrate the functionality of the framework for a use case of multilevel text analytics using deep learning for language understanding and sentiment analysis including data indexing and query processing. Our framework combines Spark streaming for real time text processing, the Long Short Term Memory (LSTM) deep learning model for higher level sentiment analysis, and other tools for SQL-based analytical processing to provide a scalable solution for multilevel streaming text analytics.",https://ieeexplore.ieee.org/document/8754149/,2019 IEEE 43rd Annual Computer Software and Applications Conference (COMPSAC),15-19 Jul 2019,ieeexplore
,A Smart Camera Architecture with Keypoint Description and Hybrid Processor Population,VDE,Conferences,"Increasing local processing capability of smart camera systems enables better scene analysis and more accurate decisions, and this leads to expand their application areas such as wireless sensor networks. Image understanding in smart cameras has been enhanced by the incorporation of keypoint detection and description mechanisms. Small memory footprint and high matching speed of Local Binary Descriptors (LBD) make them suitable to use in embedded applications and they also have several hardware implementations, leading them to meet real-time constraints. However, the flexibility of such available hardware is quite limited. On the other hand, it is known that biological neural networks which are naturally capable of solving these problems outperform today's electronics systems. Mimicking these structures with Spiking Neural Networks (SNN) gives very promising results, but this implementation requires special manufacturing process and lack of cost-efficiency in the current technology. Alternatively, Cellular Neural Networks (CNN) which have low-cost digital implementations, can boost the existing image analysis capabilities. In this paper, we extend our smart camera architecture with a modified CNN-based structure with 2-different types of cells showing either inhibitory or excitatory behavior. Cell connections are determined by the cell types stored on a binary identity matrix. Thus, this flexible network can be configured to incorparate with existing keypoint detection and description blocks. In addition, it also enables further analysis with complex comparisons and multiple iterations.",https://ieeexplore.ieee.org/document/8470479/,CNNA 2018; The 16th International Workshop on Cellular Nanoscale Networks and their Applications,28-30 Aug. 2018,ieeexplore
10.1109/ICMCCE.2018.00050,A Smart Manufacturing Compliance Architecture of Electronic Batch Recording System (eBRS) for Life Sciences Industry,IEEE,Conferences,"The paradigm shift brought about by smart manufacturing or Industrie 4.0 has posed threefold challenges to electronic batch recording system (eBRS) in Life Sciences Industry: 1) the structure of the data should be informative and standard for interoperate using information models, 2) administration of synchronization between physical world and cyber world for smart decision making and optimization using cyber physical system (CPS) and 3) so-called digital manufacturing operations management (digital MOM) characterized by decentralization, comprehensive collaboration and servitization shall be implemented. Under the new situations of smart manufacturing or Industrie 4.0, the requirements from information models, CPS and digital MOM will become the most principal criteria to be considered for future eBRS/MES and other operations management information system in shop floor. To fulfill these demands, an approach combining ISA95/88 hybrid model with activities ontology and variant domain-driven design for SOA-based eBRS development has been presented. An eBRS software platform has been developed on the theoretical basis and applied to a specific application scenario of Lyophilized Injection Production for verifying its feasibility purpose.",https://ieeexplore.ieee.org/document/8537548/,"2018 3rd International Conference on Mechanical, Control and Computer Engineering (ICMCCE)",14-16 Sept. 2018,ieeexplore
10.1109/PACET48583.2019.8956270,A Smart Recycling Bin for Waste Classification,IEEE,Conferences,"As there is an obvious and increasing need to preserve valuable resources and reduce waste and pollution, several researches are focusing into this area. However, the solutions provided are neither budget-friendly nor effective to be practical in a real-world application. In this paper, we present a Smart Recycling Bin using modern approaches for waste classification. The design of the system permits the low-cost manufacturing of the final product, while uses state of the art technologies such as neural networks and the LoRaWAN protocol. We implemented a low-cost Smart Bin prototype able to classify different types of waste with an accuracy of 92.1%. The system also remotely transmits valuable data to the corresponding authorities, which can increase their effectiveness in waste management. Index Terms-Smartbin, Recycling, LoRa, Raspberry Pi, embedded system.",https://ieeexplore.ieee.org/document/8956270/,2019 Panhellenic Conference on Electronics & Telecommunications (PACET),8-9 Nov. 2019,ieeexplore
10.1109/PTC.2019.8810705,A Smart Voltage Optimization Approach for Industrial Load Demand Response,IEEE,Conferences,"This paper proposes a generic and comprehensive Voltage Optimization (VO) strategy for energy savings by industrial customers, to lower operating expenses through the implementation of an optimal process-based Demand Response (DR) program without affecting the real-time manufacturing process. This strategy takes into account the complex nature of industrial loads and their unique set of operating constraints, to reduce energy demand for industrial customers by means of varying the voltage at the utility service entrance to the plant. The proposed approach utilizes a Neural Network (NN) model of the industrial load, trained using historical operating data, to estimate the real power consumption of the load, based on the bus voltage and overall plant process. The NN load model is incorporated into the proposed VO model, whose objective is the minimization of the energy drawn from the substation and the number of switching operations of Load Tap Changers (LTC). The proposed VO framework is tested on a real plant model developed using actual measured data. The results demonstrate that the proposed technique can be successfully implemented by industrial customers and plant operators to enhance energy savings compared to Conservation Voltage Reduction (CVR) approaches, and also as a DR strategy that effectively manages the dependence of industrial loads on time-sensitive and critical manufacturing processes.",https://ieeexplore.ieee.org/document/8810705/,2019 IEEE Milan PowerTech,23-27 June 2019,ieeexplore
10.1109/ICAIS50930.2021.9395838,A Survey on Future of Augmented Reality with AI in Education,IEEE,Conferences,"Augmented reality now has reached the stage in real time simulations which needed and contemplated. In the year 2020 and forthcoming years to 2030 augmented reality is going to play a lead role in most of industries, which includes military, education, medical, manufacturing industries, training and remote assistant, navigation, and gaming to name a few. This all-new entire article brings out the broad meaning of augmented reality sorted into a detailed survey of how it's going to be a cornerstone of the new education system and how this can used in other sectors too. The core contribution of this entire article is the discussion of recent studies on represents of the present state of field, which seek to inspire educators to improve mixed reality experiences and to carry out further more research in favor of interactive learning environments.",https://ieeexplore.ieee.org/document/9395838/,2021 International Conference on Artificial Intelligence and Smart Systems (ICAIS),25-27 March 2021,ieeexplore
10.1109/SMC.2019.8914195,A Universal Methodology to Create Digital Twins for Serial and Parallel Manipulators,IEEE,Conferences,"With the technological advances in information technology especially in sensorization, artificial intelligence, big data and visualization; smart manufacturing and industrie 4.0 are gradually becoming an implementable reality. Digital twin is one of the pillars of smart manufacturing where by the physical and virtual worlds can by synced and mimic each others' behaviour. In future, assets and products can sense their state and report back any anomalities so that meaningful insights can be drawn and actions can be taken to keep production optimized at all times. This reporting feature can be realized with the help of digital twin technology as the twin keeps record of the physical asset's behavior. A digital twin will play an integral role in defining the concept of an integrated shopfloor. It will assist in viewing holistic behavior of asset, optimizing processes and exerting control over the physical device. To demonstrate the concept of digital twin, an universal methodology was developed and deployed at Model Factory @ ARTC (Advanced Remanufacturing and Technology Centre) program in Singapore.",https://ieeexplore.ieee.org/document/8914195/,"2019 IEEE International Conference on Systems, Man and Cybernetics (SMC)",6-9 Oct. 2019,ieeexplore
10.1109/PacificVis.2018.00026,A Visual Analytics Approach for Equipment Condition Monitoring in Smart Factories of Process Industry,IEEE,Conferences,"Monitoring equipment conditions is of great value in manufacturing, which can not only reduce unplanned downtime by early detecting anomalies of equipment but also avoid unnecessary routine maintenance. With the coming era of Industry 4.0 (or industrial internet), more and more assets and machines in plants are equipped with various sensors and information systems, which brings an unprecedented opportunity to capture large-scale and fine-grained data for effective on-line equipment condition monitoring. However, due to the lack of systematic methods, analysts still find it challenging to carry out efficient analyses and extract valuable information from the mass volume of data collected, especially for process industry (e.g., a petrochemical plant) with complex manufacturing procedures. In this paper, we report the design and implementation of an interactive visual analytics system, which helps managers and operators at manufacturing sites leverage their domain knowledge and apply substantial human judgements to guide the automated analytical approaches, thus generating understandable and trustable results for real-world applications. Our system integrates advanced analytical algorithms (e.g., Gaussian mixture model with a Bayesian framework) and intuitive visualization designs to provide a comprehensive and adaptive semi-supervised solution to equipment condition monitoring. The example use cases based on a real-world manufacturing dataset and interviews with domain experts demonstrate the effectiveness of our system.",https://ieeexplore.ieee.org/document/8365986/,2018 IEEE Pacific Visualization Symposium (PacificVis),10-13 April 2018,ieeexplore
10.1109/ISCAS.2010.5537177,A camera based closed loop control system for keyhole welding processes: Algorithm comparison,IEEE,Conferences,"Real time monitoring of laser welding has a more and more importance in several manufacturing processes ranging from automobile production to precision mechanics. Despite the huge improvement in welding technology, sophisticated image based closed loop control systems have not been integrated in commercially available equipments yet. Due to the high dynamics of laser beam welding (LBW) processes, robust closed loop control systems require fast real time image processing with frame rates in the multi kilo Hertz range. In the last few years, some new high speed Cellular Neural Network (CNN) based algorithms for the full penetration hole detection in keyhole welding processes have been introduced. In particular, they can be distinguished in two categories: Orientation dependent and orientation independent algorithms. The former can be used only for the welding of straight lines, while the latter has been implemented for the control of curved weld seams. Both algorithms have been used to build up a real time closed loop control system for LBW processes. An algorithm comparison by the description of some experimental results is addressed in this paper.",https://ieeexplore.ieee.org/document/5537177/,Proceedings of 2010 IEEE International Symposium on Circuits and Systems,30 May-2 June 2010,ieeexplore
10.1049/cp:19940661,A cognitive engineering approach with AI techniques to reactive scheduling in the supervision of dynamic manufacturing processes,IET,Conferences,"Recent advances in artificial intelligence (AI) techniques have greatly increased the potential for applying them to engineering intelligent real-time control and scheduling systems as joint man-machine cognitive systems. Rapid reactivity is becoming an increasingly important competitive factor for today's manufacturing companies in planning and controlling their shop floor processes. The construction and revision of shop floor schedules in such a way to keep them in line with rapidly changing events can significantly improve the manufacturing systems' performance in dynamic environments with many uncertainties in processes. Reactive scheduling as one major function of an intelligent supervisory control system for dynamic manufacturing processes is presented in the paper from functional-decisional, AI-based cognitive engineering and related performance aspects.&lt;<ETX>&gt;</ETX>",https://ieeexplore.ieee.org/document/332000/,"Second International Conference on Intelligent Systems Engineering, 1994",5-9 Sept. 1994,ieeexplore
10.1109/BigData.2017.8258116,A data-driven approach for improving sustainability assessment in advanced manufacturing,IEEE,Conferences,"Sustainability assessment (SA) has been one of the prime contributors to advanced manufacturing analysis, and it traditionally involves life cycle assessment (LCA) techniques for retrospective and prospective evaluations. One big challenge to reach a reliable sustainability assessment comes from the inadequate understandings of the underlying activities related to each of the product lifecycle stages based on expert knowledge. Data-driven modeling, on the other hand, is an emerging approach that takes advantage of machine-learning methods in building models that would complement or replace the knowledge-based models capturing physical behaviors. Incorporating suitable data analytics models to utilize real-time product and process data could significantly improve LCA techniques. To address the complexity and uncertainty involved in multilevel SA decision-making activities, this paper proposes a modular LCA framework to accommodate a hybrid modeling paradigm that includes knowledge-based and data-driven models. We identify and emphasize on two important challenges: (1) Generalizing knowledge-based and data-driven models into analytics models so that they can be uniformly deployed and interchanged, and (2) Modularizing the LCA decision logics and model structures so that the LCA decision process can be streamlined and easily maintained. The issues related to the decomposition, standardization, deployment and execution of analytics models are discussed in this paper. Three well-adopted standards - STEP (Standard for the Exchange of Product model data), DMN (Decision Model and Notation), and PMML (Predictive Model Markup Language) are employed to capture the product-related data/information, the decision logic decomposition of analytics models, and the structure decomposition of analytics models, respectively. The feasibility and benefits of the proposed modular, hybrid sustainability assessment methodology have been illustrated with an injection molding case study, incorporating an overall modular Scorecard-based LCA architecture with a Bayesian Network predictive model.",https://ieeexplore.ieee.org/document/8258116/,2017 IEEE International Conference on Big Data (Big Data),11-14 Dec. 2017,ieeexplore
10.1109/FUZZ-IEEE.2012.6251167,A fuzzy ontology approach for representing Fuzzy Petri Nets,IEEE,Conferences,"Petri Net (PN) has proven to be quite effective tool for graphical modeling, mathematical modeling, simulation, and real time control by the use of places and transitions. However, information imprecision and uncertainty exist in many real-world applications, and PNs found themselves inadequate to address the problems of imprecision and uncertainty in data. Therefore, the Fuzzy Petri Net (FPN) was developed and had been employed in many different fields like communication, manufacturing, electronics, and etc. In particular, with the wide utilization of FPNs, many researchers suggest that FPNs should be reused and shared. Emerging the Semantic Web technologies, such as fuzzy ontology, can play an important role in this scenario. On this basis, in this paper, we propose a fuzzy ontology approach for representing FPNs. First, we propose a formal definition of FPNs. Then, we give a complete definition of fuzzy OWL ontologies, where fuzzy ontologies formulated in fuzzy OWL language are called fuzzy OWL ontologies. Based on the formalization of FPNs and fuzzy OWL ontologies, we further propose a fuzzy ontology approach for representing FPNs, where we translate the key features of FPNs into the elements of fuzzy OWL ontologies such as fuzzy classes, fuzzy properties, fuzzy individuals, and fuzzy axioms. Finally, based on the translated fuzzy ontologies, we briefly discuss how to reason on FPNs through the reasoning mechanism of fuzzy ontologies.",https://ieeexplore.ieee.org/document/6251167/,2012 IEEE International Conference on Fuzzy Systems,10-15 June 2012,ieeexplore
10.1109/AIHAS.1990.93923,A hierarchical and modular structure for FMS control and monitoring,IEEE,Conferences,"A hierarchical modular structure is presented for real-time control and monitoring of flexible manufacturing systems (FMSs), in which a clear distinction between processing of normal and unusual states is made. It is shown that this approach furthers autonomous diagnosis and recovery, because it makes it possible to integrate two techniques well suited in their own domains: Petri nets for the specification of normal sequences of control and AI techniques for dealing with diagnostic problems. The implementation techniques proposed are similar to rapid prototyping by execution of the specification (token player for Petri nets and inference engine for production rules).&lt;<ETX>&gt;</ETX>",https://ieeexplore.ieee.org/document/93923/,"Proceedings [1990]. AI, Simulation and Planning in High Autonomy Systems",26-27 March 1990,ieeexplore
10.1109/ROBOT.1999.772469,A multi-contract net protocol for dynamic scheduling in flexible manufacturing systems (FMS),IEEE,Conferences,"Deals with a multi-agent architecture and a negotiation protocol for the dynamic scheduling of flexible manufacturing systems. The originality of the multi-agent architecture resides in the existence of task agents and resource agents. The multi-contract net protocol proposed, is an innovation with regard to the contract-net protocol due to the way that it makes it possible to negotiate several tasks concurrently, in real time and with more optimal results taking into account uncertainty and conflict situations in the scheduling of operations. The paper, also, stresses the efficiency and the optimality of the distributed implementation.",https://ieeexplore.ieee.org/document/772469/,Proceedings 1999 IEEE International Conference on Robotics and Automation (Cat. No.99CH36288C),10-15 May 1999,ieeexplore
10.1109/ROBOT.2004.1308024,A real-time monitoring and diagnosis system for manufacturing automation,IEEE,Conferences,"Condition monitoring and fault diagnosis in modern engineering practices is of great practical significance for improving the quality and productivity, preventing the machinery from damages. In general, this practice consists of two parts: extracting appropriate features from sensor signals and recognizing possible faulty patterns from the features. In order to cope with the complex manufacturing operations and develop a feasible system for real-time application, we proposed three approaches. By defining the marginal energy, a new feature representation emerged, while by real-time learning algorithms with support vector techniques and hidden Markov model representations, a modular software architecture and a new similarity measure were developed for comparison, monitoring, and diagnosis. A novel intelligent computer-based system has been developed and evaluated in over 30 factories and numerous metal stamping processes as an example of manufacturing operations. The real-time operation of this system demonstrated that the proposed system is able to detect abnormal conditions efficiently and effectively resulting in a low-cost, effective approach to real-time monitoring in manufacturing. The related technologies have been transferred to industry, presenting a tremendous impact in current automation practice in Asia and the world.",https://ieeexplore.ieee.org/document/1308024/,"IEEE International Conference on Robotics and Automation, 2004. Proceedings. ICRA '04. 2004",26 April-1 May 2004,ieeexplore
10.1109/ATEE.2013.6563523,A recognition system of components from a production line using neuronal networks,IEEE,Conferences,"We developed and implemented a system for real time automatic recognition of components from a production line using neuronal networks. The capture device (Web camera) is placed over the production line and the system can identify the type of component even if this is not in the correct position or centered. The device sends the information to a recognition system (software) which identifies the type of component and its parts. Once all parts of the component are identified by the system, the operator is informed about the type of component and this is recorded in the fabrication's report. When a manufacturing defect is observed, the system alerts the operator and the component is recorded as inappropriate. This system can be used for real time recognition of the components. In addition, if the component is an electrical board for example, the system can also decide if all components are present in the correct position.",https://ieeexplore.ieee.org/document/6563523/,2013 8TH INTERNATIONAL SYMPOSIUM ON ADVANCED TOPICS IN ELECTRICAL ENGINEERING (ATEE),23-25 May 2013,ieeexplore
10.1109/ETFA.2003.1248742,A reference architecture and functional model for monitoring and diagnosis of large automated systems,IEEE,Conferences,"This research develops a reference architecture and functional model for intelligent monitoring and fault diagnosis of large complex automated systems in manufacturing and logistics. This reference architecture organises the monitoring and diagnosis functions in a modified hierarchical manner with multiple levels, and is therefore easily scalable to meet growing requirements of different application scenarios. The architecture is efficient as it allows problems to be quickly dealt with closer to their sources; therefore minimising intra-level data communication and messaging. Similarly the proposed functional model for monitoring and diagnosis unit can be adopted (i.e., scaled up or down) to suit the needs of the application, and indeed it fits well into the proposed reference architecture. A successful case of applying the proposed architecture and model is presented which serves to illustrate how they can be implemented in real-life to solve a class of monitoring and diagnosis problems for large automated systems typically found in manufacturing and logistics.",https://ieeexplore.ieee.org/document/1248742/,EFTA 2003. 2003 IEEE Conference on Emerging Technologies and Factory Automation. Proceedings (Cat. No.03TH8696),16-19 Sept. 2003,ieeexplore
10.1109/BigData.2015.7364098,A scalable solution for group feature selection,IEEE,Conferences,"In many applications, we may want to build a classifier with high confidence, while reducing the number of features. We consider the case where features are assigned to predefined groups and cannot be removed individually. An additional and important constraint is that the datasets may be very large and may not fit in memory. We use logistic regression with group penalty, which results in sparse solutions at the group level. In our implementation, we apply L-BFGS to approximate the quadratic loss function of logistic regression and use Block Co-ordinate Descent to solve for each group. Our contributions can be summarized as follows: (1) we discuss different scalable approaches, depending on characteristics of the dataset, such as, large number of data points or large number of features or large number of groups; (2) for datasets with large number of data points and few groups of features, we identify the bottlenecks for scalability; (3) we present Spark solutions in Python and discuss the advantages of our solution over alternate solutions; (4) we present the experiments and results on synthetic data and real data from manufacturing applications.",https://ieeexplore.ieee.org/document/7364098/,2015 IEEE International Conference on Big Data (Big Data),29 Oct.-1 Nov. 2015,ieeexplore
10.1109/ITAIC.2011.6030279,A solution of dynamic manufacturing resource aggregation in CPS,IEEE,Conferences,"Market diversification and economic globalization bring various uncertainties to the completion of manufacturing task. It requires manufacturing resource aggregation (MRA) could adjust to dynamic changing factors (DCFs) that happen during the life cycle of MRA. The emerging Cyber Physical Systems (CPS), in which the real-time states of physical resources are fully sensed, enable to identify the DCFs in time. This supports to realize the real dynamic MRA. Take CPS as background, the paper focuses on dynamic MRA which includes two key issues, manufacturing resource virtualization model (MRVM) and dynamic service composition. In order to give a complete manufacturing resource view, MRVM which contains both static and dynamic information is built. Based on the analysis towards life cycle of MRA, the paper proposes a dynamic MRA architecture. And as the core parts of it, the implementation process of manufacturing resource service composition and a multi-level dynamic adjustment strategy are described in detail. At last, we make a conclusion.",https://ieeexplore.ieee.org/document/6030279/,2011 6th IEEE Joint International Information Technology and Artificial Intelligence Conference,20-22 Aug. 2011,ieeexplore
10.1109/ICIAS.2007.4658557,A study on industrial communication networking: Ethernet based implementation,IEEE,Conferences,"Recent enhancement of the industrial communications and networking are possible to apply in Ethernet networks system at all levels of industrial automation, especially in the controller level whereby the data exchanges in real-time communication is mandatory. This paper is about a study on the development of industrial communications network based on the Ethernet protocol and thus implement it into computer integrated manufacturing (CIM) system. The purpose of this paper is to overcome real-time communication in which the accessibility of data exchange is very difficult in terms of retrieving data from other stations and time consuming. The Ethernet module is installed onto supervisory OMRON PLC to integrate several of stations in the CIM-70A system which is located at Robotic Laboratory in Universiti Tun Hussein Onn Malaysia (UTHM). The workability of this communication technique is analyzed and compared with the conventional serial communication which widely used in automation networking systems. It is found that, the Ethernet protocol approach through the communication and integration of CIM system can be accessed easily and available to be upgraded at the management and enterprise levels of industrial automation system.",https://ieeexplore.ieee.org/document/4658557/,2007 International Conference on Intelligent and Advanced Systems,25-28 Nov. 2007,ieeexplore
10.1109/SNPDWinter52325.2021.00046,Abrupt covariance based signal extraction for fault prediction of an aircraft engine,IEEE,Conferences,"With the advent of the 4<sup>th</sup> industrial revolution, sensor and data acquisition technology and Internet of Things have advanced, and several analog sensor signals and control commands are automatically monitored in real time. Although numerous manufacturing datasets provide us the bases or evidences for predicting faults in electromechanical systems, they are associated with high dimensionality, which complicates signal analyses. Especially, analog sensor signals from electrotechnical systems are too scattered to detect and investigate the linear or gradient relationship between the signal data and the system state. Therefore, we proposed an abrupt covariance-based signal extraction method, which is a combination of abrupt variance analysis and partial least square regression, for fault prediction. Based on the proposed signal extraction method, we can select the most important features to predict the remaining useful life of an aircraft engine. The proposed extraction approach improves the fault prediction results when numerous analog sensors are used for fault prediction.",https://ieeexplore.ieee.org/document/9403524/,"2021 21st ACIS International Winter Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing (SNPD-Winter)",28-30 Jan. 2021,ieeexplore
10.1109/BigData.2014.7004408,Advanced planning and control of manufacturing processes in steel industry through big data analytics: Case study and architecture proposal,IEEE,Conferences,"Enterprises in today's globalized world are compelled to react on threats and opportunities in a highly flexible manner. Hence, companies that are able to analyze the current state of their business processes, forecast their most optimal progresses and with this proactively control them will have a decisive competitive advantage. Technological progress in sensor technology has boosted real-time situation awareness, especially in manufacturing operations. The paper at hands examines, based on a case study stemming from the steel manufacturing industry, which production-related data is collectable using state of the art sensors forming a basis for a detailed situation awareness and for deriving accurate forecasts. However, analyses of this data point out that dedicated big data analytics approaches are required to utilize the full potential out of it. By proposing an architecture for predictive process planning and control systems, the paper intends to form a working and discussion basis for further research and implementation efforts in big data analytics.",https://ieeexplore.ieee.org/document/7004408/,2014 IEEE International Conference on Big Data (Big Data),27-30 Oct. 2014,ieeexplore
10.1109/DCOSS49796.2020.00046,An Agnostic Data-Driven Approach to Predict Stoppages of Industrial Packing Machine in Near,IEEE,Conferences,"As data awareness in manufacturing companies increases with the deployment of sensors and Internet of Things (IoT) devices, data-driven maintenance and prediction have become quite popular in the Industry 4.0 paradigm. Machine Learning (ML) has been recognised as a promising, efficient and reliable tool for fault detection use cases, as it allows to export important knowledge from monitored assets. Scientists deal with issues such as the small amount of data that indicate potential problems, or the imbalance which exists between the standard process data and the data inadequacy of the systems to make a high precision forecast. Currently, in this context, even large industries are not able to effectively predict abnormal behaviors in their tools, processes and equipment, when adopting strategies to anticipate crucial events. In this paper, we propose a methodology to enable prediction of a packing machine's stoppages in manufacturing process of a large industry, by using forecasting techniques based on univariate time series data. There are more than 100 reasons that cause the machine to stop, in a quite big production line length. However, we use a single signal, concerning the machines operational status to make our prediction, without considering other fault or warning signals, hence its characterization as ""agnostic"". A workflow is presented for cleaning and preprocessing the data, and for training and evaluating a predictive model. Two predictive models, namely ARIMA and Prophet, are applied and evaluated on real data from an advanced machining process used for packing. Training and evaluation tests indicate that the results of the applied methods perform well on a daily basis. Our work can be further extended and act as reference for future research activities that could lead to more robust and accurate prediction frameworks.",https://ieeexplore.ieee.org/document/9183540/,2020 16th International Conference on Distributed Computing in Sensor Systems (DCOSS),25-27 May 2020,ieeexplore
10.1109/ECTC32696.2021.00346,An Automated Optical Inspection System for PIP Solder Joint Classification Using Convolutional Neural Networks,IEEE,Conferences,"In the fields of electronics manufacturing, the application of through-hole devices is still required, as heat dissipation and high current carrying capacity plays an important role. To ensure the highest quality standards, these electronics production processes take a multitude of inspection processes into account. For the detection of error patterns regarding the quality of the solder connections, usually, high-end inspection machines are utilized in the industrial application. The Automated Optical Inspection is a commonly conducted process, using visible light and rule-based inspection routines, setup by process experts for the evaluation of the Region of Interest. The high overhead of creating and maintaining product-specific checking routines and machine acquisition leads to increased costs and severe dependency on expert know-how. A flexible inspection algorithm, implemented into low-cost equipment for image generation is expected to reduce acquisition and optimization costs, and lower dependency on expert knowledge and high-end machinery. In this contribution, we present a novel framework for the automatic, near real-time solder joint classification based on Convolutional Neural Networks, flexibly detecting, and classifying solder connections. We utilize existing Deep Learning architectures for detection and classification. The localization model utilizes a YOLO-architecture (you-only-look-once), learning feature inputs based on a supervised learning approach. Pseudo-labeling is carried out automatically by an anomaly detection model. The image generation is executed by an industrial low-cost camera and an industrial rack-PC. The developed prototype is integrated into the existing production infrastructure. The results indicate a satisfactory detection and classification of the investigated solder connections with the proposed system. Hence, this system represent an alternative to commercially available high-end inspection systems being used for an inline control of Pin-in-Paste and through-hole device solder connections.",https://ieeexplore.ieee.org/document/9501916/,2021 IEEE 71st Electronic Components and Technology Conference (ECTC),1 June-4 July 2021,ieeexplore
10.1109/ICDCS.2019.00139,An Industrial IoT Solution for Evaluating Workers' Performance Via Activity Recognition,IEEE,Conferences,"The Industrial Internet of Things (IIoT) is a key pillar of the Fourth Industrial Evolution or Industry 4.0. It aims to achieve direct information exchange between industrial machines, people, and processes. By tapping and analysing such data, IIoT can more importantly provide for significant improvements in productivity, product quality, and safety via proactive detection of problems in the performance and reliability of production machines, workers, and industrial processes. While the majority of existing IIoT research is currently focusing on the predictive maintenance of industrial machines (unplanned production stoppages lead to significant increases in costs and lost plant productivity), this paper focuses on monitoring and assessing worker productivity. This IIoT research is particularly important for large manufacturing plants where most production activities are performed by workers using tools and operating machines. With this aim, this paper introduces a novel industrial IoT solution for monitoring, evaluating, and improving worker and related plant productivity based on workers activity recognition using a distributed platform and wearable sensors. More specifically, this IIoT solution captures acceleration and gyroscopic data from wearable sensors in edge computers and analyses them in powerful processing servers in the cloud to provide a timely evaluation of the performance and productivity of each individual worker in the production line. These are achieved by classifying worker production activities and computing Key Performance Indicators (KPIs) from the captured sensor data. We present a real-world case study that utilises our IIoT solution in a large meat processing plant (MPP). We illustrate the design of the IIoT solution, describe the in-plant data collection during normal operation, and present the sensor data analysis and related KPI computation, as well as the outcomes and lessons learnt.",https://ieeexplore.ieee.org/document/8884821/,2019 IEEE 39th International Conference on Distributed Computing Systems (ICDCS),7-10 July 2019,ieeexplore
10.1109/COASE.2018.8560557,An Industry 4.0 Cyber-Physical Framework for Micro Devices Assembly,IEEE,Conferences,"An advanced cyber manufacturing framework to support the collaborative assembly of micro devices is presented based on Industry 4.0 principles. The distributed cyber and physical components work together to plan, assemble and monitor micro assembly related tasks; micro assembly refers to the assembly of micron sized devices which cannot be manufactured by MEMS technologies. The collaborative framework proposed includes assembly planning and path planning modules, Virtual Reality based assembly simulation environments and physical assembly work cells. An ontology based approach was implemented to address semantic interoperability issues to support formation of temporary partnerships in a Virtual Enterprise context. The key to the design and implementation of this complex framework is an information centric process modeling approach which provides a data/information oriented basis for collaboration. A collaborative cyber physical test bed has been built to demonstrate feasibility of proposed framework and approach.",https://ieeexplore.ieee.org/document/8560557/,2018 IEEE 14th International Conference on Automation Science and Engineering (CASE),20-24 Aug. 2018,ieeexplore
10.1109/CSCWD.2012.6221798,An agent-based dynamic scheduling approach for flexible manufacturing systems,IEEE,Conferences,"The paper presents a dynamic scheduling approach for flexible manufacturing systems (FMS). The scheduling approach is implemented based on the negotiation and collaboration between agents in a multi-agent system (MAS) which represents the FMS. Through the collaboration between the agents in the MAS, the system exhibits the behavior that response to the disruption caused by dynamic events arise randomly in FMS such as jobs arrive over time and machines breakdown in real time and globally. The scheduling and controlling process is done on-line, without interrupting the system's operation and without user intervention. An experiment is conducted to evaluate the efficiency of the scheduling strategies exhibited by the proposed agent-based scheduling approach. The results demonstrate the superiority of the suggested scheduling approach as well as its capacity to cope with a fast changing environment.",https://ieeexplore.ieee.org/document/6221798/,Proceedings of the 2012 IEEE 16th International Conference on Computer Supported Cooperative Work in Design (CSCWD),23-25 May 2012,ieeexplore
10.1109/ICPHYS.2018.8390779,An approach for implementing key performance indicators of a discrete manufacturing simulator based on the ISO 22400 standard,IEEE,Conferences,"Performance measurement tools and techniques have become very significant in today's industries for increasing the efficiency of their processes in order to face the competitive market. The first step towards performance measurement is the real-time monitoring and gathering of the data from the manufacturing system. Applying these performance measurement techniques on real-world industry in a way that is more general and efficient is the next challenge. This paper presents a methodology for implementing the key performance indicators defined in the ISO 22400 standard-Automation systems and integration, Key performance indicators (KPIs) for manufacturing operations management. The proposed methodology is implemented on a multi robot line simulator for measuring its performance at runtime. The approach implements a knowledge-based system within an ontology model which describes the environment, the system and the KPIs. In fact, the KPIs semantic descriptions are based on the data models presented in the Key Performance Indicators Markup Language (KPIML), which is an XML implementation of models developed by the Manufacturing Enterprise Solutions Association (MESA) international organization.",https://ieeexplore.ieee.org/document/8390779/,2018 IEEE Industrial Cyber-Physical Systems (ICPS),15-18 May 2018,ieeexplore
10.1109/IS.2012.6335231,An intelligent fuzzy-Petri reasoning supervisory control for FMS manufacturing plants,IEEE,Conferences,"The organization level of complex automated robotic and manufacturing systems, whose dynamics is discrete-event dominated, is revisited and some innovative results towards enhancing machine intelligence derived. These comprise a fuzzy-Petri-net reasoning emulator implementing the fuzzy-Petri rule-based decision-making with an appropriate knowledge base support. This fuzzy-Petri reformulation follows the original concept of entropy-based, intelligent machines of G.N. Saridis. Its generic feature in the sense of an improved fuzzy-Petri reasoning based organizing coordination controller is crucial. A software tool emulating the fuzzy-Petri-net reasoning is discussed. Its version employing knowledge-base with 10 rules takes only a few milliseconds on a standard PC to be simulated. Thus it is believed a feasible yet efficient real-time reasoning for supervisory control functions has been developed. Some computer simulations results for FMS applications are given that illustrate these novel developments.",https://ieeexplore.ieee.org/document/6335231/,2012 6th IEEE International Conference Intelligent Systems,6-8 Sept. 2012,ieeexplore
10.1109/INDIN.2005.1560474,An intelligent immune agent management system for facilitating e-diagnosis in manufacturing,IEEE,Conferences,"This paper focuses on designing and implementing an agent framework for the purposes of electronic prognostics of critical tools and machinery in the plant. It deals with dual communication i.e. one originating from the sensors and relayed to various reception centers and the other originating from an intelligent prognostics engine (IPE) being fed back to the control mechanism (manual/automated) of the machinery. The paper presents the feasibility of deploying agents in embedded and supervisory controllers for collating, managing and communicating information. The conceptual design and prototype model establishes the efficacy of the agent framework in diagnostics as well as prognostics. The framework is developed using Java agent development framework (JADE). This paper concentrates on development of peer agent communication, communication ontologies; agent behaviors based on messaging performatives, integration issues and graphical user interfaces for diverse agents and human interfaces. It also evolves a conceptual agent framework for manufacturing environment. The work can potentially improve reliability: reduce equipment downtime by using intelligence and contextual information from sensors and events. It facilitates the timely availability of the critical data for real-time monitoring and corrective signals, as well as a transparent and rapid configuration mechanism for intelligent monitoring.",https://ieeexplore.ieee.org/document/1560474/,"INDIN '05. 2005 3rd IEEE International Conference on Industrial Informatics, 2005.",10-12 Aug. 2005,ieeexplore
10.1109/ISWCS.2018.8491060,Analysis of Machine Learning Algorithms for Spectrum Decision in Cognitive Radios,IEEE,Conferences,"Technological advances in recent years have reduced the manufacturing costs of wireless devices, increasing the number of such devices and applications. Most of these applications are supported by ISM (Industrial, Scientific, and Medical) frequencies, which due to their wide use in several types of devices have suffered from harmful interference. To solve this problem, Cognitive Radios paradigm has been proposed to guarantee the quality of communication. Several frameworks were proposed for the development of a Cognitive Radios Networks (CRN), but none of them were effectively implemented in hardware. This paper presents an analysis of machine learning algorithms in architecture for the development of CRN in real hardware. Results demonstrated the feasibility of the architecture and the decision methods based on machine learning algorithms can find the best communication channel.",https://ieeexplore.ieee.org/document/8491060/,2018 15th International Symposium on Wireless Communication Systems (ISWCS),28-31 Aug. 2018,ieeexplore
10.1109/CASE48305.2020.9216855,Anomaly detection and prediction in discrete manufacturing based on cooperative LSTM networks,IEEE,Conferences,"Manufacturing processes are characterized by their temporal and spatial distributed nonlinear physics. Analytical models are not available and numerical models do not incorporate abnormal process effects that are not known to the engineer. These unknown anomalies cause reduced process stability and fluctuant product quality. To tackle the problem, numerous approaches for anomaly detection based on neural networks have been developed over the years. Long short-term memory (LSTM) networks have also been investigated intensively for prediction purposes. Current approaches lack in the capability of constructing prediction models for both process and anomaly behavior. Furthermore, they do not deliver a solution for short-term as well as long-term anomalies. Hence, the current paper presents a novel detection and prediction procedure based on a LSTM architecture to cooperatively predict process outputs and anomalies by using two separate but interacting models. The anomaly detector is realized as stacked LSTM auto-encoder and the cooperative prediction models are based on sequence-to-sequence networks with gated recurrent units for short-term and LSTM for long-term effects. The approach is evaluated within a real industrial environment by means of a production plant for hot forging at a German automotive supplier for metal components.",https://ieeexplore.ieee.org/document/9216855/,2020 IEEE 16th International Conference on Automation Science and Engineering (CASE),20-21 Aug. 2020,ieeexplore
10.23919/IConAC.2019.8895095,Ant Colony Optimization Algorithm for Industrial Robot Programming in a Digital Twin,IEEE,Conferences,"Advanced manufacturing that is adaptable to constantly changing product designs often requires dynamic changes on the factory floor to enable manufacture. The integration of robotic manufacture with machine learning approaches offers the possibility to enable such dynamic changes on the factory floor. While ensuring safety and the possibility of losses of components and waste of material are against their usage. Furthermore, developments in design of virtual environments makes it possible to perform simulations in a virtual environment, to enable human-in-the-loop production of parts correctly the first time like never before. Such powerful simulation and control software provides the means to design a digital twin of manufacturing environment in which trials are completed at almost at no cost. In this paper, ant colony optimization is used to program an industrial robot to avoid obstacles and find its way to pick and place objects during an assembly task in an environment containing obstacles that must be avoided. The optimization is completed in a digital twin environment first and movements transferred to the real robot after human inspection. It is shown that the proposed methodology can find the optimal solution, in addition to avoiding collisions, for an assembly task with minimum human intervention.",https://ieeexplore.ieee.org/document/8895095/,2019 25th International Conference on Automation and Computing (ICAC),5-7 Sept. 2019,ieeexplore
10.1109/ETFA.1995.496803,Anytime scheduling with neural networks,IEEE,Conferences,"Scheduling techniques have been intensively studied by several research communities and have been applied to a wide range of applications in computer and manufacturing environments. In computer systems, scheduling is an important approach to address real-time constraints associated with a set of computing tasks to be executed on one or several computers. Most of the scheduling problems are NP-hard, which is why heuristic and approximation algorithms must be used for large problems. Obviously these methods are of interest when they provide near optimal solutions with a polynomial computational complexity. This paper presents results for scheduling a set of nonpreemptive tasks by using a Hopfield neural network model. We present in this paper how this approach can solve scheduling problems following the ""anytime"" paradigm.",https://ieeexplore.ieee.org/document/496803/,Proceedings 1995 INRIA/IEEE Symposium on Emerging Technologies and Factory Automation. ETFA'95,10-13 Oct. 1995,ieeexplore
10.1109/ICEKIM52309.2021.00040,Application of Teaching Innovation Based on robotics engineering,IEEE,Conferences,"As the core major of “Internet + Industrial Intelligence”, robotics engineering is an upgrade and reconstruction of traditional engineering major. The industrial robot course is the professional core course of the Robotics Engineering. It is also a comprehensive course of multi-discipline integration, which involved mechanical engineering, automatic control, computer, sensor, electronic technology, artificial intelligence and other multi-disciplinary content. Robotics Engineering is characterized by broad foundation, great difficulty, emphasis on practice, rapid development and application of new knowledge. In the process of implementation of the teaching innovation, the new concept of engineering education was applied to propose a new form of curriculum system. Taking the projects of engineering as the study objects, disassemble the knowledge points involved in industrial robots, break the course boundaries, reshape the knowledge system, draw knowledge maps and then design teaching activities. In teaching innovation, teachers extend classroom through formation of subject competition teams, promote teaching and promote learning by competition, realize the integration of “teaching, class and competition”, build a bridge between theory and practice, then complete the transformation from knowledge learning to ability training. Besides, they also keep contact with intelligent manufacturing enterprises in Zhuhai and the Bay Area to obtain real-time new developments in enterprises. Thus, the latest information was introduced into classroom. Therefore, the meaning of “production, teaching, research and application” has been deepened. According to the characteristics of the knowledge points of the course, experts were invited to make special lectures for students which can bring them with international perspective and frontier knowledge.",https://ieeexplore.ieee.org/document/9479656/,"2021 2nd International Conference on Education, Knowledge and Information Management (ICEKIM)",29-31 Jan. 2021,ieeexplore
10.1109/ISIE.1993.268784,Application of fuzzy sets in the processing of linguistically expressed uncertainty,IEEE,Conferences,"In connection with the progressive introduction of highly automated manufacturing systems and CIM systems, much attention in the last two decades has been paid to the investigation of the potential formalization of manual, routine-mental and partly also creative human activity (operator, dispatcher, designer). There has been a search for software tools that achieve individual results like the 'real' human expert mainly on the basis of symbolic considerations with the use of heuristic procedures. Decision support systems and expert systems appear to be very effective tools in the realization of the obtained algorithms and formalized decision making procedures.&lt;<ETX>&gt;</ETX>",https://ieeexplore.ieee.org/document/268784/,ISIE '93 - Budapest: IEEE International Symposium on Industrial Electronics Conference Proceedings,1-3 June 1993,ieeexplore
10.1109/SKIMA.2011.6089986,Application of learning pallets for real-time scheduling by use of artificial neural network,IEEE,Conferences,"Generally, this paper deals with the problem of autonomy in logistics. Specifically here, a complex problem in inbound logistics is considered as real-time scheduling in a stochastic shop floor problem. Recently, in order to comply with real-time decisions, autonomous logistic objects have been suggested as an alternative. Since pallets are common used objects in carrying materials (finished or semi-finished), so they have the possibility to undertake the responsibility of real time dispatching jobs to machines in a shop-floor problem. By insisting on the role of pallets for this task, their sustainment's advantage in manufacturing systems motivated the idea of developing learning pallets. These pallets may deal with uncertainties and sudden changes in the assembly system. Here, among some intelligent techniques artificial neural network is selected to transmit the ability of decision making as well as learning to the pallets, as distributed objects. Besides, pallets make decisions based on their own experiences about the entire system and local situations. Consequently, the considered scheduling problem resembles an open shop problem with three alternative finished products. Finally, a discrete event simulation model is developed to solve this problem and defined the results of this transmission paradigm.",https://ieeexplore.ieee.org/document/6089986/,"2011 5th International Conference on Software, Knowledge Information, Industrial Management and Applications (SKIMA) Proceedings",8-11 Sept. 2011,ieeexplore
10.1049/cp.2011.0486,Artificial intelligence technologies in business and engineering,IET,Conferences,"Artificial intelligence (AI) is making its way back into the mainstream of corporate technology, this time at the core of business systems which are providing competitive advantage in all sorts of industries, including electronics, manufacturing, marketing, human resource, financial services software, medicine, entertainment, engineering and communications. Designed to leverage the capabilities of humans rather than replace them, today's AI technology enables an extraordinary array of applications that forge new connections among people, computers, knowledge, and the physical world. Some AI enabled applications are information distribution and retrieval, database mining, product design, manufacturing, inspection, training, user support, surgical planning, resource scheduling, and complex resource management. AI technologies help enterprises reduce latency in making business decisions, minimize fraud and enhance revenue opportunities.",https://ieeexplore.ieee.org/document/6143435/,International Conference on Sustainable Energy and Intelligent Systems (SEISCON 2011),20-22 July 2011,ieeexplore
10.1109/SMC.2013.819,Automated Sound Signalling Device Quality Assurance Tool for Embedded Industrial Control Applications,IEEE,Conferences,This paper presents a novel system for automatic detection and recognition of faulty audio signaling devices as part of an automated industrial manufacturing process. The system uses historical data labeled by human experts in detecting faulty signaling devices to train an artificial neural network based classifier for modeling their decision making process. The neural network is implemented on a real time embedded micro controller which can be more efficiently incorporated into an automated production line eliminating the need for a manual inspection within the manufacturing process. We present real world experiments based on data pertaining to the production and manufacture of audio signaling components used in car instrument clusters. Our results show that the proposed expert system is able to successfully classify faulty audio signaling devices to a high degree of accuracy. The results can be generalized to other signaling devices where an output signal is represented by a complex and changing frequency spectrum even with significant environmental noise.,https://ieeexplore.ieee.org/document/6722574/,"2013 IEEE International Conference on Systems, Man, and Cybernetics",13-16 Oct. 2013,ieeexplore
10.1109/SMC.2018.00655,Automated Training Plan Generation for Athletes,IEEE,Conferences,"In sports, athletes need detailed and individualised training plans for maintaining and improving their skills in order to achieve their best performance in competitions. This presents a considerable workload for coaches, who besides setting objectives have to formulate extremely detailed training plans. Automated Planning, which has already been successfully deployed in many real-world applications such as space exploration, robotics, and manufacturing processes, embodies a useful mechanism that can be exploited for generating training plans for athletes. In this paper, we propose the use of Automated Planning techniques for generating individual training plans, which consist of exercises the athlete has to perform during training, given the athlete's current performance, period of time, and target performance that should be achieved. Our experimental analysis, which considers general training of kickboxers, shows that apart of considerable less planning time, training plans automatically generated by the proposed approach are more detailed and individualised than plans prepared manually by an expert coach.",https://ieeexplore.ieee.org/document/8616652/,"2018 IEEE International Conference on Systems, Man, and Cybernetics (SMC)",7-10 Oct. 2018,ieeexplore
10.1109/ECAI52376.2021.9515185,Automated ceramic plate defect detection using ScaledYOLOv4-large,IEEE,Conferences,"Automated visual inspection has become a popular topic of research in the last couple of decades, as computation power available grew exponentially. Judging by the fact that visual inspection is a critical task for the quality of the products, it would be highly recommended that people only supervise the system. This paper proposes a low cost, rapid development defect detection system based on the Scaled-YOLOv4 object detection model. The original model achieves almost state of the art detection mAP on the COCO dataset with a mAP(mean average precision) of 56.0 for the largest variant, named YOLOv4-P7. Our version is derived from the ScaledYOLOv4-P5 model and is trained on ceramic plate defects and achieves 87.4 mAP at a intersection of union of 0.5, while comfortably processing a frame in 20ms on a consumer RTX3070 GPU. Thus, the real time constraint for the manufacturing system is fulfilled. Hence, the critical aspects of the development process are the: quick development process, fast training, rapid deployment on the factory floor, quick validation and feedback, using images acquired in the lab - not on the factory floor for first iteration and overall low cost of the automated inspection system.",https://ieeexplore.ieee.org/document/9515185/,"2021 13th International Conference on Electronics, Computers and Artificial Intelligence (ECAI)",1-3 July 2021,ieeexplore
10.1109/ICAIIC51459.2021.9415189,Big Data Platform for Intelligence Industrial IoT Sensor Monitoring System Based on Edge Computing and AI,IEEE,Conferences,"The cutting edge of Industry 4.0 has driven everything to be converted to disruptive innovation and digitalized. This digital revolution is imprinted by modern and advanced technology that takes advantage of Big Data and Artificial Intelligence (AI) to nurture from automatic learning systems, smart city, smart energy, smart factory to the edge computing technology, and so on. To harness an appealing, noteworthy, and leading development in smart manufacturing industry, the modern industrial sciences and technologies such as Big Data, Artificial Intelligence, Internet of things, and Edge Computing have to be integrated cooperatively. Accordingly, a suggestion on the integration is presented in this paper. This proposed paper describes the design and implementation of big data platform for intelligence industrial internet of things sensor monitoring system and conveys a prediction of any upcoming errors beforehand. The architecture design is based on edge computing and artificial intelligence. To extend more precisely, industrial internet of things sensor here is about the condition monitoring sensor data - vibration, temperature, related humidity, and barometric pressure inside facility manufacturing factory.",https://ieeexplore.ieee.org/document/9415189/,2021 International Conference on Artificial Intelligence in Information and Communication (ICAIIC),13-16 April 2021,ieeexplore
10.1109/ICDEW.2019.00-35,Big Stream Processing Systems: An Experimental Evaluation,IEEE,Conferences,"As the world gets more instrumented and connected, we are witnessing a flood of digital data generated from various hardware (e.g., sensors) or software in the format of flowing streams of data. Real-time processing for such massive amounts of streaming data is a crucial requirement in several application domains including financial markets, surveillance systems, manufacturing, smart cities, and scalable monitoring infrastructure. In the last few years, several big stream processing engines have been introduced to tackle this challenge. In this article, we present an extensive experimental study of five popular systems in this domain, namely, Apache Storm, Apache Flink, Apache Spark, Kafka Streams and Hazelcast Jet. We report and analyze the performance characteristics of these systems. In addition, we report a set of insights and important lessons that we have learned from conducting our experiments.",https://ieeexplore.ieee.org/document/8750955/,2019 IEEE 35th International Conference on Data Engineering Workshops (ICDEW),8-12 April 2019,ieeexplore
10.1109/ICII.2018.00024,Brightics-IoT: Towards Effective Industrial IoT Platforms for Connected Smart Factories,IEEE,Conferences,"Industrial Internet-of-Things (IIoT) supports machines, computers and users to enable intelligent operations using advanced device management and data analytics. In recent years, thanks to standardized IoT platforms and advanced Artificial Intelligence (AI) technologies, there have been great advances in IIoT, and now it promises revolutions on various manufacturing domains such as transport, health, factory and energy. In this paper, based on our experience operating IIoT in various factory applications, we present the technical challenges of manufacturing facilities needed to be dealt with to collect huge amount of data in real-time and counteraction points of an IoT platform regarding these technical challenges and what kinds of features need to be implemented for intelligent services in the smart manufacturing. Finally, we introduce a story of applying industrial IoT platform in production to show how iterative development approaches can achieve business requirements based on elastically scaled-out architecture.",https://ieeexplore.ieee.org/document/8539113/,2018 IEEE International Conference on Industrial Internet (ICII),21-23 Oct. 2018,ieeexplore
10.1109/CASE49439.2021.9551562,Building Skill Learning Systems for Robotics,IEEE,Conferences,"Skill-generating policies have enabled robots to perform a wide range of applications as for example assembly tasks. However, the manual engineering effort for such policies is fairly high and the environment is frequently required to be rather deterministic. For expanding robot deployment to low-volume manufacturing two challenges need to be addressed. First, the robot should acquire the skill-generating policy not from a robot programmer but rather from an expert on the task and second, the robot needs to be able to operate in unstructured environments. In this paper we present a learning approach that combines imitation learning and reinforcement learning to provide a tool for intuitive task teaching followed by self-optimization of the system. The presented approach is applied to a dual-arm assembly task using a real robot and appropriate simulation models. Whereas pure imitation learning does not result in an acceptable success rate for the considered example, after 400 episodes of reinforcement learning the robot can successfully solve the assembly task.",https://ieeexplore.ieee.org/document/9551562/,2021 IEEE 17th International Conference on Automation Science and Engineering (CASE),23-27 Aug. 2021,ieeexplore
10.1109/ISVLSI49217.2020.00-12,CPSoSaware: Cross-Layer Cognitive Optimization Tools &amp; Methods for the Lifecycle Support of Dependable CPSoS,IEEE,Conferences,"Cyber-physical Systems of Systems (CPSoS) are large complex systems where physical elements interact with and are controlled by a large number of distributed and networked computing elements as well as human users. Their increasingly stringent demands on efficient use of resources, high service and product quality levels and, of course low cost and competitiveness on the world market introduce big challenges related to the design operation continuum of dependable connected CPSs. The CPSoSaware project aims at developing the models and software tools to allocate computational power/resources to the CPS end devices and autonomously determining what cyber-physical processes will be handled by the devices' heterogeneous components (CPUs, GPUs, FPGA fabric, software stacks). The project relies on Artificial Intelligence (AI) support to strengthen reliability, fault tolerance and security at system level and also to lead to CPS designs that work in a decentralized way, collaboratively, in an equilibrium, by sharing tasks and data with minimal central intervention. The CPSoSaware system will interact with the human users/operators through extended reality visual and touchable interfaces increasing situational awareness. The CPSoSaware system will be evaluated: i) in the automotive sector, in mixed traffic environments with semi-autonomous connected vehicles and ii) in the manufacturing industry where inspection and repair scenarios are employed using collaborative robots.",https://ieeexplore.ieee.org/document/9155036/,2020 IEEE Computer Society Annual Symposium on VLSI (ISVLSI),6-8 July 2020,ieeexplore
10.1109/VRAIS.1993.380772,Calibration of head-mounted displays for augmented reality applications,IEEE,Conferences,"The authors have developed ""augmented reality"" technology, consisting of a see-through head-mounted display, a robust, accurate position/orientation sensor, and their supporting electronics and software. Their primary goal is to apply this technology to touch labor manufacturing processes, enabling a factory worker to view index markings or instructions as if they were painted on the surface of a workpiece. In order to accurately project graphics onto specific points of a workpiece, it is necessary to have the coordinates of the workpiece, the display's virtual screen, the position sensor, and the user's eyes in the same coordinate system. The linear transformation and projection of each point to be displayed from world coordinates to virtual screen coordinates are described, and the experimental procedures for determining the correct values of the calibration parameters are characterized.&lt;<ETX>&gt;</ETX>",https://ieeexplore.ieee.org/document/380772/,Proceedings of IEEE Virtual Reality Annual International Symposium,18-22 Sept. 1993,ieeexplore
10.1109/CNNA.2010.5430300,Cellular Neural Network (CNN) based control algorithms for omnidirectional laser welding processes: Experimental results,IEEE,Conferences,"The high dynamics of laser beam welding (LBW) in several manufacturing processes ranging from automobile production to precision mechanics requires the introduction of new fast real time controls. In the last few years, algorithms for the control of constant-orientation LBW processes have been introduced. Nevertheless, some real life processes are also performed changing the welding orientation during the process. In this paper experimental results obtained by the use of a new CNN based strategy for the control of curved welding seams are discussed. It is based on the real time adjustment of the laser power by the detection of the full penetration hole in process images. The control algorithm has been implemented on the Eye-RIS system v1.2 leading to a visual closed loop control solution with controlling rates up to 6 kHz.",https://ieeexplore.ieee.org/document/5430300/,2010 12th International Workshop on Cellular Nanoscale Networks and their Applications (CNNA 2010),3-5 Feb. 2010,ieeexplore
10.1109/TENCON.2004.1414983,Certain studies on sample time for a predictive fuzzy logic controller through real time implementation of phenol-formaldehyde manufacturing,IEEE,Conferences,"In polymer manufacturing industries, the automation and control of chemical process incorporating techniques of fuzzy control neural networks, and expert systems had lead to a more secured and stable operation. A sudden and unpredictable heat is often produced by the nonlinear exothermal reaction when phenol and formaldehyde are mixed together. Therefore, the polymerization process has to be controlled with a high level of precision in order to avoid temperature run-away. This paper proposes a design methodology for a sensor based process control system. The duration of ON and OFF time of certain relays are the parameters to be controlled in order to keep the exothermic reaction under control The universe of discourse for the output of the FLC system is the sample time that assigned to the relays where maximum time for heater or valve can be turned on before the next action is applied This paper discusses a detailed real time implementation of the exothermal process control using Matlab-fuzzy logic toolbox. An enhanced predictive FLC structure is developed and compared to a predictive FLC control structure. The obtained practical results thus ensure that the predictive FLC can be enhanced by modifying the rules and the membership Junctions of the universe of discourse, which is proved to be better in controlling the reaction temperature.",https://ieeexplore.ieee.org/document/1414983/,2004 IEEE Region 10 Conference TENCON 2004.,24-24 Nov. 2004,ieeexplore
10.1109/BigData.2016.7840831,Cloud-based machine learning for predictive analytics: Tool wear prediction in milling,IEEE,Conferences,"The proliferation of real-time monitoring systems and the advent of Industrial Internet of Things (IIoT) over the past few years necessitates the development of scalable and parallel algorithms that help predict mechanical failures and remaining useful life of a manufacturing system or system components. Classical model-based prognostics require an in-depth physical understanding of the system of interest and oftentimes assume certain stochastic or random processes. To overcome the limitations of model-based methods, data-driven methods such as machine learning have been increasingly applied to prognostics and health management (PHM). While machine learning algorithms are able to build accurate predictive models, large volumes of training data are required. Consequently, machine learning techniques are not computationally efficient for data-driven PHM. The objective of this research is to create a novel approach for machinery prognostics using a cloud-based parallel machine learning algorithm. Specifically, one of the most popular machine learning algorithms (i.e., random forest) is applied to predict tool wear in dry milling operations. In addition, a parallel random forest algorithm is developed using the MapReduce framework and then implemented on the Amazon Elastic Compute Cloud. Experimental results have shown that the random forest algorithm can generate very accurate predictions. Moreover, significant speedup can be achieved by implementing the parallel random forest algorithm.",https://ieeexplore.ieee.org/document/7840831/,2016 IEEE International Conference on Big Data (Big Data),5-8 Dec. 2016,ieeexplore
10.1109/WFCS.2019.8757952,Cloud-enabled Smart Data Collection in Shop Floor Environments for Industry 4.0,IEEE,Conferences,"Industry 4.0 transition is producing a remarkable change in the Smart Factories management. Modern companies can provide new services following products inside the shop floors along the entire production chain. To achieve the goal of servitization that the Industry 4.0 world requires, a modernization of current production chains is needed. This common demand comes mostly from manufacturing sector, where complex work machines collaborate with human workers. The data produced by the machines must be processed quickly, to allow the implementation of reactive services such as predictive maintenance, and remote control, always taking care of the safety of nearby people. This paper proposes a multi-layer architecture to monitor legacy production machines during their operations inside customers plants. The platform provides near real-time delivery of data collected from the machines with a high grade of customization according to customer needs. The performed tests show the scalability of the platform for a productive ecosystem with many machines at work, confirming its feasibility within different production facilities with different needs.",https://ieeexplore.ieee.org/document/8757952/,2019 15th IEEE International Workshop on Factory Communication Systems (WFCS),27-29 May 2019,ieeexplore
10.1109/WCNC45663.2020.9120761,Collaborative Learning Model for Cyberattack Detection Systems in IoT Industry 4.0,IEEE,Conferences,"Although the development of IoT Industry 4.0 has brought breakthrough achievements in many sectors, e.g., manufacturing, healthcare, and agriculture, it also raises many security issues to human beings due to a huge of emerging cybersecurity threats recently. In this paper, we propose a novel collaborative learning-based intrusion detection system which can be efficiently implemented in IoT Industry 4.0. In the system under consideration, we develop smart “filters” which can be deployed at the IoT gateways to promptly detect and prevent cyberattacks. In particular, each filter uses the collected data in its network to train its cyberattack detection model based on the deep learning algorithm. After that, the trained model will be shared with other IoT gateways to improve the accuracy in detecting intrusions in the whole system. In this way, not only the detection accuracy is improved, but our proposed system also can significantly reduce the information disclosure as well as network traffic in exchanging data among the IoT gateways. Through thorough simulations on real datasets, we show that the performance obtained by our proposed method can outperform those of the conventional machine learning methods.",https://ieeexplore.ieee.org/document/9120761/,2020 IEEE Wireless Communications and Networking Conference (WCNC),25-28 May 2020,ieeexplore
10.1109/DESSERT50317.2020.9125038,Combination of Digital Twin and Artificial Intelligence in Manufacturing Using Industrial IoT,IEEE,Conferences,"The paper focuses on Digital Twin (DT) in Manufacturing using Artificial Intelligence (AI) and Industrial IoT. According to the concept, the manufacturing includes three main units: equipment, personnel and processes. All data from these units are inherited to manufacture model (DT) and decision support system with the use of AI. DT data technology allows finding the required knowledge that can be interpreted and used to support the process of decision-making in the management of the enterprise. AI applications open up a broad spectrum of opportunities in manufacturing to add value by optimizing processes and generating new business models. The Landscape was described by a formal model to assure the possibility to analyze the state and development of landscape in detail considering DT and other technologies. DT and IIoT implementation for the simulation of real enterprise manufacturing were considered.",https://ieeexplore.ieee.org/document/9125038/,"2020 IEEE 11th International Conference on Dependable Systems, Services and Technologies (DESSERT)",14-18 May 2020,ieeexplore
10.1109/IECON.1990.149237,Commercial benefits of AI applications in CIM: value analysis approach,IEEE,Conferences,The analysis of benefits gained from the application of artificial intelligence (AI) techniques to real-time supervision and control problems in computer-integrated manufacturing (CIM) is considered. The value of AI in CIM is viewed as enhancing the benefits already achieved through the implementation of CIM systems. A hybrid technique that is an extension of the value analysis method of evaluating the qualitative and quantitative aspects of the application of conventional project deliverables is discussed. Results from a case study based on a value analysis of the benefits of applying AI techniques to CIM within a multinational project are outlined.&lt;<ETX>&gt;</ETX>,https://ieeexplore.ieee.org/document/149237/,[Proceedings] IECON '90: 16th Annual Conference of IEEE Industrial Electronics Society,27-30 Nov. 1990,ieeexplore
10.1109/ETFA.2017.8247730,Communication middleware technologies for industrial distributed control systems: A literature review,IEEE,Conferences,"Industry 4.0 is the German vision for the future of manufacturing, where smart factories use information and communication technologies to digitise their processes to achieve improved quality, lower costs, and increased efficiency. It is likely to bring a massive change to the way control systems function today. Future distributed control systems are expected to have an increased connectivity to the Internet, in order to capitalize on new offers and research findings related to digitalization, such as cloud, big data, and machine learning. A key technology in the realization of distributed control systems is middleware, which is usually described as a reusable software layer between operating system and distributed applications. Various middleware technologies have been proposed to facilitate communication in industrial control systems and hide the heterogeneity amongst the subsystems, such as OPC UA, DDS, and RT-CORBA. These technologies can significantly simplify the system design and integration of devices despite their heterogeneity. However, each of these technologies has its own characteristics that may work better for particular applications. Selection of the best middleware for a specific application is a critical issue for system designers. In this paper, we conduct a survey on available standard middleware technologies, including OPC UA, DDS, and RT-CORBA, and show new trends for different industrial domains.",https://ieeexplore.ieee.org/document/8247730/,2017 22nd IEEE International Conference on Emerging Technologies and Factory Automation (ETFA),12-15 Sept. 2017,ieeexplore
10.1109/STSIVA53688.2021.9592013,Compressive Spectral Imaging Fusion Implementation Based on an End-to-End Optimization Design,IEEE,Conferences,"Compressive spectral imaging fusion (CSIF) has recently attracted attention as a popular methodology to improve the spatial and spectral resolution simultaneously. The joint of coded aperture design with the fusion via a deep neural network is state-of-the-art for CSIF. However, the current results are focused on simulation results where implementation complications such as calibration and adjustment processes in the fusion methods are skipped. Therefore, this paper presents an efficient assemble prototype for CSIF. In particular, some implementation details such as pixel mismatch and manufacturing noise are considered during the training to reduce the calibration problems. Furthermore, a re-training of the network using captured ground truth images and the calibrated sensing matrices is presented. Real fusion results of the testbed implementation validated the proposed fusion system.",https://ieeexplore.ieee.org/document/9592013/,"2021 XXIII Symposium on Image, Signal Processing and Artificial Vision (STSIVA)",15-17 Sept. 2021,ieeexplore
10.1109/IECON43393.2020.9255001,Computation Offloading for Machine Learning in Industrial Environments,IEEE,Conferences,"Industrial applications, such as real-time manufacturing, fault classification and inference, autonomous cars, etc., are data-driven applications that require machine learning with a wealth of data generated from industrial Internet of Things (IoT) devices. However, conventional approaches of transmitting this rich data to a remote data center to learn may be undesired due to the non-negligible network transmission delay and the sensitiveness of data privacy. By deploying a number of computing-capable devices at the network edge, edge computing supports the implementation of machine learning close to the industrial environment. Considering the heterogeneous computing capability as well as network location of edge devices, there are two types of feasible edge computing based machine learning models, including the centralized learning and federated learning models. In centralized learning, a resource-rich edge server aggregates the data from different IoT devices and performs machine learning. In federated learning, distributed edge devices and a federated server collaborate to perform machine learning. The features that data should be offloaded in centralized learning while it is locally trained in federated learning make centralized learning and federated learning quite different. We study the computation offloading problem for edge computing based machine learning in an industrial environment, considering the abovementioned machine learning models. We formulate a machine learning-based offloading problem with the goal of minimizing the training delay. Then, an energy-constrained delay-greedy (ECDG) algorithm is designed to solve the problem. Finally, simulation studies based on the MNIST dataset have been conducted to illustrate the efficiency of the proposal.",https://ieeexplore.ieee.org/document/9255001/,IECON 2020 The 46th Annual Conference of the IEEE Industrial Electronics Society,18-21 Oct. 2020,ieeexplore
10.1109/ICAICA50127.2020.9182598,Construction of Industrial Internet of Things Based on MQTT and OPC UA Protocols,IEEE,Conferences,"At present, the Internet of Things has become a hot area of global concern and is considered to be another major scientific and technological innovation after the Internet. The Internet of Things can be analyzed from two aspects, one is to access the Internet, and the other is the connection between things. The Industrial Internet of Things, as a new type of industrial ecosystem, collects resources and data during the industrial production process by using miniature low-cost controllers and high-bandwidth wireless networks to achieve flexible configuration of manufacturing raw materials and improve resource utilization. The Industrial Internet of Things not only includes traditional software elements, but also requires hardware controllers and sensors, as well as cloud service platforms, to ultimately achieve intelligent manufacturing. The Industrial Internet of Things based on MQTT adopts the method of upper computer (WeChat applet) + server (Alibaba Cloud Server) + lower computer (WiFi module ESP8266 NodeMcu) to realize data collection and control of industrial production process. Based on OPC UA protocol, The Industrial Internet of Things, by designing an intelligent node device based on embedded Linux system, uses Qt to build the OPC UA protocol. OPC UA is a C / S architecture real-time database framework. By building an OPC UA Client on a handheld terminal device, An OPC UA Serve is set up on the smart terminal device, and the information collected by the handheld terminal is sent to the smart terminal. At the same time, multiple smart terminals are connected through the OPC UA protocol to perform data interaction and information transfer. At the same time, an edge computing algorithm is embedded in the smart terminal, and a large amount of data collected is processed for analysis, screening, and calculation. At the same time, the processed information is transmitted or processed. Reduce data interactions between data sources and data centers, and reduce the amount of information transmitted and stored.",https://ieeexplore.ieee.org/document/9182598/,2020 IEEE International Conference on Artificial Intelligence and Computer Applications (ICAICA),27-29 June 2020,ieeexplore
10.1109/IECON.2011.6119745,Context and implications of learning in Evolvable Production Systems,IEEE,Conferences,"More than ever the impact of market turbulence, high product customization and sustainability can be perceived through the increase of dynamics and complexity of manufacturing and business environments. Modular and distributed control structures are nowadays a consensual way, common to the majority of modern paradigms, to deal with unpredictability and volatility of markets. With modern paradigms aiming on adaptability, evolution and re-configuration as a way to deal with this reality, learning mechanism have become very important to achieve such requirements. This paper presents an exploratory discussion on the topic of agent-based learning applied in a modern production paradigm such as Evolvable Production Systems (EPS).",https://ieeexplore.ieee.org/document/6119745/,IECON 2011 - 37th Annual Conference of the IEEE Industrial Electronics Society,7-10 Nov. 2011,ieeexplore
10.1109/ICSIP49896.2020.9339378,Critical Analysis of Edge Computing,IEEE,Conferences,"The edge computing is facing many problems and efforts are being made to overcome those challenges. Hybrid Mobile Edge Computing is introduced for the mobile devices to overcome the limited battery issues and performance constraints in the devices. There were some difficulties in the integrated development environment of the edge computing, and to overcome those problems, a container-based method is introduced which improves the performance of the coding environment in EC, and also it facilitates in-place debugging. An EC architecture is presented that provides local data processing, management, and quick reaction for the Virtual IoT Devices. With EC's support, a hybrid computing framework is built and an intelligent resource scheduling strategy to fulfill the real-time requirements in smart manufacturing; which showed satisfactory results. A Multi-source Transmission Protocol is presented to counter problems such as the low video streaming and high bandwidth usage. An ECD device is presented for the displaying of results in real-time, with short-time response and also to overcome the network limits. A decentralized resource management technique is introduced along with a technical framework for the latency-sensitive applications' deployment on the edge devices while protecting the privacy of devices. A Dynamic Edge-Fabric Environment is presented; this platform can automatically learn on the basis of past performance of the available resources using machine learning techniques and it decides which task should be best executed where, based on real-time system status and task requirements; the results have proven that it can improve overall performance for the selection of resources.",https://ieeexplore.ieee.org/document/9339378/,2020 IEEE 5th International Conference on Signal and Image Processing (ICSIP),23-25 Oct. 2020,ieeexplore
10.1109/COINS51742.2021.9524088,Cyber Security Risks of Technical Components in Industry 4.0,IEEE,Conferences,"Industry 4.0 is a new concept of automation data exchange in manufacturing. Industry 4.0 consists of various technical components such as Artificial Intelligence (AI), IoT, OPC UA, and cloud. There could be cyber risks when these technical components are used in Industrial Control System (ICS) without security consideration. Therefore cyber risk assessment, reconsideration of implementation, operation are necessary to adopt Industry 4.0. Problems of previous research for ICS risk assessments are some of them are theoretical and conceptual methods such as simulations. Moreover, they do not focus on Industry 4.0 components.The objective of this research is to prove the concrete cyber risks and introduces corresponding secure implementations for Industry 4.0 components such as Artificial Intelligence (AI), the Internet of Things (IoT), OPC Unified Architecture (OPC UA). The proposed method has benefits for clarifying impacts on the real world by cyber-attacks through penetration tests against ICS testbed with actual machines.",https://ieeexplore.ieee.org/document/9524088/,2021 IEEE International Conference on Omni-Layer Intelligent Systems (COINS),23-25 Aug. 2021,ieeexplore
10.1109/INDIN41052.2019.8972310,Data-driven modeling of semi-batch manufacturing: a rubber compounding test case,IEEE,Conferences,"The continuously growing amount of available data from manufacturing processes supports the development of data-driven models. The typical target application of these models is optimal control and continuous quality management within an objective of zero-defect manufacturing. However, data obtained from batch processes are characterized by its high dimensionality that exceeds the computational capabilities of online applications and data-driven model's reliability must be guaranteed for proper industrial implementation. We explore two approaches to reduce problem's size: feature extraction and feature selection; several multivariate regression methods are also compared regarding it precision and robustness. We base our analysis on an industrial rubber compounding process where natural rubber is blended in a semi-batch mixer with several additives, then it is further mixed up using cylinders and it is conditioned in bands for storing. For this process, real production data is collected and stored in the manufacturing execution system of the company. The objective of the analysis is to predict mechanical properties of the rubber at the end of the processes. Based on the provided data, several data-driven models are built and tested. From the comparison among them it is concluded: models based on feature extraction and artificial neural networks yield the highest accuracy, while feature-selected models provide better physical interpretability and increased robustness regarding industrial deployment.",https://ieeexplore.ieee.org/document/8972310/,2019 IEEE 17th International Conference on Industrial Informatics (INDIN),22-25 July 2019,ieeexplore
10.1109/AERO.2018.8396547,Data-driven quality prognostics for automated riveting processes,IEEE,Conferences,"Technologies based in robotics and automatics are reshaping the aerospace industry. Aircraft manufacturers and top-tier suppliers now rely on robotics to perform most of its operational tasks. Over the years, a succession of implemented mobile robots has been developed with the mission of automating important industrial processes such as welding, material handling or assembly procedures. However, despite the progress achieved, a major limitation is that the process still requires human supervision and an extensive quality control process. An approach to address this limitation is to integrate machine learning methods within the quality control process. The idea is to develop algorithms that can direct manufacturing experts towards critical areas requiring human supervision and quality control. In this paper we present an application of machine learning to a concrete industrial problem involving the quality control of a riveting machine. The proposal consists of an intelligent predictive model that can be integrated within the existing real time sensing and pre-processing sub-systems at the equipment level. The framework makes use of several data-driven techniques for pre-processing and feature engineering, combined with the most accurate algorithms, validated through k-folds cross validation technique which also estimates prediction errors. The model is able to classify the manufacturing process of the machine as nominal or anomalous according to a real-world data set of design requirements and operational data. Several machine learning algorithms are compared such as linear regression, nearest neighbor, support vector machines, decision trees, random forests and extreme gradient boost. Results obtained from the case study suggest that the proposed model produces accurate predictions which meet industrial standards.",https://ieeexplore.ieee.org/document/8396547/,2018 IEEE Aerospace Conference,3-10 March 2018,ieeexplore
10.1109/IMCEC51613.2021.9482089,Data-driven scheduling for smart shop floor via reinforcement learning with model-based clustering algorithm,IEEE,Conferences,"Various information technologies provide the manufacturing system massive data, which can support the decision optimization of product lifecycle management. However, the lack of effective use for advanced technologies prevents manufacturing industry from being automated and intelligent. Therefore, this paper proposes the smart shop floor and implementation mechanism. Meanwhile, based on the clustering and reinforcement learning, the brain agent and scheduling agent are designed to determine the approriate rule according to the shop floor state information, thus realizing the data-driven real-time scheduling. Experimental results show that the smart shop floor can effectively deal with disturbance events and has better performance compared with composite dispatching rules.",https://ieeexplore.ieee.org/document/9482089/,"2021 IEEE 4th Advanced Information Management, Communicates, Electronic and Automation Control Conference (IMCEC)",18-20 June 2021,ieeexplore
10.1109/CASE48305.2020.9216961,Decentralized coordination of autonomous AGVs for flexible factory automation in the context of Industry 4.0,IEEE,Conferences,"Future smart factories feature flexible systems that can dynamically reconFigure manufacturing systems via near real-time system monitoring and learning-based self-optimization. Automated guided vehicles (AGVs), as a critical method of transporting goods and material within a factory, is vital for flexible automation in a smart factory. However, there is an urgent gap in the ability to dynamically schedule and assign tasks for AGVs in a dynamic environment. In this research, we propose a decentralized AGV fleet architecture and task allocation method to enable dynamic allocation/reallocation of tasks in an AGV fleet. The developed algorithm can also reconFigure AGV task allocations to adapt to system changes, such as AGV failure and new AGVs joining the system. The system modeling, setup and algorithms are presented with a case study in a lab environment that demonstrates flexible collaboration between an AGV fleet and a robotic assembly cell.",https://ieeexplore.ieee.org/document/9216961/,2020 IEEE 16th International Conference on Automation Science and Engineering (CASE),20-21 Aug. 2020,ieeexplore
10.1109/IJCNN.2019.8852303,Deep Capsule Network based Automatic Batch Code Identification Pipeline for a Real-life Industrial Application,IEEE,Conferences,"Automatic recognition of text, such as a batch code printed on a box placed on a moving conveyor belt, is still a challenging problem. This paper proposes an end-to-end character recognition technique while addressing the major challenges encountered in a real environment, such as motion blur in the acquired images, slanted or oriented characters, creased batch codes due to wear and tear of boxes, variations in label formats, and variations in printing styles. The major contribution of this work lies in development of three sequential modules: text localization using Connectionist Text Proposal Network(CTPN), character detection and character recognition using a modified version of the capsule network (CapsNet). In contrast to CapsNet, where only a standard single convolution is used, the proposed method uses a series of feature blocks, making it a deep CapsNet which is later proven to generate more comprehensive and better separable feature vectors over its counterpart. The feature generation module is further enhanced by setting a smaller kernel size than CapsNet. The proposed system is validated on a real-world box / packet dataset generated in a retail manufacturing industry. The proposed recognition network architecture is also validated on a standard public dataset (ICDAR 2013). The comparative results are presented with statistical analysis in the experimental results section.",https://ieeexplore.ieee.org/document/8852303/,2019 International Joint Conference on Neural Networks (IJCNN),14-19 July 2019,ieeexplore
10.1109/ICRoM48714.2019.9071857,Deep Learning Approach For Object Tracking Of RoboEye,IEEE,Conferences,"RoboEye is a spherical 3RRR parallel robot which has been developed for its high precision. It can provide high speeds, so can be used for fast tracking tasks. To this end, in this paper proper deep learning approaches are combined with classical control methods. Deep learning algorithms are employed to detect an object of interest among various ones in a monocular image, and then obtain an estimatation of the distance to the camera. So, simultaneous depth estimation, and object detection with a monocular camera for real time implementation is proposed here. For fast calculations, also to overcome manufacturing uncertainties, inverse kinematic equations are computed by a multi-layer perceptron (MLP) network based on real data. Finally, a classical PID controller can perform a fast tracking of the object.",https://ieeexplore.ieee.org/document/9071857/,2019 7th International Conference on Robotics and Mechatronics (ICRoM),20-21 Nov. 2019,ieeexplore
10.1109/MSM49833.2020.9201666,Deep Learning-based Algorithm for Mobile Robot Control in Textureless Environment,IEEE,Conferences,"For the implementation of stereo image-based visual servoing algorithm in the eye-in-hand robotics applications, one of the main concerns is the accurate point feature detection and matching algorithm. Since the visual servoing is carried out in the textureless environment, the feature detection process is even more challenging. To fulfill the requirement of a robust and reliable point feature detection process, in this paper we present the novel deep learning-based algorithm. The approach based on convolutional neural networks and algorithm for detection of manufacturing entities is proposed and detected regions of interest are utilized for the improvement of the point feature detection algorithm. The proposed algorithm is experimentally evaluated in real-world settings by using wheeled nonholonomic mobile robot RAICO equipped with stereo vision system. The experimental results show the improvement of 58% in the accuracy of matched point features in the images obtained during the visual servoing process. Moreover, with the implementation of the proposed deep learning-based approach, the number of successful experimental runs has increased by 80%.",https://ieeexplore.ieee.org/document/9201666/,2020 International Conference Mechatronic Systems and Materials (MSM),1-3 July 2020,ieeexplore
10.1109/ICC40277.2020.9148818,Deep Learning-based Human Implantable Nano Molecular Communications,IEEE,Conferences,"In this paper, we propose a novel nano-molecular communication system, including a nano receiver design and detection strategies. We show how machine intelligence can be incorporated into the practical implementation of nano communications. We introduce a testbed employing a biosensor chip as a receiver. The chip is made to be sufficiently small to be implanted under the human skin with no harm while detecting concentrations of glucose molecules over time. Molecules are released by a transmitter, to convey information through a thin pipe. For this configuration, the channel model is unknown, and the sensor dynamics can differ with according to the manufacturing process. Therefore, it is more desirable to find a universal strategy than using closed-form channel expressions so that it can be less sensitive to the channel and sensor variation. Learning-based approaches are likely to solve the problem. Therefore, in this paper, we suggest detection strategies with and without machine learning. We first describe our intuitions of nanomachine design from observations, and we show how the learning-based techniques can benefit the system by reducing the design burden and enhancing the accuracy of data detection. The study concludes by showing sample results of real data transmission.",https://ieeexplore.ieee.org/document/9148818/,ICC 2020 - 2020 IEEE International Conference on Communications (ICC),7-11 June 2020,ieeexplore
10.1109/COINS51742.2021.9524156,Defective Wafer Detection Using Sensed Numerical Features,IEEE,Conferences,"One of the fundamental processes in semiconductor manufacturing is slicing, which means cutting an ingot into many wafers. During the slicing process, it is possible to produce defective wafers. Unfortunately, the inspection to identify defective wafers is time-consuming and difficult. To solve this problem, we build a system, which uses sensors to collect features (e.g., temperature, thickness, pattern on wafer surface, etc.) during the slicing process to detect if the wafers are defective in the manufacturing process. Two different models, the GRU neural network and XGBoost, are implemented in the proposed system. After fine-tuning both models, experimental results based on real dataset indicate that the GRU neural network outperforms XGBoost for wafer defective detection in both the prediction accuracy and model training time.",https://ieeexplore.ieee.org/document/9524156/,2021 IEEE International Conference on Omni-Layer Intelligent Systems (COINS),23-25 Aug. 2021,ieeexplore
10.1109/ISIE45063.2020.9152441,Deployment of a Smart and Predictive Maintenance System in an Industrial Case Study,IEEE,Conferences,"Industrial manufacturing environments are often characterized as being stochastic, dynamic and chaotic, being crucial the implementation of proper maintenance strategies to ensure the production efficiency, since the machines' breakdown leads to a degradation of the system performance, causing the loss of productivity and business opportunities. In this context, the use of emergent ICT technologies, such as Internet of Things (IoT), machine learning and augmented reality, allows to develop smart and predictive maintenance systems, contributing for the reduction of unplanned machines' downtime by predicting possible failures and recovering faster when they occur. This paper describes the deployment of a smart and predictive maintenance system in an industrial case study, that considers IoT and machine learning technologies to support the online and real-time data collection and analysis for the earlier detection of machine failures, allowing the visualization, monitoring and schedule of maintenance interventions to mitigate the occurrence of such failures. The deployed system also integrates machine learning and augmented reality technologies to support the technicians during the execution of maintenance interventions.",https://ieeexplore.ieee.org/document/9152441/,2020 IEEE 29th International Symposium on Industrial Electronics (ISIE),17-19 June 2020,ieeexplore
10.23919/URSIGASS51995.2021.9560218,Design and Implementation of a Low-Cost Core Board for Mobile IoT Rapid System Prototyping and Service Roll-Out,IEEE,Conferences,"To enable fast development of specific and general mobile Internet of Things (IoT) systems and services, apart from writing cloud-side applications, the requisite hardware production, e.g., procurement of chips and materials, design and manufacturing of printed circuit boards (PCBs), completion of surface mounted technology (SMT) operations, etc., can be a source of delay. Here a low-cost generic core board, which is IoT-service and loT-system prototyping ready, is proposed. Flexible and adaptable to many mobile IoT applications, it is based on a robust low-power consumption microcontroller unit (MCU), with boot-loading and remote software upgrading attributes, and running an open-source real time (RT) operating system (OS), which is well supported with IoT sensor and peripheral sensor drivers. The key hardware, supporting IoT communication protocols embedded, comprises GPS/GPRS, NB-IoT, and LTE modules. The paper sets out the basics of the board design.",https://ieeexplore.ieee.org/document/9560218/,2021 XXXIVth General Assembly and Scientific Symposium of the International Union of Radio Science (URSI GASS),28 Aug.-4 Sept. 2021,ieeexplore
10.1109/AIEA53260.2021.00021,Design and Implementation of the Prototype for Hybrid Production of Multi-Type Products,IEEE,Conferences,"A smart manufacturing prototype called iCandy Box, used for hybrid packing of assorted candies, was designed to study and verify the cyber-physical control methods. The prototype is aimed to provide personalized consumption, as it can perform flexible and customized production. The prototype is powered by a cloud-edge-end enabled collaborative information framework, which can support both industrial big data and artificial intelligence applications. Furthermore, it is characterized by modularization and interdisciplinarity; therefore, it can be used to carry out both experiments and training in several major fields, including smart manufacturing and IoT. The experimental results have shown that the prototype can carry out hybrid production, paving the way for the study and verification of cyber-physical control methods.",https://ieeexplore.ieee.org/document/9525542/,2021 International Conference on Artificial Intelligence and Electromechanical Automation (AIEA),14-16 May 2021,ieeexplore
10.1109/IPFA49335.2020.9260582,"Detection and prevention of assembly defects, by machine learning algorithms, in semiconductor industry for automotive",IEEE,Conferences,"Years of experience within semiconductor manufacturing facilities have led to optimize processes to serve both quality and cost. The solution to achieve next generational levels requires a new approach: this one is fitting with implementation of advanced analytics and machine learning algorithms. Applied to manufacturing data which corresponds with a real big data context, these algorithms can provide insights and automate responses to detect, prevent and ultimately eliminate the most severe failure modes. The project described in this paper targets a wafer sawing process. Various challenges that are raised in such a project are of different natures. A first one is the need for a high level of technical expertise in the manufacturing process of focus: this is essential to define the meaningful dataset that represents comprehensively the desired output of the process. Another component is the data collection aspect: many data have to be collected, stored and parsed, and some small signals found will become the leading indicator to an upcoming process degradation and capability of capturing them is essential. Another key data is traceability of the processed material. Additionally, ensuring an informatic technology architecture to support collection, storage, parsing and computation of the datasets is a significant challenge. Lastly, project success is related to the data scientist expertise to build adequate machine learning algorithms. Optimization of the models can take several iterations with back and forth communication between data scientists and process technical experts. This paper describes issues revealed, some solutions found, and future expectations.",https://ieeexplore.ieee.org/document/9260582/,2020 IEEE International Symposium on the Physical and Failure Analysis of Integrated Circuits (IPFA),20-23 July 2020,ieeexplore
10.23919/ICAC50006.2021.9594118,Deterministic Planning for Flexible Intralogistics,IEEE,Conferences,"An automated planning unit that enables the user to deterministically schedule transportation tasks for intralogistics use cases is proposed. The developed solution aims at inducing a high degree of determinism into transportation task planning in manufacturing industries while at the same time providing the user with the opportunity to flexibly react to rapidly changing constraints, such as updated order situations. The main objective of the software tool is to facilitate the order management process and ensure conflict-free path planning and following of a centrally guided fleet of mobile robots serving transportation tasks. Furthermore, in order to meet customer demands in terms of responsiveness to altered circumstances, the system is able to re-allocate already planned transportation tasks in favor of more urgent ones that may come in without further notice. This is achieved by adopting concepts commonly used in real-time operating systems to the complex problem of intralogistics task scheduling. Sporadically incoming transportation tasks are scheduled dynamically with regard to deadlines, priority levels, available resources as well as estimated execution effort. Flexibility and system responsiveness are increased noticeable by applying on-line task migration mechanisms. The eligibility of the adapted concepts is demonstrated by deploying the proposed solution to test cases within a simulation environment. For this purpose a scalable Key Performance Indicator (KPI) function is proposed as well.",https://ieeexplore.ieee.org/document/9594118/,2021 26th International Conference on Automation and Computing (ICAC),2-4 Sept. 2021,ieeexplore
10.1109/IAI50351.2020.9262203,Digital Twin Enabled Smart Control Engineering as an Industrial AI: A New Framework and Case Study,IEEE,Conferences,"In Industry 4.0, the increasing complexity of industrial systems introduces unknown dynamics that affect the performance of manufacturing processes. Thus, Digital Twin appears as a breaking technology to develop virtual representations of any complex system design, analysis, and behavior prediction tasks to enhance the system understanding via enabling capabilities like real-time analytics, or Smart Control Engineering. In this paper, a novel framework is proposed for the design and implementation of Digital Twin applications to the development of Smart Control Engineering. The framework involve the steps of system documentation, Multidomain Simulation, Behavioral Matching, and real-time monitoring, which is applied to develop the Digital Twin for a real-time vision feedback temperature uniformity control. The obtained results show that Digital Twin is a fundamental part of the transformation into Industry 4.0.",https://ieeexplore.ieee.org/document/9262203/,2020 2nd International Conference on Industrial Artificial Intelligence (IAI),23-25 Oct. 2020,ieeexplore
10.1109/AIHAS.1991.138480,"EMM-networking model for FMS modeling, simulation and control",IEEE,Conferences,"The major challenge of implementing FMS (flexible manufacturing systems) at the factory level is to realize automated control. An appropriate FMS model is required for the purpose of control software design and implementation. In this paper, the EMM-networking (extended Moore machine) model based on automata theory for FMS control systems is presented. The basic concept behind the EMM-networking model is to identify the structure of complex systems in a hierarchy of abstractions. By using object-oriented methodology, the EMM-networking model of a FMS can be implemented into object-based software modules, based on which an integrated environment for simulation, software construction and real-time control can be realized. The methodology is demonstrated via the simulation of a prototype FMS.&lt;<ETX>&gt;</ETX>",https://ieeexplore.ieee.org/document/138480/,"[1991] Proceedings. The Second Annual Conference on AI, Simulation and Planning in High Autonomy Systems",1-2 April 1991,ieeexplore
10.1109/CSCloud-EdgeCom49738.2020.00050,Edge Computing-based 3D Pose Estimation and Calibration for Robot Arms,IEEE,Conferences,"Industrial robots are widely used in current production lines, and complex pipeline processes, especially those with different assembly requirements, are designed for intelligent manufacturing in the era of industry 4.0. During the new crown epidemic, a large number of car companies used the production line to transform production of medical materials such as masks and protective clothing, which provided a strong guarantee for fighting the epidemic. In this scenario, a pipeline is often assembled from robotic arms from multiple suppliers. The traditional methods is complex and takes a lot of time. In this paper, we propose a novel deep learning based robot arm 3D pose estimation and calibration model with simple Kinect stereo cameras which can be deployed on light-weight edge computing systems. The light-weight deep CNN model can detection 5 predefined key points based on RGB-D data. In this way, when the assembly line composed of different robot arms needs to be reassembled, our model can quickly provide the robot's pose information without additional tuning processes. Testing in Webots with Rokae xb4 robot arm model shows that our model can quickly estimate the key point of the robot arm.",https://ieeexplore.ieee.org/document/9170983/,2020 7th IEEE International Conference on Cyber Security and Cloud Computing (CSCloud)/2020 6th IEEE International Conference on Edge Computing and Scalable Cloud (EdgeCom),1-3 Aug. 2020,ieeexplore
10.1109/ICICT50816.2021.9358469,Efficient Fault Isolation Method to Monitor Industrial Batch Processes,IEEE,Conferences,Industrial batch processes are very popular manufacturing system with large number of process variables involved. Monitoring of batch processes using statistical process monitoring becomes very difficult in view of the complex correlations between the process variables. This paper focuses on a fault isolation based process monitoring method without prior information of fault where fault isolation problem is converted into a variable selection. Variable selection is a learning algorithm used here to solve the problem of selection and isolation of variables from a model. The method discussed here uses a sparse coefficient based dissimilarity analysis algorithm known as Sparse Dissimilarity Algorithm(SDISSIM) which checks a calculated D-index for identifying fault in the process. A sparse coefficient is tabulated to verify the process variables contributing to the fault and an absolute variance difference is calculated to select the variables for fault isolation. Finally SDISSIM method is explained by successful implementation in MATLAB with real time industrial process data.,https://ieeexplore.ieee.org/document/9358469/,2021 6th International Conference on Inventive Computation Technologies (ICICT),20-22 Jan. 2021,ieeexplore
10.1109/CASE48305.2020.9249228,Efficiently Learning a Distributed Control Policy in Cyber-Physical Production Systems Via Simulation Optimization,IEEE,Conferences,"The manufacturing industry is becoming more dynamic than ever. The limitations of non-deterministic network delays and real-time requirements call for decentralized control. For such dynamic and complex systems, learning methods stand out as a transformational technology to have a more flexible control solution. Using simulation for learning enables the description of highly dynamic systems and provides samples without occupying a real facility. However, it requires prohibitively expensive computation. In this paper, we argue that simulation optimization is a powerful tool that can be applied to various simulation-based learning processes for tremendous effects. We proposed an efficient policy learning framework, ROSA (Reinforcement-learning enhanced by Optimal Simulation Allocation), with unprecedented integration of learning, control, and simulation optimization techniques, which can drastically improve the efficiency of policy learning in a cyber-physical system. A proof-of-concept is implemented on a conveyer-switch network, demonstrating how ROSA can be applied for efficient policy learning, with an emphasis on the industrial distributed control system.",https://ieeexplore.ieee.org/document/9249228/,2020 IEEE 16th International Conference on Automation Science and Engineering (CASE),20-21 Aug. 2020,ieeexplore
10.1109/CIMSA.2003.1227220,Embedded e-diagnostic for distributed industrial machinery,IEEE,Conferences,"Industrial process machine failure often causes severe financial implications. This is compounded by the lack of availability of experts and the complications of getting them to site. One solution is to give the expert access to the machine remotely with the addition of an Artificial Intelligence (AI) based diagnostics software to assist with the decision making process. Our research is based on such a system, which combines modern communications with intelligent diagnostics software. Accessibility to process machines can now be global with the promise of predictability to the diagnosis. It is felt the importance of this research work cannot be overstated with the constantly moving worldwide manufacturing base and the real situation of the machine designers being based in a different country to their customer. The most vulnerable areas of a machine are its parts that consist of electro-mechanical actuation. The author utilises conventional Newtonian physics and differential calculus to model these and an AI technique of fault prediction and detection.",https://ieeexplore.ieee.org/document/1227220/,"The 3rd International Workshop on Scientific Use of Submarine Cables and Related Technologies, 2003.",31-31 July 2003,ieeexplore
10.1109/RO-MAN46459.2019.8956327,End-User Programming of Low-and High-Level Actions for Robotic Task Planning,IEEE,Conferences,"Programming robots for general purpose applications is extremely challenging due to the great diversity of end-user tasks ranging from manufacturing environments to personal homes. Recent work has focused on enabling end-users to program robots using Programming by Demonstration. However, teaching robots new actions from scratch that can be reused for unseen tasks remains a difficult challenge and is generally left up to robotic experts. We propose iRoPro, an interactive Robot Programming framework that allows end-users to teach robots new actions from scratch and reuse them with a task planner. In this work we provide a system implementation on a two-armed Baxter robot that (i) allows simultaneous teaching of low-and high-level actions by demonstration, (ii) includes a user interface for action creation with condition inference and modification, and (iii) allows creating and solving previously unseen problems using a task planner for the robot to execute in real-time. We evaluate the generalisation power of the system on six benchmark tasks and show how taught actions can be easily reused for complex tasks. We further demonstrate its usability with a user study (N=21), where users completed eight tasks to teach the robot new actions that are reused with a task planner. The study demonstrates that users with any programming level and educational background can easily learn and use the system.",https://ieeexplore.ieee.org/document/8956327/,2019 28th IEEE International Conference on Robot and Human Interactive Communication (RO-MAN),14-18 Oct. 2019,ieeexplore
10.1109/CBD.2014.24,Energy Consumption Data Based Machine Anomaly Detection,IEEE,Conferences,"The ever increasing of product development and the scarcity of the energy resources that those manufacturing activities heavily rely on have made it of great significance the study on how to improve the energy efficiency in manufacturing environment. Energy consumption sensing and collection enables the development of effective solutions to higher energy efficiency. Further, it is found that the data on energy consumption of manufacturing machines also contains the information on the conditions of these machines. In this paper, methods of machine anomaly detection based on energy consumption information are developed and applied to cases on our Syil X4 computer numerical control (CNC) milling machine. Further, given massive amount of energy consumption data from large amount machining tasks, the proposed algorithms are being implemented on a Storm and Hadoop based framework aiming at online real-time machine anomaly detection.",https://ieeexplore.ieee.org/document/7176083/,2014 Second International Conference on Advanced Cloud and Big Data,20-22 Nov. 2014,ieeexplore
10.1109/IIAI-AAI.2016.111,Estimating Job Flow Times by Using an Agent-Based Approach,IEEE,Conferences,"We consider the problem of estimating flow times of jobs that arrive dynamically in a manufacturing system. A job's flow time refers to the time between the job's arrival and completion. Most existing methods use some predefined equations for such estimation, and most of the equations are designed for single machine manufacturing systems. To better estimate the flow time of a job in a more complex system in which there are multiple machines and multiple workstations, we propose a distributed learning approach that divides the manufacturing system into multiple small parts and collects real-time local information in each part to predict the waiting time for a job. We evaluate the proposed approach by comparing it with existing methods using a variety of problem instances. The results show that the proposed approach outperforms existing methods and accordingly might improve the level of customer service when being used for due date promising.",https://ieeexplore.ieee.org/document/7557754/,2016 5th IIAI International Congress on Advanced Applied Informatics (IIAI-AAI),10-14 July 2016,ieeexplore
10.1109/EUROSIM.2013.39,Experimental and Computational Materials Defects Investigation,IEEE,Conferences,"Production of railway axles (i.e., one of the basic material of the modern train) is an elaborate process unfree from faults and problems. Errors during the manufacturing or the plies' overlapping, in fact, can cause particular flaws in the resulting material, so compromising its same integrity. Within this framework, ultrasonic tests could be useful to characterize the presence of defect, depending on its dimensions. On the contrary, the requirement of a perfect state for used materials is unavoidable in order to assure both transport reliability and passenger safety. Therefore, a real-time approach able to recognize and classify the defect starting from the finite element simulated ultrasonic echoes could be very useful in industrial applications. The ill-posedness of the so defined process induces a regularization method. In this paper, a finite element and a heuristic approach are proposed. Particularly, the proposed method is based on the use of a Neural Network approach, the so called ""learning by sample techniques"", and on the use of Support Vector Machines in order to classify the kind of defect. Results assure good performances of the implemented approach, with very interesting applications.",https://ieeexplore.ieee.org/document/7004937/,2013 8th EUROSIM Congress on Modelling and Simulation,10-13 Sept. 2013,ieeexplore
10.1109/WSC.1989.718775,Expert Simulation For On-line Scheduling,IEEE,Conferences,"In recent years, the automotive industry has realized the importance of speed of new products to market and has mounted efforts for improving it. The Expert System Scheduler (ESS) facilitates these efforts by enabling manufacturing plants to generate viable schedules under increasing constraints and demands for flexibility. The scheduler takes advantage of the Computer Integrated Manufacturing (CIM) environment by utilizing the real-time information from the factory for responsive scheduling. The Expert System Scheduler uses heuristics developed by an experiences factory scheduler. It uses simulation concepts and these heuristics to generate schedules. Forward and ""backward"" simulation are used at different stages of the schedule generation process. The system is used to control parts flow on the factory floor at one automated facility. This highly automated facility is a testbed for implementation of CIM concepts. The scheduler runs on a Texas Instruments (TI) Explorer II computer using software developed inhouse utilizing IntelliCorp's Knowledge Engineering Environment (KEE) shell and the LISP language. The scheduling computer is networked to the factory control computer, which actually controls the plant floor. The TI Explorer II acquires current plant floor information from the factory control system, generates a new schedule and sends it back within a short time. The configuration allows fast response to changes in requirements and plant floor conditions.",https://ieeexplore.ieee.org/document/718775/,1989 Winter Simulation Conference Proceedings,4-6 Dec. 1989,ieeexplore
10.1109/IS.2018.8710554,Exploiting the Digital Twin in the Assessment and Optimization of Sustainability Performances,IEEE,Conferences,"Digitalization has shown the potential to disrupt industrial value chains by supporting real-time, risk-free and inexpensive inputs to decision making towards enhanced companies' productivity and value networks flexibility. Developing a reliable and robust digital replica of the physical systems of the value chain is one of the most advanced (and challenging) approaches to digitalization, condensed in the concept of Digital Twin (DT). DT plays a fundamental role in creating a data-rich environment where simulation and optimization procedures can be run. With DT expected to become a commodity in the coming years, simulation and optimization become therefore a more accessible instrument for the improvement of manufacturing and business processes also in small enterprises with limited investment capacity. While scientific literature has analysed the adoption of DT in the optimization of products lifecycle, no contributions have yet focused on the exploitation of DT to improve the sustainability performances of whole value chains. In this paper we propose a reference framework where DTs built upon process and system data gathered from the field, allow to quickly assess the sustainability performances of both existing and planned production mixes and to compare achievable impacts with changing processes and technologies, thus enabling advisory features for sustainability-aware decision making in structured, multi-entity value networks. Internal validation will be deployed referring to real case studies.",https://ieeexplore.ieee.org/document/8710554/,2018 International Conference on Intelligent Systems (IS),25-27 Sept. 2018,ieeexplore
10.1109/INFOCT.2018.8356831,Fault class prediction in unsupervised learning using model-based clustering approach,IEEE,Conferences,"Manufacturing industries have been on a steady path considering for new methods to achieve near-zero downtime to have flexibility in the manufacturing process and being economical. In the last decade with the availability of industrial internet of things (IIoT) devices, this has made it possible to monitor the machine continuously using wireless sensors, assess the degradation and predict the failures of time. Condition-based predictive maintenance has made a significant influence in monitoring the asset and predicting the failure of time. This has minimized the impact on production, quality, and maintenance cost. Numerous approaches have been in proposed over the years and implemented in supervised learning. In this paper, challenges of supervised learning such as need for historical data and incapable of classifying new faults accurately will be overcome with a new methodology using unsupervised learning for rapid implementation of predictive maintenance activity which includes fault prediction and fault class detection for known and unknown faults using density estimation via Gaussian Mixture Model Clustering and K-means algorithm and compare their results with a real case vibration data.",https://ieeexplore.ieee.org/document/8356831/,2018 International Conference on Information and Computer Technologies (ICICT),23-25 March 2018,ieeexplore
10.1109/CNNA.2008.4588677,Feature extraction in laser welding processes,IEEE,Conferences,"There is a rapidly growing demand for laser welding in a wide variety of manufacturing processes ranging from automobile production to precision mechanics. Up to now, the high dynamics of the process has made it impossible to construct a camera based real time quality and process control. Since new pixel parallel architectures are existing, which are now available in systems such as the ACE16k, Q-Eye, and SCAMP-3 (P. Dudek et al., 2006), one has become able to implement a real time laser welding processing. In this paper we will propose a feature extraction algorithm, running at a frame rate of 10 kHz, for a laser welding process. The performance of the algorithm has been studied in detail. In particular, it has been implemented on an Eye-RIS v.1.1 system and has been applied to laser welding processes.",https://ieeexplore.ieee.org/document/4588677/,2008 11th International Workshop on Cellular Neural Networks and Their Applications,14-16 July 2008,ieeexplore
10.1109/AI4I51902.2021.00022,Generating Reinforcement Learning Environments for Industrial Communication Protocols,IEEE,Conferences,"An important part of any reinforcement learning application is interfacing the agent to its environment. To enable an easier use of reinforcement learning agents in manufacturing and automation-related real-world environments, we propose an environment generator which acts as an adapter between the interface of the agent and existing industrial communication protocols. This paper describes the functionality and architecture of such an environment generator.",https://ieeexplore.ieee.org/document/9565507/,2021 4th International Conference on Artificial Intelligence for Industries (AI4I),20-22 Sept. 2021,ieeexplore
10.1109/CACRE52464.2021.9501291,Give Me a Wrench!: Finding Tools for Human Partners in Human-Robot Collaborative Manufacturing Contexts,IEEE,Conferences,"Manufacturing processes can be optimized by enabling human-robot collaboration. A relevant goal in this area is to create a collaborative solution in which robots can provide assisting actions to humans, thereby, reducing menial labor as well as increasing productivity. The solution is based on implementing efficient hand-over of mechanical tools from robots to humans. Hand-over tasks are inevitable in human-robot collaborative manufacturing contexts. These tasks need three-step mechanism: object identification, object grasping, and the actual hand-over. This paper presents an approach for robots to find tools for human partners in human-robot collaboration via deep learning. This is achieved using the object detection system YOLOv3 for identification of commonly used mechanical tools. By training on a custom dataset of 800 images of mechanical tools created for the study, the tool recognition is implemented in realworld human-robot hand-over tasks. Experimental results show that the proposed approach achieves a high accuracy for identification of tools in real-world human-robot collaboration. Future work of this study is also discussed.",https://ieeexplore.ieee.org/document/9501291/,"2021 6th International Conference on Automation, Control and Robotics Engineering (CACRE)",15-17 July 2021,ieeexplore
10.1109/YAC51587.2020.9337595,Hardware Trojan Detection Based on SRC,IEEE,Conferences,"The security of integrated circuits (IC) plays a very significant role on military, economy, communication and other industries. Due to the globalization of the integrated circuit (IC) from design to manufacturing process, the IC chip is vulnerable to be implanted malicious circuit, which is known as hardware Trojan (HT). When the HT is activated, it will modify the functionality, reduce the reliability of IC, and even leak confidential information about the system and seriously threatens national security. The HT detection theory and method is hotspot in the security of integrated circuit. However, most methods are focusing on the simulated data. Moreover, the measurement data of the real circuit are greatly affected by the measurement noise and process disturbances and few methods are available with small size of the Trojan circuit. In this paper, the problem of detection was cast as signal representation among multiple linear regression and sparse representation-based classifier (SRC) were first applied for Trojan detection. We assume that the training samples from a single class do lie on a subspace, and the test samples can be represented by the single class. The proposed SRC HT detection method on real integrated circuit shows high accuracy and efficiency.",https://ieeexplore.ieee.org/document/9337595/,2020 35th Youth Academic Annual Conference of Chinese Association of Automation (YAC),16-18 Oct. 2020,ieeexplore
10.1109/ASPDAC.2011.5722294,High performance lithographic hotspot detection using hierarchically refined machine learning,IEEE,Conferences,"Under real and continuously improving manufacturing conditions, lithography hotspot detection faces several key challenges. First, real hotspots become less but harder to fix at post-layout stages; second, false alarm rate must be kept low to avoid excessive and expensive post-processing hotspot removal; third, full chip physical verification and optimization require fast turn-around time. To address these issues, we propose a high performance lithographic hotspot detection flow with ultra-fast speed and high fidelity. It consists of a novel set of hotspot signature definitions and a hierarchically refined detection flow with powerful machine learning kernels, ANN (artificial neural network) and SVM (support vector machine). We have implemented our algorithm with industry-strength engine under real manufacturing conditions in 45nm process, and showed that it significantly outperforms previous state-of-the-art algorithms in hotspot detection false alarm rate (2.4X to 2300X reduction) and simulation run-time (5X to 237X reduction), meanwhile archiving similar or slightly better hotspot detection accuracies. Such high performance lithographic hotspot detection under real manufacturing conditions is especially suitable for guiding lithography friendly physical design.",https://ieeexplore.ieee.org/document/5722294/,16th Asia and South Pacific Design Automation Conference (ASP-DAC 2011),25-28 Jan. 2011,ieeexplore
10.1109/IOLTS50870.2020.9159704,High-level Modeling of Manufacturing Faults in Deep Neural Network Accelerators,IEEE,Conferences,"The advent of data-driven real-time applications requires the implementation of Deep Neural Networks (DNNs) on Machine Learning accelerators. Google's Tensor Processing Unit (TPU) is one such neural network accelerator that uses systolic array-based matrix multiplication hardware for computation in its crux. Manufacturing faults at any state element of the matrix multiplication unit can cause unexpected errors in these inference networks. In this paper, we propose a formal model of permanent faults and their propagation in a TPU using the Discrete-Time Markov Chain (DTMC) formalism. The proposed model is analyzed using the probabilistic model checking technique to reason about the likelihood of faulty outputs. The obtained quantitative results show that the classification accuracy is sensitive to the type of permanent faults as well as their location, bit position and the number of layers in the neural network. The conclusions from our theoretical model have been validated using experiments on a digit recognition-based DNN.",https://ieeexplore.ieee.org/document/9159704/,2020 IEEE 26th International Symposium on On-Line Testing and Robust System Design (IOLTS),13-15 July 2020,ieeexplore
10.1109/ICSMC.2000.886346,Holonic self-organization of multi-agent systems by fuzzy modeling with application to intelligent manufacturing,IEEE,Conferences,"Holonic manufacturing aims to design standardized, modular manufacturing systems made of interchangeable parts, to enable flexibility, online reconfigurability and self-organizing capabilities for the production systems. Recent advances in distributed artificial intelligence and networking technologies have proven that theoretical multi-agent systems (MAS) concepts are very suitable for the real life implementation of holonic concepts. Building on our recent results in the design and implementation of holonic reconfigurable architectures, the paper introduces a novel approach to the online self-organization of distributed systems. By using fuzzy set and uncertainty theoretical concepts, we construct a mathematical foundation for modeling MAS, where appropriate holonic structures are identified for each particular application. This approach opens new possibilities for the design of any distributed system that needs self-organization as an intrinsic property.",https://ieeexplore.ieee.org/document/886346/,"Smc 2000 conference proceedings. 2000 ieee international conference on systems, man and cybernetics. 'cybernetics evolving to systems, humans, organizations, and their complex interactions' (cat. no.0",8-11 Oct. 2000,ieeexplore
10.1109/NDS.2017.8070626,"Image-driven, model-free control of repetitive processes based on machine learning",IEEE,Conferences,"An image-driven, model-free approach to design control systems for a large class of industrial process is proposed. A mathematical model of the process is replaced by sequences of subsequent images which play the role of the process (plant) states. The length of this sequences depends on the speed of the process dynamics and on the frame rate. Firstly, a learning sequence of the system states is collected and then, it is used for classifying (clustering) its states. A decision of the control system is attached by an expert to each class (cluster) of the states. At the implementation stage images from a camera in the loop are collected into subsequences corresponding to the system states, then they are classified and a control action corresponding to a class at hand is undertaken. This general idea is exemplified by the case study of a laser power control in an additive manufacturing, which is a repetitive process. A tree-like, hierarchical classifier is proposed in order to recognize the process states, each consisting of three consecutive images. Its performance is tested on real-life images from the process of laser cladding additive manufacturing.",https://ieeexplore.ieee.org/document/8070626/,2017 10th International Workshop on Multidimensional (nD) Systems (nDS),13-15 Sept. 2017,ieeexplore
10.1109/IAS.2006.256774,Implementation of Emotional Controller for Interior Permanent Magnet Synchronous Motor Drive,IEEE,Conferences,"This paper presents for the first time the real-time implementation of an emotional controller for interior permanent magnet synchronous motor (IPMSM) drives. The proposed novel controller is called brain emotional learning based intelligent controller (BELBIC). The utilization of BELBIC is based on emotion processing mechanism in brain, and is essentially an action, which is based on sensory inputs and emotional cues. The emotional learning occurs mainly in the amygdala. The amygdala is mathematically modeled, and the BELBIC controller is introduced. This type of controller is insensitive to noise and variance of the parameters. The controller is successfully implemented in real-time using a digital signal processor board ds1102 for a laboratory 1-hp IPMSM. The results show superior control characteristics especially very fast response, simple implementation and robustness with respect to disturbances and manufacturing imperfections. The proposed method enables the designer to shape the response in accordance with the multiple objectives of choices",https://ieeexplore.ieee.org/document/4025462/,Conference Record of the 2006 IEEE Industry Applications Conference Forty-First IAS Annual Meeting,8-12 Oct. 2006,ieeexplore
10.1109/ICPHYS.2019.8780271,Implementation of Industrial Cyber Physical System: Challenges and Solutions,IEEE,Conferences,"The Industry Internet of Things (IIoT) and the Industry Cyber-Physical System (ICPS) for real industry are becoming vitally necessary in the smart manufacturing environment. Very large number of intelligent sensors are being available generating an exploding amount of data. Several issues come with the big data in real industry, including the a grand-scale connected network construction with the data security and access protocol issues, data quality with considerable noise when gathered from industrial factories, efficient data storage, smart interconnection with cloud services, and real-time analytics requirements. This paper proposes an integrated CPS based architecture for smart manufacturing and provides the deployment details, addressing all the potential problems in an appropriate way. It has been successfully implemented in a real industry environment, and won the Best Industry Application of IoT at the BigInsights Data &amp; AI Innovation Awards.",https://ieeexplore.ieee.org/document/8780271/,2019 IEEE International Conference on Industrial Cyber Physical Systems (ICPS),6-9 May 2019,ieeexplore
10.1109/ICCSP.2018.8524377,Implementation of Robotic Vision to Perform Threaded Assembly,IEEE,Conferences,"In manufacturing of mechanical parts and assemblies, proper thread-engagement between a bolt and a nut is vital for the performance and reliability of the product. Typically, this is a precision work, requiring repetitive manual operations. In this paper, we explain how such assembly operations can be carried out by collaborative robots (co-bots) by monitoring the position and orientation of the nut and bolt using an image-sensor (camera). The focus of our discussion is the assembly-operation of bolting of a nut by the grippers of a co-bot. Slips and misalignment, leading to wrong positioning of the nut and the bolt, are identified by capturing the images of the two components in real time using Microsoft Kinect camera-sensor. 3D Reconstruction of the image captured by the camera-sensor is carried out using the Kinect Fusion application. The reconstructed image is in the form of a polygonal mesh which is further converted to 3D Point Cloud data which is less sensitive to noise. Thereafter, the Point Cloud is segmented by dividing the entire scene into many clusters in order to distinguish the objects of the scene as grippers and nut and bolt. These clusters can be used for the training of the co-bot for the proposed operation. This method of extracting object-boundaries leading to recognition of objects is a vital operation in the field of robotic vision. We provide baseline description of various machine learning techniques that can be applied to realize proper assembly of a nut and a bolt.",https://ieeexplore.ieee.org/document/8524377/,2018 International Conference on Communication and Signal Processing (ICCSP),3-5 April 2018,ieeexplore
10.1109/ACC.1997.609709,Increasing signal accuracy of automotive wheel-speed sensors by online learning,IEEE,Conferences,"The wheel speed is an important signal for modern automotive control systems. The performance of these systems is closely related to the quality of the processed wheel speed. However, due to manufacturing tolerances or corrosion respectively of the sensor gear wheel, the quality of the signal is affected negatively. In this paper a software-based method to compensate for the mechanical inaccuracy of the sensor wheel is introduced. The approach increases the signal accuracy of conventional automotive wheel-speed sensors significantly. Moreover, with the presented method, the demand for manufacturing precision of the sensor wheel is reduced, which in turn cuts down the costs for mass production considerably. The correction is obtained by online learning of a correction factor for each sensor impulse using a fuzzy model for reconstructing the actual wheel-speed and recursive parameter identification for estimating the correction factors. A method to determine the angular position of the gear wheel without a reference impulse is proposed. The introduced approach is assessed by applying it to real world data.",https://ieeexplore.ieee.org/document/609709/,Proceedings of the 1997 American Control Conference (Cat. No.97CH36041),6-6 June 1997,ieeexplore
10.1109/CASE48305.2020.9216902,Industrial Robot Grasping with Deep Learning using a Programmable Logic Controller (PLC),IEEE,Conferences,"Universal grasping of a diverse range of previously unseen objects from heaps is a grand challenge in e-commerce order fulfillment, manufacturing, and home service robotics. Recently, deep learning based grasping approaches have demonstrated results that make them increasingly interesting for industrial deployments. This paper explores the problem from an automation systems point-of-view. We develop a robotics grasping system using Dex-Net, which is fully integrated at the controller level. Two neural networks are deployed on a novel industrial AI hardware acceleration module close to a PLC with a power footprint of less than 10 W for the overall system. The software is tightly integrated with the hardware allowing for fast and efficient data processing and real-time communication. The success rate of grasping an object form a bin is up to 95% with more than 350 picks per hour, if object and receptive bins are in close proximity. The system was presented at the Hannover Fair 2019 (world's largest industrial trade fair) and other events, where it performed over 5,000 grasps per event.",https://ieeexplore.ieee.org/document/9216902/,2020 IEEE 16th International Conference on Automation Science and Engineering (CASE),20-21 Aug. 2020,ieeexplore
10.23919/ACC45564.2020.9147268,Inferential Methods for Additive Manufacturing Feedback,IEEE,Conferences,"Adaptive manufacturing has revolutionized desktop prototyping and the production of physical models for non-load bearing or stress inducing applications. Many extrusion-based printers are available for purchase by entrepreneurial enthusiasts or businesses with manufacturing space limitations. These low-cost printers allow for quick prototyping but are not designed or intended for high quality production or high-cycle production, requiring extensive user tuning and upkeep to maintain the printer in usable condition. In a quest to apply modern deep learning and reinforcement learning based models, this work focuses on the development of control systems and infrastructure needed to resolve many of these intrinsic limitations of desktop 3D printers. A series of real-time agents were designed and deployed to actively monitor the printing of every layer and make continuous corrections in the printing parameters and G-code commands to reduce the variance in the tensile strength of homogeneous parts printed in a large batch.",https://ieeexplore.ieee.org/document/9147268/,2020 American Control Conference (ACC),1-3 July 2020,ieeexplore
10.1109/SNPD.2007.401,Integrating Agents and Web Services into Cooperative Design Platform of Vehicle Headlights,IEEE,Conferences,"Web-based distributed cooperative design is an important technology in the computer supported advanced manufacturing. After analyzing the advantages and drawbacks of the existing research and work, this paper presents an agent-based cooperative design platform which utilizes Web service to realize interoperability in vehicle headlights design. Compared with other strategies, we put emphases on real-time communication between designers and high speed browse of all models. Our platform also has a good compatibility with various model-design kits by adopting intelligent agents. The proposed approach provides designers with excellent human-computer interaction, more flexibility and enables distributed collaborators to cooperate efficiently. The platform can greatly shorten product development period as well as time to market.",https://ieeexplore.ieee.org/document/4287468/,"Eighth ACIS International Conference on Software Engineering, Artificial Intelligence, Networking, and Parallel/Distributed Computing (SNPD 2007)",30 July-1 Aug. 2007,ieeexplore
10.1109/IPMM.1999.792505,"Integration of newly developed AI assembly, production, and material flow virtual tools",IEEE,Conferences,We discuss the applicability of artificial intelligence virtual tools in addressing real world design for manufacturing issues and the lessons learned from experimenting with this approach. A current project that has predicted a 70% reduction in scheduling as achievable is addressed. Operation of the real production line of the project is discussed with a comparison of predicted values versus actual.,https://ieeexplore.ieee.org/document/792505/,Proceedings of the Second International Conference on Intelligent Processing and Manufacturing of Materials. IPMM'99 (Cat. No.99EX296),10-15 July 1999,ieeexplore
10.1109/ISIC.1990.128585,Intelligent processing in spray forming applications,IEEE,Conferences,The authors describe a program to develop sensor and control technology for real-time implementation with spray forming. The objectives of this effort are to ensure reproducibility and quality of spray-formed products and to expand the capability of manufacturing asymmetric shapes. Sensor data are used by a fuzzy logic intelligent controller to make adjustments to spray-forming process parameters during preform deposition. Advanced manipulation capabilities are required to produce asymmetric components.&lt;<ETX>&gt;</ETX>,https://ieeexplore.ieee.org/document/128585/,Proceedings. 5th IEEE International Symposium on Intelligent Control 1990,5-7 Sept. 1990,ieeexplore
10.1109/ICAT.2013.6684074,Intelligent system for inspection and selection of parts in a manufacturing cell,IEEE,Conferences,"This paper addresses the design and implementation of an artificial vision system implemented in a manufacturing cell. The vision system recognizes and selects in an intelligent manner the manufactured parts through a feedforward artificial neural network and the decisions are completely based on the part's color and its geometry. A simple digital camera is used as an image acquisition device. This image is then processed by an artificial neural network, which is able to identify the part's color. Then a Programmable Logic Controller (PLC) drives an electropneumatic system, in order to store the identified part into a corresponding repository. An interface based on power electronic devices and a Data Acquisition Card (DAQ) system implements the communication between the PLC and the computer. The proposed system is completely implemented and tested in a real Flexible Manufacturing System (FMS) of FESTO© showing good results.",https://ieeexplore.ieee.org/document/6684074/,"2013 XXIV International Conference on Information, Communication and Automation Technologies (ICAT)",30 Oct.-1 Nov. 2013,ieeexplore
10.1109/WF-IoT51360.2021.9595035,Interpretable Multi-Step Production Optimization Utilizing IoT Sensor Data,IEEE,Conferences,"In an industrial manufacturing process, such as petroleum, chemical, and food processing, with the deployment of thousands of sensors in the plants, we have the chance to provide real-time onsite management for the processes. Beyond the real-time status update, utilizing vast IoT data and creating machine learning and optimization models provide us with intelligent business recommendations. Those are used by the site engineers and managers to make real-time decisions in a situation with multiple conflicting operational and business goals. Those goals include maximizing financial gain, minimizing costs, limiting the usage of certain raw materials or additives, decreasing environmental impact, and more. When formalizing these decision-making tasks, often there is no prior knowledge of compromise between the conflicting goals. That poses a challenge to generate a proper objective function. In this paper, we create a Multi-Step optimization process to address this uncertainty of selecting proper objectives and their preferences. Instead of using an explicit trade-off to create a single weighted objective function (as a traditional approach) and rely on a single attempt to find the optimal solution, we decompose this problem into multiple steps. In each step, we optimize only one objective from one KPI with an exact semantic meaning. We demonstrate the usability of the approach using a practical application from an oil sands processing facility, provide modeling results focusing on the response to business priorities, performance, and interpretability. The multi-step approach presents the convergence of the target goal with an outcome KPI with comparison for each step to illustrate the enhanced interpretability.",https://ieeexplore.ieee.org/document/9595035/,2021 IEEE 7th World Forum on Internet of Things (WF-IoT),14 June-31 July 2021,ieeexplore
10.1109/DASA51403.2020.9317177,"IoT Driven Resiliency with Artificial Intelligence, Machine Learning and Analytics for Digital Transformation",IEEE,Conferences,"A new manufacturing era “Industry 4.0” is emerging with two unique characteristics: intelligent manufacturing and integrated manufacturing. This pattern rationale with the progress of digital transformation, in which efficient manufacturing and production systems is being continuously pursued. Digital transformation initiatives generate large data sets due to massive integration of devices in internet of things (IoT) environment. This scenario demands the fastest insights to respond on time considering three key pillars: communication network evolution, digital business, and customer experience. IoT driven resiliency with traditional analytics has limited value without artificial intelligence (AI), and machine learning (ML). This study aims to explore this phenomenon of interest by conducting group discussion with software vendors. The results will helpful to utilize the power of AI and ML with analytics to leverage a large amount of data which would contribute to the success of digital transformation of organizations with real-time decision-making.",https://ieeexplore.ieee.org/document/9317177/,2020 International Conference on Decision Aid Sciences and Application (DASA),8-9 Nov. 2020,ieeexplore
10.1109/IoTDI49375.2020.00026,IoT-ID: A Novel Device-Specific Identifier Based on Unique Hardware Fingerprints,IEEE,Conferences,"A significant number of IoT devices are being deployed in the wild, mostly in remote locations and in untrusted conditions. This could include monitoring an electronic perimeter fence or a critical infrastructure such as telecom and power grids. Such applications rely on the fidelity of data reported from the IoT devices, and hence it is imperative to identify the trustworthiness of the remote device before taking decisions. Existing approaches use a secret key usually stored in volatile or non-volatile memory for creating an encrypted digital signature. However, these techniques are vulnerable to malicious attacks and have significant computation and energy overhead. This paper presents a novel device-specific identifier, IoT-ID that captures the device characteristics and can be used towards device identification. IoT-ID is based on physically unclonable functions (PUFs), that exploit variations in the manufacturing process to derive a unique fingerprint for integrated circuits. In this work, we design novel PUFs for Commercially Off the Shelf (COTS) components such as clock oscillators and ADC, to derive IoT-ID for a device. Hitherto, system component PUFs are invasive and rely on additional dedicated hardware circuitry to create a unique fingerprint. A highlight of our PUFs is doing away with special hardware. IoT-ID is non-invasive and can be invoked using simple software APIs running on COTS components. IoT-ID has the following key properties viz., constructability, real-time, uniqueness, and reproducibility, making them robust device-specific identifiers. We present detailed experimental results from our live deployment of 50 IoT devices running over a month. Our edge machine learning algorithm has 100% accuracy in uniquely identifying the 50 devices in our deployment and can run locally on the resource-constrained IoT device. We show the scalability of IoT-ID with the help of numerical analysis on 1000s of IoT devices.",https://ieeexplore.ieee.org/document/9097592/,2020 IEEE/ACM Fifth International Conference on Internet-of-Things Design and Implementation (IoTDI),21-24 April 2020,ieeexplore
10.1109/IEEM45057.2020.9309776,Job Shop Scheduling Problem Neural Network Solver with Dispatching Rules,IEEE,Conferences,"Job Shop Scheduling Problem (JSSP) is an optimization problem in computer science and operations research. Many problems in real-world manufacturing processes can be translated into JSSP. In recent years, Machine Learning has shown great promises in solving optimization problems and can be used to solve JSSP instances. In this paper, an Artificial Neural Network (ANN) was designed and trained to solve JSSP instances using the priority of the operations as the learning output. Dispatching rules were implemented to break ties during the decoding of the priorities. Our experiment results showed that a hybrid algorithm that combines the best of ANN with dispatching rules and standalone dispatching rule-based heuristic outperforms previously reported results.",https://ieeexplore.ieee.org/document/9309776/,2020 IEEE International Conference on Industrial Engineering and Engineering Management (IEEM),14-17 Dec. 2020,ieeexplore
10.1109/ReConFig48160.2019.8994747,Keynote2: Global-Scale FPGA-Accelerated Deep Learning Inference with Microsoft's Project Brainwave,IEEE,Conferences,"The computational challenges involved in accelerating deep learning have attracted the attention of computer architects across academia and industry. While many deep learning accelerators are currently theoretical or exist only in prototype form, Microsoft's Project Brainwave is in massive-scale production in data centers across the world. Project Brainwave runs on our Catapult networked FPGAs, which provide latency that is low enough to enable ""Real-time Al"" - deep learning inference that is fast enough for interactive services and achieves peak throughput at a batch size of 1. Project Brainwave powers the latest version of Microsoft's Bing search engine, which uses cutting-edge neural network models that are much larger than the neural networks used in typical benchmarks. In this talk, I'll discuss how Project Brainwave's FPGA-based and software components work together to accelerate both first-party workloads - like Bing search - and third-party applications using neural network models, like high energy physics and manufacturing quality control. I'll also talk about how FPGAs are the perfect platform for the fast-changing world of deep neural networks, since their reconfigurability allows us to update our accelerator in place to keep up with the state of the art.",https://ieeexplore.ieee.org/document/8994747/,2019 International Conference on ReConFigurable Computing and FPGAs (ReConFig),9-11 Dec. 2019,ieeexplore
10.1109/SMC.2017.8122711,Knowledge extracted from recurrent deep belief network for real time deterministic control,IEEE,Conferences,"Recently, the market on deep learning including not only software but also hardware is developing rapidly. Big data is collected through IoT devices and the industry world will analyze them to improve their manufacturing process. Deep Learning has the hierarchical network architecture to represent the complicated features of input patterns. Although deep learning can show the high capability of classification, prediction, and so on, the implementation on GPU devices are required. We may meet the trade-off between the higher precision by deep learning and the higher cost with GPU devices. We can success the knowledge extraction from the trained deep learning with high classification capability. The knowledge that can realize faster inference of pre-trained deep network is extracted as IF-THEN rules from the network signal flow given input data. Some experiment results with benchmark tests for time series data sets showed the effectiveness of our proposed method related to the computational speed.",https://ieeexplore.ieee.org/document/8122711/,"2017 IEEE International Conference on Systems, Man, and Cybernetics (SMC)",5-8 Oct. 2017,ieeexplore
10.1109/INDIN.2015.7281881,Knowledge-driven finite-state machines. Study case in monitoring industrial equipment,IEEE,Conferences,Traditionally state machines are implemented by coding the desired behavior of a given system. This work proposes the use of ontological models to describe and perform computations on state machines by using SPARQL queries. This approach represents a paradigm shift relating to the customary manner in which state machines are stored and computed. The main contribution of the work is an ontological model to represent state machines and a set of generic queries that can be used in any knowledge-driven state machine to compute valuable information. The approach was tested in a study case were the state machines of industrial robots in a manufacturing line were modeled as ontological models and used for monitoring the behavior of these devices on real time.,https://ieeexplore.ieee.org/document/7281881/,2015 IEEE 13th International Conference on Industrial Informatics (INDIN),22-24 July 2015,ieeexplore
10.1109/WF-IoT48130.2020.9221446,Learner’s Dilemma: IoT Devices Training Strategies in Collaborative Deep Learning,IEEE,Conferences,"With the growth of Internet of Things (IoT) and mobile edge computing, billions of smart devices are interconnected to develop applications used in various domains including smart homes, healthcare and smart manufacturing. Deep learning has been extensively utilized in various IoT applications which require huge amount of data for model training. Due to privacy requirements, smart IoT devices do not release data to a remote third party for their use. To overcome this problem, collaborative approach to deep learning, also known as Collaborative Deep Learning (CDL) has been largely employed in data-driven applications. This approach enables multiple edge IoT devices to train their models locally on mobile edge devices. In this paper, we address IoT device training problem in CDL by analyzing the behavior of mobile edge devices using a game-theoretic model, where each mobile edge device aims at maximizing the accuracy of its local model at the same time limiting the overhead of participating in CDL. We analyze the Nash Equilibrium in an N-player static game model. We further present a novel clusterbased fair strategy to approximately solve the CDL game to enforce mobile edge devices for cooperation. Our experimental results and evaluation analysis in a real-world smart home deployment show that 80% mobile edge devices are ready to cooperate in CDL, while 20% of them do not train their local models collaboratively.",https://ieeexplore.ieee.org/document/9221446/,2020 IEEE 6th World Forum on Internet of Things (WF-IoT),2-16 June 2020,ieeexplore
10.1109/ICSMC.1997.633274,Learning and reasoning method using fuzzy coloured Petri nets under uncertainty,IEEE,Conferences,"Petri nets have been widely used to model computer systems. Manufacturing systems, robotics systems, knowledge-based systems, and other kinds of engineering applications. Further, to present complex real-world knowledge fuzzy Petri net models have been proposed to perform fuzzy reasoning automatically. However, in Petri nets one has to represent all kinds of processes by separate subnets even though the process has the same behavior as another. Real-world knowledge often contains many parts which are similar, but not identical. This means that the total number of Petri nets becomes very large. Therefore, it becomes difficult to see the similarities and the differences among the individual subnets representing similar parts. The problems may be annoying for a small system, and catastrophic for the description of a large-scale system. To avoid this kind of problem the authors propose a learning and reasoning method using fuzzy coloured Petri nets (FCPN) under uncertainty. For the correction of rules of the knowledge-based system a hand-built classifier and empirical learning method based on domain theory have been proposed as machine learning methods, where there is a significant gap between the knowledge-intensive approach in the former and the virtually knowledge-free approach in the letter. To resolve such problems simultaneously they propose a hybrid learning method which is built on top of the knowledge-based fuzzy coloured Petri net and genetic algorithms.",https://ieeexplore.ieee.org/document/633274/,"1997 IEEE International Conference on Systems, Man, and Cybernetics. Computational Cybernetics and Simulation",12-15 Oct. 1997,ieeexplore
10.1109/COASE.2016.7743572,Learning-based dynamic scheduling of semiconductor manufacturing system,IEEE,Conferences,"A learning-based scheduling framework for semiconductor manufacturing system is studied in this paper. This framework obtains a dynamic scheduling model by applying machine learning algorithm based on optimal data samples, through which an approximate optimal scheduling strategy under a certain production state can be acquired on time. And then an implementation of a dynamic scheduling model based on extreme learning machine (ELM) is proposed. In order to improve efficiency, a hybrid feature selection and classification algorithm is suggested, which combines filter feature selection method and wrapper feature selection method. Finally, the proposed dynamic scheduling model is tested in a real semiconductor manufacturing system to compare and analysis between the algorithm performance and production performance. The result indicates that the learning-based scheduling method is superior to single scheduling rules and it also meets the requirements of real-time manufacturing scheduling.",https://ieeexplore.ieee.org/document/7743572/,2016 IEEE International Conference on Automation Science and Engineering (CASE),21-25 Aug. 2016,ieeexplore
10.1109/SNPD.2012.137,MAST: From a Toy to Real-Life Manufacturing Control,IEEE,Conferences,"Paper reports on the evolution of agent-based simulation and control system called MAST. The system was designed originally for agent-based simulation of product routing but over the years matured into a generic purpose manufacturing simulation and control tool featuring real-time connectivity to legacy PLCs, ontology-based dynamic scheduling, advanced diagnostics, etc. Paper describes MAST architecture, behavior of agents and capabilities for dynamic reconfiguration. In addition, two examples of application of MAST to real problems from manufacturing domain are given. The latest trend of exploitation of semantic technologies in industrial agents is discussed.",https://ieeexplore.ieee.org/document/6299316/,"2012 13th ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing",8-10 Aug. 2012,ieeexplore
10.1109/ICECCE49384.2020.9179391,Machine Learning Based Approach to Process Characterization for Smart Devices in 3D Industrial Manufacturing,IEEE,Conferences,"A key differentiator between the additive manufacturing and the traditional injection molding is the precision-manufacturing. An error-free print job significantly guarantees good part quality with minimum wastage of the material and energy. In practice, however, achieving error-free production is quite challenging and this emphasizes the need to learn what behavior of the machine leads to an erroneous job. Knowing the health of the printer or the print job helps quantify print job performance, as well as build system alerts to take reactive actions. Intending to run the model dynamically while the machine is printing, time-series based deep learning models like LSTM are most suitable. This paper presents a machine learning based anomaly detection approach to discover patterns in sensors' measurements in the streaming mode in MultiJet Fusion 3d printer developed at HP Inc. A hybrid architecture of LSTM and Auto-encoder has been proposed to learn the printer behavior generate an alarm in the event of an anomaly. The results of both LSTM and LSTM-Autoencoder models have also been discussed by taking real-life examples of 3D printing jobs.",https://ieeexplore.ieee.org/document/9179391/,"2020 International Conference on Electrical, Communication, and Computer Engineering (ICECCE)",12-13 June 2020,ieeexplore
10.1109/PACET48583.2019.8956251,Machine Learning for Hardware Trojan Detection: A Review,IEEE,Conferences,"Every year, the rate at which technology is applied on areas of our everyday life is increasing at a steady pace. This rapid development drives the technology companies to design and fabricate their integrated circuits (ICs) in non-trustworthy outsourcing foundries in order to reduce the cost. Thus, a synchronous form of virus, known as Hardware Trojans (HTs), was developed. HTs leak encrypted information, degrade device performance or lead to total destruction. To reduce the risks associated with these viruses, various approaches have been developed aiming to prevent and detect them, based on conventional or machine learning methods. Ideally, any undesired modification made to an IC should be detectable by pre-silicon verification/simulation and post-silicon testing. The infected circuit can be inserted in different stages of the manufacturing process, rendering the detection of HTs a complicated procedure. In this paper, we present a comprehensive review of research dedicated to applications based on Machine Learning for the detection of HTs in ICs. The literature is categorized in (a) reverse-engineering development for the imaging phase, (b) real-time detection, (c) golden model-free approaches, (d) detection based on gate-level netlists features and (e) classification approaches.",https://ieeexplore.ieee.org/document/8956251/,2019 Panhellenic Conference on Electronics & Telecommunications (PACET),8-9 Nov. 2019,ieeexplore
10.1109/ICC40277.2020.9148684,Machine Learning for Predictive Diagnostics at the Edge: an IIoT Practical Example,IEEE,Conferences,"Edge Computing is becoming more and more essential for the Industrial Internet of Things (IIoT) for data acquisition from shop floors. The shifting from central (cloud) to distributed (edge nodes) approaches will enhance the capabilities of handling real-time big data from IoT. Furthermore, these paradigms allow moving storage and network resources at the edge of the network closer to IoT devices, thus ensuring low latency, high bandwidth, and location-based awareness. This research aims at developing a reference architecture for data collecting, smart processing, and manufacturing control system in an IIoT environment. In particular, our architecture supports data analytics and Artificial Intelligence (AI) techniques, in particular decentralized and distributed hybrid twins, at the edge of the network. In addition, we claim the possibility to have distributed Machine Learning (ML) by enabling edge devices to learn local ML models and to store them at the edge. Furthermore, edges have the possibility of improving the global model (stored at the cloud) by sending the reinforced local models (stored in different shop floors) towards the cloud. In this paper, we describe our architectural proposal and show a predictive diagnostics case study deployed in an edge-enabled IIoT infrastructure. Reported experimental results show the potential advantages of using the proposed approach for dynamic model reinforcement by using real-time data from IoT instead of using an offline approach at the cloud infrastructure.",https://ieeexplore.ieee.org/document/9148684/,ICC 2020 - 2020 IEEE International Conference on Communications (ICC),7-11 June 2020,ieeexplore
10.1109/CASE48305.2020.9216842,Machine Learning to Empower a Cyber-Physical Machine Tool,IEEE,Conferences,"Machine learning is used to empower a machine tool, which gives rise to a new generation machine tool, i.e. cyber-physical machine tool. The use of four sensors to measure the cutting force, vibration, acoustic emission, and spindle motor current of an end milling machine is proposed. Sixty-five cutting tests using an end milling machine were conducted, during which sensor data was recorded. The flank wear exhibited on the tool following each cut was then measured using a microscope. This provided a labelled data set on which to train four machine learning algorithms: Support Vector Regression, Random Forests, Feed-Forward Back-Propagation Artificial Neural Networks, and Polynomial Regression. These were then compared and it was found that an artificial neural network provides the most accurate predictions of tool flank wear, with a mean absolute percentage accuracy of 90.11%. Using this trained neural network model, a real-time tool wear prediction system was implemented in LabVIEW. This tool condition monitoring system can be used to increase efficiency of manufacturing processes",https://ieeexplore.ieee.org/document/9216842/,2020 IEEE 16th International Conference on Automation Science and Engineering (CASE),20-21 Aug. 2020,ieeexplore
10.1109/PHM-Besancon49106.2020.00009,Machine Performance Monitoring and Fault Classification using Vibration Frequency Analysis,IEEE,Conferences,"Machine anomalies in manufacturing directly affect the production yield and factory operation efficiency if such anomalies cannot be detected in time. Real-time monitoring of machine health condition not only improves machine throughput by reducing unplanned downtime caused by machine failure but also saves cost for unnecessary routine maintenance. This paper presents a systematic approach for real-time or near real-time machine performance monitoring solution development from data collection, feature extraction, data analytics to real-time machine fault and machine status classification. Three data-driven machine-learning approaches using one vibration sensor data are proposed to detect two common machine failure modes during machine turning process. To evaluate the the performance of each approach, three machine-learning algorithms (Random Forest, K Nearest Neighborhood, and Support Vector Machine) are implemented and tested. Evaluation results on the actual machine data shows that a two-layered classification structure with random forest algorithm as the base has high classification accuracy on the machine status including machine fault detection. The developed data-driven machine health monitoring solution is deployed in the IoT device for real-time data collection and processing and results are sent data server for data visualization.",https://ieeexplore.ieee.org/document/9115516/,2020 Prognostics and Health Management Conference (PHM-Besançon),4-7 May 2020,ieeexplore
10.1109/ICMNN.1994.593731,Massively parallel VLSI-implementation of a dedicated neural network for anomaly detection in automated visual quality control,IEEE,Conferences,"In this work we will present the VLSI-implementation of a dedicated neural network architecture which we have developed in prior work for anomaly detection in automated visual industrial quality control. The network, denoted as NOVAS performs a filtering of inspection images and highlights defects or anomalies in an isomorphic image representation, allowing the detection and localisation of faults on objects. Training of NOVAS is achieved by simply presenting a set of tolerable objects to the network in a single sweep. NOVAS works with single and with multichannel image representations. The processing principle of NOVAS is closely related to nearest neighbor and hypersphere classifier approaches. We have designed an ASIC for the efficient implementation of the nearest neighbor search. Based on that ASIC we will present an architecture of a modular massively parallel computer suited to meet the real-time constraints of manufacturing processes. Further we will report on the status of a prototype system which is close to completion.",https://ieeexplore.ieee.org/document/593731/,Proceedings of the Fourth International Conference on Microelectronics for Neural Networks and Fuzzy Systems,26-28 Sept. 1994,ieeexplore
10.1109/SNPDWinter52325.2021.00049,Measurement System Analysis for Semiconductor Measurement Process,IEEE,Conferences,"Quality management is an essential factor for enhancing the competitiveness of a company, and it is important to determine the capability of the measurement system in process improvement activities for quality improvement. In general, some observed dispersions in measurement-related activities are inherent in the product being measured, while others are due to the measurement system used. Semiconductor products are processed through a complex process, and quality control is performed through various measurement processes during the production process. Among them, wafer testing, which is the last step in manufacturing, is an important process to judge the quality of semiconductor products. In the wafer test process, an automatic test equipment (ATE) is used to check the electrical features of the created chips on the wafer, and to judge whether good or not products. To test wafers in ATE, it is necessary to connect to the chip electrode, PAD. Probe Card is used at this time. As a replaceable part, it is made and used according to the product and is generally in a parallel structure to test more quickly and efficiently many chips on a wafer. Since the probe card is extremely sensitive and detailed component, deformation such as abrasion occurs as it is used and affects the wafer test result. This paper presents an algorithm that can analyze the measurement system using the test result data of the measured wafer in order to maintain the optimum state of the ATE and Probe Card, the measurement system of the wafer test process. And using this algorithm, we propose the automated measurement process quality management system (AMPQMS) for the wafer test process. The real-time measurement system analysis algorithms are utilized several statistical analysis methods that analyzes the repeatability and reproducibility of the instrument. AMPQMS can properly combine various analysis algorithms for each product. The quality of the measurement system is real time managed using the analyzed results by the analysis algorithms.",https://ieeexplore.ieee.org/document/9403498/,"2021 21st ACIS International Winter Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing (SNPD-Winter)",28-30 Jan. 2021,ieeexplore
10.1109/DTPI52967.2021.9540077,Mechanical Design Paradigm based on ACP Method in Parallel Manufacturing,IEEE,Conferences,"Parallel Manufacturing is a new manufacturing paradigm in industry, deeply integrating informalization, automation, and artificial intelligence. In this paper we propose a new mechanical design paradigm in Parallel Manufacturing based on ACP method. The key is to regard the design procedure based on artificial design and emulation method as two independent procedures, which can be modeled as a parallel system. The design procedure based on ACP method does not include a real system, which is an inventive extension of the traditional parallel system. This method can be implemented with social information by introducing the definition of SDV, SDM, and Intelligent Design Manager, making it highly adaptive for social manufacturing and Parallel Manufacturing.",https://ieeexplore.ieee.org/document/9540077/,2021 IEEE 1st International Conference on Digital Twins and Parallel Intelligence (DTPI),15 July-15 Aug. 2021,ieeexplore
10.1109/ICAT.2006.85,Modeling and Application of Virtual Machine Tool,IEEE,Conferences,"The recent years of the 21th Century are associated with the advent of virtual reality technologies for modern industry and manufacturing engineering. Virtual Machine Tool Technology is given to design, test, control and machine parts in a virtual reality environment. This paper presents the methods to model and simulate the virtual machine tools in response to change in the machining requirements. Specifically, a set of module combination rules and a modeling method of the structure of machine tools using connectivity graph are developed. By this way virtual machine tool is implemented. The developed virtual machine tool can be efficiently used for industry training and machine leaning and operating.",https://ieeexplore.ieee.org/document/4089203/,16th International Conference on Artificial Reality and Telexistence--Workshops (ICAT'06),29 Nov.-1 Dec. 2006,ieeexplore
10.1109/RCAR52367.2021.9517563,Modeling and Implementation of Tacking for Wing Sail Land-Yacht,IEEE,Conferences,"Wing sail land-yacht is one of the promising vehicles which utilizes wind energy. However, its autonomous driving in all directions is challenging, especially the upwind steering (tacking) maneuver. This paper proposes a novel design of low cost three-wheeled land-yacht with a T-frame and a foamed wing sail, which is able to move and tack upwind owing to the lightweight design. To achieve the successful tacking, a model describing the steering process is developed based on the law of energy conservation and used to predict the minimum initial velocity for steering. In the model, an acceleration error function C induced by the environment disturbance and manufacturing error is identified by a series of experiments. Finally, the tacking experiments verify the steering model and show that the land-yacht can achieve the tacking with a high success rate of 94.7%, based on the predicted minimum initial steering velocity.",https://ieeexplore.ieee.org/document/9517563/,2021 IEEE International Conference on Real-time Computing and Robotics (RCAR),15-19 July 2021,ieeexplore
10.1109/ISCSCT.2008.104,Monitoring and Early-Warning of the Supply Chain by Using System Dynamics and Neural Networks,IEEE,Conferences,"Effective management of a supply chain requires the ability to detect unexpected variations at an early stage, which brings the possibility of taking preventive decisions to mitigate the variations. This paper proposes a methodology that monitors the dynamic trends of supply chain performance indicators, and gives early-warnings for potential risks. Initially, a supply chain model is built using system dynamics. Then, based on this model, a neural network which can be trained to adapt to the real supply chain is developed. Acting as the kernel of monitoring and early-warning module, the neural network can make online predictions of dynamic trend of indicators so that an enterprise would have enough time to respond to any unwanted situations. The architecture of monitoring and early-warning module is proposed and a case study of the manufacturing industry is presented to illustrate the methodology and architecture.",https://ieeexplore.ieee.org/document/4731437/,2008 International Symposium on Computer Science and Computational Technology,20-22 Dec. 2008,ieeexplore
10.1109/ISCAS.2019.8702575,Multi-View Fusion Neural Network with Application in the Manufacturing Industry,IEEE,Conferences,"In recent years the research community and industry have paid high attention to the field of machine learning, especially deep learning. Nowadays many real-world classification or rather prediction applications are implemented by neural network models. We propose a multi-view fusion neural network with application in the manufacturing industry. Image information of multiple cameras is fused and used by the proposed model to predict the state of a manufacturing machine. Experiments show that the overall classification performance is increased from a baseline of 92.7% to 99.5% by the fusion model.",https://ieeexplore.ieee.org/document/8702575/,2019 IEEE International Symposium on Circuits and Systems (ISCAS),26-29 May 2019,ieeexplore
10.1109/IJCNN48605.2020.9207055,Multi-agent system for dynamic scheduling,IEEE,Conferences,"This paper proposes a flexible manufacturing system based on intelligent computational agents. A Multi-Agent System composed of 4 types of reactive agents was designed to control the operation of a real implementation in the Intelligent Automation Lab at Instituto Superior Técnico. This implementation was based and constructed analogously to a known benchmark, AIP-PRIMECA. The agents were modelled using Petri nets and agent communications were defined through the combination of FIPA Interaction Protocols. The system was tested under the conditions of static and dynamic scenarios, having its performance validated whenever possible by comparison with results from a Potential Fields Approach in the same benchmark. Overall, the performance exhibited by the proposed MAS was slightly better and it is worth highlighting the simple behaviour of each agent and ability to respond in real-time to all the dynamic scenarios tested.",https://ieeexplore.ieee.org/document/9207055/,2020 International Joint Conference on Neural Networks (IJCNN),19-24 July 2020,ieeexplore
10.1109/AQTR.2014.6857897,Multi-agent system for heterarchical product-driven manufacturing,IEEE,Conferences,"Product-driven manufacturing has gained a lot of traction recently among practitioners as it has the potential to take flexibility and agility of the manufacturing system to a new level compared to hierarchical control models. The advances in embedded technology have created the premises for the emergence of truly intelligent products that are capable not only of identification and information storage, but also of complex behavior and local decision making. In this context, this paper proposes a multi-agent control system that aims to solve the new challenges introduced by the shift to product-driven manufacturing, specifically addressing the special needs for information flow between shop floor entities and the MES system. The paper presents the pilot implementation, using the JADE multi-agent platform, a backtracking scheduler, an artificial neural network (ANN) for local decision making and the experimental results outlining the agent processing requirements during the product lifecycle.",https://ieeexplore.ieee.org/document/6857897/,"2014 IEEE International Conference on Automation, Quality and Testing, Robotics",22-24 May 2014,ieeexplore
10.1109/ICDMW.2019.00065,Mímir: Building and Deploying an ML Framework for Industrial IoT,IEEE,Conferences,"In this paper we describe Mímir, a production grade cloud and edge spanning ML framework for Industrial IoT applications. We first describe our infrastructure for optimized capture, streaming and multi-resolution storage of manufacturing data and its context. We then describe our workflow for scalable ML model training, validation, and deployment that leverages a manufacturing taxonomy and parameterized ML pipelines to determine the best metrics, hyper-parameters and models to use for a given task. We also discuss our design decisions on model deployment for real-time and batch data in the cloud and at the edge. Finally, we describe the use of the framework in building and deploying an application for Predictive Quality monitoring during a Plastics Extrusion manufacturing process.",https://ieeexplore.ieee.org/document/8955638/,2019 International Conference on Data Mining Workshops (ICDMW),8-11 Nov. 2019,ieeexplore
10.1109/CEC.2002.1004450,Network performance of distributed coevolutionary agents,IEEE,Conferences,"Innovations in software, networks, and database systems are enabling widely distributed organizations to integrate activities, share information, collaborate on decisions, and execute transactions. However, successful enterprise-wide collaboration is increasingly dependent on the availability of generalized decision-support tools that can efficiently access and utilize distributed information. This paper presents a basic theory and network-based performance evaluation of a class of coevolutionary algorithms that supports efficient planning in a distributed environment. Performance of these coevolutionary algorithms is evaluated in a distributed information architecture (coevolutionary virtual design environment) that supports integrated design-supplier-manufacturing planning. In this architecture, distributed evolutionary agents and mobile agents are principal entities that support a network-efficient exploration of planning alternatives in which successive populations systematically select superior planning alternatives.",https://ieeexplore.ieee.org/document/1004450/,Proceedings of the 2002 Congress on Evolutionary Computation. CEC'02 (Cat. No.02TH8600),12-17 May 2002,ieeexplore
10.1109/CW.2016.52,Neuroscience Based Design: Fundamentals and Applications,IEEE,Conferences,"Neuroscience-based or neuroscience-informed design is a new application area of Brain-Computer Interaction (BCI). It takes its roots in study of human well-being in architecture, human factors study in engineering and manufacturing including neuroergonomics. In traditional human factors studies and/or well-being study, mental workload, stress, and emotion are obtained through questionnaires that are administered upon completion of some task and/or the whole experiment. Recent advances in BCI research allow for using Electroencephalogram (EEG) based brain state recognition algorithms to assess the interaction between brain and human performance. We propose and develop an EEG-based system CogniMeter to monitor and analyze human factors measurements of newly designed software/hardware systems and/or working places. Machine learning techniques are applied to the EEG data to recognize levels of mental workload, stress and emotions during each task. The EEG is used as a tool to monitor and record the brain states of subjects during human factors study experiments. We describe two applications of CogniMeter system: human performance assessment in maritime simulator and EEG-based human factors evaluation in Air Traffic Control (ATC) workplace. By utilizing the proposed EEG-based system, true understanding of subjects working patterns can be obtained. Based on the analyses of the objective real time EEG-based data together with the subjective feedback from the subjects, we are able to reliably evaluate current systems/hardware and/or working place design and refine new concepts and design of future systems.",https://ieeexplore.ieee.org/document/7756163/,2016 International Conference on Cyberworlds (CW),28-30 Sept. 2016,ieeexplore
10.1109/IISA50023.2020.9284345,New Advanced Technology Methods for Energy Efficiency of Buildings,IEEE,Conferences,"Energy consumed by buildings represents a large part of the world’s total energy consumption with a total share of 40%. This is the reason why energy efficiency of buildings has become a very important scientific field. For the purpose of this paper a critical review of old and new methods of controlling the parts of a building’s automation and thus achieving energy savings are compared, analyzed and presented. The method of Fuzzy Cognitive Maps (FCM) and its significant impact on the improvement of the management of a building is being presented. FCMs is a new soft computing method which combine neural networks and Fuzzy Logic. They have been used with very promising results in many fields such as medicine, transportation, manufacturing agriculture, food industry and energy. In this paper the use of FCMs is exploited and specifically used in issues of energy efficiency of buildings. Obtained results, simulation and experimental, for case studies where FCMs were used in buildings, of residential and commercial use, in Southern Greece will be presented. Software tools based on the aforementioned applications will be briefly presented. In the near future these tools are going to be integrated in even more buildings thus giving us real data which can and will be used in future research for moving from high energy consumption to Net-Zero Energy Buildings (NZEB).",https://ieeexplore.ieee.org/document/9284345/,"2020 11th International Conference on Information, Intelligence, Systems and Applications (IISA",15-17 July 2020,ieeexplore
10.1109/ISCAS.2009.5118362,New CNN based algorithms for the full penetration hole extraction in laser welding processes,IEEE,Conferences,"In this paper new CNN based visual algorithms for the control of welding processes are proposed. The high dynamics of laser welding in several manufacturing processes ranging from automobile production to precision mechanics requires the introduction of new fast real time controls. In the last few years, analogic circuits like cellular neural networks (CNN) have obtained a primary place in the development of efficient electronic devices because of their real-time signal processing properties. Furthermore, several pixel parallel CNN based architectures are now included within devices like the family of EyeRis systems [1]. In particular, the algorithms proposed in the following have been implemented on the EyeRis system v1.2 with the aim to be run at frame rates up to 20 kHz.",https://ieeexplore.ieee.org/document/5118362/,2009 IEEE International Symposium on Circuits and Systems,24-27 May 2009,ieeexplore
10.1109/MIV.1989.40559,New framework for dynamic scheduling of production systems,IEEE,Conferences,"A concept for dynamic scheduling in manufacturing systems is proposed. The scope of 'dynamic scheduling' treated includes online dynamic change of some scheduling parameters such as rules for part dispatching, machine selection, or routing. IF-THEN-type heuristic operators are utilized to perform this online real-time rule selection, and offline machine learning is used to obtain more detailed and powerful heuristics than those implemented by human experts or programmers. A learning algorithm has been developed to formulate operators that can treat quantity-type as well as quality-type information. A prototype computer program named learning aided dynamic scheduler (LADS) has been developed. A simulation study using LADS indicates good results for dynamic scheduling.&lt;<ETX>&gt;</ETX>",https://ieeexplore.ieee.org/document/40559/,"International Workshop on Industrial Applications of Machine Intelligence and Vision,",10-12 April 1989,ieeexplore
10.1109/IEMBS.1988.95233,New technical approaches to monitoring and interpreting the dynamics of real neural networks,IEEE,Conferences,"Understanding the functional organization of large-scale real neural networks will require the development of techniques capable of predicting, recording, and interpreting the activities of the very large numbers of neurons that participate in their operation. This presents a considerable technical problem for systems-level neurobiologists and associated biomedical engineers. The authors describe their recent work using silicon manufacturing techniques to make multineuron in vivo recording devices, as well as the software tools developed for realistically simulating the neural networks studied.&lt;<ETX>&gt;</ETX>",https://ieeexplore.ieee.org/document/95233/,Proceedings of the Annual International Conference of the IEEE Engineering in Medicine and Biology Society,4-7 Nov. 1988,ieeexplore
10.1109/SNPD.2016.7515945,Notice of Violation of IEEE Publication Principles: Adaptive Fuzzy PID speed control of DC belt conveyor system,IEEE,Conferences,Conveyor belt system is one of the most common transfer systems used in industry to transfer goods from one point to another in a limited distance. It is used in industries such as the electromechanical /mechanical assembly manufacturing to transfer work piece from one station to another or one process to another in food industries. The belt conveyor system discussed in this paper is driven by a DC motor and two speed controllers. The PID speed controller is designed to provide comparison to the main controller which is the Adaptive Fuzzy PID Speed controller. Both controllers are implemented in a real hardware where the algorithm will be written in PLC using SCL language. The experimental result shows that Adaptive Fuzzy PID controller performs better and adapted to the changes in load much faster than the conventional PID controller. This project has also proved that PLC is capable of performing high level control system tasks..,https://ieeexplore.ieee.org/document/7515945/,"2016 17th IEEE/ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing (SNPD)",30 May-1 June 2016,ieeexplore
10.1109/OCEANSE.2019.8867398,Ocean of Things : Affordable Maritime Sensors with Scalable Analysis,IEEE,Conferences,"DARPA's Ocean of Things (OoT) program enables persistent maritime situational awareness over large ocean areas by deploying thousands of low-cost, intelligent floats that drift as a distributed sensor network. Each float manages a suite of commercially available sensors to collect environmental data such as sea surface temperature, sea state, and location as well as activity data about vessels and marine mammals moving across the ocean. The floats periodically transmit processed data, or immediately report events based on internal prioritization schemes. Messages travel via commercial satellite to a government cloud for storage and real-time analysis. Cloud-based data analytics feature machine learning aimed at discovering emergent features and behaviors from sparse data. The multiple performers manufacturing floats and developing software are being led by a government management team to employ commercial design methodology and agile best practices. At-sea float deployments are planned in two phases over 2019 (1-month) and 2020 (3-month). Program benefits include ocean environmental products derived from high-density, in-situ measurements and analytical applications, which can simultaneously provide users a range of outputs to include ocean circulation prediction, vessel and marine mammal tracking, and dynamic ocean resource management.",https://ieeexplore.ieee.org/document/8867398/,OCEANS 2019 - Marseille,17-20 June 2019,ieeexplore
10.1109/BigData.2017.8258105,On event-driven knowledge graph completion in digital factories,IEEE,Conferences,"Smart factories are equipped with machines that can sense their manufacturing environments, interact with each other, and control production processes. Smooth operation of such factories requires that the machines and engineering personnel that conduct their monitoring and diagnostics share a detailed common industrial knowledge about the factory, e.g., in the form of knowledge graphs. Creation and maintenance of such knowledge is expensive and requires automation. In this work we show how machine learning that is specifically tailored towards industrial applications can help in knowledge graph completion. In particular, we show how knowledge completion can benefit from event logs that are common in smart factories. We evaluate this on the knowledge graph from a real world-inspired smart factory with encouraging results.",https://ieeexplore.ieee.org/document/8258105/,2017 IEEE International Conference on Big Data (Big Data),11-14 Dec. 2017,ieeexplore
10.1109/CASE48305.2020.9216979,Online Computation Performance Analysis for Distributed Machine Learning Pipelines in Fog Manufacturing,IEEE,Conferences,"Smart manufacturing enables real-time data streaming from interconnected manufacturing processes to improve manufacturing quality, throughput, flexibility, and cost reduction via computation services. In these computation services, machine learning pipelines integrate various types of computation method options to match the contextualized, on-demand computation needs for the maximum prediction accuracy or the best model structure interpretation. On the other hand, there is a pressing need to integrate Fog computing in manufacturing, which will reduce communication time latency and dependency on connections, improve responsiveness and reliability of the computation services, and maintain data privacy. However, there is a knowledge gap in using machine learning pipelines in Fog manufacturing. Existing offloading strategies are not effective, due to the lack of accurate prediction model for the performance of computation services before the execution of those heterogeneous computation tasks. In this paper, machine learning pipelines are implemented in Fog manufacturing. The computation performance of each sub-step of pipelines is predicted and analyzed via linear regression models and random forest regression models. A Fog manufacturing testbed is adopted to validate the performance of the employed models. The results show that the models can adequately predict the performance of computation services, which can be further integrated into Fog manufacturing to better support offloading strategies for machine learning pipelines.",https://ieeexplore.ieee.org/document/9216979/,2020 IEEE 16th International Conference on Automation Science and Engineering (CASE),20-21 Aug. 2020,ieeexplore
10.1109/ICSMC.2003.1244658,Online DES control behavior verification via virtual supervisor,IEEE,Conferences,"Vast majority of manufacturing devices and systems are controlled by programmable software logic controllers. The primary issues in design, development, and operation of such systems are related to the relative complexity of involved control logic. The focus for DES in supervisory control is how to deal to with uncontrollable events, blocking in the controlled system and unobservable events. This paper presents a new methodology for mapping operational pattern of States-Events-Times during control logic execution. This approach enables identification of conflicts, such as blockings or bottlenecks, which may occur due to exceptions (such as sensor failures). Proposed approach, relying on the historical data, pre-determined operational pattern, and relations of states-events-times, generates a virtual supervisor which not only enables return of the process to deterministic behavior, but also run-time verification of its properties. This new approach offers the flexibility to perform engineering changes, add new specifications, and facilitates future system modifications.",https://ieeexplore.ieee.org/document/1244658/,"SMC'03 Conference Proceedings. 2003 IEEE International Conference on Systems, Man and Cybernetics. Conference Theme - System Security and Assurance (Cat. No.03CH37483)",8-8 Oct. 2003,ieeexplore
10.1109/CSO.2009.254,Online Scheduling of Flexible Flow Shop Manufacturing,IEEE,Conferences,"The flexible flow shop refers to such a manufacturing environment in which jobs are to be processed through serial stages, with one or multiple machines available at each stage. It is usually a complex task when specific objective is demanded such as minimum cost, minimum time, etc. Static scheduling of such problems has been much researched, however, little efforts have been made on realtime scheduling when the release time of each job is unknown. In this paper, some online scheduling methods are presented to deal with the tough problem of realtime Just-In-Time manufacturing. In addition to applicable dispatching rules, agent-based approaches are also proposed featuring feedback, learning, and realtime prediction. The simulation result reveals that the presented distributed learning approach, especially when combined with realtime prediction, delivers a high performance.",https://ieeexplore.ieee.org/document/5193655/,2009 International Joint Conference on Computational Sciences and Optimization,24-26 April 2009,ieeexplore
10.1109/INM.2015.7140329,Ontology integration for advanced manufacturing collaboration in cloud platforms,IEEE,Conferences,"Advances in the field of cloud computing and networking have led to rapid development and market growth in areas such as online retail, gaming and healthcare. In the field of advanced manufacturing however, the impact has been significantly lesser than expected due to limitations in cloud platforms for fostering community engagement. To address this problem, we study a new cloud-based architecture that provides Platform-asa-Service (PaaS) management capabilities to the manufacturing community for delivering Software-as-a-Service (SaaS) “Apps” to their customers. Our architecture aims at supporting an “App Marketplace” that thrives on agile development, organic collaboration and scalable sales of next generation manufacturing Apps requiring high-performance simulation and modeling. Towards realizing the vision of the above architecture, our paper involves investigation and implementation of an Ontology Service that interoperates with other common web services related to resource brokering and accounting. Our Ontology Service uses principles of mapping and merging to translate a manufacturing App's collaboration requirements to suitable resource specifications on public cloud platforms. Integrated resultant ontology can be queried to provision the required resource parameters such as amount of memory/storage, number of processing units, and network protocol configurations needed for deployment of an App. We validate the effectiveness of our Ontology Service using the Protégé framework in a pilot testbed of a real-world “WheelSim” App in the NSF GENI Cloud platform. Our ontology integration results show benefits to an App developer in terms of: optimal user experience, lower design time and lower cost/simulation.",https://ieeexplore.ieee.org/document/7140329/,2015 IFIP/IEEE International Symposium on Integrated Network Management (IM),11-15 May 2015,ieeexplore
10.1109/BigData.2015.7363882,Open research challenges with Big Data — A data-scientist's perspective,IEEE,Conferences,"In this paper, we discuss data-driven discovery challenges of the Big Data era. We observe that recent innovations in being able to collect, access, organize, integrate, and query massive amounts of data from a wide variety of data sources have brought statistical data mining and machine learning under more scrutiny and evaluation for gleaning insights from the data than ever before. In that context, we pose and debate the question - Are data mining algorithms scaling with the ability to store and compute? If yes, how? If not, why not? We survey recent developments in the state-of-the-art to discuss emerging and outstanding challenges in the design and implementation of machine learning algorithms at scale. We leverage experience from real-world Big Data knowledge discovery projects across domains of national security, healthcare and manufacturing to suggest our efforts be focused along the following axes: (i) the `data science' challenge - designing scalable and flexible computational architectures for machine learning (beyond just data-retrieval); (ii) the ` science of data' challenge - the ability to understand characteristics of data before applying machine learning algorithms and tools; and (iii) the `scalable predictive functions' challenge - the ability to construct, learn and infer with increasing sample size, dimensionality, and categories of labels. We conclude with a discussion of opportunities and directions for future research.",https://ieeexplore.ieee.org/document/7363882/,2015 IEEE International Conference on Big Data (Big Data),29 Oct.-1 Nov. 2015,ieeexplore
10.1109/TAAI.2015.7407079,PCBA demand forecasting using an evolving Takagi-Sugeno system,IEEE,Conferences,"This paper investigates the use of using an evolving fuzzy system for printed circuit board (PCBA) demand forecasting. The algorithm is based on the evolving Takagi-Sugeno (eTS) fuzzy system, which has the ability to incorporate new patterns by changing its internal structure in an on-line fashion. We argue that these capabilities could aid in forecasting dynamic demand patterns such as those experienced in the electronic manufacturing (EMS) industry. An eTS fuzzy system is implemented in the R statistical programming language and is tested on both synthetic and real-world data. To our knowledge, this is one of the first applications of an evolving fuzzy system to forecast product demand. The results indicate that the evolving fuzzy system outperforms competing approaches for the application considered.",https://ieeexplore.ieee.org/document/7407079/,2015 Conference on Technologies and Applications of Artificial Intelligence (TAAI),20-22 Nov. 2015,ieeexplore
10.1109/BigDataCongress.2019.00032,"PREMISES, a Scalable Data-Driven Service to Predict Alarms in Slowly-Degrading Multi-Cycle Industrial Processes",IEEE,Conferences,"In recent years, the number of industry-4.0-enabled manufacturing sites has been continuously growing, and both the quantity and variety of signals and data collected in plants are increasing at an unprecedented rate. At the same time, the demand of Big Data processing platforms and analytical tools tailored to manufacturing environments has become more and more prominent. Manufacturing companies are collecting huge amounts of information during the production process through a plethora of sensors and networks. To extract value and actionable knowledge from such precious repositories, suitable data-driven approaches are required. They are expected to improve the production processes by reducing maintenance costs, reliably predicting equipment failures, and avoiding quality degradation. To this aim, Machine Learning techniques tailored for predictive maintenance analysis have been adopted in PREMISES (PREdictive Maintenance service for Industrial procesSES), an innovative framework providing a scalable Big Data service able to predict alarming conditions in slowly-degrading processes characterized by cyclic procedures. PREMISES has been experimentally tested and validated on a real industrial use case, resulting efficient and effective in predicting alarms. The framework has been designed to address the main Big Data and industrial requirements, by being developed on a solid and scalable processing framework, Apache Spark, and supporting the deployment on modularized containers, specifically upon the Docker technology stack.",https://ieeexplore.ieee.org/document/8818217/,2019 IEEE International Congress on Big Data (BigDataCongress),8-13 July 2019,ieeexplore
10.1109/COMPSAC51774.2021.00303,"Panel Discussion: Deriving Past, Present, and Future Tech to More Intelligent and Resilient Digital Realities for a Collaborative World",IEEE,Conferences,"No abstract or record of the panel discussion was made available for publication as part of the conference proceedings. There is the philosophical question of “does history repeat itself”. One learns from the past to affect the present, which builds our future. Advances in technology are built upon previous innovations. New technologies are derived from existing technologies. The IEEE leverages past and current technologies to advance work on new and emerging technologies through serving as a catalyst for developing new innovations, products and services. IEEE Future Directions serves as an incubator for these new initiatives. One of its focus areas, Digital Reality serves to explore and enable the coming Intelligent and Resilient Digital Realities through collaboration among technologists, engineers, regulators, practitioners, and ethicists around the world. The Digital Transformation is fueled by advances in technology, such as Artificial Intelligence (AI), Machine Learning (ML), and applications using the copious amounts of continuously generated data. By leveraging these technologies and others developed such as Augmented Reality (AR), Virtual Reality (VR), and Digital Twins, the line between the physical world and the digital world will be increasingly less distinct. Applications are already quickly emerging across the broad fields of gaming, entertainment, medicine, automotive, education, manufacturing, enabling the sharing of services, and more. Emphasis is upon presenting practical applications and its implementations of interest to attendees. Subject matter expert speakers comment on current and past implementations. Of course, the speakers look ahead to the future.",https://ieeexplore.ieee.org/document/9529363/,"2021 IEEE 45th Annual Computers, Software, and Applications Conference (COMPSAC)",12-16 July 2021,ieeexplore
10.1109/BigData.2018.8622389,Performance and Memory Trade-offs of Deep Learning Object Detection in Fast Streaming High-Definition Images,IEEE,Conferences,"Deep learning models are associated with various deployment challenges. Inference of such models is typically very compute-intensive and memory-intensive. In this paper, we investigate the performance of deep learning models for a computer vision application used in the automotive manufacturing industry. This application has demanding requirements that are characteristic of Big Data systems, including high volume and high velocity. The application has to process a very large set of high-definition images in real-time with appropriate accuracy requirements using a deep learning-based object detection model. Meeting the run time, accuracy, and resource requirements require a careful consideration of the choice of model, model parameters, hardware, and environmental support. In this paper, we investigate the trade-offs of the most popular deep neural network-based object detection models on four hardware platforms. We report the trade-offs of resource consumption, run time, and accuracy for a realistic real-time application environment.",https://ieeexplore.ieee.org/document/8622389/,2018 IEEE International Conference on Big Data (Big Data),10-13 Dec. 2018,ieeexplore
10.1109/SEC50012.2020.00019,Poster: Lambda architecture for robust condition based maintenance with simulated failure modes,IEEE,Conferences,"Condition based maintenance (CBM) is increasingly seen as a promising approach for addressing downtime issues which are a common occurrence in the manufacturing industry and are a major cause of lost productivity. However, it has been a challenge to develop a generic CBM solution that works for all assets since each asset has unique sources of noise. This mandates use of manual diagnostics to custom tailor a solution for each asset for accurate failure mode identification (FMI). This problem is further compounded by the scarcity of failure data. In this paper, we propose a lambda architecture for FMI of industrial assets that achieves low initial deployment cost while securing a reasonable classification accuracy. The lambda architecture consists of a light-compute edge node, such as Raspberry Pi, that processes high-speed vibration data in real-time to extract useful features and applies a deep-learning (DL) engine which is trained in a cloud platform, such as AWS. In addition, we also incorporate a failure modes' feature simulator so that DL models can adapt to different industrial assets without costly failure data collection. Finally, experimental results are provided using the bearings' failures dataset validating the proposed cost-effective CBM architecture with high accuracy and scalability.",https://ieeexplore.ieee.org/document/9355694/,2020 IEEE/ACM Symposium on Edge Computing (SEC),12-14 Nov. 2020,ieeexplore
10.1109/GloSIC.2018.8570124,Probabilistic Estimations of Increasing Expected Reliability and Safety for Intelligent Manufacturing,IEEE,Conferences,"In the near future the possibilities of the modern probabilistic models, artificial intelligence and machine learning methods can provide an intelligent support of making decisions by an operator in real time. An agile recovery of intelligent manufacturing integrity can be implemented owing to the development of industrial robotics. For intelligent manufacturing it means the expected reliability and safety may be in the near future at the expense of intelligent support of decision making and the agile recovery of integrity. To answer the question “How much essential may be this increasing?” here are proposed: general analytical approaches for a probabilistic estimation of the expected reliability and safety for every monitored element or the system of intelligent manufacturing on a level of probability distribution functions (PDF) of the time between the losses of system integrity; estimations of increasing the expected reliability and safety for intelligent manufacturing at the expense of the intelligent support of decision making and agile recovery of integrity; the comparisons of the estimations on a prognostic period up to 10 years using the identical model in applications to expected reliability and safety. The applications of the proposed approaches allow the customers, designers, developers, users and experts of Industry 4.0 intelligent manufacturing to be guided by the proposed probabilistic estimations for solving problems of reliability and safety in the system life cycle. The results are demonstrated by examples.",https://ieeexplore.ieee.org/document/8570124/,2018 Global Smart Industry Conference (GloSIC),13-15 Nov. 2018,ieeexplore
10.1109/CSCWD.2006.253263,Proceedings 2006 10th International Conference on Computer Supported Cooperative Work in Design,IEEE,Conferences,The following topics are dealt with: computer supported cooperative work in design; collaborative design methods and tools; coordination methods; CSCW system security; grid computing; data and information management; networked manufacturing; workflow management; agents and multi-agent systems; ontology and knowledge management; Web services and semantic Web; e-commerce and e-businesses; virtual reality and CAD; design management; e-learning,https://ieeexplore.ieee.org/document/4019299/,2006 10th International Conference on Computer Supported Cooperative Work in Design,3-5 May 2006,ieeexplore
10.1109/UEMCON47517.2019.8992974,Quality Model for Testing Augmented Reality Applications,IEEE,Conferences,"Augmented Reality applications have the capability of merging virtual objects into physical setting, or alternatively they can wrap physical objects within a virtual scene. Augmented reality applications are similar to virtual reality applications in that aspects of the visualizations are computer generated, but augmented reality apps also must contain a view of the physical world. Augmented reality applications are being utilized in service, manufacturing, product areas, as well as gaming. Mobile devices are becoming common runtime environments for augmented reality applications and the mobile device proliferation is enabling a wave of AR applications. Due to the combined nature of digital and physical objects, as well as the environmental and contextual constraints, a traditional test plan is not sufficient. A new quality model is proposed that takes these issues into account, and an example of how machine learning can assist with aspects of the model is discussed.",https://ieeexplore.ieee.org/document/8992974/,"2019 IEEE 10th Annual Ubiquitous Computing, Electronics & Mobile Communication Conference (UEMCON)",10-12 Oct. 2019,ieeexplore
10.1109/CAC51589.2020.9327198,Real time production scheduling based on Asynchronous Advanced Actor Critic and composite dispatching rule,IEEE,Conferences,"In the era of smart manufacturing, the requirements of real-time, adaptability and long-term optimization of the semiconductor manufacturing system (SMS) are increased due to the further expanded, more complicated and unpredictable uncertainties. This paper addresses the real time production scheduling of SMS to maximize on productivity (PROD) and average daily movement (AvgMOV), and minimize mean cycle time (MCT). We propose an Asynchronous Advanced Actor Critic and composite dispatching rule based real time production scheduling (A3C-CR2) framework, which involves a scheduling knowledge training module and a deployment module. The action space is designed as a combination of the composite dispatching rule (CDR) based continuous scheduling actions. In terms of various performance indices over a long period, the proposed A3C-CR2 approach outperforms other dispatching rules.",https://ieeexplore.ieee.org/document/9327198/,2020 Chinese Automation Congress (CAC),6-8 Nov. 2020,ieeexplore
10.1109/ICAR46387.2019.8981549,Real-time RGB-D semantic keyframe SLAM based on image segmentation learning from industrial CAD models,IEEE,Conferences,"This paper presents methods for performing realtime semantic SLAM aimed at autonomous navigation and control of a humanoid robot in a manufacturing scenario. A novel multi-keyframe approach is proposed that simultaneously minimizes a semantic cost based on class-level features in addition to common photometric and geometric costs. The approach is shown to robustly construct a 3D map with associated class labels relevant to robotic tasks. Alternatively to existing approaches, the segmentation of these semantic classes have been learnt using RGB-D sensor data aligned with an industrial CAD manufacturing model to obtain noisy pixel-wise labels. This dataset confronts the proposed approach in a complicated real-world setting and provides insight into the practical use case scenarios. The semantic segmentation network was fine tuned for the given use case and was trained in a semi-supervised manner using noisy labels. The developed software is real-time and integrated with ROS to obtain a complete semantic reconstruction for the control and navigation of the HRP4 robot. Experiments in-situ at the Airbus manufacturing site in Saint-Nazaire validate the proposed approach.",https://ieeexplore.ieee.org/document/8981549/,2019 19th International Conference on Advanced Robotics (ICAR),2-6 Dec. 2019,ieeexplore
10.1109/SACI51354.2021.9465544,Real-time locating system and digital twin in Lean 4.0,IEEE,Conferences,"Digital twin plays a key role in the current development of smart manufacturing systems. Through simulation in the cyber world, real phenomena in the physical world can be predicted and optimized before the final implementation. The usage of the digital twin is enhanced along with the uprising of Industry 4.0, in which data availability supports the further insight of system status, helping the operation managers understand their system and perform resources adjustment more easily. Based on this digitization mature, Lean 4.0, a new concept elaborated from Lean manufacturing, has been interested recently. There are several technologies constituted digital twin that provide a favourable condition for Lean 4.0, such as augmented reality, cloud computing. In this paper, the integration of the Real-time Locating System (RTLS) into digital twin is proposed, which facilitates the performance of Lean 4.0 in manufacturing operation. Not only gain effective control over the facility's assets, but this integration also enhances the resources utilization, cut down operational wastes, thus brings a better turnover for industrial systems. A case study of successful implementation is shown, which proved the possible advantages of this approach.",https://ieeexplore.ieee.org/document/9465544/,2021 IEEE 15th International Symposium on Applied Computational Intelligence and Informatics (SACI),19-21 May 2021,ieeexplore
10.1109/RTOSS.1994.292553,Real-time platforms and environments for time constrained flexible manufacturing,IEEE,Conferences,"The Spring Kernel and associated algorithms, languages, and tools provide system support for static or dynamic real-time applications that require predictable operation. Spring currently consists of two major parts: (1) the development environment, where application and target systems are described, preprocessed and downloaded, and (2) the run-time environment, where the operating system, the Spring Kernel, creates and ensures predictable executions of application tasks. We have integrated our real-time systems technology with component technologies from robotics, computer vision, and real-time artificial intelligence, to develop a test platform for flexible manufacturing. The results being produced are generic so that they should be in many other real-time applications such as air traffic control and chemical plants. We describe this platform, identify new features developed, and comment on some lessons learned to date from this experiment.&lt;<ETX>&gt;</ETX>",https://ieeexplore.ieee.org/document/292553/,Proceedings of 11th IEEE Workshop on Real-Time Operating Systems and Software,18-19 May 1994,ieeexplore
10.1109/CDC.1994.411283,Real-time tool wear identification using sensor integration with neural network,IEEE,Conferences,"Real-time identification of tool wear in shop floor environment is essential for optimization of machining processes and implementation of automated manufacturing systems. In this paper. the signals obtained from acoustic emission and power sensors during machining processes are analyzed and a set of feature parameters characterizing the tool wear condition are extracted. In order to realize the realtime tool wear condition monitoring for different cutting conditions, a sensor integration strategy which combines the information from multiple sensors (acoustic emission sensor and power sensor) and machining parameters is proposed. A neural network based on improved back-propogation algorithm is developed and a prototype scheme for realtime identification of tool wear is implemented. Experiments under different conditions have proved that a higher rate of tool wear identification can be achieved by using the sensor integration model with neural network. The results also indicated that the neural network is a very effective method of sensor integration for online monitoring of tool abnormalities.&lt;<ETX>&gt;</ETX>",https://ieeexplore.ieee.org/document/411283/,Proceedings of 1994 33rd IEEE Conference on Decision and Control,14-16 Dec. 1994,ieeexplore
10.1109/DEXA.2001.953126,Reconfiguring real-time holonic manufacturing systems,IEEE,Conferences,We describe a general approach for dynamic and intelligent reconfiguration of real-time distributed control systems that utilises the IEC 61499 function block model. This work is central to the development of distributed intelligent control systems that are inherently adaptable and dynamically reconfigurable. The approach that is used takes advantage of distributed artificial intelligence at the planning and control levels to achieve significantly shorter up-front commissioning times as well as significantly more responsiveness to change. This approach is based on object-oriented and agent-based methods and aims at overcoming the difficulties associated with managing real-time reconfiguration of a holonic manufacturing system.,https://ieeexplore.ieee.org/document/953126/,12th International Workshop on Database and Expert Systems Applications,3-7 Sept. 2001,ieeexplore
10.1109/INDIN45582.2020.9442114,Reinforcement Learning Driven Adaptive VR Streaming with Optical Flow Based QoE,IEEE,Conferences,"With the merit of containing full panoramic content in one camera, Virtual Reality (VR) and 360° videos have arisen in the field of industrial cloud manufacturing and training. Industrial Internet of Things (IoT), where many VR terminals needed to be online at the same time, can hardly guarantee VR's bandwidth requirement. However, by making use of users' quality of experience (QoE) awareness factors, including the relative moving speed and depth difference between the viewpoint and other content, bandwidth consumption can be reduced. In this paper, we propose Optical Flow Based VR(OFB-VR), an interactive method of VR streaming that can make use of VR users' QoE awareness to ease the bandwidth pressure. The Just-Noticeable Difference through Optical Flow Estimation (JND-OFE) is explored to quantify users' awareness of quality distortion in 360° videos. Accordingly, a novel 360° videos QoE metric based on Peak Signal-to-Noise Ratio and JND-OFE (PSNR-OF) is proposed. With the help of PSNR-OF, OFB-VR proposes a versatile-size tiling scheme to lessen the tiling overhead. A Reinforcement Learning (RL) method is implemented to make use of historical data to perform Adaptive BitRate (ABR). For evaluation, we take two prior VR streaming schemes, Pano and Plato, as baselines. Vast evaluations show that our system can increase the mean PSNR-OF score by 9.5-15.8% while maintaining the same rebuffer ratio compared with Pano and Plato in a fluctuate LTE bandwidth dataset. Evaluation results show that OFB-VR is a promising prototype for actual interactive industrial VR. A prototype of OFB-VR can be found in https://github.com/buptexplorers/OFB-VR.",https://ieeexplore.ieee.org/document/9442114/,2020 IEEE 18th International Conference on Industrial Informatics (INDIN),20-23 July 2020,ieeexplore
10.1109/AI4I46381.2019.00027,Reinforcement Learning of a Robot Cell Control Logic using a Software-in-the-Loop Simulation as Environment,IEEE,Conferences,"This paper introduces a method for automatic robot programming of industrial robots using reinforcement learning on a Software-in-the-loop simulation. The focus of the the paper is on the higher levels of a hierarchical robot programming problem. While the lower levels the skills are stored as domain specific program code, the combination of the skills into a robot control program to solve a specific task is automated. The reinforcement learning learning approach allows the shopfloor workers and technicians just to define the end result of the manufacturing process through a reward function. The programming and process optimization is done within the learning procedure. The Software-in-the-loop simulation with the robot control software makes it possible to to interpret the real program code and generate the exact motion. The exact motion of the robot is needed in order to find not just an optimal but also a collision-free policy.",https://ieeexplore.ieee.org/document/9027783/,2019 Second International Conference on Artificial Intelligence for Industries (AI4I),25-27 Sept. 2019,ieeexplore
10.1109/IAAI51705.2020.9332828,Remote Monitoring System of Mechanical Manufacturing Equipment based on Digital Image Processing,IEEE,Conferences,"Precision instrument, machinery manufacturing equipment needs to use a monitoring system to monitor production details in real time during its production and processing. However, the traditional remote monitoring system has a weak ability to process monitoring images, resulting in a low peak signal-to-noise ratio of the system. The surveillance image is blurry. Therefore, based on digital image processing, a new remote monitoring system for machinery manufacturing equipment is designed. In hardware, the front-end of voltage signal acquisition is redesigned, and the communication circuit is optimized. In software, digital image processing technology is used to sharpen the image fuzzy contour, correct the image texture, and enhance the fuzzy details of the monitoring image; the remote positioning and monitoring function of the system is set to realize the remote monitoring of mechanical manufacturing equipment. Experimental results: Compared with the monitoring system under the traditional design, the peak signal-to-noise ratio of the remote monitoring system designed this time is much higher than that of the traditional monitoring system. Visible digital image processing technology can help the surveillance system to enhance the fuzzy details of the image.",https://ieeexplore.ieee.org/document/9332828/,2020 IEEE International Conference on Industrial Application of Artificial Intelligence (IAAI),25-27 Dec. 2020,ieeexplore
10.1109/DTPI52967.2021.9540104,Research and practice of lightweight digital twin speeding up the implementation of flexible manufacturing systems,IEEE,Conferences,"Parallel manufacturing in Industry 5.0 requires digital twin to digitize physical systems, building virtual models to open up channels connecting physical systems, information systems, and social systems, and transforming the physical models of the existing production environment to achieve two-way feedback of virtual and real is the current research direction. This paper proposes the modeling idea of lightweight digital twin, extracts core dimensions and performs digital virtual simulation, so as to quickly realize the complete process of two-way feedback, and realize a set of chess flexible parallel manufacturing production lines as a practice for the design of complete lightweight digital twin.",https://ieeexplore.ieee.org/document/9540104/,2021 IEEE 1st International Conference on Digital Twins and Parallel Intelligence (DTPI),15 July-15 Aug. 2021,ieeexplore
10.1109/ICET51757.2021.9450991,Research on Artificial Intelligence Industrial Big Data Platform for Industrial Internet Applications,IEEE,Conferences,"With the development of industry, enterprises have put forward higher requirements for real-time data collection and analysis. According to this, a distributed industrial big data platform for Industrial Internet applications is designed, which can effectively support the industrial field big data collection and storage. The system contains a distributed database, real-time database and offline big data platform to implement the collection and storage of all data in the system. Thus, the industrial big data platform architecture and related data hierarchical processing flow are proposed, which can effectively meet the requirements of the multi-source data and real-time processing of smart factory. The proposed data platform technical architecture will have important reference value for the realization of smart manufacturing and smart factory.",https://ieeexplore.ieee.org/document/9450991/,2021 IEEE 4th International Conference on Electronics Technology (ICET),7-10 May 2021,ieeexplore
10.1109/CYBER53097.2021.9588229,Research on product identification and positioning technology of machine vision based on Yolo detection framework,IEEE,Conferences,"Machine vision is one of the core technologies for modern enterprises to develop unmanned, automated, and intelligent systems. First, conduct research on image acquisition and data expansion technology, carry out product identification analysis based on deep learning in order to further improve the unmanned and intelligent level of the production site, then perform product positioning based on the YOLO detection framework, and finally complete the application verification of product identification and positioning. Faced with the reality that the real-time requirements of product inspection on the production line are constantly increasing, the YOLO inspection framework is selected, and the model convolution structure and anchor points are added to meet the needs of product identification and positioning on the production line. The software environment and model construction are improved through the selection of light source, camera and lens, and finally the application practice of product identification and positioning is completed. Research and practice reveal that the experimental model can meet the real-time needs of intelligent manufacturing at a detection speed of more than 20 FPS under the premise of ensuring accuracy.",https://ieeexplore.ieee.org/document/9588229/,"2021 IEEE 11th Annual International Conference on CYBER Technology in Automation, Control, and Intelligent Systems (CYBER)",27-31 July 2021,ieeexplore
10.1109/ICRA.2019.8794127,Residual Reinforcement Learning for Robot Control,IEEE,Conferences,"Conventional feedback control methods can solve various types of robot control problems very efficiently by capturing the structure with explicit models, such as rigid body equations of motion. However, many control problems in modern manufacturing deal with contacts and friction, which are difficult to capture with first-order physical modeling. Hence, applying control design methodologies to these kinds of problems often results in brittle and inaccurate controllers, which have to be manually tuned for deployment. Reinforcement learning (RL) methods have been demonstrated to be capable of learning continuous robot controllers from interactions with the environment, even for problems that include friction and contacts. In this paper, we study how we can solve difficult control problems in the real world by decomposing them into a part that is solved efficiently by conventional feedback control methods, and the residual which is solved with RL. The final control policy is a superposition of both control signals. We demonstrate our approach by training an agent to successfully perform a real-world block assembly task involving contacts and unstable objects.",https://ieeexplore.ieee.org/document/8794127/,2019 International Conference on Robotics and Automation (ICRA),20-24 May 2019,ieeexplore
10.1109/ICE/ITMC52061.2021.9570221,Resilient Manufacturing Systems enabled by AI support to AR equipped operator,IEEE,Conferences,"Supply chains and manufacturing systems robustness and resilience are, for many years, but especially nowadays, key features requested to ensure reliable and efficient production processes. Two domains are crucial to achieve such purpose: the former is fast and comprehensive monitoring, efficient and reliable condition detection and effective and explicable support for decision making. The latter refers to the intervention by operators, able to better identify problems and to put in place effective operations aimed at fixing it or, better, to prevent such circumstances. This paper presents an integrated approach encompassing a sophisticated IoT and AI-based approach to monitor and detect critical situations, fully integrated with an AR (Augmented Reality) system supporting operators in the field to take informed actions in bi-directional continuous connection. Activities in the context of EC funded project Qu4lity developed in Politecnico di Milano Industry 4.0 Lab, a test environment implementing the proposed approach and demonstrating in an automated production line the effectiveness of the approach, significantly improving performances. Analysis of performance indicators demonstrates the soundness of the proposed solution and implementation methodology to make the overall production process more resilient, efficient and with product defects reduction.",https://ieeexplore.ieee.org/document/9570221/,"2021 IEEE International Conference on Engineering, Technology and Innovation (ICE/ITMC)",21-23 June 2021,ieeexplore
10.1109/SPAC53836.2021.9539961,SDN-based Industrial Internet Security Gateway,IEEE,Conferences,"Industrial Internet is widely used in the production field. As the openness of networks increases, industrial networks facing increasing security risks. Information and communication technologies are now available for most industrial manufacturing. This industry-oriented evolution has driven the emergence of cloud systems, the Internet of Things (IoT), Big Data, and Industry 4.0. However, new technologies are always accompanied by security vulnerabilities, which often expose unpredictable risks. Industrial safety has become one of the most essential and challenging requirements. In this article, we highlight the serious challenges facing Industry 4.0, introduce industrial security issues and present the current awareness of security within the industry. In this paper, we propose solutions for the anomaly detection and defense of the industrial Internet based on the demand characteristics of network security, the main types of intrusions and their vulnerability characteristics. The main work is as follows: This paper first analyzes the basic network security issues, including the network security needs, the security threats and the solutions. Secondly, the security requirements of the industrial Internet are analyzed with the characteristics of industrial sites. Then, the threats and attacks on the network are analyzed, i.e., system-related threats and process-related threats; finally, the current research status is introduced from the perspective of network protection, and the research angle of this paper, i.e., network anomaly detection and network defense, is proposed in conjunction with relevant standards. This paper proposes a software-defined network (SDN)-based industrial Internet security gateway for the security protection of the industrial Internet. Since there are some known types of attacks in the industrial network, in order to fully exploit the effective information, we combine the ExtratreesClassifier to enhance the detection rate of anomaly detection. In order to verify the effectiveness of the algorithm, this paper simulates an industrial network attack, using the acquired training data for testing. The test data are industrial network traffic datasets, and the experimental results show that the algorithm is suitable for anomaly detection in industrial networks.",https://ieeexplore.ieee.org/document/9539961/,"2021 International Conference on Security, Pattern Analysis, and Cybernetics（SPAC)",18-20 June 2021,ieeexplore
10.1109/ICRA48506.2021.9561020,SQRP: Sensing Quality-aware Robot Programming System for Non-expert Programmers,IEEE,Conferences,"Robot programming typically makes use of a set of mechanical skills that is acquired by machine learning. Because there is in general no guarantee that machine learning produces robot programs that are free of surprising behavior, the safe execution of a robot program must utilize monitoring modules that take sensor data as inputs in real time to ensure the correctness of the skill execution. Owing to the fact that sensors and monitoring algorithms are usually subject to physical restrictions and that effective robot programming is sensitive to the selection of skill parameters, these considerations may lead to different sensor input qualities such as the view coverage of a vision system that determines whether a skill can be successfully deployed in performing a task. Choosing improper skill parameters may cause the monitoring modules to delay or miss the detection of important events such as a mechanical failure. These failures may reduce the throughput in robotic manufacturing and could even cause a destructive system crash. To address above issues, we propose a sensing quality-aware robot programming system that automatically computes the sensing qualities as a function of the robot’s environment and uses the information to guide non-expert users to select proper skill parameters in the programming phase. We demonstrate our system framework on a 6DOF robot arm for an object pick-up task.",https://ieeexplore.ieee.org/document/9561020/,2021 IEEE International Conference on Robotics and Automation (ICRA),30 May-5 June 2021,ieeexplore
10.1109/RoboSoft48309.2020.9116004,Scalable sim-to-real transfer of soft robot designs,IEEE,Conferences,"The manual design of soft robots and their controllers is notoriously challenging, but it could be augmented-or, in some cases, entirely replaced-by automated design tools. Machine learning algorithms can automatically propose, test, and refine designs in simulation, and the most promising ones can then be manufactured in reality (sim2real). However, it is currently not known how to guarantee that behavior generated in simulation can be preserved when deployed in reality. Although many previous studies have devised training protocols that facilitate sim2real transfer of control polices, little to no work has investigated the simulation-reality gap as a function of morphology. This is due in part to an overall lack of tools capable of systematically designing and rapidly manufacturing robots. Here we introduce a low cost, open source, and modular soft robot design and construction kit, and use it to simulate, fabricate, and measure the simulation-reality gap of minimally complex yet soft, locomoting machines. We prove the scalability of this approach by transferring an order of magnitude more robot designs from simulation to reality than any other method. The kit and its instructions can be found here: github.com/skriegman/sim2real4designs.",https://ieeexplore.ieee.org/document/9116004/,2020 3rd IEEE International Conference on Soft Robotics (RoboSoft),15 May-15 July 2020,ieeexplore
10.1109/IS3C50286.2020.00134,Screw defect detection system based on AI image recognition technology,IEEE,Conferences,"In the past ten years, smart manufacturing has been widely discussed and gradually introduced into various manufacturing fields. Since Germany proposed the concept of “Industry 4.0” in 2011, it has been spreading and feverish all over the world. For Industry 4.0, information digitization, intelligent defect detection and database platform management are their main core technologies. Aiming at a large screw industry manufacturing field in central and southern Taiwan, this paper proposes a screw defect detection system based on AI image recognition technology to detect damage to the nut during the “molding” process in the screw production process, and it is determined whether the inspected screw passes the inspection. The recognition result is given as shown in Figure 1. This paper uses 500 non-defective screw samples and 20 defective screw samples provided by the screw factory. The above samples collected real-time images through the sampling structure designed in this article, and we adopt Microsoft Corporation's ML.NET suite to model AI images, and uses the following four deep learning models: ResNetV2 50, ResNetV2 101, InceptionV3, MoblieNetV2 for learning; in the process of learning, this article divides the data set into three types of data sets (one is the unknown set that is not used for training but mixed with correct and defective samples, and the other is used for post-training verification of mixed samples with correct and defective samples. The third is a training set for training a mixture of correct and defective samples) This arrangement is used for subsequent verification models; after training, a PC-based screw defect detection system is implemented as shown in Figure 2; finally, with Detect screw defects in the form of instant photography. After the experiment, in 1,000 repeated tests, the success rate of defect detection reached 97%, while the false positive rate was only 2%.",https://ieeexplore.ieee.org/document/9394116/,"2020 International Symposium on Computer, Consumer and Control (IS3C)",13-16 Nov. 2020,ieeexplore
10.1109/ICCAD.2015.7372550,Self learning analog/mixed-signal/RF systems: Dynamic adaptation to workload and environmental uncertainties,IEEE,Conferences,"Real-time systems for wireless communication, digital signal processing and control experience a wide gamut of operating conditions (signal/channel noise, workload demand, perturbed process conditions). As device bandwidths expand, it becomes increasingly expensive, from a power consumption and reliability perspective, to operate such real-time systems for worst-case (static) performance requirements. In contrast, it is attractive to design algorithms, architectures and circuits that are power-performance tunable and can adapt dynamically, via self-learning techniques, to the requirements of system-level applications for extended battery usage and device lifetime. Such future systems will feed application level demands to the underlying algorithm-architecture-circuit design fabric through built-in sense-and-control infrastructure (hardware, software). The sense functions assess instantaneous application level demands (e.g. throughput, signal integrity) as well as the performances of the individual hardware components as determined by manufacturing process conditions. The control functions actuate algorithm-through-circuit level tuning knobs that continuously trade off performance vs. power of the individual software and hardware modules in such a way as to deliver the end-to-end desired application level Quality of Service (QoS), while minimizing energy/power consumption. Application to wireless communications systems, digital signal processing and control algorithms is discussed.",https://ieeexplore.ieee.org/document/7372550/,2015 IEEE/ACM International Conference on Computer-Aided Design (ICCAD),2-6 Nov. 2015,ieeexplore
10.1109/CHICC.2008.4605508,Service-oriented design and implementation strategy of real-time distributed embedded control software,IEEE,Conferences,"Real-time embedded systems are core issue related to information technology and own huge application demand. This paper presents a service-oriented design methodology that reduces complexity by separating data-related computational parts and interaction among components. Architectures are composed hierarchically to manage embedded models and achieve real-time actor and architecture reuse. We introduce a notion of supervisor to manage the architecture and discuss how to aggregate individual componentpsilas computation into a well-defined composite computation from a view of interaction semantics contract. A component-oriented hierarchically real-time quality analysis and processing control system of manufacturing data in Kunming Iron &amp; Steel Co, Ltd.(KISC) is implemented to prove the feasibility and flexibility of our methodology.",https://ieeexplore.ieee.org/document/4605508/,2008 27th Chinese Control Conference,16-18 July 2008,ieeexplore
10.1109/RO-MAN50785.2021.9515431,Simplifying the A.I. Planning modeling for Human-Robot Collaboration,IEEE,Conferences,"For an effective deployment in manufacturing, Collaborative Robots should be capable of adapting their behavior to the state of the environment and to keep the user safe and engaged during the interaction. Artificial Intelligence (AI) enables robots to autonomously operate understanding the environment, planning their tasks and acting to achieve some given goals. However, the effective deployment of AI technologies in real industrial environments is not straightforward. There is a need for engineering tools facilitating communication and interaction between AI engineers and Domain experts. This paper proposes a novel software tool, called TENANT (Tool fostEriNg Ai plaNning in roboTics) whose aim is to facilitate the use of AI planning technologies by providing domain experts like e.g., production engineers, with a graphical software framework to synthesize AI planning models abstracting from syntactic features of the underlying planning formalism.",https://ieeexplore.ieee.org/document/9515431/,2021 30th IEEE International Conference on Robot & Human Interactive Communication (RO-MAN),8-12 Aug. 2021,ieeexplore
10.1109/IJCNN.1993.714078,Simulation of manufacturing models and its learning with artificial neural networks,IEEE,Conferences,"Introduces some new faces of simulation for intelligent manufacturing systems. Not only the productive parameters are treated from a multi-level point of view but the economic indicators were calculated with global optimistic features. The general marketing aspects are considered as critical conditions. To solve several decision making problems of a complex manufacturing organization, the authors extract meaningful variables to observe by data analysis and construct an artificial neural net. The authors try to take some advantages of non-symbolic processing such that parallel treatment and its implementation guide toward a real-time system, learning capabilities also can be applied independently to problems.",https://ieeexplore.ieee.org/document/714078/,"Proceedings of 1993 International Conference on Neural Networks (IJCNN-93-Nagoya, Japan)",25-29 Oct. 1993,ieeexplore
10.1109/ICSI.1992.217250,Software engineering environment development (SEED): an integration project of ROC,IEEE,Conferences,"The SEED development environment composes a software developer's service center, workstation environment (user site), and a communications network. Applications emphasis is on business software written in Cobol, scientific and engineering software written in Fortran 77, and system software written in C. Some AI or real-time and military-usable languages such as Lisp, Prolog, and Ada are also provided for software developers. Instead of having a heavy economic reliance on manufacturing, Taiwan must achieve even greater success in the services and information industries. In these industries, there is no more important component than software. It is no exaggeration to state that the continued elevation of the living standard in Taiwan depends on the growth and near term future domestic dominance and international competitiveness of its information industry. This growth demands sincere and focused efforts on the software industry. The government, related institutes, education institutions, and private industries will continue to plan and work together to make Taiwan software industry world-class in size and technology.&lt;<ETX>&gt;</ETX>",https://ieeexplore.ieee.org/document/217250/,Proceedings of the Second International Conference on Systems Integration,15-18 June 1992,ieeexplore
10.1109/WRC-SARA.2019.8931920,Software-defined Cloud Manufacturing in the Context of Industry 4.0,IEEE,Conferences,"In the practice of “Cloud Manufacturing (CMfg)” or “Industrial Internet”, there still exist key problems, including: 1) big data analytics and decision-making in the cloud could not meet the requirements of time-sensitive manufacturing applications, moreover uploading ZettaBytes of future device data to the cloud may cause serious network congestion, 2) the manufacturing system lacks openness and evolvability, thus restricting the rapid optimization and transformation of the system, 3) big data from the shop-floor IoT devices and the internet has not been effectively utilized to guide the optimization and upgrade of the manufacturing system. In view of these key practical problems, we propose an open evolutionary architecture of intelligent CMfg system with collaborative edge and cloud processing capability. Hierarchical gateways near shop-floor things are introduced to enable fast processing for time-sensitive applications. Big data in another dimension from the software defined perspective will be used to decide the efficient operations and highly dynamic upgrade of the system. From the software system view, we also propose a new mode - AI-Mfg-Ops (AI-enabled Cloud Manufacturing Operations) with a supporting framework, which can promote the fast operation and upgrading of CMfg systems with AI enabled monitoring-analysis-planning-execution close loop. This work can improve the universality of CMfg for real-time fast response and operation &amp; upgrading.",https://ieeexplore.ieee.org/document/8931920/,2019 WRC Symposium on Advanced Robotics and Automation (WRC SARA),21-22 Aug. 2019,ieeexplore
10.1109/SysEng.2017.8088320,Software-in-the-loop testbed for multi-agent-systems in a discrete event simulation: Integration of the Java Agent Development Framework into Plant Simulation,IEEE,Conferences,"Today's research projects propose a modular manufacturing environment for production sites, which adapt itself autonomously and makes manufacturing decisions without human interaction. Therefore, it is necessary that the next generations of production lines, especially the intralogistics transportation systems, are designed more adaptable and flexible. The object in this paper is a cyber-physical material flow system with flexible, autonomous and collaborative vehicles combined with centralized sensors to digitize the workspace. For this purpose, an interface was developed which allows a discrete event simulation tool to communicate with a Multi-Agent-System. Thereby, the decision-making of the agents is integrated directly into the simulation process of the discrete event simulation software. The architecture of this interface is presented as well as a test of its functionality. The architecture is implemented with the Java Agent Development Framework and Plant Simulation as the discrete event simulation tool. The result is an interface, which allows to transfer data from the simulation, in case of an event, to the agent platform. The Multi-Agent-System solves the event specific problem due to its ontology and responses it to the simulation. Therefore, it is possible to integrate the ontology implemented in the physical system as software-in-the-loop in the simulation environment. Furthermore, the possibility is given to improve the ontology iteratively based on historical production data. Different strategies of agents can be combined and improved through machine-learning algorithms by using real production data from the task specific hardware. This leads into a continuous improvement process.",https://ieeexplore.ieee.org/document/8088320/,2017 IEEE International Systems Engineering Symposium (ISSE),11-13 Oct. 2017,ieeexplore
10.1109/ICRITO48877.2020.9198036,State of Art: Energy Efficient Protocols for Self-Powered Wireless Sensor Network in IIoT to Support Industry 4.0,IEEE,Conferences,"Up gradation of manufacturing systems in industries by means of implementing innovative manufacturing techniques that captures real time data, applies machine learning algorithms, makes entire system self-decisive and provides inter connectivity to the whole system is the prime focus of the Industry 4.0. It is aimed at bringing new industrial revolution with the help of internet of things technology due to its considerable influence in the industrial manufacturing process. Though impact of internet of things in industrial sector is huge, a practical implementation incorporates challenges in energy efficiency, self-powered sensor nodes and security. For processing of gathered data self-powered sensor nodes may sinks energy from ambient energy sources. A considerable amount of efforts has been put by researchers to address the challenges for development of energy efficient routing protocol for such nodes. The presented survey is aimed at analyzing the protocols for contribution towards the goals of industry 4.0 Energy efficient protocol will support the system to consume least energy for its operation. Henceforth combination of self-powered wireless sensor network and energy efficient protocol will be useful to power up many industrial IoT applications.",https://ieeexplore.ieee.org/document/9198036/,"2020 8th International Conference on Reliability, Infocom Technologies and Optimization (Trends and Future Directions) (ICRITO)",4-5 June 2020,ieeexplore
10.1109/CSCWD.2005.194338,Study of ASP service lifecycle management technologies for networked manufacturing system,IEEE,Conferences,"Nowadays, networked manufacturing system based on ASP (application service provider) has become one of the hotspots of research and application. How to manage large amount of ASP services, and how to provide better service quality, has become very important. The paper mainly studies the ASP service management technologies. We present the concept of service lifecycle management (SLM), defines ASP service, and set up a state model for service lifecycle. Then, we give several key technologies' solutions for implementation of service management. Finally the application of these technologies in a real networked manufacturing system is introduced.",https://ieeexplore.ieee.org/document/1504245/,"Proceedings of the Ninth International Conference on Computer Supported Cooperative Work in Design, 2005.",24-26 May 2005,ieeexplore
10.1109/ICMLC.2006.258811,Study on Virtual Intelligent Assembly System for Machine Tools Based on Multi-Agent and Petri-Net,IEEE,Conferences,"Virtual reality is a technology, which is often regarded as a natural extension to 3D computer graphics with advanced input and output devices. It can help manufacturing enterprises attain their common goal for lowering the total manufacturing costs, producing products in shorter times and with higher quality, and deliver products on time. This paper explored the application of several advance technologies such as VRML, multi-agent and Petri-net in virtual intelligent assembly system. A proof-of-concept desktop system, namely, virtual intelligent assembly system for machine tools (VIASMT) based on multi-agent and Petri-net has been proposed and developed to display assembly process and demonstrate correlative functions of virtual machine tool. The common feature of traditional methods is that establishing a precise state equation to simulate the dynamic process of system. But, VIASMT can't be described by accurate mathematical method in fact. Relatively speaking, Petri net has stronger ability to describe the dynamic property of system perfectly, and its graphic means is more easily to be understood and accepted. Some software development kits such as visual C++ and Open Inventor are used to implement the VIASMT with the help of multi-agent and Petri-net technologies. The work flowchart is given. In this virtual intelligent assembly system, a new machine tool can be assembled easily and rapidly. In this paper, several application instances are provided",https://ieeexplore.ieee.org/document/4028028/,2006 International Conference on Machine Learning and Cybernetics,13-16 Aug. 2006,ieeexplore
10.1109/ICE/ITMC49519.2020.9198430,Supporting SMEs in the Lake Constance Region in the Implementation of Cyber-Physical-Systems: Framework and Demonstrator,IEEE,Conferences,"With the emergence of the recent Industry 4.0 movement, data integration is now also being driven along the production line, made possible primarily by the use of established concepts of intelligent supply chains, such as the digital avatars. Digital avatars - sometimes also called Digital Twins or more broadly Cyber-Physical Systems (CPS) - are already successfully used in holistic systems for intelligent transport ecosystems, similar to the use of Big Data and artificial intelligence technologies interwoven with modern production and supply chains. The goal of this paper is to describe how data from interwoven, autonomous and intelligent supply chains can be integrated into the diverse data ecosystems of the Industry 4.0, influenced by a multitude of data exchange formats and varied data schemas. In this paper, we describe how a framework for supporting SMEs was established in the Lake Constance region and describe a demonstrator sprung from the framework. The demonstrator project's goal is to exhibit and compare two different approaches towards optimisation of manufacturing lines. The first approach is based upon static optimisation of production demand, i.e. exact or heuristic algorithms are used to plan and optimise the assignment of orders to individual machines. In the second scenario, we use real-time situational awareness - implemented as digital avatar - to assign local intelligence to jobs and raw materials in order to compare the results to the traditional planning methods of scenario one. The results are generated using event-discrete simulation and are compared to common (heuristic) job scheduling algorithms.",https://ieeexplore.ieee.org/document/9198430/,"2020 IEEE International Conference on Engineering, Technology and Innovation (ICE/ITMC)",15-17 June 2020,ieeexplore
10.1109/ISIC.1991.187403,Symbolic feature-based representation and planning for an agent based robot controller,IEEE,Conferences,"A prototype system has been developed to input 3-D computer-aided-design (CAD) data to automatically generate and implement robot trajectories for such application as waterjet cutting, surface finishing and polishing. The system can drive various robot configurations and handle contingencies such as collisions and singularities. The system provides a framework for integration of high-level reasoning, real-time path and trajectory planning, and various levels of feedback based on contingency detection algorithms or sensor data. Although initial CAD data have been in the form of IGES geometric entities, a higher-level CAD description based on manufacturing features which incorporates both geometric and process information is being developed. This feature-based CAD representation provides a direct interface between the CAD design data and the planning system for robot control. An agent actor paradigm is proposed as the representation for software/hardware system specifications and associated software and hardware modules.&lt;<ETX>&gt;</ETX>",https://ieeexplore.ieee.org/document/187403/,Proceedings of the 1991 IEEE International Symposium on Intelligent Control,13-15 Aug. 1991,ieeexplore
10.1109/IJCNN.1990.137594,Synergy of artificial neural networks and knowledge-based expert systems for intelligent FMS scheduling,IEEE,Conferences,"A hybrid architecture that integrates artificial neural networks and knowledge-based expert systems to generate solutions for the real-time scheduling of flexible manufacturing systems is described. The artificial neural networks perform pattern recognition and, due to their inherent characteristics, support the implementation of automated knowledge acquisition and refinement schemes through a feedback mechanism. The artificial neural network structures enable the system to recognize patterns in the tasks to be solved in order to select the best scheduling rule according to different demands. The knowledge-based expert systems are the higher-order elements which drive the inference strategy and interpret the constraints and restrictions imposed by the upper levels of the flexible manufacturing system control hierarchy. The level of self-organization achieved provides a system with a higher probability of success than traditional approaches",https://ieeexplore.ieee.org/document/5726554/,1990 IJCNN International Joint Conference on Neural Networks,17-21 June 1990,ieeexplore
10.1109/AIMS.2013.4,Table of contents,IEEE,Conferences,"The following topics are dealt with: artificial intelligence; neural networks and fuzzy systems; evolutionary computation; bioinformatics and bioengineering; data and semantic mining; games, VR and visualization; intelligent systems and applications; systems intelligence; control intelligence; e-science and e-systems; robotics, cybernetics, engineering, and manufacturing; operations research; discrete-event and real-time systems; image, speech and signal processing; industry, business, management, human factors and social issues; energy, power, transport, logistics, harbour, shipping and marine simulation; parallel, distributed, and software architectures and systems; mobile-ad hoc wireless networks, Mobicast, sensor placement, and target tracking; performance engineering of computer and communications systems; and circuits and devices.",https://ieeexplore.ieee.org/document/6959881/,"2013 1st International Conference on Artificial Intelligence, Modelling and Simulation",3-5 Dec. 2013,ieeexplore
10.1109/CSCWD.2012.6221788,Table of contents,IEEE,Conferences,The following topics are dealt with: multiagent systems; collaboration methods; collaboration platforms; software tools; collaborative workflows; ontology; interoperability; cloud computing; service-oriented computing; collaborative virtual environments; collaborative wireless sensor networks; social aspects; human factors; manufacturing collaboration technology applications; green products; collaborative networks productions; service collaborative network; Internet of things and logistics.,https://ieeexplore.ieee.org/document/6221788/,Proceedings of the 2012 IEEE 16th International Conference on Computer Supported Cooperative Work in Design (CSCWD),23-25 May 2012,ieeexplore
10.1109/CSCWD.2013.6580927,Table of contents,IEEE,Conferences,The following topics are dealt with: collaboration methods; collaboration techniques; multiagent systems; collaborative workflows; collaboration platforms; software tools; ontology; interoperability; collaborative computing; cloud computing; grid computing; Web services; collaborative virtual environments; collaborative manufacturing technology; healthcare applications; collaborative supply chains; collaborative enterprise networks; Internet of Things; and logistics.,https://ieeexplore.ieee.org/document/6580927/,Proceedings of the 2013 IEEE 17th International Conference on Computer Supported Cooperative Work in Design (CSCWD),27-29 June 2013,ieeexplore
10.1109/SMC.2013.4,Table of contents,IEEE,Conferences,The following topics are dealt with: collaborative wireless sensor networks; Internet of Things technology; decision support systems; energy-efficient systems; intelligent Internet systems; human-machine interaction; systems science; soft computing; discrete event systems; healthcare system; distributed manufacturing systems; distributed adaptive systems; granular computing; environmental sensing; decision making; agent-based systems; intelligent real-time automation; medical mechatronics; machine learning; Big Data; medical image processing; medical signal processing; biomedical systems; hybrid metaheuristics; intelligent learning; control systems; unmanned aerial vehicles; collaborative manufacturing and supply chains; computational awareness; pervasive computing; ubiquitous computing; intelligent network computing; complex systems; cybernetic intelligent systems; human-centered transportation systems; driver-vehicle systems; haptics interface; cloud computing; and enterprise systems.,https://ieeexplore.ieee.org/document/6721755/,"2013 IEEE International Conference on Systems, Man, and Cybernetics",13-16 Oct. 2013,ieeexplore
10.1109/ICCRD.2011.5764067,Table of contents vol. 01,IEEE,Conferences,The following topics are dealt with: computer research and development; event driven programming; artificial intelligence; expert systems; algorithm analysis; high performance computing; automated software engineering; human computer interaction; bioinformatics; scientific computing; image processing; information retrieval; compilers; interpreters; computational intelligence; computer architecture; embedded systems; computer animation; Internet; Web applications; communication/networking; knowledge data engineering; computer system implementation; logics; VLSI; mathematical software; information systems; computer based education; mathematical logic; mobile computing; computer games; multimedia applications; computer graphics; virtual reality; natural language processing; neural networks; computer modeling; parallel computing; distributed computing; computer networks; pattern recognition; computer security; computer simulation; computer vision; probability; statistics; performance evaluation; computer aided design/manufacturing; computing ethics; programming languages; problem complexity; control systems; physical sciences; engineering; discrete mathematics; reconfigurable computing systems; data communications; robotics; automation; system security; cryptography; data compression; data encryption; data mining; database systems; document processing; text processing; educational technology; digital library; technology management; digital signal processing; theoretical computer science; digital systems; logic design; ubiquitous computing; and visualizations.,https://ieeexplore.ieee.org/document/5764067/,2011 3rd International Conference on Computer Research and Development,11-13 March 2011,ieeexplore
10.1109/NAECON.1995.522000,Task allocation in distributed computer systems through an AI planner solver,IEEE,Conferences,"Many avionics systems use specialized parallel architectures to speed processing and to increase system reliability. The software used therein is frequently divided into tasks and executed concurrently on multiple processors under strict real-time constraints critical to the mission's successful performance. Scheduling and planning are needed for effectively managing the computational resources on such avionics architectures. Since most real-time scheduling problems are known to be NP-hard, an approximation approach that applies heuristic methods using conventional computer algorithms has been used to solve these scheduling problems. Artificial intelligence (AI) planners have been used extensively in manufacturing scheduling and operations research. In this paper, we demonstrate the idea of using AI planners to perform scheduling through an example. We derive a solution to scheduling several image tasks on a distributed computer system, using the AI planner PRODIGY. The basic characteristics of AI planners in general and the PRODIGY solver in particular are described, the domain theory and problem specification for our problem through the PRODIGY description language PDL are presented.",https://ieeexplore.ieee.org/document/522000/,Proceedings of the IEEE 1995 National Aerospace and Electronics Conference. NAECON 1995,22-26 May 1995,ieeexplore
10.1109/TAI.1992.246450,The G2 development and deployment environment,IEEE,Conferences,"G2 is an object-oriented development and deployment environment, combining rule-based and procedural reasoning, user interface graphics, database interface capabilities, dynamic simulation, and real-time execution in a single package. The applications of G2 typically have large knowledge bases, with tens of thousands of objects, and generally real-time execution is required. The online installations span the process industries, discrete manufacturing, aerospace, telecommunications, electric utilities, and others. Primarily these installations have been implemented directly by plant engineers, who have defined the knowledge contained in the applications using G2's structured natural language and active-object graphics. The design of G2 is described.&lt;<ETX>&gt;</ETX>",https://ieeexplore.ieee.org/document/246450/,Proceedings Fourth International Conference on Tools with Artificial Intelligence TAI '92,10-13 Nov. 1992,ieeexplore
10.1109/icABCD49160.2020.9183853,The Impact of Smart Manufacturing Approach On The South African Manufacturing Industry,IEEE,Conferences,"SM is a technology-driven approach that mainly utilises machines to monitor the entire production of an organisation. The objective of SM in an organisation is to identify ways to automatize the manufacturing process while using data analytics to optimize the manufacturing performance. This research mitigates the impact of technology, in this case expressed as SM in South African industries. The research followed a quantitative approach whereby 42 respondents from low, medium low and high technology industries took part in the study. Data has been amassed from first-hand experience by mean of an adapted questionnaire constituted of three sections: The first section was about the general demographic information of the respondents. Section two investigates the respondent's awareness on SM. Finally, section three assessed the impact that SM had on the performance of the organisation. The findings of this study revealed that Smart Manufacturing has a positive impact in South African manufacturing organisations as it allows effective operations, fast response to customers demand, real time operations optimisation. Nevertheless, Smart Manufacturing is a new concept under the fourth industrial revolution in South Africa and will need time before being totally implemented in all organisations as it is costly.",https://ieeexplore.ieee.org/document/9183853/,"2020 International Conference on Artificial Intelligence, Big Data, Computing and Data Communication Systems (icABCD)",6-7 Aug. 2020,ieeexplore
10.1109/ICICIC.2007.583,The Prediction of Ink Thickness of Touch Panel by Neural Network,IEEE,Conferences,"The thickness of printing ink on touch panel is an important work, which is related with the quality of panel in real use. Therefore, how to make a well control in the film printing of touch panel is very important work in the manufacturing process of touch panel. In this paper, the prediction of ink thickness of touch panel based on neural technique is proposed. This intelligent predictor is expected to provide the accurate information about the control parameters so that the technician could make a good setting work before the panel is on the real-line manufacturing operation. Without enough experiences, the junior technician still can make a good decision in the setting of control parameters for ink printing based on this predictor. Thus, this intelligent prediction system not only can help technicians and greatly improve their working efficiency, but also can save the cost of production.",https://ieeexplore.ieee.org/document/4427918/,"Second International Conference on Innovative Computing, Informatio and Control (ICICIC 2007)",5-7 Sept. 2007,ieeexplore
10.1109/IMTC.2001.928200,The development of an artificial neural network embedded automated inspection quality management system,IEEE,Conferences,This paper describes in detail the development of an innovative artificial neural network embedded automated inspection scheme for the manufacturing industry employing digital image processing techniques. Such a system is capable of performing real-time image processing tasks and identifies the size and location of the finished components on manufactured products as well as the flaws and scratches on surface of products during the manufacturing process. The proposed artificial neural network embedded quality management system provides a user-friendly user interface that has been implemented and tested on a case study from a printed circuit board manufacture. The experimental results have demonstrated the functionality and superiority of the developed artificial neural network embedded inspection system.,https://ieeexplore.ieee.org/document/928200/,IMTC 2001. Proceedings of the 18th IEEE Instrumentation and Measurement Technology Conference. Rediscovering Measurement in the Age of Informatics (Cat. No.01CH 37188),21-23 May 2001,ieeexplore
10.1109/PICMET.2015.7273077,The effects of disruption on different types of tile manufacturing Industry-layouts: An empirical investigation on tile manufacturing industry,IEEE,Conferences,"Almost all manufacturing facilities need to use production scheduling systems to increase productivity and reduce production costs. Most manufacturing industries invest huge amount of money to manufacture and supply products on time in order to meet customers demand and objectives but due to unforeseen disruptions, these objectives are difficult to achieve. In Real-life, production operations are subject to a large number of unexpected disruptions that may invalidate an original schedule. This work considered effects of disruption on different types of industry-layouts that are seldom been used in manufacturing industries namely: fixed, product, production and process industry-layouts. Questionnaires were used for data collection from a number of companies in Cameroon and reliably theory, simulation software's were using to analyze the data. The study reveals that, disruptions varies per industry-layout and the leading sources of disruptions are machine breakdown, power failure, employee's absenteeism and material shortage. It is concluded that disruption of one type may not greatly affect productivity of a certain industry-layout whilst similar disruptions can have devastating effects on another type and also, the impacts of disruption are dependent on the Industry-layouts.",https://ieeexplore.ieee.org/document/7273077/,2015 Portland International Conference on Management of Engineering and Technology (PICMET),2-6 Aug. 2015,ieeexplore
10.1109/IEMC.1998.727776,The importance of artificial intelligence-expert systems in computer integrated manufacturing,IEEE,Conferences,"In order to maintain their competitiveness, companies feel compelled to adopt productivity increasing measures. Yet, they cannot relinquish the flexibility their production cycles need in order to improve their response, and thus, their positioning in the market. To achieve this, companies must combine these two seemingly opposed principles. Thanks to new technological advances, this combination is already a working reality in some companies. It is made possible today by the implementation of computer integrated manufacturing (CIM) and artificial intelligence (AI) techniques, fundamentally by means of expert systems (ES) and robotics. Depending on how these (AI/CIM) techniques contribute to automation, their immediate effects are an increase in productivity and cost reductions. Yet also, the system's flexibility allows for easier adaptation and, as a result, an increased ability to generate value, in other words, competitiveness is improved. The authors have analyzed three studies to identify the possible benefits or advantages, as well as the inconveniences, that this type of technique may bring to companies, specifically in the production field. Although the scope of the studies and their approach differ from one to the other, their joint contribution can be of unquestionable value in order to understand a little better the importance of ES within the production system.",https://ieeexplore.ieee.org/document/727776/,IEMC '98 Proceedings. International Conference on Engineering and Technology Management. Pioneering New Technologies: Management Issues and Challenges in the Third Millennium (Cat. No.98CH36266),11-13 Oct. 1998,ieeexplore
10.1109/ICCD46524.2019.00019,TiLA: Twin-in-the-Loop Architecture for Cyber-Physical Production Systems,IEEE,Conferences,"Digital twin is a virtual replica of a real-world object that lives simultaneously with its physical counterpart. Since its first introduction in 2003 by Grieves, digital twin has gained momentum in a wide range of applications such as industrial manufacturing, automotive and artificial intelligence. However, many digital-twin-related approaches, found in industries as well as literature, mainly focus on modelling individual physical things with high-fidelity methods with limited scalability. In this paper, we introduce a digital-twin architecture called TiLA (Twin-in-the-Loop Architecture). TiLA employs heterogeneous models and online data to create a digital twin, which follows a Globally Asynchronous Locally Synchronous (GALS) model of computation. It facilitates the creation of a scalable digital twin with different levels of modelling abstraction as well as giving GALS formalism for execution strategy. Furthermore, TiLA provides facilities to develop applications around the twin as well as an interface to synchronise the twin with the physical system through an industrial communication protocol. A digital twin for a manufacturing line has been developed as a case study using TiLA. It demonstrates the use of digital twin models together with online data for monitoring and analysing failures in the physical system.",https://ieeexplore.ieee.org/document/8988620/,2019 IEEE 37th International Conference on Computer Design (ICCD),17-20 Nov. 2019,ieeexplore
10.1109/AQTR.2018.8402748,Time series forecasting for dynamic scheduling of manufacturing processes,IEEE,Conferences,"Manufacturing control systems evolved in the recent decades from pre-programmed rigid systems to adaptable, data driven, cloud based implementations, capable to respond to environment changes and new requirements in real time. A byproduct of this transformation is represented by large amounts of structured and semi-structured information, both historical and real-time data that is made available on various layers of the system. This accumulation of information brings the opportunity to move from the rule based decision making algorithms used traditionally by these control systems towards more intelligent approaches, driven by modern deep learning mechanisms. This paper proposes a time series forecasting model using recursive neural networks (RNN) for operation scheduling and sequencing in a virtual shop floor environment. The time series aspect of the RNN is novel in manufacturing domain, in the sense that the new best prediction produced considers the previous decisions and outcomes. The proposed implementation explains how the RNN can be mapped to the specifics of a manufacturing control system and introduces a bidding mechanism to allow dynamic evaluation of individual forecasts. The pilot implementation, initial experiments on sample data sets and results presented show how using recursive neural networks can optimize resource utilization and energy consumption.",https://ieeexplore.ieee.org/document/8402748/,"2018 IEEE International Conference on Automation, Quality and Testing, Robotics (AQTR)",24-26 May 2018,ieeexplore
10.1109/ICMLC.2006.258465,Time-Frequency Analysis for Cutting Tools Wear Characteristics,IEEE,Conferences,"To monitor the tool wear states in drilling, a new method is proposed to obtain the signal characteristics based on the wavelet transformation. The method can reflect the tool wear states by using discrete dyadic wavelet transform. The cutting force signals of cutting process are decomposed; and the values of the decomposed signals in different scales are taken as the feature vectors. The pattern identification is used to monitor the tool wear states in real time. The method can identify the tool wear states correctly by choosing the suitable standard samples. The result shows that the proposed method is suitable for real-time implementation in manufacturing application, and has good identification precision and high efficiency",https://ieeexplore.ieee.org/document/4028638/,2006 International Conference on Machine Learning and Cybernetics,13-16 Aug. 2006,ieeexplore
10.1109/ETFA46521.2020.9212097,Towards Real-time Process Monitoring and Machine Learning for Manufacturing Composite Structures,IEEE,Conferences,"Components made from carbon fiber reinforced plastics (CFRP) offer attractive stability properties for the automotive or aerospace industry despite their light weight. To automate CFRP production, resin transfer molding (RTM) based on thermoset plastics is commonly applied. However, this manufacturing process has its shortcomings in quality and costs. The project CosiMo aims for a highly automated and cost-attractive manufacturing process using cheaper thermoplastic materials. In a thermoplastic RTM (T-RTM) process, the polymerization of ε-caprolactam to polyamide 6 is investigated using an intelligent mold tooling. Multiple sensor types integrated into the mold allow for tracking of process-relevant variables, such as material flow and polymerization state. In addition to monitoring the T-RTM process, a digital twin visualizes progress and makes predictions about issues and countermeasures based on machine learning.",https://ieeexplore.ieee.org/document/9212097/,2020 25th IEEE International Conference on Emerging Technologies and Factory Automation (ETFA),8-11 Sept. 2020,ieeexplore
10.1109/SERVICES51467.2021.00055,Transformation: Case Studies and Lessons Learned : Keynote 2,IEEE,Conferences,"Industrial AI, Big Data Analytics, Machine Learning, and Cyber Physical Systems are changing the way we design product, manufacturing, and service systems. It is clear that as more sensors and smart analytics software are integrated in the networked industrial products and manufacturing systems, predictive technologies can further learn and autonomously optimize service productivity and performance. This presentation will address the trends of Industrial AI for smart service realization. First, Industrial AI systematic approach will be introduced. Case studies on advanced predictive analytics technologies for different maintenance and service operations will be demonstrated. In addition, issues on data quality for high performance and real-time data analytics in future digital service will be discussed.",https://ieeexplore.ieee.org/document/9604414/,2021 IEEE World Congress on Services (SERVICES),5-10 Sept. 2021,ieeexplore
10.1109/DEST.2009.5276766,Transforming SME manufacturing plants into evolvable systems through agents,IEEE,Conferences,"Manufacturing plants need to be flexible and evolvable, which might be achieved by stepping up the level of abstraction at which they are designed. In this work a methodology that starts by creating ontologies of the plants that represent information flows is proposed. Agent-based technology is then used to provide systems with the ability to evolve. The proposal is designed to suit the needs of SME' plants, whose operation cannot be disturbed while progressively undergo a transformation into more advanced ones. The methodology has been based on a partial implementation on a real case that, in its turn, has served to validate the whole procedure.",https://ieeexplore.ieee.org/document/5276766/,2009 3rd IEEE International Conference on Digital Ecosystems and Technologies,1-3 June 2009,ieeexplore
10.1109/VLSID.2018.20,Tutorial T2A: Safe Autonomous Systems: Real-Time Error Detection and Correction in Safety-Critical Signal Processing and Control Algorithms,IEEE,Conferences,"While the last two decades have seen revolutions in computing and communications systems, the next few decades will see a revolution in the use of every-day robotics and artificial intelligence in broad societal applications. Examples of such systems include sensor networks, the smart power grid, self-driven cars and autonomous drones. Such systems are driven by signal processing, control and learning algorithms that process sensor data, actuate control functions and learn about the environment in which these systems operate. The trustworthiness and safety of such systems is of paramount importance and has significant impact on the commercial viability of the underlying technology. As a consequence, anomalies in system operation due to computation errors in on-board processors, degradation and failure of embedded sensors, actuators and electro-mechanical subsystems and unforeseen changes in their operation environment need to detected with minimum latency. Such anomalies also need to be mitigated in ways that ensure the safety of such systems under all possible failure scenarios. Many future systems will be selflearning in the field. It is necessary to ensure that such learning does not compromise the safety of all human personnel involved in the operation of such systems. To enable safe operation of such systems, the underlying hardware needs to be tuned in the field to maximize performance, reliability and error-resilience while minimizing power consumption. To enable such dynamic adaptation, device operating conditions and the onset of soft errors are sensed using post-manufacture and real-time checking mechanisms. These mechanisms rely on the use of built-in sensors and/or low-overhead function encoding techniques to detect anomalies in system functions. A key capability is that of being able to deduce multiple performance parameters of the system-under-test using compact optimized stimulus using learning algorithms. The sensors and function encodings assess the loss in performance of the relevant systems due to workload uncertainties, manufacturing process imperfections, soft errors and hardware malfunction and failures induced by electromechanical degradation. These are then mitigated through the use of algorithm-through-circuit level compensation techniques based on pre-deployment simulation and post-deployment self-learning. These techniques continuously trade off performance vs. power of the individual software and hardware modules in such a way as to deliver the end-to-end desired application level Quality of Service (QoS), while minimizing energy/power consumption and maximizing reliability and safety. Applications to signal processing, and control algorithms for example autonomous systems will be discussed.",https://ieeexplore.ieee.org/document/8326883/,2018 31st International Conference on VLSI Design and 2018 17th International Conference on Embedded Systems (VLSID),6-10 Jan. 2018,ieeexplore
10.1109/IAI50351.2020.9262158,Virtual Commissioning and Machine Learning of a Reconfigurable Assembly System,IEEE,Conferences,"The digital twin application in manufacturing is mainly based on the virtual simulation model of a digital twin to build a solid model, which is applied to the product processing and assembly to achieve precise production control. This paper presents a virtual commissioning digital twin model for the modularized automatic assembly system running in our lab. First, the Siemens NX MCD software tool is used to develop the virtual commissioning digital twin model for the system. Then the different working scenarios are simulated and implemented in the virtual physical simulation environment. The data from the proposed virtual commissioning digital twin model is collected and trained with 6 different machine learning algorithm such as Logistic Regression (LR), Linear Discriminant Analysis (LDA), K-Nearest Neighbors (KNN), Classification and Regression Trees (CART), Gaussian Naive Bayes (NB) and Support Vector Machines (SVM). The advantage of our newly developed virtual commissioning model is that it is able to simulate different working conditions without risk and cost-free. It is also convenient to mimic the worsening working status and failed operation scenarios which need long time to collect for the real system. We use the collected data as input for the machine learning to implement the system monitoring and predicting. The machine learning results for 6 learning algorithms are presented and it shows the possibilities and advantages of our proposed virtual commissioning digital twin model.",https://ieeexplore.ieee.org/document/9262158/,2020 2nd International Conference on Industrial Artificial Intelligence (IAI),23-25 Oct. 2020,ieeexplore
10.1109/ISWCS.2019.8877305,Visible Light Positioning for Location-Based Services in Industry 4.0,IEEE,Conferences,"Industry 4.0 refers to the evolution in manufacturing from computerization to fully cyberphysical systems that exploit rich sensor data, adaptive real-time safety-critical control, and machine learning. An important aspect of this vision is the sensing and subsequent association of objects in the physical world with their cyber and virtual counterparts. In this paper we propose Visible Light Positioning (VLP) as an enabler for these Industry 4.0 applications. We also explore sensing techniques, including cameras (and depth sensors), and other light-based solutions for object positioning and detection along with their respective limitations. We then demonstrate an application of positioning for real time robot control in an interactive multiparty cyber-physical-virtual deployment. Lastly, based on our experience with this cyberphysical-virtual application, we propose Ray-Surface Positioning (RSP), a novel VLP technique, as a low cost positioning system for Industry 4.0.",https://ieeexplore.ieee.org/document/8877305/,2019 16th International Symposium on Wireless Communication Systems (ISWCS),27-30 Aug. 2019,ieeexplore
10.1109/ICRA.2019.8794123,Visual Guidance and Automatic Control for Robotic Personalized Stent Graft Manufacturing,IEEE,Conferences,"Personalized stent graft is designed to treat Abdominal Aortic Aneurysms (AAA). Due to the individual difference in arterial structures, stent graft has to be custom made for each AAA patient. Robotic platforms for autonomous personalized stent graft manufacturing have been proposed in recently which rely upon stereo vision systems for coordinating multiple robots for fabricating customized stent grafts. This paper proposes a novel hybrid vision system for real-time visual-sevoing for personalized stent-graft manufacturing. To coordinate the robotic arms, this system is based on projecting a dynamic stereo microscope coordinate system onto a static wide angle view stereo webcam coordinate system. The multiple stereo camera configuration enables accurate localization of the needle in 3D during the sewing process. The scale-invariant feature transform (SIFT) method and color filtering are implemented for stereo matching and feature identifications for object localization. To maintain the clear view of the sewing process, a visual-servoing system is developed for guiding the stereo microscopes for tracking the needle movements. The deep deterministic policy gradient (DDPG) reinforcement learning algorithm is developed for real-time intelligent robotic control. Experimental results have shown that the robotic arm can learn to reach the desired targets autonomously.",https://ieeexplore.ieee.org/document/8794123/,2019 International Conference on Robotics and Automation (ICRA),20-24 May 2019,ieeexplore
10.1109/ECC.2015.7330620,Web tension regulation with partially known periodic disturbances in roll-to-roll manufacturing systems,IEEE,Conferences,"This paper deals with the problem of synthesizing feedforward control to aid the regulation of web tension in the presence of partially known exogenous inputs. The problem appears in many engineering applications including Roll-to-Roll (R2R) manufacturing systems where the governing equation for tension is nonlinear. Currently known methods for the nonlinear output regulation problem either require the solution of a constrained partial differential equation or the preview information of the signal to be tracked. In this paper, we consider the problem of regulating web tension while rejecting periodic disturbances and use a novel approach to synthesize feedforward control as the solution of a system of differential-algebraic equations, which is considerably less complex and suitable for practical implementation. We assume that the disturbance signal is the output of a known exogenous system with unknown initial conditions. A parameter identification scheme to estimate the unknown initial conditions is developed. The proposed technique is successfully applied to web tension regulation in a large R2R machine which contains real-time hardware and software that is used in industrial practice. Extensive experiments were conducted to evaluate the proposed scheme under various experimental conditions, including different web speeds and materials. We will discuss a representative sample of the results with the proposed nonlinear tension regulator and provide a comparison with a well-tuned industrial PI control scheme to highlight the benefits of using the proposed scheme.",https://ieeexplore.ieee.org/document/7330620/,2015 European Control Conference (ECC),15-17 July 2015,ieeexplore
10.1109/ICSMC.2011.6083632,[Copyright notice],IEEE,Conferences,The following topics are dealt with: brain-machine interface; machine learning technology; service systems; homeland security systems; virtual reality; agent-based modeling; human centered transportation systems; awareness science and engineering; soft computing; enterprise information systems; social signal processing; infrastructure system; manufacturing systems; pattern recognition; medical mechatronics; minimally invasive surgery; medical robotics; medical technology; intelligent power systems; discrete event systems; Petri nets; biometrics; bioinformatics; computational intelligence; supply chain management; shared control; fault diagnosis; systems engineering; Internet; support vector machines; knowledge acquisition; cloud computing; grey systems; humanoid robots; redundant manipulators; formal methods; granular computing; wireless sensor networks; nonlinear control systems; gesture-based interaction; software engineering; multi-agent systems; cognitive computing; social robotics; natural language processing; conflict resolution; intelligent transportation systems; human-robot interaction; image processing; medical informatics; decision support systems; assistive technology; human-centered design; data mining; and anti-terrorism applications.,https://ieeexplore.ieee.org/document/6083632/,"2011 IEEE International Conference on Systems, Man, and Cybernetics",9-12 Oct. 2011,ieeexplore
10.1109/UKSim.2012.123,[Cover art],IEEE,Conferences,The following topics are dealt with: neural networks; evolutionary computation; adaptive dynamic programming; re-enforcement learning; bio-informatics; bio-engineering; computational finance; economics; semantic mining; data mining; virtual reality; data visualization; intelligent systems; soft computing; hybrid computing; e-science; e-systems; robotics; cybernetics; manufacturing; engineering; operations research; discrete event systems; real time systems; image processing; speech processing; signal processing; industry; business; social issues; human factors; marine simulation; power systems; logistics;parallel systems; distributed systems; software architectures;Internet modelling; semantic Web; ontologies; mobile ad hoc wireless networks; Mobicast; sensor placement; target tracking; circuits; sensors and devices.,https://ieeexplore.ieee.org/document/6205540/,2012 UKSim 14th International Conference on Computer Modelling and Simulation,28-30 March 2012,ieeexplore
10.1109/SISY.2010.5647094,[Front cover],IEEE,Conferences,The following topics are dealt with: computational intelligence; machine learning; genetic algorithms; neural nets; neuro fuzzy control; knowledge based systems; expert systems; intelligent robotics; flexible arm control; perception; recognition; reasoning; human robotic interaction; service robots; surgery robots; machine vision; intelligent mechatronics; sensor data fusion; motion control; intelligent actuators; CAD/CAM/CAE Systems; product modeling; manufacturing process planning; advanced modeling techniques; shape modeling; intelligent manufacturing systems; flexible manufacturing systems; production planning; system simulation; rapid prototyping; concurrent engineering; virtual reality; informatics; digital culture; databases; graphics; digital audio; photography; operating systems; security software engineering; healthcare informatics; teaching informatics; and informatics in education process.,https://ieeexplore.ieee.org/document/5647094/,IEEE 8th International Symposium on Intelligent Systems and Informatics,10-11 Sept. 2010,ieeexplore
10.1109/AIMS.2015.1,[Title page i],IEEE,Conferences,The following topics are dealt with: artificial intelligence; neural networks; fuzzy systems; evolutionary computation; bioinformatics; bioengineering; data mining; semantic mining; games; VR; visualization; intelligent systems applications; hybrid computing; soft computing; intelligent systems control; control intelligence; e-science; e-systems; robotics; cybernetics; manufacturing system; operations research; discrete event systems; real time systems; signal processing; speech processing; image processing; natural language processing; human factors; social issues; shipping; marine simulation; transport; logistics; mobile ad hoc wireless networks; Mobicast; sensor placement; target tracking; software architectures; distributed systems; parallel systems; power simulation; performance engineering; communication systems; and circuits.,https://ieeexplore.ieee.org/document/7604531/,"2015 3rd International Conference on Artificial Intelligence, Modelling and Simulation (AIMS)",2-4 Dec. 2015,ieeexplore
10.1109/UKSim.2013.1,[Title page i],IEEE,Conferences,The following topics are dealt with: neural networks; fuzzy systems; evolutionary computation; dynamic programming; reinforcement learning; bioinformatics; bioengineering; computational finance; computational economics; computer games; virtual reality; data visualization; computer networks; intelligent systems; soft computing; intelligent control; e-science; e-systems; robotics; cybernetics; manufacturing systems; operations research; discrete event systems; realtime systems; image processing; speech processing; signal processing; natural language processing; business management; human factors; renewable energy; logistics; parallel architecture; distributed architecture; software architecture; Internet; ontologies; wireless networks; target tracking; performance engineering; circuits; and sensors.,https://ieeexplore.ieee.org/document/6527370/,2013 UKSim 15th International Conference on Computer Modelling and Simulation,10-12 April 2013,ieeexplore
10.1109/UKSim.2015.1,[Title page i],IEEE,Conferences,The following topics are dealt with: neural networks; fuzzy systems; evolutionary computation; bioinformatics; bioengineering; data mining; semantic mining; games; VR; visualization; emergent technologies; intelligent systems; control intelligence; e-science; e-systems; robotics; cybernetics; manufacturing; operations research; discrete event systems; real time systems; image processing; speech processing; signal processing; natural language processing; language technologies; human factors; social issues; energy; power; transport; logistics; harbour; shipping; marine simulation; parallel architectures; distributed architectures; software architectures; Internet modelling; semantic Web; ontologies; mobile ad hoc wireless networks; mobicast; sensor placement and target tracking.,https://ieeexplore.ieee.org/document/7576501/,2015 17th UKSim-AMSS International Conference on Modelling and Simulation (UKSim),25-27 March 2015,ieeexplore
10.1109/CSSim.2009.1,[Title page i],IEEE,Conferences,"The 2009 International conference on Computational Intelligence, Modelling and Simulation dealt with topics related to the following: hybrid intelligent systems; agent based modelling and simulation; Web based simulation; visualization; security modelling and simulation; simulation in education; bioinformatics and biomedical simulation; discrete event and real time systems; image, speech, and signal processing; human factors and social issues; engineering, manufacturing, and control; energy, power generation, and distribution; parallel and distributed architectures and systems; performance engineering of computer and communication systems; and circuits, sensors, and devices.",https://ieeexplore.ieee.org/document/5350212/,"2009 International Conference on Computational Intelligence, Modelling and Simulation",7-9 Sept. 2009,ieeexplore
10.1109/ICSESS.2011.5982490,[Title page],IEEE,Conferences,The following topics are dealt with: BS-SVM multi-classification model; conformance checking; release management; multi-sensors data fusion; security policies; software engineer behavior analysis measurement process; data clustering support; agile adoption experience; business-related design; interoperability testing; energy-aware backbone construction algorithm; distributed file system; sheep and goat expert system;npost internship management system; multi-bank flash memory storage systems; fair-exchange protocols; udp-based high-speed transport protocols; e-commerce; table tennis game; SOA; collision arbitration protocol; life-cycle oriented design model; computer network virtual lab system design; assertion testing framework; privacy preserving data mining; MJ2-RSA cryptosystem; community health records;traffic simulation system;price uncertainty inventory problem; oil theft signal detection; gesture recognition;text clustering algorithm; timed automata;software structure evaluation; wireless data acquisition system; Web service schema matching; software construction; distributed database systems; software development; object oriented analysis; data mining approach; face detection algorithm; neural networks; data replication; video motion compensation errors; software design; dynamic vehicle scheduling; service oriented architecture; customer satisfaction; e-manufacturing system; cryptanalysis; item-based collaborative filtering method; recommender system; savant middleware; ontology-based mobile publishing framework; high-speed data acquisition system; MAC protocols; safety-critical systems; program slicing algorithm; wireless sensor network; augmented reality; spam detection; image retrieval algorithm; tax income; foreign direct investment; IT outsourcing; enterprise mobile e-business system model; medium-sized and small enterprises; supply chain management; immune intrusion detection system; adaptive wiener filtering; VPN technology; GIS sharing platform and image restoration.,https://ieeexplore.ieee.org/document/5982490/,2011 IEEE 2nd International Conference on Software Engineering and Service Science,15-17 July 2011,ieeexplore
10.1109/ICSMC.2009.5346382,[Title page],IEEE,Conferences,"The following topics are dealt with: human-computer interaction; assistive technology; systems safety; systems security; human-computer symbiosis; information retrieval; soft computing; image processing; pattern recognition; discrete event systems; computational intelligence; human centered transportation systems; type-2 fuzzy logic systems; type-2 fuzzy logic control; manufacturing systems; manufacturing automation; smart sensor networks; environmental decision support systems; environmental visualization systems; intelligent learning; user interface design; biometric systems; bioinformatics; evolutionary computation; grey systems; human-machine cooperation; human-machine systems; virtual reality systems; augmented reality systems; systems engineering; systems sustainability; medical systems; health care systems; conflict resolution; intelligent Internet systems; intelligent RFID systems; Web intelligence; Web interaction; agent-based modeling; intelligent signal processing; human-machine interface; human-machine communications; human factors; design information systems; marketing information systems; brain-based information communications; swarm intelligence; management engineering; machine learning; cognitive radio; mobile robot intelligent control; distributed intelligent systems; vehicle, driver, environment and control system; multimedia systems; knowledge acquisition; robotic systems; human performance modeling; interactive media; digital media; granular computing; heuristic algorithms; fuzzy systems; self-organization systems; complex distributed systems; Petri nets; Kansei; image sharing; image retrieval; collaborative wireless sensor networks; enterprise information systems; visual information processing; fault monitoring; fault diagnosis; large-scale systems; intelligent transportation systems; neural networks; machine vision; fuzzy forecasting; information assurance; homeland security; intelligent multimedia computation; decision making; infrastructure systems management; collaborative virtual workspaces; distributed software systems; media computing; optimization; collaborative commerce; uncertain systems control; cybernetics; intelligent power systems; artificial immune systems; systems biology; collaborative manufacturing; supply chains; mechatronics; nonlinear control systems; intelligent multimedia-mobile communications; nano systems; micro systems; reliability engineering; role-based systems; role-based quality; and cooperative systems.",https://ieeexplore.ieee.org/document/5346382/,"2009 IEEE International Conference on Systems, Man and Cybernetics",11-14 Oct. 2009,ieeexplore
10.1109/BDCloud.2018.00136,"iSTEP, an Integrated Self-Tuning Engine for Predictive Maintenance in Industry 4.0",IEEE,Conferences,"The recent expansion of IoT-enabled (Internet of Things) devices in manufacturing contexts and their subsequent data-driven exploitation paved the way to the advent of the Industry 4.0, promoting a full integration of IT services, smart devices, and control systems with physical objects, their electronics and sensors. The real-time transmission and analysis of collected data from factories has the potential to create manufacturing intelligence, of which predictive maintenance is an expression. Hence the need to design new approaches able to manage not only the data volume, but also the variety and velocity, extracting actual value from the humongous amounts of collected data. To this aim, we present iSTEP, an integrated Self-Tuning Engine for Predictive maintenance, based on Big Data technologies and designed for Industry 4.0 applications. The proposed approach targets some of the most common needs of manufacturing enterprises: compatibility with both the on-premises and the in-the-cloud environments, exploitation of reliable and largely supported Big Data platforms, easy deployment through containerized software modules, virtually unlimited horizontal scalability, fault-tolerant self-reconfiguration, flexible yet friendly streaming-KPI computations, and above all, the integrated provisioning of self-tuning machine learning techniques for predictive maintenance. The current implementation of iSTEP exploits a distributed architecture based on Apache Kafka, Spark Streaming, MLlib, and Cassandra; iSTEP provides (i) a specific feature engineering block aimed at automatically extracting metrics from the production monitoring time series, which improves the predictive performance by 77% on average, and (ii) a self-tuning approach that dynamically selects the best prediction algorithm, which improves the predictive performance up to 60%. The iSTEP engine provides transparent predictive models, able to provide end users with insights into the knowledge learned, and it has been experimentally evaluated on a public unbalanced failure dataset, whose extensive results are discussed in the paper.",https://ieeexplore.ieee.org/document/8672266/,"2018 IEEE Intl Conf on Parallel & Distributed Processing with Applications, Ubiquitous Computing & Communications, Big Data & Cloud Computing, Social Computing & Networking, Sustainable Computing & Communications (ISPA/IUCC/BDCloud/SocialCom/SustainCom)",11-13 Dec. 2018,ieeexplore
10.1109/TII.2019.2915846,A Global Manufacturing Big Data Ecosystem for Fault Detection in Predictive Maintenance,IEEE,Journals,"Artificial intelligence, big data, machine learning, cloud computing, and Internet of Things (IoT) are terms which have driven the fourth industrial revolution. The digital revolution has transformed the manufacturing industry into smart manufacturing through the development of intelligent systems. In this paper, a big data ecosystem is presented for the implementation of fault detection and diagnosis in predictive maintenance with real industrial big data gathered directly from large-scale global manufacturing plants, aiming to provide a complete architecture which could be used in industrial IoT-based smart manufacturing in an industrial 4.0 system. The proposed architecture overcomes multiple challenges including big data ingestion, integration, transformation, storage, analytics, and visualization in a real-time environment using various technologies such as the data lake, NoSQL database, Apache Spark, Apache Drill, Apache Hive, OPC Collector, and other techniques. Transformation protocols, authentication, and data encryption methods are also utilized to address data and network security issues. A MapReduce-based distributed PCA model is designed for fault detection and diagnosis. In a large-scale manufacturing system, not all kinds of failure data are accessible, and the absence of labels precludes all the supervised methods in the predictive phase. Furthermore, the proposed framework takes advantage of some of the characteristics of PCA such as its ease of implementation on Spark, its simple algorithmic structure, and its real-time processing ability. All these elements are essential for smart manufacturing in the evolution to Industry 4.0. The proposed detection system has been implemented into the real-time industrial production system in a cooperated company, running for several years, and the results successfully provide an alarm warning several days before the fault happens. A test case involving several outages in 2014 is reported and analyzed in detail during the experiment section.",https://ieeexplore.ieee.org/document/8710319/,IEEE Transactions on Industrial Informatics,Jan. 2020,ieeexplore
10.1109/ACCESS.2019.2945337,A Linearization Model of Turbofan Engine for Intelligent Analysis Towards Industrial Internet of Things,IEEE,Journals,"Big data processing technologies, e.g., multi-sensor data fusion and cloud computing are being widely used in research, development, manufacturing, health monitoring and maintenance of aero-engines, driven by the ever-rapid development of intelligent manufacturing and Industrial Internet of Things (IIoT). This has promoted rapid development of the aircraft engine industry, increasing the aircraft engine safety, reliability and intelligence. At present, the aero-engine data computing and processing platform used in the industrial Internet of things is not complete, and the numerical calculation and control of aero-engine are inseparable from the linear model, while the existing aero-engine model linearization method is not accurate enough to quickly calculate the dynamic process parameters of the engine. Therefore, in this paper, we propose a linear model of turbofan engine for intelligent analysis in IIoT, with the aim to provide a new perspective for the analysis of engine dynamics. The construction of the proposed model includes three steps: First, a nonlinear mathematical model of a turbofan engine is established by adopting the component modeling approach. Then, numerous parameters of the turbofan engine components and their operating data are obtained by simulating various working conditions. Finally, based on the simulated data for the engine under these conditions, the model at the points during the dynamic process is linearized, such that a dynamic real-time linearized model of turbofan engine is obtained. Simulation results show that the proposed model can accurately depict the dynamic process of the turbofan engine and provide a valuable reference for designing the aero-engine control system and supporting intelligent analysis in IIoT.",https://ieeexplore.ieee.org/document/8856194/,IEEE Access,2019,ieeexplore
10.1109/ACCESS.2019.2963723,A Smart Collaborative Routing Protocol for Delay Sensitive Applications in Industrial IoT,IEEE,Journals,"In the industrial Internet of things (IIoT), there is always a strong demand for real-time information transfer. Especially when deploying wireless/wired hybrid networks in smart factories, the requirement for low delay interaction is more prominent. Although tree routing protocols have been successfully executed in simple networks, more challenges in transmission speed can be observed in the manufacturing broadband communication system. Motivated by the progresses in deep learning, a smart collaborative routing protocol with low delay and high reliability is proposed to accommodate mixed link scenarios. First, we establish a one-hop delay model to investigate the potential affects of Media Access Control (MAC) layer parameters, which supports the subsequent design. Second, forwarding, maintenance, and efficiency strategies are created to construct the basic functionalities for our routing protocol. Relevant procedures and key approaches are highlighted as well. Third, two sub-protocols are generated and the corresponding implementation steps are described. The experimental results demonstrate that the end-to-end delay can be effectively cut down through comprehensive improvements. Even more sensor nodes and larger network scale are involved, our proposed protocol can still illustrate the advantages comparing with existing solutions within IIoT.",https://ieeexplore.ieee.org/document/8949516/,IEEE Access,2020,ieeexplore
10.1109/ACCESS.2021.3121302,A Survey on Industry 4.0 for the Oil and Gas Industry: Upstream Sector,IEEE,Journals,"The market volatility in the oil and gas (O&amp;G) sector, the dwindling demand for oil due to the impact of COVID-19, and the push for alternative greener energy are driving the need for innovation and digitization in the O&amp;G industry. This has attracted research interest from academia and the industry in the application of industry 4.0 (I4.0) technologies in the O&amp;G sector. The application of some of these I4.0 technologies has been presented in the literature, but the domain still lacks a comprehensive survey of the application of I4.0 in the O&amp;G upstream sector. This paper investigates the state-of-the-art efforts directed toward I4.0 technologies in the O&amp;G upstream sector. To achieve this, first, an overview of the I4.0 is discussed followed by a systematic literature review from an integrative perspective for publications between 2012–2021 with 223 analyzed documents. The benefits and challenges of the adoption of I4.0 have been identified. Moreover, the paper adds value by proposing a framework for the implementation of I4.0 in the O&amp;G upstream sector. Finally, future directions and research opportunities such as framework, edge computing, quantum computing, communication technologies, standardization, and innovative areas related to the implementation of I4.0 in the upstream sector are presented. The findings from this review show that I4.0 technologies are currently being explored and deployed for various aspects of the upstream sector. However, some of the I4.0 technologies like additive manufacturing and virtual reality are least explored.",https://ieeexplore.ieee.org/document/9579415/,IEEE Access,2021,ieeexplore
10.1109/JIOT.2019.2940131,A Two-Stage Transfer Learning-Based Deep Learning Approach for Production Progress Prediction in IoT-Enabled Manufacturing,IEEE,Journals,"In make-to-order manufacturing enterprises, accurate production progress (PP) prediction is an important basis for dynamic production process optimization and on-time delivery of orders. The implementation of Internet of Things (IoT) makes it possible to take real-time production state as an important factor affecting PP. In the IoT-enabled workshop, a two-stage transfer learning-based prediction method using both historical production data and real-time state data is proposed to solve the problem of low-prediction accuracy and poor generalization performance caused by insufficient data of target order. The deep autoencoder (DAE) model with transfer learning is designed to extract the generalized features of target order in the first stage, which uses bootstrap sampling to avoid over fitting. The deep belief network (DBN) model with transfer learning is constructed to fit the nonlinear relation for PP prediction in the second stage. A real case from an IoT enabled machining workshop is taken to validate the performance of the proposed method over the other methods such as DBN, deep neural network.",https://ieeexplore.ieee.org/document/8827506/,IEEE Internet of Things Journal,Dec. 2019,ieeexplore
10.1109/TRO.2004.833801,A hybrid strategy to solve the forward kinematics problem in parallel manipulators,IEEE,Journals,"A parallel manipulator is a closed kinematic structure with the necessary rigidity to provide a high payload to self-weight ratio suitable for many applications in manufacturing, flight simulation systems, and medical robotics. Because of its closed structure, the kinematic control of such a mechanism is difficult. The inverse kinematics problem for such manipulators has a mathematical solution; however, the forward kinematics problem (FKP) is mathematically intractable. This work addresses the FKP and proposes a neural-network-based hybrid strategy that solves the problem to a desired level of accuracy, and can achieve the solution in real time. Two neural-network (NN) concepts using a modified form of multilayered perceptrons with backpropagation learning were implemented. The better performing concept was then combined with a standard Newton-Raphson numerical technique to yield a hybrid solution strategy. Simulation studies were carried out on a flight simulation syystem to check the validity o the approach. Accuracy of close to 0.01 mm and 0.01/spl deg/ in the position and orientation parameters was achieved in less than two iterations and 0.02 s of execution time for the proposed strategy.",https://ieeexplore.ieee.org/document/1391011/,IEEE Transactions on Robotics,Feb. 2005,ieeexplore
10.1109/TII.2019.2959021,An Integrated Histogram-Based Vision and Machine-Learning Classification Model for Industrial Emulsion Processing,IEEE,Journals,"Existing techniques in emulsion quality evaluation are found to be highly subjective, time-consuming, and prone to overprocessing. Other conventional droplet analysis techniques such as laser diffraction, which require dilution of samples, introduce an additional complexity to industrial processes. The possibility of developing a fully automated technique for droplet characterization during emulsification holds remarkable potential for overcoming the existing challenges. In this article, a histogram-based image segmentation technique detects droplets from emulsion micrographs. The evolution of droplet characteristics and their significance are studied by performing statistical analysis, and the significant characteristics are selected. The principal component analysis is applied to obtain a reduced set of uncorrelated components from the selected characteristics. The linear discriminant analysis classifies the micrographs into a set of quality categories called target, acceptable, marginal, and unacceptable. The model accuracy is validated using stratified five-fold cross-validation and is successful in classifying the micrographs obtained from two different manufacturing facilities with high accuracy up to 100%. The histogram-based technique is successful in detecting smaller droplets than previously reflected in the literature. The current approach is fully automated and is implemented as a soft-sensor, which supports its real-time deployment into an industrial environment. The entire approach has promising potential in the in-line prediction of emulsion quality leading to more efficient and sustainable manufacturing.",https://ieeexplore.ieee.org/document/8968624/,IEEE Transactions on Industrial Informatics,Sept. 2020,ieeexplore
10.1109/TASE.2006.886833,An Intelligent Online Monitoring and Diagnostic System for Manufacturing Automation,IEEE,Journals,"Condition monitoring and fault diagnosis in modern manufacturing automation is of great practical significance. It improves quality and productivity, and prevents damage to machinery. In general, this practice consists of two parts: 1)extracting appropriate features from sensor signals and 2)recognizing possible faulty patterns from the features. Through introducing the concept of marginal energy in signal processing, a new feature representation is developed in this paper. In order to cope with the complex manufacturing operations, three approaches are proposed to develop a feasible system for online applications. This paper develops intelligent learning algorithms using hidden Markov models and the newly developed support vector techniques to model manufacturing operations. The algorithms have been coded in modular architecture and hierarchical architecture for the recognition of multiple faulty conditions. We define a novel similarity measure criterion for the comparison of signal patterns which will be incorporated into a novel condition monitoring system. The sensor-based intelligent system has been implemented in stamping operations as an example. We demonstrate that the proposed method is substantially more effective than the previous approaches. Its unique features benefit various real-world manufacturing automation engineering, and it has great potential for shop floor applications.",https://ieeexplore.ieee.org/document/4358068/,IEEE Transactions on Automation Science and Engineering,Jan. 2008,ieeexplore
10.1109/TRA.2002.802211,An agent-based approach to reconfiguration of real-time distributed control systems,IEEE,Journals,"We describe a general approach for dynamic and intelligent reconfiguration of real-time distributed control systems that utilizes the IEC 61499 function block model. This work is central to the development of distributed intelligent control systems that are inherently adaptable and dynamically reconfigurable. The approach that is used takes advantage of distributed artificial intelligence at the planning and control levels to achieve significantly shorter up-front commissioning times as well as significantly more responsiveness to change. This approach is based on object-oriented and agent-based methods, and aims at overcoming the difficulties associated with managing real-time reconfiguration of an intelligent manufacturing system.",https://ieeexplore.ieee.org/document/1044357/,IEEE Transactions on Robotics and Automation,Aug. 2002,ieeexplore
10.1109/TIM.2021.3087826,Auto-Annotated Deep Segmentation for Surface Defect Detection,IEEE,Journals,"This article presents a deep learning scheme for automatic defect detection in material surfaces. The success of deep learning model training is generally determined by the number of representative training samples and the quality of the annotation. It is extremely tedious and tiresome to annotate defects pixel-by-pixel in an image to train a semantic network model for defect segmentation. In this study, we propose a two-stage deep learning scheme to tackle the pixel-wise defect detection in textured surfaces without manual annotation. The first stage of the deep learning scheme uses two cycle-consistent adversarial network (CycleGAN) models to automatically synthesize and annotate defect pixels in an image. The synthesized defect images and their corresponding annotated results from the CycleGAN models are then used as the input-output pairs for training the U-Net semantic network. The proposed scheme requires only a few real defect samples for the training and completely requires no manual annotation work. It is practical and computationally very efficient for the implementation in manufacturing. Experimental results show that the proposed deep learning scheme can be applied for defect detection in a variety of textured and patterned surfaces, and results in high detection accuracy.",https://ieeexplore.ieee.org/document/9449912/,IEEE Transactions on Instrumentation and Measurement,2021,ieeexplore
10.1109/TII.2018.2816971,Automatic Selection of Optimal Parameters Based on Simple Soft-Computing Methods: A Case Study of Micromilling Processes,IEEE,Journals,"Nowadays, the application of novel soft-computing methods to new industrial processes is often limited by the actual capacity of the industry to assimilate state-of-the-art computational methods. The selection of optimal parameters for efficient operation is very challenging in microscale manufacturing processes, because of intrinsic nonlinear behavior and reduced dimensions. In this paper, a decision-making system for selecting optimal parameters in micromilling operations is designed and implemented using simple and efficient soft-computing techniques. The procedure primarily consists of four steps: an experimental characterization; the modeling of cutting force and surface roughness by means of a multilayer perceptron; multiobjective optimization using the cross-entropy method, taking into account productivity and surface quality; and a decision-making procedure for selecting the most appropriate parameters using a fuzzy inference system. Finally, two different alloys for micromilling processes are considered, in order to evaluate the proposed system: a titanium-based alloy and a tungsten-copper alloy. The experimental study demonstrated the effectiveness of the proposed solution for automated decision-making, based on simple soft-computing methods, and its successful application to a real-life industrial challenge.",https://ieeexplore.ieee.org/document/8325494/,IEEE Transactions on Industrial Informatics,Feb. 2019,ieeexplore
10.1109/LRA.2017.2737046,Baxter's Homunculus: Virtual Reality Spaces for Teleoperation in Manufacturing,IEEE,Journals,"We demonstrate a low-cost telerobotic system that leverages commercial virtual reality (VR) technology and integrates it with existing robotics control infrastructure. The system runs on a commercial gaming engine using off-the-shelf VR hardware and can be deployed on multiple network architectures. The system is based on the homunculus model of mind wherein we embed the user in a VR control room. The control room allows for multiple sensor displays, and dynamic mapping between the user and robot. This dynamic mapping allows for selective engagement between the user and the robot. We compared our system with state-of-the-art automation algorithms and standard VR-based telepresence systems by performing a user study. The study showed that new users were faster and more accurate than the automation or a direct telepresence system. We also demonstrate that our system can be used for pick and place, assembly, and manufacturing tasks.",https://ieeexplore.ieee.org/document/8003431/,IEEE Robotics and Automation Letters,Jan. 2018,ieeexplore
10.1109/ACCESS.2020.2977846,Big Data Driven Edge-Cloud Collaboration Architecture for Cloud Manufacturing: A Software Defined Perspective,IEEE,Journals,"In the practice of cloud manufacturing, there still exist some major challenges, including: 1) cloud based big data analytics and decision-making cannot meet the requirements of many latency-sensitive applications on shop floors; 2) existing manufacturing systems lack enough reconfigurability, openness and evolvability to deal with shop-floor disturbances and market changes; and 3) big data from shop-floors and the Internet has not been effectively utilized to guide the optimization and upgrade of manufacturing systems. This paper proposes an open evolutionary architecture of the intelligent cloud manufacturing system with collaborative edge and cloud processing. Hierarchical gateways connecting and managing shop-floor things at the “edge” side are introduced to support latency-sensitive applications for real-time responses. Big data processed both at the gateways and in the cloud will be used to guide continuous improvement and evolution of edge-cloud systems for better performance. As software tools are becoming dominant as the “brain” of manufacturing control and decision-making, this paper also proposes a new mode - “AI-Mfg-Ops” (AI enabled Manufacturing Operations) with a supporting software defined framework, which can promote fast operation and upgrading of cloud manufacturing systems with smart monitoring-analysis-planning-execution in a closed loop. This research can contribute to the rapid response and efficient operation of cloud manufacturing systems.",https://ieeexplore.ieee.org/document/9020166/,IEEE Access,2020,ieeexplore
10.1109/ACCESS.2021.3106797,CNC Machine Tool Fault Diagnosis Integrated Rescheduling Approach Supported by Digital Twin-Driven Interaction and Cooperation Framework,IEEE,Journals,"The problems of CNC machine tool (CNCMT) fault diagnosis and production rescheduling have attracted continuous attention because of their great significance to the manufacturing industry. Digital twin is a supporting technology for achieving smart manufacturing and provides a new paradigm for solving these problems. This paper explores a digital twin-driven interaction and cooperation framework and proposes the architecture and implementation mechanism to enable the sharing of data, knowledge, and resource, to realize the fusion of physical space and cyber space, and to improve the accuracy of fault diagnosis. Under this framework, aiming at the influence of CNCMT failure on the initial production planning, a self-adaptation rescheduling method based on Monte Carlo Tree Search (MCTS) algorithm is proposed to provide support for developing more efficient production planning. Finally, the effectiveness of the proposed framework is validated by experimental study. The framework and integrated rescheduling approach can provide guidance for enterprises in implementing CNCMT maintenance and production scheduling to meet high accuracy and reliability requirements.",https://ieeexplore.ieee.org/document/9520390/,IEEE Access,2021,ieeexplore
10.1109/TASE.2020.3010536,Condition-Driven Data Analytics and Monitoring for Wide-Range Nonstationary and Transient Continuous Processes,IEEE,Journals,"Frequent and wide changes in operation conditions are quite common in real process industry, resulting in typical wide-range nonstationary and transient characteristics along time direction. The considerable challenge is, thus, how to solve the conflict between the learning model accuracy and change complexity for analysis and monitoring of nonstationary and transient continuous processes. In this work, a novel condition-driven data analytics method is developed to handle this problem. A condition-driven data reorganization strategy is designed which can neatly restore the time-wise nonstationary and transient process into different condition slices, revealing similar process characteristics within the same condition slice. Process analytics can then be conducted for the new analysis unit. On the one hand, coarse-grained automatic condition-mode division is implemented with slow feature analysis to track the changing operation characteristics along condition dimension. On the other hand, fine-grained distribution evaluation is performed for each condition mode with Gaussian mixture model. Bayesian inference-based distance (BID) monitoring indices are defined which can clearly indicate the fault effects and distinguish different operation scenarios with meaningful physical interpretation. A case study on a real industrial process shows the feasibility of the proposed method which, thus, can be generalized to other continuous processes with typical wide-range nonstationary and transient characteristics along time direction. <italic>Note to Practitioners</italic>—Industrial processes in general have nonstationary characteristics which are ubiquitous in real world data, often reflected by a time-variant mean, a time-variant autocovariance, or both resulting from various factors. The focus of this study is to develop a universal analytics and monitoring method for wide-range nonstationary and transient continuous processes. Condition-driven concept takes the place of time-driven thought. For the first time, it is recognized that there are similar process characteristics within the same condition slice and changes in the process correlations may relate to its condition modes. Besides, the proposed method can provide enhanced physical interpretation for the monitoring results with concurrent analysis of the static and dynamic information which carry different information, analogous to the concepts of “position” and “velocity” in physics, respectively. The static information can tell the current operation condition, while the dynamic information can clarify whether the process status is switching between different steady states. It is noted that the condition-driven concept is universal and can be extended to other applications for industrial manufacturing applications.",https://ieeexplore.ieee.org/document/9158352/,IEEE Transactions on Automation Science and Engineering,Oct. 2021,ieeexplore
10.1109/TII.2021.3067915,Diagnosis of Interturn Short-Circuit Faults in Permanent Magnet Synchronous Motors Based on Few-Shot Learning Under a Federated Learning Framework,IEEE,Journals,"A large amount of labeled data are important to enhance the performance of deep-learning-based methods in the area of fault diagnosis. Because it is difficult to obtain high-quality samples in real industrial applications, federated learning is an effective framework for solving the problem of sparse samples by using the distributed data. Its global model is updated by the local client without sharing data at each round. Considering computing resources and communication loss of multiple clients, an efficient method based on stacked sparse autoencoders (SSAEs) and Siamese networks is proposed to detect interturn short-circuit (ITSC) faults in permanent magnet synchronous motors. In this article, to achieve an accurate ITSC fault detection, an SSAE was employed to extract sparse features in a limited number of samples, and Siamese networks were used to determine the similarity between the given samples. The problem of fault diagnosis is transformed into a classification problem under few-shot learning. Furthermore, the proposed method is trained successfully in the frameworks of centralized learning and decentralized structure. The experimental results indicate that the proposed method achieved high fault diagnosis accuracy. Moreover, it is suitable for deployment in smart manufacturing systems.",https://ieeexplore.ieee.org/document/9384245/,IEEE Transactions on Industrial Informatics,Dec. 2021,ieeexplore
10.1109/JPROC.2020.2998530,"Digital Twin in the IoT Context: A Survey on Technical Features, Scenarios, and Architectural Models",IEEE,Journals,"Digital twin (DT) is an emerging concept that is gaining attention in various industries. It refers to the ability to clone a physical object (PO) into a software counterpart. The softwarized object, termed logical object, reflects all the important properties and characteristics of the original object within a specific application context. To fully determine the expected properties of the DT, this article surveys the state-of-the-art starting from the original definition within the manufacturing industry. It takes into account related proposals emerging in other fields, namely augmented and virtual reality (e.g., avatars), multiagent systems, and virtualization. This survey thereby allows for the identification of an extensive set of DT features that point to the “softwarization” of POs. To properly consolidate a shared DT definition, a set of foundational properties is identified and proposed as a common ground outlining the essential characteristics (must-haves) of a DT. Once the DT definition has been consolidated, its technical and business value is discussed in terms of applicability and opportunities. Four application scenarios illustrate how the DT concept can be used and how some industries are applying it. The scenarios also lead to a generic DT architectural model. This analysis is then complemented by the identification of software architecture models and guidelines in order to present a general functional framework for the DT. This article, eventually, analyses a set of possible evolution paths for the DT considering its possible usage as a major enabler for the softwarization process.",https://ieeexplore.ieee.org/document/9120192/,Proceedings of the IEEE,Oct. 2020,ieeexplore
10.1109/ACCESS.2021.3079447,ECT-LSTM-RNN: An Electrical Capacitance Tomography Model-Based Long Short-Term Memory Recurrent Neural Networks for Conductive Materials,IEEE,Journals,"Image reconstruction for industrial applications based on Electrical Capacitance Tomography (ECT) has been broadly applied. The goal of image reconstruction based ECT is to locate the distribution of permittivity for the dielectric substances along the cross-section based on the collected capacitance data. In the ECT-based image reconstruction process: (1) the relationship between capacitance measurements and permittivity distribution is nonlinear, (2) the capacitance measurements collected during image reconstruction are inadequate due to the limited number of electrodes, and (3) the reconstruction process is subject to noise leading to an ill-posed problem. Thence, constructing an accurate algorithm for real images is critical to overcoming such restrictions. This paper presents novel image reconstruction methods using Deep Learning for solving the forward and inverse problems of the ECT system for generating high-quality images of conductive materials in the Lost Foam Casting (LFC) process. Here, Long Short-Term Memory Recurrent Neural Network (LSTM-RNN) models were implemented to predict the distribution of metal filling for the LFC process-based ECT. The recurrent connection and the gating mechanism of the LSTM is capable of extracting the contextual information that is repeatedly passing through the neural network while filtering out the noise caused by adverse factors. Experimental results showed that the presented ECT-LSTM-RNN model is highly reliable for industrial applications and can be utilized for other manufacturing processes.",https://ieeexplore.ieee.org/document/9429218/,IEEE Access,2021,ieeexplore
10.1109/TII.2019.2949347,Edge Coordinated Query Configuration for Low-Latency and Accurate Video Analytics,IEEE,Journals,"To develop smart city and intelligent manufacturing, video cameras are being increasingly deployed. In order to achieve fast and accurate response to live video queries (e.g., license plate recording and object tracking), the real-time high-volume video streams should be delivered and analyzed efficiently. In this article, we introduce an end-edge-cloud coordination framework for low-latency and accurate live video analytics. Considering the locality of video queries, edge platform is designated as the system coordinator. It accepts live video queries and configures the related end cameras to generate video frames that meet quality requirements. By taking into account the latency constraint, edge computing resources are subtly distributed to process the live video frames from different sources such that the analytic accuracy of the accepted video queries can be maximized. Since the amount of required edge computing resource and video quality to accurately address different video queries are unknown in advance, we propose an online video quality and computing resource configuration algorithm to gradually learn the optimal configuration strategy. Extensive simulation results show that as compared to other benchmarks, the proposed configuration algorithm can effectively improve the analytic accuracy, while providing low-latency response.",https://ieeexplore.ieee.org/document/8882341/,IEEE Transactions on Industrial Informatics,July 2020,ieeexplore
10.1109/TII.2020.3034627,Guest Editorial: Special Section on Advanced Signal Processing and AI Technologies for Industrial Big Data,IEEE,Journals,The papers in this special section focus on advanced signal processing and artificial intelligence (AI) technologies for industrial Big Data (IBD) powered by Industry 4.0. Modern industry has evolved from the traditional manufacturing industry to digital and intelligent industry. Huge amount of complex real-time data are generated from the thousands of industrial sensors in physical and man-made environments. Industrial big data (IBD) afford us an unprecedented opportunity to obtain an in-depth understanding of Internet of Things and facilitate data-driven approaches for industrial optimization and scheduling. The papers in this section collect the latest ideas and research on advanced signal processing and artificial intelligence (AI) technologies for IBD.,https://ieeexplore.ieee.org/document/9361683/,IEEE Transactions on Industrial Informatics,May 2021,ieeexplore
10.1109/TNNLS.2011.2179309,Hybrid Neural Prediction and Optimized Adjustment for Coke Oven Gas System in Steel Industry,IEEE,Journals,"An energy system is the one of most important parts of the steel industry, and its reasonable operation exhibits a critical impact on manufacturing cost, energy security, and natural environment. With respect to the operation optimization problem for coke oven gas, a two-phase data-driven based forecasting and optimized adjusting method is proposed, where a Gaussian process-based echo states network is established to predict the gas real-time flow and the gasholder level in the prediction phase. Then, using the predicted gas flow and gasholder level, we develop a certain heuristic to quantify the user's optimal gas adjustment. The proposed operation measure has been verified to be effective by experimenting with the real-world on-line energy data sets coming from Shanghai Baosteel Corporation, Ltd., China. At present, the scheduling software developed with the proposed model and ensuing algorithms have been applied to the production practice of Baosteel. The application effects indicate that the software system can largely improve the real-time prediction accuracy of the gas units and provide with the optimized gas balance direction for the energy optimization.",https://ieeexplore.ieee.org/document/6126048/,IEEE Transactions on Neural Networks and Learning Systems,March 2012,ieeexplore
10.1109/70.63270,Hybrid hierarchical scheduling and control systems in manufacturing,IEEE,Journals,"Some experiments on the integration of algorithmic techniques with knowledge-based ones are discussed. Two case studies are presented: an FMS cell and a press shop. It was found that the algorithmic procedures developed for production scheduling resulted in limiting the ability to cope with the complexity of the real manufacturing world. The scheduling problem, seen as a constraint satisfaction problem, can be approached with rule-based techniques. Nevertheless, algorithmic techniques are found to be valuable for their efficiency and ability to deal with aggregated data. This ability is fundamental for an efficient implementation of hierarchical control systems in general and in the manufacturing context in particular. This suggests that the integration of rule-based techniques with algorithmic ones can increase the efficiency of searching in the space of possible solutions. The ability to deal with aggregated data can have little value when detailed real-time operation scheduling is needed. In this case, simple dispatching rules are often used, and sophisticated operations research methods are not used. In such a dynamic situation, a purely-rule based approach may be more suitable.&lt;<ETX>&gt;</ETX>",https://ieeexplore.ieee.org/document/63270/,IEEE Transactions on Robotics and Automation,Dec. 1990,ieeexplore
10.1109/ACCESS.2021.3090143,IEEE Access Special Section Editorial: Edge Computing and Networking for Ubiquitous AI,IEEE,Journals,"With its rapid development recently, edge computing with processing, storage, and networking capabilities has become an important solution to break through the bottleneck of emerging technology development by virtue of its advantages in reducing data transmission, decreasing service latency, and easing cloud computing pressure. Among several application scenarios such as network optimization, intelligent manufacturing, and real-time video analytics, edge computing can work with artificial intelligence (AI) synergistically. Therefore, many researchers are investigating edge computing with AI from two perspectives. One is that the emergence of AI solves the optimization problem of edge computing. For example, when network devices need to process some complex and fuzzy information, the powerful learning and reasoning ability of AI can help to extract valuable information from the massive data and realize intelligent management. Another is how edge computing supports AI in a networking environment. For example, AI training and inference can be efficiently enabled by a multitude of computing resources from edge computing. Therefore, edge computing and AI are mutually beneficial in networking.",https://ieeexplore.ieee.org/document/9467255/,IEEE Access,2021,ieeexplore
10.1109/ACCESS.2018.2809681,Industrial Big Data Analytics for Prediction of Remaining Useful Life Based on Deep Learning,IEEE,Journals,"Due to the recent development of cyber-physical systems, big data, cloud computing, and industrial wireless networks, a new era of industrial big data is introduced. Deep learning, which brought a revolutionary change in computer vision, natural language processing, and a variety of other applications, has significant potential for solutions providing in sophisticated industrial applications. In this paper, a concept of device electrocardiogram (DECG) is presented, and an algorithm based on deep denoising autoencoder (DDA) and regression operation is proposed for the prediction of the remaining useful life of industrial equipment. First, the concept of electrocardiogram is explained. Then, a problem statement based on manufacturing scenario is presented. Subsequently, the architecture of the proposed algorithm called integrated DDA and the algorithm workflow are provided. Moreover, DECG is compared with traditional factory information system, and the feasibility and effectiveness of the proposed algorithm are validated experimentally. The proposed concept and algorithm combine typical industrial scenario and advance artificial intelligence, which has great potential to accelerate the implementation of industry 4.0.",https://ieeexplore.ieee.org/document/8302913/,IEEE Access,2018,ieeexplore
10.1109/TADVP.2004.828824,Intelligent SOP manufacturing,IEEE,Journals,"Microsystems packaging is fundamentally dependent on the manufacture of microelectronic, photonic, radio frequency (RF), and MEMS devices. The system-on-package (SOP) approach has been identified as a key strategy for integrating these strategic packaging technologies. Because of rising costs, the challenge before SOP manufacturers is to offset capital investment with greater automation and technological innovation in the fabrication process. To reduce manufacturing cost, several important subtasks have emerged, including increasing fabrication yield, reducing product cycle time, maintaining consistent levels of product quality and performance, and improving the reliability of processing equipment. Because of the large number of steps involved, maintaining product quality in an SOP manufacturing facility requires the control of hundreds of process variables. The interdependent issues of high yield, high quality, and low cycle time are addressed by the ongoing development of several critical capabilities in state-of-the-art computer-integrated manufacturing (CIM) systems: in situ process monitoring, process/equipment modeling, real-time process control, and equipment diagnosis. Recently, the use of computational intelligence in various manufacturing applications has increased, and the SOP manufacturing arena is no exception to this trend. Artificial neural networks, genetic algorithms (GAs), and other techniques have emerged as powerful tools for assisting CIM systems in performing various process monitoring, modeling, and control functions. This paper reviews current research in these areas, as well as the potential for deployment of these capabilities in state-of-the-art SOP manufacturing facilities.",https://ieeexplore.ieee.org/document/1331523/,IEEE Transactions on Advanced Packaging,May 2004,ieeexplore
10.1109/TASE.2020.3044620,Knowledge-Based Automation for Smart Manufacturing Systems,IEEE,Journals,"Smart manufacturing is targeted as the next generation of manufacturing by many national and international strategic development. The increasingly rich production data, the integration and extensive application of information technology, and the intelligent data processing and system modeling methods have collectively enabled smart manufacturing. Building upon them, manufacturing system modeling, knowledge acquisition, design, and real-time control are the key components [item 1) in the Appendix], [item 2) in the Appendix]. It is still one of the really huge challenges to gather data, transform them into information, and derive knowledge out of this information, especially given the requirement of knowledge that can be trusted as manufacturing systems may harm humans and the environment if they come to the wrong conclusion. Despite the learning and derivation of knowledge, it could be modeled beforehand and taken, for example, as an environmental model for online decisions like in deliberative agent-based systems. Nevertheless, for decisions during operation, real-time requirements, dependability, and security issues are to be guaranteed. Finally, for acceptance and trust, humans need to “understand” the reasons behind automated decisions; therefore, explainability or at least a white box description is an issue of such knowledge-based systems.",https://ieeexplore.ieee.org/document/9316429/,IEEE Transactions on Automation Science and Engineering,Jan. 2021,ieeexplore
10.1109/TASE.2017.2783342,MASD: A Multimodal Assembly Skill Decoding System for Robot Programming by Demonstration,IEEE,Journals,"Programming by demonstration (PBD) transforms the robot programming from the code level to automated interface between robot and human, promoting the flexibility of robotized automation. In this paper, we focus on programming the industrial robot for assembly tasks by parsing the human demonstration into a series of assembly skills and compiling the skill to the robot executables. To achieve this goal, an identification system using multimodal information to recognize the assembly skill, called MASD, is proposed including: 1) an initial learning stage using a hierarchical model to recognize the action by considering the features from action-object effect, gesture, and trajectory and 2) a retrospective thinking stage using a segmentation method to cut the continuous demonstrations into multiple assembly skills optimally. Using MASD, the demonstration of assembly tasks can be explained with high accuracy in real time, driving a hypothesis that a PBD system on the top of MASD can be extended to more realistic assembly tasks beyond pure positional moving and picking. In experiments, the skill identification module is used to recognize the five kinds of assembly skills in demonstrations of both single and multiple assembly skills, and outperforms the comparative action identification methods. Besides integrated with the MASD, the PBD system can generate the program based on the demonstration and successfully enable an ABB industrial robotic arm simulator to assemble a flashlight and a switch, verifying the initial hypothesis. Note to Practitioners-In the conventional robotized automation, the key role of the robot mainly owes to its capacity for repeating a wide variety of tasks with high speed and accuracy in long term, with a cost of days to months of programming for deployment. On the other hand, the new trend of customization brings the new characteristics: production in short cycle and small volume. This irreversible momentum urges the robot to switch from task to task efficiently. The biggest bottleneck here is the tedious programming, which also has high prerequisites for most practitioners in manufacturing. This situation motivates the development of a PBD system that can understand the assembly skills performed by the human experts in the demonstration and accordingly generate the program for robot's execution of the taught task. In this paper, we present a skill decoding system to parse the observational raw demonstration into symbolic sequences, which is the crucial bridge to enable the automatic programming. The system achieves high performance in recognition and is tailored for the PBD in assembly tasks by considering both advantages and disadvantages in the background of assembly, such as controllable environment and limited computational resources. It is particularly useful for assembly tasks with modularized actions based on a set of standard parts. At the perspective of industrial application, the PBD upon the proposed system is a promising solution to improve the flexibility of manufacture, which is expected to be true in midterm but an important step toward this goal.",https://ieeexplore.ieee.org/document/8263146/,IEEE Transactions on Automation Science and Engineering,Oct. 2018,ieeexplore
10.1109/ACCESS.2017.2754507,Model-Based Development of Knowledge-Driven Self-Reconfigurable Machine Control Systems,IEEE,Journals,"To accommodate the trend toward mass customization launched by intelligent manufacturing in the era of Industry 4.0, this paper proposes the combination of model-driven engineering and knowledgedriven engineering during the development process of self-reconfigurable machine control systems. The complete tool chain for model development, execution, and reconfiguration is established. For the design phase, a machine-control-domain-specific modeling language and the supporting design environment are developed. With regard to the execution stage, a runtime framework compliant with the IEC 61499 standard is proposed. On the ground of the modeling environment and the reconfigurable run-time framework, a self-adaptive control module is developed to establish the close-loop self-reconfiguration infrastructure. The ontological representation of knowledge base toward this end is described, along with extendable SQWRL rules specified to automatically initiate the reconfiguration process in the cases of external user demands and internal faults. A prototype motion control kernel in the low-level layer of machine control system architecture is developed with the proposed modeling language and is then deployed to the runtime framework. Two case studies on self-reconfiguration of the proof-of-concept motion control kernel are demonstrated, which prove the feasibility of our proposal.",https://ieeexplore.ieee.org/document/8047091/,IEEE Access,2017,ieeexplore
10.1109/ACCESS.2021.3125519,Multi-Objective Optimization of Electric Arc Furnace Using the Non-Dominated Sorting Genetic Algorithm II,IEEE,Journals,"Combining classical technologies with modern intelligent algorithms, this paper introduces a new approach for the optimisation and modelling of the EAF-based steel-making process based on a multi-objective optimisation using evolutionary computing and machine learning. Using a large amount of real-world historical data containing 6423 consecutive EAF heats collected from a melt shop in an established steel plant this work not only creates machine learning models for both EAF and ladle furnaces but also simultaneously minimises the total scrap cost and EAF energy consumption per ton of scrap. In the modelling process, several algorithms are tested, tuned, evaluated and compared before selecting Gradient Boosting as the best option to model the data analysed. A similar approach is followed for the selection of the multi-objective optimisation algorithm. For this task, six techniques are tested and compared based on the hypervolume performance indicator to just then select the Non-dominated Sorting Genetic <xref ref-type=""algorithm"" rid=""alg2"">Algorithm II</xref> (<italic>NSGA-II</italic>) as the best option. Given this applied research focus on a real manufacturing process, real-world constraints and variables such as individual scrap price, scrap availability, tap additives and ambient temperature are used in the models developed here. A comparison with an equivalent EAF model from the literature showed a 13% improvement using the mean absolute error in the EAF energy usage prediction as a comparative metric. The multi-objective optimisation resulted in reductions in the energy consumption costs that ranged from 1.87% up to 8.20% among different steel grades and scrap cost reductions ranging from 1.15% up to 5.2%. The machine learning models and the optimiser were ultimately deployed with a graphical user interface allowing the melt-shop staff members to make informed decisions while controlling the EAF operation.",https://ieeexplore.ieee.org/document/9600818/,IEEE Access,2021,ieeexplore
10.1109/TMECH.2014.2366033,Output Regulation of Nonlinear Systems With Application to Roll-to-Roll Manufacturing Systems,IEEE,Journals,"This paper deals with the problem of synthesizing feedforward control to aid the regulation of output of a nonlinear system in the presence of partially known exogenous inputs. The problem appears in many engineering applications including Roll-to-Roll (R2R) manufacturing systems. Currently known methods for this problem either require the solution of a constrained partial differential equation or the preview information of the signal to be tracked. The novelty of this paper lies in synthesizing feedforward control as the solution of a system of differential-algebraic equations, which is considerably less complex and suitable for practical implementation. In this paper, we consider the problem of regulating the output while rejecting the disturbances and apply it to R2R manufacturing systems. The problem of tracking reference signals can also be handled with the suggested technique. We assume that the disturbance signal is the output of a known exogenous system with unknown initial conditions. A parameter identification scheme to estimate the unknown initial conditions is developed. The proposed technique is successfully applied to control of web tension in a large R2R machine which mimics most of the features of industrial R2R machines and contains real-time hardware and software that is used in industrial practice. Extensive experiments were conducted to evaluate the proposed scheme for web tension control under various experimental conditions, including different web speeds and materials. We will present and discuss the representative experimental results with the proposed technique and provide a comparison with an industrial PI control scheme to highlight the benefits of using the proposed scheme.",https://ieeexplore.ieee.org/document/6963413/,IEEE/ASME Transactions on Mechatronics,June 2015,ieeexplore
10.1109/TII.2019.2940099,Performance Supervised Fault Detection Schemes for Industrial Feedback Control Systems and their Data-Driven Implementation,IEEE,Journals,"This article addresses performance supervised fault detection (PSFD) issues for industrial feedback control systems based on performance degradation prediction. To be specific, three performance indicators are first introduced based on Bellman equation to predict system performance degradations for industrial processes with the aid of machine learning techniques. Based on them, three PSFD schemes are proposed by embedding the performance indicators as supervising information. In this context, the data-driven implementation of PSFD schemes are investigated for linear systems with unmeasurable state variables. A case study on rolling mill process, a typical benchmark in the steel manufacturing processes, is given at the end of this article to illustrate the applications of the proposed fault detection schemes.",https://ieeexplore.ieee.org/document/8827307/,IEEE Transactions on Industrial Informatics,April 2020,ieeexplore
10.1109/JIOT.2018.2849324,RF-PUF: Enhancing IoT Security Through Authentication of Wireless Nodes Using <italic>In-Situ</italic> Machine Learning,IEEE,Journals,"Traditional authentication in radio-frequency (RF) systems enable secure data communication within a network through techniques such as digital signatures and hash-based message authentication codes (HMAC), which suffer from key-recovery attacks. State-of-the-art Internet of Things networks such as Nest also use open authentication (OAuth 2.0) protocols that are vulnerable to cross-site-recovery forgery (CSRF), which shows that these techniques may not prevent an adversary from copying or modeling the secret IDs or encryption keys using invasive, side channel, learning or software attacks. Physical unclonable functions (PUFs), on the other hand, can exploit manufacturing process variations to uniquely identify silicon chips which makes a PUF-based system extremely robust and secure at low cost, as it is practically impossible to replicate the same silicon characteristics across dies. Taking inspiration from human communication, which utilizes inherent variations in the voice signatures to identify a certain speaker, we present RF-PUF: a deep neural network-based framework that allows real-time authentication of wireless nodes, using the effects of inherent process variation on RF properties of the wireless transmitters (Tx), detected through in-situ machine learning at the receiver (Rx) end. The proposed method utilizes the already-existing asymmetric RF communication framework and does not require any additional circuitry for PUF generation or feature extraction. The burden of device identification is completely shifted to the gateway Rx, similar to the operation of a human listener's brain. Simulation results involving the process variations in a standard 65-nm technology node, and features such as local oscillator offset and I-Q imbalance detected with a neural network having 50 neurons in the hidden layer indicate that the framework can distinguish up to 4800 Tx(s) with an accuracy of 99.9% [≈99% for 10000 Tx(s)] under varying channel conditions, and without the need for traditional preambles. The proposed scheme can be used as a stand-alone security feature, or as a part of traditional multifactor authentication.",https://ieeexplore.ieee.org/document/8390918/,IEEE Internet of Things Journal,Feb. 2019,ieeexplore
10.1109/ACCESS.2020.3046784,Reinforcement Learning With Composite Rewards for Production Scheduling in a Smart Factory,IEEE,Journals,"Rapid advances of sensing and cloud technologies transform the manufacturing system into a data-rich environment and make production scheduling increasingly complex. Traditional offline scheduling methods are limited in the ability to handle low-volume-high-mix workorders with diverse design specifications. Simulation-based methods show the promise for distributed scheduling of manufacturing jobs but are mostly implemented with historical data and empirical rules in a static manner. Recently, artificial intelligence (AI) algorithms fuel increasing interests to solve dynamic scheduling problems in the manufacturing setting. However, it's difficult to utilize high-dimensional data for production scheduling while considering multiple practical objectives for smart manufacturing (e.g., minimize the makespan, reduce production costs, balance workloads). Therefore, this paper presents a new AI scheduler with composite reward functions for data-driven dynamic scheduling of manufacturing jobs under uncertainty in a smart factory. Internet-enabled sensor networks are deployed in the smart factory to track real-time statuses of workorders, machines, and material handling systems. A novel manufacturing value network is developed to take high-dimensional data as the input and then learn the state-action values for real-time decision making. Based on reinforcement learning (RL), composite rewards help the AI scheduler learn efficiently to achieve multiple objectives for production scheduling in real time. The proposed methodology is evaluated and validated with experimental studies in a smart manufacturing setting. Experimental results show that the new AI scheduler not only improves the multi-objective performance metrics in the production scheduling problem but also effectively copes with unexpected events (e.g., urgent workorders, machine failures) in manufacturing systems.",https://ieeexplore.ieee.org/document/9305707/,IEEE Access,2021,ieeexplore
10.1109/TC.2020.3033627,Revealing DRAM Operating GuardBands Through Workload-Aware Error Predictive Modeling,IEEE,Journals,"Improving the energy efficiency of DRAMs becomes very challenging due to the growing demand for storage capacity and failures induced by the manufacturing process. To protect against failures, vendors adopt conservative margins in the refresh period and supply voltage. Previously, it was shown that these margins are too pessimistic and will become impractical due to high-power costs, especially in future DRAM technologies. In this article, we present a new technique for automatic scaling the DRAM refresh period under reduced supply voltage that minimizes the probability of failures. The main idea behind the proposed approach is that DRAM error behavior is workload-dependent and can be predicted based on particular program inherent features. We use a Machine Learning (ML) method to build a workload-aware DRAM error behavior model based on the program features which we extract from real workloads during our DRAM error characterization campaign. With such a model, we identify the marginal value of the DRAM refresh period under relaxed voltage for each DRAM module of a server that enable us to reduce the DRAM power. We implement a temperature-driven OS governor which automatically sets the module-specific marginal DRAM parameters discovered by the ML model. Our governor reduces the DRAM power by 24 percent on average while minimizing the probability of failures. Unlike previous studies, our technique: i) does not require intrusive changes to hardware; ii) is implemented on a real server; iii) uses a mechanism that prevents any abnormal DRAM error behavior; and iv) can be easily deployed in data centers.",https://ieeexplore.ieee.org/document/9239888/,IEEE Transactions on Computers,1 Nov. 2021,ieeexplore
10.1109/ACCESS.2020.3045563,SDN Enhanced Resource Orchestration of Containerized Edge Applications for Industrial IoT,IEEE,Journals,"With the rise of the Industrial Internet of Things (IIoT), there is an intense pressure on resource and performance optimization leveraging on existing technologies, such as Software Defined Networking (SDN), edge computing, and container orchestration. Industry 4.0 emphasizes the importance of lean and efficient operations for sustainable manufacturing. Achieving this goal would require engineers to consider all layers of the system, from hardware to software, and optimizing for resource efficiency at all levels. This emphasizes the need for container-based virtualization tools such as Docker and Kubernetes, offering Platform as a Service (PaaS), while simultaneously leveraging on edge technologies to reduce related latencies. For network management, SDN is poised to offer a cost-effective and dynamic scalability solution by customizing packet handling for various edge applications and services. In this paper, we investigate the energy and latency trade-offs involved in combining these technologies for industrial applications. As a use case, we emulate a 3D-drone-based monitoring system aimed at providing real-time visual monitoring of industrial automation. We compare a native implementation to a containerized implementation where video processing is orchestrated while streaming is handled by an external UE representing the IIoT device. We compare these two scenarios for energy utilization, latency, and responsiveness. Our test results show that only roughly 16 percent of the total power consumption happens on the mobile node when orchestrated. Virtualization adds up about 4.5 percent of the total power consumption while the latency difference between the two approaches becomes negligible after the streaming session is initialized.",https://ieeexplore.ieee.org/document/9296769/,IEEE Access,2020,ieeexplore
10.1109/ACCESS.2016.2619898,Segmentation of Factories on Electricity Consumption Behaviors Using Load Profile Data,IEEE,Journals,"In recent years, the new achievements in the field of technology and data science allowed to gather detailed and well-structured information about electricity consumption behaviors of industrial enterprises. Such type of information can find numerous applications in the power distribution industry. The utilities often use the data from contracts to assign each industrial customer a class label according to this type defined in predetermined industry segmentation. Such type of fixed-chart segmentation is not able to satisfy the needs of modern enterprises for the flexible and dynamic determination of production modes. In this paper, we address this problem by proposing a new method for the segmentation of various types of factories based on their electricity consumption patterns represented in load profile data. It exploits the evolution-based characteristics of smart meter data of multiple types of factories to remove irrelevant features. We use data visualization to estimate the number of clusters and apply the well-known k-means algorithm on filtered data to generate segmentation. Experimental results on real load profile data collected with smart meters from manufacturing industries in Guangdong province of China have shown that the new clustering approach produced the meaningful segmentation of factories that reflect production operations.",https://ieeexplore.ieee.org/document/7752771/,IEEE Access,2016,ieeexplore
10.1109/TSMCC.2013.2265234,Self-Organized P2P Approach to Manufacturing Service Discovery for Cross-Enterprise Collaboration,IEEE,Journals,"The combination of service-oriented architecture (SOA) and peer-to-peer (P2P) architecture plays a promising role in distributed manufacturing environments in that the peer service can be used to facilitate the integration and discovery of distributed manufacturing resources and achieve communication and collaboration across distributed virtual enterprises. However, the large size, dynamic nature, and heterogeneous expression of distributed manufacturing resources bring forth a serious challenge in scalability and efficiency. This paper presents a self-organized P2P framework that supports scalable and efficient manufacturing service (MS) discovery for cross-enterprise collaboration by forming and maintaining autonomous enterprise peer groups (PG). Each enterprise exhibits as a peer that provides some sharable MSs that are represented comprehensively and formally with a generalized ontology. Each enterprise PG dynamically clusters a set of enterprise peers offering semantically similar MSs, and elects the most reputed peer through multicriteria trust evaluation as its core (i.e., super peer, SP). Then, a MS request can be first routed to the suitable SP and further to its leaf peer in a systematic way, thus supporting efficient service discovery. A prototype system is implemented on JXTA for real application and validated through an experimental case study.",https://ieeexplore.ieee.org/document/6600968/,"IEEE Transactions on Systems, Man, and Cybernetics: Systems",March 2014,ieeexplore
10.1109/ACCESS.2018.2809439,The Collaborative System Workflow Management of Industrial Design Based on Hierarchical Colored Petri-Net,IEEE,Journals,"Industrial design is a requisite and incontrovertible front-end design for Industrie 4.0 to realize personalization, agility, intelligence, and standardization, so it has very important practical significance to achieve industrial design collaborative workflow flexible management. This paper proposed a hierarchical colored Petri net (HCPN) for modeling the industrial design collaborative system workflow, which aims to provide a valid workflow model for the process management of the industrial design collaborative system. First, it is built the top-level workflow CPN model from product planning, markets research, product design, and product evaluation to prototype production, then the model is extended layer by layer to build the subnet model by the model refinement method. In the model, the activity of the workflow and the activity execution results are represented by the transitions (T) and places (S), respectively. The running state of the whole workflow is represented by the distribution of the token in the places. The token' color sets expresses the real-time discrete state. By use of the function of token coloring places (S) and the token' color sets of HCPN, this paper extracts the common characteristics of process knowledge, including markets research results, design ideas, conceptual sketches, design schemes, human resources, and tacit creative knowledge ,then uses the what, when, where, who, and how method to build knowledge information units for specific design event. Thus, realizes the industrial design process knowledge base construction and knowledge acquisition, which provides the necessary information and knowledge resources for the design reuse, flexibility management, and intelligent manufacturing of subsequent products.",https://ieeexplore.ieee.org/document/8302489/,IEEE Access,2018,ieeexplore
,The application of simulation technique in whole CIMS life cycle,BIAI,Journals,"The life cycle of CIMS (Computer Integrated Manufacturing System) includes four phases: requirement analyzing, designing, implementation and running. For reducing the risk of investment and achieving better economic results the simulation technique is needed in the above four phases of CIMS life cycle. Under the support of China 863 / CIMS plan a series of simulation projects are established. Some of them are finished with succeed and have been used in application. In this paper four simulation projects are introduced.(1) The Integrated Manufacturing Simulation Software (IMSS). It is an integrated platform, based on the discrete event simulation principle. It can be used to analyze and design CIMS, especially FMS, and evaluate the daily production plan.(2) The Advanced Hierarchical Control System Emulator (AHCSE), a software system, based on the finite state machine principle. It can be used to analyze and design of CIMS hierarchical control system, and check expanded system performance before expanding.(3) The Factory Scheduling Environment (FASE), a software system based on the discrete event simulation principle and artificial intelligence technology. It can be used for shop floor scheduling.(4) The Machining Process Simulator (MPS). It can simulate the machining process of machining center by computer. It can check the correctness of NC code (including interference and confliction) and replace the real machining center to support the simulation environment for shop floor scheduling and controlling. There are three companies and universities joining in these four projects, they are: Tsinghua University, Huazhong University of Technology, Beijing Institute of Computer Application and Simulation.",https://ieeexplore.ieee.org/document/6074468/,Journal of Systems Engineering and Electronics,Sept. 1994,ieeexplore
10.1109/TSMCC.2010.2059012,Toward Self-Reconfiguration of Manufacturing Systems Using Automation Agents,IEEE,Journals,"The reconfiguration of control software is regarded as an important ability to enhance the effectiveness and efficiency in future manufacturing systems. Agent technology is considered as a promising approach to provide reconfiguration abilities, but existing work has been focused mainly on the reconfiguration of higher layers concerned with production scheduling and planning. In this paper, we present an automation agent architecture for controlling physical components that integrates “on the fly” reconfiguration abilities on the low-level layer. Our approach is combined with an ontological representation of the low-level functionality at the high-level control layer, which is able to reason and initiate reconfiguration processes to modify the low-level control (LLC). As current control systems are mostly based on standards and principles that do not support reconfiguration, leading to rigid control software architectures, we base our approach on the promising Standard IEC 61499 for the LLC, extended by an innovative reconfiguration infrastructure. We demonstrate this approach with a case study of a reconfiguration process that modifies the LLC functionality provided by the automation agent of a physical component. Thereby, we obtain the ability to support numerous different LLC configurations without increasing the LLC's complexity. By applying our automation agent architecture, we enhance not only the flexibility of each component's control software, but also achieve the precondition for reconfiguring the entire manufacturing system.",https://ieeexplore.ieee.org/document/5557828/,"IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)",Jan. 2011,ieeexplore
10.1109/70.508435,Virtual-reality-based point-and-direct robotic inspection in manufacturing,IEEE,Journals,"This paper explores a flexible manufacturing paradigm in which robot grasping is interactively specified and skeletal images are efficiently used in combination to allow rapidly setting up surface flaw identification tasks in small-quantity/large-variety manufacturing. Two complementary technologies are combined to make implementation of inspection as rapid as possible. First, a novel material handling approach is described for robotic picking and placing of parts onto an inspection table using virtual tools. This allows an operator to point and give directives to set up robotic inspection tasks. Second, since specification may be approximate using this method, a fast and flexible means of identifying images of perfect and flawed parts is explored that avoids rotational or translational restrictions on workpiece placement. This is accomplished by using skeleton pixel counts as neural network inputs. The total system, including material handling and skeleton-based inspection, features flexibility during manufacturing set-up, and reduces the process time and memory requirements for workpiece inspection.",https://ieeexplore.ieee.org/document/508435/,IEEE Transactions on Robotics and Automation,Aug. 1996,ieeexplore
10.1109/ACCESS.2021.3082934,Visual Product Tracking System Using Siamese Neural Networks,IEEE,Journals,"Management of unstructured production data is a key challenge for Industry 4.0. Effective product tracking endorses data integration and productivity improvements throughout the manufacturing processes. Radio-frequency identification (RFID) tags are used in many tracking cases, but in some manufacturing environments, those cannot be used as they might get damaged or removed during processing. In this paper, we propose an alternative visual product tracking system. The physical system uses two cameras placed at the two ends of the tracked process(es). Product pairs are then matched with a Siamese neural network operating on the product images and trained offline on the problem at hand with labeled data. The proposed system can track products solely based on their visual appearance and without any physical interference with the products or production processes. Unlike other existing image-based methods, the proposed system is invariant to major positional and visual changes in the products. As a proof-of-concept, we tested the proposed system with real plywood factory data and were able to track the products with 98.5 % accuracy in a realistic test scenario. The implementation of the proposed method and the Veneer21 dataset are publicly available at https://github.com/TuomasJalonen/visual-product-tracking-system.",https://ieeexplore.ieee.org/document/9439511/,IEEE Access,2021,ieeexplore
10.1109/TSM.2021.3068974,Wafer Reflectance Prediction for Complex Etching Process Based on <italic>K</italic>-Means Clustering and Neural Network,IEEE,Journals,"In LED semiconductor manufacturing, it is important to evaluate the wafer reflectance in order to validate the quality of wafers. In this work, a learning model based on K-means clustering and neural networks is proposed to analyze the relationship between etching parameters and wafer reflectance under a complex etching environment. The implemented clustering algorithm is used to cluster historical data and reduce the effect caused by different processing environments. Then, for each obtained cluster, a neural network is developed to learn the relationship between etching parameters and wafer reflectance. Finally, a real case study from an LED semiconductor fab is conducted to show the application of the proposed method. Experiments show that the proposed method achieves much better performance in comparison with support vector machine for mapping the relationship between etching parameters and wafer reflectance. Also, by the proposed method, the average prediction accuracy of the wafer reflectance can be improved by up to 9.38%, and the mean square error is reduced by 21.64% compared with the methods without clustering.",https://ieeexplore.ieee.org/document/9387413/,IEEE Transactions on Semiconductor Manufacturing,May 2021,ieeexplore
10.1109/TASE.2020.2997718,Weighted Double-Low-Rank Decomposition With Application to Fabric Defect Detection,IEEE,Journals,"Recently, many methods based on low-rank representation have been proposed for fabric defect detection. Most of them relax the low-rank decomposition problem to a nuclear norm minimization (NNM) problem to pursue the convexity of the objective function. When solving the standard NNM problem, matrix singular values have to be treated equally. This, however, would be impractical in the scenario of fabric defect detection as the matrix singular values have clear physical meanings, and thus, they should be treated differently. In this article, we propose a weighted double-low-rank decomposition method (WDLRD) to treat the matrix singular values differently by assigning different weights. Thus, the most important/distinguishing characteristics of a fabric image can be preserved. Another difference between WDLRD and the other existing low-rank-based methods is that WDLRD considers a defective fabric image being decomposed to two low-rank matrices, i.e., low-rank defect-free matrix and low-rank defect matrix, as the defect-free and defective regions are usually composed of homogeneous objects that have a high correlation. Besides, WDLRD is more robust for defect detection in various situations by adding a noise term to avoid noise or other interference on the fabric surface. In addition, a defect prior is incorporated into the objective function of WDLRD to guide locating the defective regions. The proposed optimization problem can be easily solved by an iterative algorithm based on augmented Lagrange multipliers. Experimental results on TILDA, periodically patterned fabric, and Textile &amp; Apparel Artificial Intelligence databases show that the proposed WDLRD obtains better performance than state-of-the-art methods in locating the defective regions on fabric images. <italic>Note to Practitioners</italic>—This article is motivated by the problem that the performance of fabric defect detection in the textile industry is poor. It is necessary to develop an effective method to improve the defect detection accuracy and reduce overall manufacturing cost. Existing automatic defect detection approaches usually contain two stages: first, capture fabric images from the weaving machine and then use a defect detection algorithm in a host computer to conduct a real-time inspection and give an alarm if defects occur. This article focuses on locating defects for given defective images after the procedure of binary classification (which determines an image as defective or defect free). The article proposes an objective function to mathematically interpret the optimization problem between fabric images and predictive defects. The optimal solution can be obtained by employing the alternating direction method of multipliers (ADMMs). The proposed method is described as a new defect detection algorithm. Extensive experiments were conducted to evaluate the algorithm, and the experimental results indicate that the proposed method is superior to many existing fabric defect detection methods. Preliminary experiments suggest that this method is feasible but has not yet been really used in production. In future research, we will collect more fabric images from the textile industry and develop large-scale databases for verifying the proposed method for real-life applications.",https://ieeexplore.ieee.org/document/9123438/,IEEE Transactions on Automation Science and Engineering,July 2021,ieeexplore
10.1109/TAI.2021.3057027,ZJU-Leaper: A Benchmark Dataset for Fabric Defect Detection and a Comparative Study,IEEE,Journals,"Fabric inspection plays an important role in the process of quality control in textile manufacturing. There is a growing demand in the textile industry to leverage computer vision technology for more efficient quality control in the hope that it will replace the traditional labor-intensive inspection by naked eyes. However, there is an underlying viewpoint in most existing fabric datasets that automatic defect detection is a traditional image classification problem, thus more samples help better, which lacks enough consideration about the problem itself and real application environments. After deep communication with users, we find these facts that an assembly line usually has only a few fixed texture fabrics for a long period, users prefer fast deployment and easily upgradable model to a general model and long-time tuning, and users hope the process of collecting samples, annotating, and deployment affects assembly lines as little as possible. This implies that defect detection is different from popular deep learning problems. Multiple-stage models and fast training become more attractive since users are able to train and deploy models by themselves according to the real conditions of samples that can be obtained. Based on this analysis, we propose a new fabric dataset “ZJU-Leaper”. It provides a series of task settings in accordance with the progressive strategy dealing with the problem, from only normal samples to many defective samples with precise annotations, to facilitate real-world application. To avoid misleading information and inconsistency issues associated with the prior evaluation metrics, we propose a new evaluation protocol by experimental analysis of task-specific indexes, which can tell a truthful comparison between different inspection methods. We also offer some novel solutions to address the new challenges of our dataset, as part of the baseline experiments. It is our hope that ZJU-Leaper can accelerate the research of automated visual inspection and empower the practitioners with more opportunities for manufacturing automation in the textile industry. <p><italic>Impact Statement</italic>—Automatic defect inspection is very important in quality control of the fabric industry by helping manufacturers to identify production problems early, hence improving product quality and production efficiency. Meanwhile, it is able to reduce the high labor cost of manual inspection and boost the productivity of the textile industry. To develop effective mathematical inspection algorithms, the fabric dataset serves as an indispensable component to present a practical application environment and enable fair evaluation for algorithms. This paper proposes a new dataset, called “ZJU-Leaper” designed from a viewpoint of multiple-stage models and fast training, containing threefold novelty: 1) the data collection and organization consider the actual requirements and special characteristics of assembly lines in textile factories; 2) it has several designed task settings in order to meet the different levels of requirements in the practical inspection task; 3) it provides a reasonable evaluation protocol for comprehensive comparisons between different inspection algorithms. The preliminary experiments show that some existing algorithms still cannot reach the satisfying performance by this benchmark, which implies more effort should be made to develop new methods for the real use of automatic defect inspection.</p>",https://ieeexplore.ieee.org/document/9346038/,IEEE Transactions on Artificial Intelligence,Dec. 2020,ieeexplore
10.1109/CMPEUR.1992.218439,Applications of neural networks in factory automation,IEEE,Conferences,"Examples of the application of artificial neural networks (ANNs) to real-world problems in factory automation are presented. A few of the advantages and disadvantages of ANNs in general are given. Three typical applications for visual classification, optical measuring, and adaptive control systems, including hardware requirements to achieve real-time speed, are shown. A brief outlook on further developments of ANNs in general and possible areas of application in particular as well as possible combinations with other methods is included. A deliberate mixture of classical, fuzzy, and neural techniques is advocated.&lt;<ETX>&gt;</ETX>",https://ieeexplore.ieee.org/document/218439/,CompEuro 1992 Proceedings Computer Systems and Software Engineering,4-8 May 1992,ieeexplore
10.1109/COASE.2018.8560430,Bringing Automated Intelligence to Cyber-Physical Production Systems in Factory Automation,IEEE,Conferences,"Learning and intelligent algorithms are the basis for Cyber-Physical Production Systems (CPPS). They enable flexibility through reconfiguration ease and fault tolerance by ensuring that the CPPS adapts to changing conditions. However, in order to exhibit such characteristics, CPPS require a proper support for reliably handling the real time behavior of the physical systems they are in control of. This paper presents and discusses basic requirements for control software, which enables flexible and adaptable automated Production Systems (aPS) modelled according to the CPPS concept. In doing so, the paper exposes the main architectural guidelines and rationale for where to place and operate intelligent algorithms in the context of industrial automation for continuous processes.",https://ieeexplore.ieee.org/document/8560430/,2018 IEEE 14th International Conference on Automation Science and Engineering (CASE),20-24 Aug. 2018,ieeexplore
10.1109/CASE48305.2020.9216961,Decentralized coordination of autonomous AGVs for flexible factory automation in the context of Industry 4.0,IEEE,Conferences,"Future smart factories feature flexible systems that can dynamically reconFigure manufacturing systems via near real-time system monitoring and learning-based self-optimization. Automated guided vehicles (AGVs), as a critical method of transporting goods and material within a factory, is vital for flexible automation in a smart factory. However, there is an urgent gap in the ability to dynamically schedule and assign tasks for AGVs in a dynamic environment. In this research, we propose a decentralized AGV fleet architecture and task allocation method to enable dynamic allocation/reallocation of tasks in an AGV fleet. The developed algorithm can also reconFigure AGV task allocations to adapt to system changes, such as AGV failure and new AGVs joining the system. The system modeling, setup and algorithms are presented with a case study in a lab environment that demonstrates flexible collaboration between an AGV fleet and a robotic assembly cell.",https://ieeexplore.ieee.org/document/9216961/,2020 IEEE 16th International Conference on Automation Science and Engineering (CASE),20-21 Aug. 2020,ieeexplore
10.1109/GIOTS49054.2020.9119497,Industrial IoT and Digital Twins for a Smart Factory : An open source toolkit for application design and benchmarking,IEEE,Conferences,"The rapid evolution of digital technology and designed intelligence, such as the Internet of Things (IoT), Big data analytics, Artificial Intelligence (AI), Cyber Physical Systems (CPS), has been a catalyst for the 4th industrial revolution (known as industry 4.0). Among other, the two key state-of-the-art concepts in Industry 4.0, are Industrial IoT (IIoT) and digital twins. IIoT facilitates real-time data acquisition, processing and analytics over large amount of sensor data streams produced by sensors installed within a smart factory, while the `digital twin' concept aims to enable smart factories via the digital replication or representation of physical machines, processes, people in cyber-space. This paper explores the capability of present-state open-source platforms to collectively achieve digital twin capabilities, including IoT real-time data acquisition, virtual representation, analytics, and visualisation. The aim of this work is to `close the gap' between research and implementation, through a collective open source IoT and Digital Twin architecture. The performance of the open-source architecture in this work, is demonstrated in a use-case utilising industry `open data', and is bench-marked with universal testing tools.",https://ieeexplore.ieee.org/document/9119497/,2020 Global Internet of Things Summit (GIoTS),3-3 June 2020,ieeexplore
10.1109/SAI.2015.7237244,Object oriented eco-simulator system as predictor &amp; exploratory system to track impact of human induced activities on environmental resource: A case study: Evaluation of unsuitable ground water consumption of petrochemical and power factory of Shazand on ecological status of Markazi province,IEEE,Conferences,"Environment system studies regardless of main reasons are very difficult whereas they have been constructed from natural ecosystems, manmade infrastructure, socio-economic virtual objects and also their perplexing inner relations. Furthermore, ecosystems as any other elements, have iterative multi-scale structure which makes it even more sophisticated to study. That's why real compound system studies look breathtaking and also too difficult. Gradual depletion of perched water tables in “Shazand” watershed of “Markazi” province in Iran, which mainly stems from bulky extraction of water resources by huge factories and intensive agriculture e.g., doesn't treated well by policy makers not only due to lack of well-based studies but also via devoid of potent monitoring and prediction simulator system to manifest decision side effects to managers and those whom are responsible to keep the environment system balanced. Although GI functionalities have exhibited wonder abilities to handle spatial problems, they seem weak in processing ecosystem real problems as there is few specified environment modules. While combination of GIS and environment modeling software intensively suffers from integration issue, simulator software are mainly confined due to filed limitation, non-spatial functionalities and spatial ontologies. To present a proper solution which makes up the mentioned deficiencies, first of all, a problem tree was designed to decompose real cybernetic correlations between “Shazand” watershed elements including natural or manmade ones. Then, object oriented concept was deployed to simulate sophisticated real environmental systems and also to simplify programming issues regarding to exact spatial ontology. To complete the solution, a category based on “Category Theorem” was defined to support temporal concepts on the backdrop of real ecosystem phenomena regarding Spatio-temporal modeling concepts and object oriented foundations. Likewise group of functors were designed via functional language to allay process burden and also programming of object counteractions. Emerging spatial simulator system, supports monitoring issues as well as prediction missions in compound environment systems. It has been also indicated how to use it as an exploratory analyzing system which conduct real analytical system consisted of virtual smart analytical objects.",https://ieeexplore.ieee.org/document/7237244/,2015 Science and Information Conference (SAI),28-30 July 2015,ieeexplore
10.1109/AIVR.2018.00024,The Virtual Factory: Hologram-Enabled Control and Monitoring of Industrial IoT Devices,IEEE,Conferences,"Augmented reality (AR) has been exploited in manifold fields but is yet to be used at its full potential. With the massive diffusion of smart devices, opportunities to build immersive human-computer interfaces are continually expanding. In this study, we conceptualize a virtual factory: an interactive, dynamic, holographic abstraction of the physical machines deployed in a factory. Through our prototype implementation, we conducted a user-study driven evaluation of holographic interfaces compared to traditional interfaces, highlighting its pros and cons. Our study shows that the majority of the participants found holographic manipulation more attractive and natural to interact with. However, current performance characteristics of head-mounted displays must be improved to be applied in production.",https://ieeexplore.ieee.org/document/8613643/,2018 IEEE International Conference on Artificial Intelligence and Virtual Reality (AIVR),10-12 Dec. 2018,ieeexplore
10.1109/ACCESS.2017.2770180,An Integrated Industrial Ethernet Solution for the Implementation of Smart Factory,IEEE,Journals,"Smart factory addresses the vertical integration of physical entities and information systems. Network and cloud are two essential infrastructures to achieve this goal. Among them, the network provides interconnection for communication and data flow, while the cloud provides powerful and elastic computing and storage abilities for big data and intelligent applications. This paper presents a cloud-centric framework for the implementation of the smart factory. Based on this framework, three high leveled protocols viz., EtherCAT, DDS, and OPC UA are selected to implement machine network (controller to sensors/actuators communication), machine to machine communication, and machine to cloud communication respectively, to satisfy diverse communication requirements of the smart factory. An integrated architecture for combining ontology knowledge and semantic data to support intelligent applications is also proposed. These network and data process schemes are verified in a smart factory prototype for personalized candy packing application.",https://ieeexplore.ieee.org/document/8096995/,IEEE Access,2017,ieeexplore
10.1109/ACCESS.2020.3046784,Reinforcement Learning With Composite Rewards for Production Scheduling in a Smart Factory,IEEE,Journals,"Rapid advances of sensing and cloud technologies transform the manufacturing system into a data-rich environment and make production scheduling increasingly complex. Traditional offline scheduling methods are limited in the ability to handle low-volume-high-mix workorders with diverse design specifications. Simulation-based methods show the promise for distributed scheduling of manufacturing jobs but are mostly implemented with historical data and empirical rules in a static manner. Recently, artificial intelligence (AI) algorithms fuel increasing interests to solve dynamic scheduling problems in the manufacturing setting. However, it's difficult to utilize high-dimensional data for production scheduling while considering multiple practical objectives for smart manufacturing (e.g., minimize the makespan, reduce production costs, balance workloads). Therefore, this paper presents a new AI scheduler with composite reward functions for data-driven dynamic scheduling of manufacturing jobs under uncertainty in a smart factory. Internet-enabled sensor networks are deployed in the smart factory to track real-time statuses of workorders, machines, and material handling systems. A novel manufacturing value network is developed to take high-dimensional data as the input and then learn the state-action values for real-time decision making. Based on reinforcement learning (RL), composite rewards help the AI scheduler learn efficiently to achieve multiple objectives for production scheduling in real time. The proposed methodology is evaluated and validated with experimental studies in a smart manufacturing setting. Experimental results show that the new AI scheduler not only improves the multi-objective performance metrics in the production scheduling problem but also effectively copes with unexpected events (e.g., urgent workorders, machine failures) in manufacturing systems.",https://ieeexplore.ieee.org/document/9305707/,IEEE Access,2021,ieeexplore
10.1109/RAMS48097.2021.9605729,A Data Mining Approach for Forecasting Machine Related Disruptions,IEEE,Conferences,"SUMMARY &amp; CONCLUSIONSProduction disruptions in high-tech mass production companies producing many parts every single minute will lead to considerable economic impact and affect manufacturing efficiency. The root causes of disruptions are classified into three categories: human-related, machine-related, and material related. Using different management, hiring, and training strategies, companies are generally successful in reducing human-related and material related disruptions. However, machine-related disruptions (MRDs) are still occurring even in companies employing a solid maintenance program.The MRDs pose random pauses of various durations in a production. Forecasting the characteristics of such pauses (downtimes) can assist in real-time manufacturing process adjustment and real-time rescheduling of a production. This study aims at utilizing available recorded MRDs for forecasting time to forthcoming MRD and its duration. Our general approach is to evaluate the performance of different data mining-based learning techniques for predicting both the duration and time to forthcoming MRDs and determining the outperforming approach. We consider a smart factory located in the northern part of Toronto great area active in the field of thermoplastic injection molding of various components. We use the historical data on the MRDs recorded from 2013 to 2019 to conduct the investigation. In this study, a set of different classifiers, including rule-based, function-based, tree-based, and lazy, are implemented for forecasting each of the duration and time to forthcoming MRD. The part ID, machine ID, mold age, and the ordinal number of the forthcoming MRD are considered as the input attributes of the developed data mining-based classifiers.In order to determine how effectively the data mining-based methods perform, we calculate different performance criteria, including the Mean Absolute Error (MAE), Root Mean Square Error (RMSE), Bias, Correlation Coefficient (CC), and R<sup>2</sup>. The overall accuracy rate for some tree-based algorithms is significant (CC exceeded 0.92 and MAE&lt;0.06). It shows the capabilities of data mining-based approaches in forecasting the durations and times to MRDs. The obtained trained models are accurate enough to be coupled with stochastic optimization algorithms for real-time manufacturing process adjustment and rescheduling when an MDR takes place.",https://ieeexplore.ieee.org/document/9605729/,2021 Annual Reliability and Maintainability Symposium (RAMS),24-27 May 2021,ieeexplore
10.1109/ECTC.2019.00261,A Methodology to Correct in-Fixture Measurement of Impedance by a Machine Learning Model,IEEE,Conferences,"It is usual to measure the characteristic impedance of the transmission line in the factory by the time-domain reflectometer (TDR) with the test fixture, but the insertion loss of the test fixture would raise the measurement error, especially when the transmission line designed with unconventional impedance that is deviated from the system impedance of the measurement instrument. The traditional calibration that requires the standard kits is not generally implemented in production line due to the complicated operation and high cost. Therefore, a method is proposed to correct the measurement by a generalized transformation that is a numeric model trained by the machine learning from the measurements corresponding to the well-chosen features of the test fixture. Based on the precise circuit model of test fixture presented in this paper, a large amount of data used for training can be produced by circuit simulation instead of real measurements. An experimental instance is given to demonstrate that the measurement error could be suppressed from around 20% to below 3%.",https://ieeexplore.ieee.org/document/8811021/,2019 IEEE 69th Electronic Components and Technology Conference (ECTC),28-31 May 2019,ieeexplore
10.1109/SMC.2019.8914195,A Universal Methodology to Create Digital Twins for Serial and Parallel Manipulators,IEEE,Conferences,"With the technological advances in information technology especially in sensorization, artificial intelligence, big data and visualization; smart manufacturing and industrie 4.0 are gradually becoming an implementable reality. Digital twin is one of the pillars of smart manufacturing where by the physical and virtual worlds can by synced and mimic each others' behaviour. In future, assets and products can sense their state and report back any anomalities so that meaningful insights can be drawn and actions can be taken to keep production optimized at all times. This reporting feature can be realized with the help of digital twin technology as the twin keeps record of the physical asset's behavior. A digital twin will play an integral role in defining the concept of an integrated shopfloor. It will assist in viewing holistic behavior of asset, optimizing processes and exerting control over the physical device. To demonstrate the concept of digital twin, an universal methodology was developed and deployed at Model Factory @ ARTC (Advanced Remanufacturing and Technology Centre) program in Singapore.",https://ieeexplore.ieee.org/document/8914195/,"2019 IEEE International Conference on Systems, Man and Cybernetics (SMC)",6-9 Oct. 2019,ieeexplore
10.1109/MetroInd4.0IoT51437.2021.9488498,Adaptive LoRaWAN Transmission exploiting Reinforcement Learning: the Industrial Case,IEEE,Conferences,"Wireless technologies play a key role in the Industrial Internet of Things (IIoT) scenario, for the development of increasingly flexible and interconnected factory systems. A significant opportunity in this context is represented by the advent of Low Power Wide Area Network (LPWAN) wireless technologies, that enable a reliable, secure, and effective transmission of measurement data over long communication ranges and with very low power consumption. Nevertheless, reliability in harsh environments (as typically occurs in the industrial scenario) is a significant issue to deal with. Focusing on LoRaWAN, adaptive strategies can be profitably devised concerning the above tradeoff. To this aim, this paper proposes to exploit Reinforcement Learning (RL) techniques to design an adaptive LoRaWAN strategy for industrial applications. The RL is spreading in many fields since it allows the design of intelligent systems using a stochastic discrete-time system approach. The proposed technique has been implemented within a purposely designed simulator, allowing to draw a preliminary performance assessment in a real-world scenario. A high density of independent nodes per square km has been considered, showing a significant improvement (about 10%) of the overall reliability in terms of data extraction rate (DER) without compromising full compatibility with the standard specifications.",https://ieeexplore.ieee.org/document/9488498/,2021 IEEE International Workshop on Metrology for Industry 4.0 & IoT (MetroInd4.0&IoT),7-9 June 2021,ieeexplore
10.23919/IConAC.2019.8895095,Ant Colony Optimization Algorithm for Industrial Robot Programming in a Digital Twin,IEEE,Conferences,"Advanced manufacturing that is adaptable to constantly changing product designs often requires dynamic changes on the factory floor to enable manufacture. The integration of robotic manufacture with machine learning approaches offers the possibility to enable such dynamic changes on the factory floor. While ensuring safety and the possibility of losses of components and waste of material are against their usage. Furthermore, developments in design of virtual environments makes it possible to perform simulations in a virtual environment, to enable human-in-the-loop production of parts correctly the first time like never before. Such powerful simulation and control software provides the means to design a digital twin of manufacturing environment in which trials are completed at almost at no cost. In this paper, ant colony optimization is used to program an industrial robot to avoid obstacles and find its way to pick and place objects during an assembly task in an environment containing obstacles that must be avoided. The optimization is completed in a digital twin environment first and movements transferred to the real robot after human inspection. It is shown that the proposed methodology can find the optimal solution, in addition to avoiding collisions, for an assembly task with minimum human intervention.",https://ieeexplore.ieee.org/document/8895095/,2019 25th International Conference on Automation and Computing (ICAC),5-7 Sept. 2019,ieeexplore
10.1109/CYBER53097.2021.9588269,Application of YOLO Object Detection Network In Weld Surface Defect Detection,IEEE,Conferences,"As industrial production becomes more modern and intelligent today, the inspection of product quality of the workshop is becoming more and more accustomed to replacing the old manual visual inspection methods with automated inspection systems. In the welding field, automated welding robots are not only used in traditional large-scale automobile assembly lines. In more general welding work, welding robots also plays an important role. The inspection of the welding quality of the welding robot is mainly to detect the four main types of weld defects. Compared to traditional defect classification based on support vector machines and defect detection based on template matching, this paper uses a welding surface defect detection system designed based on deep learning methods. By working with workshop welding experts, a large-scale image of nearly 5000 pictures is built. Large-scale weld defect datasets, while using the real-time and accuracy of the YOLO series of deep learning object detection frameworks, the weld defects detection model reaches 75.5% mean average precision(mAP) in constructed weld defect data set. In addition, the construction cost of the detection model and the deployment time of the detection system are greatly reduced. During the field test of the system in the workshop, among a batch of welding workpieces provided by the factory, the detection accuracy of weld defects reached 71%, which initially met the requirements of the workshop for an automated defect detection system.",https://ieeexplore.ieee.org/document/9588269/,"2021 IEEE 11th Annual International Conference on CYBER Technology in Automation, Control, and Intelligent Systems (CYBER)",27-31 July 2021,ieeexplore
10.1109/CMPEUR.1992.218439,Applications of neural networks in factory automation,IEEE,Conferences,"Examples of the application of artificial neural networks (ANNs) to real-world problems in factory automation are presented. A few of the advantages and disadvantages of ANNs in general are given. Three typical applications for visual classification, optical measuring, and adaptive control systems, including hardware requirements to achieve real-time speed, are shown. A brief outlook on further developments of ANNs in general and possible areas of application in particular as well as possible combinations with other methods is included. A deliberate mixture of classical, fuzzy, and neural techniques is advocated.&lt;<ETX>&gt;</ETX>",https://ieeexplore.ieee.org/document/218439/,CompEuro 1992 Proceedings Computer Systems and Software Engineering,4-8 May 1992,ieeexplore
10.1109/ECAI52376.2021.9515185,Automated ceramic plate defect detection using ScaledYOLOv4-large,IEEE,Conferences,"Automated visual inspection has become a popular topic of research in the last couple of decades, as computation power available grew exponentially. Judging by the fact that visual inspection is a critical task for the quality of the products, it would be highly recommended that people only supervise the system. This paper proposes a low cost, rapid development defect detection system based on the Scaled-YOLOv4 object detection model. The original model achieves almost state of the art detection mAP on the COCO dataset with a mAP(mean average precision) of 56.0 for the largest variant, named YOLOv4-P7. Our version is derived from the ScaledYOLOv4-P5 model and is trained on ceramic plate defects and achieves 87.4 mAP at a intersection of union of 0.5, while comfortably processing a frame in 20ms on a consumer RTX3070 GPU. Thus, the real time constraint for the manufacturing system is fulfilled. Hence, the critical aspects of the development process are the: quick development process, fast training, rapid deployment on the factory floor, quick validation and feedback, using images acquired in the lab - not on the factory floor for first iteration and overall low cost of the automated inspection system.",https://ieeexplore.ieee.org/document/9515185/,"2021 13th International Conference on Electronics, Computers and Artificial Intelligence (ECAI)",1-3 July 2021,ieeexplore
10.1109/ICAIIC51459.2021.9415189,Big Data Platform for Intelligence Industrial IoT Sensor Monitoring System Based on Edge Computing and AI,IEEE,Conferences,"The cutting edge of Industry 4.0 has driven everything to be converted to disruptive innovation and digitalized. This digital revolution is imprinted by modern and advanced technology that takes advantage of Big Data and Artificial Intelligence (AI) to nurture from automatic learning systems, smart city, smart energy, smart factory to the edge computing technology, and so on. To harness an appealing, noteworthy, and leading development in smart manufacturing industry, the modern industrial sciences and technologies such as Big Data, Artificial Intelligence, Internet of things, and Edge Computing have to be integrated cooperatively. Accordingly, a suggestion on the integration is presented in this paper. This proposed paper describes the design and implementation of big data platform for intelligence industrial internet of things sensor monitoring system and conveys a prediction of any upcoming errors beforehand. The architecture design is based on edge computing and artificial intelligence. To extend more precisely, industrial internet of things sensor here is about the condition monitoring sensor data - vibration, temperature, related humidity, and barometric pressure inside facility manufacturing factory.",https://ieeexplore.ieee.org/document/9415189/,2021 International Conference on Artificial Intelligence in Information and Communication (ICAIIC),13-16 April 2021,ieeexplore
10.1109/ICII.2018.00024,Brightics-IoT: Towards Effective Industrial IoT Platforms for Connected Smart Factories,IEEE,Conferences,"Industrial Internet-of-Things (IIoT) supports machines, computers and users to enable intelligent operations using advanced device management and data analytics. In recent years, thanks to standardized IoT platforms and advanced Artificial Intelligence (AI) technologies, there have been great advances in IIoT, and now it promises revolutions on various manufacturing domains such as transport, health, factory and energy. In this paper, based on our experience operating IIoT in various factory applications, we present the technical challenges of manufacturing facilities needed to be dealt with to collect huge amount of data in real-time and counteraction points of an IoT platform regarding these technical challenges and what kinds of features need to be implemented for intelligent services in the smart manufacturing. Finally, we introduce a story of applying industrial IoT platform in production to show how iterative development approaches can achieve business requirements based on elastically scaled-out architecture.",https://ieeexplore.ieee.org/document/8539113/,2018 IEEE International Conference on Industrial Internet (ICII),21-23 Oct. 2018,ieeexplore
10.1109/VRAIS.1993.380772,Calibration of head-mounted displays for augmented reality applications,IEEE,Conferences,"The authors have developed ""augmented reality"" technology, consisting of a see-through head-mounted display, a robust, accurate position/orientation sensor, and their supporting electronics and software. Their primary goal is to apply this technology to touch labor manufacturing processes, enabling a factory worker to view index markings or instructions as if they were painted on the surface of a workpiece. In order to accurately project graphics onto specific points of a workpiece, it is necessary to have the coordinates of the workpiece, the display's virtual screen, the position sensor, and the user's eyes in the same coordinate system. The linear transformation and projection of each point to be displayed from world coordinates to virtual screen coordinates are described, and the experimental procedures for determining the correct values of the calibration parameters are characterized.&lt;<ETX>&gt;</ETX>",https://ieeexplore.ieee.org/document/380772/,Proceedings of IEEE Virtual Reality Annual International Symposium,18-22 Sept. 1993,ieeexplore
10.1109/ICIT.2002.1189341,Computer based robot training in a virtual environment,IEEE,Conferences,"As more market segments are welcoming automation, the robotic field continues to expand. With the accepted breadth of viable industrial robotic applications increasing, the need for flexible robotic training also grows. In the area of simulation and offline programming there have been innovative developments to Computer Aided Robotics (CAR) Systems. New and notable releases have been introduced to the public, especially among the small, affordable, and easy to use systems. These CAR-Systems are mainly aimed at system integrators in general industry business fields to whom the complex, powerful software tools used by the automotive industry (and its suppliers) are oversized. In general, CAR-Systems are used to design robot cells and to create the offline programs necessary to reduce start-up time and to achieve a considerable degree of planning reliability. Another potential yet to be fully considered, is the use of such CAR-Systems as an inexpensive and user-friendly tool for robotics training. This paper will show the educational potential and possibility inherent in simulation and introduce a successful example of this new method of training. Finally, this presentation should be seen as an attempt to outline novel methods for future education in an industrial environment characterized by the increased occurrence and implementation of the virtual factory.",https://ieeexplore.ieee.org/document/1189341/,"2002 IEEE International Conference on Industrial Technology, 2002. IEEE ICIT '02.",11-14 Dec. 2002,ieeexplore
10.1109/INES.2017.8118553,Data integration in scalable data analytics platform for process industries,IEEE,Conferences,"The main objective of work presented in this paper is to introduce the architectural overview of the big data analytics platform for support of process industries. Our aim was to design and develop the cross-sectorial scalable environment, which will enable the data collection from different sources and support the development of predictive functions to help the process industries in optimizing of their production processes. This paper introduces the components of Big Data Storage and Analytics platform which is the core component of the developed cross-sectorial environment. Currently, it is built on top of the Apache Hadoop technology stack and relies on Hadoop distributed file system. On the other hand, we present the idea of integration of the data obtained from different production environments. Data integration is implemented using the Apache Nifi and we designed the workflows for processing both interval and real-time data from the production sites. In this case, we consider two pilot cases, an aluminium factory in France and a plastic molding factory in Portugal.",https://ieeexplore.ieee.org/document/8118553/,2017 IEEE 21st International Conference on Intelligent Engineering Systems (INES),20-23 Oct. 2017,ieeexplore
10.1109/CASE48305.2020.9216961,Decentralized coordination of autonomous AGVs for flexible factory automation in the context of Industry 4.0,IEEE,Conferences,"Future smart factories feature flexible systems that can dynamically reconFigure manufacturing systems via near real-time system monitoring and learning-based self-optimization. Automated guided vehicles (AGVs), as a critical method of transporting goods and material within a factory, is vital for flexible automation in a smart factory. However, there is an urgent gap in the ability to dynamically schedule and assign tasks for AGVs in a dynamic environment. In this research, we propose a decentralized AGV fleet architecture and task allocation method to enable dynamic allocation/reallocation of tasks in an AGV fleet. The developed algorithm can also reconFigure AGV task allocations to adapt to system changes, such as AGV failure and new AGVs joining the system. The system modeling, setup and algorithms are presented with a case study in a lab environment that demonstrates flexible collaboration between an AGV fleet and a robotic assembly cell.",https://ieeexplore.ieee.org/document/9216961/,2020 IEEE 16th International Conference on Automation Science and Engineering (CASE),20-21 Aug. 2020,ieeexplore
10.23919/ECC.1999.7100000,Dynamic GMDH neural networks and their application in fault detection systems,IEEE,Conferences,"In this paper, the problem of the dynamic GMDH (Group Method and Data Handling) neural networks and their application in fault detection systems is presented. Such networks can be considered as feedforward networks with a growing structure during the training process. The GMDH networks application in fault detection systems improves their efficiency with lack of information regarding the structure and dynamics of the diagnosed system. The proposed networks have been implemented in fault detection systems using the real data from the Lublin sugar factory.",https://ieeexplore.ieee.org/document/7100000/,1999 European Control Conference (ECC),31 Aug.-3 Sept. 1999,ieeexplore
10.1109/AIHAS.1991.138480,"EMM-networking model for FMS modeling, simulation and control",IEEE,Conferences,"The major challenge of implementing FMS (flexible manufacturing systems) at the factory level is to realize automated control. An appropriate FMS model is required for the purpose of control software design and implementation. In this paper, the EMM-networking (extended Moore machine) model based on automata theory for FMS control systems is presented. The basic concept behind the EMM-networking model is to identify the structure of complex systems in a hierarchy of abstractions. By using object-oriented methodology, the EMM-networking model of a FMS can be implemented into object-based software modules, based on which an integrated environment for simulation, software construction and real-time control can be realized. The methodology is demonstrated via the simulation of a prototype FMS.&lt;<ETX>&gt;</ETX>",https://ieeexplore.ieee.org/document/138480/,"[1991] Proceedings. The Second Annual Conference on AI, Simulation and Planning in High Autonomy Systems",1-2 April 1991,ieeexplore
10.1109/ISAECT50560.2020.9523700,Edge-Cloud Architectures Using UAVs Dedicated To Industrial IoT Monitoring And Control Applications,IEEE,Conferences,"The deployment of new technologies to ease the control and management of a massive data volume and its uncertainty is a very significant challenge in the industry. Under the name ""Smart Factory"", the Industrial Internet of Things (IoT) aims to send data from systems that monitor and control the physical world to data processing systems for which cloud computing has proven to be an important tool to meet processing needs. unmanned aerial vehicles (UAVs) are now being introduced as part of IIoT and can perform important tasks. UAVs are now considered one of the best remote sensing techniques for collecting data over large areas. In the field of fog and edge computing, the IoT gateway connects various objects and sensors to the Internet. It function as a common interface for different networks and support different communication protocols. Edge intelligence is expected to replace Deep Learning (DL) computing in the cloud, providing a variety of distributed, low-latency and reliable intelligent services. In this paper, An unmanned aerial vehicle is automatically integrated into an industrial control system through an IoT gateway platform. Rather than sending photos from the UAV to the cloud for processing, an AI cloud trained model is deployed in the IoT gateway and used to process the taken photos. This model is designed to overcome the latency channels of the cloud computing architecture. The results show that the monitoring and tracking process using advanced computing in the IoT gateway is significantly faster than in the cloud.",https://ieeexplore.ieee.org/document/9523700/,2020 International Symposium on Advanced Electrical and Communication Technologies (ISAECT),25-27 Nov. 2020,ieeexplore
10.1109/ICE/ITMC49519.2020.9198492,Enhancing Cognition for Digital Twins,IEEE,Conferences,"In the era of Industry 4.0, Digital Twins (DTs) pave the way for the creation of the Cognitive Factory. By virtualizing and twinning information stemming from the real and the digital world, it is now possible to connect all parts of the production process by having virtual copies of physical elements interacting with each other in the digital and physical realms. However, this alone does not imply cognition. Cognition requires modelling not only the physical characteristics but also the behavior of production elements and processes. The latter can be founded upon data-driven models produced via Data Analytics and Machine Learning techniques, giving rise to the so-called Cognitive (Digital) Twin. To further enable the Cognitive Factory, a novel concept, dubbed as Enhanced Cognitive Twin (ECT), is proposed in this paper as a way to introduce advanced cognitive capabilities to the DT artefact that enable supporting decisions, with the end goal to enable DTs to react to inner or outer stimuli. The Enhanced Cognitive Twin can be deployed at different hierarchical levels of the production process, i.e., at sensor-, machine-, process-, employee- or even factory-level, aggregated to allow both horizontal and vertical interplay. The ECT notion is proposed in the context of process industries, where cognition is particularly important due to the continuous, non-linear, and varied nature of the respective production processes.",https://ieeexplore.ieee.org/document/9198492/,"2020 IEEE International Conference on Engineering, Technology and Innovation (ICE/ITMC)",15-17 June 2020,ieeexplore
10.1109/WSC.1989.718775,Expert Simulation For On-line Scheduling,IEEE,Conferences,"In recent years, the automotive industry has realized the importance of speed of new products to market and has mounted efforts for improving it. The Expert System Scheduler (ESS) facilitates these efforts by enabling manufacturing plants to generate viable schedules under increasing constraints and demands for flexibility. The scheduler takes advantage of the Computer Integrated Manufacturing (CIM) environment by utilizing the real-time information from the factory for responsive scheduling. The Expert System Scheduler uses heuristics developed by an experiences factory scheduler. It uses simulation concepts and these heuristics to generate schedules. Forward and ""backward"" simulation are used at different stages of the schedule generation process. The system is used to control parts flow on the factory floor at one automated facility. This highly automated facility is a testbed for implementation of CIM concepts. The scheduler runs on a Texas Instruments (TI) Explorer II computer using software developed inhouse utilizing IntelliCorp's Knowledge Engineering Environment (KEE) shell and the LISP language. The scheduling computer is networked to the factory control computer, which actually controls the plant floor. The TI Explorer II acquires current plant floor information from the factory control system, generates a new schedule and sends it back within a short time. The configuration allows fast response to changes in requirements and plant floor conditions.",https://ieeexplore.ieee.org/document/718775/,1989 Winter Simulation Conference Proceedings,4-6 Dec. 1989,ieeexplore
10.1109/ATC52653.2021.9598204,Fast and Accurate Fall Detection and Warning System Using Image Processing Technology,IEEE,Conferences,"Accidental falls can cause serious injuries, which can lead to serious medical problems, especially for construction and factory workers. This paper proposes a study on a fall detection system based on computer vision. This system is applied to help detect people falling in harsh working environment such as dust, loud noise, few people working. From the recorded video streams, the data is processed to recognize a person falling, lying motionless. Algorithms for tracking people are implemented on a compact, easy-to-install embedded system. Experimental results show that the system ensures safety and can provide emergency assistance to people who have fallen within the view of the camera.",https://ieeexplore.ieee.org/document/9598204/,2021 International Conference on Advanced Technologies for Communications (ATC),14-16 Oct. 2021,ieeexplore
10.1109/GLOCOMW.2017.8269228,How Effective is the Artificial Noise? Real-Time Analysis of a PHY Security Scenario,IEEE,Conferences,"Expanding usage of mobile technologies and devices causes new challenges, especially in terms of security. In the near future, the number of battery powered devices will also significantly increase. Current security approaches are not sufficient for these issues and high-complexity cryptographic techniques are not suitable for such devices. To address these problems, physical layer (PHY) security solutions have recently emerged. Some 5G candidate techniques such as full-duplex communication (FD) and artificial noise transmission (AN) are exploited in various PHY security solutions due to their performance advantages. In this study, a scenario resembling a factory environment is considered and security level is analyzed in real-time by utilizing FD and AN methods. The aim of this analysis is to investigate physically secure transmission regions and to expand these regions by intelligently using AN. Our proposed approach is evaluated in real- time with a testbed that consists of software defined radios (SDRs). Two devices with very different front-end characteristics are utilized as eavesdroppers to analyze hardware effects on the eavesdropping performance. As observed with experiments, robust secure regions can be created by the use of AN on eavesdroppers effectively.",https://ieeexplore.ieee.org/document/8269228/,2017 IEEE Globecom Workshops (GC Wkshps),4-8 Dec. 2017,ieeexplore
10.1109/IDAACS-SWS.2018.8525503,Hybrid MAC for Low Latency Wireless Communication Enabling Industrial HMI Applications,IEEE,Conferences,"Wireless technologies are one of the core components of the future industrial applications. They provide flexibility and scalability to the factory floor in parallel with deployment cost reduction. In our paper, we concentrate on future-oriented human-machine interaction (HMI) applications such as augmented reality (AR) or mobile control. Based on their requirements, we provide an investigation of IEEE 802.11 channel access techniques with respect to their suitability for industrial applications.",https://ieeexplore.ieee.org/document/8525503/,2018 IEEE 4th International Symposium on Wireless Systems within the International Conferences on Intelligent Data Acquisition and Advanced Computing Systems (IDAACS-SWS),20-21 Sept. 2018,ieeexplore
10.1109/LISS.2018.8593220,IOT Enabled Smart Logistics Using Smart Contracts,IEEE,Conferences,"Advancements in sensors and devices have enabled Internet of Things (IoT) adoption in various sectors, especially in domains looking to automate and increase their real-time decision making capabilities to improve efficiencies. Supply chain management in logistics is a perfect fit for adoption of IoT, since it involves shipment of assets being moved, tracked and housed by a number of machines, vehicles and people each day. Smart Contracts are terms and conditions parties can specify that assure trust in the enforceability of the contract and provide visibility at every step of a supply chain. IoT devices can write to a smart contract as a product moves from the factory floor to the store shelves, providing real-time visibility of an enterprises entire supply chain. This paper proposes a smart logistics solution encapsulating smart contracts, logistics planner and condition monitoring of the assets in the Supply Chain Management area. A prototype of the solution is implemented which demonstrates accountability, traceability and liability for asset handling across the supply chain by various parties involved in a logistics scenario.",https://ieeexplore.ieee.org/document/8593220/,"2018 8th International Conference on Logistics, Informatics and Service Sciences (LISS)",3-6 Aug. 2018,ieeexplore
10.1109/GIOTS49054.2020.9119497,Industrial IoT and Digital Twins for a Smart Factory : An open source toolkit for application design and benchmarking,IEEE,Conferences,"The rapid evolution of digital technology and designed intelligence, such as the Internet of Things (IoT), Big data analytics, Artificial Intelligence (AI), Cyber Physical Systems (CPS), has been a catalyst for the 4th industrial revolution (known as industry 4.0). Among other, the two key state-of-the-art concepts in Industry 4.0, are Industrial IoT (IIoT) and digital twins. IIoT facilitates real-time data acquisition, processing and analytics over large amount of sensor data streams produced by sensors installed within a smart factory, while the `digital twin' concept aims to enable smart factories via the digital replication or representation of physical machines, processes, people in cyber-space. This paper explores the capability of present-state open-source platforms to collectively achieve digital twin capabilities, including IoT real-time data acquisition, virtual representation, analytics, and visualisation. The aim of this work is to `close the gap' between research and implementation, through a collective open source IoT and Digital Twin architecture. The performance of the open-source architecture in this work, is demonstrated in a use-case utilising industry `open data', and is bench-marked with universal testing tools.",https://ieeexplore.ieee.org/document/9119497/,2020 Global Internet of Things Summit (GIoTS),3-3 June 2020,ieeexplore
10.1109/ICIT.1996.601630,Intelligent monitoring of sheet forming process,IEEE,Conferences,"In the sheet forming process, tight QC requirements and strict economic objectives make it necessary for factory to quickly identify sheet defects and take corrective actions. A framework of sensor-based intelligent monitoring system is suggested to perform online monitoring and control of cold rolled strip forming. For successful implementation, a backpropagation neural monitor is employed to recognize the defects of the cold rolled strip and generate appropriate control strategy. The monitoring system first deals with a large amount of raw process data detected by stress sensors. From such real-time data, interesting and important features, stress series are extracted and normalized. The stress series are then trained by the neural monitor for identifying the defects, such as left slope, right slope, central buckle, edge wave, quarter-wave and compound wave et al. The output of the neural monitor will activate corresponding feedback control actions such as CVC shift, screws, adjustment of bending pressure and selection of cooling sprays. To improve the performance of the neural networks, optimal learning parameters are employed to train the neural networks. The results of a case study have shown that the output of the defect recognition is well matched to the practical situation, and have given encouragement to further improvements of the intelligent monitoring system.",https://ieeexplore.ieee.org/document/601630/,Proceedings of the IEEE International Conference on Industrial Technology (ICIT'96),2-6 Dec. 1996,ieeexplore
10.1109/ICAIIC51459.2021.9415228,IoT-Based Vibration Sensor Data Collection and Emergency Detection Classification using Long Short Term Memory (LSTM),IEEE,Conferences,"In this paper, we used a vibration sensor known as G-Link 200 to collect real time vibration data. The sensor is connected through the internet gateway and Long Short Term Memory (LSTM) used for the classification of sensor data. The classification allows for detecting normal and anomaly activity situation which allows for triggering emergency situation. This is implemented in smart homes where privacy is an issue of concern. Example of such places are toilets, bedrooms and dressing rooms. It can also be applied to smart factory where detecting excessive or abnormal vibration is of critical importance to factory operation. The system eliminates the discomfort for video surveillance to the user. The data collected is also useful for the research community in similar research areas of sensor data enhancement. MATLAB R2019b was used to develop the LSTM. The result showed that the accuracy of the LSTM is 97.39% which outperformed other machine learning algorithm and is reliable for emergency classification.",https://ieeexplore.ieee.org/document/9415228/,2021 International Conference on Artificial Intelligence in Information and Communication (ICAIIC),13-16 April 2021,ieeexplore
10.1109/PHM-Besancon49106.2020.00009,Machine Performance Monitoring and Fault Classification using Vibration Frequency Analysis,IEEE,Conferences,"Machine anomalies in manufacturing directly affect the production yield and factory operation efficiency if such anomalies cannot be detected in time. Real-time monitoring of machine health condition not only improves machine throughput by reducing unplanned downtime caused by machine failure but also saves cost for unnecessary routine maintenance. This paper presents a systematic approach for real-time or near real-time machine performance monitoring solution development from data collection, feature extraction, data analytics to real-time machine fault and machine status classification. Three data-driven machine-learning approaches using one vibration sensor data are proposed to detect two common machine failure modes during machine turning process. To evaluate the the performance of each approach, three machine-learning algorithms (Random Forest, K Nearest Neighborhood, and Support Vector Machine) are implemented and tested. Evaluation results on the actual machine data shows that a two-layered classification structure with random forest algorithm as the base has high classification accuracy on the machine status including machine fault detection. The developed data-driven machine health monitoring solution is deployed in the IoT device for real-time data collection and processing and results are sent data server for data visualization.",https://ieeexplore.ieee.org/document/9115516/,2020 Prognostics and Health Management Conference (PHM-Besançon),4-7 May 2020,ieeexplore
,Motion planning with obstacle avoidance of an UR3 robot using charge system search,IEEE,Conferences,"For a cyber-physical system (CPS) of a future intelligent factory, a robotic manipulator is requested to co-work with human efficiently and safely in an environment with flexible arrangements. Therefore, an autonomous path planning of robotic manipulator is the most necessary issue to be resolved for the factory automation. For the robotic manipulator, optimizations and artificial intelligence (AI) methods are widely used to investigate the autonomous dynamic path-planning tasks with obstacle avoidance. Among these methods, the Rapidly Exploring Random Tree (RRT) algorithm has been widely used in path planning for a complex environment, because the RRT algorithm has the advantages of perfect expansion, probability completeness, and fast exploring speed. However, for some practical cases, the existing RRT algorithm may obtain a discontinuous solution of the angular trajectory. To solve the above problem, we studied a particle swarm optimization with the charge search system (CSS) to find the optimal path planning with obstacle avoidance. The steps of the proposed method are mentioned as follows: (1) establish the configuration space with the obstacle regions, (2) formulate the motion planning with obstacle using the CSS method and (3) use the PSO method to solve the path planning problem. Finally, the simulation of the path-planning task with obstacle avoidance is visually illustrated using the software RoboDK and the proposed method is implemented by the real-time experiments of the UR3 robot.",https://ieeexplore.ieee.org/document/8571928/,"2018 18th International Conference on Control, Automation and Systems (ICCAS)",17-20 Oct. 2018,ieeexplore
10.23919/DATE48585.2020.9116407,Network Synthesis for Industry 4.0,IEEE,Conferences,"Today's factory machines are ever more connected with SCADA, MES, ERP applications as well as external systems for data analysis. Different types of network architectures must be used for this purpose. For instance, control applications at the lowest level are susceptible to delays and errors while data analysis with machine learning procedures requires to move a large amount of data without real-time constraints. Standard data formats, like Automation Markup Language (AML), have been established to document factory environment, machine placement and network deployment, however, no automatic technique is currently available in the context of Industry 4.0 to choose the best mix of network architectures according to spacial constraints, cost, and performance. We propose to fill this gap by formulating an optimization problem. First of all, spatial and communication requirements are extracted from the AML description. Then, the optimal interconnection of wired or wireless channels is obtained according to application objectives. Finally, this result is back-annotated to AML to be used in the life cycle of the production system. The proposed methodology is described through a small, but complete, smart production plant.",https://ieeexplore.ieee.org/document/9116407/,"2020 Design, Automation & Test in Europe Conference & Exhibition (DATE)",9-13 March 2020,ieeexplore
10.1109/BigData.2017.8258105,On event-driven knowledge graph completion in digital factories,IEEE,Conferences,"Smart factories are equipped with machines that can sense their manufacturing environments, interact with each other, and control production processes. Smooth operation of such factories requires that the machines and engineering personnel that conduct their monitoring and diagnostics share a detailed common industrial knowledge about the factory, e.g., in the form of knowledge graphs. Creation and maintenance of such knowledge is expensive and requires automation. In this work we show how machine learning that is specifically tailored towards industrial applications can help in knowledge graph completion. In particular, we show how knowledge completion can benefit from event logs that are common in smart factories. We evaluate this on the knowledge graph from a real world-inspired smart factory with encouraging results.",https://ieeexplore.ieee.org/document/8258105/,2017 IEEE International Conference on Big Data (Big Data),11-14 Dec. 2017,ieeexplore
10.1109/ICITACEE.2016.7892481,On the implementation of ZFS (Zettabyte File System) storage system,IEEE,Conferences,"Digital data storage is very critical in computer systems. Storage devices used to store data may at any time suffer from damage caused by its lifetime, resource failure or factory defects. Such damage may lead to loss of important data. The risk of data loss in the event of device damage can be minimized by building a storage system that supports redundancy. The design of storage based on ZFS (Zettabyte File System) aims at building a storage system that supports redundancy and data integrity without requiring additional RAID controllers. When the system fails on one of its hard drive, the stored data remains secure and data integrity is kept assured. In addition to providing redundancy, the ZFS-based storage system also supports data compression for savings on storage space. The results show that the ZFS with LZ4 compression has the highest read and write speed. For real benchmark, there is no significant difference in reading speed for a variety of different variables, whereas a significant increase in speed occurs when writing compressible files on the ZFS system with compression configuration.",https://ieeexplore.ieee.org/document/7892481/,"2016 3rd International Conference on Information Technology, Computer, and Electrical Engineering (ICITACEE)",19-20 Oct. 2016,ieeexplore
10.1109/08IAS.2008.164,Real-Time Implementation of Intelligent Modeling and Control Techniques on a PLC Platform,IEEE,Conferences,"Programmable logic controllers (PLCs) have been used for many decades for standard control in industrial and factory environments. Over the years, PLCs have become computational efficient and powerful, and a robust platform with applications beyond the standard control and factory automation. Due to the new advanced PLC's features and computational power, they are ideal platforms for exploring advanced modeling and control methods, including computational intelligence based techniques such as neural networks, particle swarm optimization (PSO) and many others. Some of these techniques require fast floating-point calculations that are now possible in real-time on the PLC. This paper focuses on the Allen-Bradley ControlLogix brand of PLCs, due to their high performance and extensive use in industry. The design and implementation of a neurocontroller consisting of two neural networks, one for modeling and the other for control, and the training of these neural networks with particle swarm optimization is presented in this paper on a single PLC. The neurocontroller in this study is a power system stabilizer (PSS) that is used for power system oscillation damping. The PLC is interfaced to a power system simulated on the real time digital simulator. Real time results are presented showing that the PLC is a suitable hardware platform for implementing advanced modeling and control techniques for industrial applications.",https://ieeexplore.ieee.org/document/4658952/,2008 IEEE Industry Applications Society Annual Meeting,5-9 Oct. 2008,ieeexplore
10.1109/DEMPED.2019.8864828,Recognition of Electric Machines Boundary as The Constraint of Over Current Relay Coordination in Real Industrial Application with Serial Firefly Algorithm Optimization,IEEE,Conferences,"The electric machines such as motor, transformer, and generator plays an essential role in the production process of a factory. Due to expansion, there is an intention to invest a new electric machine that expected to work correctly with the existing system. To integrate the new electric machines, the existing protection system has to be evaluated carefully without limiting the machine's capability. Based on the actual experiences, one of the frustrating issues during the commissioning stage is energizing failure due to relay mal-trip. The objective of this paper is to summarize and model the electrical machines in the protection system perspective and offers a new method to coordinate the relays with the electric machines recognition using the artificial intelligence of serial firefly algorithm. This proposed method is endorsed by a real industrial power system and will demonstrate the ability to coordinate the relays without violating the electric machine boundary.",https://ieeexplore.ieee.org/document/8864828/,"2019 IEEE 12th International Symposium on Diagnostics for Electrical Machines, Power Electronics and Drives (SDEMPED)",27-30 Aug. 2019,ieeexplore
10.1109/ICET51757.2021.9450991,Research on Artificial Intelligence Industrial Big Data Platform for Industrial Internet Applications,IEEE,Conferences,"With the development of industry, enterprises have put forward higher requirements for real-time data collection and analysis. According to this, a distributed industrial big data platform for Industrial Internet applications is designed, which can effectively support the industrial field big data collection and storage. The system contains a distributed database, real-time database and offline big data platform to implement the collection and storage of all data in the system. Thus, the industrial big data platform architecture and related data hierarchical processing flow are proposed, which can effectively meet the requirements of the multi-source data and real-time processing of smart factory. The proposed data platform technical architecture will have important reference value for the realization of smart manufacturing and smart factory.",https://ieeexplore.ieee.org/document/9450991/,2021 IEEE 4th International Conference on Electronics Technology (ICET),7-10 May 2021,ieeexplore
10.1109/ROBIO.2011.6181679,"Robot self-preservation and adaptation to user preferences in game play, a preliminary study",IEEE,Conferences,"It is expected that in a near future, personal robots will be endowed with enough autonomy to function and live in an individual's home. This is while commercial robots are designed with default configuration and factory settings which may often be different to an individual's operating preferences. This paper presents how reinforcement learning is applied and utilised towards personalisation of a robot's behaviour. Two-level reinforcement learning has been implemented: first level is in charge of energy autonomy, i.e. how to survive, and second level is involved in adapting robot's behaviour to user's preferences. In both levels Q-learning algorithm has been applied. First level actions have been learnt in a simulated environment and then the results have been transferred to the real robot. Second level has been fully implemented in the real robot and learnt by human-robot interaction. Finally, experiments showing the performance of the system are presented.",https://ieeexplore.ieee.org/document/6181679/,2011 IEEE International Conference on Robotics and Biomimetics,7-11 Dec. 2011,ieeexplore
10.1109/IROS.2015.7354310,Robotic agents capable of natural and safe physical interaction with human co-workers,IEEE,Conferences,"Many future application scenarios of robotics envision robotic agents to be in close physical interaction with humans: On the factory floor, robotic agents shall support their human co-workers with the dull and health threatening parts of their jobs. In their homes, robotic agents shall enable people to stay independent, even if they have disabilities that require physical help in their daily life - a pressing need for our aging societies. A key requirement for such robotic agents is that they are safety-aware, that is, that they know when actions may hurt or threaten humans and actively refrain from performing them. Safe robot control systems are a current research focus in control theory. The control system designs, however, are a bit paranoid: programmers build “software fences” around people, effectively preventing physical interactions. To physically interact in a competent manner robotic agents have to reason about the task context, the human, and her intentions. In this paper, we propose to extend cognition-enabled robot control by introducing humans, physical interaction events, and safe movements as first class objects into the plan language. We show the power of the safety-aware control approach in a real-world scenario with a leading-edge autonomous manipulation platform. Finally, we share our experimental recordings through an online knowledge processing system, and invite the reader to explore the data with queries based on the concepts discussed in this paper.",https://ieeexplore.ieee.org/document/7354310/,2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),28 Sept.-2 Oct. 2015,ieeexplore
10.1109/eScience.2019.00014,SATVAM: Toward an IoT Cyber-Infrastructure for Low-Cost Urban Air Quality Monitoring,IEEE,Conferences,"Air pollution is a public health emergency in large cities. The availability of commodity sensors and the advent of Internet of Things (IoT) enable the deployment of a city-wide network of 1000's of low-cost real-time air quality monitors to help manage this challenge. This needs to be supported by an IoT cyber-infrastructure for reliable and scalable data acquisition from the edge to the Cloud. The low accuracy of such sensors also motivates the need for data-driven calibration models that can accurately predict the science variables from the raw sensor signals. Here, we offer our experiences with designing and deploying such an IoT software platform and calibration models, and validate it through a pilot field deployment at two mega-cities, Delhi and Mumbai. Our edge data service is able to even-out the differential bandwidths from the sensing devices and to the Cloud repository, and recover from transient failures. Our analytical models reduce the errors of the sensors from a best-case of 63% using the factory baseline to as low as 21%, and substantially advances the state-of-the-art in this domain.",https://ieeexplore.ieee.org/document/9041703/,2019 15th International Conference on eScience (eScience),24-27 Sept. 2019,ieeexplore
10.1109/I2MTC50364.2021.9460075,SNR-based Reinforcement Learning Rate Adaptation for Time Critical Wi-Fi Networks: Assessment through a Calibrated Simulator,IEEE,Conferences,"Nowadays, the Internet of Things is spreading in several different research fields, such as factory automation, instrumentation and measurement, and process control, where it is referred to as Industrial Internet of Things. In these scenarios, wireless communication represents a key aspect to guarantee the required pervasive connectivity required. In particular, Wi-Fi networks are revealing ever more attractive also in time- and mission-critical applications, such as distributed measurement systems. Also, the multi-rate support feature of Wi-Fi, which is implemented by rate adaptation (RA) algorithms, demonstrated its effectiveness to improve reliability and timeliness. In this paper, we propose an enhancement of RSIN, which is a RA algorithm specifically conceived for industrial real-time applications. The new algorithm starts from the assumption that an SNR measure has been demonstrated to be effective to perform RA, and bases on Reinforcement Learning techniques. In detail, we start from the design of the algorithm and its implementation on the OmNet++ simulator. Then, the simulation model is adequately calibrated exploiting the results of a measurement campaign, to reflect the channel behavior typical of industrial environments. Finally, we present the results of an extensive performance assessment that demonstrate the effectiveness of the proposed technique.",https://ieeexplore.ieee.org/document/9460075/,2021 IEEE International Instrumentation and Measurement Technology Conference (I2MTC),17-20 May 2021,ieeexplore
10.1109/IS3C50286.2020.00134,Screw defect detection system based on AI image recognition technology,IEEE,Conferences,"In the past ten years, smart manufacturing has been widely discussed and gradually introduced into various manufacturing fields. Since Germany proposed the concept of “Industry 4.0” in 2011, it has been spreading and feverish all over the world. For Industry 4.0, information digitization, intelligent defect detection and database platform management are their main core technologies. Aiming at a large screw industry manufacturing field in central and southern Taiwan, this paper proposes a screw defect detection system based on AI image recognition technology to detect damage to the nut during the “molding” process in the screw production process, and it is determined whether the inspected screw passes the inspection. The recognition result is given as shown in Figure 1. This paper uses 500 non-defective screw samples and 20 defective screw samples provided by the screw factory. The above samples collected real-time images through the sampling structure designed in this article, and we adopt Microsoft Corporation's ML.NET suite to model AI images, and uses the following four deep learning models: ResNetV2 50, ResNetV2 101, InceptionV3, MoblieNetV2 for learning; in the process of learning, this article divides the data set into three types of data sets (one is the unknown set that is not used for training but mixed with correct and defective samples, and the other is used for post-training verification of mixed samples with correct and defective samples. The third is a training set for training a mixture of correct and defective samples) This arrangement is used for subsequent verification models; after training, a PC-based screw defect detection system is implemented as shown in Figure 2; finally, with Detect screw defects in the form of instant photography. After the experiment, in 1,000 repeated tests, the success rate of defect detection reached 97%, while the false positive rate was only 2%.",https://ieeexplore.ieee.org/document/9394116/,"2020 International Symposium on Computer, Consumer and Control (IS3C)",13-16 Nov. 2020,ieeexplore
10.1109/ICCCYB.2005.1511567,Some interactive multimedia applications in production engineering,IEEE,Conferences,"The authors of this paper give a short summary on the research and development actions carried out during a Hungarian National Research and Development Program addressed as ""Digital Factory"". The project started in 2001 with the consortium consisting of three academic and two industrial partners, and has reached an end in 2004. This paper deals with one of the three main streams of R&amp;D activities carried out when paper outlines the goals of the tasks, related to interactive multimedia and tele-presence based industrial applications. The basic research phase of the work was followed by applied research and development, while the concluding phase enabled industrial applications and test scenarios. The authors explain some details of their achievements.",https://ieeexplore.ieee.org/document/1511567/,"IEEE 3rd International Conference on Computational Cybernetics, 2005. ICCC 2005.",13-16 April 2005,ieeexplore
10.1109/CSSE.2008.1161,The Research on Data Resources Sharing of Water Resources Domain Based on Proactive Service,IEEE,Conferences,"It is a key issue to the distributed and heterogeneous domain data resource sharing. In this paper a new mode of water resources data sharing is proposed from the point of proactive service. At first, a data sharing framework, called FDSWRDPS, is provided based on SOA and domain ontology. Then service factory is used for a design pattern of data integration subsystem. Furthermore proactive service is implemented based on event-trigger mechanism. Integrated domain data can be pushed to users automatically by using FDSWRDPS.",https://ieeexplore.ieee.org/document/4722706/,2008 International Conference on Computer Science and Software Engineering,12-14 Dec. 2008,ieeexplore
10.1109/AIVR.2018.00024,The Virtual Factory: Hologram-Enabled Control and Monitoring of Industrial IoT Devices,IEEE,Conferences,"Augmented reality (AR) has been exploited in manifold fields but is yet to be used at its full potential. With the massive diffusion of smart devices, opportunities to build immersive human-computer interfaces are continually expanding. In this study, we conceptualize a virtual factory: an interactive, dynamic, holographic abstraction of the physical machines deployed in a factory. Through our prototype implementation, we conducted a user-study driven evaluation of holographic interfaces compared to traditional interfaces, highlighting its pros and cons. Our study shows that the majority of the participants found holographic manipulation more attractive and natural to interact with. However, current performance characteristics of head-mounted displays must be improved to be applied in production.",https://ieeexplore.ieee.org/document/8613643/,2018 IEEE International Conference on Artificial Intelligence and Virtual Reality (AIVR),10-12 Dec. 2018,ieeexplore
10.1109/ROBOT.1990.126044,Towards a real-time architecture for obstacle avoidance and path planning in mobile robots,IEEE,Conferences,"The design and partial implementation of a real-time architecture for a mobile robot, aimed particularly towards a vehicle developed for factory automation, is described. The authors develop a layered design to equip the robot with a number of behavioral competences. They examine sensing and a potential field algorithm especially to achieve modification of behavior at a speed close to the robot's operational speed. It is shown how the layered architecture interfaces to the original onboard architecture, which provided sophisticated localization but no ability to deal with environmental exceptions.&lt;<ETX>&gt;</ETX>",https://ieeexplore.ieee.org/document/126044/,"Proceedings., IEEE International Conference on Robotics and Automation",13-18 May 1990,ieeexplore
10.1109/ICSTCC.2019.8885611,Visual Analytics Framework for Condition Monitoring in Cyber-Physical Systems,IEEE,Conferences,"One of the biggest challenges facing the factory of the future today is to reduce the time-to-market access and increase through the improvement of competitiveness and efficiency. In order to achieve this target, data analytics in Industrial Cyber-Physical System becomes a feasible option. In this paper, a visual analytics framework for condition monitoring of the machine tool is presented with the aim to manage events and alarms at factory level. The framework is assessed in a particular use case that consists in a multi-threaded cloud-based solution for the global analysis of the behaviour of variables acquired from PLC, CNC and robot manipulator. A human-machine interface is also designed for the real-time visualization of the key performance indicators according to the user's criteria. This tool implemented is a great solution for condition monitoring and decision-making process based on data analytics from simple statistics to complex machine learning methods. The results achieved are part of the vision and implementation of the industrial test bed of “Industry and Society 5.0” platform.",https://ieeexplore.ieee.org/document/8885611/,"2019 23rd International Conference on System Theory, Control and Computing (ICSTCC)",9-11 Oct. 2019,ieeexplore
10.1109/ACCESS.2021.3061477,A Cloud-Based Platform for Big Data-Driven CPS Modeling of Robots,IEEE,Journals,"This paper proposes an improved cyber-physical systems (CPS) architecture for a smart robotic factory based on an industrial cloud platform driven by big data based on the traditional CPS architecture. This paper uses the architecture analysis and design language to model and design a total of three scales for the underlying cell-level robot, the system-level robot shop, and the overall robotic smart factory CPS, respectively, to complete the conceptual scheme for building a robotic smart factory from a local to an overall CPS system. Using the advantages of cloud computing and combining robotic CPS with cloud computing, an architecture for an industrial management system for CPS cloud computing is proposed. Base based distributed storage architecture with Storm based distributed real-time processing architecture. In terms of modeling, the advantages and disadvantages of using AADL, structural analysis, and design language, and modelers, a physical device modeling language, are combined to analyze the advantages and disadvantages of architecture analysis &amp; design language (AADL) for modeling CPS and propose a CPS analysis and design based on AADL and applicable to it. The paper also investigates the use of LeNet models for state identification in the HSV color space. The algorithm was verified on a self-built power equipment indicator dataset with a 100% detection rate and 99.8% state recognition accuracy after four consecutive frames of fusion detection. Simulink simulation of the trolley was carried out in terms of a cell-level robotic trolley CPS system to demonstrate the effectiveness of the design of a robotic CPS system driven by soaring data based on the industrial cloud platform proposed in this paper.",https://ieeexplore.ieee.org/document/9360827/,IEEE Access,2021,ieeexplore
10.1109/TIM.2019.2962565,A Lightweight Appearance Quality Assessment System Based on Parallel Deep Learning for Painted Car Body,IEEE,Journals,"The appearance quality assessment based on defect inspection for painted car-body surfaces is an essential work to monitor and analyze the level of paint appearance quality. In the industrial application, there are some challenges, such as the huge and stereo skeleton of car bodies, a variety of irregular local surface areas, low visibility of defects due to tiny real size, and specular car-body surface. To overcome these problems, a lightweight online appearance quality assessment system (OAQAS) based on parallel deep learning is proposed, it includes two parts: 1) a vision inspection subsystem with distributed multi-camera image acquisition module and 2) an appearance quality evaluation subsystem (AQES) based on parallel TinyDefectRNet for evaluating the proposed painted surface grinding difficulty criteria. TinyDefectRNet is able to inspect relatively accurate defect size, although it is trained on a coarsely annotated data set. The OAQAS is implemented in an actual painting production line of a car factory, and the application results show that our OAQAS is far superior to the manual inspection in evaluation accuracy and time consumption. Moreover, our system is lightweight so that it is easy to be plugged into existing painting production lines without rebuilding or changing the inspection room.",https://ieeexplore.ieee.org/document/8964458/,IEEE Transactions on Instrumentation and Measurement,Aug. 2020,ieeexplore
10.1109/ACCESS.2017.2770180,An Integrated Industrial Ethernet Solution for the Implementation of Smart Factory,IEEE,Journals,"Smart factory addresses the vertical integration of physical entities and information systems. Network and cloud are two essential infrastructures to achieve this goal. Among them, the network provides interconnection for communication and data flow, while the cloud provides powerful and elastic computing and storage abilities for big data and intelligent applications. This paper presents a cloud-centric framework for the implementation of the smart factory. Based on this framework, three high leveled protocols viz., EtherCAT, DDS, and OPC UA are selected to implement machine network (controller to sensors/actuators communication), machine to machine communication, and machine to cloud communication respectively, to satisfy diverse communication requirements of the smart factory. An integrated architecture for combining ontology knowledge and semantic data to support intelligent applications is also proposed. These network and data process schemes are verified in a smart factory prototype for personalized candy packing application.",https://ieeexplore.ieee.org/document/8096995/,IEEE Access,2017,ieeexplore
10.1109/TCPMT.2020.3047089,Automatic Industry PCB Board DIP Process Defect Detection System Based on Deep Ensemble Self-Adaption Method,IEEE,Journals,"A deep ensemble convolutional neural network (CNN) model to inspect printed circuit board (PCB) board dual in-line package (DIP) soldering defects with Hybrid-YOLOv2 (YOLOv2 as a foreground detector and ResNet-101 as a classifier) and Faster RCNN with ResNet-101 and Feature Pyramid Network (FPN) (FRRF) achieved a detection rate of 97.45% and a false alarm rate (FAR) of 20%-30% in the previous study [34]. However, applying the method to other production lines, environmental variations, such as lighting, orientations of the sample feeds, and mechanical deviations, led to the degradation in detection performance. This article proposes an effective self-adaption method that collects “exception data” like the samples with which the Artificial Intelligent (AI) model made mistakes from the automated optical inspection inference edge to the training server, retraining with exceptions on the server and deploying back to the edge. The proposed defect detection system has been verified with real tests that achieved a detection rate of 99.99% with an FAR 20%-30% and less than 15 s of inspection time on a resolution $7296 \times 6000$ PCB image. The proposed system has proven capable of shortening inspection and repair time for online operators, where a 33% efficiency boost from the three production lines of the collaborated factory has been reported [6]. The contribution of the proposed retraining mechanism is threefold: 1) because the retraining process directly learns from the exceptions, the model can quickly adapt to the characteristic of each production line, leading to a fast and reliable mass deployment; 2) the proposed retraining mechanism is a necessary self-service for conventional users as it incrementally improves the detection performance without professional guidance or fine-tuning; and 3) the semiautomatic exception data collection method helps to reduce the time-consuming manual labeling during the retraining process.",https://ieeexplore.ieee.org/document/9306873/,"IEEE Transactions on Components, Packaging and Manufacturing Technology",Feb. 2021,ieeexplore
10.1109/TII.2019.2937876,Federated Tensor Mining for Secure Industrial Internet of Things,IEEE,Journals,"In a vertical industry alliance, Internet of Things (IoT) deployed in different smart factories are similar. For example, most automobile manufacturers have the similar assembly lines and IoT surveillance systems. It is common to observe the industrial knowledge using deep learning and data mining methods based on the IoT data. However, some knowledge is not easy to be mined from only one factory's data because the samples are still few. If multiple factories within an alliance can gather their data together, more knowledge could be mined. However, the key concern of these factories is the data security. Existing matrix-based methods can guarantee the data security inside a factory but do not allow the data sharing among factories, and thus their mining performance is poor due to lack of correlation. To address this concern, in this article we propose the novel federated tensor mining (FTM) framework to federate multisource data together for tensor-based mining while guaranteeing the security. The key contribution of FTM is that every factory only needs to share its ciphertext data for security issue, and these ciphertexts are adequate for tensor-based knowledge mining due to its homomorphic attribution. Real-data-driven simulations demonstrate that FTM not only mines the same knowledge compared with the plaintext mining, but also is enabled to defend the attacks from distributed eavesdroppers and centralized hackers. In our typical experiment, compared with the matrix-based privacy-preserving compressive sensing (PPCS), FTM increases up to 24% on mining accuracy.",https://ieeexplore.ieee.org/document/8815886/,IEEE Transactions on Industrial Informatics,March 2020,ieeexplore
10.1109/ACCESS.2018.2809681,Industrial Big Data Analytics for Prediction of Remaining Useful Life Based on Deep Learning,IEEE,Journals,"Due to the recent development of cyber-physical systems, big data, cloud computing, and industrial wireless networks, a new era of industrial big data is introduced. Deep learning, which brought a revolutionary change in computer vision, natural language processing, and a variety of other applications, has significant potential for solutions providing in sophisticated industrial applications. In this paper, a concept of device electrocardiogram (DECG) is presented, and an algorithm based on deep denoising autoencoder (DDA) and regression operation is proposed for the prediction of the remaining useful life of industrial equipment. First, the concept of electrocardiogram is explained. Then, a problem statement based on manufacturing scenario is presented. Subsequently, the architecture of the proposed algorithm called integrated DDA and the algorithm workflow are provided. Moreover, DECG is compared with traditional factory information system, and the feasibility and effectiveness of the proposed algorithm are validated experimentally. The proposed concept and algorithm combine typical industrial scenario and advance artificial intelligence, which has great potential to accelerate the implementation of industry 4.0.",https://ieeexplore.ieee.org/document/8302913/,IEEE Access,2018,ieeexplore
10.1109/ACCESS.2020.3046784,Reinforcement Learning With Composite Rewards for Production Scheduling in a Smart Factory,IEEE,Journals,"Rapid advances of sensing and cloud technologies transform the manufacturing system into a data-rich environment and make production scheduling increasingly complex. Traditional offline scheduling methods are limited in the ability to handle low-volume-high-mix workorders with diverse design specifications. Simulation-based methods show the promise for distributed scheduling of manufacturing jobs but are mostly implemented with historical data and empirical rules in a static manner. Recently, artificial intelligence (AI) algorithms fuel increasing interests to solve dynamic scheduling problems in the manufacturing setting. However, it's difficult to utilize high-dimensional data for production scheduling while considering multiple practical objectives for smart manufacturing (e.g., minimize the makespan, reduce production costs, balance workloads). Therefore, this paper presents a new AI scheduler with composite reward functions for data-driven dynamic scheduling of manufacturing jobs under uncertainty in a smart factory. Internet-enabled sensor networks are deployed in the smart factory to track real-time statuses of workorders, machines, and material handling systems. A novel manufacturing value network is developed to take high-dimensional data as the input and then learn the state-action values for real-time decision making. Based on reinforcement learning (RL), composite rewards help the AI scheduler learn efficiently to achieve multiple objectives for production scheduling in real time. The proposed methodology is evaluated and validated with experimental studies in a smart manufacturing setting. Experimental results show that the new AI scheduler not only improves the multi-objective performance metrics in the production scheduling problem but also effectively copes with unexpected events (e.g., urgent workorders, machine failures) in manufacturing systems.",https://ieeexplore.ieee.org/document/9305707/,IEEE Access,2021,ieeexplore
10.1109/ACCESS.2021.3050484,Research on Detecting Bearing-Cover Defects Based on Improved YOLOv3,IEEE,Journals,"Detecting defects, which is a branch of target detection in the field of computer vision, is widely used in factory production. To solve the problems in existing detection algorithms that relate to their insensitivity to large or medium defect targets on bearing covers, their difficulty in detecting subtle defects effectively and their lack of real-time detection, in this work, we establish a large-scale bearing-cover defect dataset and propose an improved YOLOv3 network model. The proposed model is divided into four submodels: the bottleneck attention network (BNA-Net), the attention prediction subnet model, the defect localization subnet model, and the large-size output feature branch. To test the generality, robustness and practicability of the new model, we design a comparative experiment under abnormal illumination conditions. We design an ablation experiment to verify the validity of the proposed submodules. The experimental results show that our model solves the problem of the YOLOv3 algorithm's insensitivity to medium or large targets and satisfies real-time detection conditions. The mAP result is 69.74%, which is 16.31%, 13.4%, 13%, 10.9%, and 7.2% more than that of YOLOv3, EfficientDet-D2, YOLOv5, YOLOv4, and PP-YOLO, respectively.",https://ieeexplore.ieee.org/document/9319181/,IEEE Access,2021,ieeexplore
10.1109/TCYB.2016.2587673,Road Disturbance Estimation and Cloud-Aided Comfort-Based Route Planning,IEEE,Journals,"This paper investigates a comfort-based route planner that considers both travel time and ride comfort. We first present a framework of simultaneous road profile estimation and anomaly detection with commonly available vehicle sensors. A jump-diffusion process-based state estimator is developed and used along with a multi-input observer for road profile estimation. The estimation framework is evaluated in an experimental test vehicle and promising performance is demonstrated. Second, three objective comfort metrics are developed based on factors such as travel time, road roughness, road anomaly, and intersection. A comfort-based route planning problem is then formulated with these metrics and an extended Dijkstra's algorithm is exploited to solve the problem. A cloud-based implementation of our comfort-based route planning approach is proposed to facilitate information access and fast computation. Finally, a real-world case study, comfort-based route planning from Ford Research and Innovation Center, Michigan to Ford Rouge Factory Tour, Michigan, is presented to illustrate the efficacy of the proposed route planning framework.",https://ieeexplore.ieee.org/document/7514928/,IEEE Transactions on Cybernetics,Nov. 2017,ieeexplore
,The application of simulation technique in whole CIMS life cycle,BIAI,Journals,"The life cycle of CIMS (Computer Integrated Manufacturing System) includes four phases: requirement analyzing, designing, implementation and running. For reducing the risk of investment and achieving better economic results the simulation technique is needed in the above four phases of CIMS life cycle. Under the support of China 863 / CIMS plan a series of simulation projects are established. Some of them are finished with succeed and have been used in application. In this paper four simulation projects are introduced.(1) The Integrated Manufacturing Simulation Software (IMSS). It is an integrated platform, based on the discrete event simulation principle. It can be used to analyze and design CIMS, especially FMS, and evaluate the daily production plan.(2) The Advanced Hierarchical Control System Emulator (AHCSE), a software system, based on the finite state machine principle. It can be used to analyze and design of CIMS hierarchical control system, and check expanded system performance before expanding.(3) The Factory Scheduling Environment (FASE), a software system based on the discrete event simulation principle and artificial intelligence technology. It can be used for shop floor scheduling.(4) The Machining Process Simulator (MPS). It can simulate the machining process of machining center by computer. It can check the correctness of NC code (including interference and confliction) and replace the real machining center to support the simulation environment for shop floor scheduling and controlling. There are three companies and universities joining in these four projects, they are: Tsinghua University, Huazhong University of Technology, Beijing Institute of Computer Application and Simulation.",https://ieeexplore.ieee.org/document/6074468/,Journal of Systems Engineering and Electronics,Sept. 1994,ieeexplore
10.1109/ACCESS.2021.3082934,Visual Product Tracking System Using Siamese Neural Networks,IEEE,Journals,"Management of unstructured production data is a key challenge for Industry 4.0. Effective product tracking endorses data integration and productivity improvements throughout the manufacturing processes. Radio-frequency identification (RFID) tags are used in many tracking cases, but in some manufacturing environments, those cannot be used as they might get damaged or removed during processing. In this paper, we propose an alternative visual product tracking system. The physical system uses two cameras placed at the two ends of the tracked process(es). Product pairs are then matched with a Siamese neural network operating on the product images and trained offline on the problem at hand with labeled data. The proposed system can track products solely based on their visual appearance and without any physical interference with the products or production processes. Unlike other existing image-based methods, the proposed system is invariant to major positional and visual changes in the products. As a proof-of-concept, we tested the proposed system with real plywood factory data and were able to track the products with 98.5 % accuracy in a realistic test scenario. The implementation of the proposed method and the Veneer21 dataset are publicly available at https://github.com/TuomasJalonen/visual-product-tracking-system.",https://ieeexplore.ieee.org/document/9439511/,IEEE Access,2021,ieeexplore
10.1109/IECON.2018.8591171,A Case Study on Knowledge Driven Code Generation for Software-Defined Industrial Cyber-Physical Systems,IEEE,Conferences,"Industrial Cyber-Physical Systems (iCPS) enables coordination between various subsystems and devices based on real-time feedback data from sensors. iCPS must react rapidly to new requirements and adjust itself to fulfill new functionalities in no time. On the software side, control programs of iCPS need to be reconfigured dynamically. An efficient way for massive reconfiguration is automatic code generation. In this paper, a knowledge-driven code generation method is experimented for software-defined iCPS. Based on sensor values, actuators are controlled by the reasoning process with support of ontological knowledge base. The results demonstrate that iCPS could be driven by rules completely without programming control software.",https://ieeexplore.ieee.org/document/8591171/,IECON 2018 - 44th Annual Conference of the IEEE Industrial Electronics Society,21-23 Oct. 2018,ieeexplore
10.1109/ACCC51160.2020.9347897,A Comparative Analysis of Kinematics of Industrial Robot KUKA KR 60–3 Using Scientific Computing Languages,IEEE,Conferences,"In the field of robotics, there are kinematic analysis methods that are responsible for describing the positions and orientations of the end effectors, as well as the angles, velocities and trajectories of industrial robots; such techniques are: forward kinematics, inverse kinematics and velocity kinematics. For the solutions of these complex mathematical calculations, the use of scientific computing languages or programs is required; which more and more algorithms, libraries and complements are implemented, that achieve a reduction in programming hours and result in the creation of better solutions in areas of all kinds. For this reason, the kinematics of the Industrial Robot KUKA KR 60-3 was programmed in the languages and programs most used in scientific computing, with the aim of comparing the performance (real time) when carrying out symbolic and numerical analysis in said studies.",https://ieeexplore.ieee.org/document/9347897/,2020 Asia Conference on Computers and Communications (ACCC),18-20 Sept. 2020,ieeexplore
10.1109/CYBER53097.2021.9588142,A Fast and Energy-Saving Neural Network Inference Method for Fault Diagnosis of Industrial Equipment Based on Edge-End Collaboration,IEEE,Conferences,"Data-driven fault diagnosis algorithms represented by deep learning have been widely used in industrial equipment fault diagnosis. However, the lack of real-time performance has always restricted the development of such methods. With the development of edge computing, many edge and end computing devices are deployed in industrial environments. For this distributed computing environment, we propose a distributed neural network inference method with edge-end collaboration. This method uses an edge server to cooperate with multiple end devices for network inference. In the diagnosis of industrial equipment, it can increase the speed of inference, reduce the traffic of the edge network, and help the application of deep neural networks in industrial environments.",https://ieeexplore.ieee.org/document/9588142/,"2021 IEEE 11th Annual International Conference on CYBER Technology in Automation, Control, and Intelligent Systems (CYBER)",27-31 July 2021,ieeexplore
10.1109/CAMAD50429.2020.9209305,A Joint Decentralized Federated Learning and Communications Framework for Industrial Networks,IEEE,Conferences,"Industrial wireless networks are pushing towards distributed architectures moving beyond traditional server-client transactions. Paired with this trend, new synergies are emerging among sensing, communications and Machine Learning (ML) co-design, where resources need to be distributed across different wireless field devices, acting as both data producers and learners. Considering this landscape, Federated Learning (FL) solutions are suitable for training a ML model in distributed systems. In particular, decentralized FL policies target scenarios where learning operations must be implemented collaboratively, without relying on the server, and by exchanging model parameters updates rather than training data over capacity-constrained radio links. This paper proposes a real-time framework for the analysis of decentralized FL systems running on top of industrial wireless networks rooted in the popular Time Slotted Channel Hopping (TSCH) radio interface of the IEEE 802.15.4e standard. The proposed framework is suitable for neural networks trained via distributed Stochastic Gradient Descent (SGD), it quantifies the effects of model pruning, sparsification and quantization, as well as physical and link layer constraints, on FL convergence time and learning loss. The goal is to set the fundamentals for comprehensive methods and procedures supporting decentralized FL pre-deployment design. The proposed tool can be thus used to optimize the deployment of the wireless network and the ML model before its actual installation. It has been verified based on real data targeting smart robotic-assisted manufacturing.",https://ieeexplore.ieee.org/document/9209305/,2020 IEEE 25th International Workshop on Computer Aided Modeling and Design of Communication Links and Networks (CAMAD),14-16 Sept. 2020,ieeexplore
10.1109/COMPSAC48688.2020.0-202,A Modular Edge-/Cloud-Solution for Automated Error Detection of Industrial Hairpin Weldings using Convolutional Neural Networks,IEEE,Conferences,"The traction battery and the electric motor are the most important components of the electrified powertrain. To increase the energy efficiency of the electric motor, wound copper wires are being replaced by coated rectangular copper wires, so-called hairpins. Hence, to connect the hairpins conductively, they must be welded together. However, such new production processes are unknown compared with classic motor production. Therefore, this research aims to integrate Industry 4.0 techniques, such as cloud and edge computing, and advanced data analysis in the production process to better understand and optimize the manufacturing processes. Welding defects are classified with the help of a convolutional neural network (CNN) (predictive analysis) and, depending on the defect, a recommended course of action for reworking (prescriptive analysis) is given. However, the application of such complex algorithms as neural networks to large amounts of data requires huge computing resources. Therefore, a modular combination of an edge and cloud architecture is proposed in this paper. Furthermore, a pure cloud solution is compared with the edge solution.",https://ieeexplore.ieee.org/document/9202655/,"2020 IEEE 44th Annual Computers, Software, and Applications Conference (COMPSAC)",13-17 July 2020,ieeexplore
10.1109/ICICRS46726.2019.9555893,A Novel Approach Towards Industrial Waste Management Using Q-Learning,IEEE,Conferences,"Industrial waste refers to the unwanted solid, liquid and gaseous wastes resulting from an industrial operation. And the collection, transport, processing, disposal and monitoring of these wastes is called as Industrial Waste Management. Industrial Waste has adhered to severe pollution to air, water and soil in the recent years affecting environment and human health. So, its proper and effective management has become important also as the industries’ liability towards the environment. Since many industries don’t have in-house processing plants, they tie-up with industries for the task. This paper focuses on using Q-learning for effective path-planning from generator industries(waste generating) to aggregator industries(waste processing). Q-learning is used for predicting the most efficient path by learning from its own experiences of rewards and penalties and so we don’t need to train the model which therefore increases the efficiency. This work can be implemented at a place where industrial waste is generated and can be very helpful in managing the same. This could be implemented as a service and handed over to the probable customers or government organizations which provide waste management services to the industries. This submission is a step towards automation of the Industrial Waste Management and helps in planning of waste management in real-time.",https://ieeexplore.ieee.org/document/9555893/,2019 International Conference on Intelligent Computing and Remote Sensing (ICICRS),19-20 July 2019,ieeexplore
10.1109/MCDM.2007.369428,A Review of Two Industrial Deployments of Multi-criteria Decision-making Systems at General Electric,IEEE,Conferences,"Two industrial deployments of multi-criteria decision-making systems at General Electric are reviewed from the perspective of their multi-criteria decision-making component similarities and differences. The motivation is to present a framework for multi-criteria decision-making system development and deployment. The first deployment is a financial portfolio management system that integrates hybrid multi-objective optimization and interactive Pareto frontier decision-making techniques to optimally allocate financial assets while considering multiple measures of return and risk, and numerous regulatory constraints. The second deployment is a power plant management system that integrates predictive modeling based on neural networks, optimization based on multi-objective evolutionary algorithms, and automated decision-making based on Pareto frontier techniques. The integrated approach, embedded in a real-time plant optimization and control software environment dynamically optimizes emissions and efficiency while simultaneously meeting load demands and other operational constraints in a complex real-world power plant",https://ieeexplore.ieee.org/document/4222994/,2007 IEEE Symposium on Computational Intelligence in Multi-Criteria Decision-Making,1-5 April 2007,ieeexplore
10.1109/SMARTCOMP52413.2021.00051,A Smart System for Personal Protective Equipment Detection in Industrial Environments Based on Deep Learning,IEEE,Conferences,"The adoption of real-time object detection systems via video streaming analysis is currently exploited in several contexts, from security monitoring to safety prevention. In industrial environments, proper usage of Personal Protective Equipment (PPE) is paramount to ensure workers’ safety. However, the use of some types of PPE, such as helmets, is often neglected by workers, especially in indoor areas. Thus, in order to reduce the risks of accidents, real-time video streaming-based monitoring systems may be used to monitor areas in which workers operate and alert them not to wear PPEs via acoustic alarms or visual signals. In case of a remote analysis, there are potential issues related to the high rate of data streams to be transported and analyzed and workers’ privacy. In this work, we propose an embedded smart system for real-time PPE detection based on video streaming analysis and deep learning models. We discuss the deployment of different versions of the YOLOv4 network fine-tuned using a public PPE dataset. In the end, we assess the performance of the proposed system in terms of accuracy and latency and of the overall PPE detection procedure.",https://ieeexplore.ieee.org/document/9556246/,2021 IEEE International Conference on Smart Computing (SMARTCOMP),23-27 Aug. 2021,ieeexplore
10.1109/PTC.2019.8810705,A Smart Voltage Optimization Approach for Industrial Load Demand Response,IEEE,Conferences,"This paper proposes a generic and comprehensive Voltage Optimization (VO) strategy for energy savings by industrial customers, to lower operating expenses through the implementation of an optimal process-based Demand Response (DR) program without affecting the real-time manufacturing process. This strategy takes into account the complex nature of industrial loads and their unique set of operating constraints, to reduce energy demand for industrial customers by means of varying the voltage at the utility service entrance to the plant. The proposed approach utilizes a Neural Network (NN) model of the industrial load, trained using historical operating data, to estimate the real power consumption of the load, based on the bus voltage and overall plant process. The NN load model is incorporated into the proposed VO model, whose objective is the minimization of the energy drawn from the substation and the number of switching operations of Load Tap Changers (LTC). The proposed VO framework is tested on a real plant model developed using actual measured data. The results demonstrate that the proposed technique can be successfully implemented by industrial customers and plant operators to enhance energy savings compared to Conservation Voltage Reduction (CVR) approaches, and also as a DR strategy that effectively manages the dependence of industrial loads on time-sensitive and critical manufacturing processes.",https://ieeexplore.ieee.org/document/8810705/,2019 IEEE Milan PowerTech,23-27 June 2019,ieeexplore
10.1109/SSCI.2017.8280935,A benchmark environment motivated by industrial control problems,IEEE,Conferences,"In the research area of reinforcement learning (RL), frequently novel and promising methods are developed and introduced to the RL community. However, although many researchers are keen to apply their methods on real-world problems, implementing such methods in real industry environments often is a frustrating and tedious process. Generally, academic research groups have only limited access to real industrial data and applications. For this reason, new methods are usually developed, evaluated and compared by using artificial software benchmarks. On one hand, these benchmarks are designed to provide interpretable RL training scenarios and detailed insight into the learning process of the method on hand. On the other hand, they usually do not share much similarity with industrial real-world applications. For this reason we used our industry experience to design a benchmark which bridges the gap between freely available, documented, and motivated artificial benchmarks and properties of real industrial problems. The resulting industrial benchmark (IB) has been made publicly available to the RL community by publishing its Java and Python code, including an OpenAI Gym wrapper, on Github. In this paper we motivate and describe in detail the IB's dynamics and identify prototypic experimental settings that capture common situations in real-world industry control problems.",https://ieeexplore.ieee.org/document/8280935/,2017 IEEE Symposium Series on Computational Intelligence (SSCI),27 Nov.-1 Dec. 2017,ieeexplore
10.1109/ICBAIE52039.2021.9390053,A case study of industrial data analysis: Gearbox temperature prediction of wind turbines using ensemble deep learning regression,IEEE,Conferences,"The damage and effectiveness of gearboxes may directly lead to the shutdown of wind turbines (WTs) and increase the unplanned maintenance cost. The variation and distribution of gearbox temperature can reflect its working condition. The predicting gearbox temperature has emerged as an essential method for guaranteeing the safety, availability, and efficiency of WTs. A method called ensemble deep learning regression (EDLR) is proposed for gearbox temperature prediction. Using random forest method, the collected features are then selected as main impacting features. Long short-term memory is sequentially employed for the temperature prediction. The proposed approach is investigated with real data from a wind farm. The experimental results reveal that EDLR has excellent regression performance in terms of the adopted criteria and that it can accurately predict the gearbox temperature.",https://ieeexplore.ieee.org/document/9390053/,"2021 IEEE 2nd International Conference on Big Data, Artificial Intelligence and Internet of Things Engineering (ICBAIE)",26-28 March 2021,ieeexplore
10.1109/INDIN.2013.6622897,A genetic algorithm for optimizing vector-based paths of industrial manipulators,IEEE,Conferences,"Nowadays there is a vast amount of IT tools specialized in vector graphics. The data generated by those tools could be used to describe the path of industrial manipulators as a set of vectors. The main problem is that the sequence/direction of those vectors is not meant to be executed by a robot and attempting to do it, would result in inefficient cycle times of the robot. Therefore it is necessary to generate an execution plan that minimizes the cost of carrying out the vector-based path. The number of possible execution actions has a factorial growth and it is unfeasible to evaluate each of them. This paper proposes the use of a genetic algorithm to optimize this task. The main contribution of this work is a chromosome encoding structure and modifications to the Partially Mapped Crossover operator in order to comply with the constraints of this optimization problem. The algorithm was implemented and tested in a real industrial manipulator.",https://ieeexplore.ieee.org/document/6622897/,2013 11th IEEE International Conference on Industrial Informatics (INDIN),29-31 July 2013,ieeexplore
10.1109/DTPI52967.2021.9540189,A parallel intelligent control system and its industrial application,IEEE,Conferences,"Parallel control refers to the parallel interaction between the actual physical process and the manual calculation process. The ACP method under its theoretical framework includes artificial system, calculation experiment and parallel control. This paper presents a parallel intelligent control system implementation method, parallel control system needs a carrier, including hardware platform and software system, based on this carrier, the system completes artificial intelligence modeling and real-time optimal control. Firstly, the structure of parallel control platform is introduced, which is composed of industrial control computer, server and power supply, The program function of server is the core part of parallel control system;Secondly. The architecture of parallel intelligent control system is given, The artificial system is designed as an object modeling system, Industrial control computer output excitation signal, The server collects the response data and completes the modeling;The calculation experiment is designed as a process of human-computer interaction, which helps to realize control quality judgment and parameter setting;The parallel control is realized by the industrial controller, and the optimal parameters or control algorithm are put into the controller in parallel to realize the real-time control. Finally, an industrial application example is given to prove the effectiveness of this method.",https://ieeexplore.ieee.org/document/9540189/,2021 IEEE 1st International Conference on Digital Twins and Parallel Intelligence (DTPI),15 July-15 Aug. 2021,ieeexplore
10.1109/ICIAS.2007.4658557,A study on industrial communication networking: Ethernet based implementation,IEEE,Conferences,"Recent enhancement of the industrial communications and networking are possible to apply in Ethernet networks system at all levels of industrial automation, especially in the controller level whereby the data exchanges in real-time communication is mandatory. This paper is about a study on the development of industrial communications network based on the Ethernet protocol and thus implement it into computer integrated manufacturing (CIM) system. The purpose of this paper is to overcome real-time communication in which the accessibility of data exchange is very difficult in terms of retrieving data from other stations and time consuming. The Ethernet module is installed onto supervisory OMRON PLC to integrate several of stations in the CIM-70A system which is located at Robotic Laboratory in Universiti Tun Hussein Onn Malaysia (UTHM). The workability of this communication technique is analyzed and compared with the conventional serial communication which widely used in automation networking systems. It is found that, the Ethernet protocol approach through the communication and integration of CIM system can be accessed easily and available to be upgraded at the management and enterprise levels of industrial automation system.",https://ieeexplore.ieee.org/document/4658557/,2007 International Conference on Intelligent and Advanced Systems,25-28 Nov. 2007,ieeexplore
10.1109/INFOC.2017.8001669,A study on the fast system recovery: Selecting the number of surrogate nodes for fast recovery in industrial IoT environment,IEEE,Conferences,This paper is based on the previous research that selects the proper surrogate nodes for fast recovery mechanism in industrial IoT (Internet of Things) Environment which uses a variety of sensors to collect the data and exchange the collected data in real-time for creating added value. We are going to suggest the way that how to decide the number of surrogate node automatically in different deployed industrial IoT Environment so that minimize the system recovery time when the central server likes IoT gateway is in failure. We are going to use the network simulator to measure the recovery time depending on the number of the selected surrogate nodes according to the sub-devices which are connected to the IoT gateway.,https://ieeexplore.ieee.org/document/8001669/,2017 International Conference on Information and Communications (ICIC),26-28 June 2017,ieeexplore
10.1109/TAI.1990.130374,A tool system for knowledge-based on-line diagnosis in industrial automation,IEEE,Conferences,"A tool system which supports the development of expert systems for use in the field of industrial automation is presented. The knowledge representation language provided by the tool system supports different aspects of knowledge which are important in this domain: the representation of declarative and procedural knowledge; the representation of temporal knowledge; and the representation of knowledge about real-time behavior. As a further part of the tool system, an expert system compiler with the ability to transform expert systems into a conventional procedural program (realized in C) is described. This method allows a simple integration of knowledge-based techniques into conventional software systems.&lt;<ETX>&gt;</ETX>",https://ieeexplore.ieee.org/document/130374/,[1990] Proceedings of the 2nd International IEEE Conference on Tools for Artificial Intelligence,6-9 Nov. 1990,ieeexplore
10.1109/MetroInd4.0IoT51437.2021.9488498,Adaptive LoRaWAN Transmission exploiting Reinforcement Learning: the Industrial Case,IEEE,Conferences,"Wireless technologies play a key role in the Industrial Internet of Things (IIoT) scenario, for the development of increasingly flexible and interconnected factory systems. A significant opportunity in this context is represented by the advent of Low Power Wide Area Network (LPWAN) wireless technologies, that enable a reliable, secure, and effective transmission of measurement data over long communication ranges and with very low power consumption. Nevertheless, reliability in harsh environments (as typically occurs in the industrial scenario) is a significant issue to deal with. Focusing on LoRaWAN, adaptive strategies can be profitably devised concerning the above tradeoff. To this aim, this paper proposes to exploit Reinforcement Learning (RL) techniques to design an adaptive LoRaWAN strategy for industrial applications. The RL is spreading in many fields since it allows the design of intelligent systems using a stochastic discrete-time system approach. The proposed technique has been implemented within a purposely designed simulator, allowing to draw a preliminary performance assessment in a real-world scenario. A high density of independent nodes per square km has been considered, showing a significant improvement (about 10%) of the overall reliability in terms of data extraction rate (DER) without compromising full compatibility with the standard specifications.",https://ieeexplore.ieee.org/document/9488498/,2021 IEEE International Workshop on Metrology for Industry 4.0 & IoT (MetroInd4.0&IoT),7-9 June 2021,ieeexplore
10.1109/CIMSA.2006.250758,Adaptive Spatio-Spectral Hyperspectral Image Processing for Online Industrial Classification of Inhomogeneous Materials,IEEE,Conferences,"An approach for considering spatio-spectral information when classifying inhomogeneous materials in industrial environments is proposed. Its main application would be in the inspection and quality control tasks. They system core is an ANN based hyperspectral processing unit able to perform the online determination of the quality of the material based on its composition and grain size. A training adviser is being implemented in the system in order to automate the determination of the optimal spatial window size, as well as to reduce the number of spectral bands used and for determining the optimal spectral combination function through the automatic extraction of the discriminating features. Several tests have been carried out on synthetic and real data sets. In particular, the proposed approach is used to discriminate samples of andalusite having different purities; the results obtained show an accuracy of better than 98%",https://ieeexplore.ieee.org/document/4016840/,2006 IEEE International Conference on Computational Intelligence for Measurement Systems and Applications,12-14 July 2006,ieeexplore
10.1109/INDIN.2017.8104789,Addressing security challenges in industrial augmented reality systems,IEEE,Conferences,"In context of Industry 4.0 Augmented Reality (AR) is frequently mentioned as the upcoming interface technology for human-machine communication and collaboration. Many prototypes have already arisen in both the consumer market and in the industrial sector. According to numerous experts it will take only few years until AR will reach the maturity level to be deployed in productive applications. Especially for industrial usage it is required to assess security risks and challenges this new technology implicates. Thereby we focus on plant operators, Original Equipment Manufacturers (OEMs) and component vendors as stakeholders. Starting from several industrial AR use cases and the structure of contemporary AR applications, in this paper we identify security assets worthy of protection and derive the corresponding security goals. Afterwards we elaborate the threats industrial AR applications are exposed to and develop an edge computing architecture for future AR applications which encompasses various measures to reduce security risks for our stakeholders.",https://ieeexplore.ieee.org/document/8104789/,2017 IEEE 15th International Conference on Industrial Informatics (INDIN),24-26 July 2017,ieeexplore
10.1109/DCOSS49796.2020.00046,An Agnostic Data-Driven Approach to Predict Stoppages of Industrial Packing Machine in Near,IEEE,Conferences,"As data awareness in manufacturing companies increases with the deployment of sensors and Internet of Things (IoT) devices, data-driven maintenance and prediction have become quite popular in the Industry 4.0 paradigm. Machine Learning (ML) has been recognised as a promising, efficient and reliable tool for fault detection use cases, as it allows to export important knowledge from monitored assets. Scientists deal with issues such as the small amount of data that indicate potential problems, or the imbalance which exists between the standard process data and the data inadequacy of the systems to make a high precision forecast. Currently, in this context, even large industries are not able to effectively predict abnormal behaviors in their tools, processes and equipment, when adopting strategies to anticipate crucial events. In this paper, we propose a methodology to enable prediction of a packing machine's stoppages in manufacturing process of a large industry, by using forecasting techniques based on univariate time series data. There are more than 100 reasons that cause the machine to stop, in a quite big production line length. However, we use a single signal, concerning the machines operational status to make our prediction, without considering other fault or warning signals, hence its characterization as ""agnostic"". A workflow is presented for cleaning and preprocessing the data, and for training and evaluating a predictive model. Two predictive models, namely ARIMA and Prophet, are applied and evaluated on real data from an advanced machining process used for packing. Training and evaluation tests indicate that the results of the applied methods perform well on a daily basis. Our work can be further extended and act as reference for future research activities that could lead to more robust and accurate prediction frameworks.",https://ieeexplore.ieee.org/document/9183540/,2020 16th International Conference on Distributed Computing in Sensor Systems (DCOSS),25-27 May 2020,ieeexplore
10.1109/ICAPAI49758.2021.9462061,An Industrial Assistance System with Manual Assembly Step Recognition in Virtual Reality,IEEE,Conferences,"In the era of Industry 4.0, worker assistance systems are becoming more and more important. In order to assist shop floor workers in manual assembly tasks, we implemented an assistance system in virtual reality. A deep neural network was trained to recognize the current work step in real-time during an assembly process, thus giving the assistance system context-awareness. We defined the problem of assembly step recognition as a multivariate time series classification using the poses of the workers head, both hands and all relevant tools and objects. With this definition, the VR environment's output can also be replaced with data from the real world. For our proof-of-concept assembly step recognition system, we created an assembly process consisting of six different work steps, five movable assembly parts and one tool. We showed that we can train an activity recognition model for assembly steps with only 10 assembly recordings. To achieve this, we used multiple data augmentation techniques and proposed a novel method of synthesizing new training data, which we call Path Joining. With only 10 training recordings, we attain a categorical classification accuracy of 81 percent and with 60 recordings we achieve an accuracy of 89 percent.",https://ieeexplore.ieee.org/document/9462061/,2021 International Conference on Applied Artificial Intelligence (ICAPAI),19-21 May 2021,ieeexplore
10.1109/ICDCS.2019.00139,An Industrial IoT Solution for Evaluating Workers' Performance Via Activity Recognition,IEEE,Conferences,"The Industrial Internet of Things (IIoT) is a key pillar of the Fourth Industrial Evolution or Industry 4.0. It aims to achieve direct information exchange between industrial machines, people, and processes. By tapping and analysing such data, IIoT can more importantly provide for significant improvements in productivity, product quality, and safety via proactive detection of problems in the performance and reliability of production machines, workers, and industrial processes. While the majority of existing IIoT research is currently focusing on the predictive maintenance of industrial machines (unplanned production stoppages lead to significant increases in costs and lost plant productivity), this paper focuses on monitoring and assessing worker productivity. This IIoT research is particularly important for large manufacturing plants where most production activities are performed by workers using tools and operating machines. With this aim, this paper introduces a novel industrial IoT solution for monitoring, evaluating, and improving worker and related plant productivity based on workers activity recognition using a distributed platform and wearable sensors. More specifically, this IIoT solution captures acceleration and gyroscopic data from wearable sensors in edge computers and analyses them in powerful processing servers in the cloud to provide a timely evaluation of the performance and productivity of each individual worker in the production line. These are achieved by classifying worker production activities and computing Key Performance Indicators (KPIs) from the captured sensor data. We present a real-world case study that utilises our IIoT solution in a large meat processing plant (MPP). We illustrate the design of the IIoT solution, describe the in-plant data collection during normal operation, and present the sensor data analysis and related KPI computation, as well as the outcomes and lessons learnt.",https://ieeexplore.ieee.org/document/8884821/,2019 IEEE 39th International Conference on Distributed Computing Systems (ICDCS),7-10 July 2019,ieeexplore
10.1109/AITEST52744.2021.00024,An Industrial Workbench for Test Scenario Identification for Autonomous Driving Software,IEEE,Conferences,"Testing of autonomous vehicles involves enormous challenges for the automotive industry. The number of real-world driving scenarios is extremely large, and choosing effective test scenarios is essential, as well as combining simulated and real world testing. We present an industrial workbench of tools and workflows to generate efficient and effective test scenarios for active safety and autonomous driving functions. The workbench is based on existing engineering tools, and helps smoothly integrate simulated testing, with real vehicle parameters and software. We aim to validate the workbench with real cases and further refine the input model parameters and distributions.",https://ieeexplore.ieee.org/document/9564354/,2021 IEEE International Conference on Artificial Intelligence Testing (AITest),23-26 Aug. 2021,ieeexplore
10.1109/ICCWorkshops49005.2020.9145434,An Inter-Disciplinary Modelling Approach in Industrial 5G/6G and Machine Learning Era,IEEE,Conferences,"Unlike conventional cellular systems, the fifth generation (5G) and beyond includes intrinsic support for vertical industries with diverse service requirements. Industrial process automation with autonomous fault detection and prediction, optimised operations and proactive control can be considered as one of the key verticals of 5G and beyond. Such applications enable equipping industrial plants with a reasoning sixth sense for optimised operations and fault avoidance. In this direction, we introduce an inter-disciplinary approach integrating wireless sensor networks with machine learning-enabled industrial plants to build a step towards developing this sixth sense technology, i.e., the reasoning ability. We develop a modular-based system that can be adapted to the vertical-specific elements. Without loss of generalisation, exemplary use cases are developed and presented including a fault detection/prediction scheme in a wireless communication network with sensors and actuators to enable the sixth sense technology with guaranteed service load requirements. The proposed schemes and modelling approach are implemented in a real chemical plant for testing purposes, and a high fault detection and prediction accuracy is achieved coupled with optimised sensor density analysis.",https://ieeexplore.ieee.org/document/9145434/,2020 IEEE International Conference on Communications Workshops (ICC Workshops),7-11 June 2020,ieeexplore
10.1109/SPAC53836.2021.9539933,An LSTM based Malicious Traffic Attack Detection in Industrial Internet,IEEE,Conferences,"Current Industrial Internet faces serious threats where attackers propagate malicious flows, resulting in communication failures in the Industrial Internet. In this work, we propose a practical and novel method to detect malicious traffic attack in real time with high accuracy. Our primary idea is to capture network flow, extract adequate network flow features, construct a long short-term memory (LSTM) based deep learning model, and identify the property of the corresponding network flow. Whether the network suffers attack or not is then determined according to the detection results. The corresponding prototype is also implemented in the Industrial Internet which is equipped with Software Defined Networking (SDN). Experimental results validate that the proposed method is effective in defending against malicious traffic attack in real-world network.",https://ieeexplore.ieee.org/document/9539933/,"2021 International Conference on Security, Pattern Analysis, and Cybernetics（SPAC)",18-20 June 2021,ieeexplore
10.1109/ETFA.2014.7005346,An adaptive image processing system based on incremental learning for industrial applications,IEEE,Conferences,"Machine learning has been applied in image processing system for object recognition, inspection and measurement. It assumes that the provided training objects are representative enough to the real objects. However in real application, new (unlearned) objects always emerge over time, which may deviate from the trained (learned) objects. The conventional image processing system using machine learning is not able to learn and then recognize these new objects. In this paper, an incremental learning based image processing system is presented. The overall system consists of three layers: execution, learning and user. The conventional image processing system is constructed in execution layer. In learning layer, adviser and incremental learning are applied to generate a new classifier. The incremental learning is differentiated into different methodologies: data accumulation and ensemble learning. Through the adviser, a proper methodology can be recommended. User is able to interact with the system via user layer. Comparing to the conventional image processing system, the proposed system is robust in industrial applications, since it deals with the classification problems dynamically.",https://ieeexplore.ieee.org/document/7005346/,Proceedings of the 2014 IEEE Emerging Technology and Factory Automation (ETFA),16-19 Sept. 2014,ieeexplore
10.1109/ASIC.1994.404614,An analog VLSI neural network for real-time image processing in industrial applications,IEEE,Conferences,"In this paper we present an analog VLSI architecture that implements a neural network for image processing in industrial environment. The analog architecture is highly modular and operates in real time. The circuit implementation is based on simple and effective circuit primitives. Special care has been devoted to the analysis of the linearity and the precision of computation. A test chip, implementing the filtering stage of the architecture, has been designed and realized.&lt;<ETX>&gt;</ETX>",https://ieeexplore.ieee.org/document/404614/,Proceedings Seventh Annual IEEE International ASIC Conference and Exhibit,19-23 Sept. 1994,ieeexplore
10.1109/INDIN.2005.1560420,An analogue recurrent neural network for trajectory learning and other industrial applications,IEEE,Conferences,"A real-time analogue recurrent neural network (RNN) can extract and learn the unknown dynamics (and features) of a typical control system such as a robot manipulator. The task at hand is a tracking problem in the presence of disturbances. With reference to the tasks assigned to an industrial robot, one important issue is to determine the motion of the joints and the effector of the robot. In order to model robot dynamics we use a neural network that can be implemented in hardware. The synaptic weights are modelled as variable gain cells that can be implemented with a few MOS transistors. The network output signals portray the periodicity and other characteristics of the input signal in unsupervised mode. For the specific purpose of demonstrating the trajectory learning capabilities, a periodic signal with varying characteristics is used. The developed architecture, however, allows for more general learning tasks typical in applications of identification and control. The periodicity of the input signal ensures convergence of the output to a limit cycle. Online versions of the synaptic update can be formulated using simple CMOS circuits. Because the architecture depends on the network generating a stable limit cycle, and consequently a periodic solution which is robust over an interval of parameter uncertainties, we currently place the restriction of a periodic format for the input signals. The simulated network contains interconnected recurrent neurons with continuous-time dynamics. The system emulates random-direction descent of the error as a multidimensional extension to the stochastic approximation. To achieve unsupervised learning in recurrent dynamical systems we propose a synapse circuit which has a very simple structure and is suitable for implementation in VLSI.",https://ieeexplore.ieee.org/document/1560420/,"INDIN '05. 2005 3rd IEEE International Conference on Industrial Informatics, 2005.",10-12 Aug. 2005,ieeexplore
10.1109/ROBOT.1986.1087731,An expert-system approach to industrial job-shop scheduling,IEEE,Conferences,"The first part of this paper is devoted to the presentation of a software system for jobshop scheduling of batches under due date constraints and time-table constraints on work-stations. It is based on a heuristic technique for sequencing the jobs. It is actually run in a real workshop. The second part of the parper deals with an expert system methodology currently under development for the same kind of problems. The idea is to be able to integrate two kinds of knowledge about the production process : theoretical knowledge (issued from scheduling theory) which achieves the management of time, and practical knowledge (provided by the shop-floor manager) about specific technological constraints which must be satisfied by the actual production process. These constraints are usually not taken into account at the theoretical level. The knowledge representation and processing techniques of Artificial Intelligence enable realistic schedules to be obtained by simultaneous use of both sources of information.",https://ieeexplore.ieee.org/document/1087731/,Proceedings. 1986 IEEE International Conference on Robotics and Automation,7-10 April 1986,ieeexplore
10.1109/ICMA.2013.6618173,An intelligent object manipulation framework for industrial tasks,IEEE,Conferences,"This paper presents an intelligent object manipulation framework for industrial tasks, which integrates a sensor-rich multi-fingered robot hand, an industrial robot manipulator, a conveyor belt and employs machine learning algorithms. The framework software architecture is implemented using a Windows 7 operating system with RTX real-time extension for synchronous handling of peripheral devices. The framework uses Scale Invariant Feature Transform (SIFT) image processing algorithm, Support Vector Machine (SVM) machine learning algorithm and 3D point cloud techniques for intelligent object recognition based on RGB camera and laser rangefinder information from the robot hand end effector. The objective is automated manipulation of objects with different shapes and poses with minimum programming effort applied by a user.",https://ieeexplore.ieee.org/document/6618173/,2013 IEEE International Conference on Mechatronics and Automation,4-7 Aug. 2013,ieeexplore
10.1109/SYSCON.2018.8369547,An interactive architecture for industrial scale prediction: Industry 4.0 adaptation of machine learning,IEEE,Conferences,"According to wiki definition, there are four design principles in Industry 4.0. These principles support companies in identifying and implementing Industry 4.0 scenarios, namely, Interoperability, Information transparency, Technical assistance, Decentralized decisions. In this paper we have discussed our work on an implementation of a machine learning based interactive architecture for industrial scale prediction for dynamic distribution of water resources across the continent, keeping the four corners of Industry 4.0 in place. We report the possibility of producing most probable high resolution estimation regarding the water balance in any region within Australia by implementation of an intelligent system that can integrate spatial-temporal data from various independent sensors and models, with the ground truth data produced by 250 practitioners from the irrigation industry across Australia. This architectural implementation on a cloud computing platform linked with a freely distributed mobile application, allowing interactive ground truthing of a machine learning model on a continental scale, shows accuracy of 90% with 85% sensitivity of correct surface soil moisture estimation with end users at its complete control. Along with high level of information transparency and interoperability, providing on-demand technical supports and motivating users by allowing them to customize and control their own local predictive models, show the successfulness of principles in Industry 4.0 in real environmental issues in the future adaptation in various industries starting from resource management to modern generation soft robotics.",https://ieeexplore.ieee.org/document/8369547/,2018 Annual IEEE International Systems Conference (SysCon),23-26 April 2018,ieeexplore
10.1109/ICAC.2017.21,Ananke: A Q-Learning-Based Portfolio Scheduler for Complex Industrial Workflows,IEEE,Conferences,"Complex workflows that process sensor data are useful for industrial infrastructure management and diagnosis. Although running such workflows in clouds promises reduced operational costs, there are still numerous scheduling challenges to overcome. Such complex workflows are dynamic, exhibit periodic patterns, and combine diverse task groupings and requirements. In this work, we propose ANANKE, a scheduling system addressing these challenges. Our approach extends the state-of-the-art in portfolio scheduling for data centers with a reinforcement-learning technique, and proposes various scheduling policies for managing complex workflows. Portfolio scheduling addresses the dynamic aspect of the workload. Q-learning, allows our approach to adapt to the periodic patterns of the workload, and to tune the other configuration parameters. The proposed policies are heuristics that guide the provisioning process, and map workflow tasks to the provisioned cloud resources. Through real-world experiments based on real and synthetic industrial workloads, we analyze and compare our prototype implementation of ANANKE with a system without portfolio scheduling (baseline) and with a system equipped with a standard portfolio scheduler. Overall, our experimental results give evidence that a learning-based portfolio scheduler can perform better and consume fewer resources than state-of-the-art alternatives, in particular for workloads with uniform arrival patterns.",https://ieeexplore.ieee.org/document/8005354/,2017 IEEE International Conference on Autonomic Computing (ICAC),17-21 July 2017,ieeexplore
10.23919/SOFTCOM.2019.8903672,Anomaly-based Intrusion Detection in Industrial Data with SVM and Random Forests,IEEE,Conferences,"Attacks on industrial enterprises are increasing in number as well as in effect. Since the introduction of industrial control systems in the 1970' s, industrial networks have been the target of malicious actors. More recently, the political and warfare-aspects of attacks on industrial and critical infrastructure are becoming more relevant. In contrast to classic home and office IT systems, industrial IT, so-called OT systems, have an effect on the physical world. Furthermore, industrial devices have long operation times, sometimes several decades. Updates and fixes are tedious and often not possible. The threats on industry with the legacy requirements of industrial environments creates the need for efficient intrusion detection that can be integrated into existing systems. In this work, the network data containing industrial operation is analysed with machine learning- and time series-based anomaly detection algorithms in order to discover the attacks introduced to the data. Two different data sets are used, one Modbus-based gas pipeline control traffic and one OPC UA-based batch processing traffic. In order to detect attacks, two machine learning-based algorithms are used, namely SVM and Random Forest. Both perform well, with Random Forest slightly outperforming SVM. Furthermore, extracting and selecting features as well as handling missing data is addressed in this work.",https://ieeexplore.ieee.org/document/8903672/,"2019 International Conference on Software, Telecommunications and Computer Networks (SoftCOM)",19-21 Sept. 2019,ieeexplore
10.23919/IConAC.2019.8895095,Ant Colony Optimization Algorithm for Industrial Robot Programming in a Digital Twin,IEEE,Conferences,"Advanced manufacturing that is adaptable to constantly changing product designs often requires dynamic changes on the factory floor to enable manufacture. The integration of robotic manufacture with machine learning approaches offers the possibility to enable such dynamic changes on the factory floor. While ensuring safety and the possibility of losses of components and waste of material are against their usage. Furthermore, developments in design of virtual environments makes it possible to perform simulations in a virtual environment, to enable human-in-the-loop production of parts correctly the first time like never before. Such powerful simulation and control software provides the means to design a digital twin of manufacturing environment in which trials are completed at almost at no cost. In this paper, ant colony optimization is used to program an industrial robot to avoid obstacles and find its way to pick and place objects during an assembly task in an environment containing obstacles that must be avoided. The optimization is completed in a digital twin environment first and movements transferred to the real robot after human inspection. It is shown that the proposed methodology can find the optimal solution, in addition to avoiding collisions, for an assembly task with minimum human intervention.",https://ieeexplore.ieee.org/document/8895095/,2019 25th International Conference on Automation and Computing (ICAC),5-7 Sept. 2019,ieeexplore
10.1109/ICOS.2011.6079285,Application of multi-step time series prediction for industrial equipment prognostic,IEEE,Conferences,The use of prognostics is critically to be implemented in industrial. This paper presents an application of multi-step time series prediction to support industrial equipment prognostic. An artificial neural network technique with sliding window is considered for the multi-step prediction which is able to predict the series of future equipment condition. The structure of prognostic application is presented. The feasibility of this prediction application was demonstrated by applying real condition monitoring data of industrial equipment.,https://ieeexplore.ieee.org/document/6079285/,2011 IEEE Conference on Open Systems,25-28 Sept. 2011,ieeexplore
10.1109/USBEREIT51232.2021.9455060,Applying of Recurrent Neural Networks for Industrial Processes Anomaly Detection,IEEE,Conferences,"The paper considers the issue of recurrent neural networks applicability for detecting industrial process anomalies to detect intrusion in Industrial Control Systems. Cyberattack on Industrial Control Systems often leads to appearing of anomalies in industrial process. Thus, it is proposed to detect such anomalies by forecasting the state of an industrial process using a recurrent neural network and comparing the predicted state with actual process' state. In the course of experimental research, a recurrent neural network with one-dimensional convolutional layer was implemented. The Secure Water Treatment dataset was used to train model and assess its quality. The obtained results indicate the possibility of using the proposed method in practice. The proposed method is characterized by the absence of the need to use anomaly data for training. Also, the method has significant interpretability and allows to localize an anomaly by pointing to a sensor or actuator whose signal does not match the model's prediction.",https://ieeexplore.ieee.org/document/9455060/,"2021 Ural Symposium on Biomedical Engineering, Radioelectronics and Information Technology (USBEREIT)",13-14 May 2021,ieeexplore
10.1109/SSIC.2018.8556706,Assessing Industrial Control System Attack Datasets for Intrusion Detection,IEEE,Conferences,The following topics are dealt with: Internet; learning (artificial intelligence); cloud computing; cryptography; security of data; computer network security; invasive software; data privacy; social networking (online); greedy algorithms.,https://ieeexplore.ieee.org/document/8556706/,"2018 Third International Conference on Security of Smart Cities, Industrial Control System and Communications (SSIC)",18-19 Oct. 2018,ieeexplore
10.1109/UPEC.2017.8231939,Assessing the impact of load forecasting accuracy on battery dispatching strategies with respect to Peak Shaving and Time-of-Use (TOU) applications for industrial consumers,IEEE,Conferences,"Energy Storage Systems will play crucial role in controlling the grid of the future when increased penetration of renewable energy sources will take place. Especially batteries are expected to occupy a considerable share of the total energy storage market by simultaneously providing services to different stakeholders such as energy producers, transmission/distribution operators, residential, commercial and industrial consumers. Nowadays, Peak shaving and Time-of-Use applications are the most common services that standalone battery storage systems can provide to industrial consumers (without integrated PV-systems and/or wind turbines). A big part of the existing literature addressing such applications aims at developing an offline algorithm for optimal battery deployment based on a known load profile (or accurately predicted) without taking into consideration real time conditions. This paper investigates the impact of industrial load forecasting errors on dispatching strategies of battery storage systems on economically driven peak shaving and Time-of-Use applications. An artificial neural network has been developed and used as a prediction model of an industrial load profile. The neural network was trained, validated and tested on historical load data with time resolution of 15 minutes, provided by the local distribution operator of the Belgian electric grid. The performance of the neural network in terms of output-target regression and mean absolute error is 0.833 and 10.02% respectively. Afterwards, a simulation was carried out comparing four different scenarios of peak shaving. The results show that the prediction accuracy of the presented neural network is not competitive enough. Peak shaving based on predicted profiles becomes reliable for lower forecasting errors. For this purpose, further access into the process and types of loads of the user is required in order to come up with a more sophisticated prediction model.",https://ieeexplore.ieee.org/document/8231939/,2017 52nd International Universities Power Engineering Conference (UPEC),28-31 Aug. 2017,ieeexplore
10.1109/SMC.2013.819,Automated Sound Signalling Device Quality Assurance Tool for Embedded Industrial Control Applications,IEEE,Conferences,This paper presents a novel system for automatic detection and recognition of faulty audio signaling devices as part of an automated industrial manufacturing process. The system uses historical data labeled by human experts in detecting faulty signaling devices to train an artificial neural network based classifier for modeling their decision making process. The neural network is implemented on a real time embedded micro controller which can be more efficiently incorporated into an automated production line eliminating the need for a manual inspection within the manufacturing process. We present real world experiments based on data pertaining to the production and manufacture of audio signaling components used in car instrument clusters. Our results show that the proposed expert system is able to successfully classify faulty audio signaling devices to a high degree of accuracy. The results can be generalized to other signaling devices where an output signal is represented by a complex and changing frequency spectrum even with significant environmental noise.,https://ieeexplore.ieee.org/document/6722574/,"2013 IEEE International Conference on Systems, Man, and Cybernetics",13-16 Oct. 2013,ieeexplore
10.1109/NetSoft48620.2020.9165393,Benchmarking and Profiling 5G Verticals' Applications: An Industrial IoT Use Case,IEEE,Conferences,"The Industry 4.0 sector is evolving in a tremendous pace by introducing a set of industrial automation mechanisms tightly coupled with the exploitation of Internet of Things (IoT), 5G and Artificial Intelligence (AI) technologies. By combining such emerging technologies, interconnected sensors, instruments, and other industrial devices are networked together with industrial applications, formulating the Industrial IoT (IIoT) and aiming to improve the efficiency and reliability of the deployed applications and provide Quality of Service (QoS) guarantees. However, in a 5G era, efficient, reliable and highly performant applications' provision has to be combined with exploitation of capabilities offered by 5G networks. Optimal usage of the available resources has to be realised, while guaranteeing strict QoS requirements such as high data rates, ultra-low latency and jitter. The first step towards this direction is based on the accurate profiling of vertical industries' applications in terms of resources usage, capacity limits and reliability characteristics. To achieve so, in this paper we provide an integrated methodology and approach for benchmarking and profiling 5G vertical industries' applications. This approach covers the realisation of benchmarking experiments and the extraction of insights based on the analysis of the collected data. Such insights are considered the cornerstones for the development of AI models that can lead to optimal infrastructure usage along with assurance of high QoS provision. The detailed approach is applied in a real IIoT use case, leading to profiling of a set of 5G network functions.",https://ieeexplore.ieee.org/document/9165393/,2020 6th IEEE Conference on Network Softwarization (NetSoft),29 June-3 July 2020,ieeexplore
10.1109/ICAIIC51459.2021.9415189,Big Data Platform for Intelligence Industrial IoT Sensor Monitoring System Based on Edge Computing and AI,IEEE,Conferences,"The cutting edge of Industry 4.0 has driven everything to be converted to disruptive innovation and digitalized. This digital revolution is imprinted by modern and advanced technology that takes advantage of Big Data and Artificial Intelligence (AI) to nurture from automatic learning systems, smart city, smart energy, smart factory to the edge computing technology, and so on. To harness an appealing, noteworthy, and leading development in smart manufacturing industry, the modern industrial sciences and technologies such as Big Data, Artificial Intelligence, Internet of things, and Edge Computing have to be integrated cooperatively. Accordingly, a suggestion on the integration is presented in this paper. This proposed paper describes the design and implementation of big data platform for intelligence industrial internet of things sensor monitoring system and conveys a prediction of any upcoming errors beforehand. The architecture design is based on edge computing and artificial intelligence. To extend more precisely, industrial internet of things sensor here is about the condition monitoring sensor data - vibration, temperature, related humidity, and barometric pressure inside facility manufacturing factory.",https://ieeexplore.ieee.org/document/9415189/,2021 International Conference on Artificial Intelligence in Information and Communication (ICAIIC),13-16 April 2021,ieeexplore
10.1109/ICII.2018.00024,Brightics-IoT: Towards Effective Industrial IoT Platforms for Connected Smart Factories,IEEE,Conferences,"Industrial Internet-of-Things (IIoT) supports machines, computers and users to enable intelligent operations using advanced device management and data analytics. In recent years, thanks to standardized IoT platforms and advanced Artificial Intelligence (AI) technologies, there have been great advances in IIoT, and now it promises revolutions on various manufacturing domains such as transport, health, factory and energy. In this paper, based on our experience operating IIoT in various factory applications, we present the technical challenges of manufacturing facilities needed to be dealt with to collect huge amount of data in real-time and counteraction points of an IoT platform regarding these technical challenges and what kinds of features need to be implemented for intelligent services in the smart manufacturing. Finally, we introduce a story of applying industrial IoT platform in production to show how iterative development approaches can achieve business requirements based on elastically scaled-out architecture.",https://ieeexplore.ieee.org/document/8539113/,2018 IEEE International Conference on Industrial Internet (ICII),21-23 Oct. 2018,ieeexplore
10.1109/BigData.2016.7840859,Building a research data science platform from industrial machines,IEEE,Conferences,"Data Science research has a long history in academia which spans from large-scale data management, to data mining and data analysis using technologies from database management systems (DBMS's). While traditional HPC offers tools on leveraging existing technologies with data processing needs, the large volume of data and the speed of data generation pose significant challenges. Using the Hadoop platform and tools built on top of it drew immense interest from academia after it gained success in industry. Georgia Institute of Technology received a donation of 200 compute nodes from Yahoo. Turning these industrial machines into a research Data Science Platform (DSP) poses unique challenges, such as: nontrivial hardware design decisions, configuration tool choices, node integration into existing HPC infrastructure, partitioning resource to meet different application needs, software stack choices, etc. We have 40 nodes up and running, 24 running as a Hadoop and Spark cluster, 12 running as a HBase and OpenTSDB cluster, the others running as service nodes. We successfully tested it against Spark Machine Learning algorithms using a 88GB image dataset, Spark DataFrame and GraphFrame with a Wikipedia dataset, and Hadoop MapReduce wordcount on a 300GB dataset. The OpenTSDB cluster is for real-time time series data ingestion and storage for sensor data. We are working on bringing up more nodes. We share our first-hand experience gained in our journey, which we believe will benefit and inspire other academic institutions.",https://ieeexplore.ieee.org/document/7840859/,2016 IEEE International Conference on Big Data (Big Data),5-8 Dec. 2016,ieeexplore
10.1109/FiCloudW.2017.79,Changes of Cyber-Attacks Techniques and Patterns after the Fourth Industrial Revolution,IEEE,Conferences,"In this paper, we predicted the changes of cyber-attacks techniques and patterns after the fourth industrial revolution with the epochal shift of information and communication technology and innovation of science and technology. Cyber space will be hyper-connection, cross-domain, and super intelligence space as connecting everything in the world due to a fusion of information and communication technologies such as artificial intelligence, internet of things, and cyber-physical systems. Cyber-attacks will use all electronic devices including wireless or wire networks, hardware, software, and cyber-physical systems as a route. The hacking tool's functions will evolve into a variety of forms reflecting human thought and behavioral procedures. The attack target will not be limited to a specific object. The purpose of the cyber-attack is to focus on secondary effects and indirect attacks as well as direct attacks.",https://ieeexplore.ieee.org/document/8113773/,2017 5th International Conference on Future Internet of Things and Cloud Workshops (FiCloudW),21-23 Aug. 2017,ieeexplore
10.1109/ICNP.2016.7784407,Characterizing industrial control system devices on the Internet,IEEE,Conferences,"Industrial control system (ICS) devices with IP addresses are accessible on the Internet and play a crucial role for critical infrastructures like power grid. However, there is a lack of deep understanding of these devices' characteristics in the cyberspace. In this paper, we take a first step in this direction by investigating these accessible industrial devices on the Internet. Because of critical nature of industrial control systems, the detection of online ICS devices should be done in a real-time and non-intrusive manner. Thus, we first analyze 17 industrial protocols widely used in industrial control systems, and train a probability model through the learning algorithm to improve detection accuracy. Then, we discover online ICS devices in the IPv4 space while reducing the noise of industrial honeypots. To observe the dynamics of ICS devices in a relatively long run, we have deployed our discovery system on Amazon EC2 and detected online ICS devices in the whole IPv4 space for eight times from August 2015 to March 2016. Based on the ICS device data collection, we conduct a comprehensive data analysis to characterize the usage of ICS devices, especially in the answer to the following three questions: (1) what are the distribution features of ICS devices, (2) who use these ICS devices, and (3) what are the functions of these ICS devices.",https://ieeexplore.ieee.org/document/7784407/,2016 IEEE 24th International Conference on Network Protocols (ICNP),8-11 Nov. 2016,ieeexplore
10.1109/DESSERT50317.2020.9125038,Combination of Digital Twin and Artificial Intelligence in Manufacturing Using Industrial IoT,IEEE,Conferences,"The paper focuses on Digital Twin (DT) in Manufacturing using Artificial Intelligence (AI) and Industrial IoT. According to the concept, the manufacturing includes three main units: equipment, personnel and processes. All data from these units are inherited to manufacture model (DT) and decision support system with the use of AI. DT data technology allows finding the required knowledge that can be interpreted and used to support the process of decision-making in the management of the enterprise. AI applications open up a broad spectrum of opportunities in manufacturing to add value by optimizing processes and generating new business models. The Landscape was described by a formal model to assure the possibility to analyze the state and development of landscape in detail considering DT and other technologies. DT and IIoT implementation for the simulation of real enterprise manufacturing were considered.",https://ieeexplore.ieee.org/document/9125038/,"2020 IEEE 11th International Conference on Dependable Systems, Services and Technologies (DESSERT)",14-18 May 2020,ieeexplore
10.1109/MetroInd4.0IoT48571.2020.9138184,Combining exposure indicators and predictive analytics for threats detection in real industrial IoT sensor networks,IEEE,Conferences,"We present a framework able to combine exposure indicators and predictive analytics using AI-tools and big data architectures for threats detection inside a real industrial IoT sensors network. The described framework, able to fill the gaps between these two worlds, provides mechanisms to internally assess and evaluate products, services and share results without disclosing any sensitive and private information. We analyze the actual state of the art and a possible future research on top of a real case scenario implemented into a technological platform being developed under the H2020 ECHO project, for sharing and evaluating cybersecurity relevant informations, increasing trust and transparency among different stakeholders.",https://ieeexplore.ieee.org/document/9138184/,2020 IEEE International Workshop on Metrology for Industry 4.0 & IoT,3-5 June 2020,ieeexplore
10.1109/ETFA.2017.8247730,Communication middleware technologies for industrial distributed control systems: A literature review,IEEE,Conferences,"Industry 4.0 is the German vision for the future of manufacturing, where smart factories use information and communication technologies to digitise their processes to achieve improved quality, lower costs, and increased efficiency. It is likely to bring a massive change to the way control systems function today. Future distributed control systems are expected to have an increased connectivity to the Internet, in order to capitalize on new offers and research findings related to digitalization, such as cloud, big data, and machine learning. A key technology in the realization of distributed control systems is middleware, which is usually described as a reusable software layer between operating system and distributed applications. Various middleware technologies have been proposed to facilitate communication in industrial control systems and hide the heterogeneity amongst the subsystems, such as OPC UA, DDS, and RT-CORBA. These technologies can significantly simplify the system design and integration of devices despite their heterogeneity. However, each of these technologies has its own characteristics that may work better for particular applications. Selection of the best middleware for a specific application is a critical issue for system designers. In this paper, we conduct a survey on available standard middleware technologies, including OPC UA, DDS, and RT-CORBA, and show new trends for different industrial domains.",https://ieeexplore.ieee.org/document/8247730/,2017 22nd IEEE International Conference on Emerging Technologies and Factory Automation (ETFA),12-15 Sept. 2017,ieeexplore
10.1109/ICMLA.2015.183,Comparative Evaluation of Top-N Recommenders in e-Commerce: An Industrial Perspective,IEEE,Conferences,"We experiment on two real e-commerce datasets and survey more than 30 popular e-commerce platforms to reveal what methods work best for product recommendations in industrial settings. Despite recent academic advances in the field, we observe that simple methods such as best-seller lists dominate deployed recommendation engines in e-commerce. We find our empirical findings to be well-aligned with those of the survey, where in both cases simple personalized recommenders achieve higher ranking than more advanced techniques. We also compare the traditional random evaluation protocol to our proposed chronological sampling method, which can be used for determining the optimal time-span of the training history for optimizing the performance of algorithms. This performance is also affected by a proper hyperparameter tuning, for which we propose golden section search as a fast alternative to other optimization techniques.",https://ieeexplore.ieee.org/document/7424455/,2015 IEEE 14th International Conference on Machine Learning and Applications (ICMLA),9-11 Dec. 2015,ieeexplore
10.1109/MI-STA52233.2021.9464484,Comparison of PID and Artificial Neural Network Controller in on line of Real Time Industrial Temperature Process Control System,IEEE,Conferences,"Due to its simple structure and robustness, the traditional proportional-integral-derivative (PID) controller is commonly used in the field of industrial automation and process control, but it does not function well with nonlinear systems, time-delayed linear systems and time-varying systems. A new type of PID controller based on artificial neural networks and evolutionary algorithms is presented in this paper. An powerful instrument for a highly nonlinear system is the Artificial Neural Network. The interest in the study of the nonlinear system has increased through the implementation of a high-speed computer system,. In complex systems such as robotics and process control systems, the Neuro Control Algorithm is often applied. Systems of process management is also nonlinear and hard to control consistently.. This paper presents a comprehensive analysis in Which is offline trained by a multilayered feed forward back propagation neural network to act as a process control system controller, That is to say, a temperature control device without prior knowledge of its dynamics. Via the implementation of a range of input vectors to the neural network, the inverse dynamics model is developed. Based on these input vectors, the output of the neural network It is being studied by explicitly configuring it to monitor the operation. In this paper, based on set-point adjustment, impact of disturbances in load and variable dead time, compassion between the PID controller and ANN is conducted. The outcome shows that ANN outperforms the controller of the PID.",https://ieeexplore.ieee.org/document/9464484/,2021 IEEE 1st International Maghreb Meeting of the Conference on Sciences and Techniques of Automatic Control and Computer Engineering MI-STA,25-27 May 2021,ieeexplore
10.1109/ICSMC.1993.384847,Complex industrial control with a real-time expert system (RIGAS),IEEE,Conferences,"The increasing complexity and criticality of systems requiring real-time functionalities in several fields (control, alarm processing, fault diagnosis, etc.) has caused a growing interest in introducing artificial intelligence techniques into classically considered programming areas, in real time, producing advanced tools for the development of complex control software from different approaches. The control of complex systems is one of the natural candidates to combine real time and expert systems. A real time expert system for process control, called RIGAS, and its use for the control of a complex system is described in this paper.&lt;<ETX>&gt;</ETX>",https://ieeexplore.ieee.org/document/384847/,Proceedings of IEEE Systems Man and Cybernetics Conference - SMC,17-20 Oct. 1993,ieeexplore
10.1109/IECON43393.2020.9255001,Computation Offloading for Machine Learning in Industrial Environments,IEEE,Conferences,"Industrial applications, such as real-time manufacturing, fault classification and inference, autonomous cars, etc., are data-driven applications that require machine learning with a wealth of data generated from industrial Internet of Things (IoT) devices. However, conventional approaches of transmitting this rich data to a remote data center to learn may be undesired due to the non-negligible network transmission delay and the sensitiveness of data privacy. By deploying a number of computing-capable devices at the network edge, edge computing supports the implementation of machine learning close to the industrial environment. Considering the heterogeneous computing capability as well as network location of edge devices, there are two types of feasible edge computing based machine learning models, including the centralized learning and federated learning models. In centralized learning, a resource-rich edge server aggregates the data from different IoT devices and performs machine learning. In federated learning, distributed edge devices and a federated server collaborate to perform machine learning. The features that data should be offloaded in centralized learning while it is locally trained in federated learning make centralized learning and federated learning quite different. We study the computation offloading problem for edge computing based machine learning in an industrial environment, considering the abovementioned machine learning models. We formulate a machine learning-based offloading problem with the goal of minimizing the training delay. Then, an energy-constrained delay-greedy (ECDG) algorithm is designed to solve the problem. Finally, simulation studies based on the MNIST dataset have been conducted to illustrate the efficiency of the proposal.",https://ieeexplore.ieee.org/document/9255001/,IECON 2020 The 46th Annual Conference of the IEEE Industrial Electronics Society,18-21 Oct. 2020,ieeexplore
10.1109/AITest.2019.00015,Constraint-Based Testing of An Industrial Multi-Robot Navigation System,IEEE,Conferences,"Intelligent multi-robot systems get more and more deployed in industrial settings to solve complex and repetitive tasks. Due to safety and economic reasons they need to operate dependably. To ensure a high degree of dependability, testing the deployed system has to be done in a rigorous way. Advanced multi-robot systems show a rich set of complex behaviors. Thus, these systems are difficult to test manually. Moreover, the space of potential environments and tasks for such systems is enormous. Therefore, methods that are able to explore this space in a structured way are needed. One way to address these issues is through model-based testing. In this paper we present an approach for testing the navigation system of a fleet of industrial transport robots. We show how all potential environments and navigation behaviors as well as requirements and restrictions can be represented in a formal constraint-based model. Moreover, we present the concept of coverage criteria in order to handle the potentially infinite space of test cases. Finally, we show how test cases can be derived from this model in an efficient way. In order to show the feasibility of the proposed approach we present an empirical evaluation of a prototype implementation using a real industrial use case.",https://ieeexplore.ieee.org/document/8718216/,2019 IEEE International Conference On Artificial Intelligence Testing (AITest),4-9 April 2019,ieeexplore
10.1109/ICAICA50127.2020.9182598,Construction of Industrial Internet of Things Based on MQTT and OPC UA Protocols,IEEE,Conferences,"At present, the Internet of Things has become a hot area of global concern and is considered to be another major scientific and technological innovation after the Internet. The Internet of Things can be analyzed from two aspects, one is to access the Internet, and the other is the connection between things. The Industrial Internet of Things, as a new type of industrial ecosystem, collects resources and data during the industrial production process by using miniature low-cost controllers and high-bandwidth wireless networks to achieve flexible configuration of manufacturing raw materials and improve resource utilization. The Industrial Internet of Things not only includes traditional software elements, but also requires hardware controllers and sensors, as well as cloud service platforms, to ultimately achieve intelligent manufacturing. The Industrial Internet of Things based on MQTT adopts the method of upper computer (WeChat applet) + server (Alibaba Cloud Server) + lower computer (WiFi module ESP8266 NodeMcu) to realize data collection and control of industrial production process. Based on OPC UA protocol, The Industrial Internet of Things, by designing an intelligent node device based on embedded Linux system, uses Qt to build the OPC UA protocol. OPC UA is a C / S architecture real-time database framework. By building an OPC UA Client on a handheld terminal device, An OPC UA Serve is set up on the smart terminal device, and the information collected by the handheld terminal is sent to the smart terminal. At the same time, multiple smart terminals are connected through the OPC UA protocol to perform data interaction and information transfer. At the same time, an edge computing algorithm is embedded in the smart terminal, and a large amount of data collected is processed for analysis, screening, and calculation. At the same time, the processed information is transmitted or processed. Reduce data interactions between data sources and data centers, and reduce the amount of information transmitted and stored.",https://ieeexplore.ieee.org/document/9182598/,2020 IEEE International Conference on Artificial Intelligence and Computer Applications (ICAICA),27-29 June 2020,ieeexplore
10.1109/INTERCON50315.2020.9220246,Convolutional neural networks for the Hass avocado classification using LabVIEW in an agro-industrial plant,IEEE,Conferences,"Peru is currently the world's third-largest exporter of Hass avocados according to the latest statistics from FAOSTAT. To classify avocados efficiently in size and maturity, a robust artificial intelligence plant was implemented to classify avocados into 5 categories. This grading technique differs from traditional grading in that it is non-invasive, reducing avocado damage by manually inspecting and grading. The plant comprises the step of hardware, consisting of Aca2500 Basler camera, lens HR 2mm/F1, illuminated, and the conveyor belt 1200. The step s7 PLC software: TIA PORTAL (OPC), a sequential algorithm, and convolutional neural network decision in which the selection parameters size and color of avocado include. The classification process fulfills three main stages: image acquisition, processing, and recognition. Convolutional neural networks were used for image treatment, obtaining an average classification precision of 60% in real-time. From the results obtained, we see that the classification can be improved.",https://ieeexplore.ieee.org/document/9220246/,"2020 IEEE XXVII International Conference on Electronics, Electrical Engineering and Computing (INTERCON)",3-5 Sept. 2020,ieeexplore
10.1109/EAEEIE.2013.6576535,Cooperation between industry and university based on the evaluation of the industrial research results in the academic environment,IEEE,Conferences,"Based on the European Structural Funds it was developed the Intelligent Mobile Box, Intelligent Panel Controller with intelligent adaptive controllers within the industrial research and experimental development in the company Kybernetes, s.r.o. Within the frame of the academic-industry cooperation, the intelligent adaptive controller was tested at the Department of Cybernetics and Artificial Intelligence, Technical university of Kosice, Slovakia. The tests of the mobile intelligent adaptive controller were performed on two levels of university study, on the Bachelor level on the exercises from the subject “Control of Technological Processes” and on the Engineering level the exercises from the subject “Intelligent Control Networks” and on one Diploma project. Goals of students of the Control of the technological processes course had two goals, firstly to connect the intelligent adaptive controller to pre-defined controlled system (real plant, real model or simulated model) and next to validate the control results. Students of the Diploma project on the Engineering level had more advanced goals. Tasks defined for engineering students were to connect the intelligent adaptive controller to non-defined controlled system, setup the adaptivity process of the controller regarding the learning error, parameterize the control system, observe and validate the control results. Both sides concluded this cooperation as very valuable. Main contributions for students were (U1) the challenge to apply studied theoretical knowledge on the real industrial controllers, (U2) experience with new research results and technologies deployed in industry and (U3) the implementation of the control and adaptive algorithms from abstract mathematical area to real PLC controller. On side of industry research company the main contributions were (C1) testing of designed algorithms and (C2) user feedback from students to make the application HMI interface more understandable a native.",https://ieeexplore.ieee.org/document/6576535/,2013 24th EAEEIE Annual Conference (EAEEIE 2013),30-31 May 2013,ieeexplore
10.1109/ICSTCC.2019.8885434,Data–driven Neural Feedforward Controller Design for Industrial Linear Motors,IEEE,Conferences,"In this paper we consider the problem of feedforward controller design for industrial linear motors. These motors are safety-critical high-precision mechatronics systems that pose stringent requirements on the feedforward design: safe and predictable behavior for the desired motion profiles, tracking performance within the 10μm range in the presence of nonlinear friction and real-time implementation within the 1ms range. We investigate and compare several possibilities to design data- driven feedforward controllers using neural networks (NN) and we show that a two-step inverse estimation method is the most suitable approach, due to robustness to noisy data. We also show that basic knowledge about the system dynamics and the friction behavior can be exploited to design neural feedforward controllers with a simple structure, suitable for real-time implementation in industrial linear motors. The developed data-driven neural feedforward controllers are tested and compared with standard mass-acceleration feedforward and iterative learning controllers in realistic simulations.",https://ieeexplore.ieee.org/document/8885434/,"2019 23rd International Conference on System Theory, Control and Computing (ICSTCC)",9-11 Oct. 2019,ieeexplore
10.1109/IJCNN.2019.8852303,Deep Capsule Network based Automatic Batch Code Identification Pipeline for a Real-life Industrial Application,IEEE,Conferences,"Automatic recognition of text, such as a batch code printed on a box placed on a moving conveyor belt, is still a challenging problem. This paper proposes an end-to-end character recognition technique while addressing the major challenges encountered in a real environment, such as motion blur in the acquired images, slanted or oriented characters, creased batch codes due to wear and tear of boxes, variations in label formats, and variations in printing styles. The major contribution of this work lies in development of three sequential modules: text localization using Connectionist Text Proposal Network(CTPN), character detection and character recognition using a modified version of the capsule network (CapsNet). In contrast to CapsNet, where only a standard single convolution is used, the proposed method uses a series of feature blocks, making it a deep CapsNet which is later proven to generate more comprehensive and better separable feature vectors over its counterpart. The feature generation module is further enhanced by setting a smaller kernel size than CapsNet. The proposed system is validated on a real-world box / packet dataset generated in a retail manufacturing industry. The proposed recognition network architecture is also validated on a standard public dataset (ICDAR 2013). The comparative results are presented with statistical analysis in the experimental results section.",https://ieeexplore.ieee.org/document/8852303/,2019 International Joint Conference on Neural Networks (IJCNN),14-19 July 2019,ieeexplore
,Demonstration of a laser-based ultrasonic inspection system for industrial applications,IEEE,Conferences,"Summary form only given. We have fabricated an automated, computer-controlled prototype laser-based ultrasound system, enabling inspection of real-world workpieces, including weld-joint dimensioning of rough-cut components using neural network classification, detection of inclusions in aluminum billets, hardness, and remote temperature and thickness measurements. Our system, called Compensated Laser Ultrasonic Evaluation, or CLUE, employs various forms of all-optical wavefront compensation, including nonlinear optical phase conjugation and nonsteady-state photoinduced emf.",https://ieeexplore.ieee.org/document/864778/,Summaries of papers presented at the Conference on Lasers and Electro-Optics,2-7 June 1996,ieeexplore
10.1109/ISIE45063.2020.9152441,Deployment of a Smart and Predictive Maintenance System in an Industrial Case Study,IEEE,Conferences,"Industrial manufacturing environments are often characterized as being stochastic, dynamic and chaotic, being crucial the implementation of proper maintenance strategies to ensure the production efficiency, since the machines' breakdown leads to a degradation of the system performance, causing the loss of productivity and business opportunities. In this context, the use of emergent ICT technologies, such as Internet of Things (IoT), machine learning and augmented reality, allows to develop smart and predictive maintenance systems, contributing for the reduction of unplanned machines' downtime by predicting possible failures and recovering faster when they occur. This paper describes the deployment of a smart and predictive maintenance system in an industrial case study, that considers IoT and machine learning technologies to support the online and real-time data collection and analysis for the earlier detection of machine failures, allowing the visualization, monitoring and schedule of maintenance interventions to mitigate the occurrence of such failures. The deployed system also integrates machine learning and augmented reality technologies to support the technicians during the execution of maintenance interventions.",https://ieeexplore.ieee.org/document/9152441/,2020 IEEE 29th International Symposium on Industrial Electronics (ISIE),17-19 June 2020,ieeexplore
10.1109/CDC.2001.980681,Design and implementation of industrial neural network controller using backstepping,IEEE,Conferences,"A novel neural network (NN) backstepping controller is modified for application to an industrial motor drive system. A control system structure and NN tuning algorithms are presented that are shown to guarantee the stability and performance of the closed-loop system. The NN backstepping controller is implemented on an actual motor drive system using a two-PC control system developed at the authors' university. The implementation results show that the NN backstepping controller is highly effective in controlling the industrial motor drive system. It is also shown that the NN controller gives better results on actual systems than a standard backstepping controller developed assuming full knowledge of the dynamics. Moreover, the NN controller does not require the linear-in-the-parameters assumption or the computation of regression matrices required by standard backstepping.",https://ieeexplore.ieee.org/document/980681/,Proceedings of the 40th IEEE Conference on Decision and Control (Cat. No.01CH37228),4-7 Dec. 2001,ieeexplore
10.1109/ICSSA.2016.19,Detection of Man-in-the-Middle Attacks on Industrial Control Networks,IEEE,Conferences,"In this paper we present a method to detect Man-in-the-Middle attacks on industrial control systems. The approach uses anomaly detection by developing a model of normal behaviour of the industrial control system network. To come as close as possible to reality a simple industrial system, a conveyor belt with sensors and actuators, was set up with controllers widely used in industry. A machine learning approach based on the k-Nearest Neighbors algorithm with Bregman divergence was used to define a model of normal (valid) behaviour. Afterwards Man-in-the-Middle attacks were launched against the system and its behaviour during the attack was compared to the valid behaviour model. The results show that the approach taken was able to detect such attacks with satisfactory accuracy.",https://ieeexplore.ieee.org/document/7861654/,2016 International Conference on Software Security and Assurance (ICSSA),24-25 Aug. 2016,ieeexplore
10.1109/SPW.2019.00040,Devil in the Detail: Attack Scenarios in Industrial Applications,IEEE,Conferences,"In the past years, industrial networks have become increasingly interconnected and opened to private or public networks. This leads to an increase in efficiency and manageability, but also increases the attack surface. Industrial networks often consist of legacy systems that have not been designed with security in mind. In the last decade, an increase in attacks on cyber-physical systems was observed, with drastic consequences on the physical work. In this work, attack vectors on industrial networks are categorised. A real-world process is simulated, attacks are then introduced. Finally, two machine learning-based methods for time series anomaly detection are employed to detect the attacks. Matrix Profiles are employed more successfully than a predictor Long Short-Term Memory network, a class of neural networks.",https://ieeexplore.ieee.org/document/8844618/,2019 IEEE Security and Privacy Workshops (SPW),19-23 May 2019,ieeexplore
10.1109/ICICT52872.2021.00047,Digital Knowledge Base for Industrial Augmented Reality Systems Based on Semantic Technologies,IEEE,Conferences,"Augmented Reality is a technology that offers enormous potential in the industry. Due to a lack of expertise, however, companies are facing various challenges in exploiting this potential. This includes the demand-oriented configuration of the AR system. Depending on the planned use case and company-specific influencing factors and general conditions, suitable AR devices are to be identified and functionalities need to be purposefully selected. The necessary knowledge is implicitly available in numerous sources and difficult to access for companies. In the scope of this work, therefore, a machine-readable knowledge base is developed, in which the knowledge relevant for the AR system configuration is consolidated and formalized. Four aspects of knowledge are considered: The AR system structure including technical and functional components; Potential influences through ambient factors; Experience and application knowledge; The specification of specific AR devices. The knowledge base is realized based on the Web Ontology Language (OWL) and thus enables a digital and partially automated processing of the knowledge using software tools and algorithms. In combination with Artificial Intelligence solutions, the developed knowledge base can be transferred to other systems in the future and provide a powerful tool for system configuration.",https://ieeexplore.ieee.org/document/9476944/,2021 4th International Conference on Information and Computer Technologies (ICICT),11-14 March 2021,ieeexplore
10.1109/IAI50351.2020.9262203,Digital Twin Enabled Smart Control Engineering as an Industrial AI: A New Framework and Case Study,IEEE,Conferences,"In Industry 4.0, the increasing complexity of industrial systems introduces unknown dynamics that affect the performance of manufacturing processes. Thus, Digital Twin appears as a breaking technology to develop virtual representations of any complex system design, analysis, and behavior prediction tasks to enhance the system understanding via enabling capabilities like real-time analytics, or Smart Control Engineering. In this paper, a novel framework is proposed for the design and implementation of Digital Twin applications to the development of Smart Control Engineering. The framework involve the steps of system documentation, Multidomain Simulation, Behavioral Matching, and real-time monitoring, which is applied to develop the Digital Twin for a real-time vision feedback temperature uniformity control. The obtained results show that Digital Twin is a fundamental part of the transformation into Industry 4.0.",https://ieeexplore.ieee.org/document/9262203/,2020 2nd International Conference on Industrial Artificial Intelligence (IAI),23-25 Oct. 2020,ieeexplore
10.1109/AIKE.2018.00042,Distributed Osmotic Computing Approach to Implementation of Explainable Predictive Deep Learning at Industrial IoT Network Edges with Real-Time Adaptive Wavelet Graphs,IEEE,Conferences,"Challenges associated with developing analytics solutions at the edge of large scale Industrial Internet of Things (IIoT) networks close to where data is being generated in most cases involves developing analytics solutions from ground up. However, this approach increases IoT development costs and system complexities, delay time to market, and ultimately lowers competitive advantages associated with delivering next-generation IoT designs. To overcome these challenges, existing, widely available, hardware can be utilized to successfully participate in distributed edge computing for IIoT systems. In this paper, an osmotic computing approach is used to illustrate how distributed osmotic computing and existing low-cost hardware may be utilized to solve complex, compute-intensive Explainable Artificial Intelligence (XAI) deep learning problem from the edge, through the fog, to the network cloud layer of IIoT systems. At the edge layer, the C28x digital signal processor (DSP), an existing low-cost, embedded, real-time DSP that has very wide deployment and integration in several IoT industries is used as a case study for constructing real-time graph-based Coiflet wavelets that could be used for several analytic applications including deep learning pre-processing applications at the edge and fog layers of IIoT networks. Our implementation is the first known application of the fixed-point C28x DSP to construct Coiflet wavelets. Coiflet Wavelets are constructed in the form of an osmotic microservice, using embedded low-level machine language to program the C28x at the network edge. With the graph-based approach, it is shown that an entire Coiflet wavelet distribution could be generated from only one wavelet stored in the C28x based edge device, and this could lead to significant savings in memory at the edge of IoT networks. Pearson correlation coefficient is used to select an edge generated Coiflet wavelet and the selected wavelet is used at the fog layer for pre-processing and denoising IIoT data to improve data quality for fog layer based deep learning application. Parameters for implementing deep learning at the fog layer using LSTM networks have been determined in the cloud. For XAI, communication network noise is shown to have significant impact on results of predictive deep learning at IIoT network fog layer.",https://ieeexplore.ieee.org/document/8527474/,2018 IEEE First International Conference on Artificial Intelligence and Knowledge Engineering (AIKE),26-28 Sept. 2018,ieeexplore
10.1109/ISAECT50560.2020.9523700,Edge-Cloud Architectures Using UAVs Dedicated To Industrial IoT Monitoring And Control Applications,IEEE,Conferences,"The deployment of new technologies to ease the control and management of a massive data volume and its uncertainty is a very significant challenge in the industry. Under the name ""Smart Factory"", the Industrial Internet of Things (IoT) aims to send data from systems that monitor and control the physical world to data processing systems for which cloud computing has proven to be an important tool to meet processing needs. unmanned aerial vehicles (UAVs) are now being introduced as part of IIoT and can perform important tasks. UAVs are now considered one of the best remote sensing techniques for collecting data over large areas. In the field of fog and edge computing, the IoT gateway connects various objects and sensors to the Internet. It function as a common interface for different networks and support different communication protocols. Edge intelligence is expected to replace Deep Learning (DL) computing in the cloud, providing a variety of distributed, low-latency and reliable intelligent services. In this paper, An unmanned aerial vehicle is automatically integrated into an industrial control system through an IoT gateway platform. Rather than sending photos from the UAV to the cloud for processing, an AI cloud trained model is deployed in the IoT gateway and used to process the taken photos. This model is designed to overcome the latency channels of the cloud computing architecture. The results show that the monitoring and tracking process using advanced computing in the IoT gateway is significantly faster than in the cloud.",https://ieeexplore.ieee.org/document/9523700/,2020 International Symposium on Advanced Electrical and Communication Technologies (ISAECT),25-27 Nov. 2020,ieeexplore
10.1109/ICSME46990.2020.00082,Efficient Bug Triage For Industrial Environments,IEEE,Conferences,"Bug triage is an important task for software maintenance, especially in the industrial environment, where timely bug fixing is critical for customer experience. This process is usually done manually and often takes significant time. In this paper, we propose a machine-learning-based solution to address the problem efficiently. We argue that in the industrial environment, it is more suitable to assign bugs to software components (then to responsible developers) than to developers directly. Because developers can change their roles in industry, they may not oversee the same software module as before. We also demonstrate experimentally that assigning bugs to components rather than developers leads to much higher accuracy. Our solution is based on text-projection features extracted from bug descriptions. We use a Deep Neural Network to train the classification model. The proposed solution achieves state-of-the-art performance based on extensive experiments using multiple data sets. Moreover, our solution is computationally efficient and runs in near real-time.",https://ieeexplore.ieee.org/document/9240673/,2020 IEEE International Conference on Software Maintenance and Evolution (ICSME),28 Sept.-2 Oct. 2020,ieeexplore
10.1109/ICICT50816.2021.9358469,Efficient Fault Isolation Method to Monitor Industrial Batch Processes,IEEE,Conferences,Industrial batch processes are very popular manufacturing system with large number of process variables involved. Monitoring of batch processes using statistical process monitoring becomes very difficult in view of the complex correlations between the process variables. This paper focuses on a fault isolation based process monitoring method without prior information of fault where fault isolation problem is converted into a variable selection. Variable selection is a learning algorithm used here to solve the problem of selection and isolation of variables from a model. The method discussed here uses a sparse coefficient based dissimilarity analysis algorithm known as Sparse Dissimilarity Algorithm(SDISSIM) which checks a calculated D-index for identifying fault in the process. A sparse coefficient is tabulated to verify the process variables contributing to the fault and an absolute variance difference is calculated to select the variables for fault isolation. Finally SDISSIM method is explained by successful implementation in MATLAB with real time industrial process data.,https://ieeexplore.ieee.org/document/9358469/,2021 6th International Conference on Inventive Computation Technologies (ICICT),20-22 Jan. 2021,ieeexplore
10.1109/CIMSA.2003.1227220,Embedded e-diagnostic for distributed industrial machinery,IEEE,Conferences,"Industrial process machine failure often causes severe financial implications. This is compounded by the lack of availability of experts and the complications of getting them to site. One solution is to give the expert access to the machine remotely with the addition of an Artificial Intelligence (AI) based diagnostics software to assist with the decision making process. Our research is based on such a system, which combines modern communications with intelligent diagnostics software. Accessibility to process machines can now be global with the promise of predictability to the diagnosis. It is felt the importance of this research work cannot be overstated with the constantly moving worldwide manufacturing base and the real situation of the machine designers being based in a different country to their customer. The most vulnerable areas of a machine are its parts that consist of electro-mechanical actuation. The author utilises conventional Newtonian physics and differential calculus to model these and an AI technique of fault prediction and detection.",https://ieeexplore.ieee.org/document/1227220/,"The 3rd International Workshop on Scientific Use of Submarine Cables and Related Technologies, 2003.",31-31 July 2003,ieeexplore
10.1109/WFCS.2019.8757999,Exploiting localization systems for LoRaWAN transmission scheduling in industrial applications,IEEE,Conferences,"The Internet of Things (IoT) paradigm contaminated the industrial world. Wireless communications seem to be particularly attracting, especially when complement indoor and outdoor Real Time Location Systems (RTLS) for geo-referencing smart objects (e.g. for asset tracking). In this paper, the LoRaWAN solution is considered for long range transmission of RTLS data (LoRaWAN is an example of Low Power Wide Area Network). Given that the RTLSs use time synchronization, this work proposes to opportunistically obtain LoRaWAN Class A node time synchronization using the RTLS ranging devices. Once a common sense of time is shared in the LoRaWAN network, more efficient scheduled medium access strategies can be implemented. The experimental testbed, based on commercially available solutions, demonstrates the affordability and feasibility of the proposed approach. When low-cost GPS (outdoor) and UWB (indoor) ranging devices are considered, synchronization error of few microseconds can be easily obtained. The experimental results show the that time reference pulses disciplined by GPS have a maximum jitter of 180 ns and a standard deviation of 40 ns whereas, if time reference pulses disciplined by UWB are considered, the maximum jitter is 3.3 μs and the standard deviation is 0.7 μs.",https://ieeexplore.ieee.org/document/8757999/,2019 15th IEEE International Workshop on Factory Communication Systems (WFCS),27-29 May 2019,ieeexplore
10.1109/I2MTC43012.2020.9129595,Feature Ranking under Industrial Constraints in Continuous Monitoring Applications based on Machine Learning Techniques,IEEE,Conferences,"The design work-flow of machine learning techniques for continuous monitoring or predictive maintenance in an industrial context is usually a two step procedure: the selection of features to be computed from the observed signals and training of a suitable algorithm with real-life meaningful data, that will be next deployed in the second step. Feature selection is a relevant task since it provides a powerful optimisation of the deployed algorithm performance, for the given training data-set. The paper provides a method for feature ranking and selection that embeds constraints coming from real-life applications, including sensing device specifications, environmental noise, available processing resources, being all these latter aspects not considered in the currently available literature methods for feature selection. A practical case-study in the field on anomaly detection of machines is reported and discussed, in order to show the good properties of the provided method.",https://ieeexplore.ieee.org/document/9129595/,2020 IEEE International Instrumentation and Measurement Technology Conference (I2MTC),25-28 May 2020,ieeexplore
10.1109/COASE.2017.8256157,Full automatic path planning of cooperating robots in industrial applications,IEEE,Conferences,"Parts made of carbon fiber reinforced plastics (CFRP) for airplane components can be so huge that a single industrial robot is no longer able to handle them, and cooperating robots are required. Manual programming of cooperating robots is difficult, but with large numbers of different sized and shaped cut-pieces, it is almost impossible. This paper presents an automated production system consisting of a camera for the precise detection of the position of each cut-piece and a collision-free path planner which can dynamically react to different positions for the transfer motions. The path is planned for multiple robots adhering to motion constrains, such as the requirement that the textile cut-piece must form a catenary which can change during transport. Additionally a technique based on machine learning has been implemented which correctly resolves redundancy for a linear axis during planning. Finally, all components are tested on a real robot system in industrial scale.",https://ieeexplore.ieee.org/document/8256157/,2017 13th IEEE Conference on Automation Science and Engineering (CASE),20-23 Aug. 2017,ieeexplore
10.1109/ICNSC.2006.1673254,General Methodology for Action-Oriented Industrial Ecology Complex Systems Approach Applied to the Rotterdam Industrial Cluster,IEEE,Conferences,"A new approach for the understanding and shaping of the evolution of large scale socio-technical systems is presented. A proof-of-concept knowledge application has been developed, based on the industrial Rotterdam-Rijnmond case. The knowledge application includes the design of a model of industry-infrastructure evolution. Such networks are modeled via a system decomposition, formalization in an ontology and implementation of an agent based model. In simulation runs several network metrics are presented. The results provide insights in real world system behavior and show the validity and potential of the approach",https://ieeexplore.ieee.org/document/1673254/,"2006 IEEE International Conference on Networking, Sensing and Control",23-25 April 2006,ieeexplore
10.1109/AI4I51902.2021.00022,Generating Reinforcement Learning Environments for Industrial Communication Protocols,IEEE,Conferences,"An important part of any reinforcement learning application is interfacing the agent to its environment. To enable an easier use of reinforcement learning agents in manufacturing and automation-related real-world environments, we propose an environment generator which acts as an adapter between the interface of the agent and existing industrial communication protocols. This paper describes the functionality and architecture of such an environment generator.",https://ieeexplore.ieee.org/document/9565507/,2021 4th International Conference on Artificial Intelligence for Industries (AI4I),20-22 Sept. 2021,ieeexplore
10.1109/TSP52935.2021.9522588,Genetic Programming based Identification of an Industrial Process,IEEE,Conferences,"In the field of industrial automation, it is essential to develop and improve mathematical methods that assist in obtaining more accurate models of real-world systems. In the following paper, a machine learning tool is applied to the problem of identifying a model of an industrial process. Symbolic regression and genetic programming are a successful combination of methods using which one can identify a nonlinear model in analytical form based on data collected from a process during routine operation. In this paper, a detailed description of the method implementation as well as necessary data preprocessing steps are presented. Then, the resulting models are validated on an industrial data set and compared on the basis of performance metrics with more classical methods and previous results achieved by the authors. Finally, the encountered problems in the realization of the methods are reflected upon.",https://ieeexplore.ieee.org/document/9522588/,2021 44th International Conference on Telecommunications and Signal Processing (TSP),26-28 July 2021,ieeexplore
10.1109/IDAACS-SWS.2018.8525503,Hybrid MAC for Low Latency Wireless Communication Enabling Industrial HMI Applications,IEEE,Conferences,"Wireless technologies are one of the core components of the future industrial applications. They provide flexibility and scalability to the factory floor in parallel with deployment cost reduction. In our paper, we concentrate on future-oriented human-machine interaction (HMI) applications such as augmented reality (AR) or mobile control. Based on their requirements, we provide an investigation of IEEE 802.11 channel access techniques with respect to their suitability for industrial applications.",https://ieeexplore.ieee.org/document/8525503/,2018 IEEE 4th International Symposium on Wireless Systems within the International Conferences on Intelligent Data Acquisition and Advanced Computing Systems (IDAACS-SWS),20-21 Sept. 2018,ieeexplore
10.1109/ICACC.2013.54,Implementation of CMNN Based Industrial Controller Using VxWorks RTOS Ported to MPC8260,IEEE,Conferences,"With the development of embedded Real Time Operating System (RTOS), dedicated controllers normally used to control single process loops are being replaced by shared controllers which are ported with RTOS running multiple control algorithms parallelly. This work demonstrates a Cerebral Model Neural Network (CMNN) based control algorithm being run as a real time application in MPC8260 (PowerPC) embedded processor with VxWorks RTOS. Process signals from the sensors are given to MPC8260 board through serial port and control signals transmitted to the actuator are displayed on a client system running Hyper-Terminal application.",https://ieeexplore.ieee.org/document/6686379/,2013 Third International Conference on Advances in Computing and Communications,29-31 Aug. 2013,ieeexplore
10.1109/ICPHYS.2019.8780271,Implementation of Industrial Cyber Physical System: Challenges and Solutions,IEEE,Conferences,"The Industry Internet of Things (IIoT) and the Industry Cyber-Physical System (ICPS) for real industry are becoming vitally necessary in the smart manufacturing environment. Very large number of intelligent sensors are being available generating an exploding amount of data. Several issues come with the big data in real industry, including the a grand-scale connected network construction with the data security and access protocol issues, data quality with considerable noise when gathered from industrial factories, efficient data storage, smart interconnection with cloud services, and real-time analytics requirements. This paper proposes an integrated CPS based architecture for smart manufacturing and provides the deployment details, addressing all the potential problems in an appropriate way. It has been successfully implemented in a real industry environment, and won the Best Industry Application of IoT at the BigInsights Data &amp; AI Innovation Awards.",https://ieeexplore.ieee.org/document/8780271/,2019 IEEE International Conference on Industrial Cyber Physical Systems (ICPS),6-9 May 2019,ieeexplore
10.1109/EUSIPCO.2016.7760546,Implementation of efficient real-time industrial wireless interference identification algorithms with fuzzified neural networks,IEEE,Conferences,Real-time industrial wireless systems sharing a crowded spectrum band require active coexistence management measures. Identification of wireless interference is a key issue for this purpose. We propose an efficient implementation of a wireless interference identification (WII) approach called neuro-fuzzy signal classifier (NFSC). The implementation in Matlab / SIMULINK is based upon the wideband software defined radio Ettus USRP N210. The implementation is evaluated in six selected heterogeneous and harsh industrial scenarios within the license-free 2.4-GHz-ISM radio band with variously combined standard wireless technologies IEEE 802.11g-based WLAN and Bluetooth. The evaluation of the NFSC was performed with a binary classification test with the statistical measurement metrics sensitivity and specificity.,https://ieeexplore.ieee.org/document/7760546/,2016 24th European Signal Processing Conference (EUSIPCO),29 Aug.-2 Sept. 2016,ieeexplore
10.1109/IRCE50905.2020.9199256,Industrial Implementation and Performance Evaluation of LSD-SLAM and Map Filtering Algorithms for Obstacles Avoidance in a Cooperative Fleet of Unmanned Aerial Vehicles,IEEE,Conferences,"In this paper we present an industrial implementation and performance evaluation of the problem of obstacles detection by drones using autonomous navigation systems. The software module that has been developed as well as the tests conducted are part of a large industrial R&amp;D Vitrociset project called SWARM: an AI-Enabled Command and Control (C&amp;C) system, able to execute and review ISR missions for mini/micro cooperative fleets of heterogeneous UAVs. The presented software module, that is currently under test, has been developed to recognize obstacles and drive correctly the drones, using images acquired by low cost RGB video cameras, whose features of lightness and reduced size allow them to be installed on mini/micro UAVs. Moreover, this setup does not require special calibration and preconfiguration processes like the ones necessary for example using stereo video camera systems. The real-time recognition of obstacles in the surrounding environment has been obtained and evaluated through the implementation, performance evaluation and tests of the LSD-SLAM and map filtering algorithms; the core of the study has been realized starting from the integration of these algorithms with a simulated drone in a synthetic environment. The areas of interest have been identified through the filtering of a computer generated map: the module was then integrated into the SWARM project platform, allowing the control of a single drone's movement and making it ready for use in a cooperative fleet environment.",https://ieeexplore.ieee.org/document/9199256/,2020 3rd International Conference on Intelligent Robotic and Control Engineering (IRCE),10-12 Aug. 2020,ieeexplore
10.1109/ICAICA52286.2021.9497973,Industrial Internet Security Protection Based on an Industrial Firewall,IEEE,Conferences,"A crucial step in the development of a security system for the industrial Internet is the implementation of an industrial firewall as the first line of defense for the multi-layer defense-in-depth system and an important safeguard for industrial network security. In the design, development, deployment, application, and maintenance of industrial firewalls, the firewall performance and architecture are vital aspects. This thesis focuses on the analysis and discussion of the requirements and abilities of an industrial firewall in terms of adaptability, network isolation, industrial communication protocol identification, filtering and analysis, real-time performance and reliability, and self-protection.",https://ieeexplore.ieee.org/document/9497973/,2021 IEEE International Conference on Artificial Intelligence and Computer Applications (ICAICA),28-30 June 2021,ieeexplore
10.1109/GIOTS49054.2020.9119497,Industrial IoT and Digital Twins for a Smart Factory : An open source toolkit for application design and benchmarking,IEEE,Conferences,"The rapid evolution of digital technology and designed intelligence, such as the Internet of Things (IoT), Big data analytics, Artificial Intelligence (AI), Cyber Physical Systems (CPS), has been a catalyst for the 4th industrial revolution (known as industry 4.0). Among other, the two key state-of-the-art concepts in Industry 4.0, are Industrial IoT (IIoT) and digital twins. IIoT facilitates real-time data acquisition, processing and analytics over large amount of sensor data streams produced by sensors installed within a smart factory, while the `digital twin' concept aims to enable smart factories via the digital replication or representation of physical machines, processes, people in cyber-space. This paper explores the capability of present-state open-source platforms to collectively achieve digital twin capabilities, including IoT real-time data acquisition, virtual representation, analytics, and visualisation. The aim of this work is to `close the gap' between research and implementation, through a collective open source IoT and Digital Twin architecture. The performance of the open-source architecture in this work, is demonstrated in a use-case utilising industry `open data', and is bench-marked with universal testing tools.",https://ieeexplore.ieee.org/document/9119497/,2020 Global Internet of Things Summit (GIoTS),3-3 June 2020,ieeexplore
10.1109/CASE48305.2020.9216902,Industrial Robot Grasping with Deep Learning using a Programmable Logic Controller (PLC),IEEE,Conferences,"Universal grasping of a diverse range of previously unseen objects from heaps is a grand challenge in e-commerce order fulfillment, manufacturing, and home service robotics. Recently, deep learning based grasping approaches have demonstrated results that make them increasingly interesting for industrial deployments. This paper explores the problem from an automation systems point-of-view. We develop a robotics grasping system using Dex-Net, which is fully integrated at the controller level. Two neural networks are deployed on a novel industrial AI hardware acceleration module close to a PLC with a power footprint of less than 10 W for the overall system. The software is tightly integrated with the hardware allowing for fast and efficient data processing and real-time communication. The success rate of grasping an object form a bin is up to 95% with more than 350 picks per hour, if object and receptive bins are in close proximity. The system was presented at the Hannover Fair 2019 (world's largest industrial trade fair) and other events, where it performed over 5,000 grasps per event.",https://ieeexplore.ieee.org/document/9216902/,2020 IEEE 16th International Conference on Automation Science and Engineering (CASE),20-21 Aug. 2020,ieeexplore
10.1109/IIHMSP.2010.149,Industrial Workflows Recognition by Computer Vision and AI Technologies,IEEE,Conferences,"The introduction of the Gigabit-Ethernet for Machine Vision opened a new road for workflows recognition in industrial and SMEs environments. The network centric approach to running a production level vision system are coming on stream, which provide distributed computing across industrial developments. Humans and machines interaction can be exploited and surveyed using artificial intelligence algorithms applied on signals from multiple standalone agents on camera networks and giving to the end user, a full view of abnormal or alarm events conditions during the workflows execution. This article is presenting the architecture of such a system, where the designer of the workflows, plus the engineer of the workflow surveillance system should consider, in order either to simulate the industrial process offline, or during the real time workflow execution in the SMEs or in the industrial environment. The related methodology is based on Java technologies, which are presented and latest innovations from the multi agents and workflow processes composition.",https://ieeexplore.ieee.org/document/5636265/,2010 Sixth International Conference on Intelligent Information Hiding and Multimedia Signal Processing,15-17 Oct. 2010,ieeexplore
10.1109/ICNN.1995.487585,Industrial computer vision using undefined feature extraction,IEEE,Conferences,This paper presents an application of computer vision in a real-world uncontrolled environment found at BHP Steel Port Kembla. The task is visual identification of torpedo ladles at a blast furnace which is achieved by reading numbers attached to each ladle. Number recognition is achieved through use of feature extraction using a multi-layer perceptron (MLP) artificial neural network (ANN). The novelty in the method used in this application is that the features the MLP is being trained to extract are undefined before the MLP is initialised. The results of the MLP processing are passed to a decision tree for analysis and final classification of each object within the image. This technique is achieving a recognition rate on previously unseen images of greater than 80%.,https://ieeexplore.ieee.org/document/487585/,Proceedings of ICNN'95 - International Conference on Neural Networks,27 Nov.-1 Dec. 1995,ieeexplore
10.1109/PACRIM.1995.519580,Industrial inspection employing a three dimensional vision system and a neural network classifier,IEEE,Conferences,"An automatic inspection system for manufactured parts employing a 3D machine vision system and associated software for part identification and dimensional inspection is described. The machine vision module collects a range image of accurate data from the part surface employing a structured light approach. In order to measure specific surface parameters, the entire part data set is decomposed into its constituent surface patches. A neural network classifier is employed to recognise each part from its range data set and also to classify a specific surface patch, on a part, from the overall set of part surface patches. The output of the neural network classifier is presented to a database of part information which is created off-line. The performance of the system has been tested by experimenting on real range data.",https://ieeexplore.ieee.org/document/519580/,"IEEE Pacific Rim Conference on Communications, Computers, and Signal Processing. Proceedings",17-19 May 1995,ieeexplore
10.1109/YAC51587.2020.9337594,"Internet of Mind with General Intelligent Dispatch in Electrical Industrial: The Concept, Framework, Technology, and Future",IEEE,Conferences,"This paper proposes the concept of general intelligent dispatch, which is a novel framework of electrical power dispatch based on modern artificial intelligence (AI). The general intelligent dispatch system is based on knowledge automation, parallel system, ACP method, and fully integrated with cutting-edge technologies such as parallel deep learning, parallel dynamics programming. The general intelligent dispatch system is devised to be a comprehensive framework that can support the applications of various modern AI technologies in electrical power dispatch. General intelligent dispatch can accommodate the rapid development of the future smart grid, especially the extensive implementation of distributed energy resources and renewable energy. General intelligent dispatch also provides the practical technical path and framework for the future smart power dispatch services to achieve a higher level of automation and intelligence. Based on the general intelligent dispatch concept, this paper addresses power grid situational awareness, which provides an example for the applications of general intelligent dispatch.",https://ieeexplore.ieee.org/document/9337594/,2020 35th Youth Academic Annual Conference of Chinese Association of Automation (YAC),16-18 Oct. 2020,ieeexplore
10.1109/CCGrid51090.2021.00075,IoTwins: Design and Implementation of a Platform for the Management of Digital Twins in Industrial Scenarios,IEEE,Conferences,"With the increase of the volume of data produced by IoT devices, there is a growing demand of applications capable of elaborating data anywhere along the IoT-to-Cloud path (Edge/Fog). In industrial environments, strict real-time constraints require computation to run as close to the data origin as possible (e.g., IoT Gateway or Edge nodes), whilst batch-wise tasks such as Big Data analytics and Machine Learning model training are advised to run on the Cloud, where computing resources are abundant. The H2020 IoTwins project leverages the digital twin concept to implement virtual representation of physical assets (e.g., machine parts, machines, production/control processes) and deliver a software platform that will help enterprises, and in particular SMEs, to build highly innovative, AI-based services that exploit the potential of IoT/Edge/Cloud computing paradigms. In this paper, we discuss the design principles of the IoTwins reference architecture, delving into technical details of its components and offered functionalities, and propose an exemplary software implementation.",https://ieeexplore.ieee.org/document/9499575/,"2021 IEEE/ACM 21st International Symposium on Cluster, Cloud and Internet Computing (CCGrid)",10-13 May 2021,ieeexplore
10.1109/PerComWorkshops51409.2021.9430865,Keyword Spotting for Industrial Control using Deep Learning on Edge Devices,IEEE,Conferences,"Spoken commands promise unique advantages for the control of industrial machinery. Operators are enabled to keep their eyes on safety critical aspects of the process at all times and are free to use their hands in other parts of the process, instead of remote control. Current keyword spotting systems are prone to misunderstanding spoken utterances, especially in noisy environments, and are commonly deployed as non-realtime cloud services. Consequently, these systems can not be trusted with safety critical industrial control. We adapt a DS-CNN and a CNN for keyword spotting and use augmented training data, including real industrial noise, to increase their robustness. Furthermore, we apply post-training quantization and analyze the performance of both networks using multiple embedded systems, including a Google Edge TPU. We carry out a systematic analysis of accuracies, memory footprint and inference times using different combinations of data augmentations, hardware platforms, and quantizations. We show that augmented training data increases the inference accuracy in noisy environments by up to 20 %. Among others, this is demonstrated using an integer quantized network with a memory footprint of 0.57 MByte, reaching inference speeds of less than 5 ms on an embedded CPU and less than 1 ms on the Edge TPU. The results show that keyword spotting for industrial control is feasible on embedded systems and that the training data augmentation has a significant impact on the robustness in challenging environments.",https://ieeexplore.ieee.org/document/9430865/,2021 IEEE International Conference on Pervasive Computing and Communications Workshops and other Affiliated Events (PerCom Workshops),22-26 March 2021,ieeexplore
10.1109/INDIN.2015.7281881,Knowledge-driven finite-state machines. Study case in monitoring industrial equipment,IEEE,Conferences,Traditionally state machines are implemented by coding the desired behavior of a given system. This work proposes the use of ontological models to describe and perform computations on state machines by using SPARQL queries. This approach represents a paradigm shift relating to the customary manner in which state machines are stored and computed. The main contribution of the work is an ontological model to represent state machines and a set of generic queries that can be used in any knowledge-driven state machine to compute valuable information. The approach was tested in a study case were the state machines of industrial robots in a manufacturing line were modeled as ontological models and used for monitoring the behavior of these devices on real time.,https://ieeexplore.ieee.org/document/7281881/,2015 IEEE 13th International Conference on Industrial Informatics (INDIN),22-24 July 2015,ieeexplore
10.1109/ETFA.2019.8869172,Learning based Probabilistic Model for Migration of Industrial Control Systems,IEEE,Conferences,"The updating and upgrading of control systems is a cumbersome, expensive and time consuming task. From a software perspective, control system migration is a collective task of migrating the control logic, Human Machine Interface (HMI) and auxiliary software applications. Migrating control logic is the most challenging task owing to constraints on hard real-time behavior and execution order. Control logic typically contains engineering artifacts that specify the functionality of industrial devices taking into account various parameters. Therefore, to migrate from one Distributed Control System (DCS) system to another or to upgrade the existing DCS, one needs to map the source control entity and their parameters to the appropriate control entities in the target DCS.In this paper, we propose a machine learning based suggestion management system that identifies control entities and parameters for a source DCS and suggests the use of similar control entities and corresponding parameters for the target DCS. This in effect saves effort required in mapping of control parameters and reduces the dependence on subject matter experts. Our system uses a probabilistic approach to find these similarity mappings based on meta-data stored in an Ontology. We further describe a case study implemented for mapping heritage and legacy systems to a modern control system to verify and validate our approach.",https://ieeexplore.ieee.org/document/8869172/,2019 24th IEEE International Conference on Emerging Technologies and Factory Automation (ETFA),10-13 Sept. 2019,ieeexplore
10.1109/INDIN45523.2021.9557472,Learning-based Co-Design of Distributed Edge Sensing and Transmission for Industrial Cyber-Physical Systems,IEEE,Conferences,"Industrial cyber-physical systems (ICPS) refer to an emerging generation of intelligent systems, where distributed data acquisition is of great importance and is influenced by data transmission. In the improvement of the overall performance of sensing accuracy and energy efficiency, sensing and transmission are tightly coupled. Due to the unknown transmission channel states in the harsh industrial field environment, intelligently performing sensor scheduling for distributed sensing is challenging. In this paper, edge computing technology is utilized to enhance the level of intelligence at the edge side and deploy advanced scheduling algorithms. We propose a learning-based distributed edge sensing-transmission co-design (LEST) algorithm under the coordination of the sensors and the edge computing unit (ECU). Deep reinforcement learning is applied to perform real-time sensor scheduling under unknown channel states. The conditions for the existence of feasible scheduling policies are analyzed. The proposed algorithm is applied to estimate the slab temperature in the hot rolling process, which is a typical ICPS. The simulation results demonstrate that the overall performance of LEST is better than other suboptimal algorithms.",https://ieeexplore.ieee.org/document/9557472/,2021 IEEE 19th International Conference on Industrial Informatics (INDIN),21-23 July 2021,ieeexplore
10.1109/ICECCE49384.2020.9179391,Machine Learning Based Approach to Process Characterization for Smart Devices in 3D Industrial Manufacturing,IEEE,Conferences,"A key differentiator between the additive manufacturing and the traditional injection molding is the precision-manufacturing. An error-free print job significantly guarantees good part quality with minimum wastage of the material and energy. In practice, however, achieving error-free production is quite challenging and this emphasizes the need to learn what behavior of the machine leads to an erroneous job. Knowing the health of the printer or the print job helps quantify print job performance, as well as build system alerts to take reactive actions. Intending to run the model dynamically while the machine is printing, time-series based deep learning models like LSTM are most suitable. This paper presents a machine learning based anomaly detection approach to discover patterns in sensors' measurements in the streaming mode in MultiJet Fusion 3d printer developed at HP Inc. A hybrid architecture of LSTM and Auto-encoder has been proposed to learn the printer behavior generate an alarm in the event of an anomaly. The results of both LSTM and LSTM-Autoencoder models have also been discussed by taking real-life examples of 3D printing jobs.",https://ieeexplore.ieee.org/document/9179391/,"2020 International Conference on Electrical, Communication, and Computer Engineering (ICECCE)",12-13 June 2020,ieeexplore
10.1109/TEST.2016.7805855,Machine learning-based defense against process-aware attacks on Industrial Control Systems,IEEE,Conferences,"The modernization of Industrial Control Systems (ICS), primarily targeting increased efficiency and controllability through integration of Information Technologies (IT), introduced the unwanted side effect of extending the ICS cyber-security threat landscape. ICS are facing new security challenges and are exposed to the same vulnerabilities that plague IT, as demonstrated by the increasing number of incidents targeting ICS. Due to the criticality and unique nature of these systems, it is important to devise novel defense mechanisms that incorporate knowledge of the underlying physical model, and can detect attacks in early phases. To this end, we study a benchmark chemical process, and enumerate the various categories of attack vectors and their practical applicability on hardware controllers in a Hardware-In-The-Loop testbed. Leveraging the observed implications of the categorized attacks on the process, as well as the profile of typical disturbances, we follow a data-driven approach to detect anomalies that are early indicators of malicious activity.",https://ieeexplore.ieee.org/document/7805855/,2016 IEEE International Test Conference (ITC),15-17 Nov. 2016,ieeexplore
10.1109/MALTESQUE.2018.8368453,Machine learning-based run-time anomaly detection in software systems: An industrial evaluation,IEEE,Conferences,"Anomalies are an inevitable occurrence while operating enterprise software systems. Traditionally, anomalies are detected by threshold-based alarms for critical metrics, or health probing requests. However, fully automated detection in complex systems is challenging, since it is very difficult to distinguish truly anomalous behavior from normal operation. To this end, the traditional approaches may not be sufficient. Thus, we propose machine learning classifiers to predict the system's health status. We evaluated our approach in an industrial case study, on a large, real-world dataset of 7.5 ·10<sup>6</sup> data points for 231 features. Our results show that recurrent neural networks with long short-term memory (LSTM) are more effective in detecting anomalies and health issues, as compared to other classifiers. We achieved an area under precision-recall curve of 0.44. At the default threshold, we can automatically detect 70% of the anomalies. Despite the low precision of 31 %, the rate in which false positives occur is only 4 %.",https://ieeexplore.ieee.org/document/8368453/,2018 IEEE Workshop on Machine Learning Techniques for Software Quality Evaluation (MaLTeSQuE),20-20 March 2018,ieeexplore
10.1109/ISIE45063.2020.9152407,Modeling and Predicting an Industrial Process Using a Neural Network and Automation Data,IEEE,Conferences,"Production optimization and prevention of faults and unplanned production halts are areas of particular interest in industry. Predictive analysis is commonly implemented with data analytics and machine learning techniques. Usually, the usage of such tools requires knowledge of the machine learning theory and the subject to be studied, e.g. a pumping process. This paper presents a case study on modeling of a pumping process using stored automation data. The model is trained to predict the performance percentage of the process with minimal background knowledge of the process and data analytics. The proposed model is built with IBM SPSS Modeler, a data analysis tool not usually used in real-time industrial predictive analysis as it is not often considered the best tool when working with time series data. The model is deployed in a cloud service to implement a real-time, visualized predictive analysis system. The case study shows that Modeler can be used for data analysis, modeling, and production purposes. Depending on the case, Modeler can provide an alternative tool compared with typical machine learning tools, as models built with Modeler can be deployed into a cloud service for production use. The findings indicate that industrial automation data are a valuable resource, and data analysis can be conducted on various platforms and tools.",https://ieeexplore.ieee.org/document/9152407/,2020 IEEE 29th International Symposium on Industrial Electronics (ISIE),17-19 June 2020,ieeexplore
10.1109/IranianCEE.2017.7985123,Modeling and identification of an industrial gas turbine using classical and non-classical approaches,IEEE,Conferences,"In this paper, data-driven modeling and identification of an industrial, simple cycle, heavy duty Gas Turbine is taken into consideration. The GGOV1 model that was introduced by Western Electricity Coordinating Council (WECC), is suggested here for describing dynamics and behaviour of Gas Turbines. It is shown that GGOV1 model that is expressed as the classical approach, can fulfill our needs in academic studies and engineering purposes. Non-classical identification of Gas turbine is also done via Artificial Neural Networks. Comparison between the two methods and real data shows reliability and acceptable precision of both models.",https://ieeexplore.ieee.org/document/7985123/,2017 Iranian Conference on Electrical Engineering (ICEE),2-4 May 2017,ieeexplore
10.1109/FUZZ.2002.1005041,Multi-axis fuzzy control and performance analysis for an industrial robot,IEEE,Conferences,"Robot control systems can be considered as complex systems, the design of the controller involving the determination of the dynamic model for the system. Fuzzy logic provides functional capability without the use of a system model or the characteristics associated with capturing the approximate, varying values found in real world systems. Development of a multi-axis fuzzy logic control system was implemented on an industrial robot, replacing the existing control and hardware systems with a new developmental system. During robot control no adaptation of the rule base or membership functions was carried out online; only system gain was modified in relation to link speed and joint error within predetermined design parameters. The fuzzy control system had to manage the effects of frictional and gravitational forces whilst compensating for the varying inertia components when each linkage is moving. Testing based on ISO 9283 for path accuracy and repeatability verified that real time control of three axes was achievable with values of 938 /spl mu/m and 864 /spl mu/m recorded for accuracy and repeatability, respectively.",https://ieeexplore.ieee.org/document/1005041/,2002 IEEE World Congress on Computational Intelligence. 2002 IEEE International Conference on Fuzzy Systems. FUZZ-IEEE'02. Proceedings (Cat. No.02CH37291),12-17 May 2002,ieeexplore
10.1109/DSN.2017.34,Multi-level Anomaly Detection in Industrial Control Systems via Package Signatures and LSTM Networks,IEEE,Conferences,"We outline an anomaly detection method for industrial control systems (ICS) that combines the analysis of network package contents that are transacted between ICS nodes and their time-series structure. Specifically, we take advantage of the predictable and regular nature of communication patterns that exist between so-called field devices in ICS networks. By observing a system for a period of time without the presence of anomalies we develop a base-line signature database for general packages. A Bloom filter is used to store the signature database which is then used for package content level anomaly detection. Furthermore, we approach time-series anomaly detection by proposing a stacked Long Short Term Memory (LSTM) network-based softmax classifier which learns to predict the most likely package signatures that are likely to occur given previously seen package traffic. Finally, by the inspection of a real dataset created from a gas pipeline SCADA system, we show that an anomaly detection scheme combining both approaches can achieve higher performance compared to various current state-of-the-art techniques.",https://ieeexplore.ieee.org/document/8023128/,2017 47th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN),26-29 June 2017,ieeexplore
10.1109/ICDMW.2019.00065,Mímir: Building and Deploying an ML Framework for Industrial IoT,IEEE,Conferences,"In this paper we describe Mímir, a production grade cloud and edge spanning ML framework for Industrial IoT applications. We first describe our infrastructure for optimized capture, streaming and multi-resolution storage of manufacturing data and its context. We then describe our workflow for scalable ML model training, validation, and deployment that leverages a manufacturing taxonomy and parameterized ML pipelines to determine the best metrics, hyper-parameters and models to use for a given task. We also discuss our design decisions on model deployment for real-time and batch data in the cloud and at the edge. Finally, we describe the use of the framework in building and deploying an application for Predictive Quality monitoring during a Plastics Extrusion manufacturing process.",https://ieeexplore.ieee.org/document/8955638/,2019 International Conference on Data Mining Workshops (ICDMW),8-11 Nov. 2019,ieeexplore
10.1109/PEDES.2006.344292,Neural Approach for Automatic Identification of Induction Motor Load Torque in Real-Time Industrial Applications,IEEE,Conferences,"Induction motors are widely used in several industrial sectors. However, the dimensioning of induction motors is often inaccurate because, in most cases, the load behavior in the shaft is completely unknown. The proposal of this paper is to use artificial neural networks as a tool for dimensioning induction motors rather than conventional methods, which use classical identification techniques and mechanical load modeling. Since the proposed approach uses current, voltage and speed values as the only input parameters, one of its potentialities is related to the facility of hardware implementation for industrial environments and field applications. Simulation results are also presented to validate the proposed approach.",https://ieeexplore.ieee.org/document/4147999/,"2006 International Conference on Power Electronic, Drives and Energy Systems",12-15 Dec. 2006,ieeexplore
10.1109/ISIE45063.2020.9152527,Next generation control units simplifying industrial machine learning,IEEE,Conferences,"The increasing amount of sensor data creates new possibilities through data-driven projects in industry. The demands on the final solution architecture are different and typically include at least one controller (e.g. PLC). In this paper, we focus on these industrial controllers and identify their possible roles in smart factories: Collection, distribution and preprocessing of field data, execution of intelligence and functionalities to ease rapid prototyping approaches. Furthermore, we specify the capabilities leading to the fulfillment of these roles and present an application example of drive anomaly detection to demonstrate how to setup an industrial controller for machine learning projects.",https://ieeexplore.ieee.org/document/9152527/,2020 IEEE 29th International Symposium on Industrial Electronics (ISIE),17-19 June 2020,ieeexplore
10.1109/ICIEAM.2019.8742984,Objects Geometry Comparative Analysis Method for Industrial Robot Vision System,IEEE,Conferences,"At present, in computer vision systems, neural networks are used to process information received by the system from cameras. The recognition of all objects on the image is an extremely resource-intensive task, the solution of which consumes most of the computing power. For that reason, systems based on neural networks cannot be fully utilized for real-time systems due to limited computing resources. To build real-time computer vision systems, the authors suggested using the contour comparison method. The method allows to supervise the geometry of objects, conduct presorting and screen out defective parts, thereby the pressure on neural networks will reduce. The method is implemented in the Java. The created software performs image processing and objects search on it, that are the most similar to the template. The results of the experiment showed that the desired object is correctly determined on a noisy image and the proposed method can be used to solve the problem of pattern recognition in technical vision systems.",https://ieeexplore.ieee.org/document/8742984/,"2019 International Conference on Industrial Engineering, Applications and Manufacturing (ICIEAM)",25-29 March 2019,ieeexplore
10.1109/ICMA.2019.8816298,Online Learning of the Inverse Dynamics with Parallel Drifting Gaussian Processes: Implementation of an Approach for Feedforward Control of a Parallel Kinematic Industrial Robot,IEEE,Conferences,"The present paper deals with an online approach to learn the inverse dynamics of any robot. This is realized by the use of Gaussian Processes drifting parallel along the system data. An extension by a database enables the efficient use of data points from the past. The central component of this work is the implementation of such a method in a controller in order to achieve the actual goal: the feedforward control of an industrial robot by means of machine learning. This is done by splitting the procedure into two threads running parallel so that the prediction is decoupled from the computing-intensive training of the models. Experiments show that the method reduces the tracking errors more clearly than an elaborately identified rigid body model including friction. For a defined trajectory, the squared areas of the tracking errors of all axes are reduced by more than 54% compared to motion without pre-control. In addition, a highly dynamic pick-and-place experiment is used to investigate the possible changes in system dynamics. Compared to an offline trained model, the approximation error of the proposed online approach is smaller for the remaining time of the experiment after an initial phase. Furthermore, this error is smaller throughout the experiment for online learning with parallel drifting Gaussian Processes than when using a single one.",https://ieeexplore.ieee.org/document/8816298/,2019 IEEE International Conference on Mechatronics and Automation (ICMA),4-7 Aug. 2019,ieeexplore
10.1109/INDIN.2004.1417325,OntoSmartResource: an industrial resource generation in semantic Web,IEEE,Conferences,"Semantic Web is a logical evolution of the existing Web. It was meant to serve for machines as today's Web does for humans. The term ""machines"" according to the existing semantic Web's vocabulary mostly means ""computers"". However industry needs such applications, which consider machines also as embedded computational entities within field devices, personal devices, microwave ovens, etc. In other words, now we should involve the real (industrial) world objects as resources into semantic Web. Still the main object of such a world will be a human, which becoming a resource (not just a user of resources) in the distributed environment. In this paper we introduce an extension of the semantic Web resources to a new generation of the enhanced smart semantic Web resources (OntoSmartResources). We consider following aspects as: agent-driven resource behavior mechanism, resource semantic description and maintenance, and some aspects of the human resources related to representation and adaptation",https://ieeexplore.ieee.org/document/1417325/,"2nd IEEE International Conference on Industrial Informatics, 2004. INDIN '04. 2004",24-26 June 2004,ieeexplore
10.1109/INDIN.2006.275808,"Ontology for Cognitics, Closed-Loop Agility Constraint, and Case Study - a Mobile Robot with Industrial-Grade Components",IEEE,Conferences,"The paper refers to intelligent industrial automation. The objective is to present key elements and methods for best practice, as well as some results obtained. The first part presents an ontology for automated cognition (cognitics), where, based on information and time, the main cognitive concepts, including those of complexity, knowledge, expertise, learning, intelligence abstraction, and concretization are rigorously defined, along with corresponding metrics and specific units. Among important conclusions at this point are the fact that reality is much too complex to be approached better than through much simplified models, in very restricted contexts. Another conclusion is the necessity to be focused on goal. Extensions are made here for group behavior. The second part briefly presents a basic law governing the choice of overall control architecture: achievable performance level of control system in terms of agility, relative to process dynamics, dictates the type of approaches which is suitable, in a spectrum which ranges from simple threshold-based switching, to classical closed-loop calculus (PID, state space multivariable systems, etc.), up to ""impossible"" cases where additional controllers must be considered, leading to cascaded, hierarchical control structures. For complex cases such as latter ones, new tools and methodologies must be designed, as is typical in O<sup>3</sup>NEIDA initiative, at least for software components. Finally, a large part of the paper presents a case study, a mobile robot, i.e. an embedded autonomous system with distributed, networked control, featuring industry-grade components, designed with the main goal of robust functionality. The case illustrates several of the concepts introduced earlier in the paper.",https://ieeexplore.ieee.org/document/4053562/,2006 4th IEEE International Conference on Industrial Informatics,16-18 Aug. 2006,ieeexplore
10.23919/ChiCC.2017.8027747,Optimal operational control for industrial processes based on Q-learning method,IEEE,Conferences,"It is difficult to accurately model productive processes and describe relationship between operational indices and controlled variables for complex modem industrial processes. How to design the optimal setpoints by using only data generated by operational processes, without requiring the knowledge of model parameters of operational processes, poses a challenge on designing optimal setpoints. This paper presents a state-observer based Q-learning algorithm to learn the optimal setpoints by utilizing only data, such that the real operational indices can track the desired values in an approximately optimal manner. A simulation experiment in flotation process is implemented to show the effectiveness of the proposed method.",https://ieeexplore.ieee.org/document/8027747/,2017 36th Chinese Control Conference (CCC),26-28 July 2017,ieeexplore
10.1109/BigDataCongress.2019.00032,"PREMISES, a Scalable Data-Driven Service to Predict Alarms in Slowly-Degrading Multi-Cycle Industrial Processes",IEEE,Conferences,"In recent years, the number of industry-4.0-enabled manufacturing sites has been continuously growing, and both the quantity and variety of signals and data collected in plants are increasing at an unprecedented rate. At the same time, the demand of Big Data processing platforms and analytical tools tailored to manufacturing environments has become more and more prominent. Manufacturing companies are collecting huge amounts of information during the production process through a plethora of sensors and networks. To extract value and actionable knowledge from such precious repositories, suitable data-driven approaches are required. They are expected to improve the production processes by reducing maintenance costs, reliably predicting equipment failures, and avoiding quality degradation. To this aim, Machine Learning techniques tailored for predictive maintenance analysis have been adopted in PREMISES (PREdictive Maintenance service for Industrial procesSES), an innovative framework providing a scalable Big Data service able to predict alarming conditions in slowly-degrading processes characterized by cyclic procedures. PREMISES has been experimentally tested and validated on a real industrial use case, resulting efficient and effective in predicting alarms. The framework has been designed to address the main Big Data and industrial requirements, by being developed on a solid and scalable processing framework, Apache Spark, and supporting the deployment on modularized containers, specifically upon the Docker technology stack.",https://ieeexplore.ieee.org/document/8818217/,2019 IEEE International Congress on Big Data (BigDataCongress),8-13 July 2019,ieeexplore
10.1109/ICATE49685.2021.9465023,PV Monitoring System using Industrial Internet of Things Technologies based on Graphical Programming,IEEE,Conferences,"Photovoltaic Panel Systems are already a commodity, the widespread use of such systems being no longer a pioneering topic. Monitoring these power generation systems is not a trivial task since the hunger of real-time data is growing continuously. The advent of IoT technologies, the ubiquitous presence of communication technologies, including remote areas, are making these solutions easier to develop, deploy and use. This will constitute the nurturing bed for optimization processes, which would eventually rely on Artificial Intelligence. This paper is describing how to design and deploy such data collecting system, emphasizing on its Software Architecture based on graphical programming technologies. Ready-to-run virtual instruments are facilitating real-time measurements and analysis, process data being securely stored and visualized using cloud technologies.",https://ieeexplore.ieee.org/document/9465023/,2021 International Conference on Applied and Theoretical Electricity (ICATE),27-29 May 2021,ieeexplore
10.1109/ICCC51575.2020.9345104,Payload-based Anomaly Detection for Industrial Internet Using Encoder Assisted GAN,IEEE,Conferences,"Payload-based anomaly detection has been proved effective in discovering Internet misbehavior and potential intrusions, but highly relies on the unstructured feature engineering to generalize the distribution of normal payloads. This kind of generalization may not adapt well to the emerging industrial Internet, where the normal behaviors are more diverse and usually embedded in the raw payloads' local structures. In this paper, we tackle this generalization problem and propose a very different solution to payload-based anomaly detection without the need of feature engineering. Our basic idea is to learn the raw structures of normal payloads directly by a generative adversarial network (GAN), in which we have a generator (i.e., a reversed convolutional decoder) to sample raw payloads from a latent space as well as a discriminator (i.e., a convolutional classifier) to guide the generator produce raw payloads approximating the normal structures. We also deploy an assisted convolutional encoder to map the true payloads back to the latent space and combine with the GAN's decoder (i.e., generator) to reconstruct the payload structures. We consider anomalies appear in condition the re-constructed payloads are largely deviated from the true ones, since our encoder-decoder architecture is trained able to rebuild only the normal payload structures. We have evaluated our solution using extensive experiments on real-world industrial Internet datasets, and confirmed its effectiveness in detecting industrial Internet anomalies in the raw payloads.",https://ieeexplore.ieee.org/document/9345104/,2020 IEEE 6th International Conference on Computer and Communications (ICCC),11-14 Dec. 2020,ieeexplore
10.1109/ICICT50816.2021.9358747,Performance Analysis of Frequency Variation System using Drives (VT240s and Axpert Eazy) for Industrial Application,IEEE,Conferences,"The proposed research work describes the general system management and real-time applicability of VFD i.e. Variable frequency drive. The project embodies the automation of 2 styles of VFD series namely VT240S and Axpert eazy. These VFD are going to be managed through PLC. Further, VFD is employed for control. The speed of the motor is often serially and domestically managed by VFD. In domestic management, there will be a switch affiliation within the drive and in serial management, the motor is managed through PLC. The motor will stop by each ways. For serial management, the programs should style in software package and it will be downloaded in PLC and by giving external wire affiliation like Modbus 485 from PLC to VFD, the operation is completed. Recently, the speed of the motor control is emerging as a big issue in industrial automation. Machine control with accurate result is required in industry applications for the design and development of the tools in various domains. Machine speed is used to control and vary the frequency parameter. So, the Variable Frequency Drive [VFD] that has been used to control the speed of the motor by varying the range of frequency is proposed to design a VFD machine system with number of tools and machine learning techniques in the paper.",https://ieeexplore.ieee.org/document/9358747/,2021 6th International Conference on Inventive Computation Technologies (ICICT),20-22 Jan. 2021,ieeexplore
10.1109/METROI4.2019.8792860,Performance evaluation of full-cloud and edge-cloud architectures for Industrial IoT anomaly detection based on deep learning,IEEE,Conferences,"One of the most interesting application of data analysis to industry is the real-time detection of anomalies during production. Industrial IoT paradigm includes all the components to realize predictive systems, like the anomaly detection ones. In this case, the goal is to discover patterns, in a given dataset, that do not resemble the “normal” behavior, to identify faults, malfunctions or the effects of bad maintenance. The use of complex neural networks to implement deep learning algorithm for anomaly detection is very common. The position of the deep learning algorithm is one of the main problem: this kind of algorithm requires both high computational power and data transfer bandwidth, rising serious questions on the system scalability. Data elaboration in the edge domain (i.e. close to the machine) usually reduce data transfer but requires to instantiate expensive physical assets. Cloud computing is usually cheaper but Cloud data transfer is expensive. In this paper a test methodology for the comparison of the two architectures for anomaly detection system is proposed. A real use case is described in order to demonstrate the feasibility. The experimental results show that, by means of the proposed methodology, edge and Cloud solutions implementing deep learning algorithms for industrial applications can be easily evaluated. In details, for the considered use case (with Siemens controller and Microsoft Azure platform) the tradeoff between scalability, communication delay, and bandwidth usage, has been studied. The results show that the full-cloud architecture can outperform the edge-cloud architecture when Cloud computation power is scaled.",https://ieeexplore.ieee.org/document/8792860/,2019 II Workshop on Metrology for Industry 4.0 and IoT (MetroInd4.0&IoT),4-6 June 2019,ieeexplore
10.1109/RTEICT49044.2020.9315660,Prediction of Air Quality in Industrial Area,IEEE,Conferences,"Air quality monitoring and prediction in many industrial and urban areas, it has become one of the most important activities. Owing to different types of pollution, air quality is heavily affected. With increasing air pollution, efficient air quality monitoring models is to be implemented; these models gather data on the concentration of air pollutants. In a proposed approach, to solve three problems- prediction, interpolation and feature analysis, previously these problems were solved using three different models but now in the proposed system can solve these three problems in one model i.e Air Pollutant Prediction. This approach relates to unlabeled spatiotemporal data to enhance interpolation efficiency and air quality prediction. Experiments to test the proposed solution based on the real-time data sources collected by the Karnataka State Pollution Control Board (KSPCB), India. The goal of this research paper is to explore various strategies based on machine learning techniques for monitoring and predicating the air quality.",https://ieeexplore.ieee.org/document/9315660/,"2020 International Conference on Recent Trends on Electronics, Information, Communication & Technology (RTEICT)",12-13 Nov. 2020,ieeexplore
10.1109/IACET.1995.527530,Proceedings IEEE Conference on Industrial Automation and Control Emerging Technology Applications,IEEE,Conferences,Presents the title page of the proceedings record.,https://ieeexplore.ieee.org/document/527530/,Proceedings IEEE Conference on Industrial Automation and Control Emerging Technology Applications,22-27 May 1995,ieeexplore
10.1109/CSEET49119.2020.9206229,Project-Based Learning in a Machine Learning Course with Differentiated Industrial Projects for Various Computer Science Master Programs,IEEE,Conferences,"Graduating computer science students with skills sufficient for industrial needs is a priority in higher education teaching. Project-based approaches are promising to develop practical and social skills, needed to address real-world problems in teams. However, rapid technological transition makes an initial training of contemporary methods challenging. This affects the currently much-discussed machine learning domain as well. The study at hand describes a re-framed teaching approach for a machine learning course, offered to various computer science master programs. Project-based learning is introduced with differentiated projects provided by industrial partners that address the diverse study programs. Course attendees are supported with manuals, tools, and tutoring, passing through the Cross Industry Standard Process for Data Mining (CRISP-DM). Observations made during two iterations are reported, accompanied by a first empiric evaluation of student experiences.",https://ieeexplore.ieee.org/document/9206229/,2020 IEEE 32nd Conference on Software Engineering Education and Training (CSEE&T),9-12 Nov. 2020,ieeexplore
10.1109/SoCPaR.2011.6089156,QoS-oriented Service Management in clouds for large scale industrial activity recognition,IEEE,Conferences,"Motivated by the need of industrial enterprises for supervision services for quality, security and safety guarantee, we have developed an Activity Recognition Framework based on computer vision and machine learning tools, attaining good recognition rates. However, the deployment of multiple cameras to exploit redundancies, the large training set requirements of our time series classification models, as well as general resource limitations together with the emphasis on real-time performance, pose significant challenges and lead us to consider a decentralized approach. We thus adapt our application to a new and innovative real-time enabled framework for service-based infrastructures, which has developed QoS-oriented Service Management mechanisms in order to allow cloud environments to facilitate real-time and interactivity. Deploying the Activity Recognition Framework in a cloud infrastructure can therefore enable it for large scale industrial environments.",https://ieeexplore.ieee.org/document/6089156/,2011 International Conference of Soft Computing and Pattern Recognition (SoCPaR),14-16 Oct. 2011,ieeexplore
10.1109/COGINF.2010.5599677,Quadratic neural unit is a good compromise between linear models and neural networks for industrial applications,IEEE,Conferences,"The paper discusses the quadratic neural unit (QNU) and highlights its attractiveness for industrial applications such as for plant modeling, control, and time series prediction. Linear systems are still often preferred in industrial control applications for their solvable and single solution nature and for the clarity to the most application engineers. Artificial neural networks are powerful cognitive nonlinear tools, but their nonlinear strength is naturally repaid with the local minima problem, overfitting, and high demands for application-correct neural architecture and optimization technique that often require skilled users. The QNU is the important midpoint between linear systems and highly nonlinear neural networks because the QNU is relatively very strong in nonlinear approximation; however, its optimization and performance have fast and convex-like nature, and its mathematical structure and the derivation of the learning rules is very comprehensible and efficient for implementation.",https://ieeexplore.ieee.org/document/5599677/,9th IEEE International Conference on Cognitive Informatics (ICCI'10),7-9 July 2010,ieeexplore
10.1109/RTAS.2019.00033,Re-Thinking CNN Frameworks for Time-Sensitive Autonomous-Driving Applications: Addressing an Industrial Challenge,IEEE,Conferences,"Vision-based perception systems are crucial for profitable autonomous-driving vehicle products. High accuracy in such perception systems is being enabled by rapidly evolving convolution neural networks (CNNs). To achieve a better understanding of its surrounding environment, a vehicle must be provided with full coverage via multiple cameras. However, when processing multiple video streams, existing CNN frameworks often fail to provide enough inference performance, particularly on embedded hardware constrained by size, weight, and power limits. This paper presents the results of an industrial case study that was conducted to re-think the design of CNN software to better utilize available hardware resources. In this study, techniques such as parallelism, pipelining, and the merging of per-camera images into a single composite image were considered in the context of a Drive PX2 embedded hardware platform. The study identifies a combination of techniques that can be applied to increase throughput (number of simultaneous camera streams) without significantly increasing per-frame latency (camera to CNN output) or reducing per-stream accuracy.",https://ieeexplore.ieee.org/document/8743176/,2019 IEEE Real-Time and Embedded Technology and Applications Symposium (RTAS),16-18 April 2019,ieeexplore
10.1109/ICIT.2006.372319,Real Time Classifier For Industrial Wireless Sensor Network Using Neural Networks with Wavelet Preprocessors,IEEE,Conferences,"Wireless sensor node is embedded of computation unit, sensing unit and a radio unit for communication. Amongst three units communication is the largest consumer of energy. Energy is the prime source for wireless sensor node to function. Hence every aspects of sensor node are designed with energy constraints. Neural Networks in particular the combination of ART1 and FuzzyART(FA) can be used very efficiently for developing Real time Classifier. Wireless sensor networks demand for the real time classification of sensor data. In this paper classification technique using ART1 and Fuzzy ART is discussed. ART1 and FA have very good architectural strategy, which makes it simple for VLSI implementation. The VLSI implementation of the proposed classifier can be a part of embedded microsensor. The paper discusses classification technique, which can reduce the energy need for communication and improves communications bandwidth. The proposed sensor clustering architecture can give distributed storage space for the sensor networks. Wavelet Transform is used as preprocessor for denoising the real word data from sensor node, this makes it much suitable for industrial environment. Many methods of wavelet transforms are available. Simplest Haar 1D transform is used for preprocessing and smoothing the sensor signals. The discrete wavelet transform implemented here helps to extract important feature in the sensor data like sudden changes at various scales.",https://ieeexplore.ieee.org/document/4237641/,2006 IEEE International Conference on Industrial Technology,15-17 Dec. 2006,ieeexplore
10.1109/SKIMA47702.2019.8982486,Real-Time Video Dehazing for Industrial Image Processing,IEEE,Conferences,"In today's industries, automation, reliability, robustness and accuracy are pivotal problem to cut costs and increase productivity and quality. Visual sensor networks are vital control and monitoring tools for continues, on-line imaging and real time image processing in production and plant process. Most of the industrial videos are captured in hazy weather and usually degraded by suspended particles of atmosphere, such as smoke, fog, rain, and snow, which limits the visual quality of image. This hinders the ability of artificial intelligent driven systems to achieve automation, reliability and accuracy. Recovery of the clear visuals from the input hazy videos is challenging problem. Instead of relying on explicitly estimating the key component of atmospheric scattering model, we present end-to-end CNN model, which directly recovers the clear images from hazy images. This end-to-end architecture makes it an ideal pre-processing tool into other deep models for increasing the efficiency of various computer vision tasks in real time systems, such as Retina-Net for object detection, ResNet for object recognition. Experimental results demonstrate the effectiveness and robustness of proposed framework by outperforming the stat-of-the-art approaches in terms of time complexity and visual quality.",https://ieeexplore.ieee.org/document/8982486/,"2019 13th International Conference on Software, Knowledge, Information Management and Applications (SKIMA)",26-28 Aug. 2019,ieeexplore
10.1109/ICAR46387.2019.8981549,Real-time RGB-D semantic keyframe SLAM based on image segmentation learning from industrial CAD models,IEEE,Conferences,"This paper presents methods for performing realtime semantic SLAM aimed at autonomous navigation and control of a humanoid robot in a manufacturing scenario. A novel multi-keyframe approach is proposed that simultaneously minimizes a semantic cost based on class-level features in addition to common photometric and geometric costs. The approach is shown to robustly construct a 3D map with associated class labels relevant to robotic tasks. Alternatively to existing approaches, the segmentation of these semantic classes have been learnt using RGB-D sensor data aligned with an industrial CAD manufacturing model to obtain noisy pixel-wise labels. This dataset confronts the proposed approach in a complicated real-world setting and provides insight into the practical use case scenarios. The semantic segmentation network was fine tuned for the given use case and was trained in a semi-supervised manner using noisy labels. The developed software is real-time and integrated with ROS to obtain a complete semantic reconstruction for the control and navigation of the HRP4 robot. Experiments in-situ at the Airbus manufacturing site in Saint-Nazaire validate the proposed approach.",https://ieeexplore.ieee.org/document/8981549/,2019 19th International Conference on Advanced Robotics (ICAR),2-6 Dec. 2019,ieeexplore
10.1109/IECON.2012.6389018,Recent advances in the application of real-time computational intelligence to industrial electronics,IEEE,Conferences,"The field of computational intelligence [CI] has seen advances in both the theoretical knowledge base of these techniques, and in specific applications of these techniques to real-world problems. This work first attempts to summarize the current trends and definitions in the CI branches of fuzzy systems, artificial neural networks [ANNs], and hybrid neuro-fuzzy systems and their variants. These particular branches of CI are selected for their ability to be implemented in real-time problem solving, whether computation and processing is done in software or implemented in hardware. Then, some current applications of these CI technologies for use in industrial electronics are highlighted and summarized.",https://ieeexplore.ieee.org/document/6389018/,IECON 2012 - 38th Annual Conference on IEEE Industrial Electronics Society,25-28 Oct. 2012,ieeexplore
10.1109/DEMPED.2019.8864828,Recognition of Electric Machines Boundary as The Constraint of Over Current Relay Coordination in Real Industrial Application with Serial Firefly Algorithm Optimization,IEEE,Conferences,"The electric machines such as motor, transformer, and generator plays an essential role in the production process of a factory. Due to expansion, there is an intention to invest a new electric machine that expected to work correctly with the existing system. To integrate the new electric machines, the existing protection system has to be evaluated carefully without limiting the machine's capability. Based on the actual experiences, one of the frustrating issues during the commissioning stage is energizing failure due to relay mal-trip. The objective of this paper is to summarize and model the electrical machines in the protection system perspective and offers a new method to coordinate the relays with the electric machines recognition using the artificial intelligence of serial firefly algorithm. This proposed method is endorsed by a real industrial power system and will demonstrate the ability to coordinate the relays without violating the electric machine boundary.",https://ieeexplore.ieee.org/document/8864828/,"2019 IEEE 12th International Symposium on Diagnostics for Electrical Machines, Power Electronics and Drives (SDEMPED)",27-30 Aug. 2019,ieeexplore
10.1109/ICIEAM.2017.8076111,Redundant industrial manipulator control system,IEEE,Conferences,"We present the control system synthesis for the multilink redundant manipulator. Our control system is based on the unique algorithm that includes the novel hybrid method for solving the inverse kinematics problem. This method combines ANFIS-network and iterative refinement. As a result, the control system has high integrative capabilities and is easy to modify for another construction. The manipulator design is described by mathematical equations which are used for the workspace construction. These equations are used for creation of the neurofuzzy network and generation database (network training information). Modeling of the industrial manipulator with 5 degrees of freedom as an example of the implementation of our control system is considered in the paper. Virtual environment that displays a model motion in real time using a virtual 3-D model is also presented in the paper. We present the work results applied to the manipulator physical model. This model includes Festo servomotors and the Siemens programmable logical controller.",https://ieeexplore.ieee.org/document/8076111/,"2017 International Conference on Industrial Engineering, Applications and Manufacturing (ICIEAM)",16-19 May 2017,ieeexplore
10.1109/ICECCS.2015.32,Requirements-Aided Automatic Test Case Generation for Industrial Cyber-physical Systems,IEEE,Conferences,"Industrial cyber-physical systems require complex distributed software to orchestrate many heterogeneous mechatronic components and control multiple physical processes. Industrial automation software is typically developed in a model-driven fashion where abstractions of physical processes called plant models are co-developed and iteratively refined along with the control code. Testing such multi-dimensional systems is extremely difficult because often models might not be accurate, do not correspond accurately with subsequent refinements, and the software must eventually be tested on the real plant, especially in safety-critical systems like nuclear plants. This paper proposes a framework wherein high-level functional requirements are used to automatically generate test cases for designs at all abstraction levels in the model-driven engineering process. Requirements are initially specified in natural language and then analyzed and specified using a formalized ontology. The requirements ontology is then refined along with controller and plant models during design and development stages such that test cases can be generated automatically at any stage. A representative industrial water process system case study illustrates the strengths of the proposed formalism. The requirements meta-model proposed by the CESAR European project is used for requirements engineering while IEC 61131-3 and model-driven concepts are used in the design and development phases. A tool resulting from the proposed framework called REBATE (Requirements Based Automatic Testing Engine) is used to generate and execute test cases for increasingly concrete controller and plant models.",https://ieeexplore.ieee.org/document/7384248/,2015 20th International Conference on Engineering of Complex Computer Systems (ICECCS),9-12 Dec. 2015,ieeexplore
10.1109/ICET51757.2021.9450991,Research on Artificial Intelligence Industrial Big Data Platform for Industrial Internet Applications,IEEE,Conferences,"With the development of industry, enterprises have put forward higher requirements for real-time data collection and analysis. According to this, a distributed industrial big data platform for Industrial Internet applications is designed, which can effectively support the industrial field big data collection and storage. The system contains a distributed database, real-time database and offline big data platform to implement the collection and storage of all data in the system. Thus, the industrial big data platform architecture and related data hierarchical processing flow are proposed, which can effectively meet the requirements of the multi-source data and real-time processing of smart factory. The proposed data platform technical architecture will have important reference value for the realization of smart manufacturing and smart factory.",https://ieeexplore.ieee.org/document/9450991/,2021 IEEE 4th International Conference on Electronics Technology (ICET),7-10 May 2021,ieeexplore
10.1109/ICBASE51474.2020.00070,Research on scheduling algorithm for industrial Internet of Things,IEEE,Conferences,"The continuous development of network and communication technology has a great impact on the national economy, and all countries attach great importance to the development of industrial Internet of things. Among them, the scheduling problem of industrial Internet of Things exists, such as packet transmission delay. In this paper, a TSN scheduling algorithm (NACO algorithm) based on ant colony system is proposed. Simulation experiments, the results show that the algorithm is a good way to solve the scheduling problem of industrial Iot, and compared with traditional algorithm, can avoid falling into local optimal solution, and has better convergence and optimization ability, and has certain delay change, able to provide deterministic time sensitive network real-time guarantees.",https://ieeexplore.ieee.org/document/9403824/,2020 International Conference on Big Data & Artificial Intelligence & Software Engineering (ICBASE),30 Oct.-1 Nov. 2020,ieeexplore
10.1109/SPAC53836.2021.9539961,SDN-based Industrial Internet Security Gateway,IEEE,Conferences,"Industrial Internet is widely used in the production field. As the openness of networks increases, industrial networks facing increasing security risks. Information and communication technologies are now available for most industrial manufacturing. This industry-oriented evolution has driven the emergence of cloud systems, the Internet of Things (IoT), Big Data, and Industry 4.0. However, new technologies are always accompanied by security vulnerabilities, which often expose unpredictable risks. Industrial safety has become one of the most essential and challenging requirements. In this article, we highlight the serious challenges facing Industry 4.0, introduce industrial security issues and present the current awareness of security within the industry. In this paper, we propose solutions for the anomaly detection and defense of the industrial Internet based on the demand characteristics of network security, the main types of intrusions and their vulnerability characteristics. The main work is as follows: This paper first analyzes the basic network security issues, including the network security needs, the security threats and the solutions. Secondly, the security requirements of the industrial Internet are analyzed with the characteristics of industrial sites. Then, the threats and attacks on the network are analyzed, i.e., system-related threats and process-related threats; finally, the current research status is introduced from the perspective of network protection, and the research angle of this paper, i.e., network anomaly detection and network defense, is proposed in conjunction with relevant standards. This paper proposes a software-defined network (SDN)-based industrial Internet security gateway for the security protection of the industrial Internet. Since there are some known types of attacks in the industrial network, in order to fully exploit the effective information, we combine the ExtratreesClassifier to enhance the detection rate of anomaly detection. In order to verify the effectiveness of the algorithm, this paper simulates an industrial network attack, using the acquired training data for testing. The test data are industrial network traffic datasets, and the experimental results show that the algorithm is suitable for anomaly detection in industrial networks.",https://ieeexplore.ieee.org/document/9539961/,"2021 International Conference on Security, Pattern Analysis, and Cybernetics（SPAC)",18-20 June 2021,ieeexplore
10.1109/WF-IoT.2016.7845468,SQenloT: Semantic query engine for industrial Internet-of-Things gateways,IEEE,Conferences,"The Advent of Internet-of-Things (IoT) paradigm has brought exciting opportunities to solve many real-world problems. IoT in industries is poised to play an important role not only to increase productivity and efficiency but also to improve customer experiences. Two main challenges that are of particular interest to industry include: handling device heterogeneity and getting contextual information to make informed decisions. These challenges can be addressed by IoT along with proven technologies like the Semantic Web. In this paper, we present our work, SQenIoT: a Semantic Query Engine for Industrial IoT. SQenIoT resides on a commercial product and offers query capabilities to retrieve information regarding the connected things in a given facility. We also propose a things query language, targeted for resource-constrained gateways and non-technical personnel such as facility managers. Two other contributions include multi-level ontologies and mechanisms for semantic tagging in our commercial products. The implementation details of SQenIoT and its performance results are also presented.",https://ieeexplore.ieee.org/document/7845468/,2016 IEEE 3rd World Forum on Internet of Things (WF-IoT),12-14 Dec. 2016,ieeexplore
10.1109/ETFA.2012.6489781,Semantic design and integration of simulation models in the industrial automation area,IEEE,Conferences,"Simulations are software tools approximating and predicting the behavior of real industrial plants. Unlike real plants, the utilization of simulations cannot cause damages and it saves time and costs during series of experiments. A shortcoming of current simulation models is the complicated runtime integration into legacy industrial systems and platforms, as well as ad-hoc design phase, introducing manual and error-prone work. This paper contributes to improve the efficiency of simulation model design and integration. It utilizes a semantic knowledge base, implemented by ontologies and their mappings. The integration uses the Automation Service Bus and the paper explains how to configure the runtime integration level semantically. The main contributions are the concept of semantic configuration of the service bus and the workflows of simulation design and integration.",https://ieeexplore.ieee.org/document/6489781/,Proceedings of 2012 IEEE 17th International Conference on Emerging Technologies & Factory Automation (ETFA 2012),17-21 Sept. 2012,ieeexplore
10.1109/IECON.2016.7793206,Summer school on intelligent agents in automation: Hands-on educational experience on deploying industrial agents,IEEE,Conferences,"Cyber-physical systems constitutes a framework to develop intelligent, distributed, resilient, collaborative and cooperative systems, promoting the fusion of computational entities and physical devices. Agent technology plays a crucial role to develop this kind of systems by offering a decentralized, distributed, modular, robust and reconfigurable control structure. This paper describes the implementation of a summer school aiming to enhance the participants' knowledge in the field of multi-agent systems applied to industrial environments, being able to gain the necessary theoretical and practical skills to develop real industrial agent based applications. This is accomplished in an international framework where individual knowledge and experiences are shared in a complementary level.",https://ieeexplore.ieee.org/document/7793206/,IECON 2016 - 42nd Annual Conference of the IEEE Industrial Electronics Society,23-26 Oct. 2016,ieeexplore
10.1109/ICSE-Companion.2019.00131,Testing Untestable Neural Machine Translation: An Industrial Case,IEEE,Conferences,"Neural Machine Translation (NMT) has shown great advantages and is becoming increasingly popular. However, in practice, NMT often produces unexpected translation failures in its translations. While reference-based black-box system testing has been a common practice for NMT quality assurance during development, an increasingly critical industrial practice, named in-vivo testing, exposes unseen types or instances of translation failures when real users are using a deployed industrial NMT system. To fill the gap of lacking test oracles for in-vivo testing of NMT systems, we propose a new methodology for automatically identifying translation failures without reference translations. Our evaluation conducted on real-world datasets shows that our methodology effectively detects several targeted types of translation failures. Our experiences on deploying our methodology in both production and development environments of WeChat (a messenger app with over one billion monthly active users) demonstrate high effectiveness of our methodology along with high industry impact.",https://ieeexplore.ieee.org/document/8802818/,2019 IEEE/ACM 41st International Conference on Software Engineering: Companion Proceedings (ICSE-Companion),25-31 May 2019,ieeexplore
10.1109/AIVR.2018.00024,The Virtual Factory: Hologram-Enabled Control and Monitoring of Industrial IoT Devices,IEEE,Conferences,"Augmented reality (AR) has been exploited in manifold fields but is yet to be used at its full potential. With the massive diffusion of smart devices, opportunities to build immersive human-computer interfaces are continually expanding. In this study, we conceptualize a virtual factory: an interactive, dynamic, holographic abstraction of the physical machines deployed in a factory. Through our prototype implementation, we conducted a user-study driven evaluation of holographic interfaces compared to traditional interfaces, highlighting its pros and cons. Our study shows that the majority of the participants found holographic manipulation more attractive and natural to interact with. However, current performance characteristics of head-mounted displays must be improved to be applied in production.",https://ieeexplore.ieee.org/document/8613643/,2018 IEEE International Conference on Artificial Intelligence and Virtual Reality (AIVR),10-12 Dec. 2018,ieeexplore
10.1109/WSC.2017.8248046,The role of learning on industrial simulation design and analysis,IEEE,Conferences,"The capability of modeling real-world system operations has turned simulation into an indispensable problem-solving methodology for business system design and analysis. Today, simulation supports decisions ranging from sourcing to operations to finance, starting at the strategic level and proceeding towards tactical and operational levels of decision-making. In such a dynamic setting, the practice of simulation goes beyond being a static problem-solving exercise and requires integration with learning. This article discusses the role of learning in simulation design and analysis motivated by the needs of industrial problems and describes how selected tools of statistical learning can be utilized for this purpose.",https://ieeexplore.ieee.org/document/8248046/,2017 Winter Simulation Conference (WSC),3-6 Dec. 2017,ieeexplore
10.1109/ISIE.2008.4677173,Three layer model for digital coal mine based on industrial ethernet,IEEE,Conferences,"In order to construct digital coal mine, a three layer model for digital coal mine model is proposed in this paper. Two basic platforms, the uniform transmission network platform and the uniform data warehouse platform, are discussed. A real 1000 M industrial Ethernet transmission platform based on Siemens PROFINET is established for Yangchangwan coal mine. The network platform is an information superhighway to integrate all existing and new automation subsystems and to provide standard interfaces for future subsystems. It established a uniform hardware and software platform in all aspects from network, data structure and management decision-making. Therefore the automation system island and information island problems in traditional mine automation systems are avoided effectively. It builds a stable foundation of digital coal mine for Yangchangwan coal mine.",https://ieeexplore.ieee.org/document/4677173/,2008 IEEE International Symposium on Industrial Electronics,30 June-2 July 2008,ieeexplore
10.1109/CoASE.2014.6899348,Toward safe close-proximity human-robot interaction with standard industrial robots,IEEE,Conferences,"Allowing humans and robots to interact in close proximity to each other has great potential for increasing the effectiveness of human-robot teams across a large variety of domains. However, as we move toward enabling humans and robots to interact at ever-decreasing distances of separation, effective safety technologies must also be developed. While new, inherently human-safe robot designs have been established, millions of industrial robots are already deployed worldwide, which makes it attractive to develop technologies that can turn these standard industrial robots into human-safe platforms. In this work, we present a real-time safety system capable of allowing safe human-robot interaction at very low distances of separation, without the need for robot hardware modification or replacement. By leveraging known robot joint angle values and accurate measurements of human positioning in the workspace, we can achieve precise robot speed adjustment by utilizing real-time measurements of separation distance. This, in turn, allows for collision prevention in a manner comfortable for the human user.We demonstrate our system achieves latencies below 9.64 ms with 95% probability, 11.10 ms with 99% probability, and 14.08 ms with 99.99% probability, resulting in robust real-time performance.",https://ieeexplore.ieee.org/document/6899348/,2014 IEEE International Conference on Automation Science and Engineering (CASE),18-22 Aug. 2014,ieeexplore
10.1109/RTSI50628.2021.9597339,Towards Graph Machine Learning for Smart Grid Knowledge Graphs in Industrial Scenarios,IEEE,Conferences,"Knowledge Graphs (KGs) demonstrated promising application perspective in different scenarios, especially when combined with Graph Machine Learning (GML) techniques able to interpret and infer over facts. Given the natural network structures of Smart Grid equipment and the exponential growth of electric power data, Smart Grid Knowledge Graphs (SGKGs) provides unprecedented opportunities to manage massive power resources and provide intelligent applications. However, a single representation of the SGKGs is never sufficient to properly exploit GML techniques that leverage different aspects of the KG for various objectives. In this work, we provide a methodology to extract various significant views of the SGKG by iteratively applying a series of transformation to the description of the power network in the IEC CIM standard. Our implementation is based on a declarative approach to guarantee easier portability, and we deploy the transformations as a stateless microservice, facilitating modular integration with the rest of the Smart Grid Semantic Platform. Experimental evaluation on two real power distribution networks demonstrates the efficacy of our approach in highlighting important topological information, without discarding precious additional knowledge present in the SGKG.",https://ieeexplore.ieee.org/document/9597339/,2021 IEEE 6th International Forum on Research and Technology for Society and Industry (RTSI),6-9 Sept. 2021,ieeexplore
10.1109/AIKE.2018.00037,"Towards Low-Cost, Real-Time, Distributed Signal and Data Processing for Artificial Intelligence Applications at Edges of Large Industrial and Internet Networks",IEEE,Conferences,"Digital Signal Processors (DSP) are vital system components in industrial Artificial Intelligence (AI) applications. In this paper, FIR filters that could be used for industrial AI applications are designed from the Spline Biorthogonal 1.5 (SB1.5) mother wavelet using a real-time, low-cost, generic industrial IoT (IIoT) hardware: the C28x DSP and low-level, Embedded C, system software. Our contribution in this paper is the first reported application of the C28x for SB1.5 wavelet construction. The significance of this approach is to be able to repurpose low-cost, readily available hardware for distributed AI applications. Our approach is different from the state of the art, in which specialized hardware are always manufactured for implementing AI applications at large network edges. Our approach supports low-cost and fast single-stage FIR implementation suitable for use in real-time, distributed AI application at network edges, since in our case, successive recursion of FIR filters leading to a full implementation of Pyramid Algorithm is not implemented. The designed FIR filter is evaluated and found capable of both low-pass and high pass filtering operations. Results of this paper indicate that the C28x real-time DSP, which exists in many IoT devices, could have improved scalability by being deployed for other important AI and IoT network edge analytic applications, different from its present uses.",https://ieeexplore.ieee.org/document/8527469/,2018 IEEE First International Conference on Artificial Intelligence and Knowledge Engineering (AIKE),26-28 Sept. 2018,ieeexplore
10.1109/COASE.2018.8560470,Training CNNs from Synthetic Data for Part Handling in Industrial Environments,IEEE,Conferences,"As Convolutional Neural Network based models become reliable and efficient, two questions arise in relation to their applications for industrial purposes. The usefulness of these models in industrial environments and their implementation in these settings. This paper describes the autonomous generation of Region based CNN models trained on images from rendered CAD models and examines their applicability and performance for part handling application. The development of the automated synthetic data generation is detailed and two CNN models are trained with the aim to detect a car component and differentiate it against another similar looking part. The performance of these models is tested on real images and it was found that the proposed approach can be easily adopted for detecting a range of parts in arbitrary backgrounds. Moreover, the use of syntheic images for training CNNs automates the process of generating a detector.",https://ieeexplore.ieee.org/document/8560470/,2018 IEEE 14th International Conference on Automation Science and Engineering (CASE),20-24 Aug. 2018,ieeexplore
10.1109/ICCAIRO.2017.61,Tuning Software Based on Genetic Algorithm Applied to Industrial PID Loops,IEEE,Conferences,"Time-delay processes are frequently found in industry and the most common representations are first order plus delay time (FOPDT) and integrator plus delay time (IPDT) transfer functions. The identification of time delay systems is a challenging task and usually, the performance of its control is not optimal. This work presents a software based on a real coded genetic algorithm to identify the system, using open or close loop information, and to tune the PID controller using several methods. Results on simulation and real industrial loops are presented.",https://ieeexplore.ieee.org/document/8253004/,"2017 International Conference on Control, Artificial Intelligence, Robotics & Optimization (ICCAIRO)",20-22 May 2017,ieeexplore
10.1109/ICAIIC48513.2020.9065203,UAV-assisted Real-time Data Processing using Deep Q-Network for Industrial Internet of Things,IEEE,Conferences,"Industrial internet of things (IIoT) enables edge computing technology to provide communication between the machines that produce a large amount of data and locate at the edge network. A task scheduling is implemented in the edge node. Furthermore, the real-time data can achieve with the lowest latency that allowed by the edge node near the edge network. However, a mobile machine such as an autonomous guided vehicle can interfere in this situation. Because the vehicle also needs service by the edge node. Over that, quality of service (QoS) performance can decrease. Therefore, this paper deploys an unmanned aerial vehicle (UAV) as an edge node to provide service to the edge network through optimizing the trajectory of UAV, where the edge network request task using a Deep Q-Network (DQN) Learning. The result shows that using machine learning, notably the DQN algorithm, can increase the number of the machine that can be provided service. Subsequently, the real-time data can achieve either the interrupt occurs at the edge node.",https://ieeexplore.ieee.org/document/9065203/,2020 International Conference on Artificial Intelligence in Information and Communication (ICAIIC),19-21 Feb. 2020,ieeexplore
10.1109/CSCI.2015.116,User-Centric Workflow Ergonomics in Industrial Environments: Concept and Architecture of an Assistance System,IEEE,Conferences,"Changes in demographic developments come along with an ageing workforce and a higher retirement age. Particularly in the industrial working environment, poor workplace ergonomics can limit the workers' quality of health and thus their ability to work until they reach their statutory retirement age. This paper presents the conceptual design of an assistance system that captures information from workers' bearings based on on-body sensors. A real-time analysis of the captured sensor data enables giving feedback to workers at the assembly site as soon as they get into an ergonomic unhealthy position during workflow execution. Workflow managers benefit from a holistic evaluation of the captured ergonomic data. By this means, critical workflow activities that are characterized by a high degree of malpositions can be identified. Based on this information, workflow managers have the possibility to optimize workflows in an ergonomic-friendly way. Furthermore, the presented approach enables a differentiated evaluation of established methods of ergonomic feedback like OWAS or EAWS.",https://ieeexplore.ieee.org/document/7424190/,2015 International Conference on Computational Science and Computational Intelligence (CSCI),7-9 Dec. 2015,ieeexplore
10.1049/cp:19940284,Using expert systems for on-line data qualification and state variable estimation for an industrial fermentation process,IET,Conferences,"An industrial fed batch fermentation process is a nonlinear time-varying process. Important internal state variables such as biomass, substrate and product concentrations cannot be measured online and are usually determined by infrequent and time consuming off-line laboratory analysis. The online measurements are usually noisy and sometimes this leads to misinterpretation of the real situation inside the fermenter. These problems can lead to poor control of the batch and low productivity subsequently. To overcome these problems a real time expert system has been proposed which is based on the Poplog Flex real time expert system shell. The system is used to monitor the state variables of the process, diagnose any fault that might occur in the process, estimate the important unmeasurable state variables and to design a controller to control the state around a desired level. A neural network has been adopted for the online estimation of the unmeasurable state variables. Pattern recognition ideas have been used to improve the modelling ability of the neural network. Predictive control techniques have been used to control the state around a desired level. The model and the controller for the process have been designed and implemented within the Poplog Flex environment.&lt;<ETX>&gt;</ETX>",https://ieeexplore.ieee.org/document/327321/,1994 International Conference on Control - Control '94.,21-24 March 1994,ieeexplore
10.1109/DCOSS.2019.00126,Virtual Light Sensors in Industrial Environment Based on Machine Learning Algorithms,IEEE,Conferences,"Internet of Things (IoT) has become the backbone of current and future emerging applications both in the public and the private, industrial sector. The IoT paradigm, enhanced with intelligence and big data analytics, has found applications in a wide range of solutions such as smart home, smart city, industrial IoT etc. Even though IoT implies that cheap motes can conduct a specific task, thus a large number of them can be deployed, we aim to minimize the installed hardware while we still have a high level of quality of service. Machine Learning algorithms can support this challenge by generating virtual data via utilization of real data from a smaller subset of sensors. The generated data can replicate sensor behavior which would otherwise be difficult or impossible to track. It is also possible to use simulation models for data analysis model validation, by generating new data under varying conditions. In this paper, we propose a concept of an IoT testbed which allows virtual IoT resources to be immersed and tested in real life conditions, which are met in everyday life. Additionally, the features of the implemented testbed prototype are discussed while taking into account specific use cases, regarding luminosity scenarios in industrial environments.",https://ieeexplore.ieee.org/document/8804746/,2019 15th International Conference on Distributed Computing in Sensor Systems (DCOSS),29-31 May 2019,ieeexplore
10.1109/ITHET.2012.6246058,Virtual industrial training: Joining innovative interfaces with plant modeling,IEEE,Conferences,"Training in industry is one of the most critical and expensive tasks to be faced by the management. Furthermore, in some cases, it is dangerous or even impossible to directly train operators on the real plants where security and safety problems may arise, making it very difficult to start training programs at low cost. For these reasons, the field of training in industry is rapidly developing using software or hardware solutions coming mainly from the following research areas: i) Human-Computer interaction, i.e., the use of complex and interactive human-machine interfaces, ii) plant simulators, i.e., software systems which are delivered with the plant itself to test and to learn complex tasks and processes, iii) Intelligent Training Systems, i.e., the availability of intelligent and personalized training systems where a virtual tutor guides users through a personalized learning path. In this paper we present the overall architecture of a system for industrial training, embedded into an Intelligent Tutoring System that can provide more effective and personalized training and learning in a context where working directly on real plants can be difficult and very expensive. In particular we present a simulator for training operators in using power plants, based on a multimedia and on interactive interface. This system is particularly suitable to be used for training in industrial electric and oil plants. Moreover, the system allows operators for collaborative problem solving. Currently the system is under delivery to an Italian Electric industry.",https://ieeexplore.ieee.org/document/6246058/,2012 International Conference on Information Technology Based Higher Education and Training (ITHET),21-23 June 2012,ieeexplore
10.1109/TII.2021.3077865,A Data Stream Cleaning System Using Edge Intelligence for Smart City Industrial Environments,IEEE,Journals,"Cities are becoming smarter because of recent advances in artificial intelligence and the Internet of Things. However, heterogeneous data source in smart cities are continuously producing low-quality data, and ever-growing applications have greater real-time requirements. Therefore, this article proposes a data stream cleaning system (named DSCS) using edge intelligence to utilize the advantages of cloud servers and edge devices. The DSCS in edge nodes consists of a dynamic protocol interpreter, a structure parser, and a cleaning model activator. Meanwhile, a cloud server, which has pools of protocol and structured programs and cleaning models, supports the edge nodes to adapt massive heterogeneous data sources. To validate the proposed data cleaning system, we applied it to two scenarios: monitoring the injection molding machines, and base stations. The DSCS can have a stable processing time when the number of accessed edge devices is increased, as well as a good cleaning effect.",https://ieeexplore.ieee.org/document/9424956/,IEEE Transactions on Industrial Informatics,Feb. 2022,ieeexplore
10.1109/TII.2020.2995766,A Directed Edge Weight Prediction Model Using Decision Tree Ensembles in Industrial Internet of Things,IEEE,Journals,"As the application of the industrial Internet of Things (IIoT) becomes more widespread, the IIoT is being combined with social networks. Nodes in the network can be users, machines, and so on. Using the sensing detection technology of the IIoT, industrial machines can realize real-time informatization, which is convenient for users to perform remote management. Nodes can communicate with each other and make ratings. These ratings can be modeled as directed weighted edges between nodes and form directed weighted networks (DWNs). The edge weight represents the “strength” of relationship and the direction of edge points from the edge generator to the edge receiver. Predicting edge weights in DWNs is critical to predicting unknown ratings or recovering lost data. In this article, we propose a directed edge weight prediction model (DEWP) using decision tree ensembles. It extends the local similarity indices to DWNs and extracts a series of similarity indices between nodes as features of each edge. These features are used to construct a blended regression model of random forest, gradient boost decision tree, extreme gradient boosting, and light gradient boosting machine. The proposed algorithm was evaluated experimentally with the Bitcoin OTC and Bitcoin Alpha datasets by removing 10% to 90% of edges in the original network. Compared with other classical algorithms, DEWP has higher prediction accuracy and robustness.",https://ieeexplore.ieee.org/document/9096533/,IEEE Transactions on Industrial Informatics,March 2021,ieeexplore
10.1109/ACCESS.2019.2945337,A Linearization Model of Turbofan Engine for Intelligent Analysis Towards Industrial Internet of Things,IEEE,Journals,"Big data processing technologies, e.g., multi-sensor data fusion and cloud computing are being widely used in research, development, manufacturing, health monitoring and maintenance of aero-engines, driven by the ever-rapid development of intelligent manufacturing and Industrial Internet of Things (IIoT). This has promoted rapid development of the aircraft engine industry, increasing the aircraft engine safety, reliability and intelligence. At present, the aero-engine data computing and processing platform used in the industrial Internet of things is not complete, and the numerical calculation and control of aero-engine are inseparable from the linear model, while the existing aero-engine model linearization method is not accurate enough to quickly calculate the dynamic process parameters of the engine. Therefore, in this paper, we propose a linear model of turbofan engine for intelligent analysis in IIoT, with the aim to provide a new perspective for the analysis of engine dynamics. The construction of the proposed model includes three steps: First, a nonlinear mathematical model of a turbofan engine is established by adopting the component modeling approach. Then, numerous parameters of the turbofan engine components and their operating data are obtained by simulating various working conditions. Finally, based on the simulated data for the engine under these conditions, the model at the points during the dynamic process is linearized, such that a dynamic real-time linearized model of turbofan engine is obtained. Simulation results show that the proposed model can accurately depict the dynamic process of the turbofan engine and provide a valuable reference for designing the aero-engine control system and supporting intelligent analysis in IIoT.",https://ieeexplore.ieee.org/document/8856194/,IEEE Access,2019,ieeexplore
10.1109/TII.2020.2992728,A Projective and Discriminative Dictionary Learning for High-Dimensional Process Monitoring With Industrial Applications,IEEE,Journals,"Data-driven process monitoring methods have attracted many attentions and gained wide applications. However, the real industrial process data are much more complex which is characterized by multimode, high dimensional, corrupted, and less labeled data. In order to eliminate these unfavourable factors simultaneously, a semisupervised robust projective and discriminative dictionary learning method is proposed. First, a semisupervised strategy is introduced to label unsupervised training data. Then, by utilizing low-rank and sparse features of raw data and outliers, a robust decomposition method is used to obtain clean data. After that, a simultaneously projective and discriminative model is proposed to extracting the feature of the low-rank clean data. Finally, the projection matrix and global dictionary, as well as the threshold are obtained through iterative dictionary learning. This hybrid framework provides a robust model for process monitoring and mode identification, and its efficiency is demonstrated by both synthetic examples and real industrial process cases.",https://ieeexplore.ieee.org/document/9088293/,IEEE Transactions on Industrial Informatics,Jan. 2021,ieeexplore
10.1109/ACCESS.2019.2963723,A Smart Collaborative Routing Protocol for Delay Sensitive Applications in Industrial IoT,IEEE,Journals,"In the industrial Internet of things (IIoT), there is always a strong demand for real-time information transfer. Especially when deploying wireless/wired hybrid networks in smart factories, the requirement for low delay interaction is more prominent. Although tree routing protocols have been successfully executed in simple networks, more challenges in transmission speed can be observed in the manufacturing broadband communication system. Motivated by the progresses in deep learning, a smart collaborative routing protocol with low delay and high reliability is proposed to accommodate mixed link scenarios. First, we establish a one-hop delay model to investigate the potential affects of Media Access Control (MAC) layer parameters, which supports the subsequent design. Second, forwarding, maintenance, and efficiency strategies are created to construct the basic functionalities for our routing protocol. Relevant procedures and key approaches are highlighted as well. Third, two sub-protocols are generated and the corresponding implementation steps are described. The experimental results demonstrate that the end-to-end delay can be effectively cut down through comprehensive improvements. Even more sensor nodes and larger network scale are involved, our proposed protocol can still illustrate the advantages comparing with existing solutions within IIoT.",https://ieeexplore.ieee.org/document/8949516/,IEEE Access,2020,ieeexplore
10.1109/TAC.1981.1102738,A digital quality control system for an industrial dry process rotary cement kiln,IEEE,Journals,"A multivariate autoregressive moving average (ARMA) model for an industrial dry process rotary cement kiln is identified, from real process data, using the maximum likelihood method. The model obtained is then used in computing a controller for quality control of clinker production. It is shown that it is relevant to compute a minimum variance controller subject to restrictions both in the controller structure and the variances of the control signals. The resulting controller is finally implemented on the cement kiln process, together with a target adaptive controller for automatic adjustment of the clinker quality setpoint, in order to save energy.",https://ieeexplore.ieee.org/document/1102738/,IEEE Transactions on Automatic Control,August 1981,ieeexplore
10.1109/ACCESS.2020.2992249,An Ensemble Deep Learning-Based Cyber-Attack Detection in Industrial Control System,IEEE,Journals,"The integration of communication networks and the Internet of Things (IoT) in Industrial Control Systems (ICSs) increases their vulnerability towards cyber-attacks, causing devastating outcomes. Traditional Intrusion Detection Systems (IDSs), which are mainly developed to support information technology systems, count vastly on predefined models and are trained mostly on specific cyber-attacks. Besides, most IDSs do not consider the imbalanced nature of ICS datasets, thereby suffering from low accuracy and high false-positive when being put to use. In this paper, we propose a deep learning model to construct new balanced representations of the imbalanced datasets. The new representations are fed into an ensemble deep learning attack detection model specifically designed for an ICS environment. The proposed attack detection model leverages Deep Neural Network (DNN) and Decision Tree (DT) classifiers to detect cyber-attacks from the new representations. The performance of the proposed model is evaluated based on 10-fold cross-validation on two real ICS datasets. The results show that the proposed method outperforms conventional classifiers, including Random Forest (RF), DNN, and AdaBoost, as well as recent existing models in the literature. The proposed approach is a generalized technique, which can be implemented in existing ICS infrastructures with minimum effort.",https://ieeexplore.ieee.org/document/9086038/,IEEE Access,2020,ieeexplore
10.1109/TII.2019.2959021,An Integrated Histogram-Based Vision and Machine-Learning Classification Model for Industrial Emulsion Processing,IEEE,Journals,"Existing techniques in emulsion quality evaluation are found to be highly subjective, time-consuming, and prone to overprocessing. Other conventional droplet analysis techniques such as laser diffraction, which require dilution of samples, introduce an additional complexity to industrial processes. The possibility of developing a fully automated technique for droplet characterization during emulsification holds remarkable potential for overcoming the existing challenges. In this article, a histogram-based image segmentation technique detects droplets from emulsion micrographs. The evolution of droplet characteristics and their significance are studied by performing statistical analysis, and the significant characteristics are selected. The principal component analysis is applied to obtain a reduced set of uncorrelated components from the selected characteristics. The linear discriminant analysis classifies the micrographs into a set of quality categories called target, acceptable, marginal, and unacceptable. The model accuracy is validated using stratified five-fold cross-validation and is successful in classifying the micrographs obtained from two different manufacturing facilities with high accuracy up to 100%. The histogram-based technique is successful in detecting smaller droplets than previously reflected in the literature. The current approach is fully automated and is implemented as a soft-sensor, which supports its real-time deployment into an industrial environment. The entire approach has promising potential in the in-line prediction of emulsion quality leading to more efficient and sustainable manufacturing.",https://ieeexplore.ieee.org/document/8968624/,IEEE Transactions on Industrial Informatics,Sept. 2020,ieeexplore
10.1109/ACCESS.2017.2770180,An Integrated Industrial Ethernet Solution for the Implementation of Smart Factory,IEEE,Journals,"Smart factory addresses the vertical integration of physical entities and information systems. Network and cloud are two essential infrastructures to achieve this goal. Among them, the network provides interconnection for communication and data flow, while the cloud provides powerful and elastic computing and storage abilities for big data and intelligent applications. This paper presents a cloud-centric framework for the implementation of the smart factory. Based on this framework, three high leveled protocols viz., EtherCAT, DDS, and OPC UA are selected to implement machine network (controller to sensors/actuators communication), machine to machine communication, and machine to cloud communication respectively, to satisfy diverse communication requirements of the smart factory. An integrated architecture for combining ontology knowledge and semantic data to support intelligent applications is also proposed. These network and data process schemes are verified in a smart factory prototype for personalized candy packing application.",https://ieeexplore.ieee.org/document/8096995/,IEEE Access,2017,ieeexplore
10.1109/ACCESS.2021.3127084,Auto-NAHL: A Neural Network Approach for Condition-Based Maintenance of Complex Industrial Systems,IEEE,Journals,"Nowadays, machine learning has emerged as a promising alternative for condition monitoring of industrial processes, making it indispensable for maintenance planning. Such a learning model is able to assess health states in real time provided that both training and testing samples are complete and have the same probability distribution. However, it is rare and difficult in practical applications to meet these requirements due to the continuous change in working conditions. Besides, conventional hyperparameters tuning via grid search or manual tuning requires a lot of human intervention and becomes inflexible for users. Two objectives are targeted in this work. In an attempt to remedy the data distribution mismatch issue, we firstly introduce a feature extraction and selection approach built upon correlation analysis and dimensionality reduction. Secondly, to diminish human intervention burdens, we propose an Automatic artificial Neural network with an Augmented Hidden Layer (Auto-NAHL) for the classification of health states. Within the designed network, it is worthy to mention that the novelty of the implemented neural architecture is attributed to the new multiple feature mappings of the inputs, where such configuration allows the hidden layer to learn multiple representations from several random linear mappings and produce a single final efficient representation. Hyperparameters tuning including the network architecture, is fully automated by incorporating Particle Swarm Optimization (PSO) technique. The designed learning process is evaluated on a complex industrial plant as well as various classification problems. Based on the obtained results, it can be claimed that our proposal yields better response to new hidden representations by obtaining a higher approximation compared to some previous works.",https://ieeexplore.ieee.org/document/9610082/,IEEE Access,2021,ieeexplore
10.1109/ACCESS.2020.2993010,Bearing Intelligent Fault Diagnosis in the Industrial Internet of Things Context: A Lightweight Convolutional Neural Network,IEEE,Journals,"The advancement of Industry 4.0 and Industrial Internet of Things (IIoT) has laid more emphasis on reducing the parameter amount and storage space of the model in addition to the automatic and accurate fault diagnosis. In this case, this paper proposes a lightweight convolutional neural network (LCNN) method for intelligent fault diagnosis of bearing, which can largely satisfy the need of less parameter amount and storage space as well as high accuracy. First, depthiwise separable convolution is adopted, and a LCNN structure is constructed through an inverse residual structure and a linear bottleneck layer operation. Secondly, a novel decomposed Hierarchical Search Space is introduced to automatically explore the optimal LCNN for bearing fault diagnosis in the context of the IIoT. In the meantime, the real-time monitoring and fault diagnosis of the model are also deployed. In order to verify the validity of the designed model, Case Western Reserve University Bearing fault dataset and MFPT bearing fault dataset are adopted. The results demonstrate the great advantages of the model. The LCNN model can automatically learn and select the appropriate features, highly improving the fault diagnosis accuracy. Meanwhile, the computational and storage costs of the model are largely reduced, which contributes to its being applied to the mechanical system in the IIoT context.",https://ieeexplore.ieee.org/document/9088980/,IEEE Access,2020,ieeexplore
10.1109/TCSS.2021.3052231,Cognitive Analytics of Social Media Services for Edge Resource Pre-Allocation in Industrial Manufacturing,IEEE,Journals,"With the development of industrial intelligence, the resource requests of various social media services in smart cities are expanding rapidly. For hosting services, the edge computing (EC) platform for its low-latency resource provisioning is fully explored. However, the mapping between edge servers (ESs) and services affects the service latency. Meanwhile, the real-time dynamic distribution of resource requirements also impairs the load balance. Therefore, how to optimize the load balance of ESs while meeting the latency-critical requests remains challenging. To deal with the above challenge, in this article, we propose a resource pre-allocation (RPA) method for the social media services with cognitive analytics. Technically, the deep spatiotemporal residual network (ST-ResNet) is employed to complete the cognitive analytics of resource requests. Then based on the analysis results, the optimal resource allocation (ORA) scheme is designed with multiobjective optimization. Finally, the performance of RPA is evaluated by a real-world resource request data set.",https://ieeexplore.ieee.org/document/9340550/,IEEE Transactions on Computational Social Systems,April 2021,ieeexplore
10.1109/TII.2020.3028612,DWFCAT: Dual Watermarking Framework for Industrial Image Authentication and Tamper Localization,IEEE,Journals,"The image data received through various sensors are of significant importance in Industry 4.0. Unfortunately, these data are highly vulnerable to various malicious attacks during its transit to the destination. Although the use of pervasive edge computing (PEC) with the Internet of Things (IoT) has solved various issues, such as latency, proximity, and real-time processing, but the security and authentication of data between the nodes is still a significant concern in PEC-based industrial-IoT scenarios. In this article, we present “DWFCAT,” a dual watermarking framework for content authentication and tamper localization for industrial images. The robust and fragile watermarks along with overhead bits related to the cover image for tamper localization are embedded in different planes of the cover image. We have used discrete cosine transform coefficients and exploited their energy compaction property for robust watermark embedding. We make use of a four-point neighborhood to predict the value of a predefined pixel and use it for embedding the fragile watermark bits in the spatial domain. Chaotic and deoxyribonucleic acid encryption is used to encrypt the robust watermark before embedding to enhance its security. The results indicate that DWFCAT can withstand a range of hybrid signal processing and geometric attacks, such as Gaussian noise, salt and pepper, joint photographic experts group (JPEG) compression, rotation, low-pass filtering, resizing, cropping, sharpening, and histogram equalization. The experimental results prove that the DWFCAT is highly efficient compared with the various state-of-the-art approaches for authentication and tamper localization of industrial images.",https://ieeexplore.ieee.org/document/9214433/,IEEE Transactions on Industrial Informatics,July 2021,ieeexplore
10.1109/TII.2016.2516973,Data-Based Multiobjective Plant-Wide Performance Optimization of Industrial Processes Under Dynamic Environments,IEEE,Journals,"This paper provides a method for automatically selecting optimal operational indices for unit processes in an industrial plant using measured data and without knowing dynamical models of the unit process. A dynamic multiobjective optimization problem is defined to find operational indices that lead to plant-wide production indices close to their target values. A case-based reasoning (CBR) technique is also employed, which uses the stored experience of a human expert to determine appropriate operational indices for given target production indices. The solutions of the optimization problem and CBR technique are combined to form baseline operational indices. The dynamic models of the production indices, however, are time varying and affected by disturbances and online corrections of these baseline operational indices are required. To this end, reinforcement learning (RL) is used to provide a data-driven optimization technique to compensate for disturbances and model approximation errors and variations. The data-driven RL approach is used in two different time scales. The samples of the predicted production indices are used at a fast sampling rate, i.e., at each sample time, and the samples of actual production indices are used at a slower sampling rate, i.e., after each operational run, to correct the baseline operational indices. The effectiveness of this automated decision procedure has been demonstrated by successful implementation of the proposed approach on a large mineral processing plant in Gansu Province, China.",https://ieeexplore.ieee.org/document/7378488/,IEEE Transactions on Industrial Informatics,April 2016,ieeexplore
10.1109/ACCESS.2021.3076783,Data-Driven Fault Diagnostics for Industrial Processes: An Application to Penicillin Fermentation Process,IEEE,Journals,"We consider the problem of fault detection and isolation for the penicillin fermentation process. A penicillin fermentation process is a highly complex and nonlinear dynamic process with batch processing. A data-driven approach is utilized for fault diagnostics due to the availability of huge batch processing data and the unavailability of an analytical model. To address the non-linearity, a subspace-aided parity-based residual generation technique is proposed by using a just-in-time learning approach. For the just-in-time learning approach, the most similar data samples are selected from the database for incoming test samples and a subspace aided parity-based residual is generated using these samples. The designed fault detection technique is implemented for the penicillin fermentation process to demonstrate real-time health monitoring of the process under sensor noise and process disturbances. Two sensor faults and an actuator fault are considered and successfully detected using the proposed technique. Further, a fault isolation approach is developed to isolate these faults and their location has been identified. A case study is given to show the improvement offered by the proposed technique for the fault detection rate and minimization of the false alarm rate as compared to the existing techniques for the penicillin fermentation process.",https://ieeexplore.ieee.org/document/9420045/,IEEE Access,2021,ieeexplore
10.1109/JIOT.2020.3011726,Deep Anomaly Detection for Time-Series Data in Industrial IoT: A Communication-Efficient On-Device Federated Learning Approach,IEEE,Journals,"Since edge device failures (i.e., anomalies) seriously affect the production of industrial products in Industrial IoT (IIoT), accurately and timely detecting anomalies are becoming increasingly important. Furthermore, data collected by the edge device contain massive user's private data, which is challenging current detection approaches as user privacy has attracted more and more public concerns. With this focus, this article proposes a new communication-efficient on-device federated learning (FL)-based deep anomaly detection framework for sensing time-series data in IIoT. Specifically, we first introduce an FL framework to enable decentralized edge devices to collaboratively train an anomaly detection model, which can improve its generalization ability. Second, we propose an attention mechanism-based convolutional neural network-long short-term memory (AMCNN-LSTM) model to accurately detect anomalies. The AMCNN-LSTM model uses attention mechanism-based convolutional neural network units to capture important fine-grained features, thereby preventing memory loss and gradient dispersion problems. Furthermore, this model retains the advantages of the long short-term memory unit in predicting time-series data. Third, to adapt the proposed framework to the timeliness of industrial anomaly detection, we propose a gradient compression mechanism based on Top- k selection to improve communication efficiency. Extensive experimental studies on four real-world data sets demonstrate that our framework accurately and timely detects anomalies and also reduces the communication overhead by 50% compared to the FL framework that does not use the gradient compression scheme.",https://ieeexplore.ieee.org/document/9146846/,IEEE Internet of Things Journal,"15 April15, 2021",ieeexplore
10.1109/TII.2020.3023430,DeepFed: Federated Deep Learning for Intrusion Detection in Industrial Cyber–Physical Systems,IEEE,Journals,"The rapid convergence of legacy industrial infrastructures with intelligent networking and computing technologies (e.g., 5G, software-defined networking, and artificial intelligence), have dramatically increased the attack surface of industrial cyber-physical systems (CPSs). However, withstanding cyber threats to such large-scale, complex, and heterogeneous industrial CPSs has been extremely challenging, due to the insufficiency of high-quality attack examples. In this article, we propose a novel federated deep learning scheme, named DeepFed, to detect cyber threats against industrial CPSs. Specifically, we first design a new deep learning-based intrusion detection model for industrial CPSs, by making use of a convolutional neural network and a gated recurrent unit. Second, we develop a federated learning framework, allowing multiple industrial CPSs to collectively build a comprehensive intrusion detection model in a privacy-preserving way. Further, a Paillier cryptosystem-based secure communication protocol is crafted to preserve the security and privacy of model parameters through the training process. Extensive experiments on a real industrial CPS dataset demonstrate the high effectiveness of the proposed DeepFed scheme in detecting various types of cyber threats to industrial CPSs and the superiorities over state-of-the-art schemes.",https://ieeexplore.ieee.org/document/9195012/,IEEE Transactions on Industrial Informatics,Aug. 2021,ieeexplore
10.1109/TII.2019.2941244,DeepPAR and DeepDPA: Privacy Preserving and Asynchronous Deep Learning for Industrial IoT,IEEE,Journals,"Industrial Internet of Things (IIoT) is significant of building powerful industrial systems and applications. Deep learning has provided a promising opportunity to extract useful knowledge by utilizing vast amounts of data in IIoT. However, lacking of massive public datasets will lead to low performance and overfitting of the learned model. Therefore, the federated deep learning over distributed datasets has been proposed. Whereas, it inevitably introduces some new security challenges, i.e., disclosing participant's data privacy. However, existing methods cannot guarantee each participant's data privacy in a learning group. In this article, we propose two privacy-preserving asynchronous deep learning schemes [privacy-preserving and asynchronous deep learning via re-encryption (DeepPAR) and dynamic privacy-preserving and asynchronous deep learning (DeepDPA)]. Compared to the state-of-the-art work, DeepPAR protects each participant's input privacy while preserving dynamic update secrecy inherently. Meanwhile, DeepDPA enables to guarantee backward secrecy of group participants in a lightweight manner. Security analysis and performance evaluations on real dataset show that our proposed schemes are secure, efficient and effective.",https://ieeexplore.ieee.org/document/8836609/,IEEE Transactions on Industrial Informatics,March 2020,ieeexplore
10.1109/ACCESS.2019.2924030,Demand Response Management for Industrial Facilities: A Deep Reinforcement Learning Approach,IEEE,Journals,"As a major consumer of energy, the industrial sector must assume the responsibility for improving energy efficiency and reducing carbon emissions. However, most existing studies on industrial energy management are suffering from modeling complex industrial processes. To address this issue, a model-free demand response (DR) scheme for industrial facilities was developed. In practical terms, we first formulated the Markov decision process (MDP) for industrial DR, which presents the composition of the state, action, and reward function in detail. Then, we designed an actor-critic-based deep reinforcement learning algorithm to determine the optimal energy management policy, where both the actor (Policy) and the critic (Value function) are implemented by the deep neural network. We then confirmed the validity of our scheme by applying it to a real-world industry. Our algorithm identified an optimal energy consumption schedule, reducing energy costs without compromising production.",https://ieeexplore.ieee.org/document/8742652/,IEEE Access,2019,ieeexplore
10.1109/TII.2021.3049405,Enabling Secure Authentication in Industrial IoT With Transfer Learning Empowered Blockchain,IEEE,Journals,"Industrial Internet of Things (IIoT) is ushering in huge development opportunities in the era of Industry 4.0. However, there are significant data security and privacy challenges during automatic and real-time data collection, monitoring for industrial applications in IIoT. Data security and privacy in IIoT applications are closely related to the reliability of users, which is determined by user authentication that have been widely used as an effective approach. However, the existing user authentication mechanisms in IIoT suffer from single factor authentication and poor adaptability with the rapid growth of the number of users and the diversity of user categories. To solve the aforementioned issues, this article proposes a novel Authentication mechanism based on Transfer Learning empowered Blockchain, coined ATLB. In ATLB, blockchains are applied to achieve the privacy preservation for industrial applications. In addition, by introducing the transfer learning based authentication mechanism, trustworthy blockchains are built such that the privacy preservation for industrial applications is further enhanced. Specifically, ATLB first employs a guiding deep deterministic policy gradient algorithm to train the user authentication model of a specific region, which is then transferred locally for foreign user authentication or cross-regionally for another region's user authentication such that the model training time is significantly reduced. Experimental results show that the proposed ATLB not only provides accurate authentications for IIoT applications but also achieves high throughput and low latency.",https://ieeexplore.ieee.org/document/9314211/,IEEE Transactions on Industrial Informatics,Nov. 2021,ieeexplore
10.1109/TII.2019.2898174,Enforcing Position-Based Confidentiality With Machine Learning Paradigm Through Mobile Edge Computing in Real-Time Industrial Informatics,IEEE,Journals,"Position-based services (PBSs) that deliver networked amenities based on roaming user's positions have become progressively popular with the propagation of smart mobile devices. Position is one of the important circumstances in PBSs. For effective PBSs, extraction and recognition of meaningful positions and estimating the subsequent position are fundamental procedures. Several researchers and practitioners have tried to recognize and predict positions using various techniques; however, only few deliberate the progress of position-based real-time applications considering significant tasks of PBSs. In this paper, a method for conserving position confidentiality of roaming PBSs users using machine learning techniques is proposed. We recommend a three-phase procedure for roaming PBS users. It identifies user position by merging decision trees and k-nearest neighbor and estimates user destination along with the position track sequence using hidden Markov models. Moreover, a mobile edge computing service policy is followed in the proposed paradigm, which will ensure the timely delivery of PBSs. The benefits of mobile edge service policy offer position confidentiality and low latency by means of networking and computing services at the vicinity of roaming users. Thorough experiments are conducted, and it is confirmed that the proposed method achieved above 90% of the position confidentiality in PBSs.",https://ieeexplore.ieee.org/document/8637769/,IEEE Transactions on Industrial Informatics,July 2019,ieeexplore
10.1109/TCAD.2020.3012648,Exploring Edge Computing for Multitier Industrial Control,IEEE,Journals,"Industrial automation traditionally relies on local controllers implemented on microcontrollers or programmable logic controllers. With the emergence of edge computing, however, industrial automation evolves into a distributed two-tier computing architecture comprising local controllers and edge servers that communicate over wireless networks. Compared to local controllers, edge servers provide larger computing capacity at the cost of data loss over wireless networks. This article presents switching multitier control (SMC) to exploit edge computing for industrial control. SMC dynamically optimizes control performance by switching between local and edge controllers in response to changing network conditions. SMC employs a data-driven approach to derive switching policies based on classification models trained based on simulations while guaranteeing system stability based on an extended Simplex approach tailored for two-tier platforms. To evaluate the performance of industrial control over edge computing platforms, we have developed WCPS-EC, a real-time hybrid simulator that integrates simulated plants, real computing platforms, and real or simulated wireless networks. In a case study of an industrial robotic control system, SMC significantly outperformed both a local controller and an edge controller in face of varying data loss in a wireless network.",https://ieeexplore.ieee.org/document/9211472/,IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems,Nov. 2020,ieeexplore
10.1109/TII.2020.2984370,FADN: Fully Connected Attitude Detection Network Based on Industrial Video,IEEE,Journals,"In 3-D attitude angle estimation, monocular vision-based methods are often utilized for the advantages of short-time and high efficiency. However, the limitations of these methods lie in the complexity of the algorithm and the specificity of the scene, which needs to match the characteristics of the cooperation object and the scene. In this article, we propose a fully connected attitude detection network (FADN), which combines neural network and traditional algorithms for 3-D attitude angle estimation. FADN provides a whole process from the input of a single frame image in the industrial video stream to the output of the corresponding 3-D attitude angle estimation. Benefiting from the end-to-end estimation framework, FADN avoids tedious matching algorithms and thus has certain portability. A series of comparative experiments based on the rendering software 3-D Studio Max (3d Max) have been carried out to evaluate the performance of FADN. The experimental results show that FADN has high estimation accuracy and fast running speed. At the same time, the simulation results reliably prove the feasibility of FADN, and also promote the research in real scenarios.",https://ieeexplore.ieee.org/document/9055217/,IEEE Transactions on Industrial Informatics,March 2021,ieeexplore
10.1109/TII.2020.2988208,Fault Description Based Attribute Transfer for Zero-Sample Industrial Fault Diagnosis,IEEE,Journals,"In this article, a challenging fault diagnosis task is studied, in which no samples of the target faults are available for the model training. This scenario has hardly been studied in industrial research. But it is a common problem that massive fault samples are not available for the target faults, which limits the successes of conventional data-driven approaches in practical application. Here, we introduce the idea of zero-shot learning into the industry field, and tackle the zero-sample fault diagnosis task by proposing the fault description based attribute transfer method. Specifically, the method learns to determine the fault categories using the human-defined fault descriptions instead of the collected fault samples.The defined description consists of arbitrary attributes of the faults, including the fault positions, the consequences of the fault, and even the cause of the fault, etc. For the attribute knowledge of target faults, they can be prelearned and transferred from some readily available faults occurred in the same process. Afterwards, the target faults can be diagnosed based on the defined fault descriptions without the need for any additional data based training. Besides, the supervised principle component analysis is adopted in our method to extract the attribute related features to offer an effective attribute learning. We analyze and interpret the feasibility of the fault description based method theoretically. Also, the zero-sample fault diagnosis experiments are designed and conducted on the benchmark Tennessee-Eastman process and the real thermal power plant process to validate the effectiveness. The results show that it is indeed possible to diagnose target faults without their samples.",https://ieeexplore.ieee.org/document/9072621/,IEEE Transactions on Industrial Informatics,March 2021,ieeexplore
10.1109/TII.2019.2937876,Federated Tensor Mining for Secure Industrial Internet of Things,IEEE,Journals,"In a vertical industry alliance, Internet of Things (IoT) deployed in different smart factories are similar. For example, most automobile manufacturers have the similar assembly lines and IoT surveillance systems. It is common to observe the industrial knowledge using deep learning and data mining methods based on the IoT data. However, some knowledge is not easy to be mined from only one factory's data because the samples are still few. If multiple factories within an alliance can gather their data together, more knowledge could be mined. However, the key concern of these factories is the data security. Existing matrix-based methods can guarantee the data security inside a factory but do not allow the data sharing among factories, and thus their mining performance is poor due to lack of correlation. To address this concern, in this article we propose the novel federated tensor mining (FTM) framework to federate multisource data together for tensor-based mining while guaranteeing the security. The key contribution of FTM is that every factory only needs to share its ciphertext data for security issue, and these ciphertexts are adequate for tensor-based knowledge mining due to its homomorphic attribution. Real-data-driven simulations demonstrate that FTM not only mines the same knowledge compared with the plaintext mining, but also is enabled to defend the attacks from distributed eavesdroppers and centralized hackers. In our typical experiment, compared with the matrix-based privacy-preserving compressive sensing (PPCS), FTM increases up to 24% on mining accuracy.",https://ieeexplore.ieee.org/document/8815886/,IEEE Transactions on Industrial Informatics,March 2020,ieeexplore
10.1109/TII.2021.3054795,Guest Editorial: Softwarized Networking for Next Generation Industrial Cyber-Physical Systems,IEEE,Journals,"The papers in this special section focus on softwarized networking for next generation industrial cyber-physical systems (CPSs). With the emergence of embedded and ubiquitous cyberphysical applications, the rationale of blending the physical and the virtual worlds has become ever promising. These papers examine several topics that are recently concerned in the community, including the software defined architectures and implementations, advanced machine learning and data analytics solutions, blockchain-based network services and applications, network function allocation, dependable and trustable solutions, energy efficient networks and services, and other enabling technologies for integrating softwarized networks into CPSs. ",https://ieeexplore.ieee.org/document/9423491/,IEEE Transactions on Industrial Informatics,Aug. 2021,ieeexplore
10.1109/TII.2020.3034627,Guest Editorial: Special Section on Advanced Signal Processing and AI Technologies for Industrial Big Data,IEEE,Journals,The papers in this special section focus on advanced signal processing and artificial intelligence (AI) technologies for industrial Big Data (IBD) powered by Industry 4.0. Modern industry has evolved from the traditional manufacturing industry to digital and intelligent industry. Huge amount of complex real-time data are generated from the thousands of industrial sensors in physical and man-made environments. Industrial big data (IBD) afford us an unprecedented opportunity to obtain an in-depth understanding of Internet of Things and facilitate data-driven approaches for industrial optimization and scheduling. The papers in this section collect the latest ideas and research on advanced signal processing and artificial intelligence (AI) technologies for IBD.,https://ieeexplore.ieee.org/document/9361683/,IEEE Transactions on Industrial Informatics,May 2021,ieeexplore
10.1109/ACCESS.2018.2809681,Industrial Big Data Analytics for Prediction of Remaining Useful Life Based on Deep Learning,IEEE,Journals,"Due to the recent development of cyber-physical systems, big data, cloud computing, and industrial wireless networks, a new era of industrial big data is introduced. Deep learning, which brought a revolutionary change in computer vision, natural language processing, and a variety of other applications, has significant potential for solutions providing in sophisticated industrial applications. In this paper, a concept of device electrocardiogram (DECG) is presented, and an algorithm based on deep denoising autoencoder (DDA) and regression operation is proposed for the prediction of the remaining useful life of industrial equipment. First, the concept of electrocardiogram is explained. Then, a problem statement based on manufacturing scenario is presented. Subsequently, the architecture of the proposed algorithm called integrated DDA and the algorithm workflow are provided. Moreover, DECG is compared with traditional factory information system, and the feasibility and effectiveness of the proposed algorithm are validated experimentally. The proposed concept and algorithm combine typical industrial scenario and advance artificial intelligence, which has great potential to accelerate the implementation of industry 4.0.",https://ieeexplore.ieee.org/document/8302913/,IEEE Access,2018,ieeexplore
10.1109/TII.2020.3007407,Industrial Cyber-Physical Systems-Based Cloud IoT Edge for Federated Heterogeneous Distillation,IEEE,Journals,"Deep convoloutional networks have been widely deployed in modern cyber-physical systems performing different visual classification tasks. As the fog and edge devices have different computing capacity and perform different subtasks, models trained for one device may not be deployable on another. Knowledge distillation technique can effectively compress well trained convolutional neural networks into light-weight models suitable to different devices. However, due to privacy issue and transmission cost, manually annotated data for training the deep learning models are usually gradually collected and archived in different sites. Simply training a model on powerful cloud servers and compressing them for particular edge devices failed to use the distributed data stored at different sites. This offline training approach is also inefficient to deal with new data collected from the edge devices. To overcome these obstacles, in this article, we propose the heterogeneous brain storming (HBS) method for object recognition tasks in real-world Internet of Things (IoT) scenarios. Our method enables flexible bidirectional federated learning of heterogeneous models trained on distributed datasets with a new “brain storming” mechanism and optimizable temperature parameters. In our comparison experiments, this HBS method outperformed multiple state-of-the-art single-model compression methods, as well as the newest multinetwork knowledge distillation methods with both homogeneous and heterogeneous classifiers. The ablation experiment results proved that the trainable temperature parameter into the conventional knowledge distillation loss can effectively ease the learning process of student networks in different methods. To the best of authors' knowledge, this is the first IoT-oriented method that allows asynchronous bidirectional heterogeneous knowledge distillation in deep networks.",https://ieeexplore.ieee.org/document/9134802/,IEEE Transactions on Industrial Informatics,Aug. 2021,ieeexplore
10.1109/JIOT.2019.2912022,Machine Learning-Based Network Vulnerability Analysis of Industrial Internet of Things,IEEE,Journals,"It is critical to secure the Industrial Internet of Things (IIoT) devices because of potentially devastating consequences in case of an attack. Machine learning (ML) and big data analytics are the two powerful leverages for analyzing and securing the Internet of Things (IoT) technology. By extension, these techniques can help improve the security of the IIoT systems as well. In this paper, we first present common IIoT protocols and their associated vulnerabilities. Then, we run a cyber-vulnerability assessment and discuss the utilization of ML in countering these susceptibilities. Following that, a literature review of the available intrusion detection solutions using ML models is presented. Finally, we discuss our case study, which includes details of a real-world testbed that we have built to conduct cyber-attacks and to design an intrusion detection system (IDS). We deploy backdoor, command injection, and Structured Query Language (SQL) injection attacks against the system and demonstrate how a ML-based anomaly detection system can perform well in detecting these attacks. We have evaluated the performance through representative metrics to have a fair point of view on the effectiveness of the methods.",https://ieeexplore.ieee.org/document/8693904/,IEEE Internet of Things Journal,Aug. 2019,ieeexplore
10.1109/TII.2021.3050041,Network Traffic Prediction in Industrial Internet of Things Backbone Networks: A Multitask Learning Mechanism,IEEE,Journals,"Industrial Internet of Things (IIoT), as a common industrial application of Internet of Things, has been widely deployed in recent years. End-to-end network traffic is an essential information for many network security and management functions. This article investigates the issues of IIoT-oriented backbone network traffic prediction. Predicting the traffic of IIoT backbone networks is intractable because of the large number of prior network traffic information, which needs to consume expensive network resources for sampling. Motivated by that, we propose an effective prediction mechanism using multitask learning (MTL), which is a special paradigm of transfer learning. A deep learning architecture constructed by MTL and long short-term memory is designed. This deep architecture takes advantage of link loads as additional information to improve prediction accuracy. We provide a theoretical analysis for the MTL mechanism. The effectiveness is evaluated by implementing our mechanism on real network.",https://ieeexplore.ieee.org/document/9316934/,IEEE Transactions on Industrial Informatics,Oct. 2021,ieeexplore
10.1109/TII.2019.2953275,Neural Network-Based Model Predictive Control of a Paste Thickener Over an Industrial Internet Platform,IEEE,Journals,"This article presents a real implementation of a neural network-based model predictive control scheme (NNMPC) to control an industrial paste thickener. The implementation is done over an Industrial Internet of Things (IIoT) platform designed using the seven layer reference model for IIoT systems. Modeling is achieved using an encoder-decoder with attention recurrent neural network, while MPC search is done using particle swarm optimization. An industrial evaluation is presented, which highlights the set-point tracking and disturbance rejection capabilities of the proposed NNMPC technique.",https://ieeexplore.ieee.org/document/8897590/,IEEE Transactions on Industrial Informatics,April 2020,ieeexplore
10.1109/TIE.2013.2266086,Nonlinear Model-Predictive Control for Industrial Processes: An Application to Wastewater Treatment Process,IEEE,Journals,"Because of their complex behavior, wastewater treatment processes (WWTPs) are very difficult to control. In this paper, the design and implementation of a nonlinear model-predictive control (NMPC) system are discussed. The proposed NMPC comprises a self-organizing radial basis function neural network (SORBFNN) identifier and a multiobjective optimization method. The SORBFNN with concurrent structure and parameter learning is developed as a model identifier for approximating the online states of dynamic systems. Then, the solution of the multiobjective optimization is obtained by a gradient method which can shorten the solution time of optimal control problems. Moreover, the conditions for the stability analysis of NMPC are presented. Experiments reveal that the proposed control technique gives satisfactory tracking and disturbance rejection performance for WWTPs. Experimental results on a real WWTP show the efficacy of the proposed NMPC for industrial processes in many applications.",https://ieeexplore.ieee.org/document/6523075/,IEEE Transactions on Industrial Electronics,April 2014,ieeexplore
10.1109/TIE.2011.2161652,Novel Online Speed Profile Generation for Industrial Machine Tool Based on Flexible Neuro-Fuzzy Approximation,IEEE,Journals,"Reference trajectory generation is one of the most important tasks in the control of machine tools. Such a trajectory must guarantee a smooth kinematics profile to avoid exciting the natural frequencies of the mechanical structure or servo control system. Moreover, the trajectory must be generated online to enable some feed rate adaptation mechanism working. This paper presents the online smooth speed profile generator used in trajectory interpolation in milling machines. Smooth kinematic profile is obtained by imposing limit on the jerk-which is the first derivative of acceleration. This generator is based on the neuro-fuzzy system and is able to adapt online the current feed rate to changing external conditions. Such an approach improves the machining quality, reduces the tool wear, and shortens total machining time. The proposed trajectory generation algorithm has been successfully tested and can be implemented on a multiaxis milling machine.",https://ieeexplore.ieee.org/document/5951767/,IEEE Transactions on Industrial Electronics,Feb. 2012,ieeexplore
10.1109/ACCESS.2019.2958284,On the Generation of Anomaly Detection Datasets in Industrial Control Systems,IEEE,Journals,"In recent decades, Industrial Control Systems (ICS) have been affected by heterogeneous cyberattacks that have a huge impact on the physical world and the people's safety. Nowadays, the techniques achieving the best performance in the detection of cyber anomalies are based on Machine Learning and, more recently, Deep Learning. Due to the incipient stage of cybersecurity research in ICS, the availability of datasets enabling the evaluation of anomaly detection techniques is insufficient. In this paper, we propose a methodology to generate reliable anomaly detection datasets in ICS that consists of four steps: attacks selection, attacks deployment, traffic capture and features computation. The proposed methodology has been used to generate the Electra Dataset, whose main goal is the evaluation of cybersecurity techniques in an electric traction substation used in the railway industry. Using the Electra dataset, we train several Machine Learning and Deep Learning models to detect anomalies in ICS and the performed experiments show that the models have high precision and, therefore, demonstrate the suitability of our dataset for use in production systems.",https://ieeexplore.ieee.org/document/8926471/,IEEE Access,2019,ieeexplore
10.1109/ACCESS.2019.2941436,Opportunities and Challenges in Health Sensing for Extreme Industrial Environment: Perspectives From Underground Mines,IEEE,Journals,"Occupational health and safety hazards in the extreme work environment of underground mines remained a serious concern for both mine management and regulatory agency. Miners are often employed to perform different mining activities across the mine and are always exposed to health risks such as lung cancer. With the technology advancements, it is now possible to keep track of mine individuals and their health parameters. The current practice of health analysis is periodic in nature and is highly dependent on voluntary participation. Wearable health sensing system is an alternative solution to overcome these challenges and is able to provide insights on miners' health conditions. Timely analysis of physiological parameters of the miners is immensely helpful to minimize the injuries and can also provide preventive measures for potential health hazards. In this paper, we propose a wireless health monitoring system, especially for underground mines. The contributions of this paper are twofold. First, it presents and discusses our proposed system architecture and solution followed by challenges of such system in the context of underground mines. Second, as a preliminary analysis, detailed discussion on the wireless link behavior for reliable data transmission and communication are presented. We performed real-world experimental measurements in an operational underground coal mine considering several deployment settings in straight, near face and curved mine galleries. The communication metrics (e.g., received signal strength and packet reception rate) are extensively evaluated.",https://ieeexplore.ieee.org/document/8836591/,IEEE Access,2019,ieeexplore
10.1109/JIOT.2020.2994200,Optimization of Edge-PLC-Based Fault Diagnosis With Random Forest in Industrial Internet of Things,IEEE,Journals,"Facing globalized competition, there have been increasing requirements for safety and efficiency in smart factories, where the industrial Internet of Things can enable the monitoring of equipment's status and the detecting of faults before they go critical. Regarding cloud computing, data-driven methods running at clouds are adopted to train the model with a large amount of raw data at the beginning, then end machines upload their real-time readings to the cloud center for processing. However, this incurs considerable computational costs and may sometimes bear a severe delay. In this article, we consider a hierarchical structure where edge-PLCs are employed to gather sensed data locally and reduce communication costs. Since a single fault may be related to multiple influencing features, we want to first minimize the number of features that need to determine a fault, then try to find out the minimal set of edge-PLCs which can cover all key features so as to save the deployment cost. We propose a random-forest-based method to handle the features selection problem, and then the selection of edge-PLCs by solving the set coverage problem. Through the simulation on real data trace, we compare our method with other artificial-intelligence-based methods, such as the logistics regression model and its extensions. The results prove the efficiency and performance of the proposed method, which reaches or even exceeds the accuracy of methods using the full set of data.",https://ieeexplore.ieee.org/document/9091605/,IEEE Internet of Things Journal,Oct. 2020,ieeexplore
10.1109/TIE.2016.2612160,PLC-Based Real-Time Realization of Flatness-Based Feedforward Control for Industrial Compression Systems,IEEE,Journals,"In this paper, we present a novel programmable logic controller (PLC)-based real-time realization of a flatness-based feedforward control (FFC) scheme. The proposed approach is applied to an industrial fuel-gas compression system which is used to supply fuel gas to the gas turbines in combined cycle power plants. Due to the increasing demand for fast operation point transitions with high performance and accuracy requirements, the currently applied decentralized proportional-integral-derivative controllers appear to be not appropriate any more. Hence, by means of system simulations, a new flatness-based FFC design has been shown to provide improved control performance. In this paper, we bridge the gap between simulation-based control design and practical applicability, in that, we present the real-time realization of the approach on a PLC. Furthermore, the PLC-based controller is tested on a hardware-in-the-loop platform running with a complex compression system model in real time. The results reveal that the flatness-based control design can be implemented on a real compressor system.",https://ieeexplore.ieee.org/document/7572893/,IEEE Transactions on Industrial Electronics,Feb. 2017,ieeexplore
10.1109/TII.2012.2222034,"Past, Present, and Future of Industrial Agent Applications",IEEE,Journals,"Industrial agents technology leverages the benefits of multiagent systems, distributed computing, artificial intelligence techniques and semantics in the field of production, services and infrastructure sectors, providing a new way to design and engineer control solutions based on the decentralization of control over distributed structures. The key drivers for this application are the benefits of agent-based industrial systems, namely in terms of robustness, scalability, reconfigurability and productivity, all of which translate to a greater competitive advantage. This manuscript monitors the chronology of research and development of the industrial applications of multiagent and holonic systems. It provides the comprehensive overview of methodologies, architectures and applications of agents in industrial domain from early nineties up to present. It also gives an outlook of the current trends as well as challenges and possible future application domains of industrial agents.",https://ieeexplore.ieee.org/document/6319392/,IEEE Transactions on Industrial Informatics,Nov. 2013,ieeexplore
10.1109/TII.2019.2940099,Performance Supervised Fault Detection Schemes for Industrial Feedback Control Systems and their Data-Driven Implementation,IEEE,Journals,"This article addresses performance supervised fault detection (PSFD) issues for industrial feedback control systems based on performance degradation prediction. To be specific, three performance indicators are first introduced based on Bellman equation to predict system performance degradations for industrial processes with the aid of machine learning techniques. Based on them, three PSFD schemes are proposed by embedding the performance indicators as supervising information. In this context, the data-driven implementation of PSFD schemes are investigated for linear systems with unmeasurable state variables. A case study on rolling mill process, a typical benchmark in the steel manufacturing processes, is given at the end of this article to illustrate the applications of the proposed fault detection schemes.",https://ieeexplore.ieee.org/document/8827307/,IEEE Transactions on Industrial Informatics,April 2020,ieeexplore
10.1109/TII.2021.3061579,Resource Management for Pervasive-Edge-Computing-Assisted Wireless VR Streaming in Industrial Internet of Things,IEEE,Journals,"Wireless virtual reality (VR) is increasingly used in industrial Internet of Things (IIoTs). However, ultra-high viewport rendering demands and excessive terminal energy consumption restrict the application of wireless VR. Pervasive edge computing emerges as a promising method for wireless VR. In this article, we propose an energy-aware resource management scheme for wireless-VR-supported IIoTs. To reduce the energy consumption of VR equipments (VEs) while ensuring a smooth immersive VR experience, we formulate the viewport rendering offloading, computing, and spectrum resource allocation to be a joint optimization problem, considering content correlation between VEs, fluctuating channel conditions, and VR quality of experience. By applying dual approximation, the original problem is transformed to be a Markov decision process and an reinforcement learning (RL)-based online learning algorithm is designed to find the optimal policy. To improve the learning efficiency, the quantum parallelism is integrated into the RL to overcome “curse of dimensionality”. In the simulations, the convergence rate and the performance in terms of energy consumption and stalling rate are evaluated. Simulation results demonstrate the effectiveness of the proposed scheme.",https://ieeexplore.ieee.org/document/9361168/,IEEE Transactions on Industrial Informatics,Nov. 2021,ieeexplore
10.1109/TII.2019.2903224,Risk-Based Scheduling of Security Tasks in Industrial Control Systems With Consideration of Safety,IEEE,Journals,"Industrial control systems (ICSs) in networked environments face severe cyber-security risks and challenges. A timely response to cyber-attacks is of paramount importance for mitigating risks. However, the security policy developed for an ICS may be conflicting with the ICS's safety policy, on which much attention has been paid for a long time in industrial control. An inappropriate enforcement of the security policy may deteriorate the ICS performance or even result in severe unexpected consequences. To tackle this problem, a risk-based security task scheduling approach is presented for ICSs with consideration of the safety policy. It ensures a timely response to cyber-attacks without compromising safety. More specifically, the approach reconciles security tasks and safety tasks according to a designed resolution policy, so as to acquire contradiction-free security and safety (S&amp;S) tasks. Then, a real-time risk assessment method is developed to characterize the subtle change of the system risk with the implementation of the reconciled S&amp;S tasks. After that, a task scheduling method is designed with the risk as the optimization objective, i.e., it searches the optimal task scheduling scheme by minimizing the risk posture. The resulting scheduling scheme ensures the smooth implementation of the S&amp;S policy, which reflects the optimal recovery process against the risk. Finally, case studies on a hardware-in-the-loop testbed are conducted to demonstrate the effectiveness of the proposed approach.",https://ieeexplore.ieee.org/document/8661651/,IEEE Transactions on Industrial Informatics,May 2020,ieeexplore
10.1109/ACCESS.2020.3045563,SDN Enhanced Resource Orchestration of Containerized Edge Applications for Industrial IoT,IEEE,Journals,"With the rise of the Industrial Internet of Things (IIoT), there is an intense pressure on resource and performance optimization leveraging on existing technologies, such as Software Defined Networking (SDN), edge computing, and container orchestration. Industry 4.0 emphasizes the importance of lean and efficient operations for sustainable manufacturing. Achieving this goal would require engineers to consider all layers of the system, from hardware to software, and optimizing for resource efficiency at all levels. This emphasizes the need for container-based virtualization tools such as Docker and Kubernetes, offering Platform as a Service (PaaS), while simultaneously leveraging on edge technologies to reduce related latencies. For network management, SDN is poised to offer a cost-effective and dynamic scalability solution by customizing packet handling for various edge applications and services. In this paper, we investigate the energy and latency trade-offs involved in combining these technologies for industrial applications. As a use case, we emulate a 3D-drone-based monitoring system aimed at providing real-time visual monitoring of industrial automation. We compare a native implementation to a containerized implementation where video processing is orchestrated while streaming is handled by an external UE representing the IIoT device. We compare these two scenarios for energy utilization, latency, and responsiveness. Our test results show that only roughly 16 percent of the total power consumption happens on the mobile node when orchestrated. Virtualization adds up about 4.5 percent of the total power consumption while the latency difference between the two approaches becomes negligible after the streaming session is initialized.",https://ieeexplore.ieee.org/document/9296769/,IEEE Access,2020,ieeexplore
10.1109/TII.2018.2799907,SIGMM: A Novel Machine Learning Algorithm for Spammer Identification in Industrial Mobile Cloud Computing,IEEE,Journals,"An industrial mobile network is crucial for industrial production in the Internet of Things. It guarantees the normal function of machines and the normalization of industrial production. However, this characteristic can be utilized by spammers to attack others and influence industrial production. Users who only share spams, such as links to viruses and advertisements, are called spammers. With the growth of mobile network membership, spammers have organized into groups for the purpose of benefit maximization, which has caused confusion and heavy losses to industrial production. It is difficult to distinguish spammers from normal users owing to the characteristics of multidimensional data. To address this problem, this paper proposes a spammer identification scheme based on Gaussian mixture model (SIGMM) that utilizes machine learning for industrial mobile networks. It provides intelligent identification of spammers without relying on flexible and unreliable relationships. SIGMM combines the presentation of data, where each user node is classified into one class in the construction process of the model. We validate the SIGMM by comparing it with the reality mining algorithm and hybrid fuzzy c-means (FCM) clustering algorithm using a mobile network dataset from a cloud server. Simulation results show that SIGMM outperforms these previous schemes in terms of recall, precision, and time complexity.",https://ieeexplore.ieee.org/document/8274937/,IEEE Transactions on Industrial Informatics,April 2019,ieeexplore
10.1109/TII.2020.2990741,Seamless Authentication: For IoT-Big Data Technologies in Smart Industrial Application Systems,IEEE,Journals,"Technological developments in communication technologies in the form of hardware and software have made unilateral sensor connectivity over Internet access that facilitates data observation and measurement of physical entities. A technology known as Internet of Things (IoT) is commonly referred to as the connectivity of Internet devices that provides the communication interactivity between the physical and the cyber objects. One of the key objectives of Internet computing is to simplify human activities and improve the user experience and device access. To explore its basic challenges, big data is somehow diversified into smart-data intelligence that transforms the raw semantic data into smart-data. The transformation approaches realize the significance of productivity and financial gain, which in turn offers a better decision-making process and privacy preservation. Moreover, the intelligent system collects raw data from different devices that analyze the extracted information. Since IoT plays a significant role in the development of a new source dataset, a seamless authentication protocol (SAP) is preferably chosen to coalesce data inference, algorithm development, and technological advancement. The comparative analysis proves that the proposed SAP consumes less computation and communication overhead as compared to other authentication schemes.",https://ieeexplore.ieee.org/document/9079577/,IEEE Transactions on Industrial Informatics,April 2021,ieeexplore
10.1109/TVCG.2020.2969007,Security in Process: Visually Supported Triage Analysis in Industrial Process Data,IEEE,Journals,"Operation technology networks, i.e. hard- and software used for monitoring and controlling physical/industrial processes, have been considered immune to cyber attacks for a long time. A recent increase of attacks in these networks proves this assumption wrong. Several technical constraints lead to approaches to detect attacks on industrial processes using available sensor data. This setting differs fundamentally from anomaly detection in IT-network traffic and requires new visualization approaches adapted to the common periodical behavior in OT-network data. We present a tailored visualization system that utilizes inherent features of measurements from industrial processes to full capacity to provide insight into the data and support triage analysis by laymen and experts. The novel combination of spiral plots with results from anomaly detection was implemented in an interactive system. The capabilities of our system are demonstrated using sensor and actuator data from a real-world water treatment process with introduced attacks. Exemplary analysis strategies are presented. Finally, we evaluate effectiveness and usability of our system and perform an expert evaluation.",https://ieeexplore.ieee.org/document/8968740/,IEEE Transactions on Visualization and Computer Graphics,1 April 2020,ieeexplore
10.1109/ACCESS.2020.3020799,Short-Term Industrial Load Forecasting Based on Ensemble Hidden Markov Model,IEEE,Journals,"Short-term load forecasting (STLF) for industrial customers has been an essential task to reduce the cost of energy transaction and promote the stable operation of smart grid throughout the development of the modern power system. Traditional STLF methods commonly focus on establishing the non-linear relationship between loads and features, but ignore the temporal relationship between them. In this paper, an STLF method based on ensemble hidden Markov model (e-HMM) is proposed to track and learn the dynamic characteristics of industrial customer's consumption patterns in correlated multivariate time series, thereby improving the prediction accuracy. Specifically, a novel similarity measurement strategy of log-likelihood space is designed to calculate the log-likelihood value of the multivariate time series in sliding time windows, which can effectively help the hidden Markov model (HMM) to capture the dynamic temporal characteristics from multiple historical sequences in similar patterns, so that the prediction accuracy is greatly improved. In order to improve the generalization ability and stability of a single HMM, we further adopt the framework of Bagging ensemble learning algorithm to reduce the prediction errors of a single model. The experimental study is implemented on a real dataset from a company in Hunan Province, China. We test the model in different forecasting periods. The results of multiple experiments and comparison with several state-of-the-art models show that the proposed approach has higher prediction accuracy.",https://ieeexplore.ieee.org/document/9183956/,IEEE Access,2020,ieeexplore
10.1109/ACCESS.2018.2809439,The Collaborative System Workflow Management of Industrial Design Based on Hierarchical Colored Petri-Net,IEEE,Journals,"Industrial design is a requisite and incontrovertible front-end design for Industrie 4.0 to realize personalization, agility, intelligence, and standardization, so it has very important practical significance to achieve industrial design collaborative workflow flexible management. This paper proposed a hierarchical colored Petri net (HCPN) for modeling the industrial design collaborative system workflow, which aims to provide a valid workflow model for the process management of the industrial design collaborative system. First, it is built the top-level workflow CPN model from product planning, markets research, product design, and product evaluation to prototype production, then the model is extended layer by layer to build the subnet model by the model refinement method. In the model, the activity of the workflow and the activity execution results are represented by the transitions (T) and places (S), respectively. The running state of the whole workflow is represented by the distribution of the token in the places. The token' color sets expresses the real-time discrete state. By use of the function of token coloring places (S) and the token' color sets of HCPN, this paper extracts the common characteristics of process knowledge, including markets research results, design ideas, conceptual sketches, design schemes, human resources, and tacit creative knowledge ,then uses the what, when, where, who, and how method to build knowledge information units for specific design event. Thus, realizes the industrial design process knowledge base construction and knowledge acquisition, which provides the necessary information and knowledge resources for the design reuse, flexibility management, and intelligent manufacturing of subsequent products.",https://ieeexplore.ieee.org/document/8302489/,IEEE Access,2018,ieeexplore
10.1109/JIOT.2019.2963635,Toward Edge-Based Deep Learning in Industrial Internet of Things,IEEE,Journals,"As a typical application of the Internet of Things (IoT), the Industrial IoT (IIoT) connects all the related IoT sensing and actuating devices ubiquitously so that the monitoring and control of numerous industrial systems can be realized. Deep learning, as one viable way to carry out big-data-driven modeling and analysis, could be integrated in IIoT systems to aid the automation and intelligence of IIoT systems. As deep learning requires large computation power, it is commonly deployed in cloud servers. Thus, the data collected by IoT devices must be transmitted to the cloud for training process, contributing to network congestion and affecting the IoT network performance as well as the supported applications. To address this issue, in this article, we leverage the fog/edge computing paradigm and propose an edge computing-based deep learning model, which utilizes edge computing to migrate the deep learning process from cloud servers to edge nodes, reducing data transmission demands in the IIoT network and mitigating network congestion. Since edge nodes have limited computation ability compared to servers, we design a mechanism to optimize the deep learning model so that its requirements for computational power can be reduced. To evaluate our proposed solution, we design a testbed implemented in the Google cloud and deploy the proposed convolutional neural network (CNN) model, utilizing a real-world IIoT data set to evaluate our approach.<sup>1</sup> Our experimental results confirm the effectiveness of our approach, which cannot only reduce the network traffic overhead for IIoT but also maintain the classification accuracy in comparison with several baseline schemes.<sup>1</sup>Certain commercial equipment, instruments, or materials are identified in this article in order to specify the experimental procedure adequately. Such identification is not intended to imply recommendation or endorsement by the National Institute of Standards and Technology, nor is it intended to imply that the materials or equipment identified are necessarily the best available for the purpose.",https://ieeexplore.ieee.org/document/8948000/,IEEE Internet of Things Journal,May 2020,ieeexplore
10.1109/TII.2020.2994747,Trustworthiness in Industrial IoT Systems Based on Artificial Intelligence,IEEE,Journals,"The intelligent industrial environment developed with the support of the new generation network cyber-physical system (CPS) can realize the high concentration of information resources. In order to carry out the analysis and quantification for the reliability of CPS, an automatic online assessment method for the reliability of CPS is proposed in this article. It builds an evaluation framework based on the knowledge of machine learning, designs an online rank algorithm, and realizes the online analysis and assessment in real time. The preventive measures can be taken timely, and the system can operate normally and continuously. Its reliability has been greatly improved. Based on the credibility of the Internet and the Internet of Things, a typical CPS control model based on the spatiotemporal correlation detection model is analyzed to determine the comprehensive reliability model analysis strategy. Based on this, in this article, we propose a CPS trusted robust intelligent control strategy and a trusted intelligent prediction model. Through the simulation analysis, the influential factors of attack defense resources and the dynamic process of distributed cooperative control are obtained. CPS defenders in the distributed cooperative control mode can be guided and select the appropriate defense resource input according to the CPS attack and defense environment.",https://ieeexplore.ieee.org/document/9093957/,IEEE Transactions on Industrial Informatics,Feb. 2021,ieeexplore
10.1109/IJCNN48605.2020.9207496,"""I’m Sorry Dave, I’m Afraid I Can’t Do That"" Deep Q-Learning from Forbidden Actions",IEEE,Conferences,"The use of Reinforcement Learning (RL) is still restricted to simulation or to enhance human-operated systems through recommendations. Real-world environments (e.g. industrial robots or power grids) are generally designed with safety constraints in mind implemented in the shape of valid actions masks or contingency controllers. For example, the range of motion and the angles of the motors of a robot can be limited to physical boundaries. Violating constraints thus results in rejected actions or entering in a safe mode driven by an external controller, making RL agents incapable of learning from their mistakes. In this paper, we propose a simple modification of a state-of-the-art deep RL algorithm (DQN), enabling learning from forbidden actions. To do so, the standard Q-learning update is enhanced with an extra safety loss inspired by structured classification. We empirically show that it reduces the number of hit constraints during the learning phase and accelerates convergence to near-optimal policies compared to using standard DQN. Experiments are done on a Visual Grid World Environment and the TextWorld domain.",https://ieeexplore.ieee.org/document/9207496/,2020 International Joint Conference on Neural Networks (IJCNN),19-24 July 2020,ieeexplore
10.1109/ICIT.2006.372290,3D Modelling from Multi-view Registered Range Images Using K-means Clustering,IEEE,Conferences,"3D modelling from range images captured using laser scanning systems finds a wide range of applications in computer vision and industrial robotics. However due to the presence of scanning noise, accumulative registration errors, and improper data fusion, the reconstructed surfaces from multiple registered range images captured from different viewpoints are often distorted with thick patches, false connections and blurred features. Moreover, the existing integration methods are often expensive in the sense of computational time and data storage. These shortcomings will hinder the wide applications of 3D modelling using the latest laser scanning systems. In this paper, the k-means clustering approach from the pattern recognition and machine learning literatures is employed to optimally fuse the overlapping areas between two range images captured from two neighbouring viewpoints and to iteratively minimize the integration error. The final fused point set is then triangulated using an improved Delaunay method, guaranteeing a watertight surface. The new method is theoretically guaranteed to converge. A comparative study based on real images shows that the proposed algorithm is computationally efficient and significantly reduces the integration error, while desirably retaining geometric details of object surface.",https://ieeexplore.ieee.org/document/4237612/,2006 IEEE International Conference on Industrial Technology,15-17 Dec. 2006,ieeexplore
10.1109/RTCSA.2018.00012,A Case Study of Cyber-Physical System Design: Autonomous Pick-and-Place Robot,IEEE,Conferences,"Although modern robots in warehousing systems can perform adequately in a goods-to-person model using hand-designed algorithms that are specialized to a particular environment, developing a robotic system that is capable of handling new products at an inexpensive cost remains a challenge. A conspicuous example of this challenge is seen in Amazon's use of autonomous robots to fetch customers' orders in their massive warehouses. To encourage advance in this technology, Amazon organized the competition, Amazon Picking Challenge that asked participants to develop their own hardware and software for the general task of picking a designated set of products from inventory shelves and then placing them at a target location (called a pick-and-place task). Current technology for pick-and-place tasks is still insufficient to meet the demand for low-cost automation. Handling awkward or oddly shaped object must still depend on hand-programming or specialized robotic systems, making manufacturing automation less flexible and expensive. In this paper, we shall present the design and implementation of a software system that is a step in advancing the technology toward full automation at reasonable costs. Our system integrates a set of state-of-the-art techniques in computer vision, deep-learning, trajectory optimization, visual servoing to create a library of skills that can be composed to perform a variety of robotic tasks. We demonstrate the capability of our system for performing autonomous pick-and-place tasks with an implementation using Hoppy, an industrial robotic arm in an environment similar to the Amazon Picking Challenge.",https://ieeexplore.ieee.org/document/8607230/,2018 IEEE 24th International Conference on Embedded and Real-Time Computing Systems and Applications (RTCSA),28-31 Aug. 2018,ieeexplore
10.1109/IECON.2018.8591171,A Case Study on Knowledge Driven Code Generation for Software-Defined Industrial Cyber-Physical Systems,IEEE,Conferences,"Industrial Cyber-Physical Systems (iCPS) enables coordination between various subsystems and devices based on real-time feedback data from sensors. iCPS must react rapidly to new requirements and adjust itself to fulfill new functionalities in no time. On the software side, control programs of iCPS need to be reconfigured dynamically. An efficient way for massive reconfiguration is automatic code generation. In this paper, a knowledge-driven code generation method is experimented for software-defined iCPS. Based on sensor values, actuators are controlled by the reasoning process with support of ontological knowledge base. The results demonstrate that iCPS could be driven by rules completely without programming control software.",https://ieeexplore.ieee.org/document/8591171/,IECON 2018 - 44th Annual Conference of the IEEE Industrial Electronics Society,21-23 Oct. 2018,ieeexplore
10.1109/ACCC51160.2020.9347897,A Comparative Analysis of Kinematics of Industrial Robot KUKA KR 60–3 Using Scientific Computing Languages,IEEE,Conferences,"In the field of robotics, there are kinematic analysis methods that are responsible for describing the positions and orientations of the end effectors, as well as the angles, velocities and trajectories of industrial robots; such techniques are: forward kinematics, inverse kinematics and velocity kinematics. For the solutions of these complex mathematical calculations, the use of scientific computing languages or programs is required; which more and more algorithms, libraries and complements are implemented, that achieve a reduction in programming hours and result in the creation of better solutions in areas of all kinds. For this reason, the kinematics of the Industrial Robot KUKA KR 60-3 was programmed in the languages and programs most used in scientific computing, with the aim of comparing the performance (real time) when carrying out symbolic and numerical analysis in said studies.",https://ieeexplore.ieee.org/document/9347897/,2020 Asia Conference on Computers and Communications (ACCC),18-20 Sept. 2020,ieeexplore
10.1109/INOCON50539.2020.9298344,A Comparative Study of Distinct Speed Controllers for a Separately Excited DC Motor (SEDM),IEEE,Conferences,"A Separately Excited DC Motor (SEDM) is most employed machine, especially in the industrial sector. Tractions, conveyors, heavy planners, actuators are some of the most commonly known machines which are dependent on the SEDM for their stable and efficient operation. This paper proposes an application of the Artificial Neural Networks (ANNs), one of the most accurate and efficient techniques for non-linear systems, to achieve a precise trajectory control of the speed for a real-time system. In this paper, a comparison of distinct controllers such as Proportional Integral (PI) and Fuzzy Logic Controller (FLC), ANNs by analyzing the system attributes like peak overshoot time, steady-state time for varying load conditions to determine the most efficient speed controller for SEDM is implemented. The neural control scheme comprises two parts: the neural identifier and the neural controller which are used to regulate the motor speed and trigger the control signal respectively. Known for its self-adapting, learning ability, and super-fast computing features of ANN, the NARMA L-2 controller is well-suited as a speed controller for DC motors.",https://ieeexplore.ieee.org/document/9298344/,2020 IEEE International Conference for Innovation in Technology (INOCON),6-8 Nov. 2020,ieeexplore
10.1109/FDL53530.2021.9568376,A Container-based Design Methodology for Robotic Applications on Kubernetes Edge-Cloud architectures,IEEE,Conferences,"Programming modern Robots' missions and behavior has become a very challenging task. The always increasing level of autonomy of such platforms requires the integration of multi-domain software applications to implement artificial intelligence, cognition, and human-robot/robot-robot interaction applications. In addition, to satisfy both functional and nonfunctional requirements such as reliability and energy efficiency, robotic SW applications have to be properly developed to take advantage of heterogeneous (Edge-Fog-Cloud) architectures. In this context, containerization and orchestration are becoming a standard practice as they allow for better information flow among different network levels as well as increased modularity in the use of software components. Nevertheless, the adoption of such a practice along the design flow, from simulation to the deployment of complex robotic applications by addressing the de-facto development standards (i.e., robotic operating system - ROS - compliancy for robotic applications) is still an open problem. We present a design methodology based on Docker and Kubernetes that enables containerization and orchestration of ROS-based robotic SW applications for heterogeneous and hierarchical HW architectures. The design methodology allows for (i) integration and verification of multi-domain components since early in the design flow, (ii) task-to-container mapping techniques to guarantee minimum overhead in terms of performance and memory footprint, and (iii) multi-domain verification of functional and non-functional constraints before deployment. We present the results obtained in a real case of study, in which the design methodology has been applied to program the mission of a Robotnik RB-Kairos mobile robot in an industrial agile production chain. The source code of the mobile robot is publicly available on GitHub.",https://ieeexplore.ieee.org/document/9568376/,2021 Forum on specification & Design Languages (FDL),8-10 Sept. 2021,ieeexplore
10.1109/AIAM48774.2019.00157,A Digital Twin-Based Approach for Quality Control and Optimization of Complex Product Assembly,IEEE,Conferences,"To address the problems caused by low ability of quality analysis and decision-making in the process of complex product assembly, in this paper, we proposed a digital twin-based approach for quality control and optimization of complex product assembly, by providing a digital twin system to realize the timely and precisely interactive mapping between the physical world and digital world. Specifically, a quality control and optimization mechanism is presented, which provides the theoretical support to the realization of the digital twin-based approach. A data-driven quality control model is introduced to solve the optimization problem by considering the panoramic assembly quality data. A digital twin system for complex product assembly is elaborated by providing detailed deployment and implementation procedures, which includes (1) building of the digital entity of an assembly line, (2) real-time online sensing in multi-source heterogeneous environment, (3) real-time simulation of equipment and assembly process, (4) realization of the intelligent production scheduling under uncertainty conditions, and (5) dynamical adjustment of the assembly process. Finally, the paper presents the validation results considering the practical applications of the proposed approach in real industrial fields.",https://ieeexplore.ieee.org/document/8950866/,2019 International Conference on Artificial Intelligence and Advanced Manufacturing (AIAM),16-18 Oct. 2019,ieeexplore
10.1109/CYBER53097.2021.9588142,A Fast and Energy-Saving Neural Network Inference Method for Fault Diagnosis of Industrial Equipment Based on Edge-End Collaboration,IEEE,Conferences,"Data-driven fault diagnosis algorithms represented by deep learning have been widely used in industrial equipment fault diagnosis. However, the lack of real-time performance has always restricted the development of such methods. With the development of edge computing, many edge and end computing devices are deployed in industrial environments. For this distributed computing environment, we propose a distributed neural network inference method with edge-end collaboration. This method uses an edge server to cooperate with multiple end devices for network inference. In the diagnosis of industrial equipment, it can increase the speed of inference, reduce the traffic of the edge network, and help the application of deep neural networks in industrial environments.",https://ieeexplore.ieee.org/document/9588142/,"2021 IEEE 11th Annual International Conference on CYBER Technology in Automation, Control, and Intelligent Systems (CYBER)",27-31 July 2021,ieeexplore
10.1109/ICRA.2019.8793690,A Fog Robotics Approach to Deep Robot Learning: Application to Object Recognition and Grasp Planning in Surface Decluttering,IEEE,Conferences,"The growing demand of industrial, automotive and service robots presents a challenge to the centralized Cloud Robotics model in terms of privacy, security, latency, bandwidth, and reliability. In this paper, we present a `Fog Robotics' approach to deep robot learning that distributes compute, storage and networking resources between the Cloud and the Edge in a federated manner. Deep models are trained on non-private (public) synthetic images in the Cloud; the models are adapted to the private real images of the environment at the Edge within a trusted network and subsequently, deployed as a service for low-latency and secure inference/prediction for other robots in the network. We apply this approach to surface decluttering, where a mobile robot picks and sorts objects from a cluttered floor by learning a deep object recognition and a grasp planning model. Experiments suggest that Fog Robotics can improve performance by sim-to-real domain adaptation in comparison to exclusively using Cloud or Edge resources, while reducing the inference cycle time by 4× to successfully declutter 86% of objects over 213 attempts.",https://ieeexplore.ieee.org/document/8793690/,2019 International Conference on Robotics and Automation (ICRA),20-24 May 2019,ieeexplore
10.1109/SmartCloud49737.2020.00014,A Hardware/Software Framework for the Integration of FPGA-based Accelerators into Cloud Computing Infrastructures,IEEE,Conferences,"The need for high computing power has increased enormously in recent years, particularly in the field of image signal processing and machine learning applications, very powerful computing systems are required. It has been shown that homogeneous architectures in data centers work very inefficiently regarding these special applications, showing high latency of the response times and providing a very poor power efficiency. In order to integrate FPGA (Field Programming Gate Array)- as well as GPU-based accelerators into cloud computing infrastructures as compute nodes we present a generic hardware/software framework for using heterogeneous computing systems. A real industrial image processing application shows the acceleration achieved.",https://ieeexplore.ieee.org/document/9265948/,2020 IEEE International Conference on Smart Cloud (SmartCloud),6-8 Nov. 2020,ieeexplore
10.1109/HPCC.and.EUC.2013.124,A Hypervisor for MIPS-Based Architecture Processors - A Case Study in Loongson Processors,IEEE,Conferences,"Loongson is a family of general purpose processors based on MIPS architecture designed and manufactured in Mainland China. With the maturity of Loongson CPUs, applications are widely available with the increasing development of software tools and hardware platforms by research teams in academia and industry. In recent years, products based on Loongson have been mainly used in education, personal computers and server systems. Meanwhile, it is not yet popularly used in industrial real-time control fields, so such products have large room and potential to further development and deployment. The M-Hyper visor discussed in this paper is a real-time hyper visor designed for MIPS architecture and implemented in Loongson2F processor. It is based on the management program of para-virtualization whilst multiple partitions are scheduled to execute according to their priorities. The design and implementation of M-Hyper visor is discussed, along with details as timer, interrupts, memory management, partition loading and scheduling, to enrich real-time virtualized applications for MIPS architecture. Evaluation results show the performance and viability of proposed design, being promising to new deployments.",https://ieeexplore.ieee.org/document/6832006/,2013 IEEE 10th International Conference on High Performance Computing and Communications & 2013 IEEE International Conference on Embedded and Ubiquitous Computing,13-15 Nov. 2013,ieeexplore
10.1109/CAMAD50429.2020.9209305,A Joint Decentralized Federated Learning and Communications Framework for Industrial Networks,IEEE,Conferences,"Industrial wireless networks are pushing towards distributed architectures moving beyond traditional server-client transactions. Paired with this trend, new synergies are emerging among sensing, communications and Machine Learning (ML) co-design, where resources need to be distributed across different wireless field devices, acting as both data producers and learners. Considering this landscape, Federated Learning (FL) solutions are suitable for training a ML model in distributed systems. In particular, decentralized FL policies target scenarios where learning operations must be implemented collaboratively, without relying on the server, and by exchanging model parameters updates rather than training data over capacity-constrained radio links. This paper proposes a real-time framework for the analysis of decentralized FL systems running on top of industrial wireless networks rooted in the popular Time Slotted Channel Hopping (TSCH) radio interface of the IEEE 802.15.4e standard. The proposed framework is suitable for neural networks trained via distributed Stochastic Gradient Descent (SGD), it quantifies the effects of model pruning, sparsification and quantization, as well as physical and link layer constraints, on FL convergence time and learning loss. The goal is to set the fundamentals for comprehensive methods and procedures supporting decentralized FL pre-deployment design. The proposed tool can be thus used to optimize the deployment of the wireless network and the ML model before its actual installation. It has been verified based on real data targeting smart robotic-assisted manufacturing.",https://ieeexplore.ieee.org/document/9209305/,2020 IEEE 25th International Workshop on Computer Aided Modeling and Design of Communication Links and Networks (CAMAD),14-16 Sept. 2020,ieeexplore
10.1109/SMC.2019.8914519,A Multimodal Perception System for Detection of Human Operators in Robotic Work Cells,IEEE,Conferences,"Workspace monitoring is a critical hw/sw component of modern industrial work cells or in service robotics scenarios, where human operators share their workspace with robots. Reliability of human detection is a major requirement not only for safety purposes but also to avoid unnecessary robot stops or slowdowns in case of false positives. The present paper introduces a novel multimodal perception system for human tracking in shared workspaces based on the fusion of depth and thermal images. A machine learning approach is pursued to achieve reliable detection performance in multi-robot collaborative systems. Robust experimental results are finally demonstrated on a real robotic work cell.",https://ieeexplore.ieee.org/document/8914519/,"2019 IEEE International Conference on Systems, Man and Cybernetics (SMC)",6-9 Oct. 2019,ieeexplore
10.1109/SNPD.2007.425,A Novel Algorithm for Detecting Air Holes in Steel Pipe Welding Based on Hopfield Neural Network,IEEE,Conferences,"The paper segment x-ray images of steel pipe welding to assess the quality of welding. Image segmentation is posed as an optimization problem, and is correlated with the energy function of the multistage Hopfield neural network. The algorithm for optimization and the principle of selecting coefficient are also given. The algorithm is easy to be programmed. As an application, we successfully segment some real industrial welding x-ray images.",https://ieeexplore.ieee.org/document/4287478/,"Eighth ACIS International Conference on Software Engineering, Artificial Intelligence, Networking, and Parallel/Distributed Computing (SNPD 2007)",30 July-1 Aug. 2007,ieeexplore
10.1109/ICICRS46726.2019.9555893,A Novel Approach Towards Industrial Waste Management Using Q-Learning,IEEE,Conferences,"Industrial waste refers to the unwanted solid, liquid and gaseous wastes resulting from an industrial operation. And the collection, transport, processing, disposal and monitoring of these wastes is called as Industrial Waste Management. Industrial Waste has adhered to severe pollution to air, water and soil in the recent years affecting environment and human health. So, its proper and effective management has become important also as the industries’ liability towards the environment. Since many industries don’t have in-house processing plants, they tie-up with industries for the task. This paper focuses on using Q-learning for effective path-planning from generator industries(waste generating) to aggregator industries(waste processing). Q-learning is used for predicting the most efficient path by learning from its own experiences of rewards and penalties and so we don’t need to train the model which therefore increases the efficiency. This work can be implemented at a place where industrial waste is generated and can be very helpful in managing the same. This could be implemented as a service and handed over to the probable customers or government organizations which provide waste management services to the industries. This submission is a step towards automation of the Industrial Waste Management and helps in planning of waste management in real-time.",https://ieeexplore.ieee.org/document/9555893/,2019 International Conference on Intelligent Computing and Remote Sensing (ICICRS),19-20 July 2019,ieeexplore
10.1109/ICNSC48988.2020.9238123,A Novel Reinforcement-Learning-Based Approach to Scientific Workflow Scheduling,IEEE,Conferences,"Recently, the Cloud Computing paradigm is becoming increasingly popular in supporting large-scale and complex workflow applications. The workflow scheduling problem, which refers to finding the most suitable resource for each task of the workflow to meet user defined quality of service (QoS), attracts considerable research attention. Multi-objective optimization algorithms in workflow scheduling have many limitations, e.g., the encoding schemes in most existing heuristic-based scheduling algorithms require prior experts' knowledge and thus they can be ineffective when scheduling workflows upon dynamic cloud infrastructures with real-time. To address this problem, we propose a novel Reinforcement-Learning-Based algorithm to multi-workflow scheduling over IaaS clouds. The proposed algorithm aims at optimizing make-span and dwell time and is to achieve a unique set of correlated equilibrium solution. In the experiment, our algorithm is evaluated for famous scientific workflow templates and real-world industrial IaaS cloud platforms by a simulation process and we compare our algorithm to the current state-of-the-art heuristic algorithms, e.g., NSGA-II, MOPSO, GTBGA. The result shows that our algorithm performs better than compared algorithm.",https://ieeexplore.ieee.org/document/9238123/,"2020 IEEE International Conference on Networking, Sensing and Control (ICNSC)",30 Oct.-2 Nov. 2020,ieeexplore
10.1109/IRI.2019.00040,A Power Efficient Neural Network Implementation on Heterogeneous FPGA and GPU Devices,IEEE,Conferences,"Deep neural networks (DNNs) have seen tremendous industrial successes in various applications, including image recognition, machine translation, audio processing, etc. However, they require massive amounts of computations and take a lot of time to process. This quickly becomes a problem in mobile and handheld devices where real-time multimedia applications such as face detection, disaster management, and CCTV require lightweight, fast, and effective computing solutions. The objective of this project is to utilize specialized devices such as Field Programmable Gate Arrays (FPGAs) and Graphics Processing Units (GPUs) in a heterogeneous computing environment to accelerate the deep learning computations with the constraints of power efficiency. We investigate an efficient DNN implementation and make use of FPGA for fully-connected layer and GPU for floating-point operations. This requires the deep neural network architecture to be implemented in a model parallelism system where the DNN model is broken down and processed in a distributed fashion. The proposed heterogeneous framework idea is implemented using an Nvidia TX2 GPU and a Xilinx Artix-7 FPGA. Experimental results indicate that the proposed framework can achieve faster computation and much lower power consumption.",https://ieeexplore.ieee.org/document/8843495/,2019 IEEE 20th International Conference on Information Reuse and Integration for Data Science (IRI),30 July-1 Aug. 2019,ieeexplore
,"A Practical, Collaborative Approach for Modeling Big Data Analytics Application Requirements",IEEE,Conferences,"Data analytics application development introduces many challenges including: new roles not in traditional software engineering practices – e.g. data scientists and data engineers; use of sophisticated machine learning (ML) model-based approaches; uncertainty inherent in the models; interfacing with models to fulfill software functionalities; deploying models at scale and rapid evolution of business goals and data sources. We describe our Big Data Analytics Modeling Languages (BiDaML) toolset to bring all stakeholders around one tool to specify, model and document big data applications. We report on our experience applying BiDaML to three real-world large-scale applications. Our approach successfully supports complex data analytics application development in industrial settings.",https://ieeexplore.ieee.org/document/9270407/,2020 IEEE/ACM 42nd International Conference on Software Engineering: Companion Proceedings (ICSE-Companion),5-11 Oct. 2020,ieeexplore
10.1109/ASE.2019.00102,A Quantitative Analysis Framework for Recurrent Neural Network,IEEE,Conferences,"Recurrent neural network (RNN) has achieved great success in processing sequential inputs for applications such as automatic speech recognition, natural language processing and machine translation. However, quality and reliability issues of RNNs make them vulnerable to adversarial attacks and hinder their deployment in real-world applications. In this paper, we propose a quantitative analysis framework - DeepStellar - to pave the way for effective quality and security analysis of software systems powered by RNNs. DeepStellar is generic to handle various RNN architectures, including LSTM and GRU, scalable to work on industrial-grade RNN models, and extensible to develop customized analyzers and tools. We demonstrated that, with DeepStellar, users are able to design efficient test generation tools, and develop effective adversarial sample detectors. We tested the developed applications on three real RNN models, including speech recognition and image classification. DeepStellar outperforms existing approaches three hundred times in generating defect-triggering tests and achieves 97% accuracy in detecting adversarial attacks. A video demonstration which shows the main features of DeepStellar is available at: https://sites.google.com/view/deepstellar/tool-demo.",https://ieeexplore.ieee.org/document/8952565/,2019 34th IEEE/ACM International Conference on Automated Software Engineering (ASE),11-15 Nov. 2019,ieeexplore
10.1109/ICRAE50850.2020.9310899,A ROS Based Automatic Control Implementation for Precision Landing on Slow Moving Platforms Using a Cooperative Fleet of Rotary-Wing UAVs,IEEE,Conferences,"In this paper we present an industrial implementation of an efficient method to solve the problem of the automatic precision landing for rotary-wing UAVs, ready to be used inside a cooperative fleet of drones. The realized software module and tests are part of a large industrial R&amp;D Vitrociset project called SWARM: an AI-Enabled Command and Control (C&amp;C) system, able to execute and review ISR missions for mini/micro cooperative fleets of heterogeneous UAVs. Preparatory to the presented results, it was the identification of a non-linear mathematical model as well as the realization of a robust PID-based control system capable of controlling a single drone of the fleet. A discrete-time Kalman filter was integrated and tested to estimate the possible displacement of the landing points, in order to improve the control law through predictive connotations in case of slow moving tags. The presented approach is featured by the balance between computational efficiency and versatility, in particular in the discovering stage of multiple and different AprilTag during the landing phase. The still under test software module uses the Open Source Robotic Operating System (ROS) libraries for both the acquisition of the data necessary to the control laws, and for the execution of the computer vision algorithms implemented for the precision landing. After analyses and simulations campaigns in a synthetic environment and multiple hardware in the loop (HIL) stress tests, the final prototype algorithm was deployed on a commercial-off-the-shelf mini-class UAV, demonstrating landing capacity on a fixed target with an error of less than ten centimeters; moreover, with slow-moving tags, appreciable tracking abilities emerged on sufficiently smooth trajectories. A special interface with the HIL flight controller was then integrated, with the capability of using its telemetry data for distributing them to all the members of the cooperative fleet, making it possible to access the real-time estimate of the states of each single drone, and making each one of them aware of the selected landing areas of the others, by navigation sensors data fusion with a five meters GPS precision.",https://ieeexplore.ieee.org/document/9310899/,2020 5th International Conference on Robotics and Automation Engineering (ICRAE),20-22 Nov. 2020,ieeexplore
10.1109/ICARM52023.2021.9536056,A Review of Bilateral Teleoperation Control Strategies with Soft Environment,IEEE,Conferences,"In the past two decades, bilateral teleoperation with haptic feedback has attracted great research and application interests in both robotics and other areas. Initially triggered by the need to handle dangerous and remote distance tasks such as nuclear materials manipulation and space exploration, bilateral teleoperation has found its way into other applications as a result of development of control theory, robotic technology (both hardware and software) and latest breakthrough in artificial intelligence and machine learning. Consequently, bilateral teleoperation is found facing new challenges brought by these new applications. One major and obvious change is the working environment for the slave manipulator: different from rigid or solid contact environments which are reasonably assumed in early applications in industrial, nuclear and aerospace applications, the slave environment is now more complex and often the objects in contact are much softer in term of stiffness and can not be described by simple elastic model if good teleoperation performance (accurate and transparent) is expected. In this paper, the research of bilateral teleoperation system considering soft environment in recent 20 years has been surveyed for the first time in literature, to the knowledge of the authors. Following the difference in real applications, in this review the definition of soft environment covers linear elastic environment with much lower stiffness than conventional industrial environment and nonlinear complex soft environment with/out time-varying characteristics. Accordingly, the surveyed control strategies and structures in recent literature to improve the stability and accuracy of bilateral teleoperation with soft environment are classified and explained. Finally, the main applications, current challenges and future perspectives of bilateral teleoperation with soft environment are discussed.",https://ieeexplore.ieee.org/document/9536056/,2021 6th IEEE International Conference on Advanced Robotics and Mechatronics (ICARM),3-5 July 2021,ieeexplore
10.1109/MCDM.2007.369428,A Review of Two Industrial Deployments of Multi-criteria Decision-making Systems at General Electric,IEEE,Conferences,"Two industrial deployments of multi-criteria decision-making systems at General Electric are reviewed from the perspective of their multi-criteria decision-making component similarities and differences. The motivation is to present a framework for multi-criteria decision-making system development and deployment. The first deployment is a financial portfolio management system that integrates hybrid multi-objective optimization and interactive Pareto frontier decision-making techniques to optimally allocate financial assets while considering multiple measures of return and risk, and numerous regulatory constraints. The second deployment is a power plant management system that integrates predictive modeling based on neural networks, optimization based on multi-objective evolutionary algorithms, and automated decision-making based on Pareto frontier techniques. The integrated approach, embedded in a real-time plant optimization and control software environment dynamically optimizes emissions and efficiency while simultaneously meeting load demands and other operational constraints in a complex real-world power plant",https://ieeexplore.ieee.org/document/4222994/,2007 IEEE Symposium on Computational Intelligence in Multi-Criteria Decision-Making,1-5 April 2007,ieeexplore
10.1109/ACSOS-C52956.2021.00075,A Self-Learning Architecture for Digital Twins with Self-Protection,IEEE,Conferences,"The digital twin paradigm is a promising enabling technology to accelerate the decarbonisation of industrial sites that use process heat. With digital representations that look-like, behave-like, and connect to a physical system, digital twins bring together critical operational and asset data into a single knowledge store. However, a high-fidelity digital twin relying on the cloud in real-time with direct influence on operations exposes the plant to cyber attacks. We propose a software architecture for a Digital Twin that adaptively generates more accurate representations of its operations to detect malicious activities and mitigate their effects. To achieve this adaptivity, our solution leverages ML, time-series forecasting, concept drift detection and control stability analysis. To evaluate our solution, we develop a simulation of a simple industrial plant consisting of one PID-controlled steam-boiler and a variety of uncertainties. Our experimental evaluation suggests that Dynamic Mode Decomposition with Control, a system identification technique, best contributes towards Self-Learning by producing verifiable models that better align the need for retraining with concept drifts.",https://ieeexplore.ieee.org/document/9599244/,2021 IEEE International Conference on Autonomic Computing and Self-Organizing Systems Companion (ACSOS-C),27 Sept.-1 Oct. 2021,ieeexplore
10.1109/PCIC31437.2018.9080444,"A Smart Condition Monitoring System for HV Networks with Artificial Intelligence, Augmented Reality and Virtual Reality: Copyright Material IEEE, Paper No. PCIC-2018-37",IEEE,Conferences,"The authors present a conceptual design for a SMART asset monitoring solution for high voltage (HV) networks in the petrochemical industry. The paper discusses the potential for incorporating artificial intelligence (AI), augmented reality (AR) and virtual reality (VR) into an application of the Industrial Internet of Things (IIoT) for condition monitoring. The paper is a continuation of the work presented by the authors at the IEEE-PCIC 2017 conference in Calgary. The proposed asset management system analyses condition monitoring (CM) data and assesses the risk of failure data across complete HV networks. Knowledge of deteriorating asset condition provides the operator with an advanced, early warning of incipient mechanical and electrical faults. With knowledge of the severity and source of such faults, pinpointed preventative maintenance interventions can then made during planned maintenance outages. The complete HV network asset monitoring solution described includes permanent sensors and monitoring nodes deployed at strategic locations across the network. Processed data is passed via a local area network to local servers and then via secure data cloud transmission to a centralized monitoring server located at the CM headquarters. This central server operates a CM database that logs, displays, benchmarks and trends the condition data with comparison to a statistically-significant database of measurements. It is proposed in the IIoT solution proposed that this database will be downloadable to a smartphone/tablet for use by the field engineer. The monitoring technology will likely also incorporate a number of AI machine learning software modules for the de-noising of raw signals and the diagnosis of different types of defects within different types of HV plant items. The proposed SMART CM system includes an advanced graphical user interface (GUI) for viewing HV asset CM data along with operational and maintenance (O&amp;M) data. The GUI will also be able to display both condition criticality and operational criticality (on a color-coded range of 0-100%) for individual HV plant items on a digitized mimic of the HV network's single-line diagram (SLD). This could also be combined with geometric positioning data of assets across the facility (including HV cable routes and lengths) to provide a fully digitized SMART network diagram for use in the IIoT asset management solution. Asset management data, combined with the application of the developing techniques of AI, AR and VR, will greatly help the user to visualize the plant items in 3-D, their position within the network, their condition and operational criticality along with all related asset management information together on one dashboard screen, downloaded onto smartphone/tablet. The paper concludes with a case study showing the development of a specification for a SMART IIoT asset condition monitoring solution suitable for a large petrochemical refining facility.",https://ieeexplore.ieee.org/document/9080444/,2018 IEEE Petroleum and Chemical Industry Technical Conference (PCIC),24-26 Sept. 2018,ieeexplore
10.1109/SMARTCOMP52413.2021.00051,A Smart System for Personal Protective Equipment Detection in Industrial Environments Based on Deep Learning,IEEE,Conferences,"The adoption of real-time object detection systems via video streaming analysis is currently exploited in several contexts, from security monitoring to safety prevention. In industrial environments, proper usage of Personal Protective Equipment (PPE) is paramount to ensure workers’ safety. However, the use of some types of PPE, such as helmets, is often neglected by workers, especially in indoor areas. Thus, in order to reduce the risks of accidents, real-time video streaming-based monitoring systems may be used to monitor areas in which workers operate and alert them not to wear PPEs via acoustic alarms or visual signals. In case of a remote analysis, there are potential issues related to the high rate of data streams to be transported and analyzed and workers’ privacy. In this work, we propose an embedded smart system for real-time PPE detection based on video streaming analysis and deep learning models. We discuss the deployment of different versions of the YOLOv4 network fine-tuned using a public PPE dataset. In the end, we assess the performance of the proposed system in terms of accuracy and latency and of the overall PPE detection procedure.",https://ieeexplore.ieee.org/document/9556246/,2021 IEEE International Conference on Smart Computing (SMARTCOMP),23-27 Aug. 2021,ieeexplore
10.1109/PTC.2019.8810705,A Smart Voltage Optimization Approach for Industrial Load Demand Response,IEEE,Conferences,"This paper proposes a generic and comprehensive Voltage Optimization (VO) strategy for energy savings by industrial customers, to lower operating expenses through the implementation of an optimal process-based Demand Response (DR) program without affecting the real-time manufacturing process. This strategy takes into account the complex nature of industrial loads and their unique set of operating constraints, to reduce energy demand for industrial customers by means of varying the voltage at the utility service entrance to the plant. The proposed approach utilizes a Neural Network (NN) model of the industrial load, trained using historical operating data, to estimate the real power consumption of the load, based on the bus voltage and overall plant process. The NN load model is incorporated into the proposed VO model, whose objective is the minimization of the energy drawn from the substation and the number of switching operations of Load Tap Changers (LTC). The proposed VO framework is tested on a real plant model developed using actual measured data. The results demonstrate that the proposed technique can be successfully implemented by industrial customers and plant operators to enhance energy savings compared to Conservation Voltage Reduction (CVR) approaches, and also as a DR strategy that effectively manages the dependence of industrial loads on time-sensitive and critical manufacturing processes.",https://ieeexplore.ieee.org/document/8810705/,2019 IEEE Milan PowerTech,23-27 June 2019,ieeexplore
10.1109/IOLTS52814.2021.9486704,A Suitability Analysis of Software Based Testing Strategies for the On-line Testing of Artificial Neural Networks Applications in Embedded Devices,IEEE,Conferences,"Electronic devices based on artificial intelligence solutions are pervading our everyday life. Nowadays, human decision processes are supported by real-time data gathered from intelligent systems. Artificial Neural Networks (ANNs) are one of the most used deep learning predictive models due to their outstanding computational capabilities. However, assessing their reliability is still an open issue faced by both the academic and industrial worlds, especially when ANNs are deployed on safety-critical systems, such as self-driving cars in the automotive world. In these systems, a strategy for identifying hardware faults is required by industry standards (e.g., ISO26262 for automotive, and DO254 for avionics). Among the existing in-field test strategies, the periodic scheduling of on-line Software Test Library (STL) is a wide strategy adopted; STL allows to reach an acceptable fault coverage without the need for additional hardware. However, when dealing with ANN-based applications, the execution of on-line tests interleaving the ANN inferences may jeopardise the strive for performance maximization. The paper presents a comprehensive analysis of six possible scenarios concerning the execution of on-line self-test programs in embedded devices running ANN-based applications. In the proposed scenarios, the impact of the STL execution on the ANN performance is analyzed; in particular, the execution times of an inference and the Fault Detection Time (FDT) of the STL are discussed and compared. Experimental analyses are provided by relying on: an open-source RISC-V platform running two different convolutional neural networks; a STL for RISC-V cores with a maximum achievable fault coverage of 90%.",https://ieeexplore.ieee.org/document/9486704/,2021 IEEE 27th International Symposium on On-Line Testing and Robust System Design (IOLTS),28-30 June 2021,ieeexplore
10.1109/ETFA46521.2020.9212130,A Systematic Assessment of Embedded Neural Networks for Object Detection,IEEE,Conferences,"Object detection is arguably one of the most important and complex tasks to enable the advent of next-generation autonomous systems. Recent advancements in deep learning techniques allowed a significant improvement in detection accuracy and latency of modern neural networks, allowing their adoption in automotive, avionics and industrial embedded systems, where performances are required to meet size, weight and power constraints.Multiple benchmarks and surveys exist to compare state-of-the-art detection networks, profiling important metrics, like precision, latency and power efficiency on Commercial-off-the-Shelf (COTS) embedded platforms. However, we observed a fundamental lack of fairness in the existing comparisons, with a number of implicit assumptions that may significantly bias the metrics of interest. This includes using heterogeneous settings for the input size, training dataset, threshold confidences, and, most importantly, platform-specific optimizations, that are especially important when assessing latency and energy-related values. The lack of uniform comparisons is mainly due to the significant effort required to re-implement network models, whenever openly available, on the specific platforms, to properly configure the available acceleration engines for optimizing performance, and to re-train the model using a homogeneous dataset.This paper aims at filling this gap, providing a comprehensive and fair comparison of the best-in-class Convolution Neural Networks (CNNs) for real-time embedded systems, detailing the effort made to achieve an unbiased characterization on cutting-edge system-on-chips. Multi-dimensional trade-offs are explored for achieving a proper configuration of the available programmable accelerators for neural inference, adopting the best available software libraries. To stimulate the adoption of fair benchmarking assessments, the framework is released to the public in an open source repository.",https://ieeexplore.ieee.org/document/9212130/,2020 25th IEEE International Conference on Emerging Technologies and Factory Automation (ETFA),8-11 Sept. 2020,ieeexplore
10.1109/PacificVis.2018.00026,A Visual Analytics Approach for Equipment Condition Monitoring in Smart Factories of Process Industry,IEEE,Conferences,"Monitoring equipment conditions is of great value in manufacturing, which can not only reduce unplanned downtime by early detecting anomalies of equipment but also avoid unnecessary routine maintenance. With the coming era of Industry 4.0 (or industrial internet), more and more assets and machines in plants are equipped with various sensors and information systems, which brings an unprecedented opportunity to capture large-scale and fine-grained data for effective on-line equipment condition monitoring. However, due to the lack of systematic methods, analysts still find it challenging to carry out efficient analyses and extract valuable information from the mass volume of data collected, especially for process industry (e.g., a petrochemical plant) with complex manufacturing procedures. In this paper, we report the design and implementation of an interactive visual analytics system, which helps managers and operators at manufacturing sites leverage their domain knowledge and apply substantial human judgements to guide the automated analytical approaches, thus generating understandable and trustable results for real-world applications. Our system integrates advanced analytical algorithms (e.g., Gaussian mixture model with a Bayesian framework) and intuitive visualization designs to provide a comprehensive and adaptive semi-supervised solution to equipment condition monitoring. The example use cases based on a real-world manufacturing dataset and interviews with domain experts demonstrate the effectiveness of our system.",https://ieeexplore.ieee.org/document/8365986/,2018 IEEE Pacific Visualization Symposium (PacificVis),10-13 April 2018,ieeexplore
10.1109/SSCI.2017.8280935,A benchmark environment motivated by industrial control problems,IEEE,Conferences,"In the research area of reinforcement learning (RL), frequently novel and promising methods are developed and introduced to the RL community. However, although many researchers are keen to apply their methods on real-world problems, implementing such methods in real industry environments often is a frustrating and tedious process. Generally, academic research groups have only limited access to real industrial data and applications. For this reason, new methods are usually developed, evaluated and compared by using artificial software benchmarks. On one hand, these benchmarks are designed to provide interpretable RL training scenarios and detailed insight into the learning process of the method on hand. On the other hand, they usually do not share much similarity with industrial real-world applications. For this reason we used our industry experience to design a benchmark which bridges the gap between freely available, documented, and motivated artificial benchmarks and properties of real industrial problems. The resulting industrial benchmark (IB) has been made publicly available to the RL community by publishing its Java and Python code, including an OpenAI Gym wrapper, on Github. In this paper we motivate and describe in detail the IB's dynamics and identify prototypic experimental settings that capture common situations in real-world industry control problems.",https://ieeexplore.ieee.org/document/8280935/,2017 IEEE Symposium Series on Computational Intelligence (SSCI),27 Nov.-1 Dec. 2017,ieeexplore
10.1145/1985793.1985901,A comparison of model-based and judgment-based release planning in incremental software projects,IEEE,Conferences,"Numerous factors are involved when deciding when to implement which features in incremental software development. To facilitate a rational and efficient planning process, release planning models make such factors explicit and compute release plan alternatives according to optimization principles. However, experience suggests that industrial use of such models is limited. To investigate the feasibility of model and tool support, we compared input factors assumed by release planning models with factors considered by expert planners. The former factors were cataloged by systematically surveying release planning models, while the latter were elicited through repertory grid interviews in three software organizations. The findings indicate a substantial overlap between the two approaches. However, a detailed analysis reveals that models focus on only select parts of a possibly larger space of relevant planning factors. Three concrete areas of mismatch were identified: (1) continuously evolving requirements and specifications, (2) continuously changing prioritization criteria, and (3) authority-based decision processes. With these results in mind, models, tools and guidelines can be adjusted to address better real-life development processes.",https://ieeexplore.ieee.org/document/6032518/,2011 33rd International Conference on Software Engineering (ICSE),21-28 May 2011,ieeexplore
10.1109/BTAS.2017.8272758,A competition on generalized software-based face presentation attack detection in mobile scenarios,IEEE,Conferences,"In recent years, software-based face presentation attack detection (PAD) methods have seen a great progress. However, most existing schemes are not able to generalize well in more realistic conditions. The objective of this competition is to evaluate and compare the generalization performances of mobile face PAD techniques under some real-world variations, including unseen input sensors, presentation attack instruments (PAI) and illumination conditions, on a larger scale OULU-NPU dataset using its standard evaluation protocols and metrics. Thirteen teams from academic and industrial institutions across the world participated in this competition. This time typical liveness detection based on physiological signs of life was totally discarded. Instead, every submitted system relies practically on some sort of feature representation extracted from the face and/or background regions using hand-crafted, learned or hybrid descriptors. Interesting results and findings are presented and discussed in this paper.",https://ieeexplore.ieee.org/document/8272758/,2017 IEEE International Joint Conference on Biometrics (IJCB),1-4 Oct. 2017,ieeexplore
10.1109/IJCNN.1991.155570,A data compressed ART-1 neural network algorithm (group technology application),IEEE,Conferences,"Summary form only given, as follows. Adaptive resonance theory (ART) neural networks are being developed for application to the industrial engineering problem of group technology. Two and three dimensional representations of engineering designs are input to ART-1 networks to produce groups or families of similar parts. These representations, in their basic form, amount to bit maps of the part, and can become very large when the part is represented in high resolution. An enhancement to the algorithmic form of ART-1 to allow it to operate directly on compressed input representations and to generate compressed memory templates has been developed. The performance of this compressed algorithm was compared to that of the regular algorithm on real engineering designs. Significant savings in memory storage as well as a speed-up in execution were observed.&lt;<ETX>&gt;</ETX>",https://ieeexplore.ieee.org/document/155570/,IJCNN-91-Seattle International Joint Conference on Neural Networks,8-12 July 1991,ieeexplore
10.1109/icABCD49160.2020.9183810,A deep learning algorithm for detection of potassium deficiency in a red grapevine and spraying actuation using a raspberry pi3,IEEE,Conferences,"The fourth industrial revolution (4IR) has ushered in technological advancement, which is currently reshaping all sectors of the economy, including the agricultural domain. This paper describes the application of artificial intelligence technique on an embedded device. It involves the smart detection of potassium deficiency in red grape vines using the deep learning algorithm. This was deployed on a raspberry pi-3 for real-time actuation and effective prediction. The light-emitting diode (LED) was lit when a potassium deficient red grapevine leaf was brought close to the pi-camera. Image data obtained was fed as input into the model. Training, validation, and testing accuracies of 89%, 81%, and 80% were obtained respectively for the CNN model which surpassed the performance of the Support Vector Machines (SVM) classifier. This research has demonstrated a paradigm shift from the conventional agricultural method of detecting nutrient deficiency to a more effective real-time deep learning algorithm which prompt a corresponding actuation to effectively spray of fertilizers. This technique in no doubt would lead to tremendous increase in food production.",https://ieeexplore.ieee.org/document/9183810/,"2020 International Conference on Artificial Intelligence, Big Data, Computing and Data Communication Systems (icABCD)",6-7 Aug. 2020,ieeexplore
10.1109/ICE/ITMC49519.2020.9198625,A digital twin model of a pasteurization system for food beverages: tools and architecture,IEEE,Conferences,"Many enabling technologies of Industry 4.0 (Internet of Things “IoT”, Cloud systems, Big Data Analytics) contribute to the creation of what is the Digital Twin or virtual twin of a physical process, that is a mathematical model capable of describing the process, product or service in a precise way in order to carry out analyses and apply strategies. Digital Twin models integrate artificial intelligence, machine learning and analytics software with the data collected from the production plants to create digital simulation models that update when the parameters of the production processes or the working conditions change. This is a self-learning mechanism, which makes use of data collected from various sources (sensors that transmit operating conditions; experts, such as engineers with deep knowledge of the industrial domain; other similar machines or fleets of similar machines) and integrates also historical data relating to the past use of the machine. Starting from the virtual twin vision, simulation plays a key role within the Industry 4.0 transformation. Creating a virtual prototype has become necessary and strategic to raise the safety levels of the operators engaged in the maintenance phases, but above all the integration of the digital model with the IoT has become particularly effective, as the advent of software platforms offers the possibility of integrating real-time data with all the digital information that a company owns on a given process, ensuring the realization of the Digital Twin. In this context, this work aims at developing optimized solutions for application in a beverage pasteurization system using the Digital Twin approach, capable of creating a virtual modelling of the process and preventing high-risk events for operators.",https://ieeexplore.ieee.org/document/9198625/,"2020 IEEE International Conference on Engineering, Technology and Innovation (ICE/ITMC)",15-17 June 2020,ieeexplore
10.1109/ASMC.2017.7969203,A dynamic sampling strategy based on confidence level of virtual metrology predictions,IEEE,Conferences,"Metrology is a costly and time consuming activity in semiconductor fabrication; for this reason, Dynamic Sampling strategies and Virtual Metrology approaches have proliferated in the past recent years. Both Dynamic Sampling strategies and Virtual Metrology techniques aim at minimizing the amount of performed measures while keeping acceptable levels of production quality. In this work we study a Dynamic Sampling scheme recently proposed in literature that takes into account the availability of a Virtual Metrology module in the advanced process control architecture. The idea supporting the investigated strategy is based on the availability of a confidence level in the Virtual Metrology predictions; in our implementation of this scheme, this is achieved by exploiting a popular Machine Learning approach for supervised learning tasks, called Random Forests. The aforementioned scheme is tested on a real industrial dataset related to Plasma Etching and it is compared with classical metrology strategies.",https://ieeexplore.ieee.org/document/7969203/,2017 28th Annual SEMI Advanced Semiconductor Manufacturing Conference (ASMC),15-18 May 2017,ieeexplore
10.1109/ISIE.1995.497028,A fixed-point DSP based Cantonese recognition system,IEEE,Conferences,"Speech recognition systems have found useful applications in the field of automation. The development and implementation of a practical speech recognition system has aroused many industrial interests. In this experiment, hardware and software have been designed and implemented for a real-time Cantonese word recognition module using a Texas Instruments digital signal processor (DSP). This paper describes the realisation of the acoustic front-end of this system.",https://ieeexplore.ieee.org/document/497028/,1995 Proceedings of the IEEE International Symposium on Industrial Electronics,10-14 July 1995,ieeexplore
10.1109/INDIN.2013.6622897,A genetic algorithm for optimizing vector-based paths of industrial manipulators,IEEE,Conferences,"Nowadays there is a vast amount of IT tools specialized in vector graphics. The data generated by those tools could be used to describe the path of industrial manipulators as a set of vectors. The main problem is that the sequence/direction of those vectors is not meant to be executed by a robot and attempting to do it, would result in inefficient cycle times of the robot. Therefore it is necessary to generate an execution plan that minimizes the cost of carrying out the vector-based path. The number of possible execution actions has a factorial growth and it is unfeasible to evaluate each of them. This paper proposes the use of a genetic algorithm to optimize this task. The main contribution of this work is a chromosome encoding structure and modifications to the Partially Mapped Crossover operator in order to comply with the constraints of this optimization problem. The algorithm was implemented and tested in a real industrial manipulator.",https://ieeexplore.ieee.org/document/6622897/,2013 11th IEEE International Conference on Industrial Informatics (INDIN),29-31 July 2013,ieeexplore
10.1109/AIIA.1988.13355,A knowledge-based configurer for embedded computer systems software: real life experience,IEEE,Conferences,"The authors present a knowledge-based system that automatically configures complex embedded computer software and hardware in high-volume industrial production. This knowledge-based configurer was implemented to meet two primary goals in large-scale industrial embedded system production: to conform with just-in-time production requirements, and to ensure the efficiency of producing software. The system has been in real-life production use at several sites for more than two years, and the experiences justify both the underlying concepts and the implementation. The authors discuss the application and the implementation aspects, and summarize the experience gained from over two years' maintenance and enhancement. Particular attention is given to application-modelling and knowledge-representation issues.&lt;<ETX>&gt;</ETX>",https://ieeexplore.ieee.org/document/13355/,Proceedings of the International Workshop on Artificial Intelligence for Industrial Applications,25-27 May 1988,ieeexplore
10.1109/ITSC.2017.8317664,A new modelling framework over temporal graphs for collaborative mobility recommendation systems,IEEE,Conferences,"Over the years, collaborative mobility proved to be an important but challenging component of the smart cities paradigm. One of the biggest challenges in the smart mobility domain is the use of data science as an enabler for the implementation of large scale transportation sharing solutions. In particular, the next generation of Intelligent Transportation Systems (ITS) requires the combination of artificial intelligence and discrete simulations when exploring the effects of what-if decisions in complex scenarios with millions of users. In this paper, we address this challenge by presenting an innovative data modelling framework that can be used for ITS related problems. We demonstrate that the use of graphs and time series in multi-dimensional data models can satisfy the requirements of descriptive and predictive analytics in real-world case studies with massive amounts of continuously changing data. The features of the framework are explained in a case study of a complex collaborative mobility system that combines carpooling, carsharing and shared parking. The performance of the framework is tested with a large-scale dataset, performing machine learning tasks and interactive realtime data visualization. The outcome is a fast, efficient and complete architecture that can be easily deployed, tested and used for research as well in an industrial environment.",https://ieeexplore.ieee.org/document/8317664/,2017 IEEE 20th International Conference on Intelligent Transportation Systems (ITSC),16-19 Oct. 2017,ieeexplore
10.1109/EMSCNT.1997.658478,A novel approach to loop parallelization,IEEE,Conferences,"Parallelization environments still have limited usability for the large number of scientists and industrial users wishing to use high performance computing. This is due to software development being geared towards the expert user with knowledge of parallelization and the small percentage of real code that can be analysed because of limitations with symbolic data dependence analysis. There is a need for software that will convert sequential programs to parallel code without concerning the user with parallel programming issues in the main. During parallelization, the sequential program graph is restructured by the application of a number of transformations that remove and reduce loop carried dependences. Selecting the optimum sequence of transformations is a problem that has not been solved satisfactorily by traditional programming techniques. The paper outlines an investigation into the usefulness of neural networks in offering transformation selection guidance. The key issues of code characterization, knowledge acquisition and two neural network based parallelization tools are addressed.",https://ieeexplore.ieee.org/document/658478/,Proceedings 23rd Euromicro Conference New Frontiers of Information Technology - Short Contributions -,1-4 Sept. 1997,ieeexplore
10.1049/cp:19940180,A novel neural adaptive controller for robots,IET,Conferences,"Existing industrial robotic manipulators have proven to be limited in many applications, e.g. both their payload capability and manipulation speeds are limited. This paper presents a novel neural adaptive controller-intelligent gain scheduling-(IGS) for robotic manipulators. It advances the idea of mapping the nonlinear relationship between robot working conditions (e.g. payload, speed, etc.) and its controller gains. This scheme is simple, inexpensive, and especially, attractive for its possible implementation in real-time. Simulation has shown promising results.&lt;<ETX>&gt;</ETX>",https://ieeexplore.ieee.org/document/327097/,1994 International Conference on Control - Control '94.,21-24 March 1994,ieeexplore
10.1109/DTPI52967.2021.9540189,A parallel intelligent control system and its industrial application,IEEE,Conferences,"Parallel control refers to the parallel interaction between the actual physical process and the manual calculation process. The ACP method under its theoretical framework includes artificial system, calculation experiment and parallel control. This paper presents a parallel intelligent control system implementation method, parallel control system needs a carrier, including hardware platform and software system, based on this carrier, the system completes artificial intelligence modeling and real-time optimal control. Firstly, the structure of parallel control platform is introduced, which is composed of industrial control computer, server and power supply, The program function of server is the core part of parallel control system;Secondly. The architecture of parallel intelligent control system is given, The artificial system is designed as an object modeling system, Industrial control computer output excitation signal, The server collects the response data and completes the modeling;The calculation experiment is designed as a process of human-computer interaction, which helps to realize control quality judgment and parameter setting;The parallel control is realized by the industrial controller, and the optimal parameters or control algorithm are put into the controller in parallel to realize the real-time control. Finally, an industrial application example is given to prove the effectiveness of this method.",https://ieeexplore.ieee.org/document/9540189/,2021 IEEE 1st International Conference on Digital Twins and Parallel Intelligence (DTPI),15 July-15 Aug. 2021,ieeexplore
10.1109/IPDPSW.2017.44,A pipelined and scalable dataflow implementation of convolutional neural networks on FPGA,IEEE,Conferences,"Convolutional Neural Network (CNN) is a deep learning algorithm extended from Artificial Neural Network (ANN) and widely used for image classification and recognition, thanks to its invariance to distortions. The recent rapid growth of applications based on deep learning algorithms, especially in the context of Big Data analytics, has dramatically improved both industrial and academic research and exploration of optimized implementations of CNNs on accelerators such as GPUs, FPGAs and ASICs, as general purpose processors can hardly meet the ever increasing performance and energy-efficiency requirements. FPGAs in particular are one of the most attractive alternative, as they allow the exploitation of the implicit parallelism of the algorithm and the acceleration of the different layers of a CNN with custom optimizations, while retaining extreme flexibility thanks to their reconfigurability. In this work, we propose a methodology to implement CNNs on FPGAs in a modular, scalable way. This is done by exploiting the dataflow pattern of convolutions, using an approach derived from previous work on the acceleration of Iterative Stencil Loops (ISLs), a computational pattern that shares some characteristics with convolutions. Furthermore, this approach allows the implementation of a high-level pipeline between the different network layers, resulting in an increase of the overall performance when the CNN is employed to process batches of multiple images, as it would happen in real-life scenarios.",https://ieeexplore.ieee.org/document/7965030/,2017 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW),29 May-2 June 2017,ieeexplore
10.1109/SIPNN.1994.344835,A practical object oriented programming approach for implementing real-time speech compression algorithm,IEEE,Conferences,"A practical approach for developing a system incorporated with real-time speech compression are presented. This is a technique used in practice within the industrial sector for selecting and integrating DSP functionality into a large system. A three-stage technique is used to simulate, evaluate, debug and implement the CCITT G.728 low delay code excited linear prediction (LD-CELP) algorithm. In the first stage, the algorithm is evaluated via simulation to determine whether it meets the design criterion. Then, it is implemented in real-time based an object oriented approach. After the algorithm is thoroughly tested, it is further refined to obtain tighter and faster coding. This technique can be applied to other real-time DSP algorithms.&lt;<ETX>&gt;</ETX>",https://ieeexplore.ieee.org/document/344835/,"Proceedings of ICSIPNN '94. International Conference on Speech, Image Processing and Neural Networks",13-16 April 1994,ieeexplore
10.1109/MFI-2003.2003.1232590,A robust real time position and force (hybrid) control of a robot manipulator in presence of uncertainties,IEEE,Conferences,"We examine the living intelligent biological systems and model the computational system components. We consider the situation of a kind of ""blind-tracking"" with constant force/torque by a human hand. The problem involves hand kinematics, hand motor control, and an adaptive judgment method from the position and force/torque reflection of the uncertain hyper plane. In this study, these control levels were designed using neural networks and fuzzy logic technologies. The control levels are coordinated amongst themselves forming the distributed artificial intelligent (DAI) system. The conclusive characteristic of the proposed controller was a one-step-ahead feedback control. This DAI-based control systems was implemented in the RX-90 industrial robot. Certainly these types of control system will help an industry to be autonomous and increase the productivity as well.",https://ieeexplore.ieee.org/document/1232590/,"Proceedings of IEEE International Conference on Multisensor Fusion and Integration for Intelligent Systems, MFI2003.",1-1 Aug. 2003,ieeexplore
10.1109/INDIN.2015.7281903,A self-learning strategy for artificial cognitive control systems,IEEE,Conferences,"This paper presents a self-learning strategy for an artificial cognitive control based on a reinforcement learning strategy, in particular, an on-line version of a Q-learning algorithm. One architecture for artificial cognitive control was initially reported in [1], but without an effective self-learning strategy in order to deal with nonlinear and time variant behavior. The anticipation mode (i.e., inverse model control) and the single loop mode are two operating modes of the artificial cognitive control architecture. The main goal of the Q-learning algorithm is to deal with intrinsic uncertainty, nonlinearities and noisy behavior of processes in run-time. In order to validate the proposed method, experimental works are carried out for measuring and control the microdrilling process. The real-time application to control the drilling force is presented as a proof of concept. The performance of the artificial cognitive control system by means of the reinforcement learning is improved on the basis of good transient responses and acceptable steady-state error. The Q-learning mechanism built into a low-cost computing platform demonstrates the suitability of its implementation in an industrial setup.",https://ieeexplore.ieee.org/document/7281903/,2015 IEEE 13th International Conference on Industrial Informatics (INDIN),22-24 July 2015,ieeexplore
10.1109/ISIE.2010.5637497,A society of agents for service robots,IEEE,Conferences,"This article presents an agent based distributed software architecture for machine and robot control. The functionality of agents of this architecture has been inspired by Marvin Minsky's definition of the term in his book “The Society of Mind” (1986) [1]. Minsky, widely considered to be one of the fathers of artificial intelligence, tried to describe from an engineering point of view, in this book, how he thought the mind works: “I'll call “Society of Mind” this scheme in which each mind is made of many smaller processes. These we'll call agents. Each mental agent by itself can only do some simple thing that needs no mind or thought at all. Yet when we join these agents in societies-in certain very special ways-this leads to true intelligence.” Societies of simple behaving agents have been implemented in Fatronik, in real robots, and have been demonstrated to be able to perform complex tasks in industrial environments. This article explains the features of such societies of agents and presents their implementation in a real robot.",https://ieeexplore.ieee.org/document/5637497/,2010 IEEE International Symposium on Industrial Electronics,4-7 July 2010,ieeexplore
10.1109/SNPDWinter52325.2021.00056,A study on Medical Device Security Status in Medical Convergence Industry,IEEE,Conferences,"Beyond informatization, the development of new technologies and the introduction of innovative technologies in the Fourth Industrial Revolution are the evolution of various industries. In the early information age, the introduction of various information systems has made it possible to manage and utilize large amounts of data in high quality, thus expanding the business of other industries faster and more reliably. However, in certain industries, various devices and equipments are dependent on the existing information system, and thus, time and cost difficulties are introduced to introduce a new system. Accordingly, existing security problems rather than new security flaws are coming to reality. The purpose of this study is to investigate and analyze the security status of existing medical devices according to the arrival of convergence environment in the medical industry.",https://ieeexplore.ieee.org/document/9403515/,"2021 21st ACIS International Winter Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing (SNPD-Winter)",28-30 Jan. 2021,ieeexplore
10.1109/WCICA.2000.863460,A study on a new robot simulation and monitoring system based on PC,IEEE,Conferences,"A new robot simulation and monitoring system has been developed. Running on Pentium II PCs with Windows 95 operation system and being developed with OpenGL, this system has the capacity of real-time simulating the motion of an industrial robot through 3D animation with ray tracing. Connected with the robot controller via network, users can monitor the behavior of the robot dynamically or even directly control the action of the robot if necessary. By comparing with the traditional off-line programming system, the framework and the functions of this system and the hardware and software platform of the system are described. The principle of 3D-motion simulation and both the geometry modeling and kinematics modeling are discussed in more detail. Finally, the paper summarizes the characteristics of the system and discusses the extension prospect of the system.",https://ieeexplore.ieee.org/document/863460/,Proceedings of the 3rd World Congress on Intelligent Control and Automation (Cat. No.00EX393),26 June-2 July 2000,ieeexplore
10.1109/ICIAS.2007.4658557,A study on industrial communication networking: Ethernet based implementation,IEEE,Conferences,"Recent enhancement of the industrial communications and networking are possible to apply in Ethernet networks system at all levels of industrial automation, especially in the controller level whereby the data exchanges in real-time communication is mandatory. This paper is about a study on the development of industrial communications network based on the Ethernet protocol and thus implement it into computer integrated manufacturing (CIM) system. The purpose of this paper is to overcome real-time communication in which the accessibility of data exchange is very difficult in terms of retrieving data from other stations and time consuming. The Ethernet module is installed onto supervisory OMRON PLC to integrate several of stations in the CIM-70A system which is located at Robotic Laboratory in Universiti Tun Hussein Onn Malaysia (UTHM). The workability of this communication technique is analyzed and compared with the conventional serial communication which widely used in automation networking systems. It is found that, the Ethernet protocol approach through the communication and integration of CIM system can be accessed easily and available to be upgraded at the management and enterprise levels of industrial automation system.",https://ieeexplore.ieee.org/document/4658557/,2007 International Conference on Intelligent and Advanced Systems,25-28 Nov. 2007,ieeexplore
10.1109/INFOC.2017.8001669,A study on the fast system recovery: Selecting the number of surrogate nodes for fast recovery in industrial IoT environment,IEEE,Conferences,This paper is based on the previous research that selects the proper surrogate nodes for fast recovery mechanism in industrial IoT (Internet of Things) Environment which uses a variety of sensors to collect the data and exchange the collected data in real-time for creating added value. We are going to suggest the way that how to decide the number of surrogate node automatically in different deployed industrial IoT Environment so that minimize the system recovery time when the central server likes IoT gateway is in failure. We are going to use the network simulator to measure the recovery time depending on the number of the selected surrogate nodes according to the sub-devices which are connected to the IoT gateway.,https://ieeexplore.ieee.org/document/8001669/,2017 International Conference on Information and Communications (ICIC),26-28 June 2017,ieeexplore
10.1109/SIPS.1998.715769,A system-on-chip design of a low-power smart vision system,IEEE,Conferences,"A low-power smart imager design is proposed for real-time machine vision applications. It takes advantages of recent advances in integrated sensing/processing designs, electronic neural networks, and sub-micron VLSI technology. The smart vision system integrates an active pixel camera, with a programmable neural computer and an advanced microcomputer. A system-on-a-chip implementation of this smart vision system is shown to be feasible by integrating the whole system into a 3-cm/spl times/3-cm chip design in a 0.18 m CMOS technology. The on-chip neural computer provides one tera-operation-per-second computing power for various parallel vision operations and smart sensor functions. Its high performance is due to massively parallel computing structures, high data throughput rates, fast learning capabilities, and system-on-a-chip implementation. This highly integrated smart imager can be used for various scientific missions and other military, industrial or commercial vision applications.",https://ieeexplore.ieee.org/document/715769/,1998 IEEE Workshop on Signal Processing Systems. SIPS 98. Design and Implementation (Cat. No.98TH8374),10-10 Oct. 1998,ieeexplore
10.1109/TAI.1990.130374,A tool system for knowledge-based on-line diagnosis in industrial automation,IEEE,Conferences,"A tool system which supports the development of expert systems for use in the field of industrial automation is presented. The knowledge representation language provided by the tool system supports different aspects of knowledge which are important in this domain: the representation of declarative and procedural knowledge; the representation of temporal knowledge; and the representation of knowledge about real-time behavior. As a further part of the tool system, an expert system compiler with the ability to transform expert systems into a conventional procedural program (realized in C) is described. This method allows a simple integration of knowledge-based techniques into conventional software systems.&lt;<ETX>&gt;</ETX>",https://ieeexplore.ieee.org/document/130374/,[1990] Proceedings of the 2nd International IEEE Conference on Tools for Artificial Intelligence,6-9 Nov. 1990,ieeexplore
10.1109/ICSMC.1996.561490,AI planning in supervisory control systems,IEEE,Conferences,"This paper presents an AI planning-based framework to support the activities of a human operator in a supervisory control system. The framework uses an AI planning and learning substrate architecture and is designed for integration within a general third-party real time software. Our goal is to build a test-bed architecture for research in AI planning applications, such as electrical and industrial processes. AI planning techniques, as opposed to the more traditionally used rule-based systems, can be useful in the automation of the supervision of process systems, as they provide rich planning representations and algorithms. We present our work on developing an AI planning system for a boiler power plant domain. We develop a set of planning operators from an extended multilevel flow modeling (MFM) of the plant. Our planner reasons about goals and its subgoals, generates plans for different scenarios, including the sequence for start-up of the plant. We show our approach to acquire the domain knowledge, which is a well-known difficult enterprise for real-world applications. We demonstrate that our modeling approach built upon MFM, is successful in mapping the supervisory system knowledge into a planning representation.",https://ieeexplore.ieee.org/document/561490/,"1996 IEEE International Conference on Systems, Man and Cybernetics. Information Intelligence and Systems (Cat. No.96CH35929)",14-17 Oct. 1996,ieeexplore
10.1109/SNPDWinter52325.2021.00046,Abrupt covariance based signal extraction for fault prediction of an aircraft engine,IEEE,Conferences,"With the advent of the 4<sup>th</sup> industrial revolution, sensor and data acquisition technology and Internet of Things have advanced, and several analog sensor signals and control commands are automatically monitored in real time. Although numerous manufacturing datasets provide us the bases or evidences for predicting faults in electromechanical systems, they are associated with high dimensionality, which complicates signal analyses. Especially, analog sensor signals from electrotechnical systems are too scattered to detect and investigate the linear or gradient relationship between the signal data and the system state. Therefore, we proposed an abrupt covariance-based signal extraction method, which is a combination of abrupt variance analysis and partial least square regression, for fault prediction. Based on the proposed signal extraction method, we can select the most important features to predict the remaining useful life of an aircraft engine. The proposed extraction approach improves the fault prediction results when numerous analog sensors are used for fault prediction.",https://ieeexplore.ieee.org/document/9403524/,"2021 21st ACIS International Winter Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing (SNPD-Winter)",28-30 Jan. 2021,ieeexplore
10.1109/ICPHYS.2018.8390759,Accented visualization by augmented reality for smart manufacturing aplications,IEEE,Conferences,"Effective application of Augmented Reality user interfaces is one of the challenging trends of industrial cyber-physical systems development and implementation. Despite reach functionality of modern AR devices (like goggles, head mounted displays or tablets) the problem of their comfortable and productive use is not completely solved yet. To cover this gap, it is proposed in this paper to implement a new paradigm of “accented visualization” that allows adapting additional data presented by AR device according to the user's current interest, attention and focus. To provide such context driven functionality there was developed intelligent software based on eye tracking and capturing the user's focus in ontology as a knowledge base. Probation and testing of the proposed approach present 89 % of the solution efficiency.",https://ieeexplore.ieee.org/document/8390759/,2018 IEEE Industrial Cyber-Physical Systems (ICPS),15-18 May 2018,ieeexplore
10.23919/DATE.2019.8714961,Accurate Cost Estimation of Memory Systems Inspired by Machine Learning for Computer Vision,IEEE,Conferences,"Hardware/software co-designs are usually defined at high levels of abstractions at the beginning of the design process in order to allow plenty of options how to eventually realize a system. This allows for design exploration which in turn heavily relies on knowing the costs of different design configurations (with respect to hardware usage as well as firmware metrics). To this end, methods for cost estimation are frequently applied in industrial practice. However, currently used methods for cost estimation oversimplify the problem and ignore important features - leading to estimates which are far off from the real values. In this work, we address this problem for memory systems. To this end, we borrow and re-adapt solutions based on Machine Learning (ML) which have been found suitable for problems from the domain of Computer Vision (CV) - in particular age determination of persons depicted in images. We show that, for an ML approach, age determination from the CV domain is actually very similar to cost estimation of a memory system.",https://ieeexplore.ieee.org/document/8714961/,"2019 Design, Automation & Test in Europe Conference & Exhibition (DATE)",25-29 March 2019,ieeexplore
10.1109/CSCMP45713.2019.8976591,Adaptive Control System Based on Neural Tuner of DC Drive with Sinamics DCM,IEEE,Conferences,"The purpose of this study is to develop an adaptive control system of a DC electric drive implemented on the basis of industrial DC converter Siemens Sinamics DCM. A neural tuner adjusting speed PI-controller parameters is chosen as an adaptation method. In contrary to classical methods of adaptive control, this tuner does not require an accurate nonlinear model of the drive. Instead of this, it evaluates transients quality in a speed loop and, if it does not follow the requirements, adjusts the corresponding controller parameters. This assessment is made by a developed rule base, which calculates the value of a learning rate of the neural network online training. The network output is the controller parameters. The experiments with a real DC motor are conducted under the following conditions. The motor inertia moment is changed by 50% from its nominal value. As a result, the neural tuner adjusts the parameters of the speed PI-controller and achieves the required quality of transients. At the same time, the overshoot obtained with the help of the classic PI-controller is 11% higher than required.",https://ieeexplore.ieee.org/document/8976591/,2019 XXI International Conference Complex Systems: Control and Modeling Problems (CSCMP),3-6 Sept. 2019,ieeexplore
10.1109/MetroInd4.0IoT51437.2021.9488498,Adaptive LoRaWAN Transmission exploiting Reinforcement Learning: the Industrial Case,IEEE,Conferences,"Wireless technologies play a key role in the Industrial Internet of Things (IIoT) scenario, for the development of increasingly flexible and interconnected factory systems. A significant opportunity in this context is represented by the advent of Low Power Wide Area Network (LPWAN) wireless technologies, that enable a reliable, secure, and effective transmission of measurement data over long communication ranges and with very low power consumption. Nevertheless, reliability in harsh environments (as typically occurs in the industrial scenario) is a significant issue to deal with. Focusing on LoRaWAN, adaptive strategies can be profitably devised concerning the above tradeoff. To this aim, this paper proposes to exploit Reinforcement Learning (RL) techniques to design an adaptive LoRaWAN strategy for industrial applications. The RL is spreading in many fields since it allows the design of intelligent systems using a stochastic discrete-time system approach. The proposed technique has been implemented within a purposely designed simulator, allowing to draw a preliminary performance assessment in a real-world scenario. A high density of independent nodes per square km has been considered, showing a significant improvement (about 10%) of the overall reliability in terms of data extraction rate (DER) without compromising full compatibility with the standard specifications.",https://ieeexplore.ieee.org/document/9488498/,2021 IEEE International Workshop on Metrology for Industry 4.0 & IoT (MetroInd4.0&IoT),7-9 June 2021,ieeexplore
10.1109/CIMCA.2008.66,Adaptive Local Learning Soft Sensor for Inferential Control Support,IEEE,Conferences,"In this work we focused on the development of an adaptive Soft Sensor which may be deployed in a real-life environment, for example as inferential control support. To be able to do this, the Soft Sensor must fulfil certain constraints like being able to deal with data impurities or to adapt itself with changing data. The task is approached by training a set of models with limited validity in the data space and by proposing a statistically-based technique for the combination of the local models. The combination weights are related to the estimated performance of the local models in the neighbourhood of the processed data sample. The performance and other benefits of the proposed Soft Sensor are demonstrated in terms of a case study where the model deals with raw industrial data.",https://ieeexplore.ieee.org/document/5172632/,2008 International Conference on Computational Intelligence for Modelling Control & Automation,10-12 Dec. 2008,ieeexplore
10.1109/CIMSA.2006.250758,Adaptive Spatio-Spectral Hyperspectral Image Processing for Online Industrial Classification of Inhomogeneous Materials,IEEE,Conferences,"An approach for considering spatio-spectral information when classifying inhomogeneous materials in industrial environments is proposed. Its main application would be in the inspection and quality control tasks. They system core is an ANN based hyperspectral processing unit able to perform the online determination of the quality of the material based on its composition and grain size. A training adviser is being implemented in the system in order to automate the determination of the optimal spatial window size, as well as to reduce the number of spectral bands used and for determining the optimal spectral combination function through the automatic extraction of the discriminating features. Several tests have been carried out on synthetic and real data sets. In particular, the proposed approach is used to discriminate samples of andalusite having different purities; the results obtained show an accuracy of better than 98%",https://ieeexplore.ieee.org/document/4016840/,2006 IEEE International Conference on Computational Intelligence for Measurement Systems and Applications,12-14 July 2006,ieeexplore
10.1109/ICIT.2013.6505816,Adaptive image pre-processing for quality control in production lines,IEEE,Conferences,"Flexible and self-adaptive behaviours in automated quality control systems are features that may significantly enhance the robustness, efficiency and flexibility of the industrial production processes. However, most current approaches on automated quality control are based on rigid inspection methods and are not capable of accommodating to disturbances affecting the image acquisition quality, fact that hast direct consequences on the system's reliability and performance. In an effort to address the problem, this paper presents the development of a self-adaptive software system designed for the pre-processing (quality enhancement) of digital images captured in industrial production lines. The approach introduces the use of scene recognition as a key-feature to allow the execution of customized image pre-processing strategies, increase the system's flexibility and enable self-adapting conducts. Real images captured in a washing machines production line are presented to test and validate the system performance. Experimental results demonstrate significant image quality enhancements and a valuable reliability improvement of the automated quality control procedures.",https://ieeexplore.ieee.org/document/6505816/,2013 IEEE International Conference on Industrial Technology (ICIT),25-28 Feb. 2013,ieeexplore
10.1109/ICIT.1996.601644,Adaptive robust robot control using BP-SMENs,IEEE,Conferences,"This paper presents the development of a new adaptive recurrent neural network for the control of a nonlinear system represented by a two-link SCARA type planar robot manipulator. The standard backpropagation algorithm is used to adjust the weights of the networks. The proposed control system consists of an inverse neural model of robot (INNM), an INNM-based neural controller, a robust controller, a conventional PI controller, and a second order linear filter. To evaluate the performance of the proposed control scheme and neural network, a simulated SCARA type robot was studied and the results showed how well the proposed controller can minimise the error between an actual and desired end-effector trajectory. From simulation examples, the robot trajectory tracking showed superior performance that is very attractive for real-time implementation and application in complex industrial tasks. For comparison, the standard computed torque method is employed for controlling the robot.",https://ieeexplore.ieee.org/document/601644/,Proceedings of the IEEE International Conference on Industrial Technology (ICIT'96),2-6 Dec. 1996,ieeexplore
10.1109/ICMLA.2012.160,Adaptive soft sensor for online prediction based on moving window Gaussian process regression,IEEE,Conferences,"Very often important process variables cannot be measured online due to low sampling rate of sensors or because their values have to be obtained by laboratory analysis. In order to enable continuous process monitoring and efficient process control in such cases, soft sensors are usually used to estimate these difficult-to-measure process variables. Most industrial processes exhibit some kind of time-varying behavior. To ensure that soft sensor retains its precision, adaptation mechanism has to be implemented. In this paper adaptive soft sensor based on Gaussian Process Regression (GPR) is presented. To make GPR model training more efficient, algorithm for variable selection based on Mutual Information is proposed. Prediction capabilities of the proposed method are examined on real industrial data obtained at an oil distillation column.",https://ieeexplore.ieee.org/document/6406773/,2012 11th International Conference on Machine Learning and Applications,12-15 Dec. 2012,ieeexplore
10.1109/INDIN.2017.8104789,Addressing security challenges in industrial augmented reality systems,IEEE,Conferences,"In context of Industry 4.0 Augmented Reality (AR) is frequently mentioned as the upcoming interface technology for human-machine communication and collaboration. Many prototypes have already arisen in both the consumer market and in the industrial sector. According to numerous experts it will take only few years until AR will reach the maturity level to be deployed in productive applications. Especially for industrial usage it is required to assess security risks and challenges this new technology implicates. Thereby we focus on plant operators, Original Equipment Manufacturers (OEMs) and component vendors as stakeholders. Starting from several industrial AR use cases and the structure of contemporary AR applications, in this paper we identify security assets worthy of protection and derive the corresponding security goals. Afterwards we elaborate the threats industrial AR applications are exposed to and develop an edge computing architecture for future AR applications which encompasses various measures to reduce security risks for our stakeholders.",https://ieeexplore.ieee.org/document/8104789/,2017 IEEE 15th International Conference on Industrial Informatics (INDIN),24-26 July 2017,ieeexplore
10.1109/ISPACS.2010.5704595,Advancing multimedia technologies for smart living and learning services,IEEE,Conferences,"The development of ICT technologies to advance smart living products and creative humanity services becomes future global consensus and industrial trends in the world. The user-driven innovations, which bring products and services naturally and smartly close to human needs, should heavily adopt many interactive multimedia technologies. The interactions through the detection of human nature touch, voice, and gestures and the responses of virtually real multimedia could be implemented in highly integrated embedded and cloud computing systems. In the talk, some designs such as 3D interactive sports, Interactive arts, interactive table, and smart living and learning products in the Technologies of Ubiquitous Computing and Humanity (TOUCH) Center, will be introduced. Through smart living and learning clouds, the future plans related to smart living and efficient learning services for the students are addressed. Linked to the city government, the humanity services through living lab open innovations delivered to the citizens will be forecasted finally.",https://ieeexplore.ieee.org/document/5704595/,2010 International Symposium on Intelligent Signal Processing and Communication Systems,6-8 Dec. 2010,ieeexplore
10.1109/CIPLS.2013.6595200,Agent-based dispatching in groupage traffic,IEEE,Conferences,"The complexity and dynamics in group age traffic requires flexible, efficient, and adaptive planning and controlling processes. While the general problem refers to the Vehicle Routing Problem (VRP), additional requirements have to be fulfilled in application. Individual properties and priorities of orders, a heterogeneous fleet of vehicles, dynamically incoming orders, unexpected events etc. require a proactive and reactive system behavior. To enable automated dispatching processes, we have implemented a multiagent system where the decision making is shifted from a central system to autonomous, interacting, intelligent agents. To evaluate the approach we used multi agent-based simulation and modeled several scenarios on real world infrastructures with orders provided by our industrial partner. The results reveal that agent-based dispatching meets the increasing requirements in groupage traffic while supporting the combination of pickup and delivery tours and accommodating request priorities, time-windows, as well as capacity constraints.",https://ieeexplore.ieee.org/document/6595200/,2013 IEEE Symposium on Computational Intelligence in Production and Logistics Systems (CIPLS),16-19 April 2013,ieeexplore
10.1109/CEWIT.2013.6713745,Agent-based planning and control for groupage traffic,IEEE,Conferences,"In this research and technology transfer project, the planning and control processes of the industrial partner Hellmann Worldwide Logistics GmbH &amp; Co. KG are analyzed. An agent-based approach is presented to model current processes and to exploit the identified optimization potential. The developed system directly connects the information flow and the material flow as well as their interdependencies in order to optimize the planning and control in groupage traffic. The software system maps current processes to agents as system components and improves the efficiency by intelligent objects. To handle the high complexity and dynamics of logistics autonomous intelligent agents plan and control the way of represented objects through the logistic network by themselves and induce a flexible and reactive system behavior. We evaluate the implemented dispatching application by simulating the groupage traffic processes using effectively transported orders and process data provided by our industrial partner. Moreover, we modeled real world infrastructures and considered also the dynamics by the simulation of unexpected events and process disturbances. The results show that the system significantly decreases daily cost by reducing the required number of transport providers and shifting conventional orders to next days, which need no immediate delivery. Thus the system increases the efficiency and meets the special challenges and requirements of groupage traffic. Moreover, the system supports freight carriers and dispatchers with adequate tour and routing proposals. Computed tours were successfully validated by human dispatchers. Due to the promising results, Hellmann is highly interested in transferring the prototype to an application that optimizes the daily operations in numerous distribution centers. Finally, provide further research perspectives, and emphasize the advantages of the developed system in Industry 4.0 applications.",https://ieeexplore.ieee.org/document/6713745/,2013 10th International Conference and Expo on Emerging Technologies for a Smarter World (CEWIT),21-22 Oct. 2013,ieeexplore
10.1109/ICRAS52289.2021.9476659,Agroindustrial Plant for the Classification of Hass Avocados in Real-Time with ResNet-18 Architecture,IEEE,Conferences,"The avocado is the fruit with a growing trend in production due to its demand in the world market. Peru currently ranks third in the export of Hass type avocados. For the efficient classification of avocados in good or bad condition, a ResNet-18 algorithm applied to a robust agro-industrial plant was implemented. By using a non-invasive classification we reduce handling damage. The plant consists of a feeder system that continues with a conveyor belt, followed by the image acquisition system with its lighting system, finally, there is the classification system formed by the pneumatic system consisting of pistons that will deposit the avocados in the right containers. The treatment of the images was developed in three stages: acquisition, training, and implementation of the neural network. The Deep Learning algorithm used is ResNet-18, and the hyperparameters of the convolutional network were adjusted to obtain a precision of 98.72%, a specificity of 98.52%, and an F1 score of 98.08%.",https://ieeexplore.ieee.org/document/9476659/,2021 5th International Conference on Robotics and Automation Sciences (ICRAS),11-13 June 2021,ieeexplore
10.1109/GROUP4.2006.1708184,An Active Demodulation Pixel using a Current Assisted Photonic Demodulator Implemented in 0.6μm Standard CMOS,IEEE,Conferences,"With the ever increasing automation of industrial processes, and the growing need for intelligent systems in, for example, auto-motive and safety/security applications, the demand for adequate artificial three-dimensional (3D) vision increases. It is still the weakest link between intelligent systems in general and the real world. Systems based on the Time-of-Flight (TOF) principle provide an elegant solution to these needs. The key structure to enable wide scale use of TOF ranging systems is a photonic demodulator with high responsivity and high demodulation bandwidth. This paper intends to highlight one of the most promising Standard CMOS photonic demodulator architecture",https://ieeexplore.ieee.org/document/1708184/,"3rd IEEE International Conference on Group IV Photonics, 2006.",13-15 Sept. 2006,ieeexplore
10.1109/WiMOB.2019.8923315,An Anomaly Detector for CAN Bus Networks in Autonomous Cars based on Neural Networks,IEEE,Conferences,"The domain of securing in-vehicle networks has attracted both academic and industrial researchers due to high danger of attacks on drivers and passengers. While securing wired and wireless interfaces is important to defend against these threats, detecting attacks is still the critical phase to construct a robust secure system. There are only a few results on securing communication inside vehicles using anomaly-detection techniques despite their efficiencies in systems that need real-time detection. Therefore, we propose an intrusion detection system (IDS) based on Multi-Layer Perceptron (MLP) neural network for Controller Area Networks (CAN) bus. This IDS divides data according to the ID field of CAN packets using K-means clustering algorithm, then it extracts suitable features and uses them to train and construct the neural network. The proposed IDS works for each ID separately and finally it combines their individual decisions to construct the final score and generates alert in the presence of attack. The strength of our intrusion detection method is that it works simultaneously for two types of attacks which will eliminate the use of several separate IDS and thus reduce the complexity and cost of implementation.",https://ieeexplore.ieee.org/document/8923315/,"2019 International Conference on Wireless and Mobile Computing, Networking and Communications (WiMob)",21-23 Oct. 2019,ieeexplore
10.1109/IICPE.2006.4685399,An Ant Colony based Hybrid Intelligent Controller for looper tension in steel rolling mills,IEEE,Conferences,"Strip tension control is crucial in the tandem finishing mills for its safe operation. Most of the industrial applications are of nonlinear. Fuzzy logic controller is the most useful approach to achieve the adaptive ness in the case of nonlinear system. Since fuzzy logic control provides a systematic method of incorporating human expertise and implementing nonlinear system. Neural networks are integrated with fuzzy logic which forms a neuro fuzzy system. To get a desired solution from the vagueness of the problem, an ant colony system (ACS) is implemented. The nature of search of food by the real antpsilas colony to find the food from its source (i.e. nest), to its destination (i.e. food) is known as ACS. The same foraging behavior of ants can be used to determine the optimal solution for the membership functions of FLC and hence form the hybrid intelligent controller (HIC). This paper demonstrates the effectiveness of HIC in optimizing the looper height in steel rolling mills compared with conventional controllers, FLC. The simulation result depicts that HIC quickly restore the speed of the main drive and hence looper height is quickly reduced to its optimal (zero) value which intern ensures the safety working condition of steel rolling mills.",https://ieeexplore.ieee.org/document/4685399/,2006 India International Conference on Power Electronics,19-21 Dec. 2006,ieeexplore
10.1109/ECTC32696.2021.00346,An Automated Optical Inspection System for PIP Solder Joint Classification Using Convolutional Neural Networks,IEEE,Conferences,"In the fields of electronics manufacturing, the application of through-hole devices is still required, as heat dissipation and high current carrying capacity plays an important role. To ensure the highest quality standards, these electronics production processes take a multitude of inspection processes into account. For the detection of error patterns regarding the quality of the solder connections, usually, high-end inspection machines are utilized in the industrial application. The Automated Optical Inspection is a commonly conducted process, using visible light and rule-based inspection routines, setup by process experts for the evaluation of the Region of Interest. The high overhead of creating and maintaining product-specific checking routines and machine acquisition leads to increased costs and severe dependency on expert know-how. A flexible inspection algorithm, implemented into low-cost equipment for image generation is expected to reduce acquisition and optimization costs, and lower dependency on expert knowledge and high-end machinery. In this contribution, we present a novel framework for the automatic, near real-time solder joint classification based on Convolutional Neural Networks, flexibly detecting, and classifying solder connections. We utilize existing Deep Learning architectures for detection and classification. The localization model utilizes a YOLO-architecture (you-only-look-once), learning feature inputs based on a supervised learning approach. Pseudo-labeling is carried out automatically by an anomaly detection model. The image generation is executed by an industrial low-cost camera and an industrial rack-PC. The developed prototype is integrated into the existing production infrastructure. The results indicate a satisfactory detection and classification of the investigated solder connections with the proposed system. Hence, this system represent an alternative to commercially available high-end inspection systems being used for an inline control of Pin-in-Paste and through-hole device solder connections.",https://ieeexplore.ieee.org/document/9501916/,2021 IEEE 71st Electronic Components and Technology Conference (ECTC),1 June-4 July 2021,ieeexplore
10.1109/WCICA.2006.1713761,An Embedded Platform for Intelligent Mobile Robot,IEEE,Conferences,"To overcome the limitations of the special architectures adopted by traditional industrial robots, an embedded intelligent robot platform based on Windows CE.NET is established by customizing the operating system. On this intelligent robot platform, all major necessary sensors are included, and abundant control interfaces and driver modules are available, such as movement control interface, USB camera driver, laser driver, etc. Besides, various testing software and application modules for intelligent robot are designed, such as multi-sensor data fusion, path planning, speech recognition, wireless network communication and graphic human-robot interface. The platform is modularized, extensible, transplantable and customizable. Compared to the previous robot platform, it also has many advantages such as more compact hardware, lower-power consumption, better real-time performance and higher reliability",https://ieeexplore.ieee.org/document/1713761/,2006 6th World Congress on Intelligent Control and Automation,21-23 June 2006,ieeexplore
10.1109/ASE.2019.00080,An Empirical Study Towards Characterizing Deep Learning Development and Deployment Across Different Frameworks and Platforms,IEEE,Conferences,"Deep Learning (DL) has recently achieved tremendous success. A variety of DL frameworks and platforms play a key role to catalyze such progress. However, the differences in architecture designs and implementations of existing frameworks and platforms bring new challenges for DL software development and deployment. Till now, there is no study on how various mainstream frameworks and platforms influence both DL software development and deployment in practice. To fill this gap, we take the first step towards understanding how the most widely-used DL frameworks and platforms support the DL software development and deployment. We conduct a systematic study on these frameworks and platforms by using two types of DNN architectures and three popular datasets. (1) For development process, we investigate the prediction accuracy under the same runtime training configuration or same model weights/biases. We also study the adversarial robustness of trained models by leveraging the existing adversarial attack techniques. The experimental results show that the computing differences across frameworks could result in an obvious prediction accuracy decline, which should draw the attention of DL developers. (2) For deployment process, we investigate the prediction accuracy and performance (refers to time cost and memory consumption) when the trained models are migrated/quantized from PC to real mobile devices and web browsers. The DL platform study unveils that the migration and quantization still suffer from compatibility and reliability issues. Meanwhile, we find several DL software bugs by using the results as a benchmark. We further validate the results through bug confirmation from stakeholders and industrial positive feedback to highlight the implications of our study. Through our study, we summarize practical guidelines, identify challenges and pinpoint new research directions, such as understanding the characteristics of DL frameworks and platforms, avoiding compatibility and reliability issues, detecting DL software bugs, and reducing time cost and memory consumption towards developing and deploying high quality DL systems effectively.",https://ieeexplore.ieee.org/document/8952401/,2019 34th IEEE/ACM International Conference on Automated Software Engineering (ASE),11-15 Nov. 2019,ieeexplore
10.1109/ICETA.2018.8572099,An Implementation of Efficient Hierarchical Access Control Method for VR/AR Platform,IEEE,Conferences,"With the growth of Virtual Reality (VR) and Augmented Reality (AR) in technologies such as artificial intelligence, wireless, 5G, big data, massive compute, industrial 4.0 and virtual stores. An improved secure mechanism which was previously still existed some shortcomings, was presented in this paper. Another new mechanism regards to achieve the decentralized environment access control is introduced to attain the needs of the non-specific internet. Furthermore, it is very important to look attentively to the administrative previleges in VR/AR clouds. The issues which gained from VR and AR could be tackled with the new mechanism. This new studies achieves a higher circumstance. Developer worker's duty can be distributed; the database system can be compatibly coordinates; moreover, the users' information security can be entirely secured.",https://ieeexplore.ieee.org/document/8572099/,2018 16th International Conference on Emerging eLearning Technologies and Applications (ICETA),15-16 Nov. 2018,ieeexplore
10.23919/ChiCC.2018.8483465,An Improved Ensemble Adaptive Kernel PLS Soft Sensor Model and its Application,IEEE,Conferences,"To avoid the disadvantage of traditional PLS model in dealing with nonlinear data, Kernel PLS (KPLS) algorithm has been proposed. By mapping nonlinear data into high-dimensional space with kernel function, the original data set can be processed using linear models in the new space. However, when facing diverse complicated nonlinear features, the simple kernel method also exhibits some limitations. To tackle this problem, an improved k-means based Ensemble Adaptive Kernel Partial Least Squares (EAKPLS) is proposed. Its whole processes are implemented as follows. In the modeling phase, the data is first divided into k sub datasets by k-means clustering algorithm. Then for each subset, different kernels and corresponding kernel parameters are chosen adaptively by introducing PSO algorithm. In the prediction phase, ensemble learning is introduced to obtain the final predictable value where Bayes' theorem is applied, where an improved weights assignment strategy is also presented. Ultimately, numerical and real industrial test cases are both given to demonstrate its feasibility and effectiveness.",https://ieeexplore.ieee.org/document/8483465/,2018 37th Chinese Control Conference (CCC),25-27 July 2018,ieeexplore
10.1109/ICDCS.2019.00139,An Industrial IoT Solution for Evaluating Workers' Performance Via Activity Recognition,IEEE,Conferences,"The Industrial Internet of Things (IIoT) is a key pillar of the Fourth Industrial Evolution or Industry 4.0. It aims to achieve direct information exchange between industrial machines, people, and processes. By tapping and analysing such data, IIoT can more importantly provide for significant improvements in productivity, product quality, and safety via proactive detection of problems in the performance and reliability of production machines, workers, and industrial processes. While the majority of existing IIoT research is currently focusing on the predictive maintenance of industrial machines (unplanned production stoppages lead to significant increases in costs and lost plant productivity), this paper focuses on monitoring and assessing worker productivity. This IIoT research is particularly important for large manufacturing plants where most production activities are performed by workers using tools and operating machines. With this aim, this paper introduces a novel industrial IoT solution for monitoring, evaluating, and improving worker and related plant productivity based on workers activity recognition using a distributed platform and wearable sensors. More specifically, this IIoT solution captures acceleration and gyroscopic data from wearable sensors in edge computers and analyses them in powerful processing servers in the cloud to provide a timely evaluation of the performance and productivity of each individual worker in the production line. These are achieved by classifying worker production activities and computing Key Performance Indicators (KPIs) from the captured sensor data. We present a real-world case study that utilises our IIoT solution in a large meat processing plant (MPP). We illustrate the design of the IIoT solution, describe the in-plant data collection during normal operation, and present the sensor data analysis and related KPI computation, as well as the outcomes and lessons learnt.",https://ieeexplore.ieee.org/document/8884821/,2019 IEEE 39th International Conference on Distributed Computing Systems (ICDCS),7-10 July 2019,ieeexplore
10.1109/AITEST52744.2021.00024,An Industrial Workbench for Test Scenario Identification for Autonomous Driving Software,IEEE,Conferences,"Testing of autonomous vehicles involves enormous challenges for the automotive industry. The number of real-world driving scenarios is extremely large, and choosing effective test scenarios is essential, as well as combining simulated and real world testing. We present an industrial workbench of tools and workflows to generate efficient and effective test scenarios for active safety and autonomous driving functions. The workbench is based on existing engineering tools, and helps smoothly integrate simulated testing, with real vehicle parameters and software. We aim to validate the workbench with real cases and further refine the input model parameters and distributions.",https://ieeexplore.ieee.org/document/9564354/,2021 IEEE International Conference on Artificial Intelligence Testing (AITest),23-26 Aug. 2021,ieeexplore
10.1109/ICCWorkshops49005.2020.9145434,An Inter-Disciplinary Modelling Approach in Industrial 5G/6G and Machine Learning Era,IEEE,Conferences,"Unlike conventional cellular systems, the fifth generation (5G) and beyond includes intrinsic support for vertical industries with diverse service requirements. Industrial process automation with autonomous fault detection and prediction, optimised operations and proactive control can be considered as one of the key verticals of 5G and beyond. Such applications enable equipping industrial plants with a reasoning sixth sense for optimised operations and fault avoidance. In this direction, we introduce an inter-disciplinary approach integrating wireless sensor networks with machine learning-enabled industrial plants to build a step towards developing this sixth sense technology, i.e., the reasoning ability. We develop a modular-based system that can be adapted to the vertical-specific elements. Without loss of generalisation, exemplary use cases are developed and presented including a fault detection/prediction scheme in a wireless communication network with sensors and actuators to enable the sixth sense technology with guaranteed service load requirements. The proposed schemes and modelling approach are implemented in a real chemical plant for testing purposes, and a high fault detection and prediction accuracy is achieved coupled with optimised sensor density analysis.",https://ieeexplore.ieee.org/document/9145434/,2020 IEEE International Conference on Communications Workshops (ICC Workshops),7-11 June 2020,ieeexplore
10.1109/ISSE46696.2019.8984462,An IoT Reconfigurable SoC Platform for Computer Vision Applications,IEEE,Conferences,"The field of Internet of Things (IoT) and smart sensors has expanded rapidly in various fields of research and industrial applications. The area of IoT robotics has become a critical component in the evolution of Industry 4.0 standard. In this paper, we developed an IoT based reconfigurable System on Chip (SoC) robot that is fast and efficient for computer vision applications. It can be deployed in other IoT robotics applications and achieve its intended function. A Terasic Hexapod Spider Robot (TSR) was used with its DE0-Nano SoC board to implement our IoT robotics system. The TSR was designed to provide a competent computer vision application to recognize different shapes using a machine learning classifier. The data processing for image detection was divided into two parts, the first part involves hardware implementation on the SoC board and to provide real-time interaction of the robot with the surrounding environment. The second part of implementation is based on the cloud processing technique, where further data analysis was performed. The image detection algorithm for the computer vision component was tested and successfully implemented to recognize shapes. The TSR moves or reacts based on the detected image. The Field Programmable Gate Array (FPGA) part is programmed to handle the movement of the robot and the Hard Processor System (HPS) handles the shape recognition, Wi-Fi connectivity, and Bluetooth communication. This design is implemented, tested and can be used in real-time applications in harsh environments where movements of other robots are restricted.",https://ieeexplore.ieee.org/document/8984462/,2019 International Symposium on Systems Engineering (ISSE),1-3 Oct. 2019,ieeexplore
10.1109/SPAC53836.2021.9539933,An LSTM based Malicious Traffic Attack Detection in Industrial Internet,IEEE,Conferences,"Current Industrial Internet faces serious threats where attackers propagate malicious flows, resulting in communication failures in the Industrial Internet. In this work, we propose a practical and novel method to detect malicious traffic attack in real time with high accuracy. Our primary idea is to capture network flow, extract adequate network flow features, construct a long short-term memory (LSTM) based deep learning model, and identify the property of the corresponding network flow. Whether the network suffers attack or not is then determined according to the detection results. The corresponding prototype is also implemented in the Industrial Internet which is equipped with Software Defined Networking (SDN). Experimental results validate that the proposed method is effective in defending against malicious traffic attack in real-world network.",https://ieeexplore.ieee.org/document/9539933/,"2021 International Conference on Security, Pattern Analysis, and Cybernetics（SPAC)",18-20 June 2021,ieeexplore
10.1109/ANTS47819.2019.9117981,An SD-WAN Controller for Delay Jitter Minimization in Coded Multi-access Systems,IEEE,Conferences,"For an industrial network that is expected to perform with high reliability and availability, we consider the problem of designing a low-cost approach to continuous monitoring and maintenance of the communication links. A software defined wide area network (SD-WAN) based solution is proposed to adaptively monitor and manage the connectivity links from an end device as per the application requirement. Each such device may have multiple cellular interfaces, and we control the end-to-end delay jitter variation across the different interfaces via an intelligent traffic splitting scheme. We propose use of interstream coding to achieve target delay jitter and also a high reliability/availability. We use a decoupling approach to adapt the probabilistic traffic split into various interfaces and the extent of inter-stream coding. We provide an end-to-end measurement-based traffic splitting scheme that relies on a Machine Learning algorithm. We use a stochastic approximation-like algorithm (operating at a slower timescale) to obtain the right coding level. The various modules developed are pluggable in a pipeline-manner and work with real interfaces as well as simulators like openairinterface. We validate our analysis, and provide traffic split performance results from our working multi-access system (using cellular dongles, and also over openairinterface).",https://ieeexplore.ieee.org/document/9117981/,2019 IEEE International Conference on Advanced Networks and Telecommunications Systems (ANTS),16-19 Dec. 2019,ieeexplore
10.1109/ETFA.2014.7005346,An adaptive image processing system based on incremental learning for industrial applications,IEEE,Conferences,"Machine learning has been applied in image processing system for object recognition, inspection and measurement. It assumes that the provided training objects are representative enough to the real objects. However in real application, new (unlearned) objects always emerge over time, which may deviate from the trained (learned) objects. The conventional image processing system using machine learning is not able to learn and then recognize these new objects. In this paper, an incremental learning based image processing system is presented. The overall system consists of three layers: execution, learning and user. The conventional image processing system is constructed in execution layer. In learning layer, adviser and incremental learning are applied to generate a new classifier. The incremental learning is differentiated into different methodologies: data accumulation and ensemble learning. Through the adviser, a proper methodology can be recommended. User is able to interact with the system via user layer. Comparing to the conventional image processing system, the proposed system is robust in industrial applications, since it deals with the classification problems dynamically.",https://ieeexplore.ieee.org/document/7005346/,Proceedings of the 2014 IEEE Emerging Technology and Factory Automation (ETFA),16-19 Sept. 2014,ieeexplore
10.1109/ASIC.1994.404614,An analog VLSI neural network for real-time image processing in industrial applications,IEEE,Conferences,"In this paper we present an analog VLSI architecture that implements a neural network for image processing in industrial environment. The analog architecture is highly modular and operates in real time. The circuit implementation is based on simple and effective circuit primitives. Special care has been devoted to the analysis of the linearity and the precision of computation. A test chip, implementing the filtering stage of the architecture, has been designed and realized.&lt;<ETX>&gt;</ETX>",https://ieeexplore.ieee.org/document/404614/,Proceedings Seventh Annual IEEE International ASIC Conference and Exhibit,19-23 Sept. 1994,ieeexplore
10.1109/INDIN.2005.1560420,An analogue recurrent neural network for trajectory learning and other industrial applications,IEEE,Conferences,"A real-time analogue recurrent neural network (RNN) can extract and learn the unknown dynamics (and features) of a typical control system such as a robot manipulator. The task at hand is a tracking problem in the presence of disturbances. With reference to the tasks assigned to an industrial robot, one important issue is to determine the motion of the joints and the effector of the robot. In order to model robot dynamics we use a neural network that can be implemented in hardware. The synaptic weights are modelled as variable gain cells that can be implemented with a few MOS transistors. The network output signals portray the periodicity and other characteristics of the input signal in unsupervised mode. For the specific purpose of demonstrating the trajectory learning capabilities, a periodic signal with varying characteristics is used. The developed architecture, however, allows for more general learning tasks typical in applications of identification and control. The periodicity of the input signal ensures convergence of the output to a limit cycle. Online versions of the synaptic update can be formulated using simple CMOS circuits. Because the architecture depends on the network generating a stable limit cycle, and consequently a periodic solution which is robust over an interval of parameter uncertainties, we currently place the restriction of a periodic format for the input signals. The simulated network contains interconnected recurrent neurons with continuous-time dynamics. The system emulates random-direction descent of the error as a multidimensional extension to the stochastic approximation. To achieve unsupervised learning in recurrent dynamical systems we propose a synapse circuit which has a very simple structure and is suitable for implementation in VLSI.",https://ieeexplore.ieee.org/document/1560420/,"INDIN '05. 2005 3rd IEEE International Conference on Industrial Informatics, 2005.",10-12 Aug. 2005,ieeexplore
10.1109/FUZZ45933.2021.9494483,An approach to bridge the gap between ubiquitous embedded devices and JFML: A new module for Internet of Things,IEEE,Conferences,"Internet of Things enables sensors and actuators to share heterogeneous data between different devices. Such data can be used to create intelligent systems to control diverse structures available in houses, cities, or industrial environments among others. In this context, one of the most used approaches to handle these intelligent systems is based on Fuzzy Rule-Based Systems (FRBS) due to their suitability for addressing complex data and managing their imprecision. However, most of the current developments in this area are usually ad-hoc solutions limited by the intercommunication between FRBS and IoT devices. This results into significant challenges in reusing these solutions to solve latent problems. To bridge this gap, a new module for the open source library JFML is proposed to offer a complete implementation of an IoT infrastructure to develop intelligent IoT solutions based on the IEEE std 1855-2016. Moreover, a case study with real IoT devices is presented to showcase the use of the proposed module.",https://ieeexplore.ieee.org/document/9494483/,2021 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE),11-14 July 2021,ieeexplore
10.1109/MMICA.1999.833612,An artificial neural network on a field programmable gate array as a virtual sensor,IEEE,Conferences,"This paper presents a brief description of our achievements and current research on ""an artificial neural network on a field programmable gate array as a virtual sensor"". One of the characteristics of many industrial processes is the complex interrelations among the variables of the process. This has lead to the development of ""software sensors"" (soft-sensors or virtual sensor) where, by means of a computer program, variables are estimated from the information gathered by other measurements. Our proposal is based on a modular design of a neural network on using digital nonlinear activation functions. The design methodology, the size and complexity of the synthesised hardware and the results of the tests in a real world problem are presented together with our conclusions on the viability for its wider use on instrumentation applications.",https://ieeexplore.ieee.org/document/833612/,Proceedings of the Third International Workshop on Design of Mixed-Mode Integrated Circuits and Applications (Cat. No.99EX303),28-28 July 1999,ieeexplore
10.1109/AIIA.1988.13342,An expert PID controller,IEEE,Conferences,"The authors report on the development of an expert PID (proportional-integral-differential) controller for industrial processes. It uses the optimal property of a certain region in the P, I, and D parameter space providing minimum integrated (average) error when the required damping is ensured otherwise. The expert system uses an algorithm which basically modifies the settings of a conventional real-time PID controller continually until the parameters enter the optimum region mentioned above. The optimum region for a given process depends on the characterization of that process which is carried out in a simple indentification procedure prior to convergence. The authors describe the theory, the algorithm used, the hardware, and the software aspects of the implementation of the expert system as well as the test results obtained on a simulated process control system.&lt;<ETX>&gt;</ETX>",https://ieeexplore.ieee.org/document/13342/,Proceedings of the International Workshop on Artificial Intelligence for Industrial Applications,25-27 May 1988,ieeexplore
10.1109/ICNN.1994.374213,An implementation and evaluation of the ART1 neural network for pattern recognition,IEEE,Conferences,"A key to solving the stability-plasticity dilemma is to add a feedback mechanism between the competitive and the input layer of a network. This feedback mechanism facilitates the learning of new information without destroying old information, automatic switching between stable and plastic modes, and stabilization of the encoding of the classes done by the nodes resulting from this approach we have a neural network architecture that is particularly suited for pattern-classification problems in real world environments. For industrial use, ART1 neural networks have the potential of becoming an important component in a variety of commercial and military systems. Efficient software emulations of these networks are adequate in many of today's low-end applications such as information retrieval or group technology; but for larger applications, special purpose hardware is required to achieve the expected performance requirements.&lt;<ETX>&gt;</ETX>",https://ieeexplore.ieee.org/document/374213/,Proceedings of 1994 IEEE International Conference on Neural Networks (ICNN'94),28 June-2 July 1994,ieeexplore
10.1109/SIPNN.1994.344942,An improved JPEG image coder using the adaptive fast approximate Karhunen-Loeve transform (AKLT),IEEE,Conferences,"A new fast approximate Karhunen-Loeve transform (AKLT) [Lan and Reed, 1993; Reed and Lan, 1993] was discovered. This novel transform is derived by use of the perturbation theory of linear operators. The theoretical advantages of the AKLT over the DCT, the current industrial standard for image compression, are discussed thoroughly in Reed and Lan, and Lan and Reed for the first-order Markov image model. In the present paper, an improved JPEG image coder which uses the adaptive AKLT is presented. An improvement of 5% in compression ratio as compared with the DCT-JPEG system is obtained when the desired nominal data rate is above 0.4 bits/pixel for typical real-life images. The rate-distortion curves indicate a superiority of the adaptive AKLT scheme over the DCT scheme for all ranges of the data rate. It is worthwhile to note that this new adaptive AKLT-JPEG system can be really implemented using the existing JPEG chip set with only a slight modification.&lt;<ETX>&gt;</ETX>",https://ieeexplore.ieee.org/document/344942/,"Proceedings of ICSIPNN '94. International Conference on Speech, Image Processing and Neural Networks",13-16 April 1994,ieeexplore
10.1109/CDC.2012.6426325,An improved Predictive Optimal Controller with elastic search space for steam temperature control of large-scale supercritical power unit,IEEE,Conferences,"Predictive optimal control (POC) combined with artificial neural networks (ANNs) modeling and advanced heuristic optimization is a powerful technique for intelligent control. But actual implementation of the POC in complex industrial processes is limited by its known drawbacks, including the oscillation resulting from random search direction, difficulty in meeting the real-time requirement, and unresolved adaptability and generalization ability of the ANN predictive model. In resolving these problems, an improved Intelligent Predictive Optimal Controller (IPOC) with elastic search space is proposed in this paper. A new simpler and high-efficiency Particle Swarm Optimization (PSO) algorithm is adopted to find the optimal solution in fewer epochs to meet the real-time control requirements. The system output error in each control step is fed back to adjust the search space dynamically to prevent control oscillation and also make it easier to find the optimal solution. An improved recurrent neural network with external delayed inputs and outputs is constructed to model the dynamic response of the highly nonlinear system. The proposed IPOC is used to superheater steam temperature control of a 600MW supercritical power unit. Extensive control simulation tests are made to verify the validity of the new control scheme in a full-scope simulator.",https://ieeexplore.ieee.org/document/6426325/,2012 IEEE 51st IEEE Conference on Decision and Control (CDC),10-13 Dec. 2012,ieeexplore
10.1109/COGINF.2011.6016132,An intelligent fault recognizer for rotating machinery via remote characteristic vibration signal detection,IEEE,Conferences,"Monitoring industrial machine health in real-time is not only highly demanded but also significantly complicated and difficult. Possible reasons for this include: (a) Access to the machines on site is sometimes impracticable; and (b) The environment in which they operate is usually not human-friendly due to pollution, noise, hazardous wastes, etc. Despite the theoretically sound findings on developing intelligent solutions for machine condition based monitoring, there are few commercial tools in the market that can readily be used. This paper reports on the development of an intelligent fault recognition and monitoring system (Melvin I), which detects and diagnoses rotating machine conditions according to changes in fault frequency indicators. The signals and data are remotely collected from designated sections of machines via data acquisition cards. They are processed by a signal processor in order to extract characteristic vibration signals of ten key performance indicators (KPIs). A 3-layer neural network is designed to recognize and classify faults based on the set of KPIs. The system implemented in our laboratory and applied in the field can also incorporate new experiences into the knowledge base without overwriting previous training. Preliminary results have demonstrated that Melvin I is a smart tool for both system vibration analysts and industrial machine operators.",https://ieeexplore.ieee.org/document/6016132/,IEEE 10th International Conference on Cognitive Informatics and Cognitive Computing (ICCI-CC'11),18-20 Aug. 2011,ieeexplore
10.1109/ICIT.2017.7915520,An intelligent maintenance planning framework prototype for production systems,IEEE,Conferences,"The Intelligent Maintenance Planner (IMP) is designed to automate and improve maintenance processes in industrial applications. The system tracks the entire process cycle beginning with data acquisition and management, it then detects and classifies failure states, initializes maintenance cases, and selects and assigns the required resources. IMP guides maintenance work processes, by automatically providing instructions and augmented reality information. Subsequent feedback of the maintenance process and new or updated information is added to the system and used to train selection algorithms. A prototype of IMP was implemented based on an industrial SCADA system and cloud solutions for storage and machine learning capabilities. This report explains the stages of the maintenance process and provides an outline of the implementation and project results.",https://ieeexplore.ieee.org/document/7915520/,2017 IEEE International Conference on Industrial Technology (ICIT),22-25 March 2017,ieeexplore
10.1109/ICMA.2013.6618173,An intelligent object manipulation framework for industrial tasks,IEEE,Conferences,"This paper presents an intelligent object manipulation framework for industrial tasks, which integrates a sensor-rich multi-fingered robot hand, an industrial robot manipulator, a conveyor belt and employs machine learning algorithms. The framework software architecture is implemented using a Windows 7 operating system with RTX real-time extension for synchronous handling of peripheral devices. The framework uses Scale Invariant Feature Transform (SIFT) image processing algorithm, Support Vector Machine (SVM) machine learning algorithm and 3D point cloud techniques for intelligent object recognition based on RGB camera and laser rangefinder information from the robot hand end effector. The objective is automated manipulation of objects with different shapes and poses with minimum programming effort applied by a user.",https://ieeexplore.ieee.org/document/6618173/,2013 IEEE International Conference on Mechatronics and Automation,4-7 Aug. 2013,ieeexplore
10.1109/SYSCON.2018.8369547,An interactive architecture for industrial scale prediction: Industry 4.0 adaptation of machine learning,IEEE,Conferences,"According to wiki definition, there are four design principles in Industry 4.0. These principles support companies in identifying and implementing Industry 4.0 scenarios, namely, Interoperability, Information transparency, Technical assistance, Decentralized decisions. In this paper we have discussed our work on an implementation of a machine learning based interactive architecture for industrial scale prediction for dynamic distribution of water resources across the continent, keeping the four corners of Industry 4.0 in place. We report the possibility of producing most probable high resolution estimation regarding the water balance in any region within Australia by implementation of an intelligent system that can integrate spatial-temporal data from various independent sensors and models, with the ground truth data produced by 250 practitioners from the irrigation industry across Australia. This architectural implementation on a cloud computing platform linked with a freely distributed mobile application, allowing interactive ground truthing of a machine learning model on a continental scale, shows accuracy of 90% with 85% sensitivity of correct surface soil moisture estimation with end users at its complete control. Along with high level of information transparency and interoperability, providing on-demand technical supports and motivating users by allowing them to customize and control their own local predictive models, show the successfulness of principles in Industry 4.0 in real environmental issues in the future adaptation in various industries starting from resource management to modern generation soft robotics.",https://ieeexplore.ieee.org/document/8369547/,2018 Annual IEEE International Systems Conference (SysCon),23-26 April 2018,ieeexplore
10.1109/ISWCS.2018.8491060,Analysis of Machine Learning Algorithms for Spectrum Decision in Cognitive Radios,IEEE,Conferences,"Technological advances in recent years have reduced the manufacturing costs of wireless devices, increasing the number of such devices and applications. Most of these applications are supported by ISM (Industrial, Scientific, and Medical) frequencies, which due to their wide use in several types of devices have suffered from harmful interference. To solve this problem, Cognitive Radios paradigm has been proposed to guarantee the quality of communication. Several frameworks were proposed for the development of a Cognitive Radios Networks (CRN), but none of them were effectively implemented in hardware. This paper presents an analysis of machine learning algorithms in architecture for the development of CRN in real hardware. Results demonstrated the feasibility of the architecture and the decision methods based on machine learning algorithms can find the best communication channel.",https://ieeexplore.ieee.org/document/8491060/,2018 15th International Symposium on Wireless Communication Systems (ISWCS),28-31 Aug. 2018,ieeexplore
10.1109/ICAC.2017.21,Ananke: A Q-Learning-Based Portfolio Scheduler for Complex Industrial Workflows,IEEE,Conferences,"Complex workflows that process sensor data are useful for industrial infrastructure management and diagnosis. Although running such workflows in clouds promises reduced operational costs, there are still numerous scheduling challenges to overcome. Such complex workflows are dynamic, exhibit periodic patterns, and combine diverse task groupings and requirements. In this work, we propose ANANKE, a scheduling system addressing these challenges. Our approach extends the state-of-the-art in portfolio scheduling for data centers with a reinforcement-learning technique, and proposes various scheduling policies for managing complex workflows. Portfolio scheduling addresses the dynamic aspect of the workload. Q-learning, allows our approach to adapt to the periodic patterns of the workload, and to tune the other configuration parameters. The proposed policies are heuristics that guide the provisioning process, and map workflow tasks to the provisioned cloud resources. Through real-world experiments based on real and synthetic industrial workloads, we analyze and compare our prototype implementation of ANANKE with a system without portfolio scheduling (baseline) and with a system equipped with a standard portfolio scheduler. Overall, our experimental results give evidence that a learning-based portfolio scheduler can perform better and consume fewer resources than state-of-the-art alternatives, in particular for workloads with uniform arrival patterns.",https://ieeexplore.ieee.org/document/8005354/,2017 IEEE International Conference on Autonomic Computing (ICAC),17-21 July 2017,ieeexplore
10.1109/ICNC.2008.44,Anomaly Intrusion Detection Methods for Wireless LAN,IEEE,Conferences,"Nowday, wireless LANs are widely deployed in various places such as corporate office conference rooms, industrial warehouses, Internet-ready classrooms, etc. However, new concerns regarding security have been raised. Intrusion detection, as the second line of defense, is an indispensable tool for highly survivable networks. In this paper two anomaly intrusion detection methods are proposed for wireless LANs. One method uses hidden Markov model to check reflector DoS attacks, another based on adaptive resonance theory, which can learn the normal behavior with unsupervised method. The advantages of the methods are that they donpsilat need attack signatures and can detect intrusion in real-time. Experiments exhibit fairly good results, the attacks being collaboratively detected immediately.",https://ieeexplore.ieee.org/document/4667421/,2008 Fourth International Conference on Natural Computation,18-20 Oct. 2008,ieeexplore
10.1109/CASE48305.2020.9216855,Anomaly detection and prediction in discrete manufacturing based on cooperative LSTM networks,IEEE,Conferences,"Manufacturing processes are characterized by their temporal and spatial distributed nonlinear physics. Analytical models are not available and numerical models do not incorporate abnormal process effects that are not known to the engineer. These unknown anomalies cause reduced process stability and fluctuant product quality. To tackle the problem, numerous approaches for anomaly detection based on neural networks have been developed over the years. Long short-term memory (LSTM) networks have also been investigated intensively for prediction purposes. Current approaches lack in the capability of constructing prediction models for both process and anomaly behavior. Furthermore, they do not deliver a solution for short-term as well as long-term anomalies. Hence, the current paper presents a novel detection and prediction procedure based on a LSTM architecture to cooperatively predict process outputs and anomalies by using two separate but interacting models. The anomaly detector is realized as stacked LSTM auto-encoder and the cooperative prediction models are based on sequence-to-sequence networks with gated recurrent units for short-term and LSTM for long-term effects. The approach is evaluated within a real industrial environment by means of a production plant for hot forging at a German automotive supplier for metal components.",https://ieeexplore.ieee.org/document/9216855/,2020 IEEE 16th International Conference on Automation Science and Engineering (CASE),20-21 Aug. 2020,ieeexplore
10.1109/MIPRO.2015.7160443,Anomaly detection in thermal power plant using probabilistic neural network,IEEE,Conferences,"Anomalies are integral part of every system's behavior and sometimes cannot be avoided. Therefore it is very important to timely detect such anomalies in real-world running power plant system. Artificial neural networks are one of anomaly detection techniques. This paper gives a type of neural network (probabilistic) to solve the problem of anomaly detection in selected sections of thermal power plant. Selected sections are steam superheaters and steam drum. Inputs for neural networks are some of the most important process variables of these sections. It is noteworthy that all of the inputs are observable in the real system installed in thermal power plant, some of which represent normal behavior and some anomalies. In addition to the implementation of this network for anomaly detection, the effect of key parameter change on anomaly detection results is also shown. Results confirm that probabilistic neural network is excellent solution for anomaly detection problem, especially in real-time industrial applications.",https://ieeexplore.ieee.org/document/7160443/,"2015 38th International Convention on Information and Communication Technology, Electronics and Microelectronics (MIPRO)",25-29 May 2015,ieeexplore
10.23919/SOFTCOM.2019.8903672,Anomaly-based Intrusion Detection in Industrial Data with SVM and Random Forests,IEEE,Conferences,"Attacks on industrial enterprises are increasing in number as well as in effect. Since the introduction of industrial control systems in the 1970' s, industrial networks have been the target of malicious actors. More recently, the political and warfare-aspects of attacks on industrial and critical infrastructure are becoming more relevant. In contrast to classic home and office IT systems, industrial IT, so-called OT systems, have an effect on the physical world. Furthermore, industrial devices have long operation times, sometimes several decades. Updates and fixes are tedious and often not possible. The threats on industry with the legacy requirements of industrial environments creates the need for efficient intrusion detection that can be integrated into existing systems. In this work, the network data containing industrial operation is analysed with machine learning- and time series-based anomaly detection algorithms in order to discover the attacks introduced to the data. Two different data sets are used, one Modbus-based gas pipeline control traffic and one OPC UA-based batch processing traffic. In order to detect attacks, two machine learning-based algorithms are used, namely SVM and Random Forest. Both perform well, with Random Forest slightly outperforming SVM. Furthermore, extracting and selecting features as well as handling missing data is addressed in this work.",https://ieeexplore.ieee.org/document/8903672/,"2019 International Conference on Software, Telecommunications and Computer Networks (SoftCOM)",19-21 Sept. 2019,ieeexplore
10.23919/IConAC.2019.8895095,Ant Colony Optimization Algorithm for Industrial Robot Programming in a Digital Twin,IEEE,Conferences,"Advanced manufacturing that is adaptable to constantly changing product designs often requires dynamic changes on the factory floor to enable manufacture. The integration of robotic manufacture with machine learning approaches offers the possibility to enable such dynamic changes on the factory floor. While ensuring safety and the possibility of losses of components and waste of material are against their usage. Furthermore, developments in design of virtual environments makes it possible to perform simulations in a virtual environment, to enable human-in-the-loop production of parts correctly the first time like never before. Such powerful simulation and control software provides the means to design a digital twin of manufacturing environment in which trials are completed at almost at no cost. In this paper, ant colony optimization is used to program an industrial robot to avoid obstacles and find its way to pick and place objects during an assembly task in an environment containing obstacles that must be avoided. The optimization is completed in a digital twin environment first and movements transferred to the real robot after human inspection. It is shown that the proposed methodology can find the optimal solution, in addition to avoiding collisions, for an assembly task with minimum human intervention.",https://ieeexplore.ieee.org/document/8895095/,2019 25th International Conference on Automation and Computing (ICAC),5-7 Sept. 2019,ieeexplore
10.1109/ICPHM.2019.8819421,Application of Deep Learning for Fault Diagnostic in Induction Machine’s Bearings,IEEE,Conferences,"Recent developments in sensor technologies and advances in communication systems have resulted in deployment of a large number of sensors for collecting condition monitoring (CM) data in order to monitor health condition of a manufac-tring/industrial system. Efficient utilization of sensory data leads to highly accurate results in system fault diagnostics/prognostics. The exponential growth of CM data poses significant analytical challenges, due to their high variety, high dimensionality and high velocity rendering conventional health monitoring tools impractical. In this regard, the paper proposes a deep learning-based framework for fault diagnosis of an induction machine's bearing based on real data set provided by Case Western Reserve University bearing data center. In particular, we focus on deep bidirectional long short-term memory (BiD-LSTM) networks fed with raw signals for fault diagnosis to address drawbacks of conventional machine learning (ML) solutions such as support vector machines. A numerical example is provided to illustrate the complete procedure of the proposed framework, which shows the great potentials of the BiD-LSTM for detection of different types of the bearing fault with high accuracy. The effectiveness of the proposed model is demonstrated through a comparison with a recently developed deep neural network (DNN) that considers temporal coherence for the same data set. The results indicate that the proposed framework provides considerably improved performance in comparison to its counterparts.",https://ieeexplore.ieee.org/document/8819421/,2019 IEEE International Conference on Prognostics and Health Management (ICPHM),17-20 June 2019,ieeexplore
10.1109/ICEKIM52309.2021.00040,Application of Teaching Innovation Based on robotics engineering,IEEE,Conferences,"As the core major of “Internet + Industrial Intelligence”, robotics engineering is an upgrade and reconstruction of traditional engineering major. The industrial robot course is the professional core course of the Robotics Engineering. It is also a comprehensive course of multi-discipline integration, which involved mechanical engineering, automatic control, computer, sensor, electronic technology, artificial intelligence and other multi-disciplinary content. Robotics Engineering is characterized by broad foundation, great difficulty, emphasis on practice, rapid development and application of new knowledge. In the process of implementation of the teaching innovation, the new concept of engineering education was applied to propose a new form of curriculum system. Taking the projects of engineering as the study objects, disassemble the knowledge points involved in industrial robots, break the course boundaries, reshape the knowledge system, draw knowledge maps and then design teaching activities. In teaching innovation, teachers extend classroom through formation of subject competition teams, promote teaching and promote learning by competition, realize the integration of “teaching, class and competition”, build a bridge between theory and practice, then complete the transformation from knowledge learning to ability training. Besides, they also keep contact with intelligent manufacturing enterprises in Zhuhai and the Bay Area to obtain real-time new developments in enterprises. Thus, the latest information was introduced into classroom. Therefore, the meaning of “production, teaching, research and application” has been deepened. According to the characteristics of the knowledge points of the course, experts were invited to make special lectures for students which can bring them with international perspective and frontier knowledge.",https://ieeexplore.ieee.org/document/9479656/,"2021 2nd International Conference on Education, Knowledge and Information Management (ICEKIM)",29-31 Jan. 2021,ieeexplore
10.1109/CYBER53097.2021.9588269,Application of YOLO Object Detection Network In Weld Surface Defect Detection,IEEE,Conferences,"As industrial production becomes more modern and intelligent today, the inspection of product quality of the workshop is becoming more and more accustomed to replacing the old manual visual inspection methods with automated inspection systems. In the welding field, automated welding robots are not only used in traditional large-scale automobile assembly lines. In more general welding work, welding robots also plays an important role. The inspection of the welding quality of the welding robot is mainly to detect the four main types of weld defects. Compared to traditional defect classification based on support vector machines and defect detection based on template matching, this paper uses a welding surface defect detection system designed based on deep learning methods. By working with workshop welding experts, a large-scale image of nearly 5000 pictures is built. Large-scale weld defect datasets, while using the real-time and accuracy of the YOLO series of deep learning object detection frameworks, the weld defects detection model reaches 75.5% mean average precision(mAP) in constructed weld defect data set. In addition, the construction cost of the detection model and the deployment time of the detection system are greatly reduced. During the field test of the system in the workshop, among a batch of welding workpieces provided by the factory, the detection accuracy of weld defects reached 71%, which initially met the requirements of the workshop for an automated defect detection system.",https://ieeexplore.ieee.org/document/9588269/,"2021 IEEE 11th Annual International Conference on CYBER Technology in Automation, Control, and Intelligent Systems (CYBER)",27-31 July 2021,ieeexplore
10.1109/ISIC.2002.1157861,Application of a model-based fault detection and diagnosis using parameter estimation and fuzzy inference to a DC-servomotor,IEEE,Conferences,"Fault detection and diagnosis are very much needed in many industrial applications. One of the most popular scheme is the model-based fault diagnostic. Recently, artificial intelligence techniques have been found to be suitable for fault detection and diagnosis and a variety of techniques have been proposed in this area. However, only few applications and real time implementation of the schemes have been reported. In this paper, we use a fault detection and diagnostic scheme based on the model-based approach using parameter estimation and fuzzy inference and experimented it on a DC motor servo trainer. The model of the plant is obtained using the recursive least squares parameter estimation technique, and fuzzy inference is used for the interpretation of the fault. Several faults have been identified on the system. The faults are then simulated on the motor and experiments are carried out to diagnose the types of faults. Experiments show the effectiveness of the proposed technique for real time applications.",https://ieeexplore.ieee.org/document/1157861/,Proceedings of the IEEE Internatinal Symposium on Intelligent Control,30-30 Oct. 2002,ieeexplore
10.1109/ICOS.2011.6079285,Application of multi-step time series prediction for industrial equipment prognostic,IEEE,Conferences,The use of prognostics is critically to be implemented in industrial. This paper presents an application of multi-step time series prediction to support industrial equipment prognostic. An artificial neural network technique with sliding window is considered for the multi-step prediction which is able to predict the series of future equipment condition. The structure of prognostic application is presented. The feasibility of this prediction application was demonstrated by applying real condition monitoring data of industrial equipment.,https://ieeexplore.ieee.org/document/6079285/,2011 IEEE Conference on Open Systems,25-28 Sept. 2011,ieeexplore
10.1109/ICONIP.2002.1201906,Application of neural network for predicting software development faults using object-oriented design metrics,IEEE,Conferences,"In this paper, we present the application of neural network for predicting software development faults including object-oriented faults. Object-oriented metrics can be used in quality estimation. In practice, quality estimation means either estimating reliability or maintainability. In the context of object-oriented metrics work, reliability is typically measured as the number of defects. Object-oriented design metrics are used as the independent variables and the number of faults is used as dependent variable in our study. Software metrics used include those concerning inheritance measures, complexity measures, coupling measures and object memory allocation measures. We also test the goodness of fit of neural network model by comparing the prediction result for software faults with multiple regression model. Our study is conducted on three industrial real-time systems that contain a number of natural faults that has been reported for three years (Mei-Huei Tang et al., 1999).",https://ieeexplore.ieee.org/document/1201906/,"Proceedings of the 9th International Conference on Neural Information Processing, 2002. ICONIP '02.",18-22 Nov. 2002,ieeexplore
10.1109/NetSoft48620.2020.9165317,Applying Machine Learning to End-to-end Slice SLA Decomposition,IEEE,Conferences,"5G is set to revolutionize the network service industry with unprecedented use-cases in industrial automation, augmented reality, virtual reality and many other domains. Network slicing is a key enabler to realize this concept, and comes with various SLA requirements in terms of latency, throughput, and reliability. Network slicing is typically performed in an end-to-end (e2e) manner across multiple domains, for example, in mobile networks, a slice can span access, transport and core networks. Thus, if an SLA requirement is specified for e2e services, we need to ensure that the total SLA budget is appropriately proportioned to each participating domain in an adaptive manner. Such an SLA decomposition can be extremely useful for network service operators as they can plan accordingly for actual deployment. In this paper we design and implement an SLA decomposition planner for network slicing using supervised machine learning algorithms. Traditional optimization based approaches cannot deal with the dynamic nature of such services. We design machine learning models for SLA decomposition, based on random forest, gradient boosting and neural network. We then evaluate each class of algorithms in terms of accuracy, sample complexity, and model explainability. Our experiments reveal that, in terms of these three requirements, the gradient boosting and neural network algorithms for SLA decomposition out-perform random forest algorithms, given emulated data sets.",https://ieeexplore.ieee.org/document/9165317/,2020 6th IEEE Conference on Network Softwarization (NetSoft),29 June-3 July 2020,ieeexplore
10.1109/USBEREIT51232.2021.9455060,Applying of Recurrent Neural Networks for Industrial Processes Anomaly Detection,IEEE,Conferences,"The paper considers the issue of recurrent neural networks applicability for detecting industrial process anomalies to detect intrusion in Industrial Control Systems. Cyberattack on Industrial Control Systems often leads to appearing of anomalies in industrial process. Thus, it is proposed to detect such anomalies by forecasting the state of an industrial process using a recurrent neural network and comparing the predicted state with actual process' state. In the course of experimental research, a recurrent neural network with one-dimensional convolutional layer was implemented. The Secure Water Treatment dataset was used to train model and assess its quality. The obtained results indicate the possibility of using the proposed method in practice. The proposed method is characterized by the absence of the need to use anomaly data for training. Also, the method has significant interpretability and allows to localize an anomaly by pointing to a sensor or actuator whose signal does not match the model's prediction.",https://ieeexplore.ieee.org/document/9455060/,"2021 Ural Symposium on Biomedical Engineering, Radioelectronics and Information Technology (USBEREIT)",13-14 May 2021,ieeexplore
10.1109/DSMP.2016.7583579,Artificial intelligence methods for data based modeling and analysis of complex processes: Real life examples,IEEE,Conferences,In this paper two computational intelligence methods are considered. In the first one the Neural Network based Controller with combination of Genetic Algorithm network structure optimization is presented. In the second example fuzzy logic control system is developed and implemented on industrial heating plant. Obtained knowledge considered as a part of the modular ICS (Intelligent Control System) software.,https://ieeexplore.ieee.org/document/7583579/,2016 IEEE First International Conference on Data Stream Mining & Processing (DSMP),23-27 Aug. 2016,ieeexplore
10.1109/ICHQP.1998.760178,Artificial neural networks for power systems harmonic estimation,IEEE,Conferences,"An artificial neural network model was developed and implemented for power system harmonics estimation. The model was given the name FNN, which stands for Fourier neural network. It was tested offline under different conditions and was compared with FFT. The results of the offline tests indicate that the FNN has very high estimation accuracy. It has a recursive nature that makes it a candidate for real-time measurements. It also gave good results in a noisy environment where SNR is as low as 10 dB. The FNN model was implemented on a PC using a data acquisition board. The system was used for an online harmonic estimation study. The FNN was able to estimate the harmonic components of voltage and current at various levels. The estimation results were compared with the data obtained using a FLUKE 41 harmonics meter. The comparison revealed that the ANN based harmonic estimation model performs similarly to industrial-approved meters.",https://ieeexplore.ieee.org/document/760178/,8th International Conference on Harmonics and Quality of Power. Proceedings (Cat. No.98EX227),14-16 Oct. 1998,ieeexplore
10.1109/UPEC.2017.8231939,Assessing the impact of load forecasting accuracy on battery dispatching strategies with respect to Peak Shaving and Time-of-Use (TOU) applications for industrial consumers,IEEE,Conferences,"Energy Storage Systems will play crucial role in controlling the grid of the future when increased penetration of renewable energy sources will take place. Especially batteries are expected to occupy a considerable share of the total energy storage market by simultaneously providing services to different stakeholders such as energy producers, transmission/distribution operators, residential, commercial and industrial consumers. Nowadays, Peak shaving and Time-of-Use applications are the most common services that standalone battery storage systems can provide to industrial consumers (without integrated PV-systems and/or wind turbines). A big part of the existing literature addressing such applications aims at developing an offline algorithm for optimal battery deployment based on a known load profile (or accurately predicted) without taking into consideration real time conditions. This paper investigates the impact of industrial load forecasting errors on dispatching strategies of battery storage systems on economically driven peak shaving and Time-of-Use applications. An artificial neural network has been developed and used as a prediction model of an industrial load profile. The neural network was trained, validated and tested on historical load data with time resolution of 15 minutes, provided by the local distribution operator of the Belgian electric grid. The performance of the neural network in terms of output-target regression and mean absolute error is 0.833 and 10.02% respectively. Afterwards, a simulation was carried out comparing four different scenarios of peak shaving. The results show that the prediction accuracy of the presented neural network is not competitive enough. Peak shaving based on predicted profiles becomes reliable for lower forecasting errors. For this purpose, further access into the process and types of loads of the user is required in order to come up with a more sophisticated prediction model.",https://ieeexplore.ieee.org/document/8231939/,2017 52nd International Universities Power Engineering Conference (UPEC),28-31 Aug. 2017,ieeexplore
10.1109/SMC.2013.819,Automated Sound Signalling Device Quality Assurance Tool for Embedded Industrial Control Applications,IEEE,Conferences,This paper presents a novel system for automatic detection and recognition of faulty audio signaling devices as part of an automated industrial manufacturing process. The system uses historical data labeled by human experts in detecting faulty signaling devices to train an artificial neural network based classifier for modeling their decision making process. The neural network is implemented on a real time embedded micro controller which can be more efficiently incorporated into an automated production line eliminating the need for a manual inspection within the manufacturing process. We present real world experiments based on data pertaining to the production and manufacture of audio signaling components used in car instrument clusters. Our results show that the proposed expert system is able to successfully classify faulty audio signaling devices to a high degree of accuracy. The results can be generalized to other signaling devices where an output signal is represented by a complex and changing frequency spectrum even with significant environmental noise.,https://ieeexplore.ieee.org/document/6722574/,"2013 IEEE International Conference on Systems, Man, and Cybernetics",13-16 Oct. 2013,ieeexplore
10.1109/ICASSP39728.2021.9415049,Automatic Fine-Grained Localization of Utility Pole Landmarks on Distributed Acoustic Sensing Traces Based on Bilinear Resnets,IEEE,Conferences,"In distributed acoustic sensing (DAS) on aerial fiber-optic cables, utility pole localization is a prerequisite for any subsequent event detection. Currently, localizing the utility poles on DAS traces relies on human experts who manually label the poles’ locations by examining DAS signal patterns generated in response to hammer knocks on the poles. This process is inefficient, error-prone and expensive, thus impractical and non-scalable for industrial applications. In this paper, we propose two machine learning approaches to automate this procedure for large-scale implementation. In particular, we investigate both unsupervised and supervised methods for fine-grained pole localization. Our methods are tested on two real-world datasets from field trials, and demonstrate successful estimation of pole locations at the same level of accuracy as human experts and strong robustness to label noises.",https://ieeexplore.ieee.org/document/9415049/,"ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",6-11 June 2021,ieeexplore
10.1109/IJCNN.1991.155523,Automatic classification of solder joint images,IEEE,Conferences,"Summary form only given, as follows. Elaborate techniques have been developed to obtain data from specimens in industrial quality control tasks. However, a problem in the field of visual inspection is how to process complex data in real time. An approach to classification of solder joint images by means of a neuronlike binary associative memory has been developed. All the algorithms and architectures considered could easily be implemented with digital VLSI technology to realize an extremely fast classifier.&lt;<ETX>&gt;</ETX>",https://ieeexplore.ieee.org/document/155523/,IJCNN-91-Seattle International Joint Conference on Neural Networks,8-12 July 1991,ieeexplore
10.1109/INDIN.2012.6301355,Automation of chlorination process for drinking water treatment plant: Control strategies,IEEE,Conferences,"The technological advances of industrial communications in the last decade allowed automatically to control processes that were manually controlled in a separate way. This paper presents the design and implementation for a novel automated chlorination system at the drinking water treatment plant in Sant Joan Despí (Barcelona, Spain). The automation of chlorination process meant two important challenges: firstly, the impact of dead time inherent to the measurement of process' variable of the water and its dosage has been minimized; and second, the system has been provided with the capacity of adaptation to the variability in the water quality and to the appearance of the ammonium in the treated water. The obtained results in this real process by the combination of a feedforward control strategy together with a classical feedback scheme have been largely satisfactory. The establishment of this new control system has represented a significant change in the working model.",https://ieeexplore.ieee.org/document/6301355/,IEEE 10th International Conference on Industrial Informatics,25-27 July 2012,ieeexplore
10.1109/ICCEAI52939.2021.00056,Automobile airbag defect detection algorithm based on improved Faster RCNN,IEEE,Conferences,"The traditional image processing method has a low detection rate for various kinds of automobile airbag surface defects in the production process, which is difficult to meet the actual demand of industrial production. In order to improve the detection rate of automobile airbag surface defects and meet the real-time requirements of industrial detection, this paper proposes an improved Faster RCNN deep learning algorithm. Firstly, the method adopts the E-FPN to enhance the feature extraction ability of the network for multi-scale targets. Then, ROI Align algorithm is introduced instead of ROI Pooling algorithm to improve the detection ability of small targets. Finally, the designed Light Head is used to improve the running speed of the network. The experimental results show that the average precision of the improved Faster RCNN algorithm for automobile airbag defect detection reaches 97.2%, and the detection time is 23.73 milliseconds, which is obviously superior to the original algorithm and has higher detection accuracy and practicability.",https://ieeexplore.ieee.org/document/9544111/,2021 International Conference on Computer Engineering and Artificial Intelligence (ICCEAI),27-29 Aug. 2021,ieeexplore
10.1109/CEWIT.2014.7021143,Autonomous and flexible multiagent systems enhance transport logistics,IEEE,Conferences,"This paper presents an autonomous multiagent system which optimizes the planning and scheduling of industrial processes using the example of courier and express services. In order to handle the rising demands and to capitalize on the increasing optimization potential in transport logistics, which both result from the consequent integration of industrial processes into the Internet of Things and Services, the presented dispAgent solution ensures a flexible, adaptive, and proactive system behavior. Intelligent, selfishly acting agents represent logistic entities, which communicate and negotiate with each other to optimize the allocation of orders to transport facilities. The system has been developed in cooperation with our industrial partner tiramizoo, which is an expert in courier and express services. In order to determine the quality of the computed solutions, we evaluated the system using an established benchmark set and compared the results to best-known solutions. In addition, we further validated the system's performance by multiple simulations of real-world scenarios relying on data which was provided by our industrial partner. The results show that the system achieves high quality solutions for the benchmark set and outperforms a standard dispatching software product in real-world scenarios.",https://ieeexplore.ieee.org/document/7021143/,2014 11th International Conference & Expo on Emerging Technologies for a Smarter World (CEWIT),29-30 Oct. 2014,ieeexplore
10.1109/ICDL-EpiRob48136.2020.9278071,Bayesian Optimization for Developmental Robotics with Meta-Learning by Parameters Bounds Reduction,IEEE,Conferences,"In robotics, methods and softwares usually require optimizations of hyperparameters in order to be efficient for specific tasks, for instance industrial bin-picking from homogeneous heaps of different objects. We present a developmental framework based on long-term memory and reasoning modules (Bayesian Optimisation, visual similarity and parameters bounds reduction) allowing a robot to use meta-learning mechanism increasing the efficiency of such continuous and constrained parameters optimizations. The new optimization, viewed as a learning for the robot, can take advantage of past experiences (stored in the episodic and procedural memories) to shrink the search space by using reduced parameters bounds computed from the best optimizations realized by the robot with similar tasks of the new one (e.g. bin-picking from an homogenous heap of a similar object, based on visual similarity of objects stored in the semantic memory). As example, we have confronted the system to the constrained optimizations of 9 continuous hyperparameters for a professional software (Kamido) in industrial robotic arm bin-picking tasks, a step that is needed each time to handle correctly new object. We used a simulator to create bin-picking tasks for 8 different objects (7 in simulation and one with real setup, without and with meta-learning with experiences coming from other similar objects) achieving goods results despite a very small optimization budget, with a better performance reached when meta-learning is used (84.3 % vs 78.9 % of success overall, with a small budget of 30 iterations for each optimization) for every object tested (p-value=0.036).",https://ieeexplore.ieee.org/document/9278071/,2020 Joint IEEE 10th International Conference on Development and Learning and Epigenetic Robotics (ICDL-EpiRob),26-30 Oct. 2020,ieeexplore
10.1109/ITNEC48623.2020.9084876,Bearing Fault Diagnosis Based on Multi-Scale Convolution Neural Network and Dropout,IEEE,Conferences,"With the development of hardware, deep learning, which based on high computation complexity, has become a popular tool for intelligent fault diagnosis. Compared with traditional methods, fault diagnosis method based on deep learning could approximate complex non-linear relationship and achieve very high precision under the same load. However, the performance of these deep learning-based fault diagnosis methods will be greatly reduced when the load changes, which are significant in fault diagnosis issues since the working load is changing all the time in real world industrial applications. Thus, a fault diagnosis method based on multi-scale convolutional neural network (CNN) is proposed to solve this problem. CNN is a type of neural network which can extract feature from the fault signal directly. In order to obtain sufficient fault information, the proposed method uses multi-scale convolution kernels to extract features of different scales. In addition, dropout is also added to the network to enhance the robustness and generalization capabilities of the model. The experimental results show that the proposed model not only performs well under the same load, but also achieves high fault diagnosis accuracy when the load changes.",https://ieeexplore.ieee.org/document/9084876/,"2020 IEEE 4th Information Technology, Networking, Electronic and Automation Control Conference (ITNEC)",12-14 June 2020,ieeexplore
10.1109/ICATCCT.2016.7912031,Bearing fault diagnosis using discrete Wavelet Transform and Artificial Neural Network,IEEE,Conferences,"Rotating machinery has vast industrial applications in fields of petroleum, automotive, HVAC and food processing. Rotating machineries use bearings to perform rotational or linear movement of various subcomponents while reducing friction and stress. Compared other types of bearing, REBs offer a good balance of key attributes like friction, lifetime, stiffness, speed and cost. Hence, real-time monitoring and diagnosis of bearings is crucial to prevent failures, improve safety, avoid unforeseen downtime of production assembly lines and lower cost. We propose an approach based on Wavelet Transform and ANN for analysis of vibration signals from a rolling element bearing to identify and multi-classify its component defects. The vibration signals from the REB being analyzed are passed over to the software setup consisting of Wavelet Transform and ANN. To remove noise and extract the relevant features from this signal, we pass the vibration signal through a Wavelet transform. These features are retrieved using time domain parameters like Skewness, Kurtosis, RMS and Crest Factor and they are used as an input for ANN classifier. The role of the ANN is to classify the bearing fault features produced by the Wavelet Transform and identify bearing faults, if any. To this end, we have designed a feedforward topology ANN using the sigmoid transfer function. The ANN training methodology uses three learning paradigms - namely, Levenberg-Marquardt, Resilient Back-propogation and Scaled Conjugate method. The learning models generated by each algorithm are tested to find the one which gives better accuracy. The outcome of this experiment indicates that DWT and ANN can together achieve good accuracy and reliability in detection and classification of bearing faults.",https://ieeexplore.ieee.org/document/7912031/,2016 2nd International Conference on Applied and Theoretical Computing and Communication Technology (iCATccT),21-23 July 2016,ieeexplore
10.1109/NetSoft48620.2020.9165393,Benchmarking and Profiling 5G Verticals' Applications: An Industrial IoT Use Case,IEEE,Conferences,"The Industry 4.0 sector is evolving in a tremendous pace by introducing a set of industrial automation mechanisms tightly coupled with the exploitation of Internet of Things (IoT), 5G and Artificial Intelligence (AI) technologies. By combining such emerging technologies, interconnected sensors, instruments, and other industrial devices are networked together with industrial applications, formulating the Industrial IoT (IIoT) and aiming to improve the efficiency and reliability of the deployed applications and provide Quality of Service (QoS) guarantees. However, in a 5G era, efficient, reliable and highly performant applications' provision has to be combined with exploitation of capabilities offered by 5G networks. Optimal usage of the available resources has to be realised, while guaranteeing strict QoS requirements such as high data rates, ultra-low latency and jitter. The first step towards this direction is based on the accurate profiling of vertical industries' applications in terms of resources usage, capacity limits and reliability characteristics. To achieve so, in this paper we provide an integrated methodology and approach for benchmarking and profiling 5G vertical industries' applications. This approach covers the realisation of benchmarking experiments and the extraction of insights based on the analysis of the collected data. Such insights are considered the cornerstones for the development of AI models that can lead to optimal infrastructure usage along with assurance of high QoS provision. The detailed approach is applied in a real IIoT use case, leading to profiling of a set of 5G network functions.",https://ieeexplore.ieee.org/document/9165393/,2020 6th IEEE Conference on Network Softwarization (NetSoft),29 June-3 July 2020,ieeexplore
10.1109/ICAIIC51459.2021.9415189,Big Data Platform for Intelligence Industrial IoT Sensor Monitoring System Based on Edge Computing and AI,IEEE,Conferences,"The cutting edge of Industry 4.0 has driven everything to be converted to disruptive innovation and digitalized. This digital revolution is imprinted by modern and advanced technology that takes advantage of Big Data and Artificial Intelligence (AI) to nurture from automatic learning systems, smart city, smart energy, smart factory to the edge computing technology, and so on. To harness an appealing, noteworthy, and leading development in smart manufacturing industry, the modern industrial sciences and technologies such as Big Data, Artificial Intelligence, Internet of things, and Edge Computing have to be integrated cooperatively. Accordingly, a suggestion on the integration is presented in this paper. This proposed paper describes the design and implementation of big data platform for intelligence industrial internet of things sensor monitoring system and conveys a prediction of any upcoming errors beforehand. The architecture design is based on edge computing and artificial intelligence. To extend more precisely, industrial internet of things sensor here is about the condition monitoring sensor data - vibration, temperature, related humidity, and barometric pressure inside facility manufacturing factory.",https://ieeexplore.ieee.org/document/9415189/,2021 International Conference on Artificial Intelligence in Information and Communication (ICAIIC),13-16 April 2021,ieeexplore
10.1109/RTAS52030.2021.00048,Brief Industry Paper: optimizing Memory Efficiency of Graph Neural Networks on Edge Computing Platforms,IEEE,Conferences,"Graph neural networks (GNN) have achieved state-of-the-art performance on various industrial tasks. However, the poor efficiency of GNN inference and frequent Out-of-Memory (OOM) problem limit the successful application of GNN on edge computing platforms. To tackle these problems, a feature decomposition approach is proposed for memory efficiency optimization of GNN inference. The proposed approach could achieve outstanding optimization on various GNN models, covering a wide range of datasets, which speeds up the inference by up to 3×. Furthermore, the proposed feature decomposition could significantly reduce the peak memory usage (up to 5× in memory efficiency improvement) and mitigate OOM problems during GNN inference.",https://ieeexplore.ieee.org/document/9470441/,2021 IEEE 27th Real-Time and Embedded Technology and Applications Symposium (RTAS),18-21 May 2021,ieeexplore
10.1109/ICII.2018.00024,Brightics-IoT: Towards Effective Industrial IoT Platforms for Connected Smart Factories,IEEE,Conferences,"Industrial Internet-of-Things (IIoT) supports machines, computers and users to enable intelligent operations using advanced device management and data analytics. In recent years, thanks to standardized IoT platforms and advanced Artificial Intelligence (AI) technologies, there have been great advances in IIoT, and now it promises revolutions on various manufacturing domains such as transport, health, factory and energy. In this paper, based on our experience operating IIoT in various factory applications, we present the technical challenges of manufacturing facilities needed to be dealt with to collect huge amount of data in real-time and counteraction points of an IoT platform regarding these technical challenges and what kinds of features need to be implemented for intelligent services in the smart manufacturing. Finally, we introduce a story of applying industrial IoT platform in production to show how iterative development approaches can achieve business requirements based on elastically scaled-out architecture.",https://ieeexplore.ieee.org/document/8539113/,2018 IEEE International Conference on Industrial Internet (ICII),21-23 Oct. 2018,ieeexplore
10.1109/COASE.2018.8560430,Bringing Automated Intelligence to Cyber-Physical Production Systems in Factory Automation,IEEE,Conferences,"Learning and intelligent algorithms are the basis for Cyber-Physical Production Systems (CPPS). They enable flexibility through reconfiguration ease and fault tolerance by ensuring that the CPPS adapts to changing conditions. However, in order to exhibit such characteristics, CPPS require a proper support for reliably handling the real time behavior of the physical systems they are in control of. This paper presents and discusses basic requirements for control software, which enables flexible and adaptable automated Production Systems (aPS) modelled according to the CPPS concept. In doing so, the paper exposes the main architectural guidelines and rationale for where to place and operate intelligent algorithms in the context of industrial automation for continuous processes.",https://ieeexplore.ieee.org/document/8560430/,2018 IEEE 14th International Conference on Automation Science and Engineering (CASE),20-24 Aug. 2018,ieeexplore
10.1109/BigData.2016.7840859,Building a research data science platform from industrial machines,IEEE,Conferences,"Data Science research has a long history in academia which spans from large-scale data management, to data mining and data analysis using technologies from database management systems (DBMS's). While traditional HPC offers tools on leveraging existing technologies with data processing needs, the large volume of data and the speed of data generation pose significant challenges. Using the Hadoop platform and tools built on top of it drew immense interest from academia after it gained success in industry. Georgia Institute of Technology received a donation of 200 compute nodes from Yahoo. Turning these industrial machines into a research Data Science Platform (DSP) poses unique challenges, such as: nontrivial hardware design decisions, configuration tool choices, node integration into existing HPC infrastructure, partitioning resource to meet different application needs, software stack choices, etc. We have 40 nodes up and running, 24 running as a Hadoop and Spark cluster, 12 running as a HBase and OpenTSDB cluster, the others running as service nodes. We successfully tested it against Spark Machine Learning algorithms using a 88GB image dataset, Spark DataFrame and GraphFrame with a Wikipedia dataset, and Hadoop MapReduce wordcount on a 300GB dataset. The OpenTSDB cluster is for real-time time series data ingestion and storage for sensor data. We are working on bringing up more nodes. We share our first-hand experience gained in our journey, which we believe will benefit and inspire other academic institutions.",https://ieeexplore.ieee.org/document/7840859/,2016 IEEE International Conference on Big Data (Big Data),5-8 Dec. 2016,ieeexplore
10.1109/ICCE-Berlin.2018.8576251,CNN Inference: Dynamic and Predictive Quantization,IEEE,Conferences,"Deep Learning techniques like Convolutional Neural Networks (CNN) are the de-facto method for image classification with broad usage spanning across automotive, industrial, medicine, robotics etc. Efficient implementation of CNN inference on embedded device requires a quantization method, which minimizes the accuracy loss, ability to generalize across deployment scenarios as well as real-time processing. Existing literature doesn't address all these three requirements simultaneously. In this paper, we propose a novel quantization algorithm to overcome above mentioned challenges. The proposed solution dynamically selects the scale for quantizing activations and uses Kalman filter to predict quantization scale to reduce accuracy loss. The proposed solution exploits the range statistics from previous inference processes to estimate quantization scale, enabling real-time solution. The proposed solution is implemented on TI's TDA family of embedded automotive processors. The proposed solution is running real time semantic segmentation on TDA2x processor within 0.1% accuracy loss compared floating point algorithm. The solution performs well across multiple deployment scenarios (e.g. rain, snow, night etc) demonstrating generalization capability of the solution.",https://ieeexplore.ieee.org/document/8576251/,2018 IEEE 8th International Conference on Consumer Electronics - Berlin (ICCE-Berlin),2-5 Sept. 2018,ieeexplore
10.1109/MetroInd4.0IoT51437.2021.9488536,CNN-LSTM Neural Network Applied for Thermal Infrared Underground Water Leakage,IEEE,Conferences,"In this paper is proposed a methodological approach for the detection of leaks in water pipelines. The approach is based on the use of Infrared Thermography (IRT) for the real time monitoring, and of Artificial Neural Networks (ANNs) for the identification of potential leaks not easy visible by IRT. The input data source consists of radiometric data processed by Convolutional Neural Networks (CNNs) providing as output information about the presence or absence of the water leakages. A preliminary study has been carried out about a fixed monitoring station created to identify leaks in underground pipelines. In addition, has been implemented a platform to remotely acquire the thermograms and to analyze them by CNN networks detecting leakages, combined with Long Short-Term Memory (LSTM) neural networks, and image filtering algorithms, such as image segmentation and active contour snake approach. The LSTM network allows the prediction and calculation of the propagation trend of the leak plume. Finally, image filtering improves the visualization of leaks as it allows to draw the contours of the pixel clusters representing the leakages areas in the thermograms. The work was developed within the research framework of an industrial project. The proposed approach is suitable also for oil spill and gas leakages detections.",https://ieeexplore.ieee.org/document/9488536/,2021 IEEE International Workshop on Metrology for Industry 4.0 & IoT (MetroInd4.0&IoT),7-9 June 2021,ieeexplore
10.1109/BigDataService.2015.61,Carotene: A Job Title Classification System for the Online Recruitment Domain,IEEE,Conferences,"In the online job recruitment domain, accurate classification of jobs and resumes to occupation categories is important for matching job seekers with relevant jobs. An example of such a job title classification system is an automatic text document classification system that utilizes machine learning. Machine learning-based document classification techniques for images, text and related entities have been well researched in academia and have also been successfully applied in many industrial settings. In this paper we present Carotene, a machine learning-based semi-supervised job title classification system that is currently in production at CareerBuilder. Carotene leverages a varied collection of classification and clustering tools and techniques to tackle the challenges of designing a scalable classification system for a large taxonomy of job categories. It encompasses these techniques in a cascade classifier architecture. We first present the architecture of Carotene, which consists of a two-stage coarse and fine level classifier cascade. We compare Carotene to an early version that was based on a flat classifier architecture and also compare and contrast Carotene with a third party occupation classification system. The paper concludes by presenting experimental results on real world industrial data using both machine learning metrics and actual user experience surveys.",https://ieeexplore.ieee.org/document/7184892/,2015 IEEE First International Conference on Big Data Computing Service and Applications,30 March-2 April 2015,ieeexplore
10.1109/IJCNN.1999.832669,Cascade steepest descent learning algorithm for multilayer feedforward neural network,IEEE,Conferences,"In the article, a new and efficient multilayer neural networks learning algorithm is presented. The key concept of this new algorithm is the two-stage implementation of the steepest descent method. At the first stage, it is used to search the optimal learning constant /spl eta/ and momentum term /spl alpha/ for each weights updating process. At the second stage, the Delta learning rule is then employed to modify the connecting weights in terms of the optimal /spl eta/ and /spl alpha/. Computer simulations show that the proposed new algorithm outmatches other learning algorithms both in convergence speed and success rate. On real industrial application, a self-tuning neural-network based PID controller for precise temperature control of an injection mode barrel system by using the developed algorithm is developed. Experiments show that the proposed self-tuning PID controller can precisely control the barrel temperature within /spl plusmn/0.5/spl deg/C.",https://ieeexplore.ieee.org/document/832669/,IJCNN'99. International Joint Conference on Neural Networks. Proceedings (Cat. No.99CH36339),10-16 July 1999,ieeexplore
10.1109/FiCloudW.2017.79,Changes of Cyber-Attacks Techniques and Patterns after the Fourth Industrial Revolution,IEEE,Conferences,"In this paper, we predicted the changes of cyber-attacks techniques and patterns after the fourth industrial revolution with the epochal shift of information and communication technology and innovation of science and technology. Cyber space will be hyper-connection, cross-domain, and super intelligence space as connecting everything in the world due to a fusion of information and communication technologies such as artificial intelligence, internet of things, and cyber-physical systems. Cyber-attacks will use all electronic devices including wireless or wire networks, hardware, software, and cyber-physical systems as a route. The hacking tool's functions will evolve into a variety of forms reflecting human thought and behavioral procedures. The attack target will not be limited to a specific object. The purpose of the cyber-attack is to focus on secondary effects and indirect attacks as well as direct attacks.",https://ieeexplore.ieee.org/document/8113773/,2017 5th International Conference on Future Internet of Things and Cloud Workshops (FiCloudW),21-23 Aug. 2017,ieeexplore
10.1109/ICNP.2016.7784407,Characterizing industrial control system devices on the Internet,IEEE,Conferences,"Industrial control system (ICS) devices with IP addresses are accessible on the Internet and play a crucial role for critical infrastructures like power grid. However, there is a lack of deep understanding of these devices' characteristics in the cyberspace. In this paper, we take a first step in this direction by investigating these accessible industrial devices on the Internet. Because of critical nature of industrial control systems, the detection of online ICS devices should be done in a real-time and non-intrusive manner. Thus, we first analyze 17 industrial protocols widely used in industrial control systems, and train a probability model through the learning algorithm to improve detection accuracy. Then, we discover online ICS devices in the IPv4 space while reducing the noise of industrial honeypots. To observe the dynamics of ICS devices in a relatively long run, we have deployed our discovery system on Amazon EC2 and detected online ICS devices in the whole IPv4 space for eight times from August 2015 to March 2016. Based on the ICS device data collection, we conduct a comprehensive data analysis to characterize the usage of ICS devices, especially in the answer to the following three questions: (1) what are the distribution features of ICS devices, (2) who use these ICS devices, and (3) what are the functions of these ICS devices.",https://ieeexplore.ieee.org/document/7784407/,2016 IEEE 24th International Conference on Network Protocols (ICNP),8-11 Nov. 2016,ieeexplore
10.1109/EUVIP.2018.8611701,Classification of Building Information Model (BIM) Structures with Deep Learning,IEEE,Conferences,"In this work we study an application of machine learning to the construction industry and we use classical and modern machine learning methods to categorize images of building designs into three classes: Apartment building, Industrial building or Other. No real images are used, but only images extracted from Building Information Model (BIM) software, as these are used by the construction industry to store building designs. For this task, we compared four different methods: the first is based on classical machine learning, where Histogram of Oriented Gradients (HOG) was used for feature extraction and a Support Vector Machine (SVM) for classification; the other three methods are based on deep learning, covering common pre-trained networks as well as one designed from scratch. To validate the accuracy of the models, a database of 240 images was used. The accuracy achieved is 57% for the HOG + SVM model, and above 89% for the neural networks.",https://ieeexplore.ieee.org/document/8611701/,2018 7th European Workshop on Visual Information Processing (EUVIP),26-28 Nov. 2018,ieeexplore
10.1109/MED.2017.7984310,Cloud computing for big data analytics in the Process Control Industry,IEEE,Conferences,"The aim of this article is to present an example of a novel cloud computing infrastructure for big data analytics in the Process Control Industry. Latest innovations in the field of Process Analyzer Techniques (PAT), big data and wireless technologies have created a new environment in which almost all stages of the industrial process can be recorded and utilized, not only for safety, but also for real time optimization. Based on analysis of historical sensor data, machine learning based optimization models can be developed and deployed in real time closed control loops. However, still the local implementation of those systems requires a huge investment in hardware and software, as a direct result of the big data nature of sensors data being recorded continuously. The current technological advancements in cloud computing for big data processing, open new opportunities for the industry, while acting as an enabler for a significant reduction in costs, making the technology available to plants of all sizes. The main contribution of this article stems from the presentation for a fist time ever of a pilot cloud based architecture for the application of a data driven modeling and optimal control configuration for the field of Process Control. As it will be presented, these developments have been carried in close relationship with the process industry and pave a way for a generalized application of the cloud based approaches, towards the future of Industry 4.0.",https://ieeexplore.ieee.org/document/7984310/,2017 25th Mediterranean Conference on Control and Automation (MED),3-6 July 2017,ieeexplore
10.1109/BigData.2016.7840831,Cloud-based machine learning for predictive analytics: Tool wear prediction in milling,IEEE,Conferences,"The proliferation of real-time monitoring systems and the advent of Industrial Internet of Things (IIoT) over the past few years necessitates the development of scalable and parallel algorithms that help predict mechanical failures and remaining useful life of a manufacturing system or system components. Classical model-based prognostics require an in-depth physical understanding of the system of interest and oftentimes assume certain stochastic or random processes. To overcome the limitations of model-based methods, data-driven methods such as machine learning have been increasingly applied to prognostics and health management (PHM). While machine learning algorithms are able to build accurate predictive models, large volumes of training data are required. Consequently, machine learning techniques are not computationally efficient for data-driven PHM. The objective of this research is to create a novel approach for machinery prognostics using a cloud-based parallel machine learning algorithm. Specifically, one of the most popular machine learning algorithms (i.e., random forest) is applied to predict tool wear in dry milling operations. In addition, a parallel random forest algorithm is developed using the MapReduce framework and then implemented on the Amazon Elastic Compute Cloud. Experimental results have shown that the random forest algorithm can generate very accurate predictions. Moreover, significant speedup can be achieved by implementing the parallel random forest algorithm.",https://ieeexplore.ieee.org/document/7840831/,2016 IEEE International Conference on Big Data (Big Data),5-8 Dec. 2016,ieeexplore
10.1109/MSM49833.2020.9202398,Collaborative Robot System for Playing Chess,IEEE,Conferences,"In recent years, number of collaborative robots industrial applications has made a significant increasment. Implementation of collaborative robots is a safe and effective way for designing robot-human cooperation systems. Combined with constantly developing artificial intelligence, collaborative systems are actually able to solve complex problems that require some sort of intelligence. For humans, board games are a good example of the visualization of robot intelligence. Such systems require estimation and detection of board and pieces in manipulator workspace, some kind of decision-making algorithms and robot control system to move pieces. The flagship of such systems are chess playing robots. The chess game has a defined and easy to understand set of rules which makes it interesting example of intelligent robotics systems application. In this paper, we present an implementation of collaborative robots for chess playing system which was designed to play against human or another robot. The system is able to track state of the game via camera, calculate the optimal move using implemented decision-making algorithm, detect illegal moves and execute pick-and-place task to physically move pieces. We test the developed system in a real-world setup and provide experimental results documenting the performance of proposed approach.",https://ieeexplore.ieee.org/document/9202398/,2020 International Conference Mechatronic Systems and Materials (MSM),1-3 July 2020,ieeexplore
10.1109/IRI-05.2005.1506442,Collaborative knowledge management by integrating knowledge modeling and workflow modeling,IEEE,Conferences,"Recently both industrial and academic researches show great interest in knowledge management. However, users still find it hard to obtain a suitable knowledge management tool that fits for their needs. Although business requirements always change, current software systems still cannot be adapted to these changes quickly. In this paper, a framework is put forward to facilitate the design of an adaptive knowledge management system. In this framework, the structural knowledge modeling is combined with processes, which are used for ensuring the quality of knowledge acquisition in the framework. Two kinds of spaces, knowledge space and process space, and a knowledge state model are introduced. Finally, application systems based on this framework, which are being used in three real business enterprises for controlling life cycle of knowledge management in China, are discussed.",https://ieeexplore.ieee.org/document/1506442/,"IRI -2005 IEEE International Conference on Information Reuse and Integration, Conf, 2005.",15-17 Aug. 2005,ieeexplore
10.1109/ECAI46879.2019.9041923,Color-Based Sorting System for Agriculture Applications,IEEE,Conferences,"This paper presents the design and the implementation of a real-time color based sorting system that is intended to be used in agriculture applications. The control part of the system is realized with the versatile microcontroller Atmega328. The color detection of the analyzed objects is obtained with the specialized digital sensor TCS230 that contain all the circuitry that is necessary for converting the light into frequency signal for convenient acquiring and processing with the microcontroller. The actuation elements of the sorting system are realized with a set of TowerPro SG90 servomotors which have a very lightweight construction but are capable to generate relatively high output power. Being a microcontroller-based system that relies on an advanced color sensor and dedicated proprietary control software, the proposed sorting system is characterized by improved reconfigurability and adaptability. The system can be easily integrated within a very broad spectrum of applications including Internet of Things or quality control in industrial domain.",https://ieeexplore.ieee.org/document/9041923/,"2019 11th International Conference on Electronics, Computers and Artificial Intelligence (ECAI)",27-29 June 2019,ieeexplore
10.1109/CNNA.2002.1035095,Colour space transformation and exact colour reproduction with CNN technology,IEEE,Conferences,"Nowadays many problems requiring huge computing power have risen. Although the performance of digital processors doubles every year, there are certain tasks where the computation cannot be carried out within a reasonable time interval. Such hard problems are the analysis of big dynamical systems or real-time exact colour reproduction. The exact colour visualization of motion pictures is necessary in industrial, medical and scientific research areas. Thus, for example, exact colour reproduction is required for remote medical diagnosis or remote operation. The doctor has to see the same image that appears in reality. Device dependent colour appearance may cause faulty decisions. Nowadays these problems cannot be solved perfectly because many steps of the transformation are not completely known and the huge number of computations cannot be done in real-time even by the fastest PC. In this article we describe some methods to produce exact colours in a remote medical diagnostic system.",https://ieeexplore.ieee.org/document/1035095/,Proceedings of the 2002 7th IEEE International Workshop on Cellular Neural Networks and Their Applications,24-24 July 2002,ieeexplore
10.1109/DESSERT50317.2020.9125038,Combination of Digital Twin and Artificial Intelligence in Manufacturing Using Industrial IoT,IEEE,Conferences,"The paper focuses on Digital Twin (DT) in Manufacturing using Artificial Intelligence (AI) and Industrial IoT. According to the concept, the manufacturing includes three main units: equipment, personnel and processes. All data from these units are inherited to manufacture model (DT) and decision support system with the use of AI. DT data technology allows finding the required knowledge that can be interpreted and used to support the process of decision-making in the management of the enterprise. AI applications open up a broad spectrum of opportunities in manufacturing to add value by optimizing processes and generating new business models. The Landscape was described by a formal model to assure the possibility to analyze the state and development of landscape in detail considering DT and other technologies. DT and IIoT implementation for the simulation of real enterprise manufacturing were considered.",https://ieeexplore.ieee.org/document/9125038/,"2020 IEEE 11th International Conference on Dependable Systems, Services and Technologies (DESSERT)",14-18 May 2020,ieeexplore
10.1109/MetroInd4.0IoT48571.2020.9138184,Combining exposure indicators and predictive analytics for threats detection in real industrial IoT sensor networks,IEEE,Conferences,"We present a framework able to combine exposure indicators and predictive analytics using AI-tools and big data architectures for threats detection inside a real industrial IoT sensors network. The described framework, able to fill the gaps between these two worlds, provides mechanisms to internally assess and evaluate products, services and share results without disclosing any sensitive and private information. We analyze the actual state of the art and a possible future research on top of a real case scenario implemented into a technological platform being developed under the H2020 ECHO project, for sharing and evaluating cybersecurity relevant informations, increasing trust and transparency among different stakeholders.",https://ieeexplore.ieee.org/document/9138184/,2020 IEEE International Workshop on Metrology for Industry 4.0 & IoT,3-5 June 2020,ieeexplore
10.1109/ETFA.2017.8247730,Communication middleware technologies for industrial distributed control systems: A literature review,IEEE,Conferences,"Industry 4.0 is the German vision for the future of manufacturing, where smart factories use information and communication technologies to digitise their processes to achieve improved quality, lower costs, and increased efficiency. It is likely to bring a massive change to the way control systems function today. Future distributed control systems are expected to have an increased connectivity to the Internet, in order to capitalize on new offers and research findings related to digitalization, such as cloud, big data, and machine learning. A key technology in the realization of distributed control systems is middleware, which is usually described as a reusable software layer between operating system and distributed applications. Various middleware technologies have been proposed to facilitate communication in industrial control systems and hide the heterogeneity amongst the subsystems, such as OPC UA, DDS, and RT-CORBA. These technologies can significantly simplify the system design and integration of devices despite their heterogeneity. However, each of these technologies has its own characteristics that may work better for particular applications. Selection of the best middleware for a specific application is a critical issue for system designers. In this paper, we conduct a survey on available standard middleware technologies, including OPC UA, DDS, and RT-CORBA, and show new trends for different industrial domains.",https://ieeexplore.ieee.org/document/8247730/,2017 22nd IEEE International Conference on Emerging Technologies and Factory Automation (ETFA),12-15 Sept. 2017,ieeexplore
10.23919/ACC.1993.4793202,Comparative Analysis of Control Design Techniques for a Cart-Inverted-Pendulum in Real-Time Implementation,IEEE,Conferences,"Conventional controllers such as PID controllers have a long history of successful industrial applications. However, in recent years, many nonlinear controllers have been applied to deal with nonlinear systems. Sliding mode control has been successfully used for SISO non-linear systems and for certain multivariable systems, fuzzy-logic has been successfully applied in many practical control systems. Meanwhile, there has been interest in developing expert systems for control that involve necessary process knowledge required for good control. Neural network control has been used to determine adaptive laws for the adjustment of the control parameters. This paper will evaluate and compare PD, sliding mode, fuzzy, expert system, and neural network control methods in controlling the cart-inverted-pendulum. Performance is evaluated in terms of control surface, system response, stability, and robustness. Moreover the comparison of these controllers is validated through experimentation. Strengths and weaknesses, in the real-time control are indicated.",https://ieeexplore.ieee.org/document/4793202/,1993 American Control Conference,2-4 June 1993,ieeexplore
10.1109/ICMLA.2015.183,Comparative Evaluation of Top-N Recommenders in e-Commerce: An Industrial Perspective,IEEE,Conferences,"We experiment on two real e-commerce datasets and survey more than 30 popular e-commerce platforms to reveal what methods work best for product recommendations in industrial settings. Despite recent academic advances in the field, we observe that simple methods such as best-seller lists dominate deployed recommendation engines in e-commerce. We find our empirical findings to be well-aligned with those of the survey, where in both cases simple personalized recommenders achieve higher ranking than more advanced techniques. We also compare the traditional random evaluation protocol to our proposed chronological sampling method, which can be used for determining the optimal time-span of the training history for optimizing the performance of algorithms. This performance is also affected by a proper hyperparameter tuning, for which we propose golden section search as a fast alternative to other optimization techniques.",https://ieeexplore.ieee.org/document/7424455/,2015 IEEE 14th International Conference on Machine Learning and Applications (ICMLA),9-11 Dec. 2015,ieeexplore
10.1109/MI-STA52233.2021.9464484,Comparison of PID and Artificial Neural Network Controller in on line of Real Time Industrial Temperature Process Control System,IEEE,Conferences,"Due to its simple structure and robustness, the traditional proportional-integral-derivative (PID) controller is commonly used in the field of industrial automation and process control, but it does not function well with nonlinear systems, time-delayed linear systems and time-varying systems. A new type of PID controller based on artificial neural networks and evolutionary algorithms is presented in this paper. An powerful instrument for a highly nonlinear system is the Artificial Neural Network. The interest in the study of the nonlinear system has increased through the implementation of a high-speed computer system,. In complex systems such as robotics and process control systems, the Neuro Control Algorithm is often applied. Systems of process management is also nonlinear and hard to control consistently.. This paper presents a comprehensive analysis in Which is offline trained by a multilayered feed forward back propagation neural network to act as a process control system controller, That is to say, a temperature control device without prior knowledge of its dynamics. Via the implementation of a range of input vectors to the neural network, the inverse dynamics model is developed. Based on these input vectors, the output of the neural network It is being studied by explicitly configuring it to monitor the operation. In this paper, based on set-point adjustment, impact of disturbances in load and variable dead time, compassion between the PID controller and ANN is conducted. The outcome shows that ANN outperforms the controller of the PID.",https://ieeexplore.ieee.org/document/9464484/,2021 IEEE 1st International Maghreb Meeting of the Conference on Sciences and Techniques of Automatic Control and Computer Engineering MI-STA,25-27 May 2021,ieeexplore
10.1109/RTSS.2009.14,Component-Based Abstraction Refinement for Timed Controller Synthesis,IEEE,Conferences,"We present a novel technique for synthesizing controllers for distributed real-time environments with safety requirements. Our approach is an abstraction refinement extension to the on-the-fly algorithm by Cassez et al. from 2005. Based on partial compositions of some environment components, each refinement cycle constructs a sound abstraction that can be used to obtain under- and over-approximations of all valid controller implementations. This enables (1) early termination if an implementation does not exist in the over-approximation, or, if one does exist in the under-approximation, and (2) pruning of irrelevant moves in subsequent refinement cycles. In our refinement loop, the precision of the abstractions incrementally increases and converges to all specification-critical components. We implemented our approach in a prototype synthesis tool and evaluated it on an industrial benchmark. In comparison with the timed game solver UPPAAL-Tiga, our technique outperforms the nonincremental approach by an order of magnitude.",https://ieeexplore.ieee.org/document/5368182/,2009 30th IEEE Real-Time Systems Symposium,1-4 Dec. 2009,ieeexplore
10.1109/IECON43393.2020.9255001,Computation Offloading for Machine Learning in Industrial Environments,IEEE,Conferences,"Industrial applications, such as real-time manufacturing, fault classification and inference, autonomous cars, etc., are data-driven applications that require machine learning with a wealth of data generated from industrial Internet of Things (IoT) devices. However, conventional approaches of transmitting this rich data to a remote data center to learn may be undesired due to the non-negligible network transmission delay and the sensitiveness of data privacy. By deploying a number of computing-capable devices at the network edge, edge computing supports the implementation of machine learning close to the industrial environment. Considering the heterogeneous computing capability as well as network location of edge devices, there are two types of feasible edge computing based machine learning models, including the centralized learning and federated learning models. In centralized learning, a resource-rich edge server aggregates the data from different IoT devices and performs machine learning. In federated learning, distributed edge devices and a federated server collaborate to perform machine learning. The features that data should be offloaded in centralized learning while it is locally trained in federated learning make centralized learning and federated learning quite different. We study the computation offloading problem for edge computing based machine learning in an industrial environment, considering the abovementioned machine learning models. We formulate a machine learning-based offloading problem with the goal of minimizing the training delay. Then, an energy-constrained delay-greedy (ECDG) algorithm is designed to solve the problem. Finally, simulation studies based on the MNIST dataset have been conducted to illustrate the efficiency of the proposal.",https://ieeexplore.ieee.org/document/9255001/,IECON 2020 The 46th Annual Conference of the IEEE Industrial Electronics Society,18-21 Oct. 2020,ieeexplore
10.1109/ICIT.2002.1189341,Computer based robot training in a virtual environment,IEEE,Conferences,"As more market segments are welcoming automation, the robotic field continues to expand. With the accepted breadth of viable industrial robotic applications increasing, the need for flexible robotic training also grows. In the area of simulation and offline programming there have been innovative developments to Computer Aided Robotics (CAR) Systems. New and notable releases have been introduced to the public, especially among the small, affordable, and easy to use systems. These CAR-Systems are mainly aimed at system integrators in general industry business fields to whom the complex, powerful software tools used by the automotive industry (and its suppliers) are oversized. In general, CAR-Systems are used to design robot cells and to create the offline programs necessary to reduce start-up time and to achieve a considerable degree of planning reliability. Another potential yet to be fully considered, is the use of such CAR-Systems as an inexpensive and user-friendly tool for robotics training. This paper will show the educational potential and possibility inherent in simulation and introduce a successful example of this new method of training. Finally, this presentation should be seen as an attempt to outline novel methods for future education in an industrial environment characterized by the increased occurrence and implementation of the virtual factory.",https://ieeexplore.ieee.org/document/1189341/,"2002 IEEE International Conference on Industrial Technology, 2002. IEEE ICIT '02.",11-14 Dec. 2002,ieeexplore
10.1109/ITSC.1997.660540,Computer vision control of an intelligent forklift truck,IEEE,Conferences,"The paper describes an application of computer vision in the autonomous guidance of a traditional forklift truck, ROBOLIFT(TM), which is a product of Elsag Bailey Telerobot and FIAT OM. Computer vision represents the main sensory system for both navigation and load recognition. Artificial ""H-shaped"" landmarks placed along the truck path are used for the vehicle pose detection. Standard Europallets, supporting the loads to be transported, are recognized by the vision system in order to provide their estimated pose to the automatic fork control subsystem. The system is now tested in real industrial applications and preliminary results are reported.",https://ieeexplore.ieee.org/document/660540/,Proceedings of Conference on Intelligent Transportation Systems,12-12 Nov. 1997,ieeexplore
10.1109/PCCC.1990.101701,Concept for combining features of real-time and expert systems for on-line plant diagnosis,IEEE,Conferences,"A description is presented of an AI implementation in an industrial power plant: malfunction diagnosis based on the evaluation of plant performance. It is intended to support the development and maintenance of a knowledge-based real-time control system. The features of a programming tool which combines characteristics of expert systems and real-time supervision systems are explained. The system is used for online diagnosis of a plant. The real-time properties are achieved by dividing data processing into several processing units which are computed in parallel. Since plant diagnosis differs from most expert system applications, the expert system part has been developed with regard to this. A prototype of this system has been implemented for the domain of thermal spraying.&lt;<ETX>&gt;</ETX>",https://ieeexplore.ieee.org/document/101701/,Ninth Annual International Phoenix Conference on Computers and Communications. 1990 Conference Proceedings,21-23 March 1990,ieeexplore
10.1109/AITest.2019.00015,Constraint-Based Testing of An Industrial Multi-Robot Navigation System,IEEE,Conferences,"Intelligent multi-robot systems get more and more deployed in industrial settings to solve complex and repetitive tasks. Due to safety and economic reasons they need to operate dependably. To ensure a high degree of dependability, testing the deployed system has to be done in a rigorous way. Advanced multi-robot systems show a rich set of complex behaviors. Thus, these systems are difficult to test manually. Moreover, the space of potential environments and tasks for such systems is enormous. Therefore, methods that are able to explore this space in a structured way are needed. One way to address these issues is through model-based testing. In this paper we present an approach for testing the navigation system of a fleet of industrial transport robots. We show how all potential environments and navigation behaviors as well as requirements and restrictions can be represented in a formal constraint-based model. Moreover, we present the concept of coverage criteria in order to handle the potentially infinite space of test cases. Finally, we show how test cases can be derived from this model in an efficient way. In order to show the feasibility of the proposed approach we present an empirical evaluation of a prototype implementation using a real industrial use case.",https://ieeexplore.ieee.org/document/8718216/,2019 IEEE International Conference On Artificial Intelligence Testing (AITest),4-9 April 2019,ieeexplore
10.1109/ISNCC49221.2020.9297198,Construction Safety Surveillance Using Machine Learning,IEEE,Conferences,"Safety has always been a matter of concern in all industrial activities, especially construction. Hard hats or safety helmets act as the first line of protection against serious head injuries. Some workers don't quite follow the instructions and signs that require them to follow safety measures. And in case of any accidents, the workers will not be eligible for insurance, if they were not wearing the safety equipments. This applied research paper mainly focuses to detect persons with and without a helmet in the construction site. The accuracy and performance of Neural Networks were tested and compared with other hand crafted features like Haar and LBP classifiers, Histogram of Oriented Gradients and Sequential Classifiers. These hand crafted features gave more false detections than neural nets, when tested in real conditions. Different Neural Networks were tested on Edge Devices such as Nvidia Jetson TX2 and Jetson Nano, for commercial deployment. Compared to the other neural networks, the SSD MobileNet model showed better performance without considerable drop in accuracy, when tested on edge devices in real-time. This makes it a preferable solution for this application-oriented problem.",https://ieeexplore.ieee.org/document/9297198/,"2020 International Symposium on Networks, Computers and Communications (ISNCC)",20-22 Oct. 2020,ieeexplore
10.1109/ICAICA50127.2020.9182598,Construction of Industrial Internet of Things Based on MQTT and OPC UA Protocols,IEEE,Conferences,"At present, the Internet of Things has become a hot area of global concern and is considered to be another major scientific and technological innovation after the Internet. The Internet of Things can be analyzed from two aspects, one is to access the Internet, and the other is the connection between things. The Industrial Internet of Things, as a new type of industrial ecosystem, collects resources and data during the industrial production process by using miniature low-cost controllers and high-bandwidth wireless networks to achieve flexible configuration of manufacturing raw materials and improve resource utilization. The Industrial Internet of Things not only includes traditional software elements, but also requires hardware controllers and sensors, as well as cloud service platforms, to ultimately achieve intelligent manufacturing. The Industrial Internet of Things based on MQTT adopts the method of upper computer (WeChat applet) + server (Alibaba Cloud Server) + lower computer (WiFi module ESP8266 NodeMcu) to realize data collection and control of industrial production process. Based on OPC UA protocol, The Industrial Internet of Things, by designing an intelligent node device based on embedded Linux system, uses Qt to build the OPC UA protocol. OPC UA is a C / S architecture real-time database framework. By building an OPC UA Client on a handheld terminal device, An OPC UA Serve is set up on the smart terminal device, and the information collected by the handheld terminal is sent to the smart terminal. At the same time, multiple smart terminals are connected through the OPC UA protocol to perform data interaction and information transfer. At the same time, an edge computing algorithm is embedded in the smart terminal, and a large amount of data collected is processed for analysis, screening, and calculation. At the same time, the processed information is transmitted or processed. Reduce data interactions between data sources and data centers, and reduce the amount of information transmitted and stored.",https://ieeexplore.ieee.org/document/9182598/,2020 IEEE International Conference on Artificial Intelligence and Computer Applications (ICAICA),27-29 June 2020,ieeexplore
10.1109/PHM-Paris.2019.00061,Convolutional Neural Network Based Rolling-Element Bearing Fault Diagnosis for Naturally Occurring and Progressing Defects Using Time-Frequency Domain Features,IEEE,Conferences,"Convolutional Neural Networks (CNN) are becoming increasingly popular for bearing fault diagnosis due to their ability to automatically capture the sensitive fault information without the need for expert knowledge. Most of these applications are developed considering vibration data from artificially induced faults. However, bearing failure in real-life can show huge damage variations even within a single category of failure which artificially induced failures are unable to represent. Thus, in this paper, the performance of classical CNN is evaluated on bearings with naturally occurring and progressing defects from the Paderborn University Dataset. A three-class (Healthy, Inner Race Fault and Outer Race Fault) classification problem is solved considering five bearing conditions within each class. These conditions vary in terms of bearing operating hours, damage mode, damage repetition pattern, the extent of damage, etc. The classification accuracy is evaluated under two cases: (1) at least a portion of data from each bearing condition from all classes is used in training; (2) data from all available conditions are considered for training except from one condition which is used explicitly for testing. Within each case, the effect of changing the domain of the input data is evaluated on the achieved accuracy. Three input signals based on vibration data (raw time domain signal, envelope spectrum, and spectrogram) were explored for their representation effectiveness. The proposed CNN with a spectrogram of the vibration signal as input achieves better results than similar architectures. Finally, the potential challenges that come along with the implementation of Deep Learning technologies for industrial applications are discussed and future research directions are proposed.",https://ieeexplore.ieee.org/document/8756423/,2019 Prognostics and System Health Management Conference (PHM-Paris),2-5 May 2019,ieeexplore
10.1109/EAEEIE.2013.6576535,Cooperation between industry and university based on the evaluation of the industrial research results in the academic environment,IEEE,Conferences,"Based on the European Structural Funds it was developed the Intelligent Mobile Box, Intelligent Panel Controller with intelligent adaptive controllers within the industrial research and experimental development in the company Kybernetes, s.r.o. Within the frame of the academic-industry cooperation, the intelligent adaptive controller was tested at the Department of Cybernetics and Artificial Intelligence, Technical university of Kosice, Slovakia. The tests of the mobile intelligent adaptive controller were performed on two levels of university study, on the Bachelor level on the exercises from the subject “Control of Technological Processes” and on the Engineering level the exercises from the subject “Intelligent Control Networks” and on one Diploma project. Goals of students of the Control of the technological processes course had two goals, firstly to connect the intelligent adaptive controller to pre-defined controlled system (real plant, real model or simulated model) and next to validate the control results. Students of the Diploma project on the Engineering level had more advanced goals. Tasks defined for engineering students were to connect the intelligent adaptive controller to non-defined controlled system, setup the adaptivity process of the controller regarding the learning error, parameterize the control system, observe and validate the control results. Both sides concluded this cooperation as very valuable. Main contributions for students were (U1) the challenge to apply studied theoretical knowledge on the real industrial controllers, (U2) experience with new research results and technologies deployed in industry and (U3) the implementation of the control and adaptive algorithms from abstract mathematical area to real PLC controller. On side of industry research company the main contributions were (C1) testing of designed algorithms and (C2) user feedback from students to make the application HMI interface more understandable a native.",https://ieeexplore.ieee.org/document/6576535/,2013 24th EAEEIE Annual Conference (EAEEIE 2013),30-31 May 2013,ieeexplore
10.1109/WAIN52551.2021.00009,Corner Case Data Description and Detection,IEEE,Conferences,"As the major factors affecting the safety of deep learning models, corner cases and related detection are crucial in AI quality assurance for constructing safety- and security-critical systems. The generic corner case researches involve two interesting topics. One is to enhance DL models' robustness to corner case data via the adjustment on parameters/structure. The other is to generate new corner cases for model retraining and improvement. However, the complex architecture and the huge amount of parameters make the robust adjustment of DL models not easy, meanwhile it is not possible to generate all real-world corner cases for DL training. Therefore, this paper proposes a simple and novel approach aiming at corner case data detection via a specific metric. This metric is developed on surprise adequacy (SA) which has advantages on capture data behaviors. Furthermore, targeting at characteristics of corner case data, three modifications on distanced-based SA are developed for classification applications in this paper. Consequently, through the experiment analysis on MNIST data and industrial data, the feasibility and usefulness of the proposed method on corner case data detection are verified.",https://ieeexplore.ieee.org/document/9474385/,2021 IEEE/ACM 1st Workshop on AI Engineering - Software Engineering for AI (WAIN),30-31 May 2021,ieeexplore
10.1109/IPDPS.2019.00105,Cpp-Taskflow: Fast Task-Based Parallel Programming Using Modern C++,IEEE,Conferences,"In this paper we introduce Cpp-Taskflow, a new C++ tasking library to help developers quickly write parallel programs using task dependency graphs. Cpp-Taskflow leverages the power of modern C++ and task-based approaches to enable efficient implementations of parallel decomposition strategies. Our programming model can quickly handle not only traditional loop-level parallelism, but also irregular patterns such as graph algorithms, incremental flows, and dynamic data structures. Compared with existing libraries, Cpp-Taskflow is more cost efficient in performance scaling and software integration. We have evaluated Cpp-Taskflow on both micro-benchmarks and real-world applications with million-scale tasking. In a machine learning example, Cpp-Taskflow achieved 1.5-2.7× less coding complexity and 14-38% speed-up over two industrial-strength libraries OpenMP Tasking and Intel Threading Building Blocks (TBB).",https://ieeexplore.ieee.org/document/8821011/,2019 IEEE International Parallel and Distributed Processing Symposium (IPDPS),20-24 May 2019,ieeexplore
10.1109/COMPSAC.2015.203,Creative Computing for Bespoke Ideation,IEEE,Conferences,"Today, idea generation is an extremely important activity for both academic researchers and industrial groups. A considerable number of applications and research studies have been made in the past years in order to increase the effectiveness of idea making process. However, there is lack of efforts on making machine generated ideas novel, surprising and valuable. This research uses concept machine ideation for machinery creative ideas generation and aims to discuss the realistic of bespoke machines ideation, which is simplified as bespoke ideation in the main text. After carefully reviewed the current situation of human ideation, it raises challenge issue to be addressed in this paper. Then, it reviews representative human ideation cases and summarises an overview process for general human ideation. Moreover, it simulates the overview process into a machine ideation process. Next, related concepts and techniques are discussed including Creative Computing, new idea, and creativity. Based on the discussions and the simulated machine ideation process, this paper provides a Creative Computing method to achieve bespoke ideation. Then, detailed techniques and rules that needed to be designed and applied are planned and explained for all the phases to fulfil the machine ideation approach as bespoke approach. Furthermore, a prototype of Research Topics Generation System (RTGS) is developed as a bespoke ideation system to illustrate the proposed approach as well as prove the approach's feasibility, in which a set of new research topics on a designated domain is generated to inspire further studies. The main contribution of this paper is the proposed bespoke ideation approach, by which machine ideation is possible to be achieved in reality.",https://ieeexplore.ieee.org/document/7273290/,2015 IEEE 39th Annual Computer Software and Applications Conference,1-5 July 2015,ieeexplore
10.1109/AINS47559.2019.8968698,Cyber Security Risk Assessment on Industry 4.0 using ICS testbed with AI and Cloud,IEEE,Conferences,"Industry 4.0 is a new concept, thus risk assessment is necessary. Several risk assessment methods for Industrial Control System (ICS) and Industry 4.0 have been proposed, however, it is difficult to identify impacts on the physical world caused by cyber attacks against ICS since many of these are based on tabletop analysis or software simulations. Therefore, we focus on the risk assessment using actual machines (ICS testbed) which can help to solve the above problems. In Industry 4.0, autonomous judgment and execution are required for the cyber-physical system, it is based on information exchange using Artificial Intelligence (AI) and cloud technologies. In this research, we evaluate cyber risks through attacks against ICS with AI and cloud using ICS testbed. The proposed method can clarify cyber risks and impacts on the real world, and corresponding countermeasures.",https://ieeexplore.ieee.org/document/8968698/,"2019 IEEE Conference on Application, Information and Network Security (AINS)",19-21 Nov. 2019,ieeexplore
10.1109/COINS51742.2021.9524088,Cyber Security Risks of Technical Components in Industry 4.0,IEEE,Conferences,"Industry 4.0 is a new concept of automation data exchange in manufacturing. Industry 4.0 consists of various technical components such as Artificial Intelligence (AI), IoT, OPC UA, and cloud. There could be cyber risks when these technical components are used in Industrial Control System (ICS) without security consideration. Therefore cyber risk assessment, reconsideration of implementation, operation are necessary to adopt Industry 4.0. Problems of previous research for ICS risk assessments are some of them are theoretical and conceptual methods such as simulations. Moreover, they do not focus on Industry 4.0 components.The objective of this research is to prove the concrete cyber risks and introduces corresponding secure implementations for Industry 4.0 components such as Artificial Intelligence (AI), the Internet of Things (IoT), OPC Unified Architecture (OPC UA). The proposed method has benefits for clarifying impacts on the real world by cyber-attacks through penetration tests against ICS testbed with actual machines.",https://ieeexplore.ieee.org/document/9524088/,2021 IEEE International Conference on Omni-Layer Intelligent Systems (COINS),23-25 Aug. 2021,ieeexplore
10.1109/BigData47090.2019.9006096,DSSLP: A Distributed Framework for Semi-supervised Link Prediction,IEEE,Conferences,"Link prediction is widely used in a variety of industrial applications, such as merchant recommendation, fraudulent transaction detection, and so on. However, it's a great challenge to train and deploy a link prediction model on industrial-scale graphs with billions of nodes and edges. In this work, we present a scalable and distributed framework for semi-supervised link prediction problem (named DSSLP), which is able to handle industrial-scale graphs. Instead of training model on the whole graph, DSSLP is proposed to train on the k-hops neighborhood of nodes in a mini-batch setting, which helps reduce the scale of the input graph and distribute the training procedure. In order to generate negative examples effectively, DSSLP contains a distributed batched runtime sampling module. It implements uniform and dynamic sampling approaches, and is able to adaptively construct positive and negative examples to guide the training process. Moreover, DSSLP proposes a model-split strategy to accelerate the speed of inference process of the link prediction task. Experimental results demonstrate that the effectiveness and efficiency of DSSLP in serval public datasets as well as real-world datasets of industrial-scale graphs.",https://ieeexplore.ieee.org/document/9006096/,2019 IEEE International Conference on Big Data (Big Data),9-12 Dec. 2019,ieeexplore
10.1109/ICTC49870.2020.9289505,Data-driven IoT-based Water Quality Monitoring and Potability Classification System in Rural Areas,IEEE,Conferences,"Access to a safe and sustainable water source is a major problem especially in rural areas of developing countries. Water monitoring in different water resources has been practiced to ensure safe drinking water. However, manual monitoring of safe drinking water is known to be inconvenient since it requires high operational and transportation costs, and time-consuming. This work develops a water quality monitoring and potability classification system utilizing an Internet of Things framework. Portable sensor nodes capable of collecting physicochemical properties of water are deployed in different water sources from rural household areas. Data collected by nodes are being sent out wirelessly to a base station in real-time. The base station performs potability classification using ensemble learning. In addition, the base station sends the result of classification to households using 2G/3G communication. Also, the predicted output and the actual sensor data are being sent to a cloud server for remote monitoring via a web interface. Results show that the system achieves a 93.33% match with conventional industrial water laboratory tests. Moreover, the system is able to communicate the water potability status to households with minimal delay.",https://ieeexplore.ieee.org/document/9289505/,2020 International Conference on Information and Communication Technology Convergence (ICTC),21-23 Oct. 2020,ieeexplore
10.1109/WOCC48579.2020.9114917,Data-driven Surplus Material Prediction in Steel Coil Production,IEEE,Conferences,"A steel enterprise is currently trying to avoid the presence of surplus materials since they can greatly increase its operational cost. The complicated production process of steel products makes it difficult to find the causes of surplus materials. In this work, we propose a surplus material prediction problem and solve it based on statistical analysis and machine learning methods. In the concerned problem, we predict whether there are surplus materials under a given group of production parameters. The dataset used in this work is from a real-world three-month steel coil production process. First, data cleaning is conducted to standardize the industrial dataset. Then, the production parameters highly correlated with surplus material prediction results are selected by a series of feature selection methods. Finally, two prediction models based on extreme gradient boosting and logistic regression are presented according to the selected features. The experimental results reveal that the proposed prediction models have similar effectiveness. A visible regression function makes the logistic regression method more suitable for practical application.",https://ieeexplore.ieee.org/document/9114917/,2020 29th Wireless and Optical Communications Conference (WOCC),1-2 May 2020,ieeexplore
10.1109/INDIN41052.2019.8972310,Data-driven modeling of semi-batch manufacturing: a rubber compounding test case,IEEE,Conferences,"The continuously growing amount of available data from manufacturing processes supports the development of data-driven models. The typical target application of these models is optimal control and continuous quality management within an objective of zero-defect manufacturing. However, data obtained from batch processes are characterized by its high dimensionality that exceeds the computational capabilities of online applications and data-driven model's reliability must be guaranteed for proper industrial implementation. We explore two approaches to reduce problem's size: feature extraction and feature selection; several multivariate regression methods are also compared regarding it precision and robustness. We base our analysis on an industrial rubber compounding process where natural rubber is blended in a semi-batch mixer with several additives, then it is further mixed up using cylinders and it is conditioned in bands for storing. For this process, real production data is collected and stored in the manufacturing execution system of the company. The objective of the analysis is to predict mechanical properties of the rubber at the end of the processes. Based on the provided data, several data-driven models are built and tested. From the comparison among them it is concluded: models based on feature extraction and artificial neural networks yield the highest accuracy, while feature-selected models provide better physical interpretability and increased robustness regarding industrial deployment.",https://ieeexplore.ieee.org/document/8972310/,2019 IEEE 17th International Conference on Industrial Informatics (INDIN),22-25 July 2019,ieeexplore
10.1109/AERO.2018.8396547,Data-driven quality prognostics for automated riveting processes,IEEE,Conferences,"Technologies based in robotics and automatics are reshaping the aerospace industry. Aircraft manufacturers and top-tier suppliers now rely on robotics to perform most of its operational tasks. Over the years, a succession of implemented mobile robots has been developed with the mission of automating important industrial processes such as welding, material handling or assembly procedures. However, despite the progress achieved, a major limitation is that the process still requires human supervision and an extensive quality control process. An approach to address this limitation is to integrate machine learning methods within the quality control process. The idea is to develop algorithms that can direct manufacturing experts towards critical areas requiring human supervision and quality control. In this paper we present an application of machine learning to a concrete industrial problem involving the quality control of a riveting machine. The proposal consists of an intelligent predictive model that can be integrated within the existing real time sensing and pre-processing sub-systems at the equipment level. The framework makes use of several data-driven techniques for pre-processing and feature engineering, combined with the most accurate algorithms, validated through k-folds cross validation technique which also estimates prediction errors. The model is able to classify the manufacturing process of the machine as nominal or anomalous according to a real-world data set of design requirements and operational data. Several machine learning algorithms are compared such as linear regression, nearest neighbor, support vector machines, decision trees, random forests and extreme gradient boost. Results obtained from the case study suggest that the proposed model produces accurate predictions which meet industrial standards.",https://ieeexplore.ieee.org/document/8396547/,2018 IEEE Aerospace Conference,3-10 March 2018,ieeexplore
10.1109/AITest.2019.00018,Datamorphic Testing: A Method for Testing Intelligent Applications,IEEE,Conferences,"Adequate testing of AI applications is essential to ensure their quality. However, it is often prohibitively difficult to generate realistic test cases or to check software correctness. This paper proposes a new method called datamorphic testing, which consists of three components: a set of seed test cases, a set of datamorphisms for transforming test cases, and a set of metamorphisms for checking test results. With an example of face recognition application, the paper demonstrates how to develop datamorphic test frameworks, and illustrates how to perform testing in various strategies, and validates the approach using an experiment with four real industrial applications of face recognition.",https://ieeexplore.ieee.org/document/8718220/,2019 IEEE International Conference On Artificial Intelligence Testing (AITest),4-9 April 2019,ieeexplore
10.1109/ICSTCC.2019.8885434,Data–driven Neural Feedforward Controller Design for Industrial Linear Motors,IEEE,Conferences,"In this paper we consider the problem of feedforward controller design for industrial linear motors. These motors are safety-critical high-precision mechatronics systems that pose stringent requirements on the feedforward design: safe and predictable behavior for the desired motion profiles, tracking performance within the 10μm range in the presence of nonlinear friction and real-time implementation within the 1ms range. We investigate and compare several possibilities to design data- driven feedforward controllers using neural networks (NN) and we show that a two-step inverse estimation method is the most suitable approach, due to robustness to noisy data. We also show that basic knowledge about the system dynamics and the friction behavior can be exploited to design neural feedforward controllers with a simple structure, suitable for real-time implementation in industrial linear motors. The developed data-driven neural feedforward controllers are tested and compared with standard mass-acceleration feedforward and iterative learning controllers in realistic simulations.",https://ieeexplore.ieee.org/document/8885434/,"2019 23rd International Conference on System Theory, Control and Computing (ICSTCC)",9-11 Oct. 2019,ieeexplore
10.1109/ICDMW.2015.121,Deep Learning for Image Retrieval: What Works and What Doesn't,IEEE,Conferences,"To build an industrial content-based image retrieval system (CBIRs), it is highly recommended that feature extraction, feature processing and feature indexing need to be fully considered. Although research that bloomed in the past years suggest that the convolutional neural network (CNN) be in a leading position on feature extraction &amp; representation for CBIRs, there are less instructions on the deep analysis of feature related topics, for example the kind of feature representation that has the best performance among the candidates provided by CNN, the extracted features generalization ability, the relationship between the dimensional reduction and the accuracy loss in CBIRs, the best distance measure technique in CBIRs and the benefit of the coding techniques in improving the efficiency of CBIRs, etc. Therefore, several practicing studies were conducted and a thorough analysis was made in this research attempting to answer the above questions. The results in the study on both ImageNet-2012 and an industrial dataset provided by Sogou demonstrate that fc4096a and fc4096b perform the best on the datasets from unseen categories. Several interesting and practicing conclusions are drawn, for instance, fc4096a and fc4096b are found to have a better generalization ability than other features of CNN and could be considered as the first choice for industrial CBIRs. Furthermore, a novel feature binarization approach is presented in this paper for better efficiency of CBIRs. More specifically, the binarization is capable of reducing 31/32 space usage of original data. To sum up, the conclusions seem to provide practical instructions on real industrial CBIRs.",https://ieeexplore.ieee.org/document/7395863/,2015 IEEE International Conference on Data Mining Workshop (ICDMW),14-17 Nov. 2015,ieeexplore
10.1109/CBMS.2019.00040,Deep-Learning and HPC to Boost Biomedical Applications for Health (DeepHealth),IEEE,Conferences,"This document introduces the DeepHealth project: ""Deep-Learning and HPC to Boost Biomedical Applications for Health"". This project is funded by the European Commission under the H2020 framework program and aims to reduce the gap between the availability of mature enough AI-solutions and their deployment in real scenarios. Several existing software platforms provided by industrial partners will integrate state-of-the-art machine-learning algorithms and will be used for giving support to doctors in diagnosis, increasing their capabilities and efficiency. The DeepHealth consortium is composed by 21 partners from 9 European countries including hospitals, universities, large industry and SMEs.",https://ieeexplore.ieee.org/document/8787438/,2019 IEEE 32nd International Symposium on Computer-Based Medical Systems (CBMS),5-7 June 2019,ieeexplore
10.1109/ICECS.1999.813221,Defect detection and classification on web textile fabric using multiresolution decomposition and neural networks,IEEE,Conferences,"In this paper a pilot system for defect detection and classification of web textile fabric in real-time is presented. The general hardware and software platform, developed for solving this problem, is presented while a powerful novel method for defect detection after multiresolution decomposition of the fabric images is proposed. This method gives good results in the detection of low contrast defects under real industrial conditions, where many types of noise are present. An artificial neural network, trained by a back-propagation algorithm, performs the defect classification in categories.",https://ieeexplore.ieee.org/document/813221/,"ICECS'99. Proceedings of ICECS '99. 6th IEEE International Conference on Electronics, Circuits and Systems (Cat. No.99EX357)",5-8 Sept. 1999,ieeexplore
10.1109/ISIE45063.2020.9152441,Deployment of a Smart and Predictive Maintenance System in an Industrial Case Study,IEEE,Conferences,"Industrial manufacturing environments are often characterized as being stochastic, dynamic and chaotic, being crucial the implementation of proper maintenance strategies to ensure the production efficiency, since the machines' breakdown leads to a degradation of the system performance, causing the loss of productivity and business opportunities. In this context, the use of emergent ICT technologies, such as Internet of Things (IoT), machine learning and augmented reality, allows to develop smart and predictive maintenance systems, contributing for the reduction of unplanned machines' downtime by predicting possible failures and recovering faster when they occur. This paper describes the deployment of a smart and predictive maintenance system in an industrial case study, that considers IoT and machine learning technologies to support the online and real-time data collection and analysis for the earlier detection of machine failures, allowing the visualization, monitoring and schedule of maintenance interventions to mitigate the occurrence of such failures. The deployed system also integrates machine learning and augmented reality technologies to support the technicians during the execution of maintenance interventions.",https://ieeexplore.ieee.org/document/9152441/,2020 IEEE 29th International Symposium on Industrial Electronics (ISIE),17-19 June 2020,ieeexplore
10.1109/IPDPSW.2012.178,Deriving a Methodology for Code Deployment on Multi-Core Platforms via Iterative Manual Optimizations,IEEE,Conferences,"In recent years, there has been what can only be described as an explosion in the types of processing devices one can expect to find within a given computer system. These include the multi-core CPU, the General Purpose Graphics Processing Unit (GPGPU) and the Accelerated Processing Unit (APU), to name but a few. The widespread uptake of these systems presents would-be users with at least two problems. Firstly, each device exposes a complex underlying architecture which must be appreciated in order to attain optimal performance. This is coupled with the fact that a single system can support an arbitrary number of such devices. Consequently, fully leveraging the performance capabilities of such a system must come at a cost -- increasingly prolonged development times. Adhering to a methodology will have the significant industrial impact of reducing these development times. This paper describes the continued formulation of such a novel methodology. Two real world scientific programs are optimized for execution on the CUDA platform. Double precision accuracy and optimized speedups (which include PCI-E transfer times) of 15x and 17x are achieved.",https://ieeexplore.ieee.org/document/6270808/,2012 IEEE 26th International Parallel and Distributed Processing Symposium Workshops & PhD Forum,21-25 May 2012,ieeexplore
10.1109/iSAI-NLP48611.2019.9045302,Design and Development of a Low-Cost Indigenous Solar Powered 4-DOF Robotic Manipulator on an Unmanned Ground Vehicle,IEEE,Conferences,"We present in this paper the design, control and implementation of a versatile low cost manipulator with an arm gripper configured on an existing unmanned ground vehicle (UGV) for lifting payload (PL) and performing real world tasks. The major development in this work is the robust and efficient stable customized design of manipulator having four degrees of freedom (DOF) capable of lifting up to 1.5 kg weight for various industrial and non-industrial applications. The communication link is established using two human supervisory controlled wireless four channel 2. 4GHz remote controllers, which are separately used for UGV and manipulator for effective maneuvering and control of a 6-DOF system (UGV and Manipulator) overall. The controlling of RC servo motors is made using Arduino Uno controller board. An on board solar panel is used for charging batteries run time during the day. A50W, 1SV standard solar panel is used to enhance the maneuvering time of UGV and manipulator. The unique feature of the selected UGV is its two rotating head on flippers capable of controlled maneuvering especially in uneven terrain surfaces, stair climbing etc. Trial run experiments have shown the developed system is capable to perform future tasks in human unapproachable situations like contaminated or hazardous areas in several industrial and military applications.",https://ieeexplore.ieee.org/document/9045302/,2019 14th International Joint Symposium on Artificial Intelligence and Natural Language Processing (iSAI-NLP),30 Oct.-1 Nov. 2019,ieeexplore
10.1109/ICIAI.2019.8850785,Design and Implementation of Intelligent Building Control System Based on Real-time Database,IEEE,Conferences,"In this paper, we propose an intelligent building control system based on BrowserIServer(B/S) architecture that allows intelligent control of construction equipment and integrate multiple subsystems. The system is compatible with three communication protocols and intellectualized control is realized by logic control based on Event-Condition-Action(ECA) rules. The system adopts Agilor distributed real-time database system to ensure high real-time performance. The design of distributed system ensures the stability of system by ensuring that the service failure in a single area of system does not affect normal operation of other areas. The system provides six kinds of interfaces, which makes system have strong integration. The research of system architecture provides a general model in the field of intelligent building for enabling communication and computing infrastructure for industrial Artificial Intelligence(AI).",https://ieeexplore.ieee.org/document/8850785/,2019 1st International Conference on Industrial Artificial Intelligence (IAI),23-27 July 2019,ieeexplore
10.1109/INCOS45849.2019.8951312,Design and Implementation of Non-Intrusive Load Monitoring using Machine Learning Algorithm for Appliance Monitoring,IEEE,Conferences,"Energy Conservation and management is gaining popularity in research area due to the increase in energy demand. It also plays a vital role in industrial/commercial/domestic/power sectors for reducing the carbon emission, the energy bill. Thus, increase in conservation would change the economy of the world energy crisis. To conserve energy, it is very important to have a better monitoring and identification system. The existing monitoring technique has conventional energy meter or smart energy meter that gives total energy consumption only. To enhance a better quality of services in existing monitoring technique, there is a need to monitor energy consumption of individual appliances and hence one meter/sensor for each appliance are necessary. Due to more sensors and its associated installation cost, this technique is not a cost effective in nature. To overcome Non-intrusive Load monitoring technique was introduced to disaggregate the total energy consumption from a single meter using Machine learning disaggregation algorithm. Thus, to identify the appliances malfunctioning Non-intrusive load monitoring (NILM) technique can be used as a Real time Monitoring technique. In this paper, it is proposed to use single energy meter for the set of appliances to monitor the status of the individual appliances. Non-intrusive Load Monitoring technique using machine learning algorithms has been discussed for appliances identification and monitoring for energy conservation. The MATLAB/Simulink Software has been used for designing and mathematical modeling of each appliance. The NILM technique mainly involves the three stages via; Data acquisition, feature extraction and training of data under different classification algorithm for appliance identification. Data acquisition used for acquiring the voltage and current from a single phase system. Using the features extracted like active, reactive power the different load patterns of individual appliances can be studied. Training of data under DT and K-NN which are supervised learning techniques are used as disaggregation algorithm. Moreover, the algorithms are compared using the Confusion matrix and ROC curve for the prediction of accuracy. The result shows that the K-NN algorithm is having a better accuracy of performance compared with DT algorithm.",https://ieeexplore.ieee.org/document/8951312/,"2019 IEEE International Conference on Intelligent Techniques in Control, Optimization and Signal Processing (INCOS)",11-13 April 2019,ieeexplore
10.1109/ICETET.2009.71,Design and Implementation of Real Time Neurofuzzy Based pH Controller,IEEE,Conferences,"The quality of an intelligent control system is the ability to control the process with a certain degree of autonomy. These requirements, as an autonomous process controller are ever increasing. Considering the fact that existing algorithm based controllers such as the adaptive PID have their own limitations/are inadequate, the controllers are designed to emulate human mental faculties such as adaptation, learning, and planning under uncertainties and also coping up with large amounts of data by reducing the complexity of their representation. The family of intelligent controllers includes those based on neural nets, fuzzy logic, classic artificial intelligence and the genetic algorithms. This paper describes design and development of real time neurofuzzy based pH controller, which can be used in water treatment processes, laboratory studies and other industrial applications.",https://ieeexplore.ieee.org/document/5395042/,2009 Second International Conference on Emerging Trends in Engineering & Technology,16-18 Dec. 2009,ieeexplore
10.1109/ICSPIS.2018.8700563,Design and Implementation of a Parcel Sorter Using Deep Learning,IEEE,Conferences,"Automation in industrial environment reduces the cost of the operation while increasing the overall performance. Having an automation mechanism in the e-commerce warehouses to sort the parcels based on their destinations or shipping method will reduce the parcel processing time significantly. To automate parcel processing in Digikala's warehouse, a parcel sorter system is designed and implemented. In this system shipment method of the parcel is indicated by a set of markers. A computer vision system is developed to identify these markers using deep learning algorithms. The parcels are identified while they are moving on the conveyor belt in a relatively high speed (1 m/s). The computer vision system is capable of processing 1.3MP pictures in real-time with a rate of 100FPS. To sort the parcels an omni wheel roller mechanism is designed and utilized. To achieve the best results in a practical environment, a gap optimization mechanism and pack positioning conveyor are implemented and placed before the sorter. This system is successfully installed in the Digikala's warehouse.",https://ieeexplore.ieee.org/document/8700563/,2018 4th Iranian Conference on Signal Processing and Intelligent Systems (ICSPIS),25-27 Dec. 2018,ieeexplore
10.1109/AIEA53260.2021.00021,Design and Implementation of the Prototype for Hybrid Production of Multi-Type Products,IEEE,Conferences,"A smart manufacturing prototype called iCandy Box, used for hybrid packing of assorted candies, was designed to study and verify the cyber-physical control methods. The prototype is aimed to provide personalized consumption, as it can perform flexible and customized production. The prototype is powered by a cloud-edge-end enabled collaborative information framework, which can support both industrial big data and artificial intelligence applications. Furthermore, it is characterized by modularization and interdisciplinarity; therefore, it can be used to carry out both experiments and training in several major fields, including smart manufacturing and IoT. The experimental results have shown that the prototype can carry out hybrid production, paving the way for the study and verification of cyber-physical control methods.",https://ieeexplore.ieee.org/document/9525542/,2021 International Conference on Artificial Intelligence and Electromechanical Automation (AIEA),14-16 May 2021,ieeexplore
10.1109/CDC.2001.980681,Design and implementation of industrial neural network controller using backstepping,IEEE,Conferences,"A novel neural network (NN) backstepping controller is modified for application to an industrial motor drive system. A control system structure and NN tuning algorithms are presented that are shown to guarantee the stability and performance of the closed-loop system. The NN backstepping controller is implemented on an actual motor drive system using a two-PC control system developed at the authors' university. The implementation results show that the NN backstepping controller is highly effective in controlling the industrial motor drive system. It is also shown that the NN controller gives better results on actual systems than a standard backstepping controller developed assuming full knowledge of the dynamics. Moreover, the NN controller does not require the linear-in-the-parameters assumption or the computation of regression matrices required by standard backstepping.",https://ieeexplore.ieee.org/document/980681/,Proceedings of the 40th IEEE Conference on Decision and Control (Cat. No.01CH37228),4-7 Dec. 2001,ieeexplore
10.1109/ICCNEA.2017.25,Design of Multi-channel Temperature Control Inspection System Based on PLC,IEEE,Conferences,"The temperature control system is widely used in the field of industrial control, such as the boiler's temperature control system in Steel, chemical plants and thermal power plants. For the requirements of remote centralized management and security monitor in temperature control system, a temperature control inspection system consisted by down-computer clew and up-computer, is designed in this paper. In this system, a programmable logic controller (PLC) is use as up-computer, multiple AI smart meters are use as down-computer clew. The structure of the system hardware and the interconnection of the various parts are introduced simply, the design and implementation of communication system of down-computer is elaborated in detail, and the part of the communication system program is given. The actual operation shows that the remote monitoring function can be realized and design requirements be satisfied by the application of intelligent instruments of real-time collection, processing and feedback on the site temperature, and high efficiency, high universality and reliable stability are the advantages of the system.",https://ieeexplore.ieee.org/document/8128601/,"2017 International Conference on Computer Network, Electronic and Automation (ICCNEA)",23-25 Sept. 2017,ieeexplore
10.1109/IAEAC50856.2021.9390638,Design of Wireless Temperature Acquisition System,IEEE,Conferences,"With the development of production technology, temperature is particularly important in modern industrial control. Measuring temperature parameters and analyzing them can play a role in real-time monitoring of equipment operation status, production environment and other external environment, so as to ensure equipment safety and efficient production of the whole production line. As an indispensable element of automated production lines, sensor is developed while also facing challenges. For demand of industrial automation persecution, sensors continuously improve measurement capability. This paper presents an idea, that designing correction algorithm in software to improve the accuracy of the temperature sensor, which is reduce the updated hardware investment.",https://ieeexplore.ieee.org/document/9390638/,"2021 IEEE 5th Advanced Information Technology, Electronic and Automation Control Conference (IAEAC)",12-14 March 2021,ieeexplore
10.1109/ICSSA.2016.19,Detection of Man-in-the-Middle Attacks on Industrial Control Networks,IEEE,Conferences,"In this paper we present a method to detect Man-in-the-Middle attacks on industrial control systems. The approach uses anomaly detection by developing a model of normal behaviour of the industrial control system network. To come as close as possible to reality a simple industrial system, a conveyor belt with sensors and actuators, was set up with controllers widely used in industry. A machine learning approach based on the k-Nearest Neighbors algorithm with Bregman divergence was used to define a model of normal (valid) behaviour. Afterwards Man-in-the-Middle attacks were launched against the system and its behaviour during the attack was compared to the valid behaviour model. The results show that the approach taken was able to detect such attacks with satisfactory accuracy.",https://ieeexplore.ieee.org/document/7861654/,2016 International Conference on Software Security and Assurance (ICSSA),24-25 Aug. 2016,ieeexplore
10.1109/CSNT.2017.8418538,Detection of attacks in IoT based on ontology using SPARQL,IEEE,Conferences,"Nowadays, the concept of Internet of Things (IoT) has been manifested into reality with the help of latest developments or transformations in hardware circuitry, devices and protocols. IoT is such a diversified field in which a lot of challenges are faced during implementations of IoT applications including smart cities, smart homes, industrial sectors etc. The current scenario is highly demanding for deployment of smart sensors into existing applications to deliver a fully automated system. The major issue faced by IoT's existing system is security issue. In this paper, various attacks in IoT systems has discussed and focuses on ontology based model to deal with various attacks.",https://ieeexplore.ieee.org/document/8418538/,2017 7th International Conference on Communication Systems and Network Technologies (CSNT),11-13 Nov. 2017,ieeexplore
10.1109/ICAMechS49982.2020.9310079,Developing Robotic System for Harvesting Pineapples,IEEE,Conferences,"This paper develops a robotic system to harvest pineapple autonomously. The system contains a machine vision unit, two robotic manipulators mounted on a platform, custom end-effectors, and an image-based harvesting control unit. The manipulators with Gantry 3DOF PPP configuration are geometrically optimized to move the end-effectors approaching pineapples. Each end-effector is actuated by pneumatic actuator and equipped with a cage-shaped gripper to fix the selective pineapple inside and a cutting device to cut its stalk. YOLOv3 approach is implemented for detecting and recognizing pineapple fruits that meet requirements for harvest. The experiment results demonstrate the success of pineapple recognition with 90.82% mAP. The 3D position of the recognized pineapples will be calculated and sent to the control system. The control system, including an industrial computer communicating with PLCs to conducts the manipulators and end-effectors to approach and d the recognized pineapples. The complete system has been tested on the experimental field-model. The success rate of pineapple harvesting is 95.55% and the average time is 12 seconds per one fruit. In the future, this system will be improved for automatic harvesting in real pineapple fields.",https://ieeexplore.ieee.org/document/9310079/,2020 International Conference on Advanced Mechatronic Systems (ICAMechS),10-13 Dec. 2020,ieeexplore
10.1109/ICBAIE52039.2021.9389863,Development and application of low-latency edge IoT agent device for ubiquitous power Internet of Things,IEEE,Conferences,"With the development of smart grid, IOT devices in the field of power control are widely used. The diversity of types, features and functions of IOT devices not only integrates detailed big data, but also challenges the real-time, compatibility and big data processing capability of system control. In this paper, the low-latency edge IoT agent device for ubiquitous power Internet of Things is studied, which performs nearby processing of massive intelligent electricity data to avoid the bottleneck of communication channel caused by massive data transmission, and meet the special requirements of energy management and control for quick response and accurate execution. The software and hardware decoupling and deep programmable ideas are proposed. In addition, adopting a common hardware platform and resource virtualization function platform to realize low latency guarantee mechanism and mode from three levels. Moreover, the low latency energy industrial application scenarios are selected to demonstrate the application, and the proposed scheme is proved to provide safe and fast control methods, and also make the system have stronger big data collection capability of electricity consumption information.",https://ieeexplore.ieee.org/document/9389863/,"2021 IEEE 2nd International Conference on Big Data, Artificial Intelligence and Internet of Things Engineering (ICBAIE)",26-28 March 2021,ieeexplore
10.1109/FarEastCon.2018.8602651,Development of a Transport Robot for Automated Warehouses,IEEE,Conferences,"Industrial robots and manipulators are widely used as transport-loading devices in automated production. It is possible to combine equipment into coordinated production complexes of various sizes with the help of robots and they will not be bound by rigid planning and the number of installed units. Transport robots have proven themselves as flexible automated means of realizing intra-shop and interoperation material connections. More and more companies are developing technologies for vehicles through which they can communicate with each other and use real-time data from production infrastructure facilities. Electric vehicles and unmanned vehicles have become a new technological trend. In this regard, the paper deals with a prototype of an innovative transport robot, created for automated warehouses. It is proposed to use a computer vision system with image recognition based on the embedded software for the transport robot positioning inside the production facilities. The algorithm of deep machine learning was adapted to solve this problem. Using this algorithm, the prototype tests were performed successfully.",https://ieeexplore.ieee.org/document/8602651/,2018 International Multi-Conference on Industrial Engineering and Modern Technologies (FarEastCon),3-4 Oct. 2018,ieeexplore
10.1109/ICMLC.2010.5580600,Development of a distributed control system for PLC-based applications,IEEE,Conferences,"This paper presents experiments aims to built a platform to monitor and control PLC-based processes over PROFIBUS-DP network. The platform is built using industry-standard off-the-shelf PLCs. Integrated with each PLC are communication processors that can be used for connectivity to the network and to a DP modem. The communication processor module used in this work provides an industrial compatible protocol over PROFIBUS-DP. The mode of communication of the industrial automation control system base on the PROFIBUS agreement where variable frequency drives and motor control centers can incorporate bus technologies, and developed a variable-frequency speed base on PROFIBUS-DP. The system can real-time control and collect data from diversified equipment, and the data get across the PROFIBUS is carried to industrial control computer. After data analysis, system carry on the real-time monitoring to the scene, and realize the automation control. This system is very important to study bus, distributed system, the frequency conversion velocity and PLC. It is channel to research on automation control system.",https://ieeexplore.ieee.org/document/5580600/,2010 International Conference on Machine Learning and Cybernetics,11-14 July 2010,ieeexplore
10.1109/IAS.2004.1348846,Development of a self-tuned neuro-fuzzy controller for induction motor drives,IEEE,Conferences,"In This work a novel adaptive neuro-fuzzy (NF) based speed control of an induction motor (IM) is presented. The proposed neuro-fuzzy controller (NFC) incorporates fuzzy logic laws with a five-layer artificial neural network (ANN) scheme. In this controller only three membership functions are used for each input keeping in mind for low computational burden, which will be suitable for real-time implementation. Furthermore, for the proposed NFC an improved self-tuning method is developed based on the IM theory and its high performance requirements. The main task of the tuning method is to adjust the parameters of the fuzzy logic controller (FLC) in order to minimize the square of the error between actual and reference outputs. This work also demonstrates how the proposed NFC can easily be adjusted to work with different size of induction motors. A complete simulation model for indirect field oriented control of IM incorporating the proposed NFC is developed. The performance of the proposed NFC based IM drive is investigated extensively at different operating conditions in simulation. In order to prove the superiority of the proposed NFC, the results for the proposed controller are also compared to those obtained by a conventional PI controller. The proposed NFC based IM drive is found to be more robust as compared to conventional PI controller based drive and hence found suitable for high performance industrial drive applications.",https://ieeexplore.ieee.org/document/1348846/,"Conference Record of the 2004 IEEE Industry Applications Conference, 2004. 39th IAS Annual Meeting.",3-7 Oct. 2004,ieeexplore
10.1109/TELFOR48224.2019.8971360,Development of intelligent systems and application of gamification in artificial intelligent learning,IEEE,Conferences,"CToday, intelligent systems are used in many fields - medicine, agriculture, transport, telecommunications, industrial process management and control, finance, commerce, the computer game industry and many others. This paper describes a complete way to develop an intelligent software system for simulation and visualization of artificial intelligence algorithms. The system includes artificial intelligence algorithms from basic search strategies and game theory, inference algorithms and knowledge representation models, to advanced search techniques, machine and inductive learning. The application of these algorithms to real everyday problems and the application of this software system as an auxiliary tool for analysis of input data and inference in various fields are presented. The system is also applicable to education in an introductory artificial intelligence course at the university, so the last phase of the research involved the transition of the software system into a game-based tool and application of gamification.",https://ieeexplore.ieee.org/document/8971360/,2019 27th Telecommunications Forum (TELFOR),26-27 Nov. 2019,ieeexplore
10.1109/DEVLRN.2018.8761037,Developmental Bayesian Optimization of Black-Box with Visual Similarity-Based Transfer Learning,IEEE,Conferences,"We present a developmental framework based on a long-term memory and reasoning mechanisms (Vision Similarity and Bayesian Optimisation). This architecture allows a robot to optimize autonomously hyper-parameters that need to be tuned from any action and/or vision module, treated as a black-box. The learning can take advantage of past experiences (stored in the episodic and procedural memories) in order to warm-start the exploration using a set of hyper-parameters previously optimized from objects similar to the new unknown one (stored in a semantic memory). As example, the system has been used to optimized 9 continuous hyper-parameters of a professional software (Kamido) both in simulation and with a real robot (industrial robotic arm Fanuc) with a total of 13 different objects. The robot is able to find a good object-specific optimization in 68 (simulation) or 40 (real) trials. In simulation, we demonstrate the benefit of the transfer learning based on visual similarity, as opposed to an amnesic learning (i.e. learning from scratch all the time). Moreover, with the real robot, we show that the method consistently outperforms the manual optimization from an expert with less than 2 hours of training time to achieve more than 88% of success.",https://ieeexplore.ieee.org/document/8761037/,2018 Joint IEEE 8th International Conference on Development and Learning and Epigenetic Robotics (ICDL-EpiRob),17-20 Sept. 2018,ieeexplore
10.1109/SPW.2019.00040,Devil in the Detail: Attack Scenarios in Industrial Applications,IEEE,Conferences,"In the past years, industrial networks have become increasingly interconnected and opened to private or public networks. This leads to an increase in efficiency and manageability, but also increases the attack surface. Industrial networks often consist of legacy systems that have not been designed with security in mind. In the last decade, an increase in attacks on cyber-physical systems was observed, with drastic consequences on the physical work. In this work, attack vectors on industrial networks are categorised. A real-world process is simulated, attacks are then introduced. Finally, two machine learning-based methods for time series anomaly detection are employed to detect the attacks. Matrix Profiles are employed more successfully than a predictor Long Short-Term Memory network, a class of neural networks.",https://ieeexplore.ieee.org/document/8844618/,2019 IEEE Security and Privacy Workshops (SPW),19-23 May 2019,ieeexplore
10.1109/BIGCOMP.2019.8679267,Diagnosis of Corporate Insolvency Using Massive News Articles for Credit Management,IEEE,Conferences,"In the aftermath of the 4th Industrial Revolution, AI and Big data technology have been used in various fields in South Korea, and the techniques are being applied to and complemented in various service fields which were implemented without them before. Especially, in order to secure credit stability for borrowed companies from financial institutions and to preemptively respond to the risks about-by means of online news articles and SNS data-the attempts to forecast the possibility of insolvency and adopt them into actual business are actively conducted by major domestic banks. In this study, we describe several analytical methods, outputs, and problems that are encountered during the processes of developing the unstructured text-based prediction system to detect the possibility of corporate insolvency-which ordered by a national government bank and discuss related issues with a real case. As a result, we have implemented an automatic tagger program for labeling largely unlabeled articles, and newly devised a prediction algorithm of the possibility of corporate insolvency. We achieved the accuracy of 92% (AUC 0.96) in aspect of performance and the hit ratio of 50% among the number of predicted 26 candidates that have the possibility of insolvency. Thus, the result of our study is revealed to be complementary to the financial data analysis sufficiently in performance, but yet have several limitations such as data coverage, reliability, and the characteristics of Korean language.",https://ieeexplore.ieee.org/document/8679267/,2019 IEEE International Conference on Big Data and Smart Computing (BigComp),27 Feb.-2 March 2019,ieeexplore
10.1109/UPCON47278.2019.8980279,Diagnosis of Induction Motor Faults Using Frequency Occurrence Image Plots—A Deep Learning Approach,IEEE,Conferences,"Accurate diagnosis of induction motor faults is important for reliable and safe operation of industrial processes. Majority of the faults which occur in induction motors are mainly diagnosed using motor current signature analysis. However, the accuracy of fault detection depends on selection of suitable features from motor current, the failure of which may result in incorrect interpretation. Considering the aforesaid fact, this paper presents an image processing aided deep learning framework for reliable diagnosis of induction motor faults, which eliminates the need of separate feature extraction stage. To this end, the motor current signals under different types of fault conditions were procured and were subsequently processed into frequency occurrence plots. The frequency occurrence image plots for different fault scenarios were finally used as inputs to a deep convolution neural network for the purpose of classification. Transfer learning technique was adopted to reduce the computation time of Convolution Neural Network and classification of motor faults was done at five different loading conditions. Four types of classification tasks have been addressed here and comprehensive analysis was done using a variety of CNN architectures. It has been observed that the proposed method returns a highest mean classification accuracy of 96.67% in segregating different types of faults which can be implemented in real-life for condition monitoring of induction motors.",https://ieeexplore.ieee.org/document/8980279/,"2019 International Conference on Electrical, Electronics and Computer Engineering (UPCON)",8-10 Nov. 2019,ieeexplore
10.1109/CMPEUR.1992.218438,Diagnosis using neural nets,IEEE,Conferences,"Diagnosis is an important part of the product quality assurance cycle. The construction of diagnosis systems with neural nets, which solves problems hitherto unsolved, creates effective real-time opportunities, and a new level of robustness is discussed. The application of neural nets is illustrated in three examples. The first example shows how the effect of medication can be diagnosed. The other two involve automotive applications. They were derived from industrial products created by Expert Informatik GmbH over the years 1989 to 1992.&lt;<ETX>&gt;</ETX>",https://ieeexplore.ieee.org/document/218438/,CompEuro 1992 Proceedings Computer Systems and Software Engineering,4-8 May 1992,ieeexplore
10.1109/IAI50351.2020.9262203,Digital Twin Enabled Smart Control Engineering as an Industrial AI: A New Framework and Case Study,IEEE,Conferences,"In Industry 4.0, the increasing complexity of industrial systems introduces unknown dynamics that affect the performance of manufacturing processes. Thus, Digital Twin appears as a breaking technology to develop virtual representations of any complex system design, analysis, and behavior prediction tasks to enhance the system understanding via enabling capabilities like real-time analytics, or Smart Control Engineering. In this paper, a novel framework is proposed for the design and implementation of Digital Twin applications to the development of Smart Control Engineering. The framework involve the steps of system documentation, Multidomain Simulation, Behavioral Matching, and real-time monitoring, which is applied to develop the Digital Twin for a real-time vision feedback temperature uniformity control. The obtained results show that Digital Twin is a fundamental part of the transformation into Industry 4.0.",https://ieeexplore.ieee.org/document/9262203/,2020 2nd International Conference on Industrial Artificial Intelligence (IAI),23-25 Oct. 2020,ieeexplore
10.1109/ICVES.2009.5400189,Digital implementation of fuzzy logic controller for wide range speed control of brushless DC motor,IEEE,Conferences,"The brushless DC motors find wide applications such as in battery operated vehicles, wheel chairs, automotive fuel pumps, robotics, machine tools, aerospace and in many industrial applications due to their superior electrical and mechanical characteristics and its capability to operate in hazardous environment. Conventional controllers fail to yield desired performance in BLDC motor control systems due to the non-linearity arising out of variation in the system parameters and change in load. The main focus is now on the application of artificial intelligent techniques such as fuzzy logic to solve this problem. Another great challenge is to reduce the size and cost of the drive system without compromising the performance. In this paper, the design and digital implementation of fuzzy logic controller using a versatile ADUC812 microcontroller, and low-cost, compact, superior performance components are used in order to reduce the cost and size of the drive system. The experimental results are presented to prove the flexibility of the control scheme in real time.",https://ieeexplore.ieee.org/document/5400189/,2009 IEEE International Conference on Vehicular Electronics and Safety (ICVES),11-12 Nov. 2009,ieeexplore
10.1109/AIKE.2018.00042,Distributed Osmotic Computing Approach to Implementation of Explainable Predictive Deep Learning at Industrial IoT Network Edges with Real-Time Adaptive Wavelet Graphs,IEEE,Conferences,"Challenges associated with developing analytics solutions at the edge of large scale Industrial Internet of Things (IIoT) networks close to where data is being generated in most cases involves developing analytics solutions from ground up. However, this approach increases IoT development costs and system complexities, delay time to market, and ultimately lowers competitive advantages associated with delivering next-generation IoT designs. To overcome these challenges, existing, widely available, hardware can be utilized to successfully participate in distributed edge computing for IIoT systems. In this paper, an osmotic computing approach is used to illustrate how distributed osmotic computing and existing low-cost hardware may be utilized to solve complex, compute-intensive Explainable Artificial Intelligence (XAI) deep learning problem from the edge, through the fog, to the network cloud layer of IIoT systems. At the edge layer, the C28x digital signal processor (DSP), an existing low-cost, embedded, real-time DSP that has very wide deployment and integration in several IoT industries is used as a case study for constructing real-time graph-based Coiflet wavelets that could be used for several analytic applications including deep learning pre-processing applications at the edge and fog layers of IIoT networks. Our implementation is the first known application of the fixed-point C28x DSP to construct Coiflet wavelets. Coiflet Wavelets are constructed in the form of an osmotic microservice, using embedded low-level machine language to program the C28x at the network edge. With the graph-based approach, it is shown that an entire Coiflet wavelet distribution could be generated from only one wavelet stored in the C28x based edge device, and this could lead to significant savings in memory at the edge of IoT networks. Pearson correlation coefficient is used to select an edge generated Coiflet wavelet and the selected wavelet is used at the fog layer for pre-processing and denoising IIoT data to improve data quality for fog layer based deep learning application. Parameters for implementing deep learning at the fog layer using LSTM networks have been determined in the cloud. For XAI, communication network noise is shown to have significant impact on results of predictive deep learning at IIoT network fog layer.",https://ieeexplore.ieee.org/document/8527474/,2018 IEEE First International Conference on Artificial Intelligence and Knowledge Engineering (AIKE),26-28 Sept. 2018,ieeexplore
10.1109/ICARCV.2014.7064599,Distributed signature analysis of induction motors using Artificial Neural Networks,IEEE,Conferences,Motor current signature analysis is a modern approach to fault diagnose and classification for induction motors. Many studies reported successful implementation of MCSA in laboratory situations whereas the method was not so successful in real industrial situation due to propagation of neighbor faults and unwanted noise signals. This paper investigate the correlation between different observations of events in order to provide a more accurate estimation of behavior of electrical motors at a given site. An analytical framework has been implemented to correlate and classify independent fault observations and diagnose the type and identify the origin of fault symptoms. The fault diagnosis algorithm has two layers. Initially outputs of all sensors are processed to generate fault indicators. These fault indicators then are to be classified using an Artificial Neural Network. A typical industrial site is taken as a case study and simulated to evaluate the concept of distributed fault analysis.,https://ieeexplore.ieee.org/document/7064599/,2014 13th International Conference on Control Automation Robotics & Vision (ICARCV),10-12 Dec. 2014,ieeexplore
10.1109/ICSE-SEIP.2017.8,Domain Adaptation for Test Report Classification in Crowdsourced Testing,IEEE,Conferences,"In crowdsourced testing, it is beneficial to automatically classify the test reports that actually reveal a fault - a true fault, from the large number of test reports submitted by crowd workers. Most of the existing approaches toward this task simply leverage historical data to train a machine learning classifier and classify the new incoming reports. However, our observation on real industrial data reveals that projects under crowdsourced testing come from various domains, and the submitted reports usually contain different technical terms to describe the software behavior for each domain. The different data distribution across domains could significantly degrade the performance of classification models when utilized for cross-domain report classification. To build an effective cross-domain classification model, we leverage deep learning to discover the intermediate representation that is shared across domains, through the co-occurrence between domain-specific terms and domain-unaware terms. Specifically, we use the Stacked Denoising Autoencoders to automatically learn the high-level features from raw textual terms, and utilize these features for classification. Our evaluation on 58 commercial projects of 10 domains from one of the Chinese largest crowdsourced testing platforms shows that our approach can generate promising results, compared to three commonly-used and state-of-the-art baselines. Moreover, we also evaluate its usefulness using real-world case studies. The feedback from real-world testers demonstrates its practical value.",https://ieeexplore.ieee.org/document/7965432/,2017 IEEE/ACM 39th International Conference on Software Engineering: Software Engineering in Practice Track (ICSE-SEIP),20-28 May 2017,ieeexplore
10.1109/PHM-Paris.2019.00054,Domain Adaptive Transfer Learning for Fault Diagnosis,IEEE,Conferences,"Thanks to digitization of industrial assets in fleets, the ambitious goal of transferring fault diagnosis models from one machine to the other has raised great interest. Solving these domain adaptive transfer learning tasks has the potential to save large efforts on manually labeling data and modifying models for new machines in the same fleet. Although data-driven methods have shown great potential in fault diagnosis applications, their ability to generalize on new machines and new working conditions are limited because of their tendency to overfit to the training set in reality. One promising solution to this problem is to use domain adaptation techniques. It aims to improve model performance on the target new machine. Inspired by its successful implementation in computer vision, we introduced Domain-Adversarial Neural Networks (DANN) to our context, along with two other popular methods existing in previous fault diagnosis research. We then carefully justify the applicability of these methods in realistic fault diagnosis settings, and offer a unified experimental protocol for a fair comparison between domain adaptation methods for fault diagnosis problems.",https://ieeexplore.ieee.org/document/8756463/,2019 Prognostics and System Health Management Conference (PHM-Paris),2-5 May 2019,ieeexplore
10.1109/ICIAFS.2007.4544783,Dynamic power management of an embedded sensor network based on actor-critic reinforcement based learning,IEEE,Conferences,"Wireless sensor networks (WSNs) have gained tremendous popularity in recent years due to the wide range of applications envisioned - ranging from aerospace and defense to industrial and commercial. Although limited by communication and energy constraints, the low cost, small sensor nodes lend themselves to be deployed in large numbers to form a network with high spatial distribution. The overall effectiveness of the sensor network depends on how well the mutually contradicting objectives of conserving the limited on-board battery power and keeping the sensors awake for stimuli, are managed. In this paper, we have proposed an actor-critic based reinforcement learning mechanism that can be practically implemented on an embedded sensor with limited memory and processing power. Specifically, the contribution of this paper is the development of the value function (or critic/reinforcement function) that is implemented on each sensor node which aids in dynamic power scheduling based on different situations. The effectiveness of the proposed method has been demonstrated with real world experiments.",https://ieeexplore.ieee.org/document/4544783/,2007 Third International Conference on Information and Automation for Sustainability,4-6 Dec. 2007,ieeexplore
10.1109/ICCT46805.2019.8947193,EDGE AI for Heterogeneous and Massive IoT Networks,IEEE,Conferences,"By combining multiple sensing and wireless access technologies, the Internet of Things (IoT) shall exhibit features with large-scale, massive, and heterogeneous sensors and data. To integrate diverse radio access technologies, we present the architecture of heterogeneous IoT system for smart industrial parks and build an IoT experimental platform. Various sensors are installed on the IoT devices deployed on the experimental platform. To efficiently process the raw sensor data and realize edge artificial intelligence (AI), we describe four statistical features of the raw sensor data that can be effectively extracted and processed at the network edge in real time. The statistical features are calculated and fed into a back-propagation neural network (BPNN) for sensor data classification. By comparing to the k-nearest neighbor classification algorithm, we examine the BPNN-based classification method with a great amount of raw data gathered from various sensors. We evaluate the system performance according to the classification accuracy of BPNN and the performance indicators of the cloud server, which shows that the proposed approach can effectively enable the edge-AI-based heterogeneous IoT system to process the sensor data at the network edge in real time while reducing the demand for computing and network resources of the cloud.",https://ieeexplore.ieee.org/document/8947193/,2019 IEEE 19th International Conference on Communication Technology (ICCT),16-19 Oct. 2019,ieeexplore
10.1109/SECON52354.2021.9491609,EFCam: Configuration-Adaptive Fog-Assisted Wireless Cameras with Reinforcement Learning,IEEE,Conferences,"Visual sensing has been increasingly employed in industrial processes. This paper presents the design and implementation of an industrial wireless camera system, namely, EFCam, which uses low-power wireless communications and edge-fog computing to achieve cordless and energy-efficient visual sensing. The camera performs image pre-processing (i.e., compression or feature extraction) and transmits the data to a resourceful fog node for advanced processing using deep models. EFCam admits dynamic configurations of several parameters that form a configuration space. It aims to adapt the configuration to maintain desired visual sensing performance of the deep model at the fog node with minimum energy consumption of the camera in image capture, pre-processing, and data communications, under dynamic variations of application requirement and wireless channel conditions. However, the adaptation is challenging due primarily to the complex relationships among the involved factors. To address the complexity, we apply deep reinforcement learning to learn the optimal adaptation policy. Extensive evaluation based on trace-driven simulations and experiments show that EFCam complies with the accuracy and latency requirements with lower energy consumption for a real industrial product object tracking application, compared with four baseline approaches incorporating hysteresis-based adaptation.",https://ieeexplore.ieee.org/document/9491609/,"2021 18th Annual IEEE International Conference on Sensing, Communication, and Networking (SECON)",6-9 July 2021,ieeexplore
10.1109/QUATIC.2016.043,Early Diagnostics on Team Communication: Experience-Based Forecasts on Student Software Projects,IEEE,Conferences,"Effective team communication is a prerequisite for software quality and project success. It implies correctly elicited customer requirements, conduction of occurring change requests and to adhere releases. Team communication is a complex construct that consists of numerous characteristics, individual styles, influencing factors and dynamic intensities during a project. These elements are complicated to be measured or scheduled, especially in newly formed teams. According to software developers with few experiences in teams, it would be highly desirable to recognize dysfunctional or underestimated communication behaviors already in early project phases. Otherwise, negative affects may cause delay of releases or even endanger software quality. We introduce an approach on the feasibility of forecasting team's communication behavior in student software projects. We build a very first forecasting model that involves software engineering and industrial psychological terms to extract multi week communication forecasts with accurate results. The model consists of a k-nearest neighbor machine learning algorithm and is trained and evaluated with 34 student software projects from a previously taken field study. This study is an encouraging first step towards forecasting team communication to reveal potential miscommunications during a project. It is our aim to give young software developing teams an experience-based assistance about their information flow and enable adjustment for dysfunctional communication, to avoid fire fighting situation or even risks of alternating software qualities.",https://ieeexplore.ieee.org/document/7814540/,2016 10th International Conference on the Quality of Information and Communications Technology (QUATIC),6-9 Sept. 2016,ieeexplore
10.1109/SNPD.2008.97,Early-Life Cycle Reuse Approach for Component-Based Software of Autonomous Mobile Robot System,IEEE,Conferences,"Applying software reuse to many embedded realtime systems, such as autonomous mobile robot system poses significant challenges to industrial software processes due to the resource-constrained and realtime requirements of the systems. An approach for early life-cycle systematic reuse for component-based software engineering (ELCRA) of autonomous mobile robot software is developed. The approach allows reuse at the early stage of software development process by integrating analysis patterns, component model, and component-oriented programming framework. The results of applying the approach in developing software for real robots show that the strategies and processes proposed in the approach can fulfill requirements for self-contained, platform-independent and real-time predictable mobile robot.",https://ieeexplore.ieee.org/document/4617381/,"2008 Ninth ACIS International Conference on Software Engineering, Artificial Intelligence, Networking, and Parallel/Distributed Computing",6-8 Aug. 2008,ieeexplore
10.1109/CSCloud-EdgeCom49738.2020.00050,Edge Computing-based 3D Pose Estimation and Calibration for Robot Arms,IEEE,Conferences,"Industrial robots are widely used in current production lines, and complex pipeline processes, especially those with different assembly requirements, are designed for intelligent manufacturing in the era of industry 4.0. During the new crown epidemic, a large number of car companies used the production line to transform production of medical materials such as masks and protective clothing, which provided a strong guarantee for fighting the epidemic. In this scenario, a pipeline is often assembled from robotic arms from multiple suppliers. The traditional methods is complex and takes a lot of time. In this paper, we propose a novel deep learning based robot arm 3D pose estimation and calibration model with simple Kinect stereo cameras which can be deployed on light-weight edge computing systems. The light-weight deep CNN model can detection 5 predefined key points based on RGB-D data. In this way, when the assembly line composed of different robot arms needs to be reassembled, our model can quickly provide the robot's pose information without additional tuning processes. Testing in Webots with Rokae xb4 robot arm model shows that our model can quickly estimate the key point of the robot arm.",https://ieeexplore.ieee.org/document/9170983/,2020 7th IEEE International Conference on Cyber Security and Cloud Computing (CSCloud)/2020 6th IEEE International Conference on Edge Computing and Scalable Cloud (EdgeCom),1-3 Aug. 2020,ieeexplore
10.1109/ISAECT50560.2020.9523700,Edge-Cloud Architectures Using UAVs Dedicated To Industrial IoT Monitoring And Control Applications,IEEE,Conferences,"The deployment of new technologies to ease the control and management of a massive data volume and its uncertainty is a very significant challenge in the industry. Under the name ""Smart Factory"", the Industrial Internet of Things (IoT) aims to send data from systems that monitor and control the physical world to data processing systems for which cloud computing has proven to be an important tool to meet processing needs. unmanned aerial vehicles (UAVs) are now being introduced as part of IIoT and can perform important tasks. UAVs are now considered one of the best remote sensing techniques for collecting data over large areas. In the field of fog and edge computing, the IoT gateway connects various objects and sensors to the Internet. It function as a common interface for different networks and support different communication protocols. Edge intelligence is expected to replace Deep Learning (DL) computing in the cloud, providing a variety of distributed, low-latency and reliable intelligent services. In this paper, An unmanned aerial vehicle is automatically integrated into an industrial control system through an IoT gateway platform. Rather than sending photos from the UAV to the cloud for processing, an AI cloud trained model is deployed in the IoT gateway and used to process the taken photos. This model is designed to overcome the latency channels of the cloud computing architecture. The results show that the monitoring and tracking process using advanced computing in the IoT gateway is significantly faster than in the cloud.",https://ieeexplore.ieee.org/document/9523700/,2020 International Symposium on Advanced Electrical and Communication Technologies (ISAECT),25-27 Nov. 2020,ieeexplore
10.1109/AEMCSE50948.2020.00057,Effective Risk Prediction of Tailings Ponds Using Machine Learning,IEEE,Conferences,"A tailings pond is a place for storing industrial waste, which is a major hazard source with high potential energy. The stability of the tailings dam is usually evaluated by saturation line height. The measuring sensors for saturation line are very expensive and have poor lightning protection abilities. To solve this problem, this paper takes Jiande tailings pond as the study area. Machine learning models are built after integrating various sensor monitoring data, relying on two sensors to accurately predict the saturation line height to predict the safety of tailings pond. The average R2 of real-time regression of saturation line is 99.16%, and average accuracy of real-time warning is 99.89%.",https://ieeexplore.ieee.org/document/9131337/,"2020 3rd International Conference on Advanced Electronic Materials, Computers and Software Engineering (AEMCSE)",24-26 April 2020,ieeexplore
10.1109/ICEETS.2016.7583860,Efficiency optimization of induction motor drive using Artificial Neural Network,IEEE,Conferences,"Induction motors are the workhorse of industry, have good efficiency at rated load, but long duration usage of IM at partial load shows poor efficiency which leads to waste in energy and revenue as well. These motors are reliable, robust, high power/mass ratio and economic, hence replaced all other motors in the industry, so even minute increment in induction motor efficiency can have a major impact on consumption of electricity and saving of revenue, globally. This paper utilizes, a combination of two key concepts of efficiency optimization-loss model control (LMC) and search control (SC) for efficient operation of induction motors used in various industrial applications, in aforesaid load condition. At first, to estimate optimal I<sub>ds</sub> values for various load conditions, an optimal I<sub>ds</sub> expression in terms of machine parameters and load parameters, based on machine loss model in d-q frame along with classical optimization technique, is utilized. Secondly, an offline trained artificial neural network (ANN) controller is used to reproduce the optimal I<sub>ds</sub> values, in run-time load condition. This eliminates run-time computations and perturbation for optimal flux, as in conventional SC method. The (ANN) optimal controller is designed for optimal I<sub>ds</sub> as output, while providing load torque and speed information as inputs. The training is performed in MATLAB and good accuracy of the training model is seen. Dynamic and steady-state performances are compared for proposed optimal (optimal I<sub>ds</sub>) operations and conventional vector operations (constant I<sub>ds</sub>), with the help of a simulation model, developed in MATLAB. Excellent dynamic response in load transients as well as superior efficiency performance (1- 18%) at steady-state, for a wide range of speed and torque in simulation is attained. Assimilated with similar earlier work, the proposed methodology offers effortless implementation in real-time industrial facilities, ripple free operations, fast response and higher energy savings.",https://ieeexplore.ieee.org/document/7583860/,2016 International Conference on Energy Efficient Technologies for Sustainability (ICEETS),7-8 April 2016,ieeexplore
10.1109/ICSME46990.2020.00082,Efficient Bug Triage For Industrial Environments,IEEE,Conferences,"Bug triage is an important task for software maintenance, especially in the industrial environment, where timely bug fixing is critical for customer experience. This process is usually done manually and often takes significant time. In this paper, we propose a machine-learning-based solution to address the problem efficiently. We argue that in the industrial environment, it is more suitable to assign bugs to software components (then to responsible developers) than to developers directly. Because developers can change their roles in industry, they may not oversee the same software module as before. We also demonstrate experimentally that assigning bugs to components rather than developers leads to much higher accuracy. Our solution is based on text-projection features extracted from bug descriptions. We use a Deep Neural Network to train the classification model. The proposed solution achieves state-of-the-art performance based on extensive experiments using multiple data sets. Moreover, our solution is computationally efficient and runs in near real-time.",https://ieeexplore.ieee.org/document/9240673/,2020 IEEE International Conference on Software Maintenance and Evolution (ICSME),28 Sept.-2 Oct. 2020,ieeexplore
10.1109/ICICT50816.2021.9358469,Efficient Fault Isolation Method to Monitor Industrial Batch Processes,IEEE,Conferences,Industrial batch processes are very popular manufacturing system with large number of process variables involved. Monitoring of batch processes using statistical process monitoring becomes very difficult in view of the complex correlations between the process variables. This paper focuses on a fault isolation based process monitoring method without prior information of fault where fault isolation problem is converted into a variable selection. Variable selection is a learning algorithm used here to solve the problem of selection and isolation of variables from a model. The method discussed here uses a sparse coefficient based dissimilarity analysis algorithm known as Sparse Dissimilarity Algorithm(SDISSIM) which checks a calculated D-index for identifying fault in the process. A sparse coefficient is tabulated to verify the process variables contributing to the fault and an absolute variance difference is calculated to select the variables for fault isolation. Finally SDISSIM method is explained by successful implementation in MATLAB with real time industrial process data.,https://ieeexplore.ieee.org/document/9358469/,2021 6th International Conference on Inventive Computation Technologies (ICICT),20-22 Jan. 2021,ieeexplore
10.1109/CASE48305.2020.9249228,Efficiently Learning a Distributed Control Policy in Cyber-Physical Production Systems Via Simulation Optimization,IEEE,Conferences,"The manufacturing industry is becoming more dynamic than ever. The limitations of non-deterministic network delays and real-time requirements call for decentralized control. For such dynamic and complex systems, learning methods stand out as a transformational technology to have a more flexible control solution. Using simulation for learning enables the description of highly dynamic systems and provides samples without occupying a real facility. However, it requires prohibitively expensive computation. In this paper, we argue that simulation optimization is a powerful tool that can be applied to various simulation-based learning processes for tremendous effects. We proposed an efficient policy learning framework, ROSA (Reinforcement-learning enhanced by Optimal Simulation Allocation), with unprecedented integration of learning, control, and simulation optimization techniques, which can drastically improve the efficiency of policy learning in a cyber-physical system. A proof-of-concept is implemented on a conveyer-switch network, demonstrating how ROSA can be applied for efficient policy learning, with an emphasis on the industrial distributed control system.",https://ieeexplore.ieee.org/document/9249228/,2020 IEEE 16th International Conference on Automation Science and Engineering (CASE),20-21 Aug. 2020,ieeexplore
10.1109/CIMSA.2003.1227220,Embedded e-diagnostic for distributed industrial machinery,IEEE,Conferences,"Industrial process machine failure often causes severe financial implications. This is compounded by the lack of availability of experts and the complications of getting them to site. One solution is to give the expert access to the machine remotely with the addition of an Artificial Intelligence (AI) based diagnostics software to assist with the decision making process. Our research is based on such a system, which combines modern communications with intelligent diagnostics software. Accessibility to process machines can now be global with the promise of predictability to the diagnosis. It is felt the importance of this research work cannot be overstated with the constantly moving worldwide manufacturing base and the real situation of the machine designers being based in a different country to their customer. The most vulnerable areas of a machine are its parts that consist of electro-mechanical actuation. The author utilises conventional Newtonian physics and differential calculus to model these and an AI technique of fault prediction and detection.",https://ieeexplore.ieee.org/document/1227220/,"The 3rd International Workshop on Scientific Use of Submarine Cables and Related Technologies, 2003.",31-31 July 2003,ieeexplore
10.1109/MARK.2011.6046555,Enabling hazard identification from requirements and reuse-oriented HAZOP analysis,IEEE,Conferences,"The capability to identify potential system hazards and operability problems, and to recommend appropriate mitigation mechanisms is vital to the development of safety critical embedded systems. Hazard and Operability (HAZOP) analysis which is mostly used to achieve these objectives is a complex and largely human-centred process, and increased tool support could reduce costs and improve quality. This work presents a framework and tool prototype that facilitates the early identification of potential system hazards from requirements and the reuse of previous experience for conducting HAZOP. The results from the preliminary evaluation of the tool suggest its potential viability for application in real industrial context.",https://ieeexplore.ieee.org/document/6046555/,2011 4th International Workshop on Managing Requirements Knowledge,30-30 Aug. 2011,ieeexplore
10.1109/IJCNN52387.2021.9533808,End-to-End Federated Learning for Autonomous Driving Vehicles,IEEE,Conferences,"In recent years, with the development of computation capability in devices, companies are eager to investigate and utilize suitable ML/DL methods to improve their service quality. However, with the traditional learning strategy, companies need to first build up a powerful data center to collect and analyze data from the edge and then perform centralized model training, which turns out to be inefficient. Federated Learning has been introduced to solve this challenge. Because of its characteristics such as model-only exchange and parallel training, the technique can not only preserve user data privacy but also accelerate model training speed. The method can easily handle real-time data generated from the edge without taking up a lot of valuable network transmission resources. In this paper, we introduce an approach to end-to-end on-device Machine Learning by utilizing Federated Learning. We validate our approach with an important industrial use case in the field of autonomous driving vehicles, the wheel steering angle prediction. Our results show that Federated Learning can significantly improve the quality of local edge models and also reach the same accuracy level as compared to the traditional centralized Machine Learning approach without its negative effects. Furthermore, Federated Learning can accelerate model training speed and reduce the communication overhead, which proves that this approach has great strength when deploying ML/DL components to various real-world embedded systems.",https://ieeexplore.ieee.org/document/9533808/,2021 International Joint Conference on Neural Networks (IJCNN),18-22 July 2021,ieeexplore
10.1109/ICECCO48375.2019.9043233,Enhanced Decision Tree-J48 With SMOTE Machine Learning Algorithm for Effective Botnet Detection in Imbalance Dataset,IEEE,Conferences,"Botnet is one of the major security threats in the field of information technology today (IT). The increase in the rate of attack on industrial IT infrastructures, theft of personal data and attacks on financial information is becoming critical. Majority of available dataset for botnet detection are very old and may not be able to stand the present reality in this research area. One of the latest dataset from Canadian Institute of Cyber Security labeled “CICIDS2017” was noted as an imbalance data distribution ratio of 99% to 1%. This distribution represents majority to minority class ratio. This may pose a challenge of over-fitting in majority class to the research and create a bias in the analysis of results. This research work has adopted J48 decision tree machine learning algorithm with application of SMOTE technique in solving the problem of imbalance dataset, thereby leading to an improved detection of botnets. The accuracy of the highest scenario was 99.95%. This is a significant improvement in detection rate compare to the previous research work.",https://ieeexplore.ieee.org/document/9043233/,"2019 15th International Conference on Electronics, Computer and Computation (ICECCO)",10-12 Dec. 2019,ieeexplore
10.1109/ICSE43902.2021.00120,Enhancing Genetic Improvement of Software with Regression Test Selection,IEEE,Conferences,"Genetic improvement uses artificial intelligence to automatically improve software with respect to non-functional properties (AI for SE). In this paper, we propose the use of existing software engineering best practice to enhance Genetic Improvement (SE for AI). We conjecture that existing Regression Test Selection (RTS) techniques (which have been proven to be efficient and effective) can and should be used as a core component of the GI search process for maximising its effectiveness. To assess our idea, we have carried out a thorough empirical study assessing the use of both dynamic and static RTS techniques with GI to improve seven real-world software programs. The results of our empirical evaluation show that incorporation of RTS within GI significantly speeds up the whole GI process, making it up to 78% faster on our benchmark set, being still able to produce valid software improvements. Our findings are significant in that they can save hours to days of computational time, and can facilitate the uptake of GI in an industrial setting, by significantly reducing the time for the developer to receive feedback from such an automated technique. Therefore, we recommend the use of RTS in future test-based automated software improvement work. Finally, we hope this successful application of SE for AI will encourage other researchers to investigate further applications in this area.",https://ieeexplore.ieee.org/document/9401972/,2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE),22-30 May 2021,ieeexplore
10.1109/BICTA.2010.5645288,Enhancing MOEA with component-emphasizing mechanism for multi-objective optimization,IEEE,Conferences,"Multi-objective optimization is an important and challenging topic in the field of industrial design and scientific research because real-world problems usually involve several conflicting objectives. Since a multi-objective evolutionary algorithms (MOEA) is able to obtain an approximation to the Pareto optimal set and provide substantial information of the tradeoff between objectives, it is becoming one of the most successful methods for multi-objective optimization. Usually, an MOEA generates new trial solutions (offspring) with some candidate decision vectors (parents) to search for the promising areas and make the population evolve towards the Pareto optimal set. Moreover, in the reproduction procedure, most of MOEAs view the decision vector as a whole, and do not recognize the effects of a single component on the new trial solutions. In this paper, we propose the component-emphasizing mechanism for enhancing the search ability of MOEAs. In this mechanism, each component of a decision vector is viewed as an independent factor affecting the quality of the solution. Based on the mechanism, a new MOEA is presented. Finally, the performance of this new algorithm is compared with two other promising MOEAs, namely, NSGA-II and GDE3, on a set of test instances. The experimental results have shown that the proposed algorithm outperforms the others in solution quality and time cost.",https://ieeexplore.ieee.org/document/5645288/,2010 IEEE Fifth International Conference on Bio-Inspired Computing: Theories and Applications (BIC-TA),23-26 Sept. 2010,ieeexplore
10.1109/ICECS.2010.5724589,Estimating design quality of digital systems via machine learning,IEEE,Conferences,"Although the term design quality of digital systems can be assessed from many aspects, the distribution and density of bugs are two decisive factors. This paper presents the application of machine learning techniques to model the relationship between specified metrics of high-level design and its associated bug information. By employing the project repository (i.e., high level design and bug repository), the resultant models can be used to estimate the quality of associated designs, which is very beneficial for design, verification and even maintenance processes of digital systems. A real industrial microprocessor is employed to validate our approach. We hope that our work can shed some light on the application of software techniques to help improve the reliability of various digital designs.",https://ieeexplore.ieee.org/document/5724589/,"2010 17th IEEE International Conference on Electronics, Circuits and Systems",12-15 Dec. 2010,ieeexplore
10.1109/IPIN.2018.8533862,Evaluation Criteria for Inside-Out Indoor Positioning Systems Based on Machine Learning,IEEE,Conferences,"Real-time tracking allows to trace goods and enables the optimization of logistics processes in many application areas. Camera-based inside-out tracking that uses an infrastructure of fixed and known markers is costly as the markers need to be installed and maintained in the environment. Instead, systems that use natural markers suffer from changes in the physical environment. Recently a number of approaches based on machine learning (ML) aim to address such issues. This paper proposes evaluation criteria that consider algorithmic properties of ML-based positioning schemes and introduces a dataset from an indoor warehouse scenario to evaluate for them. Our dataset consists of images labeled with millimeter precise positions that allows for a better development and performance evaluation of learning algorithms. This allows an evaluation of machine learning algorithms for monocular optical positioning in a realistic indoor position application for the first time. We also show the feasibility of ML-based positioning schemes for an industrial deployment.",https://ieeexplore.ieee.org/document/8533862/,2018 International Conference on Indoor Positioning and Indoor Navigation (IPIN),24-27 Sept. 2018,ieeexplore
10.1109/WCICSS.2015.7420323,Evolving decision trees to detect anomalies in recurrent ICS networks,IEEE,Conferences,"Researchers have previously attempted to apply machine learning techniques to network anomaly detection problems. Due to the staggering amount of variety that can occur in normal networks, as well as the difficulty in capturing realistic data sets for supervised learning or testing, the results have often been underwhelming. These challenges are far less pronounced when considering industrial control system (ICS) networks. The recurrent nature of these networks results in less noise and more consistent patterns for a machine learning algorithm to recognize. We propose a method of evolving decision trees through genetic programming (GP) in order to detect network anomalies, such as device outages. Our approach extracts over a dozen features from network packet captures and netflows, normalizes them, and relates them in decision trees using fuzzy logic operators. We used the trees to detect three specific network events from three different points on the network across a statistically significant number of runs and achieved 100% accuracy on five of the nine experiments. When the trees attempted to detect more challenging events at points of presence further from the occurrence, the accuracy averaged to above 98%. On cases where the trees were many hops away and not enough information was available, the accuracy dipped to roughly 50%, or that of a random search. Using our method, all of the evolutionary cycles of the GP algorithm are computed a-priori, allowing the best resultant trees to be deployed as semi-real-time sensors with little overhead. In order for the trees to perform optimally, buffered packets and flows need to be ingested at twenty minute intervals.",https://ieeexplore.ieee.org/document/7420323/,2015 World Congress on Industrial Control Systems Security (WCICSS),14-16 Dec. 2015,ieeexplore
10.1109/QUATIC.2012.29,Experimental Software Engineering in Educational Context,IEEE,Conferences,"Empirical studies are important in software engineering to evaluate new tools, techniques, methods and technologies in a structured way before they are introduced in the industrial (real) software process. Within this PhD thesis we will develop a framework of a consistent process for involving students as subjects of empirical studies of software engineering. In concrete, our experiences with software development teams composed of students will analyze how RUP (Rational Unified Process) processes can be compliant with the CMMI (Capability Maturity Model Integration), namely in the context of MLs (maturity levels) 2 and 3. Additionally, we will also analyze the influence of project management tools to improve the process maturity of the teams. Our final goal of carrying out empirical studies with students is to understand its validity when compared with the corresponding studies in real industrial settings.",https://ieeexplore.ieee.org/document/6511839/,2012 Eighth International Conference on the Quality of Information and Communications Technology,3-6 Sept. 2012,ieeexplore
10.1109/EUROSIM.2013.39,Experimental and Computational Materials Defects Investigation,IEEE,Conferences,"Production of railway axles (i.e., one of the basic material of the modern train) is an elaborate process unfree from faults and problems. Errors during the manufacturing or the plies' overlapping, in fact, can cause particular flaws in the resulting material, so compromising its same integrity. Within this framework, ultrasonic tests could be useful to characterize the presence of defect, depending on its dimensions. On the contrary, the requirement of a perfect state for used materials is unavoidable in order to assure both transport reliability and passenger safety. Therefore, a real-time approach able to recognize and classify the defect starting from the finite element simulated ultrasonic echoes could be very useful in industrial applications. The ill-posedness of the so defined process induces a regularization method. In this paper, a finite element and a heuristic approach are proposed. Particularly, the proposed method is based on the use of a Neural Network approach, the so called ""learning by sample techniques"", and on the use of Support Vector Machines in order to classify the kind of defect. Results assure good performances of the implemented approach, with very interesting applications.",https://ieeexplore.ieee.org/document/7004937/,2013 8th EUROSIM Congress on Modelling and Simulation,10-13 Sept. 2013,ieeexplore
10.1109/MELCON.1994.380920,Expert system for control purpose based on CLIPS,IEEE,Conferences,"The use of AI techniques in complex process control of industrial environments, introduces some problems. The first is related to which kind of tools can be used and their match to system requirements. The next one is how to incorporate new features if needed and how to integrate them. The last one is related to the final implementation. Expert systems available in public domain source code seems a good solution in order to reach the advantages of an ES developed specially for the application without spending time developing low level ES features. An ES in source code allows us to select some parts of an ES kernel useful for the particular application. Actually we can find commercial ES in source code like CLIPS. We describe the problem from the point of view of developing ES with CLIPS. This tool is a complete ES language written in C code, it uses the RETE fast pattern matching algorithm to implement forward chaining inference engine. The availability of the source code allows us to cut the parts not used in a particular application and recompile it in a specific machine for control purposes. Therefore we can link the modified code with real time run-time libraries, obtaining applications dealing with real time constraints. Based on this tool, an intelligent controller has been developed which uses fuzzy logic for uncertainty handling.&lt;<ETX>&gt;</ETX>",https://ieeexplore.ieee.org/document/380920/,Proceedings of MELECON '94. Mediterranean Electrotechnical Conference,12-14 April 1994,ieeexplore
10.1109/SMC.2019.8913901,Explainable Machine Learning in Industry 4.0: Evaluating Feature Importance in Anomaly Detection to Enable Root Cause Analysis,IEEE,Conferences,"In the past recent years, Machine Learning methodologies have been applied in countless application areas. In particular, they play a key role in enabling Industry 4.0. However, one of the main obstacles to the diffusion of Machine Learning-based applications is related to the lack of interpretability of most of these methods. In this work, we propose an approach for defining a `feature importance' in Anomaly Detection problems. Anomaly Detection is an important Machine Learning task that has an enormous applicability in industrial scenarios. Indeed, it is extremely relevant for the purpose of quality monitoring. Moreover, it is often the first step towards the design of a Machine Learning-based smart monitoring solution because Anomaly Detection can be implemented without the need of labelled data. The proposed feature importance evaluation approach is designed for Isolation Forest, one of the most commonly used algorithm for Anomaly Detection. The efficacy of the proposed method is tested on synthetic and real industrial datasets.",https://ieeexplore.ieee.org/document/8913901/,"2019 IEEE International Conference on Systems, Man and Cybernetics (SMC)",6-9 Oct. 2019,ieeexplore
10.1109/WFCS.2019.8757999,Exploiting localization systems for LoRaWAN transmission scheduling in industrial applications,IEEE,Conferences,"The Internet of Things (IoT) paradigm contaminated the industrial world. Wireless communications seem to be particularly attracting, especially when complement indoor and outdoor Real Time Location Systems (RTLS) for geo-referencing smart objects (e.g. for asset tracking). In this paper, the LoRaWAN solution is considered for long range transmission of RTLS data (LoRaWAN is an example of Low Power Wide Area Network). Given that the RTLSs use time synchronization, this work proposes to opportunistically obtain LoRaWAN Class A node time synchronization using the RTLS ranging devices. Once a common sense of time is shared in the LoRaWAN network, more efficient scheduled medium access strategies can be implemented. The experimental testbed, based on commercially available solutions, demonstrates the affordability and feasibility of the proposed approach. When low-cost GPS (outdoor) and UWB (indoor) ranging devices are considered, synchronization error of few microseconds can be easily obtained. The experimental results show the that time reference pulses disciplined by GPS have a maximum jitter of 180 ns and a standard deviation of 40 ns whereas, if time reference pulses disciplined by UWB are considered, the maximum jitter is 3.3 μs and the standard deviation is 0.7 μs.",https://ieeexplore.ieee.org/document/8757999/,2019 15th IEEE International Workshop on Factory Communication Systems (WFCS),27-29 May 2019,ieeexplore
10.1109/IS.2018.8710554,Exploiting the Digital Twin in the Assessment and Optimization of Sustainability Performances,IEEE,Conferences,"Digitalization has shown the potential to disrupt industrial value chains by supporting real-time, risk-free and inexpensive inputs to decision making towards enhanced companies' productivity and value networks flexibility. Developing a reliable and robust digital replica of the physical systems of the value chain is one of the most advanced (and challenging) approaches to digitalization, condensed in the concept of Digital Twin (DT). DT plays a fundamental role in creating a data-rich environment where simulation and optimization procedures can be run. With DT expected to become a commodity in the coming years, simulation and optimization become therefore a more accessible instrument for the improvement of manufacturing and business processes also in small enterprises with limited investment capacity. While scientific literature has analysed the adoption of DT in the optimization of products lifecycle, no contributions have yet focused on the exploitation of DT to improve the sustainability performances of whole value chains. In this paper we propose a reference framework where DTs built upon process and system data gathered from the field, allow to quickly assess the sustainability performances of both existing and planned production mixes and to compare achievable impacts with changing processes and technologies, thus enabling advisory features for sustainability-aware decision making in structured, multi-entity value networks. Internal validation will be deployed referring to real case studies.",https://ieeexplore.ieee.org/document/8710554/,2018 International Conference on Intelligent Systems (IS),25-27 Sept. 2018,ieeexplore
10.1109/Agro-Geoinformatics.2019.8820424,Farming on the edge: Architectural Goals,IEEE,Conferences,"This research investigates how advances in Internet of Things (IoT) and availability of internet connection would enable Edge Solutions to promote smart utilization of existing machines at the edge. The presented results are based on experiments performed in real scenarios using the proposed solution. Whereas scenarios were cloned from real environments it is important to have in mind that experiments were performed with low load in terms of data and small number of devices in terms of distribution. As result of extensive architecture investigation for an optimal edge solution and its possible correlation to industrial applications, this paper will provide evidences supporting the use of edge solutions in challenging conditions which arise at the edge, including smart factories and smart agriculture. The present work assumes that the reader has some exposition to Edge computing, Cloud computing and software development. The paper will present some important findings on this area, compare main architectural aspects and will provide a broad view of how edge solutions might be built for this particular scenario. Having discussed how the ideal architecture works and having provided an overview about how it may be applied to industrial plants, the final section of this paper addresses how artificial intelligence will fit into edge solutions, forming a new source of “smart capabilities” to existing environments.",https://ieeexplore.ieee.org/document/8820424/,2019 8th International Conference on Agro-Geoinformatics (Agro-Geoinformatics),16-19 July 2019,ieeexplore
10.1109/ITOEC.2018.8740558,Fast Inter Mode Decision Algorithms for x265,IEEE,Conferences,"The latest High-Efficiency Video Coding (HEVC) standard achieves nearly 50% bit rates reduction for similar quality relative to H.264/Advanced Video Coding(AVC) . However, its complexity is enormously increased ,which becomes one of the most challenges for its deployment in real time applications. The only solution to decrease the coding complexity is to set up different settings by adjusting various coding parameters. Among them, low complexity settings are suitable for industrial applications and conducive to the popularization of HEVC. Traditional fast mode decision algorithms mainly aim at decreasing coding complexity for high complexity settings. In this paper, we propose a fast mode decision method for HEVC with low complexity settings according to machine learning. A decision tree is constructed to decide whether to check 2N×2N mode or the SKIP/MERGE mode by exploiting relevant information from spatiotemporal adjacent Coding Units(CUs). Further mode skipping is performed based on the result of the first step. Experiments show that the proposed scheme can only increase by 1.42% Bjotegaard Delta Bit rate(BDBR) with an average time reduction of 22.45% for HEVC with low complexity settings.",https://ieeexplore.ieee.org/document/8740558/,2018 IEEE 4th Information Technology and Mechatronics Engineering Conference (ITOEC),14-16 Dec. 2018,ieeexplore
10.1109/ICSE43902.2021.00085,Fast Outage Analysis of Large-Scale Production Clouds with Service Correlation Mining,IEEE,Conferences,"Cloud-based services are surging into popularity in recent years. However, outages, i.e., severe incidents that always impact multiple services, can dramatically affect user experience and incur severe economic losses. Locating the root-cause service, i.e., the service that contains the root cause of the outage, is a crucial step to mitigate the impact of the outage. In current industrial practice, this is generally performed in a bootstrap manner and largely depends on human efforts: the service that directly causes the outage is identified first, and the suspected root cause is traced back manually from service to service during diagnosis until the actual root cause is found. Unfortunately, production cloud systems typically contain a large number of interdependent services. Such a manual root cause analysis is often time-consuming and labor-intensive. In this work, we propose COT, the first outage triage approach that considers the global view of service correlations. COT mines the correlations among services from outage diagnosis data. After learning from historical outages, COT can infer the root cause of emerging ones accurately. We implement COT and evaluate it on a real-world dataset containing one year of data collected from Microsoft Azure, one of the representative cloud computing platforms in the world. Our experimental results show that COT can reach a triage accuracy of 82.1%-83.5%, which outperforms the state-of-the-art triage approach by 28.0%-29.7%.",https://ieeexplore.ieee.org/document/9402074/,2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE),22-30 May 2021,ieeexplore
10.1109/INFOCT.2018.8356831,Fault class prediction in unsupervised learning using model-based clustering approach,IEEE,Conferences,"Manufacturing industries have been on a steady path considering for new methods to achieve near-zero downtime to have flexibility in the manufacturing process and being economical. In the last decade with the availability of industrial internet of things (IIoT) devices, this has made it possible to monitor the machine continuously using wireless sensors, assess the degradation and predict the failures of time. Condition-based predictive maintenance has made a significant influence in monitoring the asset and predicting the failure of time. This has minimized the impact on production, quality, and maintenance cost. Numerous approaches have been in proposed over the years and implemented in supervised learning. In this paper, challenges of supervised learning such as need for historical data and incapable of classifying new faults accurately will be overcome with a new methodology using unsupervised learning for rapid implementation of predictive maintenance activity which includes fault prediction and fault class detection for known and unknown faults using density estimation via Gaussian Mixture Model Clustering and K-means algorithm and compare their results with a real case vibration data.",https://ieeexplore.ieee.org/document/8356831/,2018 International Conference on Information and Computer Technologies (ICICT),23-25 March 2018,ieeexplore
10.1109/I2MTC43012.2020.9129595,Feature Ranking under Industrial Constraints in Continuous Monitoring Applications based on Machine Learning Techniques,IEEE,Conferences,"The design work-flow of machine learning techniques for continuous monitoring or predictive maintenance in an industrial context is usually a two step procedure: the selection of features to be computed from the observed signals and training of a suitable algorithm with real-life meaningful data, that will be next deployed in the second step. Feature selection is a relevant task since it provides a powerful optimisation of the deployed algorithm performance, for the given training data-set. The paper provides a method for feature ranking and selection that embeds constraints coming from real-life applications, including sensing device specifications, environmental noise, available processing resources, being all these latter aspects not considered in the currently available literature methods for feature selection. A practical case-study in the field on anomaly detection of machines is reported and discussed, in order to show the good properties of the provided method.",https://ieeexplore.ieee.org/document/9129595/,2020 IEEE International Instrumentation and Measurement Technology Conference (I2MTC),25-28 May 2020,ieeexplore
10.1109/ICS51289.2020.00088,Feature Selection for Malicious Traffic Detection with Machine Learning,IEEE,Conferences,"The network technology plays an important role in the emerging industry 4.0. Industrial control systems (ICS) are related to all aspects of human life and have become the target of cyber-attackers. Attacks on ICS may not only cause economic loss, but also damage equipment and hurt staff. The biggest challenges in establishing a secure network communication system is how to effectively detect and prevent malicious network behavior. A Network Intrusion Detection System (NIDS) can be deployed as a defense mechanism for cyberattacks. However, for industrial internet-of-things (IIoT) applications with limited computing resources, designing an effective NIDS is challenging. In this paper, we propose to use machine learning as the core technology to build a compact and effective NIDS for IIoT. The proposed method is validated by using the more recent UNSW-NB 15 dataset to improve the detection capability against new types of attacks in the real world. Furthermore, we demonstrate that the method is also valid for traditional KDD-CUP-99 dataset. Experimental results show that the proposed method achieves better performance than previous methods.",https://ieeexplore.ieee.org/document/9359069/,2020 International Computer Symposium (ICS),17-19 Dec. 2020,ieeexplore
10.1109/APSEC51365.2020.00047,Federated Learning Systems: Architecture Alternatives,IEEE,Conferences,"Machine Learning (ML) and Artificial Intelligence (AI) have increasingly gained attention in research and industry. Federated Learning, as an approach to distributed learning, shows its potential with the increasing number of devices on the edge and the development of computing power. However, most of the current Federated Learning systems apply a single-server centralized architecture, which may cause several critical problems, such as the single-point of failure as well as scaling and performance problems. In this paper, we propose and compare four architecture alternatives for a Federated Learning system, i.e. centralized, hierarchical, regional and decentralized architectures. We conduct the study by using two well-known data sets and measuring several system performance metrics for all four alternatives. Our results suggest scenarios and use cases which are suitable for each alternative. In addition, we investigate the trade-off between communication latency, model evolution time and the model classification performance, which is crucial to applying the results into real-world industrial systems.",https://ieeexplore.ieee.org/document/9359305/,2020 27th Asia-Pacific Software Engineering Conference (APSEC),1-4 Dec. 2020,ieeexplore
10.1109/DAS.2018.29,Field Extraction by Hybrid Incremental and A-Priori Structural Templates,IEEE,Conferences,"In this paper, we present an incremental frame-work for extracting information fields from administrative documents. First, we demonstrate some limits of the existing state-of-the-art methods such as the delay of the system efficiency. This is a concern in industrial context when we have only few samples of each document class. Based on this analysis, we propose a hybrid system combining incremental learning by means of itf-df statistics and a-priori generic models. We report in the experimental section our results obtained with a dataset of real invoices.",https://ieeexplore.ieee.org/document/8395204/,2018 13th IAPR International Workshop on Document Analysis Systems (DAS),24-27 April 2018,ieeexplore
10.1109/ICTTA.2008.4530058,Filters Bank Derived from the Wavelet Transform for Real Time Change Detection in Signal,IEEE,Conferences,"The aim of this paper is to detect the faults in industrial systems, through on-line monitoring. The faults that are concerned correspond to changes in frequency components of the signal. Thus, early fault detection, which reduces the possibility of catastrophic damage, is possible by detecting the changes of characteristic features of the signal. This approach combines the filters bank technique, for extracting frequency and energy characteristic features, and the dynamic cumulative sum method (DCS), which is a recursive calculation of the logarithm of the likelihood ratio between two local hypotheses. The main contribution is to derive the filters coefficients from the wavelet in order to use the filters bank as a wavelet transform. The advantage of our approach is that the filters bank can be hardware implemented and can be used for online detection.",https://ieeexplore.ieee.org/document/4530058/,2008 3rd International Conference on Information and Communication Technologies: From Theory to Applications,7-11 April 2008,ieeexplore
10.1109/ICESC48915.2020.9155625,Fire and Gun Violence based Anomaly Detection System Using Deep Neural Networks,IEEE,Conferences,"Real-time object detection to improve surveillance methods is one of the sought-after applications of Convolutional Neural Networks (CNNs). This research work has approached the detection of fire and handguns in areas monitored by cameras. Home fires, industrial explosions, and wildfires are a huge problem that cause adverse effects on the environment. Gun violence and mass shootings are also on the rise in certain parts of the world. Such incidents are time-sensitive and can cause a huge loss to life and property. Hence, the proposed work has built a deep learning model based on the YOLOv3 algorithm that processes a video frame-by-frame to detect such anomalies in real-time and generate an alert for the concerned authorities. The final model has a validation loss of 0.2864, with a detection rate of 45 frames per second and has been benchmarked on datasets like IMFDB, UGR, and FireNet with accuracies of 89.3%, 82.6% and 86.5% respectively. Experimental result satisfies the goal of the proposed model and also shows a fast detection rate that can be deployed indoor as well as outdoors.",https://ieeexplore.ieee.org/document/9155625/,2020 International Conference on Electronics and Sustainable Communication Systems (ICESC),2-4 July 2020,ieeexplore
10.1109/IECON.2000.973216,Force control in robotic assembly under extreme uncertainty using ANN,IEEE,Conferences,"Robotic assembly operations can be performed by specifying an exact model of the operation. However, the uncertainties involved during assembly make it difficult to conceive such a model In these cases, the use of a connectionist model may be advantageous. In this paper, the design of a robotic cell based on the adaptive resonance theory artificial neural network and a PC host-slave architecture that overcame these uncertainties is presented. Different sources of uncertainty under real conditions are identified and their contribution in a typical assembly operation evaluated. The robotic system is implemented using a PUMA 761 industrial robot with six degrees of freedom (DOF) and a force/torque (F/T) sensor attached to its wrist which conveys force information to the neural network controller (NNC). Results during assembly operations are presented which validate the approach. Furthermore, the method is generic and can be implemented onto other manipulators.",https://ieeexplore.ieee.org/document/973216/,"2000 26th Annual Conference of the IEEE Industrial Electronics Society. IECON 2000. 2000 IEEE International Conference on Industrial Electronics, Control and Instrumentation. 21st Century Technologies",22-28 Oct. 2000,ieeexplore
10.1109/PerComWorkshops51409.2021.9430942,Forecasting Parking Lots Availability: Analysis from a Real-World Deployment,IEEE,Conferences,"Smart parking technologies are rapidly being deployed in cities and public/private places around the world for the sake of enabling users to know in real time the occupancy of parking lots and offer applications and services on top of that information. In this work, we detail a real-world deployment of a full-stack smart parking system based on industrial-grade components. We also propose innovative forecasting models (based on CNN-LSTM) to analyze and predict parking occupancy ahead of time. Experimental results show that our model can predict the number of available parking lots in a ±3% range with about 80% accuracy over the next 1-8 hours. Finally, we describe novel applications and services that can be developed given such forecasts and associated analysis.",https://ieeexplore.ieee.org/document/9430942/,2021 IEEE International Conference on Pervasive Computing and Communications Workshops and other Affiliated Events (PerCom Workshops),22-26 March 2021,ieeexplore
10.1109/AiDAS47888.2019.8970881,"Framework Of Malay Intelligent Autonomous Helper (Min@H): Text, Speech And Knowledge Dimension Towards Artificial Wisdom For Future Military Training System",IEEE,Conferences,"Industrial Revolution 4.0 is expected to improve the way of military training system. Most of the assistant systems use English for their Human Machine Interaction (HMI) such `SARA' a virtual socially aware robot assistant which exclude Malay socio-emotional aspects. This scenario opens a suggestion, to internalize socio-emotional aspects based on Malay culture, custom and beliefs to military autonomous training systems (i.e. MIN@H) that can improve the `collaborative' skills between Malaysian military personnel and the systems. Therefore, to increase the wisdom of the systems, they must have feature to capture information for their human users or helping human users to learn new knowledge and ensure the interaction is comfortable and engaging. For that reason, the systems must understand Malay language and be able to interpret emotion and expression behavior according to the Malay culture and custom, furthermore, the systems able to differentiate the level of user's understanding and build a good rapport or feeling of harmony that makes communication possible or easy between the systems and users. This concept of the systems is referred as Malay Artificial Wisdom System (AWS). There are three fundamental aspects to achieve the AWS. First, to computationally model the conversational strategies and rapport between the system and human users based-on user's understanding and system's articulation. Second, to computationally model, recognize and synthesize the emotion and expression behavior according to the Malay culture, custom and beliefs. Third, the AWS can do analytical reasoning and responding in relation to falsehood analysis and users' understanding level. Knowledge discovery and inference technique as well as HMI that cater the inputs and output of the MIN@H will be developed to accomplish the AWS concept. This program could embrace military training system in Malaysia to enhance military personnel skills and experts in various areas.",https://ieeexplore.ieee.org/document/8970881/,2019 1st International Conference on Artificial Intelligence and Data Sciences (AiDAS),19-19 Sept. 2019,ieeexplore
10.1109/SEST.2018.8495711,From M&amp;V to M&amp;T: An artificial intelligence-based framework for real-time performance verification of demand-side energy savings,IEEE,Conferences,"The European Union's Energy Efficiency Directive is placing an increased focus on the measurement and verification (M&amp;V) of demand side energy savings. The objective of M&amp;V is to quantify energy savings with minimum uncertainty. M&amp;V is currently undergoing a transition to practices, known as M&amp;V 2.0, that employ automated advanced analytics to verify performance. This offers the opportunity to effectively manage the transition from short-term M&amp;V to long-term monitoring and targeting (M&amp;T) in industrial facilities. The original contribution of this paper consists of a novel, robust and technology agnostic framework that not only satisfies the requirements of M&amp;V 2.0, but also bridges the gap between M&amp;V and M&amp;T by ensuring persistence of savings. The approach features a unique machine learning-based energy modelling methodology, model deployment and an exception reporting system that ensures early identification of performance degradation. A case study demonstrates the effectiveness of the approach. Savings from a real-world project are found to be 177,962 +/- 12,334 kWh with a 90% confidence interval. The uncertainty associated with the savings is 8.6% of the allowable uncertainty, thus highlighting the viability of the framework as a reliable and effective tool.",https://ieeexplore.ieee.org/document/8495711/,2018 International Conference on Smart Energy Systems and Technologies (SEST),10-12 Sept. 2018,ieeexplore
10.1109/ICCI-CC.2014.6921432,From information revolution to intelligence revolution: Big data science vs. intelligence science,IEEE,Conferences,"The hierarchy of human knowledge is categorized at the levels of data, information, knowledge, and intelligence. For instance, given an AND-gate with 1,000-input pins, it may be described very much differently at various levels of perceptions in the knowledge hierarchy. At the data level on the bottom, it represents a 2<sup>1,000</sup> state space, known as `big data' in recent terms, which appears to be a big issue in engineering. However, at the information level, it just represents 1,000 bit information that is equivalent to the numbers of inputs. Further, at the knowledge level, it expresses only two rules that if all inputs are one, the output is one; and if any input is zero, the output is zero. Ultimately, at the intelligence level, it is simply an instance of the logical model of an AND-gate with arbitrary inputs. This problem reveals that human intelligence and wisdom are an extremely efficient and a fast convergent induction mechanism for knowledge and wisdom elicitation and abstraction where data are merely factual materials and arbitrary instances in the almost infinite state space of the real world. Although data and information processing have been relatively well studied, the nature, theories, and suitable mathematics underpinning knowledge and intelligence are yet to be systematically studied in cognitive informatics and cognitive computing. This will leads to a new era of human intelligence revolution following the industrial, computational, and information revolutions. This is also in accordance with the driving force of the hierarchical human needs from low-level material requirements to high-level ones such as knowledge, wisdom, and intelligence. The trend to the emerging intelligent revolution is to meet the ultimate human needs. The basic approach to intelligent revolution is to invent and embody cognitive computers, cognitive robots, and cognitive systems that extend human memory capacity, learning ability, wisdom, and creativity. Via intelligence revolution, an interconnected cognitive intelligent Internet will enable ordinary people to access highly intelligent systems created based on the latest development of human knowledge and wisdom. Highly professional systems may help people to solve typical everyday problems. Towards these objectives, the latest advances in abstract intelligence and intelligence science investigated in cognitive informatics and cognitive computing are well positioned at the center of intelligence revolution. A wide range of applications of cognitive computers have been developing in ICIC [http://www.ucalgary.ca/icic/] such as, inter alia, cognitive computers, cognitive robots, cognitive learning engines, cognitive Internet, cognitive agents, cognitive search engines, cognitive translators, cognitive control systems, cognitive communications systems, and cognitive automobiles.",https://ieeexplore.ieee.org/document/6921432/,2014 IEEE 13th International Conference on Cognitive Informatics and Cognitive Computing,18-20 Aug. 2014,ieeexplore
10.1109/ICSE.2013.6606744,From models to code and back: Correct-by-construction code from UML and ALF,IEEE,Conferences,"Ever increasing complexity of modern software systems demands new powerful development mechanisms. Model-driven engineering (MDE) can ease the development process through problem abstraction and automated code generation from models. In order for MDE solutions to be trusted, such generation should preserve the system's properties defined at modelling level, both functional and extra-functional, all the way down to the target code. The outcome of our research is an approach that aids the preservation of system's properties in MDE of embedded systems. More specifically, we provide generation of full source code from design models defined using the CHESS-ML, monitoring of selected extra-functional properties at code level, and back-propagation of observed values to design models. The approach is validated against industrial case-studies in the telecommunications applicative domain.",https://ieeexplore.ieee.org/document/6606744/,2013 35th International Conference on Software Engineering (ICSE),18-26 May 2013,ieeexplore
10.1109/IECON.2013.6699377,Fuel Cells prognostics using echo state network,IEEE,Conferences,"One remaining technological bottleneck to develop industrial Fuel Cell (FC) applications resides in the system limited useful lifetime. Consequently, it is important to develop failure diagnostic and prognostic tools enabling the optimization of the FC. Among all the existing prognostics approaches, datamining methods such as artificial neural networks aim at estimating the process' behavior without huge knowledge about the underlying physical phenomena. Nevertheless, this kind of approach needs huge learning dataset. Also, the deployment of such an approach can be long (trial and error method), which represents a real problem for industrial applications where real-time complying algorithms must be developed. According to this, the aim of this paper is to study the application of a reservoir computing tool (the Echo State Network) as a prognostics system enabling the estimation of the Remaining Useful Life of a Proton Exchange Membrane Fuel Cell. Developments emphasize on the prediction of the mean voltage cells of a degrading FC. Accuracy and time consumption of the approach are studied, as well as sensitivity of several parameters of the ESN. Results appear to be very promising.",https://ieeexplore.ieee.org/document/6699377/,IECON 2013 - 39th Annual Conference of the IEEE Industrial Electronics Society,10-13 Nov. 2013,ieeexplore
10.1109/COASE.2017.8256157,Full automatic path planning of cooperating robots in industrial applications,IEEE,Conferences,"Parts made of carbon fiber reinforced plastics (CFRP) for airplane components can be so huge that a single industrial robot is no longer able to handle them, and cooperating robots are required. Manual programming of cooperating robots is difficult, but with large numbers of different sized and shaped cut-pieces, it is almost impossible. This paper presents an automated production system consisting of a camera for the precise detection of the position of each cut-piece and a collision-free path planner which can dynamically react to different positions for the transfer motions. The path is planned for multiple robots adhering to motion constrains, such as the requirement that the textile cut-piece must form a catenary which can change during transport. Additionally a technique based on machine learning has been implemented which correctly resolves redundancy for a linear axis during planning. Finally, all components are tested on a real robot system in industrial scale.",https://ieeexplore.ieee.org/document/8256157/,2017 13th IEEE Conference on Automation Science and Engineering (CASE),20-23 Aug. 2017,ieeexplore
10.1109/ICMLC.2017.8107749,Fuzzy logic based solar panel and battery control system design,IEEE,Conferences,"Photovoltaic systems, whether they are domestic, commercial or industrial often incorporate some forms of system protection. However, elaborate real-time fault detection is not defined for most such systems. To address this shortfall, a comprehensive photovoltaic installation system fault detection and control strategy is presented in this paper. The designed system is made up of fault detection in any of one of the photovoltaic system components that include the solar panel, charge controller, battery and inverter. The system also includes battery and user load current control. Fuzzy logic principle, due to its powerful non-linear problem solving capabilities is used in formulation of the fault detection and control algorithms as opposed to the classical method. This results in simpler, cheaper and faster hardware, which in this case is implemented on the PIC18F4550 microcontroller.",https://ieeexplore.ieee.org/document/8107749/,2017 International Conference on Machine Learning and Cybernetics (ICMLC),9-12 July 2017,ieeexplore
10.1109/FUZZ-IEEE.2014.6891715,Fuzzy uncertainty assessment in RBF Neural Networks using neutrosophic sets for multiclass classification,IEEE,Conferences,"In this paper we introduce a fuzzy uncertainty assessment methodology based on Neutrosophic Sets (NS). This is achieved via the implementation of a Radial Basis Function Neural-Network (RBF-NN) for multiclass classification that is functionally equivalent to a class of Fuzzy Logic Systems (FLS). Two types of uncertainties are considered: a) fuzziness and b) ambiguity, with both uncertainty types measured in each receptive unit (RU) of the hidden layer of the RBF-NN. The use of NS assists in the quantification of the uncertainty and formation of the rulebase; the resulting RBF-NN modelling structure proves to have enhanced transparency features to interpretation that enables us to understand the influence of each system parameter thorughout the parameter identification. The presented methodology is based on firstly constructing a neutrosophic set by calculating the associated fuzziness in each rule - and then use this information to train the RBF-NN; and secondly, an ambiguity measure that is defined via the truth and falsity measures related to each normalised consequence of the fuzzy rules within the RUs. In order to evaluate the individual ambiguity in the RUs and then the average ambiguity of the whole system, a neutrosophic set is constructed. Finally, the proposed methodology is tested against two case studies: a benchmark dataset problem and a real industrial case study. On both cases we demonstrate the effectiveness of the developed methodology in automatically creating uncertainty measures and utilising this new information to improve the quality of the trained model.",https://ieeexplore.ieee.org/document/6891715/,2014 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE),6-11 July 2014,ieeexplore
10.1109/WCMEIM52463.2020.00032,Garbage Classification and Recognition Based on SqueezeNet,IEEE,Conferences,"In this study, an intelligent garbage classification and recognition system was deployed on the industry integrated computer with the I3-7100U processor and 2G memory. Considering the unit prediction time and prediction accuracy, SqueezeNet was selected as the classification network training model among ResNet, InceptionV3, and SqueezeNet. The pretraining SqueezeNet network model on the ImageNet-1000 dataset was used for transfer learning, and the model predication accuracy was improved by using image enhancement and Adam optimizer. The comparison between the comprehensive test set and the real garbage image showed that the model predication accuracy reached 87.7% after training, and the prediction time in the industrial integrated machine was less than 2 seconds, which met the needs of practical applications.",https://ieeexplore.ieee.org/document/9409502/,2020 3rd World Conference on Mechanical Engineering and Intelligent Manufacturing (WCMEIM),4-6 Dec. 2020,ieeexplore
10.1109/ICECCO.2018.8634719,Gas Turbine Fault Classification Based on Machine Learning Supervised Techniques,IEEE,Conferences,"Nowadays Machinery Diagnostic becomes a major part for many industrial applications. It allows to predict and prevent of breakages. An analysis of the trends in the development of power machines show that the most advanced installations can be created using gas turbine technologies. Quite justified, many energy specialists consider the XXI century - the century of gas turbine technologies. It is very important to prevent gas turbine failure. In this paper investigated machine learning classification techniques with further implementation for fault detection in gas turbine running data trends. Investigation was done for real gas compression station running parameters.",https://ieeexplore.ieee.org/document/8634719/,2018 14th International Conference on Electronics Computer and Computation (ICECCO),29 Nov.-1 Dec. 2018,ieeexplore
10.1109/ICNSC.2006.1673254,General Methodology for Action-Oriented Industrial Ecology Complex Systems Approach Applied to the Rotterdam Industrial Cluster,IEEE,Conferences,"A new approach for the understanding and shaping of the evolution of large scale socio-technical systems is presented. A proof-of-concept knowledge application has been developed, based on the industrial Rotterdam-Rijnmond case. The knowledge application includes the design of a model of industry-infrastructure evolution. Such networks are modeled via a system decomposition, formalization in an ontology and implementation of an agent based model. In simulation runs several network metrics are presented. The results provide insights in real world system behavior and show the validity and potential of the approach",https://ieeexplore.ieee.org/document/1673254/,"2006 IEEE International Conference on Networking, Sensing and Control",23-25 April 2006,ieeexplore
10.1109/MERCon.2019.8818945,Generalised Framework for Automated Conversational Agent Design via QFD,IEEE,Conferences,"Automated conversational agents are often regarded as the most promising method of responding to customer queries with minimum human intervention. Focus of the existing literature is mostly on technological innovations such as artificial intelligence and learning. A generalized text based real-time conversational agent or a chatbot development framework can be both conceptually and practically appealing, as a way to develop technologies to improve the responsiveness and customer friendliness of the chatbots. Different forms of technological advancements could help the firms to deploy right chatbot technology with regard to the user requirements in their organizations. This paper explores some product design ideas such as Analytic Hierarchy Process (AHP) and Quality Function Deployment (QFD) drawn from industrial engineering literature for chatbot development.",https://ieeexplore.ieee.org/document/8818945/,2019 Moratuwa Engineering Research Conference (MERCon),3-5 July 2019,ieeexplore
10.1109/AI4I51902.2021.00022,Generating Reinforcement Learning Environments for Industrial Communication Protocols,IEEE,Conferences,"An important part of any reinforcement learning application is interfacing the agent to its environment. To enable an easier use of reinforcement learning agents in manufacturing and automation-related real-world environments, we propose an environment generator which acts as an adapter between the interface of the agent and existing industrial communication protocols. This paper describes the functionality and architecture of such an environment generator.",https://ieeexplore.ieee.org/document/9565507/,2021 4th International Conference on Artificial Intelligence for Industries (AI4I),20-22 Sept. 2021,ieeexplore
10.1109/AI4I.2018.8665690,Genetic Algorithm Based Parallelization Planning for Legacy Real-Time Embedded Programs,IEEE,Conferences,"Multicore platforms are pervasively deployed in many different sectors of industry. Hence, it is appealing to accelerate the execution through adapting the sequential programs to the underlying architecture to efficiently utilize the hardware resources, e.g., the multi-cores. However, the parallelization of legacy sequential programs remains a grand challenge due to the complexity of the program analysis and dynamics of the runtime environment. This paper focuses on parallelization planning in that the best parallelization candidates would be determined after the parallelism discovery in the target large sequential programs. In this endeavor, a genetic algorithm based method is deployed to help find an optimal solution considering different aspects from the task decomposition to solution evaluation while achieving the maximized speedup. We have experimented the proposed approach on industrial real time embedded application to reveal excellent speedup results.",https://ieeexplore.ieee.org/document/8665690/,2018 First International Conference on Artificial Intelligence for Industries (AI4I),26-28 Sept. 2018,ieeexplore
10.1109/CEC.2019.8790171,Genetic Algorithm for Topology Optimization of an Artificial Neural Network Applied to Aircraft Turbojet Engine Identification,IEEE,Conferences,"Artificial neural networks (ANN) has attracted attention of the academic community by the current progress that this technique has provided in speech recognition and digital media such as as image, video, audio, and signal processing. Some fields, as industrial process control and product development can be highly benefited by the development of techniques based on the proven potentialities of ANN models, allowing more accurate simulation, better adaptation to changing environments, and greater robustness in model-based fault diagnosis. Along with the advance of ANNs, there is a trend of open-source softwares use for soft computing which facilitates the access of the interested readers to implement their own codes and to explore other applications. Historically evolutionary algorithms such as the Genetic Algorithm (GA) have been implemented to evolve the architectures to search for solutions, in order to solve this fundamental issue that is still an open problem in the general case. Therefore, the present paper investigates the application of ANN to model the nonlinear aircraft turbojet engine through black-box approach. For that purpose it was used real-world measurements of aircraft engine's fuel and rotation as input and output, respectively. In order to facilitate the design, the ANN was optimized aiming to determine the best topology according to the one-step-ahead and free-run simulation. The results obtained encourage the use of automatically generated ANN architectures for dynamic system modeling.",https://ieeexplore.ieee.org/document/8790171/,2019 IEEE Congress on Evolutionary Computation (CEC),10-13 June 2019,ieeexplore
10.1109/TSP52935.2021.9522588,Genetic Programming based Identification of an Industrial Process,IEEE,Conferences,"In the field of industrial automation, it is essential to develop and improve mathematical methods that assist in obtaining more accurate models of real-world systems. In the following paper, a machine learning tool is applied to the problem of identifying a model of an industrial process. Symbolic regression and genetic programming are a successful combination of methods using which one can identify a nonlinear model in analytical form based on data collected from a process during routine operation. In this paper, a detailed description of the method implementation as well as necessary data preprocessing steps are presented. Then, the resulting models are validated on an industrial data set and compared on the basis of performance metrics with more classical methods and previous results achieved by the authors. Finally, the encountered problems in the realization of the methods are reflected upon.",https://ieeexplore.ieee.org/document/9522588/,2021 44th International Conference on Telecommunications and Signal Processing (TSP),26-28 July 2021,ieeexplore
10.1109/FUZZY.2006.1681714,Granular Auto-regressive Moving Average (grARMA) Model for Predicting a Distribution from Other Distributions. Real-world Applications,IEEE,Conferences,"Industrial products are often output in batches at discrete times. A batch gives rise to distributions of measurements, one distribution per variable of interest. There may be a need for modeling to predict a distribution from other distributions. This work represents a distribution by a fuzzy interval number (FIN) interpreted as an information granule. Based on vector lattice theory it is shown that the lattice F<sub>+</sub> of positive FINs is a cone in a non-linearly tunable, metric, linear space. In conclusion, a multivariate granular autoregressive moving average (grARMA) model is proposed for predicting a distribution from other distributions. A recursive neural network implementation is shown. We report preliminary results regarding two real-world applications including, first, industrial fertilizer production and, second, environmental pollution monitoring along seashore in northern Greece. The far-reaching potential of novel techniques is discussed.",https://ieeexplore.ieee.org/document/1681714/,2006 IEEE International Conference on Fuzzy Systems,16-21 July 2006,ieeexplore
10.1109/VETECS.2000.851509,"Graphical control of autonomous, virtual vehicles",IEEE,Conferences,"This paper presents some of the developments we made with the goal of allowing a friendly control and simulation of a large number of autonomous agents based in behavior in interactive real-time systems. Our work has been specially oriented to the simulation and control of autonomous vehicles and pedestrians in the preparation of scenarios to driving simulation experiments in the DriS simulator. Because every element is intrinsically autonomous, only a few of them are usually addressed to implement the desired study event. Also, because our model is autonomous and controllable, we can use the same model in the implementation of both environment traffic and controlled vehicles. Our scripting language is based in Grafcet, a well known graphical language used in the specification and programming of industrial controllers. Our technique allows the imposition of both short time orders and long time goals to each autonomous element. Orders can be triggered reactively using sensors that monitor the state of virtual traffic and configurable timers that generate all the necessary fixed and variable time events.",https://ieeexplore.ieee.org/document/851509/,VTC2000-Spring. 2000 IEEE 51st Vehicular Technology Conference Proceedings (Cat. No.00CH37026),15-18 May 2000,ieeexplore
10.1109/ETFA.2019.8868968,Hard Real-Time Capable OPC UA Server as Hardware Peripheral for Single Chip IoT Systems,IEEE,Conferences,"The fast semantics project examines the use of the OPC Unified Architecture (OPC UA) in embedded industrial systems and proposes the design of a customizable, hard real time capable OPC UA Intellectual Property Core (IP Core) for single chip computing plattforms. This allows using OPC UA in both novel energy efficient sensor applications and in state of the art field devices. These single chip OPC UA servers form the semantic data sources for future applications such as cloud based added value services or machine learning applications. This article presents the design alternatives and first synthesis results for the implementation of OPC UA servers in embedded systems.",https://ieeexplore.ieee.org/document/8868968/,2019 24th IEEE International Conference on Emerging Technologies and Factory Automation (ETFA),10-13 Sept. 2019,ieeexplore
10.1109/SOFA.2010.5565626,Hierarchical neural model for workflow scheduling in Utility Management Systems,IEEE,Conferences,"The emerging computational grid infrastructure consists of heterogeneous resources in widely distributed autonomous domains, which makes job scheduling very challenging. Although there is much work on static scheduling approaches for workflow applications in parallel environments, little work has been done on a real-world Grid environment for industrial systems. Utility Management Systems (UMS) are executing very large numbers of workflows with very high resource requirements. Unlike the grid approach for standard scientific workflows, UMS workflows have different set of computation requirements and thereby optimization of resource usage has to be made in a different way. This paper proposes architecture for a new scheduling mechanism that dynamically executes a scheduling algorithm using near real-time feedback about current status Grid nodes. Two Artificial Neural Networks (ANN) were created in order to solve scheduling problem. First ANN predicts future state of Grid based on current state and types of workflows that are currently executing. Second ANN output is optimal workflow type that should be executed. Inputs for second ANN are current state of the Grid and predicted future state (output of first ANN). Performance tests show that significant improvement of overall execution time can be achieved by this Hierarchical Artificial Neural Networks.",https://ieeexplore.ieee.org/document/5565626/,4th International Workshop on Soft Computing Applications,15-17 July 2010,ieeexplore
10.1109/ETFA.2018.8502527,Holo Pick'n'Place,IEEE,Conferences,"In this paper we contribute to the research on facilitating industrial robot programming by presenting a concept for intuitive drag and drop like programming of pick and place tasks with Augmented Reality (AR). We propose a service-oriented architecture to achieve easy exchangeability of components and scalability with respect to AR devices and robot workplaces. Our implementation uses a HoloLens and a UR5 robot, which are integrated into a framework of RESTful web services. The user can drag recognized objects and drop them at a desired position to initiate a pick and place task. Although the positioning accuracy is unsatisfactory yet, our implemented prototype achieves most of the desired advantages to proof the concept.",https://ieeexplore.ieee.org/document/8502527/,2018 IEEE 23rd International Conference on Emerging Technologies and Factory Automation (ETFA),4-7 Sept. 2018,ieeexplore
10.1109/SMC42975.2020.9283392,Human-in-the-Loop Error Precursor Detection using Language Translation Modeling of HMI States,IEEE,Conferences,"Situational Awareness (SA) is paramount to ensuring operational safety in Nuclear Power Plant (NPP) and Commercial aviation industry. An increase in Human-in-the-loop (HITL) error rate may be indicative of reduced operator SA while undermining safety. In this paper, natural language processing (NLP) is applied for modelling industrial Human Machine Interface (HMI) state transitions as a means to detect operator HITL error precursors in real-time. A custom seq2seq encode-decoder deep-learning model design is implemented and evaluated using real-plant scenario dataset obtained from a NPP Operator training simulator. Results support NLP HMI state model may be employed to detect HITL error precursor within the desired N time-steps prior to an accident event.",https://ieeexplore.ieee.org/document/9283392/,"2020 IEEE International Conference on Systems, Man, and Cybernetics (SMC)",11-14 Oct. 2020,ieeexplore
10.1109/IDAACS-SWS.2018.8525503,Hybrid MAC for Low Latency Wireless Communication Enabling Industrial HMI Applications,IEEE,Conferences,"Wireless technologies are one of the core components of the future industrial applications. They provide flexibility and scalability to the factory floor in parallel with deployment cost reduction. In our paper, we concentrate on future-oriented human-machine interaction (HMI) applications such as augmented reality (AR) or mobile control. Based on their requirements, we provide an investigation of IEEE 802.11 channel access techniques with respect to their suitability for industrial applications.",https://ieeexplore.ieee.org/document/8525503/,2018 IEEE 4th International Symposium on Wireless Systems within the International Conferences on Intelligent Data Acquisition and Advanced Computing Systems (IDAACS-SWS),20-21 Sept. 2018,ieeexplore
10.1109/ROMAN.1992.253866,Hybrid architectures for intelligent robotic systems,IEEE,Conferences,"Hybrid architectures, based on combinations of analogic, symbolic, and neural methods, are well suited for real-time applications in advanced robotics. Real-time industrial applications are mainly based on the correction of preplanned programs. So far, the planning and control modules of these kind of applications are often unable to react and/or classify un-expected events. The approach described attempts to integrate the sensor-based analogic method and the neural method into a multiple-level architecture that operates on an analogic world model, so that the action planning can be performed in a smart, reactive way. Given the task, the system builds the world model of the scenario. The reasoning and planning modules act both at the strategic as well as reactive levels, and the activated sensor-based motor strategies handle the sensorial data inputs and drive the robot controller module in the execution of the stream of motor commands. The interaction between the different levels is mainly based on the idea of maintaining and updating in real-time the world model, so that each module can locally operate on specific parts of the whole world model.&lt;<ETX>&gt;</ETX>",https://ieeexplore.ieee.org/document/253866/,[1992] Proceedings IEEE International Workshop on Robot and Human Communication, 1992,ieeexplore
10.1109/ICISCE48695.2019.00071,Hydropower Generation Forecasting via Deep Neural Network,IEEE,Conferences,"With the advances of deep learning, its applications in our daily life have attracted considerable attention from both academics and industry. However, most of the existing works focus on computer vision and natural language processing, while few studies in the real industrial manufactures. The main reasons are that the data resources are difficult to obtain and the relationships between industrial data are too complex to be modeled. In this paper, we propose a deep neural network based approach for hydroelectric power generation prediction, which, to our knowledge, is the first attempt modeling power generation data with the combination of residual neural networks and recurrent neural networks. Furthermore, we consider different grains of the hydropower generation by dividing the data into four-levels, i.e. recent, daily, weekly, and time-series sequences, which can greatly improve the prediction performance. To this end, we employ a multi-information fusion method to fuse the four components (i.e. closeness, period, trend, long-period) predicted results, among which different component is learned with different weights to determine their influence on final hydropower prediction. Experiments conducting on the real hydropower generation prove the effectiveness of the proposed model, which significantly outperform the baselines. We hope this research will open a new perspective of improving data usage in the industry, especially in power generation areas.",https://ieeexplore.ieee.org/document/9107696/,2019 6th International Conference on Information Science and Control Engineering (ICISCE),20-22 Dec. 2019,ieeexplore
10.1109/ICCCE50029.2021.9467162,ICS Cyber Attack Detection with Ensemble Machine Learning and DPI using Cyber-kit Datasets,IEEE,Conferences,"Digitization has pioneered to drive exceptional changes across all industries in the advancement of analytics, automation, and Artificial Intelligence (AI) and Machine Learning (ML). However, new business requirements associated with the efficiency benefits of digitalization are forcing increased connectivity between IT and OT networks, thereby increasing the attack surface and hence the cyber risk. Cyber threats are on the rise and securing industrial networks are challenging with the shortage of human resource in OT field, with more inclination to IT/OT convergence and the attackers deploy various hi-tech methods to intrude the control systems nowadays. We have developed an innovative real-time ICS cyber test kit to obtain the OT industrial network traffic data with various industrial attack vectors. In this paper, we have introduced the industrial datasets generated from ICS test kit, which incorporate the cyber-physical system of industrial operations. These datasets with a normal baseline along with different industrial hacking scenarios are analyzed for research purposes. Metadata is obtained from Deep packet inspection (DPI) of flow properties of network packets. DPI analysis provides more visibility into the contents of OT traffic based on communication protocols. The advancement in technology has led to the utilization of machine learning/artificial intelligence capability in IDS ICS SCADA. The industrial datasets are pre-processed, profiled and the abnormality is analyzed with DPI. The processed metadata is normalized for the easiness of algorithm analysis and modelled with machine learning-based latest deep learning ensemble LSTM algorithms for anomaly detection. The deep learning approach has been used nowadays for enhanced OT IDS performances.",https://ieeexplore.ieee.org/document/9467162/,2021 8th International Conference on Computer and Communication Engineering (ICCCE),22-23 June 2021,ieeexplore
10.1109/CCECE.2002.1013006,IEEE CCECE2002. Canadian Conference on Electrical and Computer Engineering. Conference Proceedings (Cat. No.02CH37373),IEEE,Conferences,"The following topics are dealt with: power system modelling, planning and operation; power electronics and machines; electromagnetics and optics; antenna theory, design and applications; remote sensing; SAR ; acoustic ranging; microelectronics and VLSI; nanotechnology and micromachining; instrumentation and sensors; circuits and systems; robotics and mechatronics; reliability engineering; computers and digital hardware; real-time systems; software and information technology; computational intelligence; neural networks; genetic algorithms and fuzzy logic; pattern recognition; image processing; video processing; signal processing and filter design; biomedical engineering; health-care systems; communications systems; computer networks; wireless networks; telecommunication traffic analysis; QoS; industrial applications.",https://ieeexplore.ieee.org/document/1013006/,IEEE CCECE2002. Canadian Conference on Electrical and Computer Engineering. Conference Proceedings (Cat. No.02CH37373),12-15 May 2002,ieeexplore
10.1109/MetroInd4.0IoT51437.2021.9488447,IOT data-driven experimental process optimisation for kevlar fiberglass components for aeronautic,IEEE,Conferences,"This paper describes the work carried out during the PROOF experiment (IOT data-driven experimental PROcess Optimization for kevlar Fiberglass components for aeronautic), winner of the second open call of the MIDIH EU project. The main objectives of the experiment are the integration of smart sensing devices with the Energy@Work IoT gateways and the development of cloudified innovative data-driven methodologies and data analytics tools to support process optimization in the production of hybrid composite material parts for the aeronautical sector. Collection of real-time production-data from multiple sensors with several industrial protocols and data transfer to the MIDIH project platform has been performed adopting the IoT gateway developed by Energy@Work, following MIDIH reference architecture for advanced data processing and visualization (e.g., Fiware Orion Context Broker, Apache Flink and Fiware Knowage) by using MQTT protocol. Then, historical and new acquired data has been analysed using advanced clustering techniques and trends, with the purpose to allow a novel CPS-based predictive system on the production process. Machine-Learning algorithms and visualisations (GUI based on Fiware Knowage) in real operating conditions have been used to validate the performance and assess the outcome. Finally, thanks to the implementation of specific optimization rules, able to process data gathered from the sensor network, a framework for distributed processing engine has been exploited by (i) generating tips for energy efficiency and process optimization and (ii) providing different type of alarms based on expected consumptions, resulting in concrete support to production managers for the improvement of the whole production value chain.",https://ieeexplore.ieee.org/document/9488447/,2021 IEEE International Workshop on Metrology for Industry 4.0 & IoT (MetroInd4.0&IoT),7-9 June 2021,ieeexplore
10.1109/ICPADS.2016.0155,IRMD: Malware Variant Detection Using Opcode Image Recognition,IEEE,Conferences,"Malware detection becomes mission critical as its threats spread from personal computers to industrial control systems. Modern malware generally equips with sophisticated anti-detection mechanisms such as code-morphism, which allows the malware to evolve into many variants and bypass traditional code feature based detection systems. In this paper, we propose to disassemble binary executables into opcodes sequences, and then convert the opcodes into images. By using convolutional neural network to compare the opcode images generated from binary targets with the opcode images generated from known malware sample codes, we can detect if the target binary executables is malicious. Theoretical analysis and real-life experiments results show that malware detection using visualized analysis is comparable in terms of accuracy, our approach can significantly improve 15% of detection accuracy when the detection set contains a large quantity of binaries and the training set is much smaller.",https://ieeexplore.ieee.org/document/7823870/,2016 IEEE 22nd International Conference on Parallel and Distributed Systems (ICPADS),13-16 Dec. 2016,ieeexplore
10.1109/ICA-ACCA.2018.8609763,"Identification and Process Control for MISO systems, with Artificial Neural Networks and PID Controller",IEEE,Conferences,"Industrial processes with multiple input and single manipulated variables are very complex systems to control in automatic models. Such is the case with processes related to gases extraction or transport phenomena. The present research is focused on the development of a control algorithm (automatic control strategy), based on artificial neural networks, to identify an industrial process by using process historical records, as well as knowledge from the operation itself. The output of the identification stage feeds a classic PID controller to perform control actions (hybrid controller). Here, an actuator or final control element is modeled, estimating its space-state dynamic equation. With the estimated model, a local control loop is conformed controlling the main process or manipulated variable. For this, the process of gases transport in a copper smelter plant was chosen, where the necessary data and scenarios for the proposed control algorithm testing was obtained. This application attempts to present a solution to problems inherent to manual control, multiple key variables coexisting in a system, mechanical stress in equipment due to manual actions, etc. The control strategy is based on a computer simulation made with real process data, showing improvement of the transient periods in the final actuators due to control signals, as well as establishing that these kinds of technologies could be implemented in both, an existing plant hardware/software or in a conventional control system.",https://ieeexplore.ieee.org/document/8609763/,2018 IEEE International Conference on Automation/XXIII Congress of the Chilean Association of Automatic Control (ICA-ACCA),17-19 Oct. 2018,ieeexplore
10.1109/ICDMW.2017.40,Identifying Irregular Power Usage by Turning Predictions into Holographic Spatial Visualizations,IEEE,Conferences,"Power grids are critical infrastructure assets that face non-technical losses (NTL) such as electricity theft or faulty meters. NTL may range up to 40% of the total electricity distributed in emerging countries. Industrial NTL detection systems are still largely based on expert knowledge when deciding whether to carry out costly on-site inspections of customers. Electricity providers are reluctant to move to large-scale deployments of automated systems that learn NTL profiles from data due to the latter's propensity to suggest a large number of unnecessary inspections. In this paper, we propose a novel system that combines automated statistical decision making with expert knowledge. First, we propose a machine learning framework that classifies customers into NTL or non-NTL using a variety of features derived from the customers' consumption data. The methodology used is specifically tailored to the level of noise in the data. Second, in order to allow human experts to feed their knowledge in the decision loop, we propose a method for visualizing prediction results at various granularity levels in a spatial hologram. Our approach allows domain experts to put the classification results into the context of the data and to incorporate their knowledge for making the final decisions of which customers to inspect. This work has resulted in appreciable results on a real-world data set of 3.6M customers. Our system is being deployed in a commercial NTL detection software.",https://ieeexplore.ieee.org/document/8215672/,2017 IEEE International Conference on Data Mining Workshops (ICDMW),18-21 Nov. 2017,ieeexplore
10.1109/NDS.2017.8070626,"Image-driven, model-free control of repetitive processes based on machine learning",IEEE,Conferences,"An image-driven, model-free approach to design control systems for a large class of industrial process is proposed. A mathematical model of the process is replaced by sequences of subsequent images which play the role of the process (plant) states. The length of this sequences depends on the speed of the process dynamics and on the frame rate. Firstly, a learning sequence of the system states is collected and then, it is used for classifying (clustering) its states. A decision of the control system is attached by an expert to each class (cluster) of the states. At the implementation stage images from a camera in the loop are collected into subsequences corresponding to the system states, then they are classified and a control action corresponding to a class at hand is undertaken. This general idea is exemplified by the case study of a laser power control in an additive manufacturing, which is a repetitive process. A tree-like, hierarchical classifier is proposed in order to recognize the process states, each consisting of three consecutive images. Its performance is tested on real-life images from the process of laser cladding additive manufacturing.",https://ieeexplore.ieee.org/document/8070626/,2017 10th International Workshop on Multidimensional (nD) Systems (nDS),13-15 Sept. 2017,ieeexplore
10.1109/I2MTC43012.2020.9129119,Impact of Noise on Machine Learning-based Condition Monitoring Applications: a Case Study,IEEE,Conferences,"In the paper, application of Machine Learning (ML) techniques for the continuous monitoring of the health status of mild mission-critical industrial equipment is considered. A meaningful real-life case-study is presented in order to show how acquisition conditions may severely impact on the performance of the system. In particular, it is shown that a wrong estimate of noise effects in the deployed system may induce a wrong choice of the best features feeding the ML monitoring algorithm, hence affecting accuracy of the target devices. The discussed results may provide an useful guidance to the practitioner in the field during the design phase of ML-based devices depending of the equipment specifications and environmental conditions.",https://ieeexplore.ieee.org/document/9129119/,2020 IEEE International Instrumentation and Measurement Technology Conference (I2MTC),25-28 May 2020,ieeexplore
10.1109/CIIMA50553.2020.9290302,Implementación de SCADA a través del protocolo MQTT,IEEE,Conferences,"This document describes an implementation of a SCADA system powered by MQTT &amp; OPC-UA protocols and hosted within the Google Cloud Platform system. This combination allows to have integrated, scalable, secure and reliable industrial communications while allowing real-time data acquisition and sensor feed that can then be used in real-time OEE tracking or predictive maintenance models, to name some examples. This in line with the Industry 4.0 initiatives mainly fueled by data and machine learning autonomous systems.",https://ieeexplore.ieee.org/document/9290302/,2020 IX International Congress of Mechatronics Engineering and Automation (CIIMA),4-6 Nov. 2020,ieeexplore
10.1109/ICPHYS.2019.8780271,Implementation of Industrial Cyber Physical System: Challenges and Solutions,IEEE,Conferences,"The Industry Internet of Things (IIoT) and the Industry Cyber-Physical System (ICPS) for real industry are becoming vitally necessary in the smart manufacturing environment. Very large number of intelligent sensors are being available generating an exploding amount of data. Several issues come with the big data in real industry, including the a grand-scale connected network construction with the data security and access protocol issues, data quality with considerable noise when gathered from industrial factories, efficient data storage, smart interconnection with cloud services, and real-time analytics requirements. This paper proposes an integrated CPS based architecture for smart manufacturing and provides the deployment details, addressing all the potential problems in an appropriate way. It has been successfully implemented in a real industry environment, and won the Best Industry Application of IoT at the BigInsights Data &amp; AI Innovation Awards.",https://ieeexplore.ieee.org/document/8780271/,2019 IEEE International Conference on Industrial Cyber Physical Systems (ICPS),6-9 May 2019,ieeexplore
10.1109/EUSIPCO.2016.7760546,Implementation of efficient real-time industrial wireless interference identification algorithms with fuzzified neural networks,IEEE,Conferences,Real-time industrial wireless systems sharing a crowded spectrum band require active coexistence management measures. Identification of wireless interference is a key issue for this purpose. We propose an efficient implementation of a wireless interference identification (WII) approach called neuro-fuzzy signal classifier (NFSC). The implementation in Matlab / SIMULINK is based upon the wideband software defined radio Ettus USRP N210. The implementation is evaluated in six selected heterogeneous and harsh industrial scenarios within the license-free 2.4-GHz-ISM radio band with variously combined standard wireless technologies IEEE 802.11g-based WLAN and Bluetooth. The evaluation of the NFSC was performed with a binary classification test with the statistical measurement metrics sensitivity and specificity.,https://ieeexplore.ieee.org/document/7760546/,2016 24th European Signal Processing Conference (EUSIPCO),29 Aug.-2 Sept. 2016,ieeexplore
10.1109/ETFA.2005.1612698,Implementation of neural networks in foundation fieldbus environment using standard function blocks,IEEE,Conferences,"This paper presents an implementation approach of artificial neural networks in industrial network environment, through the use of function blocks standardized by field-bus foundation (FF). This enables the implementation of a wide range of applications that involve this mathematical tool, such as intelligent control, failure detection, etc in standard FF system. For validation propose, some examples are presented from of real experiments",https://ieeexplore.ieee.org/document/1612698/,2005 IEEE Conference on Emerging Technologies and Factory Automation,19-22 Sept. 2005,ieeexplore
10.1109/ICPR48806.2021.9412842,Improved anomaly detection by training an autoencoder with skip connections on images corrupted with Stain-shaped noise,IEEE,Conferences,"In industrial vision, the anomaly detection problem can be addressed with an autoencoder trained to map an arbitrary image, i.e. with or without any defect, to a clean image, i.e. without any defect. In this approach, anomaly detection relies conventionally on the reconstruction residual or, alternatively, on the reconstruction uncertainty. To improve the sharpness of the reconstruction, we consider an autoencoder architecture with skip connections. In the common scenario where only clean images are available for training, we propose to corrupt them with a synthetic noise model to prevent the convergence of the network towards the identity mapping, and introduce an original Stain noise model for that purpose. We show that this model favors the reconstruction of clean images from arbitrary real-world images, regardless of the actual defects appearance. In addition to demonstrating the relevance of our approach, our validation provides the first consistent assessment of reconstruction-based methods, by comparing their performance over the MVTec AD dataset [1], both for pixel- and image-wise anomaly detection. Our implementation is available at https://github.com/anncollin/AnomalyDetection-Keras.",https://ieeexplore.ieee.org/document/9412842/,2020 25th International Conference on Pattern Recognition (ICPR),10-15 Jan. 2021,ieeexplore
10.1109/ICAICA50127.2020.9182590,Improvement of Model Simplification Algorithm Based on LOD and Implementation of WebGL,IEEE,Conferences,"With the continuous progress of computer graphics, virtual reality and other technologies, 3D models in the field of industrial production have become more and more sophisticated, and the triangular surfaces need to be rendered are more than one million, which poses a great challenge to the storage, transmission and computing power of computers. Therefore, in order to adapt to the current performance of the computer while taking into account the rendering effect of the model, the Level-of-Detail(LOD) technology has been spawned. Industrial models tend to have complex structures and need to be displayed accurately in the rendering process, and in the case of a large number of holes in the model, the common algorithm is difficult to maintain the topology of the model well. Therefore, the article uses the edge collapse algorithm. To improve it, an algorithm that uses the mean deviation to guide the simplification process of edge collapse is proposed. While ensuring sufficient triangular mesh simplification, the topology of complex industrial models is maintained.",https://ieeexplore.ieee.org/document/9182590/,2020 IEEE International Conference on Artificial Intelligence and Computer Applications (ICAICA),27-29 June 2020,ieeexplore
10.1109/ICIT.2015.7125235,Improving accuracy of long-term prognostics of PEMFC stack to estimate remaining useful life,IEEE,Conferences,"Proton Exchange Membrane Fuel cells (PEMFC) are energy systems that facilitate electrochemical reactions to create electrical energy from chemical energy of hydrogen. PEMFC are promising source of renewable energy that can operate on low temperature and have the advantages of high power density and low pollutant emissions. However, PEMFC technology is still in the developing phase, and its large-scale industrial deployment requires increasing the life span of fuel cells and decreasing their exploitation costs. In this context, Prognostics and Health Management of fuel cells is an emerging field, which aims at identifying degradation at early stages and estimating the Remaining Useful Life (RUL) for life cycle management. Indeed, due to prognostics capability, the accurate estimates of RUL enables safe operation of the equipment and timely decisions to prolong its life span. This paper contributes data-driven prognostics of PEMFC by an ensemble of constraint based Summation Wavelet-Extreme Learning Machine (SW-ELM) algorithm to improve accuracy and robustness of long-term prognostics. The SW-ELM is used for ensemble modeling due to its enhanced applicability for real applications as compared to conventional data-driven algorithms. The proposed prognostics model is validated on run-to-failure data of PEMFC stack, which had the life span of 1750 hours. The results confirm capability of the prognostics model to achieve accurate RUL estimates.",https://ieeexplore.ieee.org/document/7125235/,2015 IEEE International Conference on Industrial Technology (ICIT),17-19 March 2015,ieeexplore
10.23919/FPL.2017.8056761,In-network online data analytics with FPGAs,IEEE,Conferences,"The growth of sensor technology, communication systems and computation have led to vast quantities of data being available for relevant parties to utilise. Applications such as the monitoring and analysis of industrial equipment, smart surveillance, and fraud detection rely on the `real-time' analysis of time sensitive data gathered from distributed sources. A variety of processing tasks, such as filtering, aggregation, machine learning algorithms, or other transformations to be carried out on this data in order to extract value from it. Centralised computation strategies are often deployed in these scenarios, with the majority of the data being forwarded though the network to a datacenter environment, typically due to the lack of required computational or storage resources at the leaves of the network, and data from other sources or historical data being required. This approach has also traditionally been viewed as more scalable, as resources can be augmented through the addition of extra compute hardware and cloud services.",https://ieeexplore.ieee.org/document/8056761/,2017 27th International Conference on Field Programmable Logic and Applications (FPL),4-8 Sept. 2017,ieeexplore
10.1109/I2MTC.2013.6555470,Increase of PLC computability with neural network for recovery of faults in electrical distribution substation,IEEE,Conferences,"Due to the increasing in technological development and modernization of industrial process, control techniques to address high performance are being developed. These not only solve new problems in more complicated plants, but also improve the performance of existing controllers. To improve the control performance of electrical distribution substations, embedded optimization techniques have been developed utilizing the programmable logic controllers (PLC). In this work an industrial artificial neural controller, that is the union between industrial controller and neural network, in associated with an intelligent electronic device (IED) for data acquisition is developed. The PLC execute operational tasks. The neural network controller performs data processing in a MATLAB environment that communicates with the PLC, to receive and to send data, via OPC protocol. The proposed methodology is evaluated in virtual electrical power substations, where the automation devices are: 1)Smart Relays, Remote Transmission Units (RTU) and PLC are real devices and 2) transformers, circuit breakers and capacitor banks simulated with software or hardware.",https://ieeexplore.ieee.org/document/6555470/,2013 IEEE International Instrumentation and Measurement Technology Conference (I2MTC),6-9 May 2013,ieeexplore
10.1109/IRCE50905.2020.9199256,Industrial Implementation and Performance Evaluation of LSD-SLAM and Map Filtering Algorithms for Obstacles Avoidance in a Cooperative Fleet of Unmanned Aerial Vehicles,IEEE,Conferences,"In this paper we present an industrial implementation and performance evaluation of the problem of obstacles detection by drones using autonomous navigation systems. The software module that has been developed as well as the tests conducted are part of a large industrial R&amp;D Vitrociset project called SWARM: an AI-Enabled Command and Control (C&amp;C) system, able to execute and review ISR missions for mini/micro cooperative fleets of heterogeneous UAVs. The presented software module, that is currently under test, has been developed to recognize obstacles and drive correctly the drones, using images acquired by low cost RGB video cameras, whose features of lightness and reduced size allow them to be installed on mini/micro UAVs. Moreover, this setup does not require special calibration and preconfiguration processes like the ones necessary for example using stereo video camera systems. The real-time recognition of obstacles in the surrounding environment has been obtained and evaluated through the implementation, performance evaluation and tests of the LSD-SLAM and map filtering algorithms; the core of the study has been realized starting from the integration of these algorithms with a simulated drone in a synthetic environment. The areas of interest have been identified through the filtering of a computer generated map: the module was then integrated into the SWARM project platform, allowing the control of a single drone's movement and making it ready for use in a cooperative fleet environment.",https://ieeexplore.ieee.org/document/9199256/,2020 3rd International Conference on Intelligent Robotic and Control Engineering (IRCE),10-12 Aug. 2020,ieeexplore
10.1109/ICAICA52286.2021.9497973,Industrial Internet Security Protection Based on an Industrial Firewall,IEEE,Conferences,"A crucial step in the development of a security system for the industrial Internet is the implementation of an industrial firewall as the first line of defense for the multi-layer defense-in-depth system and an important safeguard for industrial network security. In the design, development, deployment, application, and maintenance of industrial firewalls, the firewall performance and architecture are vital aspects. This thesis focuses on the analysis and discussion of the requirements and abilities of an industrial firewall in terms of adaptability, network isolation, industrial communication protocol identification, filtering and analysis, real-time performance and reliability, and self-protection.",https://ieeexplore.ieee.org/document/9497973/,2021 IEEE International Conference on Artificial Intelligence and Computer Applications (ICAICA),28-30 June 2021,ieeexplore
10.1109/GIOTS49054.2020.9119497,Industrial IoT and Digital Twins for a Smart Factory : An open source toolkit for application design and benchmarking,IEEE,Conferences,"The rapid evolution of digital technology and designed intelligence, such as the Internet of Things (IoT), Big data analytics, Artificial Intelligence (AI), Cyber Physical Systems (CPS), has been a catalyst for the 4th industrial revolution (known as industry 4.0). Among other, the two key state-of-the-art concepts in Industry 4.0, are Industrial IoT (IIoT) and digital twins. IIoT facilitates real-time data acquisition, processing and analytics over large amount of sensor data streams produced by sensors installed within a smart factory, while the `digital twin' concept aims to enable smart factories via the digital replication or representation of physical machines, processes, people in cyber-space. This paper explores the capability of present-state open-source platforms to collectively achieve digital twin capabilities, including IoT real-time data acquisition, virtual representation, analytics, and visualisation. The aim of this work is to `close the gap' between research and implementation, through a collective open source IoT and Digital Twin architecture. The performance of the open-source architecture in this work, is demonstrated in a use-case utilising industry `open data', and is bench-marked with universal testing tools.",https://ieeexplore.ieee.org/document/9119497/,2020 Global Internet of Things Summit (GIoTS),3-3 June 2020,ieeexplore
10.1109/CASE48305.2020.9216902,Industrial Robot Grasping with Deep Learning using a Programmable Logic Controller (PLC),IEEE,Conferences,"Universal grasping of a diverse range of previously unseen objects from heaps is a grand challenge in e-commerce order fulfillment, manufacturing, and home service robotics. Recently, deep learning based grasping approaches have demonstrated results that make them increasingly interesting for industrial deployments. This paper explores the problem from an automation systems point-of-view. We develop a robotics grasping system using Dex-Net, which is fully integrated at the controller level. Two neural networks are deployed on a novel industrial AI hardware acceleration module close to a PLC with a power footprint of less than 10 W for the overall system. The software is tightly integrated with the hardware allowing for fast and efficient data processing and real-time communication. The success rate of grasping an object form a bin is up to 95% with more than 350 picks per hour, if object and receptive bins are in close proximity. The system was presented at the Hannover Fair 2019 (world's largest industrial trade fair) and other events, where it performed over 5,000 grasps per event.",https://ieeexplore.ieee.org/document/9216902/,2020 IEEE 16th International Conference on Automation Science and Engineering (CASE),20-21 Aug. 2020,ieeexplore
10.1109/IIHMSP.2010.149,Industrial Workflows Recognition by Computer Vision and AI Technologies,IEEE,Conferences,"The introduction of the Gigabit-Ethernet for Machine Vision opened a new road for workflows recognition in industrial and SMEs environments. The network centric approach to running a production level vision system are coming on stream, which provide distributed computing across industrial developments. Humans and machines interaction can be exploited and surveyed using artificial intelligence algorithms applied on signals from multiple standalone agents on camera networks and giving to the end user, a full view of abnormal or alarm events conditions during the workflows execution. This article is presenting the architecture of such a system, where the designer of the workflows, plus the engineer of the workflow surveillance system should consider, in order either to simulate the industrial process offline, or during the real time workflow execution in the SMEs or in the industrial environment. The related methodology is based on Java technologies, which are presented and latest innovations from the multi agents and workflow processes composition.",https://ieeexplore.ieee.org/document/5636265/,2010 Sixth International Conference on Intelligent Information Hiding and Multimedia Signal Processing,15-17 Oct. 2010,ieeexplore
10.1109/MetroInd4.0IoT48571.2020.9138310,Infrared Thermography and Image Processing applied on Weldings Quality Monitoring,IEEE,Conferences,"The present paper proposes a methodological approach to evaluate welding quality in the context of tank production. In particular, infrared thermography was adopted to control the structural homogeneity achieved after welding. The analysis was implemented by applying the K-Means algorithm, morphological image processing and artificial neural network based on the Long Short Term Memory - LSTM - technique. The adopted approach was chosen to perform different image post-processing analysis and therefore highlights, identifies, and quantifies welding inhomogeneities, as cracks and defects. The work was developed within the research framework of an industrial project. The proposed approach could be implemented in inline production systems which integrate an artificial intelligence processor for real time quality monitoring.",https://ieeexplore.ieee.org/document/9138310/,2020 IEEE International Workshop on Metrology for Industry 4.0 & IoT,3-5 June 2020,ieeexplore
10.1109/ICPHYS.2018.8387672,Integrated IPC for data-driven fault detection,IEEE,Conferences,"Condition monitoring and fault detection are of critical importance in modern industrial processes for efficient and productive machine operation. Early and accurate detection of faults facilitates in the efficient and secure operation of the plant by preventing losses due to machinery breakdown. In this study, we present a novel and rapid fault detection methodology using an integrated Industrial PC. The approach is based on integrating existing hardware components and software libraries for efficient application of machine learning algorithms to an industrial process. The Industrial PC is integrated within the fieldbus system of a Bulk Good Laboratory Plant providing superior network security as compared to an external connection to a cloud platform. Additionally, the Industrial PC provides predominantly better numerical computation functionality as compared to traditional PLC environment due to the availability of machine learning libraries in high-level languages. The fault detection procedure is accomplished with the Principal Component Analysis dimensionality reduction algorithm along with Hotelling's T<sup>2</sup> statistic. A variety of sensor fault cases pertinent to the Bulk Good Laboratory Plant are analysed and experimental results show that all fault cases were detected with low detection delays. The time required to detect faults reflects the real-time capabilities of the system in an industrial scenario.",https://ieeexplore.ieee.org/document/8387672/,2018 IEEE Industrial Cyber-Physical Systems (ICPS),15-18 May 2018,ieeexplore
10.1109/APPEEC.2010.5448235,Integrated Modeling and Simulating of the Three-axis Turbine Power Generation based on the Neural Network Identification,IEEE,Conferences,"Gas turbine units are used in industrial applications more and more widely, and many people have researched widely in the fault diagnosis technology and the control technology for the gas turbine units. Simulation of Gas Turbine technology is the basis and the mathematical model of real-time is the basis of all relevant digital simulation. Using the neural network recognition technology of the radial basis function and the curve-fitting technology of part-characteristics, the mathematical model of three-axis gas turbine is established. At the same time the simulation model is established based on Matlab/simulink software. The dynamic simulation of the gas turbine for the burden loading and reducing has been researched and the response of for the output speed and fuel capacity is obtained. It is shown that the model of the gas turbine has the fast learning speed, high sampling rate and high accuracy.",https://ieeexplore.ieee.org/document/5448235/,2010 Asia-Pacific Power and Energy Engineering Conference,28-31 March 2010,ieeexplore
10.1109/ISIC.1988.65409,Integrated architecture for intelligent control,IEEE,Conferences,"A novel architecture for controlling and managing large-scale intelligent systems is presented, in which the different expert systems and numerical computation routines are coordinated by a metasystem. These expert systems and numerical routines may be written in different languages or programming tools, debugged and used separately. In this way, it is possible to easily add new programs and reduce the scope of rule search to enhance efficiency. Moreover, the optimal solution can be selected among the conflict results, and parallel processing become feasible in the integrated intelligent system. It is concluded that this architecture can serve as a universal configuration to develop high-performance intelligent systems for many complicated industrial applications in real-world domains.&lt;<ETX>&gt;</ETX>",https://ieeexplore.ieee.org/document/65409/,Proceedings IEEE International Symposium on Intelligent Control 1988,24-26 Aug. 1988,ieeexplore
10.1109/MoRSE48060.2019.8998694,Integration of Blockchains with Management Information Systems,IEEE,Conferences,"In the era of the fourth industrial revolution (In-dustry 4.0), many Management Information Systems (MIS) integrate real-time data collection and use technologies such as big data, machine learning, and cloud computing, to foster a wide range of creative innovations, business improvements, and new business models and processes. However, the integration of blockchain with MIS offers the blockchain trilemma of security, decentralisation and scalability. MIS are usually Web 2.0 client-server applications that include the front end web systems and back end databases; while blockchain systems are Web 3.0 decentralised applications. MIS are usually private systems that a single party controls and manages; while blockchain systems are usually public, and any party can join and participate. This paper clarifies the key concepts and illustrates with figures, the implementation of public, private and consortium blockchains on the Ethereum platform. Ultimately, the paper presents a framework for building a private blockchain system on the public Ethereum blockchain. Then, integrating the Web 2.0 client-server applications that are commonly used in MIS with Web 3.0 decentralised blockchain applications.",https://ieeexplore.ieee.org/document/8998694/,"2019 International Conference on Mechatronics, Robotics and Systems Engineering (MoRSE)",4-6 Dec. 2019,ieeexplore
10.1109/SDS.2018.8370412,Intel Optane™ technology as differentiator for internet of everything and fog computing,IEEE,Conferences,"Traditional network and cloud solutions cannot address effectively the infrastructure and data architecture challenges introduced by the emergence of the Internet of Everything (IoE). IoE creates unprecedented volume of complex data which must be stored, transferred, processed and analyzed in real time for time sensitive applications. Indeed, most of the applications in IoE domain (autonomous driving, health monitoring, banking, industrial control systems ...) are time and quality of service sensitive and need new solutions for data integrity and data availability for successful decision. At Intel, we are working on new computer paradigm and storage technologies such as Intel® Optane™ SSDs and Intel® 3D NAND SSDs that can address some of these challenges. With Intel® Optane™, it is now possible to create a high performance and real time available Software-Defined Infrastructure (SDI) by enabling full dis-aggregate and pool of the underlying hardware resources, creating distributed memory/storage imperatives and giving research community and enterprise the performance and capabilities to benefit from revolutionary technologies such as Fog Computing and Artificial Intelligence by dynamically assigning compute, storage and network resource in real-time time sensitive workloads. In this keynote, we will discuss data storage, availability, integrity, quality of service and movement in this compute paradigm shift. We will also touch on how it impacts mobile edge, fog computing and cloud infrastructures. We will then conclude with the challenges and opportunities this new solution will bring.",https://ieeexplore.ieee.org/document/8370412/,2018 Fifth International Conference on Software Defined Systems (SDS),23-26 April 2018,ieeexplore
10.1109/MSN48538.2019.00085,Intelligent Log Analysis System for Massive and Multi-Source Security Logs: MMSLAS Design and Implementation Plan,IEEE,Conferences,"In the Internet of Things and industrial controlnetwork servers, a large number of logs will be formed everymoment. This log information, as an important basis for eventrecording and security auditing, provides important informa-tion for identifying threat sources, identifying threat degreeand judging threat impact. However, the current security loganalysis system usually only standardizes the logs separately, and lacks the correlation analysis of the information fromvarious sources. Thus, this paper presents an intelligent loganalysis system for massive and multi-source security logs-MMSLAS(Massive and Multi-Source Security Log AnalysisSystem). In the log analysis module, the system integratesbusiness rule analysis and behavior analysis and additionallyadopts a machine learning-based analysis method, which fullyexploits the correlation between security logs and realizes thecomprehensive analysis of multi-source security logs. At thesame time, the distributed architecture scheme is also sufficientto cope with the system load caused by a large amount ofdata. The final implementation results show that MMSLAScan quickly locate the improper behavior in the log, and detectthe abnormal requests in advance according to the analysis ofthe behavior trajectory.",https://ieeexplore.ieee.org/document/9066044/,2019 15th International Conference on Mobile Ad-Hoc and Sensor Networks (MSN),11-13 Dec. 2019,ieeexplore
10.1109/ICIN51074.2021.9385543,Intelligent Monitoring of IoT Devices using Neural Networks,IEEE,Conferences,"The Internet of Things (IoT) has seen expeditious growth in recent times with 7 billion connected devices in 2020, thus leading to the vital importance of real-time monitoring of IoT devices. Through this paper, we demonstrate the idea of building a cloud-native application to monitor smart home devices. The application intends to provide valuable performance metrics from the perspective of end-users and react to anomalies in real-time. In this demo paper, we conduct the demonstration using Autoencoder (an unsupervised technique) based Deep Neural Networks (DNNs) to learn the normal operating conditions of power consumption of smart devices. When an anomaly is detected, the DNNs take proactive action and send appropriate commands back to the device. In addition, the users are provided with a real-time graphical representation of power consumption. This will help to save electricity on a domestic as well as industrial level. Finally, we discuss the future prospects of monitoring IoT devices.",https://ieeexplore.ieee.org/document/9385543/,"2021 24th Conference on Innovation in Clouds, Internet and Networks and Workshops (ICIN)",1-4 March 2021,ieeexplore
10.1109/GCCE.2014.7031179,Intelligent PID control based on PSO type NN for USM,IEEE,Conferences,"Ultrasonic Motors (USMs) are a kind of actuators with attractive features. Owing to the features, they are expected to be applied widely in industrial fields. Especially, since they work well in or near MRI environment, they are expected to play more important roles in medical and welfare area. In this research, for the control of USM, an intelligent PID control method using Neural Network (NN) combined with type Particle Swarm Optimization (PSO) is developed. In the method, the intelligent controller is designed based on variable gain type PID control using NN. The learning of the NN unit is implemented by the PSO. The PID gains are adjusted by the intelligent controller in real-time environment. The effectiveness of the method is confirmed by experiments.",https://ieeexplore.ieee.org/document/7031179/,2014 IEEE 3rd Global Conference on Consumer Electronics (GCCE),7-10 Oct. 2014,ieeexplore
10.1109/CCECE.2009.5090245,Intelligent speed controllers for IPM motor drives,IEEE,Conferences,"In this paper the comparative performances of the interior permanent magnet synchronous motor (IPMSM) drive system using proportional integral (PI) controller, proportional integral derivative (PID) controller, adaptive neural network (NN) controller, and wavelet based multiresolution proportional integral derivative (MRPID) controller are presented. In the proposed wavelet based MRPID controller, the discrete wavelet transform is used to decompose the error between actual and command speeds into different frequency components at various scales. The wavelet transformed coefficients of different scales are scaled by their respective gains, and then are added together to generate the control signal. The performances of the IPMSM drive system are investigated in simulation and experiments at different dynamic operating conditions. The vector control scheme of the conventional and proposed speed controllers based IPMSM drive system is successfully implemented in real-time using the digital signal processor board ds1102 on the laboratory 1-hp IPMSM. The simulation and laboratory test results confirm the superiority of the proposed wavelet based MRPID controller over the conventional speed controllers for wide spread applications in high performance industrial motor drive systems.",https://ieeexplore.ieee.org/document/5090245/,2009 Canadian Conference on Electrical and Computer Engineering,3-6 May 2009,ieeexplore
10.1109/CIMCA.2006.133,International Conference on Computational Inteligence for Modelling Control and Automation and International Conference on Intelligent Agents Web Technologies and International Commerce - Title,IEEE,Conferences,"The following topics are dealt with: intelligent agents and ontologies; data mining, knowledge discovery and decision making; intelligent systems; Web technologies and Web services; virtual reality and games; image processing and image understanding techniques; adaptive control and automation; modelling, prediction and control; multi-agent systems and computational intelligence; agent systems, personal assistant agents and profiling; fuzzy systems for industrial automation; control strategies; neural network applications; clustering, classification, data mining and risk analysis; dynamics systems; innovative control systems, hardware design and implementation; robotics and automation; e-business, e-commerce, innovative Web applications; Web databases; diagnosis and medical applications; learning systems; optimization, hybrid systems, genetic algorithms and evolutionary computation control applications; online learning and ERP; knowledge acquisition and classification; nanomechatronics; simulation and control; mobile network applications; information retrieval; Bayesian networks; human computer interaction; cognitive science; mobile agents; knowledge management; intelligent control; e-search and navigation; security.",https://ieeexplore.ieee.org/document/4052645/,2006 International Conference on Computational Inteligence for Modelling Control and Automation and International Conference on Intelligent Agents Web Technologies and International Commerce (CIMCA'06),28 Nov.-1 Dec. 2006,ieeexplore
10.1109/WF-IoT51360.2021.9595035,Interpretable Multi-Step Production Optimization Utilizing IoT Sensor Data,IEEE,Conferences,"In an industrial manufacturing process, such as petroleum, chemical, and food processing, with the deployment of thousands of sensors in the plants, we have the chance to provide real-time onsite management for the processes. Beyond the real-time status update, utilizing vast IoT data and creating machine learning and optimization models provide us with intelligent business recommendations. Those are used by the site engineers and managers to make real-time decisions in a situation with multiple conflicting operational and business goals. Those goals include maximizing financial gain, minimizing costs, limiting the usage of certain raw materials or additives, decreasing environmental impact, and more. When formalizing these decision-making tasks, often there is no prior knowledge of compromise between the conflicting goals. That poses a challenge to generate a proper objective function. In this paper, we create a Multi-Step optimization process to address this uncertainty of selecting proper objectives and their preferences. Instead of using an explicit trade-off to create a single weighted objective function (as a traditional approach) and rely on a single attempt to find the optimal solution, we decompose this problem into multiple steps. In each step, we optimize only one objective from one KPI with an exact semantic meaning. We demonstrate the usability of the approach using a practical application from an oil sands processing facility, provide modeling results focusing on the response to business priorities, performance, and interpretability. The multi-step approach presents the convergence of the target goal with an outcome KPI with comparison for each step to illustrate the enhanced interpretability.",https://ieeexplore.ieee.org/document/9595035/,2021 IEEE 7th World Forum on Internet of Things (WF-IoT),14 June-31 July 2021,ieeexplore
10.1109/MetroInd4.0IoT48571.2020.9138288,Introducing a cloud based architecture for the distributed analysis of Real-Time Ethernet traffic,IEEE,Conferences,"The use of industrial communication protocols based on Real-Time Ethernet (RTE) standards is completely replacing traditional industrial fieldbuses. As usual, when a technology becomes mature, the need of efficient diagnostic and maintenance tools quickly raises. Very often, following the paradigm of Industry 4.0, the most effective diagnostic systems are today based on distributed, cloud-centric, architectures and artificial intelligence. However, the distributed analysis of RTE systems is challenging, considering the plurality of protocols and the stringent cost constrains which are common in industry. In this paper, a new architecture for the distributed analysis of RTE networks is proposed, leveraging on distributed probes that send traffic samples to Matlab cloud for remote analysis. The paper also proposes a software conversion tool to adapt general PCAP files captured by popular sniffers (e.g. Wireshark) into MAT file for easier Matlab elaboration. Last, a test bench for characterization (in terms of transfer delays) of the first part of the chain for RTE traffic sampling is described. The results show that in less than 10 seconds it is possible to transfer chunks of RTE traffic data (captured on industrial networks with hundreds of real-time devices) directly to the cloud and to have them converted in Matlab format.",https://ieeexplore.ieee.org/document/9138288/,2020 IEEE International Workshop on Metrology for Industry 4.0 & IoT,3-5 June 2020,ieeexplore
10.1109/ISSCS52333.2021.9497411,Inverted Pendulum Control with a Robotic Arm using Deep Reinforcement Learning,IEEE,Conferences,"Inverted pendulum control is a benchmark control problem that researchers have used to test the new control strategies over the past 50 years. Deep Reinforcement Learning Algorithm is used recently on the inverted pendulum on a straightforward form. The inverted pendulum had only one degree of freedom and was moving on a plane. This paper demonstrates a successful implementation of a deep reinforcement learning algorithm on an inverted pendulum that rotates freely on a spherical joint with an industrial 6 degrees freedom robot arm. This research used the Deep Reinforcement Learning algorithm in Robot Operating System (ROS) and Gazebo Simulation. Experimental results show that the proposed method achieved promising outputs and reaches the control objectives. We were able to control the inverted pendulum upward for 30 and 20 seconds in two case studies. Two other significant novelties in this research are using an inertial measurement unit (IMU) on the tip of the pendulum, that will facilitate implementation on the real robot for future work and different reward functions in comparing to past publications that enable continuous learning and mastering control in a vertical position",https://ieeexplore.ieee.org/document/9497411/,"2021 International Symposium on Signals, Circuits and Systems (ISSCS)",15-16 July 2021,ieeexplore
10.1109/PerComWorkshops48775.2020.9156201,"Invited Talk: Software Engineering, AI and autonomous vehicles: Security assurance",IEEE,Conferences,"In this talk, I will first walk through some real industrial requirements and research challenges in autonomous vehicles. I will then talk about research works which can potentially solve these issues, mainly covering training, testing and anomaly detection for autonomous systems and driver behaviour detection. The talk will be broad but covering some state of the art interesting research questions and directions in autonomous vehicles safety and security assurance, and human vehicle interaction which shall be suitable both for researchers and industry practitioners for in-depth enquiry and collaboration.",https://ieeexplore.ieee.org/document/9156201/,2020 IEEE International Conference on Pervasive Computing and Communications Workshops (PerCom Workshops),23-27 March 2020,ieeexplore
10.1109/PARC49193.2020.236616,IoT and Cloud Computing based Smart Water Metering System,IEEE,Conferences,"This paper focuses on the developmental and implementation methodology of smart water meter based on Internet of Things (IoT) and Cloud computing equipped with machine learning algorithms, to differentiate between normal and excessive water usage at industrial, domestic and all other sectors having an abundance of water usage, both for Indian and worldwide context. Recognizing that intelligent metering of water has the potential to alter customer engagement of water usage in urban and rural water supplies, this paper fosters for sustainable water management, a need of the present. With shrinking reserves of clean water resources worldwide, it is becoming cumbersome to cater for this resource to masses in the coming years on a consistent basis. Using our smart water meter, water resources can be managed efficiently and an optimum use could save water for the future generations. Sensors will provide for real time monitoring of hydraulic data, automated control and alarming from Cloud platform in case of events such as water leakages, excessive usage, etc. Analysis of the same will help in taking meaningful actions. Thus we do propose for a smart water metering technology that can be utilized by Indian citizens, and worldwide, to curb wastage of water. With an ease of monitoring and visualization of the data through the Cloud platform combined with machine learning based tools to detect excess water consumption, the server-less architecture we propose can be easily adopted and implemented in a large scale.",https://ieeexplore.ieee.org/document/9087024/,2020 International Conference on Power Electronics & IoT Applications in Renewable Energy and its Control (PARC),28-29 Feb. 2020,ieeexplore
10.1109/CCGrid51090.2021.00075,IoTwins: Design and Implementation of a Platform for the Management of Digital Twins in Industrial Scenarios,IEEE,Conferences,"With the increase of the volume of data produced by IoT devices, there is a growing demand of applications capable of elaborating data anywhere along the IoT-to-Cloud path (Edge/Fog). In industrial environments, strict real-time constraints require computation to run as close to the data origin as possible (e.g., IoT Gateway or Edge nodes), whilst batch-wise tasks such as Big Data analytics and Machine Learning model training are advised to run on the Cloud, where computing resources are abundant. The H2020 IoTwins project leverages the digital twin concept to implement virtual representation of physical assets (e.g., machine parts, machines, production/control processes) and deliver a software platform that will help enterprises, and in particular SMEs, to build highly innovative, AI-based services that exploit the potential of IoT/Edge/Cloud computing paradigms. In this paper, we discuss the design principles of the IoTwins reference architecture, delving into technical details of its components and offered functionalities, and propose an exemplary software implementation.",https://ieeexplore.ieee.org/document/9499575/,"2021 IEEE/ACM 21st International Symposium on Cluster, Cloud and Internet Computing (CCGrid)",10-13 May 2021,ieeexplore
10.1109/FMEC.2018.8364035,Keynote speech 3: Intel Optane™ technology as differentiator for Internet of everything and fog computing,IEEE,Conferences,"Traditional network and cloud solutions cannot address effectively the infrastructure and data architecture challenges introduced by the emergence of the Internet of Everything (IoE). IoE creates unprecedented volume of complex data which must be stored, transferred, processed and analyzed in real time for time sensitive applications. Indeed, most of the applications in IoE domain (autonomous driving, health monitoring, banking, industrial control systems ...) are time and quality of service sensitive and need new solutions for data integrity and data availability for successful decision. At Intel, we are working on new computer paradigm and storage technologies such as Intel® Optane™ SSDs and Intel® 3D NAND SSDs that can address some of these challenges. With Intel® Optane™, it is now possible to create a high performance and real time available Software-Defined Infrastructure (SDI) by enabling full dis-aggregate and pool of the underlying hardware resources, creating distributed memory/storage imperatives and giving research community and enterprise the performance and capabilities to benefit from revolutionary technologies such as Fog Computing and Artificial Intelligence by dynamically assigning compute, storage and network resource in real-time time sensitive workloads. In this keynote, we will discuss data storage, availability, integrity, quality of service and movement in this compute paradigm shift. We will also touch on how it impacts mobile edge, fog computing and cloud infrastructures. We will then conclude with the challenges and opportunities this new solution will bring.",https://ieeexplore.ieee.org/document/8364035/,2018 Third International Conference on Fog and Mobile Edge Computing (FMEC),23-26 April 2018,ieeexplore
10.1109/PerComWorkshops51409.2021.9430865,Keyword Spotting for Industrial Control using Deep Learning on Edge Devices,IEEE,Conferences,"Spoken commands promise unique advantages for the control of industrial machinery. Operators are enabled to keep their eyes on safety critical aspects of the process at all times and are free to use their hands in other parts of the process, instead of remote control. Current keyword spotting systems are prone to misunderstanding spoken utterances, especially in noisy environments, and are commonly deployed as non-realtime cloud services. Consequently, these systems can not be trusted with safety critical industrial control. We adapt a DS-CNN and a CNN for keyword spotting and use augmented training data, including real industrial noise, to increase their robustness. Furthermore, we apply post-training quantization and analyze the performance of both networks using multiple embedded systems, including a Google Edge TPU. We carry out a systematic analysis of accuracies, memory footprint and inference times using different combinations of data augmentations, hardware platforms, and quantizations. We show that augmented training data increases the inference accuracy in noisy environments by up to 20 %. Among others, this is demonstrated using an integer quantized network with a memory footprint of 0.57 MByte, reaching inference speeds of less than 5 ms on an embedded CPU and less than 1 ms on the Edge TPU. The results show that keyword spotting for industrial control is feasible on embedded systems and that the training data augmentation has a significant impact on the robustness in challenging environments.",https://ieeexplore.ieee.org/document/9430865/,2021 IEEE International Conference on Pervasive Computing and Communications Workshops and other Affiliated Events (PerCom Workshops),22-26 March 2021,ieeexplore
10.23919/ACC.1988.4789792,Knowledge-Based Approach to Real-Time Supervisory Control,IEEE,Conferences,"Increasing complexity of industrial plants necessitates the usage of advanced programming techniques in process control. Artificial Intelligence programming offers new opportunities in constructing complex software systems. This paper describes an approach which has been used to build Intelligent Process Control System (IPCS). IPCS expand the conventional process control systems with an additional, ""knowledge-based layer"", where declarative programming methods are used extensively. The knowledge-based components represent a model of the process from multiple viewpoints and the simulator, monitor, control, diagnostics and operator interface subsystems are coupled to a symbolic process database. A special architecture, the Multigraph Architecture has been used as a general framework for integrating symbolic and numeric programming techniques in a distributed environment. The knowledge-based components have been implemented as ""Autonomous Communicating Objects"". The application of concurrent programming techniques made possible the introduction of new approaches in the design of the monitoring and diagnostic system. A prototype IPCS has been implemented on a network of VAX computers.",https://ieeexplore.ieee.org/document/4789792/,1988 American Control Conference,15-17 June 1988,ieeexplore
10.1109/INDIN.2015.7281881,Knowledge-driven finite-state machines. Study case in monitoring industrial equipment,IEEE,Conferences,Traditionally state machines are implemented by coding the desired behavior of a given system. This work proposes the use of ontological models to describe and perform computations on state machines by using SPARQL queries. This approach represents a paradigm shift relating to the customary manner in which state machines are stored and computed. The main contribution of the work is an ontological model to represent state machines and a set of generic queries that can be used in any knowledge-driven state machine to compute valuable information. The approach was tested in a study case were the state machines of industrial robots in a manufacturing line were modeled as ontological models and used for monitoring the behavior of these devices on real time.,https://ieeexplore.ieee.org/document/7281881/,2015 IEEE 13th International Conference on Industrial Informatics (INDIN),22-24 July 2015,ieeexplore
10.1109/CVPR.2019.00655,L3-Net: Towards Learning Based LiDAR Localization for Autonomous Driving,IEEE,Conferences,"We present L3-Net - a novel learning-based LiDAR localization system that achieves centimeter-level localization accuracy, comparable to prior state-of-the-art systems with hand-crafted pipelines. Rather than relying on these hand-crafted modules, we innovatively implement the use of various deep neural network structures to establish a learning-based approach. L3-Net learns local descriptors specifically optimized for matching in different real-world driving scenarios. 3D convolutions over a cost volume built in the solution space significantly boosts the localization accuracy. RNNs are demonstrated to be effective in modeling the vehicle's dynamics, yielding better temporal smoothness and accuracy. We comprehensively validate the effectiveness of our approach using freshly collected datasets. Multiple trials of repetitive data collection over the same road and areas make our dataset ideal for testing localization systems. The SunnyvaleBigLoop sequences, with a year's time interval between the collected mapping and testing data, made it quite challenging, but the low localization error of our method in these datasets demonstrates its maturity for real industrial implementation.",https://ieeexplore.ieee.org/document/8954371/,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),15-20 June 2019,ieeexplore
10.1109/TELE52840.2021.9482484,Learning Activities for Mastering Flexible Roof Constructions,IEEE,Conferences,"Flexible roofs become an integral part of modern building constructions. Being widely used for public buildings, they have great potential in the field of industrial construction as well. Flexible roofs, however, possess complex structural behavior. They require deep understanding of the key concepts by designers and project managers, who implement them into a real life. Civil engineering and architectural specializations need to incorporate flexible constructions into the academic activity. It will ensure competitiveness of prospective specialists in the job market. Active learning approach is considered for mastering flexible roof structures. Prototyping is an essential tool for long-term memorization of the key concepts. Modern computer technologies and specialized software packages, however, gradually displace laboratory modeling. Structural deformability, geometrically nonlinear behavior, preliminary stressing and equilibrium structural shape can be easily grasped by means of computer simulation and numerical investigation of exemplary constructions.",https://ieeexplore.ieee.org/document/9482484/,2021 1st International Conference on Technology Enhanced Learning in Higher Education (TELE),24-25 June 2021,ieeexplore
10.1109/MSR52588.2021.00019,Learning Off-By-One Mistakes: An Empirical Study,IEEE,Conferences,"Mistakes in binary conditions are a source of error in many software systems. They happen when developers use, e.g., `&lt;;' or `&gt;' instead of `&lt;;=' or `&gt;='. These boundary mistakes are hard to find and impose manual, labor-intensive work for software developers. While previous research has been proposing solutions to identify errors in boundary conditions, the problem remains open. In this paper, we explore the effectiveness of deep learning models in learning and predicting mistakes in boundary conditions. We train different models on approximately 1.6M examples with faults in different boundary conditions. We achieve a precision of 85% and a recall of 84% on a balanced dataset, but lower numbers in an imbalanced dataset. We also perform tests on 41 real-world boundary condition bugs found from GitHub, where the model shows only a modest performance. Finally, we test the model on a large-scale Java code base from Adyen, our industrial partner. The model reported 36 buggy methods, but none of them were confirmed by developers.",https://ieeexplore.ieee.org/document/9463090/,2021 IEEE/ACM 18th International Conference on Mining Software Repositories (MSR),17-19 May 2021,ieeexplore
10.1109/ETFA.2019.8869172,Learning based Probabilistic Model for Migration of Industrial Control Systems,IEEE,Conferences,"The updating and upgrading of control systems is a cumbersome, expensive and time consuming task. From a software perspective, control system migration is a collective task of migrating the control logic, Human Machine Interface (HMI) and auxiliary software applications. Migrating control logic is the most challenging task owing to constraints on hard real-time behavior and execution order. Control logic typically contains engineering artifacts that specify the functionality of industrial devices taking into account various parameters. Therefore, to migrate from one Distributed Control System (DCS) system to another or to upgrade the existing DCS, one needs to map the source control entity and their parameters to the appropriate control entities in the target DCS.In this paper, we propose a machine learning based suggestion management system that identifies control entities and parameters for a source DCS and suggests the use of similar control entities and corresponding parameters for the target DCS. This in effect saves effort required in mapping of control parameters and reduces the dependence on subject matter experts. Our system uses a probabilistic approach to find these similarity mappings based on meta-data stored in an Ontology. We further describe a case study implemented for mapping heritage and legacy systems to a modern control system to verify and validate our approach.",https://ieeexplore.ieee.org/document/8869172/,2019 24th IEEE International Conference on Emerging Technologies and Factory Automation (ETFA),10-13 Sept. 2019,ieeexplore
10.1109/INDIN45523.2021.9557472,Learning-based Co-Design of Distributed Edge Sensing and Transmission for Industrial Cyber-Physical Systems,IEEE,Conferences,"Industrial cyber-physical systems (ICPS) refer to an emerging generation of intelligent systems, where distributed data acquisition is of great importance and is influenced by data transmission. In the improvement of the overall performance of sensing accuracy and energy efficiency, sensing and transmission are tightly coupled. Due to the unknown transmission channel states in the harsh industrial field environment, intelligently performing sensor scheduling for distributed sensing is challenging. In this paper, edge computing technology is utilized to enhance the level of intelligence at the edge side and deploy advanced scheduling algorithms. We propose a learning-based distributed edge sensing-transmission co-design (LEST) algorithm under the coordination of the sensors and the edge computing unit (ECU). Deep reinforcement learning is applied to perform real-time sensor scheduling under unknown channel states. The conditions for the existence of feasible scheduling policies are analyzed. The proposed algorithm is applied to estimate the slab temperature in the hot rolling process, which is a typical ICPS. The simulation results demonstrate that the overall performance of LEST is better than other suboptimal algorithms.",https://ieeexplore.ieee.org/document/9557472/,2021 IEEE 19th International Conference on Industrial Informatics (INDIN),21-23 July 2021,ieeexplore
10.1109/CVPR46437.2021.01493,LightTrack: Finding Lightweight Neural Networks for Object Tracking via One-Shot Architecture Search,IEEE,Conferences,"Object tracking has achieved significant progress over the past few years. However, state-of-the-art trackers become increasingly heavy and expensive, which limits their deployments in resource-constrained applications. In this work, we present LightTrack, which uses neural architecture search (NAS) to design more lightweight and efficient object trackers. Comprehensive experiments show that our LightTrack is effective. It can find trackers that achieve superior performance compared to handcrafted SOTA trackers, such as SiamRPN++ [30] and Ocean [56], while using much fewer model Flops and parameters. Moreover, when deployed on resource-constrained mobile chipsets, the discovered trackers run much faster. For example, on Snapdragon 845 Adreno GPU, LightTrack runs 12× faster than Ocean, while using 13× fewer parameters and 38× fewer Flops. Such improvements might narrow the gap between academic models and industrial deployments in object tracking task. LightTrack is released at here.",https://ieeexplore.ieee.org/document/9578709/,2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),20-25 June 2021,ieeexplore
10.1109/ETFA.2018.8502485,Linear Classification of Badly Conditioned Data,IEEE,Conferences,"We present a method for the fast and robust linear classification of badly conditioned data. In our considerations, badly conditioned data are such data which are numerically difficult to handle. Due to, e.g. a large number of features or a large number of objects representing classes as well as noise, outliers or incompleteness, the common software computation of the discriminating linear combination of features between classes fails or is extremely time consuming. The theoretical foundations of our approach are based on the single feature ranking, which allows fast calculation of the approximative initial classification boundary. For the increasing of classification accuracy of this boundary, the refinement is performed in the lower dimensional space. Our approach is tested on several datasets from UCI Reposi-tiory. Experimental results indicate high classification accuracy of the approach. For the modern real industrial applications such a method is especially suitable in the Cyber-Physical-System environments and provides a part of the workflow for the automated classifier design.",https://ieeexplore.ieee.org/document/8502485/,2018 IEEE 23rd International Conference on Emerging Technologies and Factory Automation (ETFA),4-7 Sept. 2018,ieeexplore
10.1109/SNPD.2012.137,MAST: From a Toy to Real-Life Manufacturing Control,IEEE,Conferences,"Paper reports on the evolution of agent-based simulation and control system called MAST. The system was designed originally for agent-based simulation of product routing but over the years matured into a generic purpose manufacturing simulation and control tool featuring real-time connectivity to legacy PLCs, ontology-based dynamic scheduling, advanced diagnostics, etc. Paper describes MAST architecture, behavior of agents and capabilities for dynamic reconfiguration. In addition, two examples of application of MAST to real problems from manufacturing domain are given. The latest trend of exploitation of semantic technologies in industrial agents is discussed.",https://ieeexplore.ieee.org/document/6299316/,"2012 13th ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing",8-10 Aug. 2012,ieeexplore
10.1109/TELSIKS52058.2021.9606379,Machine Learning for Air Quality Classification in IoT-based Network with Low-cost Sensors,IEEE,Conferences,"Air pollution is a rising problem, with its effects being especially severe in urban and industrial areas. A constant and local monitoring of air quality, and the suitable presentation of the results to population, demands deployment of large-scale IoT-based monitoring networks in which low-cost, low-quality sensors would be predominantly used. However, the inherent measurement errors could incur large AQI (Air Quality Index) calculation error. Also, appropriate presentation of air pollution demands that measurements of air pollutants’ concentrations are classified into Air Quality Classes, thus making the classification task for AQI of large interest. In this paper we analyzed a wide variety of Machine Learning (ML) and Deep Learning (DL) models in order to solve classification task for AQI, but under the assumption of low-cost sensor deployment in the real-world application. The results of comprehensive analysis suggest that DL models designed, optimized and tested in this paper present a viable and the most suitable solution under these demands.",https://ieeexplore.ieee.org/document/9606379/,"2021 15th International Conference on Advanced Technologies, Systems and Services in Telecommunications (TELSIKS)",20-22 Oct. 2021,ieeexplore
10.1109/ICC40277.2020.9148684,Machine Learning for Predictive Diagnostics at the Edge: an IIoT Practical Example,IEEE,Conferences,"Edge Computing is becoming more and more essential for the Industrial Internet of Things (IIoT) for data acquisition from shop floors. The shifting from central (cloud) to distributed (edge nodes) approaches will enhance the capabilities of handling real-time big data from IoT. Furthermore, these paradigms allow moving storage and network resources at the edge of the network closer to IoT devices, thus ensuring low latency, high bandwidth, and location-based awareness. This research aims at developing a reference architecture for data collecting, smart processing, and manufacturing control system in an IIoT environment. In particular, our architecture supports data analytics and Artificial Intelligence (AI) techniques, in particular decentralized and distributed hybrid twins, at the edge of the network. In addition, we claim the possibility to have distributed Machine Learning (ML) by enabling edge devices to learn local ML models and to store them at the edge. Furthermore, edges have the possibility of improving the global model (stored at the cloud) by sending the reinforced local models (stored in different shop floors) towards the cloud. In this paper, we describe our architectural proposal and show a predictive diagnostics case study deployed in an edge-enabled IIoT infrastructure. Reported experimental results show the potential advantages of using the proposed approach for dynamic model reinforcement by using real-time data from IoT instead of using an offline approach at the cloud infrastructure.",https://ieeexplore.ieee.org/document/9148684/,ICC 2020 - 2020 IEEE International Conference on Communications (ICC),7-11 June 2020,ieeexplore
10.1109/21CW48944.2021.9532537,Machine diagnosis using acoustic analysis: a review,IEEE,Conferences,"Diagnosis or fault identification in real industrial machines using audio or sound signals is a challenging task. Active research has been pursued to determine acoustic features, classification &amp; clustering algorithms that could estimate the state of an industrial machine. Acoustic features &amp; classifiers from different domains have been successfully implemented for fault identification in industrial machines. This paper is a comparative study of propositions, experiments, applications and systems developed by various researchers. Effort has been made to generate a collection of test benches developed, results observed and conclusion arrived. These insights suggest deep learning and anomaly detection techniques as a promising technology for preventive maintenance in real industrial machines.",https://ieeexplore.ieee.org/document/9532537/,2021 IEEE Conference on Norbert Wiener in the 21st Century (21CW),22-25 July 2021,ieeexplore
10.1109/WCICSS.2016.7882607,Machine learning algorithms for process analytical technology,IEEE,Conferences,"Increased globalisation and competition are drivers for process analytical technologies (PAT) that enable seamless process control, greater flexibility and cost efficiency in the process industries. The paper will discuss process modelling and control for industrial applications with an emphasis on solutions enabling the real-time data analytics of sensor measurements that PAT demands. This research aims to introduce an integrated process control approach, embedding novel sensors for monitoring in real time the critical control parameters of key processes in the minerals, ceramics, non-ferrous metals, and chemical process industries. The paper presents a comparison of machine learning algorithms applied to sensor data collected for a polymerisation process. Several machine learning algorithms including Adaptive Neuro-Fuzzy Inference Systems, Neural Networks and Genetic Algorithms were implemented using MATLAB® Software and compared in terms of accuracy (MSE) and robustness in modelling process progression. The results obtained show that machine learning-based approaches produce significantly more accurate and robust process models compared to models developed manually while also being more adaptable to new data. The paper presents perspectives on the potential benefits of machine learning algorithms with a view to their future in the industrial process industry.",https://ieeexplore.ieee.org/document/7882607/,2016 World Congress on Industrial Control Systems Security (WCICSS),12-14 Dec. 2016,ieeexplore
10.1109/MMAR.2016.7575171,Machine learning based power quality event classification using wavelet — Entropy and basic statistical features,IEEE,Conferences,"Today's industrial environment is smarter than ever before. Most production lines include electrical devices which are able to communicate each other and controlled from a single station with automation systems. Most of those elements have an internet connection link known as industrial internet. Development of smart technology with industrial internet comes with a need of monitoring. Monitoring technologies are emergent systems that focus on fault detection, grid self - healings and online tracking of power quality issues. Present study deals with one of the essential part of an electricity grid monitoring system called power quality event classification in a manner of machine learning topic. Power quality events to be processed are generated synthetically by means of a comprehensive software tool. Classification of real-like dataset is executed using extreme learning machine which is an extremely fast learning algorithm applied to single layer neural networks. Basic statistical criteria and wavelet - entropy methods are handled to achieve distinctive features of dataset. As a performance evaluation instrument, conventional artificial neural network structure is run too. Detailed results are discussed to prove the satisfactory performance of proposed pattern recognition model.",https://ieeexplore.ieee.org/document/7575171/,2016 21st International Conference on Methods and Models in Automation and Robotics (MMAR),29 Aug.-1 Sept. 2016,ieeexplore
10.1109/CompComm.2017.8322600,Machine learning to detect anomalies in web log analysis,IEEE,Conferences,"As the information technology develops rapidly, Web servers are easily to be attacked because of their high value. Therefore, Web security has aroused great concern in both academia and industry. Anomaly detection plays a significant role in the field of Web security, and log messages recording detailed system runtime information has become an important data analysis object accordingly. Traditional log anomaly detection relies on programmers to manually inspect by keyword search and regular expression match. Although the programmers can use intrusion detection system to reduce their workload, yet the log system data is huge, attack types are various, and hacking skills are improving, which make the traditional detection not efficient enough. To improve the traditional detection technology, many of anomaly detection mechanisms have been proposed in recent years, especially the machine learning method. In this paper, an anomaly detection system for web log files has been proposed, which adopts a two-level machine learning algorithm. The decision tree model classifies normal and anomalous data sets. The normal data set is manually checked for the establishment of multiple HMMs. The experimental data comes from the real industrial environment where log files have been collected, which contain many true intrusion messages. After comparing with three types of machine learning algorithms used in anomaly detection, the experimental results on this data set suggest that this system achieves higher detection accuracy and can detect unknown anomaly data.",https://ieeexplore.ieee.org/document/8322600/,2017 3rd IEEE International Conference on Computer and Communications (ICCC),13-16 Dec. 2017,ieeexplore
10.1109/TEST.2016.7805855,Machine learning-based defense against process-aware attacks on Industrial Control Systems,IEEE,Conferences,"The modernization of Industrial Control Systems (ICS), primarily targeting increased efficiency and controllability through integration of Information Technologies (IT), introduced the unwanted side effect of extending the ICS cyber-security threat landscape. ICS are facing new security challenges and are exposed to the same vulnerabilities that plague IT, as demonstrated by the increasing number of incidents targeting ICS. Due to the criticality and unique nature of these systems, it is important to devise novel defense mechanisms that incorporate knowledge of the underlying physical model, and can detect attacks in early phases. To this end, we study a benchmark chemical process, and enumerate the various categories of attack vectors and their practical applicability on hardware controllers in a Hardware-In-The-Loop testbed. Leveraging the observed implications of the categorized attacks on the process, as well as the profile of typical disturbances, we follow a data-driven approach to detect anomalies that are early indicators of malicious activity.",https://ieeexplore.ieee.org/document/7805855/,2016 IEEE International Test Conference (ITC),15-17 Nov. 2016,ieeexplore
10.1109/MALTESQUE.2018.8368453,Machine learning-based run-time anomaly detection in software systems: An industrial evaluation,IEEE,Conferences,"Anomalies are an inevitable occurrence while operating enterprise software systems. Traditionally, anomalies are detected by threshold-based alarms for critical metrics, or health probing requests. However, fully automated detection in complex systems is challenging, since it is very difficult to distinguish truly anomalous behavior from normal operation. To this end, the traditional approaches may not be sufficient. Thus, we propose machine learning classifiers to predict the system's health status. We evaluated our approach in an industrial case study, on a large, real-world dataset of 7.5 ·10<sup>6</sup> data points for 231 features. Our results show that recurrent neural networks with long short-term memory (LSTM) are more effective in detecting anomalies and health issues, as compared to other classifiers. We achieved an area under precision-recall curve of 0.44. At the default threshold, we can automatically detect 70% of the anomalies. Despite the low precision of 31 %, the rate in which false positives occur is only 4 %.",https://ieeexplore.ieee.org/document/8368453/,2018 IEEE Workshop on Machine Learning Techniques for Software Quality Evaluation (MaLTeSQuE),20-20 March 2018,ieeexplore
10.1109/CIMSA.2006.250751,Management of Complex Dynamic Systems based on Model-Predictive Multi-objective Optimization,IEEE,Conferences,"Over the past two decades, model predictive control and decision-making strategies have established themselves as powerful methods for optimally managing the behavior of complex dynamic industrial systems and processes. This paper presents a novel model-based multi-objective optimization and decision-making approach to model-predictive decision-making. The approach integrates predictive modeling based on neural networks, optimization based on multi-objective evolutionary algorithms, and decision-making based on Pareto frontier techniques. The predictive models are adaptive, and continually update themselves to reflect with high fidelity the gradually changing underlying system dynamics. The integrated approach, embedded in a real-time plant optimization and control software environment has been deployed to dynamically optimize emissions and efficiency while simultaneously meeting load demands and other operational constraints in a complex real-world power plant. While this approach is described in the context of power plants, the method is adaptable to a wide variety of industrial process control and management applications",https://ieeexplore.ieee.org/document/4016827/,2006 IEEE International Conference on Computational Intelligence for Measurement Systems and Applications,12-14 July 2006,ieeexplore
10.1109/ICMNN.1994.593731,Massively parallel VLSI-implementation of a dedicated neural network for anomaly detection in automated visual quality control,IEEE,Conferences,"In this work we will present the VLSI-implementation of a dedicated neural network architecture which we have developed in prior work for anomaly detection in automated visual industrial quality control. The network, denoted as NOVAS performs a filtering of inspection images and highlights defects or anomalies in an isomorphic image representation, allowing the detection and localisation of faults on objects. Training of NOVAS is achieved by simply presenting a set of tolerable objects to the network in a single sweep. NOVAS works with single and with multichannel image representations. The processing principle of NOVAS is closely related to nearest neighbor and hypersphere classifier approaches. We have designed an ASIC for the efficient implementation of the nearest neighbor search. Based on that ASIC we will present an architecture of a modular massively parallel computer suited to meet the real-time constraints of manufacturing processes. Further we will report on the status of a prototype system which is close to completion.",https://ieeexplore.ieee.org/document/593731/,Proceedings of the Fourth International Conference on Microelectronics for Neural Networks and Fuzzy Systems,26-28 Sept. 1994,ieeexplore
10.1109/IDAP.2019.8875881,Measuring IEEE 802.15.4 Protocol Performance over Embedded Control Systems,IEEE,Conferences,"Using the IEEE 802.15.4 could be notice in various areas such as hospitals, agricultural, industrial, as well as smart homes. All the above-mentioned areas require devices that have a mechanism that consume low energies and low costs for communication. Thus, the purpose of fulfilling the needed requirements, IEEE 802.15.4 could be seen as effective and commercial solution. Various factors exist for studying the efficiency related to the wireless networks. Emulation could be define as a toll of high importance for designing and evaluating the implementation and the efficiency related to protocols, while the majority of real applications are considered to be of high difficulty with regard to the real applications. Concerning the presented paper, the main aim has been showing performance results, which are relates to IEEE 802.15.4 through use of octave simulations. The main task of the presented paper is analyzing the protocol's performance in various conditions as well as having the ability of checking the performance with regard to reliability in real-time. One of the aims of this study is analyzing the performance parameters, that are energy consumption rate, packet received ration as well as the Received Signal Strength Indicator (RSSI). All of the mentioned parameters studied in indoor and outdoor conditions the purpose of verifying the performance of the wireless protocol. The presented paper focus on 2 nodes in for the peer-to-peer communications, where one of the nodes acts as base-station or coordinator, while the other node is considered as router. The router include Xbee with sensors and Arduino, while the coordinator consist of Xbee receiver and adapter.",https://ieeexplore.ieee.org/document/8875881/,2019 International Artificial Intelligence and Data Processing Symposium (IDAP),21-22 Sept. 2019,ieeexplore
10.1109/SCC.2010.87,Message-Based Service Brokering and Dynamic Composition in the SAI Middleware,IEEE,Conferences,"Service-Oriented Computing (SOC) is a wide and complex research area. Despite the huge effort in both industrial and academics initiatives, several challenges need to be addressed in order to effectively realize the SOC vision. One of the most relevant issues is the need of effective, flexible, reliable, low cost solutions for dynamic service brokering and composition. This paper presents results of an ongoing work on the design and development of a service- and message-oriented middleware for atomic and composite service brokering, named SAI middleware. The SAI middleware offers a set of features for service brokering and dynamic composition, while also guaranteeing loose coupling between service providers and consumers and relaxing the prerequisites for service providers to publish their capabilities in an interoperability domain. SAI dynamic composition is based on an Artificial Intelligence planning approach and on the adoption of an ontology-based functional profile encoding information for enabling automatic information extraction and combination in the service composition chain. Our main contribution consists in addressing these issues in a holistic way, as required to effectively support the SOA vision in real application scenarios, while not optimizing single aspects yet.",https://ieeexplore.ieee.org/document/5557205/,2010 IEEE International Conference on Services Computing,5-10 July 2010,ieeexplore
10.1109/ICCC51557.2021.9454641,Methods of Artificial Intelligence for Simulation of Gasification of Biomass and Communal Waste,IEEE,Conferences,"Artificial intelligence (AI) methods can simulate accurately outcomes of gasification processes based on real data sets. Quality and composition of gas obtained from solid fuels depend on the temperature of the gasifying agent (air or steam) and the composition of used solid waste (biomass, industrial waste or combustibles from municipal solid waste, such as plastics, textile, wood, paper, tires, etc.). To simulate the gasification processes, a symbolic regression software AI Feynman is tested. Finally, the results of symbolic regression are compared with measured data. The results indicate that symbolic regression of AI Feynman is useful for modelling of biomass gasification technologies from measured data.",https://ieeexplore.ieee.org/document/9454641/,2021 22nd International Carpathian Control Conference (ICCC),31 May-1 June 2021,ieeexplore
10.1109/ASE.2017.8115693,Mining constraints for event-based monitoring in systems of systems,IEEE,Conferences,"The full behavior of software-intensive systems of systems (SoS) emerges during operation only. Runtime monitoring approaches have thus been proposed to detect deviations from the expected behavior. They commonly rely on temporal logic or domain-specific languages to formally define requirements, which are then checked by analyzing the stream of monitored events and event data. Some approaches also allow developers to generate constraints from declarative specifications of the expected behavior. However, independent of the approach, deep domain knowledge is required to specify the desired behavior. This knowledge is often not accessible in SoS environments with multiple development teams independently working on different, heterogeneous systems. In this New Ideas Paper we thus describe an approach that automatically mines constraints for runtime monitoring from event logs recorded in SoS. Our approach builds on ideas from specification mining, process mining, and machine learning to mine different types of constraints on event occurrence, event timing, and event data. The approach further presents the mined constraints to users in an existing constraint language and it ranks the constraints using different criteria. We demonstrate the feasibility of our approach by applying it to event logs from a real-world industrial SoS.",https://ieeexplore.ieee.org/document/8115693/,2017 32nd IEEE/ACM International Conference on Automated Software Engineering (ASE),30 Oct.-3 Nov. 2017,ieeexplore
10.1109/AIVR50618.2020.00017,Mirrorlabs - creating accessible Digital Twins of robotic production environment with Mixed Reality,IEEE,Conferences,How to visualize recorded production data in Virtual Reality? How to use state of the art Augmented Reality displays that can show robot data? This paper introduces an opensource ICT framework approach for combining Unity-based Mixed Reality applications with robotic production equipment using ROS Industrial. This publication gives details on the implementation and demonstrates the use as a data analysis tool in the context of scientific exchange within the area of Mixed Reality enabled human-robot co-production.,https://ieeexplore.ieee.org/document/9319071/,2020 IEEE International Conference on Artificial Intelligence and Virtual Reality (AIVR),14-18 Dec. 2020,ieeexplore
10.1109/ICMEW46912.2020.9106033,Mobile Centernet for Embedded Deep Learning Object Detection,IEEE,Conferences,"Object detection is a fundamental task in computer vision with wide application prospect. And recent years, many novel methods are proposed to tackle this task. However, most algorithms suffer from high computation cost and long inference time, which makes them impossible to be deployed on embedded devices in real industrial application scenarios. In this paper, we propose the Mobile CenterNet to solve this problem. Our method is based on CenterNet but with some key improvements. To enhance detection performance, we adopt HRNet as a powerful backbone and introduce a category-balanced focal loss to deal with category imbalance problem. Moreover, to compress the model size as well as reduce inference time, knowledge distillation is employed to transfer knowledge from cumbersome model to a compact one. We conduct experiments on a large traffic detection dataset BDD100K and validate the effectiveness of all the modifications. Finally, our method achieves the 1st place in the Embedded Deep Learning Object Detection Model Compression Competition held in ICME 2020.",https://ieeexplore.ieee.org/document/9106033/,2020 IEEE International Conference on Multimedia & Expo Workshops (ICMEW),6-10 July 2020,ieeexplore
10.1109/VRAIS.1995.512486,Model based vision as feedback for virtual reality robotics environments,IEEE,Conferences,"Task definition methods for robotic systems are often difficult to use. The ""on-line"" programming methods are often time expensive or risky for the human operator or the robot itself. On the other hand, ""off-line"" techniques are tedious and complex. In addition operator training is costly and time consuming. In a Virtual Reality Robotics Environment (VRRE), users are not asked to write down complicated functions, but can operate complex robotic systems in an intuitive and cost-effective way. However a VRRE is only effective if all the environment changes and object movements are fed-back to the virtual manipulating system. The paper describes the use of a VRRE for a semi-autonomous robot system comprising an industrial 5-axis robot, its virtual equivalent and a model based vision system used as feed-back. The user is immersed in a 3-D space built out of models of the robot's environment. He directly interacts with the virtual ""components"", defining tasks and dynamically optimizing them. A model based vision system locates objects in the real workspace to update the VRRE through a bi-directional communication link. In order to enhance the capabilities of the VRRE, a reflex-type behavior based on vision has been implemented. By locally (independently of the VRRE) controlling the real robot, the operator is discharged of small environmental changes due to transmission delays. Thus once the tasks have been optimized on the VRRE, they are sent to the real robot and a semi autonomous process ensures their correct execution thanks to a camera directly mounted on the robot's end effector. On the other hand if the environmental changes are too important, the robot stops, re-actualizes the VRRE with the new environmental configuration, and waits for task redesign. Because the operator interacts with the robotic system at a task oriented high level, VRRE systems are easily portable to other robotics environments (mobile robotics and micro assembly).",https://ieeexplore.ieee.org/document/512486/,Proceedings Virtual Reality Annual International Symposium '95,11-15 March 1995,ieeexplore
10.1109/CCA.2009.5281106,Model predictive control for nonlinear affine systems based on the simplified dual neural network,IEEE,Conferences,"Model predictive control (MPC), also known as receding horizon control (RHC), is an advanced control strategy for optimizing the performance of control systems. For nonlinear systems, standard MPC schemes based on linearization would result in poor performance. In this paper, we propose an MPC scheme for nonlinear affine systems based on a recurrent neural network (RNN) called the simplified dual network. The proposed RNN-based approach is efficient and suitable for real-time MPC implementation in industrial applications. Simulation results are provided to demonstrate the effectiveness and efficiency of the proposed MPC scheme.",https://ieeexplore.ieee.org/document/5281106/,"2009 IEEE Control Applications, (CCA) & Intelligent Control, (ISIC)",8-10 July 2009,ieeexplore
10.1109/SEAA.2015.75,Model-Based Risk Assessment in Multi-disciplinary Systems Engineering,IEEE,Conferences,"In industrial production systems engineering projects, the work of software managers depends on engineering artifacts coming from multiple disciplines. In particular, it is important to software managers to assess the project risk from the status and evolution of various heterogenous distributed engineering artifacts. Thus, software risk management is most often an error prone and cumbersome task in such projects. To tackle this challenge, we introduce a model-based foundation for risk assessment in multi-disciplinary systems engineering projects. In particular, we build on the recent modeling support for the Automation ML (AML) standard which enables representing data coming from different engineering disciplines as models and employ a linking language to reason on a set of distributed engineering artifacts and their relationships. Based on this pillars, we propose in this paper a dedicated metric suite and measurement support for AML as an important ingredient for efficient risk assessment of heterogenous and distributed engineering data. We evaluate the feasibility of the proposed approach by providing tool support on top of the Eclipse Modeling Framework (EMF) and demonstrate its application with a showcase based on a real-world case study.",https://ieeexplore.ieee.org/document/7302486/,2015 41st Euromicro Conference on Software Engineering and Advanced Applications,26-28 Aug. 2015,ieeexplore
10.1109/MODELS-C.2019.00101,Modeling Adaptive Learning Agents for Domain Knowledge Transfer,IEEE,Conferences,"The implementation of intelligent agents in industrial applications is often prevented by the high cost of adopting such a system to a particular problem domain. This paper states the thesis that when learning agents are applied to work environments that require domain-specific experience, the agent benefits if it can be further adapted by a supervising domain expert. Closely interacting with the agent, a domain expert should be able to understand its decisions and update the underlying knowledge base as needed. The result would be an agent with individualized knowledge that comes in part from the domain experts. The model of such an adaptive learning agent must take into account the problem domain, the design of the learning agent and the perception of the domain user. Therefore, already in the modeling phase, more attention must be paid to make the learning element of the agent adaptable by an operator. Domain modeling and meta-modeling methods could help to make inner processes of the agent more accessible. In addition, the knowledge gained should be made reusable for future agents in similar environments. To begin with, the existing methods for modeling agent systems and the underlying concepts will be evaluated, based on the requirements for different industrial scenarios. The methods are then compiled into a framework that allows for the description and modeling of such systems in terms of adaptability to a problem domain. Where necessary, new methods or tools will be introduced to close the gap between inconsistent modeling artifacts. The framework shall then be used to build learning agents for real-life scenarios and observe their application in a case study. The results will be used to assess the quality of the adapted knowledge base and compare it to a manual knowledge modeling process.",https://ieeexplore.ieee.org/document/8904822/,2019 ACM/IEEE 22nd International Conference on Model Driven Engineering Languages and Systems Companion (MODELS-C),15-20 Sept. 2019,ieeexplore
10.1109/ISIE45063.2020.9152407,Modeling and Predicting an Industrial Process Using a Neural Network and Automation Data,IEEE,Conferences,"Production optimization and prevention of faults and unplanned production halts are areas of particular interest in industry. Predictive analysis is commonly implemented with data analytics and machine learning techniques. Usually, the usage of such tools requires knowledge of the machine learning theory and the subject to be studied, e.g. a pumping process. This paper presents a case study on modeling of a pumping process using stored automation data. The model is trained to predict the performance percentage of the process with minimal background knowledge of the process and data analytics. The proposed model is built with IBM SPSS Modeler, a data analysis tool not usually used in real-time industrial predictive analysis as it is not often considered the best tool when working with time series data. The model is deployed in a cloud service to implement a real-time, visualized predictive analysis system. The case study shows that Modeler can be used for data analysis, modeling, and production purposes. Depending on the case, Modeler can provide an alternative tool compared with typical machine learning tools, as models built with Modeler can be deployed into a cloud service for production use. The findings indicate that industrial automation data are a valuable resource, and data analysis can be conducted on various platforms and tools.",https://ieeexplore.ieee.org/document/9152407/,2020 IEEE 29th International Symposium on Industrial Electronics (ISIE),17-19 June 2020,ieeexplore
10.1109/NRSC.2002.1022671,Modeling and automatic control of nuclear reactors,IEEE,Conferences,"This paper presents a developed real time simulator for MPR (multi purpose reactor), on which the Egyptian 2/sup nd/ research reactor is based. A VisSim S/W environment with real-time add-on is employed to achieve this put-pose. VisSim S/W supports a variety of standard and industrial grade I/O cards and uses the block diagram programming technique, which keeps use of specialized code to minimum. All necessary reactivity feedback is taken into consideration and modeled by sufficiently accurate equations. A control rod withdrawal algorithm, which represents the actual one of the MPR, is stated and modeled. Application of traditional control algorithms are implemented and discussed. Advanced control algorithms such as fuzzy, neural network, and genetic are investigated to substitute the moving control rod selection logic and core parameter prediction. The simulator can be distributed with VisSim viewer or through generated C code from VisSim block diagram. The results of the proposed simulator in the power state agree well with the published experimental and analytical results.",https://ieeexplore.ieee.org/document/1022671/,Proceedings of the Nineteenth National Radio Science Conference,21-21 March 2002,ieeexplore
10.1109/IranianCEE.2017.7985123,Modeling and identification of an industrial gas turbine using classical and non-classical approaches,IEEE,Conferences,"In this paper, data-driven modeling and identification of an industrial, simple cycle, heavy duty Gas Turbine is taken into consideration. The GGOV1 model that was introduced by Western Electricity Coordinating Council (WECC), is suggested here for describing dynamics and behaviour of Gas Turbines. It is shown that GGOV1 model that is expressed as the classical approach, can fulfill our needs in academic studies and engineering purposes. Non-classical identification of Gas turbine is also done via Artificial Neural Networks. Comparison between the two methods and real data shows reliability and acceptable precision of both models.",https://ieeexplore.ieee.org/document/7985123/,2017 Iranian Conference on Electrical Engineering (ICEE),2-4 May 2017,ieeexplore
10.1109/NCA.2018.8548330,Monitoring IoT Networks for Botnet Activity,IEEE,Conferences,"The Internet of Things (IoT) has rapidly transitioned from a novelty to a common, and often critical, part of residential, business, and industrial environments. Security vulnerabilities and exploits in the IoT realm have been well documented. In many cases, improving the security of an IoT device by hardening its software is not a realistic option, especially in the cost-sensitive consumer market or in legacy-bound industrial settings. As part of a multifaceted defense against botnet activity on the IoT, this paper explores a method based on monitoring the network activity of IoT devices. A notable benefit of this approach is that it does not require any special access to the devices and adapts well to the addition of new devices. The method is evaluated on a publicly available dataset drawn from a real IoT network.",https://ieeexplore.ieee.org/document/8548330/,2018 IEEE 17th International Symposium on Network Computing and Applications (NCA),1-3 Nov. 2018,ieeexplore
10.1109/IJCNN52387.2021.9534428,Multi-Modal Multi-Instance Multi-Label Learning with Graph Convolutional Network,IEEE,Conferences,"When applying machine learning to tackle realworld problems, it is common to see that objects come with multiple labels rather than a single label. In addition, complex objects can be composed of multiple modalities, e.g. a post on social media may contain both texts and images. Previous approaches typically treat every modality as a whole, while it is not the case in real world, as every post may contain multiple images and texts with quite diverse semantic meanings. Therefore, Multi-modal Multi-instance Multi-label (M3) learning was proposed. Previous attempt at M3 learning argues that exploiting label correlations is crucial. In this paper, we find that we can handle M3 problems using graph convolutional network. Specifically, a graph is built over all labels and each label is initially represented by its word embedding. The main goal for GCN is to map those label embed dings into inter-correlated label classifiers. Moreover, multi-instance aggregation is based on attention mechanism, making it more interpretable because it naturally learns to discover which pattern triggers the labels. Empirical studies are conducted on both benchmark datasets and industrial datasets, validating the effectiveness of our method, and it is demonstrated in ablation studies that the components in our methods are essential.",https://ieeexplore.ieee.org/document/9534428/,2021 International Joint Conference on Neural Networks (IJCNN),18-22 July 2021,ieeexplore
10.1109/FUZZ.2002.1005041,Multi-axis fuzzy control and performance analysis for an industrial robot,IEEE,Conferences,"Robot control systems can be considered as complex systems, the design of the controller involving the determination of the dynamic model for the system. Fuzzy logic provides functional capability without the use of a system model or the characteristics associated with capturing the approximate, varying values found in real world systems. Development of a multi-axis fuzzy logic control system was implemented on an industrial robot, replacing the existing control and hardware systems with a new developmental system. During robot control no adaptation of the rule base or membership functions was carried out online; only system gain was modified in relation to link speed and joint error within predetermined design parameters. The fuzzy control system had to manage the effects of frictional and gravitational forces whilst compensating for the varying inertia components when each linkage is moving. Testing based on ISO 9283 for path accuracy and repeatability verified that real time control of three axes was achievable with values of 938 /spl mu/m and 864 /spl mu/m recorded for accuracy and repeatability, respectively.",https://ieeexplore.ieee.org/document/1005041/,2002 IEEE World Congress on Computational Intelligence. 2002 IEEE International Conference on Fuzzy Systems. FUZZ-IEEE'02. Proceedings (Cat. No.02CH37291),12-17 May 2002,ieeexplore
10.1109/DSN.2017.34,Multi-level Anomaly Detection in Industrial Control Systems via Package Signatures and LSTM Networks,IEEE,Conferences,"We outline an anomaly detection method for industrial control systems (ICS) that combines the analysis of network package contents that are transacted between ICS nodes and their time-series structure. Specifically, we take advantage of the predictable and regular nature of communication patterns that exist between so-called field devices in ICS networks. By observing a system for a period of time without the presence of anomalies we develop a base-line signature database for general packages. A Bloom filter is used to store the signature database which is then used for package content level anomaly detection. Furthermore, we approach time-series anomaly detection by proposing a stacked Long Short Term Memory (LSTM) network-based softmax classifier which learns to predict the most likely package signatures that are likely to occur given previously seen package traffic. Finally, by the inspection of a real dataset created from a gas pipeline SCADA system, we show that an anomaly detection scheme combining both approaches can achieve higher performance compared to various current state-of-the-art techniques.",https://ieeexplore.ieee.org/document/8023128/,2017 47th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN),26-29 June 2017,ieeexplore
10.1109/ICDMW.2019.00065,Mímir: Building and Deploying an ML Framework for Industrial IoT,IEEE,Conferences,"In this paper we describe Mímir, a production grade cloud and edge spanning ML framework for Industrial IoT applications. We first describe our infrastructure for optimized capture, streaming and multi-resolution storage of manufacturing data and its context. We then describe our workflow for scalable ML model training, validation, and deployment that leverages a manufacturing taxonomy and parameterized ML pipelines to determine the best metrics, hyper-parameters and models to use for a given task. We also discuss our design decisions on model deployment for real-time and batch data in the cloud and at the edge. Finally, we describe the use of the framework in building and deploying an application for Predictive Quality monitoring during a Plastics Extrusion manufacturing process.",https://ieeexplore.ieee.org/document/8955638/,2019 International Conference on Data Mining Workshops (ICDMW),8-11 Nov. 2019,ieeexplore
10.1109/INDIN.2015.7281723,NN-ANARX model based control of liquid level using visual feedback,IEEE,Conferences,"In this paper, the problem of liquid level control based on visual feedback is investigated in application to a real life model of an industrial plant. Visual detection of liquid level is implemented on the Raspberry Pi computer. Computational intelligence based controller uses input-output feedback linearization. Parameters of the controller are provided by the neural network ANARX structure. As communication between Raspberry Pi and control system is provided over WLAN, additional prediction module is used to overcome networking problems. Control of the SISO and MIMO systems is provided.",https://ieeexplore.ieee.org/document/7281723/,2015 IEEE 13th International Conference on Industrial Informatics (INDIN),22-24 July 2015,ieeexplore
10.1109/ICDE48307.2020.00039,Neighbor Profile: Bagging Nearest Neighbors for Unsupervised Time Series Mining,IEEE,Conferences,"Unsupervised time series mining has been attracting great interest from both academic and industrial communities. As the two most basic data mining tasks, the discoveries of frequent/rare subsequences have been extensively studied in the literature. Specifically, frequent/rare subsequences are defined as the ones with the smallest/largest 1-nearest neighbor distance, which are also known as motif/discord. However, discord fails to identify rare subsequences when it occurs more than once in the time series, which is widely known as the twin freak problem. This problem is just the ""tip of the iceberg"" due to the 1-nearest neighbor distance based definitions. In this work, we for the first time provide a clear theoretical analysis of motif/discord as the 1-nearest neighbor based nonparametric density estimation of subsequence. Particularly, we focus on matrix profile, a recently proposed mining framework, which unifies the discovery of motif and discord under the same computing model. Thereafter, we point out the inherent three issues: low-quality density estimation, gravity defiant behavior, and lack of reusable model, which deteriorate the performance of matrix profile in both efficiency and subsequence quality.To overcome these issues, we propose Neighbor Profile to robustly model the subsequence density by bagging nearest neighbors for the discovery of frequent/rare subsequences. Specifically, we leverage multiple subsamples and average the density estimations from subsamples using adjusted nearest neighbor distances, which not only enhances the estimation robustness but also realizes a reusable model for efficient learning. We check the sanity of neighbor profile on synthetic data and further evaluate it on real-world datasets. The experimental results demonstrate that neighbor profile can correctly model the subsequences of different densities and shows superior performance significantly over matrix profile on the real-world arrhythmia dataset. Also, it is shown that neighbor profile is efficient for massive datasets.",https://ieeexplore.ieee.org/document/9101736/,2020 IEEE 36th International Conference on Data Engineering (ICDE),20-24 April 2020,ieeexplore
10.1109/PEDES.2006.344292,Neural Approach for Automatic Identification of Induction Motor Load Torque in Real-Time Industrial Applications,IEEE,Conferences,"Induction motors are widely used in several industrial sectors. However, the dimensioning of induction motors is often inaccurate because, in most cases, the load behavior in the shaft is completely unknown. The proposal of this paper is to use artificial neural networks as a tool for dimensioning induction motors rather than conventional methods, which use classical identification techniques and mechanical load modeling. Since the proposed approach uses current, voltage and speed values as the only input parameters, one of its potentialities is related to the facility of hardware implementation for industrial environments and field applications. Simulation results are also presented to validate the proposed approach.",https://ieeexplore.ieee.org/document/4147999/,"2006 International Conference on Power Electronic, Drives and Energy Systems",12-15 Dec. 2006,ieeexplore
10.1109/UralCon52005.2021.9559619,Neural Network for Real-Time Signal Processing: the Nonlinear Distortions Filtering,IEEE,Conferences,"Artificial neural networks, after their training and testing, allow, in the ""if then"" mode, to process signals in real time. This is relevant for signal processing tasks in electrical engineering and the electric power industry, especially for the analysis of nonlinear signal distortions, transients in electrical networks, during switching, emergency modes, and so on. The paper shows the neuro algorithm possibilities based on an elementary perceptron for filtering nonlinear distortions of industrial frequency signals of 50 Hz over a time interval of units of milliseconds. At the same time, the accuracy for the signals’ amplitude, phase, and frequency determining is units of percent. Examples of the fundamental frequency signal selection in the presence of harmonics, the determination of the signal parameters during transients, and the correction of the transformer saturation current are given. It is shown that real-time neural network processing can be carried out in a ""sliding window"" with the duration of units of milliseconds. The estimates made during the implementation of the neuro algorithm in a microprocessor device showed its application possibility in the electric power industry secondary equipment.",https://ieeexplore.ieee.org/document/9559619/,2021 International Ural Conference on Electrical Power Engineering (UralCon),24-26 Sept. 2021,ieeexplore
10.1109/ICSMC.2010.5641990,Neural network workflow scheduling for large scale Utility Management Systems,IEEE,Conferences,"Grid computing is the future computing paradigm for enterprise applications. It can be used for executing large scale workflow applications. This paper focuses on the workflow scheduling mechanism. Although there is much work on static scheduling approaches for workflow applications in parallel environments, little work has been done on a Grid environment for industrial systems. Utility Management Systems (UMS) are executing very large numbers of workflows with very high resource requirements. Unlike the grid approach for standard scientific workflows, UMS workflows have a different set of requirements and thereby optimization of resource usage has to be made in a different way. This paper proposes a novel scheduling architecture which dynamically executes a scheduling algorithm using near real-time feedback from the execution monitor. An Artificial Neural Network was used for workflow scheduling and performance tests show that significant improvement of overall execution time can be achieved by this soft-computing method.",https://ieeexplore.ieee.org/document/5641990/,"2010 IEEE International Conference on Systems, Man and Cybernetics",10-13 Oct. 2010,ieeexplore
10.1145/2830772.2830789,Neuromorphic accelerators: A comparison between neuroscience and machine-learning approaches,IEEE,Conferences,"A vast array of devices, ranging from industrial robots to self-driven cars or smartphones, require increasingly sophisticated processing of real-world input data (image, voice, radio, ...). Interestingly, hardware neural network accelerators are emerging again as attractive candidate architectures for such tasks. The neural network algorithms considered come from two, largely separate, domains: machine-learning and neuroscience. These neural networks have very different characteristics, so it is unclear which approach should be favored for hardware implementation. Yet, few studies compare them from a hardware perspective. We implement both types of networks down to the layout, and we compare the relative merit of each approach in terms of energy, speed, area cost, accuracy and functionality. Within the limit of our study (current SNN and machine learning NN algorithms, current best effort at hardware implementation efforts, and workloads used in this study), our analysis helps dispel the notion that hardware neural network accelerators inspired from neuroscience, such as SNN+STDP, are currently a competitive alternative to hardware neural networks accelerators inspired from machine-learning, such as MLP+BP: not only in terms of accuracy, but also in terms of hardware cost for realistic implementations, which is less expected. However, we also outline that SNN+STDP carry potential for reduced hardware cost compared to machine-learning networks at very large scales, if accuracy issues can be controlled (or for applications where they are less important). We also identify the key sources of inaccuracy of SNN+STDP which are less related to the loss of information due to spike coding than to the nature of the STDP learning algorithm. Finally, we outline that for the category of applications which require permanent online learning and moderate accuracy, SNN+STDP hardware accelerators could be a very cost-efficient solution.",https://ieeexplore.ieee.org/document/7856622/,2015 48th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO),5-9 Dec. 2015,ieeexplore
10.1109/ISIE45063.2020.9152527,Next generation control units simplifying industrial machine learning,IEEE,Conferences,"The increasing amount of sensor data creates new possibilities through data-driven projects in industry. The demands on the final solution architecture are different and typically include at least one controller (e.g. PLC). In this paper, we focus on these industrial controllers and identify their possible roles in smart factories: Collection, distribution and preprocessing of field data, execution of intelligence and functionalities to ease rapid prototyping approaches. Furthermore, we specify the capabilities leading to the fulfillment of these roles and present an application example of drive anomaly detection to demonstrate how to setup an industrial controller for machine learning projects.",https://ieeexplore.ieee.org/document/9152527/,2020 IEEE 29th International Symposium on Industrial Electronics (ISIE),17-19 June 2020,ieeexplore
10.1109/ICMCS.2011.5945672,Node disjoint multi-path routing for ZigBee cluster-tree wireless sensor networks,IEEE,Conferences,"ZigBee is the emerging industrial standard for Ad hoc networks based on the IEEE 802.15.4 physical and MAC layers standard. It was originally specified for low data rate, low power consumption and low cost wireless personal area networks (WPANs). Due to these characteristics, ZigBee is expected to be used in wireless sensor networks (WSNs) for industrial sensing and control applications. Handling high data rate applications, such video surveillance in WPANs is a real challenge. To increase available bandwidth in a ZigBee network, we explore in this paper, multipath routing where multiple paths are used simultaneously to transfer data between a source and the sink. We propose Z-MHTR, a node disjoint multipath routing extension of the ZigBee hierarchical tree routing protocol in cluster-tree WSN. The aim of this work is to study and evaluate the implemented multipath routing using NS2 simulations. We mainly faced some problems that influence the efficiency of the multipath routing and the overall network performances in terms of throughput, end to end delay data transmission and network lifetime under heavy and low data rates.",https://ieeexplore.ieee.org/document/5945672/,2011 International Conference on Multimedia Computing and Systems,7-9 April 2011,ieeexplore
10.1109/ComPE49325.2020.9200139,Object Detection and Tracking Turret based on Cascade Classifiers and Single Shot Detectors,IEEE,Conferences,"The involvement of embedded systems and computer vision is increasing day by day in various segments of consumer market like industrial automation, traffic monitoring, medical imaging, modern appliance market, augmented reality systems, etc. These technologies are bound to make new developments in the domain of commercial and home security surveillance. Our project aims to make contributions to the domain of video surveillance by making use of embedded computer vision systems. Our implementation, built around the Raspberry Pi 4 SBC aims to utilize computer vision techniques like motion detection, face recognition, object detection, etc to segment the region of interest from the captured video footage. This technique is superior as compared to traditional surveillance systems as it requires minimum human interaction and intervention at the control room of such security systems. The proposed system is capable of sensing suspicious events like detection of an unknown face in the captured video or motion detection/object detection in a closed section of a building. Moreover, with the help of the turret mechanism built using servo motors, the camera integrated in the system is capable of having 360° rotation and can track a detected face or object of interest within its range. Apart from automated tracking, the system can also be manually controlled by the operator.",https://ieeexplore.ieee.org/document/9200139/,2020 International Conference on Computational Performance Evaluation (ComPE),2-4 July 2020,ieeexplore
10.1109/ROBIO.2009.4913200,Object orientation recognition based on SIFT and SVM by using stereo camera,IEEE,Conferences,"The goal of this research is to recognize an object and its orientation in space by using stereo camera. The principle of object orientation recognition in this paper was based on the scale invariant feature transform (SIFT) and support vector machine (SVM). SIFT has been successfully implemented on object recognition but it had a problem recognizing the object orientation. For many autonomous robotics applications, such as using a vision-guided industrial robot to grab a product, not only correct object recognition will be needed in this process but also object orientation recognition is required. In this paper we used SVM to recognize object orientation. SVM has been known as a promising method for classification accuracy and its generalization ability. The stereo camera system adopted in this research provided more useful information compared to single camera one. The object orientation recognition technique was implemented on an industrial robot in a real application. The proposed camera system and recognition algorithms were used to recognize a specific object and its orientation and then guide the industrial robot to perform some alignment operations on the object.",https://ieeexplore.ieee.org/document/4913200/,2008 IEEE International Conference on Robotics and Biomimetics,22-25 Feb. 2009,ieeexplore
10.1109/IECON.2006.347441,Obstacle avoidance algorithm based on biological patterns for anthropomorphic robot manipulator,IEEE,Conferences,"This study addresses the problem of collision-free controlling of 3-DOF (degree of freedom) anthropomorphic manipulators with given a priori unrestricted trajectory. The robot constraints resulting from the physical robot's actuators are also taken into account during the robot movement. Obstacle avoidance algorithm is based on penalty function, which is minimized when collision is predicted. Mathematical construction of penalty function and minimization process allows modeling of variety behaviors of robot elusion moves. Implementation of artificial neural network (ANN) inside the control process gives the additional flexibility needed to remember most important robot behaviors based on biological pattern of human arm moves. Thanks to the fast collisions' detection, the presented algorithm appears to be applicable to the industrial real-time implementations. Numerical simulations of the anthropomorphic manipulator operating in three dimensional space with obstacles is also presented",https://ieeexplore.ieee.org/document/4152937/,IECON 2006 - 32nd Annual Conference on IEEE Industrial Electronics,6-10 Nov. 2006,ieeexplore
10.1109/BigData.2017.8258105,On event-driven knowledge graph completion in digital factories,IEEE,Conferences,"Smart factories are equipped with machines that can sense their manufacturing environments, interact with each other, and control production processes. Smooth operation of such factories requires that the machines and engineering personnel that conduct their monitoring and diagnostics share a detailed common industrial knowledge about the factory, e.g., in the form of knowledge graphs. Creation and maintenance of such knowledge is expensive and requires automation. In this work we show how machine learning that is specifically tailored towards industrial applications can help in knowledge graph completion. In particular, we show how knowledge completion can benefit from event logs that are common in smart factories. We evaluate this on the knowledge graph from a real world-inspired smart factory with encouraging results.",https://ieeexplore.ieee.org/document/8258105/,2017 IEEE International Conference on Big Data (Big Data),11-14 Dec. 2017,ieeexplore
10.1109/COMPSAC51774.2021.00225,On the Use of Causal Models to Build Better Datasets,IEEE,Conferences,"In recent years, Machine Learning and Deep Learning communities have devoted many efforts to studying ever better models and more efficient training strategies. Nonetheless, the fundamental role played by dataset bias in the final behaviour of the trained models calls for strong and principled methods to collect, structure and curate datasets prior to training. In this paper we provide an overview on the use of causal models to achieve a deeper understanding of the underlying structure beneath datasets and mitigate biases, supported by several real-life use cases from the medical and industrial domains.",https://ieeexplore.ieee.org/document/9529554/,"2021 IEEE 45th Annual Computers, Software, and Applications Conference (COMPSAC)",12-16 July 2021,ieeexplore
10.1109/ISIC.1995.525123,On the design and implementation of a mobile robotic system,IEEE,Conferences,"Describes the effort being carried out in the analysis, design and implementation of the control architecture for a mobile platform for autonomous transportation, surveillance and inspection in structured and semi-structured industrial environments. The control architecture is based in a hierarchical structure organized linguistically permitting the real-time parallel execution of tasks. This architecture is composed of three levels, organization, coordination and functional, structured according to the increasing precision with decreasing intelligence principle.",https://ieeexplore.ieee.org/document/525123/,Proceedings of Tenth International Symposium on Intelligent Control,27-29 Aug. 1995,ieeexplore
10.1109/ICMA.2019.8816298,Online Learning of the Inverse Dynamics with Parallel Drifting Gaussian Processes: Implementation of an Approach for Feedforward Control of a Parallel Kinematic Industrial Robot,IEEE,Conferences,"The present paper deals with an online approach to learn the inverse dynamics of any robot. This is realized by the use of Gaussian Processes drifting parallel along the system data. An extension by a database enables the efficient use of data points from the past. The central component of this work is the implementation of such a method in a controller in order to achieve the actual goal: the feedforward control of an industrial robot by means of machine learning. This is done by splitting the procedure into two threads running parallel so that the prediction is decoupled from the computing-intensive training of the models. Experiments show that the method reduces the tracking errors more clearly than an elaborately identified rigid body model including friction. For a defined trajectory, the squared areas of the tracking errors of all axes are reduced by more than 54% compared to motion without pre-control. In addition, a highly dynamic pick-and-place experiment is used to investigate the possible changes in system dynamics. Compared to an offline trained model, the approximation error of the proposed online approach is smaller for the remaining time of the experiment after an initial phase. Furthermore, this error is smaller throughout the experiment for online learning with parallel drifting Gaussian Processes than when using a single one.",https://ieeexplore.ieee.org/document/8816298/,2019 IEEE International Conference on Mechatronics and Automation (ICMA),4-7 Aug. 2019,ieeexplore
10.1109/ICRA40945.2020.9196769,Online LiDAR-SLAM for Legged Robots with Robust Registration and Deep-Learned Loop Closure,IEEE,Conferences,"In this paper, we present a 3D factor-graph LiDAR-SLAM system which incorporates a state-of-the-art deeply learned feature-based loop closure detector to enable a legged robot to localize and map in industrial environments. Point clouds are accumulated using an inertial-kinematic state estimator before being aligned using ICP registration. To close loops we use a loop proposal mechanism which matches individual segments between clouds. We trained a descriptor offline to match these segments. The efficiency of our method comes from carefully designing the network architecture to minimize the number of parameters such that this deep learning method can be deployed in real-time using only the CPU of a legged robot, a major contribution of this work. The set of odometry and loop closure factors are updated using pose graph optimization. Finally we present an efficient risk alignment prediction method which verifies the reliability of the registrations. Experimental results at an industrial facility demonstrated the robustness and flexibility of our system, including autonomous following paths derived from the SLAM map.",https://ieeexplore.ieee.org/document/9196769/,2020 IEEE International Conference on Robotics and Automation (ICRA),31 May-31 Aug. 2020,ieeexplore
10.1109/ROMA.2017.8231735,Online system modeling of chemical process plant using U-model,IEEE,Conferences,"One of the major challenges in industrial process control is to deal with nonlinearities in the plants. There have been a significant amount of research efforts towards design and development of appropriate, reliable and promising control techniques to deployed on real-time industrial applications. Some of the widely used and acknowledged control methods lack in terms of tuning unknown system parameters. The reason is their unadaptive or fixed nature. This paves the field well for adaptive controllers. Their biggest advantage is their automatic updation of unknown system paramters, that saves quite some resources and manpower and ensures an overall stable control strategy. In this regard, system modeling happens to be the prime and pertinent task so that it can set the basis for a stable control law synthesis. This reserach work proposes a polynomial adaptive model recently introduced called U-Model to be used for online system identification of Chemical Process Plant. U-Model is a simple, stable and reliable which has previously yielded encouraging results when applied to various application in different scenario. The aforementioned plant shall be used for investigation on its Flow process. The modeling results shall be compared and validated by other commonly known and utilized modeling structures.",https://ieeexplore.ieee.org/document/8231735/,2017 IEEE 3rd International Symposium in Robotics and Manufacturing Automation (ROMA),19-21 Sept. 2017,ieeexplore
10.1109/INDIN.2004.1417325,OntoSmartResource: an industrial resource generation in semantic Web,IEEE,Conferences,"Semantic Web is a logical evolution of the existing Web. It was meant to serve for machines as today's Web does for humans. The term ""machines"" according to the existing semantic Web's vocabulary mostly means ""computers"". However industry needs such applications, which consider machines also as embedded computational entities within field devices, personal devices, microwave ovens, etc. In other words, now we should involve the real (industrial) world objects as resources into semantic Web. Still the main object of such a world will be a human, which becoming a resource (not just a user of resources) in the distributed environment. In this paper we introduce an extension of the semantic Web resources to a new generation of the enhanced smart semantic Web resources (OntoSmartResources). We consider following aspects as: agent-driven resource behavior mechanism, resource semantic description and maintenance, and some aspects of the human resources related to representation and adaptation",https://ieeexplore.ieee.org/document/1417325/,"2nd IEEE International Conference on Industrial Informatics, 2004. INDIN '04. 2004",24-26 June 2004,ieeexplore
10.1109/ICCI-CC.2018.8482025,Ontology Faults Diagnosis Model for the Hazardous Chemical Storage Device,IEEE,Conferences,"Due to high temperature, high pressure, high corrosion and many other factors, the hazardous chemical device is facing more severe security challenges than other industries. Now, the monitoring methods have been very mature, which play a basic monitoring role, not a predictive fault diagnosis. In this paper, the hazardous chemical device's status data will been collected from the existing industrial monitoring network, the real-time data will be preprocessed and then stored in a database, and the data will be imported to the real-time data into the ontology model, the data will be performed by big data processing and automatic reasoning. So that real-time status of hazardous chemical device and the warning of security risks predict are easily got at any time. The model is proposed to solving the problem of knowledge representation and reasoning of the hazardous chemical device based on ontology. The model is analyzed and implemented in Protégé software.",https://ieeexplore.ieee.org/document/8482025/,2018 IEEE 17th International Conference on Cognitive Informatics & Cognitive Computing (ICCI*CC),16-18 July 2018,ieeexplore
10.1109/INDIN.2006.275808,"Ontology for Cognitics, Closed-Loop Agility Constraint, and Case Study - a Mobile Robot with Industrial-Grade Components",IEEE,Conferences,"The paper refers to intelligent industrial automation. The objective is to present key elements and methods for best practice, as well as some results obtained. The first part presents an ontology for automated cognition (cognitics), where, based on information and time, the main cognitive concepts, including those of complexity, knowledge, expertise, learning, intelligence abstraction, and concretization are rigorously defined, along with corresponding metrics and specific units. Among important conclusions at this point are the fact that reality is much too complex to be approached better than through much simplified models, in very restricted contexts. Another conclusion is the necessity to be focused on goal. Extensions are made here for group behavior. The second part briefly presents a basic law governing the choice of overall control architecture: achievable performance level of control system in terms of agility, relative to process dynamics, dictates the type of approaches which is suitable, in a spectrum which ranges from simple threshold-based switching, to classical closed-loop calculus (PID, state space multivariable systems, etc.), up to ""impossible"" cases where additional controllers must be considered, leading to cascaded, hierarchical control structures. For complex cases such as latter ones, new tools and methodologies must be designed, as is typical in O<sup>3</sup>NEIDA initiative, at least for software components. Finally, a large part of the paper presents a case study, a mobile robot, i.e. an embedded autonomous system with distributed, networked control, featuring industry-grade components, designed with the main goal of robust functionality. The case illustrates several of the concepts introduced earlier in the paper.",https://ieeexplore.ieee.org/document/4053562/,2006 4th IEEE International Conference on Industrial Informatics,16-18 Aug. 2006,ieeexplore
10.1109/IHMSC.2009.38,Ontology-Based Fast Devices Configuration in 3D Configuration Software,IEEE,Conferences,"Using 3D user interface in the configuration software, the user interface can become realistic and friendly. And it can reflect the devices running status in the industrial field so realistically. Moreover, it can avoid the window effect effectively due to the screen size influence. The window information content has the enormous enhancement. But it brings some other problems such as configuration difficultly. This paper presents a ontology-based fast configuration method by attaching semantic information to the device model. The computer can understand the user intention and assist him do the configuration work. Using this method, the devices configuration becomes so simple and convenient. The users that are not 3D professionals can also carry on the devices configuration relaxed.",https://ieeexplore.ieee.org/document/5335932/,2009 International Conference on Intelligent Human-Machine Systems and Cybernetics,26-27 Aug. 2009,ieeexplore
10.1109/CAMAD.2019.8858503,Operational Data Based Intrusion Detection System for Smart Grid,IEEE,Conferences,"With the rapid progression of Information and Communication Technology (ICT) and especially of Internet of Things (IoT), the conventional electrical grid is transformed into a new intelligent paradigm, known as Smart Grid (SG). SG provides significant benefits both for utility companies and energy consumers such as the two-way communication (both electricity and information), distributed generation, remote monitoring, self-healing and pervasive control. However, at the same time, this dependence introduces new security challenges, since SG inherits the vulnerabilities of multiple heterogeneous, co-existing legacy and smart technologies, such as IoT and Industrial Control Systems (ICS). An effective countermeasure against the various cyberthreats in SG is the Intrusion Detection System (IDS), informing the operator timely about the possible cyberattacks and anomalies. In this paper, we provide an anomaly-based IDS especially designed for SG utilising operational data from a real power plant. In particular, many machine learning and deep learning models were deployed, introducing novel parameters and feature representations in a comparative study. The evaluation analysis demonstrated the efficacy of the proposed IDS and the improvement due to the suggested complex data representation.",https://ieeexplore.ieee.org/document/8858503/,2019 IEEE 24th International Workshop on Computer Aided Modeling and Design of Communication Links and Networks (CAMAD),11-13 Sept. 2019,ieeexplore
10.1109/ICECCE49384.2020.9179427,Opportunistic Resource Allocation for Narrowband Internet of Things: A Literature Review,IEEE,Conferences,"Nowadays, the growing adoption of the Internet of Things (IoT) is reshaping the telecommunication landscape and has penetrated every aspect of our lives with influential applications on smart health, home automation, smart logistics, smart industries and smart cities. These advanced technologies bring about numerous benefits and has begun to play a major role in daily lives, particularly data mining applied in precision agriculture to discover knowledge. Also in agro-industrial production chain, the combination of wireless and distributed specific sensor devices with the simulation of climatic conditions in order to track the evolution of grapes for wineries is outstanding. Mobile IoT such as narrow-band-IoT (NB-IoT) and the long-term evolution (LTE) for machines (LTE-M) are significant innovations of this accelerating development of IoT technologies. In the era of ubiquitous communication where everything is connected to the internet, NB-IoT systems are expected to offer better quality-of-services (QoS) to end users than the traditional IoT paradigm. However, offering better QoS satisfaction to end users will become a great challenge due to the bottleneck caused by the dual problem of increasing IoT use cases and the shortage of wireless spectrum resources. Whilst discussing the recent innovative solutions of NB-IoT resource allocation, significant challenges and open issues related to the real-time implementation of NBIoT are identified and discussed. Therefore, this paper gives a general overview of the resource allocation solutions in this NB-IoT innovation and further suggests and motivates for the intelligentization of future resource allocation solutions (i.e., the use of artificial intelligence (AI) strategies).",https://ieeexplore.ieee.org/document/9179427/,"2020 International Conference on Electrical, Communication, and Computer Engineering (ICECCE)",12-13 June 2020,ieeexplore
10.23919/ChiCC.2017.8027747,Optimal operational control for industrial processes based on Q-learning method,IEEE,Conferences,"It is difficult to accurately model productive processes and describe relationship between operational indices and controlled variables for complex modem industrial processes. How to design the optimal setpoints by using only data generated by operational processes, without requiring the knowledge of model parameters of operational processes, poses a challenge on designing optimal setpoints. This paper presents a state-observer based Q-learning algorithm to learn the optimal setpoints by utilizing only data, such that the real operational indices can track the desired values in an approximately optimal manner. A simulation experiment in flotation process is implemented to show the effectiveness of the proposed method.",https://ieeexplore.ieee.org/document/8027747/,2017 36th Chinese Control Conference (CCC),26-28 July 2017,ieeexplore
10.1109/ICOIN50884.2021.9334026,Optimization of RSSI based indoor localization and tracking to monitor workers in a hazardous working zone using Machine Learning techniques,IEEE,Conferences,"This paper proposes a method for RSSI based indoor localization and tracking in cluttered environments using Deep Neural Networks. We implemented a real-time system to localize people using wearable active RF tags and RF receivers fixed in an industrial environment with high RF noise. The proposed solution is advantageous in analysing RSSI data in cluttered-indoor environments with the presence of human body attenuation, signal distortion, and environmental noise. Simulations and experiments on a hardware testbed demonstrated that receiver arrangement, number of receivers and amount of line of sight signals captured by receivers are important parameters for improving localization and tracking accuracy. The effect of RF signal attenuation through the person who carries the tag was combined with two neural network models trained with RSSI data pertaining to two walking directions. This method was successful in predicting the walking direction of the person.",https://ieeexplore.ieee.org/document/9334026/,2021 International Conference on Information Networking (ICOIN),13-16 Jan. 2021,ieeexplore
10.1109/CIIMA50553.2020.9290289,PI tuning based on Bacterial Foraging Algorithm for flow control,IEEE,Conferences,"In the industrial field, the need has arisen to use more efficient and robust controllers using artificial intelligence techniques that optimize the operation of processes within the industry. In this way, the need arises to employ adaptive controllers such as the BFOA and implement it in real systems in which its functionality can be analyzed. This article presents the implementation and analysis in a fully instrumented functional prototype with industrial sensors. The work methodology is documented from the acquisition of the physical variables through the OPC client-server communication; the synchronization of the excitation of the input variable (variable speed drive) and obtaining the evolution of the flow in time; with the experimental data, the identification methodology by relative least squares was used to obtain the transfer function. Later, the BFOA algorithm was implemented to adjust the constants of a PI controller (Kp and Ki) and analyze the response through simulation using Matlab software, in which satisfactory results were observed based on the analysis of response to disturbances and as an end final part, the controller and the BFOA algorithm were implemented in a PLC-S7-1500 controller in SCL language, and the functionality was validated with the functional prototype, changing the flow setpoints at certain times, observing a behavior according to the simulations carried out. with a minimum overshoot of approximately 5 % and an establishment time of 20s.",https://ieeexplore.ieee.org/document/9290289/,2020 IX International Congress of Mechatronics Engineering and Automation (CIIMA),4-6 Nov. 2020,ieeexplore
10.1109/CDC.2009.5400848,PLS-based FDI of a Three-Tank laboratory system,IEEE,Conferences,"The problems of fault detection and isolation of dynamic systems has been studied intensively in the recent years and many successful industrial applications have been reported. In the main these studies have been restricted to model based techniques, with few reports of successful implementation of data driven approaches. These data driven approaches have been range from the application of linear regression techniques, to neuro-fuzzy systems. This paper reports on application of, Multivariate Statistical Process Control (MSPC) methodologies, which can provide a diagnostic tool for the on-line or real time monitoring and detection of the process malfunction is proposed. Finally the effectiveness of Partial Least Squares (PLS) in FDI of the three-tank system are represented and discussed through simulation results.",https://ieeexplore.ieee.org/document/5400848/,Proceedings of the 48h IEEE Conference on Decision and Control (CDC) held jointly with 2009 28th Chinese Control Conference,15-18 Dec. 2009,ieeexplore
10.1109/BigDataCongress.2019.00032,"PREMISES, a Scalable Data-Driven Service to Predict Alarms in Slowly-Degrading Multi-Cycle Industrial Processes",IEEE,Conferences,"In recent years, the number of industry-4.0-enabled manufacturing sites has been continuously growing, and both the quantity and variety of signals and data collected in plants are increasing at an unprecedented rate. At the same time, the demand of Big Data processing platforms and analytical tools tailored to manufacturing environments has become more and more prominent. Manufacturing companies are collecting huge amounts of information during the production process through a plethora of sensors and networks. To extract value and actionable knowledge from such precious repositories, suitable data-driven approaches are required. They are expected to improve the production processes by reducing maintenance costs, reliably predicting equipment failures, and avoiding quality degradation. To this aim, Machine Learning techniques tailored for predictive maintenance analysis have been adopted in PREMISES (PREdictive Maintenance service for Industrial procesSES), an innovative framework providing a scalable Big Data service able to predict alarming conditions in slowly-degrading processes characterized by cyclic procedures. PREMISES has been experimentally tested and validated on a real industrial use case, resulting efficient and effective in predicting alarms. The framework has been designed to address the main Big Data and industrial requirements, by being developed on a solid and scalable processing framework, Apache Spark, and supporting the deployment on modularized containers, specifically upon the Docker technology stack.",https://ieeexplore.ieee.org/document/8818217/,2019 IEEE International Congress on Big Data (BigDataCongress),8-13 July 2019,ieeexplore
10.1109/BigData.2018.8622065,Parallel Large-Scale Neural Network Training For Online Advertising,IEEE,Conferences,"Neural networks have shown great successes in many fields. Due to the complexity of the training pipeline, however, using them in an industrial setting is challenging. In online advertising, the complexity arises from the immense size of the training data, and the dimensionality of the sparse feature space (both can be hundreds of billions). To tackle these challenges, we built TrainSparse (TS), a system that parallelizes the training of neural networks with a focus on efficiently handling large-scale sparse features. In this paper, we present the design and implementation of TS, and show the effectiveness of the system by applying it to predict the ad conversion rate (pCVR), one of the key problems in online advertising. We also compare several methods for dimensionality reduction on sparse features in the pCVR task. Experiments on real-world industry data show that TS achieves outstanding performance and scalability.",https://ieeexplore.ieee.org/document/8622065/,2018 IEEE International Conference on Big Data (Big Data),10-13 Dec. 2018,ieeexplore
10.1109/MEMCOD.2015.7340480,Passive testing of production systems based on model inference,IEEE,Conferences,"This paper tackles the problem of testing production systems, i.e. systems that run in industrial environments, and that are distributed over several devices and sensors. Usually, such systems lack of models, or are expressed with models that are not up to date. Without any model, the testing process is often done by hand, and tends to be an heavy and tedious task. This paper contributes to this issue by proposing a framework called Autofunk, which combines different fields such as model inference, expert systems, and machine learning. This framework, designed with the collaboration of our industrial partner Michelin, infers formal models that can be used as specifications to perform offline passive testing. Given a large set of production messages, it infers exact models that only capture the functional behaviours of a system under analysis. Thereafter, inferred models are used as input by a passive tester, which checks whether a system under test conforms to these models. Since inferred models do not express all the possible behaviours that should happen, we define conformance with two implementation relations. We evaluate our framework on real production systems and show that it can be used in practice.",https://ieeexplore.ieee.org/document/7340480/,2015 ACM/IEEE International Conference on Formal Methods and Models for Codesign (MEMOCODE),21-23 Sept. 2015,ieeexplore
10.1109/ICCC51575.2020.9345104,Payload-based Anomaly Detection for Industrial Internet Using Encoder Assisted GAN,IEEE,Conferences,"Payload-based anomaly detection has been proved effective in discovering Internet misbehavior and potential intrusions, but highly relies on the unstructured feature engineering to generalize the distribution of normal payloads. This kind of generalization may not adapt well to the emerging industrial Internet, where the normal behaviors are more diverse and usually embedded in the raw payloads' local structures. In this paper, we tackle this generalization problem and propose a very different solution to payload-based anomaly detection without the need of feature engineering. Our basic idea is to learn the raw structures of normal payloads directly by a generative adversarial network (GAN), in which we have a generator (i.e., a reversed convolutional decoder) to sample raw payloads from a latent space as well as a discriminator (i.e., a convolutional classifier) to guide the generator produce raw payloads approximating the normal structures. We also deploy an assisted convolutional encoder to map the true payloads back to the latent space and combine with the GAN's decoder (i.e., generator) to reconstruct the payload structures. We consider anomalies appear in condition the re-constructed payloads are largely deviated from the true ones, since our encoder-decoder architecture is trained able to rebuild only the normal payload structures. We have evaluated our solution using extensive experiments on real-world industrial Internet datasets, and confirmed its effectiveness in detecting industrial Internet anomalies in the raw payloads.",https://ieeexplore.ieee.org/document/9345104/,2020 IEEE 6th International Conference on Computer and Communications (ICCC),11-14 Dec. 2020,ieeexplore
10.1109/ICCCI49374.2020.9145976,PeTIT: Perceiving the Technological Innovation Trends via the Heuristic Model of Community Detection,IEEE,Conferences,"Patent analysis is widely used in many kinds of research, such as competitive intelligence analysis, technology trends perceiving, industrial distribution planning, macro-economy regulations, and so on. Based on the patent data, this paper proposed a novel algorithm, which named PeTIT, to perceive the technological innovation trends by applying the heuristic community detection model. The PeTIT algorithm included four steps: patent ontology extraction, technological innovation tree construction, technological innovation community detection, and technological innovation trends perceiving. We implemented the PeTIT algorithm on the real dataset of invention patents in the field of artificial intelligence in China, which ranged from Jun 1st, 2000 to May 1st, 2019. The results showed that the innovation situation of artificial intelligent was mainly concentrated in 10 fields. Moreover, the application field of intelligent driving has become one of the fastest-growing industries. Finally, the experimental results demonstrated that the cubic exponential smoothing model had a higher performance by perceiving the technological innovation trends.",https://ieeexplore.ieee.org/document/9145976/,2020 2nd International Conference on Computer Communication and the Internet (ICCCI),26-29 June 2020,ieeexplore
10.1109/FPA.1994.636094,Perception systems implemented in analog VLSI for real-time applications,IEEE,Conferences,"We point out that analog VLSI can now be considered as the ideal medium to implement computational systems intended to carry out real time perceptive or even cognitive tasks that are not well handled by traditional computers. By exploiting the analog features of the transistors, only a few devices are needed to realise most of the elementary functions required to implement perceptive systems, resulting in very dense, sophisticated circuits and low power consumption. Elementary artificial retinas in silicon based on their biological counterparts have already been successfully used in industrial applications. Artificial cochleas and noses are also under development. This new enabling technology is of great interest over a wide range of industrial sectors, including robotics, automotive, surveillance and food industry.",https://ieeexplore.ieee.org/document/636094/,Proceedings of PerAc '94. From Perception to Action,7-9 Sept. 1994,ieeexplore
10.1109/ICICT50816.2021.9358747,Performance Analysis of Frequency Variation System using Drives (VT240s and Axpert Eazy) for Industrial Application,IEEE,Conferences,"The proposed research work describes the general system management and real-time applicability of VFD i.e. Variable frequency drive. The project embodies the automation of 2 styles of VFD series namely VT240S and Axpert eazy. These VFD are going to be managed through PLC. Further, VFD is employed for control. The speed of the motor is often serially and domestically managed by VFD. In domestic management, there will be a switch affiliation within the drive and in serial management, the motor is managed through PLC. The motor will stop by each ways. For serial management, the programs should style in software package and it will be downloaded in PLC and by giving external wire affiliation like Modbus 485 from PLC to VFD, the operation is completed. Recently, the speed of the motor control is emerging as a big issue in industrial automation. Machine control with accurate result is required in industry applications for the design and development of the tools in various domains. Machine speed is used to control and vary the frequency parameter. So, the Variable Frequency Drive [VFD] that has been used to control the speed of the motor by varying the range of frequency is proposed to design a VFD machine system with number of tools and machine learning techniques in the paper.",https://ieeexplore.ieee.org/document/9358747/,2021 6th International Conference on Inventive Computation Technologies (ICICT),20-22 Jan. 2021,ieeexplore
10.1109/ICIEA.2006.257304,Performance Studies of Fuzzy Logic Based PI-like Controller Designed for Speed Control of Switched Reluctance Motor,IEEE,Conferences,"Switched reluctance motor (SRM) has gained significant interest in the field of industrial drive. The controller used to drive the machine is conventional PI controller. But the machine characteristics are very much nonlinear. This poses a problem for conventional controller design as regards to maintaining steady performance. There is also a need to adapt to the variable operating conditions. Fuzzy logic based heuristics is prospective since the exact analytical modelling of the system is difficult. PC implementation of the controller offers great flexibility in both design and maintenance phase. This work implements a PI like fuzzy logic controller (FLC) for SRM, which is found to work successfully in real time conditions. The work compares the performance of the FLC with respect to the conventional PI controller",https://ieeexplore.ieee.org/document/4025905/,2006 1ST IEEE Conference on Industrial Electronics and Applications,24-26 May 2006,ieeexplore
10.1109/METROI4.2019.8792860,Performance evaluation of full-cloud and edge-cloud architectures for Industrial IoT anomaly detection based on deep learning,IEEE,Conferences,"One of the most interesting application of data analysis to industry is the real-time detection of anomalies during production. Industrial IoT paradigm includes all the components to realize predictive systems, like the anomaly detection ones. In this case, the goal is to discover patterns, in a given dataset, that do not resemble the “normal” behavior, to identify faults, malfunctions or the effects of bad maintenance. The use of complex neural networks to implement deep learning algorithm for anomaly detection is very common. The position of the deep learning algorithm is one of the main problem: this kind of algorithm requires both high computational power and data transfer bandwidth, rising serious questions on the system scalability. Data elaboration in the edge domain (i.e. close to the machine) usually reduce data transfer but requires to instantiate expensive physical assets. Cloud computing is usually cheaper but Cloud data transfer is expensive. In this paper a test methodology for the comparison of the two architectures for anomaly detection system is proposed. A real use case is described in order to demonstrate the feasibility. The experimental results show that, by means of the proposed methodology, edge and Cloud solutions implementing deep learning algorithms for industrial applications can be easily evaluated. In details, for the considered use case (with Siemens controller and Microsoft Azure platform) the tradeoff between scalability, communication delay, and bandwidth usage, has been studied. The results show that the full-cloud architecture can outperform the edge-cloud architecture when Cloud computation power is scaled.",https://ieeexplore.ieee.org/document/8792860/,2019 II Workshop on Metrology for Industry 4.0 and IoT (MetroInd4.0&IoT),4-6 June 2019,ieeexplore
10.1109/CNSC.2014.6906671,Pixelwise object class segmentation based on synthetic data using an optimized training strategy,IEEE,Conferences,"In this paper we present an approach for low-level body part segmentation based on RGB-D data. The RGB-D sensor is thereby placed at the ceiling and observes a shared workspace for human-robot collaboration in the industrial domain. The pixelwise information about certain body parts of the human worker is used by a cognitive system for the optimization of interaction and collaboration processes. In this context, for rational decision making and planning, the pixelwise predictions must be reliable despite the high variability of the appearance of the human worker. In our approach we treat the problem as a pixelwise classification task, where we train a random decision forest classifier on the information contained in depth frames produced by a synthetic representation of the human body and the ceiling sensor, in a virtual environment. As shown in similar approaches, the samples used for training need to cover a broad spectrum of the geometrical characteristics of the human, and possible transformations of the body in the scene. In order to reduce the number of training samples and the complexity of the classifier training, we therefore apply an elaborated and coupled strategy for randomized training data sampling and feature extraction. This allows us to reduce the training set size and training time, by decreasing the dimensionality of the sampling parameter space. In order to keep the creation of synthetic training samples and real-world ground truth data simple, we use a highly reduced virtual representation of the human body, in combination with KINECT skeleton tracking data from a calibrated multi-sensor setup. The optimized training and simplified sample creation allows us to deploy standard hardware for the realization of the presented approach, while yielding a reliable segmentation in real-time, and high performance scores in the evaluation.",https://ieeexplore.ieee.org/document/6906671/,2014 First International Conference on Networks & Soft Computing (ICNSC2014),19-20 Aug. 2014,ieeexplore
10.1109/IROS.1991.174701,Planning based sensing and task executing in an autonomous machine,IEEE,Conferences,"Implementing a control system for an autonomous machine is a challenging task. Several techniques have to be applied, such as task planning, hierarchical and/or distributed control, and advanced sensing techniques. In addition, to be useful these various techniques have to be integrated into a system that has to operate more or less in real-time. The authors present a control scheme based on hierarchically organized planning-executing-monitoring-cycles which is used to solve some of the problems related to real-time control of an autonomous machine. The implementation is also presented in which the control system is applied in a pilot system based on an industrial robot.&lt;<ETX>&gt;</ETX>",https://ieeexplore.ieee.org/document/174701/,Proceedings IROS '91:IEEE/RSJ International Workshop on Intelligent Robots and Systems '91,3-5 Nov. 1991,ieeexplore
10.1109/INDIN45523.2021.9557519,Platform Generation for Edge AI Devices with Custom Hardware Accelerators,IEEE,Conferences,"In recent years artificial neural networks (NNs) have been at the center of research on data processing. However, their high computational demand often prohibits deployment on resource-constrained Industrial IoT Systems. Custom hardware accelerators can enable real-time NN processing on small-scale edge devices but are generally hard to develop and integrate. In this paper we present a hardware generation approach to rapidly create, test, and deploy entire SoC platforms with application-specific NN hardware accelerators. The feasibility of the approach is demonstrated by the generation of a condition monitoring system for high-speed valves.",https://ieeexplore.ieee.org/document/9557519/,2021 IEEE 19th International Conference on Industrial Informatics (INDIN),21-23 July 2021,ieeexplore
10.1109/SEC50012.2020.00019,Poster: Lambda architecture for robust condition based maintenance with simulated failure modes,IEEE,Conferences,"Condition based maintenance (CBM) is increasingly seen as a promising approach for addressing downtime issues which are a common occurrence in the manufacturing industry and are a major cause of lost productivity. However, it has been a challenge to develop a generic CBM solution that works for all assets since each asset has unique sources of noise. This mandates use of manual diagnostics to custom tailor a solution for each asset for accurate failure mode identification (FMI). This problem is further compounded by the scarcity of failure data. In this paper, we propose a lambda architecture for FMI of industrial assets that achieves low initial deployment cost while securing a reasonable classification accuracy. The lambda architecture consists of a light-compute edge node, such as Raspberry Pi, that processes high-speed vibration data in real-time to extract useful features and applies a deep-learning (DL) engine which is trained in a cloud platform, such as AWS. In addition, we also incorporate a failure modes' feature simulator so that DL models can adapt to different industrial assets without costly failure data collection. Finally, experimental results are provided using the bearings' failures dataset validating the proposed cost-effective CBM architecture with high accuracy and scalability.",https://ieeexplore.ieee.org/document/9355694/,2020 IEEE/ACM Symposium on Edge Computing (SEC),12-14 Nov. 2020,ieeexplore
10.1049/cp:19970735,Pre-processing of acoustic signals by neural networks for fault detection and diagnosis of rolling mill,IET,Conferences,"Incipient faults and changes in the structure of any industrial process may be detected and known by their effects: vibration and/or acoustic signals. We consider some methods for pre-processing the acoustic signal, generated by a rolling mill process, for fault detection and structural classification. The pre-processing methods are based on artificial neural networks. The methods refer to: signal decomposition algorithms, a distances measure for spectral amplitude classification and neural network structures for spectrum compression. For the signal decomposition problem an adaptive neural network algorithm is proposed in which the number of inputs is adapted to the imposed error. When the training error for two successive steps is very little, then the number of inputs in network is increased. If the spectral components are zero for sufficient time, then the number of inputs is decreased. The Hausdorff distance is proposed for spectrum classification as the distance measure for the frequency domain in a pattern recognition context. It shown that the Hausdorff distance has a monotone relationship with the signal-to-noise-ratio. Finally, the possibility of decreasing the number of spectrum components as patterns is presented, by compression with neural networks. Spectral representations of the acoustic source show that signatures collected at rolling mill sensor locations can be successfully used to identify process and structural changes in the rolling mill monitoring system. The results obtained by simulation is encouraging for real-time implementation.",https://ieeexplore.ieee.org/document/607526/,Fifth International Conference on Artificial Neural Networks (Conf. Publ. No. 440),7-9 July 1997,ieeexplore
10.1109/ICSME.2017.41,Predicting and Evaluating Software Model Growth in the Automotive Industry,IEEE,Conferences,"The size of a software artifact influences the software quality and impacts the development process. In industry, when software size exceeds certain thresholds, memory errors accumulate and development tools might not be able to cope anymore, resulting in a lengthy program start up times, failing builds, or memory problems at unpredictable times. Thus, foreseeing critical growth in software modules meets a high demand in industrial practice. Predicting the time when the size grows to the level where maintenance is needed prevents unexpected efforts and helps to spot problematic artifacts before they become critical. Although the amount of prediction approaches in literature is vast, it is unclear how well they fit with prerequisites and expectations from practice. In this paper, we perform an industrial case study at an automotive manufacturer to explore applicability and usability of prediction approaches in practice. In a first step, we collect the most relevant prediction approaches from literature, including both, approaches using statistics and machine learning. Furthermore, we elicit expectations towards predictions from practitioners using a survey and stakeholder workshops. At the same time, we measure software size of 48 software artifacts by mining four years of revision history, resulting in 4,547 data points. In the last step, we assess the applicability of state-of-the-art prediction approaches using the collected data by systematically analyzing how well they fulfill the practitioners' expectations. Our main contribution is a comparison of commonly used prediction approaches in a real world industrial setting while considering stakeholder expectations. We show that the approaches provide significantly different results regarding prediction accuracy and that the statistical approaches fit our data best.",https://ieeexplore.ieee.org/document/8094464/,2017 IEEE International Conference on Software Maintenance and Evolution (ICSME),17-22 Sept. 2017,ieeexplore
10.1109/RTEICT49044.2020.9315660,Prediction of Air Quality in Industrial Area,IEEE,Conferences,"Air quality monitoring and prediction in many industrial and urban areas, it has become one of the most important activities. Owing to different types of pollution, air quality is heavily affected. With increasing air pollution, efficient air quality monitoring models is to be implemented; these models gather data on the concentration of air pollutants. In a proposed approach, to solve three problems- prediction, interpolation and feature analysis, previously these problems were solved using three different models but now in the proposed system can solve these three problems in one model i.e Air Pollutant Prediction. This approach relates to unlabeled spatiotemporal data to enhance interpolation efficiency and air quality prediction. Experiments to test the proposed solution based on the real-time data sources collected by the Karnataka State Pollution Control Board (KSPCB), India. The goal of this research paper is to explore various strategies based on machine learning techniques for monitoring and predicating the air quality.",https://ieeexplore.ieee.org/document/9315660/,"2020 International Conference on Recent Trends on Electronics, Information, Communication & Technology (RTEICT)",12-13 Nov. 2020,ieeexplore
10.1109/ARITH.2019.00047,Privacy-Preserving Deep Learning via Additively Homomorphic Encryption,IEEE,Conferences,"We aim at creating a society where we can resolve various social challenges by incorporating the innovations of the fourth industrial revolution (e.g. IoT, big data, AI, robot, and the sharing economy) into every industry and social life. By doing so the society of the future will be one in which new values and services are created continuously, making people's lives more conformable and sustainable. This is Society 5.0, a super-smart society. Security and privacy are key issues to be addressed to realize Society 5.0. Privacy-preserving data analytics will play an important role. In this talk we show our recent works on privacy-preserving data analytics such as privacy-preserving logistic regression and privacy-preserving deep learning. Finally, we show our ongoing research project under JST CREST “AI”. In this project we are developing privacy-preserving financial data analytics systems that can detect fraud with high security and accuracy. To validate the systems, we will perform demonstration tests with several financial institutions and solve the problems necessary for their implementation in the real world.",https://ieeexplore.ieee.org/document/8877418/,2019 IEEE 26th Symposium on Computer Arithmetic (ARITH),10-12 June 2019,ieeexplore
10.1109/GloSIC.2018.8570124,Probabilistic Estimations of Increasing Expected Reliability and Safety for Intelligent Manufacturing,IEEE,Conferences,"In the near future the possibilities of the modern probabilistic models, artificial intelligence and machine learning methods can provide an intelligent support of making decisions by an operator in real time. An agile recovery of intelligent manufacturing integrity can be implemented owing to the development of industrial robotics. For intelligent manufacturing it means the expected reliability and safety may be in the near future at the expense of intelligent support of decision making and the agile recovery of integrity. To answer the question “How much essential may be this increasing?” here are proposed: general analytical approaches for a probabilistic estimation of the expected reliability and safety for every monitored element or the system of intelligent manufacturing on a level of probability distribution functions (PDF) of the time between the losses of system integrity; estimations of increasing the expected reliability and safety for intelligent manufacturing at the expense of the intelligent support of decision making and agile recovery of integrity; the comparisons of the estimations on a prognostic period up to 10 years using the identical model in applications to expected reliability and safety. The applications of the proposed approaches allow the customers, designers, developers, users and experts of Industry 4.0 intelligent manufacturing to be guided by the proposed probabilistic estimations for solving problems of reliability and safety in the system life cycle. The results are demonstrated by examples.",https://ieeexplore.ieee.org/document/8570124/,2018 Global Smart Industry Conference (GloSIC),13-15 Nov. 2018,ieeexplore
10.1109/INDIN41052.2019.8972094,Probabilistic Modelling combined with a CNN for boundary detection of carbon fiber fabrics,IEEE,Conferences,For many industrial machine vision applications it is difficult to acquire good training data to deploy deep learning techniques. In this paper we propose a method based on probabilistic modelling and rendering to generate artificial images of carbon fiber fabrics. We deploy a convolutional neural network (CNN) to learn detection of fabric contours from artificially generated images. Our network largely follows the recently proposed U-Net architecture. We provide results for a set of real images taken under controlled lighting conditions. The method can easily be adapted to similar problems in quality control for composite parts.,https://ieeexplore.ieee.org/document/8972094/,2019 IEEE 17th International Conference on Industrial Informatics (INDIN),22-25 July 2019,ieeexplore
10.1109/INDIN.2010.5549413,Process optimization of service-oriented automation devices based on Petri nets,IEEE,Conferences,"This paper introduces a novel method for the specification and selection of criteria-weighted operation modes for the orchestration of services in industrial automation using Petri nets. The objective is to provide to the internal decision support system of a service-oriented automation device or of another applicable computational system the capability to select the best path in a Petri net orchestration model considering different criteria to evaluate the quality of services, such as the time, energy efficiency and reliability. The transition-invariants obtained from the Petri net represent the set of possible modi operandi and these are then weighted with decision criteria. The result will be afterwards evaluated in order to select the optimal modus operandi to be executed by the device. Based on the experiments, this method permits the dynamic optimization of processes in real-time, considering available parameters from devices and other resources.",https://ieeexplore.ieee.org/document/5549413/,2010 8th IEEE International Conference on Industrial Informatics,13-16 July 2010,ieeexplore
10.1109/ECCTD.2011.6043628,Production test of an RF receiver chain based on ATM combining RF BIST and machine learning algorithm,IEEE,Conferences,"Testing an RF device in Production is expensive and technically difficult. At Wafer Test level, the RF probing technologies hardly fulfil the industrial test requirements in terms of accuracy, reliability and cost. At Package test level testing the RF parameters requires expensive RF equipments (RF automated test equipments (ATE)) and for complex RF transceivers, which address multi-modes (RF multi-paths and/or requiring different impedance matchings), it usually leads to prohibitive test time. In order to reduce the test costs for RF devices, different methods are proposed and evaluated in NXP and at competitions. These methods mainly target test time reduction (e.g. by testing parts in parallel) or propose ways of limiting the needs of expensive RF tester in Production (e.g. by using Alternate Test Methods, Design For Test, or Built In Test). In the proposed presentation we will focus on ATM and RF BI(s)T, providing some results on DC-RF correlation and an example of a real case BIT implementation into a fully integrated single-chip receiver operating in the sub-GHz ISM bands 315 MHz to 920 MHz.",https://ieeexplore.ieee.org/document/6043628/,2011 20th European Conference on Circuit Theory and Design (ECCTD),29-31 Aug. 2011,ieeexplore
10.1109/CSEET49119.2020.9206229,Project-Based Learning in a Machine Learning Course with Differentiated Industrial Projects for Various Computer Science Master Programs,IEEE,Conferences,"Graduating computer science students with skills sufficient for industrial needs is a priority in higher education teaching. Project-based approaches are promising to develop practical and social skills, needed to address real-world problems in teams. However, rapid technological transition makes an initial training of contemporary methods challenging. This affects the currently much-discussed machine learning domain as well. The study at hand describes a re-framed teaching approach for a machine learning course, offered to various computer science master programs. Project-based learning is introduced with differentiated projects provided by industrial partners that address the diverse study programs. Course attendees are supported with manuals, tools, and tutoring, passing through the Cross Industry Standard Process for Data Mining (CRISP-DM). Observations made during two iterations are reported, accompanied by a first empiric evaluation of student experiences.",https://ieeexplore.ieee.org/document/9206229/,2020 IEEE 32nd Conference on Software Engineering Education and Training (CSEE&T),9-12 Nov. 2020,ieeexplore
10.1109/PC.2017.7976254,Proposal of system for automatic weld evaluation,IEEE,Conferences,"The paper deals with the development of a system for automatic weld recognition using new information technologies based on cloud computing and single-board computer in the context of Industry 4.0. The proposed system is based on a visual system for weld recognition, and a neural network based on cloud computing for real-time weld evaluation, both implemented on a single-board low-cost computer. The proposed system was successfully verified on welding samples which correspond to a real welding process in the car production process. The system considerably contributes to the welds diagnostics in industrial processes of small- and medium-sized enterprises.",https://ieeexplore.ieee.org/document/7976254/,2017 21st International Conference on Process Control (PC),6-9 June 2017,ieeexplore
10.1109/SoCPaR.2011.6089156,QoS-oriented Service Management in clouds for large scale industrial activity recognition,IEEE,Conferences,"Motivated by the need of industrial enterprises for supervision services for quality, security and safety guarantee, we have developed an Activity Recognition Framework based on computer vision and machine learning tools, attaining good recognition rates. However, the deployment of multiple cameras to exploit redundancies, the large training set requirements of our time series classification models, as well as general resource limitations together with the emphasis on real-time performance, pose significant challenges and lead us to consider a decentralized approach. We thus adapt our application to a new and innovative real-time enabled framework for service-based infrastructures, which has developed QoS-oriented Service Management mechanisms in order to allow cloud environments to facilitate real-time and interactivity. Deploying the Activity Recognition Framework in a cloud infrastructure can therefore enable it for large scale industrial environments.",https://ieeexplore.ieee.org/document/6089156/,2011 International Conference of Soft Computing and Pattern Recognition (SoCPaR),14-16 Oct. 2011,ieeexplore
10.1109/COGINF.2010.5599677,Quadratic neural unit is a good compromise between linear models and neural networks for industrial applications,IEEE,Conferences,"The paper discusses the quadratic neural unit (QNU) and highlights its attractiveness for industrial applications such as for plant modeling, control, and time series prediction. Linear systems are still often preferred in industrial control applications for their solvable and single solution nature and for the clarity to the most application engineers. Artificial neural networks are powerful cognitive nonlinear tools, but their nonlinear strength is naturally repaid with the local minima problem, overfitting, and high demands for application-correct neural architecture and optimization technique that often require skilled users. The QNU is the important midpoint between linear systems and highly nonlinear neural networks because the QNU is relatively very strong in nonlinear approximation; however, its optimization and performance have fast and convex-like nature, and its mathematical structure and the derivation of the learning rules is very comprehensible and efficient for implementation.",https://ieeexplore.ieee.org/document/5599677/,9th IEEE International Conference on Cognitive Informatics (ICCI'10),7-9 July 2010,ieeexplore
10.1109/CONFLUENCE.2019.8776960,Quasi-Automated Firmware in E-Automobiles: Structural Integration,IEEE,Conferences,"Intelligent transit mechanism has become the need of the hour. The topical panacea seems to centre on the development of labyrinthine Automated Vehicles. With the inevitable boom in areas like Machine Learning, Industrial Automation coupled with highly convergent heuristics and the advent of highly efficient low latency communication devices, the idea of Automated Vehicles inches closer to practical realization every passing day. Design for any vehicle can be described as a function of various interdependent parameters, which are generally defined by the level of convolution and the level of automation defined for a system.This study explores the controller system design of Quasi-Automated Electric Vehicles. Categorically, this paper is an attempt at examining novel and innovative ways of designing a schematic for control of speed and navigation subsystems along with an exhaustive feedback capability designed to give users a real time virtual emulation of the vehicle. A wide ranging discussion on possible topologies for effective implementation of feedback have also been depicted in this paper. A column-type power steering system has been investigated as a control system for navigation of direction while an inverter based speed control mechanism has also been proposed. Furthermore, the discussed control algorithms have been rigorously tested and consequently proved capable of providing 1<sup>st</sup> level of autonomy, as defined by Society of Automotive Engineers standards while also reflecting a potential schematic for integration of intelligent learning firmware in the near future.",https://ieeexplore.ieee.org/document/8776960/,"2019 9th International Conference on Cloud Computing, Data Science & Engineering (Confluence)",10-11 Jan. 2019,ieeexplore
10.1109/ISIE.2015.7281681,RFID indoor localization based on support vector regression and k-means,IEEE,Conferences,"Systems need to know the physical locations of objects and people to optimize user experience and solve logistical and security issues. Also, there is a growing demand for applications that need to locate individual assets for industrial automation. This work proposes an indoor positioning system (IPS) able to estimate the item-level location of stationary objects using off-the-shelf equipment. By using RFID technology, a machine learning model based on support vector regression (SVR) is proposed. A multi-frequency technique is developed in order to overcome off-the-shelf equipment constraints. A k-means approach is also applied to improve accuracy. We have implemented our system and evaluated it using real experiments. The localization error is between 17 and 31 cm in 2.25m<sup>2</sup> area coverage.",https://ieeexplore.ieee.org/document/7281681/,2015 IEEE 24th International Symposium on Industrial Electronics (ISIE),3-5 June 2015,ieeexplore
10.1109/ICAIIC48513.2020.9065249,RNN-based Prediction for Network Intrusion Detection,IEEE,Conferences,"We investigate a prediction model using RNN for network intrusion detection in industrial IoT environments. For intrusion detection, we use anomaly detection methods that estimate the next packet, measure and score the distance measurement in real packets to distinguish whether it is a normal packet or an abnormal packet. When the packet was learned in the LSTM model, two-gram and sliding window of N-gram showed the best performance in terms of errors and the performance of the LSTM model was the highest compared with other data mining regression techniques. Finally, cosine similarity was used as a scoring function, and anomaly detection was performed by setting a boundary for cosine similarity that consider as normal packet.",https://ieeexplore.ieee.org/document/9065249/,2020 International Conference on Artificial Intelligence in Information and Communication (ICAIIC),19-21 Feb. 2020,ieeexplore
10.1109/ISDA.2014.7066281,ROS-based remote controlled robotic arm workcell,IEEE,Conferences,"This paper describes robotic workplace that is being developed at our faculty. It consists of industrial arm Mitsubishi Melfa RV-6SL, gripper Schunk and six Axis cameras. The purpose of this workplace is to serve students for testing algorithms from artificial intelligence and computer vision. It also gives them the opportunity to work with real industrial manipulator and allow them to test things like kinematics and dynamics of the arm. The arm is inaccessible for students so it can be operated only remotely. The software needed for remote controlling and programming is described in this paper.",https://ieeexplore.ieee.org/document/7066281/,2014 14th International Conference on Intelligent Systems Design and Applications,28-30 Nov. 2014,ieeexplore
10.1109/AQTR49680.2020.9129934,Rapid Prototyping of IoT Applications for the Industry,IEEE,Conferences,"In this article a novel approach to rapid IoT application prototype design and development is presented using an existing experimental dataset and a functional model. Using the existing data, we populate our NoSQL Apache Cassandra database cluster with legacy data and generate similar data using a python code, considering the Mosquitto MQTT protocol implementation and Node-RED node.js development environment. Using Node-RED, we display the data already collected, and dynamically create new data that can be monitored in real-time in the provided dashboard. The possibilities and utility of this approach are explored in the article, and a simple prototype application for modeling the open access Combined Cycle Power Plant (CCPP) dataset provided by the UCI Machine Learning Repository is presented to prove the efficiency and rapidity of IoT application development. The presented system development approach can be used in industrial environment for rapid development of IIoT applications.",https://ieeexplore.ieee.org/document/9129934/,"2020 IEEE International Conference on Automation, Quality and Testing, Robotics (AQTR)",21-23 May 2020,ieeexplore
10.1109/RTAS.2019.00033,Re-Thinking CNN Frameworks for Time-Sensitive Autonomous-Driving Applications: Addressing an Industrial Challenge,IEEE,Conferences,"Vision-based perception systems are crucial for profitable autonomous-driving vehicle products. High accuracy in such perception systems is being enabled by rapidly evolving convolution neural networks (CNNs). To achieve a better understanding of its surrounding environment, a vehicle must be provided with full coverage via multiple cameras. However, when processing multiple video streams, existing CNN frameworks often fail to provide enough inference performance, particularly on embedded hardware constrained by size, weight, and power limits. This paper presents the results of an industrial case study that was conducted to re-think the design of CNN software to better utilize available hardware resources. In this study, techniques such as parallelism, pipelining, and the merging of per-camera images into a single composite image were considered in the context of a Drive PX2 embedded hardware platform. The study identifies a combination of techniques that can be applied to increase throughput (number of simultaneous camera streams) without significantly increasing per-frame latency (camera to CNN output) or reducing per-stream accuracy.",https://ieeexplore.ieee.org/document/8743176/,2019 IEEE Real-Time and Embedded Technology and Applications Symposium (RTAS),16-18 April 2019,ieeexplore
10.1109/ICRA48506.2021.9562075,Reaching Pruning Locations in a Vine Using a Deep Reinforcement Learning Policy,IEEE,Conferences,"We outline a neural network-based pipeline for perception, control and planning of a 7 DoF robot for tasks that involve reaching into a dormant grapevine canopy. The proposed system consists of a 6 DoF industrial robot arm and a linear slider that can actuate on an entire grape vine. Our approach uses Convolutional Neural Networks to detect buds in dormant grape vines and a Reinforcement Learning based control strategy to reach desired cut-point locations for pruning tasks. Within this framework, three methodologies are developed and compared to reach the desired locations: the learned policy-based approach (RL), a hybrid method that uses the learned policy and an inverse kinematics solver (RL+IK), and lastly a classical approach commonly used in robotics. We first tested and validated the suitability of the proposed learning methodology in a simulated environment that resembled laboratory conditions. A reaching accuracy of up to 61.90% and 85.71% for the RL and RL+IK approaches respectively was obtained for a vine that the agent observed while learning. When testing in a new vine, the accuracy was up to 66.66% and 76.19% for RL and RL+IK, respectively. The same methods were then deployed on a real system in an end to end procedure: autonomously scan the vine using a vision system, create its model and finally use the learned policy to reach cutting points. The reaching accuracy obtained in these tests was 73.08%.",https://ieeexplore.ieee.org/document/9562075/,2021 IEEE International Conference on Robotics and Automation (ICRA),30 May-5 June 2021,ieeexplore
10.1109/ICIT.2006.372319,Real Time Classifier For Industrial Wireless Sensor Network Using Neural Networks with Wavelet Preprocessors,IEEE,Conferences,"Wireless sensor node is embedded of computation unit, sensing unit and a radio unit for communication. Amongst three units communication is the largest consumer of energy. Energy is the prime source for wireless sensor node to function. Hence every aspects of sensor node are designed with energy constraints. Neural Networks in particular the combination of ART1 and FuzzyART(FA) can be used very efficiently for developing Real time Classifier. Wireless sensor networks demand for the real time classification of sensor data. In this paper classification technique using ART1 and Fuzzy ART is discussed. ART1 and FA have very good architectural strategy, which makes it simple for VLSI implementation. The VLSI implementation of the proposed classifier can be a part of embedded microsensor. The paper discusses classification technique, which can reduce the energy need for communication and improves communications bandwidth. The proposed sensor clustering architecture can give distributed storage space for the sensor networks. Wavelet Transform is used as preprocessor for denoising the real word data from sensor node, this makes it much suitable for industrial environment. Many methods of wavelet transforms are available. Simplest Haar 1D transform is used for preprocessing and smoothing the sensor signals. The discrete wavelet transform implemented here helps to extract important feature in the sensor data like sudden changes at various scales.",https://ieeexplore.ieee.org/document/4237641/,2006 IEEE International Conference on Industrial Technology,15-17 Dec. 2006,ieeexplore
10.1109/FUZZY.2008.4630459,Real time Takagi-Sugeno fuzzy model based pattern recognition in the batch chemical industry,IEEE,Conferences,"This contribution describes the real time pressure check pattern recognition of an industrial batch dryer. The goal is to identify the start of the drying process and to calculate the time elapsed between two consequent batch starts (batch time) right after the batch has completed. The presented pattern recognition method implements a supervised learning approach based on Takagi-Sugeno fuzzy (TS) models. The decision maker design is based on plant data compressed by the PI algorithm (OSI Software, Inc). It is concluded that the developed classifier is able to perform real time classification and the compressed PI data can be used in order to design data analysis tools which are useful for chemical batch plant operation investigations.",https://ieeexplore.ieee.org/document/4630459/,2008 IEEE International Conference on Fuzzy Systems (IEEE World Congress on Computational Intelligence),1-6 June 2008,ieeexplore
10.1109/ICCICT.2018.8325873,Real time control of induction motor using neural network,IEEE,Conferences,"Induction Machine being simple in operation and highly reliable equipment with low cost involving minimal maintenance requirements it has become the most popular equipment in industry. With the development of power electronic technology, low cost DSP, micro-controllers and parameter estimation techniques the induction motor an attractive component for the future high performance drives induction motors have many applications in the industries. The PWM technique which drives a Voltage Source Inverter (VSI) in order to apply v/f control is used to control a 3 ph induction motor. In industrial applications the most widely used controllers are PI controllers because of their simple structure and their capability of delivering good performance over a large band of operating condition. PI and ANN controllers have been designed and developed using MATLAB/SIMULINK. Prototype model is developed to validate the effectiveness of the PI and ANN control of induction motor drive using dSPACE DS1104 controller. The performance of the SVPWM based induction motor in open loop and closed loop is presented with simulation. Artificial Neural Network and Conventional PI controllers have been practically implemented using SVPWM based VSI fed induction motor in open loop mode. Hardware set up has been developed using Inverter and dSPACE controller. The real time performance of ANN based induction motor is presented by validating simulation results with the hardware results.",https://ieeexplore.ieee.org/document/8325873/,2018 International Conference on Communication information and Computing Technology (ICCICT),2-3 Feb. 2018,ieeexplore
10.1109/IDAP.2017.8090180,Real time fabric defect detection system on Matlab and C++/Opencv platforms,IEEE,Conferences,"In industrial fabric productions, real time systems are needed to detect the fabric defects. This paper presents a real time defect detection approach which compares the time performances of Matlab and C++ programming languages. In the proposed method, important texture features of the fabric images are extracted using CoHOG method. Artificial neural network is used to classify the fabric defects. The developed method has been applied to detect the knitting fabric defects on a circular knitting machine. An overall defect detection success rate of 93% is achieved for the Matlab and C++ applications. To give an idea to the researches in defect detection area, real time operation speeds of Matlab and C++ codes have been examined. Especially, the number of images that can be processed in one second has been determined. While the Matlab based coding can process 3 images in 1 second, C++/Opencv based coding can process 55 images in 1 second. Previous works have rarely included the practical comparative evaluations of software environments. Therefore, we believe that the results of our industrial experiments will be a valuable resource for future works in this area.",https://ieeexplore.ieee.org/document/8090180/,2017 International Artificial Intelligence and Data Processing Symposium (IDAP),16-17 Sept. 2017,ieeexplore
10.1109/ICCCE.2008.4580693,Real time implementation of NARMA L2 feedback linearization and smoothed NARMA L2 controls of a single link manipulator,IEEE,Conferences,"Robotics is a field of modern technology which requires knowledge in vast areas such as electrical engineering, mechanical engineering, computer science as well as finance. Nonlinearities and parametric uncertainties are unavoidable problems faced in controlling robots in industrial plants. Tracking control of a single link manipulator driven by a permanent magnet brushed DC motor is a nonlinear dynamics due to effects of gravitational force, mass of the payload, posture of the manipulator and viscous friction coefficient. Furthermore uncertainties arise because of changes of the rotor resistance with temperature and random variations of friction while operating. Due to this fact classical PID controller can not be used effectively since it is developed based on linear system theory. Neural network control schemes for manipulator control problem have been proposed by researchers; in which their competency is validated through simulation studies. On the other hand, actual real time applications are rarely established. Instead of simulation studies, this paper is aimed to implement neural network controller in real time for controlling a DC motor driven single link manipulator. The work presented in this paper is concentrating on neural NARMA L2 control and its improvement called to as Smoothed NARMA L2 control. As proposed by K. S Narendra and Mukhopadhyay, Narma L2 control is one of the popular neural network architectures for prediction and control. The real time experimentation showed that the Smoothed NARMA L2 is effective for controlling the single link manipulator for both point-to-point and continuous path motion control.",https://ieeexplore.ieee.org/document/4580693/,2008 International Conference on Computer and Communication Engineering,13-15 May 2008,ieeexplore
10.1109/INDUSCON51756.2021.9529474,Real-Time Downhole Geosteering Data Processing Using Deep Neural Networks On FPGA,IEEE,Conferences,"The success of machine learning has spread the deployment of Deep neural Networks (DNNs) in numerous industrial applications. As an essential technique in today’s oilfield industry, geosteering requires performing DNN inference on the hardware devices that operates under the severe down-hole environments. However, it can produce massive power dissipation and cause long delays to execute the computation-intensive DNN inference on the current hardware platforms, e.g., CPU and GPU. In this paper, we propose an FPGA-based hardware design to efficiently conduct the DNN inference for geosteering tasks in downhole environments. At first, a comprehensive analysis is presented to choose the optimal computation mapping method for the target DNN model. A detailed description of the customized hardware implementation is then proposed to accomplish a complete DNN inference on the FPGA board. The experimental results shows that the proposed design achieves 7× (1.4×) improvement on performance and 82× (1.3×) reduction on power consumption compared with CPU(GPU).",https://ieeexplore.ieee.org/document/9529474/,2021 14th IEEE International Conference on Industry Applications (INDUSCON),15-18 Aug. 2021,ieeexplore
10.1109/08IAS.2008.164,Real-Time Implementation of Intelligent Modeling and Control Techniques on a PLC Platform,IEEE,Conferences,"Programmable logic controllers (PLCs) have been used for many decades for standard control in industrial and factory environments. Over the years, PLCs have become computational efficient and powerful, and a robust platform with applications beyond the standard control and factory automation. Due to the new advanced PLC's features and computational power, they are ideal platforms for exploring advanced modeling and control methods, including computational intelligence based techniques such as neural networks, particle swarm optimization (PSO) and many others. Some of these techniques require fast floating-point calculations that are now possible in real-time on the PLC. This paper focuses on the Allen-Bradley ControlLogix brand of PLCs, due to their high performance and extensive use in industry. The design and implementation of a neurocontroller consisting of two neural networks, one for modeling and the other for control, and the training of these neural networks with particle swarm optimization is presented in this paper on a single PLC. The neurocontroller in this study is a power system stabilizer (PSS) that is used for power system oscillation damping. The PLC is interfaced to a power system simulated on the real time digital simulator. Real time results are presented showing that the PLC is a suitable hardware platform for implementing advanced modeling and control techniques for industrial applications.",https://ieeexplore.ieee.org/document/4658952/,2008 IEEE Industry Applications Society Annual Meeting,5-9 Oct. 2008,ieeexplore
10.1109/EDPC51184.2020.9388185,Real-Time Inference of Neural Networks on FPGAs for Motor Control Applications,IEEE,Conferences,"Machine learning algorithms are increasingly used in industrial applications for a multitude of use-cases. However, using them in control tasks is a challenge due to real-time requirements and limited resources. In this paper, an implementation scheme for real-time inference of multilayer perceptron (MLP) neural networks on FPGAs is proposed. Design constraints for using MLPs in reinforcement learning agents for motor control applications are derived and accounted for in the implementation. Two MLP architectures are evaluated on an FPGA, and the timing and resource-usage data are reported. The real-time capability of the implementation for motor control applications is investigated for standard control frequencies. It is shown by experimental validation that real-time interference with an area-efficient implementation for motor control applications is achievable. Therefore, the proposed implementation scheme can be applied to deep reinforcement learning controllers with hard real-time requirements.",https://ieeexplore.ieee.org/document/9388185/,2020 10th International Electric Drives Production Conference (EDPC),8-9 Dec. 2020,ieeexplore
10.1109/I2CACIS.2019.8825093,Real-Time Robotic Grasping and Localization Using Deep Learning-Based Object Detection Technique,IEEE,Conferences,"This work aims to increase the impact of computer vision on robotic positioning and grasping in industrial assembly lines. Real-time object detection and localization problem is addressed for robotic grasp-and-place operation using Selective Compliant Assembly Robot Arm (SCARA). The movement of SCARA robot is guided by deep learning-based object detection for grasp task and edge detection-based position measurement for place task. Deep Convolutional Neural Network (CNN) model, called KSSnet, is developed for object detection based on CNN Alexnet using transfer learning approach. SCARA training dataset with 4000 images of two object categories associated with 20 different positions is created and labeled to train KSSnet model. The position of the detected object is included in prediction result at the output classification layer. This method achieved the state-of-the-art results at 100% precision of object detection, 100% accuracy for robotic positioning and 100% successful real-time robotic grasping within 0.38 seconds as detection time. A combination of Zerocross and Canny edge detectors is implemented on a circular object to simplify the place task. For accurate position measurement, the distortion of camera lens is removed using camera calibration technique where the measured position represents the desired location to place the grasped object. The result showed that the robot successfully moved to the measured position with positioning Root Mean Square Error (0.361, 0.184) mm and 100% for successful place detection.",https://ieeexplore.ieee.org/document/8825093/,2019 IEEE International Conference on Automatic Control and Intelligent Systems (I2CACIS),29-29 June 2019,ieeexplore
10.1109/SKIMA47702.2019.8982486,Real-Time Video Dehazing for Industrial Image Processing,IEEE,Conferences,"In today's industries, automation, reliability, robustness and accuracy are pivotal problem to cut costs and increase productivity and quality. Visual sensor networks are vital control and monitoring tools for continues, on-line imaging and real time image processing in production and plant process. Most of the industrial videos are captured in hazy weather and usually degraded by suspended particles of atmosphere, such as smoke, fog, rain, and snow, which limits the visual quality of image. This hinders the ability of artificial intelligent driven systems to achieve automation, reliability and accuracy. Recovery of the clear visuals from the input hazy videos is challenging problem. Instead of relying on explicitly estimating the key component of atmospheric scattering model, we present end-to-end CNN model, which directly recovers the clear images from hazy images. This end-to-end architecture makes it an ideal pre-processing tool into other deep models for increasing the efficiency of various computer vision tasks in real time systems, such as Retina-Net for object detection, ResNet for object recognition. Experimental results demonstrate the effectiveness and robustness of proposed framework by outperforming the stat-of-the-art approaches in terms of time complexity and visual quality.",https://ieeexplore.ieee.org/document/8982486/,"2019 13th International Conference on Software, Knowledge, Information Management and Applications (SKIMA)",26-28 Aug. 2019,ieeexplore
10.1109/ICSIMA47653.2019.9057343,Real-Time Wireless Monitoring for Three Phase Motors in Industry: A Cost-Effective Solution using IoT,IEEE,Conferences,"In recent days modern environment industries are facing rapid flourishing for performance capabilities and their requirements for corporate clients and industrial sector. Internet of Things (IoT) is an innovative and rapidly growing field for automation and evaluation in networks, Artificial Intelligence, data sensing, data mining, and big data. These systems have a great tendency to monitor and control different process used in industries. IoT systems have been implemented and have applications in different industries due to their cost-effectiveness and flexibility In this paper we have developed a system which includes real-time monitoring of current reading of three-phase motor through a wireless network. With the help of this system, data can be saved and monitored and then transmitted to cloud storage. This system contains Arduino-UNO board, ACS-712 current sensor, ESP-8266 Wi-Fi module which sends information to an IoT API service THING-SPEAK that behave like a cloud for various sensors to monitor data. The proposed system was successfully deployed in Aisha Steel Mills, Karachi, Pakistan.",https://ieeexplore.ieee.org/document/9057343/,"2019 IEEE International Conference on Smart Instrumentation, Measurement and Application (ICSIMA)",27-29 Aug. 2019,ieeexplore
10.1109/WCICA.2006.1713119,Real-time Advanced Intelligent Control Software Package,IEEE,Conferences,"A real-time advanced intelligent control software package is introduced in detail in this paper. It can be easily used to develop expert control system, fuzzy control system and neural network control system, meanwhile, it can work together with various industrial configuration software and equipments. It will promote the application of intelligent control technology",https://ieeexplore.ieee.org/document/1713119/,2006 6th World Congress on Intelligent Control and Automation,21-23 June 2006,ieeexplore
10.1109/COMPSAC51774.2021.00070,Real-time End-to-End Federated Learning: An Automotive Case Study,IEEE,Conferences,"With the development and the increasing interests in ML/DL fields, companies are eager to apply Machine Learning/Deep Learning approaches to increase service quality and customer experience. Federated Learning was implemented as an effective model training method for distributing and accelerating time-consuming model training while protecting user data privacy. However, common Federated Learning approaches, on the other hand, use a synchronous protocol to conduct model aggregation, which is inflexible and unable to adapt to rapidly changing environments and heterogeneous hardware settings in real-world scenarios. In this paper, we present an approach to real-time end-to-end Federated Learning combined with a novel asynchronous model aggregation protocol. Our method is validated in an industrial use case in the automotive domain, focusing on steering wheel angle prediction for autonomous driving. Our findings show that asynchronous Federated Learning can significantly improve the prediction performance of local edge models while maintaining the same level of accuracy as centralized machine learning. Furthermore, by using a sliding training window, the approach can minimize communication overhead, accelerate model training speed and consume real-time streaming data, proving high efficiency when deploying ML/DL components to heterogeneous real-world embedded systems.",https://ieeexplore.ieee.org/document/9529467/,"2021 IEEE 45th Annual Computers, Software, and Applications Conference (COMPSAC)",12-16 July 2021,ieeexplore
10.1109/CRV50864.2020.00032,Real-time Motion Planning for Robotic Teleoperation Using Dynamic-goal Deep Reinforcement Learning,IEEE,Conferences,"We propose Dynamic-goal Deep Reinforcement Learning (DGDRL) method to address the problem of robot arm motion planning in telemanipulation applications. This method intuitively maps human hand motions to a robot arm in real-time, while avoiding collisions, joint limits and singularities. We further propose a novel hardware setup, based on the HTC VIVE VR system, that enables users to smoothly control the robot tool position and orientation with hand motions, while monitoring its movements in a 3D virtual reality environment. A VIVE controller captures 6D hand movements and gives them as reference trajectories to a deep neural policy network for controlling the robot's joint movements. Our DGDRL method leverages the state-of-art Proximal Policy Optimization (PPO) algorithm for deep reinforcement learning to train the policy network with the robot joint values and reference trajectory observed at each iteration. Since training the network on a real robot is time-consuming and unsafe, we developed a simulation environment called RobotPath which provides kinematic modeling, collision analysis and a 3D VR graphical simulation of industrial robots. The deep neural network trained using RobotPath is then deployed on a physical robot (ABB IRB 120) to evaluate its performance. We show that the policies trained in the simulation environment can be successfully used for trajectory planning on a real robot. The the codes, data and video presenting our experiments are available at https://github.com/kavehkamali/ppoRobotPath.",https://ieeexplore.ieee.org/document/9108691/,2020 17th Conference on Computer and Robot Vision (CRV),13-15 May 2020,ieeexplore
10.1109/LCN.2018.8638081,Real-time Performance Evaluation of LTE for IIoT,IEEE,Conferences,"Industrial Internet of Things (IIoT) is claimed to be a global booster technology for economic development. IIoT brings bulky use-cases with a simple goal of enabling automation, autonomation or just plain digitalization of industrial processes. The abundance of interconnected IoT and CPS generate additional burden on the telecommunication networks, imposing number of challenges to satisfy the key performance requirements. In particular, the QoS metrics related to real-time data exchange for critical machine-to-machine type communication. This paper analyzes a real-world example of IIoT from a QoS perspective, such as remotely operated underground mining vehicle. As part of the performance evaluation, a software tool is developed for estimating the absolute, one-way delay in end-to-end transmissions. The measured metric is passed to a machine learning model for one-way delay prediction based on LTE RAN measurements using a commercially available cutting-edge software tool. The achieved results prove the possibility to predict the delay figures using machine learning model with a coefficient of determination up to 90%.",https://ieeexplore.ieee.org/document/8638081/,2018 IEEE 43rd Conference on Local Computer Networks (LCN),1-4 Oct. 2018,ieeexplore
10.1109/ICAR46387.2019.8981549,Real-time RGB-D semantic keyframe SLAM based on image segmentation learning from industrial CAD models,IEEE,Conferences,"This paper presents methods for performing realtime semantic SLAM aimed at autonomous navigation and control of a humanoid robot in a manufacturing scenario. A novel multi-keyframe approach is proposed that simultaneously minimizes a semantic cost based on class-level features in addition to common photometric and geometric costs. The approach is shown to robustly construct a 3D map with associated class labels relevant to robotic tasks. Alternatively to existing approaches, the segmentation of these semantic classes have been learnt using RGB-D sensor data aligned with an industrial CAD manufacturing model to obtain noisy pixel-wise labels. This dataset confronts the proposed approach in a complicated real-world setting and provides insight into the practical use case scenarios. The semantic segmentation network was fine tuned for the given use case and was trained in a semi-supervised manner using noisy labels. The developed software is real-time and integrated with ROS to obtain a complete semantic reconstruction for the control and navigation of the HRP4 robot. Experiments in-situ at the Airbus manufacturing site in Saint-Nazaire validate the proposed approach.",https://ieeexplore.ieee.org/document/8981549/,2019 19th International Conference on Advanced Robotics (ICAR),2-6 Dec. 2019,ieeexplore
10.1109/IJCNN48605.2020.9207462,"Real-time anomaly intrusion detection for a clean water supply system, utilising machine learning with novel energy-based features",IEEE,Conferences,"Industrial Control Systems have become a priority domain for cybersecurity practitioners due to the number of cyber-attacks against those systems has increased over the past few years. This paper proposes a real-time anomaly intrusion detector for a model of a clean water supply system. A testbed of such system is implemented using the Festo MPA Control Process Rig. A set of attacks to the testbed is conducted during the control process operation. During the attacks, the energy of the components is monitored and recorded to build a novel dataset for training and testing a total of five traditional supervised machine learning algorithms: K-Nearest Neighbour, Support Vector Machine, Decision Tree, Naïve Bayes and Multilayer Perceptron. The trained machine learning algorithms were built and deployed online, during the control system operation, for further testing. The performance obtained from offline and online training and testing steps are compared. The captures results show that KNN and SVM outperformed the rest of the algorithms by achieving high accuracy scores and low false-positive, false-negative alerts.",https://ieeexplore.ieee.org/document/9207462/,2020 International Joint Conference on Neural Networks (IJCNN),19-24 July 2020,ieeexplore
10.1109/ISIE.2010.5636556,Real-time evaluation of power quality using FPGA based measurement system,IEEE,Conferences,"Real-time evaluation of power quality is a desired feature in research and industrial projects, especially when embedded systems are employed and/or studied. Fast Fourier Transforms FFT is commonly used to evaluate the harmonic content of electric signals. Artificial Neural Networks (ANN) are also employed for harmonics estimation with short processing time and low implementation complexity. Commercial power quality measurement systems are available and offer good performance, communication and storage capabilities, and other special features, however in most of them the real-time information is not available or it is offered with important communication delays. This paper presents the implementation of a measurement system using Xilinx FPGA target and the Adaptive Linear Neuron (ADALINE) algorithm for real-time evaluation of power quality. Experimental results show that the implemented system can be employed for power quality monitoring and embedded control applications.",https://ieeexplore.ieee.org/document/5636556/,2010 IEEE International Symposium on Industrial Electronics,4-7 July 2010,ieeexplore
10.1109/SACI51354.2021.9465544,Real-time locating system and digital twin in Lean 4.0,IEEE,Conferences,"Digital twin plays a key role in the current development of smart manufacturing systems. Through simulation in the cyber world, real phenomena in the physical world can be predicted and optimized before the final implementation. The usage of the digital twin is enhanced along with the uprising of Industry 4.0, in which data availability supports the further insight of system status, helping the operation managers understand their system and perform resources adjustment more easily. Based on this digitization mature, Lean 4.0, a new concept elaborated from Lean manufacturing, has been interested recently. There are several technologies constituted digital twin that provide a favourable condition for Lean 4.0, such as augmented reality, cloud computing. In this paper, the integration of the Real-time Locating System (RTLS) into digital twin is proposed, which facilitates the performance of Lean 4.0 in manufacturing operation. Not only gain effective control over the facility's assets, but this integration also enhances the resources utilization, cut down operational wastes, thus brings a better turnover for industrial systems. A case study of successful implementation is shown, which proved the possible advantages of this approach.",https://ieeexplore.ieee.org/document/9465544/,2021 IEEE 15th International Symposium on Applied Computational Intelligence and Informatics (SACI),19-21 May 2021,ieeexplore
10.1109/PES.2003.1270511,Real-time power quality waveform recognition with a programmable digital signal processor,IEEE,Conferences,"Power quality (PQ) monitoring is an important issue to electric utilities and many industrial power customers. This paper presents a DSP-based hardware monitoring system based on a recently proposed PQ classification algorithm. The algorithm is implemented with a Texas Instruments (TI) TMS320VC5416 digital signal processor (DSP) with the TI THS1206 12-bit 6 MSPS analog to digital converter. A TI TMS320VC5416 DSP starter kit (DSK) is used as the host board with the THS1206 mounted on a daughter card. The implemented PQ classification algorithm is composed of two processes: feature extraction and classification. The feature extraction projects a PQ signal onto a time-frequency representation (TFR), which is designed for maximizing the separability between classes. The classifiers include a Heaviside-function linear classifier and neural networks with feedforward structures. The algorithm is optimized according to the architecture of the DSP to meet the hard realtime constraints of classifying a 5-cycle segment of the 60 Hz sinusoidal voltage/current signals in power systems. The classification output can be transmitted serially to an operator interface or control mechanism for logging and issue resolution.",https://ieeexplore.ieee.org/document/1270511/,2003 IEEE Power Engineering Society General Meeting (IEEE Cat. No.03CH37491),13-17 July 2003,ieeexplore
10.1109/ICCCN52240.2021.9522281,Realization of an Intrusion Detection use-case in ONAP with Acumos,IEEE,Conferences,"With Software-Defined Networking and Machine Learning/Artificial Intelligence (ML/AI) reaching new paradigms in their corresponding fields, both academia and industry have exhibited interests in discovering unique aspects of intelligent and autonomous communication networks. Transforming such intentions and interests to reality involves software development and deployment, which has its own story of significant evolution. There has been a notable shift in the strategies and approaches to software development. Today, the divergence of tools and technologies as per demand is so substantial that adapting a software application from one environment to another could involve tedious redesign and redevelopment. This implies enormous effort in migrating existing applications and research works to a modern industrial setup. Additionally, the struggles with sustainability maintenance of such applications could be painful. Concerning ML/AI, the capabilities to train, deploy, retrain, and re-deploy AI models as quickly as possible will be crucial for AI-driven network systems. An end-to-end workflow using unified open-source frameworks is the need of the hour to facilitate the integration of ML/AI models into the modern software-driven virtualized communication networks. Hence, in our paper, we present such a prototype by demonstrating the journey of a sample SVM classifier from being a python script to be deployed as a micro-service using ONAP and Acumos. While illustrating various features of Acumos and ONAP, this paper intends to make readers familiar with an end-to-end workflow taking advantage of the integration of both open-source platforms.",https://ieeexplore.ieee.org/document/9522281/,2021 International Conference on Computer Communications and Networks (ICCCN),19-22 July 2021,ieeexplore
10.1109/NAFIPS.2005.1548498,Reasoning about uncertainty in prognosis: a confidence prediction neural network approach,IEEE,Conferences,"Uncertainty representation and management is the Achilles heel of fault prognosis in condition based management systems. Long-term prediction of the time to failure of critical military and industrial systems entails large-grain uncertainty that must be represented effectively and managed efficiently, i.e. as more data becomes available, means must be devised to narrow or ""shrink"" the uncertainty bounds. Prediction accuracy and precision are typical performance metrics employed to access the performance of prognostic algorithms. That is, we would like the predicted time to failure to be as close as possible to the real one. Also, the bounds or limits of uncertainty must be as ""narrow"" as possible. This paper introduces a novel confidence prediction neural network construct with a confidence distribution node based on a Parzen estimate to represent uncertainty and a learning algorithm implemented as a lazy or Q-learning routine that improves online prognostics estimates. The approach is illustrated with test and simulation results obtained from a faulty helicopter planetary gear plate.",https://ieeexplore.ieee.org/document/1548498/,NAFIPS 2005 - 2005 Annual Meeting of the North American Fuzzy Information Processing Society,26-28 June 2005,ieeexplore
10.1109/IECON.2012.6389018,Recent advances in the application of real-time computational intelligence to industrial electronics,IEEE,Conferences,"The field of computational intelligence [CI] has seen advances in both the theoretical knowledge base of these techniques, and in specific applications of these techniques to real-world problems. This work first attempts to summarize the current trends and definitions in the CI branches of fuzzy systems, artificial neural networks [ANNs], and hybrid neuro-fuzzy systems and their variants. These particular branches of CI are selected for their ability to be implemented in real-time problem solving, whether computation and processing is done in software or implemented in hardware. Then, some current applications of these CI technologies for use in industrial electronics are highlighted and summarized.",https://ieeexplore.ieee.org/document/6389018/,IECON 2012 - 38th Annual Conference on IEEE Industrial Electronics Society,25-28 Oct. 2012,ieeexplore
10.1109/DEMPED.2019.8864828,Recognition of Electric Machines Boundary as The Constraint of Over Current Relay Coordination in Real Industrial Application with Serial Firefly Algorithm Optimization,IEEE,Conferences,"The electric machines such as motor, transformer, and generator plays an essential role in the production process of a factory. Due to expansion, there is an intention to invest a new electric machine that expected to work correctly with the existing system. To integrate the new electric machines, the existing protection system has to be evaluated carefully without limiting the machine's capability. Based on the actual experiences, one of the frustrating issues during the commissioning stage is energizing failure due to relay mal-trip. The objective of this paper is to summarize and model the electrical machines in the protection system perspective and offers a new method to coordinate the relays with the electric machines recognition using the artificial intelligence of serial firefly algorithm. This proposed method is endorsed by a real industrial power system and will demonstrate the ability to coordinate the relays without violating the electric machine boundary.",https://ieeexplore.ieee.org/document/8864828/,"2019 IEEE 12th International Symposium on Diagnostics for Electrical Machines, Power Electronics and Drives (SDEMPED)",27-30 Aug. 2019,ieeexplore
10.1109/ICSESS52187.2021.9522231,Recognition of Real-life Activities with Smartphone Sensors using Deep Learning Approaches,IEEE,Conferences,"Due to its vast applications in various industrial sectors, sensor-based human activity recognition (SHAR) has become a prevalent study issue in machine learning (ML) and deep learning (DL). With the improvement of numerous wearable sensors, many effective use cases have recently been revealed. According to recent research, real-world data contains more contextual information than data acquired in a laboratory environment. Three deep learning models were used to investigate real-life activities using smartphone sensors in this study. As two fundamental deep learning approaches, a convolutional neural network (CNN) and a long short-term memory (LSTM) network are used to achieve recognition. In addition, we introduced the Att-CNN-LSTM network as a hybrid DL model to handle the SHAR challenge using an attention mechanism. On a public dataset called real-life HAR (RL-HAR), these three deep learning models were evaluated using four assessment indicators: accuracy, precision, recall, and F1-score. According to the experimental data, the suggested Att-CNN-LSTM surpasses existing baseline deep learning models with the highest average accuracy of 95.76%.",https://ieeexplore.ieee.org/document/9522231/,2021 IEEE 12th International Conference on Software Engineering and Service Science (ICSESS),20-22 Aug. 2021,ieeexplore
10.1109/ICARM52023.2021.9536145,Reducing the Dimension of the Configuration Space with Self Organizing Neural Networks,IEEE,Conferences,"For robotics, especially industrial applications, it is crucial to reactively plan safe motions through efficient algorithms. Planning is more powerful in the configuration space than the task space. However, for robots with many degrees of freedom, this is challenging and computationally expensive. Sophisticated techniques for motion planning such as the Wavefront algorithm are limited by the high dimensionality of the configuration space, especially for robots with many degrees of freedom. For a neural implementation of the Wavefront algorithm in the configuration space, neurons represent discrete configurations and synapses are used for path planning. In order to decrease the complexity, we reduce the search space by pruning superfluous neurons and synapses. We present different models of self-organizing neural networks for this reduction. The approach takes real-life human motion data as input and creates a representation with reduced dimension. We compare six different neural network models and adapt the Wavefront algorithm to the different structures of the reduced output spaces. The method is backed up by an extensive evaluation of the reduced spaces, including their suitability for path planning by the Wavefront algorithm.",https://ieeexplore.ieee.org/document/9536145/,2021 6th IEEE International Conference on Advanced Robotics and Mechatronics (ICARM),3-5 July 2021,ieeexplore
10.1109/ICIEAM.2017.8076111,Redundant industrial manipulator control system,IEEE,Conferences,"We present the control system synthesis for the multilink redundant manipulator. Our control system is based on the unique algorithm that includes the novel hybrid method for solving the inverse kinematics problem. This method combines ANFIS-network and iterative refinement. As a result, the control system has high integrative capabilities and is easy to modify for another construction. The manipulator design is described by mathematical equations which are used for the workspace construction. These equations are used for creation of the neurofuzzy network and generation database (network training information). Modeling of the industrial manipulator with 5 degrees of freedom as an example of the implementation of our control system is considered in the paper. Virtual environment that displays a model motion in real time using a virtual 3-D model is also presented in the paper. We present the work results applied to the manipulator physical model. This model includes Festo servomotors and the Siemens programmable logical controller.",https://ieeexplore.ieee.org/document/8076111/,"2017 International Conference on Industrial Engineering, Applications and Manufacturing (ICIEAM)",16-19 May 2017,ieeexplore
10.1109/ICSME46990.2020.00074,Regression Testing of Massively Multiplayer Online Role-Playing Games,IEEE,Conferences,"Regression testing aims to check the functionality consistency during software evolution. Although general regression testing has been extensively studied, regression testing in the context of video games, especially Massively Multiplayer Online Role-Playing Games (MMORPGs), is largely untouched so far. One big challenge is that game testing requires a certain level of intelligence in generating suitable action sequences among the huge search space, to accomplish complex tasks in the MMORPG. Existing game testing mainly relies on either the manual playing or manual scripting, which are labor-intensive and time-consuming. Even worse, it is often unable to satisfy the frequent industrial game evolution. The recent process in machine learning brings new opportunities for automatic game playing and testing. In this paper, we propose a reinforcement learning-based regression testing technique that explores differential behaviors between multiple versions of an MMORPGs such that the potential regression bugs could be detected. The preliminary evaluation on real industrial MMORPGs demonstrates the promising of our technique.",https://ieeexplore.ieee.org/document/9240641/,2020 IEEE International Conference on Software Maintenance and Evolution (ICSME),28 Sept.-2 Oct. 2020,ieeexplore
10.1109/INDIN45582.2020.9442114,Reinforcement Learning Driven Adaptive VR Streaming with Optical Flow Based QoE,IEEE,Conferences,"With the merit of containing full panoramic content in one camera, Virtual Reality (VR) and 360° videos have arisen in the field of industrial cloud manufacturing and training. Industrial Internet of Things (IoT), where many VR terminals needed to be online at the same time, can hardly guarantee VR's bandwidth requirement. However, by making use of users' quality of experience (QoE) awareness factors, including the relative moving speed and depth difference between the viewpoint and other content, bandwidth consumption can be reduced. In this paper, we propose Optical Flow Based VR(OFB-VR), an interactive method of VR streaming that can make use of VR users' QoE awareness to ease the bandwidth pressure. The Just-Noticeable Difference through Optical Flow Estimation (JND-OFE) is explored to quantify users' awareness of quality distortion in 360° videos. Accordingly, a novel 360° videos QoE metric based on Peak Signal-to-Noise Ratio and JND-OFE (PSNR-OF) is proposed. With the help of PSNR-OF, OFB-VR proposes a versatile-size tiling scheme to lessen the tiling overhead. A Reinforcement Learning (RL) method is implemented to make use of historical data to perform Adaptive BitRate (ABR). For evaluation, we take two prior VR streaming schemes, Pano and Plato, as baselines. Vast evaluations show that our system can increase the mean PSNR-OF score by 9.5-15.8% while maintaining the same rebuffer ratio compared with Pano and Plato in a fluctuate LTE bandwidth dataset. Evaluation results show that OFB-VR is a promising prototype for actual interactive industrial VR. A prototype of OFB-VR can be found in https://github.com/buptexplorers/OFB-VR.",https://ieeexplore.ieee.org/document/9442114/,2020 IEEE 18th International Conference on Industrial Informatics (INDIN),20-23 July 2020,ieeexplore
10.1109/AI4I46381.2019.00027,Reinforcement Learning of a Robot Cell Control Logic using a Software-in-the-Loop Simulation as Environment,IEEE,Conferences,"This paper introduces a method for automatic robot programming of industrial robots using reinforcement learning on a Software-in-the-loop simulation. The focus of the the paper is on the higher levels of a hierarchical robot programming problem. While the lower levels the skills are stored as domain specific program code, the combination of the skills into a robot control program to solve a specific task is automated. The reinforcement learning learning approach allows the shopfloor workers and technicians just to define the end result of the manufacturing process through a reward function. The programming and process optimization is done within the learning procedure. The Software-in-the-loop simulation with the robot control software makes it possible to to interpret the real program code and generate the exact motion. The exact motion of the robot is needed in order to find not just an optimal but also a collision-free policy.",https://ieeexplore.ieee.org/document/9027783/,2019 Second International Conference on Artificial Intelligence for Industries (AI4I),25-27 Sept. 2019,ieeexplore
10.1109/ICECCS.2015.32,Requirements-Aided Automatic Test Case Generation for Industrial Cyber-physical Systems,IEEE,Conferences,"Industrial cyber-physical systems require complex distributed software to orchestrate many heterogeneous mechatronic components and control multiple physical processes. Industrial automation software is typically developed in a model-driven fashion where abstractions of physical processes called plant models are co-developed and iteratively refined along with the control code. Testing such multi-dimensional systems is extremely difficult because often models might not be accurate, do not correspond accurately with subsequent refinements, and the software must eventually be tested on the real plant, especially in safety-critical systems like nuclear plants. This paper proposes a framework wherein high-level functional requirements are used to automatically generate test cases for designs at all abstraction levels in the model-driven engineering process. Requirements are initially specified in natural language and then analyzed and specified using a formalized ontology. The requirements ontology is then refined along with controller and plant models during design and development stages such that test cases can be generated automatically at any stage. A representative industrial water process system case study illustrates the strengths of the proposed formalism. The requirements meta-model proposed by the CESAR European project is used for requirements engineering while IEC 61131-3 and model-driven concepts are used in the design and development phases. A tool resulting from the proposed framework called REBATE (Requirements Based Automatic Testing Engine) is used to generate and execute test cases for increasingly concrete controller and plant models.",https://ieeexplore.ieee.org/document/7384248/,2015 20th International Conference on Engineering of Complex Computer Systems (ICECCS),9-12 Dec. 2015,ieeexplore
10.1109/ICISS.2010.5656975,Research and implementation Of the temperature control system of heat treatment based on .NET and RS-485 bus,IEEE,Conferences,"RS-485 is a widely used industrial field bus. The successful application of AIBUS protocol in AI series display control instrument, make the AIDCS system's cost significantly lower than traditional DCS system. The paper successfully developed a prototype system based on RS-485 bus for high precision temperature control system of heat treatment, and based on the .net and AIBUS protocol, developed it' s system management software. This system have the characteristics of good real-time, high control-precision, high degree of automation, and friendly human-machine interface.",https://ieeexplore.ieee.org/document/5656975/,2010 International Conference on Intelligent Computing and Integrated Systems,22-24 Oct. 2010,ieeexplore
10.1109/ICET51757.2021.9450991,Research on Artificial Intelligence Industrial Big Data Platform for Industrial Internet Applications,IEEE,Conferences,"With the development of industry, enterprises have put forward higher requirements for real-time data collection and analysis. According to this, a distributed industrial big data platform for Industrial Internet applications is designed, which can effectively support the industrial field big data collection and storage. The system contains a distributed database, real-time database and offline big data platform to implement the collection and storage of all data in the system. Thus, the industrial big data platform architecture and related data hierarchical processing flow are proposed, which can effectively meet the requirements of the multi-source data and real-time processing of smart factory. The proposed data platform technical architecture will have important reference value for the realization of smart manufacturing and smart factory.",https://ieeexplore.ieee.org/document/9450991/,2021 IEEE 4th International Conference on Electronics Technology (ICET),7-10 May 2021,ieeexplore
10.1109/CAC51589.2020.9326677,Research on Ultrasonic Belt Velocimeter Based on DSC,IEEE,Conferences,"The belt conveyor has been used in the industrial production widely because of its strong conveying capacity, long distance and simple structure. A variety of protective devices are also applied to the belt conveyor. The velocimeter can detect the running speed of the belt in real time, and it can cooperate with the controller of the conveyor to protect the belt effectively when the belt is skidded. In this paper, a non-contact, high-precision integrated belt velocimeter is proposed, the basic structure and realization principle of the belt velocimeter are also introduced. The experimental data demonstrated the feasibility of the velocimeter.",https://ieeexplore.ieee.org/document/9326677/,2020 Chinese Automation Congress (CAC),6-8 Nov. 2020,ieeexplore
10.1109/ICBASE51474.2020.00070,Research on scheduling algorithm for industrial Internet of Things,IEEE,Conferences,"The continuous development of network and communication technology has a great impact on the national economy, and all countries attach great importance to the development of industrial Internet of things. Among them, the scheduling problem of industrial Internet of Things exists, such as packet transmission delay. In this paper, a TSN scheduling algorithm (NACO algorithm) based on ant colony system is proposed. Simulation experiments, the results show that the algorithm is a good way to solve the scheduling problem of industrial Iot, and compared with traditional algorithm, can avoid falling into local optimal solution, and has better convergence and optimization ability, and has certain delay change, able to provide deterministic time sensitive network real-time guarantees.",https://ieeexplore.ieee.org/document/9403824/,2020 International Conference on Big Data & Artificial Intelligence & Software Engineering (ICBASE),30 Oct.-1 Nov. 2020,ieeexplore
10.1109/IPEC51340.2021.9421229,Research on the Application of Machine Learning Big Data Mining Algorithms in Digital Signal Processing,IEEE,Conferences,"Traditional digital signal processing technology based on DSP and FPGA is more suitable for real-time signal processing, and is limited by data scale and frequency resolution, making it unsuitable for offline data processing, analysis and mining under large-scale data Application. At present, the industrial big data analysis platform can use Spark as a calculation engine for real-time signal processing and offline signal processing acceleration, but the analysis platform lacks mathematical calculation solutions such as digital signal processing suitable for distributed parallel calculation engines. This article is based on time the parity decomposition is selected, and the fast Fourier transform is realized by MATLAB software. Based on an example of the application of the compiled FFT source program, this article analyses the frequency spectrum of discrete-time and continuous-time signals of limited length.",https://ieeexplore.ieee.org/document/9421229/,"2021 IEEE Asia-Pacific Conference on Image Processing, Electronics and Computers (IPEC)",14-16 April 2021,ieeexplore
10.1109/MECHATRONIKA.2014.7018286,Robot imitation of human arm via Artificial Neural Network,IEEE,Conferences,"In this study, a robot arm that can imitate human arm is designed and presented. The potentiometers are located to the joints of the human arm in order to detect movements of human gestures, and data were collected by this way. The collected data named as “movement of human arm” are classified by the help of Artificial Neural Network (ANN). The robot performs its movements according to the classified movements of the human. Real robot and real data are used in this study. Obtained results show that the learning application of imitating human action via the robot was successfully implemented. With this application, the platforms of robot arm in an industrial environment can be controlled more easily; on the other hand, robotic automation systems which have the capability of making a standard movements of a human can become more resistant to the errors.",https://ieeexplore.ieee.org/document/7018286/,Proceedings of the 16th International Conference on Mechatronics - Mechatronika 2014,3-5 Dec. 2014,ieeexplore
10.1109/ICIAS.2012.6306173,SCARA robot control using neural networks,IEEE,Conferences,"A SCARA industrial robot model is identified based on a 4-axis structure using Lagrangian mechanics, also the dynamic model for the electromechanical actuator and motion transmission systems is identified. A conventional PD controller is implemented and compared to neural networks control system to achieve precise position control of SCARA manipulator. The performance of the modeled system is simulated using several desired tracking motion for each joint. Neural networks control method has shown a remarkable improvement of tracking capabilities for the SCARA robot over conventional PD controller. The proposed neural network controller has the potential to accurately control real-time manipulator applications.",https://ieeexplore.ieee.org/document/6306173/,2012 4th International Conference on Intelligent and Advanced Systems (ICIAS2012),12-14 June 2012,ieeexplore
10.1109/SPAC53836.2021.9539961,SDN-based Industrial Internet Security Gateway,IEEE,Conferences,"Industrial Internet is widely used in the production field. As the openness of networks increases, industrial networks facing increasing security risks. Information and communication technologies are now available for most industrial manufacturing. This industry-oriented evolution has driven the emergence of cloud systems, the Internet of Things (IoT), Big Data, and Industry 4.0. However, new technologies are always accompanied by security vulnerabilities, which often expose unpredictable risks. Industrial safety has become one of the most essential and challenging requirements. In this article, we highlight the serious challenges facing Industry 4.0, introduce industrial security issues and present the current awareness of security within the industry. In this paper, we propose solutions for the anomaly detection and defense of the industrial Internet based on the demand characteristics of network security, the main types of intrusions and their vulnerability characteristics. The main work is as follows: This paper first analyzes the basic network security issues, including the network security needs, the security threats and the solutions. Secondly, the security requirements of the industrial Internet are analyzed with the characteristics of industrial sites. Then, the threats and attacks on the network are analyzed, i.e., system-related threats and process-related threats; finally, the current research status is introduced from the perspective of network protection, and the research angle of this paper, i.e., network anomaly detection and network defense, is proposed in conjunction with relevant standards. This paper proposes a software-defined network (SDN)-based industrial Internet security gateway for the security protection of the industrial Internet. Since there are some known types of attacks in the industrial network, in order to fully exploit the effective information, we combine the ExtratreesClassifier to enhance the detection rate of anomaly detection. In order to verify the effectiveness of the algorithm, this paper simulates an industrial network attack, using the acquired training data for testing. The test data are industrial network traffic datasets, and the experimental results show that the algorithm is suitable for anomaly detection in industrial networks.",https://ieeexplore.ieee.org/document/9539961/,"2021 International Conference on Security, Pattern Analysis, and Cybernetics（SPAC)",18-20 June 2021,ieeexplore
10.1109/I2MTC50364.2021.9460075,SNR-based Reinforcement Learning Rate Adaptation for Time Critical Wi-Fi Networks: Assessment through a Calibrated Simulator,IEEE,Conferences,"Nowadays, the Internet of Things is spreading in several different research fields, such as factory automation, instrumentation and measurement, and process control, where it is referred to as Industrial Internet of Things. In these scenarios, wireless communication represents a key aspect to guarantee the required pervasive connectivity required. In particular, Wi-Fi networks are revealing ever more attractive also in time- and mission-critical applications, such as distributed measurement systems. Also, the multi-rate support feature of Wi-Fi, which is implemented by rate adaptation (RA) algorithms, demonstrated its effectiveness to improve reliability and timeliness. In this paper, we propose an enhancement of RSIN, which is a RA algorithm specifically conceived for industrial real-time applications. The new algorithm starts from the assumption that an SNR measure has been demonstrated to be effective to perform RA, and bases on Reinforcement Learning techniques. In detail, we start from the design of the algorithm and its implementation on the OmNet++ simulator. Then, the simulation model is adequately calibrated exploiting the results of a measurement campaign, to reflect the channel behavior typical of industrial environments. Finally, we present the results of an extensive performance assessment that demonstrate the effectiveness of the proposed technique.",https://ieeexplore.ieee.org/document/9460075/,2021 IEEE International Instrumentation and Measurement Technology Conference (I2MTC),17-20 May 2021,ieeexplore
10.1109/WF-IoT.2016.7845468,SQenloT: Semantic query engine for industrial Internet-of-Things gateways,IEEE,Conferences,"The Advent of Internet-of-Things (IoT) paradigm has brought exciting opportunities to solve many real-world problems. IoT in industries is poised to play an important role not only to increase productivity and efficiency but also to improve customer experiences. Two main challenges that are of particular interest to industry include: handling device heterogeneity and getting contextual information to make informed decisions. These challenges can be addressed by IoT along with proven technologies like the Semantic Web. In this paper, we present our work, SQenIoT: a Semantic Query Engine for Industrial IoT. SQenIoT resides on a commercial product and offers query capabilities to retrieve information regarding the connected things in a given facility. We also propose a things query language, targeted for resource-constrained gateways and non-technical personnel such as facility managers. Two other contributions include multi-level ontologies and mechanisms for semantic tagging in our commercial products. The implementation details of SQenIoT and its performance results are also presented.",https://ieeexplore.ieee.org/document/7845468/,2016 IEEE 3rd World Forum on Internet of Things (WF-IoT),12-14 Dec. 2016,ieeexplore
10.1109/SMARTCOMP52413.2021.00052,SWIMS: the Smart Wastewater Intelligent Management System,IEEE,Conferences,"Wastewater treatment is a critical process in urban and industrial settlements aiming to clean and protect the water as well as the overall environment. Wastewater management systems are conceived explicitly for purifying wastewater, providing clean water efficiently, but this is a hard task due to frequent and quite unpredictable fluctuations of inlet wastewater flows, arising from (random) rain water or (periodical, e.g. day-night) sewage sources, sometimes also leading to failures and outages. To ensure the quality of the clean water out above a threshold and keep the overall system operating, this paper proposes the smart wastewater intelligent management system (SWIMS). It monitors and controls inlet and outlet flows as well as the water quality and parts of the plant as a cyber-physical system (CPS), starting from an Environmental Internet of Things (EIoT) platform. The data generated from the treatment plant is collected in an information system hosted by a server together with an intelligent system that processes this information in a real-time fashion and provides the feedback for optimizing the plant to maintain a good quality of water over time. Such an intelligent system exploits deep learning approaches to control the behaviour of the wastewater treatment system through anomaly detection, supporting decision making on it. SWIMS has been implemented in a real case study deployed in Briatico, Italy. The data and results collected from such a case study are presented, analyzed and discussed in this paper, demonstrating the feasibility and the effectiveness of the SWIMS solution.",https://ieeexplore.ieee.org/document/9556275/,2021 IEEE International Conference on Smart Computing (SMARTCOMP),23-27 Aug. 2021,ieeexplore
10.1109/AVSS.2018.8639339,Scene Adaptation for Semantic Segmentation using Adversarial Learning,IEEE,Conferences,"Semantic Segmentation algorithms based on the deep learning paradigm have reached outstanding performances. However, in order to achieve good results in a new domain, it is generally demanded to fine-tune a pre-trained deep architecture using new labeled data coming from the target application domain. The fine-tuning procedure is also required when the domain application settings change, e. g., when a camera is moved, or a new camera is installed. This implies the collection and pixel-wise la-beling of images to be used for training, which slows down the deployment of semantic segmentation systems in real industrial scenarios and increases the industrial costs. Taking into account the aforementioned issues, in this paper we propose an approach based on Adversarial Learning to perform scene adaptation for semantic segmentation. We frame scene adaptation as the task of predicting semantic segmentation masks for images belonging to a Target Scene Context given labeled images coming from a Source Scene Context and unlabeled images coming from the Target Scene Context. Experiments highlight that the proposed method achieves promising performances both when the two scenes contain similar content (i.e., they are related to two different points of view of the same scene) and when the observed scenes contain unrelated content (i.e., they account to completely different scenes).",https://ieeexplore.ieee.org/document/8639339/,2018 15th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS),27-30 Nov. 2018,ieeexplore
10.1145/2372251.2372281,Seamless integration of order processing in MS Outlook using SmartOffice: An empirical evaluation,IEEE,Conferences,"MS Outlook is currently the most widespread e-mail client in corporate environments. However, e-mail management with MS Outlook is usually decoupled from enterprise processes, making it difficult to synchronize e-mails and attachments with currently running processes. In this paper, we introduce SmartOffice - an extension for MS Outlook allowing the seamless integration of e-mail management with enterprise workflows, thus increasing the effectiveness of e-mail processing as well as coupling process-relevant e-mails and documents with the respective process instances. SmartOffice was integrated with a legacy system supporting the import management process of a large German retailer. We evaluated the SmartOffice integration in an empirical study in the context of the import process, using real data, and with the employees of the retailer's import office. We conducted a semi-structured interview, where one participant answered questions after solving three typical tasks and surveyed a group after a presentation and demonstration of SmartOffice's functionality. The results show that SmartOffice has high potential for being introduced in the process with high efficiency and high user acceptance. Although the number of participants was low, the results are considered very relevant from the perspective of the domain experts, since the study took place in an industrial setting.",https://ieeexplore.ieee.org/document/6475413/,Proceedings of the 2012 ACM-IEEE International Symposium on Empirical Software Engineering and Measurement,20-21 Sept. 2012,ieeexplore
10.1109/OCEANS.2016.7761412,Self-tuned PID control based on backpropagation Neural Networks for underwater vehicles,IEEE,Conferences,"For a long time, PID-like controllers have been successfully used in academic and industrial tasks. This is thanks to its simplicity and suitable performance in linear or linearized plants, and under certain conditions, in nonlinear ones. A number of PID controller gains tuning approaches have been proposed in the literature in the last decades; most of them off-line techniques. However, in those cases wherein plants are subject to continuous parametric changes or external disturbances, online gains tuning is a need. This is the case of modular underwater ROVs (Remotely Operated Vehicles) where parameters (weight, buoyancy, added mass, among others) change according to the tool they are fitted with. In practice, some amount of time is dedicated to tune the PID gains of a ROV. Once the best set of gains has been achieved the ROV is ready to work. However, when the vehicle changes its tool or it is subject to ocean currents, its performance deteriorates since the fixed set of gains is no longer valid for the new conditions. Thus, an online PID gains tuning algorithm should be implemented to overcome this problem. In this paper, an auto-tuned PID-like controller based on Neural Networks (NN) is proposed. The NN plays the role of automatically estimating the suitable set of PID gains that achieves stability of the system. The NN adjusts online the controller gains that attain the smaller position tracking error. Simulation results are given considering an underactuated 6 DOF (degrees of freedom) underwater ROV. Real time experiments on an underactuated mini ROV are conducted to show the effectiveness of the proposed scheme.",https://ieeexplore.ieee.org/document/7761412/,OCEANS 2016 MTS/IEEE Monterey,19-23 Sept. 2016,ieeexplore
10.1109/DS-RT.2007.38,Semantic Web Service Architecture for Simulation Model Reuse,IEEE,Conferences,"COTS simulation packages (CSPs) have proved popular in an industrial setting with a number of software vendors. In contrast, options for re-using existing models seem more limited. Re-use of simulation component models by collaborating organizations is restricted by the same semantic issues however that restrict the inter-organization use of web services. The current representations of web components are predominantly syntactic in nature lacking the fundamental semantic underpinning required to support discovery on the emerging semantic web. Semantic models, in the form of ontology, utilized by web service discovery and deployment architecture provide one approach to support simulation model reuse. Semantic interoperation is achieved through the use of simulation component ontology to identify required components at varying levels of granularity (including both abstract and specialized components). Selected simulation components are loaded into a CSP, modified according to the requirements of the new model and executed. The paper presents the development of ontology, connector software and web service discovery architecture in order to understand how such ontology are created, maintained and subsequently used for simulation model reuse. The ontology is extracted from health service simulation - comprising hospitals and the National Blood Service. The ontology engineering framework and discovery architecture provide a novel approach to inter- organization simulation, uncovering domain semantics and adopting a less intrusive interface between participants. Although specific to CSPs the work has wider implications for the simulation community.",https://ieeexplore.ieee.org/document/4384541/,11th IEEE International Symposium on Distributed Simulation and Real-Time Applications (DS-RT'07),22-26 Oct. 2007,ieeexplore
10.1109/ETFA.2012.6489781,Semantic design and integration of simulation models in the industrial automation area,IEEE,Conferences,"Simulations are software tools approximating and predicting the behavior of real industrial plants. Unlike real plants, the utilization of simulations cannot cause damages and it saves time and costs during series of experiments. A shortcoming of current simulation models is the complicated runtime integration into legacy industrial systems and platforms, as well as ad-hoc design phase, introducing manual and error-prone work. This paper contributes to improve the efficiency of simulation model design and integration. It utilizes a semantic knowledge base, implemented by ontologies and their mappings. The integration uses the Automation Service Bus and the paper explains how to configure the runtime integration level semantically. The main contributions are the concept of semantic configuration of the service bus and the workflows of simulation design and integration.",https://ieeexplore.ieee.org/document/6489781/,Proceedings of 2012 IEEE 17th International Conference on Emerging Technologies & Factory Automation (ETFA 2012),17-21 Sept. 2012,ieeexplore
10.1109/ETFA.2014.7005195,Semantic repository for case-based reasoning in CBM services,IEEE,Conferences,"Condition-based maintenance (CBM) has been implemented in industry to arrange the maintenance work as efficiently as possible. Case-based reasoning (CBR) can be used to automate part of the CBM decision process. However, in complex situations the final decisions have to be made by domain maintenance experts based on information gathered from several sources. This paper presents an approach for utilizing Semantic Web technologies and CBR in a knowledge base system supporting CBM services. The case knowledge base (CKB) is built over a semantic repository with an inference engine supporting ontology based information integration and data access using SPARQL queries. The knowledge base model developed for the system contains CBR task ontology and domain ontology for industrial control valves. Feasibility of the prototype CKB system was evaluated in experiments with real industrial case data.",https://ieeexplore.ieee.org/document/7005195/,Proceedings of the 2014 IEEE Emerging Technology and Factory Automation (ETFA),16-19 Sept. 2014,ieeexplore
10.1109/CVPR.2019.00715,ShieldNets: Defending Against Adversarial Attacks Using Probabilistic Adversarial Robustness,IEEE,Conferences,"Defending adversarial attack is a critical step towards reliable deployment of deep learning empowered solutions for industrial applications. Probabilistic adversarial robustness (PAR), as a theoretical framework, is introduced to neutralize adversarial attacks by concentrating sample probability to adversarial-free zones. Distinct to most of the existing defense mechanisms that require modifying the architecture/training of the target classifier which is not feasible in the real-world scenario, e.g., when a model has already been deployed, PAR is designed in the first place to provide proactive protection to an existing fixed model. ShieldNet is implemented as a demonstration of PAR in this work by using PixelCNN. Experimental results show that this approach is generalizable, robust against adversarial transferability and resistant to a wide variety of attacks on the Fashion-MNIST and CIFAR10 datasets, respectively.",https://ieeexplore.ieee.org/document/8953209/,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),15-20 June 2019,ieeexplore
10.1109/RO-MAN50785.2021.9515431,Simplifying the A.I. Planning modeling for Human-Robot Collaboration,IEEE,Conferences,"For an effective deployment in manufacturing, Collaborative Robots should be capable of adapting their behavior to the state of the environment and to keep the user safe and engaged during the interaction. Artificial Intelligence (AI) enables robots to autonomously operate understanding the environment, planning their tasks and acting to achieve some given goals. However, the effective deployment of AI technologies in real industrial environments is not straightforward. There is a need for engineering tools facilitating communication and interaction between AI engineers and Domain experts. This paper proposes a novel software tool, called TENANT (Tool fostEriNg Ai plaNning in roboTics) whose aim is to facilitate the use of AI planning technologies by providing domain experts like e.g., production engineers, with a graphical software framework to synthesize AI planning models abstracting from syntactic features of the underlying planning formalism.",https://ieeexplore.ieee.org/document/9515431/,2021 30th IEEE International Conference on Robot & Human Interactive Communication (RO-MAN),8-12 Aug. 2021,ieeexplore
10.1109/IntelCIS.2015.7397201,Simulation Model of the Decision-Making Support for Human-Machine Systems Operators,IEEE,Conferences,Simulation Model for the Decision-Making Support of the Human-Machine Systems Operator was developed. The term of operator's professional confidence as an index of the operator's ability to maintain or for a certain time lead to a stabile state of the Human-Machine Systems by performing the motor operator activity as the final component of the implementation of the decision was proposed. The motor operator's activity speed typed classification was developed. Algorithm and tool for the identification of the motor operator's activity speed of the Human-Machine Systems were offered. The Simulation Experiment Results were presented. The ways of the Simulation Model industrial realization and implementation as part of the real Automated Decision Support System were formulated.,https://ieeexplore.ieee.org/document/7397201/,2015 IEEE Seventh International Conference on Intelligent Computing and Information Systems (ICICIS),12-14 Dec. 2015,ieeexplore
10.23919/AEIT.2018.8577226,Smart Farms for a Sustainable and Optimized Model of Agriculture,IEEE,Conferences,"Nowadays, public and private companies, are in a constant race to increase profitability, chasing the costs reduction while facing the market competition. Also in the agriculture an analysis of cost-effectiveness, measuring technological innovation and profitability becomes necessary. The `smart farm' model exploits information coming from technologies like sensors, intelligent systems and the Internet of Things (IoT) paradigm to understand the influential and non-influential factors while considering environmental, productive and structural data coming from a large number of sources. The goal of this work is to design and deploy practical tasks that exploit heterogeneous real datasets with the aim to forecast and reconstruct values using and comparing innovative machine learning techniques with more standard ones. The application of these methodologies, in fields that are only apparently refractory to the technology such as the agricultural one, shows that there are ample margins for innovation and investment while supporting requests and needs coming from companies that wish to employ a sustainable and optimized agricultural industrial business.",https://ieeexplore.ieee.org/document/8577226/,2018 AEIT International Annual Conference,3-5 Oct. 2018,ieeexplore
10.1109/IDAP.2018.8620887,Smart Grids and Industry 4.0,IEEE,Conferences,"Since the beginning of the Industrial Revolution, the concept of Industrial Revolution in the 4<sup>th</sup>/ Industry 4.0 which is a new turning point in the technologies used, is in every field as well as in the energy field which is considered as the key point of all revolutions. It is aimed that energy generation, transmission and distribution are more efficient and highly reliable together with the next generation software and hardware which is the result of this concept.It has been seen that existing networks can not be enough to achieve this goal.At this point, flexible, reliable, clean, sustainable and highly efficient electric energy due to smart grids come into play. However, due to the increase in demand for electricity due to increase in population and industrialization, the dependence of electric energy on production resources, the availability of renewable energy sources in a dispersed geographical structure and the loss and leakage are becoming more important in our country. The Cyber-Physical System, the Internet of Things, M2M (machine to machine), etc components are used in the energy field, especially in intelligent network systems, resulting in intensive use of technology at every stage from power generation to end-user distribution point. This includes better system operation, reduced costs, increased energy efficiency, reduced greenhouse effect, reduced downtime with reduced downtime, reduced loss/leakage rates, improved energy quality, better management of production and storage systems, intelligent meter reading and load management and real-time supply-demand management. Industry 4.0/4.Industrial Revolution and smart grids have many common aspects such as optimization, automation, efficient use and management of energy, intelligent production, everything internet. In this study, the development process of the Industrial 4.0 revolution in the world and in our country will be examined and the interaction with these process smart grids will be examined.",https://ieeexplore.ieee.org/document/8620887/,2018 International Conference on Artificial Intelligence and Data Processing (IDAP),28-30 Sept. 2018,ieeexplore
10.1109/IEMCON.2018.8615072,Smart Mirror - a Secured Application of Artificial Intelligence Recognizing Human Face and Voice,IEEE,Conferences,"Smart mirrors are a brand new addition to the IoT product family that has been obtaining a great deal of attention in recent years by each industrial makers and hobbyists. This paper describes the planning associated implementation of a voice controlled wall mirror, referred to as “Magic Mirror” with Artificial Intelligence for the home environment. It is a mirror, which can display real time content like time, date, weather and news at the same time. The Magic Mirror consists of functionalities like real time information and data updates, voice commands, face recognition. The user can control the magic mirror by voice commands.",https://ieeexplore.ieee.org/document/8615072/,"2018 IEEE 9th Annual Information Technology, Electronics and Mobile Communication Conference (IEMCON)",1-3 Nov. 2018,ieeexplore
10.1109/ICCPEIC.2017.8290335,Smart personalized learning system for energy management in buildings,IEEE,Conferences,"Integration of energy management systems into existing buildings brings in several challenges and financial constraints. Some of the challenges in the existing smart building solutions are that they require large-scale deployment of sensors, high rate of data collection, real-time data analysis in short span of time, and lack of knowledge about the energy usage with respect to the behavior of individuals and groups. This work proposes an affordable wearable device system as an alternative for large-scale deployment of sensors in industrial buildings. For effective energy management in the buildings, a personalized behavior analysis has been done in machine learning and neural networks algorithm and integrated with the proposed system. The complete system is implemented and tested extensively. The results show that the proposed system could provide 85% user comfort and 23% energy savings.",https://ieeexplore.ieee.org/document/8290335/,"2017 International Conference on Computation of Power, Energy Information and Commuincation (ICCPEIC)",22-23 March 2017,ieeexplore
10.1109/SMCIA.1999.782701,Soft computing techniques for intelligent classification system: a case study,IEEE,Conferences,"Soft computing techniques are becoming popular in designing real world industrial applications. Researchers are trying to integrate different soft computing paradigms such as fuzzy logic, artificial neural network, genetic algorithms etc., to develop hybrid intelligent autonomous systems that provide more flexibility by exploiting tolerance and uncertainty of real life situations. Intelligent classification systems are the most well known attempts. In this work a neuro fuzzy feature selector has been designed which is capable of extracting information in the form of fuzzy rules from numeric as well as non-numeric (linguistic) data. Conventional MLP and a variation of it have been used as the neural models and their performance has been compared by simulation with two different data sets. It is found that the proposed variation of the conventional MLP is better in respect to training time and classification accuracy.",https://ieeexplore.ieee.org/document/782701/,SMCia/99 Proceedings of the 1999 IEEE Midnight - Sun Workshop on Soft Computing Methods in Industrial Applications (Cat. No.99EX269),18-18 June 1999,ieeexplore
10.1109/CCAA.2016.7813753,Soft computing techniques to address various issues in wireless sensor networks: A survey,IEEE,Conferences,"Wireless sensor network (WSN) is a collection of large number of self-organized types of sensors which chain together to monitor and record physical or environmental conditions (i.e. used to measure temperature, sound, pressure) and passes gathered information to the central location. WSN build bridge between real world and virtual environment, which makes it more utilizable for many applications. Mainly WSN was used for military arena but now a days it is used in various area like industrial applications, consumer applications, health care applications and many more. Despite of having many advantages there are some issues also occurred in WSNs like hotspot problem, energy hole problem, routing, coverage problem, load balancing problem and so on. These issues effect on different factors of WSN named energy consumption, stability, quality, deployment time, lifetime of network, which degrade the performance of the WSN. To solve these issues various researchers develop different mechanisms. Among all of them, in this paper, we survey different kind of soft computing paradigms. Soft computing is a technique to use of improper solutions to solve the complicated problem in robust time. There are various types of soft computing techniques developed: swarm intelligence, fuzzy logic, neural network, reinforcement learning and evolutionary algorithm, which used to solve WSN problems so that performance of the network will be increased.",https://ieeexplore.ieee.org/document/7813753/,"2016 International Conference on Computing, Communication and Automation (ICCCA)",29-30 April 2016,ieeexplore
10.1109/MELCON.2010.5476042,Soft sensors and artificial intelligence for nuclear fusion experiments,IEEE,Conferences,"Soft sensors are mathematical models able to estimate process variables. They can work in parallel with hardware sensors, and can be implemented at a low-cost on existing hardware. They are useful for back-up of measuring devices, reduction of measuring hardware requirements, real-time estimation for monitoring and control, sensor validation, fault detection and diagnosis, what-if analysis. In industrial applications, data-driven approaches, especially based on soft-computing techniques, are very promising. In this paper we review important issues in soft sensor design and applications, especially concerning the applications in the field of nuclear fusion.",https://ieeexplore.ieee.org/document/5476042/,Melecon 2010 - 2010 15th IEEE Mediterranean Electrotechnical Conference,26-28 April 2010,ieeexplore
10.1109/CAC.2017.8243743,Soft-sensor software design of dissolved oxygen in aquaculture,IEEE,Conferences,"A software is designed for water quality monitoring in the aquaculture industry. It is mainly for dissolved oxygen soft sensing. Dissolved oxygen is an important dependent factor of water quality and has effect on fish growth. At present, most of the dissolved oxygen sensors are expensive for aquaculture farmers, so they won't use them for real-time detection to perform control and optimal operation. To deal with this problem, a soft-sensor software was designed and developed based on a data-driven model which is proposed by partial least squares (PLS) and neural networks. The software included three parts which were control software, monitoring software and model calculation software. An industrial case study demonstrated the feasibility and efficiency of the proposed soft-sensor software, and it was simple to use, real-time and generic. It was also the foundation for the control and optimization of dissolved oxygen in the aquaculture process.",https://ieeexplore.ieee.org/document/8243743/,2017 Chinese Automation Congress (CAC),20-22 Oct. 2017,ieeexplore
10.1109/ASE.2011.6100070,Software process evaluation: A machine learning approach,IEEE,Conferences,"Software process evaluation is essential to improve software development and the quality of software products in an organization. Conventional approaches based on manual qualitative evaluations (e.g., artifacts inspection) are deficient in the sense that (i) they are time-consuming, (ii) they suffer from the authority constraints, and (iii) they are often subjective. To overcome these limitations, this paper presents a novel semi-automated approach to software process evaluation using machine learning techniques. In particular, we formulate the problem as a sequence classification task, which is solved by applying machine learning algorithms. Based on the framework, we define a new quantitative indicator to objectively evaluate the quality and performance of a software process. To validate the efficacy of our approach, we apply it to evaluate the defect management process performed in four real industrial software projects. Our empirical results show that our approach is effective and promising in providing an objective and quantitative measurement for software process evaluation.",https://ieeexplore.ieee.org/document/6100070/,2011 26th IEEE/ACM International Conference on Automated Software Engineering (ASE 2011),6-10 Nov. 2011,ieeexplore
10.1109/WRC-SARA.2019.8931920,Software-defined Cloud Manufacturing in the Context of Industry 4.0,IEEE,Conferences,"In the practice of “Cloud Manufacturing (CMfg)” or “Industrial Internet”, there still exist key problems, including: 1) big data analytics and decision-making in the cloud could not meet the requirements of time-sensitive manufacturing applications, moreover uploading ZettaBytes of future device data to the cloud may cause serious network congestion, 2) the manufacturing system lacks openness and evolvability, thus restricting the rapid optimization and transformation of the system, 3) big data from the shop-floor IoT devices and the internet has not been effectively utilized to guide the optimization and upgrade of the manufacturing system. In view of these key practical problems, we propose an open evolutionary architecture of intelligent CMfg system with collaborative edge and cloud processing capability. Hierarchical gateways near shop-floor things are introduced to enable fast processing for time-sensitive applications. Big data in another dimension from the software defined perspective will be used to decide the efficient operations and highly dynamic upgrade of the system. From the software system view, we also propose a new mode - AI-Mfg-Ops (AI-enabled Cloud Manufacturing Operations) with a supporting framework, which can promote the fast operation and upgrading of CMfg systems with AI enabled monitoring-analysis-planning-execution close loop. This work can improve the universality of CMfg for real-time fast response and operation &amp; upgrading.",https://ieeexplore.ieee.org/document/8931920/,2019 WRC Symposium on Advanced Robotics and Automation (WRC SARA),21-22 Aug. 2019,ieeexplore
10.1109/ICCCYB.2005.1511567,Some interactive multimedia applications in production engineering,IEEE,Conferences,"The authors of this paper give a short summary on the research and development actions carried out during a Hungarian National Research and Development Program addressed as ""Digital Factory"". The project started in 2001 with the consortium consisting of three academic and two industrial partners, and has reached an end in 2004. This paper deals with one of the three main streams of R&amp;D activities carried out when paper outlines the goals of the tasks, related to interactive multimedia and tele-presence based industrial applications. The basic research phase of the work was followed by applied research and development, while the concluding phase enabled industrial applications and test scenarios. The authors explain some details of their achievements.",https://ieeexplore.ieee.org/document/1511567/,"IEEE 3rd International Conference on Computational Cybernetics, 2005. ICCC 2005.",13-16 April 2005,ieeexplore
10.1109/PQ.2016.7724114,Speed control for a permanent magnet synchronous motor based on fuzzy logic with reduced perturbations on the supply network,IEEE,Conferences,"Fuzzy logic controllers, FLCs, are elements of the artificial intelligence, and, today, they are widely use in command and control of many industrial processes. It involves the acquisition of better signal wave forms at the output and the reduction of all oscillations (in case of electric drives, we consider speed variation, torque variation, current variation for all the three phases ore more) in comparison with the classic control procedures. This paper presents two case studies of two FLCs applied for some exterior permanent magnet synchronous machines, PMSM. One fuzzy logic algorithm has 25 rules and the other has 49 rules. These FLCs are developed for less computational issues, which make them suitable for real time implementation. The permanent magnet synchronous machines are used today in state of the art drives due to their dynamic performances. It is known that PMSM has a high torque density, with reduced losses/torque rates, a high power factor and a quick torque and speed time response. All the references concerning the real time implementation are made on the digital signal processor controller board type DS1104.",https://ieeexplore.ieee.org/document/7724114/,2016 Electric Power Quality and Supply Reliability (PQ),29-31 Aug. 2016,ieeexplore
10.1109/ICRITO48877.2020.9198036,State of Art: Energy Efficient Protocols for Self-Powered Wireless Sensor Network in IIoT to Support Industry 4.0,IEEE,Conferences,"Up gradation of manufacturing systems in industries by means of implementing innovative manufacturing techniques that captures real time data, applies machine learning algorithms, makes entire system self-decisive and provides inter connectivity to the whole system is the prime focus of the Industry 4.0. It is aimed at bringing new industrial revolution with the help of internet of things technology due to its considerable influence in the industrial manufacturing process. Though impact of internet of things in industrial sector is huge, a practical implementation incorporates challenges in energy efficiency, self-powered sensor nodes and security. For processing of gathered data self-powered sensor nodes may sinks energy from ambient energy sources. A considerable amount of efforts has been put by researchers to address the challenges for development of energy efficient routing protocol for such nodes. The presented survey is aimed at analyzing the protocols for contribution towards the goals of industry 4.0 Energy efficient protocol will support the system to consume least energy for its operation. Henceforth combination of self-powered wireless sensor network and energy efficient protocol will be useful to power up many industrial IoT applications.",https://ieeexplore.ieee.org/document/9198036/,"2020 8th International Conference on Reliability, Infocom Technologies and Optimization (Trends and Future Directions) (ICRITO)",4-5 June 2020,ieeexplore
10.1109/ICPST.2002.1053583,State self-adaptive monitor and control system for 6 kV/1600 kVA adjustable-speed drive,IEEE,Conferences,"Based on proposed state self-adaptive monitor logic and volts per hertz (V/f) control method with current and voltage limitation, a compact operation monitor and control system (OMCS) is developed for real-time monitoring and control of a 6 kV/1600 kVA adjustable-speed drive (ASD) installation. The hardware of OMCS is based on industrial PC and composed of AI and DI data acquisition cards and TMS320C32 DSP based pulse system with specially designed configuration to exempt from strong electromagnetic interference. Implemented under Chinese Window 98 operating system with VC6.0 programming language, the software of OMCS can provide friendly man-machine interface with multi-functions. Experimental results show that the proposed monitor and control system can reliably supervise the operation conditions of ASD and efficiently control acceleration and deceleration of the induction motor without violating limits of current and voltage.",https://ieeexplore.ieee.org/document/1053583/,Proceedings. International Conference on Power System Technology,13-17 Oct. 2002,ieeexplore
10.1109/IECON.2016.7793206,Summer school on intelligent agents in automation: Hands-on educational experience on deploying industrial agents,IEEE,Conferences,"Cyber-physical systems constitutes a framework to develop intelligent, distributed, resilient, collaborative and cooperative systems, promoting the fusion of computational entities and physical devices. Agent technology plays a crucial role to develop this kind of systems by offering a decentralized, distributed, modular, robust and reconfigurable control structure. This paper describes the implementation of a summer school aiming to enhance the participants' knowledge in the field of multi-agent systems applied to industrial environments, being able to gain the necessary theoretical and practical skills to develop real industrial agent based applications. This is accomplished in an international framework where individual knowledge and experiences are shared in a complementary level.",https://ieeexplore.ieee.org/document/7793206/,IECON 2016 - 42nd Annual Conference of the IEEE Industrial Electronics Society,23-26 Oct. 2016,ieeexplore
10.1109/IJCNN.2006.246929,Synthesis of Pulsed-Coupled Neural Networks in FPGAs for Real-Time Image Segmentation,IEEE,Conferences,"This paper describes the implementation of a system based on Pulse Coupled Neural Networks (PCNNs) and Field Programmable Gate Arrays (FPGAs). The PCNN implemented is oriented to the industrial application of segmentation in sequences of images. The work went through several real physical stages of implementation and optimization to achieve the needed performance. The greatest performance achieved by the digital system was of 250M pixels per second, enough to process a sequence of images in real time. Details of these stages about the neuron implementation with different Altera's FPGAs families are presented. Furthermore, the implementation is compared with previous implemented schemes based on floating point DSP microprocessor.",https://ieeexplore.ieee.org/document/1716657/,The 2006 IEEE International Joint Conference on Neural Network Proceedings,16-21 July 2006,ieeexplore
10.1109/CONECCT50063.2020.9198399,System and Methodology to Generate Dynamic Real-time Transparent and Bias-free Feature Modules used to Augment Model Learning Process,IEEE,Conferences,"Features are the backbone of any Machine Learning(ML) system. During the t<sup>th</sup> epoch of the model learning process, learnt features are the measurable outcomes of the model for a given input function. One of the biggest areas of research has been to try to impose even the slightest amount of control on what features a model learns. Achieving this in-turn poses various advantages, including re-usability of the features. Some of the state-of-the-art reusability techniques work by training the model weights and freeze at a random point k, on the source system and test the accuracy on the target until the ideal k point gives the highest possible accuracy. However, some of the current challenges remain in trying to control what is being sent to the target model and also in sending bias-free content. Thus, if we can aid this by adding a ""bias-free control-factor"" to the features being re-used in the model we would truly be able to perform many more tasks efficiently and achieve much higher accuracy along with lower consumption of resources required for training a model, thereby enabling ML towards many more industrial applications. We propose to provide a methodology by which we augment dynamic and bias-free feature modules for a model building process by taking an example in a Natural Language Processing(NLP) context.",https://ieeexplore.ieee.org/document/9198399/,"2020 IEEE International Conference on Electronics, Computing and Communication Technologies (CONECCT)",2-4 July 2020,ieeexplore
10.1109/CSR51186.2021.9527936,TRUSTY: A Solution for Threat Hunting Using Data Analysis in Critical Infrastructures,IEEE,Conferences,"The rise of the Industrial Internet of Things (IIoT) plays a crucial role in the era of hyper-connected digital economies. Despite the valuable benefits, such as increased resiliency, self-monitoring and pervasive control, IIoT raises severe cybersecurity and privacy risks, allowing cyberattackers to exploit a plethora of vulnerabilities and weaknesses that can lead to disastrous consequences. Although the Intrusion Detection and Prevention Systems (IDPS) constitute valuable solutions, they suffer from several gaps, such as zero-day attacks, unknown anomalies and false positives. Therefore, the presence of supporting mechanisms is necessary. To this end, honeypots can protect the real assets and trap the cyberattackers. In this paper, we provide a web-based platform called TRUSTY , which is capable of aggregating, storing and analysing the detection results of multiple industrial honeypots related to Modbus/Transmission Control Protocol (TCP), IEC 60870-5-104, BACnet, Message Queuing Telemetry Transport (MQTT) and EtherNet/IP. Based on this analysis, we provide a dataset related to honeypot security events. Moreover, this paper provides a Reinforcement Learning (RL) method, which decides about the number of honeypots that can be deployed in an industrial environment in a strategic way. In particular, this decision is converted into a Multi-Armed Bandit (MAB), which is solved with the Thompson Sampling (TS) method. The evaluation analysis demonstrates the efficiency of the proposed method.",https://ieeexplore.ieee.org/document/9527936/,2021 IEEE International Conference on Cyber Security and Resilience (CSR),26-28 July 2021,ieeexplore
10.1109/ISSRE.2014.9,Table of contents,IEEE,Conferences,The following topics are dealt with: software reliability; modeling; mixed reality application; program logic; fault localization; Big Data; safety-critical system; data analysis; industrial systems; software failure; software testing; and machine learning.,https://ieeexplore.ieee.org/document/6982343/,2014 IEEE 25th International Symposium on Software Reliability Engineering,3-6 Nov. 2014,ieeexplore
10.1109/ICSE-Companion.2019.00131,Testing Untestable Neural Machine Translation: An Industrial Case,IEEE,Conferences,"Neural Machine Translation (NMT) has shown great advantages and is becoming increasingly popular. However, in practice, NMT often produces unexpected translation failures in its translations. While reference-based black-box system testing has been a common practice for NMT quality assurance during development, an increasingly critical industrial practice, named in-vivo testing, exposes unseen types or instances of translation failures when real users are using a deployed industrial NMT system. To fill the gap of lacking test oracles for in-vivo testing of NMT systems, we propose a new methodology for automatically identifying translation failures without reference translations. Our evaluation conducted on real-world datasets shows that our methodology effectively detects several targeted types of translation failures. Our experiences on deploying our methodology in both production and development environments of WeChat (a messenger app with over one billion monthly active users) demonstrate high effectiveness of our methodology along with high industry impact.",https://ieeexplore.ieee.org/document/8802818/,2019 IEEE/ACM 41st International Conference on Software Engineering: Companion Proceedings (ICSE-Companion),25-31 May 2019,ieeexplore
10.1109/SNPDWinter52325.2021.00068,The Case Study on Use of Bigdata and AI in Distribution Industry,IEEE,Conferences,"With the development of telecommunication technology, a commerce platform centered on mobile devices is developing. Global mobile commerce companies such as Amazon are trying to recommend optimized products to customers and provide optimized logistics services through real-time big data. As online-oriented commerce develops, rival local offline stores are making changes for survival. In Korea, the government is trying to find a balance with offline stores by controlling big data-oriented online commerce, as is the regulation that sought to protect the local market by limiting the business hours of Super Supermarket (SSM) to balance local offline stores and Supermarket. However, in the data-driven fourth industrial revolution, we will have to find ways to develop by utilizing data, which is the main raw material. This paper tried to investigate cases of applying and operating big data in the distribution industry and seek ways to promote focused on big-data sharing that can develop with offline stores by conducting a Delphi survey to experts.",https://ieeexplore.ieee.org/document/9403518/,"2021 21st ACIS International Winter Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing (SNPD-Winter)",28-30 Jan. 2021,ieeexplore
10.1109/icABCD49160.2020.9183853,The Impact of Smart Manufacturing Approach On The South African Manufacturing Industry,IEEE,Conferences,"SM is a technology-driven approach that mainly utilises machines to monitor the entire production of an organisation. The objective of SM in an organisation is to identify ways to automatize the manufacturing process while using data analytics to optimize the manufacturing performance. This research mitigates the impact of technology, in this case expressed as SM in South African industries. The research followed a quantitative approach whereby 42 respondents from low, medium low and high technology industries took part in the study. Data has been amassed from first-hand experience by mean of an adapted questionnaire constituted of three sections: The first section was about the general demographic information of the respondents. Section two investigates the respondent's awareness on SM. Finally, section three assessed the impact that SM had on the performance of the organisation. The findings of this study revealed that Smart Manufacturing has a positive impact in South African manufacturing organisations as it allows effective operations, fast response to customers demand, real time operations optimisation. Nevertheless, Smart Manufacturing is a new concept under the fourth industrial revolution in South Africa and will need time before being totally implemented in all organisations as it is costly.",https://ieeexplore.ieee.org/document/9183853/,"2020 International Conference on Artificial Intelligence, Big Data, Computing and Data Communication Systems (icABCD)",6-7 Aug. 2020,ieeexplore
10.1109/AIMS52415.2021.9466046,The Implementation of Building Intelligent Smart Energy using LSTM Neural Network,IEEE,Conferences,"Internet of Things (IoT) makes many devices getting smarter and more connected in the 4.0 industrial revolution. One of the implementations of the Internet of Things is smart energy. It allows communication between humans or between things that make a building smarter. This paper proposes the implementation of the MQTT-based smart meter. The smart meter is used to make it easier for users to monitor and manage the energy consumption of buildings in real-time. It is considered as the main component of a smart network to make efficient and manage energy consumption remotely. Taking into account the increasing demand for electricity in Indonesia, smart meters can reduce overall energy use and reduce global warming by optimizing energy utilization through the internet of things and artificial intelligence. This paper proposes the implementation of the MQTT-based smart meter. This smart meter can measure energy consumption, transmit information related to the energy used, and provide an early warning system to stakeholders through the website in real-time analytics with predictive data on the following month and what days are most used to support energy consumption efficiency planning. This study conducted LTSM and ARIMA to determine forecasting energy consumption with 59 epochs, 8 batch sizes, 64 hidden layers with the results of MSE Error, RMSE Error, Mean Accuracy 0.14,0.373, and 95.16%, respectively. This result is better than ARIMA with MSE error results of 0.812 and 0.66 and RMSE error.",https://ieeexplore.ieee.org/document/9466046/,2021 International Conference on Artificial Intelligence and Mechatronics Systems (AIMS),28-30 April 2021,ieeexplore
10.1109/ICIPS.1997.669380,The design approach to DAI system eased on software engineering,IEEE,Conferences,"As more challenging applications are automated, cooperative problem solving will be an important paradigm for the next generation of intelligent industrial systems. A key problem with using it in the engineering domain is the development of a structured design method. The authors suggest a design approach for a distributed artificial intelligence (DAI) system based on software engineering, describe the detailed design process of a real DAI system through the example of a simulative transformer substation system (STSS), and present some key problems and techniques of DAI in the engineering domain, such as system modeling, task decomposition and allocation, cooperative mechanism, etc.",https://ieeexplore.ieee.org/document/669380/,1997 IEEE International Conference on Intelligent Processing Systems (Cat. No.97TH8335),28-31 Oct. 1997,ieeexplore
10.1109/IVS.1994.639471,The development of a fully autonomous ground vehicle (FAGV),IEEE,Conferences,"As a first step toward the creation of a fully autonomous vehicle that operates in a real world environment, we are currently developing a prototype autonomous ground vehicle (AGV) for use in factories and other industrial/business sites based on behavior-based artificial intelligence (AI) control. This flexible and fully autonomous AGV (FAGV) is expected to operate efficiently in a normal industrial environment without any external guidance. The crucial technique employed is a non-Cartesian way of organizing software agents for the creation of a highly responsive control program. The resulting software is considerably reduced in size. Through numerous experiments using mobile robots we confirmed that these new control programs excel in functionality, efficiency, flexibility and robustness. The second key technique in the planning stage is evolutionary computation, of which genetic algorithms are a principal technique. An online, real-time evolution of the control program will be incorporated in later phases of the project to make FAGVs adaptable to any given operational environment after deployment. The first prototype FAGV has an active vision and behaviour-based control system.",https://ieeexplore.ieee.org/document/639471/,Proceedings of the Intelligent Vehicles '94 Symposium,24-26 Oct. 1994,ieeexplore
10.1109/ICACITE51222.2021.9404738,The learning approaches using Augmented Reality in learning environments: Meta-Analysis,IEEE,Conferences,"With the emergence of Industrial Revolution 4.0, the educational settings are changing quickly. Augmented Reality (AR) is one of the upcoming technologies. AR enhances the real world by overlaying/augmenting the virtual/digital information over it. It provides the user with the ability to interact with the created virtual world in real space. The aim of this study is to classify the learning approaches implemented through AR technology. The technique used for the analysis is derived from systematic search of online literature databases like Taylor Francis, Web of Science, Springer, ScienceDirect and Scopus. The keywords used for the search include learning approaches, AR, AR in education, AR in learning and teaching and integration approaches. The findings of this research work highlights 4 categories of educational learning approaches that highlight AR. The approaches are experimental learning, game-based, interactive and collaborative learning. The research findings can be referred by other researchers and educators to identify the potential of AR in education and the learning approaches currently used with AR for their further research on how these approaches can be effectively and efficiently implemented in educational settings.",https://ieeexplore.ieee.org/document/9404738/,2021 International Conference on Advance Computing and Innovative Technologies in Engineering (ICACITE),4-5 March 2021,ieeexplore
10.1109/WSC.2017.8248046,The role of learning on industrial simulation design and analysis,IEEE,Conferences,"The capability of modeling real-world system operations has turned simulation into an indispensable problem-solving methodology for business system design and analysis. Today, simulation supports decisions ranging from sourcing to operations to finance, starting at the strategic level and proceeding towards tactical and operational levels of decision-making. In such a dynamic setting, the practice of simulation goes beyond being a static problem-solving exercise and requires integration with learning. This article discusses the role of learning in simulation design and analysis motivated by the needs of industrial problems and describes how selected tools of statistical learning can be utilized for this purpose.",https://ieeexplore.ieee.org/document/8248046/,2017 Winter Simulation Conference (WSC),3-6 Dec. 2017,ieeexplore
10.1109/WCICA.2010.5553990,The system for testing electrical performance of power generating sets based on technique of AC sampling,IEEE,Conferences,"A multilevel distributed system for testing electrical performance of power generating sets based on CAN bus is presented. Sampling and control for electrical parameters are realized utilizing Digital Signal Processor (DSP) and sixteen-bit Microcontroller Unit (MCU) that M16C/6N of renesas, an intelligent load based on the MCU of AT89S52 is designed, of which value can change automatically according to the need of testing, remote real-time supervision and integrated analysis for the electrical parameters are realized by Industrial Personal Computer (IPC) and configuration control software. A synchronous AC sampling algorithm based on Discrete Fourier Transform (DFT) is presented, and sampling precision is improved by tracing the frequency of signal under test using the method of “three points”. It indicates that the system is characterized by flexible configuration, high precision and high reliability in practical application.",https://ieeexplore.ieee.org/document/5553990/,2010 8th World Congress on Intelligent Control and Automation,7-9 July 2010,ieeexplore
10.1109/ISIE.2008.4677173,Three layer model for digital coal mine based on industrial ethernet,IEEE,Conferences,"In order to construct digital coal mine, a three layer model for digital coal mine model is proposed in this paper. Two basic platforms, the uniform transmission network platform and the uniform data warehouse platform, are discussed. A real 1000 M industrial Ethernet transmission platform based on Siemens PROFINET is established for Yangchangwan coal mine. The network platform is an information superhighway to integrate all existing and new automation subsystems and to provide standard interfaces for future subsystems. It established a uniform hardware and software platform in all aspects from network, data structure and management decision-making. Therefore the automation system island and information island problems in traditional mine automation systems are avoided effectively. It builds a stable foundation of digital coal mine for Yangchangwan coal mine.",https://ieeexplore.ieee.org/document/4677173/,2008 IEEE International Symposium on Industrial Electronics,30 June-2 July 2008,ieeexplore
10.1109/ICCD46524.2019.00019,TiLA: Twin-in-the-Loop Architecture for Cyber-Physical Production Systems,IEEE,Conferences,"Digital twin is a virtual replica of a real-world object that lives simultaneously with its physical counterpart. Since its first introduction in 2003 by Grieves, digital twin has gained momentum in a wide range of applications such as industrial manufacturing, automotive and artificial intelligence. However, many digital-twin-related approaches, found in industries as well as literature, mainly focus on modelling individual physical things with high-fidelity methods with limited scalability. In this paper, we introduce a digital-twin architecture called TiLA (Twin-in-the-Loop Architecture). TiLA employs heterogeneous models and online data to create a digital twin, which follows a Globally Asynchronous Locally Synchronous (GALS) model of computation. It facilitates the creation of a scalable digital twin with different levels of modelling abstraction as well as giving GALS formalism for execution strategy. Furthermore, TiLA provides facilities to develop applications around the twin as well as an interface to synchronise the twin with the physical system through an industrial communication protocol. A digital twin for a manufacturing line has been developed as a case study using TiLA. It demonstrates the use of digital twin models together with online data for monitoring and analysing failures in the physical system.",https://ieeexplore.ieee.org/document/8988620/,2019 IEEE 37th International Conference on Computer Design (ICCD),17-20 Nov. 2019,ieeexplore
10.1109/RTAS48715.2020.000-1,Timing of Autonomous Driving Software: Problem Analysis and Prospects for Future Solutions,IEEE,Conferences,"The software used to implement advanced functionalities in critical domains (e.g. autonomous operation) impairs software timing. This is not only due to the complexity of the underlying high-performance hardware deployed to provide the required levels of computing performance, but also due to the complexity, non-deterministic nature, and huge input space of the artificial intelligence (AI) algorithms used. In this paper, we focus on Apollo, an industrial-quality Autonomous Driving (AD) software framework: we statistically characterize its observed execution time variability and reason on the sources behind it. We discuss the main challenges and limitations in finding a satisfactory software timing analysis solution for Apollo and also show the main traits for the acceptability of statistical timing analysis techniques as a feasible path. While providing a consolidated solution for the software timing analysis of Apollo is a huge effort far beyond the scope of a single research paper, our work aims to set the basis for future and more elaborated techniques for the timing analysis of AD software.",https://ieeexplore.ieee.org/document/9113112/,2020 IEEE Real-Time and Embedded Technology and Applications Symposium (RTAS),21-24 April 2020,ieeexplore
10.1109/IECON.2016.7793038,Tool compensation in walk-through programming for admittance-controlled robots,IEEE,Conferences,"This paper describes a walk-through programming technique, based on admittance control and tool dynamics compensation, to ease and simplify the process of trajectory learning in common industrial setups. In the walk-through programming, the human operator grabs the tool attached at the robot end-effector and “walks” the robot through the desired positions. During the teaching phase, the robot records the positions and then it will be able to interpolate them to reproduce the trajectory back. In the proposed control architecture, the admittance control allows to provide a compliant behavior during the interaction between the human operator and the robot end-effector, while the algorithm of compensation of the tool dynamics allows to directly use the real tool in the teaching phase. In this way, the setup used for the teaching can directly be the one used for performing the reproduction task. Experiments have been performed to validate the proposed control architecture and a pick and place example has been implemented to show a possible application in the industrial field.",https://ieeexplore.ieee.org/document/7793038/,IECON 2016 - 42nd Annual Conference of the IEEE Industrial Electronics Society,23-26 Oct. 2016,ieeexplore
10.23919/SOFTCOM.2019.8903866,Toward a Smart Real Time Monitoring System for Drinking Water Based on Machine Learning,IEEE,Conferences,"Drinking-water distribution systems facilitate to carry portable water from water resources such as reservoirs, river, and water tanks to industrial, commercial and residential consumers through complex pipe networks. This system may be affected by acts of pollution that may be intentional or accidental. Hence, it's necessary to prevent any intrusion into water distribution systems and to detect pollution as soon as possible. Therefore, water monitoring is required to maintain a good water quality for human and animal life. In this paper we intend to control the quality of the drinking-water using wireless sensor networks. First, we start with a detailed architecture of our smart system. This architecture uses a new generation of wireless sensors to detect the chemical, physical and microbiological water parameters. After, the water quality limits according to the Tunisian standard will exposed. Then we develop a new detection model of water anomalies. Our model is based on machine learning to detect anomalies and malicious acts in real time. In our solution a data aggregation method is created to minimize the amount of data and reduce the processing time.",https://ieeexplore.ieee.org/document/8903866/,"2019 International Conference on Software, Telecommunications and Computer Networks (SoftCOM)",19-21 Sept. 2019,ieeexplore
10.1109/CoASE.2014.6899348,Toward safe close-proximity human-robot interaction with standard industrial robots,IEEE,Conferences,"Allowing humans and robots to interact in close proximity to each other has great potential for increasing the effectiveness of human-robot teams across a large variety of domains. However, as we move toward enabling humans and robots to interact at ever-decreasing distances of separation, effective safety technologies must also be developed. While new, inherently human-safe robot designs have been established, millions of industrial robots are already deployed worldwide, which makes it attractive to develop technologies that can turn these standard industrial robots into human-safe platforms. In this work, we present a real-time safety system capable of allowing safe human-robot interaction at very low distances of separation, without the need for robot hardware modification or replacement. By leveraging known robot joint angle values and accurate measurements of human positioning in the workspace, we can achieve precise robot speed adjustment by utilizing real-time measurements of separation distance. This, in turn, allows for collision prevention in a manner comfortable for the human user.We demonstrate our system achieves latencies below 9.64 ms with 95% probability, 11.10 ms with 99% probability, and 14.08 ms with 99.99% probability, resulting in robust real-time performance.",https://ieeexplore.ieee.org/document/6899348/,2014 IEEE International Conference on Automation Science and Engineering (CASE),18-22 Aug. 2014,ieeexplore
10.1109/ICSE43902.2021.00027,Towards Automating Code Review Activities,IEEE,Conferences,"Code reviews are popular in both industrial and open source projects. The benefits of code reviews are widely recognized and include better code quality and lower likelihood of introducing bugs. However, since code review is a manual activity it comes at the cost of spending developers' time on reviewing their teammates' code. Our goal is to make the first step towards partially automating the code review process, thus, possibly reducing the manual costs associated with it. We focus on both the <i>contributor</i> and the <i>reviewer</i> sides of the process, by training two different Deep Learning architectures. The first one learns code changes performed by developers during real code review activities, thus providing the <i>contributor</i> with a revised version of her code implementing code transformations usually recommended during code review before the code is even submitted for review. The second one automatically provides the <i>reviewer</i> commenting on a submitted code with the revised code implementing her comments expressed in natural language. The empirical evaluation of the two models shows that, on the contributor side, the trained model succeeds in replicating the code transformations applied during code reviews in up to 16% of cases. On the reviewer side, the model can correctly implement a comment provided in natural language in up to 31% of cases. While these results are encouraging, more research is needed to make these models usable by developers.",https://ieeexplore.ieee.org/document/9402025/,2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE),22-30 May 2021,ieeexplore
10.1145/3377811.3380368,Towards Characterizing Adversarial Defects of Deep Learning Software from the Lens of Uncertainty,IEEE,Conferences,"Over the past decade, deep learning (DL) has been successfully applied to many industrial domain-specific tasks. However, the current state-of-the-art DL software still suffers from quality issues, which raises great concern especially in the context of safety- and security-critical scenarios. Adversarial examples (AEs) represent a typical and important type of defects needed to be urgently addressed, on which a DL software makes incorrect decisions. Such defects occur through either intentional attack or physical-world noise perceived by input sensors, potentially hindering further industry deployment. The intrinsic uncertainty nature of deep learning decisions can be a fundamental reason for its incorrect behavior. Although some testing, adversarial attack and defense techniques have been recently proposed, it still lacks a systematic study to uncover the relationship between AEs and DL uncertainty. In this paper, we conduct a large-scale study towards bridging this gap. We first investigate the capability of multiple uncertainty metrics in differentiating benign examples (BEs) and AEs, which enables to characterize the uncertainty patterns of input data. Then, we identify and categorize the uncertainty patterns of BEs and AEs, and find that while BEs and AEs generated by existing methods do follow common uncertainty patterns, some other uncertainty patterns are largely missed. Based on this, we propose an automated testing technique to generate multiple types of uncommon AEs and BEs that are largely missed by existing techniques. Our further evaluation reveals that the uncommon data generated by ourmethod is hard to be defended by the existing defense techniques with the average defense success rate reduced by 35%. Our results call for attention and necessity to generate more diverse data for evaluating quality assurance solutions of DL software.",https://ieeexplore.ieee.org/document/9284139/,2020 IEEE/ACM 42nd International Conference on Software Engineering (ICSE),5-11 Oct. 2020,ieeexplore
10.1109/AIKE.2018.00037,"Towards Low-Cost, Real-Time, Distributed Signal and Data Processing for Artificial Intelligence Applications at Edges of Large Industrial and Internet Networks",IEEE,Conferences,"Digital Signal Processors (DSP) are vital system components in industrial Artificial Intelligence (AI) applications. In this paper, FIR filters that could be used for industrial AI applications are designed from the Spline Biorthogonal 1.5 (SB1.5) mother wavelet using a real-time, low-cost, generic industrial IoT (IIoT) hardware: the C28x DSP and low-level, Embedded C, system software. Our contribution in this paper is the first reported application of the C28x for SB1.5 wavelet construction. The significance of this approach is to be able to repurpose low-cost, readily available hardware for distributed AI applications. Our approach is different from the state of the art, in which specialized hardware are always manufactured for implementing AI applications at large network edges. Our approach supports low-cost and fast single-stage FIR implementation suitable for use in real-time, distributed AI application at network edges, since in our case, successive recursion of FIR filters leading to a full implementation of Pyramid Algorithm is not implemented. The designed FIR filter is evaluated and found capable of both low-pass and high pass filtering operations. Results of this paper indicate that the C28x real-time DSP, which exists in many IoT devices, could have improved scalability by being deployed for other important AI and IoT network edge analytic applications, different from its present uses.",https://ieeexplore.ieee.org/document/8527469/,2018 IEEE First International Conference on Artificial Intelligence and Knowledge Engineering (AIKE),26-28 Sept. 2018,ieeexplore
10.1109/SP.2019.00001,Towards Practical Differentially Private Convex Optimization,IEEE,Conferences,"Building useful predictive models often involves learning from sensitive data. Training models with differential privacy can guarantee the privacy of such sensitive data. For convex optimization tasks, several differentially private algorithms are known, but none has yet been deployed in practice. In this work, we make two major contributions towards practical differentially private convex optimization. First, we present Approximate Minima Perturbation, a novel algorithm that can leverage any off-the-shelf optimizer. We show that it can be employed without any hyperparameter tuning, thus making it an attractive technique for practical deployment. Second, we perform an extensive empirical evaluation of the state-of-the-art algorithms for differentially private convex optimization, on a range of publicly available benchmark datasets, and real-world datasets obtained through an industrial collaboration. We release open-source implementations of all the differentially private convex optimization algorithms considered, and benchmarks on as many as nine public datasets, four of which are high-dimensional.",https://ieeexplore.ieee.org/document/8835258/,2019 IEEE Symposium on Security and Privacy (SP),19-23 May 2019,ieeexplore
10.1109/IECON48115.2021.9589594,Towards Sustainable Models of Computation for Artificial Intelligence in Cyber-Physical Systems,IEEE,Conferences,"This paper confronts with a reflection about a deep problem in computational models for cyber-physical systems (CPS). The problem arises in the contact between digital computing and the physical realm, and affects heavily the design, modeling, and implementation of CPS. Problems are exacerbated by the introduction of artificial intelligence and autonomy in industrial applications that have to meet sustainability of solutions, both in technical and societal sense. After a brief review, a new perspective and position on the future of sustainable CPS is addressed, and a pragmatic research path is presented. The RMAS (Relational-model Multi-Agent System) architecture is proposed as a test framework for the deep integration of real-world semantics into the advancements brought about by the digital transformation wave.",https://ieeexplore.ieee.org/document/9589594/,IECON 2021 – 47th Annual Conference of the IEEE Industrial Electronics Society,13-16 Oct. 2021,ieeexplore
10.1109/IS.2006.348456,Towards an Agent-Based Framework for Online After-Sale Services,IEEE,Conferences,"The multi-agent paradigm for building intelligent systems has gradually been accepted by researchers and practitioners in the research field of artificial intelligence. There are also attempts of adapting agents and agent-based systems for creating industrial applications and providing e-services. In this paper, we present an attempt to use agents for constructing an online after-sale services system. The system is decomposed into four major cooperative agents, and in which each agent concentrates on particular aspects in the system and expresses intelligence by using various techniques. The proposed agent-based framework for the system is presented at both the micro-level and the macro-level according to the Gala methodology. UML notations are also used to represent some software design models. As the result of this, agents are implemented into a framework for which exploits case-based reasoning (CBR) technique to fulfil real life on-line services' diagnoses and tasks",https://ieeexplore.ieee.org/document/4155463/,2006 3rd International IEEE Conference Intelligent Systems,4-6 Sept. 2006,ieeexplore
10.23919/SMAGRIMET.2018.8369850,Towards statistical machine learning for edge analytics in large scale networks: Real-time Gaussian function generation with generic DSP,IEEE,Conferences,"The smart grid (SG) is a large-scale network and it is an integral part of the Internet of Things (IoT). For a more effective big data analytic in large-scale IoT networks, reliable solutions are being designed such that many real-time decisions will be taken at the edge of the network close to where data is being generated. Gaussian functions are extensively applied in the field of statistical machine learning, pattern recognition, adaptive algorithms for function approximation, etc. It is envisaged that soon, some of these machine learning solutions and other gaussian function based applications that have low computation and low-memory footprint will be deployed for edge analytics in large-scale IoT networks. Hence, it will be of immense benefit if an adaptive, low-cost, method of designing gaussian functions becomes available. In this paper, gaussian distribution functions are designed using C28x real-time digital signal processor (DSP) that is embedded in the TMS320C2000 modem designed for powerline communication (PLC) at the low voltage distribution end of the smart grid, where numerous devices that generate massive amount of data exist. Open-source embedded C programming language is used to program the C28x for real-time gaussian function generation. The designed gaussian waveforms are stored in lookup tables (LUTs) in the C28x embedded DSP, and could be deployed for a variety of applications at the edge of the SG and IoT network. The novelty of the design is that the gaussian functions are designed with a generic, low-cost, fixed-point DSP, different from state of the art in which gaussian functions are designed using expensive arbitrary waveform generators and other specialized circuits. C28x DSP is selected for this design since it is already existing as an embedded DSP in many smart grid applications and in other numerous industrial systems that are part of the large scale IoT network, hence it is envisaged that integration of any gaussian function based solution using this DSP in the smart grid and other IoT systems may not be too challenging.",https://ieeexplore.ieee.org/document/8369850/,2018 First International Colloquium on Smart Grid Metrology (SmaGriMet),24-27 April 2018,ieeexplore
10.1109/COASE.2018.8560470,Training CNNs from Synthetic Data for Part Handling in Industrial Environments,IEEE,Conferences,"As Convolutional Neural Network based models become reliable and efficient, two questions arise in relation to their applications for industrial purposes. The usefulness of these models in industrial environments and their implementation in these settings. This paper describes the autonomous generation of Region based CNN models trained on images from rendered CAD models and examines their applicability and performance for part handling application. The development of the automated synthetic data generation is detailed and two CNN models are trained with the aim to detect a car component and differentiate it against another similar looking part. The performance of these models is tested on real images and it was found that the proposed approach can be easily adopted for detecting a range of parts in arbitrary backgrounds. Moreover, the use of syntheic images for training CNNs automates the process of generating a detector.",https://ieeexplore.ieee.org/document/8560470/,2018 IEEE 14th International Conference on Automation Science and Engineering (CASE),20-24 Aug. 2018,ieeexplore
10.1109/ISCAS.1992.230637,Trajectory tracking using neural networks,IEEE,Conferences,"Presents a method for tracking prespecified trajectories in industrial drive systems with a multilayered feedforward neural network. The method utilizes the backpropagation technique to learn feedback-error ranges, in which appropriate control actions can be generated from a lookup table. It can follow arbitrarily prescribed trajectories even when they are not present in the training phase. This approach is simple and practical for real-time implementation. Examples are included to demonstrate the effectiveness. The analogy between this scheme and a fuzzy logic control strategy is also investigated.&lt;<ETX>&gt;</ETX>",https://ieeexplore.ieee.org/document/230637/,[Proceedings] 1992 IEEE International Symposium on Circuits and Systems,10-13 May 1992,ieeexplore
10.1109/SERVICES51467.2021.00055,Transformation: Case Studies and Lessons Learned : Keynote 2,IEEE,Conferences,"Industrial AI, Big Data Analytics, Machine Learning, and Cyber Physical Systems are changing the way we design product, manufacturing, and service systems. It is clear that as more sensors and smart analytics software are integrated in the networked industrial products and manufacturing systems, predictive technologies can further learn and autonomously optimize service productivity and performance. This presentation will address the trends of Industrial AI for smart service realization. First, Industrial AI systematic approach will be introduced. Case studies on advanced predictive analytics technologies for different maintenance and service operations will be demonstrated. In addition, issues on data quality for high performance and real-time data analytics in future digital service will be discussed.",https://ieeexplore.ieee.org/document/9604414/,2021 IEEE World Congress on Services (SERVICES),5-10 Sept. 2021,ieeexplore
10.1109/ICCAIRO.2017.61,Tuning Software Based on Genetic Algorithm Applied to Industrial PID Loops,IEEE,Conferences,"Time-delay processes are frequently found in industry and the most common representations are first order plus delay time (FOPDT) and integrator plus delay time (IPDT) transfer functions. The identification of time delay systems is a challenging task and usually, the performance of its control is not optimal. This work presents a software based on a real coded genetic algorithm to identify the system, using open or close loop information, and to tune the PID controller using several methods. Results on simulation and real industrial loops are presented.",https://ieeexplore.ieee.org/document/8253004/,"2017 International Conference on Control, Artificial Intelligence, Robotics & Optimization (ICCAIRO)",20-22 May 2017,ieeexplore
10.1109/ICAIIC48513.2020.9065203,UAV-assisted Real-time Data Processing using Deep Q-Network for Industrial Internet of Things,IEEE,Conferences,"Industrial internet of things (IIoT) enables edge computing technology to provide communication between the machines that produce a large amount of data and locate at the edge network. A task scheduling is implemented in the edge node. Furthermore, the real-time data can achieve with the lowest latency that allowed by the edge node near the edge network. However, a mobile machine such as an autonomous guided vehicle can interfere in this situation. Because the vehicle also needs service by the edge node. Over that, quality of service (QoS) performance can decrease. Therefore, this paper deploys an unmanned aerial vehicle (UAV) as an edge node to provide service to the edge network through optimizing the trajectory of UAV, where the edge network request task using a Deep Q-Network (DQN) Learning. The result shows that using machine learning, notably the DQN algorithm, can increase the number of the machine that can be provided service. Subsequently, the real-time data can achieve either the interrupt occurs at the edge node.",https://ieeexplore.ieee.org/document/9065203/,2020 International Conference on Artificial Intelligence in Information and Communication (ICAIIC),19-21 Feb. 2020,ieeexplore
10.1109/ICASSP39728.2021.9414882,Unsupervised Clustering of Time Series Signals Using Neuromorphic Energy-Efficient Temporal Neural Networks,IEEE,Conferences,"Unsupervised time series clustering is a challenging problem with diverse industrial applications such as anomaly detection, bio-wearables, etc. These applications typically involve small, low-power devices on the edge that collect and process real-time sensory signals. State-of-the-art time-series clustering methods perform some form of loss minimization that is extremely computationally intensive from the perspective of edge devices. In this work, we propose a neuromorphic approach to unsupervised time series clustering based on Temporal Neural Networks that is capable of ultra low-power, continuous online learning. We demonstrate its clustering performance on a subset of UCR Time Series Archive datasets. Our results show that the proposed approach either outperforms or performs similarly to most of the existing algorithms while being far more amenable for efficient hardware implementation. Our hardware assessment analysis shows that in 7 nm CMOS the proposed architecture, on average, consumes only about 0.005 mm<sup>2</sup> die area and 22 μW power and can process each signal with about 5 ns latency.",https://ieeexplore.ieee.org/document/9414882/,"ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",6-11 June 2021,ieeexplore
10.1109/RoEduNet51892.2020.9324883,Usage of Asymetric Small Binning to Compute Histogram of Oriented Gradients for Edge Computing Image Sensors,IEEE,Conferences,"In case of multiple imaging sensors used in different networks (home security, surveillance, automotive, industrial), there is a challenge to perform object detection algorithms in real time, even on the cloud, for a large number of sensors. This is why there is an intensive effort in the industry to move object detection processing on the edge, with the benefits of reducing the bandwidth needs and allowing for scalability in large networks. In this paper we present a hardware friendly optimization technique to compute Histogram of Oriented Gradients (HOG) on the edge, by approximating the HOG orientation as a multitude of small bins. The technique is implemented in RTL for FPGA or ASIC and serves as the first step in a standard object detection algorithm (using Histogram of Oriented Gradients as feature extractor and Support Vector Machine as the detection algorithm). We verified the results of the proposed optimizations for errors by comparison to a reference method and the overall object detection algorithm for robustness.",https://ieeexplore.ieee.org/document/9324883/,2020 19th RoEduNet Conference: Networking in Education and Research (RoEduNet),11-12 Dec. 2020,ieeexplore
10.1109/QUATIC.2016.011,Use of Ontologies in Embedded Systems: A Systematic Mapping,IEEE,Conferences,"Many domains of Embedded Systems (e.g., automotive, avionics, consumer electronics, industrial automation, medical...) are rapidly evolving toward solutions that integrate hardware and software, or incorporate complete systems in a single chip. The specificities of the concepts have a complexity on data analysis and during the identification of relationship between them. The usage of an ontology for embedded systems seems to be necessary. Indeed, ontology is an important engineering artifact used in several domains, that provides uniformity to the concepts in terms of syntax and semantics, facilitates the communication in various fields and improves the understanding of requirements. This work conducted a systematic mapping, to collect evidence about the need of ontologies in embedded systems development. We focus on the identification of ontology that represents languages used for the development of embedded systems, considering the various domains of these, as well as the benefits of using an ontology for this purpose. After applying selection criteria in the mapping driving phases, 19 papers were selected and analyzed. This mapping provides evidences of real benefits in using ontologies for embedded systems.",https://ieeexplore.ieee.org/document/7814508/,2016 10th International Conference on the Quality of Information and Communications Technology (QUATIC),6-9 Sept. 2016,ieeexplore
10.1109/CSCI.2015.116,User-Centric Workflow Ergonomics in Industrial Environments: Concept and Architecture of an Assistance System,IEEE,Conferences,"Changes in demographic developments come along with an ageing workforce and a higher retirement age. Particularly in the industrial working environment, poor workplace ergonomics can limit the workers' quality of health and thus their ability to work until they reach their statutory retirement age. This paper presents the conceptual design of an assistance system that captures information from workers' bearings based on on-body sensors. A real-time analysis of the captured sensor data enables giving feedback to workers at the assembly site as soon as they get into an ergonomic unhealthy position during workflow execution. Workflow managers benefit from a holistic evaluation of the captured ergonomic data. By this means, critical workflow activities that are characterized by a high degree of malpositions can be identified. Based on this information, workflow managers have the possibility to optimize workflows in an ergonomic-friendly way. Furthermore, the presented approach enables a differentiated evaluation of established methods of ergonomic feedback like OWAS or EAWS.",https://ieeexplore.ieee.org/document/7424190/,2015 International Conference on Computational Science and Computational Intelligence (CSCI),7-9 Dec. 2015,ieeexplore
10.1109/CONMEDIA46929.2019.8981830,Using Naïve Bayes Classifier for Application Feedback Classification and Management in Bahasa Indonesia,IEEE,Conferences,"The world keeps moving, software products too. An application's objectives, structures, requirements, and assumptions that have been elicited and analyzed previously may need to be reassessed and updated. In order to fully understand these requirements evolutions, what changes are necessary, and why those changes are needed, one essential source of requirements is user feedback. However, handling and analyzing so many user feedbacks can be time-consuming. Using natural language processing tools for Bahasa Indonesia and Naive Bayes classifier, this research aims to develop a tool to process natural language and classify user feedbacks. The developed tool is expected to make feedback classification less time-consuming so that developers can project their energy to more productive and creative works. The machine learning models are built using the feedback dataset taken from an up-and-running university e-learning system and show promising results. The highest confusion matrix scores are 92.5% for accuracy, 85.6% precision, 85.1% recall, and lastly, 85.4% for the F-measure score. The resulting web application for feedback management is then evaluated to the users, and even though it still needs to be further polished and improved for real industrial use, it is perceived to be useful and easy to use.",https://ieeexplore.ieee.org/document/8981830/,2019 5th International Conference on New Media Studies (CONMEDIA),9-11 Oct. 2019,ieeexplore
10.1049/cp:19940284,Using expert systems for on-line data qualification and state variable estimation for an industrial fermentation process,IET,Conferences,"An industrial fed batch fermentation process is a nonlinear time-varying process. Important internal state variables such as biomass, substrate and product concentrations cannot be measured online and are usually determined by infrequent and time consuming off-line laboratory analysis. The online measurements are usually noisy and sometimes this leads to misinterpretation of the real situation inside the fermenter. These problems can lead to poor control of the batch and low productivity subsequently. To overcome these problems a real time expert system has been proposed which is based on the Poplog Flex real time expert system shell. The system is used to monitor the state variables of the process, diagnose any fault that might occur in the process, estimate the important unmeasurable state variables and to design a controller to control the state around a desired level. A neural network has been adopted for the online estimation of the unmeasurable state variables. Pattern recognition ideas have been used to improve the modelling ability of the neural network. Predictive control techniques have been used to control the state around a desired level. The model and the controller for the process have been designed and implemented within the Poplog Flex environment.&lt;<ETX>&gt;</ETX>",https://ieeexplore.ieee.org/document/327321/,1994 International Conference on Control - Control '94.,21-24 March 1994,ieeexplore
10.1109/DCOSS.2019.00126,Virtual Light Sensors in Industrial Environment Based on Machine Learning Algorithms,IEEE,Conferences,"Internet of Things (IoT) has become the backbone of current and future emerging applications both in the public and the private, industrial sector. The IoT paradigm, enhanced with intelligence and big data analytics, has found applications in a wide range of solutions such as smart home, smart city, industrial IoT etc. Even though IoT implies that cheap motes can conduct a specific task, thus a large number of them can be deployed, we aim to minimize the installed hardware while we still have a high level of quality of service. Machine Learning algorithms can support this challenge by generating virtual data via utilization of real data from a smaller subset of sensors. The generated data can replicate sensor behavior which would otherwise be difficult or impossible to track. It is also possible to use simulation models for data analysis model validation, by generating new data under varying conditions. In this paper, we propose a concept of an IoT testbed which allows virtual IoT resources to be immersed and tested in real life conditions, which are met in everyday life. Additionally, the features of the implemented testbed prototype are discussed while taking into account specific use cases, regarding luminosity scenarios in industrial environments.",https://ieeexplore.ieee.org/document/8804746/,2019 15th International Conference on Distributed Computing in Sensor Systems (DCOSS),29-31 May 2019,ieeexplore
10.1109/ITHET.2012.6246058,Virtual industrial training: Joining innovative interfaces with plant modeling,IEEE,Conferences,"Training in industry is one of the most critical and expensive tasks to be faced by the management. Furthermore, in some cases, it is dangerous or even impossible to directly train operators on the real plants where security and safety problems may arise, making it very difficult to start training programs at low cost. For these reasons, the field of training in industry is rapidly developing using software or hardware solutions coming mainly from the following research areas: i) Human-Computer interaction, i.e., the use of complex and interactive human-machine interfaces, ii) plant simulators, i.e., software systems which are delivered with the plant itself to test and to learn complex tasks and processes, iii) Intelligent Training Systems, i.e., the availability of intelligent and personalized training systems where a virtual tutor guides users through a personalized learning path. In this paper we present the overall architecture of a system for industrial training, embedded into an Intelligent Tutoring System that can provide more effective and personalized training and learning in a context where working directly on real plants can be difficult and very expensive. In particular we present a simulator for training operators in using power plants, based on a multimedia and on interactive interface. This system is particularly suitable to be used for training in industrial electric and oil plants. Moreover, the system allows operators for collaborative problem solving. Currently the system is under delivery to an Italian Electric industry.",https://ieeexplore.ieee.org/document/6246058/,2012 International Conference on Information Technology Based Higher Education and Training (ITHET),21-23 June 2012,ieeexplore
10.1109/ICSTCC.2019.8885611,Visual Analytics Framework for Condition Monitoring in Cyber-Physical Systems,IEEE,Conferences,"One of the biggest challenges facing the factory of the future today is to reduce the time-to-market access and increase through the improvement of competitiveness and efficiency. In order to achieve this target, data analytics in Industrial Cyber-Physical System becomes a feasible option. In this paper, a visual analytics framework for condition monitoring of the machine tool is presented with the aim to manage events and alarms at factory level. The framework is assessed in a particular use case that consists in a multi-threaded cloud-based solution for the global analysis of the behaviour of variables acquired from PLC, CNC and robot manipulator. A human-machine interface is also designed for the real-time visualization of the key performance indicators according to the user's criteria. This tool implemented is a great solution for condition monitoring and decision-making process based on data analytics from simple statistics to complex machine learning methods. The results achieved are part of the vision and implementation of the industrial test bed of “Industry and Society 5.0” platform.",https://ieeexplore.ieee.org/document/8885611/,"2019 23rd International Conference on System Theory, Control and Computing (ICSTCC)",9-11 Oct. 2019,ieeexplore
10.1109/CLEOE-EQEC.2019.8872523,Wavelength Independent Image Classification through a Multimode Fiber using Deep Neural Networks,IEEE,Conferences,"Deep Neural Networks (DNNs) have been increasingly implemented in different research fields or industrial applications. Large amounts of data are processed daily in order to extract useful information using machine learning techniques. Many research groups have shown impressive results on improving resolution in microscopy and quantitative phase retrieval by training DNNs on real datasets [1,2]. Recently, recovery and reconstruction of images after they have propagated through multimode optical fibers (MMFs) have also been achieved using DNNs [3,4]. When images propagate through MMFs they suffer severe scrambling because the information gets distributed among the different spatial modes that the fiber supports. Furthermore, since the fiber modes propagate with different velocities, the local information of the input decorrelates after a few millimeters along the MMF, thus resulting in the formation of a speckle pattern at the output. Recovery of information from such speckle patterns is of practical interest for integrating the MMFs for endoscopic applications in medicine or for signal recovery in telecommunications.",https://ieeexplore.ieee.org/document/8872523/,2019 Conference on Lasers and Electro-Optics Europe & European Quantum Electronics Conference (CLEO/Europe-EQEC),23-27 June 2019,ieeexplore
10.1109/CASE48305.2020.9216781,Weak Scratch Detection of Optical Components Using Attention Fusion Network,IEEE,Conferences,"Scratches on the optical surface can directly affect the reliability of the optical system. Machine vision-based methods have been widely applied in various industrial surface defect inspection scenarios. Since weak scratches imaging in the dark field has an ambiguous edge and low contrast, which brings difficulty in automatic defect detection. Recently, many existing visual inspection methods based on deep learning cannot effectively inspect weak scratches due to the lack of attention-aware features. To address the problems arising from industry-specific characteristics, this paper proposes “Attention Fusion Network;”, a convolutional neural network using attention mechanism built by hard and soft attention modules to generate attention-aware features. The hard attention module is implemented by integrating the brightness adjustment operation in the network, and the soft attention module is composed of scale attention and channel attention. The proposed model is trained on a real-world industrial scratch dataset and compared with other defect inspection methods. The proposed method can achieve the best performance to detect the weak scratch inspection of optical components compared to the traditional scratch detection methods and other deep learning-based methods.",https://ieeexplore.ieee.org/document/9216781/,2020 IEEE 16th International Conference on Automation Science and Engineering (CASE),20-21 Aug. 2020,ieeexplore
10.1109/ECC.2015.7330620,Web tension regulation with partially known periodic disturbances in roll-to-roll manufacturing systems,IEEE,Conferences,"This paper deals with the problem of synthesizing feedforward control to aid the regulation of web tension in the presence of partially known exogenous inputs. The problem appears in many engineering applications including Roll-to-Roll (R2R) manufacturing systems where the governing equation for tension is nonlinear. Currently known methods for the nonlinear output regulation problem either require the solution of a constrained partial differential equation or the preview information of the signal to be tracked. In this paper, we consider the problem of regulating web tension while rejecting periodic disturbances and use a novel approach to synthesize feedforward control as the solution of a system of differential-algebraic equations, which is considerably less complex and suitable for practical implementation. We assume that the disturbance signal is the output of a known exogenous system with unknown initial conditions. A parameter identification scheme to estimate the unknown initial conditions is developed. The proposed technique is successfully applied to web tension regulation in a large R2R machine which contains real-time hardware and software that is used in industrial practice. Extensive experiments were conducted to evaluate the proposed scheme under various experimental conditions, including different web speeds and materials. We will discuss a representative sample of the results with the proposed nonlinear tension regulator and provide a comparison with a well-tuned industrial PI control scheme to highlight the benefits of using the proposed scheme.",https://ieeexplore.ieee.org/document/7330620/,2015 European Control Conference (ECC),15-17 July 2015,ieeexplore
10.1109/CCA.2000.897405,Winner take all experts network for sensor validation,IEEE,Conferences,"The validation of sensor measurements has become an integral part of the operation and control of modern industrial equipment. The sensor under a harsh environment must be shown to consistently provide the correct measurements. Analysis of the validation hardware or software should trigger an alarm when the sensor signals deviate appreciably from the correct values. Neural network based models can be used to estimate critical sensor values when neighboring sensor measurements are used as inputs. The discrepancy between the measured and predicted sensor values may then be used as an indicator for sensor health. The proposed winner take all experts (WTAE) network is based on a 'divide and conquer' strategy. It employs a growing fuzzy clustering algorithm to divide a complicated problem into a series of simpler sub-problems and assigns an expert to each of them locally. After the sensor approximation, the outputs from the estimator and the real sensor value are compared both in the time domain and the frequency domain. Three fault indicators are used to provide analytical redundancy to detect the sensor failure. In the decision stage, the intersection of three fuzzy sets accomplishes a decision level fusion, which indicates the confidence level of the sensor health. Two data sets, the Spectra Quest Machinery Fault Simulator data set and the Westland vibration data set, were used in simulations to demonstrate the performance of the proposed WTAE network. The simulation results show the proposed WTAE is competitive with or even superior to the existing approaches.",https://ieeexplore.ieee.org/document/897405/,Proceedings of the 2000. IEEE International Conference on Control Applications. Conference Proceedings (Cat. No.00CH37162),27-27 Sept. 2000,ieeexplore
10.1109/IECON.2012.6389251,ZnO crystalline nanowires array for application in gas ionization sensor,IEEE,Conferences,"The air monitoring becomes daily necessity not only in industrial environment and in aerospace applications but also in a living milieu as a consequence of the gas pollution. For detection of gaseous pollutants gas sensors are employed. In this work the successful p-type and n-type ZnO nanowires (NWs) were accomplished during electrochemical deposition. P-type ZnO NWs doped with Ag dopant were achieved with omitted post annealing procedure. Also, the novel gas ionization sensor (GIS) with integrated p-type ZnO NWs as the anode is proposed. P-type ZnO NWs-based gas sensor's characteristics compared with the gold NWs-based GIS, which was developed and reported by our group previously. It showed the comparable breakdown voltages in inert gas (Ar) atmosphere. P-type ZnO NWs-based GIS demonstrated good repeatability. The practical and low cost p-type ZnO NWs-based gas sensor presented in this article shows potential for future implementation in real world gas sensors' applications.",https://ieeexplore.ieee.org/document/6389251/,IECON 2012 - 38th Annual Conference on IEEE Industrial Electronics Society,25-28 Oct. 2012,ieeexplore
10.1109/RCAR47638.2019.9043946,libSmart: an Open-Source Tool for Simple Integration of Deep Learning into Intelligent Robotic Systems,IEEE,Conferences,"Intelligent robotic systems can be empowered by advanced deep learning techniques. Robotic operations such as object recognition are well investigated by researchers involved in machine learning. However, these solutions have often led to ad-hoc implementation in experimental settings. Less reported is systematic implementation of deep learning models in industrial robots. The lack of standard implementation platforms has impeded widespread use of deep learning modules in industrial robots. It is of great importance to have development platforms that can coordinate several deep learning modules of a complex system. In this paper, a scalable deep-learning friendly robot task organization system named libSmart is introduced. Similar to ROS, the architecture of the proposed system allows users to plug and play various devices but the proposed architecture is also highly compatible with deep learning modules. Specifically, the deployment of deep learning models is handled using a novel data graph method with distributed computing. In this way, the computationally expensive training and inferencing processes of deep learning models can be handled with isolated accelerating hardware to reduce the overall system latency. Successful implementation of simultaneous object recognition and pose estimation by an industrial robot has been presented as a case study. The proposed system is open source for all users to build their own intelligent systems with customized deep-learning models. (https://github.com/RustIron/libSmart.git).",https://ieeexplore.ieee.org/document/9043946/,2019 IEEE International Conference on Real-time Computing and Robotics (RCAR),4-9 Aug. 2019,ieeexplore
10.1109/IROS45743.2020.9340956,robo-gym – An Open Source Toolkit for Distributed Deep Reinforcement Learning on Real and Simulated Robots,IEEE,Conferences,"Applying Deep Reinforcement Learning (DRL) to complex tasks in the field of robotics has proven to be very successful in the recent years. However, most of the publications focus either on applying it to a task in simulation or to a task in a real world setup. Although there are great examples of combining the two worlds with the help of transfer learning, it often requires a lot of additional work and fine-tuning to make the setup work effectively. In order to increase the use of DRL with real robots and reduce the gap between simulation and real world robotics, we propose an open source toolkit: robo-gym<sup>1</sup>. We demonstrate a unified setup for simulation and real environments which enables a seamless transfer from training in simulation to application on the robot. We showcase the capabilities and the effectiveness of the framework with two real world applications featuring industrial robots: a mobile robot and a robot arm. The distributed capabilities of the framework enable several advantages like using distributed algorithms, separating the workload of simulation and training on different physical machines as well as enabling the future opportunity to train in simulation and real world at the same time. Finally, we offer an overview and comparison of robo-gym with other frequently used state-of-the-art DRL frameworks.",https://ieeexplore.ieee.org/document/9340956/,2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),24 Oct.-24 Jan. 2021,ieeexplore
10.1109/JIOT.2020.3035431,A Blockchain-Driven IIoT Traffic Classification Service for Edge Computing,IEEE,Journals,"Nowadays, more and more sensors, devices and applications are connected in Industrial Internet of Things (IIoT), producing massive real-time flows which need to be scheduled for Quality-of-Service provision. To realize application-aware and adaptive flow scheduling, the problem of traffic classification must be addressed at first. When edge computing paradigm is introduced into IIoT, the traffic classification service can be deployed on edge node in the near-end. Recently, deep-learning-based IIoT traffic classification methods show better performance, but the computational cost of deep learning model is too high to be deployed on edge node. Moreover, increasingly unknown flows generated by new devices and emerging industrial APPs lead to frequent training of traffic classifiers. It is difficult to migrate the complex process of classifier training from cloud server to edge nodes with limited resources. To address these issues, we take the benefits of hash mechanism and consensus mechanism in blockchain to design a lightweight IIoT traffic classification service, which is more applicable for edge computing paradigm. First, inspired by the hash mechanism in blockchain and the learning to hash for big data, we propose a new learning-to-hash method named extension hashing. By this method, we can build the set of binary coding tress (BCT set), then generating hash table for more efficient k-nearest neighbor-based classification without complex classifier training. Then, we design a new voting-based consensus algorithm to synchronize the BCT sets and the hash tables across edge nodes, thereby providing the traffic classification service. Finally, we conduct data-driven simulations to evaluate the proposed service. By comparing traffic classification results on public data set, we can see that the proposed service achieves the highest classification accuracy with the minimal time cost and memory usage.",https://ieeexplore.ieee.org/document/9247248/,IEEE Internet of Things Journal,"15 Feb.15, 2021",ieeexplore
10.1109/ACCESS.2021.3061477,A Cloud-Based Platform for Big Data-Driven CPS Modeling of Robots,IEEE,Journals,"This paper proposes an improved cyber-physical systems (CPS) architecture for a smart robotic factory based on an industrial cloud platform driven by big data based on the traditional CPS architecture. This paper uses the architecture analysis and design language to model and design a total of three scales for the underlying cell-level robot, the system-level robot shop, and the overall robotic smart factory CPS, respectively, to complete the conceptual scheme for building a robotic smart factory from a local to an overall CPS system. Using the advantages of cloud computing and combining robotic CPS with cloud computing, an architecture for an industrial management system for CPS cloud computing is proposed. Base based distributed storage architecture with Storm based distributed real-time processing architecture. In terms of modeling, the advantages and disadvantages of using AADL, structural analysis, and design language, and modelers, a physical device modeling language, are combined to analyze the advantages and disadvantages of architecture analysis &amp; design language (AADL) for modeling CPS and propose a CPS analysis and design based on AADL and applicable to it. The paper also investigates the use of LeNet models for state identification in the HSV color space. The algorithm was verified on a self-built power equipment indicator dataset with a 100% detection rate and 99.8% state recognition accuracy after four consecutive frames of fusion detection. Simulink simulation of the trolley was carried out in terms of a cell-level robotic trolley CPS system to demonstrate the effectiveness of the design of a robotic CPS system driven by soaring data based on the industrial cloud platform proposed in this paper.",https://ieeexplore.ieee.org/document/9360827/,IEEE Access,2021,ieeexplore
10.1109/TIM.2006.881034,A Comparison Between Stereo-Vision Techniques for the Reconstruction of 3-D Coordinates of Objects,IEEE,Journals,"This paper deals with the problems in setting up stereo-vision systems for contactless measurement of dimensional parameters in industrial environments. Two implicit calibration algorithms for the reconstruction of three-dimensional (3-D) real-world coordinates of objects from pairs of two-dimensional image coordinates have been implemented and compared. The former is based on a direct linear transformation, while the latter on an Artificial Neural Network (ANN). The results of the comparison made on artificial and real objects are finally reported in terms of statistical analysis of the reconstruction error",https://ieeexplore.ieee.org/document/1703886/,IEEE Transactions on Instrumentation and Measurement,Oct. 2006,ieeexplore
10.1109/TLA.2021.9480156,A Data Governance Framework for Industry 4.0,IEEE,Journals,"The fourth industrial revolution, or Industry 4.0, represents a new stage of evolution in the organization, management and control of the value chain throughout the product or service life cycle. This digitization of the industrial environment is characterized by the connection of Information Technologies (IT) and Operations Technologies (OT) through cyber-physical systems and the Industrial IoT (IIoT). One of the main consequences of this integration is the increasing amount and variety of data generated in real time from different sources. In this environment of intensive generation of actionable information, data becomes a critical asset for Industry 4.0, at all stages of the value chain. However, in order to data become a competitive advantage for the company, it must be managed and governed like any other strategic asset, and therefore it is necessary to rely on a Data Governance system. Industry 4.0 requires a reformulation of governance since the data is a key element and the backbone of the processes of the organization. This paper proposes a Reference Framework for the implementation of Data Governance Systems for Industry 4.0. Previously, it contextualizes data governance for Industry 4.0 environments and identifies the requirements that this framework must address, which are conditioned by the specific features of Industry 4.0, among others, the intensive use of big data, cloud and edge computing, artificial intelligence and current regulations.",https://ieeexplore.ieee.org/document/9480156/,IEEE Latin America Transactions,Dec. 2021,ieeexplore
10.1109/TII.2020.2995766,A Directed Edge Weight Prediction Model Using Decision Tree Ensembles in Industrial Internet of Things,IEEE,Journals,"As the application of the industrial Internet of Things (IIoT) becomes more widespread, the IIoT is being combined with social networks. Nodes in the network can be users, machines, and so on. Using the sensing detection technology of the IIoT, industrial machines can realize real-time informatization, which is convenient for users to perform remote management. Nodes can communicate with each other and make ratings. These ratings can be modeled as directed weighted edges between nodes and form directed weighted networks (DWNs). The edge weight represents the “strength” of relationship and the direction of edge points from the edge generator to the edge receiver. Predicting edge weights in DWNs is critical to predicting unknown ratings or recovering lost data. In this article, we propose a directed edge weight prediction model (DEWP) using decision tree ensembles. It extends the local similarity indices to DWNs and extracts a series of similarity indices between nodes as features of each edge. These features are used to construct a blended regression model of random forest, gradient boost decision tree, extreme gradient boosting, and light gradient boosting machine. The proposed algorithm was evaluated experimentally with the Bitcoin OTC and Bitcoin Alpha datasets by removing 10% to 90% of edges in the original network. Compared with other classical algorithms, DEWP has higher prediction accuracy and robustness.",https://ieeexplore.ieee.org/document/9096533/,IEEE Transactions on Industrial Informatics,March 2021,ieeexplore
10.1109/TII.2019.2915846,A Global Manufacturing Big Data Ecosystem for Fault Detection in Predictive Maintenance,IEEE,Journals,"Artificial intelligence, big data, machine learning, cloud computing, and Internet of Things (IoT) are terms which have driven the fourth industrial revolution. The digital revolution has transformed the manufacturing industry into smart manufacturing through the development of intelligent systems. In this paper, a big data ecosystem is presented for the implementation of fault detection and diagnosis in predictive maintenance with real industrial big data gathered directly from large-scale global manufacturing plants, aiming to provide a complete architecture which could be used in industrial IoT-based smart manufacturing in an industrial 4.0 system. The proposed architecture overcomes multiple challenges including big data ingestion, integration, transformation, storage, analytics, and visualization in a real-time environment using various technologies such as the data lake, NoSQL database, Apache Spark, Apache Drill, Apache Hive, OPC Collector, and other techniques. Transformation protocols, authentication, and data encryption methods are also utilized to address data and network security issues. A MapReduce-based distributed PCA model is designed for fault detection and diagnosis. In a large-scale manufacturing system, not all kinds of failure data are accessible, and the absence of labels precludes all the supervised methods in the predictive phase. Furthermore, the proposed framework takes advantage of some of the characteristics of PCA such as its ease of implementation on Spark, its simple algorithmic structure, and its real-time processing ability. All these elements are essential for smart manufacturing in the evolution to Industry 4.0. The proposed detection system has been implemented into the real-time industrial production system in a cooperated company, running for several years, and the results successfully provide an alarm warning several days before the fault happens. A test case involving several outages in 2014 is reported and analyzed in detail during the experiment section.",https://ieeexplore.ieee.org/document/8710319/,IEEE Transactions on Industrial Informatics,Jan. 2020,ieeexplore
10.1109/TIM.2019.2962565,A Lightweight Appearance Quality Assessment System Based on Parallel Deep Learning for Painted Car Body,IEEE,Journals,"The appearance quality assessment based on defect inspection for painted car-body surfaces is an essential work to monitor and analyze the level of paint appearance quality. In the industrial application, there are some challenges, such as the huge and stereo skeleton of car bodies, a variety of irregular local surface areas, low visibility of defects due to tiny real size, and specular car-body surface. To overcome these problems, a lightweight online appearance quality assessment system (OAQAS) based on parallel deep learning is proposed, it includes two parts: 1) a vision inspection subsystem with distributed multi-camera image acquisition module and 2) an appearance quality evaluation subsystem (AQES) based on parallel TinyDefectRNet for evaluating the proposed painted surface grinding difficulty criteria. TinyDefectRNet is able to inspect relatively accurate defect size, although it is trained on a coarsely annotated data set. The OAQAS is implemented in an actual painting production line of a car factory, and the application results show that our OAQAS is far superior to the manual inspection in evaluation accuracy and time consumption. Moreover, our system is lightweight so that it is easy to be plugged into existing painting production lines without rebuilding or changing the inspection room.",https://ieeexplore.ieee.org/document/8964458/,IEEE Transactions on Instrumentation and Measurement,Aug. 2020,ieeexplore
10.1109/ACCESS.2019.2945337,A Linearization Model of Turbofan Engine for Intelligent Analysis Towards Industrial Internet of Things,IEEE,Journals,"Big data processing technologies, e.g., multi-sensor data fusion and cloud computing are being widely used in research, development, manufacturing, health monitoring and maintenance of aero-engines, driven by the ever-rapid development of intelligent manufacturing and Industrial Internet of Things (IIoT). This has promoted rapid development of the aircraft engine industry, increasing the aircraft engine safety, reliability and intelligence. At present, the aero-engine data computing and processing platform used in the industrial Internet of things is not complete, and the numerical calculation and control of aero-engine are inseparable from the linear model, while the existing aero-engine model linearization method is not accurate enough to quickly calculate the dynamic process parameters of the engine. Therefore, in this paper, we propose a linear model of turbofan engine for intelligent analysis in IIoT, with the aim to provide a new perspective for the analysis of engine dynamics. The construction of the proposed model includes three steps: First, a nonlinear mathematical model of a turbofan engine is established by adopting the component modeling approach. Then, numerous parameters of the turbofan engine components and their operating data are obtained by simulating various working conditions. Finally, based on the simulated data for the engine under these conditions, the model at the points during the dynamic process is linearized, such that a dynamic real-time linearized model of turbofan engine is obtained. Simulation results show that the proposed model can accurately depict the dynamic process of the turbofan engine and provide a valuable reference for designing the aero-engine control system and supporting intelligent analysis in IIoT.",https://ieeexplore.ieee.org/document/8856194/,IEEE Access,2019,ieeexplore
10.1109/TIM.2018.2866744,A Multiview Discriminant Feature Fusion-Based Nonlinear Process Assessment and Diagnosis: Application to Medical Diagnosis,IEEE,Journals,"Fusion of large-scale information is the key strategy for the complete understanding of many nonlinear and complicated industrial and medical processes. This paper presents the proposed multiview (MV) feature fusion-based learning generalizing discriminant correlation analysis (DCA) for assessment and diagnosis of nonlinear processes. The core focus of this algorithm is to explore the effectiveness of MV information embedding in learning models and viable implementation in real-time applications. Our method is capable of incorporating high-dimensional information inherent in MV features generated from available inputs using the proposed DCA-based scheme. The algorithm is tested with two real-time electromyogram data sets, which include three categories of nonlinear data-amyotrophic lateral sclerosis, myopathy, and healthy control subjects. A set of MV features is generated in both the time and the wavelet domain for all of the study groups. The features are subjected to DCA projection and optimization and obtained low-order features are concatenated using DCA-based fusion scheme. The discriminant features are applied for the statistical analysis and the model validation. The model achieves an accuracy of 99.03% with a specificity of 99.58% and sensitivities of 98.50% and 97.59%. However, the accuracy over the second data set is 100% with sensitivities of 100% and the specificity of 100%. Results are further compared with the state-of-the-art methods. The proposed scheme is promising and outperforms many state-of-the-art methods, and thus it ensures the faithfulness for industrial applications.",https://ieeexplore.ieee.org/document/8458157/,IEEE Transactions on Instrumentation and Measurement,July 2019,ieeexplore
10.1109/TNNLS.2015.2415257,A Nonnegative Latent Factor Model for Large-Scale Sparse Matrices in Recommender Systems via Alternating Direction Method,IEEE,Journals,"Nonnegative matrix factorization (NMF)-based models possess fine representativeness of a target matrix, which is critically important in collaborative filtering (CF)-based recommender systems. However, current NMF-based CF recommenders suffer from the problem of high computational and storage complexity, as well as slow convergence rate, which prevents them from industrial usage in context of big data. To address these issues, this paper proposes an alternating direction method (ADM)-based nonnegative latent factor (ANLF) model. The main idea is to implement the ADM-based optimization with regard to each single feature, to obtain high convergence rate as well as low complexity. Both computational and storage costs of ANLF are linear with the size of given data in the target matrix, which ensures high efficiency when dealing with extremely sparse matrices usually seen in CF problems. As demonstrated by the experiments on large, real data sets, ANLF also ensures fast convergence and high prediction accuracy, as well as the maintenance of nonnegativity constraints. Moreover, it is simple and easy to implement for real applications of learning systems.",https://ieeexplore.ieee.org/document/7112169/,IEEE Transactions on Neural Networks and Learning Systems,March 2016,ieeexplore
10.1109/TNSRE.2020.3044113,A Novel Point-in-Polygon-Based sEMG Classifier for Hand Exoskeleton Systems,IEEE,Journals,"In the early 2000s, data from the latest World Health Organization estimates paint a picture where one-seventh of the world population needs at least one assistive device. Fortunately, these years are also characterized by a marked technological drive which takes the name of the Fourth Industrial Revolution. In this terrain, robotics is making its way through more and more aspects of everyday life, and robotics-based assistance/rehabilitation is considered one of the most encouraging applications. Providing high-intensity rehabilitation sessions or home assistance through low-cost robotic devices can be indeed an effective solution to democratize services otherwise not accessible to everyone. However, the identification of an intuitive and reliable real-time control system does arise as one of the critical issues to unravel for this technology in order to land in homes or clinics. Intention recognition techniques from surface ElectroMyoGraphic (sEMG) signals are referred to as one of the main ways-to-go in literature. Nevertheless, even if widely studied, the implementation of such procedures to real-case scenarios is still rarely addressed. In a previous work, the development and implementation of a novel sEMG-based classification strategy to control a fully-wearable Hand Exoskeleton System (HES) have been qualitatively assessed by the authors. This paper aims to furtherly demonstrate the validity of such a classification strategy by giving quantitative evidence about the favourable comparison to some of the standard machine-learning-based methods. Real-time action, computational lightness, and suitability to embedded electronics will emerge as the major characteristics of all the investigated techniques.",https://ieeexplore.ieee.org/document/9291412/,IEEE Transactions on Neural Systems and Rehabilitation Engineering,Dec. 2020,ieeexplore
10.1109/ACCESS.2020.2990190,"A Novel Simulated-Annealing Based Electric Bus System Design, Simulation, and Analysis for Dehradun Smart City",IEEE,Journals,"Smart transportation network development with environmental issues into consideration has brought Industry 4.0 based solutions on priority. In this direction, battery-powered electric bus systems have been considered widely for ensuring flexibility, operation cost, and lesser pollutants emission. Industry 4.0 provides automation through a cyber-physical system (CPS), the interconnection of bus system entities with industrial internet-of-things (IIoT), remote information availability through cloud computing and scientific disciplines (human-computer interaction, artificial intelligence, machine learning etc.) integration. In this work, a discrete event-based simulation-optimization approach is integrated that take care of bus energy consumption according to real-time city's passenger needs and on-road friction levels. The proposed simulation optimization methodology utilizes multi-objective with dependent and independent variables for optimizing the overall system performance. In simulation optimization, objective functions are designed to tackle battery consumption, Internet-of-Thing (IoT) network performance, cloud operations efficiency and smart scientific discipline integration. Simulation parameters are based on a real-time bus system which is further analyzed, filtered and adapted as per the needs of the system. In another analysis, supercharger's capacities are varied to evaluate the performance of the proposed system and identify the low cost and efficient smart transportation system. Simulation results show different scenarios for variations in the number of buses, charging stations, bus-depots, mobile charging facilities, and bus-schedules. Simulation results show that the average passenger's waiting time in the waiting is (after ticket booking) varies between 0.2 minutes to 0.7 minutes in real-time traffic conditions. In similar traffic conditions, total passenger's time in system (ticket booking to travel) varies between 41.6 minutes (for 24 hours) to 45.5 minutes (for 1 year). In the simulation, priorities are given to those dependent and independent variables which save the battery consumption and elongate the utilization of buses. Lastly, it is also observed that the proposed system is suitable for resource-constraint devices because Gate Equivalent (GE) calculation shows that the proposed system can be implemented between 1986 GEs (communicational cost without confidentiality and authentication) and 7939 GEs (computational cost with HMAC for authentication in data storage). This ensures varies security primitivs such as confidentiality, availability and authentication.",https://ieeexplore.ieee.org/document/9078106/,IEEE Access,2020,ieeexplore
10.1109/TII.2020.2992728,A Projective and Discriminative Dictionary Learning for High-Dimensional Process Monitoring With Industrial Applications,IEEE,Journals,"Data-driven process monitoring methods have attracted many attentions and gained wide applications. However, the real industrial process data are much more complex which is characterized by multimode, high dimensional, corrupted, and less labeled data. In order to eliminate these unfavourable factors simultaneously, a semisupervised robust projective and discriminative dictionary learning method is proposed. First, a semisupervised strategy is introduced to label unsupervised training data. Then, by utilizing low-rank and sparse features of raw data and outliers, a robust decomposition method is used to obtain clean data. After that, a simultaneously projective and discriminative model is proposed to extracting the feature of the low-rank clean data. Finally, the projection matrix and global dictionary, as well as the threshold are obtained through iterative dictionary learning. This hybrid framework provides a robust model for process monitoring and mode identification, and its efficiency is demonstrated by both synthetic examples and real industrial process cases.",https://ieeexplore.ieee.org/document/9088293/,IEEE Transactions on Industrial Informatics,Jan. 2021,ieeexplore
10.1109/TII.2020.3004232,A Reinforcement Learning-Based Network Traffic Prediction Mechanism in Intelligent Internet of Things,IEEE,Journals,"Intelligent Internet of Things (IIoT) is comprised of various wireless and wired networks for industrial applications, which makes it complex and heterogeneous.The openness of IIoT has led to the intractable problems of network security and management. Many network security and management functions rely on network traffic prediction techniques, such as anomaly detection and predictive network planning. Predicting IIoT network traffic is significantly difficult because its frequently updated topology and diversified services lead to irregular network traffic fluctuations. Motivated by these observations, we proposed a reinforcement learning-based mechanism in this article. We modeled the network traffic prediction problem as a Markov decision process, and then, predicted network traffic by Monte Carlo Q-learning. Furthermore, we addressed the real-time requirement of the proposed mechanism and we proposed a residual-based dictionary learning algorithm to improve the complexity of Monte Carlo Q-learning. Finally, the effectiveness of our mechanism was evaluated using the real network traffic.",https://ieeexplore.ieee.org/document/9123922/,IEEE Transactions on Industrial Informatics,March 2021,ieeexplore
10.1109/ACCESS.2019.2963723,A Smart Collaborative Routing Protocol for Delay Sensitive Applications in Industrial IoT,IEEE,Journals,"In the industrial Internet of things (IIoT), there is always a strong demand for real-time information transfer. Especially when deploying wireless/wired hybrid networks in smart factories, the requirement for low delay interaction is more prominent. Although tree routing protocols have been successfully executed in simple networks, more challenges in transmission speed can be observed in the manufacturing broadband communication system. Motivated by the progresses in deep learning, a smart collaborative routing protocol with low delay and high reliability is proposed to accommodate mixed link scenarios. First, we establish a one-hop delay model to investigate the potential affects of Media Access Control (MAC) layer parameters, which supports the subsequent design. Second, forwarding, maintenance, and efficiency strategies are created to construct the basic functionalities for our routing protocol. Relevant procedures and key approaches are highlighted as well. Third, two sub-protocols are generated and the corresponding implementation steps are described. The experimental results demonstrate that the end-to-end delay can be effectively cut down through comprehensive improvements. Even more sensor nodes and larger network scale are involved, our proposed protocol can still illustrate the advantages comparing with existing solutions within IIoT.",https://ieeexplore.ieee.org/document/8949516/,IEEE Access,2020,ieeexplore
10.1109/JSYST.2019.2921867,A Smart Optimization of Fault Diagnosis in Electrical Grid Using Distributed Software-Defined IoT System,IEEE,Journals,"Electrical power demands have increased significantly over the last years due to rapid increase in air conditioning units and home appliances per domestic unit, particularly in Iraq. Having an uninterrupted power supply is essential for the continuity of power-generated home services and industrial platforms. Currently, in Iraq, electrical power interruption has become a big concern to the utility suppliers. Despite successive attempts to put an end to this dilemma, the issue still prevails. One of the main factors in power outages in local zones is persistent faults in distribution transformers (DTs). DT is considered one of the main elements in the electrical network that is essential for the reliability of the grid supply. Due to the internal lack of monitoring system and periodic maintenance, DT is relentlessly subject to faults due to high overhead utilization. Therefore, in order to enhance the grid reliability, transformer health check, and maintenance practices, we propose a remote condition Internet of Things monitoring and fault prediction system that is based on a customized software-defined networking (SDN) technology. This approach is a transition to smart grid implementation by fusing the power grid with efficient and real-time wireless communication architecture. The SDN implementation is considered in two phases: one is a controller installed per local zone and the other is the main controller that is installed between zones and connected to the core network. The core network consists of redundant links to recover from any future fails. Furthermore, we propose a prediction system based on an artificial neural network algorithm, called distribution transformer fault prediction, that is installed in the management plane for periodic prediction based on real-time sensor traffic to our proposed cloud. Moreover, we propose a communication protocol in the local zone called local SDN-sense. The SDN-sense ensures a reliable communication and local node selection to relay DT sensor data to the main controller. Our proposed architecture showcases an efficient approach to handle future interruption and faults in power grid using cost-effective and reliable infrastructure that can predict and provide real-time health monitoring indices for the Iraqi grid network with minimal power interruptions. After extensive testing, the prediction accuracy was about 96.1%.",https://ieeexplore.ieee.org/document/8746591/,IEEE Systems Journal,June 2020,ieeexplore
10.1109/TCSS.2021.3056410,A Spatiotemporal Recurrent Neural Network for Prediction of Atmospheric PM2.5: A Case Study of Beijing,IEEE,Journals,"With rapid industrial development, air pollution problems, especially in urban and metropolitan centers, have become a serious societal problem and require our immediate attention and comprehensive solutions to protect human and animal health and the environment. Because bad air quality brings prominent effects on our daily life, how to forecast future air quality accurately and tenuously has emerged as a priority for guaranteeing the quality of human life in many urban areas worldwide. Existing models usually neglect the influence of wind and do not consider both distance and similarity to select the most related stations, which can provide significant information in prediction. Therefore, we propose a Geographic Self-Organizing Map (GeoSOM) spatiotemporal gated recurrent unit (GRU) model, which clusters all the monitor stations into several clusters by geographical coordinates and time-series features. For each cluster, we build a GRU model and weighted different models with the Gaussian vector weights to predict the target sequence. The experimental results on real air quality data in Beijing validate the superiority of the proposed method over a number of state-of-the-art ones in metrics, such as R <sup>2</sup>, mean relative error (MRE), and mean absolute error (MAE). The MAE, MRE, and R <sup>2</sup> are 16.1, 0.79, and 0.35 at the Gucheng station and 19.53, 0.82, and 0.36 at the Dongsi station.",https://ieeexplore.ieee.org/document/9361757/,IEEE Transactions on Computational Social Systems,June 2021,ieeexplore
10.1109/ACCESS.2021.3068824,A Stochastic Local Search Algorithm for the Partial Max-SAT Problem Based on Adaptive Tuning and Variable Depth Neighborhood Search,IEEE,Journals,"The Partial Max-SAT (PMSAT) problem is an optimization variant of the well-known Propositional Boolean Satisfiability (SAT) problem. It holds an important place in theory and practice, because a huge number of real-world problems, such as timetabling, planning, routing, bioinformatics, fault diagnosis, etc., could be encoded into it. Stochastic local search (SLS) methods can solve many real-world problems that often involve large-scale instances at reasonable computation costs while delivering good-quality solutions. In this work, we propose a novel SLS algorithm called adaptive variable depth SLS for PMSAT problem solving based on a dynamic local search framework. Our algorithm exploits two algorithmic components of an SLS method: parameter tuning and neighborhood search. Our first contribution is the design of an adaptive parameter tuner that searches for the best parameter setting for each instance by considering its features. The second contribution is a variable depth neighborhood search (VDS) algorithm adopted for PMSAT problem, which our empirical evaluation proves is a more efficient w.r.t. single neighborhood search. We conducted our experiments on the PMSAT benchmarks from MaxSAT Evaluation 2014 to 2019, including more than 3600 instances which have been encoded from a broad range of domains such as verification, optimization, graph theory, automated-reasoning, pseudo Boolean, etc. Our experimental evaluation results show that AVD-SLS solver, which is implemented based on our algorithm, outperforms state-of-the-art PMSAT SLS solvers in most benchmark classes, including random, crafted, and industrial instances. Furthermore, AVD-SLS reports remarkably better results on weighted benchmark, and shows competitive results with several well-known hybrid PMSAT solvers.",https://ieeexplore.ieee.org/document/9386095/,IEEE Access,2021,ieeexplore
10.1109/TCIAIG.2017.2755699,A Tool to Design Interactive Characters Based on Embodied Cognition,IEEE,Journals,"Creating interactive characters is one of the most challenging tasks of videogame design. In order to facilitate such an endeavor, we introduce a decisional and behavior synthesis architecture integrated in the game engine Unity3D. A distinguishing feature of this architecture is that it embraces embodied cognition principles and uses them as implementation requirements. From these, we derive an architecture, which is based on a novel combination of previously proposed systems, together with some simplifications. We also argue that the architecture proposed has properties-modularity, scalability, and stability-which can be beneficial for its practical industrial adoption, particularly in the context of the recent improvement in machine learning techniques. Artificial intelligence is a quite technical domain, and we believe such a tool can facilitate interactive character creation by creative minds in industrial applications.",https://ieeexplore.ieee.org/document/8048516/,IEEE Transactions on Games,Dec. 2019,ieeexplore
10.1109/TASE.2020.3032075,A Virtual Mechanism Approach for Exploiting Functional Redundancy in Finishing Operations,IEEE,Journals,"We propose a new approach to programming by the demonstration of finishing operations. Such operations can be carried out by industrial robots in multiple ways because an industrial robot is typically functionally redundant with respect to a finishing task. In the proposed system, a human expert demonstrates a finishing operation, and the demonstrated motion is recorded in the Cartesian space. The robot’s kinematic model is augmented with a virtual mechanism, which is defined according to the applied finishing tool. This way, the kinematic model is expanded with additional degrees of freedom that can be exploited to compute the optimal joint space motion of the robot without altering the essential aspects of the Cartesian space task execution as demonstrated by the human expert. Finishing operations, such as polishing and grinding, occur in contact with the treated workpiece. Since information about the contact point position is needed to control the robot during the operation, we have developed a novel approach for accurate estimation of contact points using the measured forces and torques. Finally, we applied iterative learning control to refine the demonstrated operations and compensate for inaccurate calibration and different dynamics of the robot and human demonstrator. The proposed method was verified on real robots and real polishing and grinding tasks. <italic>Note to Practitioners</italic>—This work was motivated by the need for automation of finishing operations, such as polishing and grinding, on contemporary industrial robots. Existing approaches are both too complex and too time-consuming to be applied in flexible and small-scale production, which often requires the frequent deployment of new applications. Our approach is based on programming by demonstration and enables the programming of finishing operations also for users who are not specialists in robot programming. Programming by demonstration is especially useful for teaching finishing operations because it enables the transfer of expert knowledge about finishing skills to robots without providing lengthy task descriptions or manual coding. Besides the human demonstration of the desired operation, the proposed approach also requires the availability of the kinematic model for the machine tool applied to carry out the finishing operation. We provide several practical examples of grinding and polishing tools and how to integrate them into our approach. Another feature of the proposed system is that user demonstrations of finishing operations can be transferred between different combinations of robots and machine tools.",https://ieeexplore.ieee.org/document/9246671/,IEEE Transactions on Automation Science and Engineering,Oct. 2021,ieeexplore
10.1109/TAC.1981.1102738,A digital quality control system for an industrial dry process rotary cement kiln,IEEE,Journals,"A multivariate autoregressive moving average (ARMA) model for an industrial dry process rotary cement kiln is identified, from real process data, using the maximum likelihood method. The model obtained is then used in computing a controller for quality control of clinker production. It is shown that it is relevant to compute a minimum variance controller subject to restrictions both in the controller structure and the variances of the control signals. The resulting controller is finally implemented on the cement kiln process, together with a target adaptive controller for automatic adjustment of the clinker quality setpoint, in order to save energy.",https://ieeexplore.ieee.org/document/1102738/,IEEE Transactions on Automatic Control,August 1981,ieeexplore
10.1109/TC.2020.2968888,Accurate Cost Estimation of Memory Systems Utilizing Machine Learning and Solutions from Computer Vision for Design Automation,IEEE,Journals,"Hardware/software co-designs are usually defined at high levels of abstractions at the beginning of the design process in order to provide a variety of options on how to realize a system. This allows for design exploration which relies on knowing the costs of different design configurations (with respect to hardware usage and firmware metrics). To this end, methods for cost estimation are frequently applied in industrial practice. However, currently used methods oversimplify the problem and ignore important features, leading to estimates which are far off from real values. In this article, we address this problem for memory systems. To this end, we borrow and re-adapt solutions based on Machine Learning (ML) which have been found suitable for problems from the domain of Computer Vision (CV). Based on that, an approach is proposed which outperforms existing methods for cost estimation. Experimental evaluations within an industrial context show that, while the accuracy of the state-of-the-art approach is frequently off by more than 20 percent for area estimation and more than 15 percent for firmware estimation, the method proposed in this article comes rather close to the actual values (just 5-7 percent off for both area and firmware). Furthermore, our approach outperforms existing methods for scalability, generalization, and decrease in manual effort.",https://ieeexplore.ieee.org/document/8967025/,IEEE Transactions on Computers,1 June 2020,ieeexplore
10.1109/ACCESS.2021.3055257,Adaptive Vision-Based Method for Rotor Dynamic Balance System,IEEE,Journals,"This article presents a new adaptive vision-based method (AVBM) of performing automatic detection for rotor balancing, and the online implementation proved that the method achieved rapid real-time optimization of system balancing configuration for a rotor dynamic balance machine. The proposed AVBM integrated 3D sensors and dynamic balancing platform using 3D computer vision technique and dynamic balance algorithm to improve the efficiency of rotor dynamic balancing. AVBM applied 3D ToF sensors on active rotor dynamic balance machines to grab 3D point cloud of rotor shaft and balance sprues. By 3D depth data, the background noise can be removed to detect the positions of shaft center, key phasor and balance sprues of rotor automatically. After combining with unbalance vector from dynamic balancing machine, the AVBM system calculated the optimal balance configuration by the vector analysis algorithm. Compares to conventional methods, conventional rotor dynamic balancing process relies on technicians to mount washers on particular balance sprues based on their experience, therefore uncertainty causes productivity decline. Experiments in industrial examples showed that the proposed AVBM required fewer rounds to achieve acceptance, whereas the conventional industrial rotor balancing method performed by operators required more than three rounds in average. Consider the overall dynamic balancing process for the motor, the processing time required for each motor without AVBM was 348.9 seconds, and the daily rotor balancing count of each dynamic balancing machine was 83. The processing time with AVBM was shortened from 348.9 to 283.9 seconds, the daily rotor balancing count had increased from 83 to 101, and the production improvement had reached 22 %. That is, the proposed AVBM can accurately analyze the dynamic balance and greatly reduce the redundant dynamic balance operations. The main advantage of the proposed AVBM over conventional methods is its efficiency, effectiveness and robustness in online optimization of rotor dynamic balance.",https://ieeexplore.ieee.org/document/9337863/,IEEE Access,2021,ieeexplore
10.1109/TIE.2019.2962437,An Efficient Convolutional Neural Network Model Based on Object-Level Attention Mechanism for Casting Defect Detection on Radiography Images,IEEE,Journals,"Automatic detection of casting defects on radiography images is an important technology to automatize digital radiography defect inspection. Traditionally, in an industrial application, conventional methods are inefficient when the detection targets are small, local, and subtle in the complex scenario. Meanwhile, the outperformance of deep learning models, such as the convolutional neural network (CNN), is limited by a huge volume of data with precise annotations. To overcome these challenges, an efficient CNN model, only trained with image-level labels, is first proposed for detection of tiny casting defects in a complicated industrial scene. Then, in this article, we present a novel training strategy which can form a new object-level attention mechanism for the model during the training phase, and bilinear pooling is utilized to improve the model capability of detecting local contrast casting defects. Moreover, to enhance the interpretability, we extend class activation maps (CAM) to bilinear CAM (Bi-CAM) which is adapted to bilinear architectures as a visualization technique to reason about the model output. Experimental results show that the proposed model achieves superior performance in terms of each quantitative metric and is suitable for most actual applications. The real-time defect detection of castings is efficiently implemented in the complex scenario.",https://ieeexplore.ieee.org/document/8948332/,IEEE Transactions on Industrial Electronics,Dec. 2020,ieeexplore
10.1109/TII.2020.2965996,An Efficient Feature Extraction Method for the Detection of Material Rings in Rotary Kilns,IEEE,Journals,"Rotary kilns are important industrial plants to process material. In this article, we address the problem of detecting material rings formed in alumina rotary kilns, which leads to high waste and substantial economic loss. This necessitates the use of artificial intelligence to help kiln operators detect material rings promptly. We describe a recently developed system that extracts useful features and detects the presence of material rings efficiently and accurately in real time. Our major contribution is a novel feature extraction method based on geometric transformation and peak localization exploiting the domain knowledge. We also contribute a large data set covering many thousand labeled frames for the evaluation of ring detection. On this data set, we demonstrate that our novel feature extraction method outperforms other alternatives in the literature. Our method is also demonstrated as favorable over deep learning approaches. Our system produces an overall accuracy of nearly 96%, which is acceptable for deployment.",https://ieeexplore.ieee.org/document/8957316/,IEEE Transactions on Industrial Informatics,Sept. 2020,ieeexplore
10.1109/ACCESS.2021.3065195,An End-to-End Intelligent Fault Diagnosis Application for Rolling Bearing Based on MobileNet,IEEE,Journals,"To find out the hidden danger in the industrial production process in time, it is necessary to monitor the health condition of the key components of the mechanical system in operation. However, traditional fault diagnosis methods usually adopt manual feature extraction, which not only costs expensively and depends on prior knowledge. Therefore, it is of great significance to study the application of automatic fault identification based on the original vibration signals. Recently, existing studies have shown that most of fault diagnoses are implemented by using deep neural network. Although these methods have achieved satisfactory performances, there are obvious limitations in real applications, that is, the complexity of deep neural network requires a lot of hardware computing resources. This hinders the development of online fault diagnosis tools. To solve this problem, this paper proposes a fault diagnosis model based on lightweight convolutional neural network MobileNet, and realizes an end-to-end intelligent fault classification and diagnosis application. We evaluated the proposed method with the rolling bearing dataset from Western Reserve University. The best average precision, recall and F1 score of ten different bearing health conditions are about 96%, 82% and 88%, respectively. In addition, we also compare the accuracy of the rolling bearing fault diagnosis classification model under the standard ReLU and the improved ReLU. Experimental results show that both obtain good performances, but the improved ReLU reaches the over 96% accuracy more rapidly.",https://ieeexplore.ieee.org/document/9374431/,IEEE Access,2021,ieeexplore
10.1109/TIM.2019.2915404,An End-to-End Steel Surface Defect Detection Approach via Fusing Multiple Hierarchical Features,IEEE,Journals,"A complete defect detection task aims to achieve the specific class and precise location of each defect in an image, which makes it still challenging for applying this task in practice. The defect detection is a composite task of classification and location, leading to related methods is often hard to take into account the accuracy of both. The implementation of defect detection depends on a special detection data set that contains expensive manual annotations. In this paper, we proposed a novel defect detection system based on deep learning and focused on a practical industrial application: steel plate defect inspection. In order to achieve strong classification ability, this system employs a baseline convolution neural network (CNN) to generate feature maps at each stage, and then the proposed multilevel feature fusion network (MFN) combines multiple hierarchical features into one feature, which can include more location details of defects. Based on these multilevel features, a region proposal network (RPN) is adopted to generate regions of interest (ROIs). For each ROI, a detector, consisting of a classifier and a bounding box regressor, produces the final detection results. Finally, we set up a defect detection data set NEU-DET for training and evaluating our method. On the NEU-DET, our method achieves 74.8/82.3 mAP with baseline networks ResNet34/50 by using 300 proposals. In addition, by using only 50 proposals, our method can detect at 20 ft/s on a single GPU and reach 92% of the above performance, hence the potential for real-time detection.",https://ieeexplore.ieee.org/document/8709818/,IEEE Transactions on Instrumentation and Measurement,April 2020,ieeexplore
10.1109/ACCESS.2020.2992249,An Ensemble Deep Learning-Based Cyber-Attack Detection in Industrial Control System,IEEE,Journals,"The integration of communication networks and the Internet of Things (IoT) in Industrial Control Systems (ICSs) increases their vulnerability towards cyber-attacks, causing devastating outcomes. Traditional Intrusion Detection Systems (IDSs), which are mainly developed to support information technology systems, count vastly on predefined models and are trained mostly on specific cyber-attacks. Besides, most IDSs do not consider the imbalanced nature of ICS datasets, thereby suffering from low accuracy and high false-positive when being put to use. In this paper, we propose a deep learning model to construct new balanced representations of the imbalanced datasets. The new representations are fed into an ensemble deep learning attack detection model specifically designed for an ICS environment. The proposed attack detection model leverages Deep Neural Network (DNN) and Decision Tree (DT) classifiers to detect cyber-attacks from the new representations. The performance of the proposed model is evaluated based on 10-fold cross-validation on two real ICS datasets. The results show that the proposed method outperforms conventional classifiers, including Random Forest (RF), DNN, and AdaBoost, as well as recent existing models in the literature. The proposed approach is a generalized technique, which can be implemented in existing ICS infrastructures with minimum effort.",https://ieeexplore.ieee.org/document/9086038/,IEEE Access,2020,ieeexplore
10.1109/TII.2019.2959021,An Integrated Histogram-Based Vision and Machine-Learning Classification Model for Industrial Emulsion Processing,IEEE,Journals,"Existing techniques in emulsion quality evaluation are found to be highly subjective, time-consuming, and prone to overprocessing. Other conventional droplet analysis techniques such as laser diffraction, which require dilution of samples, introduce an additional complexity to industrial processes. The possibility of developing a fully automated technique for droplet characterization during emulsification holds remarkable potential for overcoming the existing challenges. In this article, a histogram-based image segmentation technique detects droplets from emulsion micrographs. The evolution of droplet characteristics and their significance are studied by performing statistical analysis, and the significant characteristics are selected. The principal component analysis is applied to obtain a reduced set of uncorrelated components from the selected characteristics. The linear discriminant analysis classifies the micrographs into a set of quality categories called target, acceptable, marginal, and unacceptable. The model accuracy is validated using stratified five-fold cross-validation and is successful in classifying the micrographs obtained from two different manufacturing facilities with high accuracy up to 100%. The histogram-based technique is successful in detecting smaller droplets than previously reflected in the literature. The current approach is fully automated and is implemented as a soft-sensor, which supports its real-time deployment into an industrial environment. The entire approach has promising potential in the in-line prediction of emulsion quality leading to more efficient and sustainable manufacturing.",https://ieeexplore.ieee.org/document/8968624/,IEEE Transactions on Industrial Informatics,Sept. 2020,ieeexplore
10.1109/ACCESS.2019.2960497,Analysis of Machine Learning Methods in EtherCAT-Based Anomaly Detection,IEEE,Journals,"Today, the use of Ethernet-based protocols in industrial control systems (ICS) communications has led to the emergence of attacks based on information technology (IT) on supervisory control and data acquisition systems. In addition, the familiarity of Ethernet and TCP/IP protocols and the diversity and success of attacks on them raises security risks and cyber threats for ICS. This issue is compounded by the absence of encryption, authorization, and authentication mechanisms due to the development of industrial communications protocols only for performance purposes. Recent zero-day attacks, such as Triton, Stuxnet, Havex, Dragonfly, and Blackenergy, as well as the Ukraine cyber-attack, are possible because of the vulnerabilities of the systems; these attacksare carried by the protocols used in communication between PLC and I/O units or HMI and engineering stations. It is evident that there is a need for robust solutions that detect and prevent protocol-based cyber threats. In this paper, machine learning methods are evaluated for anomaly detection, particularly for EtherCAT-based ICS. To the best of the author's knowledge, there has been no research focusing on machine learning algorithms for anomaly detection of EtherCAT. Before testing anomaly detection, an EtherCAT-based water level control system testbed was developed. Then, a total of 16 events were generated in four categories and applied on the testbed. The dataset created was used for anomaly detection. The results showed that the k-nearest neighbors (k-NN) and support vector machine with genetic algorithm (SVM GA) models perform best among the 18 techniques applied. In addition to detecting anomalies, the methods are able to flag the attack types better than other techniques and are applicable in EtherCAT networks. Also, the dataset and events can be used for further studies since it is difficult to obtain data for ICS due to its critical infrastructure and continuous real-time operation.",https://ieeexplore.ieee.org/document/8936397/,IEEE Access,2019,ieeexplore
10.1109/TIFS.2019.2923577,Anomaly Detection in Real-Time Multi-Threaded Processes Using Hardware Performance Counters,IEEE,Journals,"We propose a novel methodology for real-time monitoring of software running on embedded processors in cyber-physical systems (CPS). The approach uses real-time monitoring of hardware performance counters (HPC) and applies to multi-threaded and interrupt-driven processes typical in programmable logic controller (PLC) implementation of real-time controllers. The methodology uses a black-box approach to profile the target process using HPCs. The time series of HPC measurements over a time window under known-good operating conditions is used to train a machine learning classifier. At run-time, this trained classifier classifies the time series of HPC measurements as baseline (i.e., probabilistically corresponding to a model learned from the training data) or anomalous. The baseline versus anomalous labels over successive time windows offer robustness against the stochastic variability of code execution on the embedded processor and detect code modifications. We demonstrate effectiveness of the approach on an embedded PLC in a hardware-in-the-loop (HITL) testbed emulating a benchmark industrial process. In addition, to illustrate the scalability of the approach, we also apply the methodology to a second PLC platform running a representative embedded control process.",https://ieeexplore.ieee.org/document/8737990/,IEEE Transactions on Information Forensics and Security,2020,ieeexplore
10.1109/TSG.2021.3054375,"Anomaly Detection, Localization and Classification Using Drifting Synchrophasor Data Streams",IEEE,Journals,"With ongoing automation and digitization of the electric power system, several Phasor Measurement Units (PMUs) have been deployed for monitoring and control. PMU data can have multiple anomalies, and many of the researchers in the past have concentrated on training machine/deep learning algorithms offline for anomaly detection over PMU data (i.e., not in real-time). These machine/deep learning algorithms, when trained offline on a sample rather than a population of the dataset, fail to consider the dynamic behavior of the power grid in real-time, resulting in low accuracy. Considering the dynamic behavior of the power grid (e.g., change in load, generation, distributed energy resources (DERs) switching, network, controls), the definition of data anomalies varies in time and requires online training. A fundamental challenge is to enable online (i.e., real-time) training of machine/deep learning algorithms for anomaly detection over streaming PMU data. While machine/deep learning is often desirable to manage data streams, training a deep learning algorithm over streaming PMU data is nontrivial due to changes in data statistics caused by dynamic streaming data. This article proposes PMUNET: a novel device-level deep learning-based data-driven approach for anomaly detection, localization, and classification over streaming PMU data, using online learning and multivariate data-drift detection algorithm. Two variants of PMUNET, Dynamic data Change Driven Learning (DCDL) and Continuity Driven Learning (CDL), are proposed and compared. DCDL aims to train the deep learning algorithm whenever the definition of anomaly changes due to the power grid dynamics. On the other hand, CDL continuously trains the deep learning algorithm over the PMU data-stream. The experimental results verify that DCDL outperforms CDL and other efficient anomaly detection methods over multiple events such as faults and load/ generator/capacitor/DERs variations/switching for IEEE 14 and 39 Bus test system as well as real PMU industrial data. The result verifies that DCDL variant of PMUNET improves over existing approach with a gain of 2% - 10% in terms of accuracy, false-positive rate, and false-negative rate.",https://ieeexplore.ieee.org/document/9335975/,IEEE Transactions on Smart Grid,July 2021,ieeexplore
10.1109/ACCESS.2020.2971319,Artificial Generation of Partial Discharge Sources Through an Algorithm Based on Deep Convolutional Generative Adversarial Networks,IEEE,Journals,"The measurement of partial discharges (PD) in electrical equipment or machines subjected to high voltage can be considered as one of the most important indicators when assessing the state of an insulation system. One of the main challenges in monitoring these degradation phenomena is to adequately measure a statistically significant number of signals from each of the sources acting on the asset under test. However, in industrial environments the presence of large amplitude noise sources or the simultaneous presence of multiple PD sources may limit the acquisition of the signals and therefore the final diagnosis of the equipment status may not be the most accurate. Although different procedures and separation and identification techniques have been implemented with very good results, not having a significant number of PD pulses associated with each source can limit the effectiveness of these procedures. Based on the above, this research proposes a new algorithm of artificial generation of PD based on a Deep Convolutional Generative Adversarial Networks (DCGAN) architecture which allows artificially generating different sources of PD from a small group of real PD, in order to complement those sources that during the measurement were poorly represented in terms of signals. According to the results obtained in different experiments, the temporal and spectral behavior of artificially generated PD sources proved to be similar to that of real experimentally obtained sources.",https://ieeexplore.ieee.org/document/8979409/,IEEE Access,2020,ieeexplore
10.1109/LRA.2020.2969927,Augmented LiDAR Simulator for Autonomous Driving,IEEE,Journals,"In Autonomous Driving (AD), detection and tracking of obstacles on the roads is a critical task. Deep-learning based methods using annotated LiDAR data have been the most widely adopted approach for this. Unfortunately, annotating 3D point cloud is a very challenging, time- and money-consuming task. In this letter, we propose a novel LiDAR simulator that augments real point cloud with synthetic obstacles (e.g., vehicles, pedestrians, and other movable objects). Unlike previous simulators that entirely rely on CG (Computer Graphics) models and game engines, our augmented simulator bypasses the requirement to create high-fidelity background CAD (Computer Aided Design) models. Instead, we can deploy a vehicle with a LiDAR scanner to sweep the street of interests to obtain the background points cloud, based on which annotated point cloud can be automatically generated. This “scan-and-simulate” capability makes our approach scalable and practical, ready for large-scale industrial applications. In this letter, we describe our simulator in detail, in particular the placement of obstacles that is critical for performance enhancement. We show that detectors with our simulated LiDAR point cloud alone can perform comparably (within two percentage points) with these trained with real data. Mixing real and simulated data can achieve over 95% accuracy.",https://ieeexplore.ieee.org/document/8972449/,IEEE Robotics and Automation Letters,April 2020,ieeexplore
10.1109/ACCESS.2021.3127084,Auto-NAHL: A Neural Network Approach for Condition-Based Maintenance of Complex Industrial Systems,IEEE,Journals,"Nowadays, machine learning has emerged as a promising alternative for condition monitoring of industrial processes, making it indispensable for maintenance planning. Such a learning model is able to assess health states in real time provided that both training and testing samples are complete and have the same probability distribution. However, it is rare and difficult in practical applications to meet these requirements due to the continuous change in working conditions. Besides, conventional hyperparameters tuning via grid search or manual tuning requires a lot of human intervention and becomes inflexible for users. Two objectives are targeted in this work. In an attempt to remedy the data distribution mismatch issue, we firstly introduce a feature extraction and selection approach built upon correlation analysis and dimensionality reduction. Secondly, to diminish human intervention burdens, we propose an Automatic artificial Neural network with an Augmented Hidden Layer (Auto-NAHL) for the classification of health states. Within the designed network, it is worthy to mention that the novelty of the implemented neural architecture is attributed to the new multiple feature mappings of the inputs, where such configuration allows the hidden layer to learn multiple representations from several random linear mappings and produce a single final efficient representation. Hyperparameters tuning including the network architecture, is fully automated by incorporating Particle Swarm Optimization (PSO) technique. The designed learning process is evaluated on a complex industrial plant as well as various classification problems. Based on the obtained results, it can be claimed that our proposal yields better response to new hidden representations by obtaining a higher approximation compared to some previous works.",https://ieeexplore.ieee.org/document/9610082/,IEEE Access,2021,ieeexplore
10.1109/TII.2018.2816971,Automatic Selection of Optimal Parameters Based on Simple Soft-Computing Methods: A Case Study of Micromilling Processes,IEEE,Journals,"Nowadays, the application of novel soft-computing methods to new industrial processes is often limited by the actual capacity of the industry to assimilate state-of-the-art computational methods. The selection of optimal parameters for efficient operation is very challenging in microscale manufacturing processes, because of intrinsic nonlinear behavior and reduced dimensions. In this paper, a decision-making system for selecting optimal parameters in micromilling operations is designed and implemented using simple and efficient soft-computing techniques. The procedure primarily consists of four steps: an experimental characterization; the modeling of cutting force and surface roughness by means of a multilayer perceptron; multiobjective optimization using the cross-entropy method, taking into account productivity and surface quality; and a decision-making procedure for selecting the most appropriate parameters using a fuzzy inference system. Finally, two different alloys for micromilling processes are considered, in order to evaluate the proposed system: a titanium-based alloy and a tungsten-copper alloy. The experimental study demonstrated the effectiveness of the proposed solution for automated decision-making, based on simple soft-computing methods, and its successful application to a real-life industrial challenge.",https://ieeexplore.ieee.org/document/8325494/,IEEE Transactions on Industrial Informatics,Feb. 2019,ieeexplore
10.1109/ACCESS.2020.2993010,Bearing Intelligent Fault Diagnosis in the Industrial Internet of Things Context: A Lightweight Convolutional Neural Network,IEEE,Journals,"The advancement of Industry 4.0 and Industrial Internet of Things (IIoT) has laid more emphasis on reducing the parameter amount and storage space of the model in addition to the automatic and accurate fault diagnosis. In this case, this paper proposes a lightweight convolutional neural network (LCNN) method for intelligent fault diagnosis of bearing, which can largely satisfy the need of less parameter amount and storage space as well as high accuracy. First, depthiwise separable convolution is adopted, and a LCNN structure is constructed through an inverse residual structure and a linear bottleneck layer operation. Secondly, a novel decomposed Hierarchical Search Space is introduced to automatically explore the optimal LCNN for bearing fault diagnosis in the context of the IIoT. In the meantime, the real-time monitoring and fault diagnosis of the model are also deployed. In order to verify the validity of the designed model, Case Western Reserve University Bearing fault dataset and MFPT bearing fault dataset are adopted. The results demonstrate the great advantages of the model. The LCNN model can automatically learn and select the appropriate features, highly improving the fault diagnosis accuracy. Meanwhile, the computational and storage costs of the model are largely reduced, which contributes to its being applied to the mechanical system in the IIoT context.",https://ieeexplore.ieee.org/document/9088980/,IEEE Access,2020,ieeexplore
10.1109/TSMCC.2009.2013816,Cascaded and Hierarchical Neural Networks for Classifying Surface Images of Marble Slabs,IEEE,Journals,"Marble quality classification is an important procedure generally performed by human experts. However, using human experts for classification is error prone and subjective. Therefore, automatic and computerized methods are needed in order to obtain reproducible and objective results. Although several methods are proposed for this purpose, we demonstrate that their performance is limited when dealing with diverse datasets containing a large number of quality groups. In this work, we test several feature sets and neural network topologies to obtain a better classification performance. During these tests, it is observed that different feature sets represent different subgroup(s) in a quality group rather than representing the whole group. Therefore, our approach is to use these features in a cascaded manner in which a quality group is classified by classifying all of its subgroups. We first realize this approach by using a two-stage cascaded network. Then, we design a hierarchical radial basis function network (HRBFN) in which correctly classified marble samples are taken out of the dataset and a different feature extraction method is applied to the remaining samples at each network level. The HRBFN system produces successful results for industrial applications and facilitates the desirable property of implementation in a quasi real-time manner.",https://ieeexplore.ieee.org/document/4812089/,"IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)",July 2009,ieeexplore
10.1109/TPWRD.2007.899522,Classification of Electrical Disturbances in Real Time Using Neural Networks,IEEE,Journals,"Power-quality (PQ) monitoring is an essential service that many utilities perform for their industrial and larger commercial customers. Detecting and classifying the different electrical disturbances which can cause PQ problems is a difficult task that requires a high level of engineering knowledge. This paper presents a novel system based on neural networks for the classification of electrical disturbances in real time. In addition, an electrical pattern generator has been developed in order to generate common disturbances which can be found in the electrical grid. The classifier obtained excellent results (for both test patterns and field tests) thanks in part to the use of this generator as a training tool for the neural networks. The neural system is integrated on a software tool for a PC with hardware connected for signal acquisition. The tool makes it possible to monitor the acquired signal and the disturbances detected by the system.",https://ieeexplore.ieee.org/document/4265702/,IEEE Transactions on Power Delivery,July 2007,ieeexplore
10.1109/TCSS.2021.3052231,Cognitive Analytics of Social Media Services for Edge Resource Pre-Allocation in Industrial Manufacturing,IEEE,Journals,"With the development of industrial intelligence, the resource requests of various social media services in smart cities are expanding rapidly. For hosting services, the edge computing (EC) platform for its low-latency resource provisioning is fully explored. However, the mapping between edge servers (ESs) and services affects the service latency. Meanwhile, the real-time dynamic distribution of resource requirements also impairs the load balance. Therefore, how to optimize the load balance of ESs while meeting the latency-critical requests remains challenging. To deal with the above challenge, in this article, we propose a resource pre-allocation (RPA) method for the social media services with cognitive analytics. Technically, the deep spatiotemporal residual network (ST-ResNet) is employed to complete the cognitive analytics of resource requests. Then based on the analysis results, the optimal resource allocation (ORA) scheme is designed with multiobjective optimization. Finally, the performance of RPA is evaluated by a real-world resource request data set.",https://ieeexplore.ieee.org/document/9340550/,IEEE Transactions on Computational Social Systems,April 2021,ieeexplore
10.1109/TSMC.2014.2347265,Comparing and Combining Predictive Business Process Monitoring Techniques,IEEE,Journals,"Predictive business process monitoring aims at forecasting potential problems during process execution before they occur so that these problems can be handled proactively. Several predictive monitoring techniques have been proposed in the past. However, so far those prediction techniques have been assessed only independently from each other, making it hard to reliably compare their applicability and accuracy. We empirically analyze and compare three main classes of predictive monitoring techniques, which are based on machine learning, constraint satisfaction, and Quality-of-Service (QoS) aggregation. Based on empirical evidence from an industrial case study in the area of transport and logistics, we assess those techniques with respect to five accuracy indicators. We further determine the dependency of accuracy on the point in time during process execution when a prediction is made in order to determine lead-times for accurate predictions. Our evidence suggests that, given a lead-time of half of the process duration, all predictive monitoring techniques consistently provide an accuracy of at least 70%. Yet, it also becomes evident that the techniques differ in terms of how accurately they may predict violations and nonviolations. To improve the prediction process, we thus exploit the characteristics of the individual techniques and propose their combination. Based on our case study data, evidence indicates that certain combinations of techniques may outperform individual techniques with respect to specific accuracy indicators. Combining constraint satisfaction with QoS aggregation, for instance, improves precision by 14%; combining machine learning with constraint satisfaction shows an improvement in recall by 23%.",https://ieeexplore.ieee.org/document/6882809/,"IEEE Transactions on Systems, Man, and Cybernetics: Systems",Feb. 2015,ieeexplore
10.1109/TASE.2020.3010536,Condition-Driven Data Analytics and Monitoring for Wide-Range Nonstationary and Transient Continuous Processes,IEEE,Journals,"Frequent and wide changes in operation conditions are quite common in real process industry, resulting in typical wide-range nonstationary and transient characteristics along time direction. The considerable challenge is, thus, how to solve the conflict between the learning model accuracy and change complexity for analysis and monitoring of nonstationary and transient continuous processes. In this work, a novel condition-driven data analytics method is developed to handle this problem. A condition-driven data reorganization strategy is designed which can neatly restore the time-wise nonstationary and transient process into different condition slices, revealing similar process characteristics within the same condition slice. Process analytics can then be conducted for the new analysis unit. On the one hand, coarse-grained automatic condition-mode division is implemented with slow feature analysis to track the changing operation characteristics along condition dimension. On the other hand, fine-grained distribution evaluation is performed for each condition mode with Gaussian mixture model. Bayesian inference-based distance (BID) monitoring indices are defined which can clearly indicate the fault effects and distinguish different operation scenarios with meaningful physical interpretation. A case study on a real industrial process shows the feasibility of the proposed method which, thus, can be generalized to other continuous processes with typical wide-range nonstationary and transient characteristics along time direction. <italic>Note to Practitioners</italic>—Industrial processes in general have nonstationary characteristics which are ubiquitous in real world data, often reflected by a time-variant mean, a time-variant autocovariance, or both resulting from various factors. The focus of this study is to develop a universal analytics and monitoring method for wide-range nonstationary and transient continuous processes. Condition-driven concept takes the place of time-driven thought. For the first time, it is recognized that there are similar process characteristics within the same condition slice and changes in the process correlations may relate to its condition modes. Besides, the proposed method can provide enhanced physical interpretation for the monitoring results with concurrent analysis of the static and dynamic information which carry different information, analogous to the concepts of “position” and “velocity” in physics, respectively. The static information can tell the current operation condition, while the dynamic information can clarify whether the process status is switching between different steady states. It is noted that the condition-driven concept is universal and can be extended to other applications for industrial manufacturing applications.",https://ieeexplore.ieee.org/document/9158352/,IEEE Transactions on Automation Science and Engineering,Oct. 2021,ieeexplore
10.1109/TII.2020.3028612,DWFCAT: Dual Watermarking Framework for Industrial Image Authentication and Tamper Localization,IEEE,Journals,"The image data received through various sensors are of significant importance in Industry 4.0. Unfortunately, these data are highly vulnerable to various malicious attacks during its transit to the destination. Although the use of pervasive edge computing (PEC) with the Internet of Things (IoT) has solved various issues, such as latency, proximity, and real-time processing, but the security and authentication of data between the nodes is still a significant concern in PEC-based industrial-IoT scenarios. In this article, we present “DWFCAT,” a dual watermarking framework for content authentication and tamper localization for industrial images. The robust and fragile watermarks along with overhead bits related to the cover image for tamper localization are embedded in different planes of the cover image. We have used discrete cosine transform coefficients and exploited their energy compaction property for robust watermark embedding. We make use of a four-point neighborhood to predict the value of a predefined pixel and use it for embedding the fragile watermark bits in the spatial domain. Chaotic and deoxyribonucleic acid encryption is used to encrypt the robust watermark before embedding to enhance its security. The results indicate that DWFCAT can withstand a range of hybrid signal processing and geometric attacks, such as Gaussian noise, salt and pepper, joint photographic experts group (JPEG) compression, rotation, low-pass filtering, resizing, cropping, sharpening, and histogram equalization. The experimental results prove that the DWFCAT is highly efficient compared with the various state-of-the-art approaches for authentication and tamper localization of industrial images.",https://ieeexplore.ieee.org/document/9214433/,IEEE Transactions on Industrial Informatics,July 2021,ieeexplore
10.1109/JSEN.2008.926923,Data Processing Method Applying Principal Component Analysis and Spectral Angle Mapper for Imaging Spectroscopic Sensors,IEEE,Journals,"A data processing method to classify hyperspectral images from an imaging spectroscopic sensor is evaluated. Each image contains the whole diffuse reflectance spectra of the analyzed material for all the spatial positions along a specific line of vision. The implemented linear algorithm comes to solve real time constrains typical of industrial systems. This processing method is composed of two blocks: data compression is performed by means of principal component analysis (PCA) and the spectral interpretation algorithm for classification is the spectral angle mapper (SAM). This strategy, applying PCA and SAM, has been successfully tested for online raw material sorting in the tobacco industry, where the desired raw material (tobacco leaves) should be discriminated from other unwanted spurious materials, such as plastic, cardboard, leather, feathers, candy paper, etc. Hyperspectral images are recorded by a sensor consisting of a monochromatic camera and a passive prism-grating-prism device. Performance results are compared with a spectral interpretation algorithm based on artificial neural networks (ANN).",https://ieeexplore.ieee.org/document/4567472/,IEEE Sensors Journal,July 2008,ieeexplore
10.1109/TII.2016.2516973,Data-Based Multiobjective Plant-Wide Performance Optimization of Industrial Processes Under Dynamic Environments,IEEE,Journals,"This paper provides a method for automatically selecting optimal operational indices for unit processes in an industrial plant using measured data and without knowing dynamical models of the unit process. A dynamic multiobjective optimization problem is defined to find operational indices that lead to plant-wide production indices close to their target values. A case-based reasoning (CBR) technique is also employed, which uses the stored experience of a human expert to determine appropriate operational indices for given target production indices. The solutions of the optimization problem and CBR technique are combined to form baseline operational indices. The dynamic models of the production indices, however, are time varying and affected by disturbances and online corrections of these baseline operational indices are required. To this end, reinforcement learning (RL) is used to provide a data-driven optimization technique to compensate for disturbances and model approximation errors and variations. The data-driven RL approach is used in two different time scales. The samples of the predicted production indices are used at a fast sampling rate, i.e., at each sample time, and the samples of actual production indices are used at a slower sampling rate, i.e., after each operational run, to correct the baseline operational indices. The effectiveness of this automated decision procedure has been demonstrated by successful implementation of the proposed approach on a large mineral processing plant in Gansu Province, China.",https://ieeexplore.ieee.org/document/7378488/,IEEE Transactions on Industrial Informatics,April 2016,ieeexplore
10.1109/ACCESS.2019.2894956,Data-Driven Dynamic Active Node Selection for Event Localization in IoT Applications - A Case Study of Radiation Localization,IEEE,Journals,"In this paper, the problem of active node selection for localization tasks, on the Internet of Things (IoT) sensing applications, is addressed. IoT plays a significant role in realizing the concept of smart environments, such as in environmental, infrastructural, industrial, disaster, or threat monitoring. Several IoT sensing nodes can be deployed within an area to collect regional information for the purpose of achieving a common contextual goal. Active node selection proves useful in mitigating common IoT-related issues like resource allocation, network lifetime, and the confidence in the collected data, by having the right sensors active at a given time. Current active node selection schemes prove inefficient when adapted to localization tasks, as they- 1) are usually designed for general monitoring, not localization, 2) do not dynamically exploit data readings in the selection process, and 3) are mostly designed for systems with nodes having sensing ranges. To address these challenges, we propose a novel Data-driven active node selection approach that- 1) dynamically uses data readings from current active nodes to select future ones, 2) assesses the area coverage achieved by a group of nodes while considering range-free sensors, 3) considers parameters like residual energy, power cost, and data confidence levels in the selection process, and 4) combines group-based and individual-based selection mechanisms to enhance the localization process in terms of time and power consumption. These considerations are integrated into a two-phase active node selection mechanism that uses genetic and greedy algorithms to select optimum groups for localization tasks. The efficacy of the proposed approach is validated through an example of radioactive source localization by using real-life and synthetic datasets, and by comparing the proposed approach to existing benchmarks. The results demonstrate the ability of the proposed approach to performing faster localization at low energy cost, even with a smaller number of active nodes.",https://ieeexplore.ieee.org/document/8625410/,IEEE Access,2019,ieeexplore
10.1109/ACCESS.2017.2764474,Data-Driven Inter-Turn Short Circuit Fault Detection in Induction Machines,IEEE,Journals,"Inter-turn short circuit (ITSC) fault is one of the critical electrical faults in induction motors that affects the reliability of many industrial applications. Although the use of data-driven fault detection techniques have gained much interest, the main deterrent in using these approaches in detecting ITSC faults is in the generalization and robustness of the diagnosis. In this paper, a data-driven on-line fault detection framework, incorporated with multi-feature extraction/selection and multi-classifier ensemble is proposed, capable of detecting ITSC faults in induction motors (IMs) that subjected to variable operating conditions. By using the synchronous time series signals collected from the machines, multiple feature extraction/selection is explored to find the sensitive faulty features, and the different types of classification strategies is used to increase the diversity of single based models. With the increased diversity of the base learners, the fault detection accuracy is expected to be enhanced and the robustness can be guaranteed. The framework was implemented and tested using real data collected from a designed test bed, with the experimental results showing the effectiveness of the framework in detecting ITSC faults in IMs.",https://ieeexplore.ieee.org/document/8081735/,IEEE Access,2017,ieeexplore
10.1109/TIE.2019.2927197,"Data-Driven Modeling Based on Two-Stream <inline-formula><tex-math notation=""LaTeX"">${\rm{\lambda }}$</tex-math></inline-formula> Gated Recurrent Unit Network With Soft Sensor Application",IEEE,Journals,"Data-driven soft sensors, estimating the pivotal quality variables, have been widely employed in industrial process. This paper proposes a novel soft sensor modeling approach based on a two-stream λ gated recurrent unit (T S - λ GRU) network. First, factors λ<sub>1</sub> and λ<sub>2</sub> are implemented to alter the linear constraint existing in the original GRU unit, enriching the information passing through. Then, a two-stream network structure is designed, equipped with some advanced network parameter adjustment techniques, such as batch normalization and dropout rate, to learn diverse features of the various process data. Finally, the learned features from the two streams are fused and a supervised learning regression layer is employed to decrease the error between the output and label. The application in melt viscosity index estimation for a real polymerization industrial process has demonstrated that the proposed T S - λ GRUs algorithm for soft sensor modeling is more accurate and promising than other existing methods.",https://ieeexplore.ieee.org/document/8763912/,IEEE Transactions on Industrial Electronics,Aug. 2020,ieeexplore
10.1109/ACCESS.2021.3101284,"Data-Driven Remaining Useful Life Estimation for Milling Process: Sensors, Algorithms, Datasets, and Future Directions",IEEE,Journals,"An increase in unplanned downtime of machines disrupts and degrades the industrial business, which results in substantial credibility damage and monetary loss. The cutting tool is a critical asset of the milling machine; the failure of the cutting tool causes a loss in industrial productivity due to unplanned downtime. In such cases, a proper predictive maintenance strategy by real-time health monitoring of cutting tools becomes essential. Accurately predicting the useful life of equipment plays a vital role in the predictive maintenance arena of industry 4.0. Many active research efforts have been done to estimate tool life in varied directions. However, the consolidated study of the implemented techniques and future pathways is still missing. So, the purpose of this paper is to provide a systematic and comprehensive literature survey on the data-driven approach of Remaining Useful Life (RUL) estimation of cutting tools during the milling process. The authors have summarized different monitoring techniques, feature extraction methods, decision-making models, and available sensors currently used in the data-driven model. The authors have also presented publicly available datasets related to milling under various operating conditions to compare the accuracy of the prediction model for tool wear estimation. Finally, the article concluded with the challenges, limitations, recent advancements in RUL prognostics techniques using Artificial Intelligence (AI), and future research scope to explore more in this area.",https://ieeexplore.ieee.org/document/9502093/,IEEE Access,2021,ieeexplore
10.1109/TII.2011.2158839,Decentralized Reconfiguration of a Flexible Transportation System,IEEE,Journals,"This paper presents a decentralized approach for the local reconfiguration of control software, which is based on a multiagent system with ontology-driven reasoning. We apply this approach to a transportation system and demonstrate improvements on efficiency, fault tolerance and stability with several experiments. One key element to achieve these results is the use of ontologies to ensure the consistency of local reconfiguration of the control software with the desired global behavior of the system. To show the feasibility of our approach in a realistic industrial setting, we implemented the multiagent system on the “Testbed for Distributed Holonic Control” at the Automation and Control Institute. We also used simulation to analyze its impact on the system performance. The simulation results as well as the real system experiments indicate that our approach is able to cope with the dynamic nature of the transportation domain thereby enhancing reconfigurability, robustness, and fault tolerance.",https://ieeexplore.ieee.org/document/5928365/,IEEE Transactions on Industrial Informatics,Aug. 2011,ieeexplore
10.1109/JIOT.2020.3011726,Deep Anomaly Detection for Time-Series Data in Industrial IoT: A Communication-Efficient On-Device Federated Learning Approach,IEEE,Journals,"Since edge device failures (i.e., anomalies) seriously affect the production of industrial products in Industrial IoT (IIoT), accurately and timely detecting anomalies are becoming increasingly important. Furthermore, data collected by the edge device contain massive user's private data, which is challenging current detection approaches as user privacy has attracted more and more public concerns. With this focus, this article proposes a new communication-efficient on-device federated learning (FL)-based deep anomaly detection framework for sensing time-series data in IIoT. Specifically, we first introduce an FL framework to enable decentralized edge devices to collaboratively train an anomaly detection model, which can improve its generalization ability. Second, we propose an attention mechanism-based convolutional neural network-long short-term memory (AMCNN-LSTM) model to accurately detect anomalies. The AMCNN-LSTM model uses attention mechanism-based convolutional neural network units to capture important fine-grained features, thereby preventing memory loss and gradient dispersion problems. Furthermore, this model retains the advantages of the long short-term memory unit in predicting time-series data. Third, to adapt the proposed framework to the timeliness of industrial anomaly detection, we propose a gradient compression mechanism based on Top- k selection to improve communication efficiency. Extensive experimental studies on four real-world data sets demonstrate that our framework accurately and timely detects anomalies and also reduces the communication overhead by 50% compared to the FL framework that does not use the gradient compression scheme.",https://ieeexplore.ieee.org/document/9146846/,IEEE Internet of Things Journal,"15 April15, 2021",ieeexplore
10.1109/TII.2018.2807797,Deep Endoscope: Intelligent Duct Inspection for the Avionic Industry,IEEE,Journals,"We present the first autonomous endoscope for the visual inspection of very small ducts and cavities, up to a 6-mm diameter. The system has been designed, implemented, and tested in a challenging industrial scenario and in strict collaboration with an avionic industry partner. The inspected objects are metallic gearboxes eventually presenting different residuals (e.g., sand, machining swarfs, and metallic dust) inside the oil ducts. The automatic system is actuated by a robotic arm that moves the endoscope with a microcamera inside the gearbox duct, while a deep-learning-based spatio-temporal image analysis module detects, classifies, and localizes defects in real time. Feedback is given to the robotic arm in order to move or extract the endoscope given the detected anomalies. Evaluation provides a detection rate of nearly 98% given different tests with different types of residuals and duct structures.",https://ieeexplore.ieee.org/document/8295126/,IEEE Transactions on Industrial Informatics,April 2018,ieeexplore
10.1109/ACCESS.2021.3072916,Deep Learning Anomaly Detection for Cellular IoT With Applications in Smart Logistics,IEEE,Journals,"The number of connected Internet of Things (IoT) devices within cyber-physical infrastructure systems grows at an increasing rate. This poses significant device management and security challenges to current IoT networks. Among several approaches to cope with these challenges, data-based methods rooted in deep learning (DL) are receiving an increased interest. In this paper, motivated by the upcoming surge of 5G IoT connectivity in industrial environments, we propose to integrate a DL-based anomaly detection (AD) as a service into the 3GPP mobile cellular IoT architecture. The proposed architecture embeds autoencoder based anomaly detection modules both at the IoT devices (ADM-EDGE) and in the mobile core network (ADM-FOG), thereby balancing between the system responsiveness and accuracy. We design, integrate, demonstrate and evaluate a testbed that implements the above service in a real-world deployment integrated within the 3GPP Narrow-Band IoT (NB-IoT) mobile operator network.",https://ieeexplore.ieee.org/document/9402912/,IEEE Access,2021,ieeexplore
10.1109/TII.2020.3038745,Deep-Learning–Based App Sensitive Behavior Surveillance for Android Powered Cyber–Physical Systems,IEEE,Journals,"Android as an operating system is now increasingly being adopted in industrial information systems, especially with cyber-physical systems (CPS). This also puts Android devices onto the front line of handling security-related data and conducting sensitive behaviors, which could be misused by the increasing number of polymorphic and metamorphic malicious applications targeting the platform. The existence of such malware threats, therefore, call for more accurate identification and surveillance of sensitive Android app behaviors, which is essential to the security of CPS and Internet of Things (IoT) devices powered by Android. Nevertheless, achieving dynamic app behavior monitoring and identification on real CPS powered by Android is challenging because of restrictions from the security and privacy model of the platform. In this article, the authors investigate how the latest advances in deep learning could address this security problem with better accuracy. Specifically, a deep learning engine is proposed that detects sensitive app behaviors by classifying patterns of system-wide statistics, such as available storage space and transmitted packet volume, using a customized deep neural network based on existing models called Encoder and ResNet. Meanwhile, to handle resource limitations on typical CPS and IoT devices, sparse learning is adopted to reduce the amount of valid parameters in the trained neural network. Evaluations show that the proposed model outperforms a well-established group of baselines on time series classification in identifying sensitive app behaviors with background noise and the targeted behaviors potentially overlapping.",https://ieeexplore.ieee.org/document/9262070/,IEEE Transactions on Industrial Informatics,Aug. 2021,ieeexplore
10.1109/TII.2020.3023430,DeepFed: Federated Deep Learning for Intrusion Detection in Industrial Cyber–Physical Systems,IEEE,Journals,"The rapid convergence of legacy industrial infrastructures with intelligent networking and computing technologies (e.g., 5G, software-defined networking, and artificial intelligence), have dramatically increased the attack surface of industrial cyber-physical systems (CPSs). However, withstanding cyber threats to such large-scale, complex, and heterogeneous industrial CPSs has been extremely challenging, due to the insufficiency of high-quality attack examples. In this article, we propose a novel federated deep learning scheme, named DeepFed, to detect cyber threats against industrial CPSs. Specifically, we first design a new deep learning-based intrusion detection model for industrial CPSs, by making use of a convolutional neural network and a gated recurrent unit. Second, we develop a federated learning framework, allowing multiple industrial CPSs to collectively build a comprehensive intrusion detection model in a privacy-preserving way. Further, a Paillier cryptosystem-based secure communication protocol is crafted to preserve the security and privacy of model parameters through the training process. Extensive experiments on a real industrial CPS dataset demonstrate the high effectiveness of the proposed DeepFed scheme in detecting various types of cyber threats to industrial CPSs and the superiorities over state-of-the-art schemes.",https://ieeexplore.ieee.org/document/9195012/,IEEE Transactions on Industrial Informatics,Aug. 2021,ieeexplore
10.1109/TII.2019.2941244,DeepPAR and DeepDPA: Privacy Preserving and Asynchronous Deep Learning for Industrial IoT,IEEE,Journals,"Industrial Internet of Things (IIoT) is significant of building powerful industrial systems and applications. Deep learning has provided a promising opportunity to extract useful knowledge by utilizing vast amounts of data in IIoT. However, lacking of massive public datasets will lead to low performance and overfitting of the learned model. Therefore, the federated deep learning over distributed datasets has been proposed. Whereas, it inevitably introduces some new security challenges, i.e., disclosing participant's data privacy. However, existing methods cannot guarantee each participant's data privacy in a learning group. In this article, we propose two privacy-preserving asynchronous deep learning schemes [privacy-preserving and asynchronous deep learning via re-encryption (DeepPAR) and dynamic privacy-preserving and asynchronous deep learning (DeepDPA)]. Compared to the state-of-the-art work, DeepPAR protects each participant's input privacy while preserving dynamic update secrecy inherently. Meanwhile, DeepDPA enables to guarantee backward secrecy of group participants in a lightweight manner. Security analysis and performance evaluations on real dataset show that our proposed schemes are secure, efficient and effective.",https://ieeexplore.ieee.org/document/8836609/,IEEE Transactions on Industrial Informatics,March 2020,ieeexplore
10.1109/ACCESS.2019.2924030,Demand Response Management for Industrial Facilities: A Deep Reinforcement Learning Approach,IEEE,Journals,"As a major consumer of energy, the industrial sector must assume the responsibility for improving energy efficiency and reducing carbon emissions. However, most existing studies on industrial energy management are suffering from modeling complex industrial processes. To address this issue, a model-free demand response (DR) scheme for industrial facilities was developed. In practical terms, we first formulated the Markov decision process (MDP) for industrial DR, which presents the composition of the state, action, and reward function in detail. Then, we designed an actor-critic-based deep reinforcement learning algorithm to determine the optimal energy management policy, where both the actor (Policy) and the critic (Value function) are implemented by the deep neural network. We then confirmed the validity of our scheme by applying it to a real-world industry. Our algorithm identified an optimal energy consumption schedule, reducing energy costs without compromising production.",https://ieeexplore.ieee.org/document/8742652/,IEEE Access,2019,ieeexplore
10.1109/JSEN.2020.3041668,Design of a Pose and Force Controller for a Robotized Ultrasonic Probe Based on Neural Networks and Stochastic Gradient Approximation,IEEE,Journals,"In medicine and engineering, the implementation of a diagnostic test using an ultrasonic sensor requires suitable contact conditions, and a correct pose to attain the best signal transmission settings. A soft sensor probe provides a good surface adaptation and forces transfer, but it introduces nonlinearities and noisy measurements, making it difficult to control the probe during a real time test by conventional algorithms. In this work, a data driven controller is developed to control force and pose of a soft contact ultrasound sensor. The adaptive controller is based on a fuzzy-rules emulated network structure with the learning algorithm using a stochastic gradient approximation. The proposed control algorithm overcomes the noise environment conditions and nonlinearities of the unknown nonlinear discrete-time system. This was numerically validated and then, experimentally tested with an industrial robotic system using an ultrasonic probe designed in our lab. The results show that the proposed controller performs well under the contact-force regulation and can find the correct contact orientation with a fast convergence.",https://ieeexplore.ieee.org/document/9274482/,IEEE Sensors Journal,"1 March1, 2021",ieeexplore
10.1109/TITS.2020.3018259,Detecting Anomalies in Intelligent Vehicle Charging and Station Power Supply Systems With Multi-Head Attention Models,IEEE,Journals,"Safe and reliable intelligent charging stations are imperative in an intelligent transportation infrastructure. Over the past few years, a big number of smart charging stations have been deployed, and most of them are online and connected, resulting in potential risks of threats. Although there exists related work on securing intelligent vehicles, very little work focused on the security of charging devices. Unlike traditional network systems, these power-related Industrial Control Systems (ICSs) use many different proprietary protocols and diverse interactions. Traditional anomaly detection methods based on network traffic are thus not suitable for these systems. In this work, we propose an anomaly detection method in real vehicle power supply systems based on a deep architecture model. In particular, we propose a novel traffic anomaly detection model based on Multi-Head Attentions (MHA) that take into account the inherent correlations of traffic generated by ICSs. The MHA model is employed to substitute the traditional feature extraction and rule making process with an acceptable computational cost for classifying traffic data. It is an attention-based model that employs Google Transformer encoder architecture to extract recessive features of traffic for anomaly detection. The effectiveness of the model is demonstrated by experiments on two real-world power ICS testbeds including a substation with a slave charging station and a power generation simulation platform based on a distributed control system. Comprehensive experimental results indicate that the MHA model outperforms the Convolutional Neural Networks (CNN)-based and classical machine learning detection models with an accuracy rate of 99.86%.",https://ieeexplore.ieee.org/document/9184272/,IEEE Transactions on Intelligent Transportation Systems,Jan. 2021,ieeexplore
10.1109/TLA.2018.8447349,Detection of Anomalies Related to the Operation of the Profinet Network Through Feature Extraction and Classification,IEEE,Journals,"PROFINET networks are increasingly being applied in industrial environments. Due to its expansion, it is common for new diagnostic tools to emerge dedicated to protocol. Being more specific, it would be of the most importance if, by means of network traffic analysis, it was possible to identify the network's operating status (normal operation or with some anomaly) automatically. In view of the facts, this article aims to propose a methodology for classifying events related to the operation of the PROFINET network through feature extraction and classification with Artificial Neural Networks. All the data are real and they were collected in an industrial plant.",https://ieeexplore.ieee.org/document/8447349/,IEEE Latin America Transactions,July 2018,ieeexplore
10.1109/72.641456,Development and application of an integrated neural system for an HDCL,IEEE,Journals,"This study presents the development and industrial application of an integrated neural system in coating weight control for a modern hot dip coating line (HDCL) in a steel mill. The neural system consists of two multilayered feedforward neural networks and a neural adaptive controller. They perform coating weight real-time prediction, feedforward control (FFC), and adaptive feedback control (FBC), respectively. The production line analysis, neural system architecture, learning, associative memories, generalization and real-time applications are addressed in this paper. This integrated neural system has been successfully implemented and applied to an HDCL at Burns Harbor Division, Bethlehem Steel Co., Chesterton, IN. The industrial application results have shown significant improvements in reduction of coating weight transitional footage, variation of the error between the target and actual coating weight, and the coating material used. Some practical aspects for applying a neural system to industrial control are discussed as concluding remarks.",https://ieeexplore.ieee.org/document/641456/,IEEE Transactions on Neural Networks,Nov. 1997,ieeexplore
10.1109/TIA.2007.900472,Development of a Self-Tuned Neuro-Fuzzy Controller for Induction Motor Drives,IEEE,Journals,"In this paper, a novel adaptive neuro-fuzzy (NF)-based speed control of an induction motor (IM) is presented. The proposed NF controller (NFC) incorporates fuzzy logic laws with a five-layer artificial neural network scheme. In this controller, only three membership functions are used for each input for low computational burden, which will be suitable for real-time implementation. Furthermore, for the proposed NFC, an improved self-tuning method is developed based on the knowledge of intelligent algorithms and high-performance requirements of motor drives. The main task of the tuning method is to adjust the parameters of the fuzzy logic controller (FLC) in order to minimize the square of the error between actual and reference outputs. A complete model for indirect field-oriented control of IM incorporating the proposed NFC is developed. The performance of the proposed NFC-based IM drive is investigated extensively both in simulation and in experiment at different operating conditions. In order to prove the superiority of the proposed NFC, the results for the proposed controller are also compared to those obtained by conventional proportional-integral (PI) and FLC controllers. The proposed NFC-based IM drive is found to be more robust as compared to conventional PI and FLC controllers and, hence, suitable for high-performance industrial drive applications.",https://ieeexplore.ieee.org/document/4276869/,IEEE Transactions on Industry Applications,July-aug. 2007,ieeexplore
10.1109/TSMC.2019.2933161,Device-Free Orientation Detection Based on CSI and Visibility Graph,IEEE,Journals,"Nonintrusive orientation detection is an important yet largely unaddressed area. It can be used in many important applications, e.g., interactive games, medical care, and various industrial scenarios. Moreover, our novel techniques can be readily implemented in other critical areas, such as indoor localization, objective tracking, movement detection, etc. Many factors can limit the performance of detection algorithms in real-word applications, an important one of which is the negligence of subcarrier correlations. Therefore, we propose to build our system based on existing WiFi infrastructure and its channel state information (CSI). To explore the correlations of adjacent subcarriers, we apply the visibility graph (VG)-based network analysis method to process the CSI data. Specifically, in this article we make the following contributions: 1) we design a CSI-based orientation detection system; 2) we model the correlations between subcarriers using complex network and propose a VG-based feature extraction technique; and 3) we demonstrate the performance and effectiveness of our system with commercial products in real-world deployments. The experimental results show that our technique can achieve more than 98% accuracy and at least 26% better than the baseline approaches.",https://ieeexplore.ieee.org/document/8828088/,"IEEE Transactions on Systems, Man, and Cybernetics: Systems",July 2021,ieeexplore
10.1109/TII.2021.3067915,Diagnosis of Interturn Short-Circuit Faults in Permanent Magnet Synchronous Motors Based on Few-Shot Learning Under a Federated Learning Framework,IEEE,Journals,"A large amount of labeled data are important to enhance the performance of deep-learning-based methods in the area of fault diagnosis. Because it is difficult to obtain high-quality samples in real industrial applications, federated learning is an effective framework for solving the problem of sparse samples by using the distributed data. Its global model is updated by the local client without sharing data at each round. Considering computing resources and communication loss of multiple clients, an efficient method based on stacked sparse autoencoders (SSAEs) and Siamese networks is proposed to detect interturn short-circuit (ITSC) faults in permanent magnet synchronous motors. In this article, to achieve an accurate ITSC fault detection, an SSAE was employed to extract sparse features in a limited number of samples, and Siamese networks were used to determine the similarity between the given samples. The problem of fault diagnosis is transformed into a classification problem under few-shot learning. Furthermore, the proposed method is trained successfully in the frameworks of centralized learning and decentralized structure. The experimental results indicate that the proposed method achieved high fault diagnosis accuracy. Moreover, it is suitable for deployment in smart manufacturing systems.",https://ieeexplore.ieee.org/document/9384245/,IEEE Transactions on Industrial Informatics,Dec. 2021,ieeexplore
10.1109/ACCESS.2020.2998723,"Digital Twin for the Oil and Gas Industry: Overview, Research Trends, Opportunities, and Challenges",IEEE,Journals,"With the emergence of industry 4.0, the oil and gas (O&amp;G) industry is now considering a range of digital technologies to enhance productivity, efficiency, and safety of their operations while minimizing capital and operating costs, health and environment risks, and variability in the O&amp;G project life cycles. The deployment of emerging technologies allows O&amp;G companies to construct digital twins (DT) of their assets. Considering DT adoption, the O&amp;G industry is still at an early stage with implementations limited to isolated and selective applications instead of industry-wide implementation, limiting the benefits from DT implementation. To gain the full potential of DT and related technological adoption, a comprehensive understanding of DT technology, the current status of O&amp;G-related DT research activities, and the opportunities and challenges associated with the deployment of DT in the O&amp;G industry are of paramount importance. In order to develop this understanding, this paper presents a literature review of DT within the context of the O&amp;G industry. The paper follows a systematic approach to select articles for the literature review. First, a keywords-based publication search was performed on the scientific databases such as Elsevier, IEEE Xplore, OnePetro, Scopus, and Springer. The filtered articles were then analyzed using online text analytic software (Voyant Tools) followed by a manual review of the abstract, introduction and conclusion sections to select the most relevant articles for our study. These articles and the industrial publications cited by them were thoroughly reviewed to present a comprehensive overview of DT technology and to identify current research status, opportunities and challenges of DT deployment in the O&amp;G industry. From this literature review, it was found that asset integrity monitoring, project planning, and life cycle management are the key application areas of digital twin in the O&amp;G industry while cyber security, lack of standardization, and uncertainty in scope and focus are the key challenges of DT deployment in the O&amp;G industry. When considering the geographical distribution for the DT related research in the O&amp;G industry, the United States (US) is the leading country, followed by Norway, United Kingdom (UK), Canada, China, Italy, Netherland, Brazil, Germany, and Saudi Arabia. The overall publication rate was less than ten articles (approximately) per year until 2017, and a significant increase occurred in 2018 and 2019. The number of journal publications was noticeably lower than the number of conference publications, and the majority of the publications presented theoretical concepts rather than the industrial implementations. Both these observations suggest that the DT implementation in the O&amp;G industry is still at an early stage.",https://ieeexplore.ieee.org/document/9104682/,IEEE Access,2020,ieeexplore
10.1109/ACCESS.2019.2928141,Digital Twin-Based Optimization for Ultraprecision Motion Systems With Backlash and Friction,IEEE,Journals,"A digital twin-based optimization procedure is presented for an ultraprecision motion system with a flexible shaft connecting the motor to the (elastic) load, which is subject to both backlash and friction. The main contributions of the study are the design of the digital twin and its implementation, assuming a two-mass drive system. The procedure includes the virtual representation of mechanical and electrical components, non-linearities (backlash and friction), and the corresponding control system. A procedure for digital twin-based optimization is also presented, in which the maximum absolute position error is minimized while maintaining accuracy with no significant increase in the control effort. The optimal settings for the controller parameters and for the backlash peak amplitude, the backlash peak time, and the hysteresis amplitude are then determined, in order to guarantee an appropriate dynamic response in the presence of backlash and friction. The surface quality of certain manufactured components, such as hip and knee implants, depends on the smoothness and the accuracy of the real trajectory produced in the cutting process that is strongly influenced by the maximum position error. The simulations and experimental studies are presented using a real platform and two references for trajectory control, and a comparison of four digital twin-based optimization methods. The simulation study and the real-time experiments demonstrate the suitability of the digital twin-based optimization procedure and lay the foundations for the implementation of the proposed method at an industrial level.",https://ieeexplore.ieee.org/document/8759853/,IEEE Access,2019,ieeexplore
10.1109/ACCESS.2021.3120843,Digital Twins From Smart Manufacturing to Smart Cities: A Survey,IEEE,Journals,"Digital twins are quickly becoming a popular tool in several domains, taking advantage of recent advancements in the Internet of Things, Machine Learning and Big Data, while being used by both the industry sector and the research community. In this paper, we review the current research landscape as regards digital twins in the field of smart cities, while also attempting to draw parallels with the application of digital twins in Industry 4.0. Although digital twins have received considerable attention in the Industrial Internet of Things domain, their utilization in smart cities has not been as popular thus far. We discuss here the open challenges in the field and argue that digital twins in smart cities should be treated differently and be considered as cyber-physical “systems of systems”, due to the vastly different system size, complexity and requirements, when compared to other recent applications of digital twins. We also argue that researchers should utilize established tools and methods of the smart city community, such as co-creation, to better handle the specificities of this domain in practice.",https://ieeexplore.ieee.org/document/9576739/,IEEE Access,2021,ieeexplore
10.1109/ACCESS.2021.3079447,ECT-LSTM-RNN: An Electrical Capacitance Tomography Model-Based Long Short-Term Memory Recurrent Neural Networks for Conductive Materials,IEEE,Journals,"Image reconstruction for industrial applications based on Electrical Capacitance Tomography (ECT) has been broadly applied. The goal of image reconstruction based ECT is to locate the distribution of permittivity for the dielectric substances along the cross-section based on the collected capacitance data. In the ECT-based image reconstruction process: (1) the relationship between capacitance measurements and permittivity distribution is nonlinear, (2) the capacitance measurements collected during image reconstruction are inadequate due to the limited number of electrodes, and (3) the reconstruction process is subject to noise leading to an ill-posed problem. Thence, constructing an accurate algorithm for real images is critical to overcoming such restrictions. This paper presents novel image reconstruction methods using Deep Learning for solving the forward and inverse problems of the ECT system for generating high-quality images of conductive materials in the Lost Foam Casting (LFC) process. Here, Long Short-Term Memory Recurrent Neural Network (LSTM-RNN) models were implemented to predict the distribution of metal filling for the LFC process-based ECT. The recurrent connection and the gating mechanism of the LSTM is capable of extracting the contextual information that is repeatedly passing through the neural network while filtering out the noise caused by adverse factors. Experimental results showed that the presented ECT-LSTM-RNN model is highly reliable for industrial applications and can be utilized for other manufacturing processes.",https://ieeexplore.ieee.org/document/9429218/,IEEE Access,2021,ieeexplore
10.1109/TII.2020.3020386,Edge Computing-Enabled Deep Learning for Real-time Video Optimization in IIoT,IEEE,Journals,"Real-time multimedia applications have gained immense popularity in the industrial Internet of Things (IIoT) paradigm. Due to the impact of the complex industrial environment, the transmission of video streaming is usually unstable. In the duration of a low bandwidth transmission, existing optimization methods often reduce the original resolution of some frames in a random way to avoid the video interruption. If the key frames with some important content are selected to be transmitted with a low resolution, it will greatly reduce the effect of industrial supervision. In view of this challenge, a real-time video streaming optimization method by reducing the number of video frames transmitted in the IIoT environment is proposed. Concretely, a deep learning-based object detection algorithm is recruited to effectively select the key frames in our method. The key frames with the original resolution will be transmitted along with audio data. As some nonkey frames are selectively discarded, it is helpful for smooth network transmitting with fewer bandwidth requirements. Moreover, we employ edge servers to run the object detection algorithm, and adjust video transmission flexibly. Extensive experiments are conducted to validate the effectiveness, and dependability of our method.",https://ieeexplore.ieee.org/document/9181434/,IEEE Transactions on Industrial Informatics,April 2021,ieeexplore
10.1109/TSE.2013.2295827,Effects of Developer Experience on Learning and Applying Unit Test-Driven Development,IEEE,Journals,"Unit test-driven development (UTDD) is a software development practice where unit test cases are specified iteratively and incrementally before production code. In the last years, researchers have conducted several studies within academia and industry on the effectiveness of this software development practice. They have investigated its utility as compared to other development techniques, focusing mainly on code quality and productivity. This quasi-experiment analyzes the influence of the developers' experience level on the ability to learn and apply UTDD. The ability to apply UTDD is measured in terms of process conformance and development time. From the research point of view, our goal is to evaluate how difficult is learning UTDD by professionals without any prior experience in this technique. From the industrial point of view, the goal is to evaluate the possibility of using this software development practice as an effective solution to take into account in real projects. Our results suggest that skilled developers are able to quickly learn the UTDD concepts and, after practicing them for a short while, become as effective in performing small programming tasks as compared to more traditional test-last development techniques. Junior programmers differ only in their ability to discover the best design, and this translates into a performance penalty since they need to revise their design choices more frequently than senior programmers.",https://ieeexplore.ieee.org/document/6690135/,IEEE Transactions on Software Engineering,April 2014,ieeexplore
10.1109/ACCESS.2021.3111783,Emerging Tools for Link Adaptation on 5G NR and Beyond: Challenges and Opportunities,IEEE,Journals,"With the speeding up of the fifth generation (5G) new radio (NR) worldwide commercialization, one of the paramount questions for operators and vendors is how to optimize the radio links, considering the widely diverse scenarios envisioned. One of the key pillars of 5G has been an unprecedented flexibility on the configuration of the radio access network (RAN) on scenarios that include cellular, vehicular, and industrial networks among others. This flexibility has its main exponent on link adaptation (LA), which has evolved into a multi-domain technique where a plethora of parameters, like numerology, bandwidth part, radio frequency beam, power, modulation and coding scheme (MCS) or multiple antenna precoding can be adapted to the instantaneous link conditions. Although such enhancements open the door to a significant performance improvement, they also pose many challenges to LA optimization. In this article, we first present the signaling aspects of NR technology for multi-domain LA and the challenges that need to be faced. Then, we explore the latest advances on LA for wireless networks. We envision a combination of machine learning (ML) tools with multi-domain LA as a key enabler for 5G and beyond networks. Finally, we investigate emerging ML approaches for LA and present a promising application of ML for LA that is assessed with simulations. With this scheme, the training is performed at the network side to relieve the user equipment (UE) to do such a complex task. It is shown with simulations that our ML approach outperforms the well-known outer loop link adaptation (OLLA) algorithm in terms of instantaneous block error rate (BLER), while reaching the same average spectral efficiency (SE). Interestingly, it is shown that the proposed scheme only requires 4 bits to represent the features used to train the model, which makes it suitable for implementation in real systems with limited feedback.",https://ieeexplore.ieee.org/document/9534770/,IEEE Access,2021,ieeexplore
10.1109/TII.2021.3049405,Enabling Secure Authentication in Industrial IoT With Transfer Learning Empowered Blockchain,IEEE,Journals,"Industrial Internet of Things (IIoT) is ushering in huge development opportunities in the era of Industry 4.0. However, there are significant data security and privacy challenges during automatic and real-time data collection, monitoring for industrial applications in IIoT. Data security and privacy in IIoT applications are closely related to the reliability of users, which is determined by user authentication that have been widely used as an effective approach. However, the existing user authentication mechanisms in IIoT suffer from single factor authentication and poor adaptability with the rapid growth of the number of users and the diversity of user categories. To solve the aforementioned issues, this article proposes a novel Authentication mechanism based on Transfer Learning empowered Blockchain, coined ATLB. In ATLB, blockchains are applied to achieve the privacy preservation for industrial applications. In addition, by introducing the transfer learning based authentication mechanism, trustworthy blockchains are built such that the privacy preservation for industrial applications is further enhanced. Specifically, ATLB first employs a guiding deep deterministic policy gradient algorithm to train the user authentication model of a specific region, which is then transferred locally for foreign user authentication or cross-regionally for another region's user authentication such that the model training time is significantly reduced. Experimental results show that the proposed ATLB not only provides accurate authentications for IIoT applications but also achieves high throughput and low latency.",https://ieeexplore.ieee.org/document/9314211/,IEEE Transactions on Industrial Informatics,Nov. 2021,ieeexplore
10.1109/JSTARS.2015.2442584,Estimation of Seismic Vulnerability Levels of Urban Structures With Multisensor Remote Sensing,IEEE,Journals,"The ongoing global transformation of human habitats from rural villages to ever growing urban agglomerations induces unprecedented seismic risks in earthquake prone regions. To mitigate affiliated perils requires the seismic assessment of built environments. Numerous studies emphasize that remote sensing can play a valuable role in supporting the extraction of relevant features for preevent vulnerability analysis. However, the majority of approaches operate on building level. This induces the deployment of very high spatial resolution remote sensing data, which hampers, nowadays, utilization capabilities for larger areas due to data costs and processing requirements. In this paper, we alter the spatial scale of analysis and propose concepts and methods to estimate the seismic vulnerability level of homogeneous urban structures. A procedure is designed, which comprises four main steps dedicated to: 1) delineation of urban structures by means of a tailored unsupervised data segmentation procedure with scale optimization; 2) characterization of urban structures by a joint exploitation of multisensor data; 3) selection of most feasible features under consideration of in situ vulnerability information; and 4) estimation of seismic vulnerability levels of urban structures within a supervised learning framework. We render the prediction problem in three ways to address operational requirements that can evolve in real-life situations. 1) To discriminate two or more classes based on labeled samples of all classes present in the data under investigation, we use the framework of soft margin support vector machines (C-SVM). 2) To consider situations, where solely labeled samples are available for the class(es) of interest and not for all classes present in the data, we deploy ensembles of ν-one-class SVM (ν-OC-SVM). and 3) To fit data with a higher statistical level of measurement (interval or ratio scale), we utilize a support vector regression (SVR) approach to estimate a regression function from the training samples. Experimental results are obtained for the earthquake-prone mega city Istanbul, Turkey. We use multispectral data from the RapidEye constellation, elevation measurements from the TanDEM-X mission, and spatiotemporal analyses based on data from the Landsat archive to characterize the urban environment. In addition, different in situ data sets are incorporated for Istanbul's district Zeytinburnu and the residual settlement area of Istanbul. When estimating damage grades for Zeytinburnu with SVR, best models are characterized by mean absolute percentage errors less than 11%, and fairly strong goodness of fit (R &gt; 0.75). When aiming to identify different types of urban structures for the remaining settlement area of Istanbul (i.e., urban structures determined by large industrial/commercial buildings and tall detached residential buildings, which can be considered here as highly and slightly vulnerable, respectively), results obtained with C-SVM show a distinctive increase of accuracy compared to results obtained with ensembles of ν-OC-SVM. The latter were not able to exceed moderate agreements, with κ statistics slightly above 0.45. Instead, C-SVM allowed obtaining ν statistics expressing substantial and even excellent agreements (κ &gt; 0.6 up to κ &gt; 0.8). Overall, analyzes provide very promising empirical evidence, which confirms the potential of remote sensing to support seismic vulnerability assessment.",https://ieeexplore.ieee.org/document/7150321/,IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing,May 2016,ieeexplore
10.1109/ACCESS.2015.2490723,Evaluating the Quality of Social Media Data in Big Data Architecture,IEEE,Journals,"The use of freely available online data is rapidly increasing, as companies have detected the possibilities and the value of these data in their businesses. In particular, data from social media are seen as interesting as they can, when properly treated, assist in achieving customer insight into business decision making. However, the unstructured and uncertain nature of this kind of big data presents a new kind of challenge: how to evaluate the quality of data and manage the value of data within a big data architecture? This paper contributes to addressing this challenge by introducing a new architectural solution to evaluate and manage the quality of social media data in each processing phase of the big data pipeline. The proposed solution improves business decision making by providing real-time, validated data for the user. The solution is validated with an industrial case example, in which the customer insight is extracted from social media data in order to determine the customer satisfaction regarding the quality of a product.",https://ieeexplore.ieee.org/document/7299603/,IEEE Access,2015,ieeexplore
10.1109/TII.2018.2850001,Event-Triggered Globalized Dual Heuristic Programming and Its Application to Networked Control Systems,IEEE,Journals,"Networked control systems (NCSs) provide many benefits, such as higher control accuracy and better robustness with the successively increasing computational complexity and communication burden. This results in the traditional adaptive dynamic programming control method having difficulty meeting the real-time requirements of industrial systems. In this paper, a novel event-triggered globalized dual heuristic programming method is proposed to reduce the required samples while guaranteeing the stability of the system. In the proposed method, the NCSs can communicate and update the control law only when the designed event-triggered condition is violated. Furthermore, the Elman neural network, which is a dynamic feedback network with a memory function is implemented to reconstruct the state variables as an approximator, and it depends only on the input and output data. To obtain fewer event-triggered times, two optimization methods, i.e., the unscented Kalman filter and the multiobjective quantum particle swarm optimization, are used to optimize the initial weights of the networks and the positive constant in the event-triggered condition, respectively. The simulation results on industrial system of aluminum electrolysis production are included to verify the performance of the controller.",https://ieeexplore.ieee.org/document/8395070/,IEEE Transactions on Industrial Informatics,March 2019,ieeexplore
10.1109/TCAD.2020.3012648,Exploring Edge Computing for Multitier Industrial Control,IEEE,Journals,"Industrial automation traditionally relies on local controllers implemented on microcontrollers or programmable logic controllers. With the emergence of edge computing, however, industrial automation evolves into a distributed two-tier computing architecture comprising local controllers and edge servers that communicate over wireless networks. Compared to local controllers, edge servers provide larger computing capacity at the cost of data loss over wireless networks. This article presents switching multitier control (SMC) to exploit edge computing for industrial control. SMC dynamically optimizes control performance by switching between local and edge controllers in response to changing network conditions. SMC employs a data-driven approach to derive switching policies based on classification models trained based on simulations while guaranteeing system stability based on an extended Simplex approach tailored for two-tier platforms. To evaluate the performance of industrial control over edge computing platforms, we have developed WCPS-EC, a real-time hybrid simulator that integrates simulated plants, real computing platforms, and real or simulated wireless networks. In a case study of an industrial robotic control system, SMC significantly outperformed both a local controller and an edge controller in face of varying data loss in a wireless network.",https://ieeexplore.ieee.org/document/9211472/,IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems,Nov. 2020,ieeexplore
10.1109/TII.2020.2984370,FADN: Fully Connected Attitude Detection Network Based on Industrial Video,IEEE,Journals,"In 3-D attitude angle estimation, monocular vision-based methods are often utilized for the advantages of short-time and high efficiency. However, the limitations of these methods lie in the complexity of the algorithm and the specificity of the scene, which needs to match the characteristics of the cooperation object and the scene. In this article, we propose a fully connected attitude detection network (FADN), which combines neural network and traditional algorithms for 3-D attitude angle estimation. FADN provides a whole process from the input of a single frame image in the industrial video stream to the output of the corresponding 3-D attitude angle estimation. Benefiting from the end-to-end estimation framework, FADN avoids tedious matching algorithms and thus has certain portability. A series of comparative experiments based on the rendering software 3-D Studio Max (3d Max) have been carried out to evaluate the performance of FADN. The experimental results show that FADN has high estimation accuracy and fast running speed. At the same time, the simulation results reliably prove the feasibility of FADN, and also promote the research in real scenarios.",https://ieeexplore.ieee.org/document/9055217/,IEEE Transactions on Industrial Informatics,March 2021,ieeexplore
10.1109/TII.2020.2988208,Fault Description Based Attribute Transfer for Zero-Sample Industrial Fault Diagnosis,IEEE,Journals,"In this article, a challenging fault diagnosis task is studied, in which no samples of the target faults are available for the model training. This scenario has hardly been studied in industrial research. But it is a common problem that massive fault samples are not available for the target faults, which limits the successes of conventional data-driven approaches in practical application. Here, we introduce the idea of zero-shot learning into the industry field, and tackle the zero-sample fault diagnosis task by proposing the fault description based attribute transfer method. Specifically, the method learns to determine the fault categories using the human-defined fault descriptions instead of the collected fault samples.The defined description consists of arbitrary attributes of the faults, including the fault positions, the consequences of the fault, and even the cause of the fault, etc. For the attribute knowledge of target faults, they can be prelearned and transferred from some readily available faults occurred in the same process. Afterwards, the target faults can be diagnosed based on the defined fault descriptions without the need for any additional data based training. Besides, the supervised principle component analysis is adopted in our method to extract the attribute related features to offer an effective attribute learning. We analyze and interpret the feasibility of the fault description based method theoretically. Also, the zero-sample fault diagnosis experiments are designed and conducted on the benchmark Tennessee-Eastman process and the real thermal power plant process to validate the effectiveness. The results show that it is indeed possible to diagnose target faults without their samples.",https://ieeexplore.ieee.org/document/9072621/,IEEE Transactions on Industrial Informatics,March 2021,ieeexplore
10.1109/TIM.2021.3089240,Fault Diagnosis of Harmonic Drive With Imbalanced Data Using Generative Adversarial Network,IEEE,Journals,"Harmonic drive is the core component of the industrial robot, and its fault diagnosis is crucial to the reliability and performance of the equipment. Most machine learning methods achieve good results based on the assumption of data balance. However, the scarce fault data of harmonic drive is difficult to collect, resulting in various imbalanced health status samples, which has an adverse effect on fault diagnosis. In this article, we propose a data generation method based on generative adversarial networks (GANs) to solve the problem of data imbalance and utilize the multiscale convolutional neural network (MSCNN) to realize the fault diagnosis of the harmonic drive. First, the data collected from three vibration acceleration sensors are preprocessed by fast Fourier transform (FFT) to obtain the frequency spectrum of the vibration signal. Second, multiple GANs were adopted to generate various fault spectrum data and the data selection module (DSM) is elaborately designed to filter and purify these data. Third, the filtered generated data will be combined with the real data to form a balanced dataset, and then the MSCNN is used to achieve multiclassification of the health status of the harmonic drive. Finally, the experiments have been implemented on an industrial robot vibration test bench to validate the effectiveness of our approach. The results have shown the fault multiclassification accuracy as 98.49% under imbalanced fault data conditions, which outperforms that of the other compared methods.",https://ieeexplore.ieee.org/document/9454583/,IEEE Transactions on Instrumentation and Measurement,2021,ieeexplore
10.1109/TII.2019.2937876,Federated Tensor Mining for Secure Industrial Internet of Things,IEEE,Journals,"In a vertical industry alliance, Internet of Things (IoT) deployed in different smart factories are similar. For example, most automobile manufacturers have the similar assembly lines and IoT surveillance systems. It is common to observe the industrial knowledge using deep learning and data mining methods based on the IoT data. However, some knowledge is not easy to be mined from only one factory's data because the samples are still few. If multiple factories within an alliance can gather their data together, more knowledge could be mined. However, the key concern of these factories is the data security. Existing matrix-based methods can guarantee the data security inside a factory but do not allow the data sharing among factories, and thus their mining performance is poor due to lack of correlation. To address this concern, in this article we propose the novel federated tensor mining (FTM) framework to federate multisource data together for tensor-based mining while guaranteeing the security. The key contribution of FTM is that every factory only needs to share its ciphertext data for security issue, and these ciphertexts are adequate for tensor-based knowledge mining due to its homomorphic attribution. Real-data-driven simulations demonstrate that FTM not only mines the same knowledge compared with the plaintext mining, but also is enabled to defend the attacks from distributed eavesdroppers and centralized hackers. In our typical experiment, compared with the matrix-based privacy-preserving compressive sensing (PPCS), FTM increases up to 24% on mining accuracy.",https://ieeexplore.ieee.org/document/8815886/,IEEE Transactions on Industrial Informatics,March 2020,ieeexplore
10.1109/TCAD.2018.2884992,GPGPU-Based ATPG System: Myth or Reality?,IEEE,Journals,"General-purpose computing on graphics processing units (GPGPUs) is a programming model that uses graphics cards to perform computations traditionally done by CPU. It began to become practical with the advent of programmable shaders and floating-point support on GPU in around 2001. The spread of GPGPU has been accelerated with introduction of CUDA from NVIDIA in 2006 and later OpenCL in 2009. Nowadays GPGPU is widely deployed in various applications, such as data mining, artificial intelligence, and many scientific computations. GPGPU seemingly promises immense parallelism with massive concurrent cores, and thus much shorter run times. This is true for algorithms that bear intrinsic data and task parallelism, such as image and video processing. For an ATPG system where some algorithms are sequential in nature, the speedup is not easy to achieve in the real world. Flaws in setting up speedup evaluation can lead to false promises. Will GPGPU-based ATPG system become a reality? Or it is just a myth. In this paper, we try to provide an answer by surveying state-of-the-art works and by analyzing practical aspects of today's industrial designs.",https://ieeexplore.ieee.org/document/8558526/,IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems,Jan. 2020,ieeexplore
10.1109/ACCESS.2020.2976808,Graph-Based Method for Fault Detection in the Iron-Making Process,IEEE,Journals,"Since the iron-making process is performed in complicated environments and controlled by operators, observation labeling is difficult and time-consuming. Therefore, unsupervised fault detection methods are a promising research topic. Recently, an unsupervised graph-based change point detection method has been introduced, and the graph of observations is constructed by the minimum spanning tree. In this paper, a novel fault detection method based on the graph for an iron-making process is proposed, and a weight calculation method for constructing the minimum spanning tree is introduced. The Euclidean distance and Mahalanobis distance are combined to calculate the weights in the minimum spanning tree, which contain important relations of variables. The distance calculation method is determined by the correlation coefficients of variables. Each testing observation is set as a change point candidate, and a change point candidate divides the observations into two groups. The number of a special type of edge in the minimum spanning tree is used as a fault detection statistic. That special edge connects two observations from two different groups. The minimum number of that type of edge corresponding to the change point candidate is a true change point. Finally, numerical simulation is used to test the power of the proposed method, and a real iron-making process including low stock, cooling, and slip faults is implemented to illustrate the effectiveness of fault detection in industrial processes.",https://ieeexplore.ieee.org/document/9015995/,IEEE Access,2020,ieeexplore
10.1109/TII.2021.3054795,Guest Editorial: Softwarized Networking for Next Generation Industrial Cyber-Physical Systems,IEEE,Journals,"The papers in this special section focus on softwarized networking for next generation industrial cyber-physical systems (CPSs). With the emergence of embedded and ubiquitous cyberphysical applications, the rationale of blending the physical and the virtual worlds has become ever promising. These papers examine several topics that are recently concerned in the community, including the software defined architectures and implementations, advanced machine learning and data analytics solutions, blockchain-based network services and applications, network function allocation, dependable and trustable solutions, energy efficient networks and services, and other enabling technologies for integrating softwarized networks into CPSs. ",https://ieeexplore.ieee.org/document/9423491/,IEEE Transactions on Industrial Informatics,Aug. 2021,ieeexplore
10.1109/TII.2020.3034627,Guest Editorial: Special Section on Advanced Signal Processing and AI Technologies for Industrial Big Data,IEEE,Journals,The papers in this special section focus on advanced signal processing and artificial intelligence (AI) technologies for industrial Big Data (IBD) powered by Industry 4.0. Modern industry has evolved from the traditional manufacturing industry to digital and intelligent industry. Huge amount of complex real-time data are generated from the thousands of industrial sensors in physical and man-made environments. Industrial big data (IBD) afford us an unprecedented opportunity to obtain an in-depth understanding of Internet of Things and facilitate data-driven approaches for industrial optimization and scheduling. The papers in this section collect the latest ideas and research on advanced signal processing and artificial intelligence (AI) technologies for IBD.,https://ieeexplore.ieee.org/document/9361683/,IEEE Transactions on Industrial Informatics,May 2021,ieeexplore
10.1109/TVT.2021.3084829,Guest EditorialIntroduction to the Special Section on Vehicular Networks in the Era of 6G: End-Edge-Cloud Orchestrated Intelligence,IEEE,Journals,"The articles in this special section focus on vehicular networks in the era of 6G mobile communication. With the growth of the vehicle population, vehicular networks play a key role in building safe, efficient, and intelligent transport systems and has been attracting a lot of attention from both academic and industrial communities around the world. The rise of autonomous driving technology and the prosperity of mobile applications, e.g., real-time video analytic, image-aided navigation, natural language processing, and etc, have brought tremendous pressure on current vehicular networks, e.g., high bandwidth, ultra-low latency, high reliability, high security, powerful computation capability, and massive connections. It is necessary to continue to develop vehicular networks by combining the latest research intends in other fields to meet quickly rising communication and computation demands. The upcoming 6G technology, which provides Holographic and Artificial Intelligence (AI) enabled communications, together with the increasing implementation of artificial intelligence in mobile devices, will lead to a new research trend to end-edge-cloud orchestrated computing with intelligence. It means that, not only the intelligent communication protocols, but also the intelligent computing resource management and machine learning algorithms among the mobile vehicles, the edge and the cloud, should be redesigned to support the development of vehicular networks.",https://ieeexplore.ieee.org/document/9477551/,IEEE Transactions on Vehicular Technology,June 2021,ieeexplore
10.1109/TSMCB.2003.822956,Hardware implementation of fuzzy Petri net as a controller,IEEE,Journals,"The paper presents a new approach to fuzzy Petri net (FPN) and its hardware implementation. The authors' motivation is as follows. Complex industrial processes can be often decomposed into many parallelly working subprocesses, which can, in turn, be modeled using Petri nets. If all the process variables (or events) are assumed to be two-valued signals, then it is possible to obtain a hardware or software control device, which works according to the algorithm described by conventional Petri net. However, the values of real signals are contained in some bounded interval and can be interpreted as events which are not only true or false, but rather true in some degree from the interval [0, 1]. Such a natural interpretation from multivalued logic (fuzzy logic) point of view, concerns sensor outputs, control signals, time expiration, etc. It leads to the idea of FPN as a controller, which one can rather simply obtain, and which would be able to process both analog, and binary signals. In the paper both graphical, and algebraic representations of the proposed FPN are given. The conditions under which transitions can be fired are described. The algebraic description of the net and a theorem which enables computation of new marking in the net, based on current marking, are formulated. Hardware implementation of the FPN, which uses fuzzy JK flip-flops and fuzzy gates, are proposed. An example illustrating usefulness of the proposed FPN for control algorithm description and its synthesis as a controller device for the concrete production process are presented.",https://ieeexplore.ieee.org/document/1298882/,"IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)",June 2004,ieeexplore
10.1109/ACCESS.2021.3098163,Hierarchical Control of Microgrid Using IoT and Machine Learning Based Islanding Detection,IEEE,Journals,"Due to the increase in penetration of renewable energy sources, the control technique plays a vital role to determine the performance of Microgrid (MG). Recently, the Internet of Things (IoT) and cloud computing has gained significance in solving various industrial problems. Robust and scalable Information Communication Technology (ICT) infrastructure is critical for efficient control of MG. IoT Devices with efficient measurement and control capability can play a key role in the MG environment. In this paper three layers hierarchical control of inverter based MG was developed using cloud-based IoT infrastructure and machine learning (ML) based islanding detection scheme. MG was operated in both island and grid connected mode. In the Primary layer, a voltage frequency (V-F) droop control with virtual impedance control was applied to avoid the disturbances in island mode. Moreover, Active Reactive (P-Q) power control was used for grid connected mode. In the secondary layer voltage and frequency deviations were removed by using the decentralized averaging based method. Voltage and frequency from each distributed generator (DG) were communicated by using a lightweight IoT-based protocol through an edge device (ED). Context-aware policy (CAP) was adopted in ED to optimize traffic flow over a communication network (CN) by comparing the difference in the present and previous data values. In the tertiary layer, a cloud-based ML model was developed using an artificial neural network (ANN) for islanding detection. ANN model was trained by data produced by simulating islanding scenarios in Matlab. Phasor measurement unit (PMU) data was communicated to the cloud for island prediction. The Proposed scheme was implemented on a modified IEEE-13 bus system with four inverter-based distributed generators (DGs) in Matlab, and Microsoft cloud services were used. The successful implementation of MG hierarchical control using an IoT feedback network with less data traffic along with cloud-based islanding detection using machine learning are the main contributions in this work. The whole system achieves stability within 2 seconds of islanding according to IEEE 1547 standards.",https://ieeexplore.ieee.org/document/9490217/,IEEE Access,2021,ieeexplore
10.1109/TMECH.2013.2245337,Image-Based Visual Servoing of a 7-DOF Robot Manipulator Using an Adaptive Distributed Fuzzy PD Controller,IEEE,Journals,"This paper is concerned with the design and implementation of a distributed proportional-derivative (PD) controller of a 7-degrees of freedom (DOF) robot manipulator using the Takagi-Sugeno (T-S) fuzzy framework. Existing machine learning approaches to visual servoing involve system identification of image and kinematic Jacobians. In contrast, the proposed approach actuates a control signal primarily as a function of the error and derivative of the error in the desired visual feature space. This approach leads to a significant reduction in the computational burden as compared to model-based approaches, as well as existing learning approaches to model inverse kinematics. The simplicity of the controller structure will make it attractive in industrial implementations where PD/PID type schemes are in common use. While the initial values of PD gain are learned with the help of model-based controller, an online adaptation scheme has been proposed that is capable of compensating for local uncertainties associated with the system and its environment. Rigorous experiments have been performed to show that visual servoing tasks such as reaching a static target and tracking of a moving target can be achieved using the proposed distributed PD controller. It is shown that the proposed adaptive scheme can dynamically tune the controller parameters during visual servoing, so as to improve its initial performance based on parameters obtained while mimicking the model-based controller. The proposed control scheme is applied and assessed in real-time experiments using an uncalibrated eye-in-hand robotic system with a 7-DOF PowerCube robot manipulator.",https://ieeexplore.ieee.org/document/6471828/,IEEE/ASME Transactions on Mechatronics,April 2014,ieeexplore
10.1109/TNSM.2020.3044415,In-Band Network Monitoring Technique to Support SDN-Based Wireless Networks,IEEE,Journals,"Most industrial applications demand determinism in terms of latency, reliability, and throughput. This goes hand in hand with the increased complexity of real-time network programability possibilities. To ensure network performance low-overhead, high-granularity, and timely network verification techniques need to be deployed. The first cornerstone of network verification ability is to enable end-to-end network monitoring, including end devices too. To achieve this, this article shows a novel and low overhead in-band network telemetry and monitoring technique for wireless networks focusing on IEEE 802.11 networks. A design of in-band network telemetry enabled node architecture is proposed and its proof of concept implementation is realized. The PoC realization is used to monitor a real-life SDN-based wireless network, enabling on-the-fly (re)configuration capabilities based on monitoring data. In addition, the proposed monitoring technique is validated in terms of monitoring accuracy, monitoring overhead, and network (re)configuration accuracy. It is shown that the proposed in-band monitoring technique has 6 times lower overhead than other active monitoring techniques on a single-hop link. Besides this, it is demonstrated that (re)configuration decisions taken based on monitored data fulfill targeted application requirements, validating the suitability of the proposed monitoring technique.",https://ieeexplore.ieee.org/document/9292999/,IEEE Transactions on Network and Service Management,March 2021,ieeexplore
10.1109/ACCESS.2018.2809681,Industrial Big Data Analytics for Prediction of Remaining Useful Life Based on Deep Learning,IEEE,Journals,"Due to the recent development of cyber-physical systems, big data, cloud computing, and industrial wireless networks, a new era of industrial big data is introduced. Deep learning, which brought a revolutionary change in computer vision, natural language processing, and a variety of other applications, has significant potential for solutions providing in sophisticated industrial applications. In this paper, a concept of device electrocardiogram (DECG) is presented, and an algorithm based on deep denoising autoencoder (DDA) and regression operation is proposed for the prediction of the remaining useful life of industrial equipment. First, the concept of electrocardiogram is explained. Then, a problem statement based on manufacturing scenario is presented. Subsequently, the architecture of the proposed algorithm called integrated DDA and the algorithm workflow are provided. Moreover, DECG is compared with traditional factory information system, and the feasibility and effectiveness of the proposed algorithm are validated experimentally. The proposed concept and algorithm combine typical industrial scenario and advance artificial intelligence, which has great potential to accelerate the implementation of industry 4.0.",https://ieeexplore.ieee.org/document/8302913/,IEEE Access,2018,ieeexplore
10.1109/TASE.2018.2847222,Integration of Robotic Vision and Tactile Sensing for Wire-Terminal Insertion Tasks,IEEE,Journals,"This paper reports the development of a manipulation system for electric wires, implemented by means of a commercial gripper installed on an industrial manipulator and equipped with cameras and suitably designed tactile sensors. The purpose of this system is the execution of wire insertion on commercial electromechanical components. The synergy between computer vision and tactile sensing is necessary because, in a real environment, the tight spaces very often prevent the possibility to use the vision system, also when the same task is performed by a human being. A novel technique to speed up the generation of training data sets for convolutional neural networks (CNNs) is proposed. Therefore, this technique is used to train a CNN in order to detect small objects (such as wire terminals). Moreover, aiming to prevent faults during the task and to interact with the environment safely, several machine learning approaches are used to produce an affordable output from the tactile sensor. The proposed approach shows how a cheap sensor embedded with suitable intelligence can provide information comparable to a more expensive force sensor.",https://ieeexplore.ieee.org/document/8395267/,IEEE Transactions on Automation Science and Engineering,April 2019,ieeexplore
10.1109/TII.2019.2937905,Intelligent Embedded Vision for Summarization of Multiview Videos in IIoT,IEEE,Journals,"Nowadays, video sensors are used on a large scale for various applications, including security monitoring and smart transportation. However, the limited communication bandwidth and storage constraints make it challenging to process such heterogeneous nature of Big Data in real time. Multiview video summarization (MVS) enables us to suppress redundant data in distributed video sensors settings. The existing MVS approaches process video data in offline manner by transmitting them to the local or cloud server for analysis, which requires extra streaming to conduct summarization, huge bandwidth, and are not applicable for integration with industrial Internet of Things (IIoT). This article presents a light-weight convolutional neural network (CNN) and IIoT-based computationally intelligent (CI) MVS framework. Our method uses an IIoT network containing smart devices, Raspberry Pi (RPi) (clients and master) with embedded cameras to capture multiview video data. Each client RPi detects target in frames via light-weight CNN model, analyzes these targets for traffic and crowd density, and searches for suspicious objects to generate alert in the IIoT network. The frames of each client RPi are encoded and transmitted with approximately 17.02% smaller size of each frame to master RPi for final MVS. Empirical analysis shows that our proposed framework can be used in industrial environments for various applications such as security and smart transportation and can be proved beneficial for saving resources.<sup>11</sup>[Online]. Available: https://github.com/tanveer-hussain/Embedded-Vision-for-MVS.",https://ieeexplore.ieee.org/document/8815938/,IEEE Transactions on Industrial Informatics,April 2020,ieeexplore
10.1109/TIE.2018.2873547,Joint Image-Text Hashing for Fast Large-Scale Cross-Media Retrieval Using Self-Supervised Deep Learning,IEEE,Journals,"Recent years have witnessed the promising future of hashing in the industrial applications for fast similarity retrieval. In this paper, we propose a novel supervised hashing method for large-scale cross-media search, termed self-supervised deep multimodal hashing (SSDMH), which learns unified hash codes as well as deep hash functions for different modalities in a self-supervised manner. With the proposed regularized binary latent model, unified binary codes can be solved directly without relaxation strategy while retaining the neighborhood structures by the graph regularization term. Moreover, we propose a new discrete optimization solution, termed as binary gradient descent, which aims at improving the optimization efficiency toward real-time operation. Extensive experiments on three benchmark data sets demonstrate the superiority of SSDMH over state-of-the-art cross-media hashing approaches.",https://ieeexplore.ieee.org/document/8488700/,IEEE Transactions on Industrial Electronics,Dec. 2019,ieeexplore
10.1109/TIE.2021.3057030,KDnet-RUL: A Knowledge Distillation Framework to Compress Deep Neural Networks for Machine Remaining Useful Life Prediction,IEEE,Journals,"Machine remaining useful life (RUL) prediction is vital in improving the reliability of industrial systems and reducing maintenance cost. Recently, long short-term memory (LSTM) based algorithms have achieved state-of-the-art performance for RUL prediction due to their strong capability of modeling sequential sensory data. In many cases, the RUL prediction algorithms are required to be deployed on edge devices to support real-time decision making, reduce the data communication cost, and preserve the data privacy. However, the powerful LSTM-based methods which have high complexity cannot be deployed to edge devices with limited computational power and memory. To solve this problem, we propose a knowledge distillation framework, entitled KDnet-RUL, to compress a complex LSTM-based method for RUL prediction. Specifically, it includes a generative adversarial network based knowledge distillation (GAN-KD) for disparate architecture knowledge transfer, a learning-during-teaching based knowledge distillation (LDT-KD) for identical architecture knowledge transfer, and a sequential distillation upon LDT-KD for complicated datasets. We leverage simple and complicated datasets to verify the effectiveness of the proposed KDnet-RUL. The results demonstrate that the proposed method significantly outperforms state-of-the-art KD methods. The compressed model with 12.8 times less weights and 46.2 times less total float point operations even achieves a comparable performance with the complex LSTM model for RUL prediction.",https://ieeexplore.ieee.org/document/9351733/,IEEE Transactions on Industrial Electronics,Feb. 2022,ieeexplore
10.1109/LCOMM.2020.3039251,LOS/NLOS Identification for Indoor UWB Positioning Based on Morlet Wavelet Transform and Convolutional Neural Networks,IEEE,Journals,"In indoor ultra-wideband (UWB) positioning systems, positioning accuracy can be improved by determining the conditions of line-of-sight (LOS) and non-line-of-sight (NLOS) propagation and taking appropriate measures. The existing methods, such as support vector machine (SVM), decision tree (DT), k-Nearest Neighbor (KNN), identify LOS/NLOS mainly using time-domain characteristics. However, using only time-domain characteristics cannot achieve satisfactory performance. In this letter, we propose a LOS/NLOS identification method based on Morlet wave transform and convolutional neural networks (MWT-CNN). MWT-CNN is capable of identifying LOS/NLOS in the time-frequency domain. Our simulations are based on the 802.15.4a UWB model and an open-source dataset. The simulation results show that MWT-CNN achieves an accuracy of 100% in the office scenario, 99.89% in the industrial scenario, 96.10% in the residential scenario, and 98.84% in a real experimental scenario. Further simulation results show that MWT-CNN is more suitable to be deployed in static scenarios.",https://ieeexplore.ieee.org/document/9264213/,IEEE Communications Letters,March 2021,ieeexplore
10.1109/ACCESS.2020.3008289,Learning-Based IoT Data Aggregation for Disaster Scenarios,IEEE,Journals,"Industrial Internet of Everything (IIoE), as the deep integration of industry 6.0, the Internet of Things (IoT) and 6G mobile communication technology, pave the way for intelligent industry, enabling industrial optimization and automation. To ensure the high quality of services (QoS) in IIoE, tremendous real-time information generated by the pervasive smart things needs to be aggregated and processed quickly and reliably. However, a large-scale disaster could damage the entire communication network and cut off data aggregation such that Qos is compromised. In this paper, an Intelligent NIB based Data Aggregation Strategy, named (IDAS), is proposed for after disaster scenarios in IIoE. Specifically, IDAS first applies both iterative cubature kalman filter and radial basis function neural network to predict the data collection rates of survived infrastructures. Then, an energy efficient task distribution mechanism is design. Next, a deep reinforcement learning method is developed for the car-carrying NIB route design to perform corresponding task. Eventually, all data are aggregated toward the rescue headquarter by NIB deployment based on Fermat tree constructions. The theoretical analysis and simulations indicate that IDAS is not only energy efficient for after disaster scenarios but requires the least NIB consumption while compared with contemporary strategies.",https://ieeexplore.ieee.org/document/9137637/,IEEE Access,2020,ieeexplore
10.1109/ACCESS.2019.2956556,Light-Weight Spliced Convolution Network-Based Automatic Water Meter Reading in Smart City,IEEE,Journals,"Automatic reading for water meter is one of the practical demands in smart city applications. Due to the high cost, it is not feasible to replace the old mechanical water meter with a new embedded electronic device. Recently, image recognition based meter reading methods have become research hotspots. However, illumination, occlusion, energy and computational consuming in IoT environment bring challenges to these methods. In this paper, we design and implement a smart water meter reading system to handle this issue. Specifically, we first propose a novel light-weight spliced convolution network to recognize the meter number, which simplifies standard 3 x 3 convolutions by splicing a certain number of 1 x 1 and 3 x 3 size kernel. We then prove the superiority of our network by theoretical analysis. Second, we have implemented the prototype which can handle huge real-time data base on the distributed cloud platform. Base on this system, our system can provide industrial service. Finally, we conduct real-world dataset to verify the performance of the system. The experimental results demonstrate that our proposed light-weight spliced convolution network can reduce nearly 10x computational consuming, 7x model space, and save 3x running time comparing with standard convolution network.",https://ieeexplore.ieee.org/document/8917620/,IEEE Access,2019,ieeexplore
10.1109/TASE.2017.2783342,MASD: A Multimodal Assembly Skill Decoding System for Robot Programming by Demonstration,IEEE,Journals,"Programming by demonstration (PBD) transforms the robot programming from the code level to automated interface between robot and human, promoting the flexibility of robotized automation. In this paper, we focus on programming the industrial robot for assembly tasks by parsing the human demonstration into a series of assembly skills and compiling the skill to the robot executables. To achieve this goal, an identification system using multimodal information to recognize the assembly skill, called MASD, is proposed including: 1) an initial learning stage using a hierarchical model to recognize the action by considering the features from action-object effect, gesture, and trajectory and 2) a retrospective thinking stage using a segmentation method to cut the continuous demonstrations into multiple assembly skills optimally. Using MASD, the demonstration of assembly tasks can be explained with high accuracy in real time, driving a hypothesis that a PBD system on the top of MASD can be extended to more realistic assembly tasks beyond pure positional moving and picking. In experiments, the skill identification module is used to recognize the five kinds of assembly skills in demonstrations of both single and multiple assembly skills, and outperforms the comparative action identification methods. Besides integrated with the MASD, the PBD system can generate the program based on the demonstration and successfully enable an ABB industrial robotic arm simulator to assemble a flashlight and a switch, verifying the initial hypothesis. Note to Practitioners-In the conventional robotized automation, the key role of the robot mainly owes to its capacity for repeating a wide variety of tasks with high speed and accuracy in long term, with a cost of days to months of programming for deployment. On the other hand, the new trend of customization brings the new characteristics: production in short cycle and small volume. This irreversible momentum urges the robot to switch from task to task efficiently. The biggest bottleneck here is the tedious programming, which also has high prerequisites for most practitioners in manufacturing. This situation motivates the development of a PBD system that can understand the assembly skills performed by the human experts in the demonstration and accordingly generate the program for robot's execution of the taught task. In this paper, we present a skill decoding system to parse the observational raw demonstration into symbolic sequences, which is the crucial bridge to enable the automatic programming. The system achieves high performance in recognition and is tailored for the PBD in assembly tasks by considering both advantages and disadvantages in the background of assembly, such as controllable environment and limited computational resources. It is particularly useful for assembly tasks with modularized actions based on a set of standard parts. At the perspective of industrial application, the PBD upon the proposed system is a promising solution to improve the flexibility of manufacture, which is expected to be true in midterm but an important step toward this goal.",https://ieeexplore.ieee.org/document/8263146/,IEEE Transactions on Automation Science and Engineering,Oct. 2018,ieeexplore
10.1109/ACCESS.2019.2942390,"Machine Learning for 5G/B5G Mobile and Wireless Communications: Potential, Limitations, and Future Directions",IEEE,Journals,"Driven by the demand to accommodate today's growing mobile traffic, 5G is designed to be a key enabler and a leading infrastructure provider in the information and communication technology industry by supporting a variety of forthcoming services with diverse requirements. Considering the ever-increasing complexity of the network, and the emergence of novel use cases such as autonomous cars, industrial automation, virtual reality, e-health, and several intelligent applications, machine learning (ML) is expected to be essential to assist in making the 5G vision conceivable. This paper focuses on the potential solutions for 5G from an ML-perspective. First, we establish the fundamental concepts of supervised, unsupervised, and reinforcement learning, taking a look at what has been done so far in the adoption of ML in the context of mobile and wireless communication, organizing the literature in terms of the types of learning. We then discuss the promising approaches for how ML can contribute to supporting each target 5G network requirement, emphasizing its specific use cases and evaluating the impact and limitations they have on the operation of the network. Lastly, this paper investigates the potential features of Beyond 5G (B5G), providing future research directions for how ML can contribute to realizing B5G. This article is intended to stimulate discussion on the role that ML can play to overcome the limitations for a wide deployment of autonomous 5G/B5G mobile and wireless communications.",https://ieeexplore.ieee.org/document/8844682/,IEEE Access,2019,ieeexplore
10.1109/JIOT.2019.2912022,Machine Learning-Based Network Vulnerability Analysis of Industrial Internet of Things,IEEE,Journals,"It is critical to secure the Industrial Internet of Things (IIoT) devices because of potentially devastating consequences in case of an attack. Machine learning (ML) and big data analytics are the two powerful leverages for analyzing and securing the Internet of Things (IoT) technology. By extension, these techniques can help improve the security of the IIoT systems as well. In this paper, we first present common IIoT protocols and their associated vulnerabilities. Then, we run a cyber-vulnerability assessment and discuss the utilization of ML in countering these susceptibilities. Following that, a literature review of the available intrusion detection solutions using ML models is presented. Finally, we discuss our case study, which includes details of a real-world testbed that we have built to conduct cyber-attacks and to design an intrusion detection system (IDS). We deploy backdoor, command injection, and Structured Query Language (SQL) injection attacks against the system and demonstrate how a ML-based anomaly detection system can perform well in detecting these attacks. We have evaluated the performance through representative metrics to have a fair point of view on the effectiveness of the methods.",https://ieeexplore.ieee.org/document/8693904/,IEEE Internet of Things Journal,Aug. 2019,ieeexplore
10.1109/TSE.2018.2844788,Machine Learning-Based Prototyping of Graphical User Interfaces for Mobile Apps,IEEE,Journals,"It is common practice for developers of user-facing software to transform a mock-up of a graphical user interface (GUI) into code. This process takes place both at an application's inception and in an evolutionary context as GUI changes keep pace with evolving features. Unfortunately, this practice is challenging and time-consuming. In this paper, we present an approach that automates this process by enabling accurate prototyping of GUIs via three tasks: detection, classification, and assembly. First, logical components of a GUI are detected from a mock-up artifact using either computer vision techniques or mock-up metadata. Then, software repository mining, automated dynamic analysis, and deep convolutional neural networks are utilized to accurately classify GUI-components into domain-specific types (e.g., toggle-button). Finally, a data-driven, K-nearest-neighbors algorithm generates a suitable hierarchical GUI structure from which a prototype application can be automatically assembled. We implemented this approach for Android in a system called ReDraw. Our evaluation illustrates that ReDraw achieves an average GUI-component classification accuracy of 91 percent and assembles prototype applications that closely mirror target mock-ups in terms of visual affinity while exhibiting reasonable code structure. Interviews with industrial practitioners illustrate ReDraw's potential to improve real development workflows.",https://ieeexplore.ieee.org/document/8374985/,IEEE Transactions on Software Engineering,1 Feb. 2020,ieeexplore
10.1109/ACCESS.2018.2877097,Multivariate Sensor Data Analysis for Oil Refineries and Multi-mode Identification of System Behavior in Real-time,IEEE,Journals,"Large-scale oil refineries are equipped with mission-critical heavy machinery (boilers, engines, turbines, and so on) and are continuously monitored by thousands of sensors for process efficiency, environmental safety, and predictive maintenance purposes. However, sensors themselves are also prone to errors and failure. The quality of data received from these sensors should be verified before being used in system modeling. There is a need for reliable methods and systems that can provide data validation and reconciliation in real-time with high accuracy. In this paper, we develop a novel method for real-time data validation, gross error detection and classification over multivariate sensor data streams. The validated and high-quality data obtained from these processes is used for pattern analysis and modeling of industrial plants. We obtain sensor data from the power and petrochemical plants of an oil refinery and analyze them using various time-series modeling and data mining techniques that we integrate into a complex event processing engine. Next, we study the computational performance implications of the proposed methods and uncover regimes where they are sustainable over fast streams of sensor data. Finally, we detect shifts among steady-states of data, which represent systems' multiple operating modes and identify the time when a model reconstruction is required using DBSCAN clustering algorithm.",https://ieeexplore.ieee.org/document/8501917/,IEEE Access,2018,ieeexplore
10.1109/ACCESS.2019.2916938,NASM: Nonlinearly Attentive Similarity Model for Recommendation System via Locally Attentive Embedding,IEEE,Journals,"Recommendation system, as a core service for many customer-oriented online services, is employed to predict the personalized rating of users on their potentially preferable items. In modern industrial settings, an item-based collaborative filtering (item-based CF) method has been long popular owing to its excellent interpretability and high efficiency in the real-time personalized recommendation. In this model, the current target item is recommended according to the interacted similarity from the user's profile, which implies that the key of item-based CF is in the estimation of historical item similarity. Early studies usually utilize statistical measures including cosine similarity and Pearson correlation coefficient to estimate similarity with low accuracy caused by lacking optimization tailed. Recently, there are some learning-based models attempting to learn item similarity by optimizing a recommendation-aware loss function. However, these efforts are mainly concentrated on the application of the shallow linear model, and relative works that deploy some deep learning components for item-based CF are scarce. In this paper, we propose a nonlinearly attentive similarity model (NASM) for item-based CF via locally attentive embedding by introducing local attention and novel nonlinear attention to capture local and global item information, simultaneously. The NASM is based on a neural attentive item similarity (NAIS) model and further achieves significantly superior performance. The experimental results demonstrate that the NASM achieves more competitive recommendation performance in terms of hit ration (HR) and the normalized discounted cumulative gain (NDGC) in comparison with other state-of-the-art recommendation models.",https://ieeexplore.ieee.org/document/8715403/,IEEE Access,2019,ieeexplore
10.1109/TII.2019.2947291,NELLY: Flow Detection Using Incremental Learning at the Server Side of SDN-Based Data Centers,IEEE,Journals,"The processing of big data generated by the Industrial Internet of Things (IIoT) calls for the support of processing at the edge of the network, as well as at the cloud data centers. The equal-cost multipath, which is the default routing technique in the cloud data centers, can degrade the network performance when handling mouse and elephant flows. Such degradation of performance can compromise the support of the strict quality of service requirements of the IIoT over 5G networks. Novel techniques for scheduling the elephant flows can alleviate this problem. Recently, several approaches have incorporated machine learning techniques at the controller-side in software-defined data center networks (SDDCNs) to detect elephant flows. However, these approaches can produce heavy traffic overhead, low scalability, low accuracy, and high detection time. This article introduces the Network Elephants Learner and anaLYzer (NELLY), a novel and efficient method for applying incremental learning at the server side of SDDCNs to accurately and timely identify elephant flows with low traffic overhead. Incremental learning enables NELLY to adapt to varying network traffic conditions and perform continuous learning with limited memory resources. NELLY has been extensively evaluated using real traces and various incremental learning algorithms. Results show that NELLY is accurate and supports low classification time when using adaptive decision trees algorithms.",https://ieeexplore.ieee.org/document/8868201/,IEEE Transactions on Industrial Informatics,Feb. 2020,ieeexplore
10.1109/JPROC.2018.2856739,Navigating the Landscape for Real-Time Localization and Mapping for Robotics and Virtual and Augmented Reality,IEEE,Journals,"Visual understanding of 3-D environments in real time, at low power, is a huge computational challenge. Often referred to as simultaneous localization and mapping (SLAM), it is central to applications spanning domestic and industrial robotics, autonomous vehicles, and virtual and augmented reality. This paper describes the results of a major research effort to assemble the algorithms, architectures, tools, and systems software needed to enable delivery of SLAM, by supporting applications specialists in selecting and configuring the appropriate algorithm and the appropriate hardware, and compilation pathway, to meet their performance, accuracy, and energy consumption goals. The major contributions we present are: 1) tools and methodology for systematic quantitative evaluation of SLAM algorithms; 2) automated, machine-learning-guided exploration of the algorithmic and implementation design space with respect to multiple objectives; 3) end-to-end simulation tools to enable optimization of heterogeneous, accelerated architectures for the specific algorithmic requirements of the various SLAM algorithmic approaches; and 4) tools for delivering, where appropriate, accelerated, adaptive SLAM solutions in a managed, JIT-compiled, adaptive runtime context.",https://ieeexplore.ieee.org/document/8436423/,Proceedings of the IEEE,Nov. 2018,ieeexplore
10.1109/TII.2021.3050041,Network Traffic Prediction in Industrial Internet of Things Backbone Networks: A Multitask Learning Mechanism,IEEE,Journals,"Industrial Internet of Things (IIoT), as a common industrial application of Internet of Things, has been widely deployed in recent years. End-to-end network traffic is an essential information for many network security and management functions. This article investigates the issues of IIoT-oriented backbone network traffic prediction. Predicting the traffic of IIoT backbone networks is intractable because of the large number of prior network traffic information, which needs to consume expensive network resources for sampling. Motivated by that, we propose an effective prediction mechanism using multitask learning (MTL), which is a special paradigm of transfer learning. A deep learning architecture constructed by MTL and long short-term memory is designed. This deep architecture takes advantage of link loads as additional information to improve prediction accuracy. We provide a theoretical analysis for the MTL mechanism. The effectiveness is evaluated by implementing our mechanism on real network.",https://ieeexplore.ieee.org/document/9316934/,IEEE Transactions on Industrial Informatics,Oct. 2021,ieeexplore
10.1109/TII.2019.2953275,Neural Network-Based Model Predictive Control of a Paste Thickener Over an Industrial Internet Platform,IEEE,Journals,"This article presents a real implementation of a neural network-based model predictive control scheme (NNMPC) to control an industrial paste thickener. The implementation is done over an Industrial Internet of Things (IIoT) platform designed using the seven layer reference model for IIoT systems. Modeling is achieved using an encoder-decoder with attention recurrent neural network, while MPC search is done using particle swarm optimization. An industrial evaluation is presented, which highlights the set-point tracking and disturbance rejection capabilities of the proposed NNMPC technique.",https://ieeexplore.ieee.org/document/8897590/,IEEE Transactions on Industrial Informatics,April 2020,ieeexplore
10.1109/TIE.2013.2266086,Nonlinear Model-Predictive Control for Industrial Processes: An Application to Wastewater Treatment Process,IEEE,Journals,"Because of their complex behavior, wastewater treatment processes (WWTPs) are very difficult to control. In this paper, the design and implementation of a nonlinear model-predictive control (NMPC) system are discussed. The proposed NMPC comprises a self-organizing radial basis function neural network (SORBFNN) identifier and a multiobjective optimization method. The SORBFNN with concurrent structure and parameter learning is developed as a model identifier for approximating the online states of dynamic systems. Then, the solution of the multiobjective optimization is obtained by a gradient method which can shorten the solution time of optimal control problems. Moreover, the conditions for the stability analysis of NMPC are presented. Experiments reveal that the proposed control technique gives satisfactory tracking and disturbance rejection performance for WWTPs. Experimental results on a real WWTP show the efficacy of the proposed NMPC for industrial processes in many applications.",https://ieeexplore.ieee.org/document/6523075/,IEEE Transactions on Industrial Electronics,April 2014,ieeexplore
10.1109/ACCESS.2019.2958284,On the Generation of Anomaly Detection Datasets in Industrial Control Systems,IEEE,Journals,"In recent decades, Industrial Control Systems (ICS) have been affected by heterogeneous cyberattacks that have a huge impact on the physical world and the people's safety. Nowadays, the techniques achieving the best performance in the detection of cyber anomalies are based on Machine Learning and, more recently, Deep Learning. Due to the incipient stage of cybersecurity research in ICS, the availability of datasets enabling the evaluation of anomaly detection techniques is insufficient. In this paper, we propose a methodology to generate reliable anomaly detection datasets in ICS that consists of four steps: attacks selection, attacks deployment, traffic capture and features computation. The proposed methodology has been used to generate the Electra Dataset, whose main goal is the evaluation of cybersecurity techniques in an electric traction substation used in the railway industry. Using the Electra dataset, we train several Machine Learning and Deep Learning models to detect anomalies in ICS and the performed experiments show that the models have high precision and, therefore, demonstrate the suitability of our dataset for use in production systems.",https://ieeexplore.ieee.org/document/8926471/,IEEE Access,2019,ieeexplore
10.1109/JIOT.2020.2994200,Optimization of Edge-PLC-Based Fault Diagnosis With Random Forest in Industrial Internet of Things,IEEE,Journals,"Facing globalized competition, there have been increasing requirements for safety and efficiency in smart factories, where the industrial Internet of Things can enable the monitoring of equipment's status and the detecting of faults before they go critical. Regarding cloud computing, data-driven methods running at clouds are adopted to train the model with a large amount of raw data at the beginning, then end machines upload their real-time readings to the cloud center for processing. However, this incurs considerable computational costs and may sometimes bear a severe delay. In this article, we consider a hierarchical structure where edge-PLCs are employed to gather sensed data locally and reduce communication costs. Since a single fault may be related to multiple influencing features, we want to first minimize the number of features that need to determine a fault, then try to find out the minimal set of edge-PLCs which can cover all key features so as to save the deployment cost. We propose a random-forest-based method to handle the features selection problem, and then the selection of edge-PLCs by solving the set coverage problem. Through the simulation on real data trace, we compare our method with other artificial-intelligence-based methods, such as the logistics regression model and its extensions. The results prove the efficiency and performance of the proposed method, which reaches or even exceeds the accuracy of methods using the full set of data.",https://ieeexplore.ieee.org/document/9091605/,IEEE Internet of Things Journal,Oct. 2020,ieeexplore
10.1109/TMECH.2014.2366033,Output Regulation of Nonlinear Systems With Application to Roll-to-Roll Manufacturing Systems,IEEE,Journals,"This paper deals with the problem of synthesizing feedforward control to aid the regulation of output of a nonlinear system in the presence of partially known exogenous inputs. The problem appears in many engineering applications including Roll-to-Roll (R2R) manufacturing systems. Currently known methods for this problem either require the solution of a constrained partial differential equation or the preview information of the signal to be tracked. The novelty of this paper lies in synthesizing feedforward control as the solution of a system of differential-algebraic equations, which is considerably less complex and suitable for practical implementation. In this paper, we consider the problem of regulating the output while rejecting the disturbances and apply it to R2R manufacturing systems. The problem of tracking reference signals can also be handled with the suggested technique. We assume that the disturbance signal is the output of a known exogenous system with unknown initial conditions. A parameter identification scheme to estimate the unknown initial conditions is developed. The proposed technique is successfully applied to control of web tension in a large R2R machine which mimics most of the features of industrial R2R machines and contains real-time hardware and software that is used in industrial practice. Extensive experiments were conducted to evaluate the proposed scheme for web tension control under various experimental conditions, including different web speeds and materials. We will present and discuss the representative experimental results with the proposed technique and provide a comparison with an industrial PI control scheme to highlight the benefits of using the proposed scheme.",https://ieeexplore.ieee.org/document/6963413/,IEEE/ASME Transactions on Mechatronics,June 2015,ieeexplore
10.1109/TIE.2016.2612160,PLC-Based Real-Time Realization of Flatness-Based Feedforward Control for Industrial Compression Systems,IEEE,Journals,"In this paper, we present a novel programmable logic controller (PLC)-based real-time realization of a flatness-based feedforward control (FFC) scheme. The proposed approach is applied to an industrial fuel-gas compression system which is used to supply fuel gas to the gas turbines in combined cycle power plants. Due to the increasing demand for fast operation point transitions with high performance and accuracy requirements, the currently applied decentralized proportional-integral-derivative controllers appear to be not appropriate any more. Hence, by means of system simulations, a new flatness-based FFC design has been shown to provide improved control performance. In this paper, we bridge the gap between simulation-based control design and practical applicability, in that, we present the real-time realization of the approach on a PLC. Furthermore, the PLC-based controller is tested on a hardware-in-the-loop platform running with a complex compression system model in real time. The results reveal that the flatness-based control design can be implemented on a real compressor system.",https://ieeexplore.ieee.org/document/7572893/,IEEE Transactions on Industrial Electronics,Feb. 2017,ieeexplore
10.1109/TII.2012.2222034,"Past, Present, and Future of Industrial Agent Applications",IEEE,Journals,"Industrial agents technology leverages the benefits of multiagent systems, distributed computing, artificial intelligence techniques and semantics in the field of production, services and infrastructure sectors, providing a new way to design and engineer control solutions based on the decentralization of control over distributed structures. The key drivers for this application are the benefits of agent-based industrial systems, namely in terms of robustness, scalability, reconfigurability and productivity, all of which translate to a greater competitive advantage. This manuscript monitors the chronology of research and development of the industrial applications of multiagent and holonic systems. It provides the comprehensive overview of methodologies, architectures and applications of agents in industrial domain from early nineties up to present. It also gives an outlook of the current trends as well as challenges and possible future application domains of industrial agents.",https://ieeexplore.ieee.org/document/6319392/,IEEE Transactions on Industrial Informatics,Nov. 2013,ieeexplore
10.1109/TII.2019.2940099,Performance Supervised Fault Detection Schemes for Industrial Feedback Control Systems and their Data-Driven Implementation,IEEE,Journals,"This article addresses performance supervised fault detection (PSFD) issues for industrial feedback control systems based on performance degradation prediction. To be specific, three performance indicators are first introduced based on Bellman equation to predict system performance degradations for industrial processes with the aid of machine learning techniques. Based on them, three PSFD schemes are proposed by embedding the performance indicators as supervising information. In this context, the data-driven implementation of PSFD schemes are investigated for linear systems with unmeasurable state variables. A case study on rolling mill process, a typical benchmark in the steel manufacturing processes, is given at the end of this article to illustrate the applications of the proposed fault detection schemes.",https://ieeexplore.ieee.org/document/8827307/,IEEE Transactions on Industrial Informatics,April 2020,ieeexplore
10.1109/TIM.2021.3092518,Pipeline Safety Early Warning by Multifeature-Fusion CNN and LightGBM Analysis of Signals From Distributed Optical Fiber Sensors,IEEE,Journals,"Energy pipelines are the backbones of global energy systems. Monitoring their safety and automatically identifying and locating third-party damage events are crucial to energy supply. However, most traditional methods lack in-depth consideration of distributed fiber signals and have not been tested on real-world long-distance pipelines, making it difficult to deploy them in operating long-distance pipelines. In this study, we utilize a novel real-time machine-learning method based on phase-sensitive optical time domain reflectometer technology to monitor the safety of oil and gas pipelines. Specifically, we build a multifeature-fusion convolutional neural network and LightGBM fusion model based on two novel complementary spatiotemporal features. The method was applied to a large amount of data collected from real-world oil–gas transportation pipelines of the China National Petroleum Corporation. The proposed method could accurately locate and identify third-party damage events in real-time under conditions of strong noise and various types of system hardware, and could effectively handle signal drift in the time and space dimensions. Our methodology has been deployed at real long-distance energy pipeline sites and our work will contribute to energy pipeline safety and energy supply security. Furthermore, the proposed solution could be generalized to other fields, such as industrial inspection, measurement, and monitoring.",https://ieeexplore.ieee.org/document/9541184/,IEEE Transactions on Instrumentation and Measurement,2021,ieeexplore
10.1109/TII.2015.2426012,PolyNet: A Polynomial-Based Learning Machine for Universal Approximation,IEEE,Journals,"Currently, there is a need in all disciplines for efficient and powerful machine learning (ML) algorithms for handling offline and real-time nonlinear data. Industrial applications abound from real-time control systems to modeling and simulation of complex systems and processes. Certain ML methods have become popular with researchers and engineers. Such techniques include fuzzy systems (FSs), artificial neural networks (ANNs), radial basis function (RBF) networks, and support vector regression (SVR) machines. Historically, polynomial-based learning machines (PLMs) based on the group method of data handling (GMDH) model have enjoyed usage similar to that of these other methods. However, unwieldy kernel functions in the form of large high-order polynomials, and relatively limited computer speed and capacity, have limited the use of PLMs to comparatively small problems with low dimensionality and simple functional relationships. Thus, true polynomial-based ML solutions have drifted out of vogue for at least two decades. This work attempts to reinvigorate the interest in PLMs by introducing a novel practical implementation called PolyNet. It will be shown that once certain algorithms are applied to the generation, training, and functional operation of PLMs, they can compete on par with or better than methods currently in use.",https://ieeexplore.ieee.org/document/7095595/,IEEE Transactions on Industrial Informatics,June 2015,ieeexplore
10.1109/ACCESS.2020.3016469,Prediction of Traffic Congestion Based on LSTM Through Correction of Missing Temporal and Spatial Data,IEEE,Journals,"With the rapid increase in vehicle use during the fourth Industrial Revolution, road resources have reached their supply limit. Active studies have therefore been conducted on intelligent transportation systems (ITSs) to realize traffic management systems utilizing fewer resources. As part of an ITS, real-time traffic services are provided to improve user convenience. Such services are applied to prevent traffic congestion and disperse existing traffic. Therefore, these services focus on immediacy at the expense of accuracy. As these services typically rely on measured data, the accuracy of the models are contingent on the data collection. Therefore, this study proposes a long short-term memory (LSTM)-based traffic congestion prediction approach based on the correction of missing temporal and spatial values. Before making predictions, the proposed prediction method applies pre-processing that consists of outlier removal using the median absolute deviation of the traffic data and the correction of temporal and spatial values using temporal and spatial trends and pattern data. In previous studies, data with time-series features have not been appropriately learned. To address this problem, the proposed prediction method uses an LSTM model for time-series data learning. To evaluate the performance of the proposed method, the mean absolute percentage error (MAPE) was calculated for comparison with other models. The MAPE of the proposed method was found to be the best of the compared models, at approximately 5%.",https://ieeexplore.ieee.org/document/9166475/,IEEE Access,2020,ieeexplore
10.1109/JSEN.2021.3082514,RF-Wri: An Efficient Framework for RF-Based Device-Free Air-Writing Recognition,IEEE,Journals,"Machines are becoming indispensable in our lives and the requirements of the human-machine interactions are increasing. Conventional devices, such as a keyboard or touch screen, may not be preferred in future's entertainment, home, and industrial applications. Device-free (non-contact) solutions will be even more popular. These solutions often use visual and acoustic technologies which have some disadvantages. The use of radio frequency (RF) waves for human-machine interaction such as air-writing (Wri), is a new and challenging problem. We propose a device-free machine learning-based air-writing recognition framework called RF-Wri which can effectively distinguish 26 capital letters. Two-channel low-cost software-defined radios (SDR) and oppositely polarized antennas are used to provide polarization diversity which makes the accuracy of classification superior. Another critical novelty is the usage of Discrete Cosine Transform (DCT) coefficients as new features to represent RF waveform which provides writing speed and user invariant recognition. Discrete Wavelet Transform (DWT) filters and letter segmentation algorithm are used for signal de-noising and separating the air-writing activities, respectively. It is shown that Support Vector Machine (SVM) can successfully classify the measured RF waves of air-written letters. It is verified with various real measurements that the proposed framework, RF-Wri, achieves 95.15% accuracy in the classification of all 26 air-written letters and outperforms the fairly new WiFi-based air-writing recognition approaches.",https://ieeexplore.ieee.org/document/9438656/,IEEE Sensors Journal,"15 Aug.15, 2021",ieeexplore
10.1109/81.747195,Reaction-diffusion CNN algorithms to generate and control artificial locomotion,IEEE,Journals,"In this paper a physiological-behavioral approach to neural processing is used to realize artificial locomotion in mechatronic devices. The task has been realized by using a particular model of reaction-diffusion cellular neural networks (RD-CNN's) generating autowave fronts as well as Turing patterns. Moreover a programmable hardware cellular neural network structure is presented in order to model, generate, and control in real time some biorobots. The programmable hardware implementation gives the possibility of generating locomotion in real time and also to control the transition among several types of locomotion, with particular attention to hexapodes. The approach proposed allows not only the design of walking robots, but also the ability to build structures able to efficiently solve typical problems in industrial automation, such as online routing of objects moved on conveyor belts.",https://ieeexplore.ieee.org/document/747195/,IEEE Transactions on Circuits and Systems I: Fundamental Theory and Applications,Feb. 1999,ieeexplore
10.1109/ACCESS.2021.3082641,Real-Time Facial Expression Recognition Based on Edge Computing,IEEE,Journals,"In recent years, many large-scale information systems in the Internet of Things (IoT) can be converted into interdependent sensor networks, such as smart cities, smart medical systems, and industrial Internet systems. The successful application of edge computing in the IoT will make our algorithms faster, more convenient, lower overall costs, providing better business practices, and enhance sustainability. Facial action unit (AU) detection recognizes facial expressions by analyzing cues about the movement of certain atomic muscles in the local facial area. According to the detected facial feature points, we could calculate the values of AU, and then use classification algorithms for emotion recognition. In edge devices, using optimized and custom algorithms to directly process the raw image data from each camera, the detected emotions can be more easily transmitted to the end-user. Due to the tremendous network overhead of transferring the facial action unit feature data, it poses challenges of a real-time facial expression recognition system being deployed in a distributed manner while running in production. Therefore, we designed a lightweight edge computing-based distributed system using Raspberry Pi tailed for this need, and we optimized the data transfer and components deployment. In the vicinity, the front-end and back-end processing modes are separated to reduce round-trip delay, thereby completing complex computing tasks and providing high-reliability, large-scale connection services. For IoT or smart city applications and services, they can be made into smart sensing systems that can be deployed anywhere with network connections.",https://ieeexplore.ieee.org/document/9438728/,IEEE Access,2021,ieeexplore
10.1109/TNS.2021.3090670,Real-Time Implementation of the Neutron/Gamma Discrimination in an FPGA-Based DAQ MTCA Platform Using a Convolutional Neural Network,IEEE,Journals,"These days, research on the classification of neutron/gamma waveforms in scintillators using pulse shape discrimination (PSD) techniques is a highly studied topic. Numerous methods have been explored to optimize this classification, with some of the most recent research being focused on machine learning techniques with excellent results. These approaches are mainly based on the use of 1-D convolutional neural networks (CNNs). In this field, field-programmable gate arrays (FPGAs) with high-sampling rate analog to digital converters (ADCs) have been used to perform this classification in real-time. In this work, we select a potential architecture and implement it with the help of the IntelFPGA OpenCL SDK environment. A shorter and C-like development of OpenCL enables a more straightforward modification and optimization of the network architecture. The main goal of this work is the evaluation of the needed resources and the obtained performance to prototype a complete solution in the FPGA. The FPGA design is generated as if it was connected to an ADC module streaming the data samples with the help of a Board Support Package developed for an IntelFPGA ARRIA10 available in an Advanced Mezzanine Card (AMC) module in an Micro Telecommunications Computing Architecture (MTCA.4) platform. The prototyped solution has been integrated into Experimental Physics and Industrial Control System (EPICS) using the nominal device support (NDS) model currently being developed by ITER.",https://ieeexplore.ieee.org/document/9459756/,IEEE Transactions on Nuclear Science,Aug. 2021,ieeexplore
10.1109/83.791960,Real-time DSP implementation for MRF-based video motion detection,IEEE,Journals,"This paper describes the real time implementation of a simple and robust motion detection algorithm based on Markov random field (MRF) modeling, MRF-based algorithms often require a significant amount of computations. The intrinsic parallel property of MRF modeling has led most of implementations toward parallel machines and neural networks, but none of these approaches offers an efficient solution for real-world (i.e., industrial) applications. Here, an alternative implementation for the problem at hand is presented yielding a complete, efficient and autonomous real-time system for motion detection. This system is based on a hybrid architecture, associating pipeline modules with one asynchronous module to perform the whole process, from video acquisition to moving object masks visualization. A board prototype is presented and a processing rate of 15 images/s is achieved, showing the validity of the approach.",https://ieeexplore.ieee.org/document/791960/,IEEE Transactions on Image Processing,Oct. 1999,ieeexplore
10.1109/JSEN.2021.3075535,Recent Advancements in the Development of Sensors for the Structural Health Monitoring (SHM) at High-Temperature Environment: A Review,IEEE,Journals,"With Industry 4.0 becoming increasingly pervasive, the importance and usage of sensors has increased several folds. Industry 4.0 refers to a new phase in the industrial revolution that mainly focuses on interconnectivity, automation, machine learning, and real-time data. Real-time structural health monitoring (SHM) of components in the industrial process is one of the crucial and important component of Industry 4.0. SHM of components exposed to high-temperature (<inline-formula> <tex-math notation=""LaTeX"">$\sim 650^{\circ }\text{C}$ </tex-math></inline-formula>) is becoming increasingly important nowadays. However, harsh and high temperature environments impose a great challenge towards their implementation. This review is an attempt to demonstrate the development, application, limitations and recent advancement of the existing sensors used for SHM. Some sensors such as eddy current (EC) sensors and fiber Bragg grating (FBG) sensors have been discussed in detail. A phenomenological study of the electromagnetic sensor for the SHM of engineering components that are exposed to high temperature has been addressed. State-of-the-art fabrication methodologies such as low temperature co-fired ceramic (LTCC) technology for such type of sensors for high temperature SHM applications have been elucidated. Future challenges and opportunities for SHM applications of high temperature sensors have been highlighted.",https://ieeexplore.ieee.org/document/9415648/,IEEE Sensors Journal,"15 July15, 2021",ieeexplore
10.1109/TII.2017.2752709,Recursive Total Principle Component Regression Based Fault Detection and Its Application to Vehicular Cyber-Physical Systems,IEEE,Journals,"The cyber-physical systems (CPSs) are the central research topic in the era of Industrial 4.0. Such systems interact intensively between physical entities and abstract information, and commonly exist in the industrial processes and people's daily lives. This paper investigates the practical difficulties of the vehicular CPSs online implementation, and based on that proposes a fault diagnosis and control architecture with modular units and reserved extendibility. It is elaborated that the systems' adaptability could be enhanced by either the online tracking techniques or the ensemble learning schemes. For the onboard deployment of automobile CPSs, the requirement of real-time capacity is in focus. A new recursive total principle component regression based design and implementation approach is proposed for efficient data-driven fault detection. Simulation tests were carried out on the Carsim to compare the proposed approach with multiple existing methods.",https://ieeexplore.ieee.org/document/8038833/,IEEE Transactions on Industrial Informatics,April 2018,ieeexplore
10.1109/JIOT.2020.3028325,Reinforcement-Learning-Enabled Partial Confident Information Coverage for IoT-Based Bridge Structural Health Monitoring,IEEE,Journals,"Internet-of-Things (IoT)-based bridge structural health monitoring (BSHM) has recently attracted considerable attention from both academic and industrial communities of civil engineering and computer science. In conjunction with researchers from civil engineering and computer science, this article studied a fundamental problem motivated from practical IoT-based BSHM: how to effectively prolong network lifetime while guaranteeing desired coverage. Integrating a promising reinforcement learning model named learning automata (LA) with confident information coverage (CIC) model, this article presented an energy-efficient sensor scheduling strategy for partial CIC coverage in IoT-based BSHM system to guarantee network coverage and prolong network lifetime. The proposed scheme fully exploits cooperation among deployed nodes and alternatively schedules the wake/sleep status of nodes while satisfying network connectivity and partial coverage ratio. Especially, the proposed scheme takes full advantage of the LA model to adaptively learn the optimal sensor scheduling strategy and significantly extend network lifetime. A series of comparison simulations using real data sets collected by a practical BSHM system strongly verify the effectiveness and energy efficiency of the proposed algorithm. To the best of our knowledge, this is the first study on how to combine the reinforcement learning mechanism with partial coverage for maximizing the network lifetime of the IoT-based BSHM.",https://ieeexplore.ieee.org/document/9211720/,IEEE Internet of Things Journal,"1 March1, 2021",ieeexplore
10.1109/ACCESS.2021.3085338,Research on a Product Quality Monitoring Method Based on Multi Scale PP-YOLO,IEEE,Journals,"To monitor product quality in the production process in real time, this thesis proposes a quality monitoring model based on PaddlePaddle You Only Look Once (PP-YOLO). First, in the preprocessing stage, the data enhancement method and the K-means++ method are used to improve the robustness of the algorithm, and the generated anchor box can screen more refined features earlier. Second, ResNet50-vd with the deformable convolution idea is selected as the backbone of the detection model, the feature pyramid network structure and the composition of the loss function are improved, and the feature learning ability of the model is enhanced to enable it to detect multiple scales of defects. Finally, pruning is performed on the basis of the trained model to reduce the number of model parameters so that it can be deployed in industrial scenarios with limited hardware conditions. Experimental results show that the proposed quality monitoring model can meet the requirements for detection speed and accuracy in actual production, providing a new concept for the deployment of deep learning models in the industrial field.",https://ieeexplore.ieee.org/document/9445109/,IEEE Access,2021,ieeexplore
10.1109/JSAC.2019.2951932,Residential Customer Baseline Load Estimation Using Stacked Autoencoder With Pseudo-Load Selection,IEEE,Journals,"Accurate estimation of customer baseline load (CBL) is a key factor in the successful implementation of demand response (DR). CBL technologies implemented at utilities currently are primarily designed for large industrial and commercial customers. The U.S. Federal Energy Regulatory Commission (FERC) order 745 states that DR owners, including residential customers, can sell their load reduction in the wholesale market. However, since residential load is random and un-schedulable, this tends to inherently degrade the effectiveness of existing CBL technologies. In this paper, a novel SAE based CBL method for residential customers that uses the data reconstruction capability of a stacked autoencoder (SAE) is described. In the model, two SAEs are synchronously trained-one SAE generates a pseudo-load pool and the second one is used to select a pseudo-load to reconstruct a residential CBL. A support vector machine (SVM) classifier is self-trained to conduct the pseudo-load selection. The proposed strategy is validated using a real data set consisting of 328 residential customers' smart meter readings. Benchmarks from other machine learning techniques and existing CBL methods are compared with the proposed method. Test results show that the accuracy of the residential CBL reconstruction significantly improves when compared with existing methods, such as HighXofY and exponential moving average.",https://ieeexplore.ieee.org/document/8892548/,IEEE Journal on Selected Areas in Communications,Jan. 2020,ieeexplore
10.1109/TII.2021.3061579,Resource Management for Pervasive-Edge-Computing-Assisted Wireless VR Streaming in Industrial Internet of Things,IEEE,Journals,"Wireless virtual reality (VR) is increasingly used in industrial Internet of Things (IIoTs). However, ultra-high viewport rendering demands and excessive terminal energy consumption restrict the application of wireless VR. Pervasive edge computing emerges as a promising method for wireless VR. In this article, we propose an energy-aware resource management scheme for wireless-VR-supported IIoTs. To reduce the energy consumption of VR equipments (VEs) while ensuring a smooth immersive VR experience, we formulate the viewport rendering offloading, computing, and spectrum resource allocation to be a joint optimization problem, considering content correlation between VEs, fluctuating channel conditions, and VR quality of experience. By applying dual approximation, the original problem is transformed to be a Markov decision process and an reinforcement learning (RL)-based online learning algorithm is designed to find the optimal policy. To improve the learning efficiency, the quantum parallelism is integrated into the RL to overcome “curse of dimensionality”. In the simulations, the convergence rate and the performance in terms of energy consumption and stalling rate are evaluated. Simulation results demonstrate the effectiveness of the proposed scheme.",https://ieeexplore.ieee.org/document/9361168/,IEEE Transactions on Industrial Informatics,Nov. 2021,ieeexplore
10.1109/TII.2019.2903224,Risk-Based Scheduling of Security Tasks in Industrial Control Systems With Consideration of Safety,IEEE,Journals,"Industrial control systems (ICSs) in networked environments face severe cyber-security risks and challenges. A timely response to cyber-attacks is of paramount importance for mitigating risks. However, the security policy developed for an ICS may be conflicting with the ICS's safety policy, on which much attention has been paid for a long time in industrial control. An inappropriate enforcement of the security policy may deteriorate the ICS performance or even result in severe unexpected consequences. To tackle this problem, a risk-based security task scheduling approach is presented for ICSs with consideration of the safety policy. It ensures a timely response to cyber-attacks without compromising safety. More specifically, the approach reconciles security tasks and safety tasks according to a designed resolution policy, so as to acquire contradiction-free security and safety (S&amp;S) tasks. Then, a real-time risk assessment method is developed to characterize the subtle change of the system risk with the implementation of the reconciled S&amp;S tasks. After that, a task scheduling method is designed with the risk as the optimization objective, i.e., it searches the optimal task scheduling scheme by minimizing the risk posture. The resulting scheduling scheme ensures the smooth implementation of the S&amp;S policy, which reflects the optimal recovery process against the risk. Finally, case studies on a hardware-in-the-loop testbed are conducted to demonstrate the effectiveness of the proposed approach.",https://ieeexplore.ieee.org/document/8661651/,IEEE Transactions on Industrial Informatics,May 2020,ieeexplore
10.1109/ACCESS.2020.3045563,SDN Enhanced Resource Orchestration of Containerized Edge Applications for Industrial IoT,IEEE,Journals,"With the rise of the Industrial Internet of Things (IIoT), there is an intense pressure on resource and performance optimization leveraging on existing technologies, such as Software Defined Networking (SDN), edge computing, and container orchestration. Industry 4.0 emphasizes the importance of lean and efficient operations for sustainable manufacturing. Achieving this goal would require engineers to consider all layers of the system, from hardware to software, and optimizing for resource efficiency at all levels. This emphasizes the need for container-based virtualization tools such as Docker and Kubernetes, offering Platform as a Service (PaaS), while simultaneously leveraging on edge technologies to reduce related latencies. For network management, SDN is poised to offer a cost-effective and dynamic scalability solution by customizing packet handling for various edge applications and services. In this paper, we investigate the energy and latency trade-offs involved in combining these technologies for industrial applications. As a use case, we emulate a 3D-drone-based monitoring system aimed at providing real-time visual monitoring of industrial automation. We compare a native implementation to a containerized implementation where video processing is orchestrated while streaming is handled by an external UE representing the IIoT device. We compare these two scenarios for energy utilization, latency, and responsiveness. Our test results show that only roughly 16 percent of the total power consumption happens on the mobile node when orchestrated. Virtualization adds up about 4.5 percent of the total power consumption while the latency difference between the two approaches becomes negligible after the streaming session is initialized.",https://ieeexplore.ieee.org/document/9296769/,IEEE Access,2020,ieeexplore
10.1109/TII.2018.2799907,SIGMM: A Novel Machine Learning Algorithm for Spammer Identification in Industrial Mobile Cloud Computing,IEEE,Journals,"An industrial mobile network is crucial for industrial production in the Internet of Things. It guarantees the normal function of machines and the normalization of industrial production. However, this characteristic can be utilized by spammers to attack others and influence industrial production. Users who only share spams, such as links to viruses and advertisements, are called spammers. With the growth of mobile network membership, spammers have organized into groups for the purpose of benefit maximization, which has caused confusion and heavy losses to industrial production. It is difficult to distinguish spammers from normal users owing to the characteristics of multidimensional data. To address this problem, this paper proposes a spammer identification scheme based on Gaussian mixture model (SIGMM) that utilizes machine learning for industrial mobile networks. It provides intelligent identification of spammers without relying on flexible and unreliable relationships. SIGMM combines the presentation of data, where each user node is classified into one class in the construction process of the model. We validate the SIGMM by comparing it with the reality mining algorithm and hybrid fuzzy c-means (FCM) clustering algorithm using a mobile network dataset from a cloud server. Simulation results show that SIGMM outperforms these previous schemes in terms of recall, precision, and time complexity.",https://ieeexplore.ieee.org/document/8274937/,IEEE Transactions on Industrial Informatics,April 2019,ieeexplore
10.1109/ACCESS.2019.2896129,Safety Risk Monitoring of Cyber-Physical Power Systems Based on Ensemble Learning Algorithm,IEEE,Journals,"The traditional security risk monitoring technology cannot adapt to cyber-physical power systems (CPPS) concerning evaluation criteria, real-time monitoring, and technical reliability. The aim of this paper is to propose and implement a log analysis architecture for CPPS to detect the log anomalies, which introduces the distributed streaming processing mechanism. The processing mechanism can train the network protocol feature database precisely over the big data platform, which improves the efficiency of the network in terms of log anomaly detection. Moreover, we propose an ensemble prediction algorithm based on time series (EPABT) considering the characteristics of the statistical log analysis to predict abnormal features during the network traffic analysis. We then present a new asymmetric error cost (AEC) evaluation criterion to meet the characteristics of CPPS. The experimental results demonstrate that the EPABT provides an efficient tool for detecting the accuracy and reliability of abnormal situation prediction as compared with the several state-of-the-art algorithms. Meanwhile, the AEC can effectively evaluate the differences in the cost between the high and low prediction results. To the best of our knowledge, these two algorithms provide strong support for the practical application of power industrial network security risk monitoring.",https://ieeexplore.ieee.org/document/8631031/,IEEE Access,2019,ieeexplore
10.1109/ACCESS.2021.3102157,Scalable and Multifaceted Search and Its Application for Binary Malware Files,IEEE,Journals,"Malicious binary files are a serious threat to industrial information systems. Because of their large number, an automatic assistant tool becomes essential for analysis, and finding similar files would be a great help. In this paper, we present a fast, scalable, and multifaceted search scheme to find similar binary malware files. We use a content-defined chunking algorithm to convert a file into a feature set for the first time. The proposed scheme uses MinHash to reduce any feature set of any file to a fixed size, which significantly improves search accuracy, processing speed, and space utilization. We theoretically prove that the new scheme returns similar files in jaccard index order. Through implementation and experiments with 12 million malicious files, we confirm that the search speed is increased by 600%, space is reduced by 90%, and the accuracy is increased by 400% at least, compared with the state-of-the-art of Elasticsearch.",https://ieeexplore.ieee.org/document/9504570/,IEEE Access,2021,ieeexplore
10.1109/TLA.2012.6142495,Second Life: A New Approach In Professional Education In The Study Of Work Safety.,IEEE,Journals,"The ICT (Information and Communication Technologies) are present with great intensity in education and has also presented highlighting the professional education with tools for data analysis, graphics, simulation, virtual reality and augmented reality. The knowledge gained with ICTs are used in practical activities in industrial operations. The student examines machine elements through simulations, yet during the practices with machines and equipment, errors occur from simple forgetfulness of use of PPE (Personal Protective Equipment), lack of parameters in the machinery, not removal of the ornaments the body, can somehow put at risk the life of the students in these settings wrong or manipulation of machinery and equipment that can cause accidents. Our proposal is to use a virtual reality tool specifically the SL (Second Life), having a friendly interface, programming features and usability features immersive, so we developed the simulation and representation of machine failures that could occur on the day of an industry. During the use of simulators in SL, the student learns from those mistakes in the virtual world, serving as a warning so it does not perform the same mistakes in the real world, where unsafe acts can danger your life up. The proposal also gives the possibility to use a machine for machining of smaller proportions built specifically for educational activities, plus devices that facilitate the use by PWD's (People with disabilities), connected to the virtual world.",https://ieeexplore.ieee.org/document/6142495/,IEEE Latin America Transactions,Jan. 2012,ieeexplore
10.1109/TVCG.2020.2969007,Security in Process: Visually Supported Triage Analysis in Industrial Process Data,IEEE,Journals,"Operation technology networks, i.e. hard- and software used for monitoring and controlling physical/industrial processes, have been considered immune to cyber attacks for a long time. A recent increase of attacks in these networks proves this assumption wrong. Several technical constraints lead to approaches to detect attacks on industrial processes using available sensor data. This setting differs fundamentally from anomaly detection in IT-network traffic and requires new visualization approaches adapted to the common periodical behavior in OT-network data. We present a tailored visualization system that utilizes inherent features of measurements from industrial processes to full capacity to provide insight into the data and support triage analysis by laymen and experts. The novel combination of spiral plots with results from anomaly detection was implemented in an interactive system. The capabilities of our system are demonstrated using sensor and actuator data from a real-world water treatment process with introduced attacks. Exemplary analysis strategies are presented. Finally, we evaluate effectiveness and usability of our system and perform an expert evaluation.",https://ieeexplore.ieee.org/document/8968740/,IEEE Transactions on Visualization and Computer Graphics,1 April 2020,ieeexplore
10.1109/ACCESS.2016.2619898,Segmentation of Factories on Electricity Consumption Behaviors Using Load Profile Data,IEEE,Journals,"In recent years, the new achievements in the field of technology and data science allowed to gather detailed and well-structured information about electricity consumption behaviors of industrial enterprises. Such type of information can find numerous applications in the power distribution industry. The utilities often use the data from contracts to assign each industrial customer a class label according to this type defined in predetermined industry segmentation. Such type of fixed-chart segmentation is not able to satisfy the needs of modern enterprises for the flexible and dynamic determination of production modes. In this paper, we address this problem by proposing a new method for the segmentation of various types of factories based on their electricity consumption patterns represented in load profile data. It exploits the evolution-based characteristics of smart meter data of multiple types of factories to remove irrelevant features. We use data visualization to estimate the number of clusters and apply the well-known k-means algorithm on filtered data to generate segmentation. Experimental results on real load profile data collected with smart meters from manufacturing industries in Guangdong province of China have shown that the new clustering approach produced the meaningful segmentation of factories that reflect production operations.",https://ieeexplore.ieee.org/document/7752771/,IEEE Access,2016,ieeexplore
10.1109/TASE.2019.2938316,Semiautomatic Labeling for Deep Learning in Robotics,IEEE,Journals,"In this article, we propose an augmented reality semiautomatic labeling (ARS), a semiautomatic method which leverages on moving a 2-D camera by means of a robot, proving precise camera tracking, and an augmented reality pen (ARP) to define initial object bounding box, to create large labeled data sets with minimal human intervention. By removing the burden of generating annotated data from humans, we make the deep learning technique applied to computer vision, which typically requires very large data sets, truly automated and reliable. With the ARS pipeline, we created two novel data sets effortlessly, one on electromechanical components (industrial scenario) and other on fruits (daily-living scenario) and trained two state-of-the-art object detectors robustly, based on convolutional neural networks, such as you only look once (YOLO) and single shot detector (SSD). With respect to conventional manual annotation of 1000 frames that takes us slightly more than 10 h, the proposed approach based on ARS allows to annotate 9 sequences of about 35 000 frames in less than 1 h, with a gain factor of about 450. Moreover, both the precision and recall of object detection is increased by about 15% with respect to manual labeling. All our software is available as a robot operating system (ROS) package in a public repository alongside with the novel annotated data sets. Note to Practitioners-This article was motivated by the lack of a simple and effective solution for the generation of data sets usable to train a data-driven model, such as a modern deep neural network, so as to make them accessible in an industrial environment. Specifically, a deep learning robot guidance vision system would require such a large amount of manually labeled images that it would be too expensive and impractical for a real use case, where system reconfigurability is a fundamental requirement. With our system, on the other hand, especially in the field of industrial robotics, the cost of image labeling can be reduced, for the first time, to nearly zero, thus paving the way for self-reconfiguring systems with very high performance (as demonstrated by our experimental results). One of the limitations of this approach is the need to use a manual method for the detection of objects of interest in the preliminary stages of the pipeline (ARP or graphical interface). A feasible extension, related to the field of collaborative robotics, could be used to exploit the robot itself, manually moved by the user, even for this preliminary stage, so as to eliminate any source of inaccuracy.",https://ieeexplore.ieee.org/document/8844069/,IEEE Transactions on Automation Science and Engineering,April 2020,ieeexplore
10.1109/ACCESS.2020.3020799,Short-Term Industrial Load Forecasting Based on Ensemble Hidden Markov Model,IEEE,Journals,"Short-term load forecasting (STLF) for industrial customers has been an essential task to reduce the cost of energy transaction and promote the stable operation of smart grid throughout the development of the modern power system. Traditional STLF methods commonly focus on establishing the non-linear relationship between loads and features, but ignore the temporal relationship between them. In this paper, an STLF method based on ensemble hidden Markov model (e-HMM) is proposed to track and learn the dynamic characteristics of industrial customer's consumption patterns in correlated multivariate time series, thereby improving the prediction accuracy. Specifically, a novel similarity measurement strategy of log-likelihood space is designed to calculate the log-likelihood value of the multivariate time series in sliding time windows, which can effectively help the hidden Markov model (HMM) to capture the dynamic temporal characteristics from multiple historical sequences in similar patterns, so that the prediction accuracy is greatly improved. In order to improve the generalization ability and stability of a single HMM, we further adopt the framework of Bagging ensemble learning algorithm to reduce the prediction errors of a single model. The experimental study is implemented on a real dataset from a company in Hunan Province, China. We test the model in different forecasting periods. The results of multiple experiments and comparison with several state-of-the-art models show that the proposed approach has higher prediction accuracy.",https://ieeexplore.ieee.org/document/9183956/,IEEE Access,2020,ieeexplore
10.1109/TCE.1985.289969,Skipper: A Prototype Expert System,IEEE,Journals,"In recent years, the business and industrial community has begun to take considerable interest in concepts, techniques, and systems developed by researchers in artificial intelligence. In particular, ezpert systems have been explored for application to a wide range of real-world problems. An expert system is a computer program which emulates the lines of reasoning used by a human expert to solve problems in a narrowly defined domain of expertise. For example, MYCIN[1], an expert system developed at Stanford, makes recommendations to a physician regarding the diagnosis of meningitis and related diseases. Commonly (though not universally) an expert system is able to explain its ""reasoning"" in terms understandable by those expected to use the system. These users may or may not be knowledgeable in the field, but they are not expected to be computer programmers.",https://ieeexplore.ieee.org/document/4071276/,IEEE Transactions on Consumer Electronics,Aug. 1985,ieeexplore
10.23919/JCN.2020.100039,Special issue on 6G wireless systems,KICS,Journals,"While 5G is currently being deployed around the globe, research on 6G is under way aiming at addressing the coming challenges of drastic increase of wireless data traffic and support of other usage scenarios. 6G is expected to extend 5G capabilities even further. Higher bitrates (up to Tbps) and lower latency (less than 1ms) will allow introducing new services — such as pervasive edge intelligence, ultra-massive machine-type communications, extremely reliable low-latency communications, holographic rendering and high-precision communications — and meet more stringent requirements, especially in the following dimensions: energy efficiency; intelligence; spectral efficiency; security, secrecy and privacy; affordability; and customization. Artificial intelligence approaches and techniques, such as machine learning (of which deep learning and reinforcement learning are specific examples), and machine reasoning (which includes planning, scheduling, knowledge representation and reasoning, search and optimization), are the new fundamental enablers to operate networks more efficiently, enhance the overall end user experience and provide innovative service applications. Quantum Optics Computing (QOC) and Quantum Key Distribution (QKD) are almost ready for industrial applications. In particular, massive Internet of Things (mIoT), Industrial IoT (IloT), fully automated robotic platforms (which include control, perception, sensors and actuators, as well as the integration of other techniques into cyber-physical systems), vehicles and multisensory extended reality are examples of the new data-demanding applications, which will impose new performance targets and motivate 6G design and deployment.",https://ieeexplore.ieee.org/document/9321190/,Journal of Communications and Networks,Dec. 2020,ieeexplore
10.1109/TIA.2011.2125710,Speed and Flux Control of Induction Motors Using Emotional Intelligent Controller,IEEE,Journals,"This paper presents a real-time implementation of an improved emotional controller for induction motor (IM) drives. The proposed controller is called brain-emotional-learning-based intelligent controller. The utilization of the new controller is based on the emotion-processing mechanism in the brain and is essentially an action selection, which is based on sensory inputs and emotional cues. This intelligent control is based on the limbic system of the mammalian brain. The controller is successfully implemented in real time using a PC-based three-phase 2.5-kW laboratory squirrel-cage IM. In this paper, a novel but simple model of the IM drive system is achieved by using the intelligent controller, which simultaneously controls the motor flux and speed. This emotional intelligent controller has a simple computational structure with high auto learning features. The proposed emotional controller has been experimentally implemented in a laboratory IM drive, and it shows good promise for niche industrial-scale utilization.",https://ieeexplore.ieee.org/document/5728907/,IEEE Transactions on Industry Applications,May-June 2011,ieeexplore
10.1109/ACCESS.2021.3106791,System Design for a Data-Driven and Explainable Customer Sentiment Monitor Using IoT and Enterprise Data,IEEE,Journals,"The most important goal of customer service is to keep the customer satisfied. However, service resources are always limited and must prioritize specific customers. Therefore, it is essential to identify customers who potentially become unsatisfied and might lead to <italic>escalations</italic>. Data science on IoT data (especially log data) for machine health monitoring and analytics on enterprise data for customer relationship management (CRM) have mainly been researched and applied independently. This paper presents a data-driven decision support system framework that combines IoT and enterprise data to model customer sentiment and predicts escalations. The proposed framework includes a fully automated and interpretable machine learning pipeline using state-of-the-art methods. The framework is applied in a real-world case study with a major medical device manufacturer providing data from a fleet of thousands of high-end medical devices. An anonymized version of this industrial benchmark is released for the research community based on the presented case study, which has interesting and challenging properties. In our extensive experiments, we achieve a Recall@50 of 50.0 % for the task of predicting customer escalations. In addition, we show that combining IoT and enterprise data can improve prediction results and ease troubleshooting. Additionally, we propose a practical workflow for end-users when applying the proposed framework.",https://ieeexplore.ieee.org/document/9520354/,IEEE Access,2021,ieeexplore
10.1109/TASE.2018.2865663,TL-GDBN: Growing Deep Belief Network With Transfer Learning,IEEE,Journals,"A deep belief network (DBN) is effective to create a powerful generative model by using training data. However, it is difficult to fast determine its optimal structure given specific applications. In this paper, a growing DBN with transfer learning (TL-GDBN) is proposed to automatically decide its structure size, which can accelerate its learning process and improve model accuracy. First, a basic DBN structure with single hidden layer is initialized and then pretrained, and the learned weight parameters are frozen. Second, TL-GDBN uses TL to transfer the knowledge from the learned weight parameters to newly added neurons and hidden layers, which can achieve a growing structure until the stopping criterion for pretraining is satisfied. Third, the weight parameters derived from pretraining of TL-GDBN are further fine-tuned by using layer-by-layer partial least square regression from top to bottom, which can avoid many problems of traditional backpropagation algorithm-based fine-tuning. Moreover, the convergence analysis of the TL-GDBN is presented. Finally, TL-GDBN is tested on two benchmark data sets and a practical wastewater treatment system. The simulation results show that it has better modeling performance, faster learning speed, and more robust structure than existing models. Note to Practitioners-Transfer learning (TL) aims to improve training effectiveness by transferring knowledge from a source domain to target domain. This paper presents a growing deep belief network (DBN) with TL to improve the training effectiveness and determine the optimal model size. Facing a complex process and real-world workflow, DBN tends to require long time for its successful training. The proposed growing DBN with TL (TL-GDBN) accelerates the learning process by instantaneously transferring the knowledge from a source domain to each new deeper or wider substructure. The experimental results show that the proposed TL-GDBN model has a great potential to deal with complex system, especially the systems with high nonlinearity. As a result, it can be readily applicable to some industrial nonlinear systems.",https://ieeexplore.ieee.org/document/8478799/,IEEE Transactions on Automation Science and Engineering,April 2019,ieeexplore
10.1109/TPDS.2021.3104255,Taskflow: A Lightweight Parallel and Heterogeneous Task Graph Computing System,IEEE,Journals,"Taskflow aims to streamline the building of parallel and heterogeneous applications using a lightweight task graph-based approach. Taskflow introduces an expressive task graph programming model to assist developers in the implementation of parallel and heterogeneous decomposition strategies on a heterogeneous computing platform. Our programming model distinguishes itself as a very general class of task graph parallelism with in-graph control flow to enable end-to-end parallel optimization. To support our model with high performance, we design an efficient system runtime that solves many of the new scheduling challenges arising out of our models and optimizes the performance across latency, energy efficiency, and throughput. We have demonstrated the promising performance of Taskflow in real-world applications. As an example, Taskflow solves a large-scale machine learning workload up to 29% faster, 1.5× less memory, and 1.9× higher throughput than the industrial system, oneTBB, on a machine of 40 CPUs and 4 GPUs. We have opened the source of Taskflow and deployed it to large numbers of users in the open-source community.",https://ieeexplore.ieee.org/document/9511796/,IEEE Transactions on Parallel and Distributed Systems,1 June 2022,ieeexplore
10.1109/TII.2019.2962603,Temporal Tensor Local Binary Pattern: A Novel Local Tensor Time Series Descriptor,IEEE,Journals,"Time series is very ubiquitous in both the industrial environment and real-life. Capturing the time dependency is very useful for time series analysis. Although the one-dimensional local binary pattern (1D-LBP) can analyze the Univariate time series, it cannot effectively handle the multivariate time series (MTS). The UTS can be considered as zero-order tensor time series (TTS), while the MTS can be considered as one- or higher order TTS. Each variable in MTS depends not only on its past values, but also on the other variables. In this article, we propose a temporal tensor LBP (TTLBP) operator to extract the discriminative temporal features from TTS by extending 1D-LBP operation from the scalar-wise to the tensor-wise. The TTLBP is discriminative and can handle the TTS effectively and straightforwardly. To the best of our knowledge, TTLBP is the first LBP variant for TTS analysis. Furthermore, we also propose a stricter uniform TTLBP to improve robustness and to reduce the high dimensionality. We apply the proposed TTLBP in the edge intelligence-assisted video anomaly detection system. Qualitative and quantitative comparisons demonstrate the effectiveness of the proposed TTLBP.",https://ieeexplore.ieee.org/document/8943996/,IEEE Transactions on Industrial Informatics,Oct. 2020,ieeexplore
10.1109/ACCESS.2018.2809439,The Collaborative System Workflow Management of Industrial Design Based on Hierarchical Colored Petri-Net,IEEE,Journals,"Industrial design is a requisite and incontrovertible front-end design for Industrie 4.0 to realize personalization, agility, intelligence, and standardization, so it has very important practical significance to achieve industrial design collaborative workflow flexible management. This paper proposed a hierarchical colored Petri net (HCPN) for modeling the industrial design collaborative system workflow, which aims to provide a valid workflow model for the process management of the industrial design collaborative system. First, it is built the top-level workflow CPN model from product planning, markets research, product design, and product evaluation to prototype production, then the model is extended layer by layer to build the subnet model by the model refinement method. In the model, the activity of the workflow and the activity execution results are represented by the transitions (T) and places (S), respectively. The running state of the whole workflow is represented by the distribution of the token in the places. The token' color sets expresses the real-time discrete state. By use of the function of token coloring places (S) and the token' color sets of HCPN, this paper extracts the common characteristics of process knowledge, including markets research results, design ideas, conceptual sketches, design schemes, human resources, and tacit creative knowledge ,then uses the what, when, where, who, and how method to build knowledge information units for specific design event. Thus, realizes the industrial design process knowledge base construction and knowledge acquisition, which provides the necessary information and knowledge resources for the design reuse, flexibility management, and intelligent manufacturing of subsequent products.",https://ieeexplore.ieee.org/document/8302489/,IEEE Access,2018,ieeexplore
10.1109/JIOT.2019.2963635,Toward Edge-Based Deep Learning in Industrial Internet of Things,IEEE,Journals,"As a typical application of the Internet of Things (IoT), the Industrial IoT (IIoT) connects all the related IoT sensing and actuating devices ubiquitously so that the monitoring and control of numerous industrial systems can be realized. Deep learning, as one viable way to carry out big-data-driven modeling and analysis, could be integrated in IIoT systems to aid the automation and intelligence of IIoT systems. As deep learning requires large computation power, it is commonly deployed in cloud servers. Thus, the data collected by IoT devices must be transmitted to the cloud for training process, contributing to network congestion and affecting the IoT network performance as well as the supported applications. To address this issue, in this article, we leverage the fog/edge computing paradigm and propose an edge computing-based deep learning model, which utilizes edge computing to migrate the deep learning process from cloud servers to edge nodes, reducing data transmission demands in the IIoT network and mitigating network congestion. Since edge nodes have limited computation ability compared to servers, we design a mechanism to optimize the deep learning model so that its requirements for computational power can be reduced. To evaluate our proposed solution, we design a testbed implemented in the Google cloud and deploy the proposed convolutional neural network (CNN) model, utilizing a real-world IIoT data set to evaluate our approach.<sup>1</sup> Our experimental results confirm the effectiveness of our approach, which cannot only reduce the network traffic overhead for IIoT but also maintain the classification accuracy in comparison with several baseline schemes.<sup>1</sup>Certain commercial equipment, instruments, or materials are identified in this article in order to specify the experimental procedure adequately. Such identification is not intended to imply recommendation or endorsement by the National Institute of Standards and Technology, nor is it intended to imply that the materials or equipment identified are necessarily the best available for the purpose.",https://ieeexplore.ieee.org/document/8948000/,IEEE Internet of Things Journal,May 2020,ieeexplore
10.1109/OJCAS.2020.3042463,Toward Intelligent Reconfigurable Wireless Physical Layer (PHY),IEEE,Journals,"Next-generation wireless networks are getting significant attention because they promise 10-factor enhancement in mobile broadband along with the potential to enable new heterogeneous services. Services include massive machine type communications desired for Industrial 4.0 along with ultra-reliable low latency services for remote healthcare and vehicular communications. In this article, we present the design of intelligent and reconfigurable physical layer (PHY) to bring these services to reality. First, we design and implement the reconfigurable PHY via a hardware-software co-design approach on system-on-chip consisting of the ARM processor and field-programmable gate array (FPGA). The reconfigurable PHY is then made intelligent by augmenting it with online machine learning (OML) based decision-making algorithm. Such PHY can learn the environment (for example, wireless channel) and dynamically adapt the transceivers' configuration (i.e., modulation scheme, word-length) and select the wireless channel on-the-fly. Since the environment is unknown and changes with time, we make the OML architecture reconfigurable to enable dynamic switch between various OML algorithms on-the-fly. We have demonstrated the functional correctness of the proposed architecture for different environments and word-lengths. The detailed throughput, latency, and complexity analysis validate the feasibility and importance of the proposed intelligent and reconfigurable PHY in next-generation networks.",https://ieeexplore.ieee.org/document/9336350/,IEEE Open Journal of Circuits and Systems,2021,ieeexplore
10.1109/TII.2019.2954956,Toward New Retail: A Benchmark Dataset for Smart Unmanned Vending Machines,IEEE,Journals,"Deep learning is a popular direction in computer vision and digital image processing. It is widely utilized in many fields, such as robot navigation, intelligent video surveillance, industrial inspection, and aerospace. With the extensive use of deep learning techniques, classification and object detection algorithms have been rapidly developed. In recent years, with the introduction of the concept of “unmanned retail,” object detection, and image classification play a central role in unmanned retail applications. However, open-source datasets of traditional classification and object detection have not yet been optimized for application scenarios of unmanned retail. Currently, classification and object detection datasets do not exist that focus on unmanned retail solely. Therefore, in order to promote unmanned retail applications by using deep learning-based classification and object detection, in this article we collected more than 30 000 images of unmanned retail containers using a refrigerator affixed with different cameras under both static and dynamic recognition environments. These images were categorized into ten kinds of beverages. After manual labeling, images in our constructed dataset contained 155 153 instances, each of which was annotated with a bounding box. We performed extensive experiments on this dataset using ten state-of-the-art deep learning-based models. Experimental results indicate great potential of using these deep learning-based models for real-world smart unmanned vending machines.",https://ieeexplore.ieee.org/document/8908822/,IEEE Transactions on Industrial Informatics,Dec. 2020,ieeexplore
10.1109/ACCESS.2021.3080517,Towards Open and Expandable Cognitive AI Architectures for Large-Scale Multi-Agent Human-Robot Collaborative Learning,IEEE,Journals,"Learning from Demonstration (LfD) constitutes one of the most robust methodologies for constructing efficient cognitive robotic systems. Despite the large body of research works already reported, current key technological challenges include those of multi-agent learning and long-term autonomy. Towards this direction, a novel cognitive architecture for multi-agent LfD robotic learning is introduced in this paper, targeting to enable the reliable deployment of open, scalable and expandable robotic systems in large-scale and complex environments. In particular, the designed architecture capitalizes on the recent advances in the Artificial Intelligence (AI) (and especially the Deep Learning (DL)) field, by establishing a Federated Learning (FL)-based framework for incarnating a multi-human multi-robot collaborative learning environment. The fundamental conceptualization relies on employing multiple AI-empowered cognitive processes (implementing various robotic tasks) that operate at the edge nodes of a network of robotic platforms, while global AI models (underpinning the aforementioned robotic tasks) are collectively created and shared among the network, by elegantly combining information from a large number of human-robot interaction instances. Regarding pivotal novelties, the designed cognitive architecture a) introduces a new FL-based formalism that extends the conventional LfD learning paradigm to support large-scale multi-agent operational settings, b) elaborates previous FL-based self-learning robotic schemes so as to incorporate the human in the learning loop and c) consolidates the fundamental principles of FL with additional sophisticated AI-enabled learning methodologies for modelling the multi-level inter-dependencies among the robotic tasks. The applicability of the proposed framework is explained using an example of a real-world industrial case study (subject to ongoing research activities) for agile production-based Critical Raw Materials (CRM) recovery.",https://ieeexplore.ieee.org/document/9431107/,IEEE Access,2021,ieeexplore
10.1109/ACCESS.2021.3104472,Transfer Learning Strategies for Credit Card Fraud Detection,IEEE,Journals,"Credit card fraud jeopardizes the trust of customers in e-commerce transactions. This led in recent years to major advances in the design of automatic Fraud Detection Systems (FDS) able to detect fraudulent transactions with short reaction time and high precision. Nevertheless, the heterogeneous nature of the fraud behavior makes it difficult to tailor existing systems to different contexts (e.g. new payment systems, different countries and/or population segments). Given the high cost (research, prototype development, and implementation in production) of designing data-driven FDSs, it is crucial for transactional companies to define procedures able to adapt existing pipelines to new challenges. From an AI/machine learning perspective, this is known as the problem of <italic>transfer learning</italic>. This paper discusses the design and implementation of transfer learning approaches for e-commerce credit card fraud detection and their assessment in a real setting. The case study, based on a six-month dataset (more than 200 million e-commerce transactions) provided by the industrial partner, relates to the transfer of detection models developed for a European country to another country. In particular, we present and discuss 15 transfer learning techniques (ranging from naive baselines to state-of-the-art and new approaches), making a critical and quantitative comparison in terms of precision for different transfer scenarios. Our contributions are twofold: (i) we show that the accuracy of many transfer methods is strongly dependent on the number of labeled samples in the target domain and (ii) we propose an ensemble solution to this problem based on self-supervised and semi-supervised domain adaptation classifiers. The thorough experimental assessment shows that this solution is both highly accurate and hardly sensitive to the number of labeled samples.",https://ieeexplore.ieee.org/document/9512084/,IEEE Access,2021,ieeexplore
10.1109/TII.2020.2994747,Trustworthiness in Industrial IoT Systems Based on Artificial Intelligence,IEEE,Journals,"The intelligent industrial environment developed with the support of the new generation network cyber-physical system (CPS) can realize the high concentration of information resources. In order to carry out the analysis and quantification for the reliability of CPS, an automatic online assessment method for the reliability of CPS is proposed in this article. It builds an evaluation framework based on the knowledge of machine learning, designs an online rank algorithm, and realizes the online analysis and assessment in real time. The preventive measures can be taken timely, and the system can operate normally and continuously. Its reliability has been greatly improved. Based on the credibility of the Internet and the Internet of Things, a typical CPS control model based on the spatiotemporal correlation detection model is analyzed to determine the comprehensive reliability model analysis strategy. Based on this, in this article, we propose a CPS trusted robust intelligent control strategy and a trusted intelligent prediction model. Through the simulation analysis, the influential factors of attack defense resources and the dynamic process of distributed cooperative control are obtained. CPS defenders in the distributed cooperative control mode can be guided and select the appropriate defense resource input according to the CPS attack and defense environment.",https://ieeexplore.ieee.org/document/9093957/,IEEE Transactions on Industrial Informatics,Feb. 2021,ieeexplore
10.26599/TST.2020.9010055,Underground pipeline surveillance with an algorithm based on statistical time-frequency acoustic features,TUP,Journals,"Underground pipeline networks suffer from severe damage by earth-moving devices due to rapid urbanization. Thus, designing a round-the-clock intelligent surveillance system has become crucial and urgent. In this study, we develop an acoustic signal-based excavation device recognition system for underground pipeline protection. The front-end hardware system is equipped with an acoustic sensor array, an Analog-to-Digital Converter (ADC) module (ADS1274), and an industrial processor Advanced RISC Machine (ARM) cortex-A8 for signal collection and algorithm implementation. Then, a novel Statistical Time-Frequency acoustic Feature (STFF) is proposed, and a fast Extreme Learning Machine (ELM) is adopted as the classifier. Experiments on real recorded data show that the proposed STFF achieves better discriminative capability than the conventional acoustic cepstrum features. In addition, the surveillance platform is applicable for encountering big data owing to the fast learning speed of ELM.",https://ieeexplore.ieee.org/document/9552663/,Tsinghua Science and Technology,April 2022,ieeexplore
10.1109/JSEN.2020.3033754,Virtual Sensors for Fault Diagnosis: A Case of Induction Motor Broken Rotor Bar,IEEE,Journals,"This article presents an industrial implementation of a virtual sensor in the process of fault detection of an induction motor. An ensemble-learning soft-sensor is developed to detect broken rotor bar that is essential to prevent irreparable damage. Most of the existing diagnostic methods assume that the data distribution is static and that all data is available during the training, while in real applications, the data become available as data streams. The proposed method is inspired by the ensemble learning algorithm, which is combined with a new drift detection mechanism. The advantages of the proposed approach are three-fold. First, a fair comparison with other algorithms show the effectiveness of the soft sensor scheme. Second, the presented concept change detection algorithm is capable of detecting a new class in the data stream as well as data distribution change, and last but not least, the efficacy of the proposed algorithm is demonstrated using benchmark concept drift data streams.",https://ieeexplore.ieee.org/document/9245589/,IEEE Sensors Journal,"15 Feb.15, 2021",ieeexplore
10.1109/ACCESS.2019.2931194,Vison-Based 3D Shape Measurement System for Transparent Microdefect Characterization,IEEE,Journals,"The main task of vision-based industrial defect inspection is to implement efficient non-contact visual quality control, i.e., to detect if there is a defect and to achieve an accurate 3D shape measurement of such a defect, and this kind of vision defect inspection system has been widely applied in various industrial application. However, it is still not the case in the inspection of transparent microdefect on the polarizer (which is the most important part of an LCD screen). Optical measurement devices (such as confocal microscopy) are often utilized to fulfil this task. To solve problems lied in the current confocal microscopy inspection system, such as expensive and non-real-time processing, this research aims to develop a novel vision-based 3D shape measurement system for polarizer transparent microdefect characterization. The innovation of this system, which has been verified by our optical model simulation, is that the 3D sizes of microdefect have a monotonically relation to the grayscale of the microdefect image. Hence, a microdefect imaging system, which could acquire defect image accurately, is first well designed and implemented. Then, a support vector regression (SVR) algorithm is derived by the trained data, i.e., 100 acquired defect images and its corresponding 3D shape value by confocal microscopy. Characterized 3D measurement of microdefect is thereby obtained by this SVR algorithm. 30 polarizer microdefect samples have been imaged and measured by our proposed system, and several important performance indicators, including processing speed, accuracy and system reproducibility, have been elaborately tested. The experimental results show that the proposed system could achieve a high-accuracy measurement but in a much faster and more efficient way than the confocal microscopy. Besides, this developed imaging system has been evaluated in real applications, and over 300 samples have been detected, which also validate the effectiveness of the proposed system.",https://ieeexplore.ieee.org/document/8777162/,IEEE Access,2019,ieeexplore
10.1109/TII.2019.2929739,Vulnerable Code Clone Detection for Operating System Through Correlation-Induced Learning,IEEE,Journals,"Vulnerable code clones in the operating system (OS) threaten the safety of smart industrial environment, and most vulnerable OS code clone detection approaches neglect correlations between functions that limits the detection effectiveness. In this article, we propose a two-phase framework to find vulnerable OS code clones by learning on correlations between functions. On the training phase, functions as the training set are extracted from the latest code repository and function features are derived by their AST structure. Then, external and internal correlations are explored by graph modeling of functions. Finally, the graph convolutional network for code clone detection (GCN-CC) is trained using function features and correlations. On the detection phase, functions in the to-be-detected OS code repository are extracted and the vulnerable OS code clones are detected by the trained GCN-CC. We conduct experiments on five real OS code repositories, and experimental results show that our framework outperforms the state-of-the-art approaches.",https://ieeexplore.ieee.org/document/8765764/,IEEE Transactions on Industrial Informatics,Dec. 2019,ieeexplore
10.1109/TIFS.2019.2945619,Wiretap Code Design by Neural Network Autoencoders,IEEE,Journals,"In industrial machine type communications, an increasing number of wireless devices communicate under reliability, latency, and confidentiality constraints, simultaneously. From information theory, it is known that wiretap codes can asymptotically achieve reliability (vanishing block error rate (BLER) at the legitimate receiver Bob) while also achieving secrecy (vanishing information leakage (IL) to an eavesdropper Eve). However, under finite block length, there exists a tradeoff between the BLER at Bob and the IL at Eve. In this work, we propose a flexible wiretap code design for degraded Gaussian wiretap channels under finite block length, which can change the operating point on the Pareto boundary of the tradeoff between BLER and IL given specific code parameters. To attain this goal, we formulate a multi-objective programming problem, which takes the BLER at Bob and the IL at Eve into account. During training, we approximate the BLER by the mean square error and the IL by schemes based on Jensen's inequality and the Taylor expansion and then solve the optimization problem by neural network autoencoders. Simulation results show that the proposed scheme can find codes outperforming polar wiretap codes (PWC) with respect to both BLER and IL simultaneously. We show that the codes found by the autoencoders could be implemented with real modulation schemes with only small losses in performance.",https://ieeexplore.ieee.org/document/8859269/,IEEE Transactions on Information Forensics and Security,2020,ieeexplore
10.1109/JSEN.2020.3024094,k-Nearest Neighbor Classification for Pattern Recognition of a Reference Source Light for Machine Vision System,IEEE,Journals,"The design of machine vision applications allows automatic inspection, measuring systems, and robot guidance. Typical applications of industrial robots are based on no-contact sensors to give the robot information about the environment. Robot's machine vision requires photosensors or video cameras to make intelligent decisions about its localization. Video cameras used as image-capturing equipment are too costly in comparison with optical scanning systems (OSS). The OSS system provides spatial coordinates measurements that can be exploited to solve a wide variety of structural problems in real-time. Localization and guidance using machine learning (ML) techniques offer advantages due to signals captured can be transformed and be reduced for processing, storage, and displaying. The use of algorithms of ML enhances the performance of the optical system based on localization and guidance. Feature extraction represents an important part of ML techniques to transform the original raw data onto a low-dimensional subspace and holding relevant information. This work presents an improvement of an optical system based on <i>k</i>-nearest neighbor ( <i>k</i>-NN) technique to solve the object detection and localization problem. The utility of this improvement allows the optical system can discriminate between the reference source and the optical noise or interference. The OSS system presented in this article has been implemented in structural health monitoring to measure the angular position even under “lighting and weather conditions”. The feature extraction techniques used in this article were linear predictive coding (LPC), quartiles ( <i>Q</i><sub>iquartile</sub>), and autocorrelation coefficients (ACC). The results of using <i>k</i>-NN and autocorrelation coefficients and quartiles predicted more than 98% of correct classification by using a reference source light as a class 1 and a light bulb as an optical noise and called class 2.",https://ieeexplore.ieee.org/document/9195874/,IEEE Sensors Journal,"15 May15, 2021",ieeexplore
