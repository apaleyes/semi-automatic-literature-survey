doi,publication_date,publication,publisher,title,abstract,database
10.1145/3209582.3209606,2018-06-26,p,ACM,long term mobile traffic forecasting using deep spatio temporal neural networks," Forecasting with high accuracy the volume of data traffic that mobile users will consume is becoming increasingly important for precision traffic engineering, demand-aware network resource allocation, as well as public transportation. Measurements collection in dense urban deployments is however complex and expensive, and the post-processing required to make predictions is highly non-trivial, given the intricate spatio-temporal variability of mobile traffic due to user mobility. To overcome these challenges, in this paper we harness the exceptional feature extraction abilities of deep learning and propose a Spatio-Temporal neural Network (STN) architecture purposely designed for precise network-wide mobile traffic forecasting. We present a mechanism that fine tunes the STN and enables its operation with only limited ground truth observations. We then introduce a Double STN technique (D-STN), which uniquely combines the STN predictions with historical statistics, thereby making faithful long-term mobile traffic projections. Experiments we conduct with real-world mobile traffic data sets, collected over 60 days in both urban and rural areas, demonstrate that the proposed (D-)STN schemes perform up to 10-hour long predictions with remarkable accuracy, irrespective of the time of day when they are triggered. Specifically, our solutions achieve up to 61% smaller prediction errors as compared to widely used forecasting approaches, while operating with up to 600 times shorter measurement intervals.",project-academic
,2017-12-21,a,,long term mobile traffic forecasting using deep spatio temporal neural networks," Forecasting with high accuracy the volume of data traffic that mobile users will consume is becoming increasingly important for precision traffic engineering, demand-aware network resource allocation, as well as public transportation. Measurements collection in dense urban deployments is however complex and expensive, and the post-processing required to make predictions is highly non-trivial, given the intricate spatio-temporal variability of mobile traffic due to user mobility. To overcome these challenges, in this paper we harness the exceptional feature extraction abilities of deep learning and propose a Spatio-Temporal neural Network (STN) architecture purposely designed for precise network-wide mobile traffic forecasting. We present a mechanism that fine tunes the STN and enables its operation with only limited ground truth observations. We then introduce a Double STN technique (D-STN), which uniquely combines the STN predictions with historical statistics, thereby making faithful long-term mobile traffic projections. Experiments we conduct with real-world mobile traffic data sets, collected over 60 days in both urban and rural areas, demonstrate that the proposed (D-)STN schemes perform up to 10-hour long predictions with remarkable accuracy, irrespective of the time of day when they are triggered. Specifically, our solutions achieve up to 61% smaller prediction errors as compared to widely used forecasting approaches, while operating with up to 600 times shorter measurement intervals.",project-academic
10.1109/TII.2019.2942179,2020-03-01,a,Institute of Electrical and Electronics Engineers (IEEE),differentially private asynchronous federated learning for mobile edge computing in urban informatics," Driven by technologies such as mobile edge computing and 5G, recent years have witnessed the rapid development of urban informatics, where a large amount of data is generated. To cope with the growing data, artificial intelligence algorithms have been widely exploited. Federated learning is a promising paradigm for distributed edge computing, which enables edge nodes to train models locally without transmitting their data to a server. However, the security and privacy concerns of federated learning hinder its wide deployment in urban applications such as vehicular networks. In this article, we propose a differentially private asynchronous federated learning scheme for resource sharing in vehicular networks. To build a secure and robust federated learning scheme, we incorporate local differential privacy into federated learning for protecting the privacy of updated local models. We further propose a random distributed update scheme to get rid of the security threats led by a centralized curator. Moreover, we perform the convergence boosting in our proposed scheme by updates verification and weighted aggregation. We evaluate our scheme on three real-world datasets. Numerical results show the high accuracy and efficiency of our proposed scheme, whereas preserve the data privacy.",project-academic
,2020-04-30,a,,the 4th ai city challenge," The AI City Challenge was created to accelerate intelligent video analysis that helps make cities smarter and safer. Transportation is one of the largest segments that can benefit from actionable insights derived from data captured by sensors, where computer vision and deep learning have shown promise in achieving large-scale practical deployment. The 4th annual edition of the AI City Challenge has attracted 315 participating teams across 37 countries, who leveraged city-scale real traffic data and high-quality synthetic data to compete in four challenge tracks. Track 1 addressed video-based automatic vehicle counting, where the evaluation is conducted on both algorithmic effectiveness and computational efficiency. Track 2 addressed city-scale vehicle re-identification with augmented synthetic data to substantially increase the training set for the task. Track 3 addressed city-scale multi-target multi-camera vehicle tracking. Track 4 addressed traffic anomaly detection. The evaluation system shows two leader boards, in which a general leader board shows all submitted results, and a public leader board shows results limited to our contest participation rules, that teams are not allowed to use external data in their work. The public leader board shows results more close to real-world situations where annotated data are limited. Our results show promise that AI technology can enable smarter and safer transportation systems.",project-academic
10.1109/CVPRW50498.2020.00321,2020-06-14,p,IEEE,the 4th ai city challenge," The AI City Challenge was created to accelerate intelligent video analysis that helps make cities smarter and safer. Transportation is one of the largest segments that can benefit from actionable insights derived from data captured by sensors, where computer vision and deep learning have shown promise in achieving large-scale practical deployment. The 4th annual edition of the AI City Challenge has attracted 315 participating teams across 37 countries, who leverage city-scale real traffic data and high-quality synthetic data to compete in four challenge tracks. Track 1 addressed video-based automatic vehicle counting, where the evaluation is conducted on both algorithmic effectiveness and computational efficiency. Track 2 addressed city-scale vehicle re-identification with augmented synthetic data to substantially increase the training set for the task. Track 3 addressed city-scale multi-target multi-camera vehicle tracking. Track 4 addressed traffic anomaly detection. The evaluation system shows two leader boards, in which a general leader board shows all submitted results, and a public leader board shows results limited to our contest participation rules, that teams are not allowed to use external data in their work. The general leader board shows results more close to real-world situations where annotated data are limited. Our results show promise that AI technology can enable smarter and safer transportation systems.",project-academic
,2018-10-23,a,,design challenges of multi uav systems in cyber physical applications a comprehensive survey and future directions," Unmanned Aerial Vehicles (UAVs) have recently rapidly grown to facilitate a wide range of innovative applications that can fundamentally change the way cyber-physical systems (CPSs) are designed. CPSs are a modern generation of systems with synergic cooperation between computational and physical potentials that can interact with humans through several new mechanisms. The main advantages of using UAVs in CPS application is their exceptional features, including their mobility, dynamism, effortless deployment, adaptive altitude, agility, adjustability, and effective appraisal of real-world functions anytime and anywhere. Furthermore, from the technology perspective, UAVs are predicted to be a vital element of the development of advanced CPSs. Therefore, in this survey, we aim to pinpoint the most fundamental and important design challenges of multi-UAV systems for CPS applications. We highlight key and versatile aspects that span the coverage and tracking of targets and infrastructure objects, energy-efficient navigation, and image analysis using machine learning for fine-grained CPS applications. Key prototypes and testbeds are also investigated to show how these practical technologies can facilitate CPS applications. We present and propose state-of-the-art algorithms to address design challenges with both quantitative and qualitative methods and map these challenges with important CPS applications to draw insightful conclusions on the challenges of each application. Finally, we summarize potential new directions and ideas that could shape future research in these areas.",project-academic
10.1109/COMST.2019.2924143,2019-06-20,a,IEEE,design challenges of multi uav systems in cyber physical applications a comprehensive survey and future directions," Unmanned aerial vehicles (UAVs) have recently rapidly grown to facilitate a wide range of innovative applications that can fundamentally change the way cyber-physical systems (CPSs) are designed. CPSs are a modern generation of systems with synergic cooperation between computational and physical potentials that can interact with humans through several new mechanisms. The main advantages of using UAVs in CPS application is their exceptional features, including their mobility, dynamism, effortless deployment, adaptive altitude, agility, adjustability, and effective appraisal of real-world functions anytime and anywhere. Furthermore, from the technology perspective, UAVs are predicted to be a vital element of the development of advanced CPSs. Therefore, in this survey, we aim to pinpoint the most fundamental and important design challenges of multi-UAV systems for CPS applications. We highlight key and versatile aspects that span the coverage and tracking of targets and infrastructure objects, energy-efficient navigation, and image analysis using machine learning for fine-grained CPS applications. Key prototypes and testbeds are also investigated to show how these practical technologies can facilitate CPS applications. We present and propose state-of-the-art algorithms to address design challenges with both quantitative and qualitative methods and map these challenges with important CPS applications to draw insightful conclusions on the challenges of each application. Finally, we summarize potential new directions and ideas that could shape future research in these areas.",project-academic
,2019-09-02,a,,intelligent metasurface imager and recognizer," It is ever-increasingly demanded to remotely monitor people in daily life using radio-frequency probing signals. However, conventional systems can hardly be deployed in real-world settings since they typically require objects to either deliberately cooperate or carry a wireless active device or identification tag. To accomplish the complicated successive tasks using a single device in real time, we propose a smart metasurface imager and recognizer simultaneously, empowered by a network of artificial neural networks (ANNs) for adaptively controlling data flow. Here, three ANNs are employed in an integrated hierarchy: transforming measured microwave data into images of whole human body; classifying the specifically designated spots (hand and chest) within the whole image; and recognizing human hand signs instantly at Wi-Fi frequency of 2.4 GHz. Instantaneous in-situ imaging of full scene and adaptive recognition of hand signs and vital signs of multiple non-cooperative people have been experimentally demonstrated. We also show that the proposed intelligent metasurface system work well even when it is passively excited by stray Wi-Fi signals that ubiquitously exist in our daily lives. The reported strategy could open a new avenue for future smart cities, smart homes, human-device interactive interfaces, healthy monitoring, and safety screening free of visual privacy issues.",project-academic
10.1038/S41377-019-0209-Z,2019-10-21,a,Nature Publishing Group,intelligent metasurface imager and recognizer," There is an increasing need to remotely monitor people in daily life using radio-frequency probe signals. However, conventional systems can hardly be deployed in real-world settings since they typically require objects to either deliberately cooperate or carry a wireless active device or identification tag. To accomplish complicated successive tasks using a single device in real time, we propose the simultaneous use of a smart metasurface imager and recognizer, empowered by a network of artificial neural networks (ANNs) for adaptively controlling data flow. Here, three ANNs are employed in an integrated hierarchy, transforming measured microwave data into images of the whole human body, classifying specifically designated spots (hand and chest) within the whole image, and recognizing human hand signs instantly at a Wi-Fi frequency of 2.4 GHz. Instantaneous in situ full-scene imaging and adaptive recognition of hand signs and vital signs of multiple non-cooperative people were experimentally demonstrated. We also show that the proposed intelligent metasurface system works well even when it is passively excited by stray Wi-Fi signals that ubiquitously exist in our daily lives. The reported strategy could open up a new avenue for future smart cities, smart homes, human-device interaction interfaces, health monitoring, and safety screening free of visual privacy issues. Combining radio-frequency imaging with artificial intelligence could make it easier for computers to interact with individuals using non-verbal cues, such as sign language. Lianlin Li from Peking University in Beijing, China and Tie Jun Cui from Southeast University in Nanjing, China, and co-workers fabricated a meter-scale flat panel containing ‘meta-atoms’, tiny electronic devices that manipulate the phases of light waves, arranged in a grid-like pattern. By emitting microwave signals or manipulating stray Wi-Fi signals and detecting echoes bounced back, the metasurface can collect high-resolution imaging data on multiple non-cooperative subjects, even those behind solid walls. The teams fed the microwave data to a series of artificial intelligence algorithms that first identify human shapes, modify signal distributions to better focus on specific body parts, and recognize people's hand signs and vital signs . Experiments showed this setup could continuously monitor hand signals and breathing, even using stray Wi-Fi signals that ubiquitously exist in the daily lives.",project-academic
10.1145/3292500.3330646,2019-07-25,p,ACM,urbanfm inferring fine grained urban flows," Urban flow monitoring systems play important roles in smart city efforts around the world. However, the ubiquitous deployment of monitoring devices, such as CCTVs, induces a long-lasting and enormous cost for maintenance and operation. This suggests the need for a technology that can reduce the number of deployed devices, while preventing the degeneration of data accuracy and granularity. In this paper, we aim to infer the real-time and fine-grained crowd flows throughout a city based on coarse-grained observations. This task is challenging due to the two essential reasons: the spatial correlations between coarse- and fine-grained urban flows, and the complexities of external impacts. To tackle these issues, we develop a method entitled UrbanFM based on deep neural networks. Our model consists of two major parts: 1) an inference network to generate fine-grained flow distributions from coarse-grained inputs by using a feature extraction module and a novel distributional upsampling module; 2) a general fusion subnet to further boost the performance by considering the influences of different external factors. Extensive experiments on two real-world datasets validate the effectiveness and efficiency of our method, demonstrating its state-of-the-art performance on this problem.",project-academic
10.1109/TMC.2019.2957804,2021-03-01,a,IEEE,delay aware microservice coordination in mobile edge computing a reinforcement learning approach," As an emerging service architecture, microservice enables decomposition of a monolithic web service into a set of independent lightweight services which can be executed independently. With mobile edge computing, microservices can be further deployed in edge clouds dynamically, launched quickly, and migrated across edge clouds easily, providing better services for users in proximity. However, the user mobility can result in frequent switch of nearby edge clouds, which increases the service delay when users move away from their serving edge clouds. To address this issue, this article investigates microservice coordination among edge clouds to enable seamless and real-time responses to service requests from mobile users. The objective of this work is to devise the optimal microservice coordination scheme which can reduce the overall service delay with low costs. To this end, we first propose a dynamic programming-based offline microservice coordination algorithm, that can achieve the globally optimal performance. However, the offline algorithm heavily relies on the availability of the prior information such as computation request arrivals, time-varying channel conditions and edge cloud's computation capabilities required, which is hard to be obtained. Therefore, we reformulate the microservice coordination problem using Markov decision process framework and then propose a reinforcement learning-based online microservice coordination algorithm to learn the optimal strategy. Theoretical analysis proves that the offline algorithm can find the optimal solution while the online algorithm can achieve near-optimal performance. Furthermore, based on two real-world datasets, i.e., the Telecom's base station dataset and Taxi Track dataset from Shanghai, experiments are conducted. The experimental results demonstrate that the proposed online algorithm outperforms existing algorithms in terms of service delay and migration costs, and the achieved performance is close to the optimal performance obtained by the offline algorithm.",project-academic
10.1145/3219819.3220110,2018-07-19,p,ACM,dynamic bike reposition a spatio temporal reinforcement learning approach," Bike-sharing systems are widely deployed in many major cities, while the jammed and empty stations in them lead to severe customer loss. Currently, operators try to constantly reposition bikes among stations when the system is operating. However, how to efficiently reposition to minimize the customer loss in a long period remains unsolved. We propose a spatio-temporal reinforcement learning based bike reposition model to deal with this problem. Firstly, an inter-independent inner-balance clustering algorithm is proposed to cluster stations into groups. Clusters obtained have two properties, i.e. each cluster is inner-balanced and independent from the others. As there are many trikes repositioning in a very large system simultaneously, clustering is necessary to reduce the problem complexity. Secondly, we allocate multiple trikes to each cluster to conduct inner-cluster bike reposition. A spatio-temporal reinforcement learning model is designed for each cluster to learn a reposition policy in it, targeting at minimizing its customer loss in a long period. To learn each model, we design a deep neural network to estimate its optimal long-term value function, from which the optimal policy can be easily inferred. Besides formulating the model in a multi-agent way, we further reduce its training complexity by two spatio-temporal pruning rules. Thirdly, we design a system simulator based on two predictors to train and evaluate the reposition model. Experiments on real-world datasets from Citi Bike are conducted to confirm the effectiveness of our model.",project-academic
10.1109/TITS.2021.3069362,2021-04-19,a,IEEE,human trajectory forecasting in crowds a deep learning perspective," Since the past few decades, human trajectory forecasting has been a field of active research owing to its numerous real-world applications: evacuation situation analysis, deployment of intelligent transport systems, traffic operations, to name a few. In this work, we cast the problem of human trajectory forecasting as learning a representation of human social interactions. Early works handcrafted this representation based on domain knowledge. However, social interactions in crowded environments are not only diverse but often subtle. Recently, deep learning methods have outperformed their handcrafted counterparts, as they learn about human-human interactions in a more generic data-driven fashion. In this work, we present an in-depth analysis of existing deep learning-based methods for modelling social interactions. We propose two domain-knowledge inspired data-driven methods to effectively capture these social interactions. To objectively compare the performance of these interaction-based forecasting models, we develop a large scale interaction-centric benchmark TrajNet++, a significant yet missing component in the field of human trajectory forecasting. We propose novel performance metrics that evaluate the ability of a model to output socially acceptable trajectories. Experiments on TrajNet++ validate the need for our proposed metrics, and our method outperforms competitive baselines on both real-world and synthetic datasets.",project-academic
10.1109/IC2E.2019.00-10,2019-06-24,p,IEEE,barista efficient and scalable serverless serving system for deep learning prediction services," Pre-trained deep learning models are increasingly being used to offer a variety of compute-intensive predictive analytics services such as fitness tracking, speech, and image recognition. The stateless and highly parallelizable nature of deep learning models makes them well-suited for serverless computing paradigm. However, making effective resource management decisions for these services is a hard problem due to the dynamic workloads and diverse set of available resource configurations that have different deployment and management costs. To address these challenges, we present a distributed and scalable deep-learning prediction serving system called Barista and make the following contributions. First, we present a fast and effective methodology for forecasting workloads by identifying various trends. Second, we formulate an optimization problem to minimize the total cost incurred while ensuring bounded prediction latency with reasonable accuracy. Third, we propose an efficient heuristic to identify suitable compute resource configurations. Fourth, we propose an intelligent agent to allocate and manage the compute resources by horizontal and vertical scaling to maintain the required prediction latency. Finally, using representative real-world workloads for an urban transportation service, we demonstrate and validate the capabilities of Barista.",project-academic
10.1109/CHASE.2016.18,2016-06-27,p,IEEE,improving tuberculosis diagnostics using deep learning and mobile health technologies among resource poor and marginalized communities," Tuberculosis (TB) is a chronic infectious disease worldwide and remains a major cause of death globally. Of the estimated 9 million people who developed TB in 2013, over 80% were in South-East Asia, Western Pacific, and African. The majority of the infected populations was from resource-poor and marginalized communities with weak healthcare infrastructure. Reducing TB diagnosis delay is critical in mitigating disease transmission and minimizing the reproductive rate of the tuberculosis epidemic. The combination of machine learning and mobile computing techniques offers a unique opportunity to accelerate the TB diagnosis among these communities. The ultimate goal of our research is to reduce patient wait times for being diagnosed with this infectious disease by developing new machine learning and mobile health techniques to the TB diagnosis problem. In this paper, we first introduce major technique barriers and proposed system architecture. Then we report two major progresses we recently made. The first activity aims to develop large-scale, real-world and well-annotated X-ray image database dedicated for automated TB screening. The second research activity focus on developing effective and efficient computational models (in particularly, deep convolutional neural networks (CNN)-based models) to classify the image into different category of TB manifestations. Experimental results have demonstrated the effectiveness of our approach. Our future work includes: (1) to further improve the performance of the algorithms, and (2) to deploy our system in the city of Carabayllo in Peru, a densely occupied urban community and high-burden TB.",project-academic
10.1109/GIOTS.2017.8016227,2017-06-06,p,IEEE,identifying parking spaces detecting occupancy using vision based iot devices," The increasing number of vehicles in high density, urban areas is leading to significant parking space shortages. While systems have been developed to enable visibility into parking space vacancies for drivers, most rely on costly, dedicated sensor devices that require high installation costs. The proliferation of inexpensive Internet of Things (IoT) devices enables the use of compute platforms with integrated cameras that could be used to monitor parking space occupancy. However, even with camera-captured images, manual specification of parking space locations is required before such devices can be used by drivers after device installation. In this paper, we leverage machine learning techniques to develop a method to dynamically identify parking space topologies based on parked vehicle positions. More specifically, we designed and evaluated an occupation detection model to identify vacant parking spaces. We built a prototype implementation of the whole system using a Raspberry Pi and evaluated it on a real-world urban street near the University of Washington campus. The results show that our clustering-based learning technique coupled with our occupation detection pipeline is able to correctly identify parking spaces and determine occupancy without manual specication of parking space locations with an accuracy of 91%. By dynamically aggregating identied parking spaces from multiple IoT devices using Amazon Cloud Services, we demonstrated how a complete, city-wide parking management system can be quickly deployed at low cost.",project-academic
10.1609/AAAI.V34I01.5480,2020-04-03,p,Association for the Advancement of Artificial Intelligence (AAAI),riskoracle a minute level citywide traffic accident forecasting framework," Real-time traffic accident forecasting is increasingly important for public safety and urban management (e.g., real-time safe route planning and emergency response deployment). Previous works on accident forecasting are often performed on hour levels, utilizing existed neural networks with static region-wise correlations taken into account. However, it is still challenging when the granularity of forecasting step improves as the highly dynamic nature of road network and inherent rareness of accident records in one training sample, which leads to biased results and zero-inflated issue. In this work, we propose a novel framework RiskOracle, to improve the prediction granularity to minute levels. Specifically, we first transform the zero-risk values in labels to fit the training network. Then, we propose the Differential Time-varying Graph neural network (DTGN) to capture the immediate changes of traffic status and dynamic inter-subregion correlations. Furthermore, we adopt multi-task and region selection schemes to highlight citywide most-likely accident subregions, bridging the gap between biased risk values and sporadic accident distribution. Extensive experiments on two real-world datasets demonstrate the effectiveness and scalability of our RiskOracle framework.",project-academic
,2020-02-19,a,,riskoracle a minute level citywide traffic accident forecasting framework," Real-time traffic accident forecasting is increasingly important for public safety and urban management (e.g., real-time safe route planning and emergency response deployment). Previous works on accident forecasting are often performed on hour levels, utilizing existed neural networks with static region-wise correlations taken into account. However, it is still challenging when the granularity of forecasting step improves as the highly dynamic nature of road network and inherent rareness of accident records in one training sample, which leads to biased results and zero-inflated issue. In this work, we propose a novel framework RiskOracle, to improve the prediction granularity to minute levels. Specifically, we first transform the zero-risk values in labels to fit the training network. Then, we propose the Differential Time-varying Graph neural network (DTGN) to capture the immediate changes of traffic status and dynamic inter-subregion correlations. Furthermore, we adopt multi-task and region selection schemes to highlight citywide most-likely accident subregions, bridging the gap between biased risk values and sporadic accident distribution. Extensive experiments on two real-world datasets demonstrate the effectiveness and scalability of our RiskOracle framework.",project-academic
10.1145/3137133.3137149,2017-11-08,p,ACM,autocalib automatic traffic camera calibration at scale," Large scale camera installations are the becoming increasingly common in emerging smart cities. Though deployed primarily for surveillance, calibrating these cameras can allow them to measure real-world distances. This enables a broad spectrum of novel applications such as identifying speeding vehicles, city road planning, etc. Today, camera calibration is a tedious manual process and therefore not scalable to large camera installations. In this demo, we present AutoCalib, a system for scalable automatic calibration of traffic cameras. AutoCalib employs deep learning to identify selected key-point features from car images and uses a novel filtering and aggregation algorithm to automatically produce a robust estimate of the camera calibration parameters from just hundreds of samples. AutoCalib is implemented as a web service on Azure that ingests video feeds from traffic cameras and outputs the camera calibration parameters. This demo highlights the various stages in the AutoCalib video processing pipeline, and presents two applications: 1) Measurement of on-ground distances between two points in the image and 2) Measurement of vehicle speeds.",project-academic
,2017-11-29,a,,intelligent traffic light control using distributed multi agent q learning," The combination of Artificial Intelligence (AI) and Internet-of-Things (IoT), which is denoted as AI-powered Internet-of-Things (AIoT), is capable of processing huge amount of data generated from a large number of devices and handling complex problems in social infrastructures. As AI and IoT technologies are becoming mature, in this paper, we propose to apply AIoT technologies for traffic light control, which is an essential component for intelligent transportation system, to improve the efficiency of smart city's road system. Specifically, various sensors such as surveillance cameras provide real-time information for intelligent traffic light control system to observe the states of both motorized traffic and non-motorized traffic. In this paper, we propose an intelligent traffic light control solution by using distributed multi-agent Q learning, considering the traffic information at the neighboring intersections as well as local motorized and non-motorized traffic, to improve the overall performance of the entire control system. By using the proposed multi-agent Q learning algorithm, our solution is targeting to optimize both the motorized and non-motorized traffic. In addition, we considered many constraints/rules for traffic light control in the real world, and integrate these constraints in the learning algorithm, which can facilitate the proposed solution to be deployed in real operational scenarios. We conducted numerical simulations for a real-world map with real-world traffic data. The simulation results show that our proposed solution outperforms existing solutions in terms of vehicle and pedestrian queue lengths, waiting time at intersections, and many other key performance metrics.",project-academic
10.1109/ITSC.2017.8317730,2017-10-01,p,IEEE,intelligent traffic light control using distributed multi agent q learning," The combination of Artificial Intelligence (AI) and Internet-of-Things (IoT), which is denoted as AI powered Internet-of-Things (AIoT), is capable of processing huge amount of data generated from large number of devices and handling complex problems in social infrastructures. As AI and IoT technologies are becoming mature, in this paper, we propose to apply AIoT technologies for traffic light control, which is an essential component for intelligent transportation system, to improve the efficiency of smart city's road system. Specifically, various sensors such as surveillance cameras provide real-time information for intelligent traffic light control system to observe the states of both motorized traffic and non-motorized traffic. In this paper, we propose an intelligent traffic light control solution by using distributed multi-agent Q learning, considering the traffic information at the neighboring intersections as well as local motorized and non-motorized traffic, to improve the overall performance of the entire control system. By using the proposed multi-agent Q learning algorithm, our solution is targeting to optimize both the motorized and non-motorized traffic. In addition, we considered many constraints / rules for traffic light control in the real world, and integrate these constraints in the learning algorithm, which can facilitate the proposed solution to be deployed in real operational scenarios. We conducted numerical simulations for a real-world map with real-world traffic data. The simulation results show that our proposed solution outperforms existing solutions in terms of vehicle and pedestrian queue lengths, waiting time at intersections, and many other key performance metrics.",project-academic
10.1109/TETC.2021.3050733,2021-01-07,a,,sharks smart hacking approaches for risk scanning in internet of things and cyber physical systems based on machine learning," Cyber-physical systems (CPS) and Internet-of-Things (IoT) devices are increasingly being deployed across multiple functionalities, ranging from healthcare devices and wearables to critical infrastructures, e.g., nuclear power plants, autonomous vehicles, smart cities, and smart homes. These devices are inherently not secure across their comprehensive software, hardware, and network stacks, thus presenting a large attack surface that can be exploited by hackers. In this article, we present an innovative technique for detecting unknown system vulnerabilities, managing these vulnerabilities, and improving incident response when such vulnerabilities are exploited. The novelty of this approach lies in extracting intelligence from known real-world CPS/IoT attacks, representing them in the form of regular expressions, and employing machine learning (ML) techniques on this ensemble of regular expressions to generate new attack vectors and security vulnerabilities. Our results show that 10 new attack vectors and 122 new vulnerability exploits can be successfully generated that have the potential to exploit a CPS or an IoT ecosystem. The ML methodology achieves an accuracy of 97.4% and enables us to predict these attacks efficiently with an 87.2% reduction in the search space. We demonstrate the application of our method to the hacking of the in-vehicle network of a connected car. To defend against the known attacks and possible novel exploits, we discuss a defense-in-depth mechanism for various classes of attacks and the classification of data targeted by such attacks. This defense mechanism optimizes the cost of security measures based on the sensitivity of the protected resource, thus incentivizing its adoption in real-world CPS/IoT by cybersecurity practitioners.",project-academic
10.1016/J.ASOC.2016.06.031,2016-10-01,p,Elsevier,an online learning approach to eliminate bus bunching in real time," Graphical abstractDisplay Omitted HighlightsWe proposed a data driven method to predict Bus Bunching (BB) in real-time.Firstly, the bus headways are predicted combining Offline and Online Regression.Then, a BB likelihood is computed for each stop based on such predictions.Finally, a corrective action is selected and deployed using these likelihoods.This methodology is validated using one-year data of 18 real-world bus routes. Recent advances in telecommunications created new opportunities for monitoring public transport operations in real-time. This paper presents an automatic control framework to mitigate the Bus Bunching phenomenon in real-time. The framework depicts a powerful combination of distinct Machine Learning principles and methods to extract valuable information from raw location-based data. State-of-the-art tools and methodologies such as Regression Analysis, Probabilistic Reasoning and Perceptron's learning with Stochastic Gradient Descent constitute building blocks of this predictive methodology. The prediction's output is then used to select and deploy a corrective action to automatically prevent Bus Bunching. The performance of the proposed method is evaluated using data collected from 18 bus routes in Porto, Portugal over a period of one year. Simulation results demonstrate that the proposed method can potentially reduce bunching by 68% and decrease average passenger waiting times by 4.5%, without prolonging in-vehicle times. The proposed system could be embedded in a decision support system to improve control room operations.",project-academic
10.1145/3292500.3330777,2019-07-25,p,ACM,learning to prescribe interventions for tuberculosis patients using digital adherence data," Digital Adherence Technologies (DATs) are an increasingly popular method for verifying patient adherence to many medications. We analyze data from one city served by 99DOTS, a phone-call-based DAT deployed for Tuberculosis (TB) treatment in India where nearly 3 million people are afflicted with the disease each year. The data contains nearly 17,000 patients and 2.1M dose records. We lay the groundwork for learning from this real-world data, including a method for avoiding the effects of unobserved interventions in training data used for machine learning. We then construct a deep learning model, demonstrate its interpretability, and show how it can be adapted and trained in three different clinical scenarios to better target and improve patient care. In the real-time risk prediction setting our model could be used to proactively intervene with 21% more patients and before 76% more missed doses than current heuristic baselines. For outcome prediction, our model performs 40% better than baseline methods, allowing cities to target more resources to clinics with a heavier burden of patients at risk of failure. Finally, we present a case study demonstrating how our model can be trained in an end-to-end decision focused learning setting to achieve 15% better solution quality in an example decision problem faced by health workers.",project-academic
,2021-08-04,a,,adaptive path planning for uav based multi resolution semantic segmentation," In this paper, we address the problem of adaptive path planning for accurate semantic segmentation of terrain using unmanned aerial vehicles (UAVs). The usage of UAVs for terrain monitoring and remote sensing is rapidly gaining momentum due to their high mobility, low cost, and flexible deployment. However, a key challenge is planning missions to maximize the value of acquired data in large environments given flight time limitations. To address this, we propose an online planning algorithm which adapts the UAV paths to obtain high-resolution semantic segmentations necessary in areas on the terrain with fine details as they are detected in incoming images. This enables us to perform close inspections at low altitudes only where required, without wasting energy on exhaustive mapping at maximum resolution. A key feature of our approach is a new accuracy model for deep learning-based architectures that captures the relationship between UAV altitude and semantic segmentation accuracy. We evaluate our approach on the application of crop/weed segmentation in precision agriculture using real-world field data.",project-academic
10.1109/ECMR50962.2021.9568788,2021-08-01,p,IEEE,adaptive path planning for uav based multi resolution semantic segmentation," In this paper, we address the problem of adaptive path planning for accurate semantic segmentation of terrain using unmanned aerial vehicles (UAVs). The usage of UAVs for terrain monitoring and remote sensing is rapidly gaining momentum due to their high mobility, low cost, and flexible deployment. However, a key challenge is planning missions to maximize the value of acquired data in large environments given flight time limitations. To address this, we propose an online planning algorithm which adapts the UAV paths to obtain high-resolution semantic segmentations necessary in areas on the terrain with fine details as they are detected in incoming images. This enables us to perform close inspections at low altitudes only where required, without wasting energy on exhaustive mapping at maximum resolution. A key feature of our approach is a new accuracy model for deep learning-based architectures that captures the relationship between UAV altitude and semantic segmentation accuracy. We evaluate our approach on the application of crop/weed segmentation in precision agriculture using real-world field data.",project-academic
10.1016/J.SCS.2020.102582,2021-01-01,a,Elsevier BV,towards the sustainable development of smart cities through mass video surveillance a response to the covid 19 pandemic," Sustainable smart city initiatives around the world have recently had great impact on the lives of citizens and brought significant changes to society. More precisely, data-driven smart applications that efficiently manage sparse resources are offering a futuristic vision of smart, efficient, and secure city operations. However, the ongoing COVID-19 pandemic has revealed the limitations of existing smart city deployment; hence; the development of systems and architectures capable of providing fast and effective mechanisms to limit further spread of the virus has become paramount. An active surveillance system capable of monitoring and enforcing social distancing between people can effectively slow the spread of this deadly virus. In this paper, we propose a data-driven deep learning-based framework for the sustainable development of a smart city, offering a timely response to combat the COVID-19 pandemic through mass video surveillance. To implementing social distancing monitoring, we used three deep learning-based real-time object detection models for the detection of people in videos captured with a monocular camera. We validated the performance of our system using a real-world video surveillance dataset for effective deployment.",project-academic
,2020-07-11,a,,understanding object detection through an adversarial lens," Deep neural networks based object detection models have revolutionized computer vision and fueled the development of a wide range of visual recognition applications. However, recent studies have revealed that deep object detectors can be compromised under adversarial attacks, causing a victim detector to detect no object, fake objects, or mislabeled objects. With object detection being used pervasively in many security-critical applications, such as autonomous vehicles and smart cities, we argue that a holistic approach for an in-depth understanding of adversarial attacks and vulnerabilities of deep object detection systems is of utmost importance for the research community to develop robust defense mechanisms. This paper presents a framework for analyzing and evaluating vulnerabilities of the state-of-the-art object detectors under an adversarial lens, aiming to analyze and demystify the attack strategies, adverse effects, and costs, as well as the cross-model and cross-resolution transferability of attacks. Using a set of quantitative metrics, extensive experiments are performed on six representative deep object detectors from three popular families (YOLOv3, SSD, and Faster R-CNN) with two benchmark datasets (PASCAL VOC and MS COCO). We demonstrate that the proposed framework can serve as a methodical benchmark for analyzing adversarial behaviors and risks in real-time object detection systems. We conjecture that this framework can also serve as a tool to assess the security risks and the adversarial robustness of deep object detectors to be deployed in real-world applications.",project-academic
10.1007/978-3-030-59013-0_23,2020-09-14,p,"Springer, Cham",understanding object detection through an adversarial lens," Deep neural networks based object detection models have revolutionized computer vision and fueled the development of a wide range of visual recognition applications. However, recent studies have revealed that deep object detectors can be compromised under adversarial attacks, causing a victim detector to detect no object, fake objects, or mislabeled objects. With object detection being used pervasively in many security-critical applications, such as autonomous vehicles and smart cities, we argue that a holistic approach for an in-depth understanding of adversarial attacks and vulnerabilities of deep object detection systems is of utmost importance for the research community to develop robust defense mechanisms. This paper presents a framework for analyzing and evaluating vulnerabilities of the state-of-the-art object detectors under an adversarial lens, aiming to analyze and demystify the attack strategies, adverse effects, and costs, as well as the cross-model and cross-resolution transferability of attacks. Using a set of quantitative metrics, extensive experiments are performed on six representative deep object detectors from three popular families (YOLOv3, SSD, and Faster R-CNN) with two benchmark datasets (PASCAL VOC and MS COCO). We demonstrate that the proposed framework can serve as a methodical benchmark for analyzing adversarial behaviors and risks in real-time object detection systems. We conjecture that this framework can also serve as a tool to assess the security risks and the adversarial robustness of deep object detectors to be deployed in real-world applications.",project-academic
,2020-12-09,a,,a deep reinforcement learning approach for ramp metering based on traffic video data," Ramp metering that uses traffic signals to regulate vehicle flows from the on-ramps has been widely implemented to improve vehicle mobility of the freeway. Previous studies generally update signal timings in real-time based on predefined traffic measures collected by point detectors, such as traffic volumes and occupancies. Comparing with point detectors, traffic cameras-which have been increasingly deployed on road networks-could cover larger areas and provide more detailed traffic information. In this work, we propose a deep reinforcement learning (DRL) method to explore the potential of traffic video data in improving the efficiency of ramp metering. The proposed method uses traffic video frames as inputs and learns the optimal control strategies directly from the high-dimensional visual inputs. A real-world case study demonstrates that, in comparison with a state-of-the-practice method, the proposed DRL method results in 1) lower travel times in the mainline, 2) shorter vehicle queues at the on-ramp, and 3) higher traffic flows downstream of the merging area. The results suggest that the proposed method is able to extract useful information from the video data for better ramp metering controls.",project-academic
10.1155/2021/6669028,2021-10-29,a,Hindawi Limited,a deep reinforcement learning approach for ramp metering based on traffic video data," Ramp metering that uses traffic signals to regulate vehicle flows from the on-ramps has been widely implemented to improve vehicle mobility of the freeway. Previous studies generally update signal timings in real-time based on predefined traffic measurements collected by point detectors, such as traffic volumes and occupancies. Comparing with point detectors, traffic cameras—which have been increasingly deployed on road networks—could cover larger areas and provide more detailed traffic information. In this work, we propose a deep reinforcement learning (DRL) method to explore the potential of traffic video data in improving the efficiency of ramp metering. Vehicle locations are extracted from the traffic video frames and are reformed as position matrices. The proposed method takes the preprocessed video data as inputs and learns the optimal control strategies directly from the high-dimensional inputs. A series of simulation experiments based on real-world traffic data are conducted to evaluate the proposed approach. The results demonstrate that, in comparison with a state-of-the-practice method, the proposed DRL method results in (1) lower travel times in the mainline, (2) shorter vehicle queues at the on-ramp, and (3) higher traffic flows downstream of the merging area. The results suggest that the proposed method is able to extract useful information from the video data for better ramp metering controls.",project-academic
10.1145/3450267.3450538,2021-05-19,p,"Association for Computing Machinery, Inc",deresolver a decentralized negotiation and conflict resolution framework for smart city services," As various smart services are increasingly deployed in modern cities, many unexpected conflicts arise due to various physical world couplings. Existing solutions for conflict resolution often rely on centralized control to enforce predetermined and fixed priorities of different services, which is challenging due to the inconsistent and private objectives of the services. Also, the centralized solutions miss opportunities to more effectively resolve conflicts according to their spatiotemporal locality of the conflicts. To address this issue, we design a decentralized negotiation and conflict resolution framework named DeResolver, which allows services to resolve conflicts by communicating and negotiating with each other to reach a Pareto-optimal agreement autonomously and efficiently. Our design features a two-level semi-supervised learning-based algorithm to predict acceptable proposals and their rankings of each opponent through the negotiation. Our design is evaluated with a smart city case study of three services: intelligent traffic light control, pedestrian service, and environmental control. In this case study, a data-driven evaluation is conducted using a large data set consisting of the GPS locations of 246 surveillance cameras and an automatic traffic monitoring system with more than 3 million records per day to extract real-world vehicle routes. The evaluation results show that our solution achieves much more balanced results, i.e., only increasing the average waiting time of vehicles, the measurement metric of intelligent traffic light control service, by 6.8% while reducing the weighted sum of air pollutant emission, measured for environment control service, by 12.1%, and the pedestrian waiting time, the measurement metric of pedestrian service, by 33.1%, compared to priority-based solution.",project-academic
10.1145/3292500.3330968,2019-07-25,p,ACM,efficient and effective express via contextual cooperative reinforcement learning," Express systems are widely deployed in many major cities. Couriers in an express system load parcels at transit station and deliver them to customers. Meanwhile, they also try to serve the pick-up requests which come stochastically in real time during the delivery process. Having brought much convenience and promoted the development of e-commerce, express systems face challenges on courier management to complete the massive number of tasks per day. Considering this problem, we propose a reinforcement learning based framework to learn a courier management policy. Firstly, we divide the city into independent regions, in each of which a constant number of couriers deliver parcels and serve requests cooperatively. Secondly, we propose a soft-label clustering algorithm named Balanced Delivery-Service Burden (BDSB) to dispatch parcels to couriers in each region. BDSB guarantees that each courier has almost even delivery and expected request-service burden when departing from transit station, giving a reasonable initialization for online management later. As pick-up requests come in real time, a Contextual Cooperative Reinforcement Learning (CCRL) model is proposed to guide where should each courier deliver and serve in each short period. Being formulated in a multi-agent way, CCRL focuses on the cooperation among couriers while also considering the system context. Experiments on real-world data from Beijing are conducted to confirm the outperformance of our model.",project-academic
10.1016/J.JPDC.2017.11.009,2017-11-01,a,Academic Press,person re identification with multiple similarity probabilities using deep metric learning for efficient smart security applications," Abstract None None Surveillance video analysis plays a vital role in the daily operations of smart cities, which increasingly relies on person re-identification technology to sustain smart security applications. However, research challenges of re-identification remain especially in terms of recognizing the different appearances of the same person in a harsh real-world environment: (1) the adaptability of the selected features to the dynamic environment cannot be guaranteed, and (2) existing methods rooted from metric learning aim to find a single metric function, and they lack the ability to measure the different appearances of the same person. To address these problems, this study proposes a multiple deep metric learning method empowered by the functionality of person similarity probability measurement. The proposed method exploits multiple stacked auto-encoder networks and classification networks to quantify pedestrians’ similarity relations. The stacked auto-encoder networks directly recognize persons from surveillance images at the pixel level. The classification networks are equipped with the Softmax regression models and produce multiple similarity probabilities to characterize different appearances belonging to the same person. An Adaboost-like model is designed to fuse the probabilities corresponding to multiple metrics, which ensures a high accuracy of recognition. Experimental results on two public datasets (VIPeR and CUHK-01) indicate that the proposed method outperforms existing algorithms by 2%–10% at rank 1. Based on the similarity probabilities learned by the proposed model, the algorithm for matching the person pair can achieve a time complexity as low as None None None O None None ( None n None ) None None None , which can be deployed at a large scale on the distributed intelligent surveillance network, with each node maintaining limited computing capabilities.",project-academic
10.1109/PERCOM45495.2020.9127383,2020-03-23,p,IEEE,participants selection for from scratch mobile crowdsensing via reinforcement learning," Participant selection is a major research challenge in Mobile Crowdsensing (MCS). Previous approaches commonly assume that adequately long and fixed periods of candidate participants’ historical mobility trajectories are available before the selection process. This enables the frameworks to accurately model mobility which enables the optimization of selection. However, this assumption may not be realistic for newly-released MCS applications or platforms because the candidates have just boarded without previous mobility profiles. The sparsity or even absence of mobility traces will incur inaccurate location prediction of the individual participant, thus imposing negative effects on the participant selection process and hindering the practical deployment of new MCS applications. To this end, this paper investigates a novel problem called ""From-Scratch MCS"" (FS-MCS for short), in which we study how to intelligently select participants to minimize such ""cold-start"" effect. Specifically, we propose a novel framework based on reinforcement learning, which we name RL-Recruiter. With the gradual accumulation of mobility trajectories over time, RL-Recruiter can make a good sequence of participant selection decisions for each sensing slot by incrementally extracting and utilizing the collective mobility patterns of all candidate participants, thus avoiding the prediction of individual participant’s location that is very inaccurate when the training data is sparse. We test our approach experimentally based on two real-world mobility datasets. Our experiment results demonstrate that RL-Recruiter outperforms the baseline approaches under various settings.",project-academic
10.35833/MPCE.2020.000271,2020-12-10,a,SGEPRI,learning based green workload placement for energy internet in smart cities," The Energy Internet is a fundamental infrastructure for deploying green city applications, where energy savingand job acceleration are two critical issues to address. In contrast to existing approaches that focus on static metrics withthe assumption of complete prior knowledge of resource information, both application-level properties and energy-level requirements are realized in this paper by jointly considering energy saving and job acceleration during job runtime. Considering the online environment of smart city applications, the mainobjective is transferred as an optimization problem with a model partition and function assignment. To minimize the energycost and job completion time together, a green workload placement approach is proposed by using the multiaction deep rein-forcement learning method. Evaluations with real-world applications demonstrate the superiority of this method over state-of-the-art methods.",project-academic
10.3390/S19030685,2019-02-07,a,MDPI,towards gas discrimination and mapping in emergency response scenarios using a mobile robot with an electronic nose," Emergency personnel, such as firefighters, bomb technicians, and urban search and rescue specialists, can be exposed to a variety of extreme hazards during the response to natural and human-made disasters. In many of these scenarios, a risk factor is the presence of hazardous airborne chemicals. The recent and rapid advances in robotics and sensor technologies allow emergency responders to deal with such hazards from relatively safe distances. Mobile robots with gas-sensing capabilities allow to convey useful information such as the possible source positions of different chemicals in the emergency area. However, common gas sampling procedures for laboratory use are not applicable due to the complexity of the environment and the need for fast deployment and analysis. In addition, conventional gas identification approaches, based on supervised learning, cannot handle situations when the number and identities of the present chemicals are unknown. For the purpose of emergency response, all the information concluded from the gas detection events during the robot exploration should be delivered in real time. To address these challenges, we developed an online gas-sensing system using an electronic nose. Our system can automatically perform unsupervised learning and update the discrimination model as the robot is exploring a given environment. The online gas discrimination results are further integrated with geometrical information to derive a multi-compound gas spatial distribution map. The proposed system is deployed on a robot built to operate in harsh environments for supporting fire brigades, and is validated in several different real-world experiments of discriminating and mapping multiple chemical compounds in an indoor open environment. Our results show that the proposed system achieves high accuracy in gas discrimination in an online, unsupervised, and computationally efficient manner. The subsequently created gas distribution maps accurately indicate the presence of different chemicals in the environment, which is of practical significance for emergency response.",project-academic
,2021-04-12,a,,predicting pedestrian crossing intention with feature fusion and spatio temporal attention," Predicting vulnerable road user behavior is an essential prerequisite for deploying Automated Driving Systems (ADS) in the real-world. Pedestrian crossing intention should be recognized in real-time, especially for urban driving. Recent works have shown the potential of using vision-based deep neural network models for this task. However, these models are not robust and certain issues still need to be resolved. First, the global spatio-temproal context that accounts for the interaction between the target pedestrian and the scene has not been properly utilized. Second, the optimum strategy for fusing different sensor data has not been thoroughly investigated. This work addresses the above limitations by introducing a novel neural network architecture to fuse inherently different spatio-temporal features for pedestrian crossing intention prediction. We fuse different phenomena such as sequences of RGB imagery, semantic segmentation masks, and ego-vehicle speed in an optimum way using attention mechanisms and a stack of recurrent neural networks. The optimum architecture was obtained through exhaustive ablation and comparison studies. Extensive comparative experiments on the JAAD pedestrian action prediction benchmark demonstrate the effectiveness of the proposed method, where state-of-the-art performance was achieved. Our code is open-source and publicly available.",project-academic
10.1016/J.DCAN.2021.10.007,2021-07-04,a,Elsevier,machine learning in vehicular networking an overview," Abstract None None As vehicle complexity and road congestion increase, combined with the emergence of electric vehicles, the need for intelligent transportation systems to improve on-road safety and transportation efficiency using vehicular networks has become essential. The evolution of high mobility wireless networks will provide improved support for connected vehicles through highly dynamic heterogeneous networks. Particularly, 5G deployment introduces new features and technologies that enable operators to capitalize on emerging infrastructure capabilities. Machine Learning (ML), a powerful methodology for adaptive and predictive system development, has emerged in both vehicular and conventional wireless networks. Adopting data-centric methods enables ML to address highly dynamic vehicular network issues faced by conventional solutions, such as traditional control loop design and optimization techniques. This article provides a short survey of ML applications in vehicular networks from the networking aspect. Research topics covered in this article include network control containing handover management and routing decision making, resource management, and energy efficiency in vehicular networks. The findings of this paper suggest more attention should be paid to network forming/deforming decision making. ML applications in vehicular networks should focus on researching multi-agent cooperated oriented methods and overall complexity reduction while utilizing enabling technologies, such as mobile edge computing for real-world deployment. Research datasets, simulation environment standardization, and method interpretability also require more research attention.",project-academic
10.1007/978-3-030-61616-8_42,2020-09-15,p,"Springer, Cham",cabin a novel cooperative attention based location prediction network using internal external trajectory dependencies," Nowadays, large quantities of advanced locating sensors have been widely used, which makes it possible to deploy location-based service (LBS) enhanced by intelligent technologies. Location prediction, as one of the most fundamental technologies, aims to acquire possible location at next timestamp based on the moving pattern of current trajectories. High accuracy of location prediction could enrich and increase user experience of various LBSs and brings lots of benefits to service providers. Lots of state-of-the-art research try to model spatial-temporal trajectories based on recurrent neural networks (RNNs), yet fails to arrive at a practical usability. We observe that there exists two ways to improve through attention mechanism which performs well in computer vision and natural language processing domains. Firstly recent location prediction methods are usually equipped with single-head attention mechanism to promote accuracy, which is only able to capture limited information in a specific subspace at a specific position. Secondly, existing methods focus on external relations between spatial-temporal trajectories, but miss internal relations in each spatial-temporal trajectory. To tackle the problem of model spatial-temporal patterns of mobility, we propose a novel Cooperative Attention Based location prediction network using Internal-External trajectory dependencies correspondingly in this paper. We also design and perform experiments on two real-world check-in datasets, Foursquare data in New York and Tokyo cities. Evaluation results demonstrate that our method outperforms state-of-the-art models.",project-academic
10.1016/J.TRANPOL.2020.11.008,2021-02-01,a,Pergamon,predicting weather induced delays of high speed rail and aviation in china," Abstract None None High-speed rail (HSR) has become a competitive mode with aviation for medium-distance intercity travel, given the massive deployment of the HSR infrastructure network in China. While the travel experience with both HSR and air has become more convenient, the systems’ operational reliability in terms of punctuality remains a key concern, especially during disruptive events, such as under severe weather conditions. Although previous studies have attempted to investigate the impact of severe weather events on the operational performance of transportation systems, there is still a lack of ability to forecast to what extent the performance of different transportation systems may vary under various conditions. This study develops an integrated modeling framework that allows us to predict the performance of weather-induced delays of different transportation systems, including HSR and aviation. By applying machine-learning methods to real-world transportation performance data, the study examines the robustness of the method, variations of data characteristics and the different applications of the predictive modeling system. Overall, the concept and modeling framework provide important implications for the improvement of transportation system resilience to various severe weather-related disruptions through the understanding of the impact and its predictability of the system performance.",project-academic
10.3390/RS11121395,2019-06-12,a,MDPI AG,a hierarchical urban forest index using street level imagery and deep learning," We develop a method based on computer vision and a hierarchical multilevel model to derive an Urban Street Tree Vegetation Index which aims to quantify the amount of vegetation visible from the point of view of a pedestrian. Our approach unfolds in two steps. First, areas of vegetation are detected within street-level imagery using a state-of-the-art deep neural network model. Second, information is combined from several images to derive an aggregated indicator at the area level using a hierarchical multilevel model. The comparative performance of our proposed approach is demonstrated against a widely used image segmentation technique based on a pre-labelled dataset. The approach is deployed to a real-world scenario for the city of Cardiff, Wales, using Google Street View imagery. Based on more than 200,000 street-level images, an urban tree street-level indicator is derived to measure the spatial distribution of tree cover, accounting for the presence of obstructing objects present in images at the Lower Layer Super Output Area (LSOA) level, corresponding to the most commonly used administrative areas for policy-making in the United Kingdom. The results show a high degree of correspondence between our tree street-level score and aerial tree cover estimates. They also evidence more accurate estimates at a pedestrian perspective from our tree score by more appropriately capturing tree cover in areas with large burial, woodland, formal open and informal open spaces where shallow trees are abundant, in high density residential areas with backyard trees, and along street networks with high density of high trees. The proposed approach is scalable and automatable. It can be applied to cities across the world and provides robust estimates of urban trees to advance our understanding of the link between mental health, well-being, green space and air pollution. View Full-Text",project-academic
10.1109/ICCCN.2017.8038380,2017-07-01,p,Institute of Electrical and Electronics Engineers Inc.,witraffic low cost and non intrusive traffic monitoring system using wifi," The traffic monitoring system is an imperative tool for traffic analysis and transportation planning. In this paper, we present WiTraffic: the first WiFi-based traffic monitoring system. Compared with existing solutions, it is non-intrusive, cost- effective, and easy-to-deploy. Unique WiFi Channel State Information (CSI) patterns of passing vehicles are captured and analyzed to effectively perform vehicle classification, lane detection, and speed estimation. A machine learning technique is adopted to train vehicle classification models and efficiently categorize vehicles. An Earth Mover's Distance (EMD)-based vehicle lane detection algorithm and vehicle speed estimation mechanism are proposed to further utilize WiFi CSI to identify the lane in which a vehicle is located and to estimate the vehicle speed. We implemented WiTraffic with off-the-shelf WiFi devices and performed real-world experiments with over a week of field data collection in both local roads and highways. The results show that the mean classification accuracy, lane detection accuracy for both local road and highway settings are around 96%, and 95%, respectively. The average root-mean- square error (RMSE) of the proposed CSI-based speed estimation method on a highway was 5mph in our experimental settings.",project-academic
10.1109/TMC.2021.3077636,2021-05-05,a,Institute of Electrical and Electronics Engineers (IEEE),rl recruiter mobility predictability aware participant selection learning for from scratch mobile crowdsensing," Participant selection is a fundamental research issue in Mobile Crowdsensing (MCS). Previous approaches commonly assume that adequately long periods of candidate participants' historical mobility trajectories are available to model their patterns before the selection process, which is not realistic for some new MCS applications or platforms. The sparsity or even absence of mobility traces will incur inaccurate location prediction, thus undermining the deployment of new MCS applications. To this end, this paper investigates a novel problem called From-Scratch MCS (FS-MCS for short), in which we study how to intelligently select participants to minimize such cold-start effect. Specifically, we propose a novel framework based on reinforcement learning, named RL-Recruiter+. With the gradual accumulation of mobility trajectories over time, RL-Recruiter+ is able to make a good sequence of participant selection decisions for each sensing slot. Compared to its previous version, RL-Recruiter, Re-Recruiter+ jointly considers both the previous coverage and current mobility predictability when training the participant selection decision model. We evaluate our approach experimentally based on two real-world mobility datasets. The results demonstrate that RL-Recruiter+ outperforms the baseline approaches, including RL-Recruiter under various settings.",project-academic
10.1007/978-3-030-75472-3_5,2021-01-01,a,"Springer, Cham",autonomous navigation with mobile robots using deep learning and the robot operating system," Autonomous navigation is a long-standing field of robotics research, which provides an essential capability for mobile robots to execute a series of tasks on the same environments performed by human everyday. In this chapter, we present a set of algorithms to train and deploy deep networks for autonomous navigation of mobile robots using the Robot Operation System (ROS). We describe three main steps to tackle this problem: (i) collecting data in simulation environments using ROS and Gazebo; (ii) designing deep network for autonomous navigation, and (iii) deploying the learned policy on mobile robots in both simulation and real-world. Theoretically, we present deep learning architectures for robust navigation in normal environments (e.g., man-made houses, roads) and complex environments (e.g., collapsed cities, or natural caves). We further show that the use of visual modalities such as RGB, Lidar, and point cloud is essential to improve the autonomy of mobile robots. Our project website and demonstration video can be found at https://sites.google.com/site/autonomousnavigationros.",project-academic
10.1145/3340531.3411871,2020-10-19,p,ACM,cooperative multi agent reinforcement learning in express system," Express systems are widely deployed in many major cities. One type of important tasks in the system is to pick up packages from customers in time. As pick-up requests come in real time and there are many couriers picking up packages, how to dispatch couriers to ensure the cooperation among them and to complete more pick-up tasks in a long time, is very important but challenging. In this paper, we propose a reinforcement learning based framework to learn courier dispatching policies. At first, we divide the city into independent regions, inner each of which a constant number of couriers pick up packages at the same time. Besides reducing problem complexity, city division has practical operation benefits. Afterwards, we focus on each region separately. For each region, we propose a Cooperative Multi-Agent Reinforcement Learning model, i.e. CMARL, to learn the optimal courier dispatching policy in it. CMARL tries to maximize the total number of completed pick-up tasks by all couriers in a long time. Our model achieves this target by combining two Markov Decision Processes, one to guarantee the cooperation among couriers, and the other one to ensure the long-term optimization. After obtaining the value functions of these two MDPs, a new value function is designed to trade off them, based on which we can infer the courier dispatching policy. Experiments based on real-world road network data and historical express data from Beijing are conducted, to confirm the superiority of our model compared with nine baselines.",project-academic
10.1016/J.SCITOTENV.2020.139625,2020-10-01,a,Elsevier BV,modelling of instantaneous emissions from diesel vehicles with a special focus on nox insights from machine learning techniques," Accurate instantaneous vehicle emissions models are vital for evaluating the impacts of road transport on air pollution at high temporal and spatial resolution. In this study, we apply machine learning techniques to a dataset of 70 diesel vehicles tested in real-world driving conditions to: (i) cluster vehicles with similar emissions performance, and (ii) model instantaneous emissions. The application of dynamic time warping and clustering analysis by NOx emissions resulted in 17 clusters capturing 88% of trips in the dataset. We show that clustering effectively groups vehicles with similar emissions profiles, however no significant correlation between emissions and vehicle characteristics (i.e. engine size, vehicle weight) were found. For each cluster, we evaluate three instantaneous emissions models: a look-up table (LT) approach, a non-linear regression (NLR) model and a neural network multi-layer perceptron (MLP) model. The NLR model provides accurate instantaneous NOx predictions, on par with the MLP: relative errors in prediction of emission factors are below 20% for both models, average fractional biases are -0.01 (s.d. 0.02) and -0.0003 (s.d. 0.04), and average normalised mean squared errors are 0.25 (s.d. 0.14) and 0.29 (s.d. 0.16), for the NLR and MLP models respectively. However, neural networks are better able to deal with vehicles not belonging to a specific cluster. The new models that we present rely on simple inputs of vehicle speed and acceleration, which could be extracted from existing sources including traffic cameras and vehicle tracking devices, and can therefore be deployed immediately to enable fast and accurate prediction of vehicle NOx emissions. The speed and the ease of use of these new models make them an ideal operational tool for policy makers aiming to build emission inventories or evaluate emissions mitigation strategies.",project-academic
10.1109/ACCESS.2020.2964562,2020-01-15,a,Institute of Electrical and Electronics Engineers (IEEE),context aware proactive 5g load balancing and optimization for urban areas," In the fifth-generation (5G) mobile networks, the traffic is estimated to have a fast-changing and imbalance spatial-temporal distribution. It is challenging for a system-level optimisation to deal with while empirically maintaining quality of service. The 5G load balancing aims to address this problem by transferring the extra traffic from a high-load cell to its neighbouring idle cells. In recent literature, controller and machine learning algorithms are applied to assist the self-optimising and proactive schemes in drawing load balancing decisions. However, these algorithms lack the ability of forecasting upcoming high traffic demands, especially during popular events. This shortage leads to cold-start problems because of reacting to the changes in the heterogeneous dense deployment. Notably, the hotspots corresponding with skew load distribution will result in low convergence speed. To address these problems, this paper contributes to three aspects. Firstly, urban event detection is proposed to forecast the changes in cellular hotspots based on Twitter data for enabling context-awareness. Secondly, a proactive 5G load balancing strategy is simulated considering the prediction of the skewed-distributed hotspots in urban areas. Finally, we optimise this context-aware proactive load balancing strategy by forecasting the best activation time. This paper represents one of the first works to couple the real-world urban event detection with proactive load balancing.",project-academic
10.1109/ICCVE.2013.6799799,2013-12-01,p,IEEE,detecting urban traffic congestion with single vehicle," Traffic congestion in urban areas is a severe problem in many cities around the world. Conventional infrastructure-based solutions to detect traffic congestion, such as surveillance cameras and road surface inductive loops, have the limitations of high deployment costs and limited coverage. In recent years, due to the popularity of mobile devices, solutions that do not require pre-deployed infrastructure start to emerge; in these solutions, sensor data is collected by mobile devices onboard the vehicles, sent to a central server via vehicle-to-infrastructure (V2I) or cellular communications, and used collectively to determine the traffic states of the roads. However, existing solutions require data from a considerably large number of vehicles on the same road to accurately detect traffic congestion of a particular road. In this paper, we propose a novel approach to detect the traffic states of the roads with only the data from a single vehicle. The biggest advantage of such an approach is that, unlike previously proposed solutions, the system can function properly even if there is only a smaller number of vehicles equipped with the system, which is usually the case at the early stage of the deployment of a vehicle-to-vehicle (V2V) network or a large-scale intelligent transportation system. In our solution, machine learning mechanisms are utilized to classify the traffic state by extracting the movement behaviors of a vehicle. Our model development and performance evaluation utilize highly accurate vehicle traces collected at several real-world intersections with lidar. In addition, to properly label the obtained data traces to either congested or free-flow and accurately reflect the reality, a previously proposed theoretical method is used in combination with human labeling. Evaluation shows that our approach can achieve a detection accuracy of 88.94%.",project-academic
10.1049/CP.2019.0159,2019-01-01,p,Institution of Engineering and Technology,impact assessment for data driven innovation in cognitive iot architectures," Data-driven innovation in sectors such as built environment and transport is gaining traction in research, government and industrial communities. A number of industry-academic collaborations are emphasizing on use of data to empower emerging technologies. Large scale deployments of IoT devices and sensors are making data available at an unprecedented scale. This is leading to design and development of governed data infrastructures to harness the value of socio-technological processes and maximise the economic impact. Application of cognitive technologies such as machine learning and artificial intelligence is paving way for newer ways of data-driven innovation for decision making, predictive analysis and scenario modelling. The paper describes the various building blocks of the data-innovation engine and substantiates it through an anonymised real-world use-case from the built environment. It describes the emerging data-driven innovation models in these sectors and the corresponding value created by them. Finally, it proposes an impact assessment framework to understand and evaluate data-driven innovation in an organisation.",project-academic
10.1109/MSP.2020.3033086,2021-01-01,a,Institute of Electrical and Electronics Engineers (IEEE),autonomous driving part 2 learning and cognition from the guest editors," This special issue covering autonomous driving is presented in two parts: Part 1—Sensing and Perception was published in the July 2020 issue of IEEE Signal Processing Magazine and this issue, Part 2—Learning and Cognition. Learning and cognition models and, in particular, deep learning-based models are at the core of autonomous vehicles and automated driving. Autonomous driving and, more generally, automated driving are receiving increasing attention, and significant resources are being deployed to enable safe, reliable, and efficient automated mobility in real-world environments.",project-academic
10.1109/ICWS.2017.76,2017-06-01,p,IEEE Computer Society,early air pollution forecasting as a service an ensemble learning approach," Air quality has become a major global concern for human beings involving all social stratums, for both developing and developed countries. Web service of precise and early air pollution forecasting is of great importance as it allows people to pro-actively take preventative and protective measurements. As an endeavor on the course of machine learning based air quality forecasting, this paper presents an initiative and its technological details in solving this challenging problem. Specifically, this work involves three major highlights regarding with both algorithmic innovation and deployment with its impact: 1) We propose a multi-channel ensemble learning framework, 2) We propose a new supervised feature learning and extraction method, i.e. sufficient statistics feature mapping based on Deep Boltzman Machine, which serves as a building block for our learning system, 3) We target our air pollution prediction method to the city of Beijing, China as it is at the forefront for battling against air pollution, which is embodied as a web service for prediction. Extensive experiments of real time air pollution forecasting on the real-world data demonstrates the effectiveness of the proposed method and value of the deployed web service system.",project-academic
10.1109/ACCESS.2020.2981463,2020-03-18,a,IEEE,cooperative autonomous driving oriented mec aided 5g v2x prototype system design field tests and ai based optimization tools," Vehicle-to-Everything (V2X) requirements from cooperative autonomous driving can be characterized as ultra-reliable, low latency, high traffic, and high mobility. These requirements introduce great challenges in the air interface and protocol stack design, resource allocation, network deployment, and all the way up to mobile (or multi-access) edge computing (MEC), cloud and application layer. In this paper, we present a cooperative autonomous driving oriented MEC-aided 5G-V2X prototype system design and the rationale behind the design choices. The prototype system is developed based on a next-generation radio access network (NG-RAN) experimental platform, a cooperative driving vehicle platoon, and an MEC server providing high definition (HD) 3D dynamic map service. Field tests are conducted and the results demonstrate that the combination of 5G-V2X, MEC and cooperative autonomous driving can be pretty powerful. Considering the remaining challenges in the commercial deployment of 5G-V2X networks and future researches, we propose two artificial intelligence (AI) based optimization tools. The first is a deep-learning-based tool called deep spatio-temporal residual networks with a permutation operator (PST-ResNet). By providing city-wide user and network traffic prediction, PST-ResNet can help to reduce the capital expense (CAPEX) and operating expense (OPEX) costs of commercial 5G-V2X networks. The second is a swarm intelligence based optimization tool called subpopulation collaboration based dynamic self-adaption cuckoo Search (SC-DSCS), which can be widely used to solve complex optimization problems in future researches. The effectiveness of proposed optimization tools is verified by real-world data and benchmark functions.",project-academic
10.1109/TBDATA.2019.2935057,2019-08-14,a,Institute of Electrical and Electronics Engineers (IEEE),citywide traffic volume inference with surveillance camera records," Real-time traffic monitoring becomes an essential part of an intelligent city. In recent years, the adoption of surveillance cameras is rapidly growing because they are helpful to manage and control the traffic. However, it is impossible to install cameras on every road in a city due to the high costs of deployment and maintenance. Given the information from limited surveillance cameras, can we infer the citywide traffic volume accurately? This is a challenging question because we have no historical data on the roads without cameras. In this paper, we propose a framework named CityVolInf to infer citywide traffic volume based on surveillance camera records. Our framework combines a semi-supervised learning-based similarity module with a novel simulation module to address the above challenges. While the similarity module focuses on spatiotemporal correlations of traffic volume between road segments, the simulation module utilizes incomplete trajectories to model transitions of traffic volume between adjacent road segments. Our framework bridges the conventional data-driven approach and transportation domain knowledge from the simulator. Extensive experiments on a real-world dataset, containing 405,370,631 camera records collected from 1,704 surveillance cameras over a period of 31 days in Jinan, China, demonstrate the effectiveness of CityVolInf compared with existing methods.",project-academic
,2011-12-04,p,USENIX Association,auto learning of smtp tcp transport layer features for spam and abusive message detection," Botnets are a significant source of abusive messaging (spam, phishing, etc) and other types of malicious traffic. A promising approach to help mitigate botnet-generated traffic is signal analysis of transport-layer (i.e. TCP/IP) characteristics, e.g. timing, packet reordering, congestion, and flow-control. Prior work [4] shows that machine learning analysis of such traffic features on an SMTP MTA can accurately differentiate between botnet and legitimate sources. We make two contributions toward the real-world deployment of such techniques: i) an architecture for real-time on-line operation; and ii) auto-learning of the unsupervised model across different environments without human labeling (i.e. training). We present a ""SpamFlow"" SpamAssassin plugin and the requisite auxiliary daemons to integrate transport-layer signal analysis with a popular open-source spam filter. Using our system, we detail results from a production deployment where our auto-learning technique achieves better than 95 percent accuracy, precision, and recall after reception of ≈ 1,000 emails.",project-academic
10.1109/LCNW.2014.6927697,2014-10-20,p,IEEE,where is that car parked a wireless sensor network based approach to detect car positions," The global trend of increased urbanization makes space rare in city environments in general and for parking in particular. In addition, cars become bigger and often use more than one parking space. As a result neighboring parking spaces can be affected by a parked car. So, a basically free parking space might be too narrow for an arriving car depending on the arriving car's size. Therefore, means to detect car positions on parking spaces in a fine granular way are required to detect such situations and avoid inefficient parking space searches. Wireless sensor networks provide the possibility to sense the exact occupation of a parking space and potential influences on neighboring parking spaces. However, current solutions focus only on the detection if a parking space is occupied or not. In our work, we present a sensor deployment and a machine learning-based approach able to provide the mentioned more fine-granular detection level. We have conducted an extensive real-world evaluation of our solution, in particular considering different characteristics of today's car bodies. In our tests, our approach achieved an accuracy of more than 98%.",project-academic
10.1145/3459637.3481916,2021-10-26,p,,eta prediction with graph neural networks in google maps," Travel-time prediction constitutes a task of high importance in transportation networks, with web mapping services like Google Maps regularly serving vast quantities of travel time queries from users and enterprises alike. Further, such a task requires accounting for complex spatiotemporal interactions (modelling both the topological properties of the road network and anticipating events---such as rush hours---that may occur in the future). Hence, it is an ideal target for graph representation learning at scale. Here we present a graph neural network estimator for estimated time of arrival (ETA) which we have deployed in production at Google Maps. While our main architecture consists of standard GNN building blocks, we further detail the usage of training schedule methods such as MetaGradients in order to make our model robust and production-ready. We also provide prescriptive studies: ablating on various architectural decisions and training regimes, and qualitative analyses on real-world situations where our model provides a competitive edge. Our GNN proved powerful when deployed, significantly reducing negative ETA outcomes in several regions compared to the previous production baseline (40+% in cities like Sydney).",project-academic
10.1016/J.IOT.2020.100205,2020-09-01,p,Elsevier,multi uav allocation framework for predictive crime deterrence and data acquisition," Abstract None None The recent decline in the number of police and security force personnel has raised a serious security issue that could lead to reduced public safety and delayed response to crimes in urban areas. This may be alleviated in part by utilizing micro or small unmanned aerial vehicles (UAVs) and their high-mobility on-board sensors in conjunction with machine-learning techniques such as neural networks to offer better performance in predicting times and places that are high-risk and deterring crimes. The key to the success of such operation lies in the suitable placement of UAVs. This paper proposes a multi-UAV allocation framework for predictive crime deterrence and data acquisition that consists of the overarching methodology, a problem formulation, and an allocation method that work with a prediction model using a machine learning approach. In contrast to previous studies, our framework provides the most effective arrangement of UAVs for maximizing the chance to apprehend offenders whilst also acquiring data that will help improve the performance of subsequent crime prediction. This paper presents the system architecture assumed in this study, followed by a detailed description of the methodology, the formulation of the problem, and the UAV allocation method of the proposed framework. Our framework is tested using a real-world crime dataset to evaluate its performance with respect to the expected number of crimes deterred by the UAV patrol. Furthermore, to address the engineering practice of the proposed framework, we discuss the feasibility of the simulated deployment scenario in terms of energy consumption and the relationship between data analysis and crime prediction.",project-academic
10.3390/S20195450,2020-09-23,a,Multidisciplinary Digital Publishing Institute,cloud2edge elastic ai framework for prototyping and deployment of ai inference engines in autonomous vehicles," Self-driving cars and autonomous vehicles are revolutionizing the automotive sector, shaping the future of mobility altogether. Although the integration of novel technologies such as Artificial Intelligence (AI) and Cloud/Edge computing provides golden opportunities to improve autonomous driving applications, there is the need to modernize accordingly the whole prototyping and deployment cycle of AI components. This paper proposes a novel framework for developing so-called AI Inference Engines for autonomous driving applications based on deep learning modules, where training tasks are deployed elastically over both Cloud and Edge resources, with the purpose of reducing the required network bandwidth, as well as mitigating privacy issues. Based on our proposed data driven V-Model, we introduce a simple yet elegant solution for the AI components development cycle, where prototyping takes place in the cloud according to the Software-in-the-Loop (SiL) paradigm, while deployment and evaluation on the target ECUs (Electronic Control Units) is performed as Hardware-in-the-Loop (HiL) testing. The effectiveness of the proposed framework is demonstrated using two real-world use-cases of AI inference engines for autonomous vehicles, that is environment perception and most probable path prediction.",project-academic
10.1061/(ASCE)0733-947X(1999)125:4(281),1999-07-01,a,American Society of Civil Engineers,performance of automatic ann based incident detection on freeways," Automatic incident detection on freeways is an essential ingredient for the successful deployment of Intelligent Transportation Systems. Several incident detection algorithms have been developed in the past three decades; however, most of them have not shown the anticipated performance in terms of detection rate and false alarm rate. Recently, the artificial neural networks (ANN) have been introduced to incident detection and shown success over the traditional algorithms. This study explores the application of two neural network models, namely, the Multi-Layer Feed-Forward and the Fuzzy ART algorithm. This study was conducted on the central corridor of I-4 in Orlando using real-world data collected via the traffic surveillance system. Different scenarios were considered to improve the performance and to capture the sensitivity of the developed algorithms to some factors. The study results showed that the Fuzzy ART algorithm has generally outperformed the Multi-Layer Feed-Forward network and California algorithms #7 and #8.",project-academic
10.1109/TITS.2020.3018259,2021-01-01,a,Institute of Electrical and Electronics Engineers (IEEE),detecting anomalies in intelligent vehicle charging and station power supply systems with multi head attention models," Safe and reliable intelligent charging stations are imperative in an intelligent transportation infrastructure. Over the past few years, a big number of smart charging stations have been deployed, and most of them are online and connected, resulting in potential risks of threats. Although there exists related work on securing intelligent vehicles, very little work focused on the security of charging devices. Unlike traditional network systems, these power-related Industrial Control Systems (ICSs) use many different proprietary protocols and diverse interactions. Traditional anomaly detection methods based on network traffic are thus not suitable for these systems. In this work, we propose an anomaly detection method in real vehicle power supply systems based on a deep architecture model. In particular, we propose a novel traffic anomaly detection model based on Multi-Head Attentions (MHA) that take into account the inherent correlations of traffic generated by ICSs. The MHA model is employed to substitute the traditional feature extraction and rule making process with an acceptable computational cost for classifying traffic data. It is an attention-based model that employs Google Transformer encoder architecture to extract recessive features of traffic for anomaly detection. The effectiveness of the model is demonstrated by experiments on two real-world power ICS testbeds including a substation with a slave charging station and a power generation simulation platform based on a distributed control system. Comprehensive experimental results indicate that the MHA model outperforms the Convolutional Neural Networks (CNN)-based and classical machine learning detection models with an accuracy rate of 99.86%.",project-academic
10.1109/ICDE51399.2021.00160,2021-04-19,p,IEEE,an empirical experiment on deep learning models for predicting traffic data," To tackle ever-increasing city traffic congestion problems, researchers have proposed deep learning models to aid decision-makers in the traffic control domain. Although the proposed models have been remarkably improved in recent years, there are still questions that need to be answered before deploying models. For example, it is difficult to figure out which models provide state-of-the-art performance, as recently proposed models have often been evaluated with different datasets and experiment environments. It is also difficult to determine which models would work when traffic conditions change abruptly (e.g., rush hour). In this work, we conduct two experiments to answer the two questions. In the first experiment, we conduct an experiment with the state-of-the-art models and the identical public datasets to compare model performance under a consistent experiment environment. We then extract a set of temporal regions in the datasets, whose speeds change abruptly and use these regions to explore model performance with difficult intervals. The experiment results indicate that Graph-WaveNet and GMAN show better performance in general. We also find that prediction models tend to have varying performances with data and intervals, which calls for in-depth analysis of models on difficult intervals for real-world deployment.",project-academic
,2021-05-12,a,,an empirical experiment on deep learning models for predicting traffic data," To tackle ever-increasing city traffic congestion problems, researchers have proposed deep learning models to aid decision-makers in the traffic control domain. Although the proposed models have been remarkably improved in recent years, there are still questions that need to be answered before deploying models. For example, it is difficult to figure out which models provide state-of-the-art performance, as recently proposed models have often been evaluated with different datasets and experiment environments. It is also difficult to determine which models would work when traffic conditions change abruptly (e.g., rush hour). In this work, we conduct two experiments to answer the two questions. In the first experiment, we conduct an experiment with the state-of-the-art models and the identical public datasets to compare model performance under a consistent experiment environment. We then extract a set of temporal regions in the datasets, whose speeds change abruptly and use these regions to explore model performance with difficult intervals. The experiment results indicate that Graph-WaveNet and GMAN show better performance in general. We also find that prediction models tend to have varying performances with data and intervals, which calls for in-depth analysis of models on difficult intervals for real-world deployment.",project-academic
10.1145/3381005,2020-03-18,a,Association for Computing Machinery (ACM),d3p data driven demand prediction for fast expanding electric vehicle sharing systems," The future of urban mobility is expected to be shared and electric. It is not only a more sustainable paradigm that can reduce emissions, but can also bring societal benefits by offering a more affordable on-demand mobility option to the general public. Many car sharing service providers as well as automobile manufacturers are entering the competition by expanding both their EV fleets and renting/returning station networks, aiming to seize a share of the market and to bring car sharing to the zero emissions level. During their fast expansion, one determinant for success is the ability of predicting the demand of stations as the entire system is growing continuously. There are several challenges in this demand prediction problem: First, unlike most of the existing work which predicts demand only for static systems or at few stages of expansion, in the real world we often need to predict the demand as or even before stations are being deployed or closed, to provide information and decision support. Second, for the new stations to be deployed, there is no historical data available to help the prediction of their demand. Finally, the impact of deploying/closing stations on the other stations in the system can be complex. To address these challenges, we formulate the demand prediction problem in the context of fast expanding electric vehicle sharing systems, and propose a data-driven demand prediction approach which aims to model the expansion dynamics directly from the data. We use a local temporal encoding process to handle the historical data for each existing station, and a dynamic spatial encoding process to take correlations between stations into account with Graph Convolutional Neural Networks (GCN). The encoded features are fed to a multi-scale predictor, which forecasts both the long-term expected demand of the stations and their instant demand in the near future. We evaluate the proposed approach with real-world data collected from a major EV sharing platform for one year. Experimental results demonstrate that our approach significantly outperforms the state of the art, showing up to three-fold performance gain in predicting demand for the expanding EV sharing systems.",project-academic
10.1109/JIOT.2020.3013272,2021-02-01,a,IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC,physical activity recognition with statistical deep fusion model using multiple sensory data for smart health," Nowadays, enhancing the living standard with smart healthcare via the Internet of Things is one of the most critical goals of smart cities, in which artificial intelligence plays as the core technology. Many smart services, deployed according to wearable sensor-based physical activity recognition, have been able to early detect unhealthy daily behaviors and further medical risks. Numerous approaches have studied shallow handcrafted features coupled with traditional machine learning (ML) techniques, which find it difficult to model real-world activities. In this work, by revealing deep features from deep convolutional neural networks (DCNNs) in fusion with conventional handcrafted features, we learn an intermediate fusion framework of human activity recognition (HAR). According to transforming the raw signal value to pixel intensity value, segmentation data acquired from a multisensor system are encoded to an activity image for deep model learning. Formulated by several novel residual triple convolutional blocks, the proposed DCNN allows extracting multiscale spatiotemporal signal-level and sensor-level correlations simultaneously from the activity image. In the fusion model, the hybrid feature merged from the handcrafted and deep features is learned by a multiclass support vector machine (SVM) classifier. Based on several experiments of performance evaluation, our fusion approach for activity recognition has achieved the accuracy over 96.0% on three public benchmark data sets, including Daily and Sport Activities, Daily Life Activities, and RealWorld. Furthermore, the method outperforms several state-of-the-art HAR approaches and demonstrates the superiority of the proposed intermediate fusion model in multisensor systems.",project-academic
10.1109/MDM48529.2020.00036,2020-06-01,p,IEEE,notable site recognition using deep learning on mobile and crowd sourced imagery," Being able to automatically recognize notable sites in the physical world using artificial intelligence embedded in mobile devices can pave the way to new forms of urban exploration and open novel channels of interactivity between residents, travellers and cities. Although the development of outdoor recognition systems has been a topic of interest for a while, most works have been limited in geographic coverage due to lack of high quality image data that can be used for training site recognition engines. As a result, prior systems usually lack generality and operate on a limited scope of pre-elected sites. In this work we design a mobile system that can automatically recognise sites of interest and project relevant information to a user that navigates the city. We build a collection of notable sites using Wikipedia and then exploit online services such as Google Images and Flickr to collect large collections of crowd-sourced imagery describing those sites. These images are then used to train minimal deep learning architectures that can be effectively deployed to dedicated applications on mobile devices. By conducting an evaluation and performing a series of online and real-world experiments, we recognise a number of key challenges in deploying site recognition system and highlight the importance of incorporating mobile contextual information to facilitate the visual recognition task. The similarity in the feature maps of objects that undergo identification, the presence of noise in crowd-sourced imagery and arbitrary user induced inputs are among the factors the impede correct classification for deep learning models. We show how curating the training data through the application of a class-specific image de-noising method and the incorporation of information such as user location, orientation and attention patterns can allow for significant improvement in classification accuracy and the election of an end-to-end system that can effectively be used to recognise sites in the wild.",project-academic
10.1109/TNSM.2021.3074618,2021-04-21,a,IEEE,ai based network aware service function chain migration in 5g and beyond networks," While the 5G network technology is maturing and the number of commercial deployments is growing, the focus of the networking community is shifting to services and service delivery. 5G networks are designed to be a common platform for very distinct services with different characteristics. Network Slicing has been developed to offer service isolation between the different network offerings. Cloud-native services that are composed of a set of inter-dependent micro-services are assigned into their respective slices that usually span multiple service areas, network domains, and multiple data centers. Due to mobility events caused by moving end-users, slices with their assigned resources and services need to be re-scoped and re-provisioned. This leads to slice mobility whereby a slice moves between service areas and whereby the inter-dependent service and resources must be migrated to reduce system overhead and to ensure low-communication latency by following end-user mobility patterns. Recent advances in computational hardware, Artificial Intelligence, and Machine Learning have attracted interest within the communication community to study and experiment self-managed network slices. However, migrating a service instance of a slice remains an open and challenging process, given the needed co-ordination between inter-cloud resources, the dynamics, and constraints of inter-data center networks. For this purpose, we introduce a Deep Reinforcement Learning based agent that is using two different algorithms to optimize bandwidth allocations as well as to adjust the network usage to minimize slice migration overhead. We show that this approach results in significantly improved Quality of Experience. To validate our approach, we evaluate the agent under different configurations and in real-world settings and present the results.",project-academic
10.1109/BIGDATA47090.2019.9005655,2019-12-01,p,IEEE,detecting pedestrian crossing events in large video data from traffic monitoring cameras," Pedestrian safety on the road is a priority for transportation system managers and operators. While there are a number of treatments and technologies to effectively improve pedestrian safety, identifying the location where these are most needed remains a challenge. Mid-block locations, where safety countermeasures are often needed the most, are typically harder to monitor. Current practice often requires manual observation of candidate locations for limited time periods, leading to an identification process that is often time consuming, lags behind traffic pattern changes over time, and lacks scalability. As a result, target locations are often selected reactively, after serious traffic incidents reveal an underlying safety issue. We propose an approach to use data collected by existing traffic monitoring cameras to automatically identify pedestrian activities on the road. We propose an algorithm to detect pedestrian crossing events based on the detection of individuals on individual video frames using a deep neural network model. Resulting pedestrian locations and movement trajectories can be visualized on a background image, which is automatically extracted at the analyzed location from the video. We demonstrate and evaluate our approach with a real-world use case. The case study considered in this work uses cameras owned by the City of Austin, Texas to study pedestrian road use before and after the deployment of a pedestrian-hybrid beacon. We explore qualitative and quantitative metrics to describe pedestrian activity and corresponding changes, which may be used to prioritize the deployment of pedestrian safety solutions, or evaluate their performance. We compared the number of crossing events detected per hour with manually reviewed results from a selected day. The result shows 67 percent overall accuracy, although we observe significant variability across times-of-day. Despite observed limitations, our work illustrates how the value of existing traffic camera networks can be augmented beyond everyday traffic monitoring, and used to collect valuable information on road usage by pedestrians.",project-academic
10.1109/ICRA40945.2020.9197336,2020-04-05,p,TechRxiv,citylearn diverse real world environments for sample efficient navigation policy learning," Visual navigation tasks in real-world environments often require both self-motion and place recognition feedback. While deep reinforcement learning has shown success in solving these perception and decision-making problems in an end-to-end manner, these algorithms require large amounts of experience to learn navigation policies from high-dimensional data, which is generally impractical for real robots due to sample complexity. In this paper, we address these problems with two main contributions. We first leverage place recognition and deep learning techniques combined with goal destination feedback to generate compact, bimodal image representations that can then be used to effectively learn control policies from a small amount of experience. Second, we present an interactive framework, CityLearn, that enables for the first time training and deployment of navigation algorithms across city-sized, realistic environments with extreme visual appearance changes. CityLearn features more than 10 benchmark datasets, often used in visual place recognition and autonomous driving research, including over 100 recorded traversals across 60 cities around the world. We evaluate our approach on two CityLearn environments, training our navigation policy on a single traversal per dataset. Results show our method can be over 2 orders of magnitude faster than when using raw images, and can also generalize across extreme visual changes including day to night and summer to winter transitions.",project-academic
,2019-10-10,a,,citylearn diverse real world environments for sample efficient navigation policy learning," Visual navigation tasks in real-world environments often require both self-motion and place recognition feedback. While deep reinforcement learning has shown success in solving these perception and decision-making problems in an end-to-end manner, these algorithms require large amounts of experience to learn navigation policies from high-dimensional data, which is generally impractical for real robots due to sample complexity. In this paper, we address these problems with two main contributions. We first leverage place recognition and deep learning techniques combined with goal destination feedback to generate compact, bimodal image representations that can then be used to effectively learn control policies from a small amount of experience. Second, we present an interactive framework, CityLearn, that enables for the first time training and deployment of navigation algorithms across city-sized, realistic environments with extreme visual appearance changes. CityLearn features more than 10 benchmark datasets, often used in visual place recognition and autonomous driving research, including over 100 recorded traversals across 60 cities around the world. We evaluate our approach on two CityLearn environments, training our navigation policy on a single traversal. Results show our method can be over 2 orders of magnitude faster than when using raw images, and can also generalize across extreme visual changes including day to night and summer to winter transitions.",project-academic
10.30844/WI_2020_C1-BAIER,2020-04-01,a,,handling concept drifts in regression problems the error intersection approach," Machine learning models are omnipresent for predictions on big data. One challenge of deployed models is the change of the data over time, a phenomenon called concept drift. If not handled correctly, a concept drift can lead to significant mispredictions. We explore a novel approach for concept drift handling, which depicts a strategy to switch between the application of simple and complex machine learning models for regression tasks. We assume that the approach plays out the individual strengths of each model, switching to the simpler model if a drift occurs and switching back to the complex model for typical situations. We instantiate the approach on a real-world data set of taxi demand in New York City, which is prone to multiple drifts, e.g. the weather phenomena of blizzards, resulting in a sudden decrease of taxi demand. We are able to show that our suggested approach outperforms all regarded baselines significantly.",project-academic
10.1007/S42947-020-0138-5,2021-07-01,a,Springer Singapore,automatically detect and classify asphalt pavement raveling severity using 3d technology and machine learning," Raveling is one of the most common asphalt pavement distresses that occur on US highway pavements. Raveling results in safety concerns such as loose stones and hydroplaning; poor ride quality and road/tire noise; and shortened pavement longevity. Traditional raveling survey methods involve manual visual inspection, which is time consuming, subjective, and hazardous to highway workers. With the research project competitively selected and sponsored by the National Cooperative Highway Research Program (NCHRP) Innovation Deserving Exploratory Analysis (IDEA) program, the objective of this study is to develop an accurate raveling detection and classification algorithm using 3D pavement data that has become mainstream technologies for state Department of Transportations (DOTs) in the US for pavement condition evaluation, and to comprehensively validate these methods using large-scale, real-world data based on actual transportation agencies’ distress protocol (Severity levels 1, 2, and 3). A total of 65 miles of 3 D pavement data was collected on I-85 and I-285 in Georgia for training and testing. Three supervised machine learning techniques —AdaBoost with decision trees, support vector machine (SVM) and random forests—were developed for the detection and classification of raveling in the collected data. The random forest classifier had the b est performance, with precision values ranging from 75.6% for level 3 raveling to 97.6% for level 0 (no) raveling and recall values ranging from 86.9% for level 1 raveling to 96.1% for level 0 raveling on real world large-scale data. The developed raveling detection and severity level classification method has been successfully implemented to entire Georgia’s interstate highway system with1452.5 survey miles of asphalt pavements after the large-scale validation and refinement. The proposed method for raveling detection can be deployed to other transportation agencies for safer and more efficient assessment of roadway raveling conditions.",project-academic
10.1145/3369814,2019-12-11,a,Association for Computing Machinery (ACM),cityguard citywide fire risk forecasting using a machine learning approach," Forecasting the fire risk is of great importance to fire prevention deployments in a city, which can reduce loss even deaths caused by fires. However, it is very challenging because fires are influenced by many complex factors, including spatial correlations, temporal dependencies, even the mixture of these two and external factors. Firstly, the fire risk of a region is influenced by temporal effect of internal factors (e.g., the historical fire risk records) and temporal effect of external factors (e.g., weather). Secondly, a region's fire risk is not only influenced by its inherent geospatial attributes (e.g., POIs) but also dependent on other regions in spatial. To address these challenges, we propose a machine learning approach to forecast the fire risk, entitled NeuroFire. NeuroFire can represent internal and external temporal effect then combine the temporal representation and spatial dependencies by a spatial-temporal loss function. Experimental evaluations on real-world datasets show that our NeuroFire outperforms 9 baselines, demonstrating the performance of our approach by several visualizations. Moreover, we implement a citywide fire forecasting system named CityGuard to display the analysis and forecasting results, which can assist the fire rescue department in deploying fire prevention.",project-academic
10.1111/1467-8667.00308,2003-05-01,a,Wiley,case based reasoning for assessing intelligent transportation systems benefits," Current transport planning modeling tools have critical limitations with respect to assessing the benefits of intelligent transportation systems (ITS) deployment. In this paper, a novel framework for developing modeling tools for quantifying ITS deployments benefits is presented. The approach is based on use of case-based reasoning, an artificial intelligence paradigm, to capture and organize the insights gained from running a dynamic traffic assignment (DTA) model. To demonstrate the feasibility of the approach, this study develops a prototype system to evaluate benefits of diverting traffic away from incident locations using variable message signs. A real-world network from the Hartford (Connecticut) area is used in developing the system. Performance of the prototype is evaluated by comparing its predictions to those obtained using a detailed DTA model. The prototype system is shown to yield solutions comparable to those obtained from the DTA model, demonstrating the feasibility of the approach.",project-academic
10.24251/HICSS.2021.120,2021-01-01,p,Hawaii International Conference on System Sciences,switching scheme a novel approach for handling incremental concept drift in real world data sets," Machine learning models nowadays play a crucial role for many applications in business and industry. However, models only start adding value as soon as they are deployed into production. One challenge of deployed models is the effect of changing data over time, which is often described with the term concept drift. Due to their nature, concept drifts can severely affect the prediction performance of a machine learning system. In this work, we analyze the effects of concept drift in the context of a real-world data set. For efficient concept drift handling, we introduce the switching scheme which combines the two principles of retraining and updating of a machine learning model. Furthermore, we systematically analyze existing regular adaptation as well as triggered adaptation strategies. The switching scheme is instantiated on New York City taxi data, which is heavily influenced by changing demand patterns over time. We can show that the switching scheme outperforms all other baselines and delivers promising prediction results.",project-academic
,2020-11-05,a,,switching scheme a novel approach for handling incremental concept drift in real world data sets," Machine learning models nowadays play a crucial role for many applications in business and industry. However, models only start adding value as soon as they are deployed into production. One challenge of deployed models is the effect of changing data over time, which is often described with the term concept drift. Due to their nature, concept drifts can severely affect the prediction performance of a machine learning system. In this work, we analyze the effects of concept drift in the context of a real-world data set. For efficient concept drift handling, we introduce the switching scheme which combines the two principles of retraining and updating of a machine learning model. Furthermore, we systematically analyze existing regular adaptation as well as triggered adaptation strategies. The switching scheme is instantiated on New York City taxi data, which is heavily influenced by changing demand patterns over time. We can show that the switching scheme outperforms all other baselines and delivers promising prediction results.",project-academic
10.1145/3463495,2021-06-24,a,Association for Computing Machinery (ACM),uvlens urban village boundary identification and population estimation leveraging open government data," Urban villages refer to the residential areas lagging behind the rapid urbanization process in many developing countries. These areas are usually with overcrowded buildings, high population density, and low living standards, bringing potential risks of public safety and hindering the urban development. Therefore, it is crucial for urban authorities to identify the boundaries of urban villages and estimate their resident and floating populations so as to better renovate and manage these areas. Traditional approaches, such as field surveys and demographic census, are time consuming and labor intensive, lacking a comprehensive understanding of urban villages. Against this background, we propose a two-phase framework for urban village boundary identification and population estimation. Specifically, based on heterogeneous open government data, the proposed framework can not only accurately identify the boundaries of urban villages from large-scale satellite imagery by fusing road networks guided patches with bike-sharing drop-off patterns, but also accurately estimate the resident and floating populations of urban villages with a proposed multi-view neural network model. We evaluate our method leveraging real-world datasets collected from Xiamen Island. Results show that our framework can accurately identify the urban village boundaries with an IoU of 0.827, and estimate the resident population and floating population with R2 of 0.92 and 0.94 respectively, outperforming the baseline methods. We also deploy our system on the Xiamen Open Government Data Platform to provide services to both urban authorities and citizens.",project-academic
10.1145/3432226,2020-12-17,a,Association for Computing Machinery (ACM),mimu mobile wifi usage inference by mining diverse user behaviors," Mobile WiFi is a newly emerging service in recent years, which provides convenience for users to access online resources and increases revenues for operators via services such as advertisements and application promotions. However, in practice, the prohibitively high system implementation and operational costs, especially the costs of perpetual data traffic, hinder the further deployment of mobile WiFi services. In this paper, we present MIMU, a usage inference system for data traffic saving suitable for ubiquitous mobile WiFi services. We demonstrate the performance of the system via an example from the real-world nationwide edge computing mobile WiFi infrastructure. To address the impact of diverse user behaviors, we investigate the WiFi network usage from the perspective of users and devices, focusing on two unique features of mobile WiFi: user mobility regularity and access irregularity. In particular, we first design a deep learning-based two-dimension usage predictor to infer the future mobile WiFi usage with 1) a user dimension model with temporal attention addressing dominant users with heavy bus WiFi usage, and 2) a device dimension model with spatial attention addressing diverse WiFi usage and connection. Based on the results of the predictor, an application of content caching is implemented in an iterative fashion to save the data traffic. We evaluate MIMU by real-world bus WiFi system data sets of three major cities with 6,643 bus WiFi devices and 150k daily active users in total. Our results show that MIMU outperforms state-of-the-art methods in terms of usage inference. Moreover, we summarize the lessons learned from our large-scale bus WiFi system investigation.",project-academic
10.1109/IVS.2014.6856600,2014-06-08,p,IEEE,looking in and looking out vision for urban intelligent assistance estimation of driver attentive state and dynamic surround for safe merging and braking," This paper details the research, development, and demonstrations of real-world systems intended to assist the driver in urban environments, as part of the Urban Intelligent Assist (UIA) research initiative. A 3-year collaboration between Audi AG, Volkswagen Group of America Electronics Research Laboratory, and UC San Diego, the driver assistance portion of the UIA project focuses on two main use cases of vital importance in urban driving. The first, Driver Attention Guard, applies novel computer vision and machine learning research for accurately tracking the driver's head position and rotation using an array of cameras. The system then infers the driver's focus of attention, alerting the driver and engaging safety systems in case of extended driver inattention. The second application, Merge and Lane Change Assist, applies a novel probabilistic compact representation of the on-road environment, fusing data from a variety of sensor modalities. The system then computes safe and low-cost merge and lane-change maneuver recommendations. It communicates desired speeds to the driver via Head-up Display, when the driver touches the blinker, indicating his desired lane. The fully-implemented systems, complete with HMI, were demonstrated to the public and press in San Francisco in January of 2014.",project-academic
10.1109/JCN.2017.000025,2017-05-05,a,KICS,drivingstyles a mobile platform for driving styles and fuel consumption characterization," Intelligent transportation systems (ITS) rely on connected vehicle applications to address real-world problems. Research is currently being conducted to support safety, mobility and environmental applications. This paper presents the DrivingStyles architecture, which adopts data mining techniques and neural networks to analyze and generate a classification of driving styles and fuel consumption based on driver characterization. In particular, we have implemented an algorithm that is able to characterize the degree of aggressiveness of each driver. We have also developed a methodology to calculate, in real-time, the consumption and environmental impact of spark ignition and diesel vehicles from a set of variables obtained from the vehicle’s electronic control unit (ECU). In this paper, we demonstrate the impact of the driving style on fuel consumption, as well as its correlation with the greenhouse gas emissions generated by each vehicle. Overall, our platform is able to assist drivers in correcting their bad driving habits, while offering helpful tips to improve fuel economy and driving safety.",project-academic
,2016-11-28,a,,drivingstyles a mobile platform for driving styles and fuel consumption characterization," Intelligent Transportation Systems (ITS) rely on connected vehicle applications to address real-world problems. Research is currently being conducted to support safety, mobility and environmental applications. This paper presents the DrivingStyles architecture, which adopts data mining techniques and neural networks to analyze and generate a classification of driving styles and fuel consumption based on driver characterization. In particular, we have implemented an algorithm that is able to characterize the degree of aggressiveness of each driver. We have also developed a methodology to calculate, in real-time, the consumption and environmental impact of spark ignition and diesel vehicles from a set of variables obtained from the vehicle's Electronic Control Unit (ECU). In this paper, we demonstrate the impact of the driving style on fuel consumption, as well as its correlation with the greenhouse gas emissions generated by each vehicle. Overall, our platform is able to assist drivers in correcting their bad driving habits, while offering helpful tips to improve fuel economy and driving safety.",project-academic
10.1109/MCI.2010.938363,2010-11-01,a,IEEE,multi agent system in urban traffic signal control," Multi-agent system is a rapidly developing field of distributed artificial intelligence that has gained significant importance because of its ability to solve complex real-world problems. It provides a highly flexible and modular structure, which incorporates the domain expertise in the system, to achieve the optimal solution. Multi-agent system also allows a problem to be divided into smaller sub-problems that require less domain expertise compared to solving the problem as a whole. In recent years, multi-agent system has gained significant attention in solving traffic signal control problems because of the advantages it offers in solving complex problems with uncertainties. In this paper, two different types of multi-agent architectures that have been implemented on a simulated complex urban traffic network in Singapore for adaptive intelligent signal control are discussed. The results obtained indicate the superior performance of the multi-agent signal controller in comparison to pre-timed and signal control methods which are currently in use.",project-academic
10.1145/3199667,2018-11-27,a,ACM,autocalib automatic traffic camera calibration at scale," Emerging smart cities are typically equipped with thousands of outdoor cameras. However, these cameras are usually not calibrated, i.e., information such as their precise mounting height and orientation is not available. Calibrating these cameras allows measurement of real-world distances from the video, thereby enabling a wide range of novel applications such as identifying speeding vehicles and city road planning. Unfortunately, robust camera calibration is a manual process today and is not scalable. In this article, we propose AutoCalib, a system for scalable, automatic calibration of traffic cameras. AutoCalib exploits deep learning to extract selected key-point features from car images in the video and uses a novel filtering and aggregation algorithm to automatically produce a robust estimate of the camera calibration parameters from just hundreds of samples. We have implemented AutoCalib as a service on Azure that takes in a video segment and computes the camera calibration parameters. Using video from real-world traffic cameras, we show that AutoCalib is able to estimate real-world distances with an error of less than 12%.",project-academic
10.1109/SEAMS.2017.8,2017-05-20,p,IEEE,self adaptive learning in decentralized combinatorial optimization a design paradigm for sharing economies," The democratization of Internet of Things and ubiquitous computing equips citizens with phenomenal new ways for online participation and decision-making in application domains of smart grids and smart cities. When agents autonomously self-determine the options from which they make choices, while these choices collectively have an overall system-wide impact, an optimal decision-making turns into a combinatorial optimization problem known to be NP-hard. This paper contributes a new generic self-adaptive learning algorithm for a fully decentralized combinatorial optimization: I-EPOS, the Iterative Economic Planning and Optimized Selections. In contrast to related algorithms that simply parallelize computations or big data and deep learning systems that often require personal data and overtake of control with implication on privacy-preservation and autonomy, I-EPOS relies on coordinated local decision-making via structured interactions over tree topologies that involve the exchange of entirely local and aggregated information. Strikingly, the cost-effectiveness of I-EPOS in regards to performance vs. computational and communication cost highly outperforms other related algorithms that involve non-local brute-force operations or exchange of full information. The algorithm is also evaluated using real-world data from two state-of-the-art pilot projects of participatory sharing economies: (i) energy management and (ii) bicycle sharing. The contribution of an I-EPOS open source software suite implemented as a paradigmatic artifact for community aspires to settle a knowledge exchange for the design of new algorithms and application scenarios of sharing economies towards highly participatory and sustainable digital societies.",project-academic
10.1155/2019/7057612,2019-04-09,a,Hindawi,modern machine learning techniques for univariate tunnel settlement forecasting a comparative study," Tunnel settlement commonly occurs during the tunnel construction processes in large cities. Existing forecasting methods for tunnel settlements include model-based approaches and artificial intelligence (AI) enhanced approaches. Compared with traditional forecasting methods, artificial neural networks can be easily implemented, with high performance efficiency and forecasting accuracy. In this study, an extended machine learning framework is proposed combining particle swarm optimization (PSO) with support vector regression (SVR), back-propagation neural network (BPNN), and extreme learning machine (ELM) to forecast the surface settlement for tunnel construction in two large cities of China P.R. Based on real-world data verification, the PSO-SVR method shows the highest forecasting accuracy among the three proposed forecasting algorithms.",project-academic
10.1109/TVT.2020.2981959,2020-03-19,a,IEEE,machine learning aided air traffic flow analysis based on aviation big data," Timely and efficient air traffic flow management (ATFM) is a key issue in future dense air traffic. The emerging demands for unmanned aerial vehicles and general aviation aircraft aggravate the burden of the ATFM. Thanks to the advanced automatic dependent surveillance-broadcast (ADS-B) technique, the aerial vehicles can be tracked and monitored in a real-time and accurate manner, providing possibility for establishing a more intelligent ATFM architecture. In this article, we first form an aviation Big Data platform by using the distributed ADS-B ground stations and the obtained ADS-B messages. By exploring the constructed dataset and mapping the extracted information to the routes, the air traffic flow between different cities can be counted and predicted, where the prediction task is implemented on the basis of two machine learning methods, respectively. The experimental results based on real-world data demonstrate that the proposed traffic flow prediction model adopting long short-term memory (LSTM) can achieve better performance, especially when abnormal factors in traffic control are considered.",project-academic
,2019-02-04,a,,paracosm a language and tool for testing autonomous driving systems," Systematic testing of autonomous vehicles operating in complex real-world scenarios is a difficult and expensive problem. We present Paracosm, a reactive language for writing test scenarios for autonomous driving systems. Paracosm allows users to programmatically describe complex driving situations with specific visual features, e.g., road layout in an urban environment, as well as reactive temporal behaviors of cars and pedestrians. Paracosm programs are executed on top of a game engine that provides realistic physics simulation and visual rendering. The infrastructure allows systematic exploration of the state space, both for visual features (lighting, shadows, fog) and for reactive interactions with the environment (pedestrians, other traffic). We define a notion of test coverage for Paracosm configurations based on combinatorial testing and low dispersion sequences. Paracosm comes with an automatic test case generator that uses random sampling for discrete parameters and deterministic quasi-Monte Carlo generation for continuous parameters. Through an empirical evaluation, we demonstrate the modeling and testing capabilities of Paracosm on a suite of autonomous driving systems implemented using deep neural networks developed in research and education. We show how Paracosm can expose incorrect behaviors or degraded performance.",project-academic
10.1109/TSG.2020.2969650,2020-01-28,a,IEEE,enhanced coordinated operations of electric power and transportation networks via ev charging services," Electric power and transportation networks become increasingly coupled through electric vehicles (EV) charging station (EVCS) as the penetration of EVs continues to grow. In this paper, we propose a holistic framework to enhance the operation of coordinated electric power distribution network (PDN) and urban transportation network (UTN) via EV charging services. Under this framework, a bi-level model is formulated to optimally determine EVCS charging service fees (CSF) for guiding EV charging behaviors and minimizing the total social cost. At the upper level, PDN with wind power generation is formulated as a second-order cone problem (SOCP) where CSF is determined. Given the settings calculated at the upper level, the lower level problem is described as a traffic assignment problem (TAP) which is subject to the user equilibrium (UE) principle and captures the individual rationality of single EV owners in UTN. The uncertainties in wind power output and origin-destination (O-D) traffic demands are considered in the proposed model and a deep reinforcement learning (DRL)-based solution framework is developed to decouple and approximately solve the stochastic bi-level problem. Both gradient-based and gradient-free training algorithms are implemented in this paper and the respective results are compared. The case studies on a 5-node system, 24-node Sioux-Falls system and real-world Xi’an city in China are conducted to verify the effectiveness of the proposed model, which demonstrates the enhanced operation of coordinated PDN and UTN networks by reducing the traffic congestion and improving the integration of renewable energy.",project-academic
10.1109/TSG.2019.2924183,2020-01-01,a,Institute of Electrical and Electronics Engineers (IEEE),a hybrid distribution feeder long term load forecasting method based on sequence prediction," Distribution feeder long-term load forecast (LTLF) is a critical task many electric utility companies perform on an annual basis. The goal of this task is to forecast the annual load of distribution feeders. The previous top-down and bottom-up LTLF methods are unable to incorporate different levels of information. This paper proposes a hybrid modeling method using sequence prediction for this classic and important task. The proposed method can seamlessly integrate top-down, bottom-up, and sequential information hidden in multi-year data. Two advanced sequence prediction models long short-term memory (LSTM) and gated recurrent unit (GRU) networks are investigated in this paper. They successfully solve the vanishing and exploding gradient problems a standard recurrent neural network has. This paper first explains the theories of LSTM and GRU networks and then discusses the steps of feature selection, feature engineering and model implementation in detail. In the end, a real-world application example for a large urban grid in West Canada is provided. LSTM and GRU networks under different sequential configurations and traditional models including bottom-up, ARIMA, and feed-forward neural network are all implemented and compared in detail. The proposed method demonstrates superior performance and great practicality.",project-academic
10.1109/SSCI.2017.8285360,2017-11-01,p,IEEE,accurate vehicle position estimation using a kalman filter and neural network based approach," Accurate detection of vehicle position plays an important role in many intelligent transportation systems, especially vehicle-to-vehicle applications. In this paper, we propose an Extended Kalman Filter (EKF) based method to detect Global Positioning System (GPS) errors for such vehicle-based applications. A machine learning methodology is presented for Kalman filter parameter tuning with application to GPS error correction in vehicle positioning. We also present a model free neural network that is trained on past vehicle GPS trajectories to predict the current vehicle position. Experimental results on real-world data show that the proposed system is effective for detecting and reducing GPS errors. The machine learning algorithm for EKF parameter tuning can be implemented through in-vehicle learning, and the proposed GPS error detection method can be implemented for in-vehicle applications.",project-academic
,2020-11-15,a,,deep ordinal regression using optimal transport loss and unimodal output probabilities," We propose a framework for deep ordinal regression, based on unimodal output distribution and optimal transport loss. Despite being seemingly appropriate, in many recent works the unimodality requirement is either absent, or implemented using soft targets, which do not guarantee unimodal outputs at inference. In addition, we argue that the standard maximum likelihood objective is not suitable for ordinal regression problems, and that optimal transport is better suited for this task, as it naturally captures the order of the classes. Inspired by the well-known Proportional Odds model, we propose to modify its design by using an architectural mechanism which guarantees that the model output distribution will be unimodal. We empirically analyze the different components of our propose approach and demonstrate their contribution to the performance of the model. Experimental results on three real-world datasets demonstrate that our proposed approach performs on par with several recently proposed deep learning approaches for deep ordinal regression with unimodal output probabilities, while having guarantee on the output unimodality. In addition, we demonstrate that the level of prediction uncertainty of the model correlates with its accuracy.",project-academic
10.33012/2019.16899,2019-09-20,p,Institute of Navigation,resilient multipath prediction and detection architecture for low cost navigation in challenging urban areas," GNSS remains one of the key building blocks in mass-market positioning applications, many of which require a high level of accuracy, integrity and availability. Conventionally, the GNSS receiver and antenna are a part of a multisensor integrated solution with an inertial measurement unit (IMU) at the core of the navigation system. One of the multisensor fusion challenges is to continuously adjust the Kalman filter stochastic model to reflect the environment of operation. Apart from poor satellite geometry, the reception of multipath-contaminated signals is the main factor contributing to GNSS performance degradation in urban areas. Signal quality monitoring (SQM) techniques are implemented to first detect and then exclude, de-weight or correct the multipath-contaminated GNSS measurements to minimize the impact of multipath-induced errors on the multisensor data fusion filter performance. The implementation of such an approach for kinematic scenarios in deep urban canyons with mass-market hardware suffers from high rates of false-positive and false-negative multipath detection due to frequent cycle slips, discontinuous satellite tracking, and a complex multipath environment. The alternative approach for the IMU/GNSS integration filter stochastic model tuning is to extract the a priori statistics characterizing the probability of the multipath-contaminated signal reception from a GNSS multipath environment map. The map is generated with collectively recorded carrier-to-noise-density ratio (C/N0) readings streamed from the connected vehicles operating in a given urban area and assigned to a space-time cube. While improving positioning accuracy, the application of the concept is constrained by the GNSS multipath environment map availability only to the areas directly surveyed by the connected vehicles. The novel contributions of this paper are as follows. To extend availability of the GNSS multipath environment map, a random forest machine-learning model for predicting the spatial pattern of the map is developed. The model is trained with a real-world GNSS multipath environment map covering the area of ten square kilometres including downtown Montreal. A LiDAR elevation profile, 2D building polygons, street polygons, street types and foliage polygons are used as feature data. An 89% map prediction accuracy is reached. Further, the Extended Kalman Filter (EKF) stochastic model adjustment architecture combining the SQM multipath detection and the GNSS multipath environment map-aided multipath prediction is developed. The architecture aims to address the limitations of each method and allows for continuous multipath monitoring increasing the resilience of the multisensor data fusion. The method is tested in several use cases with low-cost hardware: loosely-coupled and tightly-coupled IMU/GNSS integration. The evaluation of the proposed method shows 20% positioning accuracy improvement compared to standard Kalman filter performance. The results of this work are expected to facilitate future improved integration of GNSS in multisensor platforms operating in challenging urban areas.",project-academic
10.1145/3365871.3365881,2019-10-22,p,ACM,interactive machine learning for the internet of things a case study on activity detection," The advances in Internet of Things lead to an increased number of devices generating and streaming data. These devices can be useful data sources for Activity Recognition by using Machine Learning. However, as the set of available sensors may vary over time, e.g. due to mobility of the sensors and technical failures, the feature space might also change over time. Moreover, the labelled data necessary for the training is often costly to acquire. Active Learning is a type of Interactive Machine Learning where the model is given a budget for requesting labels from an oracle, and aims to maximize accuracy by careful selection of what data points to label. It is generally assumed that a query always gets a correct response, but in many real-world scenarios this is not a realistic assumption. In this work we investigate different Proactive Learning strategies, which explore the human factors of the oracle and aspects that might influence a user to provide or withhold labels. We implemented four proactive strategies and hybrid versions of them. They were evaluated on two datasets to examine how a more proactive, or reluctant, user affects performance. The results show that a more proactive user can improve the performance, especially when the user is influenced by the accuracy of earlier predictions. The experiments also highlight challenges related to evaluating performance when the set of classes is changing over time.",project-academic
10.1016/J.TRC.2020.102912,2021-03-01,a,Pergamon,urban flow prediction with spatial temporal neural odes," Abstract None None With the recent advances in deep learning, data-driven methods have shown compelling performance in various application domains enabling the Smart Cities paradigm. Leveraging spatial–temporal data from multiple sources for (citywide) traffic forecasting is a key to strengthen the smart city management in areas such as urban traffic control, abnormal event detection, etc. Existing approaches of traffic flow prediction mainly rely on the development of various deep neural networks –e.g., Convolutional Neural Networks such as ResNet are used for modeling spatial dependencies among different regions, whereas recurrent neural networks are increasingly implemented for temporal dynamics modeling. Despite their advantages, the existing approaches suffer from limitations of intensive computations, lack of capabilities to properly deal with missing values, and simplistic integration of heterogeneous data. In this paper, we propose a novel urban flow prediction framework by generalizing the hidden states of the model with continuous-time dynamics of the latent states using neural ordinary differential equations (ODE). Specifically, we introduce a discretize-then-optimize approach to improve and balance the prediction accuracy and computational efficiency. It not only guarantees the prediction error but also provides high flexibility for decision-makers. Furthermore, we investigate the factors, both intrinsic and extrinsic, that affect the city traffic volume and use separate neural networks to extract and disentangle the influencing factors, which avoids the brute-force data fusion in previous works. Extensive experiments conducted on the real-world large-scale datasets demonstrate that our method outperforms the state-of-the-art baselines, while requiring significantly less memory cost and fewer model parameters.",project-academic
10.1016/J.TRC.2021.102967,2021-05-01,a,Pergamon,automated eco driving in urban scenarios using deep reinforcement learning," Abstract None None Urban settings are challenging environments to implement eco-driving strategies for automated vehicles. It is often assumed that sufficient information on the preceding vehicle pulk is available to accurately predict the traffic situation. Because vehicle-to-vehicle communication was introduced only recently, this assumption will not be valid until a sufficiently high penetration of the vehicle fleet has been reached. Thus, in the present study, we employed Reinforcement Learning (RL) to develop eco-driving strategies for cases where little data on the traffic situation are available. None An A-segment electric vehicle was simulated using detailed efficiency models to accurately determine its energy-saving potential. A probabilistic traffic environment featuring signalized urban roads and multiple preceding vehicles was integrated into the simulation model. Only information on the traffic light timing and minimal sensor data were provided to the control algorithm. A twin-delayed deep deterministic policy gradient (TD3) agent was implemented and trained to control the vehicle efficiently and safely in this environment. None Energy savings of up to 19% compared with a simulated human driver and up to 11% compared with a fine-tuned Green Light Optimal Speed Advice (GLOSA) algorithm were determined in a probabilistic traffic scenario reflecting real-world conditions. Overall, the RL agents showed a better travel time and energy consumption trade-off than the GLOSA reference.",project-academic
10.1109/FUZZ-IEEE.2015.7337843,2015-11-30,p,IEEE,a big data processing framework for uncertainties in transportation data," Transportation infrastructure takes a primary role in urban development planning. To better facilitate or understand the infrastructure status and demands, a huge amount of transportation data such as traffic flow counts has been collected from numerous transportation monitoring systems. Making full use of harvested data samples to discover important patterns has become an increasingly appealing research topic, in which a sophisticated and uncertainty-processing framework is required. In this paper, a big-data processing framework is introduced to analyse the transportation data, particularly taking the classification problem of the parking occupation status as an illustrative example. Three modules are implemented to crawl the raw records, generate high-level features, and apply the machine learning algorithm for classification. In addition, the fuzzification algorithm is also introduced to quantify the key attributes of the data, which helps in removing the data redundancy and inconsistency. The proposed framework then is evaluated using a real-world dataset collected from twelve car parks in a university. Simulation results show that the proposed framework performs well with a convincing classification accuracy.",project-academic
10.1016/J.ENVPOL.2020.115900,2021-04-01,a,Elsevier Ltd.,understanding the true effects of the covid 19 lockdown on air pollution by means of machine learning," During March 2020, most European countries implemented lockdowns to restrict the transmission of SARS-CoV-2, the virus which causes COVID-19 through their populations. These restrictions had positive impacts for air quality due to a dramatic reduction of economic activity and atmospheric emissions. In this work, a machine learning approach was designed and implemented to analyze local air quality improvements during the COVID-19 lockdown in Graz, Austria. The machine learning approach was used as a robust alternative to simple, historical measurement comparisons for various individual pollutants. Concentrations of NO2 (nitrogen dioxide), PM10 (particulate matter), O3 (ozone) and Ox (total oxidant) were selected from five measurement sites in Graz and were set as target variables for random forest regression models to predict their expected values during the city's lockdown period. The true vs. expected difference is presented here as an indicator of true pollution during the lockdown. The machine learning models showed a high level of generalization for predicting the concentrations. Therefore, the approach was suitable for analyzing reductions in pollution concentrations. The analysis indicated that the city's average concentration reductions for the lockdown period were: -36.9 to -41.6%, and -6.6 to -14.2% for NO2 and PM10, respectively. However, an increase of 11.6-33.8% for O3 was estimated. The reduction in pollutant concentration, especially NO2 can be explained by significant drops in traffic-flows during the lockdown period (-51.6 to -43.9%). The results presented give a real-world example of what pollutant concentration reductions can be achieved by reducing traffic-flows and other economic activities.",project-academic
,2021-07-05,a,,learning a model for inferring a spatial road lane network graph using self supervision," Interconnected road lanes are a central concept for navigating urban roads. Currently, most autonomous vehicles rely on preconstructed lane maps as designing an algorithmic model is difficult. However, the generation and maintenance of such maps is costly and hinders large-scale adoption of autonomous vehicle technology. This paper presents the first self-supervised learning method to train a model to infer a spatially grounded lane-level road network graph based on a dense segmented representation of the road scene generated from onboard sensors. A formal road lane network model is presented and proves that any structured road scene can be represented by a directed acyclic graph of at most depth three while retaining the notion of intersection regions, and that this is the most compressed representation. The formal model is implemented by a hybrid neural and search-based model, utilizing a novel barrier function loss formulation for robust learning from partial labels. Experiments are conducted for all common road intersection layouts. Results show that the model can generalize to new road layouts, unlike previous approaches, demonstrating its potential for real-world application as a practical learning-based lane-level map generator.",project-academic
10.1109/ITSC48978.2021.9564899,2021-07-05,p,IEEE,learning a model for inferring a spatial road lane network graph using self supervision," Interconnected road lanes are a central concept for navigating urban roads. Currently, most autonomous vehicles rely on preconstructed lane maps as designing an algorithmic model is difficult. However, the generation and maintenance of such maps is costly and hinders large-scale adoption of autonomous vehicle technology. This paper presents the first self-supervised learning method to train a model to infer a spatially grounded lane-level road network graph based on a dense segmented representation of the road scene generated from onboard sensors. A formal road lane network model is presented and proves that any structured road scene can be represented by a directed acyclic graph of at most depth three while retaining the notion of intersection regions, and that this is the most compressed representation. The formal model is implemented by a hybrid neural and search-based model, utilizing a novel barrier function loss formulation for robust learning from partial labels. Experiments are conducted for all common road intersection layouts. Results show that the model can generalize to new road layouts, unlike previous approaches, demonstrating its potential for real-world application as a practical learning-based lane-level map generator.",project-academic
10.1007/978-3-319-15702-3_46,2015-03-23,p,"Springer, Cham",a method for merging similar zones to improve intelligent models for real estate appraisal, A method for property valuation based on the concept of merging different areas of the city into uniform zones reflecting the characteristics of the real estate market was worked out. The foundations of the method were verified by experimental testing the accuracy of the models devised for the prediction of real estate prices built over the merged zones. The experiments were conducted using real-world data of sales transactions of residential premises completed in a Polish urban municipality. Two machine learning techniques implemented in the WEKA environment were employed to generate property valuation models. The comparative analysis of the methods was made with the nonparametric Friedman and Wilcoxon statistical tests. The study proved the usefulness of merging of similar areas which resulted in better reliability and accuracy of predicted prices.,project-academic
10.22119/IJTE.2020.218863.1509,2020-10-01,a,Tarrahan Parseh Transportation Research Institute,an implementation of the ai based traffic flow prediction in the resilience control scheme," Today, often a reliable and dynamic sensor system is found to be necessary to control intelligent transportation systems. While these dynamical sensor systems are often found to be useful for the ordinary situations, the resilience-control-related issues are not yet fully addressed in the literature. The traffic flow is an important resource, which if found to be disturbed by a malicious threat it may cause further insecurities, e.g. if the sensor data is not accessible due to a malicious sabotage of the on-the-road sensors. Furthermore, often centers for the data gathering and prediction are suffering from data-loss because of imperfections of the data gathering itself. To overcome the resulting difficulties, a prediction engine is required to estimate the traffic flow, with the ability to compensate for the lost sensors. In this paper, a traffic flow prediction engine is proposed in which the artificial-intelligence-based methods are used to perform the optimization task. This method is implemented for the test in the real-world situation and its efficiency in traffic estimation is proved to be reliable. The Adaptive Neuro-Fuzzy Inference System (ANFIS) is trained with the particle swarm optimization (PSO) algorithm and the Artificial Neural Network model (ANN) is used to predict the flow. In addition, The Principal Components Analysis (PCA) method is adopted to reduce the dimension of the features. The results show the method's efficiency in predicting the traffic flow. This prediction engine can be practically implemented and used as a replacement for the sensors to predict the traffic flow.",project-academic
,2020-12-21,a,,a comprehensive survey of 6g wireless communications," While fifth-generation (5G) communications are being rolled out worldwide, sixth-generation (6G) communications have attracted much attention from both the industry and the academia. Compared with 5G, 6G will have a wider frequency band, higher transmission rate, spectrum efficiency, greater connection capacity, shorter delay, broader coverage, and more robust anti-interference capability to satisfy various network requirements. This survey presents an insightful understanding of 6G wireless communications by introducing requirements, features, critical technologies, challenges, and applications. First, we give an overview of 6G from perspectives of technologies, security and privacy, and applications. Subsequently, we introduce various 6G technologies and their existing challenges in detail, e.g., artificial intelligence (AI), intelligent surfaces, THz, space-air-ground-sea integrated network, cell-free massive MIMO, etc. Because of these technologies, 6G is expected to outperform existing wireless communication systems regarding the transmission rate, latency, global coverage, etc. Next, we discuss security and privacy techniques that can be applied to protect data in 6G. Since edge devices are expected to gain popularity soon, the vast amount of generated data and frequent data exchange make the leakage of data easily. Finally, we predict real-world applications built on the technologies and features of 6G; for example, smart healthcare, smart city, and smart manufacturing will be implemented by taking advantage of AI.",project-academic
10.1038/S41746-018-0045-1,2018-12-05,a,,machine learned epidemiology real time detection of foodborne illness at scale," Machine learning has become an increasingly powerful tool for solving complex problems, and its application in public health has been underutilized. The objective of this study is to test the efficacy of a machine-learned model of foodborne illness detection in a real-world setting. To this end, we built FINDER, a machine-learned model for real-time detection of foodborne illness using anonymous and aggregated web search and location data. We computed the fraction of people who visited a particular restaurant and later searched for terms indicative of food poisoning to identify potentially unsafe restaurants. We used this information to focus restaurant inspections in two cities and demonstrated that FINDER improves the accuracy of health inspections; restaurants identified by FINDER are 3.1 times as likely to be deemed unsafe during the inspection as restaurants identified by existing methods. Additionally, FINDER enables us to ascertain previously intractable epidemiological information, for example, in 38% of cases the restaurant potentially causing food poisoning was not the last one visited, which may explain the lower precision of complaint-based inspections. We found that FINDER is able to reliably identify restaurants that have an active lapse in food safety, allowing for implementation of corrective actions that would prevent the potential spread of foodborne illness.",project-academic
,2011-07-31,a,,lean algebraic multigrid lamg fast graph laplacian linear solver," Laplacian matrices of graphs arise in large-scale computational applications such as machine learning; spectral clustering of images, genetic data and web pages; transportation network flows; electrical resistor circuits; and elliptic partial differential equations discretized on unstructured grids with finite elements. A Lean Algebraic Multigrid (LAMG) solver of the linear system Ax=b is presented, where A is a graph Laplacian. LAMG's run time and storage are linear in the number of graph edges. LAMG consists of a setup phase, in which a sequence of increasingly-coarser Laplacian systems is constructed, and an iterative solve phase using multigrid cycles. General graphs pose algorithmic challenges not encountered in traditional applications of algebraic multigrid. LAMG combines a lean piecewise-constant interpolation, judicious node aggregation based on a new node proximity definition, and an energy correction of the coarse-level systems. This results in fast convergence and substantial overhead and memory savings. A serial LAMG implementation scaled linearly for a diverse set of 1666 real-world graphs with up to six million edges. This multilevel methodology can be fully parallelized and extended to eigenvalue problems and other graph computations.",project-academic
,2020-01-01,a,,uncertainty aware energy management of extended range electric delivery vehicles with bayesian ensemble," In recent years, deep reinforcement learning (DRL) algorithms have been widely studied and utilized in the area of Intelligent Transportation Systems (ITS). DRL agents are mostly trained with transition pairs and interaction trajectories generated from simulation, and they can achieve satisfying or near optimal performances under familiar input states. However, for relative rare visited or even unvisited regions in the state space, there is no guarantee that the agent could perform well. Unfortunately, novel conditions are inevitable in real-world problems and there is always a gap between the real data and simulated data. Therefore, to implement DRL algorithms in real-world transportation systems, we should not only train the agent learn a policy that maps states to actions, but also the model uncertainty associated with each action. In this study, we adapt the method of Bayesian ensemble to train a group of agents with imposed diversity for an energy management system of a delivery vehicle. The agents in the ensemble agree well on familiar states but show diverse results on unfamiliar or novel states. This uncertainty estimation facilitates the implementation of interpretable postprocessing modules which can ensure robust and safe operations under high uncertainty conditions.",project-academic
10.1109/IV47402.2020.9304826,2020-10-19,p,IEEE,uncertainty aware energy management of extended range electric delivery vehicles with bayesian ensemble," In recent years, deep reinforcement learning (DRL) algorithms have been widely studied and utilized in the area of Intelligent Transportation Systems (ITS). DRL agents are mostly trained with transition pairs and interaction trajectories generated from simulation, and they can achieve satisfying or near optimal performances under familiar input states. However, for relative rare visited or even unvisited regions in the state space, there is no guarantee that the agent could perform well. Unfortunately, novel conditions are inevitable in real-world problems and there is always a gap between the real data and simulated data. Therefore, to implement DRL algorithms in real-world transportation systems, we should not only train the agent learn a policy that maps states to actions, but also the model uncertainty associated with each action. In this study, we adapt the method of Bayesian ensemble to train a group of agents with imposed diversity for an energy management system of a delivery vehicle. The agents in the ensemble agree well on familiar states but show diverse results on unfamiliar or novel states. This uncertainty estimation facilitates the implementation of interpretable postprocessing modules which can ensure robust and safe operations under high uncertainty conditions.",project-academic
10.1109/TVT.2020.2984038,2020-04-02,a,Institute of Electrical and Electronics Engineers (IEEE),improving the congestion control performance for mobile networks in high speed railway via deep reinforcement learning," Due to the poor Transmission Control Protocol (TCP) performance in high-speed mobile scenarios, passengers have bad network experiences on High-Speed Railway (HSR). As a result, improving network performance for HSR scenarios has become urgent and widespread concerns. Some previous works quantitatively analyzed the TCP performance on HSR and proposed relevant solutions. Other works focused on the handover problem (the leading cause of poor network performance on HSR), and proposed a series of handover algorithms for HSR scenarios. However, the existing works are either limited to only measurement studies without algorithm implementation or lack of integration with real-world scenarios. In this paper, with a large amount of field measurement data in real HSR networks, we study the main reasons why traditional TCP performs poorly in HSR scenarios. To improve the TCP performance, we propose Hd-TCP, a customized Congestion Control (CC) algorithm designed to deal with frequent handover on HSR from the transport layer perspective. For the transmission characteristic on HSR, Hd-TCP can accurately evaluate the link condition and apply a Deep Reinforcement Learning (DRL) method to make a fine control. The simulation results show that Hd-TCP outperforms traditional CC algorithms in both throughput and latency by fully utilizing the transmission gap between handovers.",project-academic
10.1109/DCAS.2018.8620187,2018-11-01,p,Institute of Electrical and Electronics Engineers Inc.,biomimetic soft material synapse for neuromorphic computing from device to network," Neuromorphic computing refers to a variety of brain-inspired computers, devices, and models inspired by the interconnectivity, performance, and energy efficiency of the human brain. Unlike the ubiquitous von Neumann computer architectures with complex processor cores and sequential computation, biological neurons and synapses operate by storing and processing information simultaneously with the capacity of flexible adaptation resulting in massive computational capability with much less power consumption. The search for a synaptic material which can closely imitate bio-synapse has led to an alamethicin-doped, synthetic biomembrane which can emulate key synaptic functions due to generic memristive property enabling learning and computation. This two-terminal, biomolecular memristor, in contrast to its solid-state counterparts, features similar structure, switching mechanism, and ionic transport modality as biological synapses while consuming considerably lower power. In this paper, we outline a methodology for using this biomolecular synapse to build neural networks capable of solving real-world problems. The physical mechanism underlying its volatile memristance is explored followed by the development of a model of this device for circuit simulation. We outline a circuit design technique to integrate this synapse with solid-state neuron circuit for hardware implementation. Based on these results, we develop a high level simulation framework and use a training scheme called Evolutionary Optimization for Neuromorphic System (EONS) to generate networks for solving two problems, namely iris dataset classification and EEG classification task. The small network size and comparable to state-of-the-art accuracy of these preliminary networks show its potential to enhance synaptic functionality in next generation neuromorphic hardware.",project-academic
10.1007/978-3-642-36632-1_9,2012-10-11,p,"Springer, Berlin, Heidelberg",visage a face interpretation engine for smartphone applications," Smartphones represent powerful mobile computing devices enabling a wide variety of new applications and opportunities for human interaction, sensing and communications. Because smartphones come with front-facing cameras, it is now possible for users to interact and drive applications based on their facial responses to enable participatory and opportunistic face-aware applications. This paper presents the design, implementation and evaluation of a robust, real-time face interpretation engine for smartphones, called Visage, that enables a new class of face-aware applications for smartphones. Visage fuses data streams from the phone’s front-facing camera and built-in motion sensors to infer, in an energy-efficient manner, the user’s 3D head poses (i.e., the pitch, roll and yaw of user’s heads with respect to the phone) and facial expressions (e.g., happy, sad, angry, etc.). Visage supports a set of novel sensing, tracking, and machine learning algorithms on the phone, which are specifically designed to deal with challenges presented by user mobility, varying phone contexts, and resource limitations. Results demonstrate that Visage is effective in different real-world scenarios. Furthermore, we developed two distinct proof-of-concept applications, Streetview+ and Mood Profiler driven by Visage.",project-academic
,2020-08-04,a,,reinforced epidemic control saving both lives and economy," Saving lives or economy is a dilemma for epidemic control in most cities while smart-tracing technology raises people's privacy concerns. In this paper, we propose a solution for the life-or-economy dilemma that does not require private data. We bypass the private-data requirement by suppressing epidemic transmission through a dynamic control on inter-regional mobility that only relies on Origin-Designation (OD) data. We develop DUal-objective Reinforcement-Learning Epidemic Control Agent (DURLECA) to search mobility-control policies that can simultaneously minimize infection spread and maximally retain mobility. DURLECA hires a novel graph neural network, namely Flow-GNN, to estimate the virus-transmission risk induced by urban mobility. The estimated risk is used to support a reinforcement learning agent to generate mobility-control actions. The training of DURLECA is guided with a well-constructed reward function, which captures the natural trade-off relation between epidemic control and mobility retaining. Besides, we design two exploration strategies to improve the agent's searching efficiency and help it get rid of local optimums. Extensive experimental results on a real-world OD dataset show that DURLECA is able to suppress infections at an extremely low level while retaining 76\% of the mobility in the city. Our implementation is available at this https URL.",project-academic
10.1016/J.TRC.2020.102715,2020-10-01,a,Pergamon,deep reinforcement learning algorithm for dynamic pricing of express lanes with multiple access locations," Abstract None None This article develops a deep reinforcement learning (Deep-RL) framework for dynamic pricing on managed lanes with multiple access locations and heterogeneity in travelers’ value of time, origin, and destination. This framework relaxes assumptions in the literature by considering multiple origins and destinations, multiple access locations to the managed lane, en route diversion of travelers, partial observability of the sensor readings, and stochastic demand and observations. The problem is formulated as a partially observable Markov decision process (POMDP) and policy gradient methods are used to determine tolls as a function of real-time observations. Tolls are modeled as continuous and stochastic variables and are determined using a feedforward neural network. The method is compared against a feedback control method used for dynamic pricing. We show that Deep-RL is effective in learning toll policies for maximizing revenue, minimizing total system travel time, and other joint weighted objectives, when tested on real-world transportation networks. The Deep-RL toll policies outperform the feedback control heuristic for the revenue maximization objective by generating revenues up to 8.5% higher than the heuristic and for the objective minimizing total system travel time (TSTT) by generating TSTT up to 8.4% lower than the heuristic. We also propose reward shaping methods for the POMDP to overcome the undesired behavior of toll policies, like the jam-and-harvest behavior of revenue-maximizing policies. Additionally, we test transferability of the algorithm trained on one set of inputs for new input distributions and offer recommendations on real-time implementations of Deep-RL algorithms. The source code for our experiments is available online at None https://github.com/venktesh22/ExpressLanes_Deep-RL .",project-academic
10.1109/SSCI.2016.7849899,2016-12-01,p,IEEE,a fuzzy based machine learning model for robot prediction of link quality," With foresight into the state of the wireless channel, a robot can make various optimization decisions with regards to routing packets, planning mobility paths, or switching between diverse radios. However, the process of predicting link quality (LQ) is nontrivial due to the streaming and dynamic nature of radio wave propagation, which is complicated by robot mobility. Due to robot movement, the wireless propagation environment can change considerably in terms of distance, obstacles, noise, and interference. Therefore, LQ must be learned and regularly updated while the robot is online. However, the existing fuzzy-based models for assessing LQ are non-adaptable due to the absence of any learning mechanism. To address this issue, we introduce a fuzzy-based prediction model designed for the efficient online and incremental learning of LQ. The unique approach uses fuzzy logic to infer LQ based on the collective output from a series of offset classifiers and their posterior probabilities. In essence, the proposed model leverages machine learning for extracting the underlying functional relationship between the input and output variables, but deeper inferences are made from the output of the learning algorithms using fuzzy logic. Wireless link data from a real-world robot network was used to compare the model with the traditional linear regression approach. The results show statistically significant improvements in three out of the six real-world indoor and outdoor environments where the robot operated. Additionally, the novel approach offers a number of other benefits, including the flexibility to use fuzzy logic for model tuning, as well as the ability to make implementation efficiencies in terms of parallelization and the conservation of labeling resources.",project-academic
10.1016/J.ADHOC.2020.102224,2020-09-01,p,Elsevier,a performance modeling and analysis of a novel vehicular traffic flow prediction system using a hybrid machine learning based model," Abstract None None Traffic prediction on the road, as a vital part of the Intelligent Transportation System (ITS) has attracted much attention recently. It is always one of the hot topics about how to implement an efficient, robust, and accurate vehicular traffic prediction system. With the help of Machine Learning-based (ML) methods, especially Deep Learning-based (DL) methods, the accuracy of the prediction model is increased. However, we also noticed that there are still many open challenges under ML-based vehicular traffic prediction model real-world implementation. Firstly, the time consumption for training DL model is relatively large when compared to parametric models, such as ARIMA, SARIMA. Second, it is still a hot topic for road traffic prediction that how to capture the spacial relationship between road detectors, which is affected by the geographic correlation, as well as the time change. The last but not the least, it is important for us to implement the prediction system into the real world; meanwhile, we should find a way to make use of the advanced technology applied in ITS to improve the prediction system itself. In this paper, we focus on improving the features of the prediction model, which can be helpful for implementing the model in the real world. We present a new hybrid deep learning model by using Graph Convolutional Network (GCN) and the deep aggregation structure (i.e., the sequence to sequence structure) of Gated Recurrent Unit (GRU). Meanwhile, in order to solve the real-world prediction problem, i.e., the online prediction task, we present a new online prediction strategy by using refinement learning. In order to further improve the model’s accuracy and efficiency when applied to ITS, we make use of an efficient parallel training strategy while taking advantage of the vehicular cloud structure.",project-academic
10.1109/JLT.2017.2781540,2018-04-01,a,IEEE,cognitive assurance architecture for optical network fault management," In face of staggering traffic growth driven by cloud-based platforms, modern optical networks—forming the backbone of such connectivity—are faced with increasing requirements in terms of operational reliability. The challenge is that of cognition-driven learning and fault management workflows, cost-effectively assuring the next-generation networks. Machine learning, an artificial intelligence tool, can be conceived as an extremely promising instrument to address network assurance via dynamic data-driven operation, as opposed to static pre-engineered solutions. In this paper, we propose and demonstrate a cognitive fault detection architecture for intelligent network assurance. We introduce the concept of cognitive fault management, elaborate on its integration in transport software defined network controller, and demonstrate its operation based on real-world fault examples. Our framework both detects and identifies significant faults, and outperforms conventional fixed threshold-triggered operations, both in terms of detection accuracy and proactive reaction time.",project-academic
,2020-11-23,a,,resonance replacing software constants with context aware models in real time communication," Large software systems tune hundreds of 'constants' to optimize their runtime performance. These values are commonly derived through intuition, lab tests, or A/B tests. A 'one-size-fits-all' approach is often sub-optimal as the best value depends on runtime context. In this paper, we provide an experimental approach to replace constants with learned contextual functions for Skype - a widely used real-time communication (RTC) application. We present Resonance, a system based on contextual bandits (CB). We describe experiences from three real-world experiments: applying it to the audio, video, and transport components in Skype. We surface a unique and practical challenge of performing machine learning (ML) inference in large software systems written using encapsulation principles. Finally, we open-source FeatureBroker, a library to reduce the friction in adopting ML models in such development environments",project-academic
,2020-06-30,a,University of Oulu,6g white paper on edge intelligence," In this white paper we provide a vision for 6G Edge Intelligence. Moving towards 5G and beyond the future 6G networks, intelligent solutions utilizing data-driven machine learning and artificial intelligence become crucial for several real-world applications including but not limited to, more efficient manufacturing, novel personal smart device environments and experiences, urban computing and autonomous traffic settings. We present edge computing along with other 6G enablers as a key component to establish the future 2030 intelligent Internet technologies as shown in this series of 6G White Papers. 
In this white paper, we focus in the domains of edge computing infrastructure and platforms, data and edge network management, software development for edge, and real-time and distributed training of ML/AI algorithms, along with security, privacy, pricing, and end-user aspects. We discuss the key enablers and challenges and identify the key research questions for the development of the Intelligent Edge services. As a main outcome of this white paper, we envision a transition from Internet of Things to Intelligent Internet of Intelligent Things and provide a roadmap for development of 6G Intelligent Edge.",project-academic
10.1109/IWQOS.2017.7969146,2017-06-14,p,IEEE,using quality of computation to enhance quality of service in mobile computing systems," Mobile devices are ubiquitous but their resources are limited. However, they must be capable to run computationally intensive software, for example for image stitching, face recognition, and simulation-based artificial intelligence. As a solution, mobile devices can use nearby resources to offload computation. Distributed computing environments provide such features but ignore the nature of mobile devices, such as mobility, network, or battery changes. This leads to long delays, which reduce the quality of experience for the user. In this paper, we present Mobile Tasklets, a mobile extension of our distributed computing middleware. The design of Mobile Tasklets includes context monitoring, context-aware scheduling mechanisms, and an Android API for application integration. We identify the challenges of the integration of mobile devices into our distributed computing environment. We evaluate Mobile Tasklets in a real-world testbed with different context settings.",project-academic
10.1007/978-3-319-25808-9,2016-05-13,b,Springer,autonomic road transport support systems," The work on Autonomic Road Transport Support (ARTS) presented here aims at meeting the challenge of engineering autonomic behavior in Intelligent Transportation Systems (ITS) by fusing research from the disciplines of traffic engineering and autonomic computing. Ideas and techniques from leading edge artificial intelligence research have been adapted for ITS over the last 30 years. Examples include adaptive control embedded in real time traffic control systems, heuristic algorithms (e.g. in SAT-NAV systems), image processing and computer vision (e.g. in automated surveillance interpretation). Autonomic computing which is inspired from the biological example of the bodys autonomic nervous system is a more recent development. It allows for a more efficient management of heterogeneous distributed computing systems. In the area of computing, autonomic systems are endowed with a number of properties that are generally referred to as self-X properties, including self-configuration, self-healing, self-optimization, self-protection and more generally self-management. Some isolated examples of autonomic properties such as self-adaptation have found their way into ITS technology and have already proved beneficial. This edited volume provides a comprehensive introduction to Autonomic Road Transport Support (ARTS) and describes the development of ARTS systems. It starts out with the visions, opportunities and challenges, then presents the foundations of ARTS and the platforms and methods used and it closes with experiences from real-world applications and prototypes of emerging applications. This makes it suitable for researchers and practitioners in the fields of autonomic computing, traffic and transport management and engineering, AI, and software engineering. Graduate students will benefit from state-of-the-art description, the study of novel methods and the case studies provided.",project-academic
10.1109/ACCESS.2019.2958380,2019-12-09,a,IEEE,dual graph for traffic forecasting," Traffic forecasting is the task of predicting future traffic based on historical traffic data. It is challenging due to the complex spatial-temporal correlation on road networks. Most existing research works use sequential Graph Neural Networks (GNN) to model traffic inference. However, they only focus on nodes (intersections) or edges (road segments) traffic forecasting alone. As a result, they could hardly provide a complete description of future traffic on road networks. Actually, nodes and edges traffic are interrelated. Both of them are important for traffic safety and efficiency, and neither one is negligible. In this paper, we exploit nodes and edges information together and make traffic forecasting on nodes and edges simultaneously. We propose a novel dual graph framework, called DualGraph, to model the propagation behavior of traffic on road networks. Inside our framework, we develop a DualMap block to simulate the recursive interactions between nodes and edges. The interaction process is realized by a message passing mechanism of nearby information flow. We employ the Simulation of Urban MObility (SUMO) software to generate real-world traffic data to illustrate the effectiveness of our method. We also empirically evaluate our model on public traffic datasets. The results show that even for node or edge traffic forecasting alone, our model still outperforms compared ones, especially for long term (one hour) prediction.",project-academic
10.1016/J.COMCOM.2020.04.011,2020-05-01,a,Elsevier,service modeling for opportunistic edge computing systems with feature engineering," Abstract None None The complex and opportunistic environment in which edge computing systems operate, poses a fundamental challenge for online edge system orchestration, resource provisioning and real-time responsiveness in response to user movement. Such a challenge needs to addressed throughout the edge system lifecycle, starting from the software development methodologies. In this paper, we propose a novel development process for modeling opportunistic edge computing services, which rely on (i) ETSI MEC reference architecture and Opportunistic Internet of Things Service modeling for the early stage of system analysis and design, i.e. domain model and service metamodel; and on (ii) feature engineering for evaluating those opportunistic aspects with data analysis. To address the identified opportunistic properties, at the service design phase we construct (both automatically and through domain expertise) Opportunistic Feature Vectors for Edge, containing the numerical representations of those properties. Such vectors enable further data analysis and machine learning techniques in the development of distributed, effective and efficient edge computing systems. Lastly, we exemplify the integrated process with a microservice-based user mobility management service, based on a real-world data set, for online analysis in MEC systems.",project-academic
10.1109/ACCESS.2018.2850226,2018-06-25,a,IEEE,deep learning coordinated beamforming for highly mobile millimeter wave systems," Supporting high mobility in millimeter wave (mmWave) systems enables a wide range of important applications, such as vehicular communications and wireless virtual/augmented reality. Realizing this in practice, though, requires overcoming several challenges. First, the use of narrow beams and the sensitivity of mmWave signals to blockage greatly impact the coverage and reliability of highly-mobile links. Second, highly-mobile users in dense mmWave deployments need to frequently hand-off between base stations (BSs), which is associated with critical control and latency overhead. Furthermore, identifying the optimal beamforming vectors in large antenna array mmWave systems requires considerable training overhead, which significantly affects the efficiency of these mobile systems. In this paper, a novel integrated machine learning and coordinated beamforming solution is developed to overcome these challenges and enable highly-mobile mmWave applications. In the proposed solution, a number of distributed yet coordinating BSs simultaneously serve a mobile user. This user ideally needs to transmit only one uplink training pilot sequence that will be jointly received at the coordinating BSs using omni or quasi-omni beam patterns. These received signals draw a defining signature not only for the user location, but also for its interaction with the surrounding environment. The developed solution then leverages a deep learning model that learns how to use these signatures to predict the beamforming vectors at the BSs. This renders a comprehensive solution that supports highly mobile mmWave applications with reliable coverage, low latency, and negligible training overhead. Extensive simulation results based on accurate ray-tracing, show that the proposed deep-learning coordinated beamforming strategy approaches the achievable rate of the genie-aided solution that knows the optimal beamforming vectors with no training overhead. Compared with traditional beamforming solutions, the results show that the proposed deep learning-based strategy attains higher rates, especially in high-mobility large-array regimes.",project-academic
,2018-04-27,a,,deep learning coordinated beamforming for highly mobile millimeter wave systems," Supporting high mobility in millimeter wave (mmWave) systems enables a wide range of important applications such as vehicular communications and wireless virtual/augmented reality. Realizing this in practice, though, requires overcoming several challenges. First, the use of narrow beams and the sensitivity of mmWave signals to blockage greatly impact the coverage and reliability of highly-mobile links. Second, highly-mobile users in dense mmWave deployments need to frequently hand-off between base stations (BSs), which is associated with critical control and latency overhead. Further, identifying the optimal beamforming vectors in large antenna array mmWave systems requires considerable training overhead, which significantly affects the efficiency of these mobile systems. In this paper, a novel integrated machine learning and coordinated beamforming solution is developed to overcome these challenges and enable highly-mobile mmWave applications. In the proposed solution, a number of distributed yet coordinating BSs simultaneously serve a mobile user. This user ideally needs to transmit only one uplink training pilot sequence that will be jointly received at the coordinating BSs using omni or quasi-omni beam patterns. These received signals draw a defining signature not only for the user location, but also for its interaction with the surrounding environment. The developed solution then leverages a deep learning model that learns how to use these signatures to predict the beamforming vectors at the BSs. This renders a comprehensive solution that supports highly-mobile mmWave applications with reliable coverage, low latency, and negligible training overhead. Simulation results show that the proposed deep-learning coordinated beamforming strategy approaches the achievable rate of the genie-aided solution that knows the optimal beamforming vectors with no training overhead.",project-academic
,2018-10-23,a,PMLR,driving policy transfer via modularity and abstraction," End-to-end approaches to autonomous driving have high sample complexity and are difficult to scale to realistic urban driving. Simulation can help end-to-end driving systems by providing a cheap, safe, and diverse training environment. Yet training driving policies in simulation brings up the problem of transferring such policies to the real world. We present an approach to transferring driving policies from simulation to reality via modularity and abstraction. Our approach is inspired by classic driving systems and aims to combine the benefits of modular architectures and end-to-end deep learning approaches. The key idea is to encapsulate the driving policy such that it is not directly exposed to raw perceptual input or low-level vehicle dynamics. We evaluate the presented approach in simulated urban environments and in the real world. In particular, we transfer a driving policy trained in simulation to a 1/5-scale robotic truck that is deployed in a variety of conditions, with no finetuning, on two continents. The supplementary video can be viewed at this https URL",project-academic
,2007-01-06,p,Morgan Kaufmann Publishers Inc.,sharing the road autonomous vehicles meet human drivers," In modern urban settings, automobile traffic and collisions lead to endless frustration as well as significant loss of life, property, and productivity. Recent advances in artificial intelligence suggest that autonomous vehicle navigation may soon be a reality. In previous work, we have demonstrated that a reservation-based approach can efficiently and safely govern interactions of multiple autonomous vehicles at intersections. Such an approach alleviates many traditional problems associated with intersections, in terms of both safety and efficiency. However, the system relies on all vehicles being equipped with the requisite technology-a restriction that would make implementing such a system in the real world extremely difficult. In this paper, we extend this system to allow for incremental deployability. The modified system is able to accommodate traditional human-operated vehicles using existing infrastructure. Furthermore, we show that as the number of autonomous vehicles on the road increases, traffic delays decrease monotonically toward the levels exhibited in our previous work. Finally, we develop a method for switching between various human-usable configurations while the system is running, in order to facilitate an even smoother transition. The work is fully implemented and tested in our custom simulator, and we present detailed experimental results attesting to its effectiveness.",project-academic
10.2514/6.2017-3085,2017-06-05,p,American Institute of Aeronautics and Astronautics,exploring concepts of operations for on demand passenger air transportation," In recent years, a surge of interest in ""flying cars"" for city commutes has led to rapid development of new technologies to help make them and similar on-demand mobility platforms a reality. To this end, this paper provides analyses of the stakeholders involved, their proposed operational concepts, and the hazards and regulations that must be addressed. Three system architectures emerged from the analyses, ranging from conventional air taxi to revolutionary fully autonomous aircraft operations, each with vehicle safety functions allocated differently between humans and machines. Advancements for enabling technologies such as distributed electric propulsion and artificial intelligence have had major investments and initial experimental success, but may be some years away from being deployed for on-demand passenger air transportation at scale.",project-academic
10.1109/MNET.011.2000371,2021-03-01,a,IEEE,generalizing ai challenges and opportunities for plug and play ai solutions," Artificial Intelligence (AI) has revolutionized today's Internet of Things (IoT) applications and services by introducing significant technological enhancements across a multitude of domains. With the deployment of the fifth generation (5G) mobile communication network, smart city visions of fast, on-demand, intelligent user-specific services are now becoming a reality. The concept of connected IoT is evolving into connected intelligent things. The advancements of both AI techniques, coupled with the sophistication of edge devices, is now leading to a new era of connected intelligence. Moving the intelligence toward end devices must account for latency demands and simplicity of selecting the type of AI technique to be used. Moreover, since most AI techniques require learning from big data sets and reasoning using a multitude of classification patterns, new simplified and collaborative solutions are now necessary more than ever. As such, the concept of introducing decentralized and distributed ‘Plug and Play’ (PnP) AI tools is now becoming more attractive given the vast numbers in edge devices, data volume and AI techniques. To this end, this article envisions a novel general AI solution that can be adapted to autonomously select the type of machine learning (ML) algorithm, the data set to be used, and provide reasoning in regards to data selection for optimal features extraction. Moreover, the solution performs the necessary training and all the necessary parameter fine-tunings to achieve the highest level of generality and simplicity for AI at the edge. We explore several aspects related to PnP-AI and its impact in the smart city ecosystem.",project-academic
10.1049/IET-SMC.2019.0034,2019-06-01,a,The Institution of Engineering and Technology,city brain practice of large scale artificial intelligence in the real world," A city is an aggregate of a huge amount of heterogeneous data. However, extracting meaningful values from that data remains a challenge. City Brain is an end-to-end system whose goal is to glean irreplaceable values from big city data, specifically from videos, with the assistance of rapidly evolving artificial intelligence technologies and fast-growing computing capacity. From cognition to optimisation, to decision-making, from search to prediction and ultimately, to intervention, City Brain improves the way to manage the city, as well as the way to live in it. In this study, the authors introduce current practices of the City Brain platform in a few cities in China, including what they can do to achieve the goal and make it a reality. Then they focus on the system overview and key technical details of each component of the City Brain system, from cognition to intervention. Lastly, they present a few deployment cases of City Brain in various cities in China.",project-academic
10.1007/S43154-021-00048-3,2021-06-01,a,Springer International Publishing,multi agent systems for search and rescue applications," The goal of this review is to evaluate the current status of multi-robot systems in the context of search and rescue. This includes an investigation of their current use in the field, what major technical challenge areas currently preclude more widespread use, and which key topics will drive future development and adoption. Work blending machine learning with classical control techniques is driving progress in perception-driven autonomy, decentralized multi-robot coordination, and human–robot interaction, among others. Ad hoc mesh networking has achieved reliability suitable for safety-critical applications and may be a partial solution for communication. New modular and multimodal platforms may overcome mobility limitations without significantly increasing cost. Multi-agent systems are not currently ready for deployment in search and rescue applications; however, progress is being made in a number of critical domains. As the field matures, research should focus on realistic evaluations of constituent technologies, and on confronting the challenges of simulation-to-reality transfer, algorithmic bias in autonomous agents that rely on machine learning, and novelty-versus-reliability incentive mismatch",project-academic
10.1109/ACCESS.2020.3026540,2020-09-24,a,Institute of Electrical and Electronics Engineers (IEEE),challenges on the way of implementing tcp over 5g networks," 5G cellular communication, especially with its hugely available bandwidth provided by millimeter-wave, is a promising technology to fulfill the coming high demand for vast data rates. These networks can support new use cases such as Vehicle to Vehicle and augmented reality due to its novel features such as network slicing along with the mmWave multi-gigabit-per-second data rate. Nevertheless, 5G cellular networks suffer from some shortcomings, especially in high frequencies because of the intermittent nature of channels when the frequency rises. Non-line of sight state, is one of the significant issues that the new generation encounters. This drawback is because of the intense susceptibility of higher frequencies to blockage caused by obstacles and misalignment. This unique characteristic can impair the performance of the reliable transport layer widely deployed protocol, TCP, in attaining high throughput and low latency throughout a fair network. As a result, the protocol needs to adjust the congestion window size based on the current situation of the network. However, TCP is not able to adjust its congestion window efficiently, and it leads to throughput degradation of the protocol. This paper presents a comprehensive analysis of reliable end-to-end communications in 5G networks. It provides the analysis of the effects of TCP in 5G mmWave networks, the discussion of TCP mechanisms and parameters involved in the performance over 5G networks, and a survey of current challenges, solutions, and proposals. Finally, a feasibility analysis proposal of machine learning-based approaches to improve reliable end-to-end communications in 5G networks is presented.",project-academic
10.1109/IEEECONF44664.2019.9049048,2019-11-01,p,IEEE Computer Society,fire frontline monitoring by enabling uav based virtual reality with adaptive imaging rate," Recently, using drones for forest fire management has gained a lot of attention from the research community due to their advantages such as low operation and deployment cost, flexible mobility, and high-quality imaging. It also minimizes human intervention, especially in hard-to-reach areas where the use of ground-based infrastructure is troublesome. Drones can provide virtual reality to firefighters by collecting on-demand high-resolution images with adjustable zoom, focus, and perspective to improve fire control and eliminate human hazards. In this paper, we propose a novel model for fire expansion as well as a distributed algorithm for drones to relocate themselves towards the front-line of an expanding fire field. The proposed algorithm comprises a light-weight image processing for fire edge detection that is highly desirable over computational expensive deep learning methods for resource-constrained drones. The positioning algorithm includes motions tangential and normal to fire frontline to follow the fire expansion while keeping minimum pairwise distances for collision avoidance and non-overlapping imaging. We proposed an action-reward mechanism to adjust the drones’ speed and processing rate based on the fire expansion rate and the available onboard processing power. Simulations results are provided to support the efficacy of the proposed algorithm.",project-academic
10.1007/978-3-030-28925-6_1,2018-11-21,a,Springer Science and Business Media Deutschland GmbH,cityflow supporting spatial temporal edge computing for urban machine learning applications," A growing trend in smart cities is the use of machine learning techniques to gather city data, formulate learning tasks and models, and use these to develop solutions to city problems. However, although these processes are sufficient for theoretical experiments, they often fail when they meet the reality of city data and processes, which by their very nature are highly distributed, heterogeneous, and exhibit high degrees of spatial and temporal variance. In order to address those problems, we have designed and implemented an integrated development environment called CityFlow that supports developing machine learning applications. With CityFlow, we can develop, deploy, and maintain machine learning applications easily by using an intuitive data flow model. To verify our approach, we conducted two case studies: deploying a road damage detection application to help monitor transport infrastructure and an automatic labeling application in support of a participatory sensing application. These applications show both the generic applicability of our approach, and its ease of use; both critical if we wish to deploy sophisticated ML based applications to smart cities.",project-academic
10.1364/JOCN.403205,2021-01-01,a,Optical Society of America,open whitebox architecture for smart integration of optical networking and data center technology invited," In this paper, we identify challenges in developing future optical network infrastructure for new services based on technologies such as 5G, virtual reality, and artificial intelligence, and we suggest approaches to handling these challenges that include a business model, architecture, and diversity. Through activities in multiservice agreement and de facto standard organizations, we have shown how the hardware abstraction layer interfaces of optical transceivers are implemented for multivendor and heterogeneous environments, coherent digital signal processor interoperability, and optical transport whiteboxes. We have driven the effort to define the transponder abstraction interface with partners. The feasibility of such implementation was verified through demonstrations and trials. In addition, we are constructing an open-transport platform by combining existing open-source software and implementing software components that automate and enhance operations. An open architecture maintains a healthy ecosystem for industry and allows for a flexible, operator-driven network.",project-academic
10.1145/3343031.3351061,2019-10-15,p,ACM,3d point cloud geometry compression on deep learning," 3D point cloud presentation has been widely used in computer vision, automatic driving, augmented reality, smart cities and virtual reality. 3D point cloud compression method with higher compression ratio and tiny loss is the key to improve data transportation efficiency. In this paper, we propose a new 3D point cloud geometry compression method based on deep learning, also an auto-encoder performing better than other networks in detail reconstruction. It can reach much higher compression ratio than the state-of-art while keeping tolerable loss. It also supports parallel compressing multiple models by GPU, which can improve processing efficiency greatly. The compression process is composed of two parts. Firstly, Raw data is compressed into codeword by extracting feature of raw model with encoder. Then, the codeword is further compressed with sparse coding. Decompression process is implemented in reverse order. Codeword is recovered and fed into decoder to reconstruct point cloud. Detail reconstruction ability is improved by a hierarchical structure in our decoder. Latter outputs are grown from former fuzzier outputs. In this way, details are added to former output by latter layers step by step to make a more precise prediction. We compare our method with PCL compression and Draco compression on ShapeNet40 part dataset. Our method may be the first deep learning-based point cloud compression algorithm. The experiments demonstrate it is superior to former common compression algorithms with large compression ratio, which can also reserve original shapes with tiny loss.",project-academic
10.1109/IV.2015.88,2015-07-22,p,IEEE,perceived realism of crowd behaviour with social forces," This paper investigates the development of an urban crowd simulation for the purposes of psychophysical experimentation. Whilst artificial intelligence (AI) is advancing to produce more concise and interesting crowd behaviours, the number or sophistication of the algorithms implemented within a system does not necessarily guarantee its perceptual realism. Human perception is highly subjective and does not always conform to the reality of the situation. Therefore it is important to consider this aspect when dealing with A implementations within a crowd system aimed at humans. In this research an initial two-alternative forced choice (2AFC) with constant stimuli psychophysical experiment is presented. The purpose of the experiment is to assess whether human participants perceive crowd behaviour with a social forces model to be more realistic. Results from the experiment suggest that participants do consider crowd behaviour with social forces to be more realistic. This research could inform the development of crowd-based systems, especially those that consider viewer perception to be important, such as for example video games and other media.",project-academic
10.1109/URS.2007.371826,2007-04-11,p,IEEE,cellular automata urban growth model calibration with genetic algorithms," Last few decades witness a dramatic increase in city population worldwide associated with excessive urbanization rates. This raises the necessity to understand the dynamics of urban growth process for sustainable distribution of available resources. Cellular automata, an artificial intelligence technique composed of pixels, states, neighborhood and transition rules, is being widely implemented to model the urban growth process due to its ability to fit such complex spatial nature using simple and effective rules. The main objective of our work is to use genetic algorithms to effectively calibrate, i.e., identify transition rule values, a cellular automata urban growth model that is designed as a function of multitemporal satellite imagery and population density. Transition rules in our model identify the required neighborhood urbanization level for a test pixel to develop. Calibration is performed spatially to find best rule values per township. Genetic algorithms calibration model, through proper design of their parameters, including objective function, initial population, selection, crossover and mutation, is prepared to fit the cellular automata model. Genetic algorithms start processing the initial solution space, through sequential implementation of the parameters, to identify the best rule values using a predefined criterion over the maximum number of iterations. Minimum objective function, representing the total modeling errors, is used to identify the optimal rule values. Each rule set is evaluated in term of urban level and pattern match with reality. Calibration with genetic algorithms proves to be effective in producing the optimal rule values in a time effective manner at an early generation. Proposed calibration algorithm is implemented to model the historical urban growth of Indianapolis-IN, USA. Urban growth results show a close match for both urban count and pattern with reality.",project-academic
,2017-03-28,p,,challenges in online updating of individual choice models for recommender systems or autonomous decision agents," Significant choice modelling work in marketing and transport research has been carried out to estimate individual level choice models (ILMs). The extent of such work is however surprisingly limited, if contrasted with the increasing importance of highly personalised products and services across a wide range of markets. None None Both in the in the transport and marketing sectors, ILMs have been used to capture heterogeneity across decision makers. This individual information can in turn be used, for example to simulate individual level choices. In the transport micro-simulations context however, in the end, results that matters, are in most cases still at the aggregate level and it is obtained aggregating simulated choices of individual agents. On the other hand, as Dumont et al. [1] argue, transport policy impacts on various population segments are also of interest. Therefore disaggregate individual agents’ travel behaviour from micro-simulations can be aggregated to varying degrees to have information of policy effects at for the population segments of interest. None It is in marketing however, that the appeal of disaggregate consumers information is far more obvious obvious, e.g. to better target products or campaigns specific groups. And today, because individuals can be easily reached from targeting groups marketing has shift to target individuals. With the digitalisation of consumers markets (e.g. online shopping of products, music, films and tv series streaming services), individual recommendation based on preference learning mechanisms have skyrocketed. It should also be pointed out that personalised services are expanding far beyond Internet consumers markets across various sectors including transport (e.g. with personalised travel information systems). Therefore individual levels preference models are becoming integral part of service design or revenue management strategies of service providers. Moreover, individual preference based recommender systems are likely to gradually mutate into autonomous robotic avatars making decision on behalf of individuals, and such decisions should reflect idiosyncratic preferences of those individuals. None Against this background, the contribution of choice modellers to development of preference learning mechanisms for recommender systems has so far been marginal (a rare example is [2]). Indeed research and development of preference learning systems for this kind of applications has and mainly left to computer scientists. In this paper we analyse and discuss how current individual level choice models estimation techniques, as devised by choice modellers for other purposes can contribute on online learning of individual preferences for recommenders systems or autonomous agent applications. None In this paper we use simulation to explore how ILMs are efficiently updated online (via Bayesian updating) as new observations from a single individual becomes available, without re-estimating a full sample models. The base ILMs are generated using three different techniques: conditional parameters estimates from sample mixed multinomial logit models (MMNL) [3], as by-product Bayesian estimation of MMNLs [3,4], and a hierarchical Bayes (HB) procedure introduced by Dumont and al. [1]. Subsequently we assess how well the individual levels will perform in updating. Specifically, with our analyses, we intend to address the two challenges detailed below. None First, underlying Bayesian estimation or updating of preference parameters there is the hypothesis of stable preferences. In reality, however, preferences can often to drift. We test how responsive the Bayesian updating approach is in capturing preference drift. We also propose a procedure to accelerate it based on artificially increasing prior variance. This procedure is inspired by [1], where controlling prior variance is a means to shift the emphasis between individual choice behaviour and the sample level model used as prior in the HB estimation of their ILMs. None The second aspect we investigate is strictly connected to the first. Preference learning systems should be robust to outliers and the implementation of a preference updating system capturing preference drift should be able to distinguish when choices outside of the recommended set represent a preference change or outlier behaviour, resulting, e.g. from an observable (extreme) perturbation of the choice context. The system’s user could expose the nature of the outlier, after he or she has been prompted by the system itself to classify an observation outside the recommendation set. Alternatively, in order to avoid user’s intervention, the mechanism capturing the preference drift could be implemented such that the preference change is triggered after a threshold of clustered observations outside the preference set is reached. A further alternative approach can be based on a rewarding system similar to that implemented in reinforcement learning. When a preference parameter change leads to recommended sets that include the preferred alternative the learning system is rewarded, otherwise it is penalised. The update in parameters would be then related to long-term maximisation of the cumulative reward. The latter two approaches are tested in simulation here. None None None None None [1]        J. Dumont, M. Giergiczny, and S. Hess, ""Individual level models vs. sample level models: contrasts and mutual benefits,"" None Transportmetrica A: Transport Science, None vol. 11, pp. 465-483, 2015/07/03 2015. None None [2]        B. H. Chaptini, ""Use of discrete choice models with recommender systems,"" Massachusetts Institute of Technology, 2005. None None [3]        K. Train. (2006). None Mixed Logit Estimation by Maximum Simulated Likelihood. Chapter 11. None Available: None http://elsa.berkeley.edu/Software/abstracts/train1006mxlmsl.html None None None [4]        K. Train. (2006). None Mixed Logit Estimation by Maximum Simulated Likelihood. Chapter 12. None Available: None http://elsa.berkeley.edu/Software/abstracts/train1006mxlmsl.html",project-academic
10.1186/S12942-018-0144-X,2018-07-05,a,BioMed Central,geospatial blockchain promises challenges and scenarios in health and healthcare," A PubMed query run in June 2018 using the keyword ‘blockchain’ retrieved 40 indexed papers, a reflection of the growing interest in blockchain among the medical and healthcare research and practice communities. Blockchain’s foundations of decentralisation, cryptographic security and immutability make it a strong contender in reshaping the healthcare landscape worldwide. Blockchain solutions are currently being explored for: (1) securing patient and provider identities; (2) managing pharmaceutical and medical device supply chains; (3) clinical research and data monetisation; (4) medical fraud detection; (5) public health surveillance; (6) enabling truly public and open geo-tagged data; (7) powering many Internet of Things-connected autonomous devices, wearables, drones and vehicles, via the distributed peer-to-peer apps they run, to deliver the full vision of smart healthy cities and regions; and (8) blockchain-enabled augmented reality in crisis mapping and recovery scenarios, including mechanisms for validating, crediting and rewarding crowdsourced geo-tagged data, among other emerging use cases. Geospatially-enabled blockchain solutions exist today that use a crypto-spatial coordinate system to add an immutable spatial context that regular blockchains lack. These geospatial blockchains do not just record an entry’s specific time, but also require and validate its associated proof of location, allowing accurate spatiotemporal mapping of physical world events. Blockchain and distributed ledger technology face similar challenges as any other technology threatening to disintermediate legacy processes and commercial interests, namely the challenges of blockchain interoperability, security and privacy, as well as the need to find suitable and sustainable business models of implementation. Nevertheless, we expect blockchain technologies to get increasingly powerful and robust, as they become coupled with artificial intelligence (AI) in various real-word healthcare solutions involving AI-mediated data exchange on blockchains.",project-academic
10.1016/J.JNS.2020.117081,2020-11-15,a,Elsevier,new technologies and amyotrophic lateral sclerosis which step forward rushed by the covid 19 pandemic," Amyotrophic Lateral Sclerosis (ALS) is a fast-progressive neurodegenerative disease leading to progressive physical immobility with usually normal or mild cognitive and/or behavioural involvement. Many patients are relatively young, instructed, sensitive to new technologies, and professionally active when developing the first symptoms. Older patients usually require more time, encouragement, reinforcement and a closer support but, nevertheless, selecting user-friendly devices, provided earlier in the course of the disease, and engaging motivated carers may overcome many technological barriers. ALS may be considered a model for neurodegenerative diseases to further develop and test new technologies. From multidisciplinary teleconsults to telemonitoring of the respiratory function, telemedicine has the potentiality to embrace other fields, including nutrition, physical mobility, and the interaction with the environment. Brain-computer interfaces and eye tracking expanded the field of augmentative and alternative communication in ALS but their potentialities go beyond communication, to cognition and robotics. Virtual reality and different forms of artificial intelligence present further interesting possibilities that deserve to be investigated. COVID-19 pandemic is an unprecedented opportunity to speed up the development and implementation of new technologies in clinical practice, improving the daily living of both ALS patients and carers. The present work reviews the current technologies for ALS patients already in place or being under evaluation with published publications, prompted by the COVID-19 pandemic.",project-academic
10.3389/FMEDT.2021.747632,2021-10-08,a,Frontiers Media SA,living with assistive robotics exploring the everyday use of exoskeleton for persons with spinal cord injury," Background. Recent advancements in sensor technology and artificial intelligence mechanisms have led to a rapid increase in research and development of robotic orthoses or ‘exoskeletons’ to support people with mobility problems. The purpose of this case study was to provide insight into the lived reality of using the assistive robotic exoskeleton ReWalk. Method. We used ethnographic techniques to explore the everyday experience and use of the assistive robotic device. Results. We found that the appropriation and integration of the technology within the patient’s everyday lives required a social and collaborative effort, which continued into use. The decisions to utilise the technology (or not) was closely tied to physical, social, cultural, environmental, and psychological factors. Consequently, there was much variation in patients’ perception of the technology and opportunities for support. Four themes emerged: a) Meaning of mobility—physical mobility represents more than functional ability. Its present socio-cultural meaning is associated with an individual’s self-identity and life priorities. b) Accomplishing body-technique—integration with the body requires a long process of skill acquisition and re-embodiment. c) Adaptation and adjustment in use—successful use of the technology was characterised by ongoing adjustment and adaptation of the technology and ways of using it. d) Human element—introduction and sustained use of the exoskeleton demand a social and collaborative effort across the user’s professional and lay resources. Conclusions. This study highlights that the development and implementation of the technology need to be grounded in a deep understanding of the day-to-day lives and experiences of the people that use them.",project-academic
10.1038/SREP27380,2016-06-07,a,Nature Publishing Group,virtual planning control and machining for a modular based automated factory operation in an augmented reality environment," This study presents a modular-based implementation of augmented reality to provide an immersive experience in learning or teaching the planning phase, control system, and machining parameters of a fully automated work cell. The architecture of the system consists of three code modules that can operate independently or combined to create a complete system that is able to guide engineers from the layout planning phase to the prototyping of the final product. The layout planning module determines the best possible arrangement in a layout for the placement of various machines, in this case a conveyor belt for transportation, a robot arm for pick-and-place operations, and a computer numerical control milling machine to generate the final prototype. The robotic arm module simulates the pick-and-place operation offline from the conveyor belt to a computer numerical control (CNC) machine utilising collision detection and inverse kinematics. Finally, the CNC module performs virtual machining based on the Uniform Space Decomposition method and axis aligned bounding box collision detection. The conducted case study revealed that given the situation, a semi-circle shaped arrangement is desirable, whereas the pick-and-place system and the final generated G-code produced the highest deviation of 3.83 mm and 5.8 mm respectively.",project-academic
10.1109/ICSSS.2019.8882847,2019-03-14,p,IEEE,nb iot based smart car parking system," The main purpose of this paper is to propose a design of an smart car parking system based on NB-IoT commanded by an software application that instructs the number of cars to be parked on assigned parking lot by automating the parking and unparking of the car with the help of features of a website or application. In recent times, the concept of smart city and artificial intelligence has drastically increased great popularity. With the dawn of Internet of things the implementation of smart city is made practically achievable. Continual experiments are made in the field of IoT in order to improve the reliability of urban infrastructure, NB-IoT was developed by 3GPP standard. the use of different modern techniques such as artificial intelligence, argumented reality, wireless sensor based, GPS based, vehicle communication based by using either Arduino or raspberry pi board can reduce parking issues practically. But the term smart car parking aims at low cost, wide area coverage, low power consumption, high connectivity. And all these features are provided by NB-IoT.",project-academic
10.1109/ISC246665.2019.9071662,2019-10-01,p,IEEE,using virtual reality environments to predict pedestrian behaviour," Pedestrian behaviour modelling and simulation play a fundamental role in reducing traffic risks and new policies implementation costs. However, representing human behaviour in this dynamic environment is not a trivial task and such models require an accurate representation of pedestrian behaviour. Virtual environments have been gaining notoriety as a behaviour elicitation tool, but it is still necessary to understand the validity of this technique in the context of pedestrian studies, as well as to create guidelines for its use. This work proposes a proper methodology for pedestrian behaviour elicitation using virtual reality environments in conjunction with surveys or questionnaires. The methodology focuses on gathering data about the subject, the context, and the action taken, as well as on analyzing the collected data to finally output a behavioural model. The resulting model can be used as a feedback signal to improve environment conditions for experiment iterations. A concrete implementation was built based on this methodology, serving as an example for future studies. A virtual reality traffic environment and two surveys were used as data sources for pedestrian crossing experiments. The subjects controlled a virtual avatar using an HTC Vive and were asked to traverse the distance between two points in a city. The data collected during the experiment was analyzed and used as input to a machine learning model capable of predicting pedestrian speed, taking into account their actions and perceptions. The proposed methodology allowed for the successful data gathering and its use to predict pedestrian behaviour with fairly acceptable accuracy.",project-academic
,2002-02-01,b,,experiential learning a best practice handbook for educators and trainers," 1. Unlocking powerful learning -- a new model Introduction The tumblers An overview of the chapters Conclusion 2. Exploring experiential learning Introduction Defining experiential learning A meaningful experience Learning is personal Painful learning Detrimental experiential learning Learning from mistakes Formal versus experiential learning The lineage of experience learning Experience as learning styles A chronology of experiential learning Challenging the concept of experiential learning Conclusion 3. Facilitation, good practice and ethics Introduction The booming business The deliverers Experiential provider roles Intruding complicators or enabling animateurs Wisdom and experience Dysfunctional and indigenous learning Setting the climate and conditions Ground rules and values Reviewing self-practice Ethical behaviour A question of balance Ethical models Codes of practice Professional bodies and the professional codes of practice Good practice: the environment Conclusion 4. Learning environments: spaces and places Introduction Indoor learning: the new classroom Outdoor learning Disappearing boundaries: indoor--outdoor, natural--artificial Reaching out: Learning in city space Artificially created learning spaces Pedagogy and personal development Empathetic strategies and the outdoor therapeutic 'effect' Outdoor environments: therapeutic experiential learning Sustainable learning environments Conclusion 5. Experiential learning activities Introduction The changing milieu Planned or unplanned activity? Dramaturgy Innovation, activities, resources and objects -- a simple experiential typology Adventurous journeys Sequencing learning activities Mind and body Rules and obstacles Constructing and deconstructing Telling the story -- using physical objects Conclusion 6. Learning activities: exploring reality Introduction What is a real experience? Fantasy Play and reality Suspending reality: drama and role-playing Metaphors and storytelling Management development and cartoons Using photographic images and computer software Reflections on reality -- reading and writing Rafts and planks... or real projects? Conclusion 7. Working with the senses Introduction Re-awakening the senses Appealing to the senses: higher education Sensory stimulation in learning and therapy Inner sensory work: presencing and anchoring Conclusion 8. Experience and emotions Introduction Emotion and experiential learning The power of the emotional state Emotional waves Experiencing emotional calm -- sorting time Flow learning Experience, learning and 'identity' Conclusion 9. Working with emotions Introduction The emotional climate -- mood setting and relaxed alertness Overcoming emotion -- fear Mapping and accessing emotions Using trilogies in emotional work Using humour and other positive emotions Accessing emotions through popular metaphors Metaphoric intervention Conclusion 1. Experience and intelligence Introduction Working with intelligence Other forms of intelligence Emotional quotient -- EQ Spiritual quotient -- SQ Naturalistic intelligence -- NQ The creative quotient -- CQ Conclusion 11. Learning and change Introduction Learning and change Theories of learning: theories of change! The development of reflective practice Using problems and challenges Reflection-in-action and reflection-on-action Single and double loop learning Encouraging conditions for reflection The danger of formal education and training Critical reflection Action learning The action learning set Timing and duration of learning sets Problems and action learning Strategies for learning and change Conclusion 12. Imagining and experiencing the future Introduction Imagination Imagination versus action Mental fitness for the future Imagining the future The value of problems Imaginative strategies Imagination and the child Conclusion",project-academic
,2002-01-01,b,"Stylus Publishing, P.O. Box 605, Herndon, VA 20172-0605 ($29.95 plus $5 shipping). Tel: 800-232-0223 (Toll Free); Web site: http://styluspub.com.",the power of experiential learning a handbook for trainers and educators," Part 1 Unlocking new combinations: Introduction The tumblers An overview of the chapters Conclusion. Part 2 Exploring experiential learning: Introduction Defining experiential learning A meaningful experience Learning is personal Painful learning Detrimental experiential learning Learning from mistakes Formal versus experiential learning The lineage of experience learning Experience as learning styles A chronology of experiential learning Challenging the concept of experiential learning Conclusion. Part 3 The design milieu: Introduction The milieu - activities, methods, techniques and materials Planned or unplanned? Innovation, activities, resources and objects - a simple experiential typology Stimulating intelligence Adventurous activities and journeys - challenge and risk Sequencing the challenges Mind challenges Sensory stimulation Change the rules and create obstacles Constructing and using physical objects Telling the story - using physical objects Conclusion. Part 4 Exploring reality: Introduction What is a real experience? Fantasy Play as experiential learning Suspending reality - drama and role-playing Metaphors and storytelling Management development and cartoons Creating comic strips - suggestions for good practice Using photographic images and computer software Reading and writing - reflections on reality Conclusion. Part 5 Places and elements: Introduction Indoor-outdoor, natural-artificial Artificial urban environments Pedagogy and personal development The sensory power of nature Empathetic strategies and the outdoor ""cure"" Rafts or wildlife projects? Eco-adventure and multiple learning Sustainable development. Part 6 The emotional experience: Introduction Emotion and experiential learning The emotional nerve centre The powerof the emotional state Emotional waves Experiencing emotional calm - sorting time Ecstasy and peak learning Experience, learning and ""identity"" Spiritual feelings Conclusion. Part 7 Working with emotions: Introduction The emotional climate - mood setting and relaxed alertness Overcoming emotion - fear Mapping fears - accessing the inner family Using trilogies in emotional work Using humour and other positive emotions Accessing emotions through popular metaphors Metaphoric intervention Conclusion. Part 8 Good practice and ethics: Introduction The booming business The deliverers Facilitator roles Intruding complicators or enabling animateurs Dysfunctional and indigenous learning Setting the climate and conditions. (Part contents.)",project-academic
10.1007/978-90-481-9151-2,2010-07-12,b,"Springer Publishing Company, Incorporated",technological developments in networking education and automation," Technological Developments in Networking, Education and Automation includes a set of rigorously reviewed world-class manuscripts addressing and detailing state-of-the-art research projects in the following areas: Computer Networks: Access Technologies, Medium Access Control, Network architectures and Equipment, Optical Networks and Switching, Telecommunication Technology, and Ultra Wideband Communications. Engineering Education and Online Learning: including development of courses and systems for engineering, technical and liberal studies programs; online laboratories; intelligent testing using fuzzy logic; taxonomy of e-courses; and evaluation of online courses. Pedagogy: including benchmarking; group-learning; active learning; teaching of multiple subjects together; ontology; and knowledge management. Instruction Technology: including internet textbooks; virtual reality labs, instructional design, virtual models, pedagogy-oriented markup languages; graphic design possibilities; open source classroom management software; automatic email response systems; tablet-pcs; personalization using web mining technology; intelligent digital chalkboards; virtual room concepts for cooperative scientific work; and network technologies, management, and architecture. Coding and Modulation: Modeling and Simulation, OFDM technology , Space-time Coding, Spread Spectrum and CDMA Systems. Wireless technologies: Bluetooth , Cellular Wireless Networks, Cordless Systems and Wireless Local Loop, HIPERLAN, IEEE 802.11, Mobile Network Layer, Mobile Transport Layer, and Spread Spectrum. Network Security and applications: Authentication Applications, Block Ciphers Design Principles, Block Ciphers Modes of Operation, Electronic Mail Security, Encryption & Message Confidentiality, Firewalls, IP Security, Key Cryptography & Message Authentication, and Web Security. Robotics, Control Systems and Automation: Distributed Control Systems, Automation, Expert Systems, Robotics, Factory Automation, Intelligent Control Systems, Man Machine Interaction, Manufacturing Information System, Motion Control, and Process Automation. Vision Systems: for human action sensing, face recognition, and image processing algorithms for smoothing of high speed motion. Electronics and Power Systems: Actuators, Electro-Mechanical Systems, High Frequency Converters, Industrial Electronics, Motors and Drives, Power Converters, Power Devices and Components, and Power Electronics.",project-academic
10.1007/978-3-642-37042-7_2,2013-01-01,b,Springer Berlin Heidelberg,digital storytelling within virtual environments the battle of thermopylae," Until recently virtual environments and videogame applications rarely incorporated any deep cultural or educational principles. Current research in the area of virtual reality applications, clearly indentify that they are extremely motivating for learners and therefore can be employed as an innovative, more accessible framework to deliver education while at the same time entertain the public. On the other hand, recent advances in videogame applications and human computer interfaces, demonstrate that a combination of game applications with effective learning principles and intuitive human computer interfaces could potentially transform virtual environments to a significant educational tool that could significantly facilitate the learning process. This paper describes an interactive virtual reality application that we developed for the museum of Thermopylae located at the site of the original battle, near the city of Lamia in Greece. We utilized storytelling techniques and principles of modern videogames to disseminate historical knowledge about the battle and the associated legends. We present the hardware and software components comprising the proposed installation, while we elaborate over the educational techniques designed to reinforce the strength of virtual reality technology as a mean of designing educational experiences in the context of cultural heritage related information.",project-academic
,2008-01-01,p,Artesis University College of Antwerp,crowd simulation for urban planning," This paper presents a semi-automatic visualization method for the evaluation of urban environments that is based on artificial intelligence. It proposes the use of agent-based crowd simulation software on a mid-scale urban planning level for design evaluation. The information on agents’ movements is noted in standard raster images. The results are maps that are easy to understand. These maps show movement paths of the agents and density and give further conclusion on bottlenecks in planning contexts. Key measures, like the occupant movement in a given district, until now relied greatly on empirical knowledge or data that could be only gathered after an urban design had become built reality. Our method focuses on the adaptation of common software technology that is originally situated in film and TV productions. A practical workflow shows how our method can be easily integrated in daily design tasks. Keywords: Artificial intelligence; agent-based; crowd simulation; urban planning; design evaluation; occupant movement.",project-academic
10.1016/B978-0-12-821326-1.00007-3,2020-01-01,c,Academic Press,internet of things from hype to reality," Abstract None None The era of the Internet of Things (IoT) is sweeping over and replacing the Internet creating a world where smart things exist connected to each other intelligently. This was predicted by Eric Emerson Schmidt, the former C.E.O. of Google over 20 years ago. The physical world is now connecting to the digital world so quickly with the emergence of the IoT that it seems the Internet will become invisible soon, meaning the physical world will be connecting to the digital world seamlessly. The world will enjoy smart connectivity in the same way that the city of Barcelona has emerged to be the smartest city in the world. We are moving toward system-to-system connection, with smart networking reaching its peak. The idea of software-defined autonomous machines is about to become hugely important, which will become ubiquitous. With the advent of the IoT, we explore how it is becoming a reality and whether it has any limits. Maciej Kranz in his book on the IoT explains the very essential detailed and inclusive idea of the IoT, with IoT expanding to businesses, and covering and impacting on a variety of technology areas. Artificial intelligence and machine learning have a huge scope because of the enormous data generated by sensors and devices connected through the IoT. We will explore in this chapter the hype around the IoT and the reality. We will also discover improved metrics in the IoT that is allowing it to be a leader in the technological world. We are witnessing the fourth revolution in the digitization world and discuss the reasons behind its exponential growth. The protocols that differentiate them from others have evolved for IOT in a new set of patterns. This also creates security concerns and data are described as the new oil, raising further challenges of data privacy.",project-academic
,2018-03-02,a,,a tutorial on uavs for wireless networks applications challenges and open problems," The use of flying platforms such as unmanned aerial vehicles (UAVs), popularly known as drones, is rapidly growing. In particular, with their inherent attributes such as mobility, flexibility, and adaptive altitude, UAVs admit several key potential applications in wireless systems. On the one hand, UAVs can be used as aerial base stations to enhance coverage, capacity, reliability, and energy efficiency of wireless networks. On the other hand, UAVs can operate as flying mobile terminals within a cellular network. Such cellular-connected UAVs can enable several applications ranging from real-time video streaming to item delivery. In this paper, a comprehensive tutorial on the potential benefits and applications of UAVs in wireless communications is presented. Moreover, the important challenges and the fundamental tradeoffs in UAV-enabled wireless networks are thoroughly investigated. In particular, the key UAV challenges such as three-dimensional deployment, performance analysis, channel modeling, and energy efficiency are explored along with representative results. Then, open problems and potential research directions pertaining to UAV communications are introduced. Finally, various analytical frameworks and mathematical tools such as optimization theory, machine learning, stochastic geometry, transport theory, and game theory are described. The use of such tools for addressing unique UAV problems is also presented. In a nutshell, this tutorial provides key guidelines on how to analyze, optimize, and design UAV-based wireless communication systems.",project-academic
10.1109/COMST.2019.2902862,2019-03-05,a,IEEE,a tutorial on uavs for wireless networks applications challenges and open problems," The use of flying platforms such as unmanned aerial vehicles (UAVs), popularly known as drones, is rapidly growing. In particular, with their inherent attributes such as mobility, flexibility, and adaptive altitude, UAVs admit several key potential applications in wireless systems. On the one hand, UAVs can be used as aerial base stations to enhance coverage, capacity, reliability, and energy efficiency of wireless networks. On the other hand, UAVs can operate as flying mobile terminals within a cellular network. Such cellular-connected UAVs can enable several applications ranging from real-time video streaming to item delivery. In this paper, a comprehensive tutorial on the potential benefits and applications of UAVs in wireless communications is presented. Moreover, the important challenges and the fundamental tradeoffs in UAV-enabled wireless networks are thoroughly investigated. In particular, the key UAV challenges such as 3D deployment, performance analysis, channel modeling, and energy efficiency are explored along with representative results. Then, open problems and potential research directions pertaining to UAV communications are introduced. Finally, various analytical frameworks and mathematical tools, such as optimization theory, machine learning, stochastic geometry, transport theory, and game theory are described. The use of such tools for addressing unique UAV problems is also presented. In a nutshell, this tutorial provides key guidelines on how to analyze, optimize, and design UAV-based wireless communication systems.",project-academic
10.1145/1378600.1378605,2008-06-17,p,ACM,the pothole patrol using a mobile sensor network for road surface monitoring," This paper investigates an application of mobile sensing: detecting and reporting the surface conditions of roads. We describe a system and associated algorithms to monitor this important civil infrastructure using a collection of sensor-equipped vehicles. This system, which we call the Pothole Patrol (P2), uses the inherent mobility of the participating vehicles, opportunistically gathering data from vibration and GPS sensors, and processing the data to assess road surface conditions. We have deployed P2 on 7 taxis running in the Boston area. Using a simple machine-learning approach, we show that we are able to identify potholes and other severe road surface anomalies from accelerometer data. Via careful selection of training data and signal features, we have been able to build a detector that misidentifies good road segments as having potholes less than 0.2% of the time. We evaluate our system on data from thousands of kilometers of taxi drives, and show that it can successfully detect a number of real potholes in and around the Boston area. After clustering to further reduce spurious detections, manual inspection of reported potholes shows that over 90% contain road anomalies in need of repair.",project-academic
10.1109/JSAC.2017.2680898,2017-03-09,a,IEEE,caching in the sky proactive deployment of cache enabled unmanned aerial vehicles for optimized quality of experience," In this paper, the problem of proactive deployment of cache-enabled unmanned aerial vehicles (UAVs) for optimizing the quality-of-experience (QoE) of wireless devices in a cloud radio access network is studied. In the considered model, the network can leverage human-centric information, such as users’ visited locations, requested contents, gender, job, and device type to predict the content request distribution, and mobility pattern of each user. Then, given these behavior predictions, the proposed approach seeks to find the user-UAV associations, the optimal UAVs’ locations, and the contents to cache at UAVs. This problem is formulated as an optimization problem whose goal is to maximize the users’ QoE while minimizing the transmit power used by the UAVs. To solve this problem, a novel algorithm based on the machine learning framework of conceptor-based echo state networks (ESNs) is proposed. Using ESNs, the network can effectively predict each user’s content request distribution and its mobility pattern when limited information on the states of users and the network is available. Based on the predictions of the users’ content request distribution and their mobility patterns, we derive the optimal locations of UAVs as well as the content to cache at UAVs. Simulation results using real pedestrian mobility patterns from BUPT and actual content transmission data from Youku show that the proposed algorithm can yield 33.3% and 59.6% gains, respectively, in terms of the average transmit power and the percentage of the users with satisfied QoE compared with a benchmark algorithm without caching and a benchmark solution without UAVs.",project-academic
,2016-10-05,a,,caching in the sky proactive deployment of cache enabled unmanned aerial vehicles for optimized quality of experience," In this paper, the problem of proactive deployment of cache-enabled unmanned aerial vehicles (UAVs) for optimizing the quality-of-experience (QoE) of wireless devices in a cloud radio access network (CRAN) is studied. In the considered model, the network can leverage human-centric information such as users' visited locations, requested contents, gender, job, and device type to predict the content request distribution and mobility pattern of each user. Then, given these behavior predictions, the proposed approach seeks to find the user-UAV associations, the optimal UAVs' locations, and the contents to cache at UAVs. This problem is formulated as an optimization problem whose goal is to maximize the users' QoE while minimizing the transmit power used by the UAVs. To solve this problem, a novel algorithm based on the machine learning framework of conceptor-based echo state networks (ESNs) is proposed. Using ESNs, the network can effectively predict each user's content request distribution and its mobility pattern when limited information on the states of users and the network is available. Based on the predictions of the users' content request distribution and their mobility patterns, we derive the optimal user-UAV association, optimal locations of the UAVs as well as the content to cache at UAVs. Simulation results using real pedestrian mobility patterns from BUPT and actual content transmission data from Youku show that the proposed algorithm can yield 40% and 61% gains, respectively, in terms of the average transmit power and the percentage of the users with satisfied QoE compared to a benchmark algorithm without caching and a benchmark solution without UAVs.",project-academic
10.1126/SCIENCE.1208365,2012-01-06,a,American Association for the Advancement of Science,the technology path to deep greenhouse gas emissions cuts by 2050 the pivotal role of electricity," The Technology Path to Deep Greenhouse Gas Emissions Cuts by 2050: The Pivotal Role of Electricity James H. Williams, 1,2 Andrew DeBenedictis, 1 Rebecca Ghanadan, 1,3 Amber Mahone, 1 Jack Moore, 1 William R. Morrow III, 4 Snuller Price, 1 Margaret S. Torn 3 * Several states and countries have adopted targets for deep reductions in greenhouse gas emissions by 2050, but there has been little physically realistic modeling of the energy and economic transformations required. We analyzed the infrastructure and technology path required to meet California’s goal of an 80% reduction below 1990 levels, using detailed modeling of infrastructure stocks, resource constraints, and electricity system operability. We found that technically feasible levels of energy efficiency and decarbonized energy supply alone are not sufficient; widespread electrification of transportation and other sectors is required. Decarbonized electricity would become the dominant form of energy supply, posing challenges and opportunities for economic growth and climate policy. This transformation demands technologies that are not yet commercialized, as well as coordination of investment, technology development, and infrastructure deployment. n 2004, Pacala and Socolow (1) proposed a way to stabilize climate using existing green- house gas (GHG) mitigation technologies, vi- sualized as interchangeable, global-scale “wedges” of equivalent emissions reductions. Subsequent work has produced more detailed analyses, but none combines the sectoral granularity, physical and resource constraints, and geographic scale needed for developing realistic technology and policy roadmaps (2–4). We addressed this gap by analyzing the specific changes in infrastructure, technology, cost, and governance required to de- carbonize a major economy, at the state level, that has primary jurisdiction over electricity supply, transportation planning, building standards, and other key components of an energy transition. California is the world’s sixth largest econ- omy and 12th largest emitter of GHGs. Its per capita GDP and GHG emissions are similar to those of Japan and western Europe, and its policy and technology choices have broad rele- vance nationally and globally (5, 6). California’s Assembly Bill 32 (AB32) requires the state to reduce GHG emissions to 1990 levels by 2020, a reduction of 30% relative to business-as-usual assumptions (7). Previous modeling work we per- formed for California’s state government formed the analytical foundation for the state’s AB32 implementation plan in the electricity and natural gas sectors (8, 9). California has also set a target of reducing 2050 emissions 80% below the 1990 level, con- I Energy and Environmental Economics, 101 Montgomery Street, Suite 1600, San Francisco, CA 94104, USA. 2 Monterey Institute of International Studies, 460 Pierce Street, Monterey, CA 93940, USA. 3 Energy and Resources Group, University of Cali- fornia,& Earth Sciences Division, Lawrence Berkeley National Laboratory (LBNL),, Berkeley, CA 94720, USA. 4 Environmental Energy Technologies Division, LBNL, Berkeley, CA 94720, USA. *To whom correspondence should be addressed. E-mail: mstorn@lbl.gov sistent with an Intergovernmental Panel on Cli- mate Change (IPCC) emissions trajectory that would stabilize atmospheric GHG concentrations at 450 parts per million carbon dioxide equivalent (CO 2 e) and reduce the likelihood of dangerous an- thropogenic interference with climate (10). Work- ing at both time scales, we found a pressing need for methodologies that bridge the analytical gap between planning for shallower, near-term GHG reductions, based entirely on existing commercialized technology, and deeper, long-term GHG reduc- tions, which will depend substantially on technol- ogies that are not yet commercialized. We used a stock-rollover methodology that simulated physical infrastructure at an aggregate level, and built scenarios to explore mitigation options (11, 12). Our model divided California’s economy into six energy demand sectors and two energy supply sectors, plus cross-sectoral eco- nomic activities that produce non-energy and non-CO 2 GHG emissions. The model adjusted the infrastructure stock (e.g., vehicle fleets, build- ings, power plants, and industrial equipment) in each sector as new infrastructure was added and old infrastructure was retired, each year from 2008 to 2050. We constructed a baseline scenario from government forecasts of population and gross state product, combined with regression-based infra- structure characteristics and emissions intensities, producing a 2050 emissions baseline of 875 Mt CO 2 e (Fig. 1). In mitigation scenarios, we used backcasting, setting 2050 emissions at the state target of 85 Mt CO 2 e as a constrained outcome, and altered the emissions intensities of new in- frastructure over time as needed to meet the tar- get, employing 72 types of physical mitigation measures (13). In the short term, measure selec- tion was driven by implementation plans for AB32 and other state policies (table S1). In the long term, technological progress and rates of in- troduction were constrained by physical feasi- bility, resource availability, and historical uptake rates rather than relative prices of technology, en- ergy, or carbon as in general equilibrium models (14). Technology penetration levels in our model are within the range of technological feasibility for the United States suggested by recent assess- ments (table S20) (15, 16). We did not include technologies expected to be far from commercial- ization in the next few decades, such as fusion- based electricity. Mitigation cost was calculated as the difference between total fuel and measure costs in the mitigation and baseline scenarios. Our fuel and technology cost assumptions, including learning curves (tables S4, S5, S11, and S12, and fig. S29), are comparable to those in other recent studies (17). Clearly, future costs are very uncertain over such a long time horizon, especially for technologies that are not yet commercialized. We did not assume explicit life-style changes (e.g., vegetarianism, bicycle transportation), which could have a substantial effect on mitigation requirements and costs (18); behavior change in our model is subsumed within conservation measures and en- ergy efficiency (EE). To ensure that electricity supply scenarios met the technical requirements for maintaining reli- able service, we included an electricity system dispatch algorithm that tested grid operability. Without a dispatch model, it is difficult to de- termine whether a generation mix has infeasibly high levels of intermittent generation. We devel- oped an electricity demand curve bottom-up from sectoral demand, by season and time of day. On the basis of the demand curve, the model con- strained generation scenarios to satisfy in succes- sion the energy, capacity, and system-balancing requirements for reliable operation. The operabil- ity constraint set physical limits on the penetra- tion of different types of generation and specified the requirements for peaking generation, on-grid energy storage, transmission capacity, and out-of- state imports and exports for a given generation mix (table S13 and figs.S20 to S31). It was as- sumed that over the long run, California would not “go it alone” in pursuing deep GHG reduc- tions, and thus that neighboring states would de- carbonize their generation such that the carbon intensity of imports would be comparable to that of California in-state generation (19). Electrification required to meet 80% reduc- tion target. Three major energy system transfor- mations were necessary to meet the target (Fig. 2). First, EE had to improve by at least 1.3% year −1 over 40 years. Second, electricity supply had to be nearly decarbonized, with 2050 emissions in- tensity less than 0.025 kg CO 2 e/kWh. Third, most existing direct fuel uses had to be electrified, with electricity constituting 55% of end-use energy in 2050 versus 15% today. Results for a mitigation scenario, including these and other measures, are shown in Fig. 1. Of the emissions reductions relative to 2050 baseline emissions, 28% came from EE, 27% from decarbonization of electricity generation, 14% from a combination of energy",project-academic
10.1109/TMC.2018.2866249,2019-08-01,a,Institute of Electrical and Electronics Engineers (IEEE),classifying iot devices in smart environments using network traffic characteristics," The Internet of Things (IoT) is being hailed as the next wave revolutionizing our society, and smart homes, enterprises, and cities are increasingly being equipped with a plethora of IoT devices. Yet, operators of such smart environments may not even be fully aware of their IoT assets, let alone whether each IoT device is functioning properly safe from cyber-attacks. In this paper, we address this challenge by developing a robust framework for IoT device classification using traffic characteristics obtained at the network level. Our contributions are fourfold. First, we instrument a smart environment with 28 different IoT devices spanning cameras, lights, plugs, motion sensors, appliances, and health-monitors. We collect and synthesize traffic traces from this infrastructure for a period of six months, a subset of which we release as open data for the community to use. Second, we present insights into the underlying network traffic characteristics using statistical attributes such as activity cycles, port numbers, signalling patterns, and cipher suites. Third, we develop a multi-stage machine learning based classification algorithm and demonstrate its ability to identify specific IoT devices with over 99 percent accuracy based on their network activity. Finally, we discuss the trade-offs between cost, speed, and performance involved in deploying the classification framework in real-time. Our study paves the way for operators of smart environments to monitor their IoT assets for presence, functionality, and cyber-security without requiring any specialized devices or protocols.",project-academic
,2018-12-30,a,,broadband analog aggregation for low latency federated edge learning," The popularity of mobile devices results in the availability of enormous data and computational resources at the network edge. To leverage the data and resources, a new machine learning paradigm, called edge learning, has emerged where learning algorithms are deployed at the edge for providing fast and intelligent services to mobile users. While computing speeds are advancing rapidly, the communication latency is becoming the bottleneck of fast edge learning. To address this issue, this work is focused on designing a low latency multi-access scheme for edge learning. We consider a popular framework, federated edge learning (FEEL), where edge-server and on-device learning are synchronized to train a model without violating user-data privacy. It is proposed that model updates simultaneously transmitted by devices over broadband channels should be analog aggregated ""over-the-air"" by exploiting the superposition property of a multi-access channel. Thereby, ""interference"" is harnessed to provide fast implementation of the model aggregation. This results in dramatical latency reduction compared with the traditional orthogonal access (i.e., OFDMA). In this work, the performance of FEEL is characterized targeting a single-cell random network. First, due to power alignment between devices as required for aggregation, a fundamental tradeoff is shown to exist between the update-reliability and the expected update-truncation ratio. This motivates the design of an opportunistic scheduling scheme for FEEL that selects devices within a distance threshold. This scheme is shown using real datasets to yield satisfactory learning performance in the presence of high mobility. Second, both the multi-access latency of the proposed analog aggregation and the OFDMA scheme are analyzed. Their ratio, which quantifies the latency reduction of the former, is proved to scale almost linearly with device population.",project-academic
10.1016/J.TRA.2021.01.020,2021-03-01,a,Pergamon,spatio temporal analysis of on demand transit a case study of belleville canada," Abstract None None The rapid increase in the cyber-physical nature of transportation, availability of GPS data, mobile applications, and effective communication technologies have led to the emergence of On-Demand Transit (ODT) systems. In September 2018, the City of Belleville in Canada started an on-demand public transit pilot project, where the late-night fixed-route (RT 11) was substituted with the ODT providing a real-time ride-hailing service. We present an in-depth analysis of the spatio-temporal demand and supply, level of service, and origin and destination patterns of Belleville ODT users, based on the data collected from September 2018 till May 2019. The independent and combined effects of the demographic characteristics (population density, working-age, and median income) on the ODT trip production and attraction levels were studied using GIS and the K-means machine learning clustering algorithm. The results indicate that ODT trips demand is highest for 11:00 pm–11:45 pm during the weekdays and 8:00 pm–8:30 pm during the weekends. We expect this to be the result of users returning home from work or shopping. Results showed that 39% of the trips were found to have a waiting time of smaller than 15 min, while 28% of trips had a waiting time of 15–30 min. The dissemination areas with higher population density, lower median income, or higher working-age percentages tend to have higher ODT trip attraction levels, except for the dissemination areas that have highly attractive places like commercial areas. For the sustainable deployment of ODT services, we recommend (a) proactively relocating the empty ODT vehicles near the neighbourhoods with high level of activity, (b) dynamically updating the fleet size and location based on the anticipated changes in the spatio-temporal demand, and (c) using medium occupancy vehicles, like vans or minibuses to ensure high level of service.",project-academic
10.1109/MCOM.2018.1700298,2018-02-01,a,IEEE,enabling cognitive smart cities using big data and machine learning approaches and challenges," The development of smart cities and their fast-paced deployment is resulting in the generation of large quantities of data at unprecedented rates. Unfortunately, most of the generated data is wasted without extracting potentially useful information and knowledge because of the lack of established mechanisms and standards that benefit from the availability of such data. Moreover, the highly dynamic nature of smart cities calls for a new generation of machine learning approaches that are flexible and adaptable to cope with the dynamicity of data to perform analytics and learn from real-time data. In this article, we shed light on the challenge of underutilizing the big data generated by smart cities from a machine learning perspective. In particular, we present the phenomenon of wasting unlabeled data. We argue that semi-supervision is a must for smart cities to address this challenge. We also propose a three-level learning framework for smart cities that matches the hierarchical nature of big data generated by smart cities with a goal of providing different levels of knowledge abstraction. The proposed framework is scalable to meet the needs of smart city services. Fundamentally, the framework benefits from semi-supervised deep reinforcement learning where a small amount of data that has users' feedback serves as labeled data, while a larger amount without such users' feedback serves as unlabeled data. The framework utilizes a mix of labeled and unlabeled data to converge toward better control policies instead of wasting the unlabeled data. This article also explores how deep reinforcement learning and its shift toward semi-supervision can handle the cognitive side of smart city services and improve their performance by providing several use cases spanning the different domains of smart cities. We also highlight several challenges as well as promising future research directions for incorporating machine learning and high-level intelligence into smart city services.",project-academic
10.1109/CCNC.2012.6181070,2012-04-12,p,IEEE,quality of experience estimation for adaptive http tcp video streaming using h 264 avc," Video services are being adopted widely in both mobile and fixed networks. For their successful deployment, the content providers are increasingly becoming interested in evaluating the performance of such traffic from the final users' perspective, that is, their Quality of Experience (QoE). For this purpose, subjective quality assessment methods are costly and can not be used in real time. Therefore, automatic estimation of QoE is highly desired. In this paper, we propose a no-reference QoE monitoring module for adaptive HTTP streaming using TCP and the H.264 video codec. HTTP streaming using TCP is the popular choice of many web based and IPTV applications due to the intrinsic advantages of the protocol. Moreover, these applications do not suffer from video data loss due to the reliable nature of the transport layer. However, there can be playout interruptions and if adaptive bitrate video streaming is used then the quality of video can vary due to lossy compression. Our QoE estimation module, based on Random Neural Networks, models the impact of both factors. The results presented in this paper show that our model accurately captures the relation between them and QoE.",project-academic
,2018-12-30,a,,low latency broadband analog aggregation for federated edge learning," The popularity of mobile devices results in the availability of enormous data and computational resources at the network edge. To leverage the data and resources, a new machine learning paradigm, called edge learning, has emerged where learning algorithms are deployed at the edge for providing fast and intelligent services to mobile users. While computing speeds are advancing rapidly, the communication latency is becoming the bottleneck of fast edge learning. To address this issue, this work is focused on designing a low latency multi-access scheme for edge learning. We consider a popular framework, federated edge learning (FEEL), where edge-server and on-device learning are synchronized to train a model without violating user-data privacy. It is proposed that model updates simultaneously transmitted by devices over broadband channels should be analog aggregated ""over-the-air"" by exploiting the superposition property of a multi-access channel. Thereby, ""interference"" is harnessed to provide fast implementation of the model aggregation. This results in dramatical latency reduction compared with the traditional orthogonal access (i.e., OFDMA). In this work, the performance of FEEL is characterized targeting a single-cell random network. First, due to power alignment between devices as required for aggregation, a fundamental tradeoff is shown to exist between the update-reliability and the expected update-truncation ratio. This motivates the design of an opportunistic scheduling scheme for FEEL that selects devices within a distance threshold. This scheme is shown using real datasets to yield satisfactory learning performance in the presence of high mobility. Second, both the multi-access latency of the proposed analog aggregation and the OFDMA scheme are analyzed. Their ratio, which quantifies the latency reduction of the former, is proved to scale almost linearly with device population.",project-academic
10.1109/INFOCOM.2018.8486321,2018-04-16,p,IEEE,real time video quality of experience monitoring for https and quic," The widespread deployment of end-to-end encryption protocols such as HTTPS and QUIC has reduced the visibility for operators into traffic on their networks. Network operators need the visibility to monitor and mitigate Quality of Experience (QoE) impairments in popular applications such as video streaming. To address this problem, we propose a machine learning based approach to monitor QoE metrics for encrypted video traffic. We leverage network and transport layer information as features to train machine learning classifiers for inferring video QoE metrics such as startup delay and rebuffering events. Using our proposed approach, network operators can detect and react to encrypted video QoE impairments in real-time. We evaluate our approach for YouTube adaptive video streams using HTTPS and QUIC. The experimental evaluations show that our approach achieves up to 90% classification accuracy for HTTPS and up to 85 % classification accuracy for QUIC.",project-academic
10.1109/ACCESS.2018.2834916,2018-05-10,a,Institute of Electrical and Electronics Engineers (IEEE),object tracking in vary lighting conditions for fog based intelligent surveillance of public spaces," With rapid development of computer vision and artificial intelligence, cities are becoming more and more intelligent. Recently, since intelligent surveillance was applied in all kind of smart city services, object tracking attracted more attention. However, two serious problems blocked the development of visual tracking in real applications. The first problem is its lower performance under intense illumination variation, while the second issue is its slow speed. This paper addressed these two problems by proposing a correlation filter-based tracker. Fog computing platform was deployed to accelerate the proposed tracking approach. The tracker was constructed by multiple positions’ detections and alternate templates. The detection position was repositioned according to the estimated speed of target by an optical flow method, and the alternate template was stored with a template update mechanism, which is all computed at the edge. Experimental results on large-scale public benchmark data sets showed the effectiveness of the proposed method in comparison with state-of-the-art methods.",project-academic
10.1609/AAAI.V34I08.7021,2020-04-03,p,Association for the Advancement of Artificial Intelligence (AAAI),fedvision an online visual object detection platform powered by federated learning," Visual object detection is a computer vision-based artificial intelligence (AI) technique which has many practical applications (e.g., fire hazard monitoring). However, due to privacy concerns and the high cost of transmitting video data, it is highly challenging to build object detection models on centrally stored large training datasets following the current approach. Federated learning (FL) is a promising approach to resolve this challenge. Nevertheless, there currently lacks an easy to use tool to enable computer vision application developers who are not experts in federated learning to conveniently leverage this technology and apply it in their systems. In this paper, we report FedVision - a machine learning engineering platform to support the development of federated learning powered computer vision applications. The platform has been deployed through a collaboration between WeBank and Extreme Vision to help customers develop computer vision-based safety monitoring solutions in smart city applications. Over four months of usage, it has achieved significant efficiency improvement and cost reduction while removing the need to transmit sensitive data for three major corporate customers. To the best of our knowledge, this is the first real application of FL in computer vision-based tasks.",project-academic
,2020-01-17,a,,fedvision an online visual object detection platform powered by federated learning," Visual object detection is a computer vision-based artificial intelligence (AI) technique which has many practical applications (e.g., fire hazard monitoring). However, due to privacy concerns and the high cost of transmitting video data, it is highly challenging to build object detection models on centrally stored large training datasets following the current approach. Federated learning (FL) is a promising approach to resolve this challenge. Nevertheless, there currently lacks an easy to use tool to enable computer vision application developers who are not experts in federated learning to conveniently leverage this technology and apply it in their systems. In this paper, we report FedVision - a machine learning engineering platform to support the development of federated learning powered computer vision applications. The platform has been deployed through a collaboration between WeBank and Extreme Vision to help customers develop computer vision-based safety monitoring solutions in smart city applications. Over four months of usage, it has achieved significant efficiency improvement and cost reduction while removing the need to transmit sensitive data for three major corporate customers. To the best of our knowledge, this is the first real application of FL in computer vision-based tasks.",project-academic
10.1109/IJCNN.2017.7966128,2017-03-03,a,,scalable deep traffic flow neural networks for urban traffic congestion prediction," Tracking congestion throughout the network road is a critical component of Intelligent transportation network management systems. Understanding how the traffic flows and short-term prediction of congestion occurrence due to rush-hour or incidents can be beneficial to such systems to effectively manage and direct the traffic to the most appropriate detours. Many of the current traffic flow prediction systems are designed by utilizing a central processing component where the prediction is carried out through aggregation of the information gathered from all measuring stations. However, centralized systems are not scalable and fail provide real-time feedback to the system whereas in a decentralized scheme, each node is responsible to predict its own short-term congestion based on the local current measurements in neighboring nodes. 
We propose a decentralized deep learning-based method where each node accurately predicts its own congestion state in real-time based on the congestion state of the neighboring stations. Moreover, historical data from the deployment site is not required, which makes the proposed method more suitable for newly installed stations. In order to achieve higher performance, we introduce a regularized Euclidean loss function that favors high congestion samples over low congestion samples to avoid the impact of the unbalanced training dataset. A novel dataset for this purpose is designed based on the traffic data obtained from traffic control stations in northern California. Extensive experiments conducted on the designed benchmark reflect a successful congestion prediction.",project-academic
,2018-12-30,a,,broadband analog aggregation for low latency federated edge learning extended version," The popularity of mobile devices results in the availability of enormous data and computational resources at the network edge. To leverage the data and resources, a new machine learning paradigm, called edge learning, has emerged where learning algorithms are deployed at the edge for providing fast and intelligent services to mobile users. While computing speeds are advancing rapidly, the communication latency is becoming the bottleneck of fast edge learning. To address this issue, this work is focused on designing a low latency multi-access scheme for edge learning. We consider a popular framework, federated edge learning (FEEL), where edge-server and on-device learning are synchronized to train a model without violating user-data privacy. It is proposed that model updates simultaneously transmitted by devices over broadband channels should be analog aggregated ""over-the-air"" by exploiting the superposition property of a multi-access channel. Thereby, ""interference"" is harnessed to provide fast implementation of the model aggregation. This results in dramatical latency reduction compared with the traditional orthogonal access (i.e., OFDMA). In this work, the performance of FEEL is characterized targeting a single-cell random network. First, due to power alignment between devices as required for aggregation, a fundamental tradeoff is shown to exist between the update-reliability and the expected update-truncation ratio. This motivates the design of an opportunistic scheduling scheme for FEEL that selects devices within a distance threshold. This scheme is shown using real datasets to yield satisfactory learning performance in the presence of high mobility. Second, both the multi-access latency of the proposed analog aggregation and the OFDMA scheme are analyzed. Their ratio, which quantifies the latency reduction of the former, is proved to scale almost linearly with device population.",project-academic
10.1109/ICCC.2018.00010,2018-07-02,p,IEEE,an edge based smart parking solution using camera networks and deep learning," The smart parking industry continues to evolve as an increasing number of cities struggle with traffic congestion and inadequate parking availability. For urban dwellers, few things are more irritating than anxiously searching for a parking space. Research results show that as much as 30% of traffic is caused by drivers driving around looking for parking spaces in congested city areas. There has been considerable activity among researchers to develop smart technologies that can help drivers find a parking spot with greater ease, not only reducing traffic congestion but also the subsequent air pollution. Many existing solutions deploy sensors in every parking spot to address the automatic parking spot detection problems. However, the device and deployment costs are very high, especially for some large and old parking structures. A wide variety of other technological innovations are beginning to enable more adaptable systems—including license plate number detection, smart parking meter, and vision-based parking spot detection. In this paper, we propose to design a more adaptable and affordable smart parking system via distributed cameras, edge computing, data analytics, and advanced deep learning algorithms. Specifically, we deploy cameras with zoom-lens and motorized head to capture license plate numbers by tracking the vehicles when they enter or leave the parking lot; cameras with wide angle fish-eye lens will monitor the large parking lot via our custom designed deep neural network. We further optimize the algorithm and enable the real-time deep learning inference in an edge device. Through the intelligent algorithm, we can significantly reduce the cost of existing systems, while achieving a more adaptable solution. For example, our system can automatically detect when a car enters the parking space, the location of the parking spot, and precisely charge the parking fee and associate this with the license plate number.",project-academic
10.1109/TSMC.2016.2626797,2018-06-01,a,IEEE,methods and tools to construct a global indoor positioning system," A global indoor positioning system (GIPS) is a system that provides positioning services in most buildings in villages and cities globally. Among the various indoor positioning techniques, WLAN-based location fingerprinting has attracted considerable attention because of the wide availability of WLAN and relatively high resolution of the fingerprint-based positioning techniques. This paper introduces methods and tools to construct a GIPS by using WLAN fingerprinting. An unsupervised learning-based method is adopted to construct radio maps using fingerprints collected via crowdsourcing, and a probabilistic indoor positioning algorithm is developed for the radio maps constructed with the crowdsourced fingerprints. Along with these techniques, collecting indoor and radio maps of buildings in villages and cities is essential for a GIPS. This paper aims to collect indoor and radio maps from volunteers who are interested in deploying indoor positioning systems for their buildings. The methods and tools for the volunteers are also described in the process of developing an indoor positioning system within the larger GIPS. An experimental GIPS, named KAIST indoor locating system (KAILOS), was developed integrating the methods and tools. Then indoor navigation systems for a university campus and a large-scale indoor shopping mall were developed on KAILOS, revealing the effectiveness of KAILOS in developing indoor positioning systems. The more volunteers who participate in developing indoor positioning systems on KAILOS-like systems, the sooner GIPS will be realized.",project-academic
10.1016/J.MICPRO.2020.103301,2021-02-01,a,Elsevier,iot enabled cancer prediction system to enhance the authentication and security using cloud computing," Abstract None None In recent days, Internet of Things, Cloud Computing, Deep learning, Machine learning and Artificial Intelligence are considered to be an emerging technologies to solve variety of real world problems. These techniques are importantly applied in various fields such as healthcare systems, transportation systems, agriculture and smart cities to produce fruitful results for number of issues in today's environment. This research work focuses on one such application in the field of IoT together with cloud computing. More number of sensors that are deployed in human body is used to collect patient related data such as deviation in body temperature and others which leads to variation in blood cells that turned to be cancerous cells. Main intention of this work is design a cancer prediction system using Internet of Things upon extracting the details of blood results to test whether it is normal or abnormal. In addition to this, encryption is done on the blood results of cancer affected patient and store it in cloud for quick reference through Internet for the doctor or healthcare nurse to handle the patient data secretly. This research work concentrates on enhancing the health care computations and processing. It provides a framework to enhance the performance of the existing health care industry across the globe. As the entire medical data has to be saved in cloud, the traditional medical treatment limitations can be overcome. Encryption and decryption is done using AES algorithm in order to provide authentication and security in handling cancer patients. The main focus is to handle healthcare data effectively for the patient when they are away from the home town since the needed cancer treatment details are stored in cloud. The task completion time is greatly reduce from 400 to 160  by using VMs. CloudSim gives an adaptable simulation structure that empowers displaying and reproduced results.",project-academic
10.1109/ACCESS.2019.2919736,2019-06-03,a,IEEE,federated learning based computation offloading optimization in edge computing supported internet of things," Recently, smart cities, smart homes, and smart medical systems have challenged the functionality and connectivity of the large-scale Internet of Things (IoT) devices. Thus, with the idea of offloading intensive computing tasks from them to edge nodes (ENs), edge computing emerged to supplement these limited devices. Benefit from this advantage, IoT devices can save more energy and still maintain the quality of the services they should provide. However, computational offload decisions involve federation and complex resource management and should be determined in the real-time face to dynamic workloads and radio environments. Therefore, in this work, we use multiple deep reinforcement learning (DRL) agents deployed on multiple edge nodes to indicate the decisions of the IoT devices. On the other hand, with the aim of making DRL-based decisions feasible and further reducing the transmission costs between the IoT devices and edge nodes, federated learning (FL) is used to train DRL agents in a distributed fashion. The experimental results demonstrate the effectiveness of the decision scheme and federated learning in the dynamic IoT system.",project-academic
10.1109/TMM.2018.2865661,2018-08-17,a,IEEE,energy aware mobile edge computing and routing for low latency visual data processing," New paradigms such as Mobile Edge Computing (MEC) are becoming feasible for use in, e.g., real-time decision-making during disaster incident response to handle the data deluge occurring in the network edge. However, MEC deployments today lack flexible IoT device data handling such as handling user preferences for real-time versus energy-efficient processing. Moreover, MEC can also benefit from a policy-based edge routing to handle sustained performance levels with efficient energy consumption. In this paper, we study the potential of MEC to address application issues related to energy management on constrained IoT devices with limited power sources, while also providing low-latency processing of visual data being generated at high resolutions. Using a facial recognition application that is important in disaster incident response scenarios, we propose a novel “offload decision-making” algorithm that analyzes the tradeoffs in computing policies to offload visual data processing (i.e., to an edge cloud or a core cloud) at low-to-high workloads. This algorithm also analyzes the impact on energy consumption in the decision-making under different visual data consumption requirements (i.e., users with thick clients or thin clients). To address the processing-throughput versus energy-efficiency tradeoffs, we propose a “Sustainable Policy-based Intelligence-Driven Edge Routing” algorithm that uses machine learning within Mobile Ad hoc Networks. This algorithm is energy aware and improves the geographic routing baseline performance (i.e., minimizes impact of local minima) for throughput performance sustainability, while also enabling flexible policy specification. We evaluate our proposed algorithms by conducting experiments on a realistic edge and core cloud testbed in the GENI Cloud infrastructure, and recreate disaster scenes of tornado damages within simulations. Our empirical results show how MEC can provide flexibility to users who desire energy conservation over low latency or vice versa in the visual data processing with a facial recognition application. In addition, our simulation results show that our routing approach outperforms existing solutions under diverse user preferences, node mobility, and severe node failure conditions.",project-academic
,2007-01-01,a,,adaptive routing in ad hoc wireless multi hop networks," Ad hoc wireless multi-hop networks (AHWMNs) are communication networks that consist entirely of wireless nodes, placed together in an ad hoc manner, i.e. with minimal prior planning. All nodes have routing capabilities, and forward data packets for other nodes in multi-hop fashion. Nodes can enter or leave the network at any time, and may be mobile, so that the network topology continuously experiences alterations during deployment. AHWMNs pose substantially different challenges to networking protocols than more traditional wired networks. These challenges arise from the dynamic and unplanned nature of these networks, from the inherent unreliability of wireless communication, from the limited resources available in terms of bandwidth, processing capacity, etc., and from the possibly large scale of these networks. Due to these different challenges, new algorithms are needed at all layers of the network protocol stack. We investigate the issue of adaptive routing in AHWMNs, using ideas from artificial intelligence (AI). Our main source of inspiration is the field of Ant Colony Optimization (ACO). This is a branch of AI that takes its inspiration from the behavior of ants in nature. ACO has been applied to a wide range of different problems, often giving state-of-the-art results. The application of ACO to the problem of routing in AHWMNs is interesting because ACO algorithms tend to provide properties such as adaptivity and robustness, which are needed to deal with the challenges present in AHWMNs. On the other hand, the field of AHWMNs forms an interesting new application domain in which the ideas of ACO can be tested and improved. In particular, we investigate the combination of ACO mechanisms with other techniques from AI to get a powerful algorithm for the problem at hand. We present the AntHocNet routing algorithm, which combines ideas from ACO routing with techniques from dynamic programming and other mechanisms taken from more traditional routing algorithms. The algorithm has a hybrid architecture, combining both reactive and proactive mechanisms. Through a series of simulation tests, we show that for a wide range of different environments and performance metrics, AntHocNet can outperform important reference algorithms in the research area. We provide an extensive investigation of the internal working of the algorithm, and we also carry out a detailed simulation study in a realistic urban environment. Finally, we discuss the implementation of ACO routing algorithms in a real world testbed.",project-academic
,2002-05-01,b,Prentice Hall PTR,multimedia communication systems techniques standards and networks," From the Book:
Preface
The past years have seen an explosion in the use of digital media. Industry is making significant investments to deliver digital audio, image and video information to consumers and customers. A new infrastructure of digital audio, image and video recorders and players; online services and electronic commerce is rapidly being deployed. At the same time, major corporations are converting their audio, image and video archives to an electronic form. Digital media offer several distinct advantages over analog media. The quality of digital audio, image and video signals is higher than that of their analog counterparts. Editing is easy because one can access the exact discrete locations that need to be changed. Copying is simple with no loss of fidelity. A copy of digital media is identical to the original. Digital audio, image and video are easily transmitted across networked information systems. These advantages have opened up many new possibilities.

Multimedia consists of Multimedia data + Set of interactions. Multimedia data is informally considered as the collection of three Ms: multisource, multitype and multiformat data. The interactions among the multimedia components consist of complex relationships without which multimedia would be a simple set of visual, audio and other data.

Multimedia and multimedia communication can be globally viewed as a hierarchical system. The multimedia software and applications provide a direct interactive environment for users. When a computer requires information from remote computers or servers, multimedia information must travel through computer networks. Because the amount of information involved inthe transmission of video and audio can be substantial, the multimedia information must be compressed before it can be sent through the network in order to reduce the communication delay. Constraints, such as limited delay and jitter, are used to ensure a reasonable video and audio effect at the receiving end. Therefore, communication networks are undergoing constant improvements in order to provide for multimedia communication capabilities. LANs are used to connect local computers and other equipment, and Wide Area Networks (WANs) and the Internet connect the LANs together. Better standards are constantly being developed, in order to provide a global information superhighway across which multimedia information will travel.
Organization of the Book
The book is organized into six chapters:

Chapter 1 describes the concept of multimedia communication modeling. It presents a brief description of elements for multimedia systems. After that, we discuss user and network requirements together with the packet transfer concept. An overview of multimedia terminals is also given.

Chapter 2 explains that multimedia communication is more than simply putting together text, audio, images and video. It reviews a recent trend in multimedia research to exploit the audio-visual interaction and to build the link between audio and video processing. The emphasis is on lip reading, synchronization and tracing audio-to-visual mapping as well as the bimodal person verification.

Chapter 3 is devoted to multimedia processing in communication. We present and analyze digital media and signal processing elements. Next, we describe a general framework for image copyright protection through digital watermarking. We then review the key attributes of neural processing essential to intelligent multimedia processing. Finally, this chapter concludes with recent large-scale-integration programmable processors designed for multimedia processing such as real-time compression and decompression of audio and video as well as the next generation of computer graphics.

Chapter 4 deals with the issues concerning distributed multimedia systems. We give an overview: main features, resource management, networking and multimedia operating systems. Next, we identify the applications like interactive television, telecooperation and hypermedia, and we survey the important enabling technologies.

Chapter 5 focuses on multimedia communication standards. We discuss Moving Pictures Experts Group (MPEG)-1, MPEG-2, MPEG-4, MPEG-4 Visual Texture Coding (VTC), Joint Photographic Experts Group (JPEG)-2000, MPEG-7, MPEG-21, International Telecommunications Union-Telecommunication Sector (ITU-T) and Internet standards. We discuss the ITU-T standardization process in multimedia communications from the video and speech coding, as well as from multimedia, multiplex and synchronization points of view (H.320, H.321, H.322, H.323, H.262, H.263, H.26L, H.221, H.222, H.223 and H.225).

Chapter 6 concentrates on multimedia communication across networks. After an introduction about packet audio-video in the network environment, we discuss the concept of video transport across generic networks. Multimedia transport over ATM networks is described, too. We then move to multimedia across IP networks, including video transmission, traffic specification for MPEG video transmission on the Internet and bandwidth allocation mechanism. We present and illustrate the concepts of Internet access networks. In addition, we discuss special issues relating to multimedia across wireless networks such as wireless broadband communication for multimedia audiovisual solutions, mobile and broadcasting networks and digital TV infrastructure for interactive multimedia services.
Appendix/Web Site
Appendix A contains useful information available on the Internet: standardization organizations, associations, alliances, fora and consortia; documents, software and hardware reference, and a products and services list. No software is provided. The appendix can be downloaded at the following Web site: www.phptr.com/rao
References
The references are grouped according to the various chapters. Special efforts have been taken to make this list as up to date and exhaustive as possible.

A number of forces are driving communications, such as the following: 

The evolution of communications and data networks in today's modern Plain Old Telephone Service (POTS) network and packet (including the Internet) networks, with major forces driving these networks into an integrated structure
The increasing availability of almost unlimited bandwidth demand in the office, the home and eventually on the road, based on high-speed data modems, cable modems, hybrid fiber-mix systems, and, recently, a number of fixed wireless access systems
The availability of ubiquitous access to the network through Local Area Networks (LANs), wireline and wireless networks providing the promise of anywhere, anytime access
The ever-increasing amount of memory and computation brought to bear on virtually any communications or computing system
The terminals, including sophisticated screen phones; digital telephones; multimedia personal computers (PCs) that handle a wide range of text, image, audio and video signals; network computers and other low-cost Internet-access terminals and Personal Digital Assistants (PDAs) of all types that can access and interact with the network using wired and wireless connections
The digitalization of virtually all devices, including cameras, video capture devices, video playback devices, handwriting terminals, sound capture devices and so forth


Multimedia Communication Systems provides a comprehensive coverage of various surveys of the current issues relating to multimedia communications. This book addresses the fundamentals of the major topics of the multimedia communication systems: audio-visual integration, multimedia processing in communications, distributed multimedia systems, multimedia communication standards and multimedia communications across networks.

We have focused our attention on these topics with the hope that the level of discussion provided will enable an engineer or a scientist to design multimedia communication systems or to conduct research on advanced and newly emerging topics. The objective of this book is not only to familiarize the reader with multimedia communication systems, but also to provide the underlying theory, concepts and principles related to these disciplines, including the power and the practical utility of the topics.

A major challenge during the preparation of this book was the rapid pace of development, both in software and hardware related to multimedia communication systems. We have tried to keep pace by including many of the latest developments. In this way, it is hoped that the book is timely and appeals to a wide audience in the engineering, scientific and technical communities. In addition, we have included more than 270 figures and more than 800 references. Although this book is primarily for graduate students, it can be also very useful for academia, researchers, scientists and engineers dealing with multimedia communication systems.",project-academic
,2019-02-14,a,,a scalable platform for distributed object tracking across a many camera network," Advances in deep neural networks (DNN) and computer vision (CV) algorithms have made it feasible to extract meaningful insights from large-scale deployments of urban cameras. Tracking an object of interest across the camera network in near real-time is a canonical problem. However, current tracking platforms have two key limitations: 1) They are monolithic, proprietary and lack the ability to rapidly incorporate sophisticated tracking models; and 2) They are less responsive to dynamism across wide-area computing resources that include edge, fog and cloud abstractions. We address these gaps using Anveshak, a runtime platform for composing and coordinating distributed tracking applications. It provides a domain-specific dataflow programming model to intuitively compose a tracking application, supporting contemporary CV advances like query fusion and re-identification, and enabling dynamic scoping of the camera network's search space to avoid wasted computation. We also offer tunable batching and data-dropping strategies for dataflow blocks deployed on distributed resources to respond to network and compute variability. These balance the tracking accuracy, its real-time performance and the active camera-set size. We illustrate the concise expressiveness of the programming model for $4$ tracking applications. Our detailed experiments for a network of 1000 camera-feeds on modest resources exhibit the tunable scalability, performance and quality trade-offs enabled by our dynamic tracking, batching and dropping strategies.",project-academic
10.1109/TMC.2020.2999852,2020-06-03,a,Institute of Electrical and Electronics Engineers (IEEE),machine learning at the edge a data driven architecture with applications to 5g cellular networks," The fifth generation of cellular networks (5G) will rely on edge cloud deployments to satisfy the ultra-low latency demand of future applications. In this paper, we argue that such deployments can also be used to enable advanced data-driven and Machine Learning (ML) applications in mobile networks. We propose an edge-controller-based architecture for cellular networks and evaluate its performance with real data from hundreds of base stations of a major U.S. operator. In this regard, we will provide insights on how to dynamically cluster and associate base stations and controllers, according to the global mobility patterns of the users. Then, we will describe how the controllers can be used to run ML algorithms to predict the number of users in each base station, and a use case in which these predictions are exploited by a higher-layer application to route vehicular traffic according to network Key Performance Indicators (KPIs). We show that the prediction accuracy improves when based on machine learning algorithms that rely on the controllers' view and, consequently, on the spatial correlation introduced by the user mobility, with respect to when the prediction is based only on the local data of each single base station.",project-academic
10.1109/ISC2.2016.7580869,2016-09-01,p,IEEE,urbansense an urban scale sensing platform for the internet of things," A critical step towards smarter and safer cities is to endow them with the abilities to massively gather a wide variety of data sets and to automatically feed those data to decision support tools and applications that leverage artificial intelligence. We present UrbanSense, a platform deployed on the streets of a mid-size European city (Porto, Portugal) to collect key environmental data. The main innovations of UrbanSense are (1) design for affordability and extensibility, (2) its ability to leverage heterogeneous networks to send the data to the cloud (using both real-time and delay-tolerant communications), and (3) its Internet of Things integration to expose the data streams to smart city tools and applications. Beyond discussing the design choices, we present operational results for 6 months of operation and give a detailed account of the challenges faced by the successful deployment of urban sensing technologies in the wild.",project-academic
10.1007/978-3-540-69170-9_33,2008-06-11,p,"Springer, Berlin, Heidelberg",efficient node discovery in mobile wireless sensor networks," Energy is one of the most crucial aspects in real deployments of mobile sensor networks. As a result of scarce resources, the duration of most real deployments can be limited to just several days, or demands considerable maintenance efforts (e.g., in terms of battery substitution). A large portion of the energy of sensor applications is spent in node discovery as nodes need to periodically advertise their presence and be awake to discover other nodes for data exchange. The optimization of energy consumption, which is generally a hard task in fixed sensor networks, is even harder in mobile sensor networks, where the neighbouring nodes change over time.

In this paper we propose an algorithm for energy efficient node discovery in sparsely connected mobile wireless sensor networks. The work takes advantage of the fact that nodes have temporal patterns of encounters and exploits these patterns to drive the duty cycling. Duty cycling is seen as a sampling process and is formulated as an optimization problem. We have used reinforcement learning techniques to detect and dynamically change the times at which a node should be awake as it is likely to encounter other nodes. We have evaluated our work using real human mobility traces, and the paper presents the performance of the protocol in this context.",project-academic
10.1101/2020.10.21.20210948,2020-10-25,a,Cold Spring Harbor Laboratory Press,predicting dengue incidence leveraging internet based data sources a case study in 20 cities in brazil," Abstract None The dengue virus affects millions of people every year worldwide, causing large epidemic outbreaks that disrupt people’s lives and severely strain healthcare systems. In the absence of a reliable vaccine against it or an effective treatment to manage the illness in humans, most efforts to combat dengue infections have focused on preventing its vectors, mainly the Aedes aegypti mosquito, from flourishing across the world. These mosquito-control strategies need reliable disease activity surveillance systems to be deployed. Despite significant efforts to estimate dengue incidence using a variety of data sources and methods, little work has been done to understand the relative contribution of the different data sources to improved prediction. Additionally, scholarship on the topic had initially focused on prediction systems at the national- and state-levels, and much remains to be done at the finer spatial resolutions at which health policy interventions often occur. We develop a methodological framework to assess and compare dengue incidence estimates at the city level, and evaluate the performance of a collection of models on 20 different cities in Brazil. The data sources we use towards this end are weekly incidence counts from prior years (seasonal autoregressive terms), weekly-aggregated weather variables, and real-time internet search data. We find that both random forest-based models and LASSO regression-based models effectively leverage these multiple data sources to produce accurate predictions, and that while the performance between them is comparable on average, the former method produces fewer extreme outliers, and can thus be considered more robust. For real-time predictions that assume long delays (6-8 weeks) in the availability of epidemiological data, we find that real-time internet search data are the strongest predictors of dengue incidence, whereas for predictions that assume short delays (1-3 weeks), in which the error rate is halved (as measured by relative RMSE), short-term and seasonal autocorrelation are the dominant predictors. Despite the difficulties inherent to city-level prediction, our framework achieves meaningful and actionable estimates across cities with different demographic, geographic and epidemic characteristics. None Author Summary None As the incidence of infectious diseases like dengue continues to increase throughout the world, tracking their spread in real time poses a significant challenge to local and national health authorities. Accurate incidence data are often difficult to obtain as outbreaks emerge and unfold, both due the partial reach of serological surveillance (especially in rural areas), and due to delays in reporting, which result in post-hoc adjustments to what should have been real-time data. Thus, a range of ‘nowcasting’ tools have been developed to estimate disease trends, using different mathematical and statistical methodologies to fill the temporal data gap. Over the past several years, researchers have investigated how to best incorporate internet search data into predictive models, since these can be obtained in real-time. Still, most such models have been regression-based, and have tended to underperform in cases when epidemiological data are only available after long reporting delays. Moreover, in tropical countries, attention has increasingly turned from testing and applying models at the national level to models at higher spatial resolutions, such as states and cities. Here, we develop machine learning models based on both LASSO regression and on random forest ensembles, and proceed to apply and compare them across 20 cities in Brazil. We find that our methodology produces meaningful and actionable disease estimates at the city level with both underlying model classes, and that the two perform comparably across most metrics, although the ensemble method produces fewer outliers. We also compare model performance and the relative contribution of different data sources across diverse geographic, demographic and epidemic conditions.",project-academic
10.5555/2484920.2485092,2013-05-06,p,International Foundation for Autonomous Agents and Multiagent Systems,a learning agent for heat pump thermostat control," Heating, Ventilation and Air Conditioning (HVAC) systems are one of the biggest energy consumers around the world. With the efforts of moving to sustainable energy consumption, heat-pump based HVAC systems have gained popularity due to their high efficiency and due to the fact that they are powered by electricity rather than by gas or oil. One drawback of heat-pump systems is that their efficiency sharply decreases when the outdoor temperature is around or below freezing. Therefore, they are backed up by an auxiliary heating system that is effective in cold whether, but that consumes twice as much energy. A popular way of saving energy in HVAC systems is setting back the thermostat, meaning, relaxing the heating/cooling requirements when occupants are not at home. While this practice leads to significant energy savings in many systems, it could in fact increase the energy consumption in a heat-pump based system, using existing control strategies, as it forces an excessive usage of the auxiliary heater. In this paper, we design and implement a complete, adaptive reinforcement learning agent which applies a new control strategy for a heat-pump thermostat. For our experiments, we use a complex, realistic simulator that was developed for the US Department of Energy. Results show that the learned control strategy (1) leads to roughly {7.0%-14.5%} energy savings in typical homes in the New York City, Boston, and Chicago areas; while (2) keeping the occupants' comfort level unchanged when compared to an existing strategy that is deployed in practice.",project-academic
10.1109/TENCON.2016.7848593,2016-11-01,p,,traffic flow prediction with long short term memory networks lstms," Accurate traffic flow information is crucial for an intelligent transportation system management and deployment. Over the past few years, many existing models have been designed for short-term traffic flow prediction. However, they fail to provide favorable results due to their shallow architectures or incapability to extract the sequence correlations in the data. In this paper, we explore the application of Long Short-Term Memory Networks (LSTMs) in short-term traffic flow prediction. As a deep learning approach, LSTMs are able to learn more abstract representations in the non-linear traffic flow data. The intrinsic feature of capturing long-term dependencies in a sequential data also makes it a suitable choice in traffic prediction. Experiments on real traffic data sets indicate a good performance of our model. The LSTMs architecture is also compared with state-of-the-art models and experiments show that our model achieves desirable results by lowering the MAPE metrics to 5.4%.",project-academic
,2011-08-07,p,AAAI Press,green driver ai in a microcosm," The Green Driver app is a dynamic routing application for GPS-enabled smartphones. Green Driver combines client GPS data with real-time traffic light information provided by cities to determine optimal routes in response to driver route requests. Routes are optimized with respect to travel time, with the intention of saving the driver both time and fuel, and rerouting can occur if warranted. During a routing session, client phones communicate with a centralized server that both collects GPS data and processes route requests. All relevant data are anonymized and saved to databases for analysis; statistics are calculated from the aggregate data and fed back to the routing engine to improve future routing. Analyses can also be performed to discern driver trends: where do drivers tend to go, how long do they stay, when and where does traffic congestion occur, and so on. The system uses a number of techniques from the field of artificial intelligence. We apply a variant of A* search for solving the stochastic shortest path problem in order to find optimal driving routes through a network of roads given light-status information. We also use dynamic programming and hidden Markov models to determine the progress of a driver through a network of roads from GPS data and light-status data. The Green Driver system is currently deployed for testing in Eugene, Oregon, and is scheduled for large-scale deployment in Portland, Oregon, in Spring 2011.",project-academic
,2008-11-01,b,,distributed intelligent systems a coordination perspective," Distributed Intelligent Systems: A Coordination Perspective addresses and comprehensively answers commonly asked questions about coordination in agent-oriented distributed systems. Characterizing the state-of-the-art research in the field of coordination with regard to the development of distributed agent-oriented systems is a particularly complex endeavour, as the space of available approaches is indeed considerable, and research is independently conducted in a great number of domains. While existing books deal with specific aspects of coordination, the major contribution of this book lies in the attempt to provide an in-depth review covering a wide range of issues regarding multi-agent coordination in Distributed Artificial Intelligence. In addition to reporting various sources of confusion, this book outlines the existence of a plethora of strategies and techniques adapted to different problems, agent-oriented systems, environments, and domains. In short, the current book identifies the absence of a single unified approach in addressing multi-agent coordination problems arising in any system or organization. This book, written by world-class leaders in this field, dedicates itself to providing a state-of-the-art review of current coordination strategies and techniques. The book also describes a broad range of application domains which implement many of the coordination strategies and techniques from the field of multi-agent systems. The application domains include defense, transportation, health care, telecommunication and e-business. Based on current practical deployed applications and existing capabilities, this book also identifies and thoroughly examines trends, challenges, and future agent-oriented research directions. Key features: Unveils the lack of coherence and order that characterizes the area of research pertaining to coordination of distributed intelligent systems Examines coordination models, frameworks, strategies and techniques to enable the development of distributed intelligent agent-oriented systems. Provides specific recommendations to realize more widespread deployment of agent-based systems Distributed Intelligent Systems: A Coordination Perspective is designed for a professional audience composed of practitioners and researchers in industry. This book is also suitable as a reference or secondary textbook for advanced-level students in computer science and engineering.",project-academic
,2019-05-08,p,,faster fusion analytics for public transport event response," Increasing urban concentration raises operational challenges that can benefit from integrated monitoring and decision support. Such complex systems need to leverage the full stack of analytical methods, from state estimation using multi-sensor fusion for situational awareness, to prediction and computation of optimal responses. The FASTER platform that we describe in this work, deployed at nation scale and handling 1.5 billion public transport trips a year, offers such a full stack of techniques for this large-scale, real-time problem. FASTER provides fine-grained situational awareness and real-time decision support with the objective of improving the public transport commuter experience. The methods employed range from statistical machine learning to agent-based simulation and mixed-integer optimization. In this work we present an overview of the challenges and methods involved, with details of the commuter movement prediction module, as well as a discussion of open problems.",project-academic
10.1109/TITS.2020.3025687,2021-03-01,a,IEEE,an edge traffic flow detection scheme based on deep learning in an intelligent transportation system," An intelligent transportation system (ITS) plays an important role in public transport management, security and other issues. Traffic flow detection is an important part of the ITS. Based on the real-time acquisition of urban road traffic flow information, an ITS provides intelligent guidance for relieving traffic jams and reducing environmental pollution. The traffic flow detection in an ITS usually adopts the cloud computing mode. The edge of the network will transmit all the captured video to the cloud computing center. However, the increasing traffic monitoring has brought great challenges to the storage, communication and processing of traditional transportation systems based on cloud computing. To address this issue, a traffic flow detection scheme based on deep learning on the edge node is proposed in this article. First, we propose a vehicle detection algorithm based on the YOLOv3 (You Only Look Once) model trained with a great volume of traffic data. We pruned the model to ensure its efficiency on the edge equipment. After that, the DeepSORT (Deep Simple Online and Realtime Tracking) algorithm is optimized by retraining the feature extractor for multiobject vehicle tracking. Then, we propose a real-time vehicle tracking counter for vehicles that combines the vehicle detection and vehicle tracking algorithms to realize the detection of traffic flow. Finally, the vehicle detection network and multiple-object tracking network are migrated and deployed on the edge device Jetson TX2 platform, and we verify the correctness and efficiency of our framework. The test results indicate that our model can efficiently detect the traffic flow with an average processing speed of 37.9 FPS (frames per second) and an average accuracy of 92.0% on the edge device.",project-academic
10.1109/IDAACS-SWS.2018.8525802,2018-09-01,p,IEEE,deep learning based massive mimo beamforming for 5g mobile network," The rapid increasing of the data volume in mobile networks forces operators to look into different options for capacity improvement. Thus, modern 5G networks became more complex in terms of deployment and management. Therefore, new approaches are needed to simplify network design and management by enabling self-organizing capabilities. In this paper, we propose a novel intelligent algorithm for performance optimization of the massive MIMO beamforming. The key novelty of the proposed algorithm is in the combination of three neural networks which cooperatively implement the deep adversarial reinforcement learning workflow. In the proposed system, one neural network is trained to generate realistic user mobility patterns, which are then used by second neural network to produce relevant antenna diagram. Meanwhile, third neural network estimates the efficiency of the generated antenna diagram returns corresponding reward to both networks. The advantage of the proposed approach is that it leans by itself and does not require large training datasets.",project-academic
10.3390/S16091501,2016-09-14,a,Multidisciplinary Digital Publishing Institute,semantic framework of internet of things for smart cities case studies," In recent years, the advancement of sensor technology has led to the generation of heterogeneous Internet-of-Things (IoT) data by smart cities. Thus, the development and deployment of various aspects of IoT-based applications are necessary to mine the potential value of data to the benefit of people and their lives. However, the variety, volume, heterogeneity, and real-time nature of data obtained from smart cities pose considerable challenges. In this paper, we propose a semantic framework that integrates the IoT with machine learning for smart cities. The proposed framework retrieves and models urban data for certain kinds of IoT applications based on semantic and machine-learning technologies. Moreover, we propose two case studies: pollution detection from vehicles and traffic pattern detection. The experimental results show that our system is scalable and capable of accommodating a large number of urban regions with different types of IoT applications.",project-academic
10.1109/MCOM.2018.1700304,2018-02-13,a,IEEE,soft sensing in smart cities handling 3vs using recommender systems machine intelligence and data analytics," Today's existing smart city research involves many overtly futuristic applications such as smart transportation, in which smart roads warn drivers of bad traffic conditions ahead, smart parking, which communicates the location of unoccupied parking spaces to drivers, and smart environment, which enables fully automated homes and workplaces to adjust their temperature to conserve energy. The realization of these applications hinges on a data acquisition structure that gathers its data from a countless number of sensors, either deployed for predefined tasks (hard sensing) or built into the mobile devices of smart city residents (soft sensing). At the core of this big data infrastructure lie the 5Vs: veracity, volume, velocity, variety, and value. The soft sensing component of a smart city sensing network is particularly affected by 3Vs: veracity, volume, and velocity. To address the unique challenges of big data, recommender systems, statistical reputation systems, and context analysis are used to ensure the veracity of acquired data, machine learning algorithms are applied to handle the data volume, and data analytics algorithms are implemented to manage data velocity. Despite its seemingly insurmountable size, the acquired data is highly redundant, and systematic use of machine intelligence and data analytics can facilitate processing by extracting only the relevant information; in this article, we study the role of these algorithms through the lens of the 3Vs in facilitating soft sensing within the framework of smart city applications.",project-academic
10.1109/AECT47998.2020.9194188,2020-09-10,p,IEEE,clustering based uav base station positioning for enhanced network capacity," Unmanned aerial vehicles (UAVs) are expected to be deployed in a variety of applications in future mobile networks due to several advantages they bring over the deployment of ground base stations. However, despite the recent interest in UAVs in mobile networks, some issues still remain, such as determining the placement of multiple UAVs in different scenarios. In this paper we propose a solution to determine the optimal 3D position of multiple UAVs in a capacity enhancement use-case, or in other words, when the ground network cannot cope with the user traffic demand. For this scenario, real data from the city of Milan, provided by Telecom Italia is utilized to simulate an event. Based on that, a solution based on k-means, a machine learning technique, to position multiple UAVs is proposed and it is compared with two other baseline methods. Results demonstrate that the proposed solution is able to significantly outperform other methods in terms of users covered and quality of service.",project-academic
,2021-06-03,a,,cloud enabled high altitude platform systems challenges and opportunities," Augmenting ground-level communications with flying networks, such as the high-altitude platform system (HAPS), is among the major innovative initiatives of the next generation of wireless systems (6G). Given HAPS quasi-static positioning at the stratosphere, HAPS-to-ground and HAPS-to-air connectivity frameworks are expected to be prolific in terms of data acquisition and computing, especially given the mild weather and quasi-constant wind speed characteristics of the stratospheric layer. This paper explores the opportunities stemming from the realization of cloud-enabled HAPS in the context of telecommunications applications and services. The paper first advocates for the potential physical advantages of deploying HAPS as flying data-centers, also known as super-macro base stations. The paper then presents the merits that can be achieved by integrating various cloud services within the HAPS, and the corresponding cloud-type applications that would utilize the HAPS for enhancing the quality, range, and types of offered services. The paper further sheds light on the challenges that need to be addressed for realizing practical cloud-enabled HAPS, mainly, those related to the high energy, processing power, quality of service (QoS), and security considerations. Finally, the paper discusses some open issues on the topic, namely, HAPS mobility and message routing, HAPS security via blockchain and machine learning, artificial intelligence-based resource allocation in cloud-enabled HAPS, and integration with vertical heterogeneous networks.",project-academic
10.1016/J.ATMOSENV.2019.06.028,2019-09-15,a,Elsevier Ltd.,in search of an optimal in field calibration method of low cost gas sensors for ambient air pollutants comparison of linear multilinear and artificial neural network approaches," The current compliance networks of automatic air-quality monitoring stations in large urban environments are not sufficient to provide spatial and temporal measurement resolution for realistic assessment of personal exposure to pollutants. Small low-cost sensor platforms with greater mobility and expected lower maintenance costs, are increasingly being used as a supplement to compliance monitoring stations. However, low-cost sensor platforms usually provide data with uncertain precision. To improve the precision, these sensor platforms require in-field calibration. Our paper aims to demonstrate that data from each individual sensor system can be corrected using that sensor system's own data to achieve much improved data quality compared to a reference. However, in this procedure, there are practical difficulties such as individual sensor outputs from the multi-sensor system not being sufficiently available due to malfunctions for instance. We explore how this can be dealt with. In our opinion, this is a novel approach, of practical importance both to users and manufacturers. We present a detailed comparative analysis of Linear Regression (univariate), Multivariate Linear Regression and Artificial Neural Networks used with a specific aim of calibrating field-deployed low-cost CO and O3 sensors. For Artificial Neural Network models, the performance of three common training algorithms was compared (Levenberg-Marquardt, Resilient back-propagation and Conjugate Gradient Powell-Beale algorithm). Data for this study were obtained from two campaigns conducted with 25 multi-sensor AQMESH v.3.5 platforms used within the activities of the CITI-SENSE project. The platforms were co-located to reference gas monitors at the Automatic Monitoring Station Stari Grad, in Belgrade, Serbia. This paper demonstrates that Multivariate Linear Regression and Artificial Neural Network calibration models can improve the output signal. This improvement can be measured by changes in the median and interquartile ranges of statistical parameters used for model evaluation. Artificial Neural Networks showed the best results compared to Linear Regression and Multivariate Linear Regression models. The best predictors for CO, in addition to CO low-cost sensor data, were PM2.5 and NO2, while for O3, in addition to O3 low-cost sensor data, the most suitable input predictors were NO and aH. Based on residual error analysis, we have shown that for CO and O3, a certain range of concentrations exists in which calibrated values differ by less than 10% from the reference method results. In addition, it was noted that for all models, CO sensors consistently showed lower variability between platforms compared to O3 sensors.",project-academic
10.1016/J.ENVSCI.2021.06.011,2021-10-01,a,Elsevier,a big data and artificial intelligence framework for smart and personalized air pollution monitoring and health management in hong kong," Abstract None None All people in the world are entitled to enjoy a clean environment and a good quality of life. With big data and artificial intelligence technologies, it is possible to estimate personalized air pollution exposure and synchronize it with activity, health, quality of life and behavioural data, and provide real-time, personalized and interactive alert and advice to improve the health and well-being of individual citizens. In this paper, we propose an overarching framework outlining five major challenges to personalized air pollution monitoring and health management, and respective methodologies in an integrated interdisciplinary manner. First, urban air quality data is sparse, rendering it difficult to provide timely personalized alert and advice. Second, collected data, especially those involving human inputs such as health perception, are often missing and erroneous. Third, the data collected are heterogeneous, and highly complex, not easily comprehensible to facilitate individual and collective decision-making. Fourth, the causal relationships between personal air pollutants exposure (specifically, PM2.5 and PM1.0 and NO2) and personal health conditions, and health-related quality of life perception, of young asthmatics and young healthy citizens in Hong Kong (HK), are yet to be established. Fifth, whether personalized and smart information and advice provided can induce behavioural change and improve health and quality of life are yet to be determined. To overcome these challenges, our first novelty is to develop an AI and big data framework to estimate and forecast air quality in high temporal-spatial resolution and real-time. Our second novelty includes the deployment of mobile pollution sensor platforms to substantially improve the accuracy of estimated and forecasted air quality data, and the collection of activity, health condition and perception data. Our third novelty is the development of visualization tools and comprehensible indexes, by correlating personal exposure with four types of personal data, to provide timely, personalized pollution, health and travel alerts and advice. Our fourth novelty is determining causal relationship, if any, between personal pollutants, PM1.0 and PM2.5, NO2 exposure and personal health condition, and personal health perception, based on a clinical experiment of 150 young asthmatics and 150 young healthy citizens in HK. Our fifth novelty is an intervention study to determine if smart information, presented via our proposed visualized platform, will induce personal behavioural change. Our novel big data AI-driven approach, when integrated with other analytical approaches, provides an integrated interdisciplinary framework for personalized air pollution monitoring and health management, easily transferrable to and applicable in other domains and countries.",project-academic
10.1109/JSAC.2020.3005495,2020-06-29,a,IEEE,deep reinforcement learning for dynamic uplink downlink resource allocation in high mobility 5g hetnet," Recently, the 5G is widely deployed for supporting communications of high mobility nodes including train, vehicular and unmanned aerial vehicles (UAVs) largely emerged as the main components for constructing the wireless heterogeneous network (HetNet). To further improve the radio utilization, the Time Division Duplex (TDD) is considered to be the potential full-duplex communication technology in the high mobility 5G network. However, the high mobility of users leads to the high dynamic network traffic and unpredicted link state change. A new method to predict the dynamic traffic and channel condition and schedule the TDD configuration in real-time is essential for the high mobility environment. In this paper, we investigate the channel model in the high mobility and heterogeneous network and proposed a novel deep reinforcement learning based intelligent TDD configuration algorithm to dynamically allocate radio resources in an online manner. In the proposal, the deep neural network is employed to extract the features of the complex network information, and the dynamic Q-value iteration based reinforcement learning with experience replay memory mechanism is proposed to adaptively change TDD Up/Down-link ratio by evaluated rewards. The simulation results show that the proposal achieves significant network performance improvement in terms of both network throughput and packet loss rate, comparing with conventional TDD resource allocation algorithms.",project-academic
10.1109/TITS.2018.2836141,2019-03-01,a,IEEE,impact of data loss for prediction of traffic flow on an urban road using neural networks," The deployment of intelligent transport systems requires efficient means of assessing the traffic situation. This involves gathering real traffic data from the road network and predicting the evolution of traffic parameters, in many cases based on incomplete or false data from vehicle detectors. Traffic flows in the network follow spatiotemporal patterns and this characteristic is used to suppress the impact of missing or erroneous data. The application of multilayer perceptrons and deep learning networks using autoencoders for the prediction task is evaluated. Prediction sensitivity to false data is estimated using traffic data from an urban traffic network.",project-academic
10.1109/CISS.2019.8692820,2019-03-20,p,IEEE,beam management in 5g nr using geolocation side information," A machine learning solution leveraging geolocation side information is proposed for enhancing beam management in 5G NR millimeter wave (mmWave) wireless systems. An important building block of our solution are the support vector machines (SVMs), which are used to model the mapping between the user equipments’ (UEs) geolocations and their serving beams/cells in a multiuser, multi-cell environment. Building upon the SVM models mentioned above, we introduce a multiuser scheduling algorithm that uses local beam assignment information from the cells adjacent to the users to reduce the amount of required real time channel state information (CSI) feedback. Simulations carried out using a realistic antenna array radiation pattern, as well as, data from a ray tracing channel model in a dense urban mmWave deployment show that the proposed multiuser scheduler has remarkably good performance, while its algorithmic complexity is kept low. We further quantify the improvements that our SVM-based beam management methods enable by comparison against the conventional exhaustive beam sweeping approach typically employed by 5G NR mmWave implementations. In this case, we show that our proposal enables the network to achieve a 50% reduction in initial access latency at a fixed signaling overhead, or 34% reduction of signaling overhead at a fixed latency requirement.",project-academic
,2018-05-04,a,,ultra low power deep learning powered autonomous nano drones," Flying in dynamic, urban, highly-populated environments represents an open problem in robotics. State-of-the-art (SoA) autonomous Unmanned Aerial Vehicles (UAVs) employ advanced computer vision techniques based on computationally expensive algorithms, such as Simultaneous Localization and Mapping (SLAM) or Convolutional Neural Networks (CNNs) to navigate in such environments. In the Internet-of-Things (IoT) era, nano-size UAVs capable of autonomous navigation would be extremely desirable as self-aware mobile IoT nodes. However, autonomous flight is considered unaffordable in the context of nano-scale UAVs, where the ultra-constrained power envelopes of tiny rotor-crafts limit the on-board computational capabilities to low-power microcontrollers. In this work, we present the first vertically integrated system for fully autonomous deep neural network-based navigation on nano-size UAVs. Our system is based on GAP8, a novel parallel ultra-low-power computing platform, and deployed on a 27 g commercial, open-source CrazyFlie 2.0 nano-quadrotor. We discuss a methodology and software mapping tools that enable the SoA CNN presented in [1] to be fully executed on-board within a strict 12 fps real-time constraint with no compromise in terms of flight results, while all processing is done with only 94 mW on average - 1% of the power envelope of the deployed nano-aircraft.",project-academic
,2008-03-15,b,,high performance computing and grids in action," Advancement of science and technology and its impact on real life applications is more and more strictly related to the progress and availability of high performance parallel computer systems and grids, the novel networked infrastructures aimed to organize and optimize the use of a huge amount of distributed data processing and computing resources. This book collects in four chapters single monographs related to the fundamental advances in parallel computer systems and their future developments from different points of view (from computer scientists, computer manufacturers, end users) and related to the establishment and evolution of grids fundamentals, implementation and deployment. The aim is to cover different points of view in the field by actors playing different roles, to orchestrate and correlate their interconnection and coherence, and - above all - to show behaviors, impacts, performances of architectures, systems, services and organizations in action. Accordingly, the expected audience would be broad, mainly made up by computer scientists, Ph.D. students, post-doc researchers, specialists of computing and data centers, computer engineers and architects, project leaders, information system planners and professional technologists.IOS Press is an international science, technical and medical publisher of high-quality books for academics, scientists, and professionals in all fields. Some of the areas we publish in: -Biomedicine -Oncology -Artificial intelligence -Databases and information systems -Maritime engineering -Nanotechnology -Geoengineering -All aspects of physics -E-governance -E-commerce -The knowledge economy -Urban studies -Arms control -Understanding and responding to terrorism -Medical informatics -Computer Sciences",project-academic
10.1109/TKDE.2020.2985954,2020-04-10,a,Institute of Electrical and Electronics Engineers (IEEE),incorporating multi source urban data for personalized and context aware multi modal transportation recommendation," Transportation recommendation is one important map service in navigation applications. Previous transportation recommendation solutions fail to deliver satisfactory user experience because their recommendations only consider routes in one transportation mode (uni-modal, e.g., taxi, bus, cycle) and largely overlook situational context. In this work, we propose Hydra, a multi-task deep learning based recommendation system that offers multi-modal transportation planning and is adaptive to various situational context (e.g., nearby point-of-interest~(POI) distribution and weather). We design a novel two-level framework that integrates uni-modal and multi-modal~(e.g., taxi-bus, bus-cycle) routes as well as heterogeneous urban data for intelligent multi-modal transportation recommendation. In addition to urban context features constructed from multi-source urban data, we learn the latent representations of users, origin-destination~(OD) pairs and transportation modes based on user implicit feedbacks. Moreover, we propose two models to recommend the proper route among various uni-modal and multi-modal transportation routes: (1) a light-weight gradient boosting decision tree (GBDT) based recommendation model; and (2) a multi-task wide and deep learning (MTWDL) based recommendation model. We also optimize the framework to support real-time, large-scale route query and recommendation. We deploy Hydra on Baidu Maps, one of the world's largest map services. The GBDT based model and MTWDL based variants achieve 82.8% and 96.6% relative improvement of user click ratio, respectively.",project-academic
10.1016/J.ESWA.2016.03.024,2016-10-01,a,"Pergamon Press, Inc.",building detection from orthophotos using a machine learning approach," Automatic building detection in orthophotos via a machine learning approach.Flexible framework that exploits supervised learning.Applying the covariance descriptor to the building detection problem.An extended performance study of several combination segmentation-descriptor.Classification performance is obtained with K-NN, Partial Least Square and SVM. Building detection from aerial images has many applications in fields like urban planning, real-estate management, and disaster relief. In the last two decades, a large variety of methods on automatic building detection have been proposed in the remote sensing literature. Many of these approaches make use of local features to classify each pixel or segment to an object label, therefore involving an extra step to fuse pixelwise decisions. This paper presents a generic framework that exploits recent advances in image segmentation and region descriptors extraction for the automatic and accurate detection of buildings on aerial orthophotos. The proposed solution is supervised in the sense that appearances of buildings are learnt from examples. For the first time in the context of building detection, we use the matrix covariance descriptor, which proves to be very informative and compact. Moreover, we introduce a principled evaluation that allows selecting the best pair segmentation algorithm-region descriptor for the task of building detection. Finally, we provide a performance evaluation at pixel level using different classifiers. This evaluation is conducted over 200 buildings using different segmentation algorithms and descriptors. The performance analysis quantifies the quality of both the image segmentation and the descriptor used. The proposed approach presents several advantages in terms of scalability, suitability and simplicity with respect to the existing methods. Furthermore, the proposed scheme (detection chain and evaluation) can be deployed for detecting multiple object categories that are present in images and can be used by intelligent systems requiring scene perception and parsing such as intelligent unmanned aerial vehicle navigation and automatic 3D city modeling.",project-academic
10.1016/J.JCLEPRO.2019.02.179,2019-06-01,a,Elsevier,evaluating air quality by combining stationary smart mobile pollution monitoring and data driven modelling," Abstract None None Air pollution impact assessment is a major objective for various community councils in large cities, which have lately redirected their attention towards using more low-cost sensing units supported by citizen involvement. However, there is a lack of research studies investigating real-time mobile air-quality measurement through smart sensing units and even more of any data-driven modelling techniques that could be deployed to predict air quality accurately from the generated data-sets. This paper addresses these challenges by: a) proposing a comparative and detailed investigation of various air quality monitoring devices (both fixed and mobile), tested through field measurements and citizen sensing in an eco-neighbourhood from Lorraine, France, and by b) proposing a machine learning approach to evaluate the accuracy and potential of such mobile generated data for air quality prediction. The air quality evaluation consists of three experimenting protocols: a) first, we installed fixed passive tubes for monitoring the nitrogen dioxide concentrations placed in strategic locations highly affected by traffic circulation in an eco-neighbourhood, b) second, we monitored the nitrogen dioxide registered by citizens using smart and mobile pollution units carried at breathing level; results revealed that mobile-captured concentrations were 3–5 times higher than the ones registered by passive-static monitoring tubes and c) third, we compared different mobile pollution stations working simultaneously, which revealed noticeable differences in terms of result variability and sensitivity. Finally, we applied a machine learning modelling by using decision trees and neural networks on the mobile-generated data and show that humidity and noise are the most important factors influencing the prediction of nitrogen dioxide concentrations of mobile stations.",project-academic
10.1145/3289175.3289177,2018-12-10,p,ACM,towards emergent microservices for client tailored design," Contemporary systems are increasingly complex, with both large codebases and constantly changing environments which make them challenging to develop, deploy and manage. We consider two recent efforts to tackle this complexity: microservices and emergent software. Microservices have gained recent popularity in industry, in which monoliths of software are broken down into compositions of single-objective, end-to-end services running on HTTP which can be scaled out on cloud hosting systems. From the research community, the emergent systems concept demonstrates promise in using real-time learning to autonomously compose and optimise software systems from small building blocks, rapidly finding the best behavioural composition to match the current deployment conditions. We argue that emergent software and microservice architectures have strong potential for synergy in complex systems, offering mutually compatible lessons in dealing with complexity via scale-out design and real-time client-tailored behaviour. We explore self-designing microservices, built with emergent software, to demonstrate the complementary boundaries of both concepts - and how future intersections may offer novel architectures that lie at a compelling point between human- and machine-designed systems. We present the conceptual synergy and demonstrate a specific microservice architecture for a smart city example where scoped microservices are continually self-composed according to the demands of the applications and operating environment. For the purpose of reproducibility of the study, we make available all the code used in the evaluation of the proposed approach.",project-academic
10.1109/MVT.2020.3019650,2020-09-25,a,IEEE,machine learning for 6g wireless networks carrying forward enhanced bandwidth massive access and ultrareliable low latency service," To satisfy the expected plethora of demanding services, the future generation of wireless networks (6G) has been mandated as a revolutionary paradigm to carry forward the capacities of enhanced broadband, massive access, and ultrareliable and lowlatency service in 5G wireless networks to a more powerful and intelligent level. Recently, the structure of 6G networks has tended to be extremely heterogeneous, densely deployed, and dynamic. Combined with tight quality of service (QoS), such complex architecture will result in the untenability of legacy network operation routines. In response, artificial intelligence (AI), especially machine learning (ML), is emerging as a fundamental solution to realize fully intelligent network orchestration and management. By learning from uncertain and dynamic environments, AI-/ML-enabled channel estimation and spectrum management will open up opportunities for bringing the excellent performance of ultrabroadband techniques, such as terahertz communications, into full play. Additionally, challenges brought by ultramassive access with respect to energy and security can be mitigated by applying AI-/ML-based approaches. Moreover, intelligent mobility management and resource allocation will guarantee the ultrareliability and low latency of services. Concerning these issues, this article introduces and surveys some state-of-the-art techniques based on AI/ML and their applications in 6G to support ultrabroadband, ultramassive access, and ultrareliable and lowlatency services.",project-academic
,2017-10-06,a,,intelligent pothole detection and road condition assessment," Poor road conditions are a public nuisance, causing passenger discomfort, damage to vehicles, and accidents. In the U.S., road-related conditions are a factor in 22,000 of the 42,000 traffic fatalities each year. Although we often complain about bad roads, we have no way to detect or report them at scale. To address this issue, we developed a system to detect potholes and assess road conditions in real-time. Our solution is a mobile application that captures data on a car's movement from gyroscope and accelerometer sensors in the phone. To assess roads using this sensor data, we trained SVM models to classify road conditions with 93% accuracy and potholes with 92% accuracy, beating the base rate for both problems. As the user drives, the models use the sensor data to classify whether the road is good or bad, and whether it contains potholes. Then, the classification results are used to create data-rich maps that illustrate road conditions across the city. Our system will empower civic officials to identify and repair damaged roads which inconvenience passengers and cause accidents. This paper details our data science process for collecting training data on real roads, transforming noisy sensor data into useful signals, training and evaluating machine learning models, and deploying those models to production through a real-time classification app. It also highlights how cities can use our system to crowdsource data and deliver road repair resources to areas in need.",project-academic
10.1007/S12652-019-01272-8,2019-03-22,a,Springer Berlin Heidelberg,object detection mechanism based on deep learning algorithm using embedded iot devices for smart home appliances control in cot," Object detection and recognition is commonly used in diverse computer vision based applications and many algorithms are proposed in literature. However, application of object detection algorithms in real-time systems demand minimal computation time and comprehensive performance analysis needs to be conducted before deployment. This paper aims to conduct performance analysis of deep learning based algorithm i.e. single shot detector (SSD) in IoT based embedded devices for smart home appliances control. We have developed a smart home automation system using object detection algorithm based on model view controller (MVC) architecture deployed on Cloud of Things (CoT) i.e. Amazon Web Service (AWS) cloud for users to remotely monitor their homes. Message queuing telemetry transport (MQTT) protocol is used for communication with connected IoT devices. For load-balancing, we have proposed the concept of distributed broker to support high number of publishers and subscribers. For experimental analysis, we have connected a camera with Raspberry Pi for object detection based on deep learning algorithm (SSD) using OpenCV library in our proposed system. Experiments are conducted to evaluate the performance of object detection algorithm under varying environmental conditions by changing the light intensity level, distance of object from camera, and frame size of video. Results show that communication delay is very low (i.e. 0.2 s) as compared to processing delay in Raspberry Pi. Furthermore, changing environmental conditions have very low/insignificant impact on the processing delay of object detection algorithm i.e. average delay of 1.7 s (stdev. 0.18) and 1.8 s (stdev. 0.24) in bright and dark lighting levels, respectively. However, accuracy is deteriorated under low lighting intensity level and increased frame sizes i.e. from 95–100 to 80–85%. Selected embedded device, camera model and object detection algorithm limits the performance of object detection in real-time systems and shall be carefully selected to fulfill the requirement.",project-academic
10.1109/ICDMW.2018.00099,2018-11-01,p,IEEE,robust commuter movement inference from connected mobile devices," The preponderance of connected devices provides unprecedented opportunities for fine-grained monitoring of the public infrastructure. However while classical models expect high quality application-specific data streams, the promise of the Internet of Things (IoT) is that of an abundance of disparate and noisy datasets from connected devices. In this context, we consider the problem of estimation of the level of service of a city-wide public transport network. We first propose a robust unsupervised model for train movement inference from wifi traces, via the application of robust clustering methods to a one dimensional spatio-temporal setting. We then explore the extent to which the demand-supply gap can be estimated from connected devices. We propose a classification model of real-time commuter patterns, including both a batch training phase and an online learning component. We describe our deployment architecture and assess our system accuracy on a large-scale anonymized dataset comprising more than 10 billion records.",project-academic
,2019-03-04,a,,robust commuter movement inference from connected mobile devices," The preponderance of connected devices provides unprecedented opportunities for fine-grained monitoring of the public infrastructure. However while classical models expect high quality application-specific data streams, the promise of the Internet of Things (IoT) is that of an abundance of disparate and noisy datasets from connected devices. In this context, we consider the problem of estimation of the level of service of a city-wide public transport network. We first propose a robust unsupervised model for train movement inference from wifi traces, via the application of robust clustering methods to a one dimensional spatio-temporal setting. We then explore the extent to which the demand-supply gap can be estimated from connected devices. We propose a classification model of real-time commuter patterns, including both a batch training phase and an online learning component. We describe our deployment architecture and assess our system accuracy on a large-scale anonymized dataset comprising more than 10 billion records.",project-academic
10.1109/TKDE.2018.2852764,2019-06-01,a,IEEE,location inference for non geotagged tweets in user timelines," Social media like Twitter have become globally popular in the past decade. Thanks to the high penetration of smartphones, social media users are increasingly going mobile. This trend has contributed to foster various location based services deployed on social media, the success of which heavily depends on the availability and accuracy of users’ location information. However, only a very small fraction of tweets in Twitter are geo-tagged. Therefore, it is necessary to infer locations for tweets in order to attain the purpose of those location based services. In this paper, we tackle this problem by scrutinizing Twitter user timelines in a novel fashion. First of all, we split each user's tweet timeline temporally into a number of clusters, each tending to imply a distinct location. Subsequently, we adapt two machine learning models to our setting and design classifiers that classify each tweet cluster into one of the pre-defined location classes at the city level. The Bayes based model focuses on the information gain of words with location implications in the user-generated contents. The convolutional LSTM model treats user-generated contents and their associated locations as sequences and employs bidirectional LSTM and convolution operation to make location inferences. The two models are evaluated on a large set of real Twitter data. The experimental results suggest that our models are effective at inferring locations for non-geotagged tweets and the models outperform the state-of-the-art and alternative approaches significantly in terms of inference accuracy.",project-academic
10.1109/ICRA.2014.6907000,2014-09-29,p,IEEE,human aware uas path planning in urban environments using nonstationary mdps," A growing concern with deploying Unmanned Aerial Vehicles (UAVs) in urban environments is the potential violation of human privacy, and the backlash this could entail. Therefore, there is a need for UAV path planning algorithms that minimize the likelihood of invading human privacy. We formulate the problem of human-aware path planning as a nonstationary Markov Decision Process, and provide a novel model-based reinforcement learning solution that leverages Gaussian process clustering. Our algorithm is flexible enough to accommodate changes in human population densities by employing Bayesian nonparametrics, and is real-time computable. The approach is validated experimentally on a large-scale long duration experiment with both simulated and real UAVs.",project-academic
10.1007/978-3-319-46131-1_19,2016-09-19,p,"Springer, Cham",pulse a real time system for crowd flow prediction at metropolitan subway stations," The fast pace of urbanization has given rise to complex transportation networks, such as subway systems, that deploy smart card readers generating detailed transactions of mobility. Predictions of human movement based on these transaction streams represents tremendous new opportunities from optimizing fleet allocation of on-demand transportation such as UBER and LYFT to dynamic pricing of services. However, transportation research thus far has primarily focused on tackling other challenges from traffic congestion to network capacity. To take on this new opportunity, we propose a real-time framework, called PULSE (Prediction Framework For Usage Load on Subway SystEms), that offers accurate multi-granular arrival crowd flow prediction at subway stations. PULSE extracts and employs two types of features such as streaming features and station profile features. Streaming features are time-variant features including time, weather, and historical traffic at subway stations (as time-series of arrival/departure streams), where station profile features capture the time-invariant unique characteristics of stations, including each station’s peak hour crowd flow, remoteness from the downtown area, and mean flow. Then, given a future prediction interval, we design novel stream feature selection and model selection algorithms to select the most appropriate machine learning models for each target station and tune that model by choosing an optimal subset of stream traffic features from other stations. We evaluate our PULSE framework using real transaction data of 11 million passengers from a subway system in Shenzhen, China. The results demonstrate that PULSE greatly improves the accuracy of predictions at all subway stations by up to \(49\,\%\) over baseline algorithms.",project-academic
10.1145/3219819.3219913,2018-07-19,p,ACM,a dynamic pipeline for spatio temporal fire risk prediction," Recent high-profile fire incidents in cities around the world have highlighted gaps in fire risk reduction efforts, as cities grapple with fewer resources and more properties to safeguard. To address this resource gap, prior work has developed machine learning frameworks to predict fire risk and prioritize fire inspections. However, existing approaches were limited by not including time-varying data, never deploying in real-time, and only predicting risk for a small subset of commercial properties in their city. Here, we have developed a predictive risk framework for all 20,636 commercial properties in Pittsburgh, based on time-varying data from a variety of municipal agencies. We have deployed our fire risk model on Pittsburgh Bureau of Fire's (PBF), and we have developed preliminary risk models for residential property fire risk prediction. Our commercial risk model outperforms the prior state of the art with a kappa of 0.33 compared to their 0.17, and is able to be applied to nearly 4 times as many properties as the prior model. In the 5 weeks since our model was first deployed, 58% of our predicted high-risk properties had a fire incident of any kind, while 23% of the building fire incidents that occurred took place in our predicted high or medium risk properties. The risk scores from our commercial model are visualized on an interactive dashboard and map to assist the PBF with planning their fire risk reduction initiatives. This work is already helping to improve fire risk reduction in Pittsburgh and is beginning to be adopted by other cities.",project-academic
10.1016/J.TRC.2018.12.007,2019-01-01,a,Pergamon,cordon control with spatially varying metering rates a reinforcement learning approach," Abstract None None The work explores how Reinforcement Learning can be used to re-time traffic signals around cordoned neighborhoods. An RL-based controller is developed by representing traffic states as graph-structured data and customizing corresponding neural network architectures to handle those data. The customizations enable the controller to: (i) model neighborhood-wide traffic based on directed-graph representations; (ii) use the representations to identify patterns in real-time traffic measurements; and (iii) capture those patterns to a spatial representation needed for selecting optimal cordon-metering rates. Input to the selection process also includes a total inflow to be admitted through a cordon. The rate is optimized in a separate process that is not part of the present work. Our RL-controller distributes that separately-optimized rate across the signalized street links that feed traffic through the cordon. The resulting metering rates vary from one feeder link to the next. The selection process can reoccur at short time intervals in response to changing traffic patterns. Once trained on a few cordons, the RL-controller can be deployed on cordons elsewhere in a city without additional training. None This portability feature is confirmed via simulations of traffic on an idealized street network. The tests also indicate that the controller can reduce the network’s vehicle hours traveled well beyond what can be achieved via spatially-uniform cordon metering. The extra reductions in VHT are found to grow larger when traffic exhibits greater in-homogeneities over the network.",project-academic
,2012-07-09,p,IEEE,large scale wireless indoor localization by clustering and extreme learning machine," Due to the widespread deployment and low cost, WLAN has gained more attention for indoor localization recently. However, when we apply these WLAN based localization algorithms to large-scale environments, such as a wireless city, they may encounter the scalability problem due to the huge RSS database. The huge database may cause long response time for the terminal clients if the localization algorithm needs to search the database for the real time localization phase. In this paper, we propose a novel clustering based localization algorithm for large scale area by utilizing Nearest Neighbor (NN) rule and Extreme Learning Machine (ELM). The proposed algorithm has shown competitive advantage in terms of the real time localization efficiency as well as the localization accuracy.",project-academic
10.1007/978-981-16-0422-5_10,2020-10-14,p,"Springer, Singapore",trust based adversarial resiliency in vehicular cyber physical systems using reinforcement learning," Vehicular cyber physical systems (VCPS), by leveraging on advancements in sensing, wireless technologies and vehicular ad hoc networks (VANET) have improved driving experience and birthed systems like cooperative adaptive cruise control, and have formed the foundation for autonomous and platooned vehicles. However, its deployment is adversely affected by security concerns even as attackers continue to improve their attacks methods. In recent years, machine learning has become an invaluable tool for research, and investigations into its application in CPS security is increasingly active. Although, supervised learning techniques were used for most tasks initially, reinforcement learning due to the excellent results obtained in peculiar cases like the environment in which vehicles operate have become popular. Trust management systems are also very useful in identifying adversaries in a vehicular network. In this paper, a data-oriented trust-based method for improving the resiliency of vehicles to adversarial attacks is investigated using RL. Improving on other works that combine direct and indirect trusts and assume that vehicles interact for a long time, this method is suited for the dynamic environment vehicles operate in and the high mobility they experience. Specifically, the Q-learning learning algorithm is used to learn and adapt the weight used to estimate the trust value and therefore reflect the real environment. The simulation results obtained show that the proposed methodology is efficient and establishes the contributions of the application of RL to CPS security.",project-academic
10.1073/PNAS.1913003117,2020-02-11,a,Proc Natl Acad Sci U S A,genome scale transcriptional dynamics and environmental biosensing," Genome-scale technologies have enabled mapping of the complex molecular networks that govern cellular behavior. An emerging theme in the analyses of these networks is that cells use many layers of regulatory feedback to constantly assess and precisely react to their environment. The importance of complex feedback in controlling the real-time response to external stimuli has led to a need for the next generation of cell-based technologies that enable both the collection and analysis of high-throughput temporal data. Toward this end, we have developed a microfluidic platform capable of monitoring temporal gene expression from over 2,000 promoters. By coupling the ""Dynomics"" platform with deep neural network (DNN) and associated explainable artificial intelligence (XAI) algorithms, we show how machine learning can be harnessed to assess patterns in transcriptional data on a genome scale and identify which genes contribute to these patterns. Furthermore, we demonstrate the utility of the Dynomics platform as a field-deployable real-time biosensor through prediction of the presence of heavy metals in urban water and mine spill samples, based on the the dynamic transcription profiles of 1,807 unique Escherichia coli promoters.",project-academic
10.1145/2983323.2983379,2016-10-24,p,ACM,routing an autonomous taxi with reinforcement learning," Singapore's vision of a Smart Nation encompasses the development of effective and efficient means of transportation. The government's target is to leverage new technologies to create services for a demand-driven intelligent transportation model including personal vehicles, public transport, and taxis. Singapore's government is strongly encouraging and supporting research and development of technologies for autonomous vehicles in general and autonomous taxis in particular. The design and implementation of intelligent routing algorithms is one of the keys to the deployment of autonomous taxis. In this paper we demonstrate that a reinforcement learning algorithm of the Q-learning family, based on a customized exploration and exploitation strategy, is able to learn optimal actions for the routing autonomous taxis in a real scenario at the scale of the city of Singapore with pick-up and drop-off events for a fleet of one thousand taxis.",project-academic
10.1155/2012/863545,2012-12-31,a,Hindawi Publishing Corporation,a very fast decision tree algorithm for real time data mining of imperfect data streams in a distributed wireless sensor network," Wireless sensor networks (WSNs) are a rapidly emerging technology with a great potential in many ubiquitous applications. Although these sensors can be inexpensive, they are often relatively unreliable when deployed in harsh environments characterized by a vast amount of noisy and uncertain data, such as urban traffic control, earthquake zones, and battlefields. The data gathered by distributed sensors—which serve as the eyes and ears of the system—are delivered to a decision center or a gateway sensor node that interprets situational information from the data streams. Although many other machine learning techniques have been extensively studied, real-time data mining of high-speed and nonstationary data streams represents one of the most promising WSN solutions. This paper proposes a novel stream mining algorithm with a programmable mechanism for handling missing data. Experimental results from both synthetic and real-life data show that the new model is superior to standard algorithms.",project-academic
10.1109/ICTC49870.2020.9289086,2020-10-21,p,IEEE,reinforcement learning based real time aerial bs positioning for dense urban 5g mobile network," Due to the recent surge in mobile users and traffic, next-generation mobile communication aims to increase network capacity through large bandwidth and small cell deployment. To respond to this situation, there have been lots of researches on aerial base stations (BSs) using unmanned aerial vehicles (UAVs) for the mobile user. Aerial BS has the advantage of providing flexible communication range by avoiding obstacles such as buildings in urban areas through 3D positioning. However, finding an optimal point for aerial BS considering the variance of user’s requirements, movement, and obstacles in real time is difficult problem to solve. Therefore, it is necessary to find an approximate optimal point for applying to the real-time flight path control of aerial BS. This paper aims to find optimal behavior in real time interacting with a given environment, through reinforcement learning. We propose the algorithm based on Q-learning with a new concept called Coarse Search to reduce convergence speed. We evaluate the performance of the algorithm by comparing it with that of other heuristic algorithms.",project-academic
10.1109/SOLI.2016.7551691,2016-07-10,p,IEEE,performance evaluation of the deep learning approach for traffic flow prediction at different times," Traffic flow prediction is very important in the deployment of intelligent transportation system. Based on our previous research on deep learning approach for traffic data prediction, we further evaluates the performance of the SAE model for traffic flow prediction at daytime and nighttime. Through 250 experimental tasks training a SAE model and evaluating its performance at daytime and nighttime with 3 different criteria, we obtain the best combination of hyper parameters for each criterion at different times on weekday and non-weekday, respectively. Experimental results show that the MAE and RMSE at daytime are larger than that at nighttime, while the MRE at daytime are smaller than that at nighttime. For different criteria, the hyper parameters of the SAE model should vary accordingly. The results in this paper indicate that in real applications, traffic flow prediction using the deep learning approach can be a combination of multiple SAE models with different parameters suitable for different periods, which is of significance in future research.",project-academic
10.3390/S19091987,2019-04-28,a,Multidisciplinary Digital Publishing Institute,object tracking for a smart city using iot and edge computing," As the Internet-of-Things (IoT) and edge computing have been major paradigms for distributed data collection, communication, and processing, smart city applications in the real world tend to adopt IoT and edge computing broadly. Today, more and more machine learning algorithms would be deployed into front-end sensors, devices, and edge data centres rather than centralised cloud data centres. However, front-end sensors and devices are usually not so capable as those computing units in huge data centres, and for this sake, in practice, engineers choose to compromise for limited capacity of embedded computing and limited memory, e.g., neural network models being pruned to fit embedded devices. Visual object tracking is one of many important elements of a smart city, and in the IoT and edge computing context, high requirements to computing power and memory space severely prevent massive and accurate tracking. In this paper, we report on our contribution to object tracking on lightweight computing including (1) using limited computing capacity and memory space to realise tracking; (2) proposing a new algorithm region proposal correlation filter fitting for most edge devices. Systematic evaluations show that (1) our techniques can fit most IoT devices; (2) our techniques can keep relatively high accuracy; and (3) the generated model size is much less than others.",project-academic
,2021-01-01,p,,deepcovid an operational deep learning driven framework for explainable real time covid 19 forecasting," How do we forecast an emerging pandemic in real time in a purely data-driven manner? How to leverage rich heterogeneous data based on various signals such as mobility, testing, and/or disease exposure for forecasting? How to handle noisy data and generate uncertainties in the forecast? In this paper, we present DEEPCOVID, an operational deep learning framework designed for real-time COVID-19 forecasting. DEEPCOVID works well with sparse data and can handle noisy heterogeneous data signals by propagating the uncertainty from the data in a principled manner resulting in meaningful uncertainties in the forecast. The deployed framework also consists of modules for both real-time and retrospective exploratory analysis to enable interpretation of the forecasts. Results from real-time predictions (featured on the CDC website and FiveThirtyEight.com) since April 2020 indicates that our approach is competitive among the methods in the COVID-19 Forecast Hub, especially for short-term predictions.",project-academic
10.5555/2023718.2023721,2010-11-29,p,"Springer, Berlin, Heidelberg",adapting distributed real time and embedded pub sub middleware for cloud computing environments," Enterprise distributed real-time and embedded (DRE) publish/subscribe (pub/sub) systems manage resources and data that are vital to users. Cloud computing---where computing resources are provisioned elastically and leased as a service---is an increasingly popular deployment paradigm. Enterprise DRE pub/sub systems can leverage cloud computing provisioning services to execute needed functionality when on-site computing resources are not available. Although cloud computing provides flexible on-demand computing and networking resources, enterprise DRE pub/sub systems often cannot accurately characterize their behavior a priori for the variety of resource configurations cloud computing supplies (e.g., CPU and network bandwidth), which makes it hard for DRE systems to leverage conventional cloud computing platforms.This paper provides two contributions to the study of how autonomic configuration of DRE pub/sub middleware can provision and use on-demand cloud resources effectively. We first describe how supervised machine learning can configure DRE pub/sub middleware services and transport protocols autonomically to support end-to-end quality-of-service (QoS) requirements based on cloud computing resources. We then present results that empirically validate how computing and networking resources affect enterprise DRE pub/sub system QoS. These results show how supervised machine learning can configure DRE pub/sub middleware adaptively in",project-academic
,2019-06-12,a,,deep reinforcement learning for unmanned aerial vehicle assisted vehicular networks," Unmanned aerial vehicles (UAVs) are envisioned to complement the 5G communication infrastructure in future smart cities. Hot spots easily appear in road intersections, where effective communication among vehicles is challenging. UAVs may serve as relays with the advantages of low price, easy deployment, line-of-sight links, and flexible mobility. In this paper, we study a UAV-assisted vehicular network where the UAV jointly adjusts its transmission control (power and channel) and 3D flight to maximize the total throughput. First, we formulate a Markov decision process (MDP) problem by modeling the mobility of the UAV/vehicles and the state transitions. Secondly, we solve the target problem using a deep reinforcement learning method under unknown or unmeasurable environment variables especially in 5G, namely, the deep deterministic policy gradient (DDPG), and propose three solutions with different control objectives. Environment variables are unknown and unmeasurable, therefore, we use a deep reinforcement learning method. Moreover, considering the energy consumption of 3D flight, we extend the proposed solutions to maximize the total throughput per energy unit by encouraging or discouraging the UAV's mobility. To achieve this goal, the DDPG framework is modified. Thirdly, in a simplified model with small state space and action space, we verify the optimality of proposed algorithms. Comparing with two baseline schemes, we demonstrate the effectiveness of proposed algorithms in a realistic model.",project-academic
,2020-12-06,a,,coedge cooperative dnn inference with adaptive workload partitioning over heterogeneous edge devices," Recent advances in artificial intelligence have driven increasing intelligent applications at the network edge, such as smart home, smart factory, and smart city. To deploy computationally intensive Deep Neural Networks (DNNs) on resource-constrained edge devices, traditional approaches have relied on either offloading workload to the remote cloud or optimizing computation at the end device locally. However, the cloud-assisted approaches suffer from the unreliable and delay-significant wide-area network, and the local computing approaches are limited by the constrained computing capability. Towards high-performance edge intelligence, the cooperative execution mechanism offers a new paradigm, which has attracted growing research interest recently. In this paper, we propose CoEdge, a distributed DNN computing system that orchestrates cooperative DNN inference over heterogeneous edge devices. CoEdge utilizes available computation and communication resources at the edge and dynamically partitions the DNN inference workload adaptive to devices' computing capabilities and network conditions. Experimental evaluations based on a realistic prototype show that CoEdge outperforms status-quo approaches in saving energy with close inference latency, achieving up to 25.5%~66.9% energy reduction for four widely-adopted CNN models.",project-academic
10.1109/TNET.2020.3042320,2021-04-01,a,IEEE,coedge cooperative dnn inference with adaptive workload partitioning over heterogeneous edge devices," Recent advances in artificial intelligence have driven increasing intelligent applications at the network edge, such as smart home, smart factory, and smart city. To deploy computationally intensive Deep Neural Networks (DNNs) on resource-constrained edge devices, traditional approaches have relied on either offloading workload to the remote cloud or optimizing computation at the end device locally. However, the cloud-assisted approaches suffer from the unreliable and delay-significant wide-area network, and the local computing approaches are limited by the constrained computing capability. Towards high-performance edge intelligence, the cooperative execution mechanism offers a new paradigm, which has attracted growing research interest recently. In this paper, we propose CoEdge, a distributed DNN computing system that orchestrates cooperative DNN inference over heterogeneous edge devices. CoEdge utilizes available computation and communication resources at the edge and dynamically partitions the DNN inference workload adaptive to devices’ computing capabilities and network conditions. Experimental evaluations based on a realistic prototype show that CoEdge outperforms status-quo approaches in saving energy with close inference latency, achieving up to 25.5% ~ 66.9% energy reduction for four widely-adopted CNN models.",project-academic
10.3390/W10020142,2018-02-02,a,Multidisciplinary Digital Publishing Institute,gene expression programming coupled with unsupervised learning a two stage learning process in multi scale short term water demand forecasts," This article proposes a new general approach in short-term water demand forecasting based on a two-stage learning process that couples time-series clustering with gene expression programming (GEP). The approach was tested on the real life water demand data of the city of Milan, in Italy. Moreover, multi-scale modeling using a series of head-time was deployed to investigate the optimum temporal resolution under study. Multi-scale modeling was performed based on rearranging hourly based patterns of water demand into 3, 6, 12, and 24 h lead times. Results showed that GEP should receive more attention among the emerging nonlinear modelling techniques if coupled with unsupervised learning algorithms in detailed spherical k-means.",project-academic
,2014-01-01,p,,multi robot human guidance using topological graphs," Prior approaches to human guidance using robots inside a building have typically been limited to a single robot guide that navigates a human from start to goal. However, due to their limited mobility, the robot is often unable to keep up with the human’s natural speed. In contrast, this paper addresses this difference in mobility between robots and people by presenting an approach that uses multiple robots to guide a human. Our approach uses a compact topological graph representation of the environment, and we first present the procedure for generating this representation. Next, we formulate the multi-robot guidance problem as a Markov Decision Process (MDP). Using a model of human motion in the presence of guiding robots, we define the transition function for this MDP. Finally, we solve the MDP using Value Iteration to obtain an optimal policy for placing robots and evaluate this policy’s effectiveness. Indoor environments such as airports, shopping malls, hospitals, and warehouse stores are characteristically full of people hurrying towards a destination or trying to locate a particular item. Often, they are unfamiliar with the environment and spend a fair amount of time locating these resources. With recent advancements in service robots, it is becoming far more feasible to deploy a large number of robots to aid humans in these environments. This paper studies how ubiquitous robots in an environment can be used to guide people efficiently to their destinations. Past research has explored the possibility of using a single robot to guide people (Thrun et al. 1999; Philippsen and Siegwart 2003). However, in environments densely packed with moving people and goods, navigating a guide robot the entire length from a human’s start location to their goal can be a significant challenge. The same navigation task can be completed far more efficiently by people, and yet they become limited by the navigation speed of the guide. A multirobot solution can make use of a human’s ease of navigation by proactively placing robots where the human is likely to need help in the future. Whenever the system needs to guide a human at a specific location, it can commission a nearby robot to direct the human towards the next objective, whether it be another guide robot or the goal. Once that Copyright c ￿ 2014, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. robot’s task is completed, it can go back to performing its other duties. Potentially, this approach can greatly reduce the time each individual robot has to spend guiding the human, allowing robots to assist more people in the same time. This paper specifically studies the problem of deciding where to place robots in an environment to guide a human as he or she moves around. First, we formulate this multi-robot guidance problem as a Markov Decision Process (MDP), and use a hand-coded model of human motion in the presence of guide robots to define the transition function for this MDP. We then use Value Iteration (Sutton and Barto 1998) to solve this MDP, generating an optimal solution for placing robots. Such a solution can take the uncertainty in a human’s movement into account and avoid actions that have a significant probability of failure. Finally, we evaluate the generated policy by comparing it against a heuristic solution for deciding robot placements. Experiments are run using the model of human motion, as well as with avatars controlled by real humans in a simulation environment. To reason about human movement and robot placements, a representation of the environment is required. This paper uses a topological graph representation of the environment for reasoning (see Fig. 1h). Topological graphs can provide a compact representation of the environment while still retaining all key locations and the connectivity between these locations. The process of topological graph generation described in this paper is built on previous work (Thrun and Bucken 1996). As such, the main contribution of this paper is the procedure for generating topological graphs, formulating the multi-robot guidance problem as an MDP using these topological graphs, and solving the MDP using Value Iteration to produce the best graph nodes for placing robots. All code in this paper has been implemented using the ROS middleware package (Quigley et al. 2009) and is available in the public domain along with videos from the experiments1.",project-academic
10.1109/ITSC.2016.7795890,2016-11-01,p,IEEE,towards autonomic urban traffic control with collaborative multi policy reinforcement learning," Various multi-agent decentralized approaches based on reinforcement learning (RL) have been proposed to increase scalability and real-time adaptiveness of urban traffic control (UTC) systems. In such approaches, traffic light control parameters are not pre-defined, but intelligent agents controlling the junctions learn the suitable traffic signal settings. In order to consider applications of RL in commercial UTC products, they need to enable fine-grained optimization of multiple traffic objectives and be validated in realistic UTC simulations. This paper presents REALT, a UTC system based on Distributed W-Learning (DWL), a multi-policy multi-agent RL-based optimization technique, which enables it to address multiple traffic optimization goals simultaneously. We introduce an extension of DWL with fine-grained traffic observation to enable adaptation of phase duration rather than just phase selection. We also evaluate the impact of action set selection in RL applications to UTC, by introducing a phase generation module which enables REALT to use different phase sets. We simulate REALT performance in VISSIM, on a detailed model of the road network representingWestern Road, the main traffic artery in Cork, Ireland. We use precise historic traffic counts as input and present results of the comparison of REALT performance to that of SCOOT signals deployed in Cork.",project-academic
10.1109/IOTAIS.2018.8600904,2018-11-01,p,IEEE,smart air quality monitoring system with lorawan," Nowadays, cities all over the globe are transforming into smart cities. Smart cities initiatives need to address environmental concerns such as air pollution to provide clean air. A scalable and cost-effective air monitoring system is imperative to monitor and control air pollution for smart city development. Air pollution has notable effects on the well-being of the population a whole, global atmosphere, and worldwide economy. This paper presents a scalable smart air quality monitoring system with low-cost sensors and long-range communication protocol. The sensors collect four parameters, temperature, humidity, dust and carbon dioxide in the air. The proposed end-to-end system has been implemented and deployed in Yangon, the business capital of Myanmar, as a case study since Jun 2018. The system allows the users to log in to an online dashboard to monitor the real-time status. In addition, based the collected air quality parameters for the past two months, a machine learning model has been trained to make predictions of parameters such that proactive actions can be taken to alleviate the impacts from air pollution.",project-academic
10.3390/IJERPH17103437,2020-05-14,a,Multidisciplinary Digital Publishing Institute,artificial intelligence empowered mobilization of assessments in covid 19 like pandemics a case study for early flattening of the curve," The global outbreak of the Coronavirus Disease 2019 (COVID-19) pandemic has uncovered the fragility of healthcare and public health preparedness and planning against epidemics/pandemics. In addition to the medical practice for treatment and immunization, it is vital to have a thorough understanding of community spread phenomena as related research reports 17.9-30.8% confirmed cases to remain asymptomatic. Therefore, an effective assessment strategy is vital to maximize tested population in a short amount of time. This article proposes an Artificial Intelligence (AI)-driven mobilization strategy for mobile assessment agents for epidemics/pandemics. To this end, a self-organizing feature map (SOFM) is trained by using data acquired from past mobile crowdsensing (MCS) campaigns to model mobility patterns of individuals in multiple districts of a city so to maximize the assessed population with minimum agents in the shortest possible time. Through simulation results for a real street map on a mobile crowdsensing simulator and considering the worst case analysis, it is shown that on the 15th day following the first confirmed case in the city under the risk of community spread, AI-enabled mobilization of assessment centers can reduce the unassessed population size down to one fourth of the unassessed population under the case when assessment agents are randomly deployed over the entire city.",project-academic
10.5555/3108009.3108012,2017-02-20,p,Junction Publishing,extending urban air quality maps beyond the coverage of a mobile sensor network data sources methods and performance evaluation," Targeting the problem of generating high-resolution air quality maps for cities, we leverage four different sources of data: (i) in-situ air quality measurements produced by our mobile sensor network deployed on public transportation vehicles, (ii) explanatory air-quality and meteorological variables obtained from two static monitoring stations, (iii) land-use data of the city, and (iv) traffic statistics. We propose two novel approaches for estimating the targeted pollutant level at desired time-location pairs, extending also to areas of the city that are beyond the coverage of our mobile sensor network. The first is a log-linear regression model which is built over a virtual dependency graph based on land-use data. The second is a deep learning framework that automatically captures the dependencies of the data based on autoencoders. We have evaluated the two proposed approaches against three canonical modeling techniques considering metrics of coefficient of determination (R²), root mean square error (RMSE), and the fraction of predictions within a factor of two of observations (FAC2). Using more than 45 million real measurements in the models, the results show consistently superior performance in respect to the canonical techniques.",project-academic
10.1109/THMS.2017.2689178,2017-05-15,a,IEEE,guest editorialspecial issue on situation activity and goal awareness in cyber physical human machine systems," The papers in this special section focus on cyber-physical man-machine systems with particular emphasis on situation, activity, and goal awareness deployed in these systems. Recent advances in sensing technologies, the Internet of Things, pervasive computing, smart environments have transformed traditional embedded ICT systems into an ecosystem of interconnected and collaborating smart objects, devices, embedded systems, and most importantly humans. Such systems, often referred to as cyber-physical systems (CPS), are usually human user-driven or user-centered, and are aimed at providing people and businesses with a wide range of innovative applications and services. For example, a “smart home” can monitor and analyze the daily activities of its inhabitants, usually the elderly or individuals with disabilities, so that personalized context-aware assistance can be provided. A “smart city” can monitor, manage, and potentially control all basic city functionalities such as transport, energy supply, and waste collection, at a higher level of automation by collecting and harnessing sensor data across a large geographic expanse. In order to respond in real-time to an individual user’s speciﬁc needs in dynamic and complex situations, and to support ergonomics and user-friendliness through consideration of human factors such as privacy, dignity, and behavior characteristics, cyber-physical human–machine systems need to be aware of the physical environment and human participant behavior. This awareness enables effective and fast feedback loops between sensing and actuation, possibly with cognitive and learning capabilities adapting to participant preferences, capabilities, and the modality of human–machine interaction as well as dynamic situations.",project-academic
10.1109/JIOT.2018.2810808,2018-03-01,a,IEEE,unsupervised crowd assisted learning enabling location aware facilities," The accelerated evolution of Internet of Things (IoT) architectures and their incorporation in vehicles, buildings, or cities provide the ideal environment for the development and optimization of smart services. Under this light, positioning services that harvest location fingerprinting based on received signal strength indications (RSSIs) are widely popular due to the massive data generation that IoT settings provide. However, the labor-intensive and repetitive task of the radio map construction through offline RSSI fingerprint collection prevents such services from becoming standard equipment for future smart facilities. In this paper, we present a location-aware infrastructure that combines a broad sensing layer, edge computing, and centralized cloud federation support. Our setting gives rise to a sensing mechanism that enables in-facility crowdsourcing able to aid fingerprinting localization services. To that end, instead of extensive offline measurements, we use the facility occupants to gather unlabeled RSSI samples. To support the localization functionality, we develop a probabilistic cell-based model that is constructed by an unsupervised learning algorithm. Our black-box approach maintains the positioning accuracy regardless of changes in the underlying hardware or indoor environment. To evaluate our approach, we have deployed a multistorey facility testbed and performed an extensive real-subject trial to gather the unlabeled fingerprint dataset. The proposed unsupervised method yields average location classification accuracy of 0.8 that can rise up to 0.9 when a semi-supervised approach is considered. We also provide insights into the performance of the proposed infrastructure regarding mobility tracking, and under varying deployment scenarios.",project-academic
10.1109/JSTARS.2015.2442584,2016-05-01,a,IEEE,estimation of seismic vulnerability levels of urban structures with multisensor remote sensing," The ongoing global transformation of human habitats from rural villages to ever growing urban agglomerations induces unprecedented seismic risks in earthquake prone regions. To mitigate affiliated perils requires the seismic assessment of built environments. Numerous studies emphasize that remote sensing can play a valuable role in supporting the extraction of relevant features for preevent vulnerability analysis. However, the majority of approaches operate on building level. This induces the deployment of very high spatial resolution remote sensing data, which hampers, nowadays, utilization capabilities for larger areas due to data costs and processing requirements. In this paper, we alter the spatial scale of analysis and propose concepts and methods to estimate the seismic vulnerability level of homogeneous urban structures. A procedure is designed, which comprises four main steps dedicated to: 1) delineation of urban structures by means of a tailored unsupervised data segmentation procedure with scale optimization; 2) characterization of urban structures by a joint exploitation of multisensor data; 3) selection of most feasible features under consideration of None in situ None vulnerability information; and 4) estimation of seismic vulnerability levels of urban structures within a supervised learning framework. We render the prediction problem in three ways to address operational requirements that can evolve in real-life situations. 1) To discriminate two or more classes based on labeled samples of all classes present in the data under investigation, we use the framework of soft margin support vector machines ( C -SVM). 2) To consider situations, where solely labeled samples are available for the class(es) of interest and not for all classes present in the data, we deploy ensembles of None None $\nu$ None -one-class SVM ( None $\nu $ None -OC-SVM). and 3) To fit data with a higher statistical level of measurement (interval or ratio scale), we utilize a support vector regression (SVR) approach to estimate a regression function from the training samples. Experimental results are obtained for the earthquake-prone mega city Istanbul, Turkey. We use multispectral data from the RapidEye constellation, elevation measurements from the TanDEM-X mission, and spatiotemporal analyses based on data from the Landsat archive to characterize the urban environment. In addition, different None in situ None data sets are incorporated for Istanbul’s district Zeytinburnu and the residual settlement area of Istanbul. When estimating damage grades for Zeytinburnu with SVR, best models are characterized by mean absolute percentage errors less than 11%, and fairly strong goodness of fit ( None ${{R}} > {{0}}.{{75}}$ None ). When aiming to identify different types of urban structures for the remaining settlement area of Istanbul (i.e., urban structures determined by large industrial/commercial buildings and tall detached residential buildings, which can be considered here as highly and slightly vulnerable, respectively), results obtained with None None $C$ None -SVM show a distinctive increase of accuracy compared to results obtained with ensembles of None None $\nu$ None -OC-SVM. The latter were not able to exceed moderate agreements, with None None $\kappa$ None None statistics slightly above 0.45. Instead, None None $C$ None -SVM allowed obtaining None None $\kappa $ None None statistics expressing substantial and even excellent agreements ( None $\kappa > {{0}}.{{6}}$ None None up to None None $\kappa > {{0}}.{{8}}$ None ). Overall, analyzes provide very promising empirical evidence, which confirms the potential of remote sensing to support seismic vulnerability assessment.",project-academic
10.1016/J.JCLEPRO.2020.122722,2020-12-01,a,Elsevier,a pm2 5 concentration prediction model based on multi task deep learning for intensive air quality monitoring stations," Abstract None None With the deployment and real-time monitoring of a large number of micro air quality monitoring stations, new application scenarios have been provided for the research of air quality prediction methods based on artificial intelligence. Integrating deep learning with multi-task learning, this paper proposes a hybrid model for air quality prediction to leverage data from intensive air quality monitoring stations. The proposed model consists of a shared layer, a task-specific layer, and a multi-loss joint optimization module. It is tested on three monitoring stations located in three different districts of Lanzhou City, China, for PM2.5 concentration prediction. The results show that: (1) When the number of convolutional layers of convolutional neural network in the shared layer and the number of gated recurrent unit layers in the task-specific layer exist in two layers, model performs the best, and its predictability of the optimization algorithm with early-stopping will be significantly improved. (2) Using the proposed model to predict PM2.5 concentration on horizon None None None None t None + None 1 None None None , the mean absolute error and root mean square error are 4.54 and 7.96, respectively, indicating better performance in intensive air quality prediction than previous models based on simple hybridization. (3) The predictive performance on different stations is different, and the proposed model performs better than other models when there are large fluctuations and sudden changes in the data. Overall, the proposed model has good temporal stability and generalization ability and provides a new method for air quality prediction in intensive air quality monitoring scenarios.",project-academic
,2020-06-10,a,,wastenet waste classification at the edge for smart bins," Smart Bins have become popular in smart cities and campuses around the world. These bins have a compaction mechanism that increases the bins' capacity as well as automated real-time collection notifications. In this paper, we propose WasteNet, a waste classification model based on convolutional neural networks that can be deployed on a low power device at the edge of the network, such as a Jetson Nano. The problem of segregating waste is a big challenge for many countries around the world. Automated waste classification at the edge allows for fast intelligent decisions in smart bins without needing access to the cloud. Waste is classified into six categories: paper, cardboard, glass, metal, plastic and other. Our model achieves a 97\% prediction accuracy on the test dataset. This level of classification accuracy will help to alleviate some common smart bin problems, such as recycling contamination, where different types of waste become mixed with recycling waste causing the bin to be contaminated. It also makes the bins more user friendly as citizens do not have to worry about disposing their rubbish in the correct bin as the smart bin will be able to make the decision for them.",project-academic
10.1016/J.JPDC.2020.06.015,2020-07-07,a,Academic Press,proof of witness presence blockchain consensus for augmented democracy in smart cities," Abstract None None Smart Cities evolve into complex and pervasive urban environments with a citizens’ mandate to meet sustainable development goals. Repositioning democratic values of citizens’ choices in these complex ecosystems has turned out to be imperative in an era of social media filter bubbles, fake news and opportunities for manipulating electoral results with such means. This paper introduces a new paradigm of augmented democracy that promises actively engaging citizens in a more informed decision-making augmented into public urban space. The proposed concept is inspired by a digital revive of the Ancient Agora of Athens, an arena of public discourse, a Polis where citizens assemble to actively deliberate and collectively decide about public matters. The core contribution of the proposed paradigm is the concept of proving witness presence: making decision-making subject of providing secure evidence and testifying for choices made in the physical space. This paper shows how the challenge of proving witness presence can be tackled with blockchain consensus to empower citizens’ trust and overcome security vulnerabilities of GPS localization. Moreover, a novel platform for collective decision-making and crowd-sensing in urban space is introduced: Smart Agora. It is shown how real-time collective measurements over citizens’ choices can be made in a fully decentralized and privacy-preserving way. Witness presence is tested by deploying a decentralized system for crowd-sensing the sustainable use of transport means. Furthermore, witness presence of cycling risk is validated using official accident data from public authorities compared against wisdom of the crowd. The paramount role of dynamic consensus, self-governance and ethically aligned artificial intelligence in the augmented democracy paradigm is outlined.",project-academic
,2019-06-30,a,,proof of witness presence blockchain consensus for augmented democracy in smart cities," Smart Cities evolve into complex and pervasive urban environments with a citizens' mandate to meet sustainable development goals. Repositioning democratic values of citizens' choices in these complex ecosystems has turned out to be imperative in an era of social media filter bubbles, fake news and opportunities for manipulating electoral results with such means. This paper introduces a new paradigm of augmented democracy that promises actively engaging citizens in a more informed decision-making augmented into public urban space. The proposed concept is inspired by a digital revive of the Ancient Agora of Athens, an arena of public discourse, a Polis where citizens assemble to actively deliberate and collectively decide about public matters. The core contribution of the proposed paradigm is the concept of proving witness presence: making decision-making subject of providing secure evidence and testifying for choices made in the physical space. This paper shows how the challenge of proving witness presence can be tackled with blockchain consensus to empower citizens' trust and overcome security vulnerabilities of GPS localization. Moreover, a novel platform for collective decision-making and crowd-sensing in urban space is introduced: Smart Agora. It is shown how real-time collective measurements over citizens' choices can be made in a fully decentralized and privacy-preserving way. Witness presence is tested by deploying a decentralized system for crowd-sensing the sustainable use of transport means. Furthermore, witness presence of cycling risk is validated using official accident data from public authorities, which are compared against wisdom of the crowd. The paramount role of dynamic consensus, self-governance and ethically aligned artificial intelligence in the augmented democracy paradigm is outlined.",project-academic
10.1016/J.FUTURE.2017.08.009,2019-03-01,a,North-Holland,agra ai augmented geographic routing approach for iot based incident supporting applications," Abstract None None Applications that cater to the needs of disaster incident response generate large amount of data and demand large computational resource access. Such datasets are usually collected in real-time at the incident scenes using different Internet of Things (IoT) devices. Hierarchical clouds, i.e., core and edge clouds, can help these applications’ real-time data orchestration challenges as well as with their IoT operations scalability, reliability and stability by overcoming infrastructure limitations at the ad-hoc wireless network edge. Routing is a crucial infrastructure management orchestration mechanism for such systems. Current geographic routing or greedy forwarding approaches designed for early wireless ad-hoc networks lack efficient solutions for disaster incident-supporting applications, given the high-speed and low-latency data delivery that edge cloud gateways impose. In this paper, we present a novel Artificial Intelligent (AI)-augmented geographic routing approach, that relies on an area knowledge obtained from the satellite imagery (available at the edge cloud) by applying deep learning. In particular, we propose a stateless greedy forwarding that uses such an environment learning to proactively avoid the local minimum problem by diverting traffic with an algorithm that emulates electrostatic repulsive forces. In our theoretical analysis, we show that our Greedy Forwarding achieves in the worst case a None None None 3 None . None 291 None None None path stretch approximation bound with respect to the shortest path, without assuming presence of symmetrical links or unit disk graphs. We evaluate our approach with both numerical and event-driven simulations, and we establish the practicality of our approach in a real incident-supporting hierarchical cloud deployment to demonstrate improvement of application level throughput due to a reduced path stretch under severe node failures and high mobility challenges of disaster response scenarios.",project-academic
10.23919/ONDM48393.2020.9133004,2020-05-18,p,IEEE,network slicing automation challenges and benefits," Network slicing is a technique widely used in 5G networks where multiple logical networks (i.e., slices) run over a single shared physical infrastructure. Each slice may realize one or multiple services, whose specific requirements are negotiated beforehand and regulated through Service Level Agreements (SLAs). In Beyond 5G (B5G) networks it is envisioned that slices should be created, deployed, and managed in an automated fashion (i.e., without human intervention) irrespective of the technological and administrative domains over which a slice may span. Achieving this vision requires a combination of novel physical layer technologies, artificial intelligence tools, standard interfaces, network function virtualization, and software-defined networking principles. This paper provides an overview of the challenges facing network slicing automation with a focus on transport networks. Results from a selected group of use cases show the benefits of applying conventional optimization tools and machine-learning-based techniques while addressing some slicing design and provisioning problems.",project-academic
10.1109/ACCESS.2020.3003020,2020-06-17,a,Institute of Electrical and Electronics Engineers (IEEE),a review on application of blockchain in 5g and beyond networks taxonomy field trials challenges and opportunities," Until now, every evolution of communication standard was driven by the need for providing
high-speed connectivity to the end-user. However, 5G marks a radical shift from this focus as 5G and
beyond networks are being designed to be future-proof by catering to diverse requirements of several use
cases. These requirements include Ultra-Reliable Low Latency Communications, Massive Machine-Type
Communications and Enhanced Mobile Broadband. To realize such features in 5G and beyond, there is a
need to rethink how current cellular networks are deployed because designing new radio access technologies
and utilizing the new spectrum are not enough. Several technologies, such as software-defined networking,
network function virtualization, machine learning and cloud computing, are being integrated into the 5G
networks to fulfil the need for diverse requirements. These technologies, however, give rise to several
challenges associated with decentralization, transparency, interoperability, privacy and security. To address
these issues, Blockchain has emerged as a potential solution due to its capabilities such as transparency, data
encryption, auditability, immutability and distributed architecture. In this paper, we review the state-of-art
application of Blockchain in 5G network and explore how it can facilitate enabling technologies of 5G and
beyond to enable various services at the front-haul, edge and the core. Based on the review, we present a
taxonomy of Blockchain application in 5G networks and discuss several issues that can be solved using
Blockchain integration. We then present various field-trials and Proof of concept that are using Blockchain
to address the challenges faced in the current 5G deployment. Finally, we discuss various challenges that
need to be addressed to realize the full potential of Blockchain in beyond 5G networks. The survey presents
a broad range of ideas related to Blockchain integration in 5G and beyond networks that address issues
such as interoperability, security, mobility, resource allocation, resource sharing and management, energy
efficiency and other desirable features.",project-academic
10.1109/IMCOM48794.2020.9001672,2020-01-01,p,IEEE,next point of attachment selection based on long short term memory model in wireless networks," Existing mobility management systems in cellular networks are ill-equipped to support Ultra-Reliable and Low Latency Communication (URLLC) requirement of next generation prevalent and real time services in dense network deployments due to their reactive approach. Proactive approach is one way to meet the URLLC requirement of these services, where resource assignment and control signaling is completed before the actual user mobility. Successful execution of proactive mobility requires accurate prediction of user next Point of Attachment (PoA) and precise estimation of user mobility instant. This paper adopts supervised deep learning approach to predict the next PoA of user. In particular, a Long Short-Term Memory (LSTM) model is developed for this purpose, which exploits the temporal characteristics of the data. We discuss different design choices of our LSTM model and show their effects on the prediction accuracy. Evaluation results reveal that proper data preprocessing and time-step increment significantly affects the prediction accuracy. The highest accuracy achieved by our model is 91% with shuffled data and stacked LSTM.",project-academic
,2015-01-01,a,,incentivizing efficiency in societal scale cyber physical systems," In the modernization of infrastructure systems such as energy, transportation, and healthcare systems we are seeing the convergence of three research domains: Cyber–Physical Systems (CPS), Big Data, and the Internet of Things (IoT). Indeed, new CPS technologies are are being deployed to create large sensor-actuator networks which produce massive quantities of data often in real-time which is, in turn, being used to inform everyday decision-making of the entities that engage with these large-scale infrastructure systems. As a consequence, such systems are quickly evolving into societal-scale cyber-physical systems.The result of this increasing connectivity and interdependence is two–fold: more and more data is being collected, transmitted, and stored, and more and more actuation modalities are available, allowing new ways to influence the behavior of infrastructure systems. These new and pervasive sensing/actuation modalities present new opportunities for improving efficiency, yet they expose novel vulnerabilities. In energy CPS, for instance, smart metering technologies increase the availability of streaming data thereby enabling monetization of energy savings. Such savings can be realized by employing novel machine learning algorithms to customize offerings to consumers. On the other hand, the availability of this fine-grained consumer/system data and the increased number of access points to the broader system expose new privacy and security risks. Hence, there is a inherent efficiency-vulnerability tradeoff. This tradeoff is becoming more pronounced due to greater dependence on CPS technologies and the push towards more human-centric operations, i.e. integration of human decision-making and preferences into the closed-loop behavior of the system. Beginning with the problem of modeling the non-cooperative agents that interact with these large-scale sociotechnical systems and thus, compete over scarce resources, we analyze the of the outcome of their strategic interactions. In particular, we create a characterization of Nash equilibria—termed differential Nash equilibria—in games on non-convex strategy spaces that is amenable to computation. We show that such non-degenerate differential Nash equilibria are structurally stable and generic thereby robust to small modeling errors and measurement noise. Introducing a planner tasked with coordinating these decision-makers, we leverage this characterization in the construction of a utility learning and incentive design algorithm. We provide convergence results in both the case where agents play according to Nash and where they play using a myopic update rule.Narrowing our focus to the demand-side of the smart grid, we consider that the planner will capitalize on new sensing/actuation modalities in the design incentives thereby exposing the efficiency-vulnerability tradeoff. We consider privacy risks introduced by smart metering technologies that produce streaming energy consumption data. On one hand the data has utility in the sense that it can help improve operations, yet on the other it exposes the consumer and the power company to greater privacy risk. We propose a solution that combines economic and statistics tools, i.e. privacy-aware service contracts in which service is differentiated according to privacy and consumers select based on their needs and wallet. We argue that the power company has an incentive to invest in security or purchase insurance because of inefficiencies that arise due to information asymmetries and we design insurance contracts accordingly. We provide a number of qualitative insights that have the potential to be useful for informing policy and regulations in the energy ecosystem. Finally, we conclude with an overview of the contributions and a discussion of futureresearch directions for the near and far terms.The contributions are the first steps towards an emerging systems theory of societal-scale cyber-physical systems in which there are many tightlycoupled human-CPS decision-making loops and socioeconomic factors intricately woven into the fabric.",project-academic
10.1109/CAMAD.2018.8515001,2018-09-01,p,IEEE,uncertainty management for wearable iot wristband sensors using laplacian based matrix completion," Contemporary sensing devices provide reliable mechanisms for continuous process monitoring, accommodating use cases related to mHealth and smart mobility, by generating real-time data streams of numerous physiological and vital parameters. Such data streams can be later utilized by machine learning algorithms and decision support systems to predict critical clinical states and motivate users to adopt behaviours that improve the quality of their life and the society as a whole. However, in many cases, even when deployed over highly sophisticated, cutting-edge network infrastructure and deployment paradigms, data may exhibit missing values and non-uniformities due to various reasons, including device malfunction, deliberate data reduction for efficient processing, or data loss due to sensing and communication failures. This work proposes a novel approach to deal with missing entries in heart rate measurements. Benefiting from the low-rank property of the generated data matrices and the proximity of neighbouring measurements, we provide a novel method that combines classical matrix completion approaches with weighted Laplacian interpolation offering high reconstruction accuracy at fast execution times. Extensive evaluation studies carried out with real measurements show that the proposed methods could be effectively deployed by modern wristband-cloud computing systems increasing the robustness, the reliability and the energy efficiency of these systems.",project-academic
10.1007/S41650-017-0006-X,2017-04-12,a,Springer Singapore,analysis and prediction of 100 km scale atmospheric duct interference in td lte networks," Atmospheric ducts are horizontal layers that occur under certain weather conditions in the lower atmosphere. Radio signals guided in atmospheric ducts tend to experience less attenuation and spread much farther, i.e, hundreds of kilometers. In a large-scale deployed TD-LTE (Time Division Long Term Evolution) network, atmospheric ducts cause faraway downlink wireless signals to propagate beyond the designed protection distance and interfere with local uplink signals, thus resulting in a large outage probability. In this paper, we analyze the characteristics of ADI atmospheric duct interference (Atmospheric Duct Interference) by the use of real network-side big data from the current operated TD-LTE network owned by China Mobile. The analysis results yield the time varying and directional characteristics of ADI. In addition, we proposed an SVM (Support Vector Machine)-classifier based spacial prediction method of ADI by machine learning over combination of real network-side big data and real meteorological data. Furthermore, an implementation of ADMM (Alternating Direction Methods of Multipliers) framework is proposed to implement a distributed SVM prediction scheme, which reduces data exchange among different regions/cities, maintains similar prediction accuracy and is thus of a more practical use to operators.",project-academic
10.1109/ACCESS.2020.2991959,2020-05-06,a,Institute of Electrical and Electronics Engineers (IEEE),syntheticnet a 3gpp compliant simulator for ai enabled 5g and beyond," The rapid evolution of cellular system design towards 5G and beyond gives rise to a need for investigation of the new features, design proposals and solutions in realistic settings for various deployments and use case scenarios. While many system level simulators for 4G and 5G exist today, there is particularly a dire need for a 3GPP compliant system level holistic and realistic simulator that can support evaluation of the plethora of AI based network automation solutions being proposed in literature. In this paper we present such a simulator developed at AI4networks Lab, called SyntheticNET. To the best of authors' knowledge, SyntheticNET is the very first python-based simulator that fully conforms to 3GPP 5G standard release 15 and is upgradable to future releases. The key distinguishing features of SyntheticNET compared to existing simulators include: 1) a modular structure to facilitate cross validation and upgrading to future releases; 2) flexible propagation modeling using measurement based, ray tracing based or AI based propagation modeling; 3) ability to import data sheet based on measurement based realistic vendor specific base station features such as antenna and energy consumption pattern; 4) support for 5G standard based adaptive numerology; 5) realistic and user-specific mobility patterns that are yielded from actual geographical maps; 6) detailed handover (HO) process implementation; and 7) incorporation of database aided edge computing. Another key feature of the SyntheticNET is the ease with which it can be used to test AI based network automation solutions. Being the first python based 5G simulator, this ease, in part stems for SyntheticNET's built-in capability to process and analyze large data sets and integrated access to Machine Learning libraries. Thus, SyntheticNET simulator offers a powerful platform for academia and industry alike to investigate not only new solutions for optimally designing, deploying and operating existing and emerging cellular networks but also for enabling AI empowered deep automation in the future.",project-academic
,2019-06-12,a,,deep reinforcement learning for unmanned aerial vehicle assisted vehicular networks in smart cities," Unmanned aerial vehicles (UAVs) are envisioned to complement the 5G communication infrastructure in future smart cities. Hot spots easily appear in road intersections, where effective communication among vehicles is challenging. UAVs may serve as relays with the advantages of low price, easy deployment, line-of-sight links, and flexible mobility. In this paper, we study a UAV-assisted vehicular network where the UAV jointly adjusts its transmission control (power and channel) and 3D flight to maximize the total throughput. First, we formulate a Markov decision process (MDP) problem by modeling the mobility of the UAV/vehicles and the state transitions. Secondly, we solve the target problem using a deep reinforcement learning method under unknown or unmeasurable environment variables especially in 5G, namely, the deep deterministic policy gradient (DDPG), and propose three solutions with different control objectives. Environment variables are unknown and unmeasurable, therefore, we use a deep reinforcement learning method. Moreover, considering the energy consumption of 3D flight, we extend the proposed solutions to maximize the total throughput per energy unit by encouraging or discouraging the UAV's mobility. To achieve this goal, the DDPG framework is modified. Thirdly, in a simplified model with small state space and action space, we verify the optimality of proposed algorithms. Comparing with two baseline schemes, we demonstrate the effectiveness of proposed algorithms in a realistic model.",project-academic
10.1145/3363347.3363360,2019-11-10,p,ACM,collaborative intelligent cross camera video analytics at edge opportunities and challenges," Nowadays, video cameras are deployed in large scale for spatial monitoring of physical places (e.g., surveillance systems in the context of smart cities). The massive camera deployment, however, presents new challenges for analyzing the enormous data, as the cost of high computational overhead of sophisticated deep learning techniques imposes a prohibitive overhead, in terms of energy consumption and processing throughput, on such resource-constrained edge devices. To address these limitations, this paper envisions a collaborative intelligent cross-camera video analytics paradigm at the network edge in which camera nodes adjust their pipelines (e.g., inference) to incorporate correlated observations and shared knowledge from other nodes' contents. By harassing redundant spatio-temporal to reduce the size of the inference search space in one hand, and intelligent collaboration between video nodes on the other, we discuss how such collaborative paradigm can considerably improve accuracy, reduce latency and decrease communication bandwidth compared to non-collaborative baselines. This paper also describes major opportunities and challenges in realizing such a paradigm.",project-academic
10.1007/S11042-017-4460-0,2017-03-08,a,Springer US,predicting the online performance of video service providers on the internet," Video services on the Internet are not able to offer consistent and assured performance to users or third-party applications. Measuring levels of performance over time is difficult, and obtaining accurate measures in real time is problematic; thus, reactive measures to address loss of performance are also problematic. The ability to predict service performance can be viewed as an important added-value, one that can help users or third-part applications select the proper online service provider. With this aim in view, we have designed a measurement system and deployed it in eleven provinces and cities in China to monitor two popular websites, Youku and Tudou. The analysis indicates that the performance trend of these two service providers followed daily changing patterns, such as rush hour traffic and lower service workloads at midnight; this is consistent with user behaviors. It was also confirmed that the future performance was related to the historical records. Based on these findings, we have decided to investigate the use of modified time series models to forecast the performance of such video services. Meanwhile, some machine learning models are implemented and compared as baseline models, such as Artificial Neural Network, Support Vector Machine, and Decision Tree. In addition, a hybrid model, which is generated by combining different machine learning models, is also studied as the baseline. An investigation shows that time series models are much more suitable to this prediction problem than baseline models in most situations. To alleviate the data sparseness problem in training the predictor, a new predictor that combines different information sources is proposed, thus improving prediction precision. Furthermore, the predictor is quite stable, and we have discovered that the average performance estimation is more accurate if the model is updated within 2–3 days, which is useful in some applications, e.g., video source analysis and recommendation systems.",project-academic
10.1109/TVT.2019.2937825,2019-08-28,a,IEEE,edge assisted vehicle mobility prediction to support v2x communications," Vehicle-to-everything (V2X) communications have a great potential of enabling future intelligent vehicle applications, and exploiting vehicle mobility is of great importance in designing efficient V2X protocols and applications. Thus, this paper proposes a novel edge-assisted algorithm that makes use of the resources in both cloud and edge sides of vehicular networks to predict vehicle mobility. The proposed algorithm adopts a hybrid architecture of convolutional and recurrent neural networks, and enables computationally efficient transfer learning in each vehicle to generate its customized mobility prediction model. Extensive evaluations have been conducted by using a real taxi mobility data set that is obtained from a testbed deployed in Tokyo, Japan. The results have validated that, compared with other state-of-art algorithms, our proposal improves the prediction F1 score of vehicle mobility by more than 30%, especially for those vehicles that own a strong individual mobility preference.",project-academic
10.1109/GLOBECOM38437.2019.9013508,2019-12-01,p,IEEE,deep learning based speed profiling for mobile users in 5g cellular networks," Future mobile networks promise to be more intelligent and to guarantee a better service for users. This intelligence can be highly accentuated by cognition of mobile usersâ€™ behavior and conditions. The user speed is an important element of user profile. We are interested in real time speed profiling by detecting speed range of an active user. We refer to this as Mobile Speed Profiling (MSP) of users. Indeed, performing MSP can notably improve the self-adaptation and self-optimization capabilities of these networks. It can help mobile network in resource management and handover management. This paper introduces a Deep Learning based solution to automatically construct the MSP of a mobile user. We empirically evaluate the effectiveness of our approach using real-time and highly representative radio data that best captures the real daily movements of users. This data includes ground truth information and the whole data set has been gathered massively from many diversified mobility situations. Results show that the profiling of UEâ€™s speed with fine granularity or on multiple ranges can be achieved with high accuracy on real data measured in heterogeneous deployments for 5G networks",project-academic
10.3390/S20041176,2020-02-20,a,Multidisciplinary Digital Publishing Institute,resource usage and performance trade offs for machine learning models in smart environments," The application of artificial intelligence enhances the ability of sensor and networking technologies to realize smart systems that sense, monitor and automatically control our everyday environments. Intelligent systems and applications often automate decisions based on the outcome of certain machine learning models. They collaborate at an ever increasing scale, ranging from smart homes and smart factories to smart cities. The best performing machine learning model, its architecture and parameters for a given task are ideally automatically determined through a hyperparameter tuning process. At the same time, edge computing is an emerging distributed computing paradigm that aims to bring computation and data storage closer to the location where they are needed to save network bandwidth or reduce the latency of requests. The challenge we address in this work is that hyperparameter tuning does not take into consideration resource trade-offs when selecting the best model for deployment in smart environments. The most accurate model might be prohibitively expensive to computationally evaluate on a resource constrained node at the edge of the network. We propose a multi-objective optimization solution to find acceptable trade-offs between model accuracy and resource consumption to enable the deployment of machine learning models in resource constrained smart environments. We demonstrate the feasibility of our approach by means of an anomaly detection use case. Additionally, we evaluate the extent that transfer learning techniques can be applied to reduce the amount of training required by reusing previous models, parameters and trade-off points from similar settings.",project-academic
10.1109/GEOINFORMATICS.2009.5293444,2009-10-23,p,IEEE Computer Society,a practical route guidance approach based on historical and real time traffic effects," Implementing convenient traveling information service is a crucial task for deploying intelligent transportation system applications and location-based services. Traditional traveling information service systems, such as car navigation systems or web maps, only provide relatively static information which doesn't truly reflect the dynamic changes of traffic situation, and result in very limited practical use. Although there have emerged some car navigation products and other applications involving dynamic traffic information, considering the rapid change of city traffic situation, these applications still face practical difficulties for all the information received real-timely will get outdated within a few minutes, which makes the so called dynamic applications basically time-slice limited static ones. Aiming at such a problem, a short-term traffic prediction approach and a consequent real-time route guidance process are presented in this paper which integrates historical traffic based statistical reasoning, real-time traffic and events processing, with a BP neural network based analytical model, to forecast the situation and evaluate the influence of traffic during the traveling process. Then a collaboration working framework is set forward to implement dynamic route guidance, with the combination of a GIS server, a traffic forecasting server and a database management system. The traffic forecasting server, integrating with historical statistics reckoning continuously receives real-time traffic information obtained from floating vehicles, traffic events described in natural language, and achieves short-term forecasting results for the whole road networks, then fed the results back into the database management system and GIS server, so that a time-dependant optimal routing can be conducted through a dynamic least traveling time algorithm developed in this study. A prototype navigation system fulfilling the above aspects has been developed and the dynamic route choice approach demonstrated on road networks in the downtown area of Beijing city. The approach presented in this paper is argued to provide a practical solution for real-time public traveling information service and dynamic web maps.",project-academic
10.1109/MDM48529.2020.00029,2020-06-01,p,Institute of Electrical and Electronics Engineers Inc.,stad spatio temporal adjustment of traffic oblivious travel time estimation," Travel time estimation is an important component in modern transportation applications. The state of the art techniques for travel time estimation use GPS traces to learn the weights of a road network, often modeled as a directed graph, then apply Dijkstra-like algorithms to find shortest paths. Travel time is then computed as the sum of edge weights on the returned path. In order to enable time-dependency, existing systems compute multiple weighted graphs corresponding to different time windows. These graphs are often optimized offline before they are deployed into production routing engines, causing a serious engineering overhead. In this paper, we present STAD, a system that adjusts – on the fly – travel time estimates for any trip request expressed in the form of origin, destination, and departure time. STAD uses machine learning and sparse trips data to learn the imperfections of any basic routing engine, before it turns it into a full-fledged time-dependent system capable of adjusting travel times to real traffic conditions in a city. STAD leverages the spatio-temporal properties of traffic by combining spatial features such as departing and destination geographic zones with temporal features such as departing time and day to significantly improve the travel time estimates of the basic routing engine. Experiments on real trip datasets from Doha, New York City, and Porto show a reduction in median absolute errors of 14% in the first two cities and 29% in the latter. We also show that STAD performs better than different commercial and research baselines in all three cities.",project-academic
,2020-06-08,a,,stad spatio temporal adjustment of traffic oblivious travel time estimation," Travel time estimation is an important component in modern transportation applications. The state of the art techniques for travel time estimation use GPS traces to learn the weights of a road network, often modeled as a directed graph, then apply Dijkstra-like algorithms to find shortest paths. Travel time is then computed as the sum of edge weights on the returned path. In order to enable time-dependency, existing systems compute multiple weighted graphs corresponding to different time windows. These graphs are often optimized offline before they are deployed into production routing engines, causing a serious engineering overhead. In this paper, we present STAD, a system that adjusts - on the fly - travel time estimates for any trip request expressed in the form of origin, destination, and departure time. STAD uses machine learning and sparse trips data to learn the imperfections of any basic routing engine, before it turns it into a full-fledged time-dependent system capable of adjusting travel times to real traffic conditions in a city. STAD leverages the spatio-temporal properties of traffic by combining spatial features such as departing and destination geographic zones with temporal features such as departing time and day to significantly improve the travel time estimates of the basic routing engine. Experiments on real trip datasets from Doha, New York City, and Porto show a reduction in median absolute errors of 14% in the first two cities and 29% in the latter. We also show that STAD performs better than different commercial and research baselines in all three cities.",project-academic
10.1007/978-3-319-07617-1_49,2014-06-11,p,"Springer, Cham",hybrid intelligent model to predict the soc of a lfp power cell type," Nowadays, batteries have two main purposes: to enable mobility and to buffer intermitent power generation facilities. Due to their electromechaminal nature, several tests are made to check battery performance, and it is very helpful to know a priori how it works in each case. Batteries, in general terms, have a complex behavior. This study describes a hybrid intelligent model aimed to predict the State Of Charge of a LFP (Lithium Iron Phosphate - LiFePO4) power cell type, deploying the results of a Capacity Confirmation Test of a battery. A large set of operating points is obtained from a real system to create the dataset for the operation range of the power cell. Clusters of the different behavior zones have been obtained to achieve the final solution. Several simple regression methods have been carried out for each cluster. Polynomial Regression, Artificial Neural Networks and Ensemble Regression were the combined techniques to develop the hybrid intelligent model proposed. The novel model allows achieving good results in all the operating range.",project-academic
,2019-01-01,p,,deepfloat resource efficient dynamic management of vehicular floating content," Opportunistic communications are expected to play a crucial role in enabling context-aware vehicular services. A widely investigated opportunistic communication paradigm for storing a piece of content probabilistically in a geographical area is Floating Content (FC). A key issue in the practical deployment of FC is how to tune content replication and caching in a way which achieves a target performance (in terms of the mean fraction of users possessing the content in a given region of space) while minimizing the use of bandwidth and host memory. Fully distributed, distance-based approaches prove highly inefficient, and may not meet the performance target, while centralized, model-based approaches do not perform well in realistic, inhomogeneous settings. In this work, we present a data-driven centralized approach to resource-efficient, QoS-aware dynamic management of FC. We propose a Deep Learning strategy, which employs a Convolutional Neural Network (CNN) to capture the relationships between patterns of users mobility, of content diffusion and replication, and FC performance in terms of resource utilization and of content availability within a given area. Numerical evaluations show the effectiveness of our approach in deriving strategies which efficiently modulate the FC operation in space, and which effectively adapt to mobility pattern changes over time.",project-academic
10.1109/ITC31.2019.00015,2019-08-27,p,IEEE,deepfloat resource efficient dynamic management of vehicular floating content," Opportunistic communications are expected to play a crucial role in enabling context-aware vehicular services. A widely investigated opportunistic communication paradigm for storing a piece of content probabilistically in a geographical area is Floating Content (FC). A key issue in the practical deployment of FC is how to tune content replication and caching in a way which achieves a target performance (in terms of the mean fraction of users possessing the content in a given region of space) while minimizing the use of bandwidth and host memory. Fully distributed, distance-based approaches prove highly inefficient, and may not meet the performance target, while centralized, model-based approaches do not perform well in realistic, inhomogeneous settings. In this work, we present a data-driven centralized approach to resource-efficient, QoS-aware dynamic management of FC. We propose a Deep Learning strategy, which employs a Convolutional Neural Network (CNN) to capture the relationships between patterns of users mobility, of content diffusion and replication, and FC performance in terms of resource utilization and of content availability within a given area. Numerical evaluations show the effectiveness of our approach in deriving strategies which efficiently modulate the FC operation in space, and which effectively adapt to mobility pattern changes over time.",project-academic
10.1117/12.2028340,2013-10-22,p,International Society for Optics and Photonics,multi modal target detection for autonomous wide area search and surveillance," Generalised wide are search and surveillance is a common-place tasking for multi-sensory equipped autonomous systems. Here we present on a key supporting topic to this task - the automatic interpretation, fusion and detected target reporting from multi-modal sensor information received from multiple autonomous platforms deployed for wide-area environment search. We detail the realization of a real-time methodology for the automated detection of people and vehicles using combined visible-band (EO), thermal-band (IR) and radar sensing from a deployed network of multiple autonomous platforms (ground and aerial). This facilities real-time target detection, reported with varying levels of confidence, using information from both multiple sensors and multiple sensor platforms to provide environment-wide situational awareness. A range of automatic classification approaches are proposed, driven by underlying machine learning techniques, that facilitate the automatic detection of either target type with cross-modal target confirmation. Extended results are presented that show both the detection of people and vehicles under varying conditions in both isolated rural and cluttered urban environments with minimal false positive detection. Performance evaluation is presented at an episodic level with individual classifiers optimized for maximal each object of interest (vehicle/person) detection over a given search path/pattern of the environment, across all sensors and modalities, rather than on a per sensor sample basis. Episodic target detection, evaluated over a number of wide-area environment search and reporting tasks, generally exceeds 90%+ for the targets considered here.",project-academic
10.1109/IWCMC48107.2020.9148208,2020-06-15,p,IEEE,inferring quality of experience for adaptive video streaming over https and quic," Nowadays, Internet traffic encryption is rapidly increasing due to privacy and security concerns. This is because of the massive usage of end-to-end security protocols over Internet such as HTTPS and QUIC. The encryption trend will continue to rapidly increase in the future, and this trend concerns video streaming applications as well. Network providers face a serious challenge in managing their networks due to such widespread deployment of end-to-end security protocols. These operators need to have a clear visibility into traffic on their networks to monitor and manage both quality of experience (QoE)-and-service (QoS) impairments in popular video streaming services, in the most effective and efficient manner. Moreover, so many factors that influence QoE need to be taken care of to get an acceptable user experience. Most of the existing solutions use the deep packet inspection to infer these factors from the encrypted traffic. However, these solutions are inefficient, most of the time, leading to low QoE inference accuracy. To bridge this gap, we propose a machine-learning based solution that leverages a random forest classifier for a better QoE inference accuracy. The proposed solution uses network-and-transport layer information to infer QoE factors such as startup delay and stall events. It helps the network providers to react quickly and in real time for any impairments in the QoE of the encrypted video traffic. We evaluate our solution using an HTTP adaptive streaming service (YouTube) that uses HTTPS and QUIC protocols. Our experimental results show that our solution achieves up to 91.1% classification accuracy for HTTPS and up to 87.3% for QUIC.",project-academic
10.1016/J.SCS.2021.102945,2021-08-01,a,Elsevier,effective task scheduling algorithm with deep learning for internet of health things ioht in sustainable smart cities," Abstract None None In the recent years, important key factor for urban planning is to analyze the sustainability and its functionality towards smart cities. Presently, many researchers employ the conservative machine learning based analysis but those are not appropriate for IoT based health data analysis because of their physical feature extraction and low accuracy. In this paper, we propose remote health monitoring and data analysis by integrating IoT and deep learning concepts. We proposed novel IoT based FoG assisted cloud network architecture that accumulates real-time health care data from patients via several medical IoT sensor networks, these data are analyzed using a deep learning algorithm deployed at Fog based Healthcare Platform. Furthermore, the proposed methodology is applied to the sustainable smart cities to evaluate the process for real-time. The proposed framework not only analyses the healthcare data but also provides immediate relief measures to the patient facing critical conditions and needs immediate consultancy of doctor. Performance is measure in terms of accuracy, precision and sensitivity of the proposed DHNN with task scheduling algorithm and it is obtained 97.6%, 97.9%, and 94.9%. While accuracy, precision and sensitivity for deep CNN is 96.5%, 97.5% and 94% and for Deep auto-encoder is 92%, 91%, and 82.5%.",project-academic
,2019-09-10,,,subway station passenger flow prediction method based on big data," The invention discloses a subway station passenger flow prediction method based on big data. The method mainly comprises the following steps of preprocessing and analyzing the historical card swipingdata of a subway station; constructing the features, selecting the features, establishing and fusing a plurality of machine learning models, and predicting the future passenger flow volume of the subway station according to the historical card swiping data of a subway station so as to help to realize more reasonable travel route selection, avoid the traffic jam, deploy the site security measures in advance and the like, and finally realize the assistance of future urban safe travel by using the technologies, such as big data, artificial intelligence, etc.",project-academic
10.1016/J.JTH.2021.101032,2021-06-01,a,Elsevier BV,reference free video to real distance approximation based urban social distancing analytics amid covid 19 pandemic," Abstract None None Introduction None The rapidly evolving COVID-19 pandemic has dramatically reshaped urban travel patterns. In this research, we explore the relationship between “social distancing,” a concept that has gained worldwide familiarity, and urban mobility during the pandemic. Understanding social distancing behavior will allow urban planners and engineers to better understand the new norm of urban mobility amid the pandemic, and what patterns might hold for individual mobility post-pandemic or in the event of a future pandemic. None None None Methods None There are still few efforts to obtain precise information on social distancing patterns of pedestrians in urban environments. This is largely attributed to numerous burdens in safely deploying any effective field data collection approaches during the crisis. This paper aims to fill that gap by developing a data-driven analytical framework that leverages existing public video data sources and advanced computer vision techniques to monitor the evolution of social distancing patterns in urban areas. Specifically, the proposed framework develops a deep-learning approach with a pre-trained convolutional neural network to mine the massive amount of public video data captured in urban areas. Real-time traffic camera data collected in New York City (NYC) was used as a case study to demonstrate the feasibility and validity of using the proposed approach to analyze pedestrian social distancing patterns. None None None Results None The results show that microscopic pedestrian social distancing patterns can be quantified by using a generalized real-distance approximation method. The estimated distance between individuals can be compared to social distancing guidelines to evaluate policy compliance and effectiveness during a pandemic. Quantifying social distancing adherence will provide decision-makers with a better understanding of prevailing social contact challenges. It also provides insights into the development of response strategies and plans for phased reopening for similar future scenarios.",project-academic
10.1109/ICASSP.2014.6854752,2014-05-04,p,IEEE,compressed prediction of large scale urban traffic," Traffic prediction lies at the core of many intelligent transport systems (ITS). Commonly deployed prediction methods such as support vector regression and neural networks achieve good performance by explicitly predicting the traffic variables (e.g., traffic speed or volume) at each road segment in the network. For large traffic networks, predicting traffic variable at each road segment may be unwieldy, especially in the setting of real-time prediction. To tackle this problem, we propose an alternative approach in this paper. We first generate low-dimensional representation of the network, leveraging on the column-based (CX) decomposition of matrices. The low-dimensional model represents the large network in terms of a small subset of road segments. The future state of the low-dimensional network is predicted by standard procedures, i.e., support vector regression. The future state of the entire network is then inferred by extrapolating the predictions of the subnetwork, using the CX decomposition. Numerical results for a large-scale road network in Singapore demonstrate the efficiency and accuracy of the proposed algorithm.",project-academic
10.5281/ZENODO.850590,2014-01-01,p,National and Kapodistrian University of Athens,citygram one one year later," Citygram is a multidisciplinary project that seeks to measure, stream, archive, analyze, and visualize spatiotemporal soundscapes. The infrastructure is built on a cyber-physical system that captures spatio-acoustic data via deployment of a flexible and scalable sensor network. This paper outlines recent project developments which includes updates on our sensor network comprised of crowd-sourced remote sensing, as well as inexpensive and high quality outdoor remote sensing solutions; development of a number of software tools for analysis, visualization, and development of machine learning; and an updated web-based exploration portal with real-time animation overlays for Google Maps. This paper also includes a summary of technologies and strategies that engage citizen scientist initiatives to measure New York City’s spatio-acoustic noise pollution in collaboration with the Center for Urban Science and Progress (CUSP).",project-academic
10.1109/TNSM.2021.3049824,2021-01-14,a,IEEE,optimized iot service chain implementation in edge cloud platform a deep learning framework," Internet of Things (IoT) services have been implemented for several network applications from smart cities to rural areas. However, there are many barriers to provide an efficient solution for the IoT service deployment underlying innovation SDN/NFV-based technologies. First, though an IoT service can flexibly deploy via virtual network functions (VNFs), a deployment scheme needs to solve the joint routing and resource allocation problem, which becomes more difficult than the traditional centralized cloud/datacenter solution due to distributed resources in the edge-cloud network. In addition, due to uncertain workloads in IoT services, static optimization solutions may not deal with uncompleted knowledge of the entire input, which is often given by assumptions, but unrealistic in current provisioning approaches. Aiming to address these issues, we model an online mechanism for the dynamic IoT service chain deployment to optimize the operational cost in a finite horizon. We propose a JOint Routing and Placement problem for IoT service chain (JORP) that can dynamically scale in/out the number of VNF instances. We then propose a learning method to efficiently solve JORP based on branch-and-bound (BnB). Our proposed learning mechanism can intelligently imitate the branching/pruning actions of BnB, and remove unlikely solutions in the search space based on the deep neural network model to improve the performance. In that respect, we take an intensive simulation that illustrates the promising result of our proposed deep learning method compared to BnB and the greedy baseline in terms of the performance of the algorithm and the operational cost reduction.",project-academic
10.1109/ECTIDAMTNCON48261.2020.9090693,2020-03-01,p,IEEE,a conceptual framework for bus arrival prediction based on spark framework and machine learning approaches," Recently, the bus system has been deployed as a new option for public transportation in Chiang Mai. However, the reason why the people hesitate to take the bus because they are not confident about accuracy of the bus schedule. Predicting the bus arriving time at a certain bus station is a challenging problem, due to concerns with real-time data pre-processing, numerous data inputs, and the level of predictive accuracy. Previous studies that related to bus time arrival prediction have applied statistical and machine-learning methods. However, the time series problem is rarely considered in the previous machine-learning prediction methods. Moreover, some models only analyze small amounts of data, which leads to poor accuracy and poor speed of prediction. This paper reviewed previous five years studies related to the prediction of bus arrival time. The paper highlights the current research gaps and applications of bus arrival time prediction. We proposed the research framework, as well as the possible research trends and challenges.",project-academic
10.3390/S20072093,2020-04-08,a,Multidisciplinary Digital Publishing Institute,wireless sensor networks for noise measurement and acoustic event recognitions in urban environments," Nowadays, urban noise emerges as a distinct threat to people’s physiological and psychological health. Previous works mainly focus on the measurement and mapping of the noise by using Wireless Acoustic Sensor Networks (WASNs) and further propose some methods that can effectively reduce the noise pollution in urban environments. In addition, the research on the combination of environmental noise measurement and acoustic events recognition are rapidly progressing. In a real-life application, there still exists the challenges on the hardware design with enough computational capacity, the reduction of data amount with a reasonable method, the acoustic recognition with CNNs, and the deployment for the long-term outdoor monitoring. In this paper, we develop a novel system that utilizes the WASNs to monitor the urban noise and recognize acoustic events with a high performance. Specifically, the proposed system mainly includes the following three stages: (1) We used multiple sensor nodes that are equipped with various hardware devices and performed with assorted signal processing methods to capture noise levels and audio data; (2) the Convolutional Neural Networks (CNNs) take such captured data as inputs and classify them into different labels such as car horn, shout, crash, explosion; (3) we design a monitoring platform to visualize noise maps, acoustic event information, and noise statistics. Most importantly, we consider how to design effective sensor nodes in terms of cost, data transmission, and outdoor deployment. Experimental results demonstrate that the proposed system can measure the urban noise and recognize acoustic events with a high performance in real-life scenarios.",project-academic
10.1109/TMECH.2021.3079409,2021-05-12,a,IEEE,valve detection for autonomous water pipeline inspection platform," Water distribution and transmission lines are indispensable to urban infrastructure. The water pipelines are subject to both structural and functional deterioration due to various reasons including aging, negligence, and high demand for water supply. Hence, to ensure a safe and reliable water supply, the water utilities need to perform routine pipe condition assessments. The condition assessment is usually carried out by visual inspection with the machine vision system carried by a robotic platform. The inspection platforms will capture the internal condition of the water pipelines in a video stream. However, the robotic platform frequently experiences difficulties while traversing through the valves installed along the pipeline. This inhibits and disrupts the inspection process of the water pipelines. Therefore, this paper proposes a deep learning-based automatic valve detection framework to facilitate the robot's navigation and ensure continuous inspection without any interruptions. The valve detection model is developed by combining MobileNet-160 and Feature Pyramid Network (FPN) and is named as MFPN. The developed framework also employs a generative adversarial network to solve the sparse data set issues and improve the generality of the framework. The comparative study and ablation analyses demonstrate that the proposed framework can achieve a higher mAP value of 89.11% in comparison with the state-of-the-art. Hence, this light-weight and efficient solution can be deployed to the robotic platform for real-time valve detection and enable autonomous navigation of the robotic platform for condition assessment of water pipelines.",project-academic
10.3390/S20040998,2020-02-13,a,Multidisciplinary Digital Publishing Institute,moreair a low cost urban air pollution monitoring system," MoreAir is a low-cost and agile urban air pollution monitoring system. This paper describes the methodology used in the development of this system along with some preliminary data analysis results. A key feature of MoreAir is its innovative sensor deployment strategy which is based on mobile and nomadic sensors as well as on medical data collected at a children's hospital, used to identify urban areas of high prevalence of respiratory diseases. Another key feature is the use of machine learning to perform prediction. In this paper, Moroccan cities are taken as case studies. Using the agile deployment strategy of MoreAir, it is shown that in many Moroccan neighborhoods, road traffic has a smaller impact on the concentrations of particulate matters (PM) than other sources, such as public baths, public ovens, open-air street food vendors and thrift shops. A geographical information system has been developed to provide real-time information to the citizens about the air quality in different neighborhoods and thus raise awareness about urban pollution.",project-academic
10.1145/3314402,2019-03-29,a,"ACMPUB27New York, NY, USA",a deep reinforcement learning enabled dynamic redeployment system for mobile ambulances," Protecting citizens' lives from emergent accidents (e.g. traffic accidents) and diseases (e.g. heart attack) is of vital importance in urban computing. Every day many people are caught in emergent accidents or diseases and thus need ambulances to transport them to hospitals. In this paper, we propose a dynamic ambulance redeployment system to reduce the time needed for ambulances to pick up patients and to increase the probability of patients being saved in time. For patients in danger, every second counts. Specifically, whenever there is an ambulance becoming available (e.g. finishing transporting a patient to a hospital), our dynamic ambulance redeployment system will redeploy it to a proper ambulance station such that it can better pick up future patients. However, the dynamic ambulance redeployment is challenging, as when we redeploy an available ambulance we need to simultaneously consider each station's multiple dynamic factors. To trade off these multiple factors using handcrafted rules are almost impossible. To deal with this issue, we propose using a deep neural network, called deep score network, to balance each station's dynamic factors into one score, leveraging the excellent representation ability of deep neural networks. And then we propose a deep reinforcement learning framework to learn the deep score network. Finally, based on the learned deep score network, we provide an effective dynamic ambulance redeployment algorithm. Experiment results using data collected in real world show clear advantages of our method over baselines, e.g. comparing with baselines, our method can save ~100 seconds (~20%) of average pickup time of patients and improve the ratio of patients being picked up within 10 minutes from 0.786 to 0.838. With our method, people in danger can be better saved.",project-academic
10.1613/JAIR.1.11352,2019-10-29,a,AI Access Foundation,acceptable planning influencing individual behavior to reduce transportation energy expenditure of a city," Our research aims at developing intelligent systems to reduce the transportation-related energy expenditure of a large city by influencing individual behavior. We introduce COPTER - an intelligent travel assistant that evaluates multi-modal travel alternatives to find a plan that is acceptable to a person given their context and preferences. We propose a formulation for acceptable planning that brings together ideas from AI, machine learning, and economics. This formulation has been incorporated in COPTER that produces acceptable plans in real-time. We adopt a novel empirical evaluation framework that combines human decision data with a high fidelity multi-modal transportation simulation to demonstrate a 4\% energy reduction and 20\% delay reduction in a realistic deployment scenario in Los Angeles, California, USA.",project-academic
10.1016/J.APENERGY.2021.117504,2021-11-01,a,Elsevier BV,deep reinforcement learning control of electric vehicle charging in the presence of photovoltaic generation," Abstract None None In recent years, the importance of electric mobility has increased in response to climate change. The fast-growing deployment of electric vehicles (EVs) worldwide is expected to decrease transportation-related None None None None C None None None O None None None 2 None None None None None None emissions, facilitate the integration of renewables, and support the grid through demand–response services. Simultaneously, inadequate EV charging patterns can lead to undesirable effects in grid operation, such as high peak-loads or low self-consumption of solar electricity, thus calling for novel methods of control. This work focuses on applying deep reinforcement learning (RL) to the EV charging control problem with the objectives to increase photovoltaic self-consumption and EV state of charge at departure. Particularly, we propose mathematical formulations of environments with discrete, continuous, and parametrized action spaces and respective deep RL algorithms to resolve them. The benchmarking of the deep RL control against naive, rule-based, deterministic optimization, and model-predictive control demonstrates that the suggested methodology can produce consistent and employable EV charging strategies, while its performance holds a great promise for real-time implementations.",project-academic
10.1007/978-3-319-47715-2_4,2017-01-01,a,"Springer, Cham",artificial neural network based real time urban road traffic state estimation framework," With the rapid increase of urban development and the surge in vehicle ownership, urban road transport problems like traffic accident and congestion caused huge waste of time, property damage and environmental pollution in recent years. To address these problems, use of information communication technology-based transport systems that can support maximum utilization of the existing road transport infrastructure has been proposed by different researchers. Road monitoring systems are one of these solutions which support road users to make informed decisions. However, the current road traffic monitoring systems use road side infrastructures for road traffic data collection and these technologies lack accurate and up-to-date traffic data covering the whole road network. By comparison, cellular networks are already widely deployed and can provide large road network coverage. Besides, 3G and 4G cellular networks provide mobile phone positioning facility with better performance accuracy and this opportunity can help to obtain accurate traffic flow information in cost effective manner on the entire road networks. The purpose of this chapter is to present our approach for real-time road traffic state estimation framework using the existing cellular network for road traffic data source and a neural network state estimation model. To evaluate the performance of the Artificial Neural Network model (ANN) both simulation and real world data is applied. The estimation accuracy using MAE and estimation availability indicated that reliable link speed estimation can be generated using this model and the estimated data can help to indicate real-time urban road traffic condition.",project-academic
10.1016/J.TRC.2018.12.002,2019-01-01,a,Pergamon,data driven activity scheduler for agent based mobility models," Abstract None None Activity-based modelling is a modern agent-based approach to travel demand modelling, in which the transport demand is derived from the agent’s needs to perform certain activities at specific places and times. The agent’s mobility is considered in a broader context, which allows the activity-based models to produce more realistic trip chains, compared to traditional trip-based models. The core of any activity-based model is an activity scheduler – a software component producing sequences of agent’s daily activities interconnected by trips, called activity schedules. Traditionally, activity schedulers used to rely heavily on hard-coded knowledge of transport behaviour experts. We introduce the concept of a Data-Driven Activity Scheduler (DDAS), which replaces numerous expert-designed components and their intricately engineered interactions with a collection of machine learning models. Its architecture is significantly simpler, making it easier to deploy and maintain. This shift towards data-driven, machine learning based approach is possible due to increased availability of mobility-related data. We demonstrate DDAS concept using our own proof-of-concept implementation, perform a rigorous analysis and compare the validity of the resulting model to one of the rule-based alternatives using the Validation Framework for Activity-Based Models (VALFRAM).",project-academic
,2011-01-01,a,,smartphone sensing," The increasing popularity of smartphones with their embedded sensing capability and the availability of new application distribution channels, such as, the Apple AppStore and the Google Android Market, is giving researchers a unique opportunity to deploy mobile sensing applications at unprecedented scale and collect sensor data way beyond the boundaries of traditional small-scale research laboratory deployments. This thesis makes a number of contributions to smartphone sensing by introducing new sensing models, algorithms, applications, and systems. 
First, we propose CenceMe, the first large-scale personal and social sensing application for smartphones, which allows users to share their real-time ""sensing presence"" (i.e., activity and context) with friends using the phone, web, and social network sites (i.e., Facebook, Myspace, Twitter). CenceMe exploits the smartphone's onboard sensors (viz. accelerometer, microphone, GPS, Bluetooth, WiFi, camera) and lightweight, efficient machine learning algorithms on the phone and backend servers to automatically infer people's activity and social context (e.g., having a conversation, in a meeting, at a party). The development, deployment, and evaluation of CenceMe opened up new problems also studied in this dissertation. 
Sensing with smartphones presents several technical challenges that need to be surmounted; for example, the smartphone's sensing context (i.e., the position of the phone relative to the event being sensed varies over time) and limited computational resources present important challenges that limit the inference accuracy using phones. To address these challenges, we propose an ""evolve-pool-collaborate"" model that allows smartphones to automatically adapt to new environments and conduct collaborative sensing among co-located phones resulting in increased robustness and classification accuracy of smartphone sensing in the wild. We call this system, Darwin Phones. 
The final contribution of this dissertation explores a new mobile sensing application called VibN, which continuously runs on smartphones allowing users to view live feeds associated with hotspots in a city; that is, what is going on at different locations, the number of people and demographics, and the context of a particular place. VibN addresses a number of critical problems to the success of smartphone sensing, such as, running continuous sensing algorithms on resource limited smartphones, resolving privacy issues, and developing a sensor data validation methodology for applications released via the app stores (i.e., validating sensor data and identifying patterns without any notion of ground truth evidence). Such a methodology is crucial to the large-scale adoption of smartphone sensing in the future. 
Smartphone sensing is an emerging field that requires significant advances in mobile computing, machine learning, and systems design. It is an exciting area of research that is cross-disciplinary and likely to touch on many application areas and scientific domains moving forward. The work presented in this dissertation identifies new problems and solutions that help advance our understanding in what is now a fast-moving area of research.",project-academic
10.3233/AIS-140248,2014-01-01,p,IOS Press,introduction to the thematic issue on ambient and smart component technologies for human centric computing," Human-Centric Computing is the discipline that studies the way humans and artificial systems interact reciprocally and exchange information. It is strictly related to other disciplines such as sociology, psychology, cognitive sciences, graphic design and partially overlaps with other ICT technologies such as Human-Computer Interaction, Affective Computing, Persuasive Computing, and others. A strong relationship exists also between HumanCentric Computing and Intelligent Environments such as Ambient Intelligence and Ambient Assisted Living applications, Smart Environments and Smart Cities. From one hand, indeed, the design of Intelligent Environments aimed at supporting actively their inhabitants cannot leave out any aspect concerning the way the user may/want interact with artificial entities and receive support from them. On the other hand, Intelligent Environments may provide technologies such as, for example, those for context and situation awareness, that can enable new ways of designing and realizing more effective interaction systems and mechanisms between humans and artificial entities. In this thematic issue both points of view are considered taking carefully into account that several technical challenges remain to be solved before effective, intelligent, secure, and reliable solutions – that fully exploit potentials of both AmI and HCC – can be deployed trustfully into real environments. 2. In this thematic issue",project-academic
10.1109/SII.2019.8700415,2019-01-01,p,IEEE,development of wearable gait assistive device using recurrent neural network," In elderly population, gait disorders are common where majority of these disorders are associated as symptoms of neurodegenerative diseases including Parkinson’s Disease (PD), Huntingtons Disease (HD), and Amyotrophic Lateral Sclerosis (ALS). In addition to affected mobility, the patients are also susceptible to greater risk of falls, hence increasing the demand for caretakers. With the trend of aging population, personal assistive device could be deployed to assist patients to regain independence and improve their quality of life. This paper proposes an end-to-end solution architecture for real-time standalone wearable gait assistive device to automate the rehabilitation activity. A key aspect of this study is to incorporate recurrent neural network (RNN) model that provides accurate pattern recognition and output actuation cue to the patients. Prototype and simulation data was used to show the feasibility of the proposed architecture and machine learning model. Preliminary results indicate favorable accuracy gait cycle detection for implementation. However, further optimizations are required to lower the computational costs and shorten the time lag between cycles to ensure low cost feasibility of the device.",project-academic
10.1007/978-3-319-07788-8_50,2014-06-22,p,"Springer, Cham",the vision of the sociable smart city," In this paper we define what is a Sociable Smart City and how this vision can be realised. This vision elaborates on recent developments in smart cities around the world where novel technologies and applications have been introduced in order to provide services and promote economic growth and sustainability. According to our approach a smart city has to also focus on social and cultural aspects, to allow people to interact with their cities in novel ways and to enable them to shape and decide the future of the city. This approach has originated from the large-scale deployment and evaluation of the CLIO urban computing system, which enables people to interact with the collective city memory. Our findings revealed that a system that exploits city infrastructure and both people’s and artificial intelligence in order to empower and engage them in social activities may enhance citizen participation and sense of belonging as well as it may enable urban social interactions. Aiming to address the Sociable Smart City vision, we held a homonymous workshop in 2013 that brought together researchers of urban computing, smart cities, pervasive technologies and hci. Among its outcomes has been a definition of the Sociable Smart City, the identification of challenges in realising it, the proposition of applications that can accelerate its adoption and what their impact can be, as well as the identification of the major stakeholders involved.",project-academic
10.1016/J.SIMPAT.2015.05.011,2015-11-01,a,Elsevier,a flexible framework for accurate simulation of cloud in memory data stores," Abstract None None In-memory (transactional) data stores, also referred to as data grids, are recognized as a first-class data management technology for cloud platforms, thanks to their ability to match the elasticity requirements imposed by the pay-as-you-go cost model. On the other hand, determining how performance and reliability/availability of these systems vary as a function of configuration parameters, such as the amount of cache servers to be deployed, and the degree of in-memory replication of slices of data, is far from being a trivial task. Yet, it is an essential aspect of the provisioning process of cloud platforms, given that it has an impact on the amount of cloud resources that are planned for usage. To cope with the issue of predicting/analysing the behavior of different configurations of cloud in-memory data stores, in this article we present a flexible simulation framework offering skeleton simulation models that can be easily specialized in order to capture the dynamics of diverse data grid systems, such as those related to the specific (distributed) protocol used to provide data consistency and/or transactional guarantees. Besides its flexibility, another peculiar aspect of the framework lies in that it integrates simulation and machine-learning (black-box) techniques, the latter being used to capture the dynamics of the data-exchange layer (e.g. the message passing layer) across the cache servers. This is a relevant aspect when considering that the actual data-transport/networking infrastructure on top of which the data grid is deployed might be unknown, hence being not feasible to be modeled via white-box (namely purely simulative) approaches. We also provide an extended experimental study aimed at validating instances of simulation models supported by our framework against execution dynamics of real data grid systems deployed on top of either private or public cloud infrastructures. Particularly, our validation test-bed has been based on an industrial-grade open-source data grid, namely Infinispan by JBoss/Red-Hat, and a de-facto standard benchmark for NoSQL platforms, namely YCSB by Yahoo. The validation study has been conducted by relying on both public and private cloud systems, scaling the underlying infrastructure up to 100 (resp. 140) Virtual Machines for the public (resp. private) cloud case. Further, we provide some experimental data related to a scenario where our framework is used for on-line capacity planning and reconfiguration of the data grid system.",project-academic
10.1016/J.AEUE.2020.153368,2020-07-24,a,Urban & Fischer,optimization of street canyon outdoor channel deployment geometry for mmwave 5g communication," Abstract None None Millimeter Wave (mmWave) channel study is important in 5G communication to assess novel technological solutions in realistic environments. Thus, it is vital to develop reliable channel models integrating the effects of mmWave atmospheric absorption, foliage loss, and the directionality of high gain antenna systems used in mmWave links. In this paper, a design is presented that combines mmWave channel modeling with machine learning for the efficient management of environment geometry and channel specification in a 5G urban microcell (UMi) street canyon (SC) outdoor channel. Accordingly, we first investigate the channel characteristics of mmWave line of sight (LOS) and non-line of sight (NLOS) directional outdoor links in various reference cases. The analysis is conducted for the backhaul and cellular access cases using a low complexity custom channel model based on ray tracing. Additional modeling components such as antenna directionality, oxygen absorption, and foliage loss modeling are also included to enhance accuracy and generality. It is observed that the estimated channel path loss is largely dependent on SC deployment parameters due to the antenna directionality combined with the small wavelength of mmWaves, and the use of correct values for them is essential to obtain optimal channel performance. Hence, we apply a metaheuristic algorithm called particle swarm optimization (PSO) for the optimal placement of position and deployment parameters of SC channel in a mmWave communication system to yield the best channel path loss.",project-academic
10.1109/ACCESS.2020.3033771,2020-10-26,a,IEEE,an accelerated edge cloud system for energy data stream processing based on adaptive incremental deep learning scheme," As smart metering technology evolves, power suppliers can make low-cost, low-risk estimation of customer-side power consumption by analyzing energy demand data collected in real-time. With advances network infrastructure, smart sensors, and various monitoring technologies, a standardized energy metering infrastructure, called advanced metering infrastructure (AMI), has been introduced and deployed to urban households to allow them to develop efficient power generation plans. Compared to traditional stochastic approaches for time-series data analysis, deep-learning methods have shown superior accuracy on many prediction applications. Because smart meters and infrastructure monitors produce a series of measurements over time, a large amount of data is accumulated, creating a large data stream, which takes much time from data generation to deployment of deep-learning model training. In this article, we propose an accelerated computing system that considers time-variant properties for accurate prediction of energy demand by processing the AMI stream data. The proposed system is a real-time training/inference system that deploys AMI data over a distributed edge cloud. It comprises two core components: an adaptive incremental learning solver and deep-learning acceleration with FPGA-GPU resource scheduling. An adaptive incremental learning scheme adjusts the batch/epoch in training iteration to reduce the time delay of the latest trained model, while trying to prevent biased-training due to the sub-optimal optimizer of incremental learning. In addition, a resource scheduling scheme manages various accelerator resources for accelerated deep-learning processing while minimizing the computational cost. The experimental results demonstrated that our method achieved good performance for adaptive batch size and epoch for incremental learning while guaranteeing a low inference error, a high model score, and queue stability with cost efficient processing.",project-academic
10.1109/TITS.2021.3066958,2021-04-19,a,IEEE,boosted genetic algorithm using machine learning for traffic control optimization," Traffic control optimization is a challenging task for various traffic centers around the world and the majority of existing approaches focus only on developing adaptive methods for normal (recurrent) traffic conditions. Optimizing the control plans when severe incidents occur still remains an open problem, especially when a high number of lanes or entire intersections are affected. This paper presents a novel methodology for optimizing the traffic signal timings in signalized urban intersections, under non-recurrent traffic incidents. With the purpose of producing fast and reliable decisions, we combine the fast running Machine Learning (ML) algorithms and the reliable Genetic Algorithms (GA) into a single optimization framework. Firstly, we deploy a typical GA algorithm by considering the phase duration as the decision variable and the objective function as the total travel time in the network. We fine tune the GA for crossover, mutation, fitness calculation and obtain the optimal parameters. Secondly, we train several regression models to predict the total travel time in the studied traffic network, and select the best performing model which we further hyper-tune. Lastly, we propose a new algorithm BGA-ML combining the GA algorithm and the extreme-gradient decision-tree (XGBT), which is the best performing regression model, together in a single optimization framework. Comparison and results are generated by two experiments (one synthetic and one from real urban traffic network) and show that the new BGA-ML is much faster than the original GA algorithm and can reduce the total travel time by almost half when used under incident conditions.",project-academic
10.1016/J.COMNET.2019.106980,2020-01-15,a,Elsevier,machine learning driven service function chain placement and scaling in mec enabled 5g networks," Abstract None None 5G mobile network technology promises to deliver unprecedented ultra-low latency and high data rates, paving the way for many novel applications and services. Network Function Virtualization (NFV) and Multi-access Edge Computing (MEC) are two of the technologies that are expected to play a pivotal role in 5G to achieve ambitious Quality of Service requirements of such applications. While NFV provides flexibility by enabling network functions to be dynamically deployed and inter-connected to realize Service Function Chains (SFC), MEC brings the computing capability to the edges of the mobile network thus reducing latency and alleviating the transport network load. However, adequate mechanisms are needed to meet the dynamically changing network service demands, to optimally utilize the network resources while, at the same time, making sure that the end-to-end latency requirement of services is always satisfied. None In this work, we first propose machine learning models, in particular neural-networks, that can perform auto-scaling by predicting the required number of virtual network function instances based on the traffic demand, using the traffic traces collected over a real-operator commercial network. We then employ Integer Linear Programming (ILP) techniques to formulate and solve a joint user association and SFC placement problem, where each SFC represents a service requested by a user with end-to-end latency and data rate requirements. Finally, we propose a heuristic to address the scalability concern of the ILP model.",project-academic
10.1109/MDAT.2020.2971201,2020-04-21,a,IEEE,guest editorial robust resource constrained systems for machine learning," Machine learning (ML) None is nowadays embedded in several computing devices, consumer electronics, and cyber-physical systems. Smart sensors are deployed everywhere, in applications such as wearables and perceptual computing devices, and intelligent algorithms power our connected world. These devices collect and aggregate volumes of data, and in doing so, they augment our society in multiple ways; from healthcare, to social networks, to consumer electronics, and many more. To process these immense volumes of data, ML is emerging as the None de facto None analysis tool that powers several aspects of our Big Data society. Applications spanning from infrastructure (smart cities, intelligent transportation systems, smart grids, and to name a few), to social networks and content delivery, to e-commerce and smart factories, and emerging concepts such as self-driving cars and autonomous robots, are powered by ML technologies. These emerging systems require real-time inference and decision support; such scenarios, therefore, may use customized hardware accelerators, are typically bound by limited resources, and are restricted to limited connectivity and bandwidth. Thus, near-sensor computation and near-sensor intelligence have started emerging as necessities to continue supporting the paradigm shift of our connected world. The need for real-time intelligent data analytics (especially in the era of Big Data) for decision support near the data acquisition points emphasizes the need for revolutionizing the way we design, build, test, and verify processors, accelerators, and systems that facilitate ML (and deep learning, in particular) implemented in resource-constrained environments for use at the edge and the fog. As such, traditional von Neumann architectures are no longer sufficient and suitable, primarily because of limitations in both performance and energy efficiency caused especially by large amounts of data movement. Furthermore, due to the connected nature of such systems, security and reliability are also critically important. Robustness, therefore, in the form of reliability and operational capability in the presence of faults, whether malicious or accidental, is a critical need for such systems. Moreover, the operating nature of these systems relies on input data that is characterized by the four “V’s”: velocity (speed of data generation), variability (variable forms and types), veracity (unreliable and unpredictable), and volume (i.e., large amounts of data). Thus, the robustness of such systems needs to consider this issue as well. Furthermore, robustness in terms of security, and in terms of reliability to hardware and software faults, in particular, besides their importance when it comes to safety-critical applications, is also a positive factor in building trustworthiness toward these disrupting technologies from our society. To achieve this envisioned robustness, we need to refocus on problems such as design, verification, architecture, scheduling and allocation policies, optimization, and many more, for determining the most efficient, secure, and reliable way of implementing these novel applications within a robust, resource-constrained system, which may or may not be connected. This special issue, therefore, addresses a key aspect of fog and edge-based ML algorithms; robustness (as defined above) under resource-constraint scenarios. The special issue presents emerging works in how we design robust systems, both in terms of reliability as well as fault tolerance and security, while operating with a limited number of resources, and possibly in the presence of harsh environments that may eliminate connectivity and pollute the input data.",project-academic
10.1109/WCNCW.2019.8902853,2019-04-15,p,IEEE,an open 5g nfv platform for smart city applications using network softwarization," Advanced wireless communication network testbeds are now widely being deployed around European and cross-continental. This represents an interesting opportunity for vertical industry and academia to perform experimentation and validation before a real deployment. In this paper, we present 5GinFIRE as a suitably flexible platform towards open 5G (Network Function Virtualization (NFV) ecosystem and playground. On top of this platform, we designed and deployed a smart city safety system as a vertical use case, exploring 5G capabilities through a combination of NFV and machine learning to provide end-to-end communication and low latency smart city service. This safety system helps detecting criminals along the city and sending a notification to the security center. A Virtual Network Function (VNF) has been developed to enable video transcoding, face detection and recognition at the cloud or the edge of the network. The validation of the overall system is performed through the deployment of the use case indoor (Smart Internet Lab) and outdoor (Millennium Square Bristol). We show the VNF specification and present a quantitative analysis in terms of bandwidth, response time, processing time and transmission speed in terms of Quality of Experience (QoE).",project-academic
10.1109/EDGE.2018.00028,2018-07-01,p,IEEE,real time traffic pattern collection and analysis model for intelligent traffic intersection," The traffic congestion hits most big cities in the world - threatening long delays and serious reductions in air quality. City and local government officials continue to face challenges in optimizing crowd flow, synchronizing traffic and mitigating threats or dangerous situations. One of the major challenges faced by city planners and traffic engineers is developing a robust traffic controller that eliminates traffic congestion and imbalanced traffic flow at intersections. Ensuring that traffic moves smoothly and minimizing the waiting time in intersections requires automated vehicle detection techniques for controlling the traffic light automatically, which are still challenging problems. In this paper, we propose an intelligent traffic pattern collection and analysis model, named TPCAM, based on traffic cameras to help in smooth vehicular movement on junctions and set to reduce the traffic congestion. Our traffic detection and pattern analysis model aims at detecting and calculating the traffic flux of vehicles and pedestrians at intersections in real-time. Our system can utilize one camera to capture all the traffic flows in one intersection instead of multiple cameras, which will reduce the infrastructure requirement and potential for easy deployment. We propose a new deep learning model based on YOLOv2 and adapt the model for the traffic detection scenarios. To reduce the network burdens and eliminate the deployment of network backbone at the intersections, we propose to process the traffic video data at the network edge without transmitting the big data back to the cloud. To improve the processing frame rate at the edge, we further propose deep object tracking algorithm leveraging adaptive multi-modal models and make it robust to object occlusions and varying lighting conditions. Based on the deep learning based detection and tracking, we can achieve pseudo-30FPS via adaptive key frame selection.",project-academic
10.1109/PERCOMWORKSHOPS51409.2021.9430985,2021-03-22,p,IEEE,flood detection using semantic segmentation and multimodal data fusion," Real-time flood detection and notifying the citizens about its risk is of utmost importance. This work discusses the real-time deployment of one such notification system called Flood-Bot. FloodBot is a vision-powered flood detection and notification prototype deployed in a flash flood-prone Ellicott City, Maryland. We discuss the real-time deployment of FloodBot and our approach in detecting the flood event using semantic segmentation and multimodal data fusion. We implement the state-of-the-art semantic segmentation model U-Net and its modified version to track landmass with an accuracy of above 80%. We augment the parsed scene data with actual flood level sensor readings and ambient weather data for better scene representation. We validate the deep learning model's outcome using the flood sensor before posting risk message into social media. We then articulate the learning and challenges around our deployment from June – November 2020.",project-academic
10.1109/ISC2.2017.8090799,2017-09-01,p,IEEE,estimation of intersection traffic density on decentralized architectures with deep networks," Intelligent Intersection Traffic Management has become increasingly important because of the need to reduce congestion and improve the overall travel experience of commuters. Given the dynamic nature of everyday city traffic, this paper proposes real-time processing of videos from cameras to estimate the traffic density and optimize the signal parameters of the intersection. The region-of-interest of the road segment is automatically extracted as a one-time operation, and the density is estimated using an ensemble of background subtraction and variants of convolutional neural networks. The architecture is scalable as the processing is entirely decentralized on the edge computing device installed at every intersection. We demonstrate that our technique is resilient to low resolution videos, long fields of view and chaotic traffic conditions with high occlusion. The proposed algorithms generalize across deployments with minimal reconfiguration.",project-academic
10.1109/PIMRC48278.2020.9217347,2020-08-01,p,IEEE,optimal transmission control and learning based trajectory design for uav assisted detection and communication," Due to their high mobility, flexible deployment and stable maneuverability, unmanned aerial vehicles (UAVs) have been deemed as a promising and indispensable role for various emerging applications (e.g., dangerous area detection, dynamic target tracking, and map remote sensing). Compared to the static monitoring equipments, UAV-mounted high-definition camera and signal transceiver can be used cost-effectively as an on-demand aerial platform to detect the unknown region and send the real-time data back at the same time. However, these highlighted limitations of battery capacity and communication resource extremely affect the UAV’s performance such as flight endurance and data transmission. Motivated by the above conflicts, this paper aims to minimize the total energy consumed by the UAV during the region detection mission through jointly optimizing the collected data size, transmission time, and flying trajectory. Toward this end, we derive the optimal data collection and transmission time in closed forms via convex optimization, and propose a model-free reinforcement learning-based algorithm for training the UAV to plan its trajectory without knowing the environment information in advance. Simulation results validate the performance of our designs in terms of convergence, energy consumption, and energy efficiency.",project-academic
10.1145/2800835.2801631,2015-09-07,p,ACM,privacy preserving crowd estimation for safer cities," As sensors are getting cheaper and being widely deployed in cities, the Internet of Things (IoT) is providing us great potential to make cities not only smart but also safer. However, how to utilize the information sensed from physical environments to improve civil safety is still a challenging issue. In this paper we present a methodology to estimate the crowd level for indoor scenarios based on data collected from inexpensive and privacy conscious sensors. Our method can be widely applied into many applications for Safer Cities, ranging from facility monitoring to emergency handling. Different from camera-based approaches, our solution can preserve user privacy, scaling better in terms of costs than existing solutions. Using the data collected from real environments, we examine different supervised learning algorithms and identify that Random Forest is the best model. Our solution has been deployed and tested in a Singapore shopping mall, showing that 95% of the overcrowded situations can be detected.",project-academic
10.1109/MWC.021.2000497,2021-09-10,a,IEEE,intelligent visual iot enabled real time 3d visualization for autonomous crowd management," Real-time crowd management systems play an important role in urban planning, disaster warning, and emergency evacuation. For example, through closed-circuit television (CCTV) cameras and artificial intelligence (AI) algorithms, crowd counting and abnormal event recognition can be realized. However, most of these applications are limited within the 2D coordinate system of each single camera, lacking a unified understanding of the 3D world as a whole. In addition, overly simple functionalities, such as framing certain areas or displaying text, limit the system intelligence and user experience. This article proposes a solution of realtime 3D visualization of outdoor scenes enabled by AI and the Visual Internet of Things (V-IoT). First, fixed and airborne cameras, infrared cameras, LiDAR, gas sensors, and so on are deployed to collect multi-modal data and send them to the clouds. Then AI algorithms are executed in the clouds to calculate the location of people, vehicles, and other moving objects, and to detect fights, stampedes, fire, or other abnormal events. Finally, the clouds send the AI algorithm results to the visualization system in terminal devices. The system displays the scene in real time through 3D animation and charts, and provides user interaction interfaces and simulation functionalities in a gamification environment. Experiments show that the proposed solution realistically visualizes the real world to sup-nort the user's decision.",project-academic
,2020-11-17,a,,passgoodpool joint passengers and goods fleet management with reinforcement learning aided pricing matching and route planning," In this paper, we present a dynamic, demand aware, and pricing-based matching and route planning framework that allows efficient pooling of multiple passengers and goods in each vehicle. This approach includes the flexibility for transferring goods through multiple hops from source to destination as well as pooling of passengers. The key components of the proposed approach include (i) Pricing by the vehicles to passengers which is based on the insertion cost, which determines the matching based on passenger's acceptance/rejection, (ii) Matching of goods to vehicles, and the multi-hop routing of goods, (iii) Route planning of the vehicles to pick. up and drop passengers and goods, (i) Dispatching idle vehicles to areas of anticipated high passenger and goods demand using Deep Reinforcement Learning, and (v) Allowing for distributed inference at each vehicle while collectively optimizing fleet objectives. Our proposed framework can be deployed independently within each vehicle as this minimizes computational costs associated with the gorwth of distributed systems and democratizes decision-making to each individual. The proposed framework is validated in a simulated environment, where we leverage realistic delivery datasets such as the New York City Taxi public dataset and Google Maps traffic data from delivery offering businesses.Simulations on a variety of vehicle types, goods, and passenger utility functions show the effectiveness of our approach as compared to baselines that do not consider combined load transportation or dynamic multi-hop route planning. Our proposed method showed improvements over the next best baseline in various aspects including a 15% increase in fleet utilization and 20% increase in average vehicle profits.",project-academic
10.1109/VTCFALL.2018.8690788,2018-08-01,p,IEEE,ultra low power iot traffic monitoring system," Given the sizable anticipated proliferation of Internet of Things (IoT) devices, Forrester Research forecasts that the fleet management and transportation industry sectors will enjoy more growth than others. This may come as no surprise, since infrastructure (e.g., roadways, bridges, airports) is a prime candidate for sensor integration to provide real-time measurements and to support intelligent decisions. The predicted increase of deployed devices makes it difficult to calculate the amount of energy required for these functions. Current estimates suggest that 2 to 4% of worldwide carbon emissions can be attributed to the information and communication industry. This paper presents novel algorithms designed to optimize power consumption of an intelligent vehicle counter and classifier sensors. Each was based on an event-driven methodology wherein a control block orchestrates the work of different components and subsystems. System duty-cycle is reduced through several techniques, and a reinforcement learning algorithm is introduced to control the system power policy, according to the traffic environment. Battery life for a sensor supported by a 2300 mAh battery was extended from 48-hour, adopted all-on policy to more than 400 days when leveraging the algorithms and techniques presented in this work.",project-academic
10.1007/978-3-030-38085-4_13,2019-05-13,p,"Springer, Cham",dynamic abstraction of optical networks with machine learning technologies," The emerging 5G network will bring a huge amount of network traffic with big variations to optical transport networks. Software-defined optical networks and network function virtualization contribute to the vision for future programmable, disaggregated, and dynamic optical networks. Future optical networks will be more dynamic in network functions and network services, with high-frequency network re-configurations. Optical connections will last shorter than that of the static optical networks. It’s straightforward that Programmable optical hardware will require a reduced link margin to improve the hardware utilization. To configure network dynamically, real-time network abstractions are required for both current links and available-for-deploy links. The former abstraction guarantees the established links not be interfered by the newly established link while the latter abstraction provides information for intelligent network planning. In this talk, we use machine-learning technologies to process the collected monitoring data in a field-trial testbed to abstract performances of multiple optical channels. Based on the abstract information, a new channel can be established with maximum performance and minimized interference on the current signals. We demonstrated the dynamic network abstraction over a 563.4-km field-trial testbed for 8 dynamic optical channels with 32 Gbaud Nyquist PM-16QAM signals. The work can be further extended to support complex optical networks.",project-academic
10.4230/LIPICS.COSIT.2017.17,2017-01-01,p,Schloss Dagstuhl - Leibniz-Zentrum fuer Informatik,on avoiding traffic jams with dynamic self organizing trip planning," Urban areas are increasingly subject to congestions. Most navigation systems and algorithms that avoid these congestions consider drivers independently and can, thus, cause novel congestions at unexpected places. Pre-computation of optimal trips (Nash equilibrium) could be a solution to the problem but is due to its static nature of no practical relevance. 

In contrast, the paper at-hand provides an approach to avoid traffic jams with dynamic self-organizing trip planning. We apply reinforcement learning to learn dynamic weights for routing from the decisions and feedback logs of the vehicles. In order to compare our routing regime against others, we validate our approach in an open simulation environment (LuST) that allows reproduction of the traffic in Luxembourg for a particular day. Additionally, in two realistic scenarios: (1) usage of stationary sensors and (2) deployment in a mobile navigation system, we perform experiments with varying penetration rates. All our experiments reveal that performance of the traffic network is increased and occurrence of traffic jams are reduced by application of our routing regime.",project-academic
,2020-04-17,a,,deep echo state networks for short term traffic forecasting performance comparison and statistical assessment," In short-term traffic forecasting, the goal is to accurately predict future values of a traffic parameter of interest occurring shortly after the prediction is queried. The activity reported in this long-standing research field has been lately dominated by different Deep Learning approaches, yielding overly complex forecasting models that in general achieve accuracy gains of questionable practical utility. In this work we elaborate on the performance of Deep Echo State Networks for this particular task. The efficient learning algorithm and simpler parametric configuration of these alternative modeling approaches make them emerge as a competitive traffic forecasting method for real ITS applications deployed in devices and systems with stringently limited computational resources. An extensive comparison benchmark is designed with real traffic data captured over the city of Madrid (Spain), amounting to more than 130 automatic Traffic Readers (ATRs) and several shallow learning, ensembles and Deep Learning models. Results from this comparison benchmark and the analysis of the statistical significance of the reported performance gaps are decisive: Deep Echo State Networks achieve more accurate traffic forecasts than the rest of considered modeling counterparts.",project-academic
10.1109/ITSC45102.2020.9294200,2020-09-20,p,IEEE,deep echo state networks for short term traffic forecasting performance comparison and statistical assessment," In short-term traffic forecasting, the goal is to accurately predict future values of a traffic parameter of interest occurring shortly after the prediction is queried. The activity reported in this long-standing research field has been lately dominated by different Deep Learning approaches, yielding overly complex forecasting models that in general achieve accuracy gains of questionable practical utility. In this work we elaborate on the performance of Deep Echo State Networks for this particular task. The efficient learning algorithm and simpler parametric configuration of these alternative modeling approaches make them emerge as a competitive traffic forecasting method for real ITS applications deployed in devices and systems with stringently limited computational resources. An extensive comparison benchmark is designed with real traffic data captured over the city of Madrid (Spain), amounting to more than 130 Automatic Traffic Readers (ATRs) and several shallow learning, ensembles and Deep Learning models. Results from this comparison benchmark and the analysis of the statistical significance of the reported performance gaps are decisive: Deep Echo State Networks achieve more accurate traffic forecasts than the rest of considered modeling counterparts.",project-academic
10.1109/ICMLC51923.2020.9469537,2020-12-02,p,IEEE,a deep learning based wearable healthcare iot device for ai enabled hearing assistance automation," With the recent booming of artificial intelligence (AI), particularly deep learning techniques, digital healthcare is one of the prevalent areas that could gain benefits from AI-enabled functionality. This research presents a novel AI-enabled Internet of Things (IoT) device operating from the ESP-8266 platform capable of assisting those who suffer from impairment of hearing or deafness to communicate with others in conversations. In the proposed solution, a server application is created that leverages Google’s online speech recognition service to convert the received conversations into texts, then deployed to a micro-display attached to the glasses to display the conversation contents to deaf people, to enable and assist conversation as normal with the general population. Furthermore, in order to raise alert of traffic or dangerous scenarios, an ‘urban-emergency’ classifier is developed using a deep learning model, Inception-v4, with transfer learning to detect/recognize alerting/alarming sounds, such as a horn sound or a fire alarm, with texts generated to alert the prospective user. The training of Inception-v4 was carried out on a consumer desktop PC and then implemented into the AI-based IoT application. The empirical results indicate that the developed prototype system achieves an accuracy rate of 92% for sound recognition and classification with real-time performance.",project-academic
,2020-05-16,a,,a deep learning based wearable healthcare iot device for ai enabled hearing assistance automation," With the recent booming of artificial intelligence (AI), particularly deep learning techniques, digital healthcare is one of the prevalent areas that could gain benefits from AI-enabled functionality. This research presents a novel AI-enabled Internet of Things (IoT) device operating from the ESP-8266 platform capable of assisting those who suffer from impairment of hearing or deafness to communicate with others in conversations. In the proposed solution, a server application is created that leverages Google's online speech recognition service to convert the received conversations into texts, then deployed to a micro-display attached to the glasses to display the conversation contents to deaf people, to enable and assist conversation as normal with the general population. Furthermore, in order to raise alert of traffic or dangerous scenarios, an 'urban-emergency' classifier is developed using a deep learning model, Inception-v4, with transfer learning to detect/recognize alerting/alarming sounds, such as a horn sound or a fire alarm, with texts generated to alert the prospective user. The training of Inception-v4 was carried out on a consumer desktop PC and then implemented into the AI based IoT application. The empirical results indicate that the developed prototype system achieves an accuracy rate of 92% for sound recognition and classification with real-time performance.",project-academic
10.1109/ITSC.2018.8569391,2018-07-02,a,,leveraging the channel as a sensor real time vehicle classification using multidimensional radio fingerprinting," Upcoming Intelligent Transportation Systems (ITSs) will transform roads from static resources to dynamic Cyber Physical Systems (CPSs) in order to satisfy the requirements of future vehicular traffic in smart city environments. Up-to-date information serves as the basis for changing street directions as well as guiding individual vehicles to a fitting parking slot. In this context, not only abstract indicators like traffic flow and density are required, but also data about mobility parameters and class information of individual vehicles. Consequently, accurate and reliable systems that are capable of providing these kinds of information in real-time are highly demanded. In this paper, we present a system for classifying vehicles based on their radio-fingerprints which applies cutting-edge machine learning models and can be non-intrusively installed into the existing road infrastructure in an ad-hoc manner. In contrast to other approaches, it is able to provide accurate classification results without causing privacy-violations or being vulnerable to challenging weather conditions. Moreover, it is a promising candidate for large-scale city deployments due to its cost-efficient installation and maintenance properties. The proposed system is evaluated in a comprehensive field evaluation campaign within an experimental live deployment on a German highway, where it is able to achieve a binary classification success ratio of more than 99% and an overall accuracy of 89.15% for a fine-grained classification task with nine different classes.",project-academic
10.1007/978-981-10-6427-2_36,2017-03-24,p,"Springer, Singapore",optimization using swarm intelligence and dynamic graph partitioning in ioe infrastructure fog computing and cloud computing," The modern society, with the advances in wireless sensor network (WSN) technology, are connected in ways more than one. Since the evolution of evolutionary computing and the Internet of Everything (IoE), the term connected has faced a more significant meaning per se. But as the IoE grows, it becomes more complicated and tackling its complications in various aspects of its architecture becomes all the more paramount. This paper aims to resolve some of the issues so faced by the IoE paradigm, with the help of meta-heuristics and to incorporate proper swarm intelligence based routing algorithms to optimize connection issues such as real time delay, network congestion. Fog Computing is used to distribute the workload and to optimize the utilization of bandwidth, which maintains a clean and efficient channel of communication between the IoE clusters and the primary cloud storage. In this approach a new algorithm based on Directed Artificial Bat Algorithm (DABA) is deployed and Particle Swarm Optimization (PSO) meta-heuristics is used to optimize the capabilities of the IoE cluster and maintain its density. The Fog servers, so implemented, grapple with the increased mobility and network usage using the Dynamic Graph Partitioning algorithm.",project-academic
10.1109/IJCNN52387.2021.9533738,2021-07-18,p,IEEE,carsnn an efficient spiking neural network for event based autonomous cars on the loihi neuromorphic research processor," Autonomous Driving (AD) related features provide new forms of mobility that are also beneficial for other kind of intelligent and autonomous systems like robots, smart transportation, and smart industries. For these applications, the decisions need to be made fast and in real-time. Moreover, in the quest for electric mobility, this task must follow low power policy, without affecting much the autonomy of the mean of transport or the robot. These two challenges can be tackled using the emerging Spiking Neural Networks (SNNs). When deployed on a specialized neuromorphic hardware, SNNs can achieve high performance with low latency and low power consumption. In this paper, we use an SNN connected to an event-based camera for facing one of the key problems for AD, i.e., the classification between cars and other objects. To consume less power than traditional frame-based cameras, we use a Dynamic Vision Sensor (DVS) [1]. The experiments are made following an offline supervised learning rule, followed by mapping the learnt SNN model on the Intel Loihi Neuromorphic Research Chip [2]. Our best experiment achieves an accuracy on offline implementation of 86%, that drops to 83% when it is ported onto the Loihi Chip. The Neuromorphic Hardware implementation has maximum 0.72 ms of latency for every sample, and consumes only 310 mW. To the best of our knowledge, this work is the first implementation of an event-based car classifier on a Neuromorphic Chip.",project-academic
,2021-07-01,a,,carsnn an efficient spiking neural network for event based autonomous cars on the loihi neuromorphic research processor," Autonomous Driving (AD) related features provide new forms of mobility that are also beneficial for other kind of intelligent and autonomous systems like robots, smart transportation, and smart industries. For these applications, the decisions need to be made fast and in real-time. Moreover, in the quest for electric mobility, this task must follow low power policy, without affecting much the autonomy of the mean of transport or the robot. These two challenges can be tackled using the emerging Spiking Neural Networks (SNNs). When deployed on a specialized neuromorphic hardware, SNNs can achieve high performance with low latency and low power consumption. In this paper, we use an SNN connected to an event-based camera for facing one of the key problems for AD, i.e., the classification between cars and other objects. To consume less power than traditional frame-based cameras, we use a Dynamic Vision Sensor (DVS). The experiments are made following an offline supervised learning rule, followed by mapping the learnt SNN model on the Intel Loihi Neuromorphic Research Chip. Our best experiment achieves an accuracy on offline implementation of 86%, that drops to 83% when it is ported onto the Loihi Chip. The Neuromorphic Hardware implementation has maximum 0.72 ms of latency for every sample, and consumes only 310 mW. To the best of our knowledge, this work is the first implementation of an event-based car classifier on a Neuromorphic Chip.",project-academic
10.1016/J.COMNET.2020.107573,2020-12-09,a,Elsevier,ai enabled mobile multimedia service instance placement scheme in mobile edge computing," Abstract None None Leveraging cloud infrastructure to the mobile edge computing helps the mobile users to get real time multimedia services in Fifth Generation (5G) network system. To ensure higher Quality-of-Experience (QoE), faster migration of mobile multimedia service instances is required to cope up with user mobility. By deploying the mobile multimedia service instances proactively in multiple edge nodes (ENs) helps the users to get higher QoE. However, excessive deployment of service replicas might increase the cost of the overall network. To establish trade-off between these two conflicting objectives, we have formulated the problem as a Multi-objective Integer Linear Programming (MILP) by integrating the users’ path prediction model. This problem is proven to be an NP-hard one for large networks, thus we develop an artificial intelligence (AI) based meta-heuristic Binary Particle Swarm Optimization (BPSO) algorithm to achieve near-optimal solution within polynomial time. The performance analysis results show the significant performance improvement in terms of QoE and user satisfaction as compared to other state-of-the-art works.",project-academic
10.1109/DSAA49011.2020.00072,2020-10-01,p,IEEE,on board unit big data short term traffic forecasting in urban transportation networks," Short-term traffic prediction in transportation net-work is a hot topic for next generation smart cities. Despite several research efforts, there is still a lack of consensus about the most effective way to predict traffic network-wide. Also, the majority of existing studies rely on either small data sets or limited portions of the transportation network. This paper presents the results of a forecasting study related to on-board units data of heavy-goods vehicles in the Brussels-Capital Region. The analysis, enabled by deployment of a big data infrastructure, concerns the short horizon (one hour ahead) forecasting of trucks flow (vehicles/hour) and mean speed (km/hour) over Brussels-Capital Region’s network. Both parametric and non-parametric (notably machine learning) models are designed and assessed along with different strategies (simple and linear forecasts combination, individual model selection). The paper shows the potential of large amounts of real-time data obtained from moving sensors where forecasting techniques are applied network-wide. In particular, simple combination schemes (notably the simple median combination forecasts) appear to outperform the other methods in terms of prediction accuracy.",project-academic
10.1145/2536714.2536722,2013-11-11,p,ACM,fusing traffic sensor data for real time road conditions," Transport authorities have been deploying and utilising sensor infrastructures in order to improve upon the level of transport-related services within cities. As existing resources are more and more constrained, novel means of utilising the data originating from these sensors are sought. Advanced IT infrastructures enable real-time processing use cases, such as travel time estimations along defined corridors. One major challenge is to deploy general purpose machine learning algorithms that are able to learn relationships between the covariates and a defined response variable. We introduce a data fusion approach using generalised additive models (GAM) to estimate journey times online in a real-time streaming platform. We experiments with bluetooth sensors and weather information to improve the estimation of journey times along a defined A3 corridor in south-central London. Our approach is able to continuously improve the journey time estimation as new (high-frequency) data becomes available. Our fusion platform also generalises to be able to process more data sources and it scales to city-wide deployments. This way, existing legacy sensor deployments can be utilised for novel value-added services and investments into infrastructural sensor deployments can be assessed in a data-driven way and on a needs basis.",project-academic
10.1109/MWC.001.2000424,2021-06-07,a,Institute of Electrical and Electronics Engineers (IEEE),artificial intelligence for smart resource management in multi user mobile heterogeneous rf light networks," Recent trends in 5G and beyond wireless networks have encouraged the migration from the already congested radio frequency (RF) spectrum to higher frequency bands. In this context, the ubiquitous presence of lighting systems supports wide scale deployment of wireless communication links via light. However, the susceptibility of light to user mobility hinders its wide adoption. Hence, the coexistence of RF and light-based wireless communications has the potential to offer seamless heterogeneous network (HetNet) coverage through intelligent vertical handover policies in the presence of users' mobility. In this article, we first present new insights on the implementation of realistic indoor mobile optical channels and the impact of crowd mobility on relevant channel statistics. Then, an artificial intelligence (AI)-based framework for efficient resource management in mobile multi-user RF-light HetNets is proposed using a deep learning-empowered optical link predictor and a multiagent reinforcement learning-based link assignment strategy. The proposed AI-based framework helps to lay down the foundations of smart resource management in mobile multi-user HetNets.",project-academic
10.1109/ICMEW.2018.8551501,2018-07-23,p,IEEE,data augmentation for cnn based people detection in aerial images," Much more than ever, many important places have deployed surveillance cameras for early detection of abnormal events and suspects. However, the monitoring ability of fixed cameras is significantly limited due to the low flexibility, blind spot, and obstacle occlusion. With high mobility, drones have high potential for supporting security surveillance. On the other hand, people detection plays a key role in intelligent surveillance system, and increasing deep learning-based methods show great results. However, the training data for aerial images are still few, even though there are many public datasets available. Thus, in this paper we research on data augmentation, try transforming general images to be aerial image-like, and make an attempt to improving the performance of deep learning-based people detection with existing datasets. The experiments conducted on the real aerial images collected by a camera drone show encouraging results.",project-academic
,2021-04-15,a,,quickloc adaptive deep learning for fast indoor localization with mobile devices," Indoor localization services are a crucial aspect for the realization of smart cyber-physical systems within cities of the future. Such services are poised to reinvent the process of navigation and tracking of people and assets in a variety of indoor and subterranean environments. The growing ownership of computationally capable smartphones has laid the foundations of portable fingerprinting-based indoor localization through deep learning. However, as the demand for accurate localization increases, the computational complexity of the associated deep learning models increases as well. We present an approach for reducing the computational requirements of a deep learning-based indoor localization framework while maintaining localization accuracy targets. Our proposed methodology is deployed and validated across multiple smartphones and is shown to deliver up to 42% reduction in prediction latency and 45% reduction in prediction energy as compared to the best-known baseline deep learning-based indoor localization model.",project-academic
10.1145/3461342,2021-09-22,a,"ACMPUB27New York, NY",quickloc adaptive deep learning for fast indoor localization with mobile devices," Indoor localization services are a crucial aspect for the realization of smart cyber-physical systems within cities of the future. Such services are poised to reinvent the process of navigation and tracking of people and assets in a variety of indoor and subterranean environments. The growing ownership of computationally capable smartphones has laid the foundations of portable fingerprinting-based indoor localization through deep learning. However, as the demand for accurate localization increases, the computational complexity of the associated deep learning models increases as well. We present an approach for reducing the computational requirements of a deep learning-based indoor localization framework while maintaining localization accuracy targets. Our proposed methodology is deployed and validated across multiple smartphones and is shown to deliver up to 42% reduction in prediction latency and 45% reduction in prediction energy as compared to the best-known baseline deep learning-based indoor localization model.",project-academic
10.1109/SMARTIOT52359.2021.00051,2021-08-01,p,IEEE,trafficnnode low power vehicle sensing platform for smart cities," Automating traffic monitoring and management can materially contribute to the realization of Smart Cities, but this demands detailed, accurate and timely characterization of traffic flows. Current methods employ video capture (high installation and operating costs), pneumatic-tube-based counters (limited detail about vehicle types and often only installed temporarily), or manual data capture (high human cost).We have developed an intelligent, inexpensive, pavement-mountable device capable of collecting information about vehicle types and speeds using an embedded, power-optimized neural network. The device is designed to fit within a raised pavement marker (RPM). RPMs are deployed throughout the world as lane markers. Packaging our sensing technology into RPMs offers the potential to significantly improve the spatial and temporal resolution of traffic flow information across a city.We outline our methodology for energy-optimized machine learning on a small, resource-constrained sensor device. We present the results of our work in terms of accuracy (96% classifying vehicle type and 89% classifying vehicle speed) and battery life (three years).",project-academic
10.1007/978-3-662-61920-9_6,2020-10-27,a,"Springer, Berlin, Heidelberg",some public procurement challenges in supporting and delivering smart urban mobility procurement data discretion and expertise," This chapter explores three of the challenges that public buyers face when designing public tenders to support smart urban mobility initiatives and when supervising the execution of the relevant contracts. First, the chapter covers emerging issues around access and re-use of transport data that may be hindering ‘outside of the box’ thinking and the deployment of artificial intelligence in this area. Second, it discusses some well-known ‘inside the box’ regulatory issues around the exercise of discretion in the choice of sustainable technological solutions, the constraints surrounding certain types of complex and collaborative procurement, and the difficulties in monitoring contract compliance clauses. Third, the chapter arrives at the realisation that the main challenges in delivering and supporting smart urban mobility through procurement relate to the higher-level or cross-cutting challenges of the professionalisation of the procurement workforce and the need to bridge significant (and growing) knowledge gaps. It thus explores existing policy interventions aimed at the professionalisation and networking of procurement officials. The chapter concludes with some overall reflections, and a call for a more active role to be taken by the new Von der Leyen Commission.",project-academic
10.1109/CCNC49032.2021.9369452,2021-01-09,p,IEEE,real time and energy efficient inference at gpu based network edge using pon," In recent years, advances in deep learning (DL) technology have greatly improved research and services related to artificial intelligence (AI). In particular, real-time object recognition has become an important technology in smart cities. To achieve this, low-cost network deployment and low-latency data transfer are the key technologies. In this paper, we focus on Time- and Wavelength-Division Multiplexed Passive Optical Network (TWDM-PON) based inference systems to deploy cost-efficient networks that accommodate many network cameras. A significant issue for a GPU-based inference system via TWDM-PON is optimally allocating upstream wavelength and bandwidth to enable real-time inference. However, it has not been considered to increase the batch size of arrival data at edge servers ensuring low-latency transmission. Therefore, this paper proposes a concept of an inference system in which a large number of cameras periodically upload image data to a GPU-based server via TWDM-PONe We also propose a cooperative wavelength and bandwidth allocation algorithm to ensure low-latency and time-synchronized data arrival at the edge. The performance of the proposed scheme is verified with computer simulation.",project-academic
10.1109/AICCSA.2016.7945679,2016-11-01,p,IEEE,qlar a q learning based adaptive routing for manets," Mobile Ad-hoc Networks are highly reconfigurable networks of mobile nodes which communicate by wireless links. The main issues in MANETs include the mobility of the network nodes, energy limitations and bandwidth. Thus, routing protocols should explicitly consider network changes into the algorithm design. In order to support service requirements of multimedia and real-time applications, the routing protocol must provide Quality of Service (QoS) in terms of packets loss and average End-to-End Delay (ETED). This work proposes a Q-Learning based Adaptive Routing model (QLAR), developed via Reinforcement Learning (RL) techniques, which has the ability to detect the level of mobility at different points of time so that each individual node can update routing metric accordingly. The proposed protocol introduces: (i) new model, developed via Q-Learning technique, to detect the level of mobility at each node in the network; (ii) a new metric, called Qmetric, which account for the static and dynamic routing metrics, and which are combined and updated to the changing network topologies. The proposed metric and routing model in this paper are deployed on the Optimized Link State Routing (OLSR) protocol. Extensive simulations validate the effectiveness of the proposed model, through comparisons with the standard OLSR protocols.",project-academic
10.1109/TNSE.2021.3051660,2021-07-01,a,Institute of Electrical and Electronics Engineers (IEEE),reinforcement learning power control algorithm based on graph signal processing for ultra dense mobile networks," Ultra-dense mobile networks (UDMNs) represent a promising technology for improving the network performance and providing the ubiquitous network accessibility in the beyond 5 G (B5G) mobile networks. Heterogenous densely deployed networks can dynamically offer high spectrum efficiency and enhance frequency reuse, which ultimately improves quality of service (QoS) and the user experience. However, mass inter- or intra-cell interference generated from overlap between small cells greatly limits network performance, especially when there is mobility between UEs and access points (APs). Even so, when network density increases, the complexity of conventional allocation methods can increase also. In this paper, we investigate a power control of downlink (DL) connection in the UNMNs with different types of APs. We propose a reinforcement learning (RL) power allocation algorithm based on graph signal processing (GSP) for ultra-dense mobile networks. Firstly, we construct a realistic system model under ultra-dense mobile networking, which includes the system channel mode and instantaneous rate. Then we employ a GSP tool to analyze network interference, the interference analysis results for the entire network are obtained to determine optimal RL power allocation. Finally, simulation results indicate that the proposed RL power control algorithm outperforms baseline algorithms when applied to a ultra-dense mobile networks.",project-academic
10.1109/JIOT.2020.2983089,2020-03-24,a,Institute of Electrical and Electronics Engineers (IEEE),a hybrid machine learning model for demand prediction of edge computing based bike sharing system using internet of things," The rapid development of Internet-of-Things technologies (such as edge computing) has promoted the development of numerous emerging urban applications, particularly smart transportation. As an anticipated aspect of smart transportation, bike-sharing systems have recently been deployed in many cities and are considered an efficient way to address the issue of “the last mile.” In a bike-sharing system, the supply and demand of shared bikes at each bike station frequently change over time. Consequently, one of the most challenging issues of a bike-sharing system is predicting the required number of shared bikes at each station. In this article, we take the real aspects of a bike-sharing system into account, e.g., the high complexity, nonlinearity, and uncertainty of the traffic flow, and propose a hybrid edge-computing-based machine learning model. Notably, our proposed model, which combines a self-organizing mapping network with a regression tree (RT), is applied to predict the bicycle demand of a certain station through the following steps: 1) the proposed model adopts self-organization mapping to assemble the original samples in the form of clusters and 2) each cluster is then built as an RT to forecast the required number of bikes at each station. Experiments based on real data from the Washington and London bike-sharing systems show that our proposed method achieves a higher prediction accuracy and better generalization than previous approaches.",project-academic
10.1016/J.COMPBIOMED.2020.103792,2020-06-01,a,Pergamon,automated detection of covid 19 cases using deep neural networks with x ray images," The novel coronavirus 2019 (COVID-2019), which first appeared in Wuhan city of China in December 2019, spread rapidly around the world and became a pandemic. It has caused a devastating effect on both daily lives, public health, and the global economy. It is critical to detect the positive cases as early as possible so as to prevent the further spread of this epidemic and to quickly treat affected patients. The need for auxiliary diagnostic tools has increased as there are no accurate automated toolkits available. Recent findings obtained using radiology imaging techniques suggest that such images contain salient information about the COVID-19 virus. Application of advanced artificial intelligence (AI) techniques coupled with radiological imaging can be helpful for the accurate detection of this disease, and can also be assistive to overcome the problem of a lack of specialized physicians in remote villages. In this study, a new model for automatic COVID-19 detection using raw chest X-ray images is presented. The proposed model is developed to provide accurate diagnostics for binary classification (COVID vs. No-Findings) and multi-class classification (COVID vs. No-Findings vs. Pneumonia). Our model produced a classification accuracy of 98.08% for binary classes and 87.02% for multi-class cases. The DarkNet model was used in our study as a classifier for the you only look once (YOLO) real time object detection system. We implemented 17 convolutional layers and introduced different filtering on each layer. Our model (available at (https://github.com/muhammedtalo/COVID-19)) can be employed to assist radiologists in validating their initial screening, and can also be employed via cloud to immediately screen patients.",project-academic
10.1371/JOURNAL.PONE.0119044,2015-03-17,a,Public Library of Science,large scale transportation network congestion evolution prediction using deep learning theory," Understanding how congestion at one location can cause ripples throughout large-scale transportation network is vital for transportation researchers and practitioners to pinpoint traffic bottlenecks for congestion mitigation. Traditional studies rely on either mathematical equations or simulation techniques to model traffic congestion dynamics. However, most of the approaches have limitations, largely due to unrealistic assumptions and cumbersome parameter calibration process. With the development of Intelligent Transportation Systems (ITS) and Internet of Things (IoT), transportation data become more and more ubiquitous. This triggers a series of data-driven research to investigate transportation phenomena. Among them, deep learning theory is considered one of the most promising techniques to tackle tremendous high-dimensional data. This study attempts to extend deep learning theory into large-scale transportation network analysis. A deep Restricted Boltzmann Machine and Recurrent Neural Network architecture is utilized to model and predict traffic congestion evolution based on Global Positioning System (GPS) data from taxi. A numerical study in Ningbo, China is conducted to validate the effectiveness and efficiency of the proposed method. Results show that the prediction accuracy can achieve as high as 88% within less than 6 minutes when the model is implemented in a Graphic Processing Unit (GPU)-based parallel computing environment. The predicted congestion evolution patterns can be visualized temporally and spatially through a map-based platform to identify the vulnerable links for proactive congestion mitigation.",project-academic
10.1145/1082473.1082545,2005-07-25,p,ACM,multiagent traffic management an improved intersection control mechanism," Traffic congestion is one of the leading causes of lost productivity and decreased standard of living in urban settings. Recent advances in artificial intelligence suggest vehicle navigation by autonomous agents will be possible in the near future. In a previous paper, we proposed a reservation-based system for alleviating traffic congestion, specifically at intersections. This paper extends our prototype implementation in several ways with the aim of making it more implementable in the real world. In particular, we 1) add the ability of vehicles to turn, 2) enable them to accelerate while in the intersection, and 3) augment their interaction capabilities with a detailed protocol such that the vehicles do not need to know anything about the intersection control policy. The use of this protocol limits the interaction of the driver agent and the intersection manager to the extent that it is a reasonable approximation of reliable wireless communication. Finally, we describe how different intersection control policies can be expressed with this protocol and limited exchange of information. All three improvements are fully implemented and tested, and we present detailed empirical results validating their effectiveness.",project-academic
10.1109/TITS.2006.874716,2006-09-01,a,IEEE,neural networks for real time traffic signal control," Real-time traffic signal control is an integral part of the urban traffic control system, and providing effective real-time traffic signal control for a large complex traffic network is an extremely challenging distributed control problem. This paper adopts the multiagent system approach to develop distributed unsupervised traffic responsive signal control models, where each agent in the system is a local traffic signal controller for one intersection in the traffic network. The first multiagent system is developed using hybrid computational intelligent techniques. Each agent employs a multistage online learning process to update and adapt its knowledge base and decision-making mechanism. The second multiagent system is developed by integrating the simultaneous perturbation stochastic approximation theorem in fuzzy neural networks (NN). The problem of real-time traffic signal control is especially challenging if the agents are used for an infinite horizon problem, where online learning has to take place continuously once the agent-based traffic signal controllers are implemented into the traffic network. A comprehensive simulation model of a section of the Central Business District of Singapore has been developed using PARAMICS microscopic simulation program. Simulation results show that the hybrid multiagent system provides significant improvement in traffic conditions when evaluated against an existing traffic signal control algorithm as well as the SPSA-NN-based multiagent system as the complexity of the simulation scenario increases. Using the hybrid NN-based multiagent system, the mean delay of each vehicle was reduced by 78% and the mean stoppage time, by 85% compared to the existing traffic signal control algorithm. The promising results demonstrate the efficacy of the hybrid NN-based multiagent system in solving large-scale traffic signal control problems in a distributed manner",project-academic
10.1101/2020.10.29.20222547,2020-11-03,a,Cold Spring Harbor Laboratory Press,public mobility data enables covid 19 forecasting and management at local and global scales," Abstract None Policymakers everywhere are working to determine the set of restrictions that will effectively contain the spread of COVID-19 without excessively stifling economic activity. We show that publicly available data on human mobility — collected by Google, Facebook, and other providers — can be used to evaluate the effectiveness of non-pharmaceutical interventions and forecast the spread of COVID-19. This approach relies on simple and transparent statistical models, and involves minimal assumptions about disease dynamics. We demonstrate the effectiveness of this approach using local and regional data from China, France, Italy, South Korea, and the United States, as well as national data from 80 countries around the world. None Summary None Background None Policymakers everywhere are working to determine the set of restrictions that will effectively contain the spread of COVID-19 without excessively stifling economic activity. In some contexts, decision-makers have access to sophisticated epidemiological models and detailed case data. However, a large number of decisions, particularly in low-income and vulnerable communities, are being made with limited or no modeling support. We examine how public human mobility data can be combined with simple statistical models to provide near real-time feedback on non-pharmaceutical policy interventions. Our objective is to provide a simple framework that can be easily implemented and adapted by local decision-makers. None Methods None We develop simple statistical models to measure the effectiveness of non-pharmaceutical interventions (NPIs) and forecast the spread of COVID-19 at local, state, and national levels. The method integrates concepts from econometrics and machine learning, and relies only upon publicly available data on human mobility. The approach does not require explicit epidemiological modeling, and involves minimal assumptions about disease dynamics. We evaluate this approach using local and regional data from China, France, Italy, South Korea, and the United States, as well as national data from 80 countries around the world. None Findings None We find that NPIs are associated with significant reductions in human mobility, and that changes in mobility can be used to forecast COVID-19 infections. The first set of results show the impact of NPIs on human mobility at all geographic scales. While different policies have different effects on different populations, we observed total reductions in mobility between 40 and 84 percent. The second set of results indicate that — even in the absence of other epidemiological information — mobility data substantially improves 10-day case rates forecasts at the county (20.75% error, US), state (21.82 % error, US), and global (15.24% error) level. Finally, for example, country-level results suggest that a shelter-in-place policy targeting a 10% increase in the amount of time spent at home would decrease the propagation of new cases by 32% by the end of a 10 day period. None Interpretation None In rapidly evolving disease outbreaks, decision-makers do not always have immediate access to sophisticated epidemiological models. In such cases, valuable insight can still be derived from simple statistic models and readily-available public data. These models can be quickly fit with a population’s own data and updated over time, thereby capturing social and epidemiological dynamics that are unique to a specific locality or time period. Our results suggest that this approach can effectively support decision-making from local (e.g., city) to national scales.",project-academic
10.1016/J.TRPRO.2016.05.240,2016-01-01,a,Elsevier Publications,advanced driver assistance system for road environments to improve safety and efficiency," The advances in Information Technologies have led to more complex road safety applications. These systems provide multiple possibilities for improving road transport. The integrated system that this paper presents deals with two aspects that have been identified as key topics: safety and efficiency. To this end, the development and implementation of an integrated advanced driver assistance system (ADAS) for rural and intercity environments is proposed. The system focuses mainly on single-carriageways roads, given the complexity of these environments compared to motorways and the high number of severe and fatal accidents on them. The proposed system is based on advanced perception techniques, vehicle automation and communications between vehicles (V2V) and with the infrastructure (V2I). Sensor fusion architecture based on computer vision and laser scanner technologies are developed. It allows real time detection and classification of obstacles, and the identification of potential risks. The driver receives this information and some warnings generated by the system. In case, he does not react in a proper way, the vehicle could perform autonomous actions (both on speed control or steering maneuvers) to improve safety and/or efficiency. Furthermore, a multimodal V2V and V2I communication system, based on GeoNetworking, facilitates the flow of information between vehicles and assists in the detection and information broadcasting processes. All this, combined with vehicle positioning, detailed digital maps and advanced map-matching algorithms, establish the decision algorithms of different ADAS systems. The applications developed include: adaptive cruise control with consumption optimization, overtaking assistance system in single-carriageways roads that takes into account appropriate speed evolution and identifies most suitable road stretches for the maneuver; assistance system in intersections with speed control during approximation maneuvers, and collision avoidance system with the possibility of evasive maneuvers. To this end, mathematical vehicle dynamics models have been used to ensure the stability, and propulsion system models are used to establish efficient patterns, Artificial Intelligence and simulation are used for experimentation and evaluation of algorithms to be implemented in the control unit. Finally, the system is designed to warn the driver if a risk is detected and, if necessary, to take control of the vehicle. The system has been implemented on a passenger car and has been tested in specific scenarios on a test track with satisfactory results. Language: en",project-academic
10.1007/S11269-006-0326-4,2006-06-01,a,Kluwer Academic Publishers,an intelligent decision support system for management of floods," Integrating human knowledge with modeling tools, an intelligent decision support system (DSS) is developed to assist decision makers during different phases of flood management. The DSS is developed as a virtual planning tool and can address both engineering and non-engineering issues related to flood management. Different models (hydrodynamic, forecasting, and economic) that are part of the DSS share data and communicate with each other by providing feedback. The DSS is able to assist in: selecting suitable flood damage reduction options (using an expert system approach); forecasting floods (using artificial neural networks approach); modeling the operation of flood control structures; and describing the impacts (area flooded and damage) of floods in time and space. The proposed DSS is implemented for the Red River Basin in Manitoba, Canada. The results from the test application of DSS for 1997 flood in the Red River Basin are very promising. The DSS is able to predict the peak flows with 2% error and reveals that with revised operating rules the contribution of Assiniboine River to the flooding of Winnipeg city can be significantly reduced. The decision support environment allows a number of ""what-if"" type questions to be asked and answered, thus, multiple decisions can be tried without having to deal with the real life consequences.",project-academic
,2018-10-12,,,smart city fire protection iot big data management system," The present invention provides a smart city fire protection IOT (Internet of Thighs) big data management system, and relates to the technical field of fire protection. The system comprises a data collection mechanism for collecting historical data and real-time data, a data processing mechanism for performing machine learning on the historical data and establishing a predictive model, a data analysis mechanism for analyzing the real-time data by using the predictive model and making a fire occurrence probability, and a data transmission mechanism for transmitting the fire occurrence probability to customers. The historical data comprises hardware information and fire records. The hardware information comprises the historical usage time of the firefighting hardware corresponding to different moments in the past, historical maintenance of the firefighting hardware, and historical fire alarm information collected by using the firefighting hardware. The fire records comprise the time, theplace and the cause of fire occurrence. The real-time data comprises the real-time usage time of the firefighting hardware, real-time maintenance of the firefighting hardware, and real-time fire alarminformation collected by using the firefighting hardware. According to the system provided by the present invention, the integrated management of the fire protection industry can be implemented, andthe work efficiency and management of fire protection can be improved.",project-academic
10.1109/ITSC.2017.8317908,2017-10-01,p,IEEE,rebalancing shared mobility on demand systems a reinforcement learning approach," Shared mobility-on-demand systems have very promising prospects in making urban transportation efficient and affordable. However, due to operational challenges among others, many mobility applications still remain niche products. This paper addresses rebalancing needs that are critical for effective fleet management in order to offset the inevitable imbalance of vehicle supply and travel demand. Specifically, we propose a reinforcement learning approach which adopts a deep Q network and adaptively moves idle vehicles to regain balance. This innovative model-free approach takes a very different perspective from the state-of-the-art network-based methods and is able to cope with large-scale shared systems in real time with partial or full data availability. We apply this approach to an agent-based simulator and test it on a London case study. Results show that, the proposed method outperforms the local anticipatory method by reducing the fleet size by 14% while inducing little extra vehicle distance traveled. The performance is close to the optimal solution yet the computational speed is 2.5 times faster. Collectively, the paper concludes that the proposed rebalancing approach is effective under various demand scenarios and will benefit both travelers and operators if implemented in shared mobility-on-demand systems.",project-academic
,2018-01-01,a,,rebalancing shared mobility on demand systems a reinforcement learning approach," Shared mobility-on-demand systems have very promising prospects in making urban transportation efficient and affordable. However, due to operational challenges among others, many mobility applications still remain niche products. This paper addresses rebalancing needs that are critical for effective fleet management in order to offset the inevitable imbalance of vehicle supply and travel demand. Specifically, we propose a reinforcement learning approach which adopts a deep Q network and adaptively moves idle vehicles to regain balance. This innovative model-free approach takes a very different perspective from the state-of-the-art network-based methods and is able to cope with large-scale shared systems in real time with partial or full data availability. We apply this approach to an agent-based simulator and test it on a London case study. Results show that, the proposed method outperforms the local anticipatory method by reducing the fleet size by 14% while inducing little extra vehicle distance traveled. The performance is close to the optimal solution yet the computational speed is 2.5 times faster. Collectively, the paper concludes that the proposed rebalancing approach is effective under various demand scenarios and will benefit both travelers and operators if implemented in shared mobility-on-demand systems.",project-academic
10.3390/SMARTCITIES1010008,2018-11-06,a,Multidisciplinary Digital Publishing Institute,digital systems in smart city and infrastructure digital as a service," Digitalization has enabled infrastructure and cities to be “smarter”; the use of physical space and energy, the transmission of information, the management of users, assets and processes, the operation of businesses and companies have been progressively digitalized. The main challenges of a Smart City is its definition, scope and interconnections; there are different approaches to Smart City implementations that vary from collaborative multidisciplinary environments, the addition of Information and Communications Technology (ICT) within its physical fabric to the use of Big Data for higher abstraction decisions. This paper presents the concept of Digital as a Service (DaaS), where any complete digitalization can be implemented independently of its associated physical infrastructure in a Cloud environment; DasS would enable an interoperable Virtual Digital Infrastructure (VDI). In addition, this paper reviews the current Digital Systems, Transmission Networks, Servers and Management Systems. The next Industrial Revolution will be founded on Artificial Intelligence that will entirely replace humans by taking production and management decisions based on the Internet of Things (IoT), the Cloud, BlockChain, Big Data, Virtual Reality and the combination of digital and real infrastructure or city. Digital as a Service would be its enabler by providing the entire interconnection, integration and virtualization of its Space, Services and Structure (3S).",project-academic
,2019-02-05,a,,global convergence of neuron birth death dynamics," Neural networks with a large number of parameters admit a mean-field description, which has recently served as a theoretical explanation for the favorable training properties of ""overparameterized"" models. In this regime, gradient descent obeys a deterministic partial differential equation (PDE) that converges to a globally optimal solution for networks with a single hidden layer under appropriate assumptions. In this work, we propose a non-local mass transport dynamics that leads to a modified PDE with the same minimizer. We implement this non-local dynamics as a stochastic neuronal birth-death process and we prove that it accelerates the rate of convergence in the mean-field limit. We subsequently realize this PDE with two classes of numerical schemes that converge to the mean-field equation, each of which can easily be implemented for neural networks with finite numbers of parameters. We illustrate our algorithms with two models to provide intuition for the mechanism through which convergence is accelerated.",project-academic
10.1002/ROB.21418,2012-05-01,a,John Wiley and Sons Ltd.,development of the six legged walking and climbing robot spaceclimber," In this article, we present SpaceClimber,1 a six-legged, bio-inspired, energy-efficient and adaptable free-climbing robot for mobility on steep gradients. The long-term stool is to provide a system for extraterrestrial surface exploration missions, paying special attention to mobility in lunar craters to retrieve or analyze scientific samples from the interior of these craters. We present an envisaged mission for SpaceClimber and summarize the deriving system requirements. The robot's morphology determination procedure is depicted, considering the predefined demands and utilizing a simulation environment in combination with evolutionary optimization strategies, followed by a detailed description of the system's hardware design. The theoretical concept for the control of such machines with an extensive sensory–motor configuration is explained, as well as the implemented locomotion control approach and attempts to optimize the behavior of the robot using machine learning techniques. In addition, the experimental plant that was built for testing and evaluating the performance of the developed system in an environment as realistic as possible is introduced, followed by a description of the experiments performed. Concluding, we summarize the results and experiences and give an outlook on further developments. © 2012 Wiley Periodicals, Inc.

(Web page: http://wwww.dfki.de/robotik.)",project-academic
10.1145/3274895.3274909,2021-06-25,a,,deeploc a ubiquitous accurate and low overhead outdoor cellular localization system," Recent years have witnessed fast growth in outdoor location-based services. While GPS is considered a ubiquitous localization system, it is not supported by low-end phones, requires direct line of sight to the satellites, and can drain the phone battery quickly. 
In this paper, we propose DeepLoc: a deep learning-based outdoor localization system that obtains GPS-like localization accuracy without its limitations. In particular, DeepLoc leverages the ubiquitous cellular signals received from the different cell towers heard by the mobile device as hints to localize it. To do that, crowd-sensed geo-tagged received signal strength information coming from different cell towers is used to train a deep model that is used to infer the user's position. As part of DeepLoc design, we introduce modules to address a number of practical challenges including scaling the data collection to large areas, handling the inherent noise in the cellular signal and geo-tagged data, as well as providing enough data that is required for deep learning models with low-overhead. 
We implemented DeepLoc on different Android devices. Evaluation results in realistic urban and rural environments show that DeepLoc can achieve a median localization accuracy within 18.8m in urban areas and within 15.7m in rural areas. This accuracy outperforms the state-of-the-art cellular-based systems by more than 470% and comes with 330% savings in power compared to the GPS. This highlights the promise of DeepLoc as a ubiquitous accurate and low-overhead localization system.",project-academic
10.1109/JSEN.2016.2592359,2016-07-19,a,IEEE,flash flood detection in urban cities using ultrasonic and infrared sensors," Floods are the most common type of natural disaster, often leading to loss of lives and properties in the thousands yearly. Among these events, urban flash floods are particularly deadly because of the short timescales on which they occur, and because of the population density of cities. Since most flood casualties are caused by a lack of information on the impending flood (type, location, and severity), sensing these events is critical to generate accurate and detailed warnings and short term forecasts. However, no dedicated flash flood sensing systems, that could monitor the propagation of flash floods, in real time, currently exist in cities. In this paper, first, a new sensing device that can simultaneously monitor urban flash floods and traffic congestion has been presented. This sensing device is based on the combination of ultrasonic range finding with remote temperature sensing, and can sense both phenomena with a high degree of accuracy, using a combination of L1-regularized reconstruction and artificial neural networks to process measurement data. Second, corresponding algorithms have been implemented on a low-power wireless sensor platform, and their performance in water level estimation in a six months test involving four different sensors is illustrated. The results demonstrate that urban water levels can be reliably estimated with error less than 2 cm, and that the preprocessing and machine learning schemes can run in real time on currently available wireless sensor platforms.",project-academic
10.1186/S13673-019-0190-9,2019-12-01,a,SpringerOpen,ciot net a scalable cognitive iot based smart city network architecture," In the recent era, artificial intelligence (AI) is being used to support numerous solutions for human beings, such as healthcare, autonomous transportation, and so on. Cognitive computing is represented as a next-generation application AI-based solutions which provide human–machine interaction with personalized interactions and services that imitate human behavior. On the other hand, a large volume of data is generated from smart city applications such as healthcare, smart transportation, retail industry, and firefighting. There is always a concern on how to efficiently manage the large volume of generated data. Recently many existing researches discussed the analysis of the large quantity of data using cognitive computing; however, these researches are failed to handle the certain problems, namely scalability, and flexibility of data gathered in a smart city environment. Data captured from millions of sensors can be cross implemented across various cognitive computing applications to ensure real-time responses. In this paper, we study the cognitive internet of things (CIoT) and propose a CIoT-based smart city network (CIoT-Net) architecture which describes how data gathered from smart city applications can be analyzed using cognitive computing and handle the scalability and flexibility problems. We discuss various technologies such as AI and big data analysis to implement the proposed architecture. Finally, we describe the possible research challenges and opportunities while implementing the proposed architecture.",project-academic
10.1021/ACSNANO.7B08272,2018-01-17,a,American Chemical Society,mimicking biological synaptic functionality with an indium phosphide synaptic device on silicon for scalable neuromorphic computing," Neuromorphic or ""brain-like"" computation is a leading candidate for efficient, fault-tolerant processing of large-scale data as well as real-time sensing and transduction of complex multivariate systems and networks such as self-driving vehicles or Internet of Things applications. In biology, the synapse serves as an active memory unit in the neural system and is the component responsible for learning and memory. Electronically emulating this element via a compact, scalable technology which can be integrated in a three-dimensional (3-D) architecture is critical for future implementations of neuromorphic processors. However, present day 3-D transistor implementations of synapses are typically based on low-mobility semiconductor channels or technologies that are not scalable. Here, we demonstrate a crystalline indium phosphide (InP)-based artificial synapse for spiking neural networks that exhibits elasticity, short-term plasticity, long-term plasticity, metaplasticity, and spike timing-dependent plasticity, emulating the critical behaviors exhibited by biological synapses. Critically, we show that this crystalline InP device can be directly integrated via back-end processing on a Si wafer using a SiO2 buffer without the need for a crystalline seed, enabling neuromorphic devices that can be implemented in a scalable and 3-D architecture. Specifically, the device is a crystalline InP channel field-effect transistor that interacts with neuron spikes by modification of the population of filled traps in the MOS structure itself. Unlike other transistor-based implementations, we show that it is possible to mimic these biological functions without the use of external factors (e.g., surface adsorption of gas molecules) and without the need for the high electric fields necessary for traditional flash-based implementations. Finally, when exposed to neuronal spikes with a waveform similar to that observed in the brain, these devices exhibit the ability to learn without the need for any external potentiating/depressing circuits, mimicking the biological process of Hebbian learning.",project-academic
10.1016/J.ENVSOFT.2003.10.003,2004-10-01,a,Elsevier,modelling so2 concentration at a point with statistical approaches," In this paper, the results obtained by inter-comparing several statistical techniques for modelling SO2 concentration at a point such as neural networks, fuzzy logic, generalised additive techniques and other recently proposed statistical approaches are reported. The results of the inter-comparison are the fruits of collaboration between some of the partners of the APPETISE project funded under the Framework V Information Societies and Technologies (IST) programme. Two different cases for study were selected: the Siracusa industrial area, in Italy, where the pollution is dominated by industrial emissions and the Belfast urban area, in the UK, where domestic heating makes an important contribution. The different kinds of pollution (industrial/urban) and different locations of the areas considered make the results more general and interesting. In order to make the inter-comparison more objective, all the modellers considered the same datasets. Missing data in the original time series was filled by using appropriate techniques. The inter-comparison work was carried out on a rigorous basis according to the performance indices recommended by the European Topic Centre on Air and Climate Change (ETC/ACC). The targets for the implemented prediction models were defined according to the EC normative relating to limit values for sulphur dioxide. According to this normative, three different kinds of targets were considered namely daily mean values, daily maximum values and hourly mean values. The inter-compared models were tested on real cases of poor air quality. In the paper, the inter-compared techniques are ranked in terms of their capability to predict critical episodes. A ranking in terms of their predictability of the three different targets considered is also proposed. Several key issues are illustrated and discussed such as the role of input variable selection, the use of meteorological data, and the use of interpolated time series. Moreover, a novel approach referred to as the technique of balancing the training pattern set, which was successfully applied to improve the capability of ANN models to predict exceedences is introduced. The results show that there is no single modelling approach, which generates optimum results in terms of the full range of performance indices considered. In view of the implementation of a warning system for air quality control, approaches that are able to work better in the prediction of critical episodes must be preferred. Therefore, the artificial neural network prediction models can be recommended for this purpose. The best forecasts were achieved for daily averages of SO2 while daily maximum and hourly mean values are difficult to predict with acceptable accuracy.",project-academic
10.3390/RS11111343,2019-06-04,a,MDPI AG,building instance change detection from large scale aerial images using convolutional neural networks and simulated samples," © 2019 by the authors. We present a novel convolutional neural network (CNN)-based change detection framework for locating changed building instances as well as changed building pixels from very high resolution (VHR) aerial images. The distinctive advantage of the framework is the self-training ability, which is highly important in deep-learning-based change detection in practice, as high-quality samples of changes are always lacking for training a successful deep learning model. The framework consists two parts: a building extraction network to produce a binary building map and a building change detection network to produce a building change map. The building extraction network is implemented with two widely used structures: a Mask R-CNN for object-based instance segmentation, and a multi-scale full convolutional network for pixel-based semantic segmentation. The building change detection network takes bi-temporal building maps produced from the building extraction network as input and outputs a building change map at the object and pixel levels. By simulating arbitrary building changes and various building parallaxes in the binary building map, the building change detection network is well trained without real-life samples. This greatly lowers the requirements of labeled changed buildings, and guarantees the algorithm's robustness to registration errors caused by parallaxes. To evaluate the proposed method, we chose a wide range of urban areas from an open-source dataset as training and testing areas, and both pixel-based and object-based model evaluation measures were used. Experiments demonstrated our approach was vastly superior: without using any real change samples, it reached 63% average precision (AP) at the object (building instance) level. In contrast, with adequate training samples, other methods-including the most recent CNN-based and generative adversarial network (GAN)-based ones-have only reached 25% AP in their best cases.",project-academic
10.1007/S11042-017-5472-5,2018-08-01,a,Springer US,real time pedestrian crossing lights detection algorithm for the visually impaired," In defect of intelligent assistant approaches, the visually impaired feel hard to cross the roads in urban environments. Aiming to tackle the problem, a real-time Pedestrian Crossing Lights (PCL) detection algorithm for the visually impaired is proposed in this paper. Different from previous works which utilize analytic image processing to detect the PCL in ideal scenarios, the proposed algorithm detects PCL using machine learning scheme in the challenging scenarios, where PCL have arbitrary sizes and locations in acquired image and suffer from the shake and movement of camera. In order to achieve the robustness and efficiency in those scenarios, the detection algorithm is designed to include three procedures: candidate extraction, candidate recognition and temporal-spatial analysis. A public dataset of PCL, which includes manually labeled ground truth data, is established for tuning parameters, training samples and evaluating the performance. The algorithm is implemented on a portable PC with color camera. The experiments carried out in various practical scenarios prove that the precision and recall of detection are both close to 100%, meanwhile the frame rate is up to 21 frames per second (FPS).",project-academic
10.1049/IET-ITS.2018.5172,2018-12-01,a,The Institution of Engineering and Technology,real time detection of distracted driving based on deep learning," Driver distraction is a leading factor in car crashes. With a goal to reduce traffic accidents and improve transportation safety, this study proposes a driver distraction detection system which identifies various types of distractions through a camera observing the driver. An assisted driving testbed is developed for the purpose of creating realistic driving experiences and validating the distraction detection algorithms. The authors collected a dataset which consists of images of the drivers in both normal and distracted driving postures. Four deep convolutional neural networks including VGG-16, AlexNet, GoogleNet, and residual network are implemented and evaluated on an embedded graphic processing unit platform. In addition, they developed a conversational warning system that alerts the driver in real-time when he/she does not focus on the driving task. Experimental results show that the proposed approach outperforms the baseline one which has only 256 neurons in the fully-connected layers. Furthermore, the results indicate that the GoogleNet is the best model out of the four for distraction detection in the driving simulator testbed.",project-academic
10.1109/JAS.2021.1003907,2021-02-19,a,Institute of Electrical and Electronics Engineers (IEEE),domain invariant similarity activation map metric learning for retrieval based long term visual localization," Visual localization is a crucial component in the application of mobile robot and autonomous driving. Image retrieval is an efficient and effective technique in image-based localization methods. Due to the drastic variability of environmental conditions, e.g., illumination changes, retrieval-based visual localization is severely affected and becomes a challenging problem. In this work, a general architecture is first formulated probabilistically to extract domain-invariant features through multi-domain image translation. Then, a novel gradient-weighted similarity activation mapping loss ( Grad-SAM ) is incorporated for finer localization with high accuracy. We also propose a new adaptive triplet loss to boost the metric learning of the embedding in a self-supervised manner. The final coarse-to-fine image retrieval pipeline is implemented as the sequential combination of models with and without Grad-SAM loss. Extensive experiments have been conducted to validate the effectiveness of the proposed approach on the CMU-Seasons dataset. The strong generalization ability of our approach is verified with the RobotCar dataset using models pre-trained on urban parts of the CMU-Seasons dataset. Our performance is on par with or even outperforms the state-of-the-art image-based localization baselines in medium or high precision, especially under challenging environments with illumination variance, vegetation, and night-time images. Moreover, real-site experiments have been conducted to validate the efficiency and effectiveness of the coarse-to-fine strategy for localization.",project-academic
,2020-09-16,a,,domain invariant similarity activation map metric learning for retrieval based long term visual localization," Visual localization is a crucial component in the application of mobile robot and autonomous driving. Image retrieval is an efficient and effective technique in image-based localization methods. Due to the drastic variability of environmental conditions, e.g. illumination, seasonal and weather changes, retrieval-based visual localization is severely affected and becomes a challenging problem. In this work, a general architecture is first formulated probabilistically to extract domain-invariant feature through multi-domain image translation. And then a novel gradient-weighted similarity activation mapping loss (Grad-SAM) is incorporated for finer localization with high accuracy. We also propose a new adaptive triplet loss to boost the metric learning of the embedding in a self-supervised manner. The final coarse-to-fine image retrieval pipeline is implemented as the sequential combination of models without and with Grad-SAM loss. Extensive experiments have been conducted to validate the effectiveness of the proposed approach on the CMU-Seasons dataset. The strong generalization ability of our approach is verified on RobotCar dataset using models pre-trained on urban part of CMU-Seasons dataset. Our performance is on par with or even outperforms the state-of-the-art image-based localization baselines in medium or high precision, especially under the challenging environments with illumination variance, vegetation and night-time images. Moreover, real-site experiments have been conducted to validate the efficiency and effectiveness of the coarse-to-fine strategy for localization.",project-academic
10.1109/TWC.2020.2969627,2020-02-03,a,IEEE,machine learning based channel prediction in massive mimo with channel aging," To support the ever increasing number of devices in massive multiple-input multiple-output (mMIMO) systems, an excessive amount of overhead is required for conventional orthogonal pilot-based channel estimation schemes. To circumvent this fundamental constraint, we design a machine learning (ML)-based time-division duplex scheme in which channel state information (CSI) can be obtained by leveraging the temporal channel correlation. The presence of the temporal channel correlation is due to the stationarity of the propagation environment across time. The proposed ML-based predictors involve a pattern extraction implemented via a convolutional neural network, and a CSI predictor realized by an autoregressive (AR) predictor or an autoregressive network with exogenous inputs recurrent neural network. Closed-form expressions for the user uplink and downlink achievable spectral efficiency and average per-user throughput are provided for the ML-based time division duplex schemes. Our numerical results demonstrate that the proposed ML-based predictors can remarkably improve the prediction quality for both low and high mobility scenarios, and offer great performance gains on the per-user achievable throughput.",project-academic
10.1016/J.IJEPES.2012.07.056,2013-01-01,a,Elsevier,adaptive centralized protection scheme for distribution systems with dg using risk analysis for protective devices placement," Abstract None None Conventional electric distribution systems are radial in nature, supplied at one end through a main source. These networks generally have a simple protection system usually implemented using fuses, re-closers, and over-current relays. Recently, great attention has been paid to applying Distributed Generation (DG) throughout electric distribution systems. Presence of such generation in a network leads to losing coordination of protection devices. Therefore, it is desired to develop an algorithm which is capable of protecting distribution systems that include DG, through diagnosis and isolation of faults. A new approach for the protection of distribution networks in the presence of DGs is presented in this paper. The algorithm is based on dividing an existing distribution network into several zones, each capable of operating in island operation. In the suggested method, risk analysis is used to optimize the protection zones by optimal placement of protective devices. Multilayer Perceptrons (MLPs) neural networks are used for determination of faults. The proposed scheme has been implemented on a selected part of a real distribution network of a large city and a MATLAB based developed software has been used to implement the proposed algorithm on the real network data.",project-academic
10.1145/3300061.3345430,2019-10-11,p,ACM,learning to coordinate video codec with transport protocol for mobile video telephony," Despite the pervasive use of real-time video telephony services, the users' quality of experience (QoE) remains unsatisfactory, especially over the mobile Internet. Previous work studied the problem via controlled experiments, while a systematic and in-depth investigation in the wild is still missing. To bridge the gap, we conduct a large-scale measurement campaign on \appname, an operational mobile video telephony service. Our measurement logs fine-grained performance metrics over 1 million video call sessions. Our analysis shows that the application-layer video codec and transport-layer protocols remain highly uncoordinated, which represents one major reason for the low QoE. We thus propose \name, a machine learning based framework to resolve the issue. Instead of blindly following the transport layer's estimation of network capacity, \name reviews historical logs of both layers, and extracts high-level features of codec/network dynamics, based on which it determines the highest bitrates for forthcoming video frames without incurring congestion. To attain the ability, we train \name with the aforementioned massive data traces using a custom-designed imitation learning algorithm, which enables \name to learn from past experience. We have implemented and incorporated \name into \appname. Our experiments show that \name outperforms state-of-the-art solutions, improving video quality while reducing stalling time by multi-folds under various practical scenarios.",project-academic
10.1109/JIOT.2019.2903211,2019-03-05,a,IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC,ramtel robust acoustic motion tracking using extreme learning machine for smart cities," Motion tracking is attractive in what concerns a smart city environment, where citizens have to interact with Internet of Things (IoT) infrastructures spread all around one particular city. Motion tracking is important for smart services and location-based services in smart cities, since it provides natural ways for users to interact with the IoT infrastructures, such as the ability to recognize of a wide range of hand motion in real-time. Compared with dedicated hardware devices, ubiquitous devices with reliable speakers and microphones can be developed to achieve cheap acoustic-based motion tracking, which is appropriate for low-power and low-cost IoT applications. However, for complex urban environments, it is very difficult for acoustic-based methods to achieve accurate motion tracking due to multipath fading and limited sampling rate at mobile devices. In this paper, a new parameter called multipath dispersion vector (MDV) is proposed to estimate and mitigate the impact of multipath fading on received signals using extreme learning machine. Based on MDV, a robust acoustic motion tracking (RAMTEL) method is proposed to calculate the moving distance based on the phase change of acoustic signals, and track the corresponding motion in 2-D plane by using multiple speakers. The method is then proposed and implemented on standard Android smartphones. Experiment results show, without any specialized hardware, RAMTEL can achieve an impressive millimeter-level accuracy for localization and motion tracking applications in multipath fading environments. Specifically, the measurement errors are less than 2 and 4 mm in 1-D and 2-D scenarios, respectively.",project-academic
10.1109/RTCSA.2018.00010,2018-08-01,p,Institute of Electrical and Electronics Engineers Inc.,deepcounter using deep learning to count garbage bags," This paper proposes DeepCounter, an automotive sensing system where deep learning based image processing technology is used to automatically count the number of collected garbage bags from the video taken by a camera mounted on the rear of a garbage truck in order to sense a fine-grain spatio-temporal distribution on the amount of disposed garbage in cities that is envisioned to be helpful to develop novel applications related to garbage collection there. A prototype system is implemented on a GPU-integrated signal-board computer. A detection-tracking-counting (DTC) algorithm is developed and implemented based on the single shot multibox detector (SSD), a well-known real-time object detection algorithm. Experimental evaluation validates the feasibility of the proposed approach using video of realistic garbage collection in Fujisawa city, Japan.",project-academic
10.1145/3274783.3275167,2018-11-04,p,ACM,using deep learning to count garbage bags," The information of daily garbage diposal can be used to develop many appealing applications in smart cities. This poster introduces DeepCounter, an automotive sensing system to providing a finegrained spatio-temporal distribution on the amount of disposed garbage bags. In the system, deep learning based image processing is used to automatically count the number of collected garbage bags from the video taken by a camera mounted on the rear of a garbage truck. A prototype system is implemented and experimental evaluation validates the feasibility of our proposal using realistic garbage collection videos in Fujisawa city Japan.",project-academic
10.1101/2021.11.01.21265487,2021-11-04,a,,lightweight mobile automated assistant to physician for global lower resource areas," Importance: Lower-resource areas in Africa and Asia face a unique set of healthcare challenges: the dual high burden of communicable and non-communicable diseases; a paucity of highly trained primary healthcare providers in both rural and densely populated urban areas; and a lack of reliable, inexpensive internet connections. Objective: To address these challenges, we designed an artificial intelligence assistant to help primary healthcare providers in lower-resource areas document demographic and medical sign/symptom data and to record and share diagnostic data in real-time with a centralized database. Design: We trained our system using multiple data sets, including US-based electronic medical records (EMRs) and open-source medical literature and developed an adaptive, general medical assistant system based on machine learning algorithms. Main outcomes and Measure: The application collects basic information from patients and provides primary care providers with diagnoses and prescriptions suggestions. The application is unique from existing systems in that it covers a wide range of common diseases, signs, and medication typical in lower-resource countries; the application works with or without an active internet connection. Results: We have built and implemented an adaptive learning system that assists trained primary care professionals by means of an Android smartphone application, which interacts with a central database and collects real-time data. The application has been tested by dozens of primary care providers. Conclusions and Relevance: Our application would provide primary healthcare providers in lower-resource areas with a tool that enables faster and more accurate documentation of medical encounters. This application could be leveraged to automatically populate local or national EMR systems.",project-academic
,2021-10-28,a,,lightweight mobile automated assistant to physician for global lower resource areas," Importance: Lower-resource areas in Africa and Asia face a unique set of healthcare challenges: the dual high burden of communicable and non-communicable diseases; a paucity of highly trained primary healthcare providers in both rural and densely populated urban areas; and a lack of reliable, inexpensive internet connections. Objective: To address these challenges, we designed an artificial intelligence assistant to help primary healthcare providers in lower-resource areas document demographic and medical sign/symptom data and to record and share diagnostic data in real-time with a centralized database. Design: We trained our system using multiple data sets, including US-based electronic medical records (EMRs) and open-source medical literature and developed an adaptive, general medical assistant system based on machine learning algorithms. Main outcomes and Measure: The application collects basic information from patients and provides primary care providers with diagnoses and prescriptions suggestions. The application is unique from existing systems in that it covers a wide range of common diseases, signs, and medication typical in lower-resource countries; the application works with or without an active internet connection. Results: We have built and implemented an adaptive learning system that assists trained primary care professionals by means of an Android smartphone application, which interacts with a central database and collects real-time data. The application has been tested by dozens of primary care providers. Conclusions and Relevance: Our application would provide primary healthcare providers in lower-resource areas with a tool that enables faster and more accurate documentation of medical encounters. This application could be leveraged to automatically populate local or national EMR systems.",project-academic
10.5194/ACP-21-773-2021,2021-01-20,a,Copernicus GmbH,time resolved emission reductions for atmospheric chemistry modelling in europe during the covid 19 lockdowns," Abstract. We quantify the reductions in primary emissions due to the COVID-19 lockdowns in Europe. Our estimates are provided in the form of a dataset of reduction factors varying per country and day that will allow modelling and identifying the associated impacts upon air quality. The country- and daily-resolved reduction factors are provided for each of the following source categories: energy industry (power plants), manufacturing industry, road traffic and aviation (landing and take-off cycle). We computed the reduction factors based on open access and near-real time measured activity data from a wide range of information sources. We also trained a machine learning model with meteorological data to derive weather-normalised electricity consumption reductions. The time period covered is from 21 February, when the first European localised lockdown was implemented in the region of Lombardy (Italy), until 26 April 2020. This period includes five weeks (23 March until 26 April) with the most severe and relatively unchanged restrictions upon mobility and socio-economic activities across Europe. The computed reduction factors were combined with the Copernicus Atmosphere Monitoring Service's European emission inventory using adjusted emission temporal profiles in order to derive time-resolved emission reductions per country and pollutant sector. During the most severe lockdown period, we estimate the average emission reductions to be −33 % for NOx, −8 % for NMVOC, −7 % for SOx and −7 % for PM2.5 at the EU-30 level (EU-28 plus Norway and Switzerland). For all pollutants more than 85 % of the total reduction is attributable to road transport, except SOx. The reductions reached −50 % (NOx), −14 % (NMVOC), −12 % (SOx) and −15 % (PM2.5) in countries where the lockdown restrictions were more severe such as Italy, France or Spain. To show the potential for air quality modelling we simulated and evaluated NO2 concentration decreases in rural and urban background regions across Europe (Italy, Spain, France, Germany, United-Kingdom and Sweden). We found the lockdown measures to be responsible for NO2 reductions of up to −58 % at urban background locations (Madrid, Spain) and −44 % at rural background areas (France), with an average contribution of the traffic sector to total reductions of 86 % and 93 %, respectively. A clear improvement of the modelled results was found when considering the emission reduction factors, especially in Madrid, Paris and London where the bias is reduced with more than 90 %. Future updates will include the extension of the COVID-19 lockdown period covered, the addition of other pollutant sectors potentially affected by the restrictions (commercial/residential combustion and shipping) and the evaluation of other air quality pollutants such as O3 and PM2.5. All the emission reduction factors are provided in the supplementary material.",project-academic
10.1109/ICSENS.2016.7808626,2016-10-01,p,,floodeye real time flash flood prediction system for urban complex water flow," This study presents a water-level sensor system that has multiple functions: real-time river-level monitoring and high accurate flood prediction for urban complex water flow (CWF) flooding caused by Localized Heavy Rain. Traditional water-level sensors for large-scale river can detect water-level rising, though they have limitations in detecting urban river dyke height to alert about flooding. This is a result of various riverside environmental limitations. Additionally, previous prediction methods are difficult to detect CWF flooding with high accuracy because water rising differ depends on a location's environment. Therefore, we propose a detection scheme for CWF by developing a water-level sensor system that works in various installation environments using infrared image processing with both low installation and operation cost. We implemented a CWF prediction system that produces accurate and early predictions using a Linear Regression (LR) of deep learning approach with data assimilation. Evaluating based on actual installations (over 1,000 days, 13 locations), our system proved to accurately monitor and predict water-level rising after 5 minutes.",project-academic
10.1016/J.IJTST.2020.03.002,2020-12-01,a,Elsevier BV,a reinforcement learning model for personalized driving policies identification," Abstract None None Optimizing driving performance by addressing personalized aspects of driving behavior and without posing unrealistic restrictions on personal mobility may have far reaching implications to traffic safety, flow operations and the environment, as well as significant benefits for users. The present work addresses the problem of delivering personalized driving policies based on Reinforcement Learning for enhancing existing Intelligent Transportation Systems (ITS) to the benefit of traffic management and road safety. The proposed framework is implemented on appropriate driving behavior metrics derived from smartphone sensors’ data streams. Aggressiveness, speeding and mobile usage are considered to describe the driving profile per trip and are presented as inputs to the Q-learning algorithm. The implementation of the proposed methodological approach produces personalized quantified driving policies to be exploited for self-improvement. Finally, this paper establishes validation measures of the quality and effectiveness of the produced policies and methodological tools for comparing and classifying the examined drivers.",project-academic
,2006-01-01,a,,human usable and emergency vehicle aware control policies for autonomous intersection management," Traffic congestion and automobile accidents are two of the leading causes of decreased standard of living and lost productivity in urban settings. Recent advances in artificial intelligence and, specifically, intelligent vehicle technology suggest that vehicles driven entirely by autonomous agents will be possible in the near future. In previous work, we presented a novel reservation-based approach for governing interactions of multiple autonomous vehicles, specifically at intersections. This approach alleviated many traditional problems associated with intersections, in terms of both safety and efficiency. However, such a system relies on all vehicles being equipped with the requisite technology — a restriction that would make implementing such a system in the real world extremely difficult. In this paper, we augment the system such that it is able to accomodate traditional human-operated vehicles using existing infrastructure. Furthermore, we show that as the number of autonomous vehicles on the road increases, traffic delays decrease monotonically toward the levels exhibited in the system involving only autonomous vehicles. Additionally, we demonstrate how the system can be extended to allow high-priority vehicles such as ambulances, police cars, or fire trucks through more quickly without placing undue burden on other vehicles. Both augmentations are fully implemented and tested in our custom simulator, and we present detailed experimental results attesting to their effectiveness.",project-academic
10.1109/61.103760,1991-01-01,a,IEEE,an expert system for locating distribution system faults," A rule-based expert system designed to locate the faults in a distribution system is presented. Distribution system component data and network topology are stored in the database. A set of heuristic rules is compiled from the dispatcher's experience and imbedded in the rule base. To locate distribution system faults, an inference engine is developed to perform deductive reasoning on the rules in the knowledge base. The inference engine comprises three major parts: the dynamic searching method, the backtracking approach, and the set intersection operation. The expert system is implemented on a personal computer using the artificial intelligence language Prolog. To demonstrate the effectiveness of the proposed approach, the expert system has been used to locate faults in a real underground distribution system within the service area of the Taipei City District Office of the Taiwan Power Company. It is found that the expert system can identify fault location very efficiently. Therefore, it can serve as a valuable tool to help distribution system dispatchers determine fault locations. >",project-academic
10.1061/(ASCE)CF.1943-5509.0000886,2016-03-21,a,American Society of Civil Engineers,assessment of remaining useful life of pipelines using different artificial neural networks models," AbstractWater distribution networks have a significant effect on public health and safety. Recent reports state that the 21st century is estimated to be the end of effective life for most water distribution networks in the United States. It is essential to implement accurate and cost-effective models that can predict deterioration rates along with estimates of remaining useful life (RUL) of the pipelines, to perform necessary intervention plans that can prevent disastrous failures. This study presents a computational model that predicts the RUL of water pipelines using an artificial neural network (ANN) model that has been developed using the Levenberg-Marquardt backpropagation algorithm. The model is implemented, tested, and trained using data collected from the city of Montreal. Results show that pipeline age, condition, length, diameter, material, and breakage rate are the most important factors in the prediction of RUL. Because the model shows robustness and accuracy in estimating the RUL of water pip...",project-academic
10.3389/FNBOT.2020.00036,2020-06-25,a,Frontiers Media SA,a path toward explainable ai and autonomous adaptive intelligence deep learning adaptive resonance and models of perception emotion and action," Biological neural network models whereby brains make minds help to understand autonomous adaptive intelligence. This article summarizes why the dynamics and emergent properties of such models for perception, cognition, emotion, and action are explainable, and thus amenable to being confidently implemented in large-scale applications. Key to their explainability is how these models combine fast activations, or short-term memory (STM) traces, and learned weights, or long-term memory (LTM) traces. Visual and auditory perceptual models have explainable conscious STM representations of visual surfaces and auditory streams in surface-shroud resonances and stream-shroud resonances, respectively. Deep Learning is often used to classify data. However, Deep Learning can experience catastrophic forgetting: At any stage of learning, an unpredictable part of its memory can collapse. Even if it makes some accurate classifications, they are not explainable and thus cannot be used with confidence. Deep Learning shares these problems with the back propagation algorithm, whose computational problems due to non-local weight transport during mismatch learning were described in the 1980s. Deep Learning became popular after very fast computers and huge online databases became available that enabled new applications despite these problems. Adaptive Resonance Theory, or ART, algorithms overcome the computational problems of back propagation and Deep Learning. ART is a self-organizing production system that incrementally learns, using arbitrary combinations of unsupervised and supervised learning and only locally computable quantities, to rapidly classify large non-stationary databases without experiencing catastrophic forgetting. ART classifications and predictions are explainable using the attended critical feature patterns in STM on which they build. The LTM adaptive weights of the fuzzy ARTMAP algorithm induce fuzzy IF-THEN rules that explain what feature combinations predict successful outcomes. ART has been successfully used in multiple large-scale real world applications, including remote sensing, medical database prediction, and social media data clustering. Also explainable are the MOTIVATOR model of reinforcement learning and cognitive-emotional interactions, and the VITE, DIRECT, DIVA, and SOVEREIGN models for reaching, speech production, spatial navigation, and autonomous adaptive intelligence. These biological models exemplify complementary computing, and use local laws for match learning and mismatch learning that avoid the problems of Deep Learning.",project-academic
,2014-07-27,p,AAAI Press,effective management of electric vehicle storage using smart charging," The growing Electric Vehicles' (EVs) popularity among commuters creates new challenges for the smart grid. The most important of them is the uncoordinated EV charging that substantially increases the energy demand peaks, putting the smart grid under constant strain. In order to cope with these peaks the grid needs extra infrastructure, a costly solution. We propose an Adaptive Management of EV Storage (AMEVS) algorithm, implemented through a learning agent that acts on behalf of individual EV owners and schedules EV charging over a weekly horizon. It accounts for individual preferences so that mobility service is not violated but also individual benefit is maximized. We observe that it reshapes the energy demand making it less volatile so that fewer resources are needed to cover peaks. It assumes Vehicle-to-Grid discharging when the customer has excess capacity. Our agent uses Reinforcement Learning trained on real world data to learn individual household consumption behavior and to schedule EV charging. Unlike previous work, AMEVS is a fully distributed approach. We show that AMEVS achieves significant reshaping of the energy demand curve and peak reduction, which is correlated with customer preferences regarding perceived utility of energy availability. Additionally, we show that the average and peak energy prices are reduced as a result of smarter energy use.",project-academic
10.1155/2020/6138637,2020-02-27,a,Hindawi Limited,waste management system using iot based machine learning in university," Along with the development of the Internet of Things (IoT), waste management has appeared as a serious issue. Waste management is a daily task in urban areas, which requires a large amount of labour resources and affects natural, budgetary, efficiency, and social aspects. Many approaches have been proposed to optimize waste management, such as using the nearest neighbour search, colony optimization, genetic algorithm, and particle swarm optimization methods. However, the results are still too vague and cannot be applied in real systems, such as in universities or cities. Recently, there has been a trend of combining optimal waste management strategies with low-cost IoT architectures. In this paper, we propose a novel method that vigorously and efficiently achieves waste management by predicting the probability of the waste level in trash bins. By using machine learning and graph theory, the system can optimize the collection of waste with the shortest path. This article presents an investigation case implemented at the real campus of Ton Duc Thang University (Vietnam) to evaluate the performance and practicability of the system’s implementation. We examine data transfer on the LoRa module and demonstrate the advantages of the proposed system, which is implemented through a simple circuit designed with low cost, ease of use, and replace ability. Our system saves time by finding the best route in the management of waste collection.",project-academic
10.1002/MP.14494,2020-12-01,a,"John Wiley & Sons, Ltd",deterministic linear boltzmann transport equation solver for patient specific ct dose estimation comparison against a monte carlo benchmark for realistic scanner configurations and patient models," Purpose None Epidemiological evidence suggests an increased risk of cancer related to computed tomography (CT) scans, with children exposed to greater risk. The purpose of this work is to test the reliability of a linear Boltzmann transport equation (LBTE) solver for rapid and patient-specific CT dose estimation. This includes building a flexible LBTE framework for modeling modern clinical CT scanners and to validate the resulting dose maps across a range of realistic scanner configurations and patient models. None Methods None In this study, computational tools were developed for modeling CT scanners, including a bowtie filter, overrange collimation, and tube current modulation. The LBTE solver requires discretization in the spatial, angular, and spectral dimensions, which may affect the accuracy of scanner modeling. To investigate these effects, this study evaluated the LBTE dose accuracy for different discretization parameters, scanner configurations, and patient models (male, female, adults, pediatric). The method used to validate the LBTE dose maps was the Monte Carlo code Geant4, which provided ground truth dose maps. LBTE simulations were implemented on a GeForce GTX 1080 graphic unit, while Geant4 was implemented on a distributed cluster of CPUs. None Results None The agreement between Geant4 and the LBTE solver quantifies the accuracy of the LBTE, which was similar across the different protocols and phantoms. The results suggest that 18 views per rotation provides sufficient accuracy, as no significant improvement in the accuracy was observed by increasing the number of projection views. Considering this discretization, the LBTE solver average simulation time was approximately 30 s. However, in the LBTE solver the phantom model was implemented with a lower voxel resolution with respect to Geant4, as it is limited by the memory of the GPU. Despite this discretization, the results showed a good agreement between the LBTE and Geant4, with root mean square error of the dose in organs of approximately 3.5% for most of the studied configurations. None Conclusions None The LBTE solver is proposed as an alternative to Monte Carlo for patient-specific organ dose estimation. This study demonstrated accurate organ dose estimates for the rapid LBTE solver when considering realistic aspects of CT scanners and a range of phantom models. Future plans will combine the LBTE framework with deep learning autosegmentation algorithms to provide near real-time patient-specific organ dose estimation.",project-academic
10.1016/J.ENERGY.2018.10.175,2019-01-15,a,Pergamon,a comparison of models for forecasting the residential natural gas demand of an urban area," Abstract None None Forecasting the residential natural gas demand for large groups of buildings is extremely important for efficient logistics in the energy sector. In this paper different forecast models for residential natural gas demand of an urban area were implemented and compared. The models forecast gas demand with hourly resolution up to 60 h into the future. The model forecasts are based on past temperatures, forecasted temperatures and time variables, which include markers for holidays and other occasional events. The models were trained and tested on gas-consumption data gathered in the city of Ljubljana, Slovenia. Machine-learning models were considered, such as linear regression, kernel machine and artificial neural network. Additionally, empirical models were developed based on data analysis. Two most accurate models were found to be recurrent neural network and linear regression model. In realistic setting such trained models can be used in conjunction with a weather-forecasting service to generate forecasts for future gas demand.",project-academic
10.1109/TITS.2020.3029537,2020-10-23,a,Institute of Electrical and Electronics Engineers (IEEE),spatial positioning token sptoken for smart mobility," We introduce a permissioned distributed ledger technology (DLT) design for crowdsourced smart mobility applications. This architecture is based on a directed acyclic graph architecture (similar to the IOTA tangle) and uses both Proof-of-Work and Proof-of-Position mechanisms to provide protection against spam attacks and malevolent actors. In addition to enabling individuals to retain ownership of their data and to monetize it, the architecture is also suitable for distributed privacy-preserving machine learning algorithms, is lightweight, and can be implemented in simple internet-of-things (IoT) devices. To demonstrate its efficacy, we apply this framework to reinforcement learning settings where a third party is interested in acquiring information from agents. In particular, one may be interested in sampling an unknown vehicular traffic flow in a city, using a DLT-type architecture and without perturbing the density, with the idea of realizing a set of virtual tokens as surrogates of real vehicles to explore geographical areas of interest. These tokens, whose authenticated position determines write access to the ledger, are thus used to emulate the probing actions of commanded (real) vehicles on a given planned route by ``jumping'' from a passing-by vehicle to another to complete the planned trajectory. Consequently, the environment stays unaffected (i.e., the autonomy of participating vehicles is not influenced by the algorithm), regardless of the number of emitted tokens. The design of such a DLT architecture is presented, and numerical results from large-scale simulations are provided to validate the proposed approach.",project-academic
,2019-01-15,,,deep reinforcement learning based low speed vehicle following decision making method," The invention discloses a deep reinforcement learning-based low-speed vehicle following decision-making method, which is implemented in the following manner: at first, receiving position, speed and acceleration information of front and back vehicles as an environmental state in real time through the Internet of vehicles, and expressing a present state and behavior of an unmanned vehicle; then, constructing an Actor-Critic framework-based deep reinforcement learning structure; and finally, selecting, by Actor, an appropriate action according to the present environmental state, and continuouslyperforming training and learning through an evaluation made by Critic, thereby obtaining an optimal control strategy to ensure that the unmanned vehicle can be kept at a certain safe distance away from the front and back vehicles and implement automatic low-speed running of the vehicle following the front vehicle under an urban congestion working condition. According to the deep reinforcement learning-based low-speed vehicle following decision-making method, the driving comfort is improved, the traffic safety is also ensured, and the clarity of a congested lane is further improved.",project-academic
10.24908/IJSLE.V3I2.2103,2008-09-01,a,Queen's University Library,service learning projects in core undergraduate engineering courses," The College of Engineering at the University of Massachusetts Lowell (UML) has 
integrated service-learning (S-L) into many of its core required undergraduate courses over the last three years. Projects that meet real community needs and that help students achieve academic objectives in these core courses are percieved to be difficult to create, but, in this paper, projects for 35 different undergraduate required courses are summarized to help faculty, staff, and students develop S-L projects for their own courses. Faculty at UML were encouraged to “start small rather than not at all.” Courses and projects include, for example, a first-year introduction to engineering course in which 340 students, divided into teams, designed and built moving displays illustrating various technologies for 60,000 middle school students that every year visit a history center that is part of a national park. Another example is a sophomore kinematics course in which student teams visited local playgrounds to assess their safety using deceleration, force, and impact equations learned from the course. Junior heat transfer courses focused in analyzing heat loss and making suggestions for heating system savings for a local food pantry, a city hall building, and a community mental health center, as well as for the university itself; these analyses were developed and presented to the stakeholders. Sophomore student teams from the materials course presented findings to the staff of a local textile history museum to help it begin updating its displays on recent developments in materials. Junior fluids, junior circuits, senior microprocessor, senior design of machine elements, and senior capstone design are having students design and build various parts of an automated canal lock opener for a local national park. Many of the projects are low-cost and can be implemented by individual faculty members without the requirement of a formal institutional program. These S-L projects are integrated into a wide variety of core courses (and not just design courses) and represent typically from 10 to 20 percent of the grade.",project-academic
10.1109/PARC49193.2020.236616,2020-02-01,p,IEEE,iot and cloud computing based smart water metering system," This paper focuses on the developmental and implementation methodology of smart water meter based on Internet of Things (IoT) and Cloud computing equipped with machine learning algorithms, to differentiate between normal and excessive water usage at industrial, domestic and all other sectors having an abundance of water usage, both for Indian and worldwide context. Recognizing that intelligent metering of water has the potential to alter customer engagement of water usage in urban and rural water supplies, this paper fosters for sustainable water management, a need of the present. With shrinking reserves of clean water resources worldwide, it is becoming cumbersome to cater for this resource to masses in the coming years on a consistent basis. Using our smart water meter, water resources can be managed efficiently and an optimum use could save water for the future generations. Sensors will provide for real time monitoring of hydraulic data, automated control and alarming from Cloud platform in case of events such as water leakages, excessive usage, etc. Analysis of the same will help in taking meaningful actions. Thus we do propose for a smart water metering technology that can be utilized by Indian citizens, and worldwide, to curb wastage of water. With an ease of monitoring and visualization of the data through the Cloud platform combined with machine learning based tools to detect excess water consumption, the server-less architecture we propose can be easily adopted and implemented in a large scale.",project-academic
10.1145/2405688.2405691,2012-12-03,p,ACM,a communication optimizing middleware for efficient wireless communication in rural environments," The realization of wireless communication in rural environments suffers from long distances, unknown node mobility and discontinuous communication paths. Combination of delay-tolerant mobile ad-hoc networks and infrastructure-based mobile communications results in an increased number of communication opportunities in many usage scenarios. However, efficient exploitation of communication opportunities remains a challenging task.This article introduces a novel middleware approach specialized for consistent and efficient wireless communication in rural environments. An important part of communication optimization is realized by processing on-hand information of the current application scenario. The middleware autonomously links scenario information to network states and applies machine-learning-based analyses to derive meaningful communication predictions. Cooperation of infrastructure-based and ad-hoc communication is implemented by a gateway component. The gateway collects connectivity information of mobile nodes, enhances communication predictions and distributes predictions across the network. Evaluation of local communication layer and application layer information as well as sharing of communication predictions using an application level API ensure efficient use of communication opportunities.",project-academic
10.3390/RS11161851,2019-01-01,a,Molecular Diversity Preservation International (MDPI),intelligent gps l1 los multipath nlos classifiers based on correlator rinex and nmea level measurements," This paper proposes to use a correlator-level global positioning system (GPS) line-of-sight/multipath/non-line-of-sight (LOS/MP/NLOS) signal reception classifier to improve positioning performance in an urban environment. Conventional LOS/MP/NLOS classifiers, referred to as national marine electronics association (NMEA)-level and receiver independent exchange format (RINEX)-level classifiers, are usually performed using attributes extracted from basic observables or measurements such as received signal strength, satellite elevation angle, code pseudorange, etc. The NMEA/RINEX-level classification rate is limited because the complex signal propagation in urban environment is not fully manifested in these end attributes. In this paper, LOS/MP/NLOS features were extracted at the baseband signal processing stage. Multicorrelator is implemented in a GPS software-defined receiver (SDR) and exploited to generate features from the autocorrelation function (ACF). A robust LOS/MP/NLOS classifier using a supervised machine learning algorithm, support vector machine (SVM), is then trained. It is also proposed that the Skymask and code pseudorange double difference observable are used to label the real signal type. Raw GPS intermediate frequency data were collected in urban areas in Hong Kong and were postprocessed using a self-developed SDR, which can easily output correlator-level LOS/MP/NLOS features. The SDR measurements were saved in the file with the format of NMEA and RINEX. A fair comparison among NMEA-, RINEX-, and correlator-level classifiers was then carried out on a common ground. Results show that the correlator-level classifier improves the metric of F1 score by about 25% over the conventional NMEA- and RINEX-level classifiers for testing data collected at different places to that of training data. In addition to this finding, correlator-level classifier is found to be more feasible in practical applications due to its less dependency on surrounding scenarios compared with the NMEA/RINEX-level classifiers.",project-academic
10.1136/BMJINNOV-2020-000420,2021-04-01,a,BMJ Specialist Journals,machine learning based hospital discharge predictions can support multidisciplinary rounds and decrease hospital length of stay," Background None Patient flow directly affects quality of care, access and financial performance for hospitals. Multidisciplinary discharge-focused rounds have proven to minimise avoidable delays experienced by patients near discharge. The study objective was to support discharge-focused rounds by implementing a machine-learning-based discharge prediction model using real-time electronic health record (EHR) data. We aimed to evaluate model predictive performance and impact on hospital length-of-stay. None Methods None Discharge prediction models were developed from hospitalised patients on four inpatient units between April 2016 and September 2018. Unit-specific models were implemented to make individual patient predictions viewable with the EHR patient track board. Predictive performance was measured prospectively for 12 470 patients (120 780 patient-predictions) across all units. A pre/poststudy design applying interrupted time series methods was used to assess the impact of the discharge prediction model on hospital length-of-stay. None Results None Prospective discharge prediction performance ranged in area under the receiver operating characteristic curve from 0.70 to 0.80 for same-day and next-day predictions; sensitivity was between 0.63 and 0.83 and specificity between 0.48 and 0.80. Elapsed length-of-stay, counts of labs and medications, mobility assessments and measures of acute kidney injury were model features providing the most predictive value. Implementing the discharge predictions resulted in a reduction in hospital length-of-stay of over 12 hours on a medicine unit (p None Conclusions None Incorporating automated patient discharge predictions into multidisciplinary rounds can support decreases in hospital length-of-stay. Variation in execution and impact across inpatient units existed.",project-academic
10.1016/J.TRC.2021.103416,2021-11-01,a,Elsevier BV,drl tp3 a learning and control framework for signalized intersections with mixed connected automated traffic," Abstract None None The emerging connected and automated vehicle (CAV) technologies offer new opportunities for urban signalized intersection management. Through wireless communication and advanced sensing capabilities, CAVs can detect the surrounding traffic environment and share real-time vehicular information with each other and the infrastructure, and individual trajectories of CAVs can be precisely controlled. This paper proposes a real-time learning and control framework for signalized intersection management, which includes both signal optimization and CAV trajectory control. The proposed framework integrates perception, prediction, planning, and optimization components and aims at improving efficiency mixed connected automated traffic in terms of traffic throughput and delay. This framework applies the Long Short Term Memory (LSTM) networks to implicitly learn traffic patterns and driver behavior and then estimate and predict the microscopic traffic conditions that are only partially observable. Then it utilizes deep reinforcement learning (DRL) to solve signal optimization problems by learning from the dynamic interactions between vehicles and the traffic environment. Under the proposed framework, the vehicular trajectories of CAVs can be controlled to maximize the utilization of the green time and reduce the start-up lost time by using a highly efficient trajectory planning algorithm. The CAV platooning operation, in coordination with traffic signals, is also implemented such that CAVs can pass the intersection efficiently. Simulations are performed at a signalized intersection a signalized intersection with multi-lane approaches, high traffic demand, and standard ring-barrier control, and results show that the proposed DRL-TP3 framework can significantly improve the throughput and reduce the average delay across different CAV market penetration rates (MPRs). We also investigate the impacts of different sensor capabilities of unobservable vehicle estimation and implementation of a lane change prohibition zone under the DRL-TP3 framework.",project-academic
10.1109/49.310965,1994-08-01,a,IEEE,a novel neural network traffic enforcement mechanism for atm networks," ATM has been recommended by the CCITT as the transport vehicle for the future B-ISDN networks. In ATM-based networks, a set of user declared parameters that describes the traffic characteristics, is required for the connection acceptance control (CAC) and traffic enforcement (policing) mechanisms. At the call set-up phase, the CAC algorithm uses those parameters to make a call acceptance decision. During the call progress, the policing mechanism uses the same parameters to control the user's traffic within its declared values in order to protect the network's resources and avoid possible congestion problems. A novel policing mechanism using neural networks (NNs) is presented. This is based upon an accurate estimation of the probability density function (pdf) of the traffic via its count process and implemented using NNs. The pdf-based policing is made possible only by NNs because pdf policing requires complex calculations, in real-time, at very high speeds. The architecture of the policing mechanism is composed of two interconnected NNs. The first one is trained to learn the pdf of ""ideal nonviolating"" traffic, whereas the second is trained to capture the ""actual"" characteristics of the ""actual"" offered traffic during the progress of the call. The output of both NNs is compared. Consequently, an error signal is generated whenever the pdf of the offered traffic violates its ""ideal"" one. The error signal is then used to shape the traffic back to its original values. >",project-academic
10.1109/25.933315,2001-05-01,a,IEEE,idutc an intelligent decision making system for urban traffic control applications," The design of systems for intelligent control of urban traffic is important in providing a safe environment for pedestrians and motorists. Artificial neural networks (ANNs) (learning systems) and expert systems (knowledge-based systems) have been extensively explored as approaches for decision-making. While the ANNs compute decisions by learning from successfully solved examples, the expert systems rely on a knowledge base developed by human reasoning for decision making. It is possible to integrate the learning abilities of an ANN and the knowledge-based decision-making ability of the expert system. This paper presents a real-time intelligent decision-making system, IDUTC, for urban traffic control applications. The system integrates a backpropagation-based ANN that can learn and adapt to the dynamically changing environment and a fuzzy expert system for decision-making. The performance of the proposed intelligent decision-making system is evaluated by mapping the adaptable traffic light control problem. The application is implemented using the ANN approach, the FES approach, and the proposed integrated system approach. The results of extensive simulations using the three approaches indicate that the integrated system provides better performance and leads to a more efficient implementation than the other two approaches.",project-academic
10.3390/W11010009,2018-12-21,a,Multidisciplinary Digital Publishing Institute,building an intelligent hydroinformatics integration platform for regional flood inundation warning systems," Flood disasters have had a great impact on city development. Early flood warning systems (EFWS) are promising countermeasures against flood hazards and losses. Machine learning (ML) is the kernel for building a satisfactory EFWS. This paper first summarizes the ML methods proposed in this special issue for flood forecasts and their significant advantages. Then, it develops an intelligent hydroinformatics integration platform (IHIP) to derive a user-friendly web interface system through the state-of-the-art machine learning, visualization and system developing techniques for improving online forecast capability and flood risk management. The holistic framework of the IHIP includes five layers (data access, data integration, servicer, functional subsystem, and end-user application) and one database for effectively dealing with flood disasters. The IHIP provides real-time flood-related data, such as rainfall and multi-step-ahead regional flood inundation maps. The interface of Google Maps fused into the IHIP significantly removes the obstacles for users to access this system, helps communities in making better-informed decisions about the occurrence of floods, and alerts communities in advance. The IHIP has been implemented in the Tainan City of Taiwan as the study case. The modular design and adaptive structure of the IHIP could be applied with similar efforts to other cities of interest for assisting the authorities in flood risk management.",project-academic
10.1016/J.AAP.2019.105319,2020-01-01,a,Pergamon,detecting motorcycle helmet use with deep learning," The continuous motorization of traffic has led to a sustained increase in the global number of road related fatalities and injuries. To counter this, governments are focusing on enforcing safe and law-abiding behavior in traffic. However, especially in developing countries where the motorcycle is the main form of transportation, there is a lack of comprehensive data on the safety-critical behavioral metric of motorcycle helmet use. This lack of data prohibits targeted enforcement and education campaigns which are crucial for injury prevention. Hence, we have developed an algorithm for the automated registration of motorcycle helmet usage from video data, using a deep learning approach. Based on 91,000 annotated frames of video data, collected at multiple observation sites in 7 cities across the country of Myanmar, we trained our algorithm to detect active motorcycles, the number and position of riders on the motorcycle, as well as their helmet use. An analysis of the algorithm's accuracy on an annotated test data set, and a comparison to available human-registered helmet use data reveals a high accuracy of our approach. Our algorithm registers motorcycle helmet use rates with an accuracy of -4.4% and +2.1% in comparison to a human observer, with minimal training for individual observation sites. Without observation site specific training, the accuracy of helmet use detection decreases slightly, depending on a number of factors. Our approach can be implemented in existing roadside traffic surveillance infrastructure and can facilitate targeted data-driven injury prevention campaigns with real-time speed. Implications of the proposed method, as well as measures that can further improve detection accuracy are discussed.",project-academic
10.1016/J.GLT.2020.09.004,2020-01-01,a,Elsevier BV,development of an iot based real time traffic monitoring system for city governance," Abstract None None A significant amount of research work carried out on traffic management systems, but intelligent traffic monitoring is still an active research topic due to the emerging technologies such as the Internet of Things (IoT) and Artificial Intelligence (AI). The integration of these technologies will facilitate the techniques for better decision making and achieve urban growth. However, the existing traffic prediction methods mostly dedicated to highway and urban traffic management, and limited studies focused on collector roads and closed campuses. Besides, reaching out to the public, and establishing active connections to assist them in decision-making is challenging when the users are not equipped with any smart devices. This research proposes an IoT based system model to collect, process, and store real-time traffic data for such a scenario. The objective is to provide real-time traffic updates on traffic congestion and unusual traffic incidents through roadside message units and thereby improve mobility. These early-warning messages will help citizens to save their time, especially during peak hours. Also, the system broadcasts the traffic updates from the administrative authorities. A prototype is implemented to evaluate the feasibility of the model, and the results of the experiments show good accuracy in vehicle detection and a low relative error in road occupancy estimation. The study is part of the Omani-funded research project, investigating Real-Time Feedback for Adaptive Traffic Signals.",project-academic
10.1108/WJE-10-2020-0529,2021-01-11,a,Emerald Publishing Limited,detection of covid 19 cases through x ray images using hybrid deep neural network," Purpose - The latest 2019 coronavirus (COVID-2019), which first appeared in December 2019 in Wuhan's city in China, rapidly spread around the world and became a pandemic None It has had a devastating impact on daily lives, the public's health and the global economy None The positive cases must be identified as soon as possible to avoid further dissemination of this disease and swift care of patients affected None The need for supportive diagnostic instruments increased, as no specific automated toolkits are available None The latest results from radiology imaging techniques indicate that these photos provide valuable details on the virus COVID-19 None User advanced artificial intelligence (AI) technologies and radiological imagery can help diagnose this condition accurately and help resolve the lack of specialist doctors in isolated areas None In this research, a new paradigm for automatic detection of COVID-19 with bare chest X-ray images is displayed None Images are presented None The proposed model DarkCovidNet is designed to provide correct binary classification diagnostics (COVID vs no detection) and multi-class (COVID vs no results vs pneumonia) classification None The implemented model computed the average precision for the binary and multi-class classification of 98 46% and 91 352%, respectively, and an average accuracy of 98 97% and 87 868% None The DarkNet model was used in this research as a classifier for a real-time object detection method only once None A total of 17 convolutionary layers and different filters on each layer have been implemented None This platform can be used by the radiologists to verify their initial application screening and can also be used for screening patients through the cloud None Design/methodology/approach - This study also uses the CNN-based model named Darknet-19 model, and this model will act as a platform for the real-time object detection system None The architecture of this system is designed in such a way that they can be able to detect real-time objects None This study has developed the DarkCovidNet model based on Darknet architecture with few layers and filters None So before discussing the DarkCovidNet model, look at the concept of Darknet architecture with their functionality None Typically, the DarkNet architecture consists of 5 pool layers though the max pool and 19 convolution layers None Assume as a convolution layer, and as a pooling layer None Findings - The work discussed in this paper is used to diagnose the various radiology images and to develop a model that can accurately predict or classify the disease None The data set used in this work is the images bases on COVID-19 and non-COVID-19 taken from the various sources None The deep learning model named DarkCovidNet is applied to the data set, and these have shown signification performance in the case of binary classification and multi-class classification None During the multi-class classification, the model has shown an average accuracy 98 97% for the detection of COVID-19, whereas in a multi-class classification model has achieved an average accuracy of 87 868% during the classification of COVID-19, no detection and Pneumonia None Research limitations/implications - One of the significant limitations of this work is that a limited number of chest X-ray images were used None It is observed that patients related to COVID-19 are increasing rapidly None In the future, the model on the larger data set which can be generated from the local hospitals will be implemented, and how the model is performing on the same will be checked None Originality/value - Deep learning technology has made significant changes in the field of AI by generating good results, especially in pattern recognition None A conventional CNN structure includes a convolution layer that extracts characteristics from the input using the filters it applies, a pooling layer that reduces calculation efficiency and the neural network's completely connected layer None A CNN model is created by integrating one or more of these layers, and its internal parameters are modified to accomplish a specific mission, such as classification or object recognition None A typical CNN structure has a convolution layer that extracts features from the input with the filters it applies, a pooling layer to reduce the size for computational performance and a fully connected layer, which is a neural network None A CNN model is created by combining one or more such layers, and its internal parameters are adjusted to accomplish a particular task, such as classification or object recognition",project-academic
,2016-12-18,a,University of Oklahoma,fully autonomous self powered intelligent wireless sensor for real time traffic surveillance in smart cities," Reliable, real-time traffic surveillance is an integral and crucial function of the 21st century intelligent transportation systems (ITS) network. This technology facilitates instantaneous decision-making, improves roadway efficiency, and maximizes existing transportation infrastructure capacity, making transportation systems safe, efficient, and more reliable. Given the rapidly approaching era of smart cities, the work detailed in this dissertation is timely in that it reports on the design, development, and implementation of a novel, fully-autonomous, self-powered intelligent wireless sensor for real-time traffic surveillance. Multi-disciplinary, innovative integration of state-of-the-art, ultra-low-power embedded systems, smart physical sensors, and the wireless sensor network—powered by intelligent algorithms—are the basis of the developed Intelligent Vehicle Counting and Classification Sensor (iVCCS) platform. The sensor combines an energy-harvesting subsystem to extract energy from multiple sources and enable sensor node self-powering aimed at potentially indefinite life. A wireless power receiver was also integrated to remotely charge the sensor’s primary battery. Reliable and computationally efficient intelligent algorithms for vehicle detection, speed and length estimation, vehicle classification, vehicle re-identification, travel-time estimation, time-synchronization, and drift compensation were fully developed, integrated, and evaluated. Several length-based vehicle classification schemes particular to the state of Oklahoma were developed, implemented, and evaluated using machine learning algorithms and probabilistic modeling of vehicle magnetic length. A feature extraction employing different techniques was developed to determine suitable and efficient features for magnetic signature-based vehicle re-identification. Additionally, two vehicle re-identification models based on matching vehicle magnetic signature from a single magnetometer were developed. Comprehensive system evaluation and extensive data analyses were performed to fine-tune and validate the sensor, ensuring reliable and robust operation. Several field studies were conducted under various scenarios and traffic conditions on a number of highways and urban roads and resulted in 99.98% detection accuracy, 97.4782% speed estimation accuracy, and 97.6951% classification rate when binning vehicles into four groups based on their magnetic length. Threshold-based, re-identification results revealed 65.25%~100% identification rate for a window of 25~500 vehicles. Voting-based, re-identification evaluation resulted in 90~100% identification rate for a window of 25~500 vehicles. The developed platform is portable and cost-effective. A single sensor node costs only $30 and can be installed for short-term use (e.g., work zone safety, traffic flow studies, roadway and bridge design, traffic management in atypical situations), as well as long-term use (e.g., collision avoidance at intersections, traffic monitoring) on highways, roadways, or roadside…",project-academic
10.1016/J.IFACOL.2021.04.197,2020-01-01,a,Elsevier,cognitive artificial population system framework and application," Abstract None None Agent-based social simulation has been comprehensively applied in the research of social and ecological systems. At its core is an artificial population, which endogenously drives the system evolution for particular applications, such as urban transportation, reginal economics, analysis of infectious disease transmission, and military simulation. In contrast with the previous population simulations where simple mathematical models are used to ‘reproduce’ actual demographic features, this paper proposes a self-evolutionary digital population system, named as Cognitive Artificial Population System (CAPS). At a more fine-grained level, CAPS focuses on the agent cognitive, reasoning and learning process in their surrounding environment, thus can exploit most advantages from cognitive computing and Artificial Intelligence. As a case study, Chinese population evolution is implemented using the proposed framework. Computational experiments indicate that CAPS is able to achieve good predicted population structures for real social systems.",project-academic
10.1007/S00521-012-0977-3,2013-07-01,a,Springer-Verlag,short term traffic flow forecasting parametric and nonparametric approaches via emotional temporal difference learning," Information signal from real case and natural complex dynamical systems such as traffic flow are usually specified by irregular motions. Chaotic nonlinear dynamics approach is now the most powerful tool for scientists to deal with complexities in real cases, and neural networks and neuro-fuzzy models are widely used for their capabilities in nonlinear modeling of chaotic systems more than the traditional methods. As mentioned, the traffic flow conditions caused the forecasting values of traffic flow to lack robustness and accuracy. In this paper, the traffic flow forecasting is analyzed with emotional concepts and multi-agent systems (MASs) points of view as a new method in this field. The findings enabled the researchers to develop a newly object-oriented method of forecasting traffic flow. Its architecture is based on a temporal difference (TD) Q-learning with a neuro-fuzzy structure, which is the nonparametric approach. The performance of TD Q-learning is improved by emotional learning. The proposed method on the present conditions and the action of the system according to the criteria could forecast traffic signals so that the objectives are reached in minimum time. The ability of presented learning algorithm to prospect gains from future actions and obtain rewards from its past experiences allows emotional TD Q-learning algorithm to improve its decisions for the best possible actions. In addition, to study in a more practical situation, the neuro-fuzzy behaviors could be modeled by MAS. The proposed method (intelligent/nonparametric approach) is compared by parametric approach, autoregressive integrated moving average (ARIMA) method, which is implemented by multi-layer perceptron neural networks and called ARIMANN. Here, the ARIMANN is updated by backpropagation and temporal difference backpropagation for the first time. The simulation results revealed that the studied forecaster could discover the optimal forecasting by means of the Q-learning algorithm. Difficult to handle through parametric and classic methods, the real traffic flow signals used for fitting the algorithms is obtained from a two-lane street I-494 in Minnesota City.",project-academic
10.5210/OJPHI.V11I1.9910,2019-05-30,a,University of Illinois Libraries,analytic fusion for essential indicators of the opioid epidemic," Objective None In a partnership between the Public Health Division of the Oregon Health Authority (OHA) and the Johns Hopkins Applied Physics Laboratory (APL), our objective was develop an analytic fusion tool using streaming data and report-based evidence to improve the targeting and timing of evidence-based interventions in the ongoing opioid overdose epidemic. The tool is intended to enable practical situational awareness in the ESSENCE biosurveillance system to target response programs at the county and state levels. Threats to be monitored include emerging events and gradual trends of overdoses in three categories: all prescription and illicit opioids, heroin, and especially high-mortality synthetic drugs such as fentanyl and its analogues. Traditional sources included emergency department (ED) visits and emergency management services (EMS) call records. Novel sources included poison center calls, death records, and report-based information such as bad batch warnings on social media. Using available data and requirements analyses thus far, we applied and compared Bayesian networks, decision trees, and other machine learning approaches to derive robust tools to reveal emerging overdose threats and identify at-risk subpopulations. None Introduction None Unlike other health threats of recent concern for which widespread mortality was hypothetical, the high fatality burden of opioid overdose crisis is present, steadily growing, and affecting young and old, rural and urban, military and civilian subpopulations. While the background of many public health monitors is mainly infectious disease surveillance, these epidemiologists seek to collaborate with behavioral health and injury prevention programs and with law enforcement and emergency medical services to combat the opioid crisis. Recent efforts have produced key terms and phrases in available data sources and numerous user-friendly dashboards allowing inspection of hundreds of plots. The current effort seeks to distill and present combined fusion alerts of greatest concern from numerous stratified data outputs. Near-term plans are to implement best-performing fusion methods as an ESSENCE module for the benefit of OHA staff and other user groups. None Methods None By analyzing historical OHA data, we formed features to monitor in each data source to adapt diagnosis codes and text strings suggested by CDC’s injury prevention division, published EMS criteria [Reference 1], and generic product codes from CDC toxicologists, with guidance from OHA Emergency Services Director David Lehrfeld and from Oregon Poison Center Director Sandy Giffen. These features included general and specific opioid abuse indicators such as daily counts of records labelled with the “poisoning” subcategory and containing “fentanyl” or other keywords in the free-text. Matrices of corresponding time series were formed for each of 36 counties and the entire state as inputs to region-specific fusion algorithms. To obtain truth data for detection, the OHA staff provided guidance and design help to generate plausible overdose threat scenarios that were quantified as realistic data distributions of monitored features accounting for time delays and historical distributions of counts in each data source. We sampled these distributions to create 1000 target sets for detection based on the event duration and affected counties for each event scenario. We used these target datasets to compare the detection performance of fusion detection algorithms. Tested algorithms included Bayesian Networks formed with the R package gRain, and also random forest, logistic regression, and support vector machine models implemented with the Python scikit-learn package using default settings. The first 800 days of the data were used for model training, and the last 400 days for testing. Model results were evaluated with the metrics: Sensitivity = (number of target event days signaled) / (all event days) and Positive predictive value (PPV) = (number of target event days signaled) / (all days signaled). These metrics were combined with specificity regarded as the expected fusion alert rate calculated from the historical dataset with no simulated cases injected. None Results None The left half of Figure 1 illustrates a threat scenario along Oregon’s I5 corridor in which string of fentanyl overdoses with a few fatalities affects the monitored data streams in three counties over a seven-day period. The right half of the figure charts the performance metrics for random forest and Bayesian network machine learning methods applied to both training and test datasets assuming total case counts of 50, 20, and 10 overdoses. Sensitivity values were encouraging, especially for the Bayesian networks and even for the 10-case scenario. Computed PPV levels suggested a manageable public health investigation burden. None Conclusions None The detection results were promising for a threat scenario of particular concern to OHA based on a data scenario deemed plausible and realistic based on historical data. Trust and acceptance from public health surveillance of outputs from supervised machine learning methods beyond traditional statistical methods will require user experience and similar evaluation with additional threat scenarios and authentic event data. Credible truth data can be generated for testing and evaluation of analytic fusion methods with the advantages of several years of historical data from multiple sources and the expertise of experienced monitors. The collaborative generation process may be standardized and extended to other threat types and data environments. Next steps include the addition to the analytic fusion capability of report-based data that can influence data interpretation, including mainstream and social media reports, events in neighboring regions, and law enforcement data. None References None 1. Rhode Island Enhanced State Opioid Overdose Surveillance (ESOOS) Case Definition for Emergency Medical Services (EMS), http://www.health.ri.gov/publications/guidelines/ESOOSCaseDefinitionForEMS.pdf, last accessed: Sept. 9, 2018.",project-academic
10.3390/S20164391,2020-08-06,a,Multidisciplinary Digital Publishing Institute,understanding sensor cities insights from technology giant company driven smart urbanism practices," The data-driven approach to sustainable urban development is becoming increasingly popular among the cities across the world. This is due to cities’ attention in supporting smart and sustainable urbanism practices. In an era of digitalization of urban services and processes, which is upon us, platform urbanism is becoming a fundamental tool to support smart urban governance, and helping in the formation of a new version of cities—i.e., City 4.0. This new version utilizes urban dashboards and platforms in its operations and management tasks of its complex urban metabolism. These intelligent systems help in maintaining the robustness of our cities, integrating various sensors (e.g., internet-of-things) and big data analysis technologies (e.g., artificial intelligence) with the aim of optimizing urban infrastructures and services (e.g., water, waste, energy), and turning the urban system into a smart one. The study generates insights from the sensor city best practices by placing some of renowned projects, implemented by Huawei, Cisco, Google, Ericsson, Microsoft, and Alibaba, under the microscope. The investigation findings reveal that the sensor city approach: (a) Has the potential to increase the smartness and sustainability level of cities; (b) Manages to engage citizens and companies in the process of planning, monitoring and analyzing urban processes; (c) Raises awareness on the local environmental, social and economic issues, and; (d) Provides a novel city blueprint for urban administrators, managers and planners. Nonetheless, the use of advanced technologies—e.g., real-time monitoring stations, cloud computing, surveillance cameras—poses a multitude of challenges related to: (a) Quality of the data used; (b) Level of protection of traditional and cybernetic urban security; (c) Necessary integration between the various urban infrastructure, and; (d) Ability to transform feedback from stakeholders into innovative urban policies.",project-academic
10.1016/J.MEHY.2019.109515,2020-03-01,a,Med Hypotheses,a novel ecg signal classification method using dea elm," Electrocardiogram (ECG) signals represent the electrical mobility of the human heart. In recent years, computer-aided systems have helped to cardiologists in the detection, classification and diagnosis of ECG. The aim of this paper is to optimize the number hidden neurons of the traditional Extreme Learning Machine (ELM) using Differential Evolution Algorithm (DEA) and contribute to the classification of ECG signals with a higher accuracy rate. In this paper, publicly ECG records in Physionet was utilized. Pan-Tompkins technique (PTT) and Discrete Wavelet Transform (DWT) approaches were implemented to obtain characteristic properties which are PR period, QT period, ST period and QRS wave of ECG signals. Then, ELM was executed to the ECG samples. Lastly, DEA on software ELM was developed for the assign of the number of hidden neurons, which were used in the ELM algorithm. The performance criterions were used in order to compare the performance of the classification exerted. Concordantly, it was realized that the highest classification achievement values were reached to Accuracy 97.5% and values 93 of number of hidden neurons, with the practice improved with the DEA compared to conventional ELM.",project-academic
10.1016/J.CONENGPRAC.2020.104630,2020-11-01,a,Pergamon,vision based robust control framework based on deep reinforcement learning applied to autonomous ground vehicles," Abstract None None Given the recent advances in computer vision, image processing and control systems, self-driving vehicles has been one of the most promising and challenging research topics nowadays. The design of vision-based robust controllers to keep an autonomous car in the center of the lane, despite uncertainties and disturbances, is still an ongoing challenge. This paper presents a hybrid control architecture that combines Deep Reinforcement Learning (DRL) and Robust Linear Quadratic Regulator (RLQR) for vision-based lateral control of an autonomous vehicle. Evolutionary estimation is used to model the vehicle uncertainties. For performance comparison, a DRL method and three other hybrid controllers are also evaluated. The inputs for each controller are real-time semantically segmented RGB camera images which serve as the basis to calculate continuous steering actions to keep the vehicle on the center of the lane with a constant velocity. Simulation results show that the proposed hybrid RLQR with evolutionary estimation of uncertainties architecture outperforms the other algorithms implemented. It presents lower tracking errors, smoother steering inputs, total collision avoidance and better generalization in new urban environments. Furthermore, it significantly decreases the required training time.",project-academic
10.1007/978-3-319-23222-5_35,2015-09-07,p,"Springer, Cham",crosswalk recognition through point cloud processing and deep learning suited to a wearable mobility aid for the visually impaired," In smart-cities, computer vision has the potential to dramatically improve the quality of life of people suffering of visual impairments. In this field, we have been working on a wearable mobility aid aimed at detecting in real-time obstacles in front of a visually impaired. Our approach relies on a custom RGBD camera, with FPGA on-board processing, worn as traditional eyeglasses and effective point-cloud processing implemented on a compact and lightweight embedded computer. This latter device also provides feedback to the user by means of an haptic interface as well as audio messages. In this paper we address crosswalk recognition that, as pointed out by several visually impaired users involved in the evaluation of our system, is a crucial requirement in the design of an effective mobility aid. Specifically, we propose a reliable methodology to detect and categorize crosswalks by leveraging on point-cloud processing and deep-learning techniques. The experimental results reported, on 10000+ frames, confirm that the proposed approach is invariant to head/camera pose and extremely effective even when dealing with large occlusions typically found in urban environments.",project-academic
10.28925/2663-4023.2020.8.97112,2020-06-25,a,Borys Grinchenko Kyiv University,application of the convolutional neural networks for the security of the object recognition in a video stream," The article is devoted to analyzing methods for recognizing images and finding them in the video stream. The evolution of the structure of convolutional neural networks used in the field of computer video flow diagnostics is analyzed. The performance of video flow diagnostics algorithms and car license plate recognition has been evaluated. The technique of recognizing the license plates of cars in the video stream of transport neural networks is described. The study focuses on the creation of a combined system that combines artificial intelligence and computer vision based on fuzzy logic. To solve the problem of license plate image recognition in the video stream of the transport system, a method of image recognition in a continuous video stream with its implementation based on the composition of traditional image processing methods and neural networks with convolutional and periodic layers is proposed. The structure and peculiarities of functioning of the intelligent distributed system of urban transport safety, which feature is the use of mobile devices connected to a single network, are described.
A practical implementation of a software application for recognizing car license plates by mobile devices on the Android operating system platform has been proposed and implemented. Various real-time vehicle license plate recognition scenarios have been developed and stored in a database for further analysis and use. The proposed application uses two different specialized neural networks: one for detecting objects in the video stream, the other for recognizing text from the selected image. Testing and analysis of software applications on the Android operating system platform for license plate recognition in real time confirmed the functionality of the proposed mathematical software and can be used to securely analyze the license plates of cars in the scanned video stream by comparing with license plates in the existing database. The authors have implemented the operation of the method of convolutional neural networks detection and recognition of license plates, personnel and critical situations in the video stream from cameras of mobile devices in real time. The possibility of its application in the field of safe identification of car license plates has been demonstrated.",project-academic
10.1016/J.TRC.2020.102678,2020-08-01,a,Pergamon,efficient proactive vehicle relocation for on demand mobility service with recurrent neural networks," Abstract None None One major challenge for on-demand mobility service (OMS) providers is to seamlessly match empty vehicles with trip requests so that the total vacant mileage is minimized. In this study, we develop an innovative data-driven approach for devising efficient vehicle relocation policy for OMS that (1) proactively relocates vehicles before the demand is observed and (2) reduces the inequality among drivers’ income so that the proactive relocation policy is fair and is likely to be followed by drivers. Our approach represents the fusion of optimization and machine learning methods, which comprises three steps: First, we formulate the optimal proactive relocation as an optimal/stable matching problems and solve for global optimal solutions based on historical data. Second, the optimal solutions are then grouped and fed to train the deep learning models which consist of fully connected layers and long short-term memory networks. Low rank approximation is introduced to reduce the model complexity and improve the training performances. Finally, we use the trained model to predict the relocation policy which can be implemented in real time. We conduct comprehensive numerical experiments and sensitivity analyses to demonstrate the performances of the proposed method using New York City taxi data. The results suggest that our method will reduce empty mileage per trip by 54–70% under the optimal matching strategy, and a 25–32% reduction can also be achieved by following the stable matching strategy. We also validate that the predicted relocation policies are robust in the presence of uncertain passenger demand level and passenger trip-requesting behavior.",project-academic
10.1109/NOMS47738.2020.9110366,2020-04-20,p,IEEE,edge concierge democratizing cost effective and flexible network operations using network layer ai at private network edges," We observe two major revolutionary trends in net-work operations: democratization of cost-effective and flexible communication means for vertical players, such as public safety, by private mobile networking combined with edge computing, and automatic and autonomic network operations empowered by Artificial Intelligence (AI). Further innovations are required for making private networking readily available for vertical players that are reluctant to acquire expertise in complex network operations. We propose Edge Concierge, of which concept is to democratize cost-effective and flexible network operations using network layer AI at private network edges. Edge Concierge assists smart network operations for private mobile network operators and energy saving by changing working state of AI-empowered anomaly detection applications by network layer AI. We also employ unsupervised machine learning using Hidden Markov Model (HMM) for estimating contexts by solely observing net-work traffic at mobile edge computing (MEC) middle boxes. In detail, we design a system of real-time and self-learning context estimation by a multi-level probabilistic state transition model trained by unsupervised learning, which is implemented in a commodity PC. In order to evaluate our proposed system, we take public safety context of smart cities as an example use case and show the benefits.",project-academic
10.1007/11760191_1,2006-05-28,p,"Springer, Berlin, Heidelberg",traffic volume forecasting based on wavelet transform and neural networks," This paper focuses on traffic volume forecasting that is an essential component of any responsive traffic control or route guidance system. A new approach for traffic volume prediction is proposed based on wavelet transform and neural networks. First, apply multi-resolution analysis to the original traffic volume time series to obtain a trend series and a hierarchy of detail series. Then apply neural networks to each obtained time series. Next sum all the forecasting values to get the final prediction of traffic volume. This hybrid method is implemented within a Matlab environment. The feasibility of the developed method as well as its validity to predict traffic volume has been demonstrated on real data gathered in Suzhou city. Moreover, a comparison between the hybrid method and conventional neural networks is conducted. The results show the proposed hybrid model outperformed the neural networks.",project-academic
10.1109/JIOT.2021.3090265,2021-06-17,a,IEEE,spatial temporal graph data mining for iot enabled air mobility prediction," Big data analytics and mining have the potential to enable real-time decision-making and control in a range of Internet of Things (IoT) application domains, such as the Internet of Vehicles, the Internet of Wings and the Airport of Things. The prediction toward air mobility, which is essential to the studies of air traffic management, has been a challenging task due to the complex spatial and temporal dependencies in air traffic data with highly nonlinear and variational patterns. Existing works for air traffic prediction only focus on either modeling static traffic patterns of individual flight or temporal correlation, with no or limited addressing of the spatial impact, namely the propagation of traffic perturbation among airports. In this paper, we propose to leverage the concept of graph and model the airports as nodes with time-series features and conduct data mining on graph-structured data. To be specific, firstly, Airline On-Time Performance (AOTP) Data is preprocessed to generate a temporal graph dataset, which includes three features: the number, average delay, and average taxiing time of departure and arrival flights. Then a spatial-temporal graph neural networks model is implemented to forecast the mobility level at each airport over time, where a combination of graph convolution and time-dimensional convolution is used to capture the spatial and temporal correlation simultaneously. Experiments on the dataset demonstrate the advantage of the model on spatial-temporal air mobility prediction, together with the impact of different priors on adjacency matrices and the effectiveness of temporal attention mechanism. Finally, we analyze the prediction performance and discuss the capability of our model. The prediction framework proposed in this work has the potential to be generalized to other spatial-temporal tasks in IoT.",project-academic
10.1016/J.SCITOTENV.2012.07.014,2012-10-01,a,Elsevier,using recorded sound spectra profile as input data for real time short term urban road traffic flow estimation," Road traffic has a heavy impact on the urban sound environment, constituting the main source of noise and widely dominating its spectral composition. In this context, our research investigates the use of recorded sound spectra as input data for the development of real-time short-term road traffic flow estimation models. For this, a series of models based on the use of Multilayer Perceptron Neural Networks, multiple linear regression, and the Fisher linear discriminant were implemented to estimate road traffic flow as well as to classify it according to the composition of heavy vehicles and motorcycles/mopeds. In view of the results, the use of the 50–400 Hz and 1–2.5 kHz frequency ranges as input variables in multilayer perceptron-based models successfully estimated urban road traffic flow with an average percentage of explained variance equal to 86%, while the classification of the urban road traffic flow gave an average success rate of 96.1%.",project-academic
10.1109/BIGDATA47090.2019.9006176,2019-12-01,p,IEEE,iot based urban noise monitoring in deep learning using historical reports," In this paper, we propose a new Internet of things (IoT) solution, called the Urban Noise Monitoring (UNM) system, which can classify real-time environmental audio sound using an embedded system such as Raspberry pi 4 and log the data in the Google Cloud. The reported events will be available for future usage, i.e., selection of the safe area for living. The real-time audio classification has been a big challenge for deep learning in environmental sounds due to the high noisy nature of sound. We have implemented a real-time IoT system for urban sound classification and monitored the historical reports generated. We have developed an advanced fusion method using normalization techniques such as peak, RMS, and EBU and an efficient data augmentation method using various factors, including time stretch, pitch shifting, and dynamic range compression. Further, we have integrated the normalization and the augmentation methods into 2D Convolutional Neural Network (CNN) with the TensorFlow framework on Raspberry pi 4 for urban sound classification. Our classification model outperformed the state of the art performance: 95% accuracy with the Urban sound dataset. The outstanding performance confirmed the effectiveness of the proposed method on the IoT system for urban noise monitoring.",project-academic
10.1007/978-981-10-1106-1_1,2016-01-01,a,"Springer, Singapore",architectural functional layout optimization in a coarse grid," This chapter describes the method for creating optimal architectural functional layouts. The methodology is based on coarse grid and three general steps: i. generation of layouts satisfying requirements given by the designer, ii. selection of the “proper” layouts, and iii. ranking of the “proper” layouts according to multiple objectives. Presented methodology can be used in architectural practice, urban or graphic design, and wherever the allocation of interrelated shapes is to be optimized. For clarity, simplified examples of a single-story two-apartment residential building are shown. Despite this simplicity, presented layouts resemble realistic functional solutions. One example of a practical-size floor-plan of three apartments of total twenty rooms is generated. The material is organized as follows: the concept of space discretization with coarse grid is introduced; the backtrack (depth-first) search algorithm is implemented for the generation of a number “potentially good” layouts. A machine learning method (feed-forward artificial neural network) is implemented for the classification of “proper” and “improper” layouts based on the “corridor criterion”. Simple examples of dynamic multi-criterial ranking of “proper” layouts are demonstrated.",project-academic
10.1109/BIGDATA47090.2019.9006009,2019-05-21,p,IEEE,high resolution road vehicle collision prediction for the city of montreal," Road accidents are an important issue of our modern societies, responsible for millions of deaths and injuries every year in the world. In Quebec only, in 2018, road accidents are responsible for 359 deaths and 33 thousands of injuries. In this paper, we show how one can leverage open datasets of a city like Montreal, Canada, to create high-resolution accident prediction models, using big data analytics. Compared to other studies in road accident prediction, we have a much higher prediction resolution, i.e., our models predict the occurrence of an accident within an hour, on road segments defined by intersections. Such models could be used in the context of road accident prevention, but also to identify key factors that can lead to a road accident, and consequently, help elaborate new policies.We tested various machine learning methods to deal with the severe class imbalance inherent to accident prediction problems. In particular, we implemented the Balanced Random Forest algorithm, a variant of the Random Forest machine learning algorithm in Apache Spark. Interestingly, we found that in our case, Balanced Random Forest does not perform significantly better than Random Forest.Experimental results show that 85% of road vehicle collisions are detected by our model with a false positive rate of 13%. The examples identified as positive are likely to correspond to high risk situations. In addition, we identify the most important predictors of vehicle collisions for the area of Montreal: the count of accidents on the same road segment during previous years, the temperature, the day of the year, the hour and the visibility.",project-academic
10.1109/ISMCR47492.2019.8955725,2019-09-01,p,IEEE,deep learning approach to control of prosthetic hands with electromyography signals," Natural muscles provide mobility in response to nerve impulses. Electromyography (EMG) measures the electrical activity of muscles in response to a nerve's stimulation. In the past few decades, EMG signals have been used extensively in the identification of user intention to potentially control assistive devices such as smart wheelchairs, exoskeletons, and prosthetic devices. In the design of conventional assistive devices, developers optimize multiple subsystems independently. Feature extraction and feature description are essential subsystems of this approach. Therefore, researchers proposed various hand-crafted features to interpret EMG signals. However, the performance of conventional assistive devices is still unsatisfactory. In this paper, we propose a deep learning approach to control prosthetic hands with raw EMG signals. We use a novel deep convolutional neural network to eschew the feature-engineering step. Removing the feature extraction and feature description is an important step toward the paradigm of end-to-end optimization. Fine-tuning and personalization are additional advantages of our approach. The proposed approach is implemented in Python with TensorFlow deep learning library, and it runs in real-time in general-purpose graphics processing units of NVIDIA Jetson TX2 developer kit. Our results demonstrate the ability of our system to predict fingers position from raw EMG signals. We anticipate our EMG-based control system to be a starting point to design more sophisticated prosthetic hands. For example, a pressure measurement unit can be added to transfer the perception of the environment to the user. Furthermore, our system can be modified for other prosthetic devices.",project-academic
10.1145/3300061.3343408,2019-10-11,p,ACM,poster optimizing mobile video telephony using deep imitation learning," Despite the pervasive use of real-time video telephony services, their quality of experience (QoE) remains unsatisfactory, especially over the mobile Internet. We conduct a large-scale measurement campaign on \appname, an operational mobile video telephony service. Our analysis shows that the application-layer video codec and transport-layer protocols remain highly uncoordinated, which represents one major reason for the low QoE. We thus propose \name, a machine learning based framework to resolve the issue. We train \name with the massive data traces from the measurement campaign using a custom-designed imitation learning algorithm, which enables \name to learn from past experience following an expert's iterative demonstration/supervision. We have implemented and incorporated \name into the \appname. Our experiments show that \name outperforms state-of-the-art solutions, improving video quality while reducing stalling time by multi-folds under various practical scenarios.",project-academic
10.2139/SSRN.3629271,2020-06-26,a,,how mathematical approaches could help decision making to epidemic control the successful experience against covid 19 in cuba," Background. There is a gap for the effective use of mathematical models for real-time decision-making. We aimed to illustrate with the Cuban experience to control the COVID-19, how mathematical models can be put in place to answer key decision-makers´ questions.

Methods. A science-policy partnership was created to mutually define questions, communicate results and facilitate the translation of modeling advice into actions. For forecasting and planning at national level mechanistic models and machine learning based on the epidemic patterns in other countries were used. Statistical models to explain the variability of transmission was used to stratify control actions. The effect of interventions was assessed using branching process models, time varying reproduction number (Rt) and social mixing patterns by location, and by age group.

Findings. The mathematical approach implemented contribute to successful control of the COVID-19 in Cuba. The urbanization, living conditions and the economic index explain the 73% of the variability of the transmission at provincial level. Increased risk of transmission were identified in 33 municipalities mostly in densely populated urban areas with high aging index. Control intervention reduced the transmission from R0=2.84 (95% CI: 1.52 - 4.76) to Rt=0.6 (95% CI:0.2-2.38 ). The highest transmission was detected among adolescents and from people older than 60 years.

Conclusions. Understanding the key questions for decision-making at all times, translating problems into a mathematical language, integrating different approaches to their solution and being able to present the results in an easy-to-understand way is vital to have a timely impact on controlling the epidemic.",project-academic
10.1136/BMJSTEL-2017-000289,2019-04-01,a,BMJ Specialist Journals,effective resource management using machine learning in medicine an applied example," Background None The field of medicine is rapidly becoming digitised, and in the process passively amassing large volumes of healthcare data. Machine learning and data analytics are advancing rapidly, but these have been slow to be taken up in the day-to-day delivery of healthcare. We present an application of machine learning to optimise a laboratory testing programme as an example of benefiting from these tools. None Methods None Canterbury District Health Board has recently implemented a system for urgent lab sample processing in the community, reducing unnecessary emergency presentations to hospital. Samples are transported from primary care facilities to a central laboratory. To improve the efficiency of this service, our team built a prototype transport scheduling platform using machine learning techniques and simulated the efficiency and cost impact of the platform using historical data. None Results None Our simulation demonstrated procedural efficiency and potential for annual savings between 5% and 14% from implementing a real-time lab sample transport scheduling platform. Advantages included providing a forward job list to the laboratory, an expected time to result and a streamlined transport request process. None Conclusion None There are a range of opportunities in healthcare to use large datasets for improved delivery of care. We have described an applied example of using machine learning techniques to improve the efficiency of community patient lab sample processing at scale. This is with a view to demonstrating practical avenues for collaboration between clinicians and machine learning engineers.",project-academic
,2015-01-01,a,hgpu.org,efficient convolutional patch networks for scene understanding," In this paper, we present convolutional patch networks, which are convolutional (neural) networks (CNN) learned to distinguish different image patches and which can be used for pixel-wise labeling. We show how to easily learn spatial priors for certain categories jointly with their appearance. Experiments for urban scene understanding demonstrate state-of-the-art results on the LabelMeFacade dataset. Our approach is implemented as a new CNN framework especially designed for semantic segmentation with fully-convolutional architectures. In the last years, the revival of convolutional (neural) networks (CNN) [5] has led to a breakthrough in computer vision and visual recognition. While the majority of works focuses on applying these techniques for object classification tasks, there is another field where CNNs can be really useful: semantic segmentation, i.e., assigning a class label to each pixel in an image. In this paper, we show how to learn spatial priors during CNN training, because some classes appear more frequently in some areas of an image. In general, predicting the label of a single pixel requires a large receptive field to incorporate as much context information as possible. We avoid this by incorporating absolute position information in a layer of the CNN as additional input. Urban scene understanding features a number of categories that need to be distinguished, such as buildings, cars, sidewalks, etc. We obtain state-ofthe-art performance in this domain on the LabelMeFacade dataset [4]. Architecture and CNN training Convolutional (neural) networks (CNNs) [5] are feed forward neural networks, which concatenate several layers of different types with convolutional layers playing a key role. The main idea is that the whole classification pipeline consists of one model, which can be jointly optimized during training. The goal of our network is to predict the object category for every single pixel in an image. The CNN architecture is completely described in [1]. However, in addition to [1], we implemented a fully-convolutional version [6] of it which input image convolutional, pooling and non-linear activation layers label estimation and segmentation absolute patch location Figure 2. Basic outline of our CNN architecture. The x and y feature maps allow for learning a spatial prior. is mathematically equivalent, but allows for fast prediction. We still train the network in a patch-wise manner, since preliminary experiments showed that training the network in a fully-convolutional manner (batches for gradient computation are comprised of full images only) resulted in slower (wall time) convergence and ultimately a less accurate network, in contrast to the results of [6] on other datasets. With image-based gradient batches, the model only learned to distinguish between the four most common classes. This may be due to our relatively small dataset resulting in a reduced randomization during optimization, altough we try to introduce more randomness by using spatial loss sampling as detailed in [6]. Incorporating spatial information Predicting the category by only using the information from a limited local receptive field can be challenging and in some cases impossible. We exploit that the absolute position of certain categories in the image is an important contextual cue. We provide the normalized position of a patch as an additional input to the CNN. In particular, the x ∈ [0, 1] and y ∈ [0, 1] coordinates are added as additional feature maps to one of the layers (Figure 2). Whereas incorporating the position information is a common and simple trick in semantic segmentation, with [4] being only one example, combining these priors with CNN feature learning has not been exploited before. New CNN library: CN24 We implemented a new open source CNN library specifically designed for semantic segmentation [1], which is publicly available. An important",project-academic
10.3390/S21020629,2021-01-18,a,Newcastle University,towards an end to end framework of cctv based urban traffic volume detection and prediction," Near real-time urban traffic analysis and prediction are paramount for effective intelligent transport systems. Whilst there is a plethora of research on advanced approaches to study traffic recently, only one-third of them has focused on urban arterials. A ready-to-use framework to support decision making in local traffic bureaus using largely available IoT sensors, especially CCTV, is yet to be developed. This study presents an end-to-end urban traffic volume detection and prediction framework using CCTV image series. The framework incorporates a novel Faster R-CNN to generate vehicle counts and quantify traffic conditions. Then it investigates the performance of a statistical-based model (SARIMAX), a machine learning (random forest; RF) and a deep learning (LSTM) model to predict traffic volume 30 min in the future. Tests at six locations with varying traffic conditions under different lengths of past time series are used to train the prediction models. RF and LSTM provided the most accurate predictions, with RF being faster than LSTM. The developed framework has been successfully applied to fill data gaps under adverse weather conditions when data are missing. It can be potentially implemented in near real time at any CCTV location and integrated into an online visualization platform.",project-academic
10.1109/BIGCOMP.2017.7881739,2017-02-01,p,IEEE,machine learning based path management for mobile devices over mptcp," Recent mobile devices are equipped with multiple network interfaces such as LTE and Wi-Fi. Transport protocols that can transfer data over multiple paths, especially MPTCP (Multipath TCP), allows the devices like smartphones and tablets to exploit both interfaces concurrently. However, in real environments, wireless devices abound and network quality changes frequently. It makes network connection affect the MPTCP performance negatively. In this paper, we propose a novel path management scheme called MPTCP-ML (MPTCP based on Machine Learning) to make MPTCP troubleshoot the problem. It manages path usage among multiple connections based on decision computed by machine learning model. For accurate capturing of path quality, we utilize various quality metrics including signal strength, data rate, TCP throughput, the number of interference APs, and RTT (Round Trip Time). We have implemented MPTCP-ML in Android and conducted experiments for various and dynamic environments. The results show that MPTCP-ML outperforms generic MPTCP, especially for mobile environments.",project-academic
10.1109/TSMC.2020.3006124,2020-07-20,a,Institute of Electrical and Electronics Engineers (IEEE),on training traffic predictors via broad learning structures a benchmark study," A fast architecture for real-time (i.e., minute-based) training of a traffic predictor is studied, based on the so-called broad learning system (BLS) paradigm. The study uses various traffic datasets by the California Department of Transportation, and employs a variety of standard algorithms (LASSO regression, shallow and deep neural networks, stacked autoencoders, convolutional, and recurrent neural networks) for comparison purposes: all algorithms are implemented in MATLAB on the same computing platform. The study demonstrates a BLS training process two-three orders of magnitude faster (tens of seconds against tens-hundreds of thousands of seconds), allowing unprecedented real-time capabilities. Additional comparisons with the extreme learning machine architecture, a learning algorithm sharing some features with BLS, confirm the fast training of least-square training as compared to gradient training.",project-academic
10.1007/S11761-017-0221-1,2018-06-01,p,Springer London,adaptive security architecture for protecting restful web services in enterprise computing environment," In this modern era of enterprise computing, the enterprise application integration (EAI) is a well-known industry-recognized architectural principle that is built based on loosely coupled application architecture, where service-oriented architecture (SOA) is the architectural pattern for the implementation of EAI, whose computational elements are called as “services.” Though SOA can be implemented in a wide range of technologies, the web services implementation of SOA becomes the current selective choice due to its simplicity that works on basic Internet protocols. Web service technology defines several supporting protocols and specifications such as SOAP and WSDL for communication with client and server for data interchange. A new architectural paradigm has emerged in SOA in recent years called REpresentational State Transfer (REST) that is also used to integrate loosely coupled service components, named RESTful web services, by system integration consortiums. This SOA implementation does not possess adequate security solutions within it, and its security is completely dependent on network/transport layer security that is obsolete owing to latest web technologies such as Web 2.0 and its upgraded version, Web 3.0. Vendor security products have major implementation constraints such as they need secured organizational environment and breach to SOA specifications, hence introducing new vulnerabilities. Herein, we examine the security vulnerabilities of RESTful web services in the view of popular OWASP rating methodologies and analyze the gaps in the existing security solutions. We hence propose an adaptive security solution for REST that uses public key infrastructure techniques to enhance the security architecture. The proposed security architecture is constructed as an adaptive way-forward Internet-of-Things (IoT) friendly security solution that is comprised of three cyclic parts: learn, predict and prevent. A novel security component named “intelligent security engine” is introduced which learns the possible occurrences of security threats on SOA using artificial neural networks learning algorithms, then it predicts the potential attacks on SOA based on obtained results by the developed theoretical security model, and the written algorithms as part of security solution prevent the SOA attacks. This paper is written to present one of such algorithms to prevent SOA attacks on RESTful web services along the discussion on the obtained results of the conducted proof-of-concept on the real-time SOA environment. A comparison of the proposed system with other competing solutions demonstrates its superiority.",project-academic
10.1117/1.JRS.13.024508,2019-04-25,a,SPIE-Intl Soc Optical Eng,deep learning based method for reconstructing three dimensional building cadastre models from aerial images," Nowadays, many of the world’s large cities are faced with the issue of land scarcity for construction due to the increasing growth of urbanization, as well as the economic downturn for exploiting lands and properties, and city officials have come up with the idea of optimal management of real estate in order to cope with these problems. The purpose of our study is to reconstruct three-dimensional (3-D) building cadastre models (3DBCMs) with an approach to improve the state of land administration in Tehran metropolis. Our study is being implemented and evaluated in three stages. The first stage involves collecting aerial images. The interior and exterior orientation parameters are preprepared in this step. The second stage involves automatic interpretation and extraction of buildings from aerial images by providing a method of interpretation called fully automatic interpretation with deep learning (FAIDL). The third stage involves 3-D building modeling and evaluating the effect of FAIDL method on the automatic interpretation of images. The results showed that the 3-D models of building have a better geometric accuracy compare to 60 cm, which are produced with the proposed algorithm.",project-academic
,2005-01-01,a,Chalmers University of Technology,integrated brake control downhill driving strategies," In this thesis, downhill driving strategies that co-ordinate the different brake actuators in heavy duty vehicles have been developed. As a starting point, the vehicle features affected by the retardation system are investigated. The main conclusions are that the retardation power demand will increase in the future and that, therefore, optimization of the brake systems will come to play a major role. In particular, strategies for the integration of foundation brakes, auxiliary brakes, and gear box have to be developed. Furthermore, these strategies must take component wear cost into consideration. Additionally, a thorough description of the current situation in terms of driver behaviour and existing systems for driver assistance is given. Optimal control and nonlinear programming have been used for the generation of open loop optimal driving strategies. Two different methods have been employed for the generation of implementable, closed loop, driving strategies. Firstly, a method that utilizes neural networks and genetic algorithms is presented. Secondly, in order to further enhance the controller transparency, and the possibility for robust implementation, the control problem is divided into two different modes of operation. Linear quadratic control using gain scheduling is then utilized for the controller design and generation of actuator reference values. Comparison with the open loop optimal strategies is also made. It is shown that transport efficiency (i.e. mean speed) and retardation economy (i.e. component wear cost) can be improved significantly, even compared to what skilled drivers can achieve, by integrating the whole retardation system. It is furthermore shown that there is a trade-off between component wear cost and transport efficiency that must be balanced in order to achieve good brake performance. The main parameters that affect the longitudinal control of the vehicle are the level of vehicle utilization (mass) and road slope. Algorithms for estimation of vehicle mass and road slope are therefore developed and presented. Additionally, a downhill driving strategy is implemented and verified in a real truck (A).",project-academic
10.1109/INDIN41052.2019.8972225,2019-07-22,p,IEEE,decentralizing air traffic flow management with blockchain based reinforcement learning," We propose and implement a decentralized, intelligent air traffic flow management (ATFM) solution to improve the efficiency of air transportation in the ASEAN region as a whole. Our system, named BlockAgent, leverages the inherent synergy between multi-agent reinforcement learning (RL) for air traffic flow optimization; and the rising blockchain technology for a secure, transparent and decentralized coordination platform. As a result, BlockAgent does not require a centralized authority for effective ATFM operations. We have implemented several novel distributed coordination approaches for RL in BlockAgent. Empirical experiments with real air traffic data concerning regional airports have demonstrated the feasibility and effectiveness of our approach. To the best of our knowledge, this is the first work that considers blockchain-based, distributed RL for ATFM.",project-academic
10.1109/COASE.2017.8256157,2017-08-01,p,IEEE,full automatic path planning of cooperating robots in industrial applications," Parts made of carbon fiber reinforced plastics (CFRP) for airplane components can be so huge that a single industrial robot is no longer able to handle them, and cooperating robots are required. Manual programming of cooperating robots is difficult, but with large numbers of different sized and shaped cut-pieces, it is almost impossible. This paper presents an automated production system consisting of a camera for the precise detection of the position of each cut-piece and a collision-free path planner which can dynamically react to different positions for the transfer motions. The path is planned for multiple robots adhering to motion constrains, such as the requirement that the textile cut-piece must form a catenary which can change during transport. Additionally a technique based on machine learning has been implemented which correctly resolves redundancy for a linear axis during planning. Finally, all components are tested on a real robot system in industrial scale.",project-academic
10.3390/S18010087,2017-12-30,a,Multidisciplinary Digital Publishing Institute,a novel segment based approach for improving classification performance of transport mode detection," Transportation planning and solutions have an enormous impact on city life. To minimize the transport duration, urban planners should understand and elaborate the mobility of a city. Thus, researchers look toward monitoring people’s daily activities including transportation types and duration by taking advantage of individual’s smartphones. This paper introduces a novel segment-based transport mode detection architecture in order to improve the results of traditional classification algorithms in the literature. The proposed post-processing algorithm, namely the Healing algorithm, aims to correct the misclassification results of machine learning-based solutions. Our real-life test results show that the Healing algorithm could achieve up to 40% improvement of the classification results. As a result, the implemented mobile application could predict eight classes including stationary, walking, car, bus, tram, train, metro and ferry with a success rate of 95% thanks to the proposed multi-tier architecture and Healing algorithm.",project-academic
10.1109/ICMLA.2016.0098,2016-12-01,p,Institute of Electrical and Electronics Engineers Inc.,constructing a deep regression model utilizing cascaded sparse autoencoders and stochastic gradient descent," This paper discusses utilizing sparse autoencoders for building regression models in order to predict real-valued timeseries data. The focus of this research is on exploiting and learning from the determining features of continuous data via stacked autoencoders, thus increasing the prediction accuracy of regression method. Archi-tecture comprising different layers of sparse autoencod-ers, where each level of autoencoders are trained based on the standard (typical) method are implemented and analyzed. In order to enhance the accuracy of the typical model of training autoencoders, cascaded model that benefits from the fusion of low-and high-level features is proposed. The objective is to improve vehicular traffic flow forecasting, since this area is a research field in pro-gress that impacts daily life. The SGD algorithm at the top level of deep architectures serves as the regression method. Evaluations are based on the precision accuracy of the algorithms applied to forecasting the traffic flow of a location down a key highway using the historical traffic data of several locations ahead in the Twin Cities Metro area of Minneapolis.",project-academic
10.1109/ECBS.2000.839880,2000-04-03,p,IEEE,enhancing network management using mobile agents," Agent mobility addresses some limitations faced by classic client/server architecture, namely, in minimising bandwidth consumption, in supporting adaptive network load balancing and in solving problems caused by intermittent or unreliable network connections. There has been a great deal of attention on the potential productivity gains expected from so-called intelligent agents. These however require complex artificial intelligence (Al) functionality. Agents can realistically be of benefit in those areas concerned with autonomy and mobility. This is especially true of network management applications and this will be the focus of this paper. The paper discusses the usage of mobile agents and the advantages that these have over traditional client/server applications. It discusses the main characteristics of an agent, and shows how Java has the main components that allow mobile agents to be easily development. To show how agents are implemented it gives a practical implemented of an agent. Finally, the paper also discusses the main Java agent development systems, which are IBM aglets, Object Space Voyager and JATLite and outlines the advantages of using each of them.",project-academic
10.3390/EN13133338,2020-06-30,a,Multidisciplinary Digital Publishing Institute,smart computational solutions for the optimization of selected technology processes as an innovation and progress in improving energy efficiency of smart cities a case study," The paper presents advanced computational solutions for selected sectors in the context of the optimization of technology processes as an innovation and progress in improving energy efficiency of smart cities. The main emphasis was placed on the sectors of critical urban infrastructure, including in particular the use of algorithmic models based on artificial intelligence implemented in supervisory control systems (SCADA-type, including Virtual SCADA) of technological processes involving the sewage treatment systems (including in particular wastewater treatment systems) and waste management systems. The novelty of the presented solution involves the use of predictive diagnostic tools, based on multi-threaded polymorphic models supporting decision making processes during the control of a complex technological process and objects of distributed network systems (smart water grid, smart sewage system, smart waste management system) and solving problems of optimal control for smart dynamic objects with logical representation of knowledge about the process, the control object and the control itself, for which the learning process consists of successive validation and updating of knowledge and the use of the results of this updating to make control decisions. The advantage of the proposed solution in relation to the existing ones lies in the use of advanced models of predictive diagnostics, validation and reconstruction of data, implemented in functional tools, allowing the stabilization of the work of technological objects through the use of FTC technology (fault tolerant control) and soft sensors, predictive measurement path diagnostics (sensors, transducers), validation and reconstruction of measurement data from sensors in the measuring paths in real time. The dedicated tools (Intelligent Real Time Diagnostic System − iRTDS) built into the system of a hierarchical, multi-threaded control optimizing system of SCADA system allow to obtain advanced diagnostics of technological processes in real time using HPC technology. In effect of the application of the proprietary iRTDS tool, we obtain a significant rise of energy efficiency of technological processes in key sectors of the economy, which in global terms, e.g., urban agglomeration, increases the economic efficiency.",project-academic
10.1109/ICC.2017.7996697,2017-05-01,p,IEEE,a novel video based application for road markings detection and recognition," Advanced Driving Assistant System (ADAS) was widely learned nowadays. As crucial parts of ADAS, video-based application like lane markings detection and other objects detection, have become more popular than before. However, most methods implemented in such areas cannot perfectly balance the performance of accuracy versus efficiency, and the mainstream methods (e.g. Machine Learning) suffer several limitations which can hardly break the wall between partial automation and fully automation. This paper proposed a real-time lane marking detection framework for ADAS, which included 4-extreme points set descriptor and a rule-based cascade classifier. Several experiments were conducted in highway and urban roads in Ottawa. The detection rate of the markings by the proposed algorithm reached an average accuracy rate of 96.77% while F 1 None Score (harmonic mean of precision and recall) also attained a rate of 90.57%. In summary, the proposed method exhibited a state-of-the-art performance and represents a significant advancement of understanding.",project-academic
10.1007/S10707-017-0314-1,2018-04-01,a,Springer US,spatio temporal prediction of crop disease severity for agricultural emergency management based on recurrent neural networks," As crop diseases bring huge losses every year in both developed and developing countries, determining how to precisely predict crop disease severity to facilitate agricultural emergency management is really a worldwide problem. Previous studies have introduced machine learning (ML) techniques into crop disease prediction and achieved better experimental results. However, the architectures of these ML models are unsuitable to model time series data. Moreover, the dependences among observations over time and across space have not been taken into account in model construction. By applying data-mining techniques to dynamic spatial panels of remote sensing data and considering features of bioclimatic, topographic and soil conditions as a supplement, we propose a novel crop disease prediction framework for agricultural emergency management based on ensemble learning techniques and spatio-temporal recurrent neural network (STRNN) which is an extension of recurrent neural network (RNN) in time and space. Empirical experiments are conducted on a specific dataset which is built based on reported cases of wheat yellow rust outbreaks in the Longnan city. Experimental results indicate that our proposed method outperforms all baseline models in crop disease severity prediction. The managerial implication of our work is that by applying the proposed methodology, some preparedness measures can be implemented in advance to prevent or mitigate the possible disasters according to predicted results. Notable economic and ecological benefits can be achieved by optimizing the frequency and timing of application of fungicide, pesticides and other preventative measures.",project-academic
10.1109/ICCSN.2009.140,2009-02-27,p,IEEE,short term traffic flow prediction based on anfis," Accurate short-term traffic flow prediction has become a critical problem in intelligent transportation systems (ITS). In the paper, a kind of adaptive prediction method for short-term traffic flow based on ANFIS (adaptive-network-based fuzzy interference system) model was presented. ANFIS is a fuzzy interference tool implemented in the framework of adaptive network. It combines the comprehensibility of fuzzy rules and the adaptability and self-learning algorithms of neural networks. The traffic flow prediction model with 104 changeable parameters will be established through the training process, the goal of which is reduce the prediction errors between real predicting output the ANFIS model and the desired output. The result of simulation research demonstrates that this method has the advantage of high precision and good adaptability. This scheme is novel and advanced in the domain of the road traffic flow prediction. The application of the scheme will remarkably improve the response efficiency and precision degree of the road traffic inducement and control system in our country.",project-academic
10.1109/IMCET.2018.8603041,2018-11-01,p,IEEE,smart traffic light system using machine learning," In Lebanon, traffic problems are a major concern for the population. The rising number of cars that exceeds the capacity of the roads, the inefficiency of public transportation infrastructures and the non-adaptive traffic light systems are contributors to the traffic crisis. Most roads in Lebanon suffer from traffic jams due to the traditional static green and red times allocations that are inconsiderate to the current state of the traffic. A solution to this problem is a system that adapts to the variations of the traffic dynamically and updates the traffic signal phases accordingly. In this paper, an adaptive traffic light system is implemented using reinforcement learning and tested using real data from Lebanese traffic. For training and testing the system, a software simulation tool is used. This tool can simulate the traffic intersection and allows the neural network to interact with it. Compared with the actual traffic light system, the proposed model displayed a reduction in average queue lengths by 62.82% and in average queuing time by 56.37%.",project-academic
10.13182/NSE04-A2417,2004-05-01,a,American Nuclear Society,improvement of mercure 6 s general formalism for calculating gamma ray buildup factors in multilayer shields," This study proposes an improvement of the general formalism for calculating gamma-ray buildup factors in multilayer shields developed by Assad et al. The main modification concerns the treatment of the double-layer shield formed by the two first layers of a multilayer shield. Instead of replacing the double-layer shield with an equivalent thickness of the layer of the second material, the improved general formalism replaces it with a single-layer shield made of an appropriate material. The determination of the appropriate material is implemented into MERCURE-6.1 thanks to neural networks trained on a large set of various configurations.One-dimensional comparisons with the TWODANT transport S{sub n} code shows the accuracy of the new formalism for shields composed of three and five layers. Indeed, for three-layer shields with an infinitesimal second layer and for multilayer shields composed of numerous thin layers (more than 15), MERCURE-6.1 matches the reference data quite well. The MERCURE-6.1 ability to solve three-dimensional realistic cases is highlighted by comparisons to the TRIPOLI-4 and MCNP-4C Monte Carlo codes.",project-academic
10.1109/MNET.011.2000007,2020-09-18,a,IEEE,cross domain resource orchestration for the edge computing enabled smart road," Intelligent driving plays a role in significantly improving the safety and efficiency of transportation systems. As the onboard capabilities of perception, comprehension, and decision making are limited, vehicles can employ the edge computing infrastructure of the smart road to enhance their intelligence. Therefore, the smart road is considered an intelligent Internet of Things system. It provides vehicles with not only the road space in the transportation domain, but also the communication, sensing, and computing resources in the information domain to improve the composite quality of intelligent driving. However, the resources in the information and transportation domains are complicatedly coupled, and the orchestration of these cross-domain resources is confronted with the huge state-action space, which cannot be solved in a real-time manner. In this article, we investigate the fundamental research challenges in cross-domain resource orchestration for the smart road, and design a multi-agent-based framework. Within the framework, each vehicle is associated with an exclusive agent on the edge cloud, and the agents utilize swarm intelligence to jointly optimize the traffic flow and information flow for their respective vehicles. Specifically, a value iteration network is used by agents to learn the routing behavior of vehicles, and a multi-agent deep reinforcement learning method is proposed, enabling agents to cooperatively learn decentralized resource optimization policies. To verify the effectiveness of the proposed framework, a cross-domain resource orchestration prototype is implemented and evaluated.",project-academic
10.1109/JIOT.2018.2834907,2018-05-10,a,IEEE,building data aware and energy efficient smart spaces," With the increase in the dependency of our life on technology and data, smart spaces have become integral in providing an environment for data collection, analysis, and machine responses. This paper discusses the current research in this field and the challenges that arise in the execution of these smart spaces. We address the major challenges of hardware design, data analysis, and energy efficiency in a new data aware smart environment that collects time-stamped data for position, movement, temperature, and vibration sensors. Data collected from these sensors is used to achieve energy efficiency, for real time localization in conjunction with machine learning mechanisms to analyze human activities. We evaluate six different machine learning algorithms for human activity detection task, on a data set collected in our laboratory. Results show high classification performance for all methods giving up to 99.95% classification accuracy. We also implemented energy-efficiency measures, leading to up to 30% energy efficiency improvement on top of our initial design. This ambient environment, along with data analytics and improved energy efficiency, provides information regarding the occupancy and behavior of people within its range. Spaces such as conference rooms, common areas such as libraries, classrooms, and even public spaces such as public transport can benefit from our design. Our system avoids privacy issues by using no audio/visual devices. This system thus provides an insight into smart spaces, their current trends, and what future direction research such as ours would lead them to.",project-academic
10.3390/S21144916,2021-07-19,a,Multidisciplinary Digital Publishing Institute,real time multipurpose smart waste classification model for efficient recycling in smart cities using multilayer convolutional neural network and perceptron," Urbanization is a big concern for both developed and developing countries in recent years. People shift themselves and their families to urban areas for the sake of better education and a modern lifestyle. Due to rapid urbanization, cities are facing huge challenges, one of which is waste management, as the volume of waste is directly proportional to the people living in the city. The municipalities and the city administrations use the traditional wastage classification techniques which are manual, very slow, inefficient and costly. Therefore, automatic waste classification and management is essential for the cities that are being urbanized for the better recycling of waste. Better recycling of waste gives the opportunity to reduce the amount of waste sent to landfills by reducing the need to collect new raw material. In this paper, the idea of a real-time smart waste classification model is presented that uses a hybrid approach to classify waste into various classes. Two machine learning models, a multilayer perceptron and multilayer convolutional neural network (ML-CNN), are implemented. The multilayer perceptron is used to provide binary classification, i.e., metal or non-metal waste, and the CNN identifies the class of non-metal waste. A camera is placed in front of the waste conveyor belt, which takes a picture of the waste and classifies it. Upon successful classification, an automatic hand hammer is used to push the waste into the assigned labeled bucket. Experiments were carried out in a real-time environment with image segmentation. The training, testing, and validation accuracy of the purposed model was 0.99% under different training batches with different input features.",project-academic
10.1109/ITEC.2019.8790451,2019-06-19,p,IEEE,an intelligent power and energy management system for fuel cell battery hybrid electric vehicle using reinforcement learning," Hybrid electric vehicles powered by fuel cells and batteries have attracted significant attention as they have the potential to eliminate emissions from the transport sector. However, fuel cells and batteries have several operational challenges, which require a power and energy management system (PEMS) to achieve optimal performance. Most of the existing PEMS methods are based on either predefined rules or prediction that are not adaptive to real-time driving conditions and may give solutions that are far from the actual optimal solution for a new drive cycle. Therefore, in this paper, an intelligent PEMS using reinforcement learning is presented, that can autonomously learn the optimal policy in real time through interaction with the onboard hybrid power system. This PEMS is implemented and tested on the simulation model of the onboard hybrid power system. The propulsion load is represented by the new European drive cycle. The results indicate that the PEMS algorithm is able to improve the lifetime of batteries and efficiency of the power system through minimizing the variation of the state of charge of battery.",project-academic
10.22115/SCCE.2020.217605.1172,2018-01-01,p,Pouyan Press,a real time warning system for rear end collision based on random forest classifier," Rear-end collision warning system has a great role to enhance driving safety. In this system, some measures are used to evaluate the safety and in the case of dangerous, the system warns drivers. This system should be executed in real-time, to remain enough time to avoid collision with the front vehicle. To this end, in this paper, a new system is developed by using a random forest classifier to extract knowledge about warning and safe situations. This knowledge can be extracted from accidents and vehicle trajectory data. Since the data of these situations are imbalanced, a combination of cost-sensitive learning and classification methods was used to improve the sensitivity, specificity, and processing time of classification. To evaluate the performance of this system, vehicle-trajectory-data of 100 cars that have been provided by Virginia tech transportation institute, are used. The comparison results are given in terms of accuracy and processing time. By using TOPSIS multi-criteria selection method, it is shown that the implemented classifier is better than different classifiers including Bayesian network, Naive Bayes, MLP neural network, support vector machine, k-nearest neighbor, rule-based methods and decision tree. The implemented random forest gets 88.4% accuracy for detection of the dangerous situations and 94.7% for detection of the safe situations. Also, the proposed system is more robust compared with the perceptual-based and kinematic-based algorithms.",project-academic
10.1016/J.KNOSYS.2011.01.007,2011-05-01,a,Elsevier,a knowledge based problem solving method in gis application," Model design for theme analysis is one of the biggest challenges in GIS. Many real applications in GIS require functioning not only in data management and visualization, but also in analysis and decision-making. Confronted with an application of planning a new metro line in a city, a typical GIS is unable to accomplish the task in the absence of human experts or artificial intelligence technologies. Apart from being models for analyzing in different themes, some applications are also instances of problem solving in AI. Therefore, in order to strengthen its ability in automatic analysis, many theories and technologies from AI can be embedded in the GIS. In this paper, a state space is defined to formalize the metro line planning problem. By utilizing the defined state evaluation function, knowledge-based rules and strategies, a heuristic searching method is developed to optimize the solutions iteratively. Experiments are implemented to illuminate the validity of this AI-enhanced automatic analysis model of GIS.",project-academic
,2019-12-03,,,urban waterlogging monitoring and pre warning system and pre warning method," The invention relates to an urban waterlogging monitoring and pre-warning system and pre-warning method. The urban waterlogging monitoring and pre-warning system comprises a processing unit, a surfacerunoff water depth monitoring unit, an underground drainage network monitoring unit, a meteorological information monitoring unit, a rainfall monitoring unit, a communication unit and a drainage unit, wherein the surface runoff water depth monitoring unit is electrically connected with the processing unit; and the processing unit is suitable for judging the urban waterlogging condition accordingto the actual surface runoff flow of various monitoring points, an excess surface water depth, an underground drainage network drainage state, weather information and rainfall, and uploading the urbanwaterlogging condition to a server through the communication unit in real time, and controlling the drainage unit to drain water when the processing unit judges urban waterlogging. According to the urban waterlogging monitoring and pre-warning system and the pre-warning method, analysis and statistics are carried out by comprehensively transmitting and processing monitoring data and combining bigdata with deep learning artificial intelligence, early prewarning information is issued, processing planning measures are generated in advance and intelligent control is implemented on electrical equipment at various sites.",project-academic
10.1109/SBESC.2018.00015,2018-11-01,p,IEEE Computer Society,an embedded automatic license plate recognition system using deep learning," An automatic system to recognize vehicle license plates is a growing need to improve safety and traffic control, specifically in major urban centers. The License Plate Recognition task is generally a computational-intensive task, where the entire input image frame is scanned, the found plates are segmented, and character recognition is then performed. Frequently, this processing is made using general purposes GPUs. This paper proposes an embedded solution to detect and recognize Brazilian license plates using convolutional neural networks (CNN). The system was implemented in a Raspberry Pi3 with a Pi NoIR v2 camera module, which was used to obtain the images of vehicles. The proposed system detects license plates in the captured image using Tiny YOLOv3 architecture and identifies its characters using a second convolutional network trained on synthetic images and fine-tuned with real license plate images. The proposed system has demonstrated to be robust to angle, lightning and noise variations. The system was validated using real license plate images under different environmental conditions reached a detection rate of 99.37% and an overall recognition rate of 97.00% while showing an average time of 2.70 seconds to process 1024x768 images with a single license plate.",project-academic
10.1145/3458864.3468161,2021-06-24,p,ACM,ztt learning based dvfs with zero thermal throttling for mobile devices," DVFS (dynamic voltage and frequency scaling) is a system-level technique that adjusts voltage and frequency levels of CPU/GPU at runtime to balance energy efficiency and high performance. DVFS has been studied for many years, but it is considered still challenging to realize a DVFS that performs ideally for mobile devices for two main reasons: i) an optimal power budget distribution between CPU and GPU in a power-constrained platform can only be defined by the application performance, but conventional DVFS implementations are mostly application-agnostic; ii) mobile platforms experience dynamic thermal environments for many reasons such as mobility and holding methods, but conventional implementations are not adaptive enough to such environmental changes. In this work, we propose a deep reinforcement learning-based frequency scaling technique, zTT. zTT learns thermal environmental characteristics and jointly scales CPU and GPU frequencies to maximize the application performance in an energy-efficient manner while achieving zero thermal throttling. Our evaluations for zTT implemented on Google Pixel 3a and NVIDIA JETSON TX2 platform with various applications show that zTT can adapt quickly to changing thermal environments, consistently resulting in high application performance with energy efficiency. In a high-temperature environment where a rendering application with the default mobile DVFS fails to keep producing more than a target frame rate, zTT successfully manages to do so even with 23.9% less average power consumption.",project-academic
10.1109/TPWRD.2020.3022750,2020-09-08,a,IEEE,real time hierarchical neural network based fault detection and isolation for high speed railway system under hybrid ac dc grid," Reliable and comfortable high-speed railway (HSR) has skyrocketed in popularity as a transportation medium for traveling around the world. High-voltage direct current (HVDC) electrification system has been introduced to the HSR gradually. However, the coexistence of AC and DC systems will last for a long time because AC railway systems are still in the dominant position. A detailed HSR traction system transient model operating under the hybrid AC/DC grid was established in PSCAD/EMTDC. We proposed a real-time fault detection and isolation (FDI) method for the simulated model using neural network (NN). Hierarchical structure of the monitoring system has been employed. Low-level sub-monitors supervised the conditions of their local regions and the top-level monitor collected all the feedback from sub-monitors making the final evaluation of the entire HSR system based on a voting strategy. Both off-line and real-time experiments were conducted to validate the effectiveness of the proposed method. In the experiments, the sub-monitors were designed based on Gated Recurrent Unit (GRU) algorithm and implemented on the Xilinx VCU128 FPGA board. For the off-line experiment, the sub-monitors used the training and testing dataset both from PSCAD/EMTDC to construct the architecture of their individual GRU networks and to verify how great the networks can be. For the real-time task, the sub-monitors interfaced with a real-time HSR system emulator running on the Xilinx VCU118 FPGA board to test the performance in the real-time application. The results proved that our proposed FDI method has the capability of real-time detection and can achieve better accuracy within reasonable time and resource consumption than other NN-based methods. Moreover, the method was capable of standing against noises from measured signals to some extent.",project-academic
10.1109/ACCESS.2020.2990190,2020-04-24,a,"IEEE, Institute of Electrical and Electronics Engineers",a novel simulated annealing based electric bus system design simulation and analysis for dehradun smart city," Smart transportation network development with environmental issues into consideration has brought Industry 4.0 based solutions on priority. In this direction, battery-powered electric bus systems have been considered widely for ensuring flexibility, operation cost, and lesser pollutants emission. Industry 4.0 provides automation through a cyber-physical system (CPS), the interconnection of bus system entities with industrial internet-of-things (IIoT), remote information availability through cloud computing and scientific disciplines (human-computer interaction, artificial intelligence, machine learning etc.) integration. In this work, a discrete event-based simulation-optimization approach is integrated that take care of bus energy consumption according to real-time city's passenger needs and on-road friction levels. The proposed simulation optimization methodology utilizes multi-objective with dependent and independent variables for optimizing the overall system performance. In simulation optimization, objective functions are designed to tackle battery consumption, Internet-of-Thing (IoT) network performance, cloud operations efficiency and smart scientific discipline integration. Simulation parameters are based on a real-time bus system which is further analyzed, filtered and adapted as per the needs of the system. In another analysis, supercharger's capacities are varied to evaluate the performance of the proposed system and identify the low cost and efficient smart transportation system. Simulation results show different scenarios for variations in the number of buses, charging stations, bus-depots, mobile charging facilities, and bus-schedules. Simulation results show that the average passenger's waiting time in the waiting is (after ticket booking) varies between 0.2 minutes to 0.7 minutes in real-time traffic conditions. In similar traffic conditions, total passenger's time in system (ticket booking to travel) varies between 41.6 minutes (for 24 hours) to 45.5 minutes (for 1 year). In the simulation, priorities are given to those dependent and independent variables which save the battery consumption and elongate the utilization of buses. Lastly, it is also observed that the proposed system is suitable for resource-constraint devices because Gate Equivalent (GE) calculation shows that the proposed system can be implemented between 1986 GEs (communicational cost without confidentiality and authentication) and 7939 GEs (computational cost with HMAC for authentication in data storage). This ensures varies security primitivs such as confidentiality, availability and authentication.",project-academic
10.1098/RSOS.180409,2018-10-01,a,The Royal Society,multi robot replication of ant collective towing behaviours," In this work, teams of small mobile robots are used to test hypotheses about cooperative transport by ants. This study attempts to explain a decrease in steady-state transport speed with increasing team size that was previously observed in the ant None Novomessor cockerelli . Two models of one-dimensional collective towing are compared: one in which transporters with different maximum speeds pull the payload with continuous, variable forces and another in which transporters with identical speeds pull with intermittent, unsynchronized forces. A statistical analysis of ant data supports the hypothesis that ants behave according to the first model, in which the steady-state transport speed is the maximum speed of the slowest teammate. By contrast, the ant data are not consistent with the second model, which predicts constant speed regardless of team size. To verify these predictions, the ant behaviours in each model are translated into decentralized controllers and implemented on teams of two to four robots. The controller for the first model incorporates a real-time reinforcement learning algorithm that successfully reproduces the observed relationship between ant team size and transport speed. The controller for the second model yields the predicted invariance of transport speed with team size. These results show the value of robotic swarms for testing mechanistic hypotheses about biological collectives.",project-academic
10.5075/EPFL-THESIS-5434,2012-01-01,a,EPFL,assessment of foot signature using wearable sensors for clinical gait analysis and real time activity recognition," Locomotion is one of the most important abilities of humans. Actually, gait locomotion provides mobility, and symbolizes freedom and independence. However, gait can be affected by several pathologies, due to aging, neurodegenerative disease, or trauma. The evaluation and treatment of mobility diseases thus requires clinical gait assessment, which is commonly done by using either qualitative analysis based on subjective observations and questionnaires, or expensive analysis established in complex motion laboratories settings. This thesis presents a new wearable system and algorithmic methods for gait assessment in natural conditions, addressing the limitations of existing methods. The proposed system provides quantitative assessment of gait performance through simple and precise outcome measures. The system includes wireless inertial sensors worn on the foot, that record data unobtrusively over long periods of time without interfering with subject's walking. Signal processing algorithms are presented for the automatic calibration and online virtual alignment of sensor signals, the detection of temporal parameters and gait phases, and the estimation of 3D foot kinematics during gait based on fusion methods and biomechanical assumptions. The resulting 3D foot trajectory during one gait cycle is defined as Foot Signature, by analogy with hand-written signature. Spatio-temporal parameters of interest in clinical assessment are derived from foot signature, including commonly parameters, such as stride velocity and gait cycle time, as well as original parameters describing inner-stance phases of gait, foot clearance, and turning. Algorithms based on expert and machine learning methods have been also adapted and implemented in real-time to provide input features to recognize locomotion activities including level walking, stairs, and ramp locomotion. Technical validation of the presented methods against gold standard systems was carried out using experimental protocols on subjects with normal and abnormal gait. Temporal aspects and quantitative estimation of foot-flat were evaluated against pressure insoles in subjects with ankle treatments during long-term gait. Furthermore, spatial parameters and foot clearance were compared in young and elderly persons to data obtained from an optical motion capture system during forward gait trials at various speeds. Finally, turning was evaluated in children with cerebral palsy and people with Parkinson's disease against optical motion capture data captured during timed up and go and figure-of-8 tests. Overall, the results demonstrated that the presently proposed system and methods were precise and accurate, and showed agreement with reference systems as well as with clinical evaluations of subjects' mobility disease using classical scores. Currently, no other methods based on wearable sensors have been validated with such precision to measure foot signature and subsequent parameters during unconstrained walking. Finally, we have used the proposed system in a large-scale clinical application involving more than 1800 subjects from age 7 to 77. This analysis provides reference data of common and original gait parameters, as well as their relationship with walking speed, and allows comparisons between different groups of subjects with normal and abnormal gait. Since the presented methods can be used with any foot-worn inertial sensors, or even combined with other systems, we believe our work to open the door to objective and quantitative routine gait evaluations in clinical settings for supporting diagnosis. Furthermore, the present studies have high potential for further research related to rehabilitation based on real-time devices, the investigation of new parameters' significance and their association with various mobility diseases, as well as for the evaluation of clinical interventions.",project-academic
10.1109/ICII.2018.00024,2018-10-01,p,IEEE,brightics iot towards effective industrial iot platforms for connected smart factories," Industrial Internet-of-Things (IIoT) supports machines, computers and users to enable intelligent operations using advanced device management and data analytics. In recent years, thanks to standardized IoT platforms and advanced Artificial Intelligence (AI) technologies, there have been great advances in IIoT, and now it promises revolutions on various manufacturing domains such as transport, health, factory and energy. In this paper, based on our experience operating IIoT in various factory applications, we present the technical challenges of manufacturing facilities needed to be dealt with to collect huge amount of data in real-time and counteraction points of an IoT platform regarding these technical challenges and what kinds of features need to be implemented for intelligent services in the smart manufacturing. Finally, we introduce a story of applying industrial IoT platform in production to show how iterative development approaches can achieve business requirements based on elastically scaled-out architecture.",project-academic
10.1109/NEUREL.2018.8587004,2018-11-01,p,IEEE,smart warehouse management system concept with implementation," Distribution companies use complex software systems called WMS (Warehouse Management System). The WMS is an important part of the company’s business and it can make processes simple to keep track of. Smart WMS optimizes processes to save resources and to create a more efficient working place. This paper describes the concept of a smart WMS that is implemented in one of the largest distribution companies in Bosnia and Herzegovina. The system uses artificial intelligence and optimization algorithms to improve working process. The paper describes the complete warehouse workflow that includes stock planning, initial product placement, transfer from stock to pick zone, order picking process, transport and tracking. The anomaly detection is used in some processes to improve the whole system. The main contribution of this paper is the presentation of an efficient and in the real world used smart WMS concept.",project-academic
10.1007/S10845-018-1433-8,2020-01-01,a,Springer US,literature review of industry 4 0 and related technologies," Manufacturing industry profoundly impact economic and societal progress. As being a commonly accepted term for research centers and universities, the Industry 4.0 initiative has received a splendid attention of the business and research community. Although the idea is not new and was on the agenda of academic research in many years with different perceptions, the term “Industry 4.0” is just launched and well accepted to some extend not only in academic life but also in the industrial society as well. While academic research focuses on understanding and defining the concept and trying to develop related systems, business models and respective methodologies, industry, on the other hand, focuses its attention on the change of industrial machine suits and intelligent products as well as potential customers on this progress. It is therefore important for the companies to primarily understand the features and content of the Industry 4.0 for potential transformation from machine dominant manufacturing to digital manufacturing. In order to achieve a successful transformation, they should clearly review their positions and respective potentials against basic requirements set forward for Industry 4.0 standard. This will allow them to generate a well-defined road map. There has been several approaches and discussions going on along this line, a several road maps are already proposed. Some of those are reviewed in this paper. However, the literature clearly indicates the lack of respective assessment methodologies. Since the implementation and applications of related theorems and definitions outlined for the 4th industrial revolution is not mature enough for most of the reel life implementations, a systematic approach for making respective assessments and evaluations seems to be urgently required for those who are intending to speed this transformation up. It is now main responsibility of the research community to developed technological infrastructure with physical systems, management models, business models as well as some well-defined Industry 4.0 scenarios in order to make the life for the practitioners easy. It is estimated by the experts that the Industry 4.0 and related progress along this line will have an enormous effect on social life. As outlined in the introduction, some social transformation is also expected. It is assumed that the robots will be more dominant in manufacturing, implanted technologies, cooperating and coordinating machines, self-decision-making systems, autonom problem solvers, learning machines, 3D printing etc. will dominate the production process. Wearable internet, big data analysis, sensor based life, smart city implementations or similar applications will be the main concern of the community. This social transformation will naturally trigger the manufacturing society to improve their manufacturing suits to cope with the customer requirements and sustain competitive advantage. A summary of the potential progress along this line is reviewed in introduction of the paper. It is so obvious that the future manufacturing systems will have a different vision composed of products, intelligence, communications and information network. This will bring about new business models to be dominant in industrial life. Another important issue to take into account is that the time span of this so-called revolution will be so short triggering a continues transformation process to yield some new industrial areas to emerge. This clearly puts a big pressure on manufacturers to learn, understand, design and implement the transformation process. Since the main motivation for finding the best way to follow this transformation, a comprehensive literature review will generate a remarkable support. This paper presents such a review for highlighting the progress and aims to help improve the awareness on the best experiences. It is intended to provide a clear idea for those wishing to generate a road map for digitizing the respective manufacturing suits. By presenting this review it is also intended to provide a hands-on library of Industry 4.0 to both academics as well as industrial practitioners. The top 100 headings, abstracts and key words (i.e. a total of 619 publications of any kind) for each search term were independently analyzed in order to ensure the reliability of the review process. Note that, this exhaustive literature review provides a concrete definition of Industry 4.0 and defines its six design principles such as interoperability, virtualization, local, real-time talent, service orientation and modularity. It seems that these principles have taken the attention of the scientists to carry out more variety of research on the subject and to develop implementable and appropriate scenarios. A comprehensive taxonomy of Industry 4.0 can also be developed through analyzing the results of this review.",project-academic
,2018-07-12,a,,assessing the scalability of biologically motivated deep learning algorithms and architectures," The backpropagation of error algorithm (BP) is impossible to implement in a real brain. The recent success of deep networks in machine learning and AI, however, has inspired proposals for understanding how the brain might learn across multiple layers, and hence how it might approximate BP. As of yet, none of these proposals have been rigorously evaluated on tasks where BP-guided deep learning has proved critical, or in architectures more structured than simple fully-connected networks. Here we present results on scaling up biologically motivated models of deep learning on datasets which need deep networks with appropriate architectures to achieve good performance. We present results on the MNIST, CIFAR-10, and ImageNet datasets and explore variants of target-propagation (TP) and feedback alignment (FA) algorithms, and explore performance in both fully- and locally-connected architectures. We also introduce weight-transport-free variants of difference target propagation (DTP) modified to remove backpropagation from the penultimate layer. Many of these algorithms perform well for MNIST, but for CIFAR and ImageNet we find that TP and FA variants perform significantly worse than BP, especially for networks composed of locally connected units, opening questions about whether new architectures and algorithms are required to scale these approaches. Our results and implementation details help establish baselines for biologically motivated deep learning schemes going forward.",project-academic
10.1038/NATURE23307,2017-08-17,a,Nature Research,chaotic dynamics in nanoscale nbo 2 mott memristors for analogue computing," A relaxation oscillator incorporating nanoscale niobium dioxide memristors that exhibit both a current- and a temperature-controlled negative differential resistance produces chaotic dynamics that aid biomimetic computing. In recent years, grids of memristor devices, with their synapse-like dynamics and adaptable conductivity, have demonstrated neural-network-type implementations of analogue (non-Boolean) computing. Suhas Kumar et al. now explore the possibility of exploiting chaotic dynamics in highly nonlinear niobium dioxide memristor devices. This idea is inspired by the theory that biological neurons operate in a regime called 'the edge of chaos', which is thought to be key to the ability of the human brain to tackle complex information processing tasks with high efficiency. The authors demonstrate a controllable regime of chaotic self-oscillations in their devices and simulate a memristor grid that can solve a typical computationally hard task—a travelling salesman problem—with higher accuracy and efficiency than an approach that does not incorporate chaotic elements. Building artificial neural networks with chaotic oscillators based on single electronic devices provides an exciting direction for unconventional analogue computing. At present, machine learning systems use simplified neuron models that lack the rich nonlinear phenomena observed in biological systems, which display spatio-temporal cooperative dynamics. There is evidence that neurons operate in a regime called the edge of chaos1 that may be central to complexity, learning efficiency, adaptability and analogue (non-Boolean) computation in brains2,3,4,5,6,7. Neural networks have exhibited enhanced computational complexity when operated at the edge of chaos2, and networks of chaotic elements have been proposed for solving combinatorial or global optimization problems8. Thus, a source of controllable chaotic behaviour that can be incorporated into a neural-inspired circuit may be an essential component of future computational systems. Such chaotic elements have been simulated using elaborate transistor circuits that simulate known equations of chaos9,10,11,12, but an experimental realization of chaotic dynamics from a single scalable electronic device has been lacking5,6,13. Here we describe niobium dioxide (NbO2) Mott memristors each less than 100 nanometres across that exhibit both a nonlinear-transport-driven current-controlled negative differential resistance and a Mott-transition-driven temperature-controlled negative differential resistance. Mott materials have a temperature-dependent metal–insulator transition that acts as an electronic switch, which introduces a history-dependent resistance into the device. We incorporate these memristors into a relaxation oscillator14 and observe a tunable range of periodic and chaotic self-oscillations15. We show that the nonlinear current transport coupled with thermal fluctuations at the nanoscale generates chaotic oscillations. Such memristors could be useful in certain types of neural-inspired computation by introducing a pseudo-random signal that prevents global synchronization and could also assist in finding a global minimum during a constrained search. We specifically demonstrate that incorporating such memristors into the hardware of a Hopfield computing network can greatly improve the efficiency and accuracy of converging to a solution for computationally difficult problems.",project-academic
10.1038/S41563-018-0248-5,2019-02-01,a,Nature Publishing Group,ionic modulation and ionic coupling effects in mos2 devices for neuromorphic computing," Coupled ionic-electronic effects present intriguing opportunities for device and circuit development. In particular, layered two-dimensional materials such as MoS2 offer highly anisotropic ionic transport properties, facilitating controlled ion migration and efficient ionic coupling among devices. Here, we report reversible modulation of MoS2 films that is consistent with local 2H-1T' phase transitions by controlling the migration of Li+ ions with an electric field, where an increase/decrease in the local Li+ ion concentration leads to the transition between the 2H (semiconductor) and 1T' (metal) phases. The resulting devices show excellent memristive behaviour and can be directly coupled with each other through local ionic exchange, naturally leading to synaptic competition and synaptic cooperation effects observed in biology. These results demonstrate the potential of direct modulation of two-dimensional materials through field-driven ionic processes, and can lead to future electronic and energy devices based on coupled ionic-electronic effects and biorealistic implementation of artificial neural networks.",project-academic
10.1016/J.COMCOM.2020.02.069,2020-03-15,a,Elsevier,applications of artificial intelligence and machine learning in smart cities," Abstract None None Smart cities are aimed to efficiently manage growing urbanization, energy consumption, maintain a green environment, improve the economic and living standards of their citizens, and raise the people’s capabilities to efficiently use and adopt the modern information and communication technology (ICT). In the smart cities concept, ICT is playing a vital role in policy design, decision, implementation, and ultimate productive services. The primary objective of this review is to explore the role of artificial intelligence (AI), machine learning (ML), and deep reinforcement learning (DRL) in the evolution of smart cities. The preceding techniques are efficiently used to design optimal policy regarding various smart city-oriented complex problems. In this survey, we present in-depth details of the applications of the prior techniques in intelligent transportation systems (ITSs), cyber-security, energy-efficient utilization of smart grids (SGs), effective use of unmanned aerial vehicles (UAVs) to assure the best services of 5G and beyond 5G (B5G) communications, and smart health care system in a smart city. Finally, we present various research challenges and future research directions where the aforementioned techniques can play an outstanding role to realize the concept of a smart city.",project-academic
10.1109/TITS.2019.2906038,2020-03-01,a,IEEE,real time sensor anomaly detection and identification in automated vehicles," Connected and automated vehicles (CAVs) are expected to revolutionize the transportation industry, mainly through allowing for a real-time and seamless exchange of information between vehicles and roadside infrastructure. Although connectivity and automation are projected to bring about a vast number of benefits, they can give rise to new challenges in terms of safety, security, and privacy. To navigate roadways, CAVs need to heavily rely on their sensor readings and the information received from other vehicles and roadside units. Hence, anomalous sensor readings caused by either malicious cyber attacks or faulty vehicle sensors can result in disruptive consequences and possibly lead to fatal crashes. As a result, before the mass implementation of CAVs, it is important to develop methodologies that can detect anomalies and identify their sources seamlessly and in real time. In this paper, we develop an anomaly detection approach through combining a deep learning method, namely convolutional neural network (CNN), with a well-established anomaly detection method, and Kalman filtering with a None None None $\chi ^{2}$ None None -detector, to detect and identify anomalous behavior in CAVs. Our numerical experiments demonstrate that the developed approach can detect anomalies and identify their sources with high accuracy, sensitivity, and F1 score. In addition, this developed approach outperforms the anomaly detection and identification capabilities of both CNNs and Kalman filtering with a None None None $\chi ^{2}$ None None -detector method alone. It is envisioned that this research will contribute to the development of safer and more resilient CAV systems that implement a holistic view toward intelligent transportation system (ITS) concepts.",project-academic
10.1063/1.5108912,2019-09-24,a,AIP Publishing LLC AIP Publishing,novel frontier of photonics for data processing photonic accelerator," In the emerging Internet of things cyber-physical system-embedded society, big data analytics needs huge computing capability with better energy efficiency. Coming to the end of Moore’s law of the electronic integrated circuit and facing the throughput limitation in parallel processing governed by Amdahl’s law, there is a strong motivation behind exploring a novel frontier of data processing in post-Moore era. Optical fiber transmissions have been making a remarkable advance over the last three decades. A record aggregated transmission capacity of the wavelength division multiplexing system per a single-mode fiber has reached 115 Tbit/s over 240 km. It is time to turn our attention to data processing by photons from the data transport by photons. A photonic accelerator (PAXEL) is a special class of processor placed at the front end of a digital computer, which is optimized to perform a specific function but does so faster with less power consumption than an electronic general-purpose processor. It can process images or time-serial data either in an analog or digital fashion on a real-time basis. Having had maturing manufacturing technology of optoelectronic devices and a diverse array of computing architectures at hand, prototyping PAXEL becomes feasible by leveraging on, e.g., cutting-edge miniature and power-efficient nanostructured silicon photonic devices. In this article, first the bottleneck and the paradigm shift of digital computing are reviewed. Next, we review an array of PAXEL architectures and applications, including artificial neural networks, reservoir computing, pass-gate logic, decision making, and compressed sensing. We assess the potential advantages and challenges for each of these PAXEL approaches to highlight the scope for future work toward practical implementation.",project-academic
10.1155/2017/5106045,2017-06-18,a,Hindawi,modeling pm2 5 urban pollution using machine learning and selected meteorological parameters," Outdoor air pollution costs millions of premature deaths annually, mostly due to anthropogenic fine particulate matter (or PM2.5). Quito, the capital city of Ecuador, is no exception in exceeding the healthy levels of pollution. In addition to the impact of urbanization, motorization, and rapid population growth, particulate pollution is modulated by meteorological factors and geophysical characteristics, which complicate the implementation of the most advanced models of weather forecast. Thus, this paper proposes a machine learning approach based on six years of meteorological and pollution data analyses to predict the concentrations of PM2.5 from wind (speed and direction) and precipitation levels. The results of the classification model show a high reliability in the classification of low ( 25 µg/m3) and low (<10 µg/m3) versus moderate (10–25 µg/m3) concentrations of PM2.5. A regression analysis suggests a better prediction of PM2.5 when the climatic conditions are getting more extreme (strong winds or high levels of precipitation). The high correlation between estimated and real data for a time series analysis during the wet season confirms this finding. The study demonstrates that the use of statistical models based on machine learning is relevant to predict PM2.5 concentrations from meteorological data.",project-academic
,2017-09-08,,,personalized recommendation system based on deep learning under social network," The invention discloses a personalized recommendation system based on deep learning under a social network. The system mainly comprises an offline learning module and an online recommendation module. The offline learning module firstly generates a training sample seat to construct a deep convolutional neural network learning module with an attention mechanism and carries out iterative optimization on parameters in the learning module; and the online recommendation module carries out real-time item recommendation on a newly-registered user based on the learning model obtained through training. Compared with the prior art, the system has the advantages of high accuracy, fast speed and simplicity and easiness in implementation and can be effectively applied to the fields, such as electronic commerce, public opinion monitoring, intelligent transportation and medical treatment and health.",project-academic
10.1109/ACCESS.2018.2846609,2018-06-12,a,IEEE,ubehealth a personalized ubiquitous cloud and edge enabled networked healthcare system for smart cities," Smart city advancements are driving massive transformations of healthcare, the largest global industry. The drivers include increasing demands for ubiquitous, preventive, and personalized healthcare, to be provided to the public at reduced risks and costs. Mobile cloud computing could potentially meet the future healthcare demands by enabling anytime, anywhere capture and analyses of patients’ data. However, network latency, bandwidth, and reliability are among the many challenges hindering the realization of next-generation healthcare. This paper proposes a ubiquitous healthcare framework, UbeHealth, that leverages edge computing, deep learning, big data, high-performance computing (HPC), and the Internet of Things (IoT) to address the aforementioned challenges. The framework enables an enhanced network quality of service using its three main components and four layers. Deep learning, big data, and HPC are used to predict network traffic, which in turn are used by the Cloudlet and network layers to optimize data rates, data caching, and routing decisions. Application protocols of the traffic flows are classified, enabling the network layer to meet applications’ communication requirements better and to detect malicious traffic and anomalous data. Clustering is used to identify the different kinds of data originating from the same application protocols. A proof of concept UbeHealth system has been developed based on the framework. A detailed literature review is used to capture the design requirements for the proposed system. The system is described in detail including the algorithmic implementation of the three components and four layers. Three widely used data sets are used to evaluate the UbeHealth system.",project-academic
10.1016/J.JENVMAN.2011.01.020,2011-06-01,a,J Environ Manage,system dynamics modeling for municipal water demand estimation in an urban region under uncertain economic impacts," Accurate prediction of municipal water demand is critically important to water utilities in fast-growing urban regions for drinking water system planning, design, and water utility asset management. Achieving the desired prediction accuracy is challenging, however, because the forecasting model must simultaneously consider a variety of factors associated with climate changes, economic development, population growth and migration, and even consumer behavioral patterns. Traditional forecasting models such as multivariate regression and time series analysis, as well as advanced modeling techniques (e.g., expert systems and artificial neural networks), are often applied for either short- or long-term water demand projections, yet few can adequately manage the dynamics of a water supply system because of the limitations in modeling structures. Potential challenges also arise from a lack of long and continuous historical records of water demand and its dependent variables. The objectives of this study were to (1) thoroughly review water demand forecasting models over the past five decades, and (2) propose a new system dynamics model to reflect the intrinsic relationship between water demand and macroeconomic environment using out-of-sample estimation for long-term municipal water demand forecasts in a fast-growing urban region. This system dynamics model is based on a coupled modeling structure that takes into account the interactions among economic and social dimensions, offering a realistic platform for practical use. Practical implementation of this water demand forecasting tool was assessed by using a case study under the most recent alternate fluctuations of economic boom and downturn environments.",project-academic
10.1016/J.MCPRO.2021.100138,2021-08-17,a,Elsevier,trapped ion mobility spectrometry and parallel accumulation serial fragmentation in proteomics," Recent advances in efficiency and ease of implementation have rekindled interest in ion mobility spectrometry, a technique which separates gas phase ions by their size and shape and which can be hybridized with conventional liquid chromatography and mass spectrometry. Here, we review the recent development of trapped ion mobility spectrometry (TIMS) coupled to time-of-flight mass analysis. In particular, the parallel accumulation - serial fragmentation (PASEF) operation mode offers unique advantages in terms of sequencing speed and sensitivity. Its defining feature is that it synchronizes the release of ions from the TIMS device with the downstream selection of precursors for fragmentation in a TIMS - quadrupole - time-of-flight (timsTOF) configuration. As ions are compressed into narrow ion mobility peaks, the number of peptide fragment ion spectra obtained in data-dependent or targeted analyses can be increased by an order of magnitude without compromising sensitivity. Taking advantage of the correlation between ion mobility and mass, the PASEF principle also multiplies the efficiency of data-independent acquisition. This makes the technology well suited for rapid proteome profiling, an increasingly important attribute in clinical proteomics, as well as for ultra-sensitive measurements down to single cells. The speed and accuracy of TIMS and PASEF also enable precise measurements of collisional cross section (CCS) values at the scale of more than a million data points, and the development of neural networks capable of predicting them based only on peptide sequences. Peptide CCS values can differ for isobaric sequences or positional isomers of post-translational modifications. This additional information may be leveraged in real-time to direct data acquisition or in post-processing to increase confidence in peptide identifications. These developments make timsTOF-PASEF a powerful and expandable platform for proteomics and beyond.",project-academic
10.1145/3191752,2018-03-26,a,ACM,third eye a mobilephone enabled crowdsensing system for air quality monitoring," Air pollution has raised people's public health concerns in major cities, especially for Particulate Matter under 2.5μm (PM2.5) due to its significant impact on human respiratory and circulation systems. In this paper, we present the design, implementation, and evaluation of a mobile application, Third-Eye, that can turn mobile phones into high-quality PM2.5 monitors, thereby enabling a crowdsensing way for fine-grained PM2.5 monitoring in the city. We explore two ways, crowdsensing and web crawling, to efficiently build large-scale datasets of the outdoor images taken by mobile phone, weather data, and air-pollution data. Then, we leverage two deep learning models, Convolutional Neural Network (CNN) for images and Long Short Term Memory (LSTM) network for weather and air-pollution data, to build an end-to-end framework for training PM2.5 inference models. Our App has been downloaded more than 2,000 times and runs more than 1 year. The real user data based evaluation shows that Third-Eye achieves 17.38 μg/m3 average error and 81.55% classification accuracy, which outperforms 5 state-of-the-art methods, including three scattered interpolations and two image based estimation methods. The results also demonstrate how Third-Eye offers substantial enhancements over typical portable PM2.5 monitors by simultaneously improving accessibility, portability, and accuracy.",project-academic
10.1007/S00477-015-1128-Z,2016-05-01,a,Springer Berlin Heidelberg,modeling urban growth with gis based cellular automata and least squares svm rules a case study in qingpu songjiang area of shanghai china," A critical issue in urban cellular automata (CA) modeling concerns the identification of transition rules that generate realistic urban land use patterns. Recent studies have demonstrated that linear methods cannot sufficiently delineate the extraordinary complex boundaries between urban and non-urban areas and as most urban CA models simulate transitions across these boundaries, there is an urgent need for good methods to facilitate such delineations. This paper presents a machine learning CA model (termed MachCA) with nonlinear transition rules based on least squares support vector machines (LS-SVM) to simulate such urban growth. By projecting the input dataset into a high dimensional space using the LS-SVM method, an optimal hyper-plane is constructed to separate the complex boundaries between urban and nonurban land, thus enabling the retrieval of nonlinear CA transition rules. In the MachCA model, the transition rules are yes–no decisions on whether a cell changes its state or not, the rules being dynamically updated for each iteration of the model implementation. The application of the MachCA for simulating urban growth in the Shanghai Qingpu–Songjiang area in China reveals that the spatial configurations of rural–urban patterns can be modeled. A comparison of the MachCA model with a conventional CA model fitted by logarithmic regression (termed LogCA) shows that the MachCA model produces more hits and less misses and false alarms due to its capability for capturing the spatial complexity of urban dynamics. This results in improved simulation accuracies, although with only less than 1 % deviation between the overall errors produced by the MachCA and LogCA models. Nevertheless, the way MachCA model use in retrieving the transition rules provides a new method for simulating the dynamic process of urban growth.",project-academic
10.1145/3360322.3360998,2019-11-13,p,ACM,citylearn v1 0 an openai gym environment for demand response with deep reinforcement learning," Demand response has the potential of reducing peaks of electricity demand by about 20% in the US, where buildings represent roughly 70% of the total electricity demand. Buildings are dynamic systems in constant change (i.e. occupants' behavior, refurbishment measures), which are costly to model and difficult to coordinate with other urban energy systems. Reinforcement learning is an adaptive control algorithm that can control these urban energy systems relying on historical and real-time data instead of models. Plenty of research has been conducted in the use of reinforcement learning for demand response applications in the last few years. However, most experiments are difficult to replicate, and the lack of standardization makes the performance of different algorithms difficult, if not impossible, to compare. In this demo, we introduce a new framework, CityLearn, based on the OpenAI Gym Environment, which will allow researchers to implement, share, replicate, and compare their implementations of reinforcement learning for demand response applications more easily. The framework is open source and modular, which allows researchers to modify and customize it, e.g., by adding additional storage, generation, or energy-consuming systems.",project-academic
10.1109/VLSID.2019.00058,2019-01-01,p,IEEE,mavi mobility assistant for visually impaired with optional use of local and cloud resources," Independent mobility of visually impaired people is key to making an inclusive society for them. Unstructured infrastructure in developing countries pose significant challenges in developing aids to address the mobility problem of visually impaired. Most of the assistive devices available internationally assume a structured and controlled environment severely restricting the applicability of such devices. In this paper, we assess the ability of state-of-the-art assistive devices for addressing the independent outdoor mobility needs of the visually impaired in an unstructured environment. We have created realistic datasets for various scenarios and evaluate deep neural networks for object detection on these datasets. We also present a portable prototype for the task. Further, we have also developed a cloud based solution to address the mobility requirements. We compare the local device based and cloud based solutions in terms of accuracy, latency, and energy. We present and discuss results from these two implementations that can provide insights for an effective solution. The results and insights open up novel research problems for embedded systems.",project-academic
10.1007/S10462-012-9383-6,2015-03-01,a,Springer Netherlands,application of reinforcement learning to routing in distributed wireless networks a review," The dynamicity of distributed wireless networks caused by node mobility, dynamic network topology, and others has been a major challenge to routing in such networks. In the traditional routing schemes, routing decisions of a wireless node may solely depend on a predefined set of routing policies, which may only be suitable for a certain network circumstances. Reinforcement Learning (RL) has been shown to address this routing challenge by enabling wireless nodes to observe and gather information from their dynamic local operating environment, learn, and make efficient routing decisions on the fly. In this article, we focus on the application of the traditional, as well as the enhanced, RL models, to routing in wireless networks. The routing challenges associated with different types of distributed wireless networks, and the advantages brought about by the application of RL to routing are identified. In general, three types of RL models have been applied to routing schemes in order to improve network performance, namely Q-routing, multi-agent reinforcement learning, and partially observable Markov decision process. We provide an extensive review on new features in RL-based routing, and how various routing challenges and problems have been approached using RL. We also present a real hardware implementation of a RL-based routing scheme. Subsequently, we present performance enhancements achieved by the RL-based routing schemes. Finally, we discuss various open issues related to RL-based routing schemes in distributed wireless networks, which help to explore new research directions in this area. Discussions in this article are presented in a tutorial manner in order to establish a foundation for further research in this field.",project-academic
10.1016/J.FOODCONT.2019.107016,2020-04-01,a,Elsevier,improving efficiency of rfid based traceability system for perishable food by utilizing iot sensors and machine learning model," Abstract None None Radio Frequency Identification (RFID) technology has significantly improved in the past few years and is presently sought for implementation in the identification and traceability of perishable food in the food sector to safeguard food safety and quality. It is currently considered a worthy successor to the barcode system and has significant advantages for monitoring products in the perishable food supply chain (PFSC). The present study proposes a traceability system that utilizes RFID and Internet of Things (IoT) sensors. RFID technology can be used to track and trace perishable food while IoT sensors can be used to measure temperature and humidity during storage and transportation. Furthermore, it is important that RFID gates can identify the direction of tags and whether products are being received or shipped through the gate. In this study, machine-learning models are utilized to detect the direction of passive RFID tags. The input features are derived from receive signal strength (RSS) and the timestamp of tags. The proposed system has been tested in the perishable food supply chain and has revealed significant benefits to managers and customers by providing real-time product information and complete temperature and humidity history. In addition, by integrating a machine-learning model into the RFID gate, tagged products that move in or out through a gate can be correctly identified and thus improve the efficiency of the traceability system.",project-academic
10.1109/TCST.2011.2180386,2013-01-01,a,IEEE,prediction of short term traffic variables using intelligent swarm based neural networks," This brief presents an innovative algorithm integrated with particle swarm optimization and artificial neural networks to develop short-term traffic flow predictors, which are intended to provide traffic flow forecasting information for traffic management in order to reduce traffic congestion and improve mobility of transportation. The proposed algorithm aims to address the issues of development of short-term traffic flow predictors which have not been addressed fully in the current literature namely that: 1) strongly non-linear characteristics are unavoidable in traffic flow data; 2) memory space for implementation of short-term traffic flow predictors is limited; 3) specification of model structures for short-term traffic flow predictors which do not involve trial and error methods based on human expertise; and 4) adaptation to newly-captured, traffic flow data is required. The proposed algorithm was applied to forecast traffic flow conditions on a section of freeway in Western Australia, whose traffic flow information is newly-captured. These results clearly demonstrate the effectiveness of using the proposed algorithm for real-time traffic flow forecasting.",project-academic
10.1016/J.ADVENGSOFT.2006.07.003,2007-03-01,a,Elsevier,an ontology based knowledge management system for flow and water quality modeling," Currently, the numerical simulation of flow and/or water quality becomes more and more sophisticated. There arises a demand on the integration of recent knowledge management (KM), artificial intelligence technology with the conventional hydraulic algorithmic models in order to assist novice application users in selection and manipulation of various mathematical tools. In this paper, an ontology-based KM system (KMS) is presented, which employs a three-stage life cycle for the ontology design and a Java/XML-based scheme for automatically generating knowledge search components. The prototype KMS on flow and water quality is addressed to simulate human expertise during the problem solving by incorporating artificial intelligence and coupling various descriptive knowledge, procedural knowledge and reasoning knowledge involved in the coastal hydraulic and transport processes. The ontology is divided into information ontology and domain ontology in order to realize the objective of semantic match for knowledge search. The architecture, the development and the implementation of the prototype system are described in details. Both forward chaining and backward chaining are used collectively during the inference process. In order to demonstrate the application of the prototype KMS, a case study is presented.",project-academic
10.1016/J.ADVWATRES.2020.103600,2020-06-01,a,Elsevier,deep reinforcement learning for the real time control of stormwater systems," Abstract None None A new generation of smart stormwater systems promises to reduce the need for new construction by enhancing the performance of the existing infrastructure through real-time control. Smart stormwater systems dynamically adapt their response to individual storms by controlling distributed assets, such as valves, gates, and pumps. This paper introduces a real-time control approach based on Reinforcement Learning (RL), which has emerged as a state-of-the-art methodology for autonomous control in the artificial intelligence community. Using a Deep Neural Network, a RL-based controller learns a control strategy by interacting with the system it controls - effectively trying various control strategies until converging on those that achieve a desired objective. This paper formulates and implements a RL algorithm for the real-time control of urban stormwater systems. This algorithm trains a RL agent to control valves in a distributed stormwater system across thousands of simulated storm scenarios, seeking to achieve water level and flow set-points in the system. The algorithm is first evaluated for the control of an individual stormwater basin, after which it is adapted to the control of multiple basins in a larger watershed (4 km2). The results indicate that RL can very effectively control individual sites. Performance is highly sensitive to the reward formulation of the RL agent. Generally, more explicit guidance led to better control performance, and more rapid and stable convergence of the learning process. While the control of multiple distributed sites also shows promise in reducing flooding and peak flows, the complexity of controlling larger systems comes with a number of caveats. The RL controller’s performance is very sensitive to the formulation of the Deep Neural Network and requires a significant amount of computational resource to achieve a reasonable performance enhancement. Overall, the controlled system significantly outperforms the uncontrolled system, especially across storms of high intensity and duration. A frank discussion is provided, which should allow the benefits and drawbacks of RL to be considered when implementing it for the real-time control of stormwater systems. An open source implementation of the full simulation environment and control algorithms is also provided.",project-academic
10.1016/J.ADVWATRES.2009.01.001,2009-04-01,a,Elsevier,pumping optimization of coastal aquifers based on evolutionary algorithms and surrogate modular neural network models," Pumping optimization of coastal aquifers involves complex numerical models. In problems with many decision variables, the computational burden for reaching the optimal solution can be excessive. Artificial Neural Networks (ANN) are flexible function approximators and have been used as surrogate models of complex numerical models in groundwater optimization. However, this approach is not practical in cases where the number of decision variables is large, because the required neural network structure can be very complex and difficult to train. The present study develops an optimization method based on modular neural networks, in which several small subnetwork modules, trained using a fast adaptive procedure, cooperate to solve a complex pumping optimization problem with many decision variables. The method utilizes the fact that salinity distribution in the aquifer, depends more on pumping from nearby wells rather than from distant ones. Each subnetwork predicts salinity in only one monitoring well, and is controlled by relatively few pumping wells falling within certain control distance from the monitoring well. While the initial control area is radial, its shape is adaptively improved using a Hermite interpolation procedure. The modular neural subnetworks are trained adaptively during optimization, and it is possible to retrain only the ones not performing well. As optimization progresses, the subnetworks are adapted to maximize performance near the current search space of the optimization algorithm. The modular neural subnetwork models are combined with an efficient optimization algorithm and are applied to a real coastal aquifer in the Greek island of Santorini. The numerical code SEAWAT was selected for solving the partial differential equations of flow and density dependent transport. The decision variables correspond to pumping rates from 34 wells. The modular subnetwork implementation resulted in significant reduction in CPU time and identified an even better solution than the original numerical model.",project-academic
10.1109/TASE.2018.2865494,2018-09-10,a,IEEE,shared subway shuttle bus route planning based on transport data analytics," The development requirements of shared buses are extremely urgent to alleviate urban traffic congestions by improving road resource utilization and to provide a neotype transportation mode with good user experiences. The key to shared bus implementation lies in accurately predicting travel requirements and planning dynamic routes. However, the sparseness and the high volatility of shared bus data bring a great resistance to accurate prediction of travel requirements. Based on the consideration of user experiences, optimization objectives of shared bus route planning are significantly different from traditional public transportation and shared bus route planning is far more challenging than online car-hailing services due to the relatively high number of passengers. In this paper, we put forward a two-stage approach (SubBus), which is composed of travel requirement prediction and dynamic routes planning, based on various crowdsourced shared bus data to generate dynamic routes for shared buses in the “last mile” scene. First, we analyze the resident travel behaviors to obtain five predictive features, such as flow, time, week, location, and bus, and utilize them to predict travel requirements accurately based on a machine learning model. Second, we design a dynamic programming algorithm to generate dynamic, optimal routes with fixed destinations for multiple operating buses utilizing prediction results based on operating characteristics of shared buses. Extensive experiments are performed on real crowdsourced shared subway shuttle bus data and demonstrate that SubBus outperforms other methods on dynamic route planning for the “last mile” scene. None Note to Practitioners —This paper is inspired by the problem of shared subway shuttle bus dynamic route planning for the “last mile” scene, and it is also applicable to other scenes, including commuting scenes, urban transportation hub scenes, and destination scenes of the tourist market. Shared bus operation routes at such scenes are usually aimed at trips with fixed destinations. Existing approaches to planning routes are generally designed for traditional transportation, such as traditional buses and taxis. In this paper, we propose a novel two-stage dynamic route planning approach (SubBus) based on the operation characteristics of shared subway shuttle buses. We perform a resident travel behavior analysis to improve the accuracy of travel requirement prediction. After that, we combine the prediction results and station properties to gain shared bus optimal routes. We then display how to apply SubBus to optimize shared bus operation status based on crowdsourced shared subway shuttle bus data generated by Panda Bus Company. We keep a continuous collaboration with the company to optimize the approach details and experimental effects, which demonstrate that our approach can generate effective routes for shared subway shuttle buses to optimize operation status on the “last mile” issue.",project-academic
10.1016/J.JCLEPRO.2019.119623,2020-04-10,a,Elsevier,energy saving behavior of urban residents in china a multi agent simulation," Abstract None None With recent improvements in residents’ quality of life and the implementation of the two-child policy, guiding the energy-saving behavior of urban residents has become a focus for achieving the national goals of the sustainable development strategy in China. Considering the subjective initiatives of individuals in a realistic environment is the key to studying energy-saving behavior and guiding policy making. This study builds a simulation model of the energy-saving behavior of urban residents using agent-based modeling and simulation (ABMS) methods, which are based on complex adaptive theory. By means of artificial neural networks and the Netlogo simulation platform, the subsequent effect of behavioral outcomes due to the short- and long-term influence of energy-saving behavior and intentions is analyzed in different policy situation. The results show that energy-saving intentions and behavior are poorly matched in the absence of an external policy framework. In the optimal policy situation, residents’ energy-saving intentions and behavior have improved significantly. Policies can significantly encourage energy-saving intentions to become behavior. Different kinds of situational factors have different effects on intentions and the four types of energy-saving behavior. Finally, relevant policy implications are proposed based on analysis of the simulation results.",project-academic
10.1109/MIS.2019.2942836,2020-01-01,a,IEEE,research on road traffic situation awareness system based on image big data," Road traffic is an important component of the national economy and social life. Promoting intelligent and Informa ionization construction in the field of road traffic is conducive to the construction of smart cities and the formulation of macro strategies and construction plans for urban traffic development. Aiming at the shortcomings of the current road traffic system, this article, on the basis of combining convolution neural network, situational awareness technology, database and other technologies, takes the road traffic situational awareness system as the research object, and analyzes the information collection, processing, and analysis process of road traffic situational awareness system. Convolutional neural networks (CNN), region-CNN (R-CNN), fast R-CNN, and faster R-CNN are used for vehicle class classification and location identification in road image big data. The deep convolutional neural network model based on road traffic image big data was further established, and the system requirements analysis and system framework design and implementation were carried out. Through the analysis and trial of actual cases, the results show the application effect of the realized road traffic situational awareness system, which provides a scientific reference and basis for the establishment of modern intelligent transportation system.",project-academic
10.1016/J.ENGAPPAI.2016.10.015,2017-01-01,a,Pergamon,fault diagnosis of marine 4 stroke diesel engines using a one vs one extreme learning ensemble," This paper proposes a novel approach for intelligent fault diagnosis for stroke Diesel marine engines, which are commonly used in on-road and marine transportation. The safety and reliability of a ship's work rely strongly on the performance of such an engine; therefore, early detection of any type of failure that affects the engine is of crucial importance. Automatic diagnostic systems are of special importance because they can operate continuously in real time, thereby providing efficient monitoring of the engine's performance. We introduce a fully automatic machine learning-based system for engine fault detection. For this purpose, we monitor various signals that are emitted by the engine, and we use them as an input for a pattern classification algorithm. This action is realized by an ensemble of Extreme Learning Machines that work in a decomposition mode. Because we address 14 different faults and a correct operation mode, we must handle a 15-class problem. We tackle this task by binarization in one-vs-one mode, where each Extreme Learning Machine is trained on a pair of classes. Next, Error-Correcting Output Codes are used to reconstruct the original multi-class task. The results from experiments that were conducted on a real-life dataset demonstrate that the proposed approach delivers superior classification accuracy and a low response time in comparison with a number of state-of-the-art methods and thus is a suitable choice for a real-life implementation on board a ship.",project-academic
,2014-01-01,a,,exploiting new sensor technologies for real time parking prediction in urban areas," This paper proposes a methodological framework - based on survival analysis and neural networks - to provide parking availability forecasts for extended prediction horizons. Two different types of predictions are provided: i. the probability of a free space to continue being free in subsequent time intervals, and ii. the short-term parking occupancy prediction in selected regions of an urban road network. The available data comes from a wide network of parking sensors installed on-street in the “smart” city of Santander, Spain. The sensor network is segmented in four different regions and, then, survival and neural network models are developed for each region separately. Findings show that the Weibull parametric models best describe the probability of a space continuing to be free in the forthcoming time intervals. Simple genetically optimized Multilayer Perceptrons accurately predict region parking occupancy up to 1 hour in the future by only exploiting 5 minute data. Finally, the real time, web based, implementation of the proposed parking prediction availability system is presented.",project-academic
10.3390/S19092206,2019-05-13,a,Sensors (Basel),smarter traffic prediction using big data in memory computing deep learning and gpus," Road transportation is the backbone of modern economies, albeit it annually costs 1.25 million deaths and trillions of dollars to the global economy, and damages public health and the environment. Deep learning is among the leading-edge methods used for transportation-related predictions, however, the existing works are in their infancy, and fall short in multiple respects, including the use of datasets with limited sizes and scopes, and insufficient depth of the deep learning studies. This paper provides a novel and comprehensive approach toward large-scale, faster, and real-time traffic prediction by bringing four complementary cutting-edge technologies together: big data, deep learning, in-memory computing, and Graphics Processing Units (GPUs). We trained deep networks using over 11 years of data provided by the California Department of Transportation (Caltrans), the largest dataset that has been used in deep learning studies. Several combinations of the input attributes of the data along with various network configurations of the deep learning models were investigated for training and prediction purposes. The use of the pre-trained model for real-time prediction was explored. The paper contributes novel deep learning models, algorithms, implementation, analytics methodology, and software tool for smart cities, big data, high performance computing, and their convergence.",project-academic
10.1109/ISCAS45731.2020.9181034,2020-10-12,p,IEEE,neuromorphic information processing with nanowire networks," Biological neural networks, unlike artificial neural networks (ANNs), can process information from data that is inherently noisy, unstructured, sparse and dynamic. How is this possible? And how can we replicate this? A crucial clue comes from neuroscience: the brain is a complex physical system and the network topology of its neural circuitry is a determinant of its emergent collective properties. Indeed, as the brain's neural network is already entrained in its physical hardware, it clearly does not require an ANN software add-on to learn from natural data. Self-assembled nanowire networks with memristive junctions represent arguably the closest hardware architecture to real biological neural networks and are thus uniquely placed to demonstrate genuinely neuromorphic information processing. Here, we present preliminary results on polymer-coated silver nanowire networks. Their neuromorphic architecture (densely structured network topology, memristive switch junctions and efficient interconnect) gives rise to a rich repertoire of collective nonlinear dynamics manifested through adaptive current transport pathways. The potential for associative learning is demonstrated in a test protocol in which a nanowire network is stimulated by multiple electrodes mapped to different spatial patterns. The capacity to process information in the temporal domain is demonstrated via simulations of a reservoir computing implementation in which nanowire networks are shown to perform tasks such as time series prediction and handwritten digit recognition. Overall, their unique properties and neuromorphic information processing capabilities make nanowire networks promising candidates for emerging applications in cognitive devices in particular, at the edge.",project-academic
10.23919/FRUCT49677.2020.9210988,2020-09-01,p,IEEE,a deep learning approach for low latency packet loss concealment of audio signals in networked music performance applications," Networked Music Performance (NMP) is envisioned as a potential game changer among Internet applications: it aims at revolutionizing the traditional concept of musical interaction by enabling remote musicians to interact and perform together through a telecommunication network. Ensuring realistic conditions for music performance, however, constitutes a significant engineering challenge due to extremely strict requirements in terms of audio quality and, most importantly, network delay. To minimize the end-to-end delay experienced by the musicians, typical implementations of NMP applications use uncompressed, bidirectional audio streams and leverage UDP as transport protocol. Being connectionless and unreliable, audio packets transmitted via UDP which become lost in transit are not retransmitted and thus cause glitches in the receiver audio playout. This article describes a technique for predicting lost packet content in real-time using a deep learning approach. The ability of concealing errors in real time can help mitigate audio impairments caused by packet losses, thus improving the quality of audio playout in realworld scenarios.",project-academic
,2020-02-25,a,,migration networks applications of network analysis to macroscale migration patterns," An emerging area of research is the study of macroscale migration patterns as a network of nodes that represent places (e.g., countries, cities, and rural areas) and edges that encode migration ties that connect those places. In this chapter, we first review advances in the study of migration networks and recent work that has employed network analysis to examine such networks at different geographical scales. In our discussion, we focus in particular on global scale migration networks. We then propose ways to leverage network analysis in concert with digital technologies and online geolocated data to examine the structure and dynamics of migration networks. The implementation of such approaches for studying migration networks faces many challenges, including ethical ones, methodological ones, socio-technological ones (e.g., data availability and reuse), and research reproducibility. We detail these challenges, and we then consider possible ways of linking digital geolocated data to administrative and survey data as a way of harnessing new technologies to construct increasingly realistic migration networks (e.g., using multiplex networks). We also briefly discuss new methods (e.g., multilayer network analysis) in network analysis and adjacent fields (e.g., machine learning) that can help advance understanding of macroscale patterns of migration.",project-academic
10.33987/VSED.1(69).2019.189-198,2019-03-26,a,Одеський національний економічний університет,корпоративна культура як складова конкурентних переваг підприємства," The article investigates the role of organizational culture in the competitive advantages formation process of an enterprise. The main scientific works on the essence of organizational culture and its role in the competitive advantages formation process are analyzed. The main points of view relatively of corporate culture impact on the enterprise’s competitiveness are presented. It has been determined that a competitive advantage is a locking of a strong competitive position of an enterprise and determines the nature of its competitive strategy. It is proved that an organization ability is realized through two components, which are carriers of formalized and non-formalized knowledge, namely: process execution technologies and personnel competencies. The authors’ opinion is highlighted about that non-formalized knowledge reflects of corporate culture quality, namely, the spirituality of business, and, consequently, makes impossible to copy it by competitors. It has been determined that informal norms represent a deep level of corporate culture, where values are perceived automatically in the subconscious as true, irreplaceable, like those that do not need to be legitimized and cannot be copied. It is proved that an influence factors, which characterize of an enterprise’s corporate culture state, are: leadership style; the level of power centralization; an effectiveness level of motivation systems; professional training for career advancement managers; managerial competence level; degree of workers involvement in decision making and leader image. On the basis of experience studying of Odessa city leading enterprises, it is proved that the key to their competitive advantages has become implementation the system of corporate culture strategic principles. The measures system of cultural adaptation is justified, namely: ensuring of information transparency and timeliness; awareness of an importance of cultural and moral values by employees; the growth of management role and the chief executive officer (leader) personally, who initiates and heads organizational changes; involvement of employees in management decisions; formation of a rotation promotion system, lifelong learning, career advancement; formation of a positive image (reputation) encouraging creative, innovative activity",project-academic
10.1016/J.TRB.2018.06.012,2019-05-01,a,Pergamon,short term travel behavior prediction with gps land use and point of interest data," Abstract None None In everyday travel, U.S. commuters will each spend 38 h a year stuck in traffic and waste over $800 in fuel (TTI, 2015). Yet, despite this statistic, the regular commute of drivers is often predictable, leading many federal projects to aim at alleviating congestion through traveler information and intelligent transportation systems (e.g., INFLO, Queue WARN, CACC, EnableATIS, ATIS2.0). Short-term destination prediction is a developing field of research that can improve these approaches through real-traveler information, such as route, traffic incidence, and congestion levels. The short-term destination prediction problem consists of capturing vehicle Global Positioning System (GPS) traces and learning from historic locations and trajectories to predict a vehicle's destination. Drivers have predictable trip destinations that can be estimated through probabilistic modeling of past trips. To study these concepts, a database of GPS driving traces (260 participants for 70 days) was collected. To model the user's trip purpose in the prediction algorithm, a new data source was explored: point of interest (POI)/land use data. An open source land use/POI dataset is merged with the GPS dataset. The resulting database includes over 20,000 trips with travel characteristics and land use/POI data. From land use/POI data and travel patterns, trip purpose was calculated with machine learning methods. To take advantage of this data source, a new prediction model structure was developed that uses trip purpose when it is available and that falls back on traditional spatial temporal Markov models when it is not. For the first time, there is an understanding of “why” a trip is taken (not just “where” and “when”), allowing the use of “why” in the prediction model. This paper explores the baseline model followed by the inclusion of trip purpose. First, a baseline tiered time origin model was developed using the Markov Chain approach. This modelling structure allows for a short training period of current modeling techniques. The other major advantage to this structure is it allows for easy implementation of the trip purpose module. Then, a machine learning technique derived the trip purpose on 5-, 15- and 30-trip learning sets, followed by results organized by purpose, time, and origin. The machine learning technique does not require future land use data and is feasible for applicable use. This model is the first to use trip purpose to make a short-term destination prediction in pseudo real-time. Results show improved accuracy and speed over the current start-of-trip destination prediction models.",project-academic
10.3390/S120607548,2012-06-07,a,Molecular Diversity Preservation International,a neural networks based hybrid routing protocol for wireless mesh networks," The networking infrastructure of wireless mesh networks (WMNs) is decentralized and relatively simple, but they can display reliable functioning performance while having good redundancy. WMNs provide Internet access for fixed and mobile wireless devices. Both in urban and rural areas they provide users with high-bandwidth networks over a specific coverage area. The main problems affecting these networks are changes in network topology and link quality. In order to provide regular functioning, the routing protocol has the main influence in WMN implementations. In this paper we suggest a new routing protocol for WMN, based on good results of a proactive and reactive routing protocol, and for that reason it can be classified as a hybrid routing protocol. The proposed solution should avoid flooding and creating the new routing metric. We suggest the use of artificial logic—i.e., neural networks (NNs). This protocol is based on mobile agent technologies controlled by a Hopfield neural network. In addition to this, our new routing metric is based on multicriteria optimization in order to minimize delay and blocking probability (rejected packets or their retransmission). The routing protocol observes real network parameters and real network environments. As a result of artificial logic intelligence, the proposed routing protocol should maximize usage of network resources and optimize network performance.",project-academic
10.1016/J.COMCOM.2017.08.005,2017-11-01,a,Elsevier,a reinforcement learning based link quality estimation strategy for rpl and its impact on topology management," Abstract None None Over the last few years, standardisation efforts are consolidating the role of the Routing Protocol for Low-Power and Lossy Networks (RPL) as the standard routing protocol for IPv6-based Wireless Sensor Networks (WSNs). Although many core functionalities are well defined, others are left implementation dependent. Among them, the definition of an efficient link-quality estimation (LQE) strategy is of paramount importance, as it influences significantly both the quality of the selected network routes and nodes’ energy consumption. In this paper, we present RL-Probe, a novel strategy for link quality monitoring in RPL, which accurately measures link quality with minimal overhead and energy waste. To achieve this goal, RL-Probe leverages both synchronous and asynchronous monitoring schemes to maintain up-to-date information on link quality and to promptly react to sudden topology changes, e.g. due to mobility. Our solution relies on a reinforcement learning model to drive the monitoring procedures in order to minimise the overhead caused by active probing operations. The performance of the proposed solution is assessed by means of simulations and real experiments. Results demonstrated that RL-Probe helps in effectively improving packet loss rates, allowing nodes to promptly react to link quality variations as well as to link failures due to node mobility.",project-academic
,2017-01-04,,,unmanned aerial vehicle transport airborne robot cargo express delivery device and implementation method," The invention discloses an unmanned aerial vehicle transport airborne robot cargo express delivery device and an implementation method. By making full use of modern network, artificial intelligence and modern aerial vehicle technology, a cargo express delivery system network operation platform receives network transaction orders of customers, cargoes and parcels, particularly fast food delivery, can be fast delivered to homes and handed over to the customers within the shortest time, the problem of 'last one-meter distance' of logistic express delivery is really solved, elder populations, residence populations, high-rise building office and living populations can be more effectively served by aerial vehicle transport, airborne delivery, robot operation, fast direct arrival, easiness, convenience and artificial intelligent delivery modes and programs, whenever and wherever possible consumption demands of people in outdoor places and even in the driving process can be met, and the fashion psychology of young people pursuing high-technology content and following trend is met.",project-academic
10.1109/IC4ME2.2018.8465630,2018-02-01,p,IEEE,a system design for license plate recognition by using edge detection and convolution neural network," Intelligent Transport System (ITS) shows significant contributions in smart system applications .Automatic License Plate Recognition(LPR) system is a fascinating component of ITS that can be used in many real life applications such as surveillance, traffic flow monitoring, tracking stolen car, parking lot maintenance. This paper focuses a system design for LPR implementation of Bangladeshi License Plate. To detect the plate from the car image is the first stage of the system. The detection consists of the preprocessing, edge detection, morphological dilation and filtering region properties of the input image. Secondly, shape of the plate is verified by using robust distance to borders (DtBs) method. Thirdly, extrema points are used to correct horizontal tilt. Then, to extract the plates object, character segmentation is executed based on region properties and morphological operation. Finally, the extracted characters are recognized through automatic feature extraction with Convolution Neural Networks. The simulation results illustrate that the accuracy is quiet remarkable.",project-academic
10.1016/J.ESWA.2020.113251,2020-07-01,a,Pergamon,integrating complex event processing and machine learning an intelligent architecture for detecting iot security attacks," Abstract None None The Internet of Things (IoT) is growing globally at a fast pace: people now find themselves surrounded by a variety of IoT devices such as smartphones and wearables in their everyday lives. Additionally, smart environments, such as smart healthcare systems, smart industries and smart cities, benefit from sensors and actuators interconnected through the IoT. However, the increase in IoT devices has brought with it the challenge of promptly detecting and combating the cybersecurity attacks and threats that target them, including malware, privacy breaches and denial of service attacks, among others. To tackle this challenge, this paper proposes an intelligent architecture that integrates Complex Event Processing (CEP) technology and the Machine Learning (ML) paradigm in order to detect different types of IoT security attacks in real time. In particular, such an architecture is capable of easily managing event patterns whose conditions depend on values obtained by ML algorithms. Additionally, a model-driven graphical tool for security attack pattern definition and automatic code generation is provided, hiding all the complexity derived from implementation details from domain experts. The proposed architecture has been applied in the case of a healthcare IoT network to validate its ability to detect attacks made by malicious devices. The results obtained demonstrate that this architecture satisfactorily fulfils its objectives.",project-academic
10.1504/IJBIDM.2016.081604,2017-01-01,a,Inderscience Publishers (IEL),data driven techniques for mass appraisals applications to the residential market of the city of bari italy," The need for evaluation models capable of returning 'slender' and reliable mass appraisals of properties belonging to different market segments has been made mandatory by the events that are covering the global real estate finance, because of the emergence of non-performing loans in the banks' balance sheets. In Italy, the non-performing loans have been estimated by the Italian Banking Association equal to about 300 billion euro in 2014. In the present paper, three approaches of data-driven techniques hedonic price model, artificial neural networks and evolutionary polynomial regression have been applied to a sample of residential apartments recently sold in a district of the city of Bari Italy, in order to test the respective performance for mass appraisals. The models obtained by the implementation of the three procedures have been compared in terms of statistical accuracy, empirical compliance of the results and complexity of the functional relationships.",project-academic
10.1016/J.PROCS.2019.04.090,2019-01-01,a,Elsevier,deep neural network method of recognizing the critical situations for transport systems by video images," Abstract None None The deep neural network method of recognizing critical situations for transport systems according to video frames from the intelligent vehicles cameras is offered, that is effective in terms of accuracy and high-speed performance. Unlike the known solutions for the objects and normal or critical situations detection and recognition, it uses the classification with the subsequent reinforcement on the basis of several video stream frames and with the automatic annotation algorithm. The adapted architectures of neural networks are offered: the dual network to identify drivers and passengers according to the face image, the network with independent recurrent layers to classify situations according to the video fragment. The scheme of the intellectual distributed city system of transport safety using the cameras and on-board computers united in a single network is offered. Software modules in Python are developed and natural experiments are made. The possibility of the offered algorithms and programs in UGV or in the driver assistant systems implementation is shown with the illustrating examples in real-time.",project-academic
10.1016/J.EJOR.2010.01.026,2010-09-16,a,North-Holland,a travel demand management strategy the downtown space reservation system," In this paper, a Travel Demand Management strategy known as the Downtown Space Reservation System (DSRS) is introduced. The purpose of this system is to facilitate the mitigation of traffic congestion in a cordon-based downtown area by requiring people who want to drive into this area to make reservations in advance. An integer programming formulation is provided to obtain the optimal mix of vehicles and trips that are characterized by a series of factors such as vehicle occupancy, departure time, and trip length with an objective of maximizing total system throughput and revenue. Based upon the optimal solution, an ""intelligent"" module is built using artificial neural networks that enables the transportation authority to make decisions in real time on whether to accept an incoming request. An example is provided that demonstrates that the solution of the ""intelligent"" module resembles the optimal solution with an acceptable error rate. Finally, implementation issues of the DSRS are addressed.",project-academic
10.1068/A32140,2000-08-01,a,SAGE Publications,agent based models and individualism is the world agent based," Agent-based models (ABMs) are an increasingly popular tool in the social sciences. This trend seems likely to continue, so that they will become widely used in geography and in urban and regional planning. We present an overview of examples of these models in the life sciences, economics, planning, sociology, and archaeology. We conclude that ABMs strongly tend towards an individualist view of the social world. This point is reinforced by closer consideration of particular examples. This discussion pays attention to the inadequacy of an individualist model of society with reference to debates in social theory. We argue that because models are closed representations of an open world it is important that institutions and other social structures be explicitly included, or that their omission be explained. A tentative explanation for the bias of ABMs is offered, based on an examination of early research in artificial intelligence and distributed artificial intelligence from which disciplines the approach is derived. Some implications of these findings are discussed. We indicate some useful research directions which are beginning to tackle the individualism issue directly. We further note that the underlying assumptions of ABMs are often hidden in the implementation details. We conclude that such models must be subject to critical examination of their assumptions, and that model builders should engage with social theory if the approach is to realise its full potential.",project-academic
10.1007/978-3-030-38181-3_6,2020-01-01,a,"Springer, Cham",secure blockchain based traffic load balancing using edge computing and reinforcement learning," Congestion represents one of the major problems in constantly growing cities, and the expansion and modernization of the traffic system are a major priority to all government infrastructures. In effect, more efficient traffic represents a more efficient economy. Solving the issue of congestion by increasing the number of roads is not always the most cost-effective solution as it represents massive changes in city infrastructures that have been present for decades. Urban planning shapes the environment around us but fails to address specifically the future traffic clogging. Our research tackles the problem of traffic congestion by proposing a system for vehicle detection, identification, and count, allied with reinforcement learning for traffic congestion anticipation and prediction. The need for a real-time and efficient system led us to push the research and development onto an Edge Computing platform using IoT and secure transactions using Hyperledger Fabric blockchain. Blockchain intervenes as a security protocol in the proposed system. The native form of Internet of Things does not include security protocols which are important to a large scale implementation. Ensuring the security of transactions and permanent access control is the main reason why blockchain is rooted in our system architecture. Our project aims to reduce the traffic load on roads experiencing significant congestion, and improve overall city traffic system without costly investment into new communication infrastructures and city planning.",project-academic
10.1109/ICMLA.2012.108,2012-12-12,p,IEEE,enhanced multiagent multi objective reinforcement learning for urban traffic light control," Traffic light control is one of the major problems in urban areas. This is due to the increasing number of vehicles and the high dynamics of the traffic network. Ordinary methods for traffic light control cause high rate of accidents, waste in time, and affect the environment negatively due to the high rates of fuel consumption. In this paper, we develop an enhanced version of our multiagent multi-objective traffic light control system that is based on a Reinforcement Learning (RL) approach. As a testbed framework for our traffic light controller, we use the open source Green Light District (GLD) vehicle traffic simulator. We analyze and fix some implementation problems in GLD that emerged when applying a more realistic continuous time acceleration model. We propose a new cooperation method between the neighboring traffic light agent controllers using specific learning and exploration rates. Our enhanced traffic light controller minimizes the trip time in major arteries and increases safety in residential areas. In addition, our traffic light controller satisfies green waves for platoons traveling in major arteries and considers as well the traffic environmental impact by keeping the vehicles speeds within the desirable thresholds for lowest fuel consumption. In order to evaluate the enhancements and new methods proposed in this paper, we have added new performance indices to GLD.",project-academic
10.1073/PNAS.1811501115,2018-12-07,a,National Academy of Sciences,approaching the adiabatic timescale with machine learning," The control and manipulation of quantum systems without excitation are challenging, due to the complexities in fully modeling such systems accurately and the difficulties in controlling these inherently fragile systems experimentally. For example, while protocols to decompress Bose–Einstein condensates (BECs) faster than the adiabatic timescale (without excitation or loss) have been well developed theoretically, experimental implementations of these protocols have yet to reach speeds faster than the adiabatic timescale. In this work, we experimentally demonstrate an alternative approach based on a machine-learning algorithm which makes progress toward this goal. The algorithm is given control of the coupled decompression and transport of a metastable helium condensate, with its performance determined after each experimental iteration by measuring the excitations of the resultant BEC. After each iteration the algorithm adjusts its internal model of the system to create an improved control output for the next iteration. Given sufficient control over the decompression, the algorithm converges to a solution that sets the current speed record in relation to the adiabatic timescale, beating out other experimental realizations based on theoretical approaches. This method presents a feasible approach for implementing fast-state preparations or transformations in other quantum systems, without requiring a solution to a theoretical model of the system. Implications for fundamental physics and cooling are discussed.",project-academic
10.1109/COLCACI.2019.8781798,2019-06-05,p,IEEE,video processing inside embedded devices using ssd mobilenet to count mobility actors," The actual number of surveillance cameras and the different methods for counting vehicles originate the question: What is the best place to process video flows? This work performs the implementation of a counting system for mobility actors like cars, pedestrians, motorcycles, bicycles, buses, and trucks in the context of an Edge computing application using deep learning. However, the implementation of Deep Neural Networks for Object Detection in low-capacity embedded devices make it difficult to perform tasks that require high processing or must be carried out in real time. To solve this problem this study presents the analysis and implementation of different techniques based on the use of an additional hardware element as is the case of a Vision Processing Unit (VPU) in combination with methods that affect the resolution, bit rate, and time of video processing. For this purpose we consider the Mobilenet-SSD model with two approaches: a pre-trained model with known data sets and a trained model with images from our specific scenarios. The use of SSD-Mobilenet’s model generates different results in terms of accuracy and time of video processing in the system. Results show that the use of an embedded device in combination with a VPU and video processing techniques reach 18.62 Frames per Second (FPS). Thus, video processing time is slightly superior (5.63 minutes) for a video of 5 minutes. Recall and precision values of 91% and 97% are reported in the best case (class car) for the vehicle counting system.",project-academic
10.26794/2220-6469-2019-13-3-81-88,2019-12-03,a,Федеральное государственное образовательное бюджетное учреждение высшего профессионального образования «Финансовый университет при Правительстве Российской Федерации» (Финансовый университет),проблемы и перспективы реализации концепции умный город в россии на примере москвы," The article is devoted to a comprehensive analysis of the possibility of realisation of the concept “Smart city” in Russia. The purpose of the work is to study the problems and prospects of digitalisation of urban management on the example of such a project in Moscow. The research methodology includes neo-institutional and system approaches, as well as analysis of statistics and social research data. The paper deals equally with theoretical and applied aspects of the presented topic. Initially, the author revealed the content of the concept, the possibility of using technologies of “smart” city for purposes of territorial management and socio-economic development. The author showed that the central role for the successful realisation of the concept is played by human capital and readiness of society for changes, infrastructure base, sustainable interaction of state structures, scientific and technical institutions and business. From now on, the author investigated the possibility of digitalisation of urban management in Moscow, such aspects as experience and preparation for implementation of programs of “smart” city, the level of digital infrastructure development, the necessary institutional base for innovation, the level of human capital, the characteristics of the technological solutions are analyzed. Despite the existence of the institutional and infrastructure base for the implementation of the concept “Smart city”, for the city administration in the future, it will be a relevant activity to build sufficient communication with the population to study the readiness of the latter to the large-scale introduction of artificial intelligence technologies and data collection. It is also necessary to develop the urban digital infrastructure. For “Smart city” projects are considered to be megaprojects, there will also be a demand for competent project management, the interaction between political institutions, scientific and technology organisations. The results of this study can be applied to the analysis of various problems related to the development and implementation of digital technologies.",project-academic
10.5194/AMT-10-695-2017,2016-07-13,a,Copernicus GmbH,evaluation of machine learning algorithms for classification of primary biological aerosol using a new uv lif spectrometer," Abstract. Characterisation of bioaerosols has important implications within environment and public health sectors. Recent developments in ultraviolet light-induced fluorescence (UV-LIF) detectors such as the Wideband Integrated Bioaerosol Spectrometer (WIBS) and the newly introduced Multiparameter Bioaerosol Spectrometer (MBS) have allowed for the real-time collection of fluorescence, size and morphology measurements for the purpose of discriminating between bacteria, fungal spores and pollen. None This new generation of instruments has enabled ever larger data sets to be compiled with the aim of studying more complex environments. In real world data sets, particularly those from an urban environment, the population may be dominated by non-biological fluorescent interferents, bringing into question the accuracy of measurements of quantities such as concentrations. It is therefore imperative that we validate the performance of different algorithms which can be used for the task of classification. None For unsupervised learning we tested hierarchical agglomerative clustering with various different linkages. For supervised learning, 11 methods were tested, including decision trees, ensemble methods (random forests, gradient boosting and AdaBoost), two implementations for support vector machines (libsvm and liblinear) and Gaussian methods (Gaussian naive Bayesian, quadratic and linear discriminant analysis, the k-nearest neighbours algorithm and artificial neural networks). None The methods were applied to two different data sets produced using the new MBS, which provides multichannel UV-LIF fluorescence signatures for single airborne biological particles. The first data set contained mixed PSLs and the second contained a variety of laboratory-generated aerosol. None Clustering in general performs slightly worse than the supervised learning methods, correctly classifying, at best, only 67. 6 and 91. 1 % for the two data sets respectively. For supervised learning the gradient boosting algorithm was found to be the most effective, on average correctly classifying 82. 8 and 98. 27 % of the testing data, respectively, across the two data sets. None A possible alternative to gradient boosting is neural networks. We do however note that this method requires much more user input than the other methods, and we suggest that further research should be conducted using this method, especially using parallelised hardware such as the GPU, which would allow for larger networks to be trained, which could possibly yield better results. None We also saw that some methods, such as clustering, failed to utilise the additional shape information provided by the instrument, whilst for others, such as the decision trees, ensemble methods and neural networks, improved performance could be attained with the inclusion of such information.",project-academic
,1995-01-01,a,National Research Council,modeling schedule deviations of buses using automatic vehicle location data and artificial neural networks," The establishment of the Advanced Public Transportation Systems program has encouraged bus transit operators to experiment with implementing automatic vehicle-location systems for real-time monitoring and supervision of operations. While the focus has primarily been on the implementation of technologies, such as automatic vehicle-location systems, it is necessary to experiment and develop advanced performance analysis and evaluation procedures that can assist in schedule planning and real-time service-control tasks. One potentially useful and effective approach to these tasks is system behavior modeling. In this study this method is used to model schedule behavior of buses on a route using schedule-deviation information. The primary objective of this study is to investigate the application of artificial neural networks, which have been shown to hold promise then applied to nonlinear dynamic system-modeling problems, for developing schedule behavior models. Models are developed using the schedule-deviation information obtained from Tidewater Regional Transit's automatic vehicle-location system. The time-series analysis approach is adopted for the development of schedule behavior models at the route level. The results of a case study are encouraging and demonstrate the usefulness of artificial neural network techniques, especially the Jordan networks and the Elman networks, for modeling schedule deviations of buses on a route.",project-academic
10.1016/J.SCS.2018.06.004,2018-08-01,a,Elsevier,provably secure pseudo identity based device authentication for smart cities environment," Abstract None None IoT based smart city idea is evolving with an intention of improving the quality of citizens’ life by practicing information and communication technologies. Smart city concept is believed to be possible by integrating the evolving technologies such as internet of things (IoT), automation and machine learning in which IoT holds the key role. Authentication has already been identified as a foremost security concern around the IoT, as millions of small devices go online and begin to share their data. Authentication in IoT environment mainly considers the three types of communications: IoT device – IoT device, IoT device – IoT gateway, and IoT gateway – mobile client. In this paper, we design a protocol to address the authentication process between IoT gateway and mobile client. The proposed protocol's security is analyzed formally and informally to demonstrate robustness. Performance analysis of the proposed protocol has also paid adequate attention to show the probability of implementation in real time applications.",project-academic
10.2478/JEE-2015-0050,2015-11-01,a,Sciendo,a robust approach for acoustic noise suppression in speech using anfis," The authors of this article deals with the implementation of a combination of techniques of the fuzzy system and artificial intelligence in the application area of non-linear noise and interference suppression. This structure used is called an Adaptive Neuro Fuzzy Inference System (ANFIS). This system finds practical use mainly in audio telephone (mobile) communication in a noisy environment (transport, production halls, sports matches, etc). Experimental methods based on the two-input adaptive noise cancellation concept was clearly outlined. Within the experiments carried out, the authors created, based on the ANFIS structure, a comprehensive system for adaptive suppression of unwanted background interference that occurs in audio communication and degrades the audio signal. The system designed has been tested on real voice signals. This article presents the investigation and comparison amongst three distinct approaches to noise cancellation in speech; they are LMS (least mean squares) and RLS (recursive least squares) adaptive filtering and ANFIS. A careful review of literatures indicated the importance of non-linear adaptive algorithms over linear ones in noise cancellation. It was concluded that the ANFIS approach had the overall best performance as it efficiently cancelled noise even in highly noise-degraded speech. Results were drawn from the successful experimentation, subjective-based tests were used to analyse their comparative performance while objective tests were used to validate them. Implementation of algorithms was experimentally carried out in Matlab to justify the claims and determine their relative performances.",project-academic
10.1002/SAM.11404,2019-01-16,a,,how to host a data competition statistical advice for design and analysis of a data competition," Data competitions rely on real-time leaderboards to rank competitor entries and stimulate algorithm improvement. While such competitions have become quite popular and prevalent, particularly in supervised learning formats, their implementations by the host are highly variable. Without careful planning, a supervised learning competition is vulnerable to overfitting, where the winning solutions are so closely tuned to the particular set of provided data that they cannot generalize to the underlying problem of interest to the host. This paper outlines some important considerations for strategically designing relevant and informative data sets to maximize the learning outcome from hosting a competition based on our experience. It also describes a post-competition analysis that enables robust and efficient assessment of the strengths and weaknesses of solutions from different competitors, as well as greater understanding of the regions of the input space that are well-solved. The post-competition analysis, which complements the leaderboard, uses exploratory data analysis and generalized linear models (GLMs). The GLMs not only expand the range of results we can explore, they also provide more detailed analysis of individual sub-questions including similarities and differences between algorithms across different types of scenarios, universally easy or hard regions of the input space, and different learning objectives. When coupled with a strategically planned data generation approach, the methods provide richer and more informative summaries to enhance the interpretation of results beyond just the rankings on the leaderboard. The methods are illustrated with a recently completed competition to evaluate algorithms capable of detecting, identifying, and locating radioactive materials in an urban environment.",project-academic
,2012-06-19,,,rendering global light transport in real time using machine learning," Some implementations disclosed herein provide techniques and arrangements to render global light transport in real-time or near real-time. For example, in a pre-computation stage, a first computing device may render points of surfaces (e.g., using multiple light bounces and the like). Attributes for each of the points may be determined. A plurality of machine learning algorithms may be trained using particular attributes from the attributes. For example, a first machine learning algorithm may be trained using a first portion of the attributes and a second machine learning algorithm may be trained using a second portion of the attributes. The trained machine learning algorithms may be used by a second computing device to render components (e.g., diffuse and specular components) of indirect shading in real-time.",project-academic
10.1038/S41699-019-0114-6,2019-08-21,a,Nature Publishing Group,graphene ferroelectric transistors as complementary synapses for supervised learning in spiking neural network," The hardware design of supervised learning (SL) in spiking neural network (SNN) prefers 3-terminal memristive synapses, where the third terminal is used to impose supervise signals. In this work we address this demand by fabricating graphene transistor gated through organic ferroelectrics of polyvinylidene fluoride. Through gate tuning not only is the nonvolatile and continuous change of graphene channel conductance demonstrated, but also the transition between electron-dominated and hole-dominated transport. By exploiting the adjustable bipolar characteristic, the graphene–ferroelectric transistor can be electrically reconfigured as potentiative or depressive synapse and in this way complementary synapses are realized. The complementary synapse and neuron circuit is then constructed to execute remote supervise method (ReSuMe) of SNN, and quick convergence to successful learning is found through network-level simulation when applying to a SL task of classifying 3 × 3-pixel images. The presented design of graphene–ferroelectric transistor-based complementary synapses and quantitative simulation may indicate a potential approach to hardware implementation of SL in SNN.",project-academic
10.3390/SU11113173,2019-06-05,a,Multidisciplinary Digital Publishing Institute,monitoring potato waste in food manufacturing using image processing and internet of things approach," Approximately one-third of the food produced globally is spoiled or wasted in the food supply chain (FSC). Essentially, it is lost before it even reaches the end consumer. Conventional methods of food waste tracking relying on paper-based logs to collect and analyse the data are costly, laborious, and time-consuming. Hence, an automated and real-time system based on the Internet of Things (IoT) concepts is proposed to measure the overall amount of waste as well as the reasons for waste generation in real-time within the potato processing industry, by using modern image processing and load cell technologies. The images captured through a specially positioned camera are processed to identify the damaged, unusable potatoes, and a digital load cell is used to measure their weight. Subsequently, a deep learning architecture, specifically the Convolutional Neural Network (CNN), is utilised to determine a potential reason for the potato waste generation. An accuracy of 99.79% was achieved using a small set of samples during the training test. We were successful enough to achieve a training accuracy of 94.06%, a validation accuracy of 85%, and a test accuracy of 83.3% after parameter tuning. This still represents a significant improvement over manual monitoring and extraction of waste within a potato processing line. In addition, the real-time data generated by this system help actors in the production, transportation, and processing of potatoes to determine various causes of waste generation and aid in the implementation of corrective actions.",project-academic
10.1016/J.BIOS.2020.112412,2020-10-01,a,Elsevier,artificial intelligence biosensors challenges and prospects," Artificial intelligence (AI) and wearable sensors are two essential fields to realize the goal of tailoring the best precision medicine treatment for individual patients. Integration of these two fields enables better acquisition of patient data and improved design of wearable sensors for monitoring the wearers' health, fitness and their surroundings. Currently, as the Internet of Things (IoT), big data and big health move from concept to implementation, AI-biosensors with appropriate technical characteristics are facing new opportunities and challenges. In this paper, the most advanced progress made in the key phases for future wearable and implantable technology from biosensing, wearable biosensing to AI-biosensing is summarized. Without a doubt, material innovation, biorecognition element, signal acquisition and transportation, data processing and intelligence decision system are the most important parts, which are the main focus of the discussion. The challenges and opportunities of AI-biosensors moving forward toward future medicine devices are also discussed.",project-academic
10.1109/ICMLA.2015.65,2015-12-01,p,IEEE,a neural network based handover management strategy for heterogeneous networks," One of the key challenges for improvement of quality of services (QoS) in Heterogeneous wireless networks is the design of Vertical Handover (VHO) Management strategy. VHO is required to guide the decision for a mobile terminal (MT) to handoff between different types of networks. This is an essential task to cope with various multimedia services QoS settings. In this paper, we present a machine learning scheme based on Neural Network for calls vertical handover in heterogeneous networks. The Neural Network Based Handover Management Scheme (NNBHMS) of this paper aims toward achieving seamless connectivity and Always Best Connected (ABC) call status for group mobility over a set of heterogeneous networks. The proposed scheme evaluates and creates relationships between different decision criteria related to heterogeneous networks conditions, terminal capabilities, application requirements, and user preferences. Afterward, the estimates of each attribute are forwarded to neural network to select the optimal access network. The proposed scheme is applied for vertical handover Management in heterogeneous networks offering both real time services (voice over IP services), and data Services (packet data traffic). Through the implementation of neural networks based machine learning approach, the proposed research scheme allows solving the complexity of the handover decision process resulting from the multitude dimensions of the decision criteria and the dynamicity of many of its components. The performance results evaluated through simulation show that the use of the a neural network based machine learning scheme to carry out the Handover process can enhance the QoS perceived by both types of voice and data service while fulfilling to great extent the user preference.",project-academic
10.1109/IDAACS.2017.8095241,2017-09-01,p,,logics based application integration for interdisciplinary scientific investigations," The approach to the integration of applications based on mathematical logic and artificial intelligence for World Data Center (WDC) interdisciplinary scientific investigations has been developed in the article. Key elements of the approach are a multilevel system architecture, a formal logical system, implementation based on the interaction of intelligent agents. The formal logical system has been proposed. The inference method and solution tree recovery mechanism have been elaborated. The implementation of application integration for interdisciplinary scientific research has been based on a stack of modern protocols, enabling communication of business processes over the transport layer of the OSI model. Application integration is also based on coordinated models of business processes, for which an integrated set of business applications have been designed and realized.",project-academic
10.1109/ISC251055.2020.9239014,2020-09-28,p,IEEE,environmental parameter modelling for thermal rating calculations of power cables in urban areas using machine learning and big data," Vienna, among other cities, has the aspirations of transforming into a smart city of tomorrow. To enable this transformation and to cope with the challenges for a smarter and more resilient grid, the grid operators have to improve the utilization of new and existing high voltage power cable systems in urban areas. Anyhow, due to the missing of international experience and the lack of suitable standards, intensive research and validation has to be done before. Therefore, for more than 2 years now, load scenarios have been carried out on a 400-kVcable test setup under realistic circumstances. The measured data, which consists of more than 90 different sensors, which are tracking temperatures, but also environmental parameters, are not only used to validate calculation results, but furthermore, to develop a digital twin of the cable system. One key part to do so is the modelling of the environment. Therefore, a statistical evaluation of the correlation of different environmental parameters on the cable temperature has been carried out. The resulting parameters of influence have then been modelled using supervised learning methods. In a final step, the models have been tested with empirical data from the test setup. In this paper, the investigation and modelling of environmental parameters as well as the evaluation of the obtained models will be discussed. Furthermore, the benefits of the derived models for thermal rating calculation are indicated by comparison to the analytical method given by the IEC 60287 on an example. Accurate mathematical models with real time calculation capabilities and prediction accuracies in the range of 98 to 99% could be derived on the basis of relatively small data sets and implementation of external data.",project-academic
10.1016/J.ESD.2018.12.002,2019-02-01,a,ELSEVIER SCIENCE BV,identifying urban geometric types as energy performance patterns," Abstract None None This paper aims to find the impact of geometric parameters on the energy performance of buildings, to using them to identify types regarding major geometric characteristics of a target area. Conventional approaches to control energy efficiency of buildings mainly focus on materials and capacity of insulation, but rarely consider urban and building geometries. By examining energy impacts on urban blocks by urban geometric forms, this paper seeks to identify urban geometric types and energy patterns on urban blocks. To achieve the aims of this study, this paper follows two steps: First, significant indicators for analyzing energy performance are identified in urban geometries; second, the types that capture urban geometry of a real city are categorized. As a result, as a reference for urban planning and design, the paper identifies 13 types that represent the characteristics of urban geometries regarding energy performance. The geometric indicators are carefully measured and their significance to energy performance of buildings is examined through regression analysis. According to these indicators, the 13 types are categorized using a hierarchical clustering algorithm, a machine learning method. Additionally, the 13 types are discussed for implementation as references in urban planning and design, particularly in block planning for a city.",project-academic
,2020-07-07,a,,drive a digital network oracle for cooperative intelligent transportation systems," In a world where Artificial Intelligence revolutionizes inference, prediction and decision-making tasks, Digital Twins emerge as game-changing tools. A case in point is the development and optimization of Cooperative Intelligent Transportation Systems (C-ITSs): a confluence of cyber-physical digital infrastructure and (semi)automated mobility. Herein we introduce Digital Twin for self-dRiving Intelligent VEhicles (DRIVE). The developed framework tackles shortcomings of traditional vehicular and network simulators. It provides a flexible, modular, and scalable implementation to ensure large-scale, city-wide experimentation with a moderate computational cost. The defining feature of our Digital Twin is a unique architecture allowing for submission of sequential queries, to which the Digital Twin provides instantaneous responses with the ""state of the world"", and hence is an Oracle. With such bidirectional interaction with external intelligent agents and realistic mobility traces, DRIVE provides the environment for development, training and optimization of Machine Learning based C-ITS solutions.",project-academic
10.1109/ISCC50000.2020.9219683,2020-04-23,p,IEEE,drive a digital network oracle for cooperative intelligent transportation systems," In a world where Artificial Intelligence revolutionizes inference, prediction and decision-making tasks, Digital Twins emerge as game-changing tools. A case in point is the development and optimization of Cooperative Intelligent Transportation Systems (C-ITSs): a confluence of cyber-physical digital infrastructure and (semi)automated mobility. Herein we introduce Digital Twin for self-dRiving Intelligent VEhicles (DRIVE). The developed framework tackles shortcomings of traditional vehicular and network simulators. It provides a flexible, modular, and scalable implementation to ensure large-scale, city-wide experimentation with a moderate computational cost. The defining feature of our Digital Twin is a unique architecture allowing for submission of sequential queries, to which the Digital Twin provides instantaneous responses with the ""state of the world"", and hence is an Oracle. With such bidirectional interaction with external intelligent agents and realistic mobility traces, DRIVE provides the environment for development, training and optimization of Machine Learning based C-ITS solutions.",project-academic
10.1016/J.TRC.2021.103191,2021-07-01,a,Pergamon,markov game modeling of cyclist pedestrian interactions in shared spaces a multi agent adversarial inverse reinforcement learning approach," Abstract None None Understanding and modeling road user dynamics and their microscopic interaction behaviour at shared space facilities are curial for several applications including safety and performance evaluations. Despite the multi-agent nature of road user interactions, the majority of previous studies modeled their interactions as a single-agent modeling framework, i.e., considering the other interaction agents as part of the passive environment. However, this assumption is unrealistic and could limit the model's accuracy and transferability in non-stationary road user environments. This study proposes a novel Multi-Agent Adversarial Inverse Reinforcement Learning approach (MA-AIRL) to model and simulate road user interactions at shared space facilities. Unlike the traditional game-theoretic framework that models multi-agent systems as a single time-step payoff, the proposed approach is based on Markov Games (MG) which models road users' sequential decisions concurrently. Moreover, the proposed model can handle bounded rationality agents, e.g., limited information access, through the implementation of the Logistic Stochastic Best Response Equilibrium (LSBRE) solution concept. The proposed algorithm recovers road users' multi-agent reward functions using adversarial deep neural network discriminators and estimates their optimal policies using Multi-agent Actor-Critic with Kronecker factors (MACK) deep reinforcement learning. Data from three shared space locations in Vancouver, BC and New York City, New York are used in this study. The model's performance is compared to a baseline single-agent Gaussian Process Inverse Reinforcement Learning (GPIRL). The results show that the multi-agent modeling framework led to a significantly more accurate prediction of road users’ behaviour and their evasive action mechanisms. Moreover, the recovered reward functions based on the single-agent modeling approach failed to capture the equilibrium solution concept similar to the multi-agent approach.",project-academic
10.1016/J.EGYPRO.2017.03.271,2017-03-01,a,Elsevier,predicting large scale fine grain energy consumption," Today a large volume of energy-related data have been continuously collected. Extracting actionable knowledge from such data is a multi-step process that opens up a variety of interesting and novel research issues across two domains: energy and computer science. The computer science aim is to provide energy scientists with cutting-edge and scalable engines to effectively support them in their daily research activities. This paper presents SPEC, a scalable and distributed predictor of fine grain energy consumption in buildings. SPEC exploits a data stream methodology analysis over a sliding time window to train a prediction model tailored to each building. The building model is then exploited to predict the upcoming energy consumption at a time instant in the near future. SPEC currently integrates the artificial neural networks technique and the random forest regression algorithm. The SPEC methodology exploits the computational advantages of distributed computing frameworks as the current implementation runs on Spark. As a case study, real data of thermal energy consumption collected in a major city have been exploited to preliminarily assess the SPEC accuracy. The initial results are promising and represent a first step towards predicting fine grain energy consumption over a sliding time window.",project-academic
,2020-12-15,a,,active learning for deep gaussian process surrogates," Deep Gaussian processes (DGPs) are increasingly popular as predictive models in machine learning (ML) for their non-stationary flexibility and ability to cope with abrupt regime changes in training data. Here we explore DGPs as surrogates for computer simulation experiments whose response surfaces exhibit similar characteristics. In particular, we transport a DGP's automatic warping of the input space and full uncertainty quantification (UQ), via a novel elliptical slice sampling (ESS) Bayesian posterior inferential scheme, through to active learning (AL) strategies that distribute runs non-uniformly in the input space -- something an ordinary (stationary) GP could not do. Building up the design sequentially in this way allows smaller training sets, limiting both expensive evaluation of the simulator code and mitigating cubic costs of DGP inference. When training data sizes are kept small through careful acquisition, and with parsimonious layout of latent layers, the framework can be both effective and computationally tractable. Our methods are illustrated on simulation data and two real computer experiments of varying input dimensionality. We provide an open source implementation in the ""deepgp"" package on CRAN.",project-academic
10.1109/MIPRO.2016.7522210,2016-05-01,p,,the challenge of cellular cooperative its services based on 5g communications technology," We live at a time when the automotive industry is going through a technological revolution with the development of vehicles changing into autonomous moving objects having the properties of artificial intelligence. Additionally, cellular communications networks introduce new technologies and concepts: SDN (Software-defined networking), NFV (Network Functions Virtualization). These advanced software-defined communications networks virtualize network functions, allowing a new way of configuration, control and management. Increasing the speed and automation are key requirements to support the most demanding services such as mobile payment of contextual services, as well as the introduction of new “Machine” users. SDN also assists in the implementation of new infrastructure for the dynamic services that are based on the concepts of IoT, Big Data and Everything-as-a-Service. Using NFV enables the Internet of Things services that provide a new way of connecting people, processes, data and devices. It is precisely these requirements that are essential for the introduction of C-ITS systems, i.e., the integration of passengers, drivers, vehicles and transport infrastructure, as well as information, statistics, predictive traffic analytics, all this in real-time. Due to this trend of development of cellular communications networks in the next 5G communications networks, automotive and ITS traffic systems are becoming the most important market for business expansion of telecom operators. Such revolutionary technological changes, together with the integration of various industries entails a series of challenges. The aim of this paper is to define important information, challenges and opportunities for the telecom industry to provide mobility for people and goods.",project-academic
10.1109/TSP.2015.7296288,2015-10-12,p,IEEE,adaptive noise suppression in voice communication using a neuro fuzzy inference system," This paper describes the implementation of a combination of techniques of the fuzzy system and artificial intelligence in the application area of non-linear suppression of noise and interference. The structure used is called ANFIS (Adaptive Neuro Fuzzy Inference System). This system finds practical use mainly in audio telephone (mobile) communication in a noisy environment (transport, production halls, sports matches, etc.). Within the experiments carried out, the authors created, based on the ANFIS structure, a comprehensive system for the adaptive suppression of unwanted background interference that occurs in audio communication and which degrades the audio signal. The system designed has been tested on real voice signals. Noise cancellation performance of the algorithms has been compared by means of SSNR (Segmental Signal to Noise Ratio) and DTW (Dynamic Time Warping). Also processing durations of the algorithms are determined for evaluating the possibility of real time implementation. The results imply that a system using ANFIS has better experimental results than conventional systems built on adaptive algorithms of the LMS (Least Mean Squares) and RLS (Recursive Least Squares) families.",project-academic
10.17863/CAM.45198,2019-10-22,a,ASCE,developing a dynamic digital twin at building and city levels a case study of the west cambridge campus," A Digital Twin (DT) refers to a digital replica of physical assets, processes and systems. DTs integrate artificial intelligence, machine learning and data analytics to create living digital simulation models that are able to learn and update from multiple sources, and to represent and predict the current and future conditions of physical counterparts. However, the current activities related to DTs are still at an early stage with respect to buildings and other infrastructure assets from an architectural and engineering/construction point of view. Less attention has been paid to the operation & maintenance (O&M) phase, which is the longest time span in the asset life cycle. A systematic and clear architecture verified with practical use cases for constructing a DT would be the foremost step for effective operation and maintenance of buildings and cities. According to current research about multi-tier architectures, this paper presents a system architecture for DTs which is specifically designed at both the building and city levels. Based on this architecture, a DT demonstrator of the West Cambridge site of the University of Cambridge was developed, which integrates heterogeneous data sources, supports effective data querying and analysing, supports decision-making processes in O&M management, and further bridges the gap between human relationships with buildings/cities. This paper aims at going through the whole process of developing DTs in building and city levels from the technical perspective and sharing lessons learnt and challenges involved in developing DTs in real practices. Through developing this DT demonstrator, the results provide a clear roadmap and present particular DT research efforts for asset management practitioners, policymakers and researchers to promote the implementation and development of DT at the building and city levels.",project-academic
10.1109/TITS.2018.2882439,2019-12-01,a,IEEE,convolutional neural networks for on street parking space detection in urban networks," The purpose of this paper is the development of data science models for the detection of empty on-street parking spaces in urban road networks based on data provided by in-vehicle cameras that are already, or soon will be, a standard vehicle equipment. A rolling spatial interval is used to identify the existence of an on-street parking space and the properties of empty spaces are used to determine the availability of the parking space. Convolutional neural networks are developed, trained, and evaluated with the use of images from a moving vehicle camera. The images are preprocessed and converted to suitable matrices, so that only the useful information for the empty on-street parking space detection problem is preserved. The optimized convolutional networks, in terms of structural and learning parameters, provided predictions for the detection of empty on-street parking spaces with approximately 90% average accuracy. The proposed model performs better than the relatively complex SVMs, which supports its appropriateness as an approach. Finally, the implementation of a framework, which integrates the developed models to produce meaningful parking information for drivers in real time, is discussed.",project-academic
10.1016/J.RESOURPOL.2018.12.013,2019-03-01,a,Pergamon,state of the art about metaheuristics and artificial neural networks applied to open pit mining," Abstract None None In search of the best way to extract and take advantage of minerals, highlighting that these are part of the most important raw materials for the economic development of today's society, the following bibliographical review is presented, which covers the main metaheuristic techniques highlighted in the optimization of mining processes and artificial neural networks (ANN), fundamental for predicting them; With this, the applications and results of these methods can be observed in mining unit operations such as: blasting, transport and mineral processing, which until now have models or techniques for their prediction that are not applicable in all mining complexes, as well as metaheuristics for three fundamental variables of open-pit planning, which are: geological uncertainty, cutting law and extraction programming. In addition to this, the proposals that have been developed in the global optimization of mining complexes are shown. There is also a brief description of how these techniques were applied to optimize the operations and previous variables of the mining planning, as well as their implementation in several mines around the world. The information shown shows available alternatives for the implementation of new actions in favor of reaching the objectives for real and hypothetical sites, yielding satisfactory results. Finally, the conclusions of this work are presented.",project-academic
10.1109/ACCESS.2018.2878799,2018-10-30,a,IEEE,travel time prediction based on gated recurrent unit method and data fusion," Travel time prediction is the basis for the implementation of advanced traveler information systems and advanced transport management systems in intelligent transportation systems. Many studies have shown that the fusion of multi-source data can achieve higher precision prediction of travel time than the travel time prediction based on single source data. In recent years, with the continuous development of China’s expressways, traffic detectors such as dedicated short-range communications (DSRC) and remote transportation microwave sensors (RTMS) have been installed on both sides of the road, which provides a basis for the prediction of travel time by fusing multi-source data. At the same times, the deep learning methods show good performance in prediction. So, this paper uses the deep learning algorithm to realize the travel time prediction based on DSRC data and the RTMS data. First, the travel times are, respectively, extracted based on the DSRC data and the RTMS data. Then, both travel time values are input into the gated recurrent unit (GRU) model to obtain travel time prediction results based on multi-source data. Finally, based on the data of the Jinggangao Highway, the accuracy of the algorithm is verified and compared with the traditional data fusion method. The results show that the GRU model can achieve better accuracy of travel time prediction with data fusion.",project-academic
10.1109/TVT.2021.3084829,2021-07-08,a,Institute of Electrical and Electronics Engineers (IEEE),guest editorialintroduction to the special section on vehicular networks in the era of 6g end edge cloud orchestrated intelligence," The articles in this special section focus on vehicular networks in the era of 6G mobile communication. With the growth of the vehicle population, vehicular networks play a key role in building safe, efficient, and intelligent transport systems and has been attracting a lot of attention from both academic and industrial communities around the world. The rise of autonomous driving technology and the prosperity of mobile applications, e.g., real-time video analytic, image-aided navigation, natural language processing, and etc, have brought tremendous pressure on current vehicular networks, e.g., high bandwidth, ultra-low latency, high reliability, high security, powerful computation capability, and massive connections. It is necessary to continue to develop vehicular networks by combining the latest research intends in other fields to meet quickly rising communication and computation demands. The upcoming 6G technology, which provides Holographic and Artificial Intelligence (AI) enabled communications, together with the increasing implementation of artificial intelligence in mobile devices, will lead to a new research trend to end-edge-cloud orchestrated computing with intelligence. It means that, not only the intelligent communication protocols, but also the intelligent computing resource management and machine learning algorithms among the mobile vehicles, the edge and the cloud, should be redesigned to support the development of vehicular networks.",project-academic
10.1145/3077839.3081670,2017-05-16,p,ACM,big data analysis for an electric vehicle charging infrastructure using open data and software," This paper describes a big data analysis strategy for electric vehicle charging infrastructure, mainly built upon open data sets and open software components. The data acquisition module periodically retrieves the real-time status information of each charger from the public data portal, while the downloaded XML files are parsed to extract fields of interest. At this stage, we present the distribution of charging facilities in Jeju City based on our own map viewer implementation, the city-wide dynamics of the number of chargers in operation based on MySQL queries, and the visualization of regional occupancy rates based on the R GISTools library. After combining a variety of statistical and machine learning techniques to understand the demand pattern of electric vehicle charging, we will integrate renewable energy to charging-intensive power grids as much as possible.",project-academic
10.5194/ISPRS-ARCHIVES-XLII-2-W13-103-2019,2019-06-04,a,Copernicus GmbH,classification of aerial point clouds with deep learning," Abstract. Due to their usefulness in various implementations, such as energy evaluation, visibility analysis, emergency response, 3D cadastre, urban planning, change detection, navigation, etc., 3D city models have gained importance over the last decades. Point clouds are one of the primary data sources for the generation of realistic city models. Beside model-driven approaches, 3D building models can be directly produced from classified aerial point clouds. This paper presents an ongoing research for 3D building reconstruction based on the classification of aerial point clouds without given ancillary data (e.g. footprints, etc.). The work includes a deep learning approach based on specific geometric features extracted from the point cloud. The methodology was tested on the ISPRS 3D Semantic Labeling Contest (Vaihingen and Toronto point clouds) showing promising results, although partly affected by the low density and lack of points on the building facades for the available clouds.",project-academic
10.1007/978-3-319-75928-9_57,2018-03-15,a,"Springer, Cham",an edge computer based driver monitoring system for assisting safety driving," Driver Monitoring System (DMS) is a promising IoT application in Intelligent Transport Systems (ITS) research field. DMS assists car drivers by monitoring their driving activities, sensing incidents to cause possible dangers, and alerting the drivers to prevent accidents. We aim to realize a new DMS that is inexpensive and highly effective. This paper proposes a method for detecting any incidents based on machine learning. The proposed method firstly configures a detector by training in-car environment data and driver’s vital signs gathered from multiple sensors. Then, the detector is embedded in a self-contained edge computer for monitoring a driver in a car. The device is always connected to the information communication network by radio waves. Those data obtained by monitoring are stored in the cloud server. The server learns and analyzes the stored data using processing such as machine learning. As a result, we acquire knowledge leading to safe driving. The edge computer uses these knowledge to process the sensor data in real time, observe the driver, sense the danger, and call attention. These mechanisms prevent occurrence of troubles such as traffic accidents. The paper describes the proposed system overview, implementation method, and initial evaluations.",project-academic
10.1016/J.AST.2019.04.048,2019-07-01,a,Elsevier Masson,real time estimation of impaired aircraft flight envelope using feedforward neural networks," Abstract None None Extensive research in recent years has focused on developing flight envelope estimation methods to improve current loss of control prevention and recovery systems. Such methods are practically efficient only if they are able to evaluate in real time the new flight envelope of damaged aircraft based on the altered dynamics. Due to nonlinear dynamics of aircraft, common approaches to estimate the entire flight envelope of high-fidelity models are numerically intensive and their real time implementation is computationally impossible. So current methods are based on reduced complexity models or flight envelopes are determined locally. This paper presents a novel method to estimate the global flight envelope of impaired aircraft in real-time for any unknown failure degree. In the proposed method, first, numerous flight envelopes are evaluated using a high fidelity model at various failure degrees and different flight conditions and prepared as training data. Then multiple feedforward neural networks are trained offline by a Bayesian regularization backpropagation algorithm. Finally, the trained networks are used to estimate flight envelopes in real time. The method is applied to rudder and aileron failure cases of the NASA Generic Transport Model. Results show that the estimated flight envelopes are good approximations of the high fidelity global flight envelopes.",project-academic
10.1016/J.ANNEMERGMED.2004.02.037,2004-09-01,a,Elsevier,effects of neural network feedback to physicians on admit discharge decision for emergency department patients with chest pain," Abstract None Study objective None Neural networks can risk-stratify emergency department (ED) patients with potential acute coronary syndromes with a high specificity, potentially facilitating ED discharge of patients to home. We hypothesized that the use of ""real-time"" neural networks would decrease the admission rate for ED chest pain patients. None Methods None We conducted a before-and-after trial. Consecutive ED patients with chest pain were evaluated before and after implementation of a neural network in an urban university ED. Data included 40 variables used in neural networks for acute myocardial infarction and acute coronary syndrome. Data were obtained in real time, and neural network outputs were provided to the treating physician while patients were in the ED. On hospital discharge, attending physicians received feedback, including neural network output, their initial clinical impression, cardiac test results, and final diagnosis. The main outcome was the actual admit/discharge decision made before versus after the implementation of the neural network. None Results None Before implementation, 4,492 patients were enrolled; after implementation, 432 patients were enrolled. Implementation of the neural network did not decrease the hospital admission rate (before: 62.7% [95% confidence interval (CI) 61.3% to 64.1%] versus after: 66.6% [95% CI 62.2% to 71.0%]). Additionally, the ICU admission rates were not different (11.4% [95% CI 10.5% to 12.3%] versus 9.3% [95% CI 6.6% to 12.0%]). Physician query found that the neural network changed management in only 2 cases ( None Conclusion None The use of real-time neural network feedback did not influence the admission decision for ED patients with chest pain, most likely because the neural network output was delayed until the return of cardiac markers, and the disposition decision had already been made by that time.",project-academic
10.1016/J.APENERGY.2020.114680,2020-04-15,a,Elsevier,household standards and socio economic aspects as a factor determining energy consumption in the city," Abstract None None Political or economic attempts to mitigate climate change by increasing fossil fuel prices lead to and an increase in energy poverty, i.e., social effects. The ideal solution would be to combine modernisation activities in terms of energy use in cities with sustainable strategies and redevelopment policies. The article's purpose is to estimate the potential for reducing energy consumption depending on socioeconomic factors (household standard and its location in the city) based on built-in scenarios and searching for the optimal way of conducting development policy at the local level. This assumption enables the implementation of the European Union climate policy. To this aim, modelling based on real and estimated data on the diversity of energy consumption in the structure of a medium-sized city in Europe (Zielona Gora) carried out. While creating scenarios, there used a modelling method based on radial artificial neural networks, which map the input set into the output set by matching many individual approximating functions to setpoints. This approach works well for data whose geolocation is in the city quarters. As a result of the simulations, the minimum and maximum achievable energy saving potential for low-intensity buildings in the quarters was estimated, taking into account the possibilities of investing in renewable energy by individual households. The observations included in the article may be relevant to other regions that are interested in reducing the energy consumption of buildings and pollution emissions from the cities. This is particularly important for the regions of Europe that benefit from the financial support of the European Union (including local development programmes based on financing European priority axes for economic development).",project-academic
10.1007/978-3-030-55187-2_51,2020-09-03,p,"Springer, Cham",big data in smart infrastructure," Infrastructure is becoming smarter due to technical advances such as the Internet of Things (IoT) that enables a greater interconnectivity between assets and Artificial Intelligence (AI) to enhance the decision making task; this paper proposes a Big Data Framework in Smart Infrastructure such as Airports, Stations Intelligent Transport Systems and Buildings. The interconnection of infrastructure systems using Local Area Networks, Wi-Fi, Radio or Mobile Networks is not enough to optimize and expand Infrastructure services and functionality; Big Data integration, management and analytics will play a key role in the next Smart Infrastructure stages when Infrastructure will learn and adapt to users. However, the inconvenient truth behind the Big Data hosted in the cloud is the intrinsically associated Cybersecurity threat and risk. This paper provides the practical application of Digital as a Service (DaaS) with considerations and recommendations that cover the implementation of Big Data in Smart Infrastructure with some practical examples of real Infrastructure projects: Buildings, Airport, Stations and Intelligent Transport Systems. In addition, flagship projects have also to be delivered on time, budget and quality within a Health and Safety environment in order to be successful where the understanding of requirements and solutions is key and an Agile approach has some benefits to the traditional project management delivery.",project-academic
10.1109/SYSOSE.2017.7994953,2017-06-18,p,IEEE,autonomous decision making for a driver less car," Autonomous driving has been a hot topic with companies like Google, Uber, and Tesla because of the complexity of the problem, seemingly endless applications, and capital gain. The technology's brain child is DARPA's autonomous urban challenge from over a decade ago. Few companies have had some success in applying algorithms to commercial cars. These algorithms range from classical control approaches to Deep Learning. In this paper, we will use Deep Learning techniques and the Tensor flow framework with the goal of navigating a driverless car through an urban environment. The novelty in this system is the use of Deep Learning vs. traditional methods of real-time autonomous operation as well as the application of the Tensorflow framework itself. This paper provides an implementation of AlexNet's Deep Learning model for identifying driving indicators, how to implement them in a real system, and any unforeseen drawbacks to these techniques and how these are minimized and overcome.",project-academic
10.1360/TB-2019-0739,2020-04-01,a,Science China Press,progresses and outlook in neuromorphic devices," In this information era with bursting data, there are three fundamental problems limiting the further development of computing power: The gradually slowing down of Moore’s law, the rapidly increasing energy consumption while scaling down, and the restriction of data transfer between separated memory and processor known as “Von Neumann bottleneck”. To deal with massive data and make all things in our daily life interconnected, it is imperative to develop new generation of computing paradigms, for example, neuromorphic computing. Inspired by the human brain, neuromorphic computing has drawn extensive attention in recent years due to its high parallelism, low power consumption and in-memory computation, especially when the tide of artificial intelligence is sweeping across the globe now. Novel neuromorphic devices are key to the construction of neuromorphic computing systems, providing an efficient implementation of artificial neural network on chip. Besides, by simulating the behavior of biological neurons and synapses at the physical level, neuromorphic devices can enable a brand new computing method, which is thought to be an essential and promising way to build a brain-like system on chip. This paper focuses on the current research progresses and future research trends of neuromorphic devices. It summarizes the physical mechanisms of neuromorphic devices, according to which they can be divided into ion-migration device, material phase change device, electron-migration device, magnetic device and so on. It further details the inner dynamics happened in a single device by characterizing the changing process in transmission electron microscope. This paper also explains how these neuromorphic devices can be used for computing, where memristor, as the missing fourth basic circuit component, is used as a theoretical support. Through making up a crossbar structure, these devices can directly calculate vector-matrix multiplication by Ohm’s law and Kirchhoff’s law. It is an elegant way that can accelerate artificial neural network in parallel by computing in memory. In addition, exploiting the intrinsic dynamics in the devices can realize complex and interesting functions of biological neural networks, such as long-term plasticity, short-term plasticity and spike timing dependent plasticity. For example, a synaptic transistor based on two-dimensional materials WSe2 can simulate the biologically transport process of calcium ion, leading precisely regulated coexistence of long-term and short-term memory. Remarkably, a significant advance is realizing heterosynaptic plasticity, a general mode in brain’s cortical network, in a single multi-terminal device, which is crucial for biologically plausible supervised learning on hardware. Furthermore, artificial neurons composed of memristors and capacitors have some natural advantages compared to traditional circuits, containing rich dynamics for neuromorphic computing, such as stochasticity, adaptive threshold and chaotic oscillation. Recently, materials with new mechanisms, like ferroelectric materials, are also being explored to realize complex neuron functions, contributing to less power and smaller area. By devices and algorithms co-design, it is time to explore the next generation of neural network, transforming current processing unit to a more efficient and intelligent brain-inspired style. As a conclusion, the outstanding challenges and trends in the field of neuromorphic devices are discussed in this paper. These researches are promising for building a neuromorphic computer in the future, which will be complementary to classical computer and outperform in many tasks.",project-academic
10.1016/J.PROCS.2019.09.442,2019-01-01,a,Elsevier,unmanned aerial vehicle in the machine learning environment," Abstract None None Unmanned Aerial Vehicles and machine learning have started gaining attentions of academic and industrial research. The Unmanned Aerial Vehicles have extended the freedom to operate and monitor the activities from remote locations. This study retrieved and synthesized research on the use of Unmanned Aerial Vehicles along with machine learning and its algorithms in different areas and regions. The objective was to synthesize the scope and importance of machine learning models in enhancing Unmanned Aerial Vehicles capabilities, solutions to problems, and numerous application areas. None The machine learning implementation has reduced numbers of challenges to Unmanned Aerial Vehicles besides enhancing the capabilities and opening the door to the different sectors. The Unmanned Aerial Vehicles and machine learning association has resulted in fast and reliable outputs. The combination of Unmanned Aerial Vehicles and machine learning has helped in real time monitoring, data collection and processing, and prediction in the computer/wireless networks, smart cities, military, agriculture, and mining.",project-academic
10.3390/S20226593,2020-11-18,a,MDPI AG,vital signs prediction and early warning score calculation based on continuous monitoring of hospitalised patients using wearable technology," In this prospective, interventional, international study, we investigate continuous monitoring of hospitalised patients’ vital signs using wearable technology as a basis for real-time early warning scores (EWS) estimation and vital signs time-series prediction. The collected continuous monitored vital signs are heart rate, blood pressure, respiration rate, and oxygen saturation of a heterogeneous patient population hospitalised in cardiology, postsurgical, and dialysis wards. Two aspects are elaborated in this study. The first is the high-rate (every minute) estimation of the statistical values (e.g., minimum and mean) of the vital signs components of the EWS for one-minute segments in contrast with the conventional routine of 2 to 3 times per day. The second aspect explores the use of a hybrid machine learning algorithm of kNN-LS-SVM for predicting future values of monitored vital signs. It is demonstrated that a real-time implementation of EWS in clinical practice is possible. Furthermore, we showed a promising prediction performance of vital signs compared to the most recent state of the art of a boosted approach of LSTM. The reported mean absolute percentage errors of predicting one-hour averaged heart rate are 4.1, 4.5, and 5% for the upcoming one, two, and three hours respectively for cardiology patients. The obtained results in this study show the potential of using wearable technology to continuously monitor the vital signs of hospitalised patients as the real-time estimation of EWS in addition to a reliable prediction of the future values of these vital signs is presented. Ultimately, both approaches of high-rate EWS computation and vital signs time-series prediction is promising to provide efficient cost-utility, ease of mobility and portability, streaming analytics, and early warning for vital signs deterioration.",project-academic
10.1016/J.TRPRO.2016.11.102,2016-01-01,a,Elsevier,application of data mining techniques for traffic density estimation and prediction," Abstract None None Advanced Traveller Information Systems (ATIS) is one of the functional areas of Intelligent Transportation Systems (ITS) and it aims at providing real time traffic information to the travellers for making better travel decisions. This information would be most effective if provided to travellers during or before the start of their trip. Therefore, accurate prediction models are required in ATIS for conveying reliable information about the future state of traffic. Different methods used for the prediction of traffic parameters include historic averaging, regression analysis, Kalman filtering, time series analysis, machine learning, etc. The objective of this research is to explore the use of automated sensor data and data driven techniques for traffic state prediction under Indian traffic conditions. Travel time and traffic density (as an indicator of congestion) are used commonly to inform users about the state of a traffic system. However, these two parameters are spatial in nature and direct measurement from field is difficult. Therefore, estimation of these parameters from location based data is a challenge in many of the ITS implementations. The present study addresses the problem of estimation of traffic density with the help of location based sensors which are capable of measuring parameters such as volume and Time Mean Speed (TMS). Machine learning techniques namely, k-Nearest Neighbour (k-NN) and Artificial Neural Network (ANN) are selected as the estimation and prediction tool in this study, based on acceptable performance of the same in earlier studies.",project-academic
10.1109/ISPACS.2011.6146215,2011-12-01,p,IEEE,learning in vehicular dynamic spectrum access networks opportunities and challenges," In this paper, we discuss various ways of incorporating machine learning techniques in the area of dynamic spectrum access, especially in vehicle communications, how learning can benefit multiple applications in vehicle communications, and the specific requirements on the implementation in this field. In particular, we describe an architecture for optimizing the overall performance of vehicular dynamic spectrum access (VDSA) networks. Due to the high level of mobility for vehicles operating under highway conditions, coupled with spatially variant spectrum allocation across a large geographical region, we envision that future vehicular communications will employ a form of dynamic spectrum access (DSA) in order to facilitate wireless transmissions between vehicles and with roadside infrastructure. Moreover, the VDSA concept will be realized via a combination of software-defined radio (SDR) technology, spectral occupancy databases, and machine learning techniques for enabling network automation. A vehicular networking scenario possesses the potential to be substantially different relative to a generic mobile scenario with respect to the particular type of mobility involved, the predictable trajectories of the vehicular traffic, and the overall scale of the network range. Consequently, this architecture is designed to enable VDSA in a more flexible wireless spectrum environment by leveraging the cognitive radio concept and existing wireless spectrum databases actively being developed while simultaneously being compatible with current spectrum regulations.",project-academic
10.1016/J.SCITOTENV.2014.08.060,2015-02-01,a,Elsevier,a general procedure to generate models for urban environmental noise pollution using feature selection and machine learning methods," The prediction of environmental noise in urban environments requires the solution of a complex and non-linear problem, since there are complex relationships among the multitude of variables involved in the characterization and modelling of environmental noise and environmental-noise magnitudes. Moreover, the inclusion of the great spatial heterogeneity characteristic of urban environments seems to be essential in order to achieve an accurate environmental-noise prediction in cities. This problem is addressed in this paper, where a procedure based on feature-selection techniques and machine-learning regression methods is proposed and applied to this environmental problem. Three machine-learning regression methods, which are considered very robust in solving non-linear problems, are used to estimate the energy-equivalent sound-pressure level descriptor (LAeq). These three methods are: (i) multilayer perceptron (MLP), (ii) sequential minimal optimisation (SMO), and (iii) Gaussian processes for regression (GPR). In addition, because of the high number of input variables involved in environmental-noise modelling and estimation in urban environments, which make LAeq prediction models quite complex and costly in terms of time and resources for application to real situations, three different techniques are used to approach feature selection or data reduction. The feature-selection techniques used are: (i) correlation-based feature-subset selection (CFS), (ii) wrapper for feature-subset selection (WFS), and the data reduction technique is principal-component analysis (PCA). The subsequent analysis leads to a proposal of different schemes, depending on the needs regarding data collection and accuracy. The use of WFS as the feature-selection technique with the implementation of SMO or GPR as regression algorithm provides the best LAeq estimation (R2 = 0.94 and mean absolute error (MAE) = 1.14–1.16 dB(A)).",project-academic
10.1109/MRA.2002.1035212,2002-11-07,a,IEEE,working with robots in disasters," The Robot World Cup Initiative (RoboCup) is an international research and education initiative. It was started in order to foster artificial intelligence search. RoboCupRescue's domain is search and rescue operations in urban disasters. The RoboCup Rescue league consists of two projects: the simulation project and the robotics and infrastructure project. A multi-agent-based approach to disaster simulation provides many research themes and supports rescue operations in real situations. Simulation Project, not only agent implementation, but also the evaluation of the social agents' performance, the architecture of the distribution system, and the quality of communications, etc. The following features are important in this project to promote the research and provide verification methods.",project-academic
10.1109/ITSC.2019.8916903,2019-10-01,p,IEEE,advanced self improving ramp metering algorithm based on multi agent deep reinforcement learning," We proposed a novel ramp metering algorithm embedding multi-agent deep reinforcement learning (DRL) techniques, based on the data of loop detectors. A multi-agent DRL framework is adopted to generate proper ramp metering scheme for each ramp meter in real time to improve the operation efficiency of urban freeway with less investment. A simulation platform is developed to simplify the implementation and training of the algorithm. A set of simulation experiments – encompassing both single and multi-ramp scenarios with various traffic demand profiles – are conducted. Comparing with the state-of-the-practice ramp metering methods, the simulation results demonstrate that the proposed DRL-based algorithm outperforms in a comprehensive evaluation index considering mainstream speed at bottleneck and queue size on ramp. The method presents robustness, scalability, and the capability of further improvement by online learning during implementation.",project-academic
10.1007/978-981-15-4218-3_7,2021-01-01,a,"Springer, Singapore",addressing security and computation challenges in iot using machine learning," The Internet of things (IoT) is widely used to implement different applications like smart home, smart health care, smart city, and smart farming system. The development of large smart devices/sensors enables smart technologies making it possible to implement the smart application in real time. The IoT system has security challenges like authentication, data privacy, access control, and intrusion detection system. Similarly, the computation of the sensed information from the environment is a challenging task. The computation must perform using distributed or decentralized architecture to overcome the centralized system difficulty. In a distributed/decentralized system when multiple nodes participate in a computational process, there is the risk of mutual consensus problem, malicious node detection, or data modification attacks. In this paper, the authors have identified machine learning as a solution to address some of the existing security and computational challenges. The paper also explained the implementation platform available for the integration of IoT with machine learning.",project-academic
,2018-06-20,a,,quick and plenty achieving low delay and high rate in 802 11ac edge networks," We consider transport layer approaches for achieving high rate, low delay communication over edge paths where the bottleneck is an 802.11ac WLAN. We first show that by regulating send rate so as to maintain a target aggregation level it is possible to realise high rate, low delay communication over 802.11ac WLANs. We then address two important practical issues arising in production networks, namely that (i) many client devices are non-rooted mobile handsets/tablets and (ii) the bottleneck may lie in the backhaul rather than the WLAN, or indeed vary between the two over time. We show that both these issues can be resolved by use of simple and robust machine learning techniques. We present a prototype transport layer implementation of our low delay rate allocation approach and use this to evaluate performance under real radio conditions.",project-academic
,2009-01-01,a,,integration of signal control and transit signal priority optimization in coordinated network using genetic algorithms and artificial neural networks," Many transit agencies are currently considering implementing priority systems providing buses with temporary green signal extensions and early green recalls at urban signalized intersections. While many studies have evaluated the potential for bus delay reductions and negative traffic impacts, most of these studies focused on implementations within traditional fixed-time traffic signal control systems. Only a few studies have addressed the problems of integrating transit signal priority (TSP) in coordinated real-time traffic signal control systems. A particular problem in this case is the uncertainty of predicting transit movements when considering the variability of dwell times at service stops. This study presents the development of a real-time traffic signal controller integrating traffic signal timing optimization and TSP control using a Genetic Algorithm (GA) and an Artificial Neural Networks (ANN) modeling. The GA is used to find near-optimal signal timings while the ANN is used to predict the travel of buses along transit routes. Evaluation results show that the proposed integrated controller can reduce transit delay, improve schedule adherence and service reliability, and benefit non-transit traffic compared to traditional fixed-time control with and without TSP, as well as a real-time GA-based control approach without TSP.",project-academic
10.3390/S19071676,2019-04-08,a,Multidisciplinary Digital Publishing Institute,a sensor network approach for violence detection in smart cities using deep learning," Citizen safety in modern urban environments is an important aspect of life quality. Implementation of a smart city approach to video surveillance depends heavily on the capability of gathering and processing huge amounts of live urban data. Analyzing data from high bandwidth surveillance video streams provided by large size distributed sensor networks is particularly challenging. We propose here an efficient method for automatic violent behavior detection designed for video sensor networks. Known solutions to real-time violence detection are not suitable for implementation in a resource-constrained environment due to the high processing power requirements. Our algorithm achieves real-time processing on a Raspberry PI-embedded architecture. To ensure separation of temporal and spatial information processing we employ a computationally effective cascaded approach. It consists of a deep neural network followed by a time domain classifier. In contrast with current approaches, the deep neural network input is fed exclusively with motion vector features extracted directly from the MPEG encoded video stream. As proven by results, we achieve state-of-the-art performance, while running on a low computational resources embedded architecture.",project-academic
10.3390/S21041254,2021-02-10,a,Multidisciplinary Digital Publishing Institute,sensors on the move onboard camera based real time traffic alerts paving the way for cooperative roads," European road safety has improved greatly in recent decades. However, the current numbers are still far away to reach the European Commission’s road safety targets. In this context, Cooperative Intelligent Transport Systems (C-ITS) are expected to significantly improve road safety, traffic efficiency and comfort of driving, by helping the driver to make better decisions and adapt to the traffic situation. This paper puts forward two vision-based applications for traffic sign recognition (TSR) and real-time weather alerts, such as for fog-banks. These modules will support operators in road infrastructure maintenance tasks as well as drivers, giving them valuable information via C-ITS messages. Different state-of-the-art methods are analysed using both publicly available datasets (GTSB) as well as our own image databases (Ceit-TSR and Ceit-Foggy). The selected models for TSR implementation are based on Aggregated Chanel Features (ACF) and Convolutional Neural Networks (CNN) that reach more than 90% accuracy in real time. Regarding fog detection, an image feature extraction method on different colour spaces is proposed to differentiate sunny, cloudy and foggy scenes, as well as its visibility level. Both applications are already running in an onboard probe vehicle system.",project-academic
10.1186/S40713-017-0007-9,2016-12-01,a,Springer,collective thinking approach for improving leak detection systems," Water mains, especially old pipelines, are consistently threatened by the formation of leaks. Leaks inherit increased direct and indirect costs and impacts on various levels such as the economic field and the environmental level. Recently, financially capable municipalities are testing acoustic early detection systems that utilize wireless noise loggers. Noise loggers would be distributed throughout the water network to detect any anomalies in the network. Loggers provide early detection via recording and analyzing acoustic signals within the network. The city of Montreal adopted one of the leak detection projects in this domain and had reported that the main issue that hinders the installed system is false alarms. False alarms consume municipality resources and funds inefficiently. Therefore, this paper aims to present a novel approach to utilize more than one data analysis and classification technique to ameliorate the leak identification process. In this research, acoustic leak signals were analyzed using Fourier Transform, and the multiple frequency bandwidths were determined. Three models were developed to identify the state of the leak using Naive Bayes (NB), Deep Learning (DL), and Decision Tree (DT) Algorithms. Each of the developed models has an accuracy ranging between 84% to 89%. An aggregator approach was developed to cultivate the collective approaches developed into one single answer. Through aggregation, the accuracy of leak detection improved from 89% at its best to 100%. The design, implementation approach and results are displayed in this paper. Using this method helps municipalities minimize and alleviate the costs of uncertain leak verifications and efficiently allocate their resources.",project-academic
10.2478/POPETS-2019-0020,2019-04-01,p,Sciendo,mitigating location privacy attacks on mobile devices using dynamic app sandboxing," We present the design, implementation and evaluation of a system, called MATRIX, developed to protect the privacy of mobile device users from location inference and sensor side-channel attacks. MATRIX gives users control and visibility over location and sensor (e.g., Accelerometers and Gyroscopes) accesses by mobile apps. It implements a PrivoScope service that audits all location and sensor accesses by apps on the device and generates real-time notifications and graphs for visualizing these accesses; and a Synthetic Location service to enable users to provide obfuscated or synthetic location trajectories or sensor traces to apps they find useful, but do not trust with their private information. The services are designed to be extensible and easy for users, hiding all of the underlying complexity from them. MATRIX also implements a Location Provider component that generates realistic privacy-preserving synthetic identities and trajectories for users by incorporating traffic information using historical data from Google Maps Directions API, and accelerations using statistical information from user driving experiments. The random traffic patterns are generated by modeling/solving user schedule using a randomized linear program and modeling/solving for user driving behavior using a quadratic program. We extensively evaluated MATRIX using user studies, popular location-driven apps and machine learning techniques, and demonstrate that it is portable to most Android devices globally, is reliable, has low-overhead, and generates synthetic trajectories that are difficult to differentiate from real mobility trajectories by an adversary.",project-academic
,2018-08-13,a,,mitigating location privacy attacks on mobile devices using dynamic app sandboxing," We present the design, implementation and evaluation of a system, called MATRIX, developed to protect the privacy of mobile device users from location inference and sensor side-channel attacks. MATRIX gives users control and visibility over location and sensor (e.g., Accelerometers and Gyroscopes) accesses by mobile apps. It implements a PrivoScope service that audits all location and sensor accesses by apps on the device and generates real-time notifications and graphs for visualizing these accesses; and a Synthetic Location service to enable users to provide obfuscated or synthetic location trajectories or sensor traces to apps they find useful, but do not trust with their private information. The services are designed to be extensible and easy for users, hiding all of the underlying complexity from them. MATRIX also implements a Location Provider component that generates realistic privacy-preserving synthetic identities and trajectories for users by incorporating traffic information using historical data from Google Maps Directions API, and accelerations using statistical information from user driving experiments. The random traffic patterns are generated by modeling/solving user schedule using a randomized linear program and modeling/solving for user driving behavior using a quadratic program. We extensively evaluated MATRIX using user studies, popular location-driven apps and machine learning techniques, and demonstrate that it is portable to most Android devices globally, is reliable, has low-overhead, and generates synthetic trajectories that are difficult to differentiate from real mobility trajectories by an adversary.",project-academic
10.1016/J.EPSR.2017.09.003,2018-01-01,a,Elsevier,new online load forecasting system for the spanish transport system operator," Abstract None None This paper presents the implementation of a new online real-time hybrid load-forecasting model based on an autoregressive model and neural networks. This new system is currently running at the Spanish Transport System Operator (REE) and provides an hourly forecast for the current day and the next nine days timely every hour for the national system as well as 18 regions of Spain. These requirements impose a heavy computational burden that needs to be considered during the design phase. The system is developed to improve forecasting accuracy specifically on difficult days like hot, cold and special days. In order to achieve this goal, a deep analysis of the temperature series from 59 stations is made for each region and the relevant series are included individually in the model. Special days are also analyzed and a thorough classification of days is proposed for the Spanish national and regional system. The model is designed and tested with data from 2005 to 2015. The results provided for the period from December 2014 to October 2015 show how the addition of the proposed model to the TSO’s ensemble causes a 5% RMSE overall error reduction and a 15% reduction on the 59 difficult days considered in the testing period.",project-academic
10.18372/2310-5461.41.13535,2019-04-30,a,National Aviation University,метод структурного синтезу системи управління засобів водного транспорту," The article presents the results of research on the ways of development of ship management systems. The analysis of operation shows that the most promising direction of co-development of these systems is their integration, integration and creation on the basis of such an association of ""integrated bridging control systems by the ship"". Also, the research of modern control systems suggests that the concept of their construction is based on the use of modernization of existing equipment in conjunction with new modern systems. Therefore, the task of multicriteria optimization of the structure of the control system according to known characteristics of existing subsystems of complexes is actual. The analysis of scientific and engineering approaches for the development and implementation of integrated bridging control systems has revealed a number of systemic deficiencies and remarks. Therefore, the scientific task is to improve existing and develop new models and methods for analyzing and synthesizing the ship management system based on a priori estimates of the technical efficiency of component subsystems. The analysis of the methods and approaches of compromising the solution in the solution of the multicriteria optimization problem, performed in the article, revealed a well-known method of successive concessions. This method most logically and effectively takes into account both the experience of the decision maker and the possibilities of analytical methods of one-criterion optimization. Also, in the article the features of the method of structural synthesis of the system of management of means of water transport improved by the authors, which unlike the existing ones, use the intellectualized approach of determining the assignment between efficiency and cost using parametric models of fuzzy expert conclusions based on neural networks. It is argued that the realization of the results obtained in the work allows to analyze the various variants of control systems, allows to carry out a comparative analysis and to determine the best value system according to the costeffectiveness criterion.",project-academic
10.1109/EEEIC/ICPSEUROPE49358.2020.9160705,2020-06-09,p,IEEE,asdvc a self driving vehicle controller using unsupervised machine learning," ASDVC is a self-driving vehicle controller that uses unsupervised machine learning methods, namely clustering based k-means, hierarchical, Gaussian Matrix Model and self-organizing mapping to optimize the path the vehicle follows from the source to destination. The real-time optimal selection of the unsupervised machine learning based motion control algorithm could provide fast response times of under one microsecond during the lateral, longitudinal and angular motion control of the autonomous vehicle. However, it is shown that a simple selection of one of the machine learning methods may not guarantee the optimality of the results. The successful implementation of ASDVC controller in self-driving vehicles could have a significant contribution towards making mobility more reliable and sustainable for the future vehicular transportation systems.",project-academic
10.1109/ICASET.2019.8714349,2019-03-01,p,IEEE,review of security in mobile edge computing with deep learning," Mobile edge computing is an emerging concept, introduced to bring the cloud services and resources to the user proximity and exploring them at the edge of the network. This helps to meet the requirements of low latency, location awareness, and mobility support. In this paper, we discuss the related work done in mobile edge computing and how it allows the cloud application services to be hosted alongside mobile network elements and facilitates the radio information in real-time. We have also explored the various machine learning methods/algorithms used by applications and services that are hosted on MEC platform. Deep learning method is one of the machine learning technique that have been studied in this paper to incorporate security measures in Mobile edge computing environments. Various categories of threats are analyzed and studied in terms of its strengths and implementation.",project-academic
10.1088/1742-6596/1529/4/042076,2020-04-01,a,IOP Publishing,bringing intelligence to iot edge machine learning based smart city image classification using microsoft azure iot and custom vision," Object detection, identification and classification techniques have seen many variants and improvements over past two decades. Together with Internet of Things (IoT) devices, improved computational algorithms and cloud support, real-time classification with low-cost devices has already been achieved. This paper discusses the real-time object detection and classification using Microsoft Custom Vision multi-class Machine Learning (ML) model operating at the Edge of IoT network. This paper further examines the use of virtual dockers or containers at the IoT edge devices for better security and isolation by decoupling physical hardware as well that supports multiple applications and services on a single hardware. The experiments are performed using emulated and simulated IoT devices on Microsoft Azure IoT platform for real-time object classification using Custom Vision Machine Learning (ML) models run directly from the edge device. The experimental results are further discussed to validate the model accuracy and its implementation in a future Smart City surveillance environment.",project-academic
10.1109/ICC40277.2020.9148964,2020-06-07,p,IEEE,the scalability analysis of machine learning based models in road traffic flow prediction," Nowadays, traffic flow prediction, as a vital part of the Intelligent Transportation System (ITS), has attracted considerable attention from both academia and industry. Many prediction methods have been proposed and can be categorized into parametric methods and non-parametric methods. Nonparametric methods, especially Machine Learning (ML)-based methods, compared to parametric methods, need less prior knowledge about the relationship among different traffic patterns and can better fit the non-linear features of traffic data. However, we notice that, due to the complex structure, ML-models require a higher cost of implementation regarding time consumption of training and predicting. Therefore, in this paper, we evaluate not only the accuracy but also the efficiency and scalability of some state-of-the-art ML-models, which is the key to apply a prediction model into the real world. Furthermore, we design an off-line optimization method, Desensitization, to improve the scalability of a given model.",project-academic
,2004-03-05,b,,introduction to autonomous mobile robots," Mobile robots range from the Mars Pathfinder mission's teleoperated Sojourner to the cleaning robots in the Paris Metro. This text offers students and other interested readers an introduction to the fundamentals of mobile robotics, spanning the mechanical, motor, sensory, perceptual, and cognitive layers the field comprises. The text focuses on mobility itself, offering an overview of the mechanisms that allow a mobile robot to move through a real world environment to perform its tasks, including locomotion, sensing, localization, and motion planning. It synthesizes material from such fields as kinematics, control theory, signal analysis, computer vision, information theory, artificial intelligence, and probability theory. The book presents the techniques and technology that enable mobility in a series of interacting modules. Each chapter treats a different aspect of mobility, as the book moves from low-level to high-level details. It covers all aspects of mobile robotics, including software and hardware design considerations, related technologies, and algorithmic techniques.] This second edition has been revised and updated throughout, with 130 pages of new material on such topics as locomotion, perception, localization, and planning and navigation. Problem sets have been added at the end of each chapter. Bringing together all aspects of mobile robotics into one volume, Introduction to Autonomous Mobile Robots can serve as a textbook or a working tool for beginning practitioners.",project-academic
10.1016/J.TRPRO.2016.05.236,2016-01-01,a,Elsevier,traffic models for self driving connected cars," Self-driving and connected vehicles, communicating with one another (V2V technology) and with the road infrastructure (V2I technology), are a subject of extensive research nowadays and are expected to revolutionize the automotive industry in the near future. The major goal of the authors' work is to design a microscopic traffic simulation model for such vehicles, including a robust protocol for exchanging information. The question arises as to whether such communication system may efficiently improve travel quality while reducing the risk of collisions. For the purpose of their research the authors created and developed a simulation software. Their tool visualizes traffic flow for custom but simplified road maps. The transport infrastructure includes multiple junctions, optionally equipped with traffic lights, and roads with varying number of travel lanes. Each vehicle is assigned a fixed route leading to a randomly chosen destination point. Any decisions made by autonomous cars (regarding acceleration or turning maneuvers) are preceded by communication stages (retrieving necessary data, negotiations). In the paper we present fundamental concepts, assumptions and design of our model and simulation software, the authors also discuss potential issues relevant to their approach. As for the future work, the authors plan to implement their model in a large-scale agent-based traffic simulation software, Traffic Simulation Framework, so that further examination will be carried out for realistic road networks taken from the OpenStreetMap project. The authors also plan to apply machine learning techniques, so that self-driving vehicles, as well as traffic light controllers, will be able to learn how to develop the best strategy and by this way improve traffic safety and efficiency in atypical cases.",project-academic
10.1109/TCCN.2021.3078147,2021-05-07,a,Institute of Electrical and Electronics Engineers (IEEE),machine learning assisted beam alignment for mmwave systems," Beam alignment is a challenging and time-consuming process for millimeter wave (mmWave) systems, particularly as they trend towards higher carrier frequencies which require ever narrower beams. We propose a beam alignment method that is assisted by machine learning (ML), where we train ML models to predict the optimal access point (AP) and beam – or the best few candidates – for a user equipment (UE) given just its GPS coordinates, which can be fed back by the UE or estimated by the network using emerging localization techniques. We train and evaluate the models with data generated by a state-of-the-art commercial ray-tracing software in a realistic urban topology. Even with dynamic scatterers and imperfect UE coordinates, our proposed method can greatly reduce the search space (number of candidates) for finding the optimal AP and beam. For example, in a 28 GHz scenario with 64 beams, our method reduces the search space by approximately 4x for AP selection and over 10x for beam selection while achieving over 95% accuracy. We provide our dataset and models for ease of reproducing and extending our results, which suggest that UE localization coupled with suitably trained ML models can significantly speed up current beam alignment procedures.",project-academic
,2011-09-07,,,intelligent green lighting management system," The utility model relates to an intelligent green lighting management system, which comprises a monitoring center, an intelligent monitoring terminal, an intelligent server and a single lamp energy-saving controller, wherein the monitoring center comprises an upper computer with intelligent management software and is used for realizing remote monitoring management on urban public lighting facilities (five remotes, namely remote detection, remote control, remote communication, remote vision and remote regulation), on-map operation and management of an electronic map for monitoring street lamps, visual presentation on a map and convenient positioning of facility resources related to the street lamps and functions of production management, warehouse management, artificial intelligence and the like; the intelligent monitoring terminal is used for realizing measurement and acquisition of electric parameters of the street lamps and control of illumination circuits and branch circuits, is connected with the intelligent server and is communicated with the upper computer through an operator public network (GPRS None , CDMA None ); the intelligent server is connected with the intelligent monitoring terminal, is communicated with the single lamp controller through a power line carrier mode with an automatic relay function and is used for transmitting a control command and receiving single lamp state data and alarm data; and the single lamp energy-saving controller is used for receiving the command of the intelligent server, realizing control on switching, power reduction, dimming and the like of a single lamp and acquiring the single lamp electric parameters, the state data, the alarm data and the like.",project-academic
10.1109/MNET.2018.1800101,2018-11-29,a,IEEE,sedative sdn enabled deep learning architecture for network traffic control in vehicular cyber physical systems," The rapid growth in the transportation sector has led to the emergence of smart vehicles that are equipped with ICT. These modern smart vehicles are connected to the Internet to access various services such as road condition information, infotainment, and energy management. This kind of scenario can be viewed as a vehicular cyber-physical system (VCPS) where the vehicles are at the physical layer and services are at the cyber layer. However, network traffic management is the biggest issue in the modern VCPS scenario as the mismanagement of network resources can degrade the quality of service (QoS) for end users. To deal with this issue, we propose a software defined networking (SDN)-enabled approach, named SeDaTiVe, which uses deep learning architecture to control the incoming traffic in the network in the VCPS environment. The advantage of using deep learning in network traffic control is that it learns the hidden patterns in data packets and creates an optimal route based on the learned features. Moreover, a virtual-controller-based scheme for flow management using SDN in VCPS is designed for effective resource utilization. The simulation scenario comprising 1000 vehicles seeking various services in the network is considered to generate the dataset using SUMO. The data obtained from the simulation study is evaluated using NS-2, and proves that the proposed scheme effectively handles real-time incoming requests in VCPS. The results also depict the improvement in performance on various evaluation metrics like delay, throughput, packet delivery ratio, and network load by using the proposed scheme over the traditional SDN and TCP/IP protocol suite.",project-academic
10.1023/A:1008822205706,1998-05-01,a,Kluwer Academic Publishers,rough terrain autonomous mobility part 2 an active vision predictive control approach," Off-road autonomous navigation is one of the most difficult automation challenges from the point of view of constraints on mobility, speed of motion, lack of environmental structure, density of hazards, and typical lack of prior information. This paper describes an autonomous navigation software system for outdoor vehicles which includes perception, mapping, obstacle detection and avoidance, and goal seeking. It has been used on several vehicle testbeds including autonomous HMMWV‘s and planetary rover prototypes. To date, it has achieved speeds of 15 km/hr and excursions of 15 km.

We introduce algorithms for optimal processing and computational stabilization of range imagery for terrain mapping purposes. We formulate the problem of trajectory generation as one of predictive control searching trajectories expressed in command space. We also formulate the problem of goal arbitration in local autonomous mobility as an optimal control problem. We emphasize the modeling of vehicles in state space form. The resulting high fidelity models stabilize coordinated control of a high speed vehicle for both obstacle avoidance and goal seeking purposes. An intermediate predictive control layer is introduced between the typical high-level strategic or artificial intelligence layer and the typical low-level servo control layer. This layer incorporates some deliberation, and some environmental mapping as do deliberative AI planners, yet it also emphasizes the real-time aspects of the problem as do minimalist reactive architectures.",project-academic
10.1061/(ASCE)1084-0699(2005)10:3(216),2005-05-01,a,American Society of Civil Engineers,artificial neural networks for forecasting watershed runoff and stream flows," This research demonstrates an application of artificial neural networks (ANN) for watershed-runoff and stream-flow forecasts. A watershed runoff prediction model was developed to predict stormwater runoff at a gauged location near the watershed outlet. Another stream flow forecasting model was formulated to forecast river flows at downstream locations along the same channel. Input data for both models include the current and preceding records of rainfall and stream flow gathered at the watershed outlet and downstream locations. Computational algorithms for both models were based on a commercially available software. A case study was conducted on a small urban watershed in Greensboro, North Carolina. These two ANN-hydrologic forecasting models were successfully applied to provide near-real-time- and near-term-flow predictions with lead times starting from the present time and advancing to a few hours later on 15-min increments. An important aspect of this research has been the development of methodology fo...",project-academic
10.1155/2018/8489326,2018-06-13,a,Hindawi,air to air path loss prediction based on machine learning methods in urban environments," Recently, unmanned aerial vehicle (UAV) plays an important role in many applications because of its high flexibility and low cost. To realize reliable UAV communications, a fundamental work is to investigate the propagation characteristics of the channels. In this paper, we propose path loss models for the UAV air-to-air (AA) scenario based on machine learning. A ray-tracing software is employed to generate samples for multiple routes in a typical urban environment, and different altitudes of Tx and Rx UAVs are taken into consideration. Two machine-learning algorithms, Random Forest and KNN, are exploited to build prediction models on the basis of the training data. The prediction performance of trained models is assessed on the test set according to the metrics including the mean absolute error (MAE) and root mean square error (RMSE). Meanwhile, two empirical models are presented for comparison. It is shown that the machine-learning-based models are able to provide high prediction accuracy and acceptable computational efficiency in the AA scenario. Moreover, Random Forest outperforms other models and has the smallest prediction errors. Further investigation is made to evaluate the impacts of five different parameters on the path loss. It is demonstrated that the path visibility is crucial for the path loss.",project-academic
10.1109/ITSC.2012.6338707,2012-10-25,p,IEEE,multi agent reinforcement learning for integrated network of adaptive traffic signal controllers marlin atsc," Traffic congestion in Greater Toronto Area costs Canada $ 6 billion /year and is expected to grow up to $ 15 billion /year in the next few decades. Adaptive Traffic Signal Control(ATSC) is a promising technique to alleviate traffic congestion. For medium-large transportation networks, coordinated ATSC is becoming a challenging problem because the number of system states and actions grows exponentially as the number of networked intersections grows. Efficient and robust controllers can be designed using a multi-agent reinforcement learning (MARL) approach in which each controller (agent) is responsible for the control of traffic lights around a single traffic junction. This paper presents a novel, decentralized and coordinated adaptive real-time traffic signal control system using Multi-Agent Reinforcement Learning for Integrated Network of Adaptive Traffic Signal Controllers (MARLINATSC) that aims to minimize the total vehicle delay in the traffic network. The system is tested using microscopic traffic simulation software (PARAMICS) on a network of 5 signalized intersections in Downtown Toronto. The performance of MARLIN-ATSC is compared against two approaches: the conventional pretimed signal control (B1) and independent RL-based control agents (B2), i.e. with no coordination. The results show that network-wide average delay savings range from 32% to 63% relative to B1 and from 7% to 12% relative to B2 under different demand levels and arrival profiles.",project-academic
10.1016/J.PMCJ.2018.10.008,2018-12-01,a,Elsevier,asfault a low cost system to evaluate pavement conditions in real time using smartphones and machine learning," Abstract None None Modern smartphones have a large variety of built-in sensors that can measure different information about users and the environment around them. Given the increasing popularity of these devices, their high processing power, and the ability to transfer data over wireless networks, different smartphone-based applications have emerged in the last years to solve old problems with new approaches more efficiently and cheaply. One example is the assessment and monitoring of asphalt quality. This task has been done manually by experts since the 1930s, and with the help of expensive equipment since the 1960s. Currently, we are experiencing the emergence of next-generation tools to perform this monitoring with smartphones, significantly reducing costs, time, and effort of experts. However, there is a trade-off between the costs and precision of smartphone sensors, requiring the use of sophisticated software solutions. In this paper, we propose Asfault, a low-cost system to evaluate and monitor road pavement conditions in real-time using smartphone sensors and machine learning algorithms. The system is composed of an Android application responsible for doing automatic evaluations and a web application that aims to show the evaluations in an informative way. We propose to employ accelerometer sensors to measure the vehicle vibration while driving and use this data to evaluate the pavement conditions. Asfault achieves a classification performance superior to 90% in a 5-class problem considering the following road qualities: Good, Average, Fair, and Poor, as well the occurrence of obstacles in the road. Our system is publicly available for use and could be useful for practitioners responsible for urban and highway maintenance, as well for regular drivers in the planning of better routes based on the pavement quality and comfort of the travel.",project-academic
10.1016/J.JNCA.2016.06.010,2016-09-01,a,Academic Press,traffic and mobility aware resource prediction using cognitive agent in mobile ad hoc networks," Mobile Ad hoc NETwork (MANET) characteristics such as limited resources, shared channel, unpredictable mobility, improper load balancing, and variation in signal strength affect the routing of real-time multimedia data that requires Quality of Service (QoS) provisioning. Accurate prediction of the resource availability assists efficient resource allocation before the routing of such data. Most of the published work on resource prediction in MANET focuses on either bandwidth or energy without considering mobility effects. Adoption of intelligent software agent such as Cognitive Agent (CA) for the accurate resource prediction has a significant potential to solve the challenges of resource prediction in MANET. The intelligence provided in CA is similar to the logical thinking like a human for decision-making. The predominant CA architecture is the Belief-Desire-Intention (BDI) model, which performs the various tasks on behalf of the human user as an assistant.In this paper, we propose a CA-based Resource Prediction mechanism considering Mobility (CA-RPM) that predicts the resources using agents through the resource prediction agency consisting of one static agent, one cognitive agent and two mobile agents. Agents predict the traffic, mobility, buffer space, energy, and bandwidth effectively that is necessary for efficient resource allocation to support real-time and multimedia communications. The mobile agents collect and distribute network traffic statistics over MANET whereas a static agent collects the local statistics. CA creates static/mobile agent during the process of resource prediction. Initially, the designed time-series Wavelet Neural Networks (WNNs) predict traffic and mobility. Buffer space, energy, and bandwidth prediction use the predicted mobility and traffic. Simulation results show that the predicted resources closely match with the real values at the cost of little overheads due to the usage of agents. Simulation analysis of predicted traffic and mobility also shows the improvement compared to recurrent WNN in terms of mean square error, covariance, memory overhead, agent overhead and computation overhead. We plan to use these predicted resources for its efficient utilization in QoS routing is our future work.",project-academic
10.1145/3139958.3140053,2017-11-07,p,ACM,traffic prediction based power saving in cellular networks a machine learning method," In smart cities, green cellular networks play a crucial role to support wireless access for numerous devices anywhere and anytime with efficiency and sustainability. Because base stations (BSes) consume more than 70% of overall cellular network infrastructure energy, saving the power consumption of BSes is the key task to build a green cellular network. Except for low power design of the BS hardware and software, the traffic-driven BS sleeping operation is an economical way to improve existing cellular networks, which can reduce the BS power consumption at low traffic load. However, prior BS sleeping strategies establish on the static temporal characteristics of traffic load, which ignore the fact that network traffic is influenced by many factors such as time, human mobility, holiday, weather, etc. Hence, prior traffic estimation is coarse, and the BS sleeping strategies cannot apply to the changing network traffic. In this paper, we exploit a machine learning method to estimate the BS traffic and propose a BS sleeping strategy based on predicted traffic for power saving in the cellular network. We analyze network traffic in multi-views: temporal influence, spatial influence, and event influence. Then, we propose a multi-view ensemble learning model to predict network traffic load, which learns the traffic in multi-views and combine the results with ensemble. Furthermore, we formulate a BS sleeping strategy based on the predicted traffic load. Finally, we evaluate our traffic prediction algorithm on real cellular network data. The evaluation shows that our traffic prediction algorithm improves about 40% than state-of-the-art machine learning methods. Also, we evaluate the proposed BS sleeping strategy, which yields about 10% more energy savings and less device damage than the competitors in the simulated environment.",project-academic
10.1109/VTCFALL.2019.8891257,2019-09-01,p,IEEE,fingerprint based localization using commercial lte signals a field trial study," Wireless localization for mobile device has attracted more and more interests by increasing the demand for location based services. Fingerprint-based localization is promising, especially in non-Line-of-Sight (NLoS) or rich scattering environments, such as urban areas and indoor scenarios. In this paper, we propose a novel fingerprint-based localization technique based on deep learning framework under commercial long term evolution (LTE) systems. Specifically, we develop a software defined user equipment to collect the real time channel state information (CSI) knowledge from LTE base stations and extract the intrinsic features among CSI observations. On top of that, we propose a time domain fusion approach to assemble multiple positioning estimations. Experimental results demonstrated that the proposed localization technique can significantly improve the localization accuracy and robustness, e.g. achieves Mean Distance Error (MDE) of 0.47 meters for indoor and of 19.9 meters for outdoor scenarios, respectively.",project-academic
,2019-07-01,a,,fingerprint based localization using commercial lte signals a field trial study," Wireless localization for mobile device has attracted more and more interests by increasing the demand for location based services. Fingerprint-based localization is promising, especially in non-Line-of-Sight (NLoS) or rich scattering environments, such as urban areas and indoor scenarios. In this paper, we propose a novel fingerprint-based localization technique based on deep learning framework under commercial long term evolution (LTE) systems. Specifically, we develop a software defined user equipment to collect the real time channel state information (CSI) knowledge from LTE base stations and extract the intrinsic features among CSI observations. On top of that, we propose a time domain fusion approach to assemble multiple positioning estimations. Experimental results demonstrated that the proposed localization technique can significantly improve the localization accuracy and robustness, e.g. achieves Mean Distance Error (MDE) of 0.47 meters for indoor and of 19.9 meters for outdoor scenarios, respectively.",project-academic
10.1109/IJCNN.2010.5596480,2010-07-18,p,IEEE,a comparative study of urban traffic signal control with reinforcement learning and adaptive dynamic programming," This paper proposes a new algorithm that employs Adaptive Dynamic Programming(ADP) to solve the distributed control problem of urban traffic with an infinite horizon. Urban traffic congestions lead to a lot of time consumption and exhaust emissions. So alleviating congested situation will have a good impact on both economy and environment. The signal control at urban intersections is an effective and most important way to reduce the traffic jams and collisions. A lot of control theories including traditional mathematical ways and modern artificial intelligent ways have been exploited. ADP is an effective and amiable intelligent control method. We proposed an algorithm to adjust the signal time plan at urban traffic intersections based on ADP theory. Simulations are taken under a microscopic traffic simulation software, TSIS(Traffic Software Integrated System). Several criteria named MOEs(Measures of Effectiveness) are collected to compare with the widely used pre-timed control, actuated control, also with a machine learning method Q-learning control. Results show that ADP control method have a better adaptability to the various traffic simulating real traffic flows.",project-academic
10.1101/2020.05.30.20117945,2020-06-03,a,Cold Spring Harbor Laboratory Press,covid sgis a smart tool for dynamic monitoring and temporal forecasting of covid 19," Abstract None Objective None The new kind of coronavirus SARS-Cov2 spread to countries in all continents in the World. The coronavirus disease 2019 (Covid-19) causes fever, cough, sore throat, and in severe cases shortness of breath and death. To evaluate strategies, it is necessary to forecast the number of cases and deaths, in order to aid the stakeholders in the process of making decisions against the disease. We propose a system for real-time forecast of the cumulative cases of Covid-19 in Brazil. None Study Design None Monitoring of all Brazilian cities using oficial information from the National Notification System, from March to May 2020, concentrated on Brazil.io databases. Training and evaluation of ARIMA and other machine learning algorithms for temporal forecasting using correlation indexes (Pearson’s, Spearman’s, and Kendall’s) and RMSE(%). Validation from the relative errors of the following six days. None Methods None Our developed software, COVID-SGIS, captures information from the 26 states and the Distrito Federal at the Brazil.io database. From these data, ARIMA models are created for the accumulation of confirmed cases and death cases by Covid-19. Finally, six-day forecasts graphs are available for Brazil and for each of its federative units, separately, with a 95% CI. In addition to these predictions, the worst and best scenarios are also presented. None Results None ARIMA models were generated for Brazil and its 27 federative units. The states of Bahia, Maranhao, Piaui, Rio Grande do Norte and Amapa, Rondonia every day of the predictions were in the projection interval. The same happened to the states of Espirito Santo, Minas Gerais, Parana and Santa Catarina. In Brazil, the percentage error between the predicted values and the actual values varied between 2.56% and 6.50%. For the days when the forecasts outside the prediction interval, the percentage errors in relation to the worst case scenario were below 5%. The states of Bahia, Maranhao, Piaui, Rio Grande do Norte, Amapa, and Rondonia every day of the predictions were in the projection interval. The same happened to the states of Espirito Santo, Minas Gerais, Parana and Santa Catarina. None Conclusion None The proposed method for dynamic forecasting may be used to guide social policies and plan direct interventions in a robust, flexible and fast way. Since it is based on information from multiple databases, it can be adapted to the different realities, becoming an important tool to guide the course of politics and action against Covid-19 pandemic worldwide.",project-academic
10.1117/12.2304403,2018-05-10,p,SPIE,large scale public lidar and satellite image data set for urban semantic labeling," Automated semantic labeling of complex urban scenes in remotely sensed 2D and 3D data is one of the most challenging steps in producing realistic 3D scene models and maps. Recent large-scale public benchmark data sets and challenges for semantic labeling with 2D imagery have been instrumental in identifying state of the art methods and enabling new research. 3D data from lidar and multi-view stereo have also been shown to provide valuable additional information to enable improved semantic labeling accuracy. In this work, we describe the development of a new large-scale data set combining public lidar and multi-view satellite imagery with pixel-level truth for ground labels and instance-level truth for building labels. We demonstrate the use of this data set to evaluate methods for ground and building labeling tasks to establish performance expectations and identify areas for improvement. We also discuss initial steps toward further leveraging this data set to enable machine learning for more complex semantic and instance segmentation and 3D reconstruction tasks. All software developed to produce this public data set and to enable metric scoring are also released as open source code.",project-academic
10.1007/978-3-030-55958-8_9,2019-09-26,a,"Springer, Cham",a privacy preserving infrastructure for driver s reputation aware automotive services," Even though the introduction of ICT in transportation systems leads to several advantages in terms of efficiency of transport, mobility, traffic management, and in improved interfaces between different transport modes, it also brings some drawbacks in terms of increasing security challenges, also related to human behavior. For this reason, in the last decades, attempts to characterize drivers’ behavior have been mostly targeted towards risk assessment and, more recently, to the training of machine learning software for autonomous driving. In this paper, we propose, for the first time, to use driver behavioral characterization to build a general reputation profile, that can be used to create innovative, reputation-aware automotive services. As a first step towards realizing this vision, we present guidelines for the design of a privacy preserving vehicular infrastructure that is capable of collecting information generated from vehicles sensors and the environment, and to compose the collected information into driver reputation profiles. In turn, these profiles are exchanged in a privacy preserving way within the infrastructure to realize reputation-aware automotive services, a sample of which are described in the paper. As a fundamental component of the infrastructure, we show that: i) multi-dimensional reputation profiles can be formed building upon the recently introduced notion of driver DNA; ii) multi-dimensional comparison of profiles can be achieved by means of a reputation lattice rooted in the notion of algebraic c-semiring; and iii) a secure two-party mechanism can used to provide services to drivers on the basis of their reputation and/or DNA’s parameters.",project-academic
10.1007/S00500-020-04696-Z,2020-08-01,p,Springer Berlin Heidelberg,k openanswer a simulation environment to analyze the dynamics of massive open online courses in smart cities," The smartness of a city is given by the technologies it put to use, and more than that, by the people empowered by such technologies; it is worth thinking about how people can be trained to be empowered by smart technologies, and how cities can become “educational.” So, while sustainability and technology solutions for smart cities are strategic challenges, one of these is surely distance education and training. In this field, the Web offers many opportunities, such as the e-learning platforms where students can learn, according to their own needs and pace. The massive open online courses (MOOCs) are particular distance learning platforms, generally offering, so far, free courses on a huge amount of topics, and characterized by a (potentially) very high number of enrollments. In a MOOC, a teacher, or tutor, has a hard life when trying to follow and manage with the learning processes of thousands of students. In particular, assessment can be managed almost exclusively by letting the student answer questions in closed answers tests. This strategy has some didactic limits, while a valid alternative is to use peer assessment (PA) over more articulated assessment activities (e.g., open-ended questions). PA makes students grade their peers’ answers, and provides learners with significant advantages, such as refining their knowledge of the subject matter, and developing their meta-cognitive skills. In this work, we present a software platform called K-OpenAnswer, which helps teachers to simulate the dynamic of a MOOC where PA is used. The system uses a machine learning technique, based on a modified version of the K-NN algorithm, and provides teachers with a statistical environment by which they can monitor the evolving dynamic of a simulated MOOC, according to the techniques we use to implement PA. An experimental evaluation is presented that highlights the advantages of using the system as a valid tool for the study of real MOOCs.",project-academic
10.1109/ROMAN.2005.1513775,2005-10-03,p,IEEE,modularity and integration in the design of a socially interactive robot," Designing robots that are capable of interacting with humans in real life settings is a challenging task. One key issue is the integration of multiple modalities (e.g., mobility, physical structure, navigation, vision, audition, dialogue, reasoning) into a coherent framework. Taking the AAAI mobile robot challenge (making a robot attend the national conference on artificial intelligence) as the experimental context, we are currently addressing hardware, software and computation integration issues involved in designing a robot capable of sophisticated interaction with humans. This paper reports on our design solutions and the current status of the work, along with the potential impacts this design on human-robot interaction research.",project-academic
10.1109/VNC.2011.6117122,2011-12-29,p,IEEE,on optimizing vehicular dynamic spectrum access networks automation and learning in mobile wireless environments," In this paper, we propose a novel architecture for optimizing the overall performance of vehicular dynamic spectrum access (VDSA) networks. Due to the high level of mobility for vehicles operating under highway conditions, coupled with spatially variant spectrum allocation across a large geographical region, we envision that future vehicular communications will employ a form of dynamic spectrum access (DSA) in order to facilitate wireless transmissions between vehicles and with roadside infrastructure. In particular, the VDSA concept will be enabled by a combination of software-defined radio (SDR) technology, spectral occupancy databases, and machine learning techniques for enabling network automation. A vehicular networking scenario is substantially different relative to a generic mobile scenario with respect to the high level of mobility involved, the predictable trajectories of the vehicular traffic, and the overall scale of the network range. Consequently, the proposed architecture is designed to enable VDSA in a more flexible wireless spectrum environment by leveraging the cognitive radio concept and existing wireless spectrum databases actively being developed while simultaneously being compatible with current spectrum regulations. Regarding practical issues for vehicular communications, vehicle mobility is taken into account in order to ensure primary user protection, databases and channel priority schemes are used in order to record temporal and spatial channel heterogeneity, and vehicle path prediction techniques are employed in order to enhance channel access in this operating environment. Specifically, we show the advantages of employing the proposed learning architecture via a case study where reinforcement learning is used in order to achieve intelligent channel selection within a realistic VDSA environment. Moreover, performance enhancements in terms of channel switching times, interference, and throughput are shown via computer simulations.",project-academic
,2018-12-10,a,,crossfire attack detection using deep learning in software defined its networks," Recent developments in intelligent transport systems (ITS) based on smart mobility significantly improves safety and security over roads and highways. ITS networks are comprised of the Internet-connected vehicles (mobile nodes), roadside units (RSU), cellular base stations and conventional core network routers to create a complete data transmission platform that provides real-time traffic information and enable prediction of future traffic conditions. However, the heterogeneity and complexity of the underlying ITS networks raise new challenges in intrusion prevention of mobile network nodes and detection of security attacks due to such highly vulnerable mobile nodes. In this paper, we consider a new type of security attack referred to as crossfire attack, which involves a large number of compromised nodes that generate low-intensity traffic in a temporally coordinated fashion such that target links or hosts (victims) are disconnected from the rest of the network. Detection of such attacks is challenging since the attacking traffic flows are indistinguishable from the legitimate flows. With the support of software-defined networking that enables dynamic network monitoring and traffic characteristic extraction, we develop a machine learning model that can learn the temporal correlation among traffic flows traversing in the ITS network, thus differentiating legitimate flows from coordinated attacking flows. We use different deep learning algorithms to train the model and study the performance using Mininet-WiFi emulation platform. The results show that our approach achieves a detection accuracy of at least 80%.",project-academic
10.1109/VTCSPRING.2019.8746594,2019-04-01,p,IEEE,crossfire attack detection using deep learning in software defined its networks," Recent developments in intelligent transport systems (ITS) based on smart mobility significantly improves safety and security over roads and highways. ITS networks are comprised of the Internet-connected vehicles (mobile nodes), roadside units (RSU), cellular base stations and conventional core network routers to create a complete data transmission platform that provides real-time traffic information and enable prediction of future traffic conditions. However, the heterogeneity and complexity of the underlying ITS networks raise new challenges in intrusion prevention of mobile network nodes and detection of security attacks due to such highly vulnerable mobile nodes. In this paper, we consider a new type of security attack referred to as crossfire attack, which involves a large number of compromised nodes that generate low-intensity traffic in a temporally coordinated fashion such that target links or hosts (victims) are disconnected from the rest of the network. Detection of such attacks is challenging since the attacking traffic flows are indistinguishable from the legitimate flows. With the support of software-defined networking that enables dynamic network monitoring and traffic characteristic extraction, we develop a machine learning model that can learn the temporal correlation among traffic flows traversing in the ITS network, thus differentiating legitimate flows from coordinated attacking flows. We use different deep learning algorithms to train the model and study the performance using Mininet-WiFi emulation platform. The results show that our approach achieves a detection accuracy of at least 80%.",project-academic
10.1016/J.JELECTROCARD.2021.07.012,2021-07-23,a,Churchill Livingstone,novel ecg features and machine learning to optimize culprit lesion detection in patients with suspected acute coronary syndrome," Abstract None None Background None Novel temporal-spatial features of the 12‑lead ECG can conceptually optimize culprit lesions' detection beyond that of classical ST amplitude measurements. We sought to develop a data-driven approach for ECG feature selection to build a clinically relevant algorithm for real-time detection of culprit lesion. None None None Methods None This was a prospective observational cohort study of chest pain patients transported by emergency medical services to three tertiary care hospitals in the US. We obtained raw 10-s, 12‑lead ECGs (500 s/s, HeartStart MRx, Philips Healthcare) during prehospital transport and followed patients 30 days after the encounter to adjudicate clinical outcomes. A total of 557 global and lead-specific features of P-QRS-T waveform were harvested from the representative average beats. We used Recursive Feature Elimination and LASSO to identify 35/557, 29/557, and 51/557 most recurrent and important features for LAD, LCX, and RCA culprits, respectively. Using the union of these features, we built a random forest classifier with 10-fold cross-validation to predict the presence or absence of culprit lesions. We compared this model to the performance of a rule-based commercial proprietary software (Philips DXL ECG Algorithm). None None None Results None Our sample included 2400 patients (age 59 ± 16, 47% female, 41% Black, 10.7% culprit lesions). The area under the ROC curves of our random forest classifier was 0.85 ± 0.03 with sensitivity, specificity, and negative predictive value of 71.1%, 84.7%, and 96.1%. This outperformed the accuracy of the automated interpretation software of 37.2%, 95.6%, and 92.7%, respectively, and corresponded to a net reclassification improvement index of 23.6%. Metrics of ST80; Tpeak-Tend; spatial angle between QRS and T vectors; PCA ratio of STT waveform; T axis; and QRS waveform characteristics played a significant role in this incremental gain in performance. None None None Conclusions None Novel computational features of the 12‑lead ECG can be used to build clinically relevant machine learning-based classifiers to detect culprit lesions, which has important clinical implications.",project-academic
10.1109/JIOT.2019.2949633,2020-07-01,a,IEEE,context aware object detection for vehicular networks based on edge cloud cooperation," Due to high mobility and high dynamic environments, object detection for vehicular networks is one of the most challenging tasks. However, the development of integration techniques, such as software-defined networking (SDN) and network function visualization (NFV), in networking, caching, and computing provides us with new approaches. In this article, we propose a novel context-aware object detection method based on edge-cloud cooperation. Specifically, an object detection model based on deep learning is established in the cloud server. Different from other methods, to further explore the underlying inner spatial features of collected images, the visual objects of images are regarded as nodes and the spatial relations between objects as edges, then a type of message-passing method is employed to update the nodes’ features. In the mobile edge computing (MEC) servers, the context information and captured images of the vehicular environments are extracted and then are used to adjust the object detection model from the cloud server. In this way, the cloud server cooperates with the MEC servers to realize context-aware object detection, which improves the adaptation and performance of the detection model under different scenarios. The simulation results also demonstrate that the proposed method is more accurate and faster than the previous methods.",project-academic
10.1109/TCCN.2019.2944399,2019-09-30,a,IEEE,blockchain based distributed software defined vehicular networks a dueling deep q learning approach," Vehicular ad hoc networks (VANETs) have become an essential part in smart transportation systems of modern cities. However, because of dynamicity and infrastructure-less of VANETs, the ever increasing number of network security issues become obstacles for the realization of smart cities. Software-defined VANETs have provided a reliable way to manage VANETs dynamically and securely. However, the traditionally centralized control plane makes it vulnerable to malicious nodes and results in performance degradation. Therefore, a distributed control plane is necessary. How to reach a consensus among multiple controllers under complex vehicular environment is an essential problem. In this paper, we propose a novel blockchain-based distributed software-defined VANET framework (block-SDV) to establish a secure architecture to overcome the above issues. The trust features of blockchain nodes, the number of consensus nodes, trust features of each vehicle, and the computational capability of the blockchain are considered in a joint optimization problem, which is modeled as a Markov decision process with state space, action space and reward function. Since it is difficult to be solved by traditional methods, we propose a novel dueling deep None None None ${Q}$ None None -learning (DDQL) with prioritized experience replay approach. Simulation results are presented to show the effectiveness of the proposed block-SDV framework.",project-academic
10.1109/MTITS.2017.8005639,2017-06-26,p,IEEE,training neural networks to approximate traffic simulation outcomes," We present results of our research on training neural networks to approximate traffic simulation outcomes, such as total times of waiting on a red signal. We developed TensorTraffic software, based on a TensorFlow library, and trained neural networks on a dataset generated by simulating traffic on a realistic road network of Warsaw using Traffic Simulation Framework software. The goal of conducted experiments was to approximate the total times of waiting on a red signal on a region of Warsaw (Stara Ochota district), with the input to neural nets representing offsets of traffic signals on that region. In the presented research, we focused on investigating different neural network models and strategies of their training. We took into account different sizes of training sets, different numbers of neurons and layers, different parameters of dropout and learning rate, in order to reduce as much as possible time required to conduct experiments and apply this method in practice (e.g., time required to generate training and test sets for neural networks, time to train neural networks and time to make inferences on a new set), while preserving a sufficient accuracy of approximations, which may be especially important from a practical point of view Results show that it is possible to train neural networks able to approximate with a high accuracy (with an average error 1.18%) outcomes of traffic simulations. Moreover, TensorTraffic allows obtaining results of approximations a few orders of magnitude faster than by running simulations using microscopic traffic models and it is possible to achieve acceptable accuracy on a relatively small training set (consisting of 10240 elements). It means that the method can be potentially applied to different traffic analysis and transport planning tasks (e.g., to find suboptimal configurations of traffic signals) and time-consuming computer simulations applied nowadays for traffic analysis can be potentially replaced by neural nets supported by computations using graphical processing units (GPU). This innovation may significantly reduce time required to complete research and engineering tasks related to designing road infrastructure and analysing vehicular traffic, as well as enable developing better traffic management systems. As an example, we applied the method to the traffic signal setting problem and accelerated existing genetic algorithm, giving opportunity to evaluate much larger set of possible settings and find better traffic management strategies.",project-academic
10.1089/DIA.2019.0038,2019-05-23,a,Diabetes Technol Ther,use of telemedicine technologies in diabetes prevention and control in resource constrained settings lessons learned from emerging economies," Telemedicine is a promising strategy that utilizes telecommunication to provide health care in remote areas, facilitating beneficial interaction between the health care provider and people in rural areas and making affordable and accessible medical care available to remote, inaccessible areas of the world. This article provides an overview of some of the ways telemedicine is improving diabetes care outcomes at the community level. Telemedicine can play a number of roles in moving quality diabetes care forward. It is currently being used to create awareness among urban and rural population about the risk factors and prevention of diabetes; to facilitate patient monitoring; for remote diabetic retinopathy screening; and in diabetes prevention at the primary, secondary, and tertiary level. We also highlight the use of automated artificial intelligence software combined with telemedicine to conduct efficient real-time screening of complications such as diabetic retinopathy in remote areas where such facilities are currently unavailable.",project-academic
10.1109/CVPR42600.2020.00609,2020-06-14,p,IEEE,a novel recurrent encoder decoder structure for large scale multi view stereo reconstruction from an open aerial dataset," A great deal of research has demonstrated recently that multi-view stereo (MVS) matching can be solved with deep learning methods. However, these efforts were focused on close-range objects and only a very few of the deep learning-based methods were specifically designed for large-scale 3D urban reconstruction due to the lack of multi-view aerial image benchmarks. In this paper, we present a synthetic aerial dataset, called the WHU dataset, we created for MVS tasks, which, to our knowledge, is the first large-scale multi-view aerial dataset. It was generated from a highly accurate 3D digital surface model produced from thousands of real aerial images with precise camera parameters. We also introduce in this paper a novel network, called RED-Net, for wide-range depth inference, which we developed from a recurrent encoder-decoder structure to regularize cost maps across depths and a 2D fully convolutional network as framework. RED-Net’s low memory requirements and high performance make it suitable for large-scale and highly accurate 3D Earth surface reconstruction. Our experiments confirmed that not only did our method exceed the current state-of-the-art MVS methods by more than 50% mean absolute error (MAE) with less memory and computational cost, but its efficiency as well. It outperformed one of the best commercial software programs based on conventional methods, improving their efficiency 16 times over. Moreover, we proved that our RED-Net model pre-trained on the synthetic WHU dataset can be efficiently transferred to very different multi-view aerial image datasets without any fine-tuning. Dataset and code are available at http://gpcv.whu.edu.cn/data.",project-academic
,2020-03-02,a,,a novel recurrent encoder decoder structure for large scale multi view stereo reconstruction from an open aerial dataset," A great deal of research has demonstrated recently that multi-view stereo (MVS) matching can be solved with deep learning methods. However, these efforts were focused on close-range objects and only a very few of the deep learning-based methods were specifically designed for large-scale 3D urban reconstruction due to the lack of multi-view aerial image benchmarks. In this paper, we present a synthetic aerial dataset, called the WHU dataset, we created for MVS tasks, which, to our knowledge, is the first large-scale multi-view aerial dataset. It was generated from a highly accurate 3D digital surface model produced from thousands of real aerial images with precise camera parameters. We also introduce in this paper a novel network, called RED-Net, for wide-range depth inference, which we developed from a recurrent encoder-decoder structure to regularize cost maps across depths and a 2D fully convolutional network as framework. RED-Net's low memory requirements and high performance make it suitable for large-scale and highly accurate 3D Earth surface reconstruction. Our experiments confirmed that not only did our method exceed the current state-of-the-art MVS methods by more than 50% mean absolute error (MAE) with less memory and computational cost, but its efficiency as well. It outperformed one of the best commercial software programs based on conventional methods, improving their efficiency 16 times over. Moreover, we proved that our RED-Net model pre-trained on the synthetic WHU dataset can be efficiently transferred to very different multi-view aerial image datasets without any fine-tuning. Dataset are available at this http URL.",project-academic
10.1109/JIOT.2019.2951593,2020-07-01,a,IEEE,trusted cloud edge network resource management drl driven service function chain orchestration for iot," Private and public networks sharing resources for Internet of Things (IoT) network through network function virtualization (NFV) and software-defined networking (SDN) forms a heterogeneous cloud-edge environment. However, the heterogeneous cloud-edge network faces trust and adaptation issues in resource allocation. To address these two problems, we introduce consortium blockchain and deep reinforcement learning (DRL) to construct the trusted and auto-adjust service function chain (SFC) orchestration architecture. In the architecture, this article integrates the consortium blockchain into the distributed SFC orchestration model to realize trusted resource sharing. In addition, for realizing auto-adjusted service provision, this article designs a dynamic hierarchical SFC orchestration algorithm (DHSOA) based on DRL to minimize the orchestration cost and improve the quality of service. Moreover, considering the dynamics of network entities, this article proposes a time-slotted model to support dynamic service migration which adapts to the high-mobility IoT network. The simulation results show that DHSOA has better performance than the link-state routing algorithm and deep None None None $Q$ None None -network placement algorithm not only in cost saving of 15.8% and 10.1% but also in time saving of 22.0% and 10.0%.",project-academic
10.1007/978-3-030-00308-1,2017-06-20,a,,the nao backpack an open hardware add on for fast software development with the nao robot," We present an open-source accessory for the NAO robot, which enables to test computationally demanding algorithms in an external platform while preserving robot's autonomy and mobility. The platform has the form of a backpack, which can be 3D printed and replicated, and holds an ODROID XU4 board to process algorithms externally with ROS compatibility. We provide also a software bridge between the B-Human's framework and ROS to have access to the robot's sensors close to real-time. We tested the platform in several robotics applications such as data logging, visual SLAM, and robot vision with deep learning techniques. The CAD model, hardware specifications and software are available online for the benefit of the community: this https URL",project-academic
10.1007/978-3-030-00308-1_25,2017-07-27,p,"Springer, Cham",the nao backpack an open hardware add on for fast software development with the nao robot," We present an open-source accessory for the NAO robot, which enables to test computationally demanding algorithms in an external platform while preserving robot’s autonomy and mobility. The platform has the form of a backpack, which can be 3D printed and replicated, and holds an ODROID XU4 board to process algorithms externally with ROS compatibility. We provide also a software bridge between the B-Human’s framework and ROS to have access to the robot’s sensors close to real-time. We tested the platform in several robotics applications such as data logging, visual SLAM, and robot vision with deep learning techniques. The CAD model, hardware specifications and software are available online for the benefit of the community.",project-academic
10.1016/J.ADHOC.2020.102305,2021-02-01,p,Elsevier,a city wide experimental testbed for the next generation wireless networks," Abstract None None To facilitate research in dynamic spectrum access, 5G, vehicular networks, underground wireless communications, and radio frequency machine learning, a city-wide experimental testbed is developed to provide realistic radio environment, standardized experimental configurations, reusable datasets, and advanced computational resources. The testbed contains 5 cognitive radio sites, and covers 1.1 square miles across two campuses of the University of Nebraska-Lincoln and a public street in the city of Lincoln, Nebraska. Each site is equipped with a 4x4 MIMO software-defined radio transceiver with 20Gbps fronthaul connectivity. Additional cognitive radio transceivers with an underground 2x2 MIMO antenna are included in a site. High speed fronthaul network based on dedicated fiber connects the 5 sites to a cloud-based central unit for data processing and storage. The testbed provides researchers rich computational resources such as arrays of CPUs and GPUs at the cloud and FPGAs at both the edge and fronthaul network. Developed via the collaboration of the university, city, and industrial partners, this testbed will facilitate education and researches in academic and industrial communities.",project-academic
10.1007/978-3-540-77465-5,2008-03-20,b,"Springer Publishing Company, Incorporated",soft computing applications in industry," Softcomputing techniques play a vital role in the industry. This book presents several important papers presented by some of the well-known scientists from all over the globe. The application domains discussed in this book include: agroecology, bioinformatics, branched fluid-transport network layout design, dam scheduling, data analysis and exploration, detection of phishing attacks, distributed terrestrial transportation, fault detection of motors, fault diagnosis of electronic circuits, fault diagnosis of power distribution systems, flood routing, hazard sensing, health care, industrial chemical processes, knowledge management in software development, local multipoint distribution systems, missing data estimation, parameter calibration of rainfall intensity models, parameter identification for systems engineering, petroleum vessel mooring, query answering in P2P systems, real-time strategy games, robot control, satellite heat pipe design, monsoon rainfall forecasting, structural design, tool condition monitoring, vehicle routing, water network design, etc. The softcomputing techniques presented in this book are on (or closely related to): ant-colony optimization, artificial immune systems, artificial neural networks, Bayesian models, case-based reasoning, clustering techniques, differential evolution, fuzzy classification, fuzzy neural networks, genetic algorithms, harmony search, hidden Markov models, locally weighted regression analysis, probabilistic principal component analysis, relevance vector machines, self-organizing maps, other machine learning and statistical techniques, and the combinations of the above techniques.",project-academic
10.1016/J.PROENG.2017.03.215,2017-01-01,a,Elsevier,smart solution to improve water energy nexus for water supply systems," Abstract None None In the last years, there has been a great interest in the complex relations between energy and water, known as the Water-Energy Nexus None [1] . Natural resources, such as energy and water, enable economy growth and support quality of life. The Water-Energy Nexus is considered as one of the most important multidisciplinary challenges None [2] None that the global growing water market None [3] None has to face in the forthcoming years. Currently, many water systems are not managed sustainably enough. Water Utilities face other challenges, such as infrastructure aging and poor cost-recovery, leading to a lack of finance for O&M (Operation and Maintenance). Energy is required in all stages of water production and distribution, from pumping and treatment to transportation. Energy costs are a top-of-mind concern for water utilities, regardless of geography, size and level of water network efficiency None [4] . On the other hand, Water Utilities are having a hard time to either improve their services or expand their network to unserved neighborhoods in developing countries. None The current trend of water transmission system to the creation of DMAs (District Metered Areas) offers great possibilities of non-structural solutions that use existing data and transform them into useful information to support decision making. The Smart Metering and the use of large amounts of data from a network enhance the use of software for decision support, but it is not the only way. Smart Solutions can also be applied to networks with less recorded data, which would enhance operators’ knowledge to these data, turn them into useful information for decision-making either for the operation or the maintenance and network design. In this scope, a Smart Solution is presented. It is developed combining key factors of the energy consumption and the water supply into water networks management to obtain improvements from both water and energy fields. This non-structural solution increases resource efficiency and environmental performance of water distribution networks by using data acquisition and geographical visualization (real time & historical), weather and water demand forecasting, detection of networks events and hydraulic simulation of the network, and finally through a decision support system based on machine learning (pattern recognition and business rules techniques). None As a conclusion, a non-structural solution for the Nexus issues can have a great impact on several matters (climate change, carbon footprint, WUs balance sheets, and water losses) with reasonable investment either in smart metering or networks with only a few sensors measuring.",project-academic
10.1007/S10514-006-9014-7,2007-05-01,a,Springer US,spartacus attending the 2005 aaai conference," Spartacus is our robot entry in the 2005 AAAI Mobile Robot Challenge, making a robot attend the National Conference on Artificial Intelligence. Designing robots that are capable of interacting with humans in real-life settings can be considered the ultimate challenge when it comes to intelligent autonomous systems. One key issue is the integration of multiple modalities (e.g., mobility, physical structure, navigation, vision, audition, dialogue, reasoning). Such integration increases the diversity and also the complexity of interactions the robot can generate. It also makes it difficult to monitor how such increased capabilities are used in unconstrained conditions, whether it is done while the robot is in operation of afterwards. This paper reports solutions and findings resulting from our hardware, software and decisional integration work on Spartacus. It also outlines perspectives in making intelligent and interaction capabilities evolve for autonomous robots.",project-academic
,2017-10-19,,,autonomous set of devices and method for detecting and identifying plant species in an agricultural crop for the selective application of agrochemicals," The invention relates to an autonomous set of devices for detecting and identifying wild and cultivated plant species on a farm, using software which, by obtaining a video in real time, can detect, isolate and identify different wild and cultivated plant species by using convolutional neural networks able to distinguish distinctive aspects of the morphology, taxonomy and philotaxy of the plants. By previously training the convolutional neural networks on the characteristics that distinguish one species from another, the system allows the particular identification of each species. By means of a system of video cameras mounted along a transport vehicle, and with the data being obtained in real time, the computer system can determine the agrochemical to be applied according to the plant identified and electronically or mechanically actuate the opening of the valve of a spray nozzle. In this way, the plant receives the exact dose and the specific agrochemical according to the necessary treatment.",project-academic
,2018-10-30,,,intelligent public transportation system based on internet of things mode," The invention discloses an intelligent public transportation system based on an internet-of-things mode. The intelligent public transportation system comprises a coin-free system and a beckoning stopsystem; the coin-free system involves locating, location stations, the number of passengers, site selection, generation of a payment two-dimensional code, scanning for payment, generation of order information, database updating, automatic billing, payment failure and payment success; the beckoning stop system involves a data set, image preprocessing, processed images, adoption of a convolutional neural network for training, a knowledge library, pre-processing, image inputting by users, personal beckoning action, prompting and non-personal beckoning action. According to the system, based on a machine learning algorithm, whether or not a person beckons for bus stopping is analyzed and judged, whether or not there is a remaining seat is judged according to a temperature sensor and a pressuresensor, useful information required by the passengers is queried in real time through a mobile phone APP client side, the coin-free purpose of the bus is achieved through the combination of an electronic screen and software, people take the bus very conveniently, and the system is easy to popularize.",project-academic
10.3390/S18124123,2018-11-24,a,Sensors (Basel),bipred a bilevel evolutionary algorithm for prediction in smart mobility," This article develops the design, installation, exploitation, and final utilization of intelligent techniques, hardware, and software for understanding mobility in a modern city. We focus on a smart-campus initiative in the University of Malaga as the scenario for building this cyber⁻physical system at a low cost, and then present the details of a new proposed evolutionary algorithm used for better training machine-learning techniques: BiPred. We model and solve the task of reducing the size of the dataset used for learning about campus mobility. Our conclusions show an important reduction of the required data to learn mobility patterns by more than 90%, while improving (at the same time) the precision of the predictions of theapplied machine-learning method (up to 15%). All this was done along with the construction of a real system in a city, which hopefully resulted in a very comprehensive work in smart cities using sensors.",project-academic
10.1109/TNNLS.2014.2323218,2015-04-01,a,IEEE Trans Neural Netw Learn Syst,a universal concept based on cellular neural networks for ultrafast and flexible solving of differential equations," This paper develops and validates a comprehensive and universally applicable computational concept for solving nonlinear differential equations (NDEs) through a neurocomputing concept based on cellular neural networks (CNNs). High-precision, stability, convergence, and lowest-possible memory requirements are ensured by the CNN processor architecture. A significant challenge solved in this paper is that all these cited computing features are ensured in all system-states (regular or chaotic ones) and in all bifurcation conditions that may be experienced by NDEs.One particular quintessence of this paper is to develop and demonstrate a solver concept that shows and ensures that CNN processors (realized either in hardware or in software) are universal solvers of NDE models. The solving logic or algorithm of given NDEs (possible examples are: Duffing, Mathieu, Van der Pol, Jerk, Chua, Rossler, Lorenz, Burgers, and the transport equations) through a CNN processor system is provided by a set of templates that are computed by our comprehensive templates calculation technique that we call nonlinear adaptive optimization. This paper is therefore a significant contribution and represents a cutting-edge real-time computational engineering approach, especially while considering the various scientific and engineering applications of this ultrafast, energy-and-memory-efficient, and high-precise NDE solver concept. For illustration purposes, three NDE models are demonstratively solved, and related CNN templates are derived and used: the periodically excited Duffing equation, the Mathieu equation, and the transport equation.",project-academic
10.1109/ACCESS.2019.2951294,2019-11-04,a,IEEE,urban commerce distribution analysis based on street view and deep learning," Urban commerce and its distribution have always been an important part of urban research. However, most previous studies were based on statistical data and did not reflect real street experience. Thanks to the Street View image and deep learning technology, researchers are able to carry out large scale studies from real human visual experience. In this article, we aim at sensing the commercial spaces in cities. In order to achieve this ultimate goal, deep learning is applied to process the raw data of Street View image. We disassemble the goal into three tasks: firstly, obtaining all the Street View images in a specific area; then classifying the Street View images according to the commercial facilities in it; and finally creating a visualization of the detected data into a map. For the first task, we get the road network coordinate information from the openstreetmap (OSM) website, set the sampling point on the road, and then download the Street View images of the sampling points' coordinate through the API provided by Baidumap. For the second task, we adopt a two-level learning strategy rather than directly using Deep Convolutional Neural Network for classification. For the final task, we choose the heat map as the expression of the results and draw the map by the existing GIS software. Furthermore, the results from this study can be conveniently combined with other data because of the use of street-network-based data structure. An application of this method combines with street-network data, the calculation of a city's 15-minute commercial service circle coverage is also shown in this study.",project-academic
10.1101/272468,2018-02-27,a,Cold Spring Harbor Laboratory,videotagger user friendly software for annotating video experiments of any duration," BACKGROUND: Scientific insight is often sought by recording and analyzing large quantities of video. While easy access to cameras has increased the quantity of collected videos, the rate at which they can be analyzed remains a major limitation. Often, bench scientists struggle with the most basic problem that there is currently no user-friendly, flexible, and open source software tool with which to watch and annotate these videos. RESULTS: We have created the VideoTagger tool to address these and many of the other associated challenges of video analysis. VideoTagger allows non-programming users to efficiently explore, annotate, and visualize large quantities of video data, within their existing experimental protocols. Further, it is built to accept programmed plugins written in Python, to enable seamless integration with other sophisticated computer-aided analyses. We tested VideoTagger ourselves, and have a growing base of users in other scientific disciplines. Capitalising on the unique features of VideoTagger to play back infinite lengths of video footage at various speeds, we annotated 39h of a Drosophila melanogaster lifespan video, at approximately 10-15x faster than real-time. We then used these labels to train a machine-learning plugin, which we used to annotate an additional 538h of footage automatically. In this way, we found that flies fall over spontaneously with increasing frequency as they age, and also spend longer durations struggling to right themselves. Ageing in flies is typically defined by length of life. We propose that this new mobility measure of ageing could help the discovery of mechanisms in biogerontology, refining our definition of what healthy ageing means in this extremely small, but widely used, invertebrate. CONCLUSIONS: We show how VideoTagger is sufficiently flexible for studying lengthy and/or numerous video experiments, thus directly improving scientists' productivity across varied domains.",project-academic
10.1117/1.JMI.8.S1.013501,2021-01-01,a,SPIE-Intl Soc Optical Eng,methodology to create 3d models of covid 19 pathologies for virtual clinical trials," Purpose: We describe the creation of computational models of lung pathologies indicative of COVID-19 disease. The models are intended for use in virtual clinical trials (VCT) for task-specific optimization of chest x-ray (CXR) imaging.

Approach: Images of COVID-19 patients confirmed by computed tomography were used to segment areas of increased attenuation in the lungs, all compatible with ground glass opacities and consolidations. Using a modeling methodology, the segmented pathologies were converted to polygonal meshes and adapted to fit the lungs of a previously developed polygonal mesh thorax phantom. The models were then voxelized with a resolution of 0.5 mm × 0.5 mm × 0.5 mm and used as input in a simulation framework to generate radiographic images. Primary projections were generated via ray tracing while the Monte Carlo transport code was used for the scattered radiation. Realistic sharpness and noise characteristics were also simulated, followed by clinical image processing. Example images generated at 120 kVp were used for the validation of themodels in a reader study. Additionally, images were uploaded to an Artificial Intelligence (AI) software for the detection of COVID-19.

Results: Nine models of COVID-19 associated pathologies were created, covering a range of disease severity. The realism of the models was confirmed by experienced radiologists and by dedicated AI software.

Conclusions: A methodology has been developed for the rapid generation of realistic 3D models of a large range of COVID-19 pathologies. The modeling framework can be used as the basis for VCTs for testing detection and triaging of COVID-19 suspected cases.",project-academic
10.1111/0885-9507.00150,1999-09-01,a,Blackwell Publishers Inc.,on line and off line routing and scheduling of dial a ride paratransit vehicles," This article presents the general concepts , mod- els, and computational techniques applied in a new dial-a- ride vehicle routing and scheduling system . The objective of this system is to improve the responsiveness , reliability, and productivity of dial-a-ride paratransit services . The de- veloped software integrates dial-a-ride routing and schedul- ing principles and practical experience and explicitly con- siders travel time variability in urban roadway networks . Such extensive and complex integration has been made pos- sible by improved data acquisition and processing capabil- ities of computer, telecommunications, and vehicle location technologies. Advanced computational methods applied in the system, such as the artificial neural network technique , which allows heuristic estimation of origin-destination travel times in a dynamic and stochastic fashion , contribute to the processing speed required to respond expeditiously and effi- ciently to paratransit user requests. A real scheduling prob- lem from the city of Edmonton , Alberta, where the system was tested, is used to illustrate the positive computational expe- rience and the capability of the developed software to handle both off-line and on-line operations .",project-academic
10.1109/FMEC49853.2020.9144828,2020-04-20,p,IEEE,keynote speech 3 big data computing and machine learning for intelligent transportation and connected vehicles," We are developing machine learning algorithms and software to fuse real-time feeds from video cameras and traffic sensor data to generate real-time detection, classification, and space-time trajectories of individual vehicles and pedestrians. This information is then transmitted to a cloud-based system and then synthesized to create a real-time city-wide traffic palette. I will discuss our research on: Smart intersections: Space-time trajectories are used to understand and improve the safety and efficiency of the intersection. Using conflict points of the vehicle-pedestrian trajectories, we identify potential collisions, or “near-misses,” and how they are related to the state of the signal cycle (transition from green to yellow, from yellow to red, etc.) and the presence of other vehicles and pedestrians. • Smart system: We are developing efficient signal re-timing for different corridors by time of day and day of the week to reflect the changes in network demand. We are also developing machine learning techniques for real-time detection of incidents and accidents on arterial networks. • Smart interactions with connected and autonomous vehicles: We have developed signalized intersection control strategies and sensor fusion algorithms for jointly optimizing vehicle trajectories and signal control for a mixture of autonomous vehicles and traditional vehicles at every intersection",project-academic
,2021-02-09,a,,automated hiring at amazon," In this fictionalized case, the protagonist is the director of talent and recruitment at Amazon and had been in charge of developing an artificial intelligence (AI) hiring tool that could shape the future of recruiting practices throughout the world. However, she begins to question the ethical implications of this tool. While the employees hired via this AI tool were performing exceptionally well, almost all of them were male, and she wonders how the tool she had helped create led to this result. There was no way to get around the fact that the hiring tool was gender biased, and the options are limited. The company could continue to use the tool as it currently existed, undoubtedly leading to a promotion for the protagonist. Or she could insist that Amazon invest more into the research and development of the AI tool, in hopes of creating an unbiased product. However, this option would be exorbitantly expensive in terms of both time and money and would reflect poorly on her efforts to produce this tool. A third option was for Amazon to scrap the AI hiring program altogether and return to traditional methods. The use of AI and automation was controversial, so perhaps showing the public that Amazon was putting people first would improve the company's image. The protagonist had to make a recommendation to senior management soon, and was uncertain what that should be.

Excerpt

UVA-E-0470

Feb. 2, 2021

Automated Hiring at Amazon

It was 2:00 p.m., and you were sitting at your desk preparing for the meeting that could make or break your career. In your six short months at Amazon as director of talent and recruitment, you had been in charge of developing an artificial intelligence (AI) hiring tool that could shape the future of recruiting practices throughout the world. The tool, which had been piloted at Amazon Crystal City for the past quarter, had the ability to process thousands of resumes in minutes and flag the most qualified candidates. You had a meeting with Chuck Mudd, your boss and director of human resources at Amazon Crystal City, at 3:00 p.m. to evaluate the tool's performance and discuss next steps.

As an African-American businesswoman and graduate from the Wharton School of Business, you knew the importance of hard work to succeed, especially in a male-dominated sector. After a long and successful career as a business technology consultant at McKinsey & Company, you had decided to shift career paths when you saw an opening for director of talent and recruitment at Amazon's second headquarters. Mudd had been especially impressed by your business-technology background and your ideas around automating Amazon's hiring process.

Shortly after the start of your contract, Mudd gave you the green light to work alongside Arthur Mortimer, Amazon's AI director, to develop a tool to automate the rsum-review process. Mudd gave you free reign to determine the specifications of the bot, and proposed a one-quarter trial period for it. If it produced successful results, he promised a nice bonus and potential promotion. In developing the rsum-processing software, Mortimer fed the bot real resumes of tens of thousands of Amazon applicants, some of whom had been hired and some of whom had not. The bot learned the key qualities of the strongest rsums, such as the best universities, past position titles, and verbs associated with strong accomplishments.

. . .",project-academic
10.3389/FPUBH.2021.578832,2021-03-11,a,Front Public Health,use of connected technologies to assess barriers and stressors for age and disability friendly communities," Background: The benefits of engaging in outdoor physical activity are numerous for older adults. However, previous work on outdoor monitoring of physical activities did not sufficiently identify how older adults characterize and respond to diverse elements of urban built environments, including structural characteristics, safety attributes, and aesthetics. Objective: To synthesize emerging multidisciplinary trends on the use of connected technologies to assess environmental barriers and stressors among older adults and for persons with disability. Methods: A multidisciplinary overview and literature synthesis. Results: First, we review measurement and monitoring of outdoor physical activity in community environments and during transport using wearable sensing technologies, their contextualization and using smartphone-based applications. We describe physiological responses (e.g., gait patterns, electrodermal activity, brain activity, and heart rate), stressors and physical barriers during outdoor physical activity. Second, we review the use of visual data (e.g., Google street images, Street score) and machine learning algorithms to assess physical (e.g., walkability) and emotional stressors (e.g., stress) in community environments and their impact on human perception. Third, we synthesize the challenges and limitations of using real-time smartphone-based data on driving behavior, incompatibility with software data platforms, and the potential for such data to be confounded by environmental signals in older adults. Lastly, we summarize alternative modes of transport for older adults and for persons with disability. Conclusion: Environmental design for connected technologies, interventions to promote independence and mobility, and to reduce barriers and stressors, likely requires smart connected age and disability-friendly communities and cities.",project-academic
10.1109/AERO47225.2020.9172439,2020-03-01,p,IEEE,smart integrated management system smart cities epidemiological control tool using drones," This paper describes the development of a real application using Drones over urban regions to help the authorities at epidemiological control through a disruptive solutions based on a customizable Smart & Integrated Management System (SIGI), devices and software based on the Enterprise Resource Planning (ERP) concept. Compound by management software, Drones and specific IoT devices, both referred to as sensors, the sensors collect the data of the interest areas in real time, creating a specified database. Based on the data collected from the interest areas, SIGI software has the ability to show real-time situational analysis of these areas and allows that the administrator can optimize resources (material and human) improving the efficiency of resource allocation in these areas. In addition to the development of the management software, the development of sensors to collect the information in the field and update these information to the database of the management software, are considered. The sensors will be recognized as IoT devices for the collection of meteorological data, images and command / control Drones. Initially the system will be customized, using an Artificial Intelligence tool, to collect data and identify the outbreaks of the dengue mosquito, zika and Chikungunya, nominee by risk areas. After the definition of the potential risk areas, in a complementary way, a totally customized Drone will be used to map these areas of interest, generating aerial photographs, identifying and geotagging the potential “targets”, which will allow the agents to identify potential mosquito breeding sites. After the identification of breeding areas, the next step will be the effective combat of the vectors, using the Drones to fly over the areas of interest, where biological defenses will be “dropped” over the targets to combat mosquitoes. Due some Drone flight restrictions over the cities, the whole process will be monitored by a situation room, that will be able to control the Drone remotely, access the air space controller, reads the sensors installed in the city (field), that will measure, for example, rainfall through weather stations installed in risk areas and subsequently processed by Intelligent System Integrated Management (SIGI), which will result to the information public official reflecting the situational analysis of the areas, which will enable a better management of available resources, helping the public agent, preventively in the decision making.",project-academic
10.1109/VTC2020-FALL49728.2020.9348566,2020-11-01,p,IEEE,multi band sub ghz technology recognition on nvidia s jetson nano," Low power wide area networks support the success of long range Internet of things applications such as agriculture, security, smart cities and homes. This enormous popularity, however, breeds new challenging problems as the wireless spectrum gets saturated which increases the probability of collisions and performance degradation. To this end, smart spectrum decisions are needed and will be supported by wireless technology recognition to allow the networks to dynamically adapt to the ever changing environment where fair co-existence with other wireless technologies becomes essential. In contrast to existing research that assesses technology recognition using machine learning on powerful graphics processing units, this work aims to propose a deep learning solution using convolutional neural networks, cheap software defined radios and efficient embedded platforms such as NVIDIA’s Jetson Nano. More specifically, this paper presents low complexity near-real time multi-band sub-GHz technology recognition and supports a wide variety of technologies using multiple settings. Results show accuracies around 99%, which are comparable with state of the art solutions, while the classification time on a NVIDIA Jetson Nano remains small and offers real-time execution. These results will enable smart spectrum management without the need of expensive and high power consuming hardware.",project-academic
,2012-12-01,a,John Willey and Sons,autonomous learning systems from data to knowledge in real time," Autonomous Learning Systems is the result of over a decade of focused research and studies in this emerging area which spans a number of well-known and well-established disciplines that include machine learning, system identification, data mining, fuzzy logic, neural networks, neuro-fuzzy systems, control theory and pattern recognition. The evolution of these systems has been both industry-driven with an increasing demand from sectors such as defence and security, aerospace and advanced process industries, bio-medicine and intelligent transportation, as well as research-driven – there is a strong trend of innovation of all of the above well-established research disciplines that is linked to their on-line and real-time application; their adaptability and flexibility. Providing an introduction to the key technologies, detailed technical explanations of the methodology, and an illustration of the practical relevance of the approach with a wide range of applications, this book addresses the challenges of autonomous learning systems with a systematic approach that lays the foundations for a fast growing area of research that will underpin a range of technological applications vital to both industry and society. Key features: • Presents the subject systematically from explaining the fundamentals to illustrating the proposed approach with numerous applications. • Covers a wide range of applications in fields including unmanned vehicles/robotics, oil refineries, chemical industry, evolving user behaviour and activity recognition. • Reviews traditional fields including clustering, classification, control, fault detection and anomaly detection, filtering and estimation through the prism of evolving and autonomously learning mechanisms • Accompanied by a website hosting additional material, including the software toolbox and lecture notes Autonomous Learning Systems provides a ‘one-stop shop’ on the subject for academics, students, researchers and practicing engineers. It is also a valuable reference for Government agencies and software developers.",project-academic
10.1109/COMSNETS.2019.8711457,2019-01-01,p,IEEE,coordinated intelligent traffic lights using uppaal stratego," Automatic decision making in traffic signal controllers, semi-automated assistance to drivers, accident detection and response, anti-collision measures in autonomous driving etc., are relatively new applications in Intelligent Transport Systems (ITS). Recent developments in radar and sensor technology coupled with algorithmic and software advances brings ITS closer to realization. In this paper, we extend the work of Ericksen et. al., “Uppaal Stratego for Intelligent Traffic Lights”, in Proc. of the 12th Int. Conf. on ITS European Congress, 2017, France where they use a tool called UPPAAL STRATEGO to synthesize traffic light timing strategies through statistical model checking and machine learning. While Ericksen et.al. consider a single traffic light controller at an isolated intersection, we consider coordination between the controllers at two traffic intersections by providing a “green wave” in the heavily congested direction which reduces the overall waiting time of cars and queue length. Our experimental results show a significant improvement over uncoordinated isolated traffic light controllers in terms of the waiting time of cars and providing a new functionality of the controller such as giving a green wave.",project-academic
10.5194/ISPRS-ARCHIVES-XLII-2-W15-735-2019,2019-08-23,a,Copernicus GmbH,deep learning for semantic segmentation of 3d point cloud," Abstract. Cultural Heritage is a testimony of past human activity, and, as such, its objects exhibit great variety in their nature, size and complexity; from small artefacts and museum items to cultural landscapes, from historical building and ancient monuments to city centers and archaeological sites. Cultural Heritage around the globe suffers from wars, natural disasters and human negligence. The importance of digital documentation is well recognized and there is an increasing pressure to document our heritage both nationally and internationally. For this reason, the three-dimensional scanning and modeling of sites and artifacts of cultural heritage have remarkably increased in recent years. The semantic segmentation of point clouds is an essential step of the entire pipeline; in fact, it allows to decompose complex architectures in single elements, which are then enriched with meaningful information within Building Information Modelling software. Notwithstanding, this step is very time consuming and completely entrusted on the manual work of domain experts, far from being automatized. This work describes a method to label and cluster automatically a point cloud based on a supervised Deep Learning approach, using a state-of-the-art Neural Network called PointNet++. Despite other methods are known, we have choose PointNet++ as it reached significant results for classifying and segmenting 3D point clouds. PointNet++ has been tested and improved, by training the network with annotated point clouds coming from a real survey and to evaluate how performance changes according to the input training data. It can result of great interest for the research community dealing with the point cloud semantic segmentation, since it makes public a labelled dataset of CH elements for further tests.",project-academic
10.1126/SCIENCE.ABA3769,2019-12-06,a,American Association for the Advancement of Science,getting the epa back on track," ![][1] 

PHOTO: THOMAS LAVERGNE

Your information will be kept confidential, and the lessons learned from your participation will serve society—those are the promises made by researchers to participants in studies designed to inform environmental policies, from clean water and air to chemical exposure limits. The United States Environmental Protection Agency (EPA) may well break this fundamental pact next year, putting the agency at odds with its very mission “to protect human health and the environment.” Hopefully, the EPA will realize that this would jeopardize regulations that keep the environment safe to live in, and correct course back to sound policy-making.

In January 2020, the EPA plans to issue a supplement to its 2018 proposed rule, Strengthening Transparency in Regulatory Science, which stated that in setting standards, the agency would only use research for which underlying raw data and models were made public. The rule could eliminate many public health studies from consideration. At a congressional hearing last month, the EPA claimed that the supplemental rule provides clarifications, but does it address major problems with the plan? Although the notion of depositing data and models from federally funded research into public databases is laudable, the rule as proposed poses substantial problems. This may account for why the majority of nearly 600,000 public responses to the 2018 proposed rule were critical.

In epidemiological and clinical studies, people provide information—their medical histories, behaviors, education, employment, and other personal details—under the condition that it will not be shared and their privacy will be protected. Anonymizing data is already difficult, if not impossible. With geographically referenced data, a capable programmer can leverage machine learning and brute computational strength to determine the location, and subsequently the identity, of a study participant. Similarly, facial recognition software has been applied to images reconstructed from cranial scans to identify study participants. Reidentification can jeopardize employment, insurance, or personal relationships for individuals, and scholarship, reputation, or funding for researchers. This will simply discourage people from participating in future health studies. Moreover, successfully recruiting and retaining participants depends on trusting relationships built on meaningful and sustained interaction between researchers and participants, especially with disadvantaged populations who are underrepresented in research. The EPA rule assumes that people will consent to their data residing in a repository where decisions about data use are made by persons unknown to them.

The proposed rule claims that additional analysis of raw data and models will improve science. Who will do this analysis? Most likely, vested interests will finance work slanted toward a particular outcome, rather than undertake scientific inquiry without an agenda. For example, lead paint industry defense attorneys have attributed children's neurological deficits to landlord neglect and parental failure. The rule also disregards the power of the “weight of the evidence.” Imagine multiple studies done by different investigators on different populations using different techniques, yet reaching similar conclusions—that's a powerful result. Ignoring the weight of evidence derived from the totality of relevant science, regardless of data availability, contravenes the EPA's directive (stated in the Clean Air Act) to set standards “requisite to protect the public health” with “an adequate margin of safety.”

Many researchers already deposit code and data into open repositories. The U.S. National Institutes of Health and other federal funding agencies require data-sharing plans to support independent reanalysis within the scientific community without compromising confidentiality. The peer review process provides an additional check on the credibility of research results. Work by the Health Effects Institute, in which an industry-government–funded partnership reanalyzed data from the Harvard Six Cities Study and the American Cancer Society Study on the link between particulate matter pollution and mortality, represents an excellent model for evaluating the validity of research pivotal to environmental health regulations without compromising confidentiality or excluding studies.

The EPA's proposed transparency rule does not ensure research rigor or improve transparency. It unquestionably excludes key science from policy-making. Once the supplemental rule is released in January 2020, there will be an open period for public comment—an opportunity for everyone to remind the EPA of its obligation to use the best science, as required in multiple environmental laws, to protect human health and the environment.

 [1]: /embed/graphic-1.gif",project-academic
10.33012/2017.15291,2017-01-01,p,Institute of Navigation,nlos multipath detection by using machine learning in urban environments," In global navigation satellite system (GNSS) positioning, GNSS satellites are often obstructed by buildings, leading to reflected and diffracted signals, which are known as non-line-of-sight (NLOS) signals. These cause major positioning (also known as “NLOS multipath”) errors in GNSS positioning. This paper proposes a novel NLOS multipath detection technique that uses a machine-learning technique to improve positioning accuracy in urban environments. The key idea behind this technique is to construct a classifier that discriminates NLOS multipath signals from the output of the multiple GNSS signal correlators of a software GNSS receiver. In the code-tracking process within GNSS receivers, the code correlation peak is determined and tracked using the outputs of the signal correlators. In the case of an NLOS signal, there are no direct signals; the first reflected signal has low power compared to the direct signal. Hence, the correlation function is expected to be more distorted in the case of NLOS signal correlation. We use this phenomenon to detect NLOS signals. For realizing machine learning, we extract the features of the NLOS signal from the shape of the NLOS correlation function, using an actual dataset, to construct an NLOS classifier. To evaluate the proposed technique, we conduct NLOS classification experiments using signal correlation data acquired at different locations in the Shinjuku area of Japan. We propose to construct an NLOS classifier based on a support vector machine. From the experiments, 87% of the LOS signals and 99% of the NLOS signals are correctly discriminated.",project-academic
10.1016/J.MEASUREMENT.2019.03.032,2019-07-01,a,Elsevier,experimental characterisation of eye tracking sensors for adaptive human machine systems," Abstract None None Adaptive Human-Machine Interfaces and Interactions (HMI2) are closed-loop cyber-physical systems comprising a network of sensors measuring human, environmental and mission parameters, in conjunction with suitable software for adapting the HMI2 (command, control and display functions) in response to these real-time measurements. Cognitive HMI2 are a particular subclass of these systems, which support dynamic HMI2 adaptations based on the user’s cognitive state. These states are estimated in real-time using various neuro-physiological parameters from gaze, cardiorespiratory and brain signals, which are processed by an Adaptive Neuro-Fuzzy Inference System (ANFIS). However, the accuracy and precision of neuro-physiological measurements are affected by a variety of environmental factors and therefore need to be accurately characterised prior to operational use. This paper describes the characterisation activities performed on two types of eye tracking devices used in the Aerospace Intelligent and Autonomous Systems (AIAS) laboratory of RMIT University to support the development of cognitive human-machine systems. The uncertainty associated with the ANFIS outputs is quantified by propagating the uncertainties in the input data (determined experimentally) through the inference engine. This process is of growing relevance because similar machine learning techniques are now being developed for an increasing number of applications including aerospace, transport, biomedical and defence cyber-physical systems.",project-academic
10.1109/MWC.001.2000130,2020-12-01,a,IEEE,resource management in space air ground integrated vehicular networks sdn control and ai algorithm design," With its potential versatility and reliability, the space-air-ground integrated vehicular network (SAGVN) is envisioned as a promising solution to deliver quality vehicular services anywhere at any time. This article proposes a software defined framework for SAGVN to achieve flexible, reliable, and scalable network resource management. First, key applications and research challenges in resource management are identified. Then we propose a hybrid and hierarchical SAGVN control architecture to balance the trade-off between system status acquisition and signaling overhead in different scenarios. Considering the dynamic networking environment with multi-dimensional resources and diverse services, it is challenging to make optimal resource management decisions in real time; thus, artificial intelligence (AI)-based engineering solutions are investigated to facilitate efficient network slicing, mobility management, and cooperative content caching and delivery. A trace-driven case study is presented to demonstrate the effectiveness of the proposed SAGVN framework with AI-based methods in increasing the SAGVN throughput performance.",project-academic
10.1016/J.SCS.2020.102252,2020-09-01,a,Elsevier,a deep learning based iot oriented infrastructure for secure smart city," Abstract None None In recent years, the Internet of Things (IoT) infrastructures are developing in various industrial applications in sustainable smart cities and societies such as smart manufacturing, smart industries. The Cyber-Physical System (CPS) is also part of IoT-oriented infrastructure. CPS has gained considerable success in industrial applications and critical infrastructure with a distributed environment. This system aims to integrate the physical world to computational facilities as cyberspace. However, there are many challenges, such as security and privacy, centralization, communication latency, scalability in such an environment. To mitigate these challenges, we propose a Deep Learning-based IoT-oriented infrastructure for a secure smart city where Blockchain provides a distributed environment at the communication phase of CPS, and Software-Defined Networking (SDN) establishes the protocols for data forwarding in the network. A deep learning-based cloud is utilized at the application layer of the proposed infrastructure to resolve communication latency and centralization, scalability. It enables cost-effective, high-performance computing resources for smart city applications such as the smart industry, smart transportation. Finally, we evaluated the performance of our proposed infrastructure. We compared it with existing methods using quantitative analysis and security and privacy analysis with different measures such as scalability and latency. The evaluation of our implementation results shows that performance is improved.",project-academic
10.1016/J.DCAN.2017.10.002,2018-02-17,a,,machine learning for internet of things data analysis a survey," Rapid developments in hardware, software, and communication technologies have allowed the emergence of Internet-connected sensory devices that provide observation and data measurement from the physical world. By 2020, it is estimated that the total number of Internet-connected devices being used will be between 25 and 50 billion. As the numbers grow and technologies become more mature, the volume of data published will increase. Internet-connected devices technology, referred to as Internet of Things (IoT), continues to extend the current Internet by providing connectivity and interaction between the physical and cyber worlds. In addition to increased volume, the IoT generates Big Data characterized by velocity in terms of time and location dependency, with a variety of multiple modalities and varying data quality. Intelligent processing and analysis of this Big Data is the key to developing smart IoT applications. This article assesses the different machine learning methods that deal with the challenges in IoT data by considering smart cities as the main use case. The key contribution of this study is presentation of a taxonomy of machine learning algorithms explaining how different techniques are applied to the data in order to extract higher level information. The potential and challenges of machine learning for IoT data analytics will also be discussed. A use case of applying Support Vector Machine (SVM) on Aarhus Smart City traffic data is presented for a more detailed exploration.",project-academic
