id,datePublished,description,title,downloadUrl,publisher,doi,journals,database
334908525,2021-01-23T00:00:00,"With the development of deep representation learning, the domain of
reinforcement learning (RL) has become a powerful learning framework now
capable of learning complex policies in high dimensional environments. This
review summarises deep reinforcement learning (DRL) algorithms and provides a
taxonomy of automated driving tasks where (D)RL methods have been employed,
while addressing key computational challenges in real world deployment of
autonomous driving agents. It also delineates adjacent domains such as behavior
cloning, imitation learning, inverse reinforcement learning that are related
but are not classical RL algorithms. The role of simulators in training agents,
methods to validate, test and robustify existing solutions in RL are discussed.Comment: Accepted for publication at IEEE Transactions on Intelligent
  Transportation System",Deep Reinforcement Learning for Autonomous Driving: A Survey,http://arxiv.org/abs/2002.00444,,,,core
334903784,2020-01-17T00:00:00,"Visual object detection is a computer vision-based artificial intelligence
(AI) technique which has many practical applications (e.g., fire hazard
monitoring). However, due to privacy concerns and the high cost of transmitting
video data, it is highly challenging to build object detection models on
centrally stored large training datasets following the current approach.
Federated learning (FL) is a promising approach to resolve this challenge.
Nevertheless, there currently lacks an easy to use tool to enable computer
vision application developers who are not experts in federated learning to
conveniently leverage this technology and apply it in their systems. In this
paper, we report FedVision - a machine learning engineering platform to support
the development of federated learning powered computer vision applications. The
platform has been deployed through a collaboration between WeBank and Extreme
Vision to help customers develop computer vision-based safety monitoring
solutions in smart city applications. Over four months of usage, it has
achieved significant efficiency improvement and cost reduction while removing
the need to transmit sensitive data for three major corporate customers. To the
best of our knowledge, this is the first real application of FL in computer
vision-based tasks","FedVision: An Online Visual Object Detection Platform Powered by
  Federated Learning",http://arxiv.org/abs/2001.06202,,,,core
480043809,2021-01-01T00:00:00,"What we did This report assesses the potential of data-driven approaches to improving transport infrastructure maintenance. It looks at trends in maintenance strategies, explores how the targeted use of data could make them more effective for different types of transport infrastructure, and looks into implications for policy. The report builds on discussions held during workshops with members of the International Transport Forum’s Corporate Partnership Board. What we found Maintenance constitutes an inevitable, albeit often invisible, part of countries’ transport policies. Increased demand for transport infrastructure accelerates infrastructure’s ageing. The effects of climate change further aggravate this. Unsurprisingly, many governments look for transport infrastructure maintenance policies that provide better value for money than current practices offer. Infrastructure maintenance strategies are gradually shifting towards data-driven approaches. They exploit the power of digital technologies, Big Data analytics and advanced forecasting methodologies. Data-driven approaches have gained momentum in transport infrastructure maintenance as a result of four simultaneous technological innovations. First, the development of digital technologies has resulted in the digitalisation of society, industry and transport, which facilitates data sharing. Second, computing technologies have provided the necessary horsepower for running the digital infrastructure. Third, the Internet of Things and sensor technology have increased the potential for automating reporting from sensors that capture and measure new phenomena and provide data sets that flow through digital infrastructures. Fourth, artificial intelligence (AI) has helped to extract information from vast amounts of data, recognising patterns beyond the capacity of individual observation and exploiting digital infrastructure and computing power. Policy makers are beginning to leverage these developments in various ways. Data-driven maintenance is becoming common in many parts of the transport industry. Railroads collect massive amounts of inspection data from different sources using various methods, such as track inspection cars and drones that gather data to model track degradation. However, the rail sector faces numerous challenges for applying Big Data analysis: a lack of specific data analysis tools, high cost of involving stakeholders and heterogeneous data sources. Also, the algorithms currently used to predict the wear of rail infrastructure only work under lab conditions. For road infrastructure, various automated inspection methods exist. These include vision-based methods, laser scanning, ground penetration radar and a combination of these. All are accurate and effective but usually costly. As a result, the coverage and collection frequency can prove insufficient for detectingchanging road conditions. Several pilot studies have tried to use smartphones to collect data on the state of roads to reduce deployment costs for data-driven maintenance. At airports, the demand for accurate real-time data has spawned systems that automatically acquire and process infrastructure data. Advanced technologies now register when deformities develop on runways. They accurately measure moisture levels, temperature, strain and other factors relevant to wear and degradation. Several airports have built, or plan to build, concrete pavements with embedded strain gauges and other sensors to monitor the stress in the material caused by aircraft. Overall, data-driven approaches to infrastructure maintenance promise to enhance fact-based decision making and capabilities to predict the remaining useful life of assets. They can also improve cost efficiency and environmental sustainability. However, some new challenges need to be addressed, notably for the use of AI. AI predicts future behaviour based on historical data. Yet all predictions can prove incorrect where events do not follow past trends. What we recommendScale up and speed up the deployment of data-driven approaches to transport infrastructure maintenance Transport infrastructure maintenance could benefit from a broader and accelerated roll-out of data-driven approaches. These could improve the quality of assets, enhance the life cycles and save costs - especially when the relevant technologies are well-known, such as sensor technologies. In some cases, more tests and pilot projects will be useful, notably where leveraging data technologies for more effective maintenance policies poses specific challenges, as is the case of artificial intelligence in the railway sector. Update regulation and guidelines for transport infrastructure maintenance to facilitate the introduction of more data-driven approaches Current regulations and guidelines apply to condition-based maintenance strategies. These may set requirements that are ill-adapted to data-driven approaches to maintenance and may hamper their roll-out. Policy makers should ensure that the policies applied to data-driven approaches do not stifle their potential benefits. Ensure data-driven infrastructure maintenance approaches follow good practices in data governance The use of data in infrastructure maintenance must be in line with privacy protection laws and regulations. All data should be anonymised and encrypted. Location and trajectory data should be covered by the most robust protection methods, as they create the severest vulnerabilities for citizens. Tools to limit privacy risks include non-disclosure agreements between data users and providers, the involvement of trusted third parties to conduct the data collection and the development of “safe answers” approaches, in which only query results are exchanged instead of raw data. Governments could also broker data-sharing partnerships for the purpose of data-driven maintenance, for instance, between data providers and infrastructure managers. However, it may want to limit such partnerships to data of public interest and require purpose specificity and data minimisation",Data-driven Transport Infrastructure Maintenance,,,,,core
475649405,2020-09-23T00:00:00,"Self-driving cars and autonomous vehicles are revolutionizing the automotive sector, shaping the future of mobility altogether. Although the integration of novel technologies such as Artificial Intelligence (AI) and Cloud/Edge computing provides golden opportunities to improve autonomous driving applications, there is the need to modernize accordingly the whole prototyping and deployment cycle of AI components. This paper proposes a novel framework for developing so-called AI Inference Engines for autonomous driving applications based on deep learning modules, where training tasks are deployed elastically over both Cloud and Edge resources, with the purpose of reducing the required network bandwidth, as well as mitigating privacy issues. Based on our proposed data driven V-Model, we introduce a simple yet elegant solution for the AI components development cycle, where prototyping takes place in the cloud according to the Software-in-the-Loop (SiL) paradigm, while deployment and evaluation on the target ECUs (Electronic Control Units) is performed as Hardware-in-the-Loop (HiL) testing. The effectiveness of the proposed framework is demonstrated using two real-world use-cases of AI inference engines for autonomous vehicles, that is environment perception and most probable path prediction.</p",Cloud2Edge elastic AI framework for prototyping and deployment of AI inference engines in autonomous vehicles,,'MDPI AG',10.3390/s20195450,,core
237183624,2020-01-01T00:00:00,"Parking vehicle is a daunting task and a common problem in many cities around the globe. The search for parking space leads to congestion, frustration and increased air pollution. Information of a vacant parking space would facilitate to reduce congestion and subsequent air pollution. Therefore, aim of the paper is to acquire vehicle occupancy in an open parking lot using deep learning. Thermal camera was used to collect the data during varying environmental conditions such as; sunny, dusk, dawn, dark and snowy conditions. Vehicle detection with deep learning was implemented where image classification and object localization were performed for multi object detection. The dataset consists of 527 images which were manually labelled as there were no pre-labelled thermal images available. Multiple deep learning networks such as Yolo, ReNet18, ResNet50 and GoogleNet with varying layers and architectures were evaluated on vehicle detection. Yolo, GoogleNet and ResNet18 are computationally efficient detectors which took less processing time while Resnet50 produced better detection results compared to other detectors. However, ResNet18 also produced minimal miss rates and is suitable for real time vehicle detection. The detected results were compared with a template of parking spaces and IoU value is used to identify vehicle occupancy information",Deep learning-based vehicle occupancy detection in an open parking lot using thermal camera,,'Institution of Engineering and Technology (IET)',10.1049/iet-its.2019.0468,,core
479510128,2021-04-23T00:00:00,"Federated Learning (FL) is one of the leading learning paradigms for enabling a more significant presence of intelligent applications in networked and Internet of Things (IoT) systems. It consists of individual user devices performing machine learning (ML) models training locally, so that only trained models due to privacy concerns, but not raw data, is transferred through the network for aggregation at the edge or cloud data centers [Li et al. 2019]. Due to the pervasive presence of connected devices such as smart phones and IoT devices in peoples lives, there is a growing concern about how we can preserve and secure users’ information. FL reduces the risk of exposing user information to attackers during transmission over networks or information leakages at the central data centers. Another advantage of FL is scalability and maintainability of intelligent applications in networked and IoT systems. Considering highly distributed environments in which such systems are deployed, collecting and transmitting raw user data for training of ML models at central data centers is a challenging task as it imposes huge workload on the networks and consumes high bandwidth. Training of ML models is distributed over locations and transmitting the trained models for aggregation alleviates these challenges.



Among others, distributed and federated learning have applications in smart healthcare systems, where very sensitive user data is involved, and industrial IoT applications, where the amount of data for training may be too large and cumbersome to transport to central data centers. However, FL has the significant shortcoming of requiring user data to be Independent Identically Distributed (IID) (i.e., users which have similar data statistical distributions and are not mutually dependent) and make reliable predictions for a given group of users aggregated into a single model. IID users have similar statistical features, and thus can be aggregated into the same ML models. Since raw data is not available at the model aggregator, it is necessary to find IID users based solely on their trained machine learning models.



We present a Neural Network-based Federated Clustering mechanism capable of clustering IID with no access to their raw data called Neural-network SIMilarity estimator, NSIM. Such mechanism performs significantly better than competing techniques for neural-network clustering [Pacheco et al. 2021]. We also present an alternative to the FedAvg aggregation algorithm used in traditional FL, which significantly increases the aggregated models’ reliability in terms of Mean Square Error by creating several training models over IID users in a real-world mobility prediction dataset. We observe improvements of up to 97.52% in terms of Pearson correlation between the similarity estimation by NSIM and ground truth based on the LCSS (Longest Common Sub-Sequence) similarity metric, in comparison with other state-of-the-art approaches. Federated Clustering of IID data in different geographical locations can improve performance of early warning applications such as flood prediction [Samikwa et al. 2020], where the data for some locations may have more statistical similarities. We further present a technique for accelerating ML inference in resource-constrained devices through distributed computation of ML models over IoT networks, while preserving privacy. This has the potential to improve the performance of time sensitive ML applications",Distributed and Federated Learning Optimization with Federated Clustering of IID-users,https://core.ac.uk/download/479510128.pdf,,,,core
387288691,2020-12-06T00:00:00,"Autonomous navigation is a long-standing field of robotics research, which
provides an essential capability for mobile robots to execute a series of tasks
on the same environments performed by human everyday. In this chapter, we
present a set of algorithms to train and deploy deep networks for autonomous
navigation of mobile robots using the Robot Operation System (ROS). We describe
three main steps to tackle this problem: i) collecting data in simulation
environments using ROS and Gazebo; ii) designing deep network for autonomous
navigation, and iii) deploying the learned policy on mobile robots in both
simulation and real-world. Theoretically, we present deep learning
architectures for robust navigation in normal environments (e.g., man-made
houses, roads) and complex environments (e.g., collapsed cities, or natural
caves). We further show that the use of visual modalities such as RGB, Lidar,
and point cloud is essential to improve the autonomy of mobile robots. Our
project website and demonstration video can be found at
https://sites.google.com/site/autonomousnavigationros.Comment: 18 pages. arXiv admin note: substantial text overlap with
  arXiv:2007.1594","Autonomous Navigation with Mobile Robots using Deep Learning and the
  Robot Operating System",http://arxiv.org/abs/2012.02417,,,,core
334928216,2020-04-01T00:00:00,"Machine learning models are omnipresent for predictions on big data. One
challenge of deployed models is the change of the data over time, a phenomenon
called concept drift. If not handled correctly, a concept drift can lead to
significant mispredictions. We explore a novel approach for concept drift
handling, which depicts a strategy to switch between the application of simple
and complex machine learning models for regression tasks. We assume that the
approach plays out the individual strengths of each model, switching to the
simpler model if a drift occurs and switching back to the complex model for
typical situations. We instantiate the approach on a real-world data set of
taxi demand in New York City, which is prone to multiple drifts, e.g. the
weather phenomena of blizzards, resulting in a sudden decrease of taxi demand.
We are able to show that our suggested approach outperforms all regarded
baselines significantly","Handling Concept Drifts in Regression Problems -- the Error Intersection
  Approach",http://arxiv.org/abs/2004.00438,'GITO mbH Verlag',10.30844/wi_2020_c1-baier,,core
357337238,2020-04-03T00:00:00,"This paper investigates the use of an automatic system for preparation of gas mixtures in a multivariate calibration problem involving near-infrared (NIR) spectrometric analysis of natural gas. The automatic system is used to prepare calibration mixtures according to a Brereton experimental design, in order to exploit a suitable range of gas concentrations and thus avoid extrapolations in the predictions. These mixtures were employed to build partial-least-squares models for NIR determination of methane, ethane and propane, which are the major components of natural gas. Prediction performance was evaluated by using a separate set of prepared mixtures and natural gas samples with composition analyzed by gas chromatography, as well as a group of certified mixtures. The resulting root-mean-square errors of prediction (RMSEP) values for methane, ethane and propane (3.0, 0.9 and 1.2% mol mol -1 , respectively) were approximately 10 times smaller than the corresponding calibration ranges, with correlations of 0.91, 0.96 and 0.86 between the predicted and reference values. Keywords: natural gas analysis, automatic system for preparation of gas mixtures, multivariate calibration, NIR spectrometry, gas chromatography Introduction The analysis of chemical composition of gas samples is usually carried out by using gas chromatography (GC), which allows accurate determinations of individual gas components even in complex matrices. The widespread use of GC is motivated by the minimization of interference effects as the result of the separation in the chromatographic column. 1 However, the operational costs related to the use of consumables and the low sample throughput associated to the time required by the separation process are inconveniences that should be taken into account. In this context, spectrometric techniques have been proposed as a faster and less costly alternative for gas analysis, 2-4 provided that multivariate calibration is used to compensate for the absence of a separation process. 5 Multivariate calibration methods are aimed at obtaining a mathematical model that relates the instrumental measurements with the chemical composition of the sample. For this purpose, the analyst must gather a representative set of calibration samples with known composition. In the case of gas analysis, calibration mixtures with certified composition can be acquired from specialized suppliers. However, the acquisition of these mixtures can be expensive, which escapes the purpose of using a less costly alternative to GC. Alternatively, real samples with composition determined by GC can be used to build the multivariate calibration model, but the variability in the composition of these samples may not be large enough to build an appropriate model.  In this context, the present work investigates the use of an automatic system for accurate preparation of gas mixtures, which was proposed in a recent paper 7 as an improvement on a simpler architecture which had been developed for non-quantitative screening applications.  Use of an Automatic System in the Preparation of Gas Mixtures for Multivariate Calibration J. Braz. Chem. Soc.  2030    system comprises a set of gas admission valves which are controlled in an automatic manner to achieve the desired partial pressures for each component of the mixture. A piston-driven diaphragm pump is used to circulate the mixture within the system in order to obtain an appropriate homogenization. In Dantas et al.,   7 the operation of the system was validated by preparing binary mixtures of nitrogen with methane, ethane or propane. As a result, the programmed molar fractions of the component gases in the prepared mixtures were found to be in good agreement with the results of GC analysis. However, the system was not tested in an actual application involving the preparation of gas mixtures for multivariate calibration. Within this scope, the present investigation is aimed at demonstrating the applicability of this automatic system in an actual analytical problem involving the simultaneous determination of the major components in natural gas samples by using nearinfrared (NIR) spectrometry and multivariate calibration. Natural gas (NG) is mainly composed by methane (CH 4 ) and heavier hydrocarbons, especially ethane (C 2 H 6 ) and propane (C 3 H 8 ). 9 The development of analytical methods for quality control of this fuel has become an important issue, 10 in view of the growing demand for domestic, commercial, industrial, utility and vehicular use of NG, motivated by both economic gains and environmental impact. 11,12 Within this scope, NIR spectrometry has been proposed as an attractive alternative to the use of GC, with advantages including reduced analysis time and little sample preparation 2,13 in addition to the possibility of deploying portable field instruments. 5 More specifically, the use of NIR spectrometry has been reported for screening analysis 8 and determination of the calorific value of NG. 14 In a broader scope, applications have also been reported in the context of screening analysis of liquefied petroleum gas 15    and quantitative analysis of gases in hydrocarbon mixtures.  18 The prediction performance of the resulting model was evaluated by using a separate set of prepared mixtures, as well as three gas mixtures with certified composition and eight actual NG samples for vehicular use. Experimental Samples Methane (99.9%), ethane (99.0%), propane (99.5%), nitrogen (99.9%) and three mixtures of these gases, with certified composition, were acquired from Linde Gas. The certified mixtures were designed in order to simulate the composition of natural gas samples. All gas contents indicated herein are expressed in % mol mol -1 . In addition, eight real NG samples were acquired at 220 bar from vehicle fuelling stations in the city of João Pessoa (Paraíba, Brazil). These samples were collected by using a lab-made sampling cylinder described elsewhere.  After the NIR spectra of the 67 prepared mixtures were recorded, the Kennard-Stone algorithm 21 was employed to select 45 of these mixtures for use in the calibration of the PLS model. This algorithm is aimed at choosing a representative subset of samples in a near-uniform manner in the space of spectral variables, by avoiding the selection of samples with similar spectra. The remaining 22 mixtures were used as a separate prediction set, together with the 3 mixtures of certified composition and the 8 real NG samples. The composition of these 33 prediction samples was analyzed by GC, in order to evaluate the predictive ability of the PLS model.  Apparatus 8 In addition, the system was connected to a gas chromatograph for the analysis of the prediction samples. The NIR spectra of the samples were acquired by using an FTIR Analyzer (AIT, Analect Diamond 20) in the range 4,000-12,000 cm -1 as the average of 16 scans with a resolution of 2.0 cm -1 . The samples were introduced in Barbosa et al.   2031  Vol. 26, No. 10, 2015 the NIR flow cell at a pressure of 1.5 bar. The overall time required by the NIR analysis was one minute per sample. The experimental procedures were carried out in a laboratory environment with air conditioning (split configuration) and dehumidifier units for temperature and humidity control. The temperature and relative humidity were controlled during the analyses in order to remain within the ranges of 23 ± 1 °C and 55 ± 1%, respectively. The gas mixing system is not fitted with internal temperature sensors. However, the internal pressure is controlled by using a digital manometer with precision of ± 0.001 bar. The pressure measurements provided by the digital manometer are employed by the system software to control the admission of the components of the gas mixture, in order to achieve partial pressures corresponding to the desired molar fractions (% mol mol -1 ). Changes in the internal temperature of the system will not affect the results in a significant manner, because the preparation of the gas mixtures is based on the actual pressure values. The GC analyses were carried out by using a gas chromatograph (GC-2014, Shimadzu) using a 30-meter capillary column (GC-GASPRO) with internal diameter of 0.32 mm. The GC injections were performed in split mode (1:100) at a temperature of 240 °C by using a sampling valve (Valco E60) with a 25 microliter loop. Helium was used as carrier gas with a flow rate of 1.4 mL min -1 . All analyses were carried out in isothermal mode with the column temperature at 90 °C. A flame ionization detector (FID) was employed with temperature set at 250 °C. The total analysis time per run was 10 min. Software Spectral preprocessing, principal component analysis and PLS modelling were carried out by using The Unscrambler 9.7 (CAMO S.A.). The optimal number of factors for each PLS model was determined by using cross-validation with the default settings of the software package. The Kennard-Stone algorithm was implemented in Matlab R2010b. Results and Discussion After a preliminary inspection of the NIR spectra, the range 4,000-6,500 cm -1 was selected in view of its large signal-to-noise ratio compared to other spectral regions. The intervals 4,000-4,600 cm -1 and 5,500-6,500 cm -1 correspond to combination bands and first overtones of CH, CH 2 , CH 3 related to the main hydrocarbons (methane, ethane, propane) of the gas samples. 22,23 Figure 2a presents the NIR spectra of three mixtures prepared in this study. In order to remove the baseline features, first-derivative spectra were obtained by using the Savitzky-Golay method with a 2 nd order polynomial and a 3-point window.  An exploratory analysis of the spectral data was carried out by using principal component analysis (PCA). As can be seen in  The PLS models for methane, ethane and propane were built by using 1, 4 and 5 factors, respectively. The three elliptical joint confidence regions (EJCRs) (obtained on the basis of a linear regression between the reference and predicted gas concentrations) are presented in  As can be seen in  Conclusions This paper investigated the use of an automatic system for preparation of gas mixtures in a multivariate calibration problem involving NIR spectrometric analysis of natural gas. The use of prepared calibration mixtures is of value to form an adequate envelope around the samples to be analyzed, which is convenient to avoid extrapolations in the model predictions. For this purpose, the automatic system is convenient to reduce the manual workload in the preparation of the mixtures and to minimize the possibility of human errors. The NIR spectra of 45 prepared mixtures in the range 4,000-6,500 cm -1 was employed to build PLS models for determination of methane, ethane and propane, which are the major components of natural gas. The prediction performance of the resulting models was evaluated by using a separate set of 22 prepared mixtures and 8 natural gas samples, with composition analyzed by gas chromatography, as well as 3 certified mixtures. Only the results associated to reference values larger than the limit of quantification were considered.The resulting RMSEP values for methane, ethane and propane (3.0, 0.9 and 1.2% mol mol -1 , respectively) were approximately 10 times smaller than the corresponding calibration ranges, with correlations of 0.91, 0.96 and 0.86 between the predicted and reference values. No systematic error was observed. In addition, the prediction errors for the certified mixtures and real NG samples were comparable to the errors obtained for the prepared mixtures. The results of this investigation reveal that the automatic system for preparation of gas mixtures is indeed of value for use in multivariate calibration applications. Supplementary Information Supplementary data (tables of concentrations of the components in the mixtures) are available free of charge at http://jbcs.sbq.org.br as PDF file",Use of an Automatic System in the Preparation of Gas Mixtures for Multivariate Calibration: A Case Study Involving NIR Analysis of Natural Gas,,,,,core
416677994,2021-01-01T00:00:00,"The railway transport system is critical infrastructure that is exposed to numerous manmade and natural threats, thus protecting this physical asset is imperative. Cyber security, privacy, and dependability (SPD) are also important, as the railway operation relies on cyber-physical systems (CPS) systems. This work presents SPD-Safe—an administration framework for railway CPS, leveraging artificial intelligence for monitoring and managing the system in real-time. The network layer protections integrated provide the core security properties of confidentiality, integrity, and authentication, along with energy-aware secure routing and authorization. The effectiveness in mitigating attacks and the efficiency under normal operation are assessed through simulations with the average delay in real equipment being 0.2–0.6 s. SPD metrics are incorporated together with safety semantics for the application environment. Considering an intelligent transportation scenario, SPD-Safe is deployed on railway critical infrastructure, safeguarding one outdoor setting on the railway’s tracks and one in-carriage setting on a freight train that contains dangerous cargo. As demonstrated, SPD-Safe provides higher security and scalability, while enhancing safety response procedures. Nonetheless, emergence response operations require a seamless interoperation of the railway system with emergency authorities’ equipment (e.g., drones). Therefore, a secure integration with external systems is considered as future work",SPD-safe: Secure administration of railway intelligent transportation systems,https://core.ac.uk/download/416677994.pdf,'MDPI AG',10.3390/electronics10010092,,core
326517520,2021-01-11T00:00:00,"Since the past few decades, human trajectory forecasting has been a field of
active research owing to its numerous real-world applications: evacuation
situation analysis, deployment of intelligent transport systems, traffic
operations, to name a few. Early works handcrafted this representation based on
domain knowledge. However, social interactions in crowded environments are not
only diverse but often subtle. Recently, deep learning methods have
outperformed their handcrafted counterparts, as they learned about human-human
interactions in a more generic data-driven fashion. In this work, we present an
in-depth analysis of existing deep learning-based methods for modelling social
interactions. We propose two knowledge-based data-driven methods to effectively
capture these social interactions. To objectively compare the performance of
these interaction-based forecasting models, we develop a large scale
interaction-centric benchmark TrajNet++, a significant yet missing component in
the field of human trajectory forecasting. We propose novel performance metrics
that evaluate the ability of a model to output socially acceptable
trajectories. Experiments on TrajNet++ validate the need for our proposed
metrics, and our method outperforms competitive baselines on both real-world
and synthetic datasets.Comment: IEEE format, Layer-wise Relevance Propagation adde",Human Trajectory Forecasting in Crowds: A Deep Learning Perspective,http://arxiv.org/abs/2007.03639,,,,core
275657363,2022-11-30T00:00:00,"Increasing concern about global warming and air quality has meant an increasing use of energetic and environmental indicators in roundabout design. This research compares different suburban roundabouts in terms of traffic performance, pollutant and noise emissions through an integrated empirical assessment. Field measurements were carried out with a light duty vehicle in single-lane (SL), compact two-lane (CTL) and multi-lane (ML) roundabouts using Portable Emission Measurements Systems, OBD scan tool and Sound Level Meter, to measure real-world exhaust emissions, engine activity and acoustic data, respectively. Afterwards, predictive discrete models that correlate the probability of speed profiles (no stop, stop once and multiple stops) with roundabout operational parameters were developed. Although SL yielded the lowest CO2 per vehicle, its implementation resulted in high LAeq because vehicles drove at moderate speeds in the approach and low conflicting traffic compared to other layouts. CTL was the worst option in terms of both CO2 and NOX. The proposed methodology can be tailored in other roundabouts in suburban areas with similar layouts by simply identifying their traffic volumes and representative speed profiles. This can help researchers, traffic planners or practitioners to reduce congestion and emissions, and enhance road traffic management near urban areas.publishe","Impacts of roundabouts in suburban areas on congestion-specific vehicle speed profiles, pollutant and noise emissions: an empirical analysis",,'Elsevier BV',10.1016/j.scs.2020.102386,"[{'title': 'Sustainable Cities and Society', 'identifiers': ['2210-6707', 'issn:2210-6707']}]",core
326452957,2020-01-01T00:00:00,"Bike sharing systems with docking stations are widely deployed in many major cities, bringing convenience to citizens and promoting eco-friendly lifestyles. However, they are facing a common problem - the congestion or deficiency of bikes in docking stations due to fluctuation in bike usage. Inefciency in re-distributing bikes among docking stations is challenging for system operators. One approach to address such inefficiency is to rebalance bikes among docking stations with trucks. To allow researchers to study the efficiency of different rebalancing strategies under different conditions, this project aims to develop a simulation testbed. 

This paper presents Rebalancer, an AI-powered Shareable Bike Rebalancing System, which is capable of loading real-world bike sharing system datasets to simulate collective usage behaviours. It is integrated with well designed models for spatial-temporal traffic prediction, and is incorporated with a spatial-temporal rebalancing algorithm as a default approach for users to adjust and extend. It allows the user to interactively simulate and evaluate the dynamic rebalancing operations of shareable bikes, providing visualization of the AI decisions, the movement of bikes, and the trucks used to re-distributing bikes. Compared to other purely machine learning-based approaches, this testbed allows system operators to incorporate their preferences and business constraints into the rebalancing operations to be visualized and evaluated under realistic conditions.

Through simulation with London bike sharing system’s dataset retrieved from the Transport for London website, Rebalancer demonstrates effectiveness of the spatial-temporal rebalancing algorithm in reducing the demand and supply gap of bikes in docking stations. Experiments are also conducted to study the performance of different models in predicting traffic for each station. The results show LSTM yields the best performance, with lowest root-mean-square error and highest stability.Bachelor of Engineering (Computer Science",Development of an AI-powered shareable bike rebalancing system,,'Nanyang Technological University',,,core
443931735,2021-06-25T00:00:00,"We present a real-time neural radiance caching method for path-traced global
illumination. Our system is designed to handle fully dynamic scenes, and makes
no assumptions about the lighting, geometry, and materials. The data-driven
nature of our approach sidesteps many difficulties of caching algorithms, such
as locating, interpolating, and updating cache points. Since pretraining neural
networks to handle novel, dynamic scenes is a formidable generalization
challenge, we do away with pretraining and instead achieve generalization via
adaptation, i.e. we opt for training the radiance cache while rendering. We
employ self-training to provide low-noise training targets and simulate
infinite-bounce transport by merely iterating few-bounce training updates. The
updates and cache queries incur a mild overhead -- about 2.6ms on full HD
resolution -- thanks to a streaming implementation of the neural network that
fully exploits modern hardware. We demonstrate significant noise reduction at
the cost of little induced bias, and report state-of-the-art, real-time
performance on a number of challenging scenarios.Comment: To appear at SIGGRAPH 2021. 16 pages, 16 figure",Real-time Neural Radiance Caching for Path Tracing,http://arxiv.org/abs/2106.12372,'Association for Computing Machinery (ACM)',10.1145/3450626.3459812,,core
363636647,2020-09-08T00:00:00,"We introduce a permissioned distributed ledger technology (DLT) design for crowdsourced smart mobility applications. This architecture is based on a directed acyclic graph architecture (similar to the IOTA tangle) and uses both Proof-of-Work and Proof-of-Position mechanisms to provide protection against spam attacks and malevolent actors. In addition to enabling individuals to retain ownership of their data and to monetize it, the architecture is also suitable for distributed privacy-preserving machine learning algorithms, is lightweight, and can be implemented in simple internet-of-things (IoT) devices. To demonstrate its efficacy, we apply this framework to reinforcement learning settings where a third party is interested in acquiring information from agents. In particular, one may be interested in sampling an unknown vehicular traffic flow in a city, using a DLT-type architecture and without perturbing the density, with the idea of realizing a set of virtual tokens as surrogates of real vehicles to explore geographical areas of interest. These tokens, whose authenticated position determines write access to the ledger, are thus used to emulate the probing actions of commanded (real) vehicles on a given planned route by ``jumping'' from a passing-by vehicle to another to complete the planned trajectory. Consequently, the environment stays unaffected (i.e., the autonomy of participating vehicles is not influenced by the algorithm), regardless of the number of emitted tokens. The design of such a DLT architecture is presented, and numerical results from large-scale simulations are provided to validate the proposed approach",Spatial positioning token (SPToken) for smart mobility,,'Institute of Electrical and Electronics Engineers (IEEE)',10.1109/TITS.2020.3029537,"[{'title': 'IEEE Transactions on Intelligent Transportation Systems', 'identifiers': ['1524-9050', 'issn:1524-9050']}]",core
388548394,2021-02-28T00:00:00,"Intelligent Computing for Interactive System Design provides a comprehensive resource on what has become the dominant paradigm in designing novel interaction methods, involving gestures, speech, text, touch and brain-controlled interaction, embedded in innovative and emerging human–computer interfaces. These interfaces support ubiquitous interaction with applications and services running on smartphones, wearables, in-vehicle systems, virtual and augmented reality, robotic systems, the Internet of Things (IoT), and many other domains that are now highly competitive, both in commercial and in research contexts.



This book presents the crucial theoretical foundations needed by any student, researcher, or practitioner working on novel interface design, with chapters on statistical methods, digital signal processing (DSP), and machine learning (ML). These foundations are followed by chapters that discuss case studies on smart cities, brain–computer interfaces, probabilistic mobile text entry, secure gestures, personal context from mobile phones, adaptive touch interfaces, and automotive user interfaces. The case studies chapters also highlight an in-depth look at the practical application of DSP and ML methods used for processing of touch, gesture, biometric, or embedded sensor inputs. A common theme throughout the case studies is ubiquitous support for humans in their daily professional or personal activities.



In addition, the book provides walk-through examples of different DSP and ML techniques and their use in interactive systems. Common terms are defined, and information on practical resources is provided (e.g., software tools, data resources) for hands-on project work to develop and evaluate multimodal and multi-sensor systems. In a series of short additions to each chapter, an expert on the legal and ethical issues explores the emergent deep concerns of the professional community, on how DSP and ML should be adopted and used in socially appropriate ways, to most effectively advance human performance during ubiquitous interaction with omnipresent computers.



This carefully edited collection is written by international experts and pioneers in the fields of DSP and ML. It provides a textbook for students and a reference and technology roadmap for developers and professionals working on interaction design on emerging platforms","Intelligent computing for interactive system design: statistics, digital signal processing and machine learning in practice",,'Association for Computing Machinery (ACM)',10.1145/3447404,,core
365287462,2020-12-22T08:00:00,"As governments and private companies alike race to achieve the vision of a smart city — where artificial intelligence (AI) technology is used to enable self-driving cars, cashier-less shopping experiences and connected home devices from thermostats to robot vacuum cleaners — advancements are being made in both software and hardware to enable increasingly real-time, accurate inference at the edge. One hardware solution adopted for this purpose is the LiDAR sensor, which utilizes infrared lasers to accurately detect and map its surroundings in 3D. On the software side, developers have turned to artificial neural networks to make predictions and recommendations with high accuracy. These neural networks have the potential, particularly run on purpose-built hardware such as GPUs and TPUs, to make inferences in near real-time, allowing the AI models to serve as a usable interface for real-world interactions with other AI-powered devices, or with human users. This paper aims to example the joint use of LiDAR sensors and AI to understand its importance in smart city environments",LiDAR Object Detection Utilizing Existing CNNs for Smart Cities,,SJSU ScholarWorks,,,core
359042094,2020-11-27T07:38:40,"Mobility concerns most daily tasks (e.g., householding, shopping), affecting life quality. Gait speed, recognized as the sixth vital sign, is a key to characterize mobility. It is also a primary outcome of many clinical interventions. Monitoring gait in unsupervised free-living situations is crucial. It offers the possibility to assess purposeful gait (e.g., catching a bus) in contextual situations (e.g., socializing), multitasking conditions requiring attention, and where the activity is affected by environmental components (e.g., buildings, streets). 
The Global Navigation Sattelite System (GNSS) measures real-world gait speed, but it suffers from high power consumption and is available only outdoors. Multiple Inertial Measurement Units (IMU, including accelerometer and gyroscope) worn on the body could be used to estimate speed accurately. However, it is challenging and cumbersome to wear them every day. Another alternative is to use a single IMU, where the wrist and the Lower Back (LB) are recognized as appropriate sensor locations for real-life conditions. The Wrist-mounted IMU could be integrated inside a watch, thus, increasing user satisfaction. The LB-worn IMU could capture robust gait patterns even for patients and could be used to extract gait parameters like asymmetry. The wrist-based algorithms are mostly validated in supervised situations. They significantly lose their performance in daily life. While many LB-based methods exist, they have not been fully compared to determine what algorithms and under what criteria (slow, normal, and fast walkers) lead to better performance.
This thesis primarily presents accurate wrist-worn IMU-based (with barometer) speed (including cadence and step length) estimation and gait bout detection algorithms using Machine Learning (ML). An online personalization was devised in which the GNSS was sporadically used to capture a few speed data of a person's gait to tune the speed model gradually. Biomechanically-derived features were also extracted based on acceleration intensity, periodicity, noisiness, and wrist posture. The gait bout detection algorithm was validated against a multiple-IMU-based system for healthy people in unsupervised daily life. High sensitivity, specificity, accuracy were achieved (90, 97, 96 %). The personalized speed algorithm was also validated against GNSS for healthy subjects in real-world conditions, reaching an accuracy of 0.05 and 0.14 m/s for walking and running.
Furthermore, this thesis performs cross-validation on the LB-based algorithms to investigate the best algorithms for different speed ranges. Twenty-nine algorithms were organized in a conceptual framework, improved, and implemented. A novel combination technique was also proposed. The cross-validation against an instrumented mat and a multiple-IMU-based algorithm on both healthy and patient populations offered the combined approach as an accurate and robust solution with an error of 0.10 m/s. Finally, this thesis demonstrates the feasibility of using the proposed wrist-based algorithms for long-duration monitoring of gait in a large cohort study (around 2800 subjects). Results showed that the gait speed significantly improves frailty and handgrip strength estimations.
Overall, the proposed algorithms are independent of sensor orientation, thus, easy-to-use. A single IMU offers a high battery life and comfort, perfect for long-duration outdoors/indoors monitoring",Gait in real world: validated algorithms for gait periods and speed estimation using a single wearable sensor,,"Lausanne, EPFL",10.5075/epfl-thesis-8099,,core
392338934,2020-12-11T00:00:00,"Managing the city and the territories is a complex task requiring expertise in a broad range of fields. During the 21st century, the city and the territories must face major environmental and social challenges such as water scarcity, global warming, air quality and many others global issues. Physical modeling and numerical simulation are promising to give help decision tool to collectivities and to provide new services to citizens. In my research works, I have proposed adjoint numerical methods to limit the number of sensor to be deployed using optimal sensor placement strategy and inverse methods to get more representative numerical simulations describing urban physical phenomena. This transversal approach has been applied to various areas such as air and water quality, thermal building problem and structural health monitoring. Finally, experiments have been designed and conducted to test and to validate the proposed numerical methods using real measurements notably from the Equipment of Excellence Sense-City.La gestion de la ville et des territoires est une tâche complexe nécessitant de multiples compétences dans des domaines très variés. Au cours de ce XXIème siècle, la ville et les territoires doivent faire face à des enjeux majeurs sur le plan environnemental et sociétal : la rareté de l&apos;eau, le réchauffement climatique, la qualité de l&apos;air et bien d&apos;autres problématiques sont au coeur des préoccupations mondiales. La modélisation physique et la simulation numérique sont des outils prometteurs pour mieux gérer la ville et les territoires et fournir de nouveaux services aux citoyens. Dans mes travaux, j&apos;ai proposé des méthodes numériques adjointes de positionnement optimal de capteurs afin de déployer le minimum d&apos;instrumentation et des méthodes inverses afin d&apos;améliorer la qualité des simulations numériques décrivant les phénomènes physiques urbains. Cette approche transversale a été appliquée à de nombreux domaines comme la qualité de l&apos;air et de l&apos;eau, la thermique du bâtiment et la surveillance de la santé des structures. Enfin, des expérimentations ont été conçues et réalisées afin de tester et de valider les méthodes numériques proposées en exploitant des mesures réelles issues notamment de l&apos;équipement d&apos;excellence Sense-City",Méthodes numériques adjointes et inverses couplant mesures et modèles physiques au service des enjeux de la ville et des territoires,,HAL CCSD,,,core
387567742,2021-01-26T00:00:00,"O conceito de cidades inteligentes é uma tendência nas grandes cidades. Sistemas Inteligentes de Transporte desempenham um papel essencial no fornecimento de informações que possibilitam a previsão de tempos de viagem de ônibus. Informações precisas sobre tempos de viagem ajuda no planejamento dos passageiros e da agência responsável pelo transporte público. O objetivo deste trabalho é propor uma nova metodologia de previsão de tempos de viagem dos ônibus com base em dados abertos coletados em tempo real. A metodologia apresenta um processo para realizar predições precisas de tempos de viagem de ônibus, combinando um método de previsão estatística, um método de aprendizagem de máquina, e em conjunto com dados coletados em tempo real. Será apresentado todas as etapas do processo, incluindo a coleta de vários tipos de dados, armazenamento, análise do banco de dados, desenvolvimento e implementação das técnicas de aprendizado de máquina. Um banco de dados (dataset) foi construído a partir da coleta dos dados de geolocalização da frota de ônibus da cidade de São Paulo, dados de tráfego em tempo real, previsão de tráfego do Google Maps, dados meteorológicos e outros dados históricos. A seguir, treinamos uma Rede Neural Artificial (RNA). No processo de treinamento da RNA, alternamos o conjunto de dados e seus hiperparâmetros para descobrir a combinação que forneceu o menor erro de previsão. O erro médio percentual absoluto obtido foi de 9,10%, refletindo em uma raiz do erro quadrático médio de 297 segundos em uma linha que possui um tempo médio de viagem de 35 minutos. Esta pesquisa demonstrou que o método proposto forneceu uma previsão mais precisa do tempo de viagem de ônibus do que os métodos anteriores, a partir de dados da coletados em tempo real pela web.The concept of smart cities is a trend in big cities. Intelligent Transport Systems plays an essential role in providing information that enables bus travel times prediction. Accurate travel times information improves the planning of the passengers and the agency responsible for public transport. The objective of this work is to propose a new methodology for buses travel times prediction based on open data collected in real time. The methodology presents a process for predicting accurate bus travel times, combining a statistical forecasting method, a machine learning method, along with real time data collected. All steps of the process will be presented, including the collect process for many different types of data, storage, database analysis, development and implementation of machine learning techniques. A dataset was built by collecting the geolocation of the bus fleet in the city of São Paulo, real-time traffic data, traffic forecast from Google Maps, meteorological data and other historical data. Finally, we train an Artificial Neural Network (ANN). In the ANN training process, we alternate the dataset and its hyperparameters to find the combination that provided the most accurate prediction. The mean absolute percentage error obtained was 9.10%, reflecting a root mean square error of 297 seconds on a bus line that has an average travel time of 35 minutes. This research demonstrated that the proposed method provided a prediction of bus travel time more accurate than previous methods, based on data collected in real time over the web",Bus travel times prediction based on traffic data forecast and artificial neural networks.,,"'Universidade de Sao Paulo, Agencia USP de Gestao da Informacao Academica (AGUIA)'",10.11606/D.3.2020.tde-20012021-163526,,core
395064460,2020-01-01T00:00:00,"In  light  of  the  tremendous  advances  in  the  fields  of  unmanned  aerial  vehicles (UAVs) and imaging sensors in recent years, UAV-photogrammetry has become an essential part of remote sensing methodology. Being more than an alternative to conventional image acquisition platforms, UAV-photogrammetry has revealed novel possibilities and explored a variety of application fields, including the generation of high-quality 3D building models which are of growing importance in the area of 3D city modeling and civil engineering. Nevertheless, practical utilization of UAVs for the task of 3D modeling is still accompanied by various cumbersome activities, such as manual flight planning, deployment of ground control points (GCPs) and manual registration of  disconnected 3D models. To this end, this thesis aims to address key challenges in the process of using UAVs for photogrammetric applications and proposes several methods for advancing the state-of-the-art in different stages of UAV-based photogrammetry. Focusing on 3D modeling of buildings, this  thesis contributes methods for an automation of the reconstruction process ranging from (i) an accurate image-based multi-modal geo-referencing of acquired images, (ii) an automatic and semantic-aware 3D UAV image acquisition flight planning, (iii) an automatic alignment between individual 3D reconstructions of interior and exterior building models and (iv) a comprehensive investigation of current deep learning-based methods for the task of single-image depth estimation (SIDE), which could contribute to certain areas of image-based 3D building reconstruction. Based on the results of several real-world experiments, the proposed image matching method achieves pixel-level registration accuracies between UAV and multi-modal remote sensing imagery despite significant geometric, radiometric and temporal differences.The model-based 3D path planning method allows for acquiring close-range multi-view stereo-capable image sequences in tightly built-up environments that cover the entire building in a demanded resolution. By incorporating semantic cues into the path generation process, the resulting trajectories are by far more desirable in terms of flight safety by respecting pre-defined restricted and hazardous airspaces such as adjacent buildings or roads. The alignment of individual image-based indoor and outdoor building models is addressed by matching insufficiently overlapping geometric structures, which are shared in both models using 3D line segments as geometric features. A wide variety of experiments on different buildings have verified an accurate registration in centimeter-level accuracy. A comprehensive assessment of current SIDE  methods with novel evaluation metrics on a high-quality RGB-D dataset reveals their current suitability for potential practical application fields and emphasizes remaining challenges in this research field. Backed by thorough experimental evaluations confirming the validity of the proposed  methods, this thesis marks a step towards an automated, fast, accurate  and safe use of UAV photogrammetry",Automated and Precise 3D Building Reconstruction using UAVs,https://core.ac.uk/download/395064460.pdf,,,,core
477680155,2021-05-05T00:00:00,"Participant selection is a fundamental research issue in Mobile Crowdsensing (MCS). Previous approaches commonly assume that adequately long periods of candidate participants' historical mobility trajectories are available to model their patterns before the selection process, which is not realistic for some new MCS applications or platforms. The sparsity or even absence of mobility traces will incur inaccurate location prediction, thus undermining the deployment of new MCS applications. To this end, this paper investigates a novel problem called From-Scratch MCS (FS-MCS for short), in which we study how to intelligently select participants to minimize such cold-start effect. Specifically, we propose a novel framework based on reinforcement learning, named RL-Recruiter+. With the gradual accumulation of mobility trajectories over time, RL-Recruiter+ is able to make a good sequence of participant selection decisions for each sensing slot. Compared to its previous version, RL-Recruiter, Re-Recruiter+ jointly considers both the previous coverage and current mobility predictability when training the participant selection decision model. We evaluate our approach experimentally based on two real-world mobility datasets. The results demonstrate that RL-Recruiter+ outperforms the baseline approaches, including RL-Recruiter under various settings",RL-Recruiter+:Mobility-Predictability-Aware Participant Selection Learning for From-Scratch Mobile Crowdsensing,,'Institute of Electrical and Electronics Engineers (IEEE)',10.1109/TMC.2021.3077636,,core
479105843,2021-05-01T07:00:00,"The transportation system is rapidly evolving with new connected and automated vehicle (CAV) technologies that integrate CAVs with other vehicles and roadside infrastructure to form a transportation cyber-physical system (TCPS). Through connectivity, CAVs affect their environments and vice versa, increasing the size of the cyberattack surface and the risk of exploitation of security vulnerabilities by malicious actors. Thus, a greater understanding of potential CAV-TCPS cyber-attacks and of ways to prevent them is a high priority. Moreover, making the CAV navigate safely in an unexpected environment is a critical safety requirement. Considering the safety while maintaining the in-vehicle security is the focus of this study, where first, in part 1, the author explores the CAV safety through machine learning models, more specifically deep neural network, to help the vehicle to navigate safely in an unexpected environment, which is required for real-world deployment and has not been fully explored by researchers and industries. In part 2, the author developed a connected vehicle application development platform (CVDeP), such that developers can develop and validate the CAV safety and mobility applications in a controlled and real-world connected vehicle testbed. Our study shows that applications developed through the platform meet the safety requirements of connected vehicle applications. Later, in part 3, the author explores the in-vehicle security aspect, where the author leverages the state-of-the-art cloud supported quantum computers to classify in-vehicle cyberattacks, more specifically amplitude shift attacks. The author develop the quantum-classical hybrid neural network to detect amplitude shift in-vehicle cyberattack. This study integrates the digital infrastructure and a CAV’s in-vehicle system, where the author has shown the potential of using a combination of quantum and classical neural network to improve the cyberattack detection accuracy compared to classical neural network and quantum neural network alone",Improving the Security and Safety of Transportation Cyber-Physical Systems,https://core.ac.uk/download/479105843.pdf,Clemson University Libraries,,,core
475079083,2021-06-01T00:00:00,"Last-mile operations in forward and reverse logistics are responsible for a large part of the costs, emissions, and times in supply chains. These operations have increased due to the growth of electronic commerce and direct-to-consumer strategies. We propose a novel data- and model-driven framework to support decision making for urban distribution. The methodology is composed of diverse, hybrid, and complementary techniques integrated by a decision support system. This approach focuses on key elements of megacities such as socio-demographic diversity, portfolio mix, logistics fragmentation, high congestion factors, and dense commercial areas. The methodological framework will allow decision makers to create early warning systems and, with the implementation of optimization, machine learning, and simulation models together, make the best utilization of resources. The advantages of the system include flexibility in decision making, social welfare, increased productivity, and reductions in cost and environmental impacts. A real-world illustrative example is presented under conditions in one of the most congested cities: the megacity of Bogota, Colombia. Data come from a retail organization operating in the city. A network of stakeholders is analyzed to understand the complex urban distribution. The execution of the methodology was capable of solving a complex problem reducing the number of vehicles utilized, increasing the resource capacity utilization, and reducing the cost of operations of the fleet, meeting all constraints. These constraints included the window of operations and accomplishing the total number of deliveries. Furthermore, the methodology could accomplish the learning function using deep reinforcement learning in reasonable computational times. This preliminary analysis shows the potential benefits, especially in understudied metropolitan areas from emerging markets, supporting a more effective delivery process, and encouraging proactive, dynamic decision making during the execution stage",Data-Driven Methodology to Support Long-Lasting Logistics and Decision Making for Urban Last-Mile Operations,,'MDPI AG',10.3390/su13116230,"[{'title': 'Sustainability', 'identifiers': ['2071-1050', 'issn:2071-1050']}]",core
479583718,2021-01-01T00:00:00,"COVID-19 has reminded us of the significance of borders. In 1989, with the fall of the Berlin Wall, many predicted that sealed gates would soon become relics of a bygone era. Today, we find a different reality. Instead of disappearing, borders are transforming. In this article, we build upon the shifting border logic to explore how responses to the global pandemic have accelerated processes of detachment of mobility control from a fixed territorial marker. From global travel bans to mandating prearrival proof of a negative test result taken within 48 or 72 hours prior to departure to requiring digital registration of a passenger's travel history to enforcing strict post-arrival mandatory quarantine orders that arrest mobility, the shifting border paradigm has provided a template for policymakers to respond to a mounting global crisis. In addition to regulating movement across international borders and within countries, we trace the surprising return of subnational and interregional division lines in managing mobility, the erosion of the once taken for granted right to return to one’s home country, and the spatial and legal techniques used to block refugees from reaching terra firma during the pandemic. Next, we critically evaluate the authorization given under emergency regulations to deploy novel biometric and AI technologies, big data, and predictive algorithms to surveil moving bodies at real time and reprimand those deemed to have breached their quarantine or related governmental emergency measures. While drastic times call for drastic measures, techniques of movement control that ""scan"" and trace our bodies raise serious questions about justice, fairness, and the risk of discrimination, which may well remain with us even long after the pandemic is over",The Body as the Border: A New Era,https://doi.org/10.12759/hsr.46.2021.3.124-150,DEU,10.12759/hsr.46.2021.3.124-150,"[{'title': None, 'identifiers': ['0172-6404', 'issn:0172-6404']}]",core
387319756,2021-02-09T00:00:00,"Efficient optimization of resources is paramount to success in many problems
faced today. In the field of operational research the efficient scheduling of
employees; packing of vans; routing of vehicles; logistics of airlines and
transport of materials can be the difference between emission reduction or
excess, profits or losses and feasibility or unworkable solutions. The video
game Factorio, by Wube Software, has a myriad of problems which are analogous
to such real-world problems, and is a useful simulator for developing solutions
for these problems. In this paper we define the logistic transport belt problem
and define mathematical integer programming model of it. We developed an
interface to allow optimizers in any programming language to interact with
Factorio, and we provide an initial benchmark of logistic transport belt
problems. We present results for Simulated Annealing, quick Genetic Programming
and Evolutionary Reinforcement Learning, three different meta-heuristic
techniques to optimize this novel problem.Comment: Submitted to GECCO 202",The Factory Must Grow: Automation in Factorio,http://arxiv.org/abs/2102.04871,,,,core
387884113,2021-01-01T00:00:00,"We quantify the reductions in primary emissions due to the COVID-19 lockdowns in Europe. Our estimates are provided in the form of a dataset of reduction factors varying per country and day that will allow the modelling and identification of the associated impacts upon air quality. The country- and daily-resolved reduction factors are provided for each of the following source categories: energy industry (power plants), manufacturing industry, road traffic and aviation (landing and take-off cycle). We computed the reduction factors based on open-access and near-real-time measured activity data from a wide range of information sources. We also trained a machine learning model with meteorological data to derive weather-normalized electricity consumption reductions. The time period covered is from 21 February, when the first European localized lockdown was implemented in the region of Lombardy (Italy), until 26 April 2020. This period includes 5 weeks (23 March until 26 April) with the most severe and relatively unchanged restrictions upon mobility and socio-economic activities across Europe. The computed reduction factors were combined with the Copernicus Atmosphere Monitoring Service's European emission inventory using adjusted temporal emission profiles in order to derive time-resolved emission reductions per country and pollutant sector. During the most severe lockdown period, we estimate the average emission reductions to be −33 % for NOx, −8 % for non-methane volatile organic compounds (NMVOCs), −7 % for SOx and −7 % for PM2.5 at the EU-30 level (EU-28 plus Norway and Switzerland). For all pollutants more than 85 % of the total reduction is attributable to road transport, except SOx. The reductions reached −50 % (NOx), −14 % (NMVOCs), −12 % (SOx) and −15 % (PM2.5) in countries where the lockdown restrictions were more severe such as Italy, France or Spain. To show the potential for air quality modelling, we simulated and evaluated NO2 concentration decreases in rural and urban background regions across Europe (Italy, Spain, France, Germany, United-Kingdom and Sweden). We found the lockdown measures to be responsible for NO2 reductions of up to −58 % at urban background locations (Madrid, Spain) and −44 % at rural background areas (France), with an average contribution of the traffic sector to total reductions of 86 % and 93 %, respectively. A clear improvement of the modelled results was found when considering the emission reduction factors, especially in Madrid, Paris and London where the bias is reduced by more than 90 %. Future updates will include the extension of the COVID-19 lockdown period covered, the addition of other pollutant sectors potentially affected by the restrictions (commercial and residential combustion and shipping) and the evaluation of other air quality pollutants such as O3 and PM2.5. All the emission reduction factors are provided in the Supplement.The research leading to these results has received funding from the Copernicus Atmosphere Monitoring Service (CAMS), which is implemented by the European Centre for Medium-Range Weather Forecasts (ECMWF) on behalf of the European Commission. We acknowledge support from the Ministerio de Ciencia, Innovación y Universidades (MICINN) as part of the BROWNING project RTI2018-099894-B-I00 and NUTRIENT project CGL2017-88911-R, the Agencia Estatal de Investigacion (AEI) as part of the VITALISE project (PID2019-108086RA-I00/AEI/0.13039/501100011033), the AXA Research Fund, and the European Research Council (grant no. 773051, FRAGMENT). We also acknowledge PRACE and RES for awarding access to Marenostrum4 based in Spain at the Barcelona Supercomputing Center through the eFRAGMENT2 and AECT-2020-1-0007 projects. This project has also received funding from the European Union's Horizon 2020 research and innovation programme under the Marie Skłodowska-Curie grant agreement H2020-MSCA-COFUND-2016-754433. Carlos Pérez García-Pando also acknowledges support received through the Ramón y Cajal programme (grant RYC-2015-18690) of the MICINN.Peer ReviewedPostprint (published version",Time-resolved emission reductions for atmospheric chemistry modelling in Europe during the COVID-19 lockdowns,,'Copernicus GmbH',10.5194/acp-21-773-2021,"[{'title': None, 'identifiers': ['1680-7316', 'issn:1680-7316']}]",core
333965628,2020-06-26T00:00:00,"Background. There is a gap for the effective use of mathematical models for real-time decision-making. We aimed to illustrate with the Cuban experience to control the COVID-19, how mathematical models can be put in place to answer key decision-makers´ questions.

Methods. A science-policy partnership was created to mutually define questions, communicate results and facilitate the translation of modeling advice into actions. For forecasting and planning at national level mechanistic models and machine learning based on the epidemic patterns in other countries were used. Statistical models to explain the variability of transmission was used to stratify control actions. The effect of interventions was assessed using branching process models, time varying reproduction number (Rt) and social mixing patterns by location, and by age group.

Findings. The mathematical approach implemented contribute to successful control of the COVID-19 in Cuba. The urbanization, living conditions and the economic index explain the 73% of the variability of the transmission at provincial level. Increased risk of transmission were identified in 33 municipalities mostly in densely populated urban areas with high aging index. Control intervention reduced the transmission from R0=2.84 (95% CI: 1.52 - 4.76) to Rt=0.6 (95% CI:0.2-2.38 ). The highest transmission was detected among adolescents and from people older than 60 years.

Conclusions. Understanding the key questions for decision-making at all times, translating problems into a mathematical language, integrating different approaches to their solution and being able to present the results in an easy-to-understand way is vital to have a timely impact on controlling the epidemic",How Mathematical Approaches Could Help Decision-Making to Epidemic Control? The Successful Experience against COVID-19 in Cuba,https://core.ac.uk/download/333965628.pdf,,,,core
343502708,2020-09-24T00:00:00,"5G cellular communication, especially with its hugely available bandwidth provided by millimeter-wave, is a promising technology to fulfill the coming high demand for vast data rates. These networks can support new use cases such as Vehicle to Vehicle and augmented reality due to its novel features such as network slicing along with the mmWave multi-gigabit-per-second data rate. Nevertheless, 5G cellular networks suffer from some shortcomings, especially in high frequencies because of the intermittent nature of channels when the frequency rises. Non-line of sight state, is one of the significant issues that the new generation encounters. This drawback is because of the intense susceptibility of higher frequencies to blockage caused by obstacles and misalignment. This unique characteristic can impair the performance of the reliable transport layer widely deployed protocol, TCP, in attaining high throughput and low latency throughout a fair network. As a result, the protocol needs to adjust the congestion window size based on the current situation of the network. However, TCP is not able to adjust its congestion window efficiently, and it leads to throughput degradation of the protocol. This paper presents a comprehensive analysis of reliable end-to-end communications in 5G networks. It provides the analysis of the effects of TCP in 5G mmWave networks, the discussion of TCP mechanisms and parameters involved in the performance over 5G networks, and a survey of current challenges, solutions, and proposals. Finally, a feasibility analysis proposal of machine learning-based approaches to improve reliable end-to-end communications in 5G networks is presented.This work was supported by the Secretaria d’Universitats i Recerca del Departament d’Empresa i Coneixement de la Generalitat de
Catalunya under Grant 2017 SGR 376.Peer ReviewedPostprint (published version",Challenges on the way of implementing TCP over 5G networks,,'Institute of Electrical and Electronics Engineers (IEEE)',10.1109/ACCESS.2020.3026540,"[{'title': 'IEEE Access', 'identifiers': ['issn:2169-3536', '2169-3536']}]",core
334861795,2020-01-14T00:00:00,"Natural muscles provide mobility in response to nerve impulses.
Electromyography (EMG) measures the electrical activity of muscles in response
to a nerve's stimulation. In the past few decades, EMG signals have been used
extensively in the identification of user intention to potentially control
assistive devices such as smart wheelchairs, exoskeletons, and prosthetic
devices. In the design of conventional assistive devices, developers optimize
multiple subsystems independently. Feature extraction and feature description
are essential subsystems of this approach. Therefore, researchers proposed
various hand-crafted features to interpret EMG signals. However, the
performance of conventional assistive devices is still unsatisfactory. In this
paper, we propose a deep learning approach to control prosthetic hands with raw
EMG signals. We use a novel deep convolutional neural network to eschew the
feature-engineering step. Removing the feature extraction and feature
description is an important step toward the paradigm of end-to-end
optimization. Fine-tuning and personalization are additional advantages of our
approach. The proposed approach is implemented in Python with TensorFlow deep
learning library, and it runs in real-time in general-purpose graphics
processing units of NVIDIA Jetson TX2 developer kit. Our results demonstrate
the ability of our system to predict fingers position from raw EMG signals. We
anticipate our EMG-based control system to be a starting point to design more
sophisticated prosthetic hands. For example, a pressure measurement unit can be
added to transfer the perception of the environment to the user. Furthermore,
our system can be modified for other prosthetic devices.Comment: Conference. Houston, Texas, USA. September, 201","Deep learning approach to control of prosthetic hands with
  electromyography signals",http://arxiv.org/abs/1909.09910,'Institute of Electrical and Electronics Engineers (IEEE)',10.1109/ISMCR47492.2019.8955725,,core
334868855,2020-03-02T00:00:00,"Visual navigation tasks in real-world environments often require both
self-motion and place recognition feedback. While deep reinforcement learning
has shown success in solving these perception and decision-making problems in
an end-to-end manner, these algorithms require large amounts of experience to
learn navigation policies from high-dimensional data, which is generally
impractical for real robots due to sample complexity. In this paper, we address
these problems with two main contributions. We first leverage place recognition
and deep learning techniques combined with goal destination feedback to
generate compact, bimodal image representations that can then be used to
effectively learn control policies from a small amount of experience. Second,
we present an interactive framework, CityLearn, that enables for the first time
training and deployment of navigation algorithms across city-sized, realistic
environments with extreme visual appearance changes. CityLearn features more
than 10 benchmark datasets, often used in visual place recognition and
autonomous driving research, including over 100 recorded traversals across 60
cities around the world. We evaluate our approach on two CityLearn
environments, training our navigation policy on a single traversal. Results
show our method can be over 2 orders of magnitude faster than when using raw
images, and can also generalize across extreme visual changes including day to
night and summer to winter transitions.Comment: Preprint version of article accepted to ICRA 202","CityLearn: Diverse Real-World Environments for Sample-Efficient
  Navigation Policy Learning",http://arxiv.org/abs/1910.04335,,,,core
440321756,2021-06-18T00:00:00,"The utilization of computer technology to solve problems in medical scenarios
has attracted considerable attention in recent years, which still has great
potential and space for exploration. Among them, machine learning has been
widely used in the prediction, diagnosis and even treatment of Sepsis. However,
state-of-the-art methods require large amounts of labeled medical data for
supervised learning. In real-world applications, the lack of labeled data will
cause enormous obstacles if one hospital wants to deploy a new Sepsis detection
system. Different from the supervised learning setting, we need to use known
information (e.g., from another hospital with rich labeled data) to help build
a model with acceptable performance, i.e., transfer learning. In this paper, we
propose a semi-supervised optimal transport with self-paced ensemble framework
for Sepsis early detection, called SPSSOT, to transfer knowledge from the other
that has rich labeled data. In SPSSOT, we first extract the same clinical
indicators from the source domain (e.g., hospital with rich labeled data) and
the target domain (e.g., hospital with little labeled data), then we combine
the semi-supervised domain adaptation based on optimal transport theory with
self-paced under-sampling to avoid a negative transfer possibly caused by
covariate shift and class imbalance. On the whole, SPSSOT is an end-to-end
transfer learning method for Sepsis early detection which can automatically
select suitable samples from two domains respectively according to the number
of iterations and align feature space of two domains. Extensive experiments on
two open clinical datasets demonstrate that comparing with other methods, our
proposed SPSSOT, can significantly improve the AUC values with only 1% labeled
data in the target domain in two transfer learning scenarios, MIMIC
$rightarrow$ Challenge and Challenge $rightarrow$ MIMIC.Comment: 14 pages, 9 figure","Semi-supervised Optimal Transport with Self-paced Ensemble for
  Cross-hospital Sepsis Early Detection",http://arxiv.org/abs/2106.10352,,,,core
426867654,2020-12-01T00:00:00,"Continuous exposure to urban noise has been found to be one of the major threats to citizens’ health. In this regard, several organizations are devoting huge efforts to designing new in-field systems to identify the acoustic sources of these threats to protect those citizens at risk. Typically, these prototype systems are composed of expensive components that limit their large-scale deployment and thus reduce the scope of their measurements. This paper aims to present a highly scalable low-cost distributed infrastructure that features a ubiquitous acoustic sensor network to monitor urban sounds. It takes advantage of (1) low-cost microphones deployed in a redundant topology to improve their individual performance when identifying the sound source, (2) a deep-learning algorithm for sound recognition, (3) a distributed data-processing middleware to reach consensus on the sound identification, and (4) a custom planar antenna with an almost isotropic radiation pattern for the proper node communication. This enables practitioners to acoustically populate urban spaces and provide a reliable view of noises occurring in real time. The city of Barcelona (Spain) and the UrbanSound8K dataset have been selected to analytically validate the proposed approach. Results obtained in laboratory tests endorse the feasibility of this proposal",Low-Cost Distributed Acoustic Sensor Network for Real-Time Urban Sound Monitoring,,'MDPI AG',,,core
389432006,2021-01-01T00:00:00,"Public transportation provides a major contribution to the mobility of modern societies.
Passengers and customers of railway transportation expect a safe, timely, and comfortable service.
To meet this demand, safety engineers have established strategies over decades to remove risks and increase safety that become manifest in railway signalling systems which are fundamental to today's safe train operation.
Over the recent years, digitalisation found its way into the signalling systems but unfortunately at the cost of making them vulnerable to cybersecurity threats.
The vulnerabilities can as well affect the safety of train operation and eventually be the root cause for train accidents with potentially severe and tragic consequences.
Hence, scientists, engineers, and practitioners dealing with railway signalling unequivocally agree that a system that is not secure can not be safe.
However, how the cybersecurity protection of safety systems in the railway domain should be shaped is subject of current discussion between domain experts.
System architectures are being proposed and tested, risk assessment methods are discussed, security controls are selected and integrated, national and international standards are written, and the interplay and integration of safety and security measures is studied.
Fortunately, railway transportation has so far been spared from major cyberattacks.
But more and more incidents in other critical infrastructures become public and combining the potential harm, the importance for the society and ongoing digitalisation, railway transportation is becoming an attractive goal for adversaries of various kinds.
The dissertation at hand contributes to the research in safety and security co-engineering.

We begin to analyse the security requirements of the safety-critical railway signalling system and building on that propose a new security architecture.
The security architecture has the advantage that it serves as the platform of safety and security functionality at the same time.
The necessary separation between safety and security is moved from the physical world to a virtual environment such that the available attack surface is reduced.
We proceed to investigate the interplay of safety and security examining security controls that can be deployed in the architecture.
First, we analyse a safe transport protocol and enhance it to provide cryptographically secure message authenticity.
Then, we propose two intrusion detection and prevention schemes to protect railway signalling against semantic attacks.
Semantic attacks are typically executed by sophisticated adversaries who exploit detailed knowledge of the controlled system's behaviour to provoke respectively serious damage and consequences.
Therefore, it is inevitable to combine the security defence strategies with the safety principles of railway signalling.
For the first scheme we encode the principles in a way that enables the actuators to distributedly validate their actions themselves and couple security with safety by allowing it to intervene in the safety communication within a controlled framework.
In the second proposed scheme, we consult artificial neural networks and train them on normal, incident free command and control communication to implicitly learn a model of the safety principles.
Similarly, we allow the scheme to intervene in the safety communication to make the signalling system more resilient against semantic attacks.
Finally, from the experience we gathered, we develop a methodology to deploy security controls in the immediate proximity of safety systems generalised as sensor-actuator cyber-physical systems and not limited to railway signalling.
Core of the methodology is the active transformation of a security incident to a safety hazard by the detecting security control.
The methodology is as well suitable to be applied to the security architecture we present in the beginning and in this way contributes towards making safety-critical systems more secure and hence more safe",Security Engineering in Safety-critical Railway Signalling,,,10.26083/tuprints-00013484,,core
388992081,2020-01-23T00:00:00,"Introduction:&nbsp;The article is the product of the research “Due to the increase in popularity of Internet of Things (IoT), a huge amount of sensor data is being generated from various smart city applications”, developed at Pondicherry University in the year 2019.
Problem:To acquire and analyze the huge amount of sensor-generated data effectively is a significant problem when processing the data.
Objective:&nbsp; To propose a novel framework for IoT sensor data analysis using machine learning based improved Gaussian Mixture Model (GMM) by acquired real-time data.&nbsp;
Methodology:In this paper, the clustering based GMM models are used to find the density patterns on a daily or weekly basis for user requirements. The ThingSpeak cloud platform used for performing analysis and visualizations.
Results:An analysis has been performed on the proposed mechanism implemented on real-time traffic data with Accuracy, Precision, Recall, and F-Score as measures.
Conclusions:The results indicate that the proposed mechanism is efficient when compared with the state-of-the-art schemes.
Originality:Applying GMM and ThingSpeak Cloud platform to perform analysis on IoT real-time data is the first approach to find traffic density patterns on busy roads.
Restrictions:There is a need to develop the application for mobile users to find the optimal traffic routes based on density patterns. The authors could not concentrate on the security aspect for finding density patterns.Introducción: el artículo es producto de la investigación ""Debido al aumento en la popularidad de Internet de las cosas (IoT), se está generando una gran cantidad de datos de sensores a partir de varias aplicaciones de ciudades inteligentes"", desarrollado en la Universidad de Pondicherry en el año 2019.
Problema: adquirir y analizar datos generados por sensores de manera efectiva pues es un problema importante al procesar los datos.
Objetivo: proponer un marco novedoso para el análisis de datos del sensor IoT utilizando el aprendizaje automático basado en mejoras desde el Modelo de mezcla gaussiana (GMM) por datos adquiridos en tiempo real.
Metodología: en este documento, los modelos GMM basados en agrupamiento se utilizan para encontrar los patrones de densidad en un día o semanalmente para los requisitos del usuario. La plataforma en la nube ThingSpeak utilizada para realizar análisis y visualizaciones.
Resultados: se realizó un análisis sobre el mecanismo propuesto implementado en datos de tráfico en tiempo real con precisión, recuperación y F-Score como medidas.
Conclusiones: los resultados indican que el mecanismo propuesto es eficiente en comparación con el estado de esquemas de arte.
Originalidad: la aplicación de la plataforma GMM y ThingSpeak Cloud para realizar análisis de datos en tiempo real de IoT es el primer enfoque para encontrar patrones de densidad de tráfico en carreteras transitadas.
Limitación: existe la necesidad de desarrollar la aplicación para que los usuarios móviles encuentren las rutas de tráfico óptimas basadas en patrones de densidad. Los autores no pudieron desarrollar el aspecto de seguridad para encontrar patrones de densidad",Aprendizaje automático basado en mezcla Gaussiana mejorada Modelo para IoT en tiempo real: Análisis de los datos,https://core.ac.uk/download/388992081.pdf,'Universidad Cooperativa de Colombia- UCC',10.16925/2357-6014.2020.01.02,,core
334951331,2020-09-01T00:00:00,"The recent decline in the number of police and security force personnel has raised a serious security issue that could lead to reduced public safety and delayed response to crimes in urban areas. This may be alleviated in part by utilizing micro or small unmanned aerial vehicles (UAVs) and their high-mobility on-board sensors in conjunction with machine-learning techniques such as neural networks to offer better performance in predicting times and places that are high-risk and deterring crimes. The key to the success of such operation lies in the suitable placement of UAVs. This paper proposes a multi-UAV allocation framework for predictive crime deterrence and data acquisition that consists of the overarching methodology, a problem formulation, and an allocation method that work with a prediction model using a machine learning approach. In contrast to previous studies, our framework provides the most effective arrangement of UAVs for maximizing the chance to apprehend offenders whilst also acquiring data that will help improve the performance of subsequent crime prediction. This paper presents the system architecture assumed in this study, followed by a detailed description of the methodology, the formulation of the problem, and the UAV allocation method of the proposed framework. Our framework is tested using a real-world crime dataset to evaluate its performance with respect to the expected number of crimes deterred by the UAV patrol. Furthermore, to address the engineering practice of the proposed framework, we discuss the feasibility of the simulated deployment scenario in terms of energy consumption and the relationship between data analysis and crime prediction",Multi-UAV Allocation Framework for predictive crime deterrence and data acquisition,https://core.ac.uk/download/334951331.pdf,'Elsevier BV',,,core
479166135,2021-01-01T00:00:00,"Air pollution introduces a major challenge for societies, where it leads to the premature deaths of millions of people each year globally. Massive deployment of air quality sensing devices and data analysis for the resultant data will pave the way for the development of real-time intelligent applications and services, e.g., minimization of exposure to poor air quality either on an individual or city scale. 5G and edge computing supports dense deployments of sensors at high resolution with ubiquitous connectivity, high bandwidth, high-speed gigabit connections, and ultralow latency analysis. This article conceptualizes AI-powered scalable air quality monitoring and presents two systems of calibrating low-cost air quality sensors and the image processing of pictures captured by hyperspectral cameras to better detect air quality. We develop and deploy different AI algorithms in these two systems on a 5G edge testbed and perform a detailed analytics regarding to 1) the performance of AI algorithms and 2) the required communication and computation resources.Peer reviewe",Intelligent and Scalable Air Quality Monitoring with 5G Edge,https://core.ac.uk/download/479166135.pdf,'Institute of Electrical and Electronics Engineers (IEEE)',10.1109/MIC.2021.3059189,"[{'title': 'IEEE Internet Computing', 'identifiers': ['1089-7801', 'issn:1089-7801']}]",core
475663426,2021-01-01T00:00:00,"The progress in technology development over the past decades, both with respect to software and hardware, offers the vision of automated vehicles as means of achieving zero fatalities in traffic. However, the promises of this new technology – an increase in road safety, traffic efficiency, and user comfort – can only be realized if this technology is smoothly introduced into the existing traffic system with all its complexities, constraints, and requirements. SHAPE- IT will contribute to this major undertaking by addressing research questions relevant for the development and introduction of automated vehicles in urban traffic scenarios. Previous research has pointed out several research areas that need more attention for a successful implementation and deployment of human-centred vehicle automation in urban environments. In SHAPE-IT, for example, a better understanding of human behaviour and the underlying psychological mechanisms will lead to improved models of human behaviour that can help to predict the effects of automated systems on human behaviour already during system development. Such models can also be integrated into the algorithms of automated vehicles, enabling them to better understand the human interaction partners’ behaviours. Further, the development of vehicle automation is much about technology (software and hardware), but the users will be humans and they will interact with humans both inside and outside of the vehicle. To be successful in the development of automated vehicles functionalities, research must be performed on a variety of aspects. Actually, a highly interdisciplinary team of researchers, bringing together expertise and background from various scientific fields related to traffic safety, human factors, human-machine interaction design and evaluation, automation, computational modelling, and artificial intelligence, is likely needed to consider the human-technology aspects of vehicle automation. Accordingly, SHAPE-IT has recruited fifteen PhD candidates (Early Stage Researchers – ESRs), that work together to facilitate this integration of automated vehicles into complex urban traffic by performing research to support the development of transparent, cooperative, accepted, trustworthy, and safe automated vehicles. With their (and their supervisors’) different scientific background, the candidates bring different theoretical concepts and methodological approaches to the project. This interdisciplinarity of the project team offers the unique possibility for each PhD candidate to address research questions from a broad perspective – including theories and methodological approaches of other interrelated disciplines. This is the main reason why SHAPE-IT has been funded by the European Commission’s Marie Skłodowska-Curie Innovative Training Network (ITN) program that is aimed to train early state researchers in multidisciplinary aspects of research including transferable skills. With the unique scope of SHAPE-IT, including the human-vehicle perspective, considering different road-users (inside and outside of the vehicle), addressing for example trust, transparency, and safety, and including a wide range of methodological approaches, the project members can substantially contribute to the development and deployment of safe and appreciated vehicle automation in the cities of the future. To achieve the goal of interdisciplinary research, it is necessary to provide the individual PhD candidate with a starting point, especially on the different and diverse methodological approaches of the different disciplines. The empirical, user-centred approach for the development and evaluation of innovative automated vehicle concepts is central to SHAPE- IT. This deliverable (D1.1 “Methodological Framework for Modelling and Empirical Approaches”) provides this starting point. That is, this document provides a broad overview of approaches and methodologies used and developed by the SHAPE-IT ESRs during their research. The SHAPE-IT PhD candidates, as well as other researchers and developers outside of SHAPE-IT, can use this document when searching for appropriate methodological approaches, or simply get a brief overview of research methodologies often employed in automated vehicle research. The first chapter of the deliverable shortly describes the major methodological approaches to collect data relevant for investigating road user behaviour. Each subchapter describes one approach, ranging from naturalistic driving studies to controlled experiments in driving simulators, with the goal to provide the unfamiliar reader with a broad overview of the approach, including its scope, the type of data collected, and its limitations. Each subchapter ends with recommendations for further reading – literature that provide much more detail and examples. The second chapter explains four different highly relevant tools for data collection, such as interviews, questionnaires, physiological measures, and as other current tools (the Wizard of Oz paradigm and Augmented and Virtual Reality). As in the first chapter this chapter provides the reader with information about advantages and disadvantages of the different tools and with proposed further readings. The third chapter deals with computational models of human/agent interaction and presents in four subchapters different modelling approaches, ranging from models based on psychological mechanisms, rule-based and artificial intelligence models to simulation models of traffic interaction. The fourth chapter is devoted to Requirements Engineering and the challenge of communicating knowledge (e.g., human factors) to developers of automated vehicles. When forming the SHAPE-IT proposal it was identified that there is a lack of communication of human factors knowledge about the highly technical development of automated vehicles. This is why it is highly important that the SHAPE-IT ESRs get training in requirement engineering. Regardless of the ESRs working in academia or industry after their studies it is important to learn how to communicate and disseminate the findings to engineers. The deliverable ends with the chapter “Method Champions”. Here the expertise and association of the different PhD candidates with the different topics are made explicit to facilitate and encourage networking between PhDs with special expertise and those seeking support, especially with regards to methodological questions",Methodological Framework for Modelling and Empirical Approaches (Deliverable D1.1 in the H2020 MSCA ITN project SHAPE-IT),,,10.17196/shape-it/2021/02/D1.1,,core
444079801,2021-06-19T00:00:00,"Smart card data has emerged in recent years and provide a comprehensive, and cheap source of information for planning and managing public transport systems. This paper presents a multi-stage machine learning framework to predict passengers’ boarding stops using smart card data. The framework addresses the challenges arising from the imbalanced nature of the data (e.g. many non-travelling data) and the ‘many-class’ issues (e.g. many possible boarding stops) by decomposing the prediction of hourly ridership into three stages: whether to travel or not in that one-hour time slot, which bus line to use, and at which stop to board. A simple neural network architecture, fully connected networks (FCN), and two deep learning architectures, recurrent neural networks (RNN) and long short-term memory networks (LSTM) are implemented. The proposed approach is applied to a real-life bus network. We show that the data imbalance has a profound impact on the accuracy of prediction at individual level. At aggregated level, FCN is able to accurately predict the rideship at individual stops, it is poor at capturing the temporal distribution of ridership. RNN and LSTM are able to measure the temporal distribution but lack the ability to capture the spatial distribution through bus lines",Multi-stage deep learning approaches to predict boarding behaviour of bus passengers,,'Elsevier BV',10.1016/j.scs.2021.103111,,core
478235725,2021-09-28T00:00:00,"In this paper we present the first safe system for full control of
self-driving vehicles trained from human demonstrations and deployed in
challenging, real-world, urban environments. Current industry-standard
solutions use rule-based systems for planning. Although they perform reasonably
well in common scenarios, the engineering complexity renders this approach
incompatible with human-level performance. On the other hand, the performance
of machine-learned (ML) planning solutions can be improved by simply adding
more exemplar data. However, ML methods cannot offer safety guarantees and
sometimes behave unpredictably. To combat this, our approach uses a simple yet
effective rule-based fallback layer that performs sanity checks on an ML
planner's decisions (e.g. avoiding collision, assuring physical feasibility).
This allows us to leverage ML to handle complex situations while still assuring
the safety, reducing ML planner-only collisions by 95%. We train our ML planner
on 300 hours of expert driving demonstrations using imitation learning and
deploy it along with the fallback layer in downtown San Francisco, where it
takes complete control of a real vehicle and navigates a wide variety of
challenging urban driving scenarios","SafetyNet: Safe planning for real-world self-driving vehicles using
  machine-learned policies",http://arxiv.org/abs/2109.13602,,,,core
437429008,2021-06-22T00:00:00,"The Internet of Things (IoT), in combination with advancements in Big Data, communications and networked systems, offers a positive impact across a range of sectors including health, energy, manufacturing and transport. By virtue of current business models adopted by manufacturers and ICT operators, IoT devices are deployed over various networked infrastructures with minimal security, opening up a range of new attack vectors. Conventional rule-based intrusion detection mechanisms used by network management solutions rely on pre-defined attack signatures and hence are unable to identify new attacks. In parallel, anomaly detection solutions tend to suffer from high false positive rates due to the limited statistical validation of ground truth data, which is used for profiling normal network behaviour. In this work we go beyond current solutions and leverage the coupling of anomaly detection and Cyber Threat Intelligence (CTI) with parallel processing for the profiling and detection of emerging cyber attacks. We demonstrate the design, implementation, and evaluation of Citrus: a novel intrusion detection framework which is adept at tackling emerging threats through the collection and labelling of live attack data by utilising diverse Internet vantage points in order to detect and classify malicious behaviour using graph-based metrics as well as a range of machine learning (ML) algorithms. Citrus considers the importance of ground truth data validation and its flexible software architecture enables both the real-time and offline profiling, detection and classification of emerging cyber-attacks under optimal computational costs. Thus, establishing it as a viable and practical solution for next generation network defence and resilience strategies",Practical Intrusion Detection of Emerging Threats,https://core.ac.uk/download/437429008.pdf,'Institute of Electrical and Electronics Engineers (IEEE)',10.1109/TNSM.2021.3091517,,core
467105843,2021-07-04T00:00:00,"The ever-growing adoption of big data technologies, smart sensing,

data science and artificial intelligence is enabling the development of new intelligent urban spaces with real-time monitoring and advanced cyber-physical situational awareness capabilities. In the S4AllCities international research project, the advancement of cyber-physical situational awareness will be experimented for achieving safer smart city spaces in Europe and beyond. The deployment of digital twins will lead to understanding real-time situation awareness and risks of potential physical and/or cyber-attacks on urban critical infrastructure specifically. The critical extraction of knowledge using digital twins, which ingest, process and fuse observation data and information, prior to machine reasoning is performed in S4AllCities. In this paper, a cyber behavior detection module, which identifies unusualness in cyber traffic networks is described. Also, a physical behaviour detection module is introduced. The two modules function within the so-called Malicious Attacks Information Detection System (MAIDS) digital

twin",Advanced Cyber and Physical Situation Awareness in Urban Smart Spaces,,'Springer Science and Business Media LLC',10.1007/978-3-030-80285-1_50,,core
479718118,2021-01-01T00:00:00,"What we did This report assesses the potential of data-driven approaches to improving transport infrastructure maintenance. It looks at trends in maintenance strategies, explores how the targeted use of data could make them more effective for different types of transport infrastructure, and looks into implications for policy. The report builds on discussions held during workshops with members of the International Transport Forum’s Corporate Partnership Board. What we found Maintenance constitutes an inevitable, albeit often invisible, part of countries’ transport policies. Increased demand for transport infrastructure accelerates infrastructure’s ageing. The effects of climate change further aggravate this. Unsurprisingly, many governments look for transport infrastructure maintenance policies that provide better value for money than current practices offer. Infrastructure maintenance strategies are gradually shifting towards data-driven approaches. They exploit the power of digital technologies, Big Data analytics and advanced forecasting methodologies. Data-driven approaches have gained momentum in transport infrastructure maintenance as a result of four simultaneous technological innovations. First, the development of digital technologies has resulted in the digitalisation of society, industry and transport, which facilitates data sharing. Second, computing technologies have provided the necessary horsepower for running the digital infrastructure. Third, the Internet of Things and sensor technology have increased the potential for automating reporting from sensors that capture and measure new phenomena and provide data sets that flow through digital infrastructures. Fourth, artificial intelligence (AI) has helped to extract information from vast amounts of data, recognising patterns beyond the capacity of individual observation and exploiting digital infrastructure and computing power. Policy makers are beginning to leverage these developments in various ways. Data-driven maintenance is becoming common in many parts of the transport industry. Railroads collect massive amounts of inspection data from different sources using various methods, such as track inspection cars and drones that gather data to model track degradation. However, the rail sector faces numerous challenges for applying Big Data analysis: a lack of specific data analysis tools, high cost of involving stakeholders and heterogeneous data sources. Also, the algorithms currently used to predict the wear of rail infrastructure only work under lab conditions. For road infrastructure, various automated inspection methods exist. These include vision-based methods, laser scanning, ground penetration radar and a combination of these. All are accurate and effective but usually costly. As a result, the coverage and collection frequency can prove insufficient for detectingchanging road conditions. Several pilot studies have tried to use smartphones to collect data on the state of roads to reduce deployment costs for data-driven maintenance. At airports, the demand for accurate real-time data has spawned systems that automatically acquire and process infrastructure data. Advanced technologies now register when deformities develop on runways. They accurately measure moisture levels, temperature, strain and other factors relevant to wear and degradation. Several airports have built, or plan to build, concrete pavements with embedded strain gauges and other sensors to monitor the stress in the material caused by aircraft. Overall, data-driven approaches to infrastructure maintenance promise to enhance fact-based decision making and capabilities to predict the remaining useful life of assets. They can also improve cost efficiency and environmental sustainability. However, some new challenges need to be addressed, notably for the use of AI. AI predicts future behaviour based on historical data. Yet all predictions can prove incorrect where events do not follow past trends. What we recommendScale up and speed up the deployment of data-driven approaches to transport infrastructure maintenance Transport infrastructure maintenance could benefit from a broader and accelerated roll-out of data-driven approaches. These could improve the quality of assets, enhance the life cycles and save costs - especially when the relevant technologies are well-known, such as sensor technologies. In some cases, more tests and pilot projects will be useful, notably where leveraging data technologies for more effective maintenance policies poses specific challenges, as is the case of artificial intelligence in the railway sector. Update regulation and guidelines for transport infrastructure maintenance to facilitate the introduction of more data-driven approaches Current regulations and guidelines apply to condition-based maintenance strategies. These may set requirements that are ill-adapted to data-driven approaches to maintenance and may hamper their roll-out. Policy makers should ensure that the policies applied to data-driven approaches do not stifle their potential benefits. Ensure data-driven infrastructure maintenance approaches follow good practices in data governance The use of data in infrastructure maintenance must be in line with privacy protection laws and regulations. All data should be anonymised and encrypted. Location and trajectory data should be covered by the most robust protection methods, as they create the severest vulnerabilities for citizens. Tools to limit privacy risks include non-disclosure agreements between data users and providers, the involvement of trusted third parties to conduct the data collection and the development of “safe answers” approaches, in which only query results are exchanged instead of raw data. Governments could also broker data-sharing partnerships for the purpose of data-driven maintenance, for instance, between data providers and infrastructure managers. However, it may want to limit such partnerships to data of public interest and require purpose specificity and data minimisation",Data-driven Transport Infrastructure Maintenance,,,,,core
427537973,2021-04-26T00:00:00,"The article presents the main conditions for the use of simulators, which are proposed to be used both for the simulators themselves, which implement the features of artificial intelligence systems, and for the organization of work with simulators in the educational process. Training of students with the use of simulators should form practical skills of the future specialty in accordance with the types of professional activity and qualification characteristics. The conditions for the use of simulators are proposed to include: the implementation of the activity approach; information security of software products; leveling of possible negative consequences for the health of the student, etc. Each of the presented conditions implies the need to ensure the formed parameters and the order of their evaluation. It is concluded that the above conditions for the use of simulators will allow for the development of new forms of professional training and appropriate scientific and methodological tools, and the need to actively introduce new high-tech simulators into the educational process of transport universities, including those that implement the capabilities of virtual/ augmented reality technologies",Conditions for the use of simulators that implement the features of artificial intelligence systems when studying at a transport university,,'EDP Sciences',10.1051/shsconf/202110103012,,core
387280891,2021-09-29T00:00:00,"Millions of battery-powered sensors deployed for monitoring purposes in a
multitude of scenarios, e.g., agriculture, smart cities, industry, etc.,
require energy-efficient solutions to prolong their lifetime. When these
sensors observe a phenomenon distributed in space and evolving in time, it is
expected that collected observations will be correlated in time and space. In
this paper, we propose a Deep Reinforcement Learning (DRL) based scheduling
mechanism capable of taking advantage of correlated information. We design our
solution using the Deep Deterministic Policy Gradient (DDPG) algorithm. The
proposed mechanism is capable of determining the frequency with which sensors
should transmit their updates, to ensure accurate collection of observations,
while simultaneously considering the energy available. To evaluate our
scheduling mechanism, we use multiple datasets containing environmental
observations obtained in multiple real deployments. The real observations
enable us to model the environment with which the mechanism interacts as
realistically as possible. We show that our solution can significantly extend
the sensors' lifetime. We compare our mechanism to an idealized, all-knowing
scheduler to demonstrate that its performance is near-optimal. Additionally, we
highlight the unique feature of our design, energy-awareness, by displaying the
impact of sensors' energy levels on the frequency of updates","Energy Aware Deep Reinforcement Learning Scheduling for Sensors
  Correlated in Time and Space",http://arxiv.org/abs/2011.09747,'Institute of Electrical and Electronics Engineers (IEEE)',10.1109/JIOT.2021.3114102,,core
478995105,2021-10-21T00:00:00,"This paper presents a neural network recommender system algorithm for
assigning vehicles to routes based on energy and cost criteria. In this work,
we applied this new approach to efficiently identify the most cost-effective
medium and heavy duty truck (MDHDT) powertrain technology, from a total cost of
ownership (TCO) perspective, for given trips. We employ a machine learning
based approach to efficiently estimate the energy consumption of various
candidate vehicles over given routes, defined as sequences of links (road
segments), with little information known about internal dynamics, i.e using
high level macroscopic route information. A complete recommendation logic is
then developed to allow for real-time optimum assignment for each route,
subject to the operational constraints of the fleet. We show how this framework
can be used to (1) efficiently provide a single trip recommendation with a
top-$k$ vehicles star ranking system, and (2) engage in more general assignment
problems where $n$ vehicles need to be deployed over $m \leq n$ trips. This new
assignment system has been deployed and integrated into the POLARIS
Transportation System Simulation Tool for use in research conducted by the
Department of Energy's Systems and Modeling for Accelerated Research in
Transportation (SMART) Mobility ConsortiumComment: 14 pages, 11 figure","A Real-Time Energy and Cost Efficient Vehicle Route Assignment Neural
  Recommender System",http://arxiv.org/abs/2110.10887,,,,core
199173119,2088-07-20T00:00:00,"During the last years, modern electronic power wheelchairs have been equipped by manipulators to compensate the deficit in the manual skills of the users due to accidents or disabling diseases. Such robotic arms are designed to perform simple operations, like knocking on a door or pressing buttons in a lift panel, turning on the light in a room, etc.
Owing to the benefits introduced by such manipulators in terms of mobility and autonomy increment, the research has been focusing on the realization of robotic arms able to perform very complex tasks like interaction with small objects.
However, for highly impaired users exploiting all the potentialities offered by such manipulators is often hard, especially because the robot arm is often controlled by the same joystick controller of the power-wheelchair. For such reason, a high number of different Human Machine Interfaces (HMIs) have been developed for users with different impairments. For example in case of severe upper-link disabilities, Brain Computer Interfaces (BCI) can be used which is very promising thanks to its usability by the users having no possibility of moving their arms. However, such HMI requires the presence of numerous electrodes placed on the user's body or dedicated helmets resulting more invasive than other interfaces. For such reason, for less severe disabilities different HMIs are more recommended.
In this thesis, we propose a user independent low cost robotic arm with 5-Degrees of Freedom(DOFs) equipped by a monocular camera and a proximity sensor, controlled by and augmented HMI featuring a real-time AI algorithm which improves the users experience. The offered HMI is based on a touch-screen in which the user can visualize the camera frames and press the desired object. Starting from the user selections, the AI extracts some image features which are elaborated by a tracking algorithm. The autonomy of the robotic arm is reached by a closed-feedback loop which exploits image features extracted by the camera images, to actuate the motors of the manipulator. The main advantages of this work is due to particular Computer Vision algorithm that uses an Artificial Intelligence (AI) to perform objects recognition and their coordinates extraction in the image frames. In fact, thanks to the presence of the AI, objects, like the buttons of a lift panel, are autonomously recognized and shown to the users surrounded by a bounding box. This allows the implementation of a robust HMIs activated by the click of the user on a touchscreen. The robustness of the HMIs is increased since it accepts clicks only inside the shown bounding boxes, avoiding to start the approach in case of user errors. For more severe impairments, a Manual raster scanning and a Time raster scanning HMIs have been implemented.
The software of the system is modular, versatile and easy to maintain thanks to its model based organization. It has been implemented inside the Robotic Operative System (ROS) environment. ROS is an open-source and multi-platform framework that manages multiple tasks as nodes of a network that exchange messages in form of topics or services. Each node is a single elaboration unit or module, usually represented by a process, in which the messages are received and sent, according to the previous and next module. In this way, a future reconfiguration of the software can be carried out without changing the entire software organization. It also permits to test new single nodes inside the existent ones, providing the message are maintained.
The entire software, with the only exception of the AI and the tracking algorithm, runs on a Raspberry PI 3 platform",Low cost eye-in-hand robotic-arm featuring AI-based human machine interface for people with disabilities,,'Pisa University Press',,,core
428345889,2021-04-27T00:00:00,"An Unmanned Aerial Vehicle (UAV) is an aircraft that operates without a human on-board and can be flown autonomously or controlled remotely. Due to its unmanned operation, UAVs need technologies so they can not only fly autonomously, but also communicate with base stations, flight controllers, computers, devices or even other UAVs. Traditionally, UAVs operate within unlicensed spectrum bands, competing against the increasing number of mobile devices and other wireless networks. This use could lead to interference that affect UAVs communication and problems with overcrowded spectrum. Cognitive Radio (CR) presents itself as a promising technology to solve these problems. CR provides a smart wireless communication which, instead of using a transmission frequency defined in the hardware, uses software defined radio. This allows CR to adapt its transmission frequency in a smart way, using free transmission channels and/or choosing them accordingly with the applications requirements. The combination of UAVs and CR can be used in missions where the conventional UAVs face limitations due to communication problems. Moreover, CR is considered a key enabler for adequately deploying communication paradigms that require high connectivity, such as Smart Cities, 5G, Internet of Things (IoT) and, thus, Internet of Flying Things (IoFT). Though both CR and UAVs are well-established fields of research, the combination of these two elements is little explored in literature. Therefore, this work identifies gaps and opportunities, as well as challenges on the field. Furthermore, this work contributes to the progress regarding the integration of CR and UAVs. To do so, this work presents the definition of CR technologies, as well as their integration on a real mission of data collection. This works results differ to the others on the literature in terms of, for example, highlighting the limitation in real scenario of traditionally deployed Machine Learning algorithms using simulated data on the field.Um Veículo Aéreo Não Tripulado (VANT) é uma aeronave que opera sem um humano a bordo, podendo voar de forma autônoma ou sendo pilotada remotamente. Devido à ausência de um piloto, os VANTs necessitam de tecnologias não só para que possam voar de forma autônoma, mas também de tecnologias de comunicação robustas que permitam a eles se comunicarem, seja com estações de base, controladores de tráfego aéreo, computadores, dispositivos, ou até mesmo com outros VANTs. Tradicionalmente, os VANTs operam em bandas de espectro não licenciadas, concorrendo com o crescente número de dispositivos móveis e com outras redes sem fio. Esse uso pode levar a interferências que afetam a comunicação dos VANTs e, além disso, pode levá-los a enfrentar o problema da superlotação de espectro. Rádio Cognitivo (RC) apresenta-se como uma tecnologia promissora para a solução desses problemas. RC permite uma comunicação sem fio inteligente que, em vez de usar uma frequência de transmissão definida no hardware, utiliza software para esta definição. Isto possibilita ao RC adaptar de forma inteligente sua frequência de transmissão, utilizando canais de transmissão livres e/ou escolhendo-os de acordo com os requisitos da aplicação. A integração de VANTs e RC pode ser empregada em missões nas quais VANTs convencionais enfrentam limitações devido a problemas de comunicação. Além disso, RC é considerado um facilitador chave para a implantação satisfatória de paradigmas de comunicação que demandam alta conectividade, tais como Cidades Inteligentes, 5G, Internet das Coisas (IoT) e, consequentemente, Internet das Coisas Voadoras (IoFT). Apesar de RC e VANTs serem áreas de pesquisa bem estabelecidas, a integração dos dois é pouco explorada na literatura. Portanto, este trabalho contribui nos avanços da integração de RC a um VANT. Neste trabalho foram identificadas as lacunas e oportundiades, bem como os desafios na área. Além disso, foram definidas as tecnologias de RC, bem como a integração do mesmo em uma missão real de coleta de dados, obtendo resultados marcantes em relação aos da literatura, como no caso de algoritmos de Aprendizagem de Máquina, tradicionalmente empregados com dados simulados, mas que demonstraram limitação quando utilizados com dados reais coletados neste trabalho",Integrando rádio cognitivo a veículos aéreos não tripulados,,"'Universidade de Sao Paulo, Agencia USP de Gestao da Informacao Academica (AGUIA)'",10.11606/D.55.2021.tde-27042021-125734,,core
478035307,2021-06-21T09:35:07,"Poor decisions and selfish behaviors give rise to seemingly intractable global problems, such as the lack of transparency in democratic processes, the spread of conspiracy theories, and the rise in greenhouse gas emissions. However, people are more predictable than we think, and with machine-learning algorithms and sufficiently large datasets, we can design accurate models of human behavior in a variety of settings. In this thesis, to gain insight into social processes, we develop highly interpretable probabilistic choice-models. We draw from the econometrics literature on discrete-choice models and combine them with matrix factorization methods, Bayesian statistics, and generalized linear models. These predictive models enable interpretability through their learned parameters and latent factors.

First, we study the social dynamics behind group collaborations for the collective creation of content, such as in Wikipedia, the Linux kernel, and the European Union law-making process. By combining the Bradley-Terry and Rasch models with matrix factorization and natural language processing, we develop a model of edit acceptance in peer-production systems. We discover controversial components (e.g., Wikipedia articles and European laws) and influential users (e.g., Wikipedia editors and parliamentarians), as well as features that correlate with a high probability of edit acceptance. The latent representations capture non-linear interactions between components and users, and they cluster well into different topics (e.g., historical figures and TV characters in Wikipedia, business and environment in European laws).

Second, we develop an algorithm for predicting the outcome of elections and of referenda by combining matrix factorization and generalized linear models. Our algorithm learns representations of votes and regions, which capture ideological and cultural voting patterns (e.g., liberal/conservative, rural/urban), and it predicts the vote results in unobserved regions from partial observations. We test our model on voting data in Germany, Switzerland, and the US, and we deploy it on a Web platform to predict Swiss referendum votes in real-time. On average, our predictions reach a mean absolute error of 1% after observing only 5% of the regions.

Third, we study how people perceive the carbon footprint of their day-to-day actions. We cast this problem as a comparison problem between pairs of actions (e.g., the difference between flying across continents and using household appliances), and we develop a statistical model of relative comparisons reminiscent of the Thurstone model in psychometrics. The model learns the usersâ perception as the parameters of a Bayesian linear regression, which enables us to derive an active-learning algorithm to collect data efficiently. Our experiments show that users overestimate the emissions of low-footprint actions and underestimate those of high-footprint actions.

Finally, we design a probabilistic model of pairwise-comparison outcomes that capture a wide range of time dynamics. We achieve this by replacing the static parameters of a class of popular pairwise-comparison models with continuous-time Gaussian processes. We also develop an efficient inference algorithm that computes, with only a few linear-time iterations over the data, an approximate Bayesian posterior distribution",Discrete-Choice Mining of Social Processes,,"Lausanne, EPFL",10.5075/epfl-thesis-7186,,core
387271528,2021-07-24T00:00:00,"Predictive monitoring -- making predictions about future states and
monitoring if the predicted states satisfy requirements -- offers a promising
paradigm in supporting the decision making of Cyber-Physical Systems (CPS).
Existing works of predictive monitoring mostly focus on monitoring individual
predictions rather than sequential predictions. We develop a novel approach for
monitoring sequential predictions generated from Bayesian Recurrent Neural
Networks (RNNs) that can capture the inherent uncertainty in CPS, drawing on
insights from our study of real-world CPS datasets. We propose a new logic
named \emph{Signal Temporal Logic with Uncertainty} (STL-U) to monitor a
flowpipe containing an infinite set of uncertain sequences predicted by
Bayesian RNNs. We define STL-U strong and weak satisfaction semantics based on
if all or some sequences contained in a flowpipe satisfy the requirement. We
also develop methods to compute the range of confidence levels under which a
flowpipe is guaranteed to strongly (weakly) satisfy an STL-U formula.
Furthermore, we develop novel criteria that leverage STL-U monitoring results
to calibrate the uncertainty estimation in Bayesian RNNs. Finally, we evaluate
the proposed approach via experiments with real-world datasets and a simulated
smart city case study, which show very encouraging results of STL-U based
predictive monitoring approach outperforming baselines.Comment: This article appears as part of the ESWEEK-TECS special issue and was
  presented in the International Conference on Embedded Software (EMSOFT), 202","Predictive Monitoring with Logic-Calibrated Uncertainty for
  Cyber-Physical Systems",http://arxiv.org/abs/2011.00384,,,,core
479176342,2021-10-28T00:00:00,"Importance: Lower-resource areas in Africa and Asia face a unique set of
healthcare challenges: the dual high burden of communicable and
non-communicable diseases; a paucity of highly trained primary healthcare
providers in both rural and densely populated urban areas; and a lack of
reliable, inexpensive internet connections. Objective: To address these
challenges, we designed an artificial intelligence assistant to help primary
healthcare providers in lower-resource areas document demographic and medical
sign/symptom data and to record and share diagnostic data in real-time with a
centralized database. Design: We trained our system using multiple data sets,
including US-based electronic medical records (EMRs) and open-source medical
literature and developed an adaptive, general medical assistant system based on
machine learning algorithms. Main outcomes and Measure: The application
collects basic information from patients and provides primary care providers
with diagnoses and prescriptions suggestions. The application is unique from
existing systems in that it covers a wide range of common diseases, signs, and
medication typical in lower-resource countries; the application works with or
without an active internet connection. Results: We have built and implemented
an adaptive learning system that assists trained primary care professionals by
means of an Android smartphone application, which interacts with a central
database and collects real-time data. The application has been tested by dozens
of primary care providers. Conclusions and Relevance: Our application would
provide primary healthcare providers in lower-resource areas with a tool that
enables faster and more accurate documentation of medical encounters. This
application could be leveraged to automatically populate local or national EMR
systems","Lightweight Mobile Automated Assistant-to-physician for Global
  Lower-resource Areas",http://arxiv.org/abs/2110.15127,,,,core
428337846,2021-04-01T00:00:00,"In the UK, £150bn of assets and 4 million people are at risk from coastal flooding. With reductions in public funding, rising sea levels and changing storm conditions, cost-effective and accurate early warning flood forecasting systems are required. However, numerical tools currently used to estimate wave overtopping are based on tank experiments and very limited previous field measurements of total overtopping volumes only. Furthermore, the setting of tolerable hazard thresholds in flood forecasting models requires site-specific information of wave overtopping during storms of varying severity. 



The National Oceanography Centre (NOC) are currently developing a new nowcast wave overtopping alert system that can be deployed in site-specific coastal settings to detect potentially dangerous flood conditions in near real-time (NRT) while validating operational forecasting services. At its core, it utilises a prototype overtopping sensor and an instance of the National Oceanic and Atmospheric Administration’s ERDDAP data server in a self-monitoring and alerting control system. In-situ detection will be performed by WireWall, a novel capacitance wire sensor that measures at the high (400 Hz) frequencies required to obtain the distribution of overtopping volume and horizontal velocity on a wave-by-wave basis. The sensor includes on-board data processing and 2-way telemetry to enable automation and control. The telemetry posts regular health summaries and high-resolution (1 sec) hazard data (produced by the on-board processing) using the standard internet protocol (https) to an open ERDDAP server so data are freely available via an application programming interface (API) alongside other NRT and delayed-mode global coastal ocean and weather information for further data exploration. ERDDAP allows NRT hazard data to be accessed by statistical algorithms and visual applications, as well as receiving alerts that are also fed to messaging queue points (RabbitMQ) that can be monitored by external systems. Combined, this will enable automated health monitoring and sensor operation as well as offer the potential for downstream hazard management tools (such as navigation systems and transport management systems) to ingest the nowcast wave overtopping hazard data. To integrate data with wider systems and different disciplines, ERDDAP data sets will be enriched with common and well-structured metadata. Data provenance, controlled vocabularies, Quality Control and attribution information embedded in the data workflow is fundamental to ensuring user trust in the data and any products generated, while enhancing FAIR data principles. 



The new nowcast wave overtopping alert system will be tested in 2021 during field deployments of multiple WireWall systems at two high energy coastal sites in the UK. Such data are crucial for validating operational flood forecast services as well as protecting local communities and minimising transport service disruptions. The addition of SMART monitoring optimises sensor maintenance and operation, reducing the costs associated with teams travelling to the site. Using ERDDAP embedded with well-structured metadata enables machines to access multiple flood parameters through a single point that abstracts users from the complexities associated with the source data, offering the potential for further data exploration through modelling or techniques such as machine learning",The use of ERDDAP in a self-monitoring and nowcast hazard alerting coastal flood system,,'Copernicus GmbH',10.5194/egusphere-egu21-14621,,core
322658302,2090-03-06T00:00:00,"Goal of the thesis is the generation of synthetic human mobility based on Deep Learning. Three different generative recurrent models have been implemented: a Seq2Seq Variational Autoencoder (VAE), a Generative Adversarial Network (GAN) and a Wasserstein GAN. The aim of this study is the generation of a synthetic dataset of GPS trajectories having characteristics and typical measures proper of the real human mobility.
Scopo della tesi è la generazione di mobilità umana sintetica basata suDeep Learning. Sono stati implementati tre modelli generativi: un Seq2Seq Variational Autoencoder (VAE), una Generative Adversarial Network (GAN) e una Wasserstein GAN. Obiettivo finale dello studio è lagenerazione di un dataset sintetico di traiettorie GPS, avente caratteristiche e misure proprie della mobilità umana",Generative Models of Human Mobility based on Deep Learning,,'Pisa University Press',,,core
474908402,2021-01-01T00:00:00,"In recent years, the automated driving system has been known to be one of the most popular research topics of artificial intelligence (AI) and intelligent transportation system (ITS). The journey experience on automated vehicles and the intelligent automated driving system could be improved by individualization driving understanding. Although previous studies have proposed methods for driving styles understanding, the individualization driving classification has not been addressed thoroughly. Therefore, in this study, a supervised method is proposed to understand driving behavioral structure and the latent driving styles by incorporating the prior knowledge. Firstly, a novel method is established for driving behavioral encoding and raw driving data mining. Then, the Labeled Latent Dirichlet Allocation (LLDA) is proposed to understand the latent driving styles from individual driving with driving behaviors. Finally, the Safety Pilot Model Deployment (SPMD) data are used to validate the performance of the proposed model. Experimental results show that the proposed model uncovers latent driving styles effectively and shows good agreement to real situations, which provides theoretical guidance on driving behavior recognition for better individual experience on automated driving vehicles",Driving Style Recognition under Connected Circumstance Using a Supervised Hierarchical Bayesian Model,,'Hindawi Limited',10.1155/2021/6687378,"[{'title': 'Journal of Advanced Transportation', 'identifiers': ['issn:2042-3195', '2042-3195']}]",core
477905332,2021-10-28T00:00:00,"As vehicle complexity and road congestion increase, combined with the emergence of electric vehicles, the need for intelligent transportation systems to improve on-road safety and transportation efficiency using vehicular networks has become essential. The evolution of high mobility wireless networks will provide improved support for connected vehicles through highly dynamic heterogeneous networks. Particularly, 5G deployment introduces new features and technologies that enable operators to capitalize on emerging infrastructure capabilities. Machine Learning (ML), a powerful methodology for adaptive and predictive system development, has emerged in both vehicular and conventional wireless networks. Adopting data-centric methods enables ML to address highly dynamic vehicular network issues faced by conventional solutions, such as traditional control loop design and optimization techniques. This article provides a short survey of ML applications in vehicular networks from the networking aspect. Research topics covered in this article include network control containing handover management and routing decision making, resource management, and energy efficiency in vehicular networks. The findings of this paper suggest more attention should be paid to network forming/deforming decision making. ML applications in vehicular networks should focus on researching multi-agent cooperated oriented methods and overall complexity reduction while utilizing enabling technologies, such as mobile edge computing for real-world deployment. Research datasets, simulation environment standardization, and method interpretability also require more research attention",Machine learning in vehicular networking: an overview,,'Elsevier BV',10.1016/j.dcan.2021.10.007,,core
478036013,2021-08-09T12:19:37,"In recent years, the importance of electric mobility has increased in response to climate change. The fast-growing deployment of electric vehicles (EVs) worldwide is expected to decrease transportation-related emissions, facilitate the integration of renewables, and support the grid through demand–response services. Simultaneously, inadequate EV charging patterns can lead to undesirable effects in grid operation, such as high peak-loads or low self-consumption of solar electricity, thus calling for novel methods of control. This work focuses on applying deep reinforcement learning (RL) to the EV charging control problem with the objectives to increase photovoltaic self-consumption and EV state of charge at departure. Particularly, we propose mathematical formulations of environments with discrete, continuous, and parametrized action spaces and respective deep RL algorithms to resolve them. The benchmarking of the deep RL control against naive, rule-based, deterministic optimization, and model-predictive control demonstrates that the suggested methodology can produce consistent and employable EV charging strategies, while its performance holds a great promise for real-time implementations",Deep reinforcement learning control of electric vehicle charging in the presence of photovoltaic generation,,'Elsevier BV',10.1016/j.apenergy.2021.117504,,core
477983070,2021-01-01T00:00:00,"The progress in technology development over the past decades, both with respect to software and hardware, offers the vision of automated vehicles as means of achieving zero fatalities in traffic. However, the promises of this new technology – an increase in road safety, traffic efficiency, and user comfort – can only be realized if this technology is smoothly introduced into the existing traffic system with all its complexities, constraints, and requirements. SHAPE- IT will contribute to this major undertaking by addressing research questions relevant for the development and introduction of automated vehicles in urban traffic scenarios. Previous research has pointed out several research areas that need more attention for a successful implementation and deployment of human-centred vehicle automation in urban environments.In SHAPE-IT, for example, a better understanding of human behaviour and the underlying psychological mechanisms will lead to improved models of human behaviour that can help to predict the effects of automated systems on human behaviour already during system development. Such models can also be integrated into the algorithms of automated vehicles, enabling them to better understand the human interaction partners’ behaviours.Further, the development of vehicle automation is much about technology (software and hardware), but the users will be humans and they will interact with humans both inside and outside of the vehicle. To be successful in the development of automated vehicles functionalities, research must be performed on a variety of aspects. Actually, a highly interdisciplinary team of researchers, bringing together expertise and background from various scientific fields related to traffic safety, human factors, human-machine interaction design and evaluation, automation, computational modelling, and artificial intelligence, is likely needed to consider the human-technology aspects of vehicle automation.Accordingly, SHAPE-IT has recruited fifteen PhD candidates (Early Stage Researchers – ESRs), that work together to facilitate this integration of automated vehicles into complex urban traffic by performing research to support the development of transparent, cooperative, accepted, trustworthy, and safe automated vehicles. With their (and their supervisors’) different scientific background, the candidates bring different theoretical concepts and methodological approaches to the project. This interdisciplinarity of the project team offers the unique possibility for each PhD candidate to address research questions from a broad perspective – including theories and methodological approaches of other interrelated disciplines. This is the main reason why SHAPE-IT has been funded by the European Commission’s Marie Skłodowska-Curie Innovative Training Network (ITN) program that is aimed to train early state researchers in multidisciplinary aspects of research including transferable skills. With the unique scope of SHAPE-IT, including the human-vehicle perspective, considering different road-users (inside and outside of the vehicle), addressing for example trust, transparency, and safety, and including a wide range of methodological approaches, the project members can substantially contribute to the development and deployment of safe and appreciated vehicle automation in the cities of the future.To achieve the goal of interdisciplinary research, it is necessary to provide the individual PhD candidate with a starting point, especially on the different and diverse methodological approaches of the different disciplines. The empirical, user-centred approach for the development and evaluation of innovative automated vehicle concepts is central to SHAPE- IT. This deliverable (D1.1 “Methodological Framework for Modelling and Empirical Approaches”) provides this starting point. That is, this document provides a broad overview of approaches and methodologies used and developed by the SHAPE-IT ESRs during their research. The SHAPE-IT PhD candidates, as well as other researchers and developers outside of SHAPE-IT, can use this document when searching for appropriate methodological approaches, or simply get a brief overview of research methodologies often employed in automated vehicle research.The first chapter of the deliverable shortly describes the major methodological approaches to collect data relevant for investigating road user behaviour. Each subchapter describes one approach, ranging from naturalistic driving studies to controlled experiments in driving simulators, with the goal to provide the unfamiliar reader with a broad overview of the approach, including its scope, the type of data collected, and its limitations. Each subchapter ends with recommendations for further reading – literature that provide much more detail and examples.The second chapter explains four different highly relevant tools for data collection, such as interviews, questionnaires, physiological measures, and as other current tools (the Wizard of Oz paradigm and Augmented and Virtual Reality). As in the first chapter this chapter provides the reader with information about advantages and disadvantages of the different tools and with proposed further readings.The third chapter deals with computational models of human/agent interaction and presents in four subchapters different modelling approaches, ranging from models based on psychological mechanisms, rule-based and artificial intelligence models to simulation models of traffic interaction.The fourth chapter is devoted to Requirements Engineering and the challenge of communicating knowledge (e.g., human factors) to developers of automated vehicles. When forming the SHAPE-IT proposal it was identified that there is a lack of communication of human factors knowledge about the highly technical development of automated vehicles. This is why it is highly important that the SHAPE-IT ESRs get training in requirement engineering. Regardless of the ESRs working in academia or industry after their studies it is important to learn how to communicate and disseminate the findings to engineers.The deliverable ends with the chapter “Method Champions”. Here the expertise and association of the different PhD candidates with the different topics are made explicit to facilitate and encourage networking between PhDs with special expertise and those seeking support, especially with regards to methodological questions.Transport and Plannin",Methodological Framework for Modelling and Empirical Approaches (Deliverable D1.1 in the H2020 MSCA ITN project SHAPE-IT),,SHAPE-IT Consortium,10.17196/shape-it/2021/02/D1.1,,core
417758260,2021-03-25T00:00:00,"Autonomous vehicles are increasingly becoming a necessary trend towards building the smart cities of the future. Numerous proposals have been presented in recent years to tackle particular aspects of the working pipeline towards creating a functional end-to-end system, such as object detection, tracking, path planning, sentiment or intent detection, amongst others. Nevertheless, few efforts have been made to systematically compile all of these systems into a single proposal that also considers the real challenges these systems will have on the road, such as real-time computation, hardware capabilities, etc. This paper reviews the latest techniques towards creating our own end-to-end autonomous vehicle system, considering the state-of-the-art methods on object detection, and the possible incorporation of distributed systems and parallelization to deploy these methods. Our findings show that while techniques such as convolutional neural networks, recurrent neural networks, and long short-term memory can effectively handle the initial detection and path planning tasks, more efforts are required to implement cloud computing to reduce the computational time that these methods demand. Additionally, we have mapped different strategies to handle the parallelization task, both within and between the networks","Object detection, distributed cloud computing and parallelization techniques for autonomous driving systems.",https://core.ac.uk/download/417758260.pdf,'MDPI AG',10.3390/app11072925,,core
475498203,2021-06-07T07:00:00,"Artificial intelligence and machine learning have recently become a hot topic in terms of software solutions to complex problems. Every year, new prototypes and projects are created to solve specific problems, ranging from self-driving vehicles to facial recognition. One project which interested our team was OpenAI Five, which created an artificial intelligence agent to play the complex online competitive game DotA 2. We wanted to create our own agent to design optimized city layouts in the game Cities: Skylines. By doing so, we hope to illustrate the viability of using artificial intelligence as a tool in urban planning for real cities.
To create the artificial intelligence agent, we took advantage of the fact that we were using a video game as an environment for the agent. We were able to customize and control the environment that the agent was operating in, and allow the agent to reliably read data on the game state and perform actions to change the game state. With these prerequisites, the agent was able to begin the process of reinforcement learning to progressively find more optimal urban design solutions for the game environment",Urban Planning Optimization via “Cities: Skylines”,https://core.ac.uk/download/475498203.pdf,Scholar Commons,,,core
443972052,2021-07-01T00:00:00,"Autonomous Driving (AD) related features provide new forms of mobility that
are also beneficial for other kind of intelligent and autonomous systems like
robots, smart transportation, and smart industries. For these applications, the
decisions need to be made fast and in real-time. Moreover, in the quest for
electric mobility, this task must follow low power policy, without affecting
much the autonomy of the mean of transport or the robot. These two challenges
can be tackled using the emerging Spiking Neural Networks (SNNs). When deployed
on a specialized neuromorphic hardware, SNNs can achieve high performance with
low latency and low power consumption. In this paper, we use an SNN connected
to an event-based camera for facing one of the key problems for AD, i.e., the
classification between cars and other objects. To consume less power than
traditional frame-based cameras, we use a Dynamic Vision Sensor (DVS). The
experiments are made following an offline supervised learning rule, followed by
mapping the learnt SNN model on the Intel Loihi Neuromorphic Research Chip. Our
best experiment achieves an accuracy on offline implementation of 86%, that
drops to 83% when it is ported onto the Loihi Chip. The Neuromorphic Hardware
implementation has maximum 0.72 ms of latency for every sample, and consumes
only 310 mW. To the best of our knowledge, this work is the first
implementation of an event-based car classifier on a Neuromorphic Chip.Comment: Accepted for publication at IJCNN 202","CarSNN: An Efficient Spiking Neural Network for Event-Based Autonomous
  Cars on the Loihi Neuromorphic Research Processor",http://arxiv.org/abs/2107.00401,,,,core
401949336,2021-05-19T00:00:00,"Target settings. This study examines the possibilities of modern geographic information and cloud technologies and prospects for their use for administrative and economic management of an airport. The study is related to the implementation of the State Target Program for Airport Development until 2023 and the Aviation Transport Strategy of Ukraine until 2030, which aims to develop the aviation industry in Ukraine, bringing airport infrastructure to the requirements of the European Union. The Law of Ukraine «On the National Infrastructure of Geospatial Data» and «Consolidated Concept of VIM Implementation in Ukraine» has a great influence on the formation of geospatial data of airports.
Actual scientific researches and issues analysis. The paper analyzes and summarizes publications on methods of obtaining geospatial data, implementation of geographic information technologies, virtual, augmented and mixed reality technologies, artificial intelligence and the concept of «smart» city for administrative and economic management of airports.
Uninvestigated parts of general matters defining. Analysis of recent research and publications has shown that the prospects for the introduction of geographic information technology for administrative and economic asset management of Ukrainian airports need further research, as these issues are very important and relevant, given the rapid growth of digital society, environment and infrastructure.
The research objective. The purpose of this study is to analyze the possibilities and prospects for the introduction of modern technologies for processing and visualization of geospatial data for administrative and economic management of the airport and the development of a conceptual model. The task of the research is to analyze the methods of obtaining geospatial data of the airport, the use of geographic information systems in airports, artificial intelligence technologies, virtual, augmented and mixed reality, the Internet of Things, digital duplicates, implementation of the concept of «smart» city, etc.
The statement of basic materials. Geospatial data is created digitally using modern information and cloud technologies that offer a wide range of equipment, software, methods and technologies for working with geospatial information. Every year, new technologies that are used in the administrative and economic management of airports appear: cloud data acquisition methods, geographic information systems, artificial intelligence technologies, virtual reality, the Internet of Things, digital counterparts, «smart» cities, and more. Successful integration and use of existing capabilities for the collection, storage, processing and visualization of geospatial data of airports will ensure their effective management and economic growth.
Conclusions. Based on the analysis of the possibilities of using virtual, augmented and mixed reality technologies, artificial intelligence, digital duplicates and the concept of «smart» cities in airports, a conceptual model of prospects for using geospatial data of the airport to address administrative and economic management of the property complex was developed.Актуальність теми дослідження. Останніми роками в аеропортах світу активно впроваджуються хмарні технології збору, опрацювання та візуалізації геопросторових даних: лазерне та лідарне сканування, інтеграція ВІМ/GIS моделей, застосування штучного інтелекту, технологій віртуальної та доповненої реальності, цифрових двійників та «розумних міст». Для України, яка активно йде по шляху цифровізації та впровадження сучасних геоінформаційних технологій у багатьох сферах діяльності, розробка нових методів та підходів для адміністративно-господарського управління аеропортовими комплексами є актуальним та перспективним напрямом.
Постановка проблеми. У цьому дослідженні розглянуто можливості сучасних геоінформаційних та хмарних технологій і перспективи їх використання для адміністративно-господарського управління територією аеропорту. Дослідження пов’язано з реалізацією Державної цільової програми розвитку аеропортів на період до 2023 року та Авіаційної транспортної стратегії України до 2030 року, метою яких є розвиток авіаційної галузі в Україні, приведення інфраструктури аеропортів до вимог Європейського Союзу. Також великий вплив на формування геопросторових даних аеропортів має Закон України «Про національну інфраструктуру геопросторових даних» та Консолідована Концепція впровадження ВІМ в Україні.
Аналіз останніх досліджень і публікацій. У роботі були проаналізовані та узагальнені публікації, що присвячені методам отримання геопросторових даних, впровадження геоінформаційних технологій, технологій віртуальної, доповненої та змішаної реальності, штучного інтелекту та концепції «розумного міста» для адміністративно-господарського управління аеропортами.
Виділення недосліджених частин загальної проблеми. Аналіз останніх досліджень і публікацій показав, що питання перспектив впровадження геоінформаційних технологій для адміністративно-господарського управління активами аеропортів України потребує додаткового дослідження, оскільки ці питання дуже важливі та актуальні, враховуючи стрімке зростання цифровізації суспільства, навколишнього середовища та інфраструктурних об’єктів.
Постановка завдання. Метою цього дослідження є аналіз можливостей та перспектив впровадження сучасних технологій опрацювання та візуалізації геопросторових даних для адміністративно-господарського управління територією аеропорту та розробка концептуальної моделі. Завданням дослідження є аналіз методів отримання геопросторових даних території аеропорту, застосування в аеропортах геоінформаційних систем, технологій штучного інтелекту, віртуальної, доповненої та змішаної реальності, інтернету речей, цифрових двійників, реалізації концепції «розумне місто», тощо.
Виклад основного матеріалу. Геопросторові дані створюються в цифровій формі з використанням сучасних інформаційних та хмарних технологій, які пропонують широкий спектр обладнання, програмного забезпечення, методів і технологій роботи з геопросторовою інформацією. З кожним роком з’являються все нові технології, які знаходять застосування в адміністративно-господарському управлінні аеропортів: хмарні методи отримання даних, геоінформаційні системи, технології штучного інтелекту, віртуальної реальності, інтернету речей, цифрові двійники, «розумні міста» тощо. Вдала інтеграція та використання наявних можливостей щодо збору, зберігання, опрацювання та візуалізації геопросторових даних аеропортів забезпечить їх ефективне управління та економічне зростання. 
Висновки відповідно до статті. За результатами проведеного аналізу можливостей використання в аеропортах технологій віртуальної, доповненої та змішаної реальності, штучного інтелекту, цифрових двійників та концепції «розумних міст», було розроблено концептуальну модель перспектив використання геопросторових даних території аеропорту для вирішення питань адміністративно-господарського управління майновим комплексом",ПЕРСПЕКТИВИ ВИКОРИСТАННЯ ГЕОІНФОРМАЦІЙНИХ ТЕХНОЛОГІЙ В АЕРОПОРТАХ УКРАЇНИ ДЛЯ АДМІНІСТРАТИВНО-ГОСПОДАРСЬКОГО УПРАВЛІННЯ,,Національний університет «Чернігівська політехніка»,,,core
477743637,2021-09-03T00:00:00,"This paper presents the design and the results of a novel approach to predict air pollutants in urban environments. The objective is to create an artificial intelligence (AI)-based system to support planning actors in taking effective and adequate short-term measures against unfavourable air quality situations. In general, air quality in European cities has improved over the past decades. Nevertheless, reductions of the air pollutants particulate matter (PM), nitrogen dioxide (NO2) and ground-level ozone (O3), in particular, are essential to ensure the quality of life and a healthy life in cities. To forecast these air pollutants for the next 48 hours, a sequence-to-sequence encoder-decoder model with a recurrent neural network (RNN) was implemented. The model was trained with historic in situ air pollutant measurements, traffic and meteorological data. An evaluation of the prediction results against historical data shows high accordance with in situ measurements and implicate the system’s applicability and its great potential for high quality forecasts of air pollutants in urban environments by including real time weather forecast data",DESIGN AND RESULTS OF AN AI-BASED FORECASTING OF AIR POLLUTANTS FOR SMART CITIES,https://core.ac.uk/download/477743637.pdf,'Copernicus GmbH',10.5194/isprs-annals-VIII-4-W1-2021-89-2021,,core
390100246,2021-03-22T18:09:49,"Overutilization of Emergency Departments (ED) is a major problem among the health care providers in the United States. In this research, a machine learning-based predictive model for predicting ED high utilizers will be designed based on a set of existing and proposed facilities and the population and social determinant of health (SDOH) factors influencing utilization. The purpose of the model will be to alert the healthcare systems and government organizations by identifying the reasons for overutilization of the medical services among the people in a particular community. Also, the novel coronavirus disease 2019 (COVID-19) developed in Whunan city, China has spread quickly to the other parts of the world. It has become a serious health threat to the United States. Moreover, in this research study, the clinical and social characteristics that are responsible for the quick spread of COVID-19 disease across the Louisiana state will be identified. The purpose of this study is to identify what kind of population gets COVID 19 and providing real time care decisions to minimize the risk of an individual acquiring COVID-19. The patient data from Electronic Health Records (EHR) of Francis Missionaries of our Lady Health System (FMOLHS) is geocoded and mapped into ArcGIS software. The socioeconomic factors and social vulnerability Index (SVI) variables available from various online sources are joined to the geocoded patient data with the help of spatial joining techniques available in the ArcGIS software. Correlation analysis between the dependent variables and factors will be conducted",Predictive Modeling of FMOL Health System Utilization Using Machine Learning Algorithms and Retrospective Study of COVID Tested Patients,https://core.ac.uk/download/390100246.pdf,LSU Digital Commons,,,core
444081897,2021-07-05T00:00:00,"Interconnected road lanes are a central concept for navigating urban roads.
Currently, most autonomous vehicles rely on preconstructed lane maps as
designing an algorithmic model is difficult. However, the generation and
maintenance of such maps is costly and hinders large-scale adoption of
autonomous vehicle technology. This paper presents the first self-supervised
learning method to train a model to infer a spatially grounded lane-level road
network graph based on a dense segmented representation of the road scene
generated from onboard sensors. A formal road lane network model is presented
and proves that any structured road scene can be represented by a directed
acyclic graph of at most depth three while retaining the notion of intersection
regions, and that this is the most compressed representation. The formal model
is implemented by a hybrid neural and search-based model, utilizing a novel
barrier function loss formulation for robust learning from partial labels.
Experiments are conducted for all common road intersection layouts. Results
show that the model can generalize to new road layouts, unlike previous
approaches, demonstrating its potential for real-world application as a
practical learning-based lane-level map generator.Comment: Accepted for IEEE ITSC 202","Learning a Model for Inferring a Spatial Road Lane Network Graph using
  Self-Supervision",http://arxiv.org/abs/2107.01784,,,,core
480186088,2021-11-04T00:00:00,"The rapid advancements of Internet of Things (IoT) and artificial
intelligence (AI) have catalyzed the development of adaptive traffic signal
control systems (ATCS) for smart cities. In particular, deep reinforcement
learning (DRL) methods produce the state-of-the-art performance and have great
potentials for practical applications. In the existing DRL-based ATCS, the
controlled signals collect traffic state information from nearby vehicles, and
then optimal actions (e.g., switching phases) can be determined based on the
collected information. The DRL models fully ""trust"" that vehicles are sending
the true information to the signals, making the ATCS vulnerable to adversarial
attacks with falsified information. In view of this, this paper first time
formulates a novel task in which a group of vehicles can cooperatively send
falsified information to ""cheat"" DRL-based ATCS in order to save their total
travel time. To solve the proposed task, we develop CollusionVeh, a generic and
effective vehicle-colluding framework composed of a road situation encoder, a
vehicle interpreter, and a communication mechanism. We employ our method to
attack established DRL-based ATCS and demonstrate that the total travel time
for the colluding vehicles can be significantly reduced with a reasonable
number of learning episodes, and the colluding effect will decrease if the
number of colluding vehicles increases. Additionally, insights and suggestions
for the real-world deployment of DRL-based ATCS are provided. The research
outcomes could help improve the reliability and robustness of the ATCS and
better protect the smart mobility systems","Attacking Deep Reinforcement Learning-Based Traffic Signal Control
  Systems with Colluding Vehicles",http://arxiv.org/abs/2111.02845,,,,core
443830597,2021-01-15T00:00:00,"In recent years, street parking in prohibited areas has become a social problem, especially
in urban and tourist areas. In addition, because street parking can cause traffic congestion and
accidents, real-time detection is required. The detection of street parking has been previously
implemented on the basis of comparisons of videos recorded by fixed-point cameras. However,
this approach has a limited detection area and low accuracy. To overcome these problems, this
study aims towards a real-time street parking detection system that uses dashboard camera
videos. We propose a machine learning method based on the characteristics of on-street parked
vehicles derived by transforming images into text. The object detection model YOLOv3 was
used to analyze videos. We created a dataset based on the coordinate information of 1765
vehicles and the recording vehicle information. We also created a model using random forest
and logistic regression algorithms and evaluated it using the holdout and stratified 5-fold
validation methods. F-measure values of up to 92% and 89% were obtained for the two types
of model, respectively. These results confirm the effectiveness of the proposed street parking
detection method based on bounding boxes and recording vehicle data",A Method for Detecting Street Parking Using Dashboard Camera Videos,,'MYU K.K.',10.18494/SAM.2021.2998,"[{'title': None, 'identifiers': ['issn:2435-0869', '2435-0869']}]",core
475597648,2021-06-28T00:00:00,"International audienceA promising potential of Unmanned Aerial Vehicles (UAV) in 5G networks is to act as Aerial Base Stations (ABSs) that dynamically extend terrestrial base stations coverage without overloading the infrastructure. However, coverage extension faces crucial challenges such as user mobility and determining the best coordinates for new base station deployment. In this paper, we address this problem based on the prediction of users' spatial distribution that allows Aerial base stations (ABS) to adjust their position accordingly. We first analyze the performance of two machine learning schemes (Long Short Term Memory (LSTM)-based encoder-decoder and self-attention-based Transformer) for user mobility prediction based on a real DataSet. Then, we use these schemes to enhance the ABS deployment algorithm. Numerical results reveal significant gains when applying the proposed mobility prediction models over traditional deployment algorithms. In four hours of the day, both the Transformer and LSTM based models show, respectively, more than 31% and 22% gain in coverage rates compared to regular deployment schemes",Mobility Prediction For Aerial Base Stations for a Coverage Extension in 5G Networks,,'Institute of Electrical and Electronics Engineers (IEEE)',10.1109/IWCMC51323.2021.9498892,,core
395002393,2021-02-22T00:00:00,"Presented online February 22, 2021, 10:00 a.m.-10:30 a.m.National Symposium on Predicting Emergence of Virulent Entities by Novel Technologies (PREVENT) : What Advances In Science, Technology, And Human Behavior Will Enable Prediction And Prevention Of Future Pandemics?The Honorable Sethuraman Panchanathan is a computer scientist and engineer and the 15th director of the U.S. National Science Foundation (NSF). Panchanathan was nominated to this position by the President of the United States in 2019 and subsequently unanimously confirmed by the U.S. Senate on June 18, 2020. NSF is an $8.5B independent federal agency and the only government agency charged with advancing all fields of scientific discovery, technological innovation and STEM education. Panchanathan is a leader in science, engineering and education with more than three decades of experience. He has a distinguished career in both higher education and government, where he has designed and built knowledge enterprises, which advance research innovation, strategic partnerships, entrepreneurship, global development and economic growth. Panchanathan previously served as the executive vice president of the Arizona State University (ASU) Knowledge Enterprise, where he was also chief research and innovation officer. He was also the founder and director of the Center for Cognitive Ubiquitous Computing at ASU. Under his leadership, ASU increased research performance fivefold, earning recognition as the fastest growing and most innovative research university in the U.S. Prior to joining NSF, Panchanathan served on the National Science Board as chair of the Committee on Strategy and as a member of the External Engagement and National Science and Engineering Policy committees. Additionally, he served on the National Advisory Council on Innovation and Entrepreneurship. He was chair of the Council on Research of the Association of Public and Land-grant Universities and co-chair of the Extreme Innovation Taskforce of the Global Federation of Competitiveness Councils. Arizona's Governor appointed Panchanathan as senior advisor for science and technology in 2018. He was the editor-in-chief of the IEEE Multimedia Magazine and editor/associate editor of several international journals. Panchanathan's scientific contributions have advanced the areas of human-centered multimedia computing, haptic user interfaces, person-centered tools and ubiquitous computing technologies for enhancing the quality of life for individuals with different abilities; machine learning for multimedia applications; medical image processing; and media processor designs. He has published close to 500 articles in refereed journals and conference proceedings, and has mentored more than 150 graduate students, postdocs, research engineers and research scientists, many now occupy leading positions in academia and industry. For his scientific contributions, Panchanathan has received numerous awards, such as Distinguished Alumnus Awards and the Governor's Innovator of the Year for Academia Award for his development of information technology centric assistive and rehabilitative environments to assist individuals with visual impairments. Panchanathan is a fellow of the National Academy of Inventors, where he also served as vice president for strategic initiatives. He is also a fellow of the American Association for the Advancement of Science, the Canadian Academy of Engineering, the Association for Computing Machinery, the Institute of Electrical and Electronics Engineers and the Society of Optical Engineering. Panchanathan is married to Sarada ""Soumya"" Panchanathan, an academic pediatrician and informatician, who has taught medical students, pediatric residents and informatics fellows. They have two adult children, Amritha and Roshan.Dr. Mitra Basu joined NSF’s Computer Information Science and Engineering (CISE) Directorate in 2008 as the Lead Program Director for the Expeditions in Computing program – the Center scale program in CISE. During her tenure at NSF, she has served as acting Deputy Division Director in CISE. Mitra is a co-Lead for NSF PIPP Working Group.Dr. Paul M. Torrens is a Professor in Tandon's Department of Computer Science and Engineering and the Center for Urban Science + Progress at New York University. His work centers on the development and application of modeling and simulation tools for exploring and explaining complex urban systems, intricacies of behavior at the interface between cities and people, and emerging cyberinfrastructure for urban spaces and places. Torrens was the recipient of a Faculty Early Career Development Award from the National Science Foundation in 2007, and in 2008 George W. Bush presented him with the Presidential Early Career Award for Scientists and Engineers.Dr. Krista Rule Wigginton is an associate professor of Civil and Environmental Engineering at the University of Michigan. Prior to joining the faculty at UM, she was an assistant professor at the University of Maryland, College Park from 2011-2012. Her research focuses on applications of environmental biotechnology in drinking water and wastewater treatment. In particular, her research group develops new methods to detect and analyze the fate of emerging pollutants in the environment. Dr. Wigginton received her B.S. degree in Chemistry from the University of Idaho, and her M.S. and Ph.D. degrees in Environmental Engineering from Virginia Tech. After completing her Ph.D. degree, she worked as a postdoctoral researcher at École Polytechnique Fédérale de Lausanne (EPFL) in Switzerland from 2008-2011. Dr. Wigginton is a Co-Principal Investigator for the NSF grant award: Advancing Technologies and Improving Communication of Urine-Derived Fertilizers for Food Production within a Risk-Based Framework.Dr. John Yin is Professor of Chemical & Biological Engineering at the University of Wisconsin-Madison. He trained at Columbia University, earning dual bachelor’s degrees in the liberal arts and chemical engineering, while pursing cello studies at Juilliard School and piano studies at Columbia. Yin earned his PhD in chemical engineering at UC-Berkeley and pursued post-doctoral research as an Alexander von Humboldt Fellow working under Nobel laureate Manfred Eigen at the Max-Planck Institute for Biophysical Chemistry in Göttingen, Germany.  He started his academic career as an assistant professor at the Thayer School of Engineering, Dartmouth College, where he was awarded an NSF Young Investigator Award and Presidential Early Career Award in Science and Engineering (PECASE). He moved to UW-Madison as associate professor with tenure in 1998 and was promoted to full professor in 2004.  In his research Yin develops new experimental measures and computational models that aim to elucidate how viruses grow and how their infections spread.  Yin has co-authored more than 70 publications in areas of computational and experimental molecular and cell biology, with an emphasis on virus-host interactions.  He has further served on the NIH study section for Modeling and Analysis of Biological Systems (MABS).  In 2009 Yin was selected to lead the Systems Biology Theme of the Wisconsin Institutes for Discovery, a joint public-private venture to promote cross-disciplinary research, education and outreach at the UW-Madison, and he was recently recognized with a Vilas Distinguished Achievement Professorship for 2015-2020. Yin plays a 1918 Andre Bernard cello, he earned a 1725 rating at the US Table Tennis Association Open in Las Vegas, and he is an enthusiastic convert to sous vide quantitative cooking.Runtime: 39:09 minutesDr. B. Aditya Prakash is an Associate Professor in the College of Computing at the Georgia Institute of Technology (“Georgia Tech”). He received a Ph.D. from the Computer Science Department at Carnegie Mellon University in 2012, and a B.Tech (in CS) from the Indian Institute of Technology (IIT) -- Bombay in 2007. He has published one book, more than 80 papers in major venues, holds two U.S. patents and has given several tutorials at leading conferences. His work has also received multiple best-paper/best-of-conference selections and travel awards. His research interests include Data Science, Machine Learning and AI, with emphasis on big-data problems in large real-world networks and time-series, with applications to epidemiology, health, urban computing, security and the Web. His work has been supported by the National Science Foundation (NSF), the Centers for Disease Control (CDC), the Department of Energy (DoE), the National Security Agency (NSA), the National Endowment for Humanities (NEH) and various companies. Tools developed by his group have been in use in many places including ORNL, the CDC, Walmart and Facebook. He received a Facebook Faculty Award in 2015, was named as one of ‘AI Ten to Watch’ 2017 by IEEE, and received the NSF CAREER award in 2018. His work has also been highlighted by many media outlets and popular press. He was previously on the faculty of Computer Science at Virginia Tech. He is a member of the infectious diseases modeling MIDAS network and core-faculty at the Center for Machine Learning (ML@GT) and the Institute for Data Engineering and Science (IDEaS) at Georgia Tech. Aditya’s Twitter handle is @badityap.In the last year, the ongoing COVID-19 pandemic has severely disrupted the livelihoods of our planet’s human inhabitants, infecting over 85 million individuals, and causing nearly 2 million deaths. What actions should have been taken to minimize the severity of this pandemic (and others before it in the past decades such as Zika, SARS and Ebola)? In retrospect, many actions could have played key roles: environmental monitoring for potential animal-to-human infection spillovers, establishment of pipelines for rapid vaccine development and optimal deployment and distribution, designing data science tools to accurately forecast trajectories, fast and adaptive syndromic surveillance and behavior tracking, designing and timing effective interventions, training susceptible individuals for measures needed to inhibit the spread of infectious agents, and others. What lessons have been learned and what gaps in our knowledge, methodologies, technologies, and policies remain?National Science Foundation (U.S.","PRedicting Emergence Of Virulent Entities By Novel Technologies (PREVENT) Symposium - Opening Remarks, Welcome Statement, and Technical Background",,Georgia Institute of Technology,,,core
433828387,2021-05-26T00:00:00,"The Problem setting. Due to the rapid development of digital technologies, the issue of status settlement and the use of artificial intelligence technologies is especially relevant. This fact indicates the need and importance of finding answers to the question and aims to intensify and unite the efforts of the scientific community to address relevant issues. One of the areas of scientific research is the doctrinal development of new phenomena and processes that have arisen and are taking place in the state and legal sphere under the influence of digitalization of economics, management and law. The tasks of scientific research are to comprehend the impact of the digitization process on the state and legal sphere of society; law as such; assessment of the transformations that are taking place and identification of trends in their dynamics; forecasting the state of these phenomena in the future; formulation of fundamental and applied problems of legal science in terms of doctrinal development of the laws of development and functioning of law, state and legal sphere of society in the conditions of digital reality, determination of approaches to their solution.
Recent research and publications analysis. An analysis of recent research and publications shows that scientific research on this issue is carried out mainly within the economic, political, computer, legal sciences, although the problems and prospects of digitization of law require a deep and thorough philosophical, including philosophical and legal understanding. The rapid development of new technologies, in particular artificial intelligence technologies, the Internet of Things, cloud technologies, etc., is contributing to changes in current legislation. Today, advanced economies are already pondering the question of regulating the status and use of AI technologies. While these are only the first bold steps, in the future, all of these can affect global changes in the legal system - perhaps full-fledged comprehensive institutions of law, even the branches of law.
Paper objectiv. The purpose of this article is a philosophical and legal understanding of the impact of digitalization on the state and legal sphere of society and law as such.
Paper main body. One of the practical aspects of digitalization is the manifestation of the state’s ability to provide various services. If necessary, citizens receive certificates, records, statements, responses to electronic inquiries, electronic payments. Other practical aspects, provided that these technologies are used wisely, can improve welfare in education, public safety, and health. In addition, digital imaging can also help address common global issues, such as climate change and greater access to health care and mobility. 
At the same time, according to many researchers, along with the benefits of digital technologies, including artificial intelligence, new types of ethical issues are being raised, namely compliance with legal ethics standards by artificial intelligence systems and justice, the most important of which are respect for human rights and democratic values. , as well as the danger of transferring prejudices from the analog to the digital world. Researchers have linked the legal challenges of using artificial intelligence technologies in legal practice to a number of issues. In particular, with such as: ensuring data confidentiality; access to confidential law enforcement information; lack of regulatory framework for the use of artificial intelligence systems in legal practice; protection of intellectual property; risk assessment of the use of artificial intelligence systems by a lawyer when working with a client; other potential problems of lawyer’s liability; dangers of unauthorized access and modification of artificial intelligence systems by attackers; damage to artificial intelligence systems by malicious virus programs; violation of the terms of providing advice from artificial intelligence systems in case of technical problems, etc. Therefore, the development of systems that transparently use artificial intelligence and are responsible for their results is critical. Artificial intelligence systems must function properly and safely. 
According to experts, the unresolved in Ukraine of many political and legal issues related to the rapid development of the information and communication sphere with the advent of digital technologies has become dangerous. It is obvious that the transformations in society associated with these processes require new approaches to the development of national policies for the digitalization of society, which should be based on international agreements. Due to these transformations, there is a need to develop strategic documents that will regulate this area. These documents should be flexible and designed to take into account the maximum amount of data, as well as ensure the free development of innovative technologies and prevent possible risks.
Issues of development of the digital economy and society of Ukraine do not fully meet today’s conditions, not enough account is taken of the transformations that have emerged and are currently taking place both in law and in the field of legal regulation under the influence of digitalization. Digital technologies are able to change the image of law, to influence its regulatory potential and efficiency, to open the way or to block its action in new dimensions of social reality. Traditional rather than digital vision of law, legal technologies and certain types of legal activity by legislators is a consequence of the lack of relevant scientific developments that will identify and explain the impact of the digitization process on the law and the legal sphere of society. The practical need for this kind of research is now greater than ever. In order to satisfy it, scientists should intensify work in this direction.
Conclusions of the research. The new digital reality puts forward new requirements for legal science and legal practice, including the development of effective tools and models of legal regulation of various spheres of public life. In modern conditions, law becomes not only a means, a tool that provides digitalization of the economy, government and other segments of social life, but also the object of digitalization. With the development of digital technologies, the contradiction between the need for quality both in terms of form and content of regulations, as well as the ability to meet it in a short time. The task of the state is both to provide favorable conditions conducive to digitalization and to create opportunities for their implementation.В статье осуществлено осмысление влияния процесса цифровизации на право как таковое. Показано, что наряду с преимуществами цифровых технологий, в частности искусственного интеллекта, возникают новые типы этических проблем и вопросы справедливости. Обоснована необходимость определения подходов к решению новых задач юридической науки и совершенствования нормативно-правовой базы для защиты цифровых прав граждан.В статті здійснено осмислення впливу процесу цифровізації на право як таке. Показано, що поряд із перевагами цифрових технологій, зокрема штучного інтелекту, порушуються нові типи етичних проблем та питання справедливості. Обґрунтовано необхідність визначення підходів до розв’язання нових завдань юридичної науки та вдосконалення нормативно-правової бази для захисту цифрових прав громадян",ПРАВО В ЦИФРОВІЙ РЕАЛЬНОСТІ,,'Yaroslav Mudryi National Law University',,,core
467106757,2021-01-01T00:00:00,"Cyber-Physical Systems (CPSs) are cross-domain, multi-model, advance information systems that play a significant role in many large-scale infrastructure sectors of smart cities public services such as traffic control, smart transportation control, and environmental and noise monitoring systems. Such systems, typically, involve a substantial number of sensor nodes and other devices that stream and exchange data in real-time and usually are deployed in uncontrolled, broad environments.
Thus, unexpected measurements may occur due to several internal and external factors, including noise, communication errors, and hardware failures, which may compromise these systems quality of data and raise serious concerns related to safety, reliability, performance, and security. In all cases, these unexpected measurements need to be carefully interpreted and managed based on domain knowledge and computational models.
Therefore, in this research, data quality challenges were investigated, and a comprehensive, proof of concept, data quality management system was developed to tackle unaddressed data quality challenges in large-scale CPSs. The data quality management system was designed to address data quality challenges associated with detecting: sensor nodes measurement errors, sensor nodes hardware failures, and mismatches in sensor nodes spatial and temporal contextual attributes. Detecting sensor nodes measurement errors associated with the primary data quality dimensions of accuracy, timeliness, completeness, and consistency in large-scale CPSs were investigated using predictive and anomaly analysis models via utilising statistical and machine-learning techniques. Time-series clustering techniques were investigated as a feasible mean for detecting long-segmental outliers as an indicator of sensor nodes’ continuous halting and incipient hardware failures. Furthermore, the quality of the spatial and temporal contextual attributes of sensor nodes observations was investigated using timestamp analysis techniques.
The different components of the data quality management system were tested and calibrated using benchmark time-series collected from a high-quality, temperature sensor network deployed at the University of East London. Furthermore, the effectiveness of the proposed data quality management system was evaluated using a real-world, large-scale environmental monitoring network consisting of more than 200 temperature sensor nodes distributed around London.
The data quality management system achieved high accuracy detection rate using LSTM predictive analysis technique and anomaly detection associated with DBSCAN. It successfully identified timeliness and completeness errors in sensor nodes’ measurements using periodicity analysis combined with a rule engine. It achieved up to 100% accuracy in detecting potentially failed sensor nodes using the characteristic-based time-series clustering technique when applied to two days or longer time-series window. Timestamp analysis was adopted effectively for evaluating the quality of temporal and spatial contextual attributes of sensor nodes observations, but only within CPS applications in which using gateway modules is possible",Data Quality Management in Large-Scale Cyber-Physical Systems,https://core.ac.uk/download/467106757.pdf,'Indiana University Mathematics Journal',10.15123/uel.8990y,,core
337295329,2021-02-27T00:00:00,"Space agencies and private companies prepare the beginning of human space
exploration for the 2030s with missions to put the first human on the Mars
surface. The absence of gravity and radiation, along with distance, isolation
and hostile environments, are expected to increase medical events where
previously unseen manifestations may arise. The current healthcare strategy
based on telemedicine and the possibility to stabilize and transport the
injured crewmember to a terrestrial definitive medical facility is not
applicable in exploration class missions. Therefore, the need for deploying the
full autonomous capability to solve medical emergencies may guide the design of
future onboard healthcare systems. We present ten basic principles and concept
design of a software suite to bring onboard decision support to help the crew
dealing with medical emergencies taking into consideration physiological
disturbances in space and spaceflight restrictions. 1) give real-time support
for emergency medical decision making, 2) give patient-specific advice for
executive problem-solving, 3) take into account available information from life
support and monitoring of crewmembers, 4) be fully autonomous from remote
facilities, 5) continuously adapt predictions to physiological disturbance and
changing conditions, 6) optimize emergency medical decision making in terms of
mission fundamental priorities, 7) take into account medical supplies and
equipment on board, 8) apply health standards for the level of care V, 9)
implement ethics responsibilities for spaceflights, and 10) apply ethical
standards for artificial intelligence. Based on these principles, we propose an
autonomous clinical decision support system (CDSS) to provide real-time advice
for emergency medical interventions on board of space exploration missions.Comment: 35 pages, 1 figure, 3 tables, 60 reference","Basic principles and concept design of a real-time clinical decision
  support system for managing medical emergencies on missions to Mars",http://arxiv.org/abs/2010.07029,,,,core
372716697,2021-01-05T00:00:00,"Machine learning models nowadays play a crucial role for many applications in business and industry. However, models only start adding value as soon as they are deployed into production. One challenge of deployed models is the effect of changing data over time, which is often described with the term concept drift. Due to their nature, concept drifts can severely affect the prediction performance of a machine learning system. In this work, we analyze the effects of concept drift in the context of a real-world data set. For efficient concept drift handling, we introduce the switching scheme which combines the two principles of retraining and updating of a machine learning model. Furthermore, we systematically analyze existing regular adaptation as well as triggered adaptation strategies. The switching scheme is instantiated on New York City taxi data, which is heavily influenced by changing demand pattern over time. We can show that the switching scheme outperforms all other baselines and delivers promising prediction results",Switching Scheme: A Novel Approach for Handling Incremental Concept Drift in Real-World Data Sets,,'HICSS Conference Office',10.24251/HICSS.2021.120,,core
475054613,2021-06-25T00:00:00,"Part 18: Smart Blockchain Applications/CybersecurityInternational audienceThis paper presents a Validation and Verification (V&V) model of Data Marketplaces. Data is extracted from the sensors embedded within the Smart city, infrastructure, or building via Application Programming Interfaces (APIs) and inserted into a Data Marketplace. The technology is based on smart contracts deployed on a private ethereum blockchain. Current issues with data in Smart cities, infrastructure, buildings, or any real estate, are the difficulty of its access and retrieval, therefore integration; its quality in terms of meaningful information; its large quantity with a reduced coverage in terms of systems and finally its authenticity, as data can be manipulated for economic advantage. In order to address these issues, this paper proposes a Data Marketplace model with a hierarchical process for data validation and verification where each stage adds a layer of data abstraction, value-added services and authenticity based on Artificial Intelligence. By using a blockchain, this presented approach is based on a decentralised method where each stakeholder stores the data. The proposed model is validated in a real application with live data: Newcastle urban observatory smart city project",Validation and Verification of Data Marketplaces,,'Springer Science and Business Media LLC',10.1007/978-3-030-79150-6_61,,core
428286733,2021-01-01T00:00:00,"Abstract

Real-world data streams pose a unique challenge to the implementation of machine learning (ML) models and data analysis. A notable problem that has been introduced by the growth of Internet of Things (IoT) deployments across the smart city ecosystem is that the statistical properties of data streams can change over time, resulting in poor prediction performance and ineffective decisions. While concept drift detection methods aim to patch this problem, emerging communication and sensing technologies are generating a massive amount of data, requiring distributed environments to perform computation tasks across smart city administrative domains. In this article, we implement and test a number of state-of-the-art active concept drift detection algorithms for time series analysis within a distributed environment. We use real-world data streams and provide critical analysis of results retrieved. The challenges of implementing concept drift adaptation algorithms, along with their applications in smart cities, are also discussed",Concept drift adaptation techniques in distributed environment for real-world data streams,,Multidisciplinary Digital Publishing Institute,,,core
480168048,2021-01-01T00:00:00,"Shared e-mobility services have been widely tested and piloted in cities across the globe, and already woven into the fabric of modern urban planning. This paper studies a practical yet important problem in those systems: how to deploy and manage their infrastructure across space and time, so that the services are ubiquitous to the users while sustainable in profitability. However, in real-world systems evaluating the performance of different deployment strategies and then finding the optimal plan is prohibitively expensive, as it is often infeasible to conduct many iterations of trial-and-error. We tackle this by designing a high-fidelity simulation environment, which abstracts the key operation details of the shared e-mobility systems at fine-granularity, and is calibrated using data collected from the real-world. This allows us to try out arbitrary deployment plans to learn the optimal given specific context, before actually implementing any in the real-world systems. In particular, we propose a novel multi-agent neural search approach, in which we design a hierarchical controller to produce tentative deployment plans. The generated deployment plans are then tested using a multi-simulation paradigm, i.e., evaluated in parallel, where the results are used to train the controller with deep reinforcement learning. With this closed loop, the controller can be steered to have higher probability of generating better deployment plans in future iterations. The proposed approach has been evaluated extensively in our simulation environment, and experimental results show that it outperforms baselines e.g., human knowledge, and state-of-the-art heuristic-based optimization approaches in both service coverage and net revenue",Deployment optimization for shared e-mobility systems with multi-agent deep neural search,https://core.ac.uk/download/480168048.pdf,'Institute of Electrical and Electronics Engineers (IEEE)',,,core
475054606,2021-06-25T00:00:00,"Part 18: Smart Blockchain Applications/CybersecurityInternational audienceThis paper presents a Validation and Verification (V&V) model of Data Marketplaces. Data is extracted from the sensors embedded within the Smart city, infrastructure, or building via Application Programming Interfaces (APIs) and inserted into a Data Marketplace. The technology is based on smart contracts deployed on a private ethereum blockchain. Current issues with data in Smart cities, infrastructure, buildings, or any real estate, are the difficulty of its access and retrieval, therefore integration; its quality in terms of meaningful information; its large quantity with a reduced coverage in terms of systems and finally its authenticity, as data can be manipulated for economic advantage. In order to address these issues, this paper proposes a Data Marketplace model with a hierarchical process for data validation and verification where each stage adds a layer of data abstraction, value-added services and authenticity based on Artificial Intelligence. By using a blockchain, this presented approach is based on a decentralised method where each stakeholder stores the data. The proposed model is validated in a real application with live data: Newcastle urban observatory smart city project",Validation and Verification of Data Marketplaces,,'Springer Science and Business Media LLC',10.1007/978-3-030-79150-6_61,,core
387304918,2021-01-07T00:00:00,"Cyber-physical systems (CPS) and Internet-of-Things (IoT) devices are
increasingly being deployed across multiple functionalities, ranging from
healthcare devices and wearables to critical infrastructures, e.g., nuclear
power plants, autonomous vehicles, smart cities, and smart homes. These devices
are inherently not secure across their comprehensive software, hardware, and
network stacks, thus presenting a large attack surface that can be exploited by
hackers. In this article, we present an innovative technique for detecting
unknown system vulnerabilities, managing these vulnerabilities, and improving
incident response when such vulnerabilities are exploited. The novelty of this
approach lies in extracting intelligence from known real-world CPS/IoT attacks,
representing them in the form of regular expressions, and employing machine
learning (ML) techniques on this ensemble of regular expressions to generate
new attack vectors and security vulnerabilities. Our results show that 10 new
attack vectors and 122 new vulnerability exploits can be successfully generated
that have the potential to exploit a CPS or an IoT ecosystem. The ML
methodology achieves an accuracy of 97.4% and enables us to predict these
attacks efficiently with an 87.2% reduction in the search space. We demonstrate
the application of our method to the hacking of the in-vehicle network of a
connected car. To defend against the known attacks and possible novel exploits,
we discuss a defense-in-depth mechanism for various classes of attacks and the
classification of data targeted by such attacks. This defense mechanism
optimizes the cost of security measures based on the sensitivity of the
protected resource, thus incentivizing its adoption in real-world CPS/IoT by
cybersecurity practitioners.Comment: This article has been accepted in IEEE Transactions on Emerging
  Topics in Computing. 17 pages, 12 figures, IEEE copyrigh","SHARKS: Smart Hacking Approaches for RisK Scanning in Internet-of-Things
  and Cyber-Physical Systems based on Machine Learning",http://arxiv.org/abs/2101.02780,'Institute of Electrical and Electronics Engineers (IEEE)',10.1109/TETC.2021.3050733,,core
478618446,2021-09-21T00:00:00,"In support of crisis management new technologies are continuously being developed. The remaining challenge for practitioner organizations is not only to identify suitable solutions to close their gaps, but also to test and evaluate its benefit before the disaster strikes. The EU-funded project DRIVER+ (Driving Innovation in Crisis Management for European Resilience) has therefore designed a methodical and technical environment to assess innovation in crisis management in a realistic but non-operational setup through trials. 

To bridge theoretical potential and practical implementation, the Center for Satellite Based Crisis Information (ZKI) at the German Aerospace Center (DLR) has been working closely with its users for years to apply new methods from remote sensing research to generate up-to-date situation information for civil security applications and disaster response products. In DRIVER+, ZKI interdisciplinary teamed up with colleagues at DLR to demonstrate near real-time contextual routing for crisis response. Experienced practitioners assessed its added value during a trial focusing on major urban flooding events. Systematically collected data through the DRIVER+ Test-bed approved that DLR’s system could improve transport planning and inter-agency situational awareness. Together with the Bavarian Red Cross, ZKI further tested the application of 3D situational awareness in an earthquake scenario during the EU civil protection exercise IRONORE2019. We will present the results of the two exercises and how user feedback is driving the further development of ZKI products. In addition, we will discuss how new developments as 3D mapping, AI and web data fusion will provide further opportunities for remote sensing in future to support emergency responders even more effectively in complex crisis scenarios",New Perspectives for Emergency Response: Lessons Learned on Crisis Mapping from Trials and Exercises,,,,,core
478035639,2021-07-16T06:55:09,"The energy industry is going through challenging times of disruptive changes caused by decarbonization, decentralization, and digitalization. As the energy value chain is restructuring itself to accommodate the growing penetration of renewables, increasing number of independent power producers, and augmented self-consumption, new energy management approaches are required to accomplish energy transition. With the Fourth Industrial Revolution underway, it becomes evident that digitalization is the key to increase energy efficiency and ensure stable, reliable, and secure operations of the electric grid. Due to the energy industry's massive inertia, most energy utilities are missing the real momentum for unleashing large-scale digitalization enabled by Information and Communication Technology (ICT). This thesis proposes a set of ICT-based software applications, models, and tools aiming to bridge the gap between strategic roadmaps focused on the energy industry's digitalization and their actual implementation in real-world scenarios through digital energy services. The research is conducted within the designed ICT-based smart building and smart community frameworks, the modular structure and scalability of which can serve as a backbone for future digital energy management solutions. The introduction of a novel unsupervised load disaggregation approach helps raise awareness of one's energy-related behavior and understand what drives the energy usage in residential households without compromising privacy and security. Showcasing algorithm's performance on real-world datasets from Norway and Germany highlights compliance with state-of-the-art disaggregation accuracy and reduced computational costs. The development of machine learning-based supervised and unsupervised building occupancy forecasting algorithms with prediction accuracies beyond 97% helps identify best-suited windows for energy-saving opportunities and deliver insights into one's presence and absence patterns. Built on top of that occupancy-centric rule-based heating and air conditioning automation algorithm strives to unlock the buildings' massive potential for energy savings without compromising the occupants' thermal comfort. Simulations on real-world datasets collected in Portugal demonstrate more than 15% potential energy savings. Zooming out from smart buildings towards smart communities, we focus on the important role of intelligent green mobility in supporting further digitalization of the electric power sector. To overcome the inconveniences posed by the sparsity of charging infrastructure and facilitate the adoption of Electric Vehicles (EVs), we present a reinforcement learning (RL)-based EV-specific routing method that guarantees paths' energy feasibility in a graph-theoretical context. Consequently, we propose several deep RL algorithms to control EV charging with the aim to increase renewables' self-consumption and EV drivers' satisfaction. Benchmarking against rule-based and model predictive control demonstrates RL's superior computational performance and better fitness for future mobility systems. Finally, we introduce an innovative decentralized blockchain-supported framework that enables secure and reliable accounting of energy exchanges within the smart community. Implementing it in a demonstration site in Switzerland shows blockchain's potential to reduce EV charging costs, transform the market's business model, and facilitate the large-scale deployment of EVs",The digitalization of energy systems: towards higher energy efficiency,,"Lausanne, EPFL",10.5075/epfl-thesis-9328,,core
405616918,2021-01-01T00:00:00,"In recent years, we have assisted to an impressive advance of augmented reality systems
and computer vision algorithms, based on image processing and artificial intelligence. Thanks to
these technologies, mainstream smartphones are able to estimate their own motion in 3D space
with high accuracy. In this paper, we exploit such technologies to support autonomous mobility
of people with visual disabilities, identifying pre-defined virtual paths and providing context
information, reducing the distance between digital and real world. In particular, we present
ARIANNA+, an extension of ARIANNA, a system explicitly designed for visually impaired
people for indoor and outdoor localization and navigation. While ARIANNA is based on the
assumption that landmarks, such as QR codes, and physical paths (composed of colored tapes,
painted lines, or tactile pavings) are deployed in the environment and recognized by the camera
of a common smartphone, ARIANNA+ eliminates the need of any physical support thanks to the
ARKit library which we exploit to build a completely virtual path. Moreover, ARIANNA+ adds
the possibility for the users to have enhanced interactions with the surrounding environment,
through Convolutional Neural Networks (CNNs) trained to recognize objects or buildings and
enabling the possibility of accessing contents associated to them. By using a common smartphone
as a mediation instrument with the environment, ARIANNA+ leverages augmented reality and
machine learning for enhancing physical accessibility. The proposed system allows visually
impaired people to easily navigate in indoor and outdoor scenarios simply by loading a previously recorded virtual path and providing automatic guidance along the route, through haptic, speech,
and sound feedback",A navigation and augmented reality system for visually impaired people,https://core.ac.uk/download/405616918.pdf,'MDPI AG',10.3390/s21093061,,core
387306082,2021-01-11T00:00:00,"A novel coronavirus disease has emerged (later named COVID-19) and caused the
world to enter a new reality, with many direct and indirect factors influencing
it. Some are human-controllable (e.g. interventional policies, mobility and the
vaccine); some are not (e.g. the weather). We have sought to test how a change
in these human-controllable factors might influence two measures: the number of
daily cases against economic impact. If applied at the right level and with
up-to-date data to measure, policymakers would be able to make targeted
interventions and measure their cost. This study aims to provide a predictive
analytics framework to model, predict and simulate COVID-19 propagation and the
socio-economic impact of interventions intended to reduce the spread of the
disease such as policy and/or vaccine. It allows policymakers, government
representatives and business leaders to make better-informed decisions about
the potential effect of various interventions with forward-looking views via
scenario planning. We have leveraged a recently launched open-source COVID-19
big data platform and used published research to find potentially relevant
variables (features) and leveraged in-depth data quality checks and analytics
for feature selection and predictions. An advanced machine learning pipeline
has been developed armed with a self-evolving model, deployed on a modern
machine learning architecture. It has high accuracy for trend prediction
(back-tested with r-squared) and is augmented with interpretability for deeper
insights","Impact of Interventional Policies Including Vaccine on Covid-19
  Propagation and Socio-Economic Factors",http://arxiv.org/abs/2101.03944,,,,core
363996402,2021-03-01T00:00:00,"International audienceIn this paper, we introduce a novel dynamic controller assignment algorithm targeting connected vehicle services and applications, also known as Internet of Vehicles (IoV). The proposed approach considers a hierarchically distributed control plane, decoupled from the data plane, and uses vehicle location and control traffic load to perform controller assignment dynamically. We model the dynamic controller assignment problem as a multi-agent Markov game and solve it with cooperative multi-agent deep reinforcement learning. Simulation results using real-world vehicle mobility traces show that the proposed approach outperforms existing ones by reducing control delay as well as packet loss. Index Terms-Internet of Vehicles (IoV), Software Defined Networking (SDN), multi-agent deep reinforcement learning, controller assignment",Dynamic Controller Assignment in Software Defined Internet of Vehicles through Multi-Agent Deep Reinforcement Learning,https://core.ac.uk/download/363996402.pdf,'Institute of Electrical and Electronics Engineers (IEEE)',10.1109/TNSM.2020.3047765,,core
478867994,2021-09-15T15:44:17,"The Optimal Transport (OT) problem naturally arises in various machine learning problems, where one needs to align data from multiple sources. For example, the training data and application scenarios oftentimes have a domain gap, e.g., the training data is annotated photos collected in the daytime, yet the application scenario is in dark hours. In this case, we need to align the two datasets, so that the annotation information can be shared across them. During my Ph.D. study, I propose scalable algorithms for efficient OT computation, and its novel applications in end-to-end learning.

For OT computation, I consider both discrete cases and continuous cases. For the discrete cases, I develop an Inexact Proximal point method for exact Optimal Transport problem (IPOT) with the proximal operator approximately evaluated at each iteration using projections to the probability simplex. The algorithm (a) converges to exact Wasserstein distance with theoretical guarantee and robust regularization parameter selection, (b) alleviates numerical stability issue, (c) has similar computational complexity to Sinkhorn, and (d) avoids the shrinking problem when apply to generative models. Furthermore, a new algorithm is proposed based on IPOT to obtain sharper Wasserstein barycenter.

For the continuous cases, I propose an implicit generative learning-based framework called SPOT (Scalable Push-forward of Optimal Transport). Specifically, we approximate the optimal transport plan by a pushforward of a reference distribution, and cast the optimal transport problem into a minimax problem. We then can solve OT problems efficiently using primal dual stochastic gradient-type algorithms.

To explore the connections between OT and end-to-end learning, I developed a differentiable top-k operator, and a differentiable permutation step.

For the top-k operation, i.e., finding the k largest or smallest elements from a collection of scores, is an important model component used in information retrieval, machine learning, and data mining. However, if the top-k operation is implemented in an algorithmic way, e.g., using bubble algorithm, the resulting model cannot be trained in an end-to-end way using prevalent gradient descent algorithms. This is because these implementations typically involve swapping indices, whose gradient cannot be computed. Moreover, the corresponding mapping from the input scores to the indicator vector of whether this element belongs to the top-k set is essentially discontinuous. To address the issue, we propose a smoothed approximation, namely the SOFT (Scalable Optimal transport-based diFferenTiable) top-k operator. Specifically, our SOFT top-k operator approximates the output of the top-k operation as the solution of an Entropic Optimal Transport (EOT) problem. The gradient of the SOFT operator can then be efficiently approximated based on the optimality conditions of EOT problem. We apply the proposed operator to the k-nearest neighbors and beam search algorithms, and demonstrate improved performance.

For the differentiable permutation step, I connect optimal transport to a variant of regression problem, where the correspondence between input and output data is not available. Such shuffled data is commonly observed in many real-world problems. Taking flow cytometry as an example, the measuring instruments may not be able to maintain the correspondence between the samples and the measurements. Due to the combinatorial nature of the problem, most existing methods are only applicable when the sample size is small, and limited to linear regression models. To overcome such bottlenecks, we propose a new computational framework -- ROBOT -- for the shuffled regression problem, which is applicable to large data and complex nonlinear models. Specifically, we reformulate the regression without correspondence as a continuous optimization problem. Then by exploiting the interaction between the regression model and the data correspondence, we develop a hypergradient approach based on differentiable programming techniques. Such a hypergradient approach essentially views the data correspondence as an operator of the regression, and therefore allows us to find a better descent direction for the model parameter by differentiating through the data correspondence. ROBOT can be further extended to the inexact correspondence setting, where there may not be an exact alignment between the input and output data. Thorough numerical experiments show that ROBOT achieves better performance than existing methods in both linear and nonlinear regression tasks, including real-world applications such as flow cytometry and multi-object tracking.Ph.D",On Computation and Application of Optimal Transport,https://core.ac.uk/download/478867994.pdf,Georgia Institute of Technology,,,core
475070866,2021-06-01T00:00:00,"Multi-class classification is one of the major challenges in machine learning and an ongoing research issue. Classification algorithms are generally binary, but they must be extended to multi-class problems for real-world application. Multi-class classification is more complex than binary classification. In binary classification, only the decision boundaries of one class are to be known, whereas in multiclass classification, several boundaries are involved. The objective of this investigation is to propose a metaheuristic, optimized, multi-level classification learning system for forecasting in civil and construction engineering. The proposed system integrates the firefly algorithm (FA), metaheuristic intelligence, decomposition approaches, the one-against-one (OAO) method, and the least squares support vector machine (LSSVM). The enhanced FA automatically fine-tunes the hyperparameters of the LSSVM to construct an optimized LSSVM classification model. Ten benchmark functions are used to evaluate the performance of the enhanced optimization algorithm. Two binary-class datasets related to geotechnical engineering, concerning seismic bumps and soil liquefaction, are then used to clarify the application of the proposed system to binary problems. Further, this investigation uses multi-class cases in civil engineering and construction management to verify the effectiveness of the model in the diagnosis of faults in steel plates, quality of water in a reservoir, and determining urban land cover. The results reveal that the system predicts faults in steel plates with an accuracy of 91.085%, the quality of water in a reservoir with an accuracy of 93.650%, and urban land cover with an accuracy of 87.274%. To demonstrate the effectiveness of the proposed system, its predictive accuracy is compared with that of a non-optimized baseline model, single multi-class classification algorithms (sequential minimal optimization (SMO), the Multiclass Classifier, the Naïve Bayes, the library support vector machine (LibSVM) and logistic regression) and prior studies. The analytical results show that the proposed system is promising project analytics software to help decision makers solve multi-level classification problems in engineering applications",Metaheuristic Optimized Multi-Level Classification Learning System for Engineering Management,,'MDPI AG',10.3390/app11125533,"[{'title': 'Applied Sciences', 'identifiers': ['2076-3417', 'issn:2076-3417']}]",core
479191448,2021-08-01T00:00:00,"Abstract In this paper, the optimal demand response strategy of a commercial building‐based virtual power plant with real‐world implementation in heavily urbanised area is studied. Instead of modelling the decision‐making process as an optimisation problem, a reinforcement learning method is used to seek the optimal strategy, which could update its performance with minimal manpower manipulation. Specifically, the data collection from several commercial buildings, including hotel, shopping mall and office, in Huangpu district, Shanghai city is analysed to deploy the demand response program. Compared with the conventional demand response strategy based on optimisation, the learnt strategy does not rely on the forecasting information as input and could adapt to the changing demand response incentive automatically. It may not produce the best result every time, but can guarantee the benefit in a non‐deterministic way in long‐term operation. The real‐world deployment of the Huangpu virtual power plant involving hardware and software platform is also introduced, as well as its future development projection",Optimal demand response strategy of commercial building‐based virtual power plant using reinforcement learning,,'Institution of Engineering and Technology (IET)',10.1049/gtd2.12179,"[{'title': 'IET Generation Transmission & Distribution', 'identifiers': ['1751-8695', 'issn:1751-8687', '1751-8687', 'issn:1751-8695']}]",core
479305281,2021-07-01T00:00:00,"Currently, distribution system operators (DSOs) are asked to operate distribution grids, managing the rise of the distributed generators (DGs), the rise of the load correlated to heat pump and e-mobility, etc. Nevertheless, they are asked to minimize investments in new sensors and telecommunication links and, consequently, several nodes of the grid are still not monitored and tele-controlled. At the same time, DSOs are asked to improve the network’s resilience, looking for a reduction in the frequency and impact of power outages caused by extreme weather events. The paper presents a machine learning GIS-based approach to estimate a secondary substation’s load profiles, even in those cases where monitoring sensors are not deployed. For this purpose, a large amount of data from different sources has been collected and integrated to describe secondary substation load profiles adequately. Based on real measurements of some secondary substations (medium-voltage to low-voltage interface) given by Unareti, the DSO of Milan, and georeferenced data gathered from open-source databases, unknown secondary substations load profiles are estimated. Three types of machine learning algorithms, regression tree, boosting, and random forest, as well as geographic information system (GIS) information, such as secondary substation locations, building area, types of occupants, etc., are considered to find the most effective approach",Machine Learning and GIS Approach for Electrical Load Assessment to Increase Distribution Networks Resilience,,'MDPI AG',10.3390/en14144133,"[{'title': 'Energies', 'identifiers': ['issn:1996-1073', '1996-1073']}]",core
435117124,2021-06-11T00:00:00,"Stroke has become a leading cause of death and long-term disability in the world, and there is no effective treatment.Deep learning-based approaches have the potential to outperform existing stroke risk prediction models, they rely on large well-labeled data. Due to the strict privacy protection policy in health-care systems, stroke data is usually distributed among different hospitals in small pieces. In addition, the positive and negative instances of such data are extremely imbalanced. Transfer learning solves small data issue by exploiting the knowledge of a correlated domain, especially when multiple source are available.In this work, we propose a novel Hybrid Deep Transfer Learning-based Stroke Risk Prediction (HDTL-SRP) scheme to exploit the knowledge structure from multiple correlated sources (i.e.,external stroke data, chronic diseases data, such as hypertension and diabetes). The proposed framework has been extensively tested in synthetic and real-world scenarios, and it outperforms the state-of-the-art stroke risk prediction models. It also shows the potential of real-world deployment among multiple hospitals aided with 5G/B5G infrastructures.National Key R&D Program of China; National Nature Science Foundation of China; Natural Science Foundation of Guangdong Province; Guangdong “Pearl River Talent Recruitment Program”; Technology Research Project of Shenzhen City; Public Technology Platform of Shenzhen Cit",Stroke Risk Prediction with Hybrid Deep Transfer Learning Framework,,'Institute of Electrical and Electronics Engineers (IEEE)',10.1109/jbhi.2021.3088750,"[{'title': 'IEEE Journal of Biomedical and Health Informatics', 'identifiers': ['issn:2168-2194', '2168-2208', 'issn:2168-2208', '2168-2194']}]",core
389643195,2021-02-18T14:02:36,"Title from PDF of title page viewed March 1, 2021Dissertation advisor: Yugyung LeeVitaIncludes bibliographical references ( page 115-125)Thesis (Ph.D.)--School of Computing and Engineering. University of Missouri--Kansas City, 2020Deep learning has improved the state-of-the-art results in an ever-growing number of domains. This success heavily relies on the development of deep learning models--an experimental, iterative process that produces tens to hundreds of models before arriving at a satisfactory result. While there has been a surge in the number of tools and frameworks that aim at facilitating deep learning, the process of managing the models and their artifacts is still surprisingly challenging and time-consuming. Existing model-management solutions are either tailored for commercial platforms or require significant code changes. Moreover, most of the existing solutions address a single phase of the modeling lifecycle, such as experiment monitoring, while ignoring other essential tasks, such as model sharing and deployment. In this dissertation, we present a software system to facilitate and accelerate the deep learning lifecycle, named ModelKB. ModelKB can \textit{automatically} manage the modeling lifecycle end-to-end, including (1) monitoring and tracking experiments; (2) visualizing, searching for, and comparing models and experiments; (3)  deploying models locally and on the cloud; and (4) sharing and publishing trained models. Our system also provides a stepping-stone for enhanced reproducibility. ModelKB currently supports TensorFlow 2.0, Keras, and PyTorch, and it can be extended to other deep learning frameworks easily. A video demo is available at https://youtu.be/XWiJpSM_jvA.

Moreover, we study static call graphs to form a stepping-stone to facilitate the \textit{comprehension} of the overall lifecycle implementation (i.e., source code). Specifically, we introduce Code2Graph to facilitate the exploration and tracking of the implementation and its changes over time. Code2Graph is used to construct and visualize the call graph of a software codebase. We evaluate the functionality by analyzing and studying real software systems throughout their entire lifespan. The tool, evaluation results, and a video demo are available at https://goo.gl/8edZ64. 

Finally, we demonstrate a software system that brings together the contributions mentioned above to build a robust, open-collaborative platform for deep learning applications in the health domain, named Medl.AI. Medl.AI enables researchers and healthcare professionals to easily and efficiently: explore, share, reuse, and discuss deep learning models specific to the medical domain. We present six illustrative deep learning medical applications using Medl.AI. We conduct an online survey to assess the feasibility and benefits of Medl.AI. The user study suggests that Medl.AI provides a promising solution to open collaborative research and applications. Our live website is currently available at http://medl.ai.Introduction -- Background and Challenges -- Automated Management of the Modeling Lifecycle -- Facilitating Program Comprehension -- Medl.AI: An application of MODELKB -- Conclusion",Automated End-to-End Management of the Deep Learning Lifecycle,https://core.ac.uk/download/389643195.pdf,,,,core
337294633,2021-04-13T00:00:00,"Capacitated spatial clustering, a type of unsupervised machine learning
method, is often used to tackle problems in compressing, classifying, logistic
optimization and infrastructure optimization. Depending on the application at
hand, a wide set of extensions may be necessary in clustering.
  In this article we propose a number of novel extensions to PACK that is a
novel capacitated spatial clustering method. These extensions are relocation
and location preference of cluster centers, outliers, and non-spatial
attributes. The strength of PACK is that it can consider all of these
extensions jointly. We demonstrate the usefulness PACK with a real world
example in edge computing server placement for a city region with various
different set ups, where we take into consideration outliers, center placement,
and non-spatial attributes. Different setups are evaluated with summary
statistics on spatial proximity and attribute similarity. As a result, the
similarity of the clusters was improved at best by 53%, while simultaneously
the proximity degraded only 18%. In alternate scenarios, both proximity and
similarity were improved. The different extensions proved to provide a valuable
way to include non-spatial information into the cluster analysis, and attain
better overall proximity and similarity. Furthermore, we provide easy-to-use
software tools (rpack) for conducting clustering analyses",Capacitated spatial clustering with multiple constraints and attributes,http://arxiv.org/abs/2010.06333,,,,core
478236920,2021-09-29T00:00:00,"Emergency vehicles in service have right-of-way over all other vehicles.
Hence, all other vehicles are supposed to take proper actions to yield
emergency vehicles with active sirens. As this task requires the cooperation
between ears and eyes for human drivers, it also needs audio detection as a
supplement to vision-based algorithms for fully autonomous driving vehicles. In
urban driving scenarios, we need to know both the existence of emergency
vehicles and their relative positions to us to decide the proper actions. We
present a novel system from collecting the real-world siren data to the
deployment of models using only two cost-efficient microphones. We are able to
achieve promising performance for each task separately, especially within the
crucial 10m to 50m distance range to react (the size of our ego vehicle is
around 5m in length and 2m in width). The recall rate to determine the
existence of sirens is 99.16% , the median and mean angle absolute error is
9.64{\deg} and 19.18{\deg} respectively, and the median and mean distance
absolute error of 9.30m and 10.58m respectively within that range. We also
benchmark various machine learning approaches that can determine the siren
existence and sound source localization which includes direction and distance
simultaneously within 50ms of latency",Emergency Vehicles Audio Detection and Localization in AutonomousDriving,http://arxiv.org/abs/2109.14797,,,,core
387301819,2021-10-14T00:00:00,"Efficient data offloading plays a pivotal role in computational-intensive
platforms as data rate over wireless channels is fundamentally limited. On top
of that, high mobility adds an extra burden in vehicular edge networks (VENs),
bolstering the desire for efficient user-centric solutions. Therefore, unlike
the legacy inflexible network-centric approach, this paper exploits a
software-defined flexible, open, and programmable networking platform for an
efficient user-centric, fast, reliable, and deadline-constrained offloading
solution in VENs. In the proposed model, each active vehicle user (VU) is
served from multiple low-powered access points (APs) by creating a noble
virtual cell (VC). A joint node association, power allocation, and distributed
resource allocation problem is formulated. As centralized learning is not
practical in many real-world problems, following the distributed nature of
autonomous VUs, each VU is considered an edge learning agent. To that end,
considering practical location-aware node associations, a joint radio and power
resource allocation non-cooperative stochastic game is formulated. Leveraging
reinforcement learning's (RL) efficacy, a multi-agent RL (MARL) solution is
proposed where the edge learners aim to learn the Nash equilibrium (NE)
strategies to solve the game efficiently. Besides, real-world map data, with a
practical microscopic mobility model, are used for the simulation. Results
suggest that the proposed user-centric approach can deliver remarkable
performances in VENs. Moreover, the proposed MARL solution delivers
near-optimal performances with approximately 3% collision probabilities in case
of distributed random access in the uplink.Comment: This paper was rejected from JSAC; we need to address the reviewers'
  comment","Vehicular Network Slicing for Reliable Access and Deadline-Constrained
  Data Offloading: A Multi-Agent On-Device Learning Approach",http://arxiv.org/abs/2012.15545,,,,core
479488816,2021-10-30T00:00:00,"Numerous COVID-19 clinical decision support systems have been developed.
However many of these systems do not have the merit for validity due to
methodological shortcomings including algorithmic bias. Methods Logistic
regression models were created to predict COVID-19 mortality, ventilator status
and inpatient status using a real-world dataset consisting of four hospitals in
New York City and analyzed for biases against race, gender and age. Simple
thresholding adjustments were applied in the training process to establish more
equitable models. Results Compared to the naively trained models, the
calibrated models showed a 57% decrease in the number of biased trials, while
predictive performance, measured by area under the receiver/operating curve
(AUC), remained unchanged. After calibration, the average sensitivity of the
predictive models increased from 0.527 to 0.955. Conclusion We demonstrate that
naively training and deploying machine learning models on real world data for
predictive analytics of COVID-19 has a high risk of bias. Simple implemented
adjustments or calibrations during model training can lead to substantial and
sustained gains in fairness on subsequent deployment.Comment: 4 pages, 1 table","Identifying and mitigating bias in algorithms used to manage patients in
  a pandemic",http://arxiv.org/abs/2111.00340,,,,core
437438109,2021-10-01T00:00:00,"Smart card data has emerged in recent years and provide a comprehensive, and cheap source of information for planning and managing public transport systems. This paper presents a multi-stage machine learning framework to predict passengers’ boarding stops using smart card data. The framework addresses the challenges arising from the imbalanced nature of the data (e.g. many non-travelling data) and the ‘many-class’ issues (e.g. many possible boarding stops) by decomposing the prediction of hourly ridership into three stages: whether to travel or not in that one-hour time slot, which bus line to use, and at which stop to board. A simple neural network architecture, fully connected networks (FCN), and two deep learning architectures, recurrent neural networks (RNN) and long short-term memory networks (LSTM) are implemented. The proposed approach is applied to a real-life bus network. We show that the data imbalance has a profound impact on the accuracy of prediction at individual level. At aggregated level, FCN is able to accurately predict the rideship at individual stops, it is poor at capturing the temporal distribution of ridership. RNN and LSTM are able to measure the temporal distribution but lack the ability to capture the spatial distribution through bus lines",Multi-stage deep learning approaches to predict boarding behaviour of bus passengers,,'Elsevier BV',10.1016/j.scs.2021.103111,,core
475069406,2021-06-01T00:00:00,"Mounting interest in ambitious clean energy goals is exposing critical gaps in our understanding of onshore wind power potential. Conventional approaches to evaluating wind power technical potential at the national scale rely on coarse geographic representations of land area requirements for wind power. These methods overlook sizable spatial variation in real-world capacity densities (i.e., nameplate power capacity per unit area) and assume that potential installation densities are uniform across space. Here, we propose a data-driven approach to overcome persistent challenges in characterizing localized deployment potentials over broad extents. We use machine learning to develop predictive relationships between observed capacity densities and geospatial variables. The model is validated against a comprehensive data set of United States (U.S.) wind facilities and subjected to interrogation techniques to reveal that key explanatory features behind geographic variation of capacity density are related to wind resource as well as urban accessibility and forest cover. We demonstrate application of the model by producing a high-resolution (2 km × 2 km) national map of capacity density for use in technical potential assessments for the United States. Our findings illustrate that this methodology offers meaningful improvements in the characterization of spatial aspects of technical potential, which are increasingly critical to draw reliable and actionable planning and research insights from renewable energy scenarios",Spatially-Explicit Prediction of Capacity Density Advances Geographic Characterization of Wind Power Technical Potential,,'MDPI AG',10.3390/en14123609,"[{'title': 'Energies', 'identifiers': ['issn:1996-1073', '1996-1073']}]",core
475073843,2021-06-01T00:00:00,"This paper proposes the use of the FASSD-Net model for semantic segmentation of human silhouettes, these silhouettes can later be used in various applications that require specific characteristics of human interaction observed in video sequences for the understanding of human activities or for human identification. These applications are classified as high-level task semantic understanding. Since semantic segmentation is presented as one solution for human silhouette extraction, it is concluded that convolutional neural networks (CNN) have a clear advantage over traditional methods for computer vision, based on their ability to learn the representations of appropriate characteristics for the task of segmentation. In this work, the FASSD-Net model is used as a novel proposal that promises real-time segmentation in high-resolution images exceeding 20 FPS. To evaluate the proposed scheme, we use the Cityscapes database, which consists of sundry scenarios that represent human interaction with its environment (these scenarios show the semantic segmentation of people, difficult to solve, that favors the evaluation of our proposal), To adapt the FASSD-Net model to human silhouette semantic segmentation, the indexes of the 19 classes traditionally proposed for Cityscapes were modified, leaving only two labels: One for the class of interest labeled as person and one for the background. The Cityscapes database includes the category “human” composed for “rider” and “person” classes, in which the rider class contains incomplete human silhouettes due to self-occlusions for the activity or transport used. For this reason, we only train the model using the person class rather than human category. The implementation of the FASSD-Net model with only two classes shows promising results in both a qualitative and quantitative manner for the segmentation of human silhouettes",FASSD-Net Model for Person Semantic Segmentation,,'MDPI AG',10.3390/electronics10121393,"[{'title': 'Electronics', 'identifiers': ['issn:2079-9292', '2079-9292']}]",core
477674348,2021-01-01T00:00:00,"The progress in technology development over the past decades, both with respect to software and hardware, offers the vision of automated vehicles as means of achieving zero fatalities in traffic. However, the promises of this new technology – an increase in road safety, traffic efficiency, and user comfort – can only be realized if this technology is smoothly introduced into the existing traffic system with all its complexities, constraints, and requirements. SHAPE- IT will contribute to this major undertaking by addressing research questions relevant for the development and introduction of automated vehicles in urban traffic scenarios. Previous research has pointed out several research areas that need more attention for a successful implementation and deployment of human-centred vehicle automation in urban environments.In SHAPE-IT, for example, a better understanding of human behaviour and the underlying psychological mechanisms will lead to improved models of human behaviour that can help to predict the effects of automated systems on human behaviour already during system development. Such models can also be integrated into the algorithms of automated vehicles, enabling them to better understand the human interaction partners’ behaviours.Further, the development of vehicle automation is much about technology (software and hardware), but the users will be humans and they will interact with humans both inside and outside of the vehicle. To be successful in the development of automated vehicles functionalities, research must be performed on a variety of aspects. Actually, a highly interdisciplinary team of researchers, bringing together expertise and background from various scientific fields related to traffic safety, human factors, human-machine interaction design and evaluation, automation, computational modelling, and artificial intelligence, is likely needed to consider the human-technology aspects of vehicle automation.Accordingly, SHAPE-IT has recruited fifteen PhD candidates (Early Stage Researchers – ESRs), that work together to facilitate this integration of automated vehicles into complex urban traffic by performing research to support the development of transparent, cooperative, accepted, trustworthy, and safe automated vehicles. With their (and their supervisors’) different scientific background, the candidates bring different theoretical concepts and methodological approaches to the project. This interdisciplinarity of the project team offers the unique possibility for each PhD candidate to address research questions from a broad perspective – including theories and methodological approaches of other interrelated disciplines. This is the main reason why SHAPE-IT has been funded by the European Commission’s Marie Skłodowska-Curie Innovative Training Network (ITN) program that is aimed to train early state researchers in multidisciplinary aspects of research including transferable skills. With the unique scope of SHAPE-IT, including the human-vehicle perspective, considering different road-users (inside and outside of the vehicle), addressing for example trust, transparency, and safety, and including a wide range of methodological approaches, the project members can substantially contribute to the development and deployment of safe and appreciated vehicle automation in the cities of the future.To achieve the goal of interdisciplinary research, it is necessary to provide the individual PhD candidate with a starting point, especially on the different and diverse methodological approaches of the different disciplines. The empirical, user-centred approach for the development and evaluation of innovative automated vehicle concepts is central to SHAPE- IT. This deliverable (D1.1 “Methodological Framework for Modelling and Empirical Approaches”) provides this starting point. That is, this document provides a broad overview of approaches and methodologies used and developed by the SHAPE-IT ESRs during their research. The SHAPE-IT PhD candidates, as well as other researchers and developers outside of SHAPE-IT, can use this document when searching for appropriate methodological approaches, or simply get a brief overview of research methodologies often employed in automated vehicle research.The first chapter of the deliverable shortly describes the major methodological approaches to collect data relevant for investigating road user behaviour. Each subchapter describes one approach, ranging from naturalistic driving studies to controlled experiments in driving simulators, with the goal to provide the unfamiliar reader with a broad overview of the approach, including its scope, the type of data collected, and its limitations. Each subchapter ends with recommendations for further reading – literature that provide much more detail and examples.The second chapter explains four different highly relevant tools for data collection, such as interviews, questionnaires, physiological measures, and as other current tools (the Wizard of Oz paradigm and Augmented and Virtual Reality). As in the first chapter this chapter provides the reader with information about advantages and disadvantages of the different tools and with proposed further readings.The third chapter deals with computational models of human/agent interaction and presents in four subchapters different modelling approaches, ranging from models based on psychological mechanisms, rule-based and artificial intelligence models to simulation models of traffic interaction.The fourth chapter is devoted to Requirements Engineering and the challenge of communicating knowledge (e.g., human factors) to developers of automated vehicles. When forming the SHAPE-IT proposal it was identified that there is a lack of communication of human factors knowledge about the highly technical development of automated vehicles. This is why it is highly important that the SHAPE-IT ESRs get training in requirement engineering. Regardless of the ESRs working in academia or industry after their studies it is important to learn how to communicate and disseminate the findings to engineers.The deliverable ends with the chapter “Method Champions”. Here the expertise and association of the different PhD candidates with the different topics are made explicit to facilitate and encourage networking between PhDs with special expertise and those seeking support, especially with regards to methodological questions",Methodological Framework for Modelling and Empirical Approaches (Deliverable D1.1 in the H2020 MSCA ITN project SHAPE-IT),,SHAPE-IT Consortium,10.17196/shape-it/2021/02/d1.1,,core
389643187,2021-02-18T14:02:03,"Title from PDF of title page viewed March 11, 2021Dissertation advisors: Praveen Rao and Sejun SonVitaIncludes bibliographical references (page 101-116)Thesis (Ph.D.)--School of Computing and Engineering. University of Missouri--Kansas City, 2020A knowledge graph represents millions of facts and reliable information about people, places, and things. These knowledge graphs have proven their reliability and their usage for providing better search results; answering ambiguous questions regarding entities; and training semantic parsers to enhance the semantic relationships over the Semantic Web. However, while there exist a plethora of datasets on the Internet related to Food, Energy, and Water (FEW), there is a real lack of reliable methods and tools that can consume these resources. This hinders the development of novel decision-making applications utilizing knowledge graphs. In this dissertation, we introduce a novel tool, called FoodKG, that enriches FEW knowledge graphs using advanced machine learning techniques. Our overarching goal is to improve decision-making, knowledge discovery, and provide improved search results for data scientists in the FEW domains. Given an input knowledge graph (constructed on raw FEW datasets), FoodKG enriches it with semantically related triples,  relations, and images based on the original dataset terms and classes. FoodKG employs an existing graph embedding technique trained on a controlled vocabulary called AGROVOC, which is published by the Food and Agriculture Organization of the United Nations. AGROVOC includes terms and classes in the agriculture and food domains. As a result, FoodKG can enhance knowledge graphs with semantic similarity scores and relations between different classes, classify the existing entities, and allow FEW experts and researchers to use scientific terms for describing FEW concepts. The resulting model obtained after training on AGROVOC was evaluated against the state-of-the-art word embedding and knowledge graph embedding models that were trained on the same dataset. We observed that this model outperformed its competitors based on the Spearman Correlation Coefficient score. 

We introduced Federated Learning (FL) techniques to further extend our work and include private datasets by training smaller version of the models at each dataset site without accessing the data and then aggregating all the models at the server-side. We propose an algorithm that we called RefinedFed to further extend the current FL work by filtering the models at each dataset site before the aggregation phase. Our algorithm improves the current FL model accuracy from 84% to 91% on MNIST dateset.Introduction -- Background -- Related work -- Approach Implementation -- Evaluation -- Conclusion and Future Wor",Enriching Knowledge Graphs Using Machine Learning Techniques,https://core.ac.uk/download/389643187.pdf,,,,core
475631095,2021-08-25T07:00:00,"The sponsor of this analysis, Global Cleveland, is an organization, but global Cleveland is also a reality. Elaborating, when it comes to the task of economic and community development, think of a city as a feather in the wind, or a stick in a rapid of water. Global forces push and pull at places, affecting a city’s relevance, or it’s standard of living. Yet some indicators are better measures of where a city fits into the global order of things than others. This analysis shows that standard measures of “success”, like population size, are relics of a bygone era where size mattered. In today’s idea economy, a better measure is gauging the quality of life in city, not the quantity of lives. This analysis looks at GDP per capita for the nation’s large metros, defined as “the amount of output or income per person in an economy…that’s indicative of average productivity or average living standards.”
The GDP per capita in the Cleveland metro is currently $ 57,700 and ranks 78th out of 374 metros. This is up from an inflation-adjusted $51,320 in 2010. To the extent Cleveland can prepare for progress entails examining what explains progress. The analysis looked at what features are driving GDP per capita growth across America’s metros from 2010 to 2019. To do this, Rust Belt Analytica deployed a machine learning algorithm called permutation feature importance. This is our “Progress Model”. Out of hundreds of variables analyzed, two clusters of features dominated the model results: educational attainment and migration. That is, the rate of a metro’s GDP per capita growth could be predictively explained by the educational attainment of a region, and the migration rates of a region. Migration features included the in-migration of college- and non-college-educated foreign born, and the in-migration of college- and non-college-educated native born, particularly if the domestic migrants were arriving from the Northeastern or Western parts of the U.S. This latter migration pattern of coastal-to-inland migration has been dubbed “The Rise of the Rest”, characterized as the convergence of American tech labor from the costly coast into the American heartland.
It is a pattern of migration that highly-educated immigrants have in fact been doing for some time. The analysis found that the percent of Cleveland’s immigrants with an advanced degree was 21.4%, which ranked 8th out the nation’s largest 40 metros. Interestingly, 6 out of the top 10 most highly-educated cities for immigrants were in the geographic area of the Rust Belt, led by Pittsburgh.
The analysis finds that migration is crucial to the evolution of cities. Migration does not only allow for the accumulation of human capital, but for global connectivity as well. Connectivity is part and parcel with the act of migration, allowing for the deepening of a city’s “thought bank”. This depth of ideation is crucial to the process of innovation which, in turn, is crucial economic evolution. Put another way, migration is economic development. It is today. It was yesterday. And it will be tomorrow. The issue for Cleveland is whether the region can leverage its global assets to incur its global relevance, and ultimately the increased well-being of its people",Migration is Economic Development,https://core.ac.uk/download/475631095.pdf,EngagedScholarship@CSU,,,core
478916715,2021-04-01T00:00:00,"The number of vehicles in urban cities has increased and raised attention towards the need for effective parking lot management in public areas such as hospital, shopping mall and office building. In this study, dynamic pricing is deployed with real time parking information to maximize the parking usage rate and alleviate traffic congestion. Dynamic pricing is a practice of varying the price of product of service reflected by the market conditions. This technique can be used to deal with vehicle flow around the parking area including peak and non-peak hour. During peak hours, the dynamic pricing mechanism will regulate the price of parking fee to a relatively high rate, and vice versa for non-peak hours. Reinforcement Learning (RL) is used in this paper to develop a dynamic pricing model for parking management. Dynamic pricing over time is divided into episodes and shuffled back and forth through an hourly increment. The parking usage rate and traffic congestion rate are regarded as the rewards for price regulation",Dynamic Pricing for Parking System Using Reinforcement Learning,,'Springer Science and Business Media LLC',10.1007/978-981-33-6385-4_15,,core
479365133,2021-01-01T00:00:00,"The ability to estimate radio coverage accurately is fundamental for planning and optimizing any wireless network, notably when a new generation, as the 5th Generation (5G), is in an early deployment phase. The knowledge acquired from radio planning of previous generations must be revisited, particularly the used path loss and antennas models, as the 5G propagation is intrinsically distinct. This paper analyses a new beamforming antenna model and distinct path loss models - 3rd Generation Partnership Project (3GPP) and Millimetre-Wave Based Mobile Radio Access Network for Fifth Generation Integrated Communications (mmMAGIC) - applying them to evaluate 5G coverage in 3-Dimensional (3D) synthetic and real scenarios, for outdoor and indoor environments. Further, real 5G Drive Tests (DTs) were used to evaluate the 3GPP path loss model accuracy in Urban Macro (UMa) scenarios. For the new antenna model, it is shown that the use of beamforming with multiple vertical beams is advantageous when the Base Station (BS) is placed below the surrounding buildings; in regular UMa surroundings, one vertical beam provides adequate indoor coverage and a maximized outdoor coverage after antenna tilt optimization. The 3GPP path loss model exhibited a Mean Absolute Error (MAE) of 21.05 dB for Line-of-Sight (LoS) and 14.48 dB for Non-Line-of-Sight (NLoS), compared with real measurements. After calibration, the MAE for LoS and NLoS decreased to 5.45 dB and 7.51 dB, respectively. Moreover, the non-calibrated 3GPP path loss model led to overestimations of the 5G coverage and user throughput up to 25&#x0025; and 163&#x0025;, respectively, when compared to the calibrated model predictions. The use of Machine Learning (ML) algorithms resulted in path loss MAEs within the range of 4.58 dB to 5.38 dB, for LoS, and within the range of 3.70 dB to 5.96 dB, for NLoS, with the Random Forest (RF) algorithm attaining the lowest error",Analysis and Optimization of 5G Coverage Predictions Using a Beamforming Antenna Model and Real Drive Test Measurements,,'Institute of Electrical and Electronics Engineers (IEEE)',10.1109/ACCESS.2021.3097633,"[{'title': 'IEEE Access', 'identifiers': ['issn:2169-3536', '2169-3536']}]",core
200800187,2019-02-06T00:00:00,"Urban flow monitoring systems play important roles in smart city efforts
around the world. However, the ubiquitous deployment of monitoring devices,
such as CCTVs, induces a long-lasting and enormous cost for maintenance and
operation. This suggests the need for a technology that can reduce the number
of deployed devices, while preventing the degeneration of data accuracy and
granularity. In this paper, we aim to infer the real-time and fine-grained
crowd flows throughout a city based on coarse-grained observations. This task
is challenging due to two reasons: the spatial correlations between coarse- and
fine-grained urban flows, and the complexities of external impacts. To tackle
these issues, we develop a method entitled UrbanFM based on deep neural
networks. Our model consists of two major parts: 1) an inference network to
generate fine-grained flow distributions from coarse-grained inputs by using a
feature extraction module and a novel distributional upsampling module; 2) a
general fusion subnet to further boost the performance by considering the
influences of different external factors. Extensive experiments on two
real-world datasets, namely TaxiBJ and HappyValley, validate the effectiveness
and efficiency of our method compared to seven baselines, demonstrating the
state-of-the-art performance of our approach on the fine-grained urban flow
inference problem",'Association for Computing Machinery (ACM)',UrbanFM: Inferring Fine-Grained Urban Flows,10.1145/3292500.3330646,http://arxiv.org/abs/1902.05377,,core
334865611,2019-09-02T00:00:00,"It is ever-increasingly demanded to remotely monitor people in daily life
using radio-frequency probing signals. However, conventional systems can hardly
be deployed in real-world settings since they typically require objects to
either deliberately cooperate or carry a wireless active device or
identification tag. To accomplish the complicated successive tasks using a
single device in real time, we propose a smart metasurface imager and
recognizer simultaneously, empowered by a network of artificial neural networks
(ANNs) for adaptively controlling data flow. Here, three ANNs are employed in
an integrated hierarchy: transforming measured microwave data into images of
whole human body; classifying the specifically designated spots (hand and
chest) within the whole image; and recognizing human hand signs instantly at
Wi-Fi frequency of 2.4 GHz. Instantaneous in-situ imaging of full scene and
adaptive recognition of hand signs and vital signs of multiple non-cooperative
people have been experimentally demonstrated. We also show that the proposed
intelligent metasurface system work well even when it is passively excited by
stray Wi-Fi signals that ubiquitously exist in our daily lives. The reported
strategy could open a new avenue for future smart cities, smart homes,
human-device interactive interfaces, healthy monitoring, and safety screening
free of visual privacy issues",,Intelligent Metasurface Imager and Recognizer,,http://arxiv.org/abs/1910.00497,,core
237216275,2019-01-01T00:00:00,"In the context of the continuous growth of the worldwide population and the rapid ongoing urbanization around the globe, the need for affordable and effective systems to detect hazardous substances and pathogens in water has gained importance. In order to address this need, multi-disciplinary research efforts from the fields of micro-technology and micro-biology have led to the emergence of microfluidic devices in the form of Lab-on-a-Chip (LoC) and Micro Total Analysis (ÂÂµTAS) devices, which are capable to host analytical and biorecognition assays, previously restricted to laboratory environments. Traditionally, the development of these devices had benefited from the microelectronics fabrication techniques, and afforded the replication of sub-micrometer channels and structures, as well as the implementation of functional materials to integrate diverse types of sensors (e.g. temperature, pressure, etc) in the same microfluidic device. Nevertheless, the reduction of the fabrication cost has become a persistent goal in order to popularize their utilization. This has urged the application of alternative materials like thermoplastics and large batch production techniques such as injection molding and hot embossing, at the same time to have set new challenges to their reliable and reproducible integration as analytical devices. The present report describes the design, development and testing of a microfluidic device for biosensing of bacteria, by means of RNA hybridization and fluorescence detection. The device consits in a fully Cyclo-Olefin Copolymer (COC) microfluidic chip, in size of 25,5 x 37,75 mm, structured by hot embossing. The microfluidic channels and cavities sum up a fluid volume of about 139 ÂÂµL, comprising a heating chamber, temperature sensor chambers, cooling channel and reaction chamber. The device layout includes 7 inlets for the sample fluid and diverse reagents plus 1 outlet. On-chip assay starts with the intake of a volume of 1 mL of water sample. The sample fluid is pumped through the heater chamber, where heat from a screen printed heater is applied to lyse the bacteria and release their RNA content to the running flow. Following the same stream, the fluid with released RNA flows across the cooling channel until the reaction chamber. The reaction chamber bottom surface, previously functionalized with capture oligomers complementary to the RNA target sequences, hosts hybridization reactions to capture the target RNA. The captured RNA is later tagged with a fluorescence molecule in a second hybridization. After washing off unbound analytes, the overall fluorescence emission is collected, filtered and quantified. The net fluorescence intensity measurement is then interpreted as an indicator of the concentration of the viable bacteria presented in the sample. The microfluidic chip was tested in a custom testbench that included particle filtering and pre-concentration of bacteria from raw samples, and fluorescence detection system that performed a limit of detection of 18 fmol, with a sensitivity of 63,08 photon count per fmol. Theoretical evaluation of the microfluidic chip at 0,1 mL/min predicted a mass transport and heat transport efficiency of 68,57% and 67,27%, respectively. Experimentally, the microfluidic device in the biosensing system completed successful detection of bacteria from raw water in less than 1 hour. Fluorescence detection was completed from hybridized bacterial RNA, that was retrieved in the same chip by heat lysis on a dilution of 2x10^8 of E. coli. Theoretical limit of detection of 24,87x10^3 CFU/mL was calculated. The microfluidic chip, integrated with the fluorescence detection system, proved its functionality as biosensing system for on-site applications, as well as its potential as a reference of a low-cost, disposable device for real-time monitoring and control of bacteria pollution",,"Design of a Biosensor for Detection of Bacteria in Water, by means of a Microfluidic System",,,,core
225251566,,"[[abstract]]Flood disasters have had a great impact on city development. Early flood warning systems (EFWS) are promising countermeasures against flood hazards and losses. Machine learning (ML) is the kernel for building a satisfactory EFWS. This paper first summarizes the ML methods proposed in this special issue for flood forecasts and their significant advantages. Then, it develops an intelligent hydroinformatics integration platform (IHIP) to derive a user-friendly web interface system through the state-of-the-art machine learning, visualization and system developing techniques for improving online forecast capability and flood risk management. The holistic framework of the IHIP includes five layers (data access, data integration, servicer, functional subsystem, and end-user application) and one database for effectively dealing with flood disasters. The IHIP provides real-time flood-related data, such as rainfall and multi-step-ahead regional flood inundation maps. The interface of Google Maps fused into the IHIP significantly removes the obstacles for users to access this system, helps communities in making better-informed decisions about the occurrence of floods, and alerts communities in advance. The IHIP has been implemented in the Tainan City of Taiwan as the study case. The modular design and adaptive structure of the IHIP could be applied with similar efforts to other cities of interest for assisting the authorities in flood risk management.[[notice]]補正完",'MDPI AG',Building an intelligent Hydroinformatics Integration platform for a regional flood inundation warning systems,10.3390/w11010009,,,core
269491081,2019-04-04T07:00:00,"The recent advancements in computing and sensor technologies, coupled with improvements in embedded system design methodologies, have resulted in the novel paradigm called the Internet of Things (IoT). IoT is essentially a network of small embedded devices enabled with sensing capabilities that can interact with multiple entities to relay information about their environments. This sensing information can also be stored in the cloud for further analysis, thereby reducing storage requirements on the devices themselves. The above factors, coupled with the ever increasing needs of modern society to stay connected at all times, has resulted in IoT technology penetrating all facets of modern life. In fact IoT systems are already seeing widespread applications across multiple industries such as transport, utility, manufacturing, healthcare, home automation, etc.
Although the above developments promise tremendous benefits in terms of productivity and efficiency, they also bring forth a plethora of security challenges. Namely, the current design philosophy of IoT devices, which focuses more on rapid prototyping and usability, results in security often being an afterthought. Furthermore, one needs to remember that unlike traditional computing systems, these devices operate under the assumption of tight resource constraints. As such this makes IoT devices a lucrative target for exploitation by adversaries. This inherent flaw of IoT setups has manifested itself in the form of various distributed denial of service (DDoS) attacks that have achieved massive throughputs without the need for techniques such as amplification, etc. Furthermore, once exploited, an IoT device can also function as a pivot point for adversaries to move laterally across the network and exploit other, potentially more valuable, systems and services. Finally, vulnerable IoT devices operating in industrial control systems and other critical infrastructure setups can cause sizable loss of property and in some cases even lives, a very sobering fact.
In light of the above, this dissertation research presents several novel strategies for identifying known and  zero-day attacks against IoT devices, as well as identifying infected IoT devices present inside a network along with some mitigation strategies. To this end, network telescopes are  leveraged to generate Internet-scale notions of maliciousness in conjunction with signatures that can be used to identify such devices in a network. This strategy is further extended by developing a taxonomy-based methodology which is capable of categorizing unsolicited IoT behavior by leveraging machine learning (ML) techniques, such as ensemble learners, to identify similar threats in near-real time. Furthermore, to overcome the challenge of insufficient (malicious) training data within the IoT realm, a generative adversarial network (GAN) based framework is also developed to identify known and unseen attacks on IoT devices. Finally, a software defined networking (SDN) based solution is proposed to mitigate threats from unsolicited IoT devices",Scholar Commons,Security Framework for the Internet of Things Leveraging Network Telescopes and Machine Learning,,https://core.ac.uk/download/269491081.pdf,,core
227482420,2019-01-01T00:00:00,"Atmospheric Pressure Plasma Jets (APPJs) are versatile tools for materials processing and medical applications. A key objective for APPJ treatment is the delivery of cumulative and spatially distributed treatment effects, i.e., a plasma dose. However, variability in APPJ operation and their sensitivity to exogenous disturbances can compromise reliable treatment. Particularly, for medical applications, it is necessary to ensure that APPJ effects are delivered safely and repeatably. Feedback control strategies can be crucial in ensuring the APPJ effects are regulated and maintained below critical limits instantaneously and cumulatively.The APPJ dynamics are nonlinear, multivariable and coupled across multiple timescales. Moreover, the dose delivery problem is spatially distributed. This presents a significant challenge for regulation of APPJ characteristics with basic control strategies based on proportional-integral type controllers. Optimization-based advanced control strategies, such as model predictive control, can systematically account for the APPJ dynamics and the complexities of the dose delivery problem. This work experimentally demonstrates the use of advanced control strategies for regulation of APPJ effects and dose delivery in the presence of common disturbances, including variations in substrate characteristics and changes in jet-tip-to-substrate separation distance. In particular, the thermal effects of the APPJ on treated substrate and thermal dose delivery is investigated. An experimental setup of a kHz-excited APPJ in He is constructed to allow implementation of control algorithms. The key issues of modeling, problem formulation, and control synthesis are undertaken for the development of control strategies.Development of control-oriented models of APPJs, which describe the dynamic response of controlled outputs to manipulated variables, are required for model-based control strategies. A lumped-parameter physics-based model is developed to describe dynamics of power dissipation in the APPJ and the consequent thermal effects on treated substrates. A fluid model of the transport phenomena in the APPJ in COMSOL informs the structure of the lumped-parameter model, which take the form of a set of differential-algebraic equations. The lumped-parameter model is found to adequately describe the experimentally observed APPJ dynamics despite its simplicity. Moreover, a linear data-driven modeling strategy is shown to be a viable alternative in describing the aspects of APPJ operation that are difficult to model based on physics, albeit for a limited range of operating conditions.Experimental investigation reveals that basic proportional-integral (PI) strategies tuned using internal model control rules allow rejection of a range of disturbances in a single-input-single-output setting. However, basic control strategies are found to be ineffective in simultaneous control of multiple APPJ effects and for dose delivery. In contrast, MPC strategies are shown to be capable of systematically addressing the multi-variable APPJ dynamics as well as the cumulative nature of the dose delivery problem. With MPC strategies simultaneous regulation of multiple APPJ effects as well as the delivery of a point-wise multi-component dose is made possible in the presence of disturbances. For spatially uniform dose delivery, a hierarchical control strategy is developed. Lower-level basic controllers are found to allow disturbance rejection in fast timescales. On the other hand, the MPC framework allows systematically addressing the different aspects of the dose delivery problem, including the total treatment time, spatially distributed APPJ effects, the multivariable system dynamics, and the translation trajectory of the APPJ over the treated substrate.Results presented in this work indicate that advanced feedback control strategies are crucial in enabling reliable and reproducible operation of APPJs, particularly in medical applications where safety considerations are stringent and high performance operation is required. The effective regulation of APPJ characteristics can create opportunities for new APPJ applications and the study of APPJ effects by creating controlled environments. Emerging areas for future work include the application of data analytics and machine learning methods for real-time diagnostics, modeling, and control of the complex time-varying APPJ phenomena","eScholarship, University of California",Advanced Control of Atmospheric Pressure Plasma Jets for Medical Applications,,,,core
343445450,2019-09-27T00:00:00,"Nowadays, reliability of sensors is one of the most important challenges for widespread

application of Internet-of-things data in key emerging fields such as the automotive and manufacturing

sectors. This paper presents a brief review of the main research and innovation actions at the European

level, as well as some on-going research related to sensor reliability in cyber-physical systems (CPS).

The research reported in this paper is also focused on the design of a procedure for evaluating the

reliability of Internet-of-Things sensors in a cyber-physical system. The results of a case study of

sensor reliability assessment in an autonomous driving scenario for the automotive sector are also

shown. A co-simulation framework is designed in order to enable real-time interaction between

virtual and real sensors. The case study consists of an IoT LiDAR-based collaborative map in order to

assess the CPS-based co-simulation framework. Specifically, the sensor chosen is the Ibeo Lux 4-layer

LiDAR sensor with IoT added capabilities. The modeling library for predicting error with machine

learning methods is implemented at a local level, and a self-learning-procedure for decision-making

based on Q-learning runs at a global level. The study supporting the experimental evaluation of the

co-simulation framework is presented using simulated and real data. The results demonstrate the

effectiveness of the proposed method for increasing sensor reliability in cyber-physical systems using

Internet-of-Things data.This work was partially supported by the project Power2Power: Providing next-generation silicon-based power solutions in transport and machinery for significant decarbonisation in the next decade, funded by the Electronic Component Systems for European Leadership (ECSEL-JU) Joint Undertaking and the Ministry of Science, Innovation and Universities (MICINN), under grant agreement No 826417. In addition, this work was also funded by the Spanish Ministry of Science, Innovation and Universities through the project COGDRIVE (DPI2017-86915-C3-1-R). Preparation of this publication was also partially co-financed by the Polish National Agency for Academic Exchange (NAWA) through the project: “Industry 4.0 in Production and Aeronautical Engineering (IPAE)”.Peer reviewe",'MDPI AG',Sensor Reliability in Cyber-Physical Systems Using Internet-of-Things Data: A Review and Case Study,10.3390/rs11192252,https://core.ac.uk/download/343445450.pdf,"[{'title': 'Remote Sensing', 'identifiers': ['2072-4292', 'issn:2072-4292']}]",core
236627962,2019-09-11T00:00:00,"The present paper provides a comparative evaluation of hybrid Singular Spectrum Analysis (SSA) and Artificial Neural Networks&nbsp;(ANN) against conventional ANN, applied on real time intraday traffic volume forecasting. The main research objective was to assess the&nbsp;applicability and functionality of intraday traffic volume forecasting, based on toll station measurements. The proposed methodology was implemented and evaluated upon a custom developed forecasting software toolbox, based on the software Mathworks MatLab, by using real data from Iasmos-Greece toll station. Experimental results demonstrated a superior ex post forecasting accuracy of the proposed hybrid forecasting methodology against conventional ANN, when compared to performance of usual statistical criteria (Mean Absolute Error, Mean Squared Error, Root Mean Squared Error, Coefficient of Determination R2, Theil's inequality coefficient). The obtained results revealed that the hybrid model could advance forecasting accuracy of a conventional ANN model in intraday traffic volume forecasting, while embedding hybrid forecasting algorithm in an Intelligent Transport System could provide an advanced decision support module for transportation system maintenance, operation and management",'Periodica Polytechnica Budapest University of Technology and Economics',Real-time Intraday Traffic Volume Forecasting – A Hybrid Application Using Singular Spectrum Analysis and Artificial Neural Networks,,https://core.ac.uk/download/236627962.pdf,,core
268909577,2019,"The Internet of Things becomes Internet of Everything when in the process of communication machine-to-machine also intelligent forms of communication between human and machine are involved. Cities can be viewed as a microcosm of this interconnected system where ICT and emerging technologies can be enabling factors to transform cities in Smart Cities. Cities can take great advantage by using information intelligence to achieve important public-policy goals and, in particular, by enabling network communication channels between citizens and public administrators in order to provide information and online services in real time through platform systems rather than by means of humans, using Artificial Intelligence and Natural Language Processing techniques. This work was the first step of a wider project aimed at providing a Spell Checking Web Service API for Smart City communication platforms able to automatically select, among the large availability of open source spell checking tools, the most suitable tool based on the semantic structure of the specific textual data. The system should manage an enhanced Italian Vocabulary Database, specifically implemented to support all the tools of the system. The goal of the present work was to test, through an experimental research, the feasibility of the entire project by implementing a Spell Checking Prototype System designed to manage two selected spell checking tools. Results showed that the Spell Checking Prototype System significantly improves performances by allowing the user to select the most suitable tool for the specific semantic structure of the text. The system also enables to manage the list of exceptions, which continuously enhance the Italian Vocabulary Database. The experimentation proved scientific evidence of the validity of the project aimed at implementing a Spell Checking Web Service API in order to improve the quality of natural language data to be stored or processed in Smart City NCeSDP systems, through the use of existing spell checking tools","'Scientific Research Publishing, Inc.'",A Spell Checking Web Service API for Smart City Communication Platforms,10.4236/ojapps.2019.912066,,,core
299788979,2019-01-01T00:00:00,"В процессе проектирования и эксплуатации городских электрических сетей возникает проблема прогнозирования
электрических нагрузок на определенный временной период, обусловленная технологическими и экономическими причинами. Используемый в большинстве случаев для прогнозирования электрических нагрузок метод экспертных оценок в реальных условиях эксплуатации не обеспечивает требуемую точность прогноза. В работе представлен подход к решению задачи прогнозирования электрических нагрузок городской электросети на основе искусственной нейронной сети. Рассмотрены возможности применения методологии нейронных сетей в энергетике, выполнен
анализ существующих задач применения нейрокомпьютеров в системах управления энергетическими системами. Поставлена задача формирования нейронной сети для прогнозирования нагрузок электросетей. Исследована возможность применения систем компьютерной математики для реализации нейронных сетей. Разработана реализация нейронной сети в системе компьютерной математики Matlab. Выполнена настройка и обучение сети на
реальных исходных данных. Приведены целевые и входные данные для создания нейронной сети, результаты ее обучения. Выполнены тестовые расчеты электрических нагрузок и электропотребления, полученные с помощью нейронной сети, вычислена погрешность моделирования, сделан вывод о том, что модель, построенная на основе нейронной сети, адекватна.In the process of design and operation of urban electrical
networks there is a problem of forecasting electrical loads for a certain time period, due to technological and economic reasons. Used in most cases to predict electrical loads method of expert assessments in real operating
conditions does not provide the required accuracy of the forecast. The paper presents an approach to solving the problem of forecasting the electrical loads of the city power grid based on an artificial neural network. The
possibilities of application of the methodology of neural networks in power engineering are considered, the analysis of existing problems of application of Neurocomputers in control systems of power systems is carried out. The task of forming a neural network to predict the loads of power grids. Possibility of application of systems of computer mathematics for
realization of neural networks is investigated. The implementation of a neural network in the system of computer mathematics Matlab is developed.
Configuration and training of the network on real source data was performed. Target and input data for creation of a neural network, results of its training are given. The test calculations of electrical loads and power consumption obtained with the help of a neural network are performed, the modeling error is calculated, the conclusion is made that the model built on the basis of a neural network is adequate",Орловский ГАУ,"COMPUTER FORECASTING OF ELECTRICAL LOADS BY
NEURAL NETWORK METHODS",,,,core
265316601,19/06/2019,"The aim of my research was to develop a digital mediation system with urban data for a pedestrian immersed in the city, a link based on digital technologies to design, analyze, represent urban space and access information on this urban space. Augmented Reality is one of the tools allowing this mediation whose critical element is the location of the pedestrian and more precisely the pose calculation of the camera it carries.Thus, the main focus of my work is geolocation on site using spatial data of different dimensions. I was interested in an upstream phase that requires the implementation of data models to keep track of spatial data changes. Finally, I touched on some uses of geolocation and pose calculation. I conclude this report by presenting my research perspectives on digital mediation with urban data for pedestrians.Le but de ma recherche a été de mettre au point un système de médiation numérique avec des données urbaines pour un piéton immergé dans la ville, un lien basé sur des technologies numériques pour concevoir, analyser, représenter l’espace urbain et accéder à des informations sur cet espace urbain. La réalité augmentée est un des outils permettant cette médiation dont l’élément critique est la localisation du piéton et plus précisément le calcul de pose de la caméra qu’il transporte.Ainsi, l’axe principal de mon travail est la géolocalisation sur site à l’aide de données spatiales de différentes dimensions. Je me suis intéressée à une phase amont qui nécessite la mise en place de modèles de données pour garder trace des modifications des données spatiales. J’ai enfin abordé quelques usages de la géolocalisation et du calcul de pose. Je conclus ce mémoire en présentant mes perspectives de recherches vers une médiation numérique avec des données urbaines pour le piéton",HAL CCSD,Méthodes pour la géolocalisation du piéton sur site - vers une médiation numérique avec les données urbaines,,https://core.ac.uk/download/pdf/265316601.pdf,,core
265418453,2019-05-27T00:00:00,"International audienceThis work aims to show the new approaches in embedded vision dedicated to object detection and tracking for drone visual control. Object/Pedestrian detection has been carried out through two methods: 1. Classical image processing approach through improved Histogram Oriented Gradient (HOG) and Deformable Part Model (DPM) based detection and pattern recognition methods. In this step, we present our improved HOG/DPM approach allowing the detection of a target object in real time. The developed approach allows us not only to detect the object (pedestrian) but also to estimates the distance between the target and the drone. 2. Object/Pedestrian detection-based Deep Learning approach. The target position estimation has been carried out within image analysis. After this, the system sends instruction to the drone engine in order to correct its position and to track target. For this visual servoing, we have applied 1 This work is carried out as part of the INTERREG VA FMA ADAPT project ""Assistive Devices for empowering disAbled People through robotic Technologies"" http://adapt-project.com/index.php. The Interreg FCE Programme is a European Territorial Cooperation programme that aims to fund high quality our improved HOG approach and implemented two kinds of PID controllers. The platform has been validated under different scenarios by comparing measured data to ground truth data given by the drone GPS. Several tests which were ca 1 rried out at ESIGELEC car park and Rouen city center validate the developed platform",'University of West Bohemia',Real Time Pedestrian and Object Detection and Tracking-based Deep Learning. Application to Drone Visual Tracking,10.24132/CSRN.2019.2902.2.5,https://core.ac.uk/download/265418453.pdf,,core
322840351,2019-08-27T00:00:00,"International audienceA study on the increasing of projects about autonomous vehicles shows that almost of research fields are concerned, from technological domains (improvement of radar and lidar technologies, AI for best choice decisions, etc…) to social sciences (acceptability for example), including economic domain or insurance and legal fields (responsibility cases). For less than 10 years, geographers and land planners have also investigated this thematic, most using simulations software. These simulations, based on theoretical cases (Fagnant and al., 2014) or on real urban territories (Spieser and al., 2014, Bösh and al., 2016), show “predictable” impacts like the decreasing of the number of cars and the growing of daily mobility, but with different rates, depending of the input scenario. Another way of investigation, still not yet explored, need to be presented: the impact on the urban form. For this reason, we purpose to build more complex scenarios combining daily mobility and residential mobility at different time steps: as predicted, the urban sprawl currently found will it continue, extending Newman and Kenworthy’s publication (Newman and Kenworthy, 1996) on the 3 ages of the city? Or the improvement of the traffic conditions provided from automated mobility which is materialized by the disappearance of the congestion and the emissions of pollutants, can it lead to a densification of the central urban spaces",HAL CCSD,Mobility simulation including autonomous vehicles: What about the urban form ?,,,,core
231896837,2019-01-01T00:00:00,"The uninterrupted growth in transportation activities has been exerting significant pressure on our socio-economics and environment in recent years. However, emerging technologies such as connected and automated vehicles (CAVs), transportation electrification, and edge computing have been stimulating increasingly dedicated efforts by engineers, researchers and policymakers to tackle these transportation-related problems, including those that are focused on energy and the environment. With the advancement towards vehicle connectivity and automation, vehicles can reduce the energy consumption, emissions and improve urban mobility and safety through environmentally-friendly eco-driving strategies, vehicle electrification, and driving coordination.In this dissertation, we developed predictive models and trajectory planning algorithms using machine learning and optimization techniques to address four key challenge: 1) Driving in real-world scenarios with constrains and interaction from downstream vehicle's trajectory and traffic; 2) Extracting essential traffic information from sparse vehicle trajectory data in a connected vehicle environment; 3) Evaluating and quantifying the behavior of a complex Vehicle-Powertrain Eco-Operation System; and 4) Optimal scheduling and coordinating automated vehicles in terms of mobility benefits and energy savings considering the tradeoff between solution optimality and computational efficiency for online performance.This research first starts with developing an electric vehicle energy consumption model based on real world data and integrating it into eco-driving algorithms considering the regenerative braking effect. By introducing a hybrid modeling approach which provides variables with actual physical meaning instead of the exhaustive method used in conventional data-driven approaches, we feature knowledge-driven variable selection and data-driven statistical synthesis together to further improve the estimation accuracy. Many of the existing eco-driving algorithms, including the eco-approach and departure (EAD) algorithm, are not flexible enough to effectively handle customized powertrain characteristics, interaction with other traffic, road grade, and traveling with the presence of the downstream vehicles. Therefore, when considering the real-world deployment of the EAD application, it is beneficial to further explore the dynamic states from downstream vehicles and incorporate these into the trajectory planning process. A machine learning technique has been applied to the snippet of downstream vehicle trajectory (which may be obtained from onboard sensors, such as radar) to predict trajectories using real-world data. By integrating the prediction on future states of the preceding vehicle into the trajectory planner, the resulting enhanced EAD algorithm provides an eco-friendly speed trajectory in the presence of preceding traffic and queues at intersections with an additional 2 - 34 % energy savings and emission reduction compared to the EAD algorithms without prediction.This dissertation also describes a technique to integrate vehicle dynamics and powertrain operations, using a comprehensive simulation study that was designed and tested for both electric buses and plug-in hybrid electric buses driving across total 11 signalized intersections in a test corridor. The overall simulation framework incorporates a two-layer vehicle optimal trajectory planning module that seamlessly integrates a graph-based trajectory planning algorithm and a deep learning-based trajectory planning algorithm while interacting with the environment calibrated using real-world data. It was found that deep learning-based EAD algorithms can achieve a good balance between solution optimality and computational efficiency. Besides, a dynamic queue prediction in the connected vehicle environment has been developed to better plan the bus trajectory for greater energy consumption. An average of 21.0% energy savings can be achieved across various traffic conditions when co-optimizing vehicle dynamics and powertrain operations. In addition, an extra around 7% energy efficiency improvement for a test PHEV bus was shown when introducing 20% connected vehicles in the network.  With a partially connected vehicle network, we not only improve the longitudinal control of the vehicle to obtain better energy efficiency but also extract essential traffic condition information such as lane-level traffic information that can be used for better lane selection. We developed a Lane-Hazard Prediction (LHP) application that can detect lane-level hazards effectively and efficiently. A machine learning approach was developed with feature extraction from the spatial-temporal domain to achieve sustainable high accurate lane-level prediction of a downstream hazard within tenths of seconds after it occurred by crowdsourcing sparse connected vehicle trajectories. The LHP application then guides the application-equipped vehicles with suggestions for proper lateral maneuvers far ahead of the hazard to avoid traffic jams. Results demonstrate that LHP-equipped vehicles may gain significant mobility and safety benefits without compromising the mobility and safety performance of the overall traffic under various traffic conditions and penetration rate of connectivity in the vehicle network.  Finally, a Bi-level Optimal Edge Computing (BOEC) methodology was developed under the fully connectivity vehicle network to maximize both the vehicle mobility benefit and energy saving by optimizing the vehicle coordination and motion planning. For the on-ramp scenario, the first-level edge computing is conducted in the roadside unit (RSU) that collects connected vehicle data, dynamically assigning each vehicle into an associated cluster group based on its state and potential merging conflict and periodically solved for the clustered vehicles with their optimal scheduling sequence and arrival time at the merge bottleneck point. Once the clustered vehicles have their assigned arrival time at the merge point, the second-level edge computing determines the optimal vehicle trajectory to guarantee vehicles meet the assigned arrival time with the minimum energy cost. It is shown that the computational cost of vehicle trajectory planning approaches can satisfy the objective of real-time performance with 63.4%-66.8% energy savings","eScholarship, University of California","Perception Learning, Prediction and Motion Planning for Energy Efficient Driving of Connected and Automated Vehicles",,,,core
250567986,2019-11-27T18:30:00,"Smart cities look to leverage technology, particularly sensors, and software to provide improved services for its citizenry and enhanced operational efficiencies. Cities look to develop applications that can process data from sensors and other sources to gain insights into operation, enable them to improve operations and inform city leadership. Such applications often need to process streams of data from sensors or other sources to provide city staff with insights into city operations. However, cities are faced with limited budgets and limited staff. The development of applications by third parties can be extremely expensive. One alternative is to identify tools for software development that city staff can use – where the development tools can simplify the development process.
This research addresses this challenge by looking at a graphical flow-based programming framework, Node-RED, as the foundation for a flexible application development environment that can accelerate and simplify the development of applications of interest to smart cities. Node-RED presents a visual programming framework composed of nodes and data flows. We look at extending Node-RED to incorporate nodes that hide the complexity of developing incremental machine learning applications by providing relatively simple and easy to use graphical interfaces. Nodes for a variety of learning methods are introduced and used for real-time analysis of data streams. Nodes providing different metrics have also been designed to enable the application developer to evaluate the trained models",Scholarship@Western,An Environment for Developing Incremental Learning Applications for Data Streams,,https://core.ac.uk/download/250567986.pdf,,core
386380658,2020-12-01T08:00:00,"With the rise of (semi)autonomous vehicles and continuum robotics technology and applications, there has been an increasing interest in controller and haptic interface designs. The presence of nonlinearities in the vehicle dynamics is the main challenge in the selection of control algorithms for real-time regulation and tracking of (semi)autonomous vehicles. Moreover, control of continuum structures with infinite dimensions proves to be difficult due to their complex dynamics plus the soft and flexible nature of the manipulator body. The trajectory tracking and control of automobile and robotic systems requires control algorithms that can effectively deal with the nonlinearities of the system without the need for approximation, modeling uncertainties, and input disturbances. Control strategies based on a linearized model are often inadequate in meeting precise performance requirements. To cope with these challenges, one must consider nonlinear techniques. Nonlinear control systems provide tools and methodologies for enabling the design and realization of (semi)autonomous vehicle and continuum robots with extended specifications based on the operational mission profiles. This dissertation provides an insight into various nonlinear controllers developed for (semi)autonomous vehicles and continuum robots as a guideline for future applications in the automobile and soft robotics field. A comprehensive assessment of the approaches and control strategies, as well as insight into the future areas of research in this field, are presented.First, two vehicle haptic interfaces, including a robotic grip and a joystick, both of which are accompanied by nonlinear sliding mode control, have been developed and studied on a steer-by-wire platform integrated with a virtual reality driving environment. An operator-in-the-loop evaluation that included 30 human test subjects was used to investigate these haptic steering interfaces over a prescribed series of driving maneuvers through real time data logging and post-test questionnaires. A conventional steering wheel with a robust sliding mode controller was used for all the driving events for comparison. Test subjects operated these interfaces for a given track comprised of a double lane-change maneuver and a country road driving event. Subjective and objective results demonstrate that the driver’s experience can be enhanced up to 75.3% with a robotic steering input when compared to the traditional steering wheel during extreme maneuvers such as high-speed driving and sharp turn (e.g., hairpin turn) passing.  Second, a cellphone-inspired portable human-machine-interface (HMI) that incorporated the directional control of the vehicle as well as the brake and throttle functionality into a single holistic device will be presented. A nonlinear adaptive control technique and an optimal control approach based on driver intent were also proposed to accompany the mechatronic system for combined longitudinal and lateral vehicle guidance. Assisting the disabled drivers by excluding extensive arm and leg movements ergonomically, the device has been tested in a driving simulator platform. Human test subjects evaluated the mechatronic system with various control configurations through obstacle avoidance and city road driving test, and a conventional set of steering wheel and pedals were also utilized for comparison. Subjective and objective results from the tests demonstrate that the mobile driving interface with the proposed control scheme can enhance the driver’s performance by up to 55.8% when compared to the traditional driving system during aggressive maneuvers. The system’s superior performance during certain vehicle maneuvers and approval received from the participants demonstrated its potential as an alternative driving adaptation for disabled drivers. Third, a novel strategy is designed for trajectory control of a multi-section continuum robot in three-dimensional space to achieve accurate orientation, curvature, and section length tracking. The formulation connects the continuum manipulator dynamic behavior to a virtual discrete-jointed robot whose degrees of freedom are directly mapped to those of a continuum robot section under the hypothesis of constant curvature. Based on this connection, a computed torque control architecture is developed for the virtual robot, for which inverse kinematics and dynamic equations are constructed and exploited, with appropriate transformations developed for implementation on the continuum robot. The control algorithm is validated in a realistic simulation and implemented on a six degree-of-freedom two-section OctArm continuum manipulator. Both simulation and experimental results show that the proposed method could manage simultaneous extension/contraction, bending, and torsion actions on multi-section continuum robots with decent tracking performance (e.g. steady state arc length and curvature tracking error of 3.3mm and 130mm-1, respectively). Last, semi-autonomous vehicles equipped with assistive control systems may experience degraded lateral behaviors when aggressive driver steering commands compete with high levels of autonomy. This challenge can be mitigated with effective operator intent recognition, which can configure automated systems in context-specific situations where the driver intends to perform a steering maneuver. In this article, an ensemble learning-based driver intent recognition strategy has been developed. A nonlinear model predictive control algorithm has been designed and implemented to generate haptic feedback for lateral vehicle guidance, assisting the drivers in accomplishing their intended action. To validate the framework, operator-in-the-loop testing with 30 human subjects was conducted on a steer-by-wire platform with a virtual reality driving environment. The roadway scenarios included lane change, obstacle avoidance, intersection turns, and highway exit. The automated system with learning-based driver intent recognition was compared to both the automated system with a finite state machine-based driver intent estimator and the automated system without any driver intent prediction for all driving events. Test results demonstrate that semi-autonomous vehicle performance can be enhanced by up to 74.1% with a learning-based intent predictor. The proposed holistic framework that integrates human intelligence, machine learning algorithms, and vehicle control can help solve the driver-system conflict problem leading to safer vehicle operations",Clemson University Libraries,Nonlinear Modeling and Control of Driving Interfaces and Continuum Robots for System Performance Gains,,https://core.ac.uk/download/386380658.pdf,,core
419937536,2020-12-15T00:00:00,"Mobility prediction is an essential enabler to provide intelligent network systems and services in the upcoming B5G/6G era. Artificial Intelligence (AI) models such as Long Short Term Memory (LSTM) offer great performance at predicting users’ locations. However, model training can be time-consuming, which brings obstacles to practical applications. In this article, we present a mobility predictor based on Long Short Term Memory (LSTM), which is a variant of Recurrent Neural Networks (RNN) to reduce the network traffic for the sake of service migration improvement and handover (HO) optimization. To speed up the model convergence rate, we employ a Reinforcement Learning (RL) technique to automate the selection procedure of the best neural network architecture. To further accelerate the RL environmental search procedure, we transfer the architecture knowledge learned from a teacher LSTM to a student LSTM via a Transfer Learning (TL) framework. We propose a HO algorithm and a service migration algorithm based on the proposed LSTM predictor. We deploy the AI models on a mobile edge computing architecture using a real-world dataset collected from Paris, and evaluation results prove the efficiency of the predictor, and its impacts on improving ping-pong handover, and the service migration performance",'Institute of Electrical and Electronics Engineers (IEEE)',Reinforced-LSTM Trajectory Prediction-driven Dynamic Service Migration: A Case Study,,,,core
334917379,2020-02-28T00:00:00,"The confluence of Internet of Things(IoT) , Blockchain(BC) and Artificial
Intelligence(AI) acts as a key accelerator for enabling Machine Economy. To be
ready for future businesses these technologies needs to be adapted by extending
the IoT capabilities to Economy of Things (EoT) capabilities. In this paper we
focus on one such implementation experience for Smart Toll Transaction
application in the domain of mobility. Our paper showcases a possible solution
by leveraging negotiations, decision making, distributed learning capabilities
at the devices level using AI-enabled Multi-Agent Systems and the real-time
smart contracts between the Cars and Tolls using Blockchain. This solution also
showcases the monetization of real time data coming from various IoT devices
which are part of vehicles and infrastructure. While blockchain secures the
privacy of the participants it also acts as an economic transactional layer and
governance layer between the devices in the networComment: 5 pages, 4 figures, 1 tabl",,"Real time Smart Contracts for IoT using Blockchain and Collaborative
  Intelligence based Dynamic Pricing for the next generation Smart Toll
  Application",,http://arxiv.org/abs/2002.12654,,core
326235429,2020-01-01T00:00:00,"In most of the developing countries, the economy is largely based on agriculture.

The poor availability of skilled personnel and of appropriate supporting infrastructure, make crop  fields   vulnerable to the outbreak of plant diseases, possibly due to spreading viruses and fungi, or to adverse environmental conditions, such as drought.



The mobile application PlantVillage Nuru, provides an invaluable tool for early detection of plant diseases and sustainable food production.

A mobile device endowed with  Nuru  is a powerful mobile sensor: it analyzes plant images and uses an AI engine to recognize health issues. 

In this paper we propose  a crowd-sensing framework, where Nuru is adopted  at large scale in the farmer population. We tackle  the device deployment problem, where device mobility is only partially controllable, mostly in an indirect manner, through incentives. 

We propose two problem formulations, and related algorithms, to minimize the number of required smartphones while providing sufficient geographical coverage. 

 We study the proposed models in simulated as well as real scenarios, showing that they outperform current solutions in terms of monitoring accuracy and completeness, with lower cost. 

Then we describe the test-bed implementation, confirming the applicability of the proposed crowd-sensing framework in a real  scenario in Kenya",,"Optimal deployment in crowd sensing for plant disease diagnosis in

developing countries",,,,core
322449435,2020-04-30T00:00:00,"The AI City Challenge was created to accelerate intelligent video analysis
that helps make cities smarter and safer. Transportation is one of the largest
segments that can benefit from actionable insights derived from data captured
by sensors, where computer vision and deep learning have shown promise in
achieving large-scale practical deployment. The 4th annual edition of the AI
City Challenge has attracted 315 participating teams across 37 countries, who
leveraged city-scale real traffic data and high-quality synthetic data to
compete in four challenge tracks. Track 1 addressed video-based automatic
vehicle counting, where the evaluation is conducted on both algorithmic
effectiveness and computational efficiency. Track 2 addressed city-scale
vehicle re-identification with augmented synthetic data to substantially
increase the training set for the task. Track 3 addressed city-scale
multi-target multi-camera vehicle tracking. Track 4 addressed traffic anomaly
detection. The evaluation system shows two leader boards, in which a general
leader board shows all submitted results, and a public leader board shows
results limited to our contest participation rules, that teams are not allowed
to use external data in their work. The public leader board shows results more
close to real-world situations where annotated data are limited. Our results
show promise that AI technology can enable smarter and safer transportation
systems.Comment: Organization summary of the 4th AI City Challenge Workshop @ CVPR
  202",,The 4th AI City Challenge,,http://arxiv.org/abs/2004.14619,,core
360730425,2020-11-15T00:00:00,"Amyotrophic Lateral Sclerosis (ALS) is a fast-progressive neurodegenerative disease leading to progressive physical immobility with usually normal or mild cognitive and/or behavioural involvement. Many patients are relatively young, instructed, sensitive to new technologies, and professionally active when developing the first symptoms. Older patients usually require more time, encouragement, reinforcement and a closer support but, nevertheless, selecting user-friendly devices, provided earlier in the course of the disease, and engaging motivated carers may overcome many technological barriers. ALS may be considered a model for neurodegenerative diseases to further develop and test new technologies. From multidisciplinary teleconsults to telemonitoring of the respiratory function, telemedicine has the potentiality to embrace other fields, including nutrition, physical mobility, and the interaction with the environment. Brain-computer interfaces and eye tracking expanded the field of augmentative and alternative communication in ALS but their potentialities go beyond communication, to cognition and robotics. Virtual reality and different forms of artificial intelligence present further interesting possibilities that deserve to be investigated. COVID-19 pandemic is an unprecedented opportunity to speed up the development and implementation of new technologies in clinical practice, improving the daily living of both ALS patients and carers. The present work reviews the current technologies for ALS patients already in place or being under evaluation with published publications, prompted by the COVID-19 pandemic",'Elsevier BV',New technologies and Amyotrophic Lateral Sclerosis &#8211; Which step forward rushed by the COVID-19 pandemic?,10.1016/j.jns.2020.117081,,,core
322369794,2020-05-01T00:00:00,"abstract: Object localization is used to determine the location of a device, an important aspect of applications ranging from autonomous driving to augmented reality. Commonly-used localization techniques include global positioning systems (GPS), simultaneous localization and mapping (SLAM), and positional tracking, but all of these methodologies have drawbacks, especially in high traffic indoor or urban environments. Using recent improvements in the field of machine learning, this project proposes a new method of localization using networks with several wireless transceivers and implemented without heavy computational loads or high costs. This project aims to build a proof-of-concept prototype and demonstrate that the proposed technique is feasible and accurate. 

Modern communication networks heavily depend upon an estimate of the communication channel, which represents the distortions that a transmitted signal takes as it moves towards a receiver. A channel can become quite complicated due to signal reflections, delays, and other undesirable effects and, as a result, varies significantly with each different location. This localization system seeks to take advantage of this distinctness by feeding channel information into a machine learning algorithm, which will be trained to associate channels with their respective locations. A device in need of localization would then only need to calculate a channel estimate and pose it to this algorithm to obtain its location.

As an additional step, the effect of location noise is investigated in this report. Once the localization system described above demonstrates promising results, the team demonstrates that the system is robust to noise on its location labels. In doing so, the team demonstrates that this system could be implemented in a continued learning environment, in which some user agents report their estimated (noisy) location over a wireless communication network, such that the model can be implemented in an environment without extensive data collection prior to release",,Leveraging Machine Learning and Wireless Sensing for Robot Localization - Location Variance Analysis,,,,core
347692653,2020-07-01T00:00:00,"Concerned about the noise pollution in urban environments, the European Commission (EC) has created an Environmental Noise Directive 2002/49/EC (END) requiring Member states to publish noise maps and noise management plans every five years for cities with a high density of inhabitants, major roads, railways and airports. The END also requires the noise pressure levels for these sources to be presented independently. Currently, data measurements and the representations
of the noise pressure levels in such maps are performed semi-manually by experts. This process is time and cost consuming, as well as limited to presenting only a static picture of the noise levels. To overcome these issues, we propose the deployment ofWireless Acoustic Sensor Networks with several nodes in urban environments that can enable the generation of real-time noise level maps, as well as detect the source of the sound thanks to machine learning algorithms. In this paper, we briefly review the state of the art of the hardware used in wireless acoustic applications and propose a low-cost sensor based on an ARM cortex-A microprocessor. This node is able to process machine learning algorithms for sound source detection in-situ, allowing the deployment of highly scalable sound identification systems",'MDPI AG',Design of a Low-Cost Configurable Acoustic Sensor for the Rapid Development of Sound Recognition Applications,,,,core
384445323,2020-12-01T00:00:00,"Continuous exposure to urban noise has been found to be one of the major threats to citizens’ health. In this regard, several organizations are devoting huge efforts to designing new in-field systems to identify the acoustic sources of these threats to protect those citizens at risk. Typically, these prototype systems are composed of expensive components that limit their large-scale deployment and thus reduce the scope of their measurements. This paper aims to present a highly scalable low-cost distributed infrastructure that features a ubiquitous acoustic sensor network to monitor urban sounds. It takes advantage of (1) low-cost microphones deployed in a redundant topology to improve their individual performance when identifying the sound source, (2) a deep-learning algorithm for sound recognition, (3) a distributed data-processing middleware to reach consensus on the sound identification, and (4) a custom planar antenna with an almost isotropic radiation pattern for the proper node communication. This enables practitioners to acoustically populate urban spaces and provide a reliable view of noises occurring in real time. The city of Barcelona (Spain) and the UrbanSound8K dataset have been selected to analytically validate the proposed approach. Results obtained in laboratory tests endorse the feasibility of this proposal",'MDPI AG',Low-cost distributed acoustic sensor network for real-time urban sound monitoring,10.3390/electronics9122119.,,,core
359945624,2020-09-15T00:00:00,"Visual navigation tasks in real-world environments often require both self-motion and place recognition feedback. While deep reinforcement learning has shown success in solving these perception and decision-making problems in an end-to-end manner, these algorithms require large amounts of experience to learn navigation policies from high-dimensional data, which is generally impractical for real robots due to sample complexity. In this paper, we address these problems with two main contributions. We first leverage place recognition and deep learning techniques combined with goal destination feedback to generate compact, bimodal image representations that can then be used to effectively learn control policies from a small amount of experience. Second, we present an interactive framework, CityLearn, that enables for the first time training and deployment of navigation algorithms across city-sized, realistic environments with extreme visual appearance changes. CityLearn features more than 10 benchmark datasets, often used in visual place recognition and autonomous driving research, including over 100 recorded traversals across 60 cities around the world. We evaluate our approach on two CityLearn environments, training our navigation policy on a single traversal per dataset. Results show our method can be over 2 orders of magnitude faster than when using raw images, and can also generalize across extreme visual changes including day to night and summer to winter transitions.</p",'Institute of Electrical and Electronics Engineers (IEEE)',CityLearn:Diverse Real-World Environments for Sample-Efficient Navigation Policy Learning,10.1109/ICRA40945.2020.9197336,,,core
343225290,2020-10-29T00:00:00,"Future mobile networks will enable the massive deployment of mobile multimedia applications anytime and anywhere. In this context, mobility management schemes, such as handover and proactive multimedia service migration, will be essential to improve network performance. In this article, we propose a proactive mobility management approach based on group user trajectory prediction. Specifically, we introduce a mobile user trajectory prediction algorithm by combining the Long-Short Term Memory networks (LSTM) with Reinforcement Learning (RL) to automate the model training procedure. We further develop a group user trajectory predictor to reduce prediction calculation overheads of users with similar movement patterns. To validate the impact of the proposed mobility management approach, we present a virtual reality (VR) service migration scheme built on the top of the proactive handover mechanism that benefits from trajectory predictions. Experiment results validate our predictor’s outstanding accuracy and its impacts on enhancing handover and service migration performance to provide quality of service assurance",'Institute of Electrical and Electronics Engineers (IEEE)',Mobility Management with Transferable Reinforcement Learning Trajectory Prediction,10.1109/TNSM.2020.3034482,,,core
429074282,2020-11-19T00:00:00,"Millions of battery-powered sensors deployed for monitoring purposes in a multitude of scenarios, e.g., agriculture, smart cities, industry, etc., require energy-efficient solutions to prolong their lifetime. When these sensors observe a phenomenon distributed in space and evolving in time, it is expected that collected observations will be correlated in time and space. In this paper, we propose a Deep Reinforcement Learning (DRL) based scheduling mechanism capable of taking advantage of correlated information. We design our solution using the Deep Deterministic Policy Gradient (DDPG) algorithm. The proposed mechanism is capable of determining the frequency with which sensors should transmit their updates, to ensure accurate collection of observations, while simultaneously considering the energy available. To evaluate our scheduling mechanism, we use multiple datasets containing environmental observations obtained in multiple real deployments. The real observations enable us to model the environment with which the mechanism interacts as realistically as possible. We show that our solution can significantly extend the sensors' lifetime. We compare our mechanism to an idealized, all-knowing scheduler to demonstrate that its performance is near-optimal. Additionally, we highlight the unique feature of our design, energy-awareness, by displaying the impact of sensors' energy levels on the frequency of updates",arXiv.org,Energy Aware Deep Reinforcement Learning Scheduling for Sensors Correlated in Time and Space,,,,core
224992986,2020-01-01T00:00:00,"Among the construction methods developed for tunneling, mechanized excavation by Tunnel Boring Machines (TBMs) is currently considered a preferred option for technical and safety reasons in an urban environment, where damage induced on pre-existing building and services should be minimized. Since the ability to predict TBM performances is a critical point required to enhance the quality of the excavation and to optimize time, cost and safety operations in a project and since real-time prediction should be done during excavation in order to adjust some parameters in very real-time, approaches based on Artificial Intelligence (AI) methodology could be crucial. This study proposes an expeditious tool based on the application of Artificial Intelligence and particularly Artificial Neural Networks (ANNs), to predict the maximum surface settlements induced by tunnelling. ANNs, taking advantage of the quality of data available and computational performances of software for data management, have been proved to be a reliable instrument in processes where a relevant number of parameters and acquired measurements have to be managed. Using data selected from the excavation of the Milan M5 metro line, the document includes details on the role played by several inner elements on the accuracy of the final prediction based on the comparison of several different ANN configurations. The obtained results showed a promising capability of the tool to swiftly predict surface settlements in mechanized tunneling projects",'Springer Science and Business Media LLC',Artificial intelligence to predict maximum surface settlements induced by mechanized tunnelling,10.1007/978-3-030-21359-6_52,,,core
357345768,2020-04-03T00:00:00,"Microarrays have the potential to significantly impact our ability to identify toxic hazards by the identification of mechanistically relevant markers of toxicity. To be useful for risk assessment, however, microarray data must be challenged to determine reliability and interlaboratory reproducibility. As part of a series of studies conducted by the International Life Sciences Institute Health and Environmental Science Institute Technical Committee on the Application of Genomics to Mechanism-Based Risk Assessment, the biological response in rats to the hepatotoxin clofibrate was investigated. Animals were treated with high (250 mg/kg/day) or low (25 mg/kg/day) doses for 1, 3, or 7 days in two laboratories. Clinical chemistry parameters were measured, livers removed for histopathological assessment, and gene expression analysis was conducted using cDNA arrays. Expression changes in genes involved in fatty acid metabolism (e.g., acyl-CoA oxidase), cell proliferation (e.g., topoisomerase II-α), and fatty acid oxidation (e.g., cytochrome P450 4A1), consistent with the mechanism of clofibrate hepatotoxicity, were detected. Observed differences in gene expression levels correlated with the level of biological response induced in the two in vivo studies. Generally, there was a high level of concordance between the gene expression profiles generated from pooled and individual RNA samples. Quantitative real-time polymerase chain reaction was used to confirm modulations for a number of peroxisome proliferator marker genes. Though the results indicate some variability in the quantitative nature of the microarray data, this appears due largely to differences in experimental and data analysis procedures used within each laboratory. In summary, this study demonstrates the potential for gene expression profiling to identify toxic hazards by the identification of mechanistically relevant markers of toxicity. of clofibrate are triggered through binding of the chemical to the peroxisome proliferator-activated receptor-alpha (PPARα), causing a pleiotropic response involving the induction of a number of proteins involved in fatty acid β-oxidation  To help address the goals of the HWG, the aims of this study were a) to investigate whether the gene expression profiles observed after exposure to clofibrate correlated with changes in known biological markers of hepatotoxicity (as identified by histopathology and clinical chemistry parameters), and b) to assess the reproducibility of the gene expression changes across different laboratories on a single microarray platform using RNA from identical in vivo studies conducted separately at different locations. In addition, comparison of gene expression data generated using RNA from pooled and individual animals was conducted, together with confirmatory analysis of a subset of genes using quantitative realtime polymerase chain reaction (PCR). Materials and Methods Different components of the study were conducted in individual laboratories.  Chemicals Clofibrate [2-(p-chlorophenoxy)-2-methylpropionic acid ethyl ester] (CAS no. 637-07-0) was obtained from Sigma Chemical Corporation (St. Louis, MO, USA) and stored according to manufacturer instructions. The same batch was used by both laboratories performing the in vivo studies and had a purity of 99.7%. Animals and Treatment In vivo studies were conducted at Abbott Laboratories (Abbott Park, IL, USA) and GlaxoSmithKline (GSK) (Ware, U.K.), as shown in  Hematology and Clinical Chemistry Blood samples (approximately 5 mL/rat) were collected via the abdominal vein at necropsy. A range of hematology and clinical chemistry parameters were measured  Histology A portion of the left lateral lobe was taken for histopathology in all studies (stored in 10% buffered saline). The right lobe (50-100 mg) was taken for measurement of acyl-CoA oxidase (ACOX) activity, and the remaining tissue was flash frozen for RNA isolation. Tissues to be examined histologically were fixed in 10% neutral-buffered formalin and subsequently sectioned and stained with hematoxylin and eosin. Acyl-CoA Oxidase Enzyme Assay At necropsy, samples of liver (50-100 mg) were collected from all treatment groups for triplicate measurement of ACOX activity. Liver was collected, flash-frozen in liquid nitrogen, and stored at -80°C. Analysis of ACOX activity was performed at Abbott Laboratories using spectrophotometric analysis via a modification of the method of  RNA Isolation and Distribution At necropsy, liver tissue was quickly chopped into small pieces and flash-frozen in liquid nitrogen. RNA isolation was performed at Abbott using Trizol reagent (Invitrogen Life Technologies, Carlsbad, CA, USA) according to manufacturer protocol, and at GSK (U.K.) using RNeasy kits (Qiagen, Crawley, West Sussex, U.K.) according to manufacturer instructions. A portion of the RNA from the four animals in each treatment group was pooled, using equivalent amounts. The remainder was retained as individual samples. RNA samples were aliquoted and precipitated in ethanol and ammonium acetate for shipment to the microarray analysis laboratories. cDNA Gene Expression Arrays cDNA probes were prepared according to manufacturer instructions and hybridized to Atlas Rat Toxicology II arrays (Clontech, Palo Alto, CA, USA) containing 465 genes. Pooled RNA samples were analyzed by GSK (U.S.) and Unilever Research (ULR; Sharnbrook, Bedfordshire, U.K.) using triplicate arrays. The U.S. Environmental Protection Agency (U.S. EPA) analyzed RNA samples from individual animals (n = 4/group). Other differences in interlaboratory experimental protocols are noted in  Image Capture and Analysis Probed arrays were developed using phosphorimagers [GSK (U.S.): Cyclone (Packard Bioscience Company, Meriden, CT, USA); U.S. EPA: FX (BioRad, Hercules, CA, USA)] or exposed to X-ray film for up to 5 days at -70°C (ULR). After scanning, individual image files were analyzed using the AtlasImage 2.0 program (Clontech), and the intensity of each spot was determined. The complete data set is currently being submitted to ArrayExpress (EMBL-European Bioinformatics Institute, Hinxton, U.K.; http://www.ebi.ac.uk/arrayexpress) and will be available for public download by the second quarter of 2004. Accession numbers referencing this data set will be available on the HESI web site (http://hesi.ilsi.org/ index.cfm?pubentityid=120). Data Analysis U.S. Environmental Protection Agency. Background was subtracted from intensity values and log 2 values were calculated for use in the analysis. For intensity values less than background, 0 was used in the analysis. Values less than twice background were flagged as not expressed. Genes that were not expressed in at least three of four arrays for at least one of three treatment groups were removed from further analysis. Normalization was carried out using a oneway analysis of variance (ANOVA) [model log(intensity) = array], so each array was centered around 0. Each gene was then analyzed in a one-way ANOVA [log(intensity) = treatment], and where the overall F test was significant, indicating some differences among treatment groups, the lowand high-dose groups were each tested for any difference from control using a t test. A significant change was deemed to have occurred where p &lt; 0.05. GlaxoSmithKline. Normalization and statistical analysis were by normalized local regression  Quantitative real-time polymerase chain reaction. Total RNA was isolated from samples as previously described. Quantitative real-time PCR was conducted using TaqMan and SYBR green methods. Briefly, primers were designed for ACOX, apolipoprotein A-1 (APO-A1), catecholamine O-methyltransferase (COMT), cytochrome P450 4A1 (CYP4A1), cytosolic epoxide hydrolase (CEH), hydroxyacyl-CoA dehydrogenase (HCD), paraoxonase 1 (PON1), PPARα, and phosphoenolpyruvate carboxykinase (PEPCK) with Primer Express software (PerkinElmer Applied Biosystems, Foster City, CA, USA) following the manufacturer&apos;s advice for optimal primer design for the TaqMan reactions. For the SYBR green method, Clontech sequences were used. Primer and probe sequences are listed i",,112(4).Tox.Pt2.MON.417-510.qx,,,,core
373291912,2020-12-13T00:00:00,"International audienceWe consider the problem of an operator controlling a fleet of electric vehicles for use in a ridehailing service. The operator, seeking to maximize revenue, must assign vehicles to requests as they arise and recharge and reposition vehicles in anticipation of future requests. To solve this problem, we employ deep reinforcement learning, developing policies whose decision making uses Q-value approximations learned by deep neural networks. We compare these policies against a common taxi dispatching heuristic and against dual bounds on the value of an optimal policy, including the value of an optimal policy with perfect information which we establish using a Benders-based decomposition. We assess performance on instances derived from real data for the island of Manhattan in New York City. We find that, across instances of varying size, our best policy trained with deep reinforcement learning outperforms the taxi dispatching heuristic. We also provide evidence that this policy may be effectively scaled and deployed on larger instances without retraining",'Institute for Operations Research and the Management Sciences (INFORMS)',Dynamic Ridehailing with Electric Vehicles,,,,core
368711745,2020-01-01T00:00:00,"In this prospective, interventional, international study, we investigate continuous monitoring of hospitalised patients’ vital signs using wearable technology as a basis for real-time early warning scores (EWS) estimation and vital signs time-series prediction. The collected continuous monitored vital signs are heart rate, blood pressure, respiration rate, and oxygen saturation of a heterogeneous patient population hospitalised in cardiology, postsurgical, and dialysis wards. Two aspects are elaborated in this study. The first is the high-rate (every minute) estimation of the statistical values (e.g., minimum and mean) of the vital signs components of the EWS for one-minute segments in contrast with the conventional routine of 2 to 3 times per day. The second aspect explores the use of a hybrid machine learning algorithm of kNN-LS-SVM for predicting future values of monitored vital signs. It is demonstrated that a real-time implementation of EWS in clinical practice is possible. Furthermore, we showed a promising prediction performance of vital signs compared to the most recent state of the art of a boosted approach of LSTM. The reported mean absolute percentage errors of predicting one-hour averaged heart rate are 4.1, 4.5, and 5% for the upcoming one, two, and three hours respectively for cardiology patients. The obtained results in this study show the potential of using wearable technology to continuously monitor the vital signs of hospitalised patients as the real-time estimation of EWS in addition to a reliable prediction of the future values of these vital signs is presented. Ultimately, both approaches of high-rate EWS computation and vital signs time-series prediction is promising to provide efficient cost-utility, ease of mobility and portability, streaming analytics, and early warning for vital signs deterioration",'MDPI AG',Vital signs prediction and early warning score calculation based on continuous monitoring of hospitalised patients using wearable technology,10.3390/s20226593,https://core.ac.uk/download/368711745.pdf,,core
327690899,2020-06-25T00:00:00,"The article is devoted to analyzing methods for recognizing images and finding them in the video stream. The evolution of the structure of convolutional neural networks used in the field of computer video flow diagnostics is analyzed. The performance of video flow diagnostics algorithms and car license plate recognition has been evaluated. The technique of recognizing the license plates of cars in the video stream of transport neural networks is described. The study focuses on the creation of a combined system that combines artificial intelligence and computer vision based on fuzzy logic. To solve the problem of license plate image recognition in the video stream of the transport system, a method of image recognition in a continuous video stream with its implementation based on the composition of traditional image processing methods and neural networks with convolutional and periodic layers is proposed. The structure and peculiarities of functioning of the intelligent distributed system of urban transport safety, which feature is the use of mobile devices connected to a single network, are described.
A practical implementation of a software application for recognizing car license plates by mobile devices on the Android operating system platform has been proposed and implemented. Various real-time vehicle license plate recognition scenarios have been developed and stored in a database for further analysis and use. The proposed application uses two different specialized neural networks: one for detecting objects in the video stream, the other for recognizing text from the selected image. Testing and analysis of software applications on the Android operating system platform for license plate recognition in real time confirmed the functionality of the proposed mathematical software and can be used to securely analyze the license plates of cars in the scanned video stream by comparing with license plates in the existing database. The authors have implemented the operation of the method of convolutional neural networks detection and recognition of license plates, personnel and critical situations in the video stream from cameras of mobile devices in real time. The possibility of its application in the field of safe identification of car license plates has been demonstrated.Стаття присвячена аналізу методів розпізнавання зображень та пошуку їх у відеопотоці. Проаналізовано еволюцію структури згорткових нейронних мереж, що використовуються в області діагностики комп'ютерних відеопотоків. Здійснено оцінку ефективності алгоритмів діагностики відеопотоків та розпізнавання номерних знаків автомобілів. Описана методика розпізнавання номерних знаків автомобілів, що знаходяться у відеопотоці транспортних нейронних мереж. В дослідженні приділено увагу створенню комбінованої системи, яка поєднує в собі технологію штучного інтелекту та комп'ютерного зору на основі нечіткої логіки. Для вирішення проблеми розпізнавання зображень номерних знаків у відеопотоці транспортної системи запропоновано метод розпізнавання зображень у безперервному відеопотоці з його реалізацією на основі складу традиційних методів обробки зображень та нейронних мереж із згортковими та періодичними шарами. Описано структуру та особливості функціонування інтелектуальної розподіленої системи безпеки міського транспорту, особливістю якої є використання мобільних пристроїв, підключених до єдиної мережі. Запропоновано та здійснено практичну реалізацію програмного застосування для розпізнавання автомобільних номерних знаків мобільними пристроями на платформі операційної системи Android. Розроблено різні сценарії розпізнавання номерних знаків автомобілів у реальному часі та збереження їх у базі даних для подальшого аналізу та використання. В запропонованому застосуванні використовуються дві різні спеціалізовані нейромережі: одна - для детектування об’єктів у відеопотоці, інша – для розпізнавання тексту з виділеного зображення. Проведене випробовування та аналіз програмного застосування на платформі операційної системи Android для розпізнавання автомобільних номерних знаків у реальному часі підтвердив працездатність запропонованого математичного забезпечення і може використовуватися для безпечного аналізу номерних знаків автомобілів у сканованому відео потоці шляхом порівняння з номерними знаками в існуючій базі даних. Авторами реалізовано функціонування метод згорткових нейронних мереж виявлення та розпізнавання номерних знаків, персоналу та критичних ситуацій у відеопотоці з камер мобільних пристроїв у режимі реального часу. Продемонстрована можливість його застосування у сфері безпечної ідентифікації номерних знаків автомобілів",'Borys Grinchenko Kyiv University',ЗАСТОСУВАННЯ ЗГОРТКОВИХ НЕЙРОННИХ МЕРЕЖ ДЛЯ БЕЗПЕКИ РОЗПІЗНАВАННЯ ОБ'ЄКТІВ У ВІДЕОПОТОЦІ,10.28925/2663-4023.2020.8.97112,https://core.ac.uk/download/327690899.pdf,,core
326452882,2020-01-01T00:00:00,"This project presents the implementation of suitable Machine Learning (ML) architecture(s) to achieve real-time object detection and classification in a quadrotor in an urban environment, with a reasonable level of accuracy. Here, a suitable architecture refers to one that is able to achieve real-time performance, generally agreed to be 30 fps or higher among the community of ML practitioners. There is a compromise to be reached between accuracy and speed. Here, the constraint for speed is limited to the requirement of real-time performance. It is satisfactory to achieve levels of prediction accuracy comparable to current standards of reasonable accuracy, although the achievement of higher accuracy would be welcomed.Bachelor of Engineering (Mechanical Engineering",'Nanyang Technological University',Development of a machine-learning based object recognition system for quadrotors in urban environments,,,,core
322820645,2020-01-01T00:00:00,"The smartness of a city is given by the technologies it put to use, and more than that, by the people empowered by such technologies; it is worth thinking about how people can be trained to be empowered by smart technologies, and how cities can become “educational.” So, while sustainability and technology solutions for smart cities are strategic challenges, one of these is surely distance education and training. In this field, the Web offers many opportunities, such as the e-learning platforms where students can learn, according to their own needs and pace. The massive open online courses (MOOCs) are particular distance learning platforms, generally offering, so far, free courses on a huge amount of topics, and characterized by a (potentially) very high number of enrollments. In a MOOC, a teacher, or tutor, has a hard life when trying to follow and manage with the learning processes of thousands of students. In particular, assessment can be managed almost exclusively by letting the student answer questions in closed answers tests. This strategy has some didactic limits, while a valid alternative is to use peer assessment (PA) over more articulated assessment activities (e.g., open-ended questions). PA makes students grade their peers’ answers, and provides learners with significant advantages, such as refining their knowledge of the subject matter, and developing their meta-cognitive skills. In this work, we present a software platform called K-OpenAnswer, which helps teachers to simulate the dynamic of a MOOC where PA is used. The system uses a machine learning technique, based on a modified version of the K-NN algorithm, and provides teachers with a statistical environment by which they can monitor the evolving dynamic of a simulated MOOC, according to the techniques we use to implement PA. An experimental evaluation is presented that highlights the advantages of using the system as a valid tool for the study of real MOOCs",'Springer Science and Business Media LLC',K-OpenAnswer: a simulation environment to analyze the dynamics of massive open online courses in smart cities,10.1007/s00500-020-04696-z,,,core
323920164,2020-09-18T00:00:00,"We investigate the ability of popular flow based methods to capture
tail-properties of a target density by studying the increasing triangular maps
used in these flow methods acting on a tractable source density. We show that
the density quantile functions of the source and target density provide a
precise characterization of the slope of transformation required to capture
tails in a target density. We further show that any Lipschitz-continuous
transport map acting on a source density will result in a density with similar
tail properties as the source, highlighting the trade-off between a complex
source density and a sufficiently expressive transformation to capture
desirable properties of a target density. Subsequently, we illustrate that flow
models like Real-NVP, MAF, and Glow as implemented originally lack the ability
to capture a distribution with non-Gaussian tails. We circumvent this problem
by proposing tail-adaptive flows consisting of a source distribution that can
be learned simultaneously with the triangular map to capture tail-properties of
a target density. We perform several synthetic and real-world experiments to
compliment our theoretical findings.Comment: Published at the 37th International Conference of Machine Learning,
  (ICML 2020",,Tails of Lipschitz Triangular Flows,,http://arxiv.org/abs/1907.04481,,core
387298378,2020-12-09T00:00:00,"Ramp metering that uses traffic signals to regulate vehicle flows from the
on-ramps has been widely implemented to improve vehicle mobility of the
freeway. Previous studies generally update signal timings in real-time based on
predefined traffic measures collected by point detectors, such as traffic
volumes and occupancies. Comparing with point detectors, traffic cameras-which
have been increasingly deployed on road networks-could cover larger areas and
provide more detailed traffic information. In this work, we propose a deep
reinforcement learning (DRL) method to explore the potential of traffic video
data in improving the efficiency of ramp metering. The proposed method uses
traffic video frames as inputs and learns the optimal control strategies
directly from the high-dimensional visual inputs. A real-world case study
demonstrates that, in comparison with a state-of-the-practice method, the
proposed DRL method results in 1) lower travel times in the mainline, 2)
shorter vehicle queues at the on-ramp, and 3) higher traffic flows downstream
of the merging area. The results suggest that the proposed method is able to
extract useful information from the video data for better ramp metering
controls",,"A Deep Reinforcement Learning Approach for Ramp Metering Based on
  Traffic Video Data",,http://arxiv.org/abs/2012.12104,,core
334984749,2020-01-01T08:00:00,"With the rapid development in wireless communications and artificial intelligence in recent years, vehicles are equipped with various smart devices to have ubiquitous access to the internet. These smart vehicles can easily exchange information with surrounding objects, e.g. vehicles, pedestrians, and roadside units, which form an enormous network. These networks that built upon vehicles are named vehicular networks. As the key enabler of the Intelligent Transportation System, vehicular networks are envisioned to simplify the traffic management, improve road safety, and provide infotainment through various vehicular services, like vehicle to everything communications, vehicular fog computing, and the location-based services. However, vehicular networks are vulnerable to various security and privacy risks due to the wireless nature and the heterogeneous network attributes. How to provide security and privacy for vehicular networks has attracted great attentions from both industry and academia. So in this dissertation, we present a study of security and privacy of vehicular networks. Novel security and privacy solutions for different vehicular services are proposed, which address the security and privacy issues in vehicle-to-everything communications, fast handover, vehicular fog computing, and location-based services. The major contributions of this dissertation are shown as follows. 1) A secure and efficient privacy-preserving authentication scheme is proposed. The proposed scheme is based on a 5G software-defined vehicular network architecture and is proved to be secure and highly efficient. 2) A secure and efficient handover scheme is proposed to provide user uninterrupted vehicular service, which is critical for the frequent handover processes on high-speed vehicles. 3) An efficient group management and key distribution scheme is proposed for vehicular fog computing paradigm, which has a lower delay compared to other existing schemes. 4) A location privacy-preserving scheme is proposed for location-based services. With the proposed scheme, vehicle users can have real-time location-based services with accurate location information updates while preserving location privacy. At the end of this dissertation, we also point out open research issues in securing vehicular networks",DigitalCommons@University of Nebraska - Lincoln,A Study of Security and Privacy in Vehicular Networks,,,,core
477679596,2020-03-23T00:00:00,"Participant selection is a major research challenge in Mobile Crowdsensing (MCS). Previous approaches commonly assume that adequately long and fixed periods of candidate participants' historical mobility trajectories are available before the selection process. This enables the frameworks to accurately model mobility which enables the optimization of selection. However, this assumption may not be realistic for newly-released MCS applications or platforms because the candidates have just boarded without previous mobility profiles. The sparsity or even absence of mobility traces will incur inaccurate location prediction of the individual participant, thus imposing negative effects on the participant selection process and hindering the practical deployment of new MCS applications. To this end, this paper investigates a novel problem called ""From-Scratch MCS"" (FS-MCS for short), in which we study how to intelligently select participants to minimize such ""cold-start"" effect. Specifically, we propose a novel framework based on reinforcement learning, which we name RL-Recruiter. With the gradual accumulation of mobility trajectories over time, RL-Recruiter can make a good sequence of participant selection decisions for each sensing slot by incrementally extracting and utilizing the collective mobility patterns of all candidate participants, thus avoiding the prediction of individual participant's location that is very inaccurate when the training data is sparse. We test our approach experimentally based on two real-world mobility datasets. Our experiment results demonstrate that RL-Recruiter outperforms the baseline approaches under various settings. © 2020 IEEE",'Institute of Electrical and Electronics Engineers (IEEE)',"Participants Selection for From-Scratch Mobile Crowdsensing via Reinforcement Learning:18th Annual IEEE International Conference on Pervasive Computing and Communications, PerCom 2020",10.1109/PerCom45495.2020.9127383,,,core
237022405,2020-09-10T00:00:00,"Unmanned aerial vehicles (UAVs) are expected to be deployed in a variety of applications in future mobile networks due to several advantages they bring over the deployment of ground base stations. However, despite the recent interest in UAVs in mobile networks, some issues still remain, such as determining the placement of multiple UAVs in different scenarios. In this paper we propose a solution to determine the optimal 3D position of multiple UAVs in a capacity enhancement use-case, or in other words, when the ground network cannot cope with the user traffic demand. For this scenario, real data from the city of Milan, provided by Telecom Italia is utilized to simulate an event. Based on that, a solution based on k-means, a machine learning technique, to position multiple UAVs is proposed and it is compared with two other baseline methods. Results demonstrate that the proposed solution is able to significantly outperform other methods in terms of users covered and quality of service",'Institute of Electrical and Electronics Engineers (IEEE)',Clustering Based UAV Base Station Positioning for Enhanced Network Capacity,10.1109/AECT47998.2020.9194188,https://core.ac.uk/download/237022405.pdf,,core
334940475,2020-09-23T00:00:00,"Self-driving cars and autonomous vehicles are revolutionizing the automotive
sector, shaping the future of mobility altogether. Although the integration of
novel technologies such as Artificial Intelligence (AI) and Cloud/Edge
computing provides golden opportunities to improve autonomous driving
applications, there is the need to modernize accordingly the whole prototyping
and deployment cycle of AI components. This paper proposes a novel framework
for developing so-called AI Inference Engines for autonomous driving
applications based on deep learning modules, where training tasks are deployed
elastically over both Cloud and Edge resources, with the purpose of reducing
the required network bandwidth, as well as mitigating privacy issues. Based on
our proposed data driven V-Model, we introduce a simple yet elegant solution
for the AI components development cycle, where prototyping takes place in the
cloud according to the Software-in-the-Loop (SiL) paradigm, while deployment
and evaluation on the target ECUs (Electronic Control Units) is performed as
Hardware-in-the-Loop (HiL) testing. The effectiveness of the proposed framework
is demonstrated using two real-world use-cases of AI inference engines for
autonomous vehicles, that is environment perception and most probable path
prediction.Comment: 21 pages Published in Sensors:
  https://www.mdpi.com/1424-8220/20/19/545",'MDPI AG',"Cloud2Edge Elastic AI Framework for Prototyping and Deployment of AI
  Inference Engines in Autonomous Vehicles",10.3390/s20195450,http://arxiv.org/abs/2009.11722,,core
334918017,2020-03-16T00:00:00,"A great deal of research has demonstrated recently that multi-view stereo
(MVS) matching can be solved with deep learning methods. However, these efforts
were focused on close-range objects and only a very few of the deep
learning-based methods were specifically designed for large-scale 3D urban
reconstruction due to the lack of multi-view aerial image benchmarks. In this
paper, we present a synthetic aerial dataset, called the WHU dataset, we
created for MVS tasks, which, to our knowledge, is the first large-scale
multi-view aerial dataset. It was generated from a highly accurate 3D digital
surface model produced from thousands of real aerial images with precise camera
parameters. We also introduce in this paper a novel network, called RED-Net,
for wide-range depth inference, which we developed from a recurrent
encoder-decoder structure to regularize cost maps across depths and a 2D fully
convolutional network as framework. RED-Net's low memory requirements and high
performance make it suitable for large-scale and highly accurate 3D Earth
surface reconstruction. Our experiments confirmed that not only did our method
exceed the current state-of-the-art MVS methods by more than 50% mean absolute
error (MAE) with less memory and computational cost, but its efficiency as
well. It outperformed one of the best commercial software programs based on
conventional methods, improving their efficiency 16 times over. Moreover, we
proved that our RED-Net model pre-trained on the synthetic WHU dataset can be
efficiently transferred to very different multi-view aerial image datasets
without any fine-tuning. Dataset are available at http://gpcv.whu.edu.cn/data",,"A Novel Recurrent Encoder-Decoder Structure for Large-Scale Multi-view
  Stereo Reconstruction from An Open Aerial Dataset",,http://arxiv.org/abs/2003.00637,,core
355099865,2020-01-01T00:00:00,"Abstract: Poor management practices of road transport assets posed a challenge to the sustainable development of the transport system in developing countries like Nigeria. Studies in the past focused mainly on the performance of road construction process. However, few studies have evaluated the effect of the fourth industrial revolution (4.0IR) on the road transport assets in developing countries such as Nigeria. The current study aimed at assessing the effect of the fourth industrial revolution towards improving the management practice of road transport assets. Survey instruments were administered to project and facility managers in the Nigerian road construction sector of the economy using a proportionate random sampling technique. Partial least square structural equation modelling was used for data analysis utilising the Warp 7.0 PLS-SEM software algorithm. The software calculates p-values with WarpPLS based on non-parametric algorithms, resampling or stable algorithms and thus does not require that the variables to be normally distributed. The study concluded that 4.0IR drivers have a moderate effect change on the management practice of road transport assets in Nigeria at the moment. The findings imply that management of road assets in Nigeria would moderately improve due to 4.0IR technologies resulting in transport, safety and general efficiency and effectiveness of road networks in Nigeria. The study identified 4.0IR drivers to include; robotics, mobility, virtual and augmented reality, internet of things and cloud computing, machine learning, artificial intelligence, blockchain, 3D printing drones that are built with an attached 3D printer, (the drone hangs a 3D printing nozzle that's fed plastic, concrete mix or other material from a tube connected to the top of the drone's printing path that precisely plotted by software, for a promised printing accuracy of 0.1mm),and digital engineering. This study emanated from the government reports and past studies in the area of road transport asset management practice which the study investigated the major causes of poor practices and assessed the effect of the fourth industrial revolution on the practice",,Effect of the Fourth Industrial Revolution on Road Transport Asset Management Practice in Nigeria,,,,core
304168838,2020-01-01T00:00:00,"Photo-realistic 3D city models that represent the physical and functional state of the city are necessary components of the nation’s digital infrastructure. It facilitates much easier governing, planning, simulation, measuring and prediction, and provides numerous potentials in both scientific and industry applications. LoD3 models contain building roof and façade geometry, as well as the functions of its different components (windows, doors, etc.). Generating accurate and standard 3D city models is a manually tedious, decisively rich and non-straightforward process, and the current practice of is LoD3 city modelling still a manually intensive process.
Given the high demand for city-scale model production in the virtual Singapore program, we aim to develop an operable workflow that could produce LoD3 with the lightest possible manual involvement. A multi-data approach is used by integrating different sources of data including oblique imagery, aerial images, airborne/mobile LiDAR, and UAV images, to produce high quality LoD3 models that meet the CityGML standards. The workflow consists of three necessary work packages (WP) that develop techniques in 1) Geometry modelling, 2) Semantic labelling and texture mapping and 3) Integration of procedural modelling and interactive geometric editing. WP1 develops novel image-based and LiDAR based roof topography and façade geometry modelling with automated and semi-automated methods. WP2 applies data fusion techniques with the latest machine learning methods to perform land-cover classification, façade element attribution and texture mapping. WP2 also develops a preliminary proof of concept in change detection. To ensure high fidelity of the resulting models, WP3 develops novel visualization-driven editing procedures that efficiently correct errors of the models and integrate the procedural modelling workflow into the 3D reconstruction of buildings with regular geometric patterns.
The major objective of this project is to work towards an operationally feasible approach to generate city-scale LoD3 models, and provide preliminary proof-of-concept on efficient model maintenance, to facilitate the broader mission of the Virtual Singapore program in developing Singapore as a more intelligent and smarter city.
In the course of this project we have developed a number of novel approaches and algorithms for data processing. We have demonstrated their functionality with real data from our NUS Campus test field. However, still needed is the integration of the software packages into a fully functional operational system and a robustification of some components","Singapore ETH Centre, Future Cities Laboratory",An Operable System for LoD3 Model Generation Using Multi-Source Data and User-Friendly Interactive Editing: Final Report,10.3929/ethz-b-000409483,,,core
386415663,2020-12-17T00:00:00,"Digitalisierung hat sich in Wirtschaft, Wissenschaft und Gesellschaft als der Change Maker schlechthin etabliert. Infrastrukturen, Arbeitsweisen und Kompetenzen stehen im Vordergrund vieler Debatten und bestimmen mehr und mehr die Zukunftsfähigkeit ganzer Branchen. Wir haben uns offenbar auf den permanenten Wandel bei zunehmender Beschleunigung eingelassen. Aber: Wo geht die Reise tatsächlich hin? Konstituieren sich Gemeinschaften ausschließlich im Wechselspiel hybrider Realitäten? Sind große Datenmengen Bedrohung oder Chance? Können wir diese überhaupt verarbeiten oder bedarf es dafür grundlegend veränderter Werkzeuge und Methoden – wie Visual Analytics, Virtuelle Rekonstruktion, Virtual Engineering, virtueller Assistenten und kooperativer VR? Waren IT-Innovationen bis vor kurzem etwas für Digital-Experten*innen so sind hybride Gemeinschaften in virtuellen Realitäten mittlerweile Alltag. Doch worauf müssen sich Führungskräfte einstellen? Digitalisierung bedeutet neue Möglichkeiten für Öffnung, Transparenz und Partizipation. Kommt es in diesem Zuge auch zu einem Revival humanzentrierter Managementaktivitäten? [... aus der Einleitung]:Gemeinschaften in Neuen Medien. Von hybriden Realitäten zu hybriden Gemeinschaften XXIX
Communities in New Media. From hybrid realities to hybrid communities XXXIV

A Eingeladene Vorträge 1
A.1 Interaktive Online Formate zur Wissensteilung: Systematisierung und Handlungsempfehlung für geeignete IT-Tools 1
A.2 Von der Hand in den Kopf in die Stadt 9

B Erfahrungen mit digitaler Praxis 10
B.1 Den Gottesdienst von zu Hause mitfeiern 10
B.2 Konzeption und Evaluation des Kompetenzzentrums Medien 19
B.3 Supporting Learning in Art History – Artificial Intelligence in Digital Humanities Education 28
B.4 Detecting Treasures in Museums with Artificial Intelligence 36

C Digitale Entwicklung in Wirtschaft und Industrie 49
C.1 Triebkräfte der digitalen Partizipation: Was Online-Community-Mitglieder zur proaktiven Beteiligung motiviert 49
C.2 Online-Panel: Communities und Netzwerke als Treiber des digitalen Wandels: Erfahrungen, Perspektiven und Ausblick 60
C.3 Digitale Innovationen im Handwerk 65
C.4 Online-Panel: Conversational Platforms als strategisches Digitalisierungsinstrument 75

D Partizipation 80
D.1 Ein systematisch gestalteter Softwareprototyp zur Erhöhung der Partizipationsbereitschaft 80
D.2 Gamification as a Means to Improve Stakeholder Management in Urban Planning Participation 90
D.3 The Right Reaction: Entwicklung und Evaluation eines emotionsbasierten Software-Prototypen 99

E Cases of digitizing higher education – a global perspective 110
E.1 EdTec Implementation in a global higher education network. Empirical data from a field study in South Asia 110
E.2 Use-Case Studie eines auf der Nutzung von Handlungsfehlern basierenden AR-Lernsystems zur kritischen Reflexion der technischen Umsetzbarkeit 126
E.3 Organizational models in virtual teaching cooperation – documentation and evaluation of organisational didactics in a collaborative higher education project 133
E.4 Ein Fall für zwei Hochschulen: Entwicklung eines modularen Manuals zur Gestaltung von Fallstudienseminaren im virtuellen Raum 144

F Future learning in der beruflichen Bildung 150
F.1 Potenziale für das technologiebasierte Lehren und Lernen in der Weiterbildung 150
F.2 Mediennutzungskonzepte an Berufsschulen – Webseitenanalyse zur Selbstdarstellung der digitalen Kompetenz 164
F.3 Spielend leicht Veränderungen lernen – Serious Games in der Schulungsumgebung von Unternehmen 173
F.4 Game-Based Learning in der beruflichen Bildung 179

G Methoden und Technologien des Assessments 186
G.1 Itempool-Management mit Microsoft Excel: Eine UX-Studie 186
G.2 KiWI-Kompetenzmodellentwicklung in der Wirtschaftsinformatik 195
G.3 „Nichts als die Wahrheit?“ – eine empirische Untersuchung des Zusammenhangs zwischen persönlichkeits- und nutzerbezogenen Faktoren und der Suggestibilität für Fake News im Internet 204
G.4 Decision-making style and trusting stance at the workplace: a socio-cultural approach 217

H Exploring Digital Realities empirically 226
H.1 Who gets the fame, who is to blame? Empirical exploration of responsibility attribution in HCI 226
H.2 VibTacX: A taxonomy for vibro-tactile patterns 236
H.3 Das Robot Impression Inventory – Ein modulares Instrument zur Erfassung des subjektiven Eindrucks von Robotern 244
H.4 Augmented Reality Passenger Information on Mobile Public Displays 250

I Teaching in Open Education 258
I.1 Parcours on Gamification – Ein Train-the-Trainer-Konzept zur Steigerung der Gamification-Readiness 258
I.2 Lehren mit OER: Förderung von Kompetenzen für Lehrende an Hochschulen für offene Bildung auf spielerischem Weg 264
I.3 Digitale Lehr und Lernunterstützung an deutschen Universitäten – Anforderungen und Rahmenbedingungen für die Implementierung einer Mentoring Workbench 279
I.4 Nach dem sog. MOOC-Hype: Welche kritischen Fragen an die Hochschullehre bleiben 289
I.5 Conducting Oral Examinations Virtually using MS Teams – An Insightful Experience Report 294

J Digitale Lern- und Spielkulturen 299
J.1 Spielerischer Zugang zu MINT-Studiengängen – das Serious Game des Learn&Play Projekts als Anwendungsbeispiel 299
J.2 Entwicklung und Evaluation digitaler Lernspiele – Wissenschaftliche Befunde jenseits des Entertainment 306
J.3 Ausgespielt? Zu Risiken und Nebenwirkungen von Gamification 318

K Betriebliche Weiterbildung 332
K.1 Leading Digital Change – Management of Hybridity and Change in Education and Social Service Institutions 332
K.2 Use Cases of Enterprise Social Software in Consulting: A Practice Perspective 342
K.3 Betriebliche Weiterbildung in sächsischen Klein- und Kleinstunter nehmen – arbeitsplatzintegriert und digital gestützt? 353
K.4 Wie „Change Maker“ Visionen für den digitalen Wandel an Bildungs einrichtungen des Handels entwickeln und umsetzen – ein Praxisbeispiel 364

L Digitalisierung im Lehramtsstudium 370
L.1 Anknüpfungspunkte zur Integration informatischer Inhalte und Kompetenzen in der Grundschule am Beispiel sächsischer Lehrpläne 370
L.2 Digitalisierungsbezogene Kompetenzen von Lehrenden in den Lehramtsstudiengängen – Entwicklung eines Kompetenzrahmens 377
L.3 DigiBlock – E-Learning im Blockpraktikum A im Lehramt an berufsbildenden Schulen 385

M Lehren und Lernen 391
M.1 Jump starting e-learning: the impact of COVID-19 on perceived learning success – A real-time case study 391
M.2 Online-Lehre im Lockdown: Analyse des Nutzungsverhaltens von kollaborativen Werkzeugen durch Studierende und Lehrende im Fachhochschul- und Berufsschulkontext 403
M.3 Teaching in a crisis? Guidance for digital education in Pandemic Times 413
M.4 Mit dem MINTcoach auf Mission 422
M.5 Onboarding in Virtuellen Kollaborativen Umgebungen – Implikationen für Lehre und Betrieb 432
M.6 Modulare Selbstlernangebote auf Basis von Videotutorien zur Vermittlung digitaler Forschungsmethoden in den Geisteswissenschaften – Forschungsstand und curriculare Perspektiven 441

N Wissenskollaboration im betrieblichen Kontext 452
N.1 Digitalisierung als Treiber in der beruflichen Bildung – Entwicklung eines Instruments zur Erfassung von Indikatoren für die Akzeptanz von virtuellen Lernortkooperationen 452
N.2 Digitaler Wissenstransfer in der beruflichen Bildung – Potentiale eines Online-Berichtsheftes 470
Autorenverzeichnis 476Digitisation has established itself as the change maker par excellence in business, science and society. Infrastructures, working methods and skills are at the forefront of many debates and increasingly determine the future viability of entire industries. We have obviously embraced the permanent change with increasing acceleration. But: Where is the journey really going? Do communities constitute themselves exclusively in the interplay of hybrid realities? Are large amounts of data a threat or an opportunity? Can we process them at all or do we need fundamentally different tools and methods – such as visual analytics, virtual reconstruction, virtual engineering, virtual assistants and cooperative VR? Until recently, IT innovations were something for digital experts*, but hybrid communities in virtual realities are now part of everyday life. But what do managers have to prepare for? Digitalisation means new opportunities for openness, transparency and participation. Will this also lead to a revival of human-centred management activities? [... from the introduction]:Gemeinschaften in Neuen Medien. Von hybriden Realitäten zu hybriden Gemeinschaften XXIX
Communities in New Media. From hybrid realities to hybrid communities XXXIV

A Eingeladene Vorträge 1
A.1 Interaktive Online Formate zur Wissensteilung: Systematisierung und Handlungsempfehlung für geeignete IT-Tools 1
A.2 Von der Hand in den Kopf in die Stadt 9

B Erfahrungen mit digitaler Praxis 10
B.1 Den Gottesdienst von zu Hause mitfeiern 10
B.2 Konzeption und Evaluation des Kompetenzzentrums Medien 19
B.3 Supporting Learning in Art History – Artificial Intelligence in Digital Humanities Education 28
B.4 Detecting Treasures in Museums with Artificial Intelligence 36

C Digitale Entwicklung in Wirtschaft und Industrie 49
C.1 Triebkräfte der digitalen Partizipation: Was Online-Community-Mitglieder zur proaktiven Beteiligung motiviert 49
C.2 Online-Panel: Communities und Netzwerke als Treiber des digitalen Wandels: Erfahrungen, Perspektiven und Ausblick 60
C.3 Digitale Innovationen im Handwerk 65
C.4 Online-Panel: Conversational Platforms als strategisches Digitalisierungsinstrument 75

D Partizipation 80
D.1 Ein systematisch gestalteter Softwareprototyp zur Erhöhung der Partizipationsbereitschaft 80
D.2 Gamification as a Means to Improve Stakeholder Management in Urban Planning Participation 90
D.3 The Right Reaction: Entwicklung und Evaluation eines emotionsbasierten Software-Prototypen 99

E Cases of digitizing higher education – a global perspective 110
E.1 EdTec Implementation in a global higher education network. Empirical data from a field study in South Asia 110
E.2 Use-Case Studie eines auf der Nutzung von Handlungsfehlern basierenden AR-Lernsystems zur kritischen Reflexion der technischen Umsetzbarkeit 126
E.3 Organizational models in virtual teaching cooperation – documentation and evaluation of organisational didactics in a collaborative higher education project 133
E.4 Ein Fall für zwei Hochschulen: Entwicklung eines modularen Manuals zur Gestaltung von Fallstudienseminaren im virtuellen Raum 144

F Future learning in der beruflichen Bildung 150
F.1 Potenziale für das technologiebasierte Lehren und Lernen in der Weiterbildung 150
F.2 Mediennutzungskonzepte an Berufsschulen – Webseitenanalyse zur Selbstdarstellung der digitalen Kompetenz 164
F.3 Spielend leicht Veränderungen lernen – Serious Games in der Schulungsumgebung von Unternehmen 173
F.4 Game-Based Learning in der beruflichen Bildung 179

G Methoden und Technologien des Assessments 186
G.1 Itempool-Management mit Microsoft Excel: Eine UX-Studie 186
G.2 KiWI-Kompetenzmodellentwicklung in der Wirtschaftsinformatik 195
G.3 „Nichts als die Wahrheit?“ – eine empirische Untersuchung des Zusammenhangs zwischen persönlichkeits- und nutzerbezogenen Faktoren und der Suggestibilität für Fake News im Internet 204
G.4 Decision-making style and trusting stance at the workplace: a socio-cultural approach 217

H Exploring Digital Realities empirically 226
H.1 Who gets the fame, who is to blame? Empirical exploration of responsibility attribution in HCI 226
H.2 VibTacX: A taxonomy for vibro-tactile patterns 236
H.3 Das Robot Impression Inventory – Ein modulares Instrument zur Erfassung des subjektiven Eindrucks von Robotern 244
H.4 Augmented Reality Passenger Information on Mobile Public Displays 250

I Teaching in Open Education 258
I.1 Parcours on Gamification – Ein Train-the-Trainer-Konzept zur Steigerung der Gamification-Readiness 258
I.2 Lehren mit OER: Förderung von Kompetenzen für Lehrende an Hochschulen für offene Bildung auf spielerischem Weg 264
I.3 Digitale Lehr und Lernunterstützung an deutschen Universitäten – Anforderungen und Rahmenbedingungen für die Implementierung einer Mentoring Workbench 279
I.4 Nach dem sog. MOOC-Hype: Welche kritischen Fragen an die Hochschullehre bleiben 289
I.5 Conducting Oral Examinations Virtually using MS Teams – An Insightful Experience Report 294

J Digitale Lern- und Spielkulturen 299
J.1 Spielerischer Zugang zu MINT-Studiengängen – das Serious Game des Learn&Play Projekts als Anwendungsbeispiel 299
J.2 Entwicklung und Evaluation digitaler Lernspiele – Wissenschaftliche Befunde jenseits des Entertainment 306
J.3 Ausgespielt? Zu Risiken und Nebenwirkungen von Gamification 318

K Betriebliche Weiterbildung 332
K.1 Leading Digital Change – Management of Hybridity and Change in Education and Social Service Institutions 332
K.2 Use Cases of Enterprise Social Software in Consulting: A Practice Perspective 342
K.3 Betriebliche Weiterbildung in sächsischen Klein- und Kleinstunter nehmen – arbeitsplatzintegriert und digital gestützt? 353
K.4 Wie „Change Maker“ Visionen für den digitalen Wandel an Bildungs einrichtungen des Handels entwickeln und umsetzen – ein Praxisbeispiel 364

L Digitalisierung im Lehramtsstudium 370
L.1 Anknüpfungspunkte zur Integration informatischer Inhalte und Kompetenzen in der Grundschule am Beispiel sächsischer Lehrpläne 370
L.2 Digitalisierungsbezogene Kompetenzen von Lehrenden in den Lehramtsstudiengängen – Entwicklung eines Kompetenzrahmens 377
L.3 DigiBlock – E-Learning im Blockpraktikum A im Lehramt an berufsbildenden Schulen 385

M Lehren und Lernen 391
M.1 Jump starting e-learning: the impact of COVID-19 on perceived learning success – A real-time case study 391
M.2 Online-Lehre im Lockdown: Analyse des Nutzungsverhaltens von kollaborativen Werkzeugen durch Studierende und Lehrende im Fachhochschul- und Berufsschulkontext 403
M.3 Teaching in a crisis? Guidance for digital education in Pandemic Times 413
M.4 Mit dem MINTcoach auf Mission 422
M.5 Onboarding in Virtuellen Kollaborativen Umgebungen – Implikationen für Lehre und Betrieb 432
M.6 Modulare Selbstlernangebote auf Basis von Videotutorien zur Vermittlung digitaler Forschungsmethoden in den Geisteswissenschaften – Forschungsstand und curriculare Perspektiven 441

N Wissenskollaboration im betrieblichen Kontext 452
N.1 Digitalisierung als Treiber in der beruflichen Bildung – Entwicklung eines Instruments zur Erfassung von Indikatoren für die Akzeptanz von virtuellen Lernortkooperationen 452
N.2 Digitaler Wissenstransfer in der beruflichen Bildung – Potentiale eines Online-Berichtsheftes 470
Autorenverzeichnis 47",TUDpress,Gemeinschaften in Neuen Medien. Von hybriden Realitäten zu hybriden Gemeinschaften: 23. Workshop GeNeMe'20 Gemeinschaften in Neuen Medien,,https://core.ac.uk/download/386415663.pdf,,core
356660858,2020-10-31T00:00:00,"Data-centric approaches are becoming increasingly common in the creation of defense mechanisms for critical infrastructure such as the electric power grid and water treatment plants. Such approaches often use well-known methods from machine learning and system identification, i.e., the Multi-Layer Perceptron, Convolutional Neural Network, and Deep Auto Encoders to create process anomaly detectors. Such detectors are then evaluated using data generated from an operational plant or a simulator; rarely is the assessment conducted in real time on a live plant. Regardless of the method to create an anomaly detector, and the data used for performance evaluation, there remain significant challenges that ought to be overcome before such detectors can be deployed with confidence in city-scale plants or large electric power grids. This position paper enumerates such challenges that the authors have faced when creating data-centric anomaly detectors and using them in a live plant",'Association for Computing Machinery (ACM)',Challenges in machine learning based approaches for real-time anomaly detection in industrial control systems,10.1145/3384941.3409588,,,core
323985334,2020-06-10T00:00:00,"Smart Bins have become popular in smart cities and campuses around the world.
These bins have a compaction mechanism that increases the bins' capacity as
well as automated real-time collection notifications. In this paper, we propose
WasteNet, a waste classification model based on convolutional neural networks
that can be deployed on a low power device at the edge of the network, such as
a Jetson Nano. The problem of segregating waste is a big challenge for many
countries around the world. Automated waste classification at the edge allows
for fast intelligent decisions in smart bins without needing access to the
cloud. Waste is classified into six categories: paper, cardboard, glass, metal,
plastic and other. Our model achieves a 97\% prediction accuracy on the test
dataset. This level of classification accuracy will help to alleviate some
common smart bin problems, such as recycling contamination, where different
types of waste become mixed with recycling waste causing the bin to be
contaminated. It also makes the bins more user friendly as citizens do not have
to worry about disposing their rubbish in the correct bin as the smart bin will
be able to make the decision for them.Comment: 8 pages, 9 figure",,WasteNet: Waste Classification at the Edge for Smart Bins,,http://arxiv.org/abs/2006.05873,,core
328757339,2020-05-20T00:00:00,"Accurate instantaneous vehicle emissions models are vital for evaluating the impacts of road transport on air pollution at high temporal and spatial resolution. In this study, we apply machine learning techniques to a dataset of 70 diesel vehicles tested in real-world driving conditions to: (i) cluster vehicles with similar emissions performance, and (ii) model instantaneous emissions. The application of dynamic time warping and clustering analysis by NOx emissions resulted in 17 clusters capturing 88% of trips in the dataset. We show that clustering effectively groups vehicles with similar emissions profiles, however no significant correlation between emissions and vehicle characteristics (i.e. engine size, vehicle weight) were found. For each cluster, we evaluate three instantaneous emissions models: a look-up table (LT) approach, a non-linear regression (NLR) model and a neural network multi-layer perceptron (MLP) model. The NLR model provides accurate instantaneous NOx predictions, on par with the MLP: relative errors in prediction of emission factors are below 20% for both models, average fractional biases are −0.01 (s.d. 0.02) and −0.0003 (s.d. 0.04), and average normalised mean squared errors are 0.25 (s.d. 0.14) and 0.29 (s.d. 0.16), for the NLR and MLP models respectively. However, neural networks are better able to deal with vehicles not belonging to a specific cluster. The new models that we present rely on simple inputs of vehicle speed and acceleration, which could be extracted from existing sources including traffic cameras and vehicle tracking devices, and can therefore be deployed immediately to enable fast and accurate prediction of vehicle NOx emissions. The speed and the ease of use of these new models make them an ideal operational tool for policy makers aiming to build emission inventories or evaluate emissions mitigation strategies",'Elsevier BV',Modelling of instantaneous emissions from diesel vehicles with a special focus on NOx: Insights from machine learning techniques,10.1016/j.scitotenv.2020.139625,,"[{'title': 'The Science of The Total Environment', 'identifiers': ['issn:0048-9697', '0048-9697']}]",core
328855123,2020-06-30T00:00:00,"In this white paper we provide a vision for 6G Edge Intelligence. Moving towards 5G and beyond the future 6G networks, intelligent solutions utilizing data-driven machine learning and artificial intelligence become crucial for several real-world applications including but not limited to, more efficient manufacturing, novel personal smart device environments and experiences, urban computing and autonomous traffic settings. We present edge computing along with other 6G enablers as a key component to establish the future 2030 intelligent Internet technologies as shown in this series of 6G White Papers. In this white paper, we focus in the domains of edge computing infrastructure and platforms, data and edge network management, software development for edge, and real-time and distributed training of ML/AI algorithms, along with security, privacy, pricing, and end-user aspects. We discuss the key enablers and challenges and identify the key research questions for the development of the Intelligent Edge services. As a main outcome of this white paper, we envision a transition from Internet of Things to Intelligent Internet of Intelligent Things and provide a roadmap for development of 6G Intelligent Edge",University of Oulu,6G White Paper on Edge Intelligence,,https://core.ac.uk/download/328855123.pdf,,core
322669240,2020-05-28T00:00:00,"The proliferation of IoT devices and rapidly developing wireless techniques boost the data volume and service demand at the edge of the Internet. Meanwhile, increased requirement for low latency feedback has become a must for most popular mobile applications, e.g., Augmented Reality (AR), Virtual Reality (VR) and Connected Vehicles. To address these challenges, edge computing has emerged as an extensional solution for cloud computing.

This thesis studies edge computing-facilitated mobile computing and communication systems. We first propose solutions to improve edge resource utilization regarding general edge systems. We present a mechanism to cluster user requests based on similarity for better Content Delivery Net- work (CDN) performance. This mechanism works directly on current CDN architecture and can be deployed incrementally. Then we extend the mechanism by adding cache resource grouping algorithm, so that the system directs similar requests to same servers and group those servers which receive similar requests. This iterative mechanism optimizes the edge utilization by concentrating the resource on similar requests to achieve higher cache hit ratio and computation efficiency.

Thereafter, we present solutions for mobile edge systems specifically for three most promising use cases, i.e., Connected Vehicles, Mobile AR (MAR) and Smart city (traffic control). We explore the potential of edge computing in connected vehicular AR applications with real data sets. We design a lightweight edge system and data flow fit for general connected vehicular AR applications and implement a prototype. With an indoor test and real data set analysis, we find out that our system can improve the performance of vehicular AR applications with reasonable cost. To optimize the system, we formulate the problem of edge server allocation and task scheduling as a mutant multiprocessor scheduling problem and develop a two-stage edge-cloud decentralized algorithm as well as a centralized algorithm to schedule the offloading tasks on the fly. We conduct a raw road test and an extensive evaluation based on the road test results and large data sets from real world. The results show that our system improve at least twice the application performance comparing with cloud solutions.

For MAR, we consider to offload tasks to multiple edge servers via multiple paths simultaneously to further improve the MAR performance. We develop a fast scheduling algorithm to split the workloads among the avail- able edge servers and show promising results with real implementations. At last, we explore the potential of combining edge computing and ma- chine learning techniques to realize intelligent traffic control by letting edge servers co-located with traffic lights learn the waiting traffic and adapt the light periods with reinforcement learning.Esineiden Internetin leviäminen ja nopeasti kehittyvät langattomat tekniikat lisäävät datan määrää ja palvelutarvetta Internetin reunalla. Samanaikaisesti lisääntyneestä alhaisen viiveen palautteen vaatimuksesta on tullut välttämätön suosituimpiin mobiilisovelluksiin, esim. lisättyyn todellisuuteen (AR), virtuaalitodellisuuteen (VR) ja yhdistettyihin ajoneuvoihin. Reunalaskenta on noussut pilvilaskennan rinnalle näihin haasteisiin vastaavaksi ratkaisuksi.

Tässä väitöskirjassa tutkitaan laskennallisesti laajennettuja mobiililaskenta- ja viestintäjärjestelmiä. Ehdotamme ensin ratkaisuja reunaresurssien käytön parantamiseksi yleisten reunajärjestelmien suhteen. Esitämme mekanismin käyttäjien pyyntöjen klusterointiin perustuen samankaltaisuuteen sisällönjakeluverkon (CDN) suorituskyvyn parantamiseksi. Tämä mekanismi toimii suoraan nykyisessä CDN-arkkitehtuureissa ja voidaan ottaa käyttöön asteittain. Sitten laajennamme mekanismia lisäämällä välimuistiresurssien ryhmittelyalgoritmin siten, että järjestelmä ohjaa samankaltaiset pyynnöt samoille palvelimille ja ryhmittelee palvelimet pyyntöjen mukaan. Tämä iteratiivinen mekanismi optimoi reunakäytön keskittämällä resurssit samanlaisiin pyyntöihin suuremman välimuistin osumissuhteen ja laskentatehokkuuden saavuttamiseksi.

Sen jälkeen esittelemme ratkaisuja liikkuviin reunajärjestelmiin erityisesti kolmeen lupaavimpaan käyttötapaukseen, ts. yhdistetyt ajoneuvot, laajennettu mobiilitodellisuus (MAR) ja älykäs kaupunki (erityisesti liikenteenohjaus). Tutkimme reunalaskennan mahdollisuuksia yhdistettyjen ajoneuvojen AR-sovelluksissa. Suunnittelemme kevyen reunajärjestelmän ja tiedonkulun, joka sopii yleisesti yhdistettyjen ajoneuvojen AR-sovelluksiin ja toteutamme prototyypin. Sisätilojen testin ja reaalimaailman datan avulla saamme selville, että järjestelmämme voi parantaa ajoneuvojen AR-sovellusten suorituskykyä kohtuullisin kustannuksin. Järjestelmän optimoimiseksi formuloimme reunapalvelimien allokoinnin ja tehtävien ajoituksen ongelman muuttuvana moniprosessorien skedulointiongelmana ja kehitämme kaksivaiheisen reunapilviin soveltuvan hajautetun algoritmin sekä keskitetyn algoritmin kuormansiirtotehtävien ajonaikaiseen ajoittamiseen. Suoritamme kokeellisen testin oikeassa ajossa ja datapohjaisen arvioinnin, joka perustuu tietestien tuloksiin ja todellisen maailman suuriin tietojoukkoihin. Tulokset osoittavat, että järjestelmämme parantaa merkittävästi sovelluksen suorituskykyä verrattuna pilviratkaisuihin.

MAR:n osalta käsittelemme tehtävien lataamista useille reunapalvelimille useiden reittien kautta samanaikaisesti MAR:n suorituskyvyn parantamiseksi. Kehitämme nopean aikataulutusalgoritmin työkuormien jakamiseen käytettävissä olevien reunapalvelimien. Lopuksi tutkimme mahdollisuuksia yhdistää reunalaskenta ja koneoppimistekniikat älykkään liikennevalo-ohjauksen toteuttamiseksi liikennevaloihin sijoitetuilla reunapalvelimilla",'University of Helsinki Libraries',Edge-Facilitated Mobile Computing and Communication,,https://core.ac.uk/download/322669240.pdf,,core
237712408,2020-05-01T00:00:00,"Next-generation audio-visual (AV) hearing aids stand as a major enabler to realize more intelligible audio. However, high data rate, low latency, low computational complexity, and privacy are some of the major bottlenecks to the successful deployment of such advanced hearing aids. To address these challenges, we propose an integration of 5G Cloud-Radio Access Network (C-RAN), Internet of Things (IoT), and strong privacy algorithms to fully benefit from the possibilities these technologies have to offer. Existing audio-only hearing aids are known to perform poorly in noisy situations where overwhelming noise is present. Current devices make the signal more audible but remain deficient in restoring intelligibility. Thus, there is a need for hearing aids that can selectively amplify the attended talker or filter out acoustic clutter. The proposed 5G IoT-enabled AV hearing-aid framework transmits the encrypted compressed AV information and receives encrypted enhanced reconstructed speech in real time to address cybersecurity attacks such as location privacy and eavesdropping. For security implementation, a real-time lightweight AV encryption is proposed, based on a piece-wise linear chaotic map (PWLSM), Chebyshev map, and a secure hash and S-Box algorithm. For speech enhancement, the received secure AV (including lip-reading) information in the cloud is used to filter noisy audio using both deep learning and analytical acoustic modelling. To offload the computational complexity and real-time optimization issues, the framework runs deep learning and big data optimization processes in the background, on the cloud. The effectiveness and security of the proposed 5G-IoT-enabled AV hearing-aid framework are extensively evaluated using widely known security metrics. Our newly reported, deep learning-driven lip-reading approach for speech enhancement is evaluated under four different dynamic real-world scenarios (cafe, street, public transport, pedestrian area) using benchmark Grid and ChiME3 corpora. Comparative critical analysis in terms of both speech enhancement and AV encryption demonstrates the potential of the envisioned technology to deliver high-quality speech reconstruction and secure mobile AV hearing aid communication. We believe our proposed 5G IoT enabled AV hearing aid framework is an effective and feasible solution and represents a step change in the development of next-generation multimodal digital hearing aids. The ongoing and future work includes more extensive evaluation and comparison with benchmark lightweight encryption algorithms and hardware prototype implementation",'Springer Science and Business Media LLC',"A Novel Real-Time, Lightweight Chaotic-Encryption Scheme for Next-Generation Audio-Visual Hearing Aids",10.1007/s12559-019-09653-z,https://core.ac.uk/download/237712408.pdf,,core
326904660,2020-07-11T00:00:00,"Deep neural networks based object detection models have revolutionized
computer vision and fueled the development of a wide range of visual
recognition applications. However, recent studies have revealed that deep
object detectors can be compromised under adversarial attacks, causing a victim
detector to detect no object, fake objects, or mislabeled objects. With object
detection being used pervasively in many security-critical applications, such
as autonomous vehicles and smart cities, we argue that a holistic approach for
an in-depth understanding of adversarial attacks and vulnerabilities of deep
object detection systems is of utmost importance for the research community to
develop robust defense mechanisms. This paper presents a framework for
analyzing and evaluating vulnerabilities of the state-of-the-art object
detectors under an adversarial lens, aiming to analyze and demystify the attack
strategies, adverse effects, and costs, as well as the cross-model and
cross-resolution transferability of attacks. Using a set of quantitative
metrics, extensive experiments are performed on six representative deep object
detectors from three popular families (YOLOv3, SSD, and Faster R-CNN) with two
benchmark datasets (PASCAL VOC and MS COCO). We demonstrate that the proposed
framework can serve as a methodical benchmark for analyzing adversarial
behaviors and risks in real-time object detection systems. We conjecture that
this framework can also serve as a tool to assess the security risks and the
adversarial robustness of deep object detectors to be deployed in real-world
applications",,Understanding Object Detection Through An Adversarial Lens,,http://arxiv.org/abs/2007.05828,,core
345182287,2020-01-01T08:00:00,"With the rapid development in wireless communications and artificial intelligence in recent years, vehicles are equipped with various smart devices to have ubiquitous access to the internet. These smart vehicles can easily exchange information with surrounding objects, e.g. vehicles, pedestrians, and roadside units, which form an enormous network. These networks that built upon vehicles are named vehicular networks. As the key enabler of the Intelligent Transportation System, vehicular networks are envisioned to simplify the traffic management, improve road safety, and provide infotainment through various vehicular services, like vehicle to everything communications, vehicular fog computing, and the location-based services. However, vehicular networks are vulnerable to various security and privacy risks due to the wireless nature and the heterogeneous network attributes. How to provide security and privacy for vehicular networks has attracted great attentions from both industry and academia. So in this dissertation, we present a study of security and privacy of vehicular networks. Novel security and privacy solutions for different vehicular services are proposed, which address the security and privacy issues in vehicle-to-everything communications, fast handover, vehicular fog computing, and location-based services. The major contributions of this dissertation are shown as follows. 1) A secure and efficient privacy-preserving authentication scheme is proposed. The proposed scheme is based on a 5G software-defined vehicular network architecture and is proved to be secure and highly efficient. 2) A secure and efficient handover scheme is proposed to provide user uninterrupted vehicular service, which is critical for the frequent handover processes on high-speed vehicles. 3) An efficient group management and key distribution scheme is proposed for vehicular fog computing paradigm, which has a lower delay compared to other existing schemes. 4) A location privacy-preserving scheme is proposed for location-based services. With the proposed scheme, vehicle users can have real-time location-based services with accurate location information updates while preserving location privacy. At the end of this dissertation, we also point out open research issues in securing vehicular networks",DigitalCommons@University of Nebraska - Lincoln,A Study of Security and Privacy in Vehicular Networks,,,,core
365068422,2020-12-22T08:00:00,"As governments and private companies alike race to achieve the vision of a smart city — where artificial intelligence (AI) technology is used to enable self-driving cars, cashier-less shopping experiences and connected home devices from thermostats to robot vacuum cleaners — advancements are being made in both software and hardware to enable increasingly real-time, accurate inference at the edge. One hardware solution adopted for this purpose is the LiDAR sensor, which utilizes infrared lasers to accurately detect and map its surroundings in 3D. On the software side, developers have turned to artificial neural networks to make predictions and recommendations with high accuracy. These neural networks have the potential, particularly run on purpose-built hardware such as GPUs and TPUs, to make inferences in near real-time, allowing the AI models to serve as a usable interface for real-world interactions with other AI-powered devices, or with human users. This paper aims to example the joint use of LiDAR sensors and AI to understand its importance in smart city environments",SJSU ScholarWorks,LiDAR Object Detection Utilizing Existing CNNs for Smart Cities,,,,core
389690391,2020-12-11T00:00:00,"Managing the city and the territories is a complex task requiring expertise in a broad range of fields. During the 21st century, the city and the territories must face major environmental and social challenges such as water scarcity, global warming, air quality and many others global issues. Physical modeling and numerical simulation are promising to give help decision tool to collectivities and to provide new services to citizens. In my research works, I have proposed adjoint numerical methods to limit the number of sensor to be deployed using optimal sensor placement strategy and inverse methods to get more representative numerical simulations describing urban physical phenomena. This transversal approach has been applied to various areas such as air and water quality, thermal building problem and structural health monitoring. Finally, experiments have been designed and conducted to test and to validate the proposed numerical methods using real measurements notably from the Equipment of Excellence Sense-City.La gestion de la ville et des territoires est une tâche complexe nécessitant de multiples compétences dans des domaines très variés. Au cours de ce XXIème siècle, la ville et les territoires doivent faire face à des enjeux majeurs sur le plan environnemental et sociétal : la rareté de l&apos;eau, le réchauffement climatique, la qualité de l&apos;air et bien d&apos;autres problématiques sont au coeur des préoccupations mondiales. La modélisation physique et la simulation numérique sont des outils prometteurs pour mieux gérer la ville et les territoires et fournir de nouveaux services aux citoyens. Dans mes travaux, j&apos;ai proposé des méthodes numériques adjointes de positionnement optimal de capteurs afin de déployer le minimum d&apos;instrumentation et des méthodes inverses afin d&apos;améliorer la qualité des simulations numériques décrivant les phénomènes physiques urbains. Cette approche transversale a été appliquée à de nombreux domaines comme la qualité de l&apos;air et de l&apos;eau, la thermique du bâtiment et la surveillance de la santé des structures. Enfin, des expérimentations ont été conçues et réalisées afin de tester et de valider les méthodes numériques proposées en exploitant des mesures réelles issues notamment de l&apos;équipement d&apos;excellence Sense-City",HAL CCSD,Méthodes numériques adjointes et inverses couplant mesures et modèles physiques au service des enjeux de la ville et des territoires,,,,core
357234808,2020-03-05T00:00:00,"Abstract * MiTAP (MITRE Text and Audio Processing) is a prototype system available for monitoring infectious disease outbreaks and other global events. MiTAP focuses on providing timely, multi-lingual, global information access to medical experts and individuals involved in humanitarian assistance and relief work. Multiple information sources in multiple languages are automatically captured, filtered, translated, summarized, and categorized by disease, region, information source, person, and organization. Critical information is automatically extracted and tagged to facilitate browsing, searching, and sorting. The system supports shared situational awareness through collaboration, allowing users to submit other articles for processing, annotate existing documents, post directly to the system, and flag messages for others to see. MiTAP currently stores over one million articles and processes an additional 2000 to 10,000 daily, delivering up-to-date information to dozens of regular users. Global Tracking of Infectious Disease Outbreaks and Emerging Biological Threats Over the years, greatly expanded trade and travel have increased the potential economic and political impacts of major disease outbreaks, given their ability to move rapidly across national borders. These diseases can affect people (West Nile virus, HIV, Ebola, Bovine Spongiform Encephalitis), animals (foot-and-mouth disease) and plants (citrus canker in Florida). More recently, the potential of biological terrorism has become a very real threat. On September 11 th , 2001, the Center for Disease Control alerted states and local public health agencies to monitor for any unusual disease patterns, including the effects of chemical and biological agents. In addition to possible disruption and loss of life, bioterrorism could foment political instability, given the panic that fast-moving plagues have historically engendered. Appropriate response to disease outbreaks and emerging threats depends on obtaining reliable and up-to-date Approved for Public Release: Distribution Unlimited. To be published in AI Magazine, special edition highlighting best work from  information, which often means monitoring many news sources, particularly local news sources, in many languages worldwide. Analysts cannot feasibly acquire, manage, and digest the vast amount of information available 24 hours a day, seven days a week. In addition, access to foreign language documents and the local news of other countries is generally limited. Even when foreign language news is available, it is usually no longer current by the time it is translated and reaches the hands of an analyst. This is a very real problem that raises an urgent need to develop automated support for global tracking of infectious disease outbreaks and emerging biological threats. The MiTAP (MITRE Text and Audio Processing) system was created to explore the integration of synergistic TIDES language processing technologies: Translation, Information Detection, Extraction, and Summarization. TIDES aims to revolutionize the way that information is obtained from human language by enabling people to find and interpret needed information quickly and effectively, regardless of language or medium. MiTAP is designed to provide the end user with timely, accurate, novel information and present it in a way that allows the analyst to spend more time on analysis and less time on finding, translating, distilling and presenting information. On September 11 th , 2001, the research prototype system became available to real users for real problems.  Text and Audio Processing for Bio-Security MiTAP focuses on providing timely, multi-lingual, global information access to analysts, medical experts and individuals involved in humanitarian assistance and relief work. Multiple information sources (epidemiological reports, newswire feeds, email, online news) in multiple languages (English, Chinese, French, German, Italian, Portuguese, Russian, and Spanish) are automatically captured, filtered, translated, summarized, and categorized into searchable newsgroups based on disease, region, information source, person, organization, and language. Critical information is automatically extracted and tagged to facilitate browsing, searching, and sorting. The system supports shared situational awareness through collaboration, allowing users to submit other articles for processing, annotate existing documents, and post directly to the system. A web-based search engine supports sourcespecific, full-text information retrieval. Additional &quot;views&quot; into the data facilitate analysis and can serve as alerts to events, such as disease outbreaks.  Information Processing Each normalized message is passed through a zoner that uses human-generated rules to identify the source, date, and other fields such as headline or title, article body, etc. The zoned messages are preprocessed to identify paragraph, sentence, and word boundaries as well as partof-speech tags. This preprocessing is carried out by the Alembic natural language analyzer  User Interface The final phase consists of the user interface and related processing. The processed messages are converted to HTML, with color-coded named entities, and routed to newsgroups hosted by a Network News Transport Protocol (NNTP) server, InterNetNews (INN 2001). See  One major advantage to using the NNTP server is that users can access the information using a standard mail/news browser such as Netscape Messenger or Outlook Express. There is no need to install custom software, and the instant sense of familiarity with the interface is crucial in gaining user acceptance -little to no training is required. Mail readers also provide additional functionality such as alerting to new messages on specified topics, flagging messages of significance, and access to local directories that can be used as a private workspace. Other newsgroups can be created as collaborative repositories for users to share collective information.  To supplement access to the data, messages are indexed using the Lucene information retrieval system (The Jakarta Project 2001), allowing users to do full text, sourcespecific Boolean queries over the entire set of messages. As the relevance of messages tends to be time dependent, we have implemented an optimized query mechanism to do faster searches over time intervals. MiTAP Development and Deployment The initial MiTAP system was put together over a 9-month period. Our goal was to build a prototype quickly to demonstrate the results of integrating multiple natural language processing (NLP) technologies. The longer-term strategy is to upgrade the components progressively as better performing modules become available and to migrate towards our developing architecture. For the initial implementation, we chose components based on availability as well as ease of integration and modification. This meant that we used components developed at MITRE (extraction, summarization) or developed with MITRE involvement (translation support), or commercial off-theshelf (COTS) components (translation engines, information retrieval, news server, news browser interface). In cases where no component was readily available, we developed a minimal capability for MiTAP, e.g., scripts for capture of news sources, or use of named entity extraction for headline generation and binning of messages into appropriate newsgroups. Since July 2000, we have been working to incorporate modules from other groups (e.g., Columbia&apos;s Newsblaster,  As part of the long-term efforts, we have been concurrently developing a framework known as Catalyst (Mardis and Burger 2001). Catalyst provides a common data model based on standoff annotation, efficient compressed data formats, distributed processing, and annotation indexing. Standoff annotation (see, for example,  Uses of AI Technology Artificial Intelligence (AI) technology and techniques pervade MiTAP to support its multi-faceted, multi-lingual and multi-functional requirements. From automated natural language processing techniques to information retrieval, the NLP modules utilize AI extensively. The techniques utilized fall predominantly into the data-driven camp of methods. Below we describe the components, roughly in their order of processing flow. The CyberTrans machine translation server employs a combination of AI techniques to optimize the performance of COTS machine translation (MT) systems. Since system developers have only the most basic insight into the MT systems, we will not describe related AI techniques in depth here, and interested readers are referred to  COTS MT systems are designed primarily for interactive use in situations where users have control over the language, formatting and well-formedness of the input text. In adapting CyberTrans for real users and real-world data, the necessity for supporting technologies was quickly apparent. Three of these are of particular interest: automated language identification, automated code set conversion, and automated spelling correction, particularly for the incorporation of diacritics. The resulting tools can be used individually and eventually as standalone modules, but are currently integrated into the CyberTrans processing flow. The first, most essential, part of automated processing of language data is to determine both the language and code set representation of the input text. While it would seem obvious that users know at least what the language of a given document is, this has proven not to be the case, particularly in non-Romanized languages such as Arabic or Chinese. In these situations, documents appear as unintelligible byte streams. In addition, some of the data sources contain documents in a mix of languages, so knowledge of the source does not necessarily determine the language. This is a classical categorization problem with a search a space of N*M where N is the number of languages to be recognized and M the number of code set representations. The categories are determined by a combination of n-graph measurements using the Acquaintance algorithm (Huffman 1996) with simple heuristics whittling down the search space. Once the code set has been determined, it is converted into a standard representation. This process is not without information loss, so spelling corrections are applied. The most straight-forward spelling correction involves the reinsertion of diacritical markers where they are missing. This is treated as a word-sense disambiguation problem (Yarowsky 1994) and relies on both language spelling rules and trained probabilities of word occurrences. Here, the solution is a hybrid system where hand-coded rules are enforced using statistical measures of likely word occurrences.  In addition, a specialized tagging operation occurs, that of temporal resolution. While dates such as 09 September 2000 are relatively unambiguous, many time references found in natural language are not, for instance last Tuesday. To get the time sequencing of events of multiple stories correct, it is necessary to resolve the possible wide range of time references accurately. In this case, the resolution algorithm also combines basic linguistic knowledge with rules learned from corpora  Similarly, place names are often only partially specified. For example, there are a great many places in South America named La Esperanza. We are currently developing a module to apply a mix of hand-written rules and machine learning to metadata and contextual clues drawn from a large corpus to disambiguate place names. This range of tagging procedures represents a strong shift in natural language processing research over the past fifteen years towards &quot;corpus-based&quot; methods. This work begins with the manual annotation of a corpus, a set of naturally occurring linguistic artifacts, by which some level of linguistic analysis (word segmentation, part-ofspeech, semantic referent, syntactic phrase, etc.) is associated with the relevant portion of text. The resulting data provides a rich basis for empirically-driven research and development, as well as formal evaluations of systems attempting to re-create this analysis automatically. The availability of such corpora have spurred a significant interest in machine learning and statistical methods in natural language processing research, of which those mentioned above are just a few. One of the benefits of the rule-sequence model adopted in MiTAP&apos;s Alembic component is its support for easily and effectively combining automatically derived heuristics with those developed manually. This was a key element in successfully modifying the Alembic NLP system for MiTAP in the absence of any significant annotated corpus",,MiTAP for Bio-Security: A Case Study Global Tracking of Infectious Disease Outbreaks and Emerging Biological Threats,,,,core
402918283,2020-11-21T00:25:20,"With smart city infrastructures growing, the Internet of Things (IoT) has been widely used in the intelligent transportation systems (ITS). The traditional adaptive traffic signal control method based on reinforcement learning (RL) has expanded from one intersection to multiple intersections. In this paper, we propose a multi-agent auto communication (MAAC) algorithm, which is an innovative adaptive global traffic light control method based on multi-agent reinforcement learning (MARL) and an auto communication protocol in edge computing architecture. The MAAC algorithm combines multi-agent auto communication protocol with MARL, allowing an agent to communicate the learned strategies with others for achieving global optimization in traffic signal control. In addition, we present a practicable edge computing architecture for industrial deployment on IoT, considering the limitations of the capabilities of network transmission bandwidth. We demonstrate that our algorithm outperforms other methods over 17% in experiments in a real traffic simulation environment",'MDPI AG',An Edge Based Multi-Agent Auto Communication Method for Traffic Light Control.,10.3390/s20154291,https://opus.lib.uts.edu.au/bitstream/10453/144204/2/An%20Edge%20Based%20Multi-Agent%20Auto%20Communication%20Method%20for%20Traffic%20Light%20Control.pdf,"[{'title': 'Sensors', 'identifiers': ['issn:1424-8220', '1424-8220']}]",core
346528589,2020-01-01T00:00:00,"For modern smart city with sustainable development we need to provide reasonable
level of safety and efficient management of the resources. Instant response to incidents and
abnormal situations will help to provide such high bars for city residents, which requires
deployment of application of intelligent information processing and data analytics into
infrastructure. Closed-circuit television (CCTV) is playing a key part in assurance of city
security - most of the modern large cities equip with powerful monitoring systems and
surveillance cameras. Video data covers most of the city and could be efficiently used to find
anomalies or trends. This hard task for non-stop video monitoring could be solved by modern
achievements in machine learning and computer vision techniques, which can automate the
process of video analysis and identify anomalies and incidents without human intervention. In
this paper, we used computer vision methods like object detection and tracking, as well as
neuron networks for classification and detection of anomalies on real time video. As a result of
this work we suggested the working approach for detection of vehicle/pedestrian violating
legal trajectory anomaly, which we tested on real-time video provided by surveillance cameras
of the city of Kazan",,Detection of traffic anomalies for a safety system of smart city,,https://core.ac.uk/download/346528589.pdf,,core
387278751,2020-11-15T00:00:00,"We propose a framework for deep ordinal regression, based on unimodal output
distribution and optimal transport loss. Despite being seemingly appropriate,
in many recent works the unimodality requirement is either absent, or
implemented using soft targets, which do not guarantee unimodal outputs at
inference. In addition, we argue that the standard maximum likelihood objective
is not suitable for ordinal regression problems, and that optimal transport is
better suited for this task, as it naturally captures the order of the classes.
Inspired by the well-known Proportional Odds model, we propose to modify its
design by using an architectural mechanism which guarantees that the model
output distribution will be unimodal. We empirically analyze the different
components of our propose approach and demonstrate their contribution to the
performance of the model. Experimental results on three real-world datasets
demonstrate that our proposed approach performs on par with several recently
proposed deep learning approaches for deep ordinal regression with unimodal
output probabilities, while having guarantee on the output unimodality. In
addition, we demonstrate that the level of prediction uncertainty of the model
correlates with its accuracy",,"Deep Ordinal Regression using Optimal Transport Loss and Unimodal Output
  Probabilities",,http://arxiv.org/abs/2011.07607,,core
334918146,2020-02-19T00:00:00,"Real-time traffic accident forecasting is increasingly important for public
safety and urban management (e.g., real-time safe route planning and emergency
response deployment). Previous works on accident forecasting are often
performed on hour levels, utilizing existed neural networks with static
region-wise correlations taken into account. However, it is still challenging
when the granularity of forecasting step improves as the highly dynamic nature
of road network and inherent rareness of accident records in one training
sample, which leads to biased results and zero-inflated issue. In this work, we
propose a novel framework RiskOracle, to improve the prediction granularity to
minute levels. Specifically, we first transform the zero-risk values in labels
to fit the training network. Then, we propose the Differential Time-varying
Graph neural network (DTGN) to capture the immediate changes of traffic status
and dynamic inter-subregion correlations. Furthermore, we adopt multi-task and
region selection schemes to highlight citywide most-likely accident subregions,
bridging the gap between biased risk values and sporadic accident distribution.
Extensive experiments on two real-world datasets demonstrate the effectiveness
and scalability of our RiskOracle framework.Comment: 8 pages, 4 figures. Conference paper accepted by AAAI 202",,"RiskOracle: A Minute-level Citywide Traffic Accident Forecasting
  Framework",,http://arxiv.org/abs/2003.00819,,core
357558089,2020-04-24T00:00:00,"Abstract The dominant interpretation of the function and role of the CIO is technology-related with business-ICT alignment as a core concept. We criticize this vision as a product of the dominant interpretive scheme and show how the logic of this scheme restricts the worldview of CIO&apos;s and researchers. To overcome these restrictions, we adopt an alternative interpretive scheme based on our twenty years of experience with collaboration with information managers. This scheme is essentially generative and synthetic. We define the function and role of the CIO as the orchestrator of the information-related activities of the organization. To be successful, the CIO should further maintain equilibrium between inspiring and innovating the organization and informing and architecturing it. Finally, we argue for the use of a new language that can involve all stakeholders in these activities to make them enthusiastic participants. Based on this new view, the CIO can emanate information leadership. Keywords: CIO, Equilibrist, Information Leadership, Information Management, Informing, Inspiring, Orchestrator, Strategic Alignment Maes and De Vries, Information Leadership: the CIO as Orchestrator and Equilibrist Twenty Ninth International Conference on Information Systems, Paris 2008 3 CIO Problems and Prospects 4 Twenty Ninth International Conference on Information Systems, Paris 2008    Information Leadership: the CIO as Orchestrator and Equilibrist The Dominant Interpretation of the CIO Ever since its emergence in the 1970s  To put these managerial issues into practice and to arrive at strategic alignment, the CIO is supposed to perform six roles  Maes and De Vries, Information Leadership: the CIO as Orchestrator and Equilibrist Twenty Ninth International Conference on Information Systems, Paris 2008 5 word, yet. Furthermore, how do we interpret the CIO&apos;s job in a world of outsourcing, in which what seems to be generally interpreted as his/her prime responsibility is outsourced to third parties on a large scale? Will CIO&apos;s become CHIPOs (CHief ICT Purchasing Officers) or will their careers be over definitively? And how do we proceed with the concept of alignment that over many years has been heavily criticized in the literature? Are CIO&apos;s, talking about alignment all day long, not just emphasizing and reinforcing differences? Do people that are categorized by IS people as &apos;the business&apos; wish to be aligned? Do they know what it means? Is alignment a manifestation of instrumental thinking (aligning ICT strategy to business strategy) or a manifestation of technological determinism and associated power games (ICT impacts on business and therefore business processes should be aligned with The dominant interpretation in the literature is not without problems and if it is an adequate representation of the CIO&apos;s practice, this function seems to be in deep trouble. What we intent to offer in this paper is an alternative interpretive scheme of what the job of CIO is all about. We will argue that CIO&apos;s are essentially orchestrators and equilibrists. We will work out what they need to orchestrate and between what they are balancing and we will explore an alternative language to shape orchestration and balancing. We will present this in section four of this paper. Our arguments for such an alternative interpretive scheme are a result from our working method and our critique on the dominant interpretive scheme. We present our working method first in the next section because the critique on the dominant interpretive scheme and indeed the whole concept of interpretive scheme is part of our working method. Our critique on the dominant interpretive scheme is set out in section three. This critique is fundamental in the sense that we strongly believe that the dominant interpretation of the CIO&apos;s work causes many troubles experienced in practice and precludes both practitioners and researchers from exploring new avenues. Our Working Method Our way of working is interpretive in nature and is therefore quite contrary to the general positivist epistemology in the IS field  Interpretists believe that the process of social construction could lead to temporal collective interpretations. It is our interpretation that the dominant interpretation in the literature cited above is that the CIO is responsible for IS/ICT related management issues (and in many cases for the IS/ICT department of the organization) and that business-ICT alignment should be strived for to be effective as a business partner. This collective interpretation works as an axiom. It has been taken for granted right from the start. This is despite the fact that Synott (1987), in one of the first definitions of the CIO, did not define the CIO explicitly in technological terms, he saw the CIO as &quot;a senior executive responsible for establishing corporate information policy, standards, and management control over   CIO Problems and Prospects The problem with this dominant scheme is that the scheme gets reinforced by itself all the time. Both practitioners and academics work from the same axioms. They have learned the dominant scheme throughout their education and have been indoctrinated with it. This reinforcement effect is especially strong in the case of dominant perspectives on leadership roles. It is well known from the management literature that top managers have themselves become the system in which they have grown to their positions. Therefore, they don&apos;t have many incentives to change that system. Moreover their roles are not only shaped by themselves but by the expectations of their surroundings as well. If there is truth in the understanding of social psychologists that our social environment shapes our identity, it would mean that the identity of CIO&apos;s is shaped by the interpretive schemes of the people surrounding them, like suppliers, consultants, employees, fellow CIO&apos;s and academics. As they all share the dominant scheme and express their mutual expectations in the dominant language, they will reinforce that scheme. Of course CIO&apos;s will meet many people from other disciplines who don&apos;t share this dominant scheme but also that has been taken care of in the dominant interpretive scheme…indeed, by the concept of business-ICT alignment. The argument we make is a structuration argument taken from Giddens&apos; structuration theory (Giddens 1976). Giddens holds that individual human agency and abstract social structure (traditions, institutions, codes, established ways of doing, language, etc) are related to each other. Social structure is produced and reproduced by individual agents repeating acts. These acts are repeated because of the social structure in the sense that the social structure has formed individual&apos;s beliefs and language and that actions which are deviant from the social structure are reacted on by other people within the same social structure with disbelief and sometimes even anger. Social structure thus serves as a save haven. This recursive relationship between human agency and social structure is called the duality of structure. Crucial in Giddens&apos; theory is that that same relationship means that social structures can be changed. To do so people need to get aware of the dominant social pattern and need to replace it or reproduce it differently. Interpretive schemes are central in such change. Interpretive schemes are the means through which meaning is communicated in human interaction  CIO Problems and Prospects Our alternative interpretive scheme has been formed in a social structure which only has been moderately influenced by the traditional dominant interpretive scheme, which has been influenced by interpretations schemes from other disciplines as well and in which we have been actively involved for over twenty years. As knowledge is path dependent, our knowledge results from the many interpretations of events, trends and interactions that we experienced throughout our professional journey. Where did our journey start? It started twenty years ago with a one year educational program for executives who were responsible for ICT related issues in businesses or governmental organizations. By that time the word CIO had just been introduced, academics and practitioners alike were debating its positioning and role taking and expectations were high. The first jokes on careers had to be made yet and the concept of alignment had not yet been introduced. The first papers on the strategic impact of information systems began to see the light and James Martin amongst others just had introduced the concept of strategic IS planning. Our program started from the notion of strategic IS planning, strategic impact and the idea that within a couple of years large Dutch companies would need someone in some sort of CIO position. We believed that such a person would need knowledge on strategy, organizational design and behavior and ICT. This educational program was targeted towards people in IT departments and business people. The program was called Information Management to emphasize that it was more than an IS program. The start of the program was quite participatory. We defined problems in the field together with our participants and sought for solutions and experts together with them. This way we built a network of lecturers right from the start. They were a mix of academics with insight in practitioners&apos; problems and reflective practitioners. Retrospectively, this could be denoted as our root definition  The appreciation of generative synthesis by practitioners asks for rather holistic frameworks (like the one in  It is this way of working that we have in mind in this paper. Because we have to deal with a quite pronounced dominant scheme of interpretation of the CIO&apos;s working field and role taking which gets expressed in a seemingly hard to change language that reinforces the dominant scheme, we need to be somewhat provocative in our approach. As we have to deal with a multi-disciplinary field of working in practice and the nomenclature of our own field reinforces the dominant scheme, we &apos;borrow&apos; meaning and language from other fields like architecture, design, organizational behavior, etc. We like to emphasize that we do not mean ICT-architecture, systems design or behavioral issues like resistance to systems because we hardly believe that that has anything to do with what non-IS people (we mean the rest of the world) think of when they talk about informational issues. As we, at the same time, have to deal with interpretative schemes of the work of practitioners, our approach has been practitioner oriented and participatory in the first place for over twenty years and we remain to do so. We are fully aware of the fact that dominant schemes have their origins in education which is in many cases based on academic research. However, as has been put forward earlier, we do not expect traditional empirical research to bring us closer to alternative schemes. This stance is in line with the one adopted by Ciborra who heavily criticizes detached academic practice for the benefit of research &quot;anchored to the unfolding of the human process of encountering the everyday world&quot; (Ciborra 2002: 6). Moreover, the academic field itself has expressed that most new issues in the IS field has been brought in by practitioners and consultants  Nevertheless, we acknowledge that the IS field is one of the primary reference fields that we work from. Therefore, we share our concerns, approach and alternative interpretations on conferences like the International Conference on Information Systems and hope to be of inspiration.  Problems with the Dominant Interpretive Scheme Denial of the Concept of Information Our first and foremost objection against the prevailing conceptualization of the CIO is the almost complete absence of the concept of information. The usage of notions such as business-ICT alignment, ICT strategy, architecture and sourcing has led to the idea that the management of ICT has become almost synonymous with the management of information. &quot;In short, the &apos;T&apos; of IT has become the focus of attention rather than the &apos;I&apos;.&quot;  Now that ICT is increasingly outsourced, the lack of information related notions and the over-emphasis on technological notions become apparent. The resulting situation is a challenging one. Now that technology no longer distracts our attention from the real issue, we can return to that issue and consider the information itself as an important variable for analysis (Glazer 1991). We can shift our attention from the means to the ends. We should however be careful in this endeavor. The heritage of the past two to three decades of technology oriented thinking has left us with a &apos;cabinet full of mechanical information notions&apos; like for instance information logistical notions, such as the gathering, storage, refinement, and distribution of information or concepts like information users and producers, information requirements or senders and receivers. The problem with this cabinet is that these notions are still technology related and systems oriented. Another way of thinking about information that will be rather tempting in an environment of ICT outsourcing would be market related notions like information demand and supply. However, such market terminology commoditizes the concept of information, such that it degenerates into a marketable product (Huizing 2007a) and will distract our attention to the medium of commodity distribution (the technology) once again. With such a set of notions, CIO&apos;s would be caught in a vicious circle from technology trap to commodity trap and back again. With such set of notions, they will only &apos;shed&apos; their image of technologists, as some ICT representatives appear to do (Booth and Philip 2005). Another set of notions, which has always been around in the working field of the CIO, is that of accounting, accounting information systems and financial information systems. It is obvious that the information related concepts in this field belong to the core competencies of another profession and that these concepts address only (a biased) part of the information related issues. To state it in a provocative way: information management is broader than management information. Adhering to these notions further puts the CIO back in the position of the responsible for IT. What we need is a genuine interest to develop management notions around the function of information in social  CIO Problems and Prospects Twenty Ninth International Conference on Information Systems, Paris 2008 These notions should be on processes of communication, interaction and learning, on how people understand and attach meaning in social processes. We need to acknowledge fully that at this moment in time only a fraction of information (and probably only the fraction of commoditizable data) can be caught in systems and that a systems orientation restricts our view on how information is dealt with by people in their social (inter)action. Understanding how people deal with information is not a means to arrive at systems but systems are a means to support people&apos;s dealing with information. We should keep in mind that there are many information and knowledge related processes that cannot be designed, but should be designed for (Wenger 1998). A set of information notions like these complements competencies on technological commodities, which are rather easy to duplicate, with socio-organizational competencies. Resource configurations consisting of a profoundly intertwined combination of socio-organizational competencies and technological systems which are infused with social values during development, implementation and usage and as a consequence bear the values of the social organization, could become complex, tacit, specific and opaque and are therefore hard to duplicate (Barney 1999). These are of high value to the organization. CIO&apos;s that orchestrate the development of such configurations from their information-related and technological systems related notions do not only support their organizations but have become part of such valuable socio-organizational configuration. As long as we conceptualize the CIO as a senior manager, responsible for IS/ICT related issues, our attention and that of those we educate will be distracted by the technology, which becomes increasingly commoditized. Furthermore, we would be tempted to further develop a language full of systems and technology-related concepts. A language that is probably highly creative but ineffective in our communication with others. For those who need examples, think about virtual reality, artificial intelligence, ambient mobility, enterprise resource planning or customer relationship management. The list could be endless. The danger of such language does not lie in the fact that others do not understand it (it could be explained), but in the fact that others understand it very well. Those who understand terms like customer relationship management or enterprise resource planning very well recognize the mechanical nature of the language and view this as a reflection of the way of thinking of the speaker. The identification of the CIO with technology related issues could lead to an endless cycle. Those who hold themselves primarily responsible for technological issues will be regarded that way and as a consequence will be asked questions about it. As they feel responsible for these technological issues, they need to speak technology-related language in response to these questions. This reconfirms their technological image. Even when the conversation is not about technology as such but about information, the conversation will be in systems-related language, thus restricting the conversation. The dominant interpretive scheme is both cause and effect. The only way out is to free ourselves from such language through the development of notions that have to do with the ends (information) instead of the means (technology) and which does not reconfirm a mechanical image. Hence, we repeat our statement: the CIO is not a CIO….yet.  The Myth of Alignment Romanticism Our second objection against the dominant interpretive scheme of the CIO targets the concept of strategic alignment. In this notion, the CIO is seen as a sort of liaison officer between business and ICT, in academic terms between the management discipline and information systems, if not computer science. The image of the business-ICT relationship has been that of &quot;a troubled marriage in need of guidance&quot; (Ward and Peppard 1996). Maes  CIO Problems and Prospects Twenty Ninth International Conference on Information Systems, Paris 2008 Taken from a linguistic perspective one could say that openly speaking of alignment and the strive for it, resembles speaking openly about the strive for a good marriage. Both emphasize what is missing and give the other party a feeling of impotence. The adding of the predicate &apos;strategic&apos; does not ease those feelings because it stresses importance. Obviously, expressing oneself in dichotomies confirms differences (Barad 2003). The concept of strategic business-ICT alignment is highly romantic in nature altogether. Everything to strive for seems beautiful, appeals to general cultural values and above all might solve a problem that at least is experienced by one of the parties. However, the road leading to it is unclear, as are the benefits once reaching it. As we encounter many people along the road which seem not be aligned yet, our search for this Holy Grail is harmful in the sense that we confirm socio-organizational differences and shape distances. In those cases where strategic alignment manifests itself as a traditional top down strategic planning exercise, the concept reinforces the mechanical image of CIO&apos;s. We conclude again that the dominant interpretive scheme is both cause and effect. We end our discussion on alignment with the statement that the CIO is not to become a partner in business, but a part of business. In a world of ubiquitous outsourcing, the romantic notion of business-ICT alignment boils down into a contractual relationship and the CIO becomes part of business or otherwise will end up as CHIPO. We repeat that the challenge is to be a genuine information officer, a CIO that orchestrates complex resource configurations of socio-organizational competencies and technological systems that are highly valued by the organization and as such be part of this valuable socio-organizational configuration itself. An Alternative Interpretive Scheme: the CIO as Orchestrator and as Equilibrist The CIO as Orchestrator Maes  Therefore, it makes the connecting factors information/communication and structure explicit as central components of information management, but it deals above all summarily with the suppositions of strategic alignment: (1) the elements of the framework represent essential, integral components of information management, which cannot be understood isolated from each other; in the terms of Barad  indicates that these components always need to be considered within their correlation. In this vision, the CIO is the orchestrator, the person that orchestrates the information-related activities of an organization (or part thereof, or across organizational borders) represented in the framework. In this, the components embodied by the middle axles of the framework are his/her main sets of tools. We further elaborate on these central axles, a more comprehensive treatment of the domains of the framework can be found in  With the I/C column in the fra",,Microsoft Word - 2008.07.01.doc,,,,core
237171923,2020-01-01T00:00:00,"Overview The fundamental motivation of this book is to contribute to the future advancement of Asset Management in the context of industrial plants and infrastructures. The book aims to foster a future perspective that takes advantage of value-based and intelligent asset management in order to make a step forward with respect to the evolution observed nowadays. Indeed, the current understanding of asset management is primarily supported by well-known standards. Nonetheless, asset management is still a young discipline and the knowledge developed by industry and academia is not set in stone yet. Furthermore, current trends ¬in new organizational concepts and technologies lead to an evolutionary path in the field. Therefore, this book aims to discuss this evolutionary path, starting first of all from the consolidated theory, then moving forward to discuss: • The strategic understanding of value-based asset management in a company; • An operational definition of value, as a concept on the background of value-based asset management; • The identification of intelligent asset management, with the aim to frame a set of “tools” recommended to support the asset-related decision-making process over the asset lifecycle. The book compiles information gathered from interesting research and innovation efforts in projects that were relevant to this scope, especially considering the evidences from state of the art and current research trends of Physical Asset Management (PAM) and Operations &amp; Maintenance (O&amp;M) of industrial plants and infrastructures. Among the new trends, digitalization is enabling new capabilities for asset management, by means of the appearance of Cyber Physical Systems (CPS), and the subsequent issues resulting from building the digital twins of the physical assets. This may lead to a new era of intelligent asset management systems. At the same time, basic principles of asset management will continue to be relevant in the new era, helping to guide the development of digitalization programs in assets intensive companies, and being transformed along the evolutionary path towards the achievement of a more digitized and intelligent management. Relevant Topics One of the main challenges in the field of physical asset management is to enhance the identification and quantification of cost and value to evaluate the total cost and value of industrial assets throughout their lifecycle. These concepts have been widely discussed in literature, by offering different perspectives and also using plenty of terms partially overlapping or providing slightly different interpretations. Terms such as TCO (Total Cost of Ownership), LCC (Life Cycle Cost), WLC (Whole Life Cost), COO (Cost of Ownership) and, if extending to values, TVO (Total Value of Ownership) and Whole Life Value (WLV), are widely cited. If one surfs the Internet, a myriad of definitions and references can be found. This does not mean that the terms are well understood and widely adopted in practice. Considering the industrial applications of TCO and TVO, it is worth remarking that their benefits are clearly envisioned (e.g., the benefits of TCO can be considered cost control support, management strategy selection, quality optimization, and best cost-effectiveness management). However, in practice, some missing links can be pointed out with regard to their use: even though the need and desire to implement life cycle costing is very much talked about, there are a number of difficulties that limit a widespread adoption by industry. This is even more challenging when extending to value and, thus, to the whole life value, which is a more recent concept. Another relevant challenge addressed by physical asset management, is the assurance of the cost and value along the asset lifecycle. Henceforth, appropriate “tools” are required in order to assure that the value delivery from industrial assets (at reasonable cost) is effectively achieved and, when not, that proper decisions are activated with the aim to guarantee value delivery. In particular, proper “tools” should be used when planning in advance, and when monitoring and controlling the effective outcomes, to eventually activate re-planning in case of extant discrepancies with respect to expectations, thus leading to a continuous improvement of what is decided over the asset life cycle. Identification and quantification of value delivered by the assets is essential in all the cases. Structure of the Book The book is divided into four Parts. In Part 1, the first Chapter introduces fundamental concepts used in this book and presents a generalized framework providing relevant dimensions of value-based and intelligent asset management. The rest of the chapters in this Part offer a long-term perspective of asset management, dealing with topics like societal impact of investments in infrastructure assets, performance and economic impacts of investments in manufacturing plants, and long-term deterioration and renewal of assets. In Part 2 the value-based decision-making approach is stressed as an overall perspective for management of the assets over their life cycle, and also exemplified in real world specific cases. The concept of value, understood as presented in the first Chapter of this book, is operationalized to drive day to day management decisions and activities. Part 3 is dedicated to different advanced developments at the operational level. Different tools are presented to predict and/or to determine properly assets conditions leading to the release and execution of the maintenance activities. Predictive analytics are used to make predictions about assets future behavior. Many techniques from data mining, statistics, modeling, machine learning, and artificial intelligence can be applied to analyze current data to make predictions about future. The scalability of these emerging models, in this new scenario of individualised asset prognostics, is another topic discussed in this part of the book, trying to find a compromise between accuracy and computational power of these tools. Part 4 is devoted to new emerging processes, and new ideas that can be implemented by exploiting the power of new technologies such as cyber-physical systems that can certainly embed more intelligence and orientation to value in existing asset management systems. European Project and Worldwide Collaboration This book results from a collaboration of the authors, strengthened within the context of SustainOwner, ‘‘Sustainable Design and Management of Industrial Assets through Total Value and Cost of Ownership’’, a project sponsored by the EU Framework Programme Horizon 2020 and based on a knowledge sharing scheme involving many universities worldwide, from the Americas, Asia and Africa. Chapters Including Previously Published Research Results This book compiles a set of Chapters that were previously published as journal papers by the research groups involved in the Sustain Owner Project. The Editors would like to idenfify the correspondence between each book Chapter and the original research paper. According to Springer policy, the publishers were asked to provide their permissions for this work to be presented in its current form. The Editors thank the publishers for their cooperation making this book possible. The referred Chapters are: - Chapter 2: Heaton, J., Parlikad, A.K., “A conceptual framework for the alignment of infrastructure assets to citizen requirements within a smart cities framework,” Cities, Volume 90, pp 32-41, 2019. - Chapter 3: Roda I., Garetti M., “Application of a Performance-driven Total Cost of Ownership (TCO) Evaluation Model for Physical Asset Management”. In: Amadi-Echendu J., Hoohlo C., Mathew J. (eds) 9th WCEAM Research Papers. Lecture Notes in Mechanical Engineering. Springer, Cham, 2015, © Springer International Publishing Switzerland 2015, DOI 10.1007/978-3-319-15536-4. - Chapter 5: Roda, I., and M Macchi. “A framework to embed Asset Management in production companies.” Proceedings of the Institution of Mechanical Engineers, Part O: Journal of Risk and Reliability 232, no. 4: 368-378, 2018, © IMechE 2018, DOI: 10.1177/1748006X17753501. - Chapter 6: Srinivasan, R., Parlikad, A.K., “An approach to value-based infrastructure asset management,” Infrastructure Asset Management, Volume 4, Issue 3, pp 87-95, 2017. - Chapter 9: Olivencia Polo F.A , Ferrero Bermejo J. Gómez Fernández JF., Crespo Márquez A..,”Failure mode prediction and energy forecasting of PV plants to assist dynamic maintenance tasks by ANN based models”. Renewable Energy, Volume 81, pp 227-238. 2015. - Chapter 10: Liu, B., Liang, Z., Parlikad, A.K., Xie, M., Kuo, W., “Condition-based maintenance for systems with aging and cumulative damage based on proportional hazards model,” Reliability Engineering &amp; System Safety, Volume 168, pp 200-209, 2017. - Chapter 11: C. Colace, L. Fumagalli, S. Pala, M. Macchi, N. R. Matarazzo, M. Rondi., “Implementation of a condition monitoring system on an electric arc furnace through a risk-based methodology.” Proceedings of the Institution of Mechanical Engineers, Part O: Journal of Risk and Reliability, Volume 229, Issue 4, August 2015, 327-342, 2015, © IMechE 2015, DOI: 10.1177/1748006X15576441. - Chapter 12: Erguido A., Crespo Márquez A.. Castellano E., Gómez Fernández JF.,”A dynamic opportunistic maintenance model to maximize energy- based availability while reducing the life cycle cost of wind farms”. Renewable Energy, Volume 114, pp 843-856. 2017. - Chapter 13: Negri E., L. Fumagalli, M. Macchi, “A Review of the Roles of Digital Twin in CPS-based Production Systems”, in Proceedings 27th International Conference on Flexible Automation and Intelligent Manufacturing, FAIM2017, Volume 11, 939-948, 27-30 June 2017, Modena, Italy, (Eds.) Marcello Pellicciari, Margherita Peruzzini, 2017, 2351-9789, © 2017 The Authors. Published by Elsevier B.V., doi: 10.1016/j.promfg.2017.07.198. - Chapter 14: Li, H., Salvador-Palau, A., Parlikad, A.K., “A Social Network of Collaborating Industrial Assets,” Proceedings of the IMechE Part O: Journal of Risk &amp; Reliability, Volume 232, Issue 4, pp. 389-400, 2018, © IMechE 2018, DOI: 10.1177/1748006X18754975. - Chapter 15: Salvador-Palau, A., Liang, Z., Lutgehetmann, D., Parlikad, A.K., “Collaborative Prognostics in Social Asset Networks,” Future Generation Computer Systems, Volume 92, pp 987-995, 2019. - Chapter 16: Chekurov S, Metsä-Kortelainen S, Salmi M, Roda I, Jussila A., “The perceived value of additively manufactured digital spare parts in industry: an empirical investigation”. International Journal of Production Economics, 2015, 87-97, 2018, 0925-5273 © 2018 The Authors. Published by Elsevier B.V. T., DOI: 10.1016/j.ijpe.2018.09.008. Adolfo Crespo Márquez Marco Macchi Ajith Kumar Parlika",'Springer Science and Business Media LLC',Value Based and Intelligent Asset Management. Mastering the Asset Management Transformation in Industrial Plants and Infrastructures,10.1007/978-3-030-20704-5,,,core
328810527,2020-08-02T00:00:00,"The data-driven approach to sustainable urban development is becoming increasingly popular among the cities across the world. This is due to cities' attention in supporting smart and sustainable urbanism practices. In an era of digitalization of urban services and processes, which is upon us, platform urbanism is becoming a fundamental tool to support smart urban governance, and helping in the formation of a new version of cities-i.e., City 4.0. This new version utilizes urban dashboards and platforms in its operations and management tasks of its complex urban metabolism. These intelligent systems help in maintaining the robustness of our cities, integrating various sensors (e.g., internet-of-things) and big data analysis technologies (e.g., artificial intelligence) with the aim of optimizing urban infrastructures and services (e.g., water, waste, energy), and turning the urban system into a smart one. The study generates insights from the sensor city best practices by placing some of renowned projects, implemented by Huawei, Cisco, Google, Ericsson, Microsoft, and Alibaba, under the microscope. The investigation findings reveal that the sensor city approach: (a) Has the potential to increase the smartness and sustainability level of cities; (b) Manages to engage citizens and companies in the process of planning, monitoring and analyzing urban processes; (c) Raises awareness on the local environmental, social and economic issues, and; (d) Provides a novel city blueprint for urban administrators, managers and planners. Nonetheless, the use of advanced technologies-e.g., real-time monitoring stations, cloud computing, surveillance cameras-poses a multitude of challenges related to: (a) Quality of the data used; (b) Level of protection of traditional and cybernetic urban security; (c) Necessary integration between the various urban infrastructure, and; (d) Ability to transform feedback from stakeholders into innovative urban policies.</p",'MDPI AG',Understanding sensor cities:Insights from technology giant company driven smart urbanism practices,10.3390/s20164391,,,core
237215238,2020-01-01T00:00:00,"Parking vehicle is a daunting task and a common problem in many cities around the globe. The search for parking space leads to congestion, frustration and increased air pollution. Information of a vacant parking space would facilitate to reduce congestion and subsequent air pollution. Therefore, aim of the paper is to acquire vehicle occupancy in an open parking lot using deep learning. Thermal camera was used to collect the data during varying environmental conditions such as; sunny, dusk, dawn, dark and snowy conditions. Vehicle detection with deep learning was implemented where image classification and object localization were performed for multi object detection. The dataset consists of 527 images which were manually labelled as there were no pre-labelled thermal images available. Multiple deep learning networks such as Yolo, ReNet18, ResNet50 and GoogleNet with varying layers and architectures were evaluated on vehicle detection. Yolo, GoogleNet and ResNet18 are computationally efficient detectors which took less processing time while Resnet50 produced better detection results compared to other detectors. However, ResNet18 also produced minimal miss rates and is suitable for real time vehicle detection. The detected results were compared with a template of parking spaces and IoU value is used to identify vehicle occupancy information",'Institution of Engineering and Technology (IET)',Deep learning-based vehicle occupancy detection in an open parking lot using thermal camera,10.1049/iet-its.2019.0468,,,core
387846376,2020-10-25T00:00:00,"Monitoring of complex processes faces several challenges mainly due to the lack of relevant sensory information or

insufficient elaborated decision-making strategies. These challenges motivate researchers to adopt complex data processing and

analysis in order to improve the process representation. This paper presents the development and implementation of quality

monitoring framework based on a model-driven approach using embedded artificial intelligence strategies. In this work, the

strategies are applied to the supervision of a microfabrication process aiming at showing the great performance of the framework

in a very complex system in the manufacturing sector. The procedure involves two methods for modelling a representative quality

variable, such as surface roughness. Firstly, the Hybrid Incremental Modelling strategy is applied. Secondly, a Generalized Fuzzy

Clustering C-Means method is developed. Finally, a comparative study of the behavior of the two models for predicting a quality

indicator, represented by surface roughness of manufactured components, is presented for specific manufacturing process. The

manufactured part used in this study is a critical structural aerospace component. In addition, the validation and testing is

performed at laboratory and industrial levels, demonstrating proper real-time operation for non-linear processes with relatively

fast dynamics. The results of this study are very promising in terms of computational efficiency and transfer of knowledge to

manufacturing industryThis work was partially supported by the project

Power2Power: Providing next-generation silicon-based

power solutions in transport and machinery for significant

decarbonisation in the next decade, funded by the Electronic

Component Systems for European Leadership (ECSEL-JU)

Joint Undertaking and the Spanish Ministry of Science,

Innovation and Universities (MICINN), under grant

agreement No 826417. In addition, this work was also funded

by the Polish National Agency for Academic Exchange

(NAWA) through the project: “Industry 4.0 in Production and

Aeronautical Engineering (IPAE)”.Peer reviewe",'Techno-Press',Quality Monitoring of Complex Manufacturing Systems on the basis of Model Driven Approach,10.12989/sss.2020.26.4.495,,"[{'title': 'Smart Structures and Systems', 'identifiers': ['1738-1584', '1738-1991', 'issn:1738-1584', 'issn:1738-1991']}]",core
322979547,2020-05-16T00:00:00,"With the recent booming of artificial intelligence (AI), particularly deep
learning techniques, digital healthcare is one of the prevalent areas that
could gain benefits from AI-enabled functionality. This research presents a
novel AI-enabled Internet of Things (IoT) device operating from the ESP-8266
platform capable of assisting those who suffer from impairment of hearing or
deafness to communicate with others in conversations. In the proposed solution,
a server application is created that leverages Google's online speech
recognition service to convert the received conversations into texts, then
deployed to a micro-display attached to the glasses to display the conversation
contents to deaf people, to enable and assist conversation as normal with the
general population. Furthermore, in order to raise alert of traffic or
dangerous scenarios, an 'urban-emergency' classifier is developed using a deep
learning model, Inception-v4, with transfer learning to detect/recognize
alerting/alarming sounds, such as a horn sound or a fire alarm, with texts
generated to alert the prospective user. The training of Inception-v4 was
carried out on a consumer desktop PC and then implemented into the AI based IoT
application. The empirical results indicate that the developed prototype system
achieves an accuracy rate of 92% for sound recognition and classification with
real-time performance",,"A Deep Learning based Wearable Healthcare IoT Device for AI-enabled
  Hearing Assistance Automation",,http://arxiv.org/abs/2005.08076,,core
324170040,2020-01-01T00:00:00,"Ageing consists one of the biggest societal challenges worldwide. Older adults’ desire of staying healthy, staying at their own home consists an important motive for the development of assistive technologies that will realize it through remote monitoring technologies. Proliferation of low-cost, off-the-shelf IoT devices has led to the implementation of the so called smart homes projects, which are solutions that are mixing the older adults’ physical spaces with monitoring and computing capabilities, allowing the collection of high-frequency daily life data. Methods: This stydy consist of two parts. An experimental one that takes place in a lab space and a real life application of sensor monitoring technology. In particular, an ecologically valid space was created within the Thessaloniki Active & Healthy Ageing Living, where fifteen (15) older adults participated in the pilot testing of the technology. Participants visited the ecologically valid space for almost two weeks, covering eight (8) 1 hour-sessions in total. There, they followed a protocol of typical daily life activities, where several unobtrusive scenario of monitoring were interweaved. No restrictions were imposed with respect to the execution of tasks, thus making sure that the data collection follows an ecologically valid paradigm. Subsequently, senor data collected from 10 participants were statistically correlated to clinical assessment tools. Then, a series of focus groups openly discussing about technology and obtrusiveness were carried out with thirteen participants. Moving to the second part of the study, some adjustments with respect to the technology had to be considered according to the Living Lab study findings. Favorable technology was installed to 5 older womens’ homes for more than a year time resulting to hundreds of thousands data points to be collected. Different modeling techniques were employed, such as statistical modeling, and machine learning, in order to explore clinical added value of digital biomarkers. As an application scenario, the study of modelling emotional disturbances and in particular the one of geriatric depression was carried out. In addition, a longitudinal study about sensor monitoring unobtrusiveness was conducted to examine end-user acceptance. A follow-up interview was carried out with each one of the participants. Finally, we explore the possibility of mapping the results of the data-driven modelling approaches with expert-driven models and knowledge representation schemata, such as the Fuzzy Cognitive Maps (FCMs) in a way that is transparent to the clinicians. Results: With respect to the first part of the study quite a few statistical significant correlations between sensor data and clinical tools were found, e.g. mobility and affective characteristics linked to emotional health and quality of life. With respect to the results of the focus groups these were transcribed and analysed with qualitative methods in order to extract the most significant themes and to categorize them to one of the obtrusiveness framework axes. For the second part o fthe study, a generalized linear mixed prediction model of PHQ-9 was developed utilizing information about TV usage patterns. Random Forests achieved mean accuracy score >80% when given to classify between healthy and depressive cases, while another RF classifier achieved an AUC>90% when having to deal with all sorts of depressive synptoms severity (from mild to severe). The initial FCM model also achieved a high classification rate up to 96% given some synthetic cases provided by experts. Finally, older adults’ longitudinal attitudes revealed a negative stance towards the use of the mirror camera and the smart watch, as well as the Kinect device. The answers to the questionnaire of the lady that left the study at month 12, were compared against the mean value of the rest four particpants to check for any particular reasons of her decision. Significant differences from the mean value of the rest four were found for the Kinect device, the smart watch and the mirror camera. Conclusions: Clinical value of digital biomarkers has been been revealed in many publications, yet their application in longitudinal studies is absent. A great challenge nowadays remains the robust operation of such technologies under real life circumstances, without any restrictions (in the wild) as well as their acceptance from older people, and their doctors, if at any time in the future we wish to integrate such data in the clinical practice.Η γήρανση του πληθυσμού αποτελεί μία από τις μεγαλύτερες κοινωνικές προκλήσεις σε ολόκληρο τον πλανήτη. Η επιθυμία των ηλικιωμένων για καλή γήρανση, παραμένοντας στο σπίτι τους αποτελεί σημαντικό παράγοντα για την ανάπτυξη υποστηρικτικών τεχνολογιών που θα προσφέρουν αυτήν την δυνατότητα μέσω τεχνολογιών απομακρυσμένης παρακολούθησης. Η εξάπλωση φθηνών έξυπνων συσκευών άμεσα διαθέσιμων στο εμπόριο και με την δυνατότητα σύνδεσης στο λεγόμενο Διαδίκτυο των Πραγμάτων (Internet of Things - IoT) έχει στρέψει την ερευνητική κοινότητα στην ανάπτυξη των λεγόμενων έξυπνων σπιτιών. Τα έξυπνα σπίτια αποτελούν λύσεις που συνδυάζουν τεχνολογίες αισθητήρων ενσωματώνοντάς τες στον φυσικό περιβάλλοντα χώρο των σπιτιών των ηλικιωμένων, επιτρέποντας την συνεχή και λεπτομερή παρακολούθηση των καθημερινών δραστηριότητων τους. Μέθοδοι: Η μελέτη μας αποτελείται από δύο σκέλη. Ένα πειραματικό σε εργαστηριακό χώρο και μια εφαρμογή της τεχνολογίας σε πραγματικό περιβάλλον. Συγκεκριμένα, δημιουργήθηκε ένας οικολογικά έγκυρος χώρος στα πλαίσια του Ζωντανού Εργαστηρίου Ενεργού και Υγιούς Γήρανσης, όπου δέκα πέντε (15) ηλικιωμένοι συμμετέχοντες έλαβαν μέρος στην πιλοτική δοκιμή της τεχνολογίας. Οι συμμετέχοντες επισκέπτονταν τον οικολογικά έγκυρο χώρο για περίπου 2 εβδομάδες (8 συνολικά συνεδρίες) όπου ακολουθούσαν ένα πρωτόκολλο δραστηριοτήτων, που περιελάμβαναν ορισμένα διακριτικά σενάρια παρακολούθησης, χωρίς αυστηρούς περιορισμούς επιτρέποντας την συλλογή ρεαλιστικών συμπεριφορικών δεδομένων από το δίκτυο αισθητήρων. Στην συνέχεια τα δεδομένα που συλλλέχθηκαν από 10 συμμετέχοντες αναλύθηκαν στατιστικά με τα κλινικά τεστ αξιολόγησης της υγείας των ηλικιωμένων ώστε να βρεθούν τυχόν συσχετίσεις και δείκτες υγείας. Μετά το πέρας αυτών των πιλοτικών δοκιμών πραγματοποιήθηκε μια σειρά από ομάδες εστιασμένης συζήτησης όπου πραγματοποιήθηκε κουβέντα γύρω από την παρεμβατικότητα της τεχνολογίας με την υιοθέτηση ενός θεωρητικού πλαισίου ορισμού από την βιβλιογραφία. Στο δεύτερο μέρος της μελέτης, μελετήθηκαν τα αποτελέσματα τόσο από την ποιοτική ανάλυση των ομάδων εστίασης, όσο και από τα μελή της ερευνητικής ομάδας και πραγματοποιήθηκε η εγκατάσταση ενός ελαφρώς τροποποιημένου τεχνολογικού συστήματος σε 5 σπίτια ηλικιωμένων για ένα διάστημα πλέον του ενός έτους, συλλέγοντας με αυτόν τον τρόπο εκατοντάδες χιλιάδες ψηφιακά στιγμιότυπα της καθημερινότητας των ηλικιωμένων. Στην συνέχεια επιχειρήθηκε η ανάπτυξη τόσο στατιστικών μοντέλων, όσο και μοντέλων μηχανικής μάθησης για την διερεύνηση της κλινικής αξίας των ψηφιακών βιοδεικτών. Σαν μελέτη εφαρμογής αυτών των μοντέλων ήταν η πρόβλεψη συναισθηματικών διαταραχών καθώς και της καταθλιπτικής συμπτωματολογίας. Παράλληλα με αυτές τις μελέτες, διενεργήθηκε και μια μακροπρόθεσμη (Longitudinal) μελέτη της αποδοχής των αισθητήρων από τους ηλικιωμένους με έμφαση και πάλι στον βαθμό παρεμβατικότητας. Η μελέτη διενεργήθηκε με την μορφή προσωπικών συνεντεύξεων με κάθε μία από τις πέντε συμμετέχουσες. Τέλος, με την πρόταση και δημιουργία ενός εμπειρικού μοντέλου (expert model) υποστήριξης της διάγνωσης της γηριατρικής κατάθλιψης (Fuzzy Cognitive Maps), αποκρυσταλλώνοντας την γνώση των ειδικών σε ένα σχήμα αναπαράστασης γνώσης προσιτό σε αυτούς, επιχειρείται η αντιστοίχηση της νέας γνώσης που παράγεται από τους αισθητήρες με την υπάρχουσα κλινική γνώση. Αποτελέσματα: Όσον αφορά το πρώτο σκέλος της διατριβής βρέθηκαν αρκετές στατιστικά σημαντικές συσχετίσεις ανάμεσα στις επιμέρους κατηγορίες ψηφιακών δεικτών, όπως κινητικοί, συναισθηματικοί και φυσιολογικοί και των επιμέρους κατηγοριών κλινικών αξιολογήσεων της υγείας των ηλικιωμένων. Όσον αφορά τα αποτελέσματα από τις ομάδες εστιασμένης συζήτησης αυτά αναλύθηκαν με ποιοτικές μεθόδους για να εξαχθούν τα πιο βασικά θέματα που προέκυψαν από τις συζητήσεις και να κατηγοριοποιηθούν σε κάθε ένα από τους άξονες του πλαισίου παρεμβατικότητας (obtrusiveness framework). Για το δεύτερο σκέλος της διατριβής, δημιουργήθηκε ένα γραμμικό μοντέλο πρόβλεψης του PHQ-9 σκορ από τα μοτίβα λειτουργίας της τηλεόρασης. Εκεί βρέθηκαν συσχετίσεις με κάποια από τα συμπτώματα της κατάθλιψης. Το μοντέλο μηχανικής μάθησης και συγκεκριμένα, Τυχαία Δάση (Random Forests, RF) πέτυχαν με ακρίβεια άνω του 80% να διακρίνουν μεταξύ καταθλιπτικών και υγιών καθημερινών στιγμυοτύπων συμπεριφοράς, ενώ το μοντέλο ταξινόμησης καθημερινών προτύπων συμπεριφοράς σε ήπια, μέτρια και σοβαρά καταθλιπτικά περιστατικά πέτυχε εμβαδόν επιφάνειας ROC >90%. Επίσης, η αρχική αξιολόγηση του μοντέλου υποστήριξης της διάγνωσης της κατάθλιψης, με βάση το σχήμα αναπαράστασης γνώσης FCM είχε μέση ακρίβεια περίπου 96%. Τέλος, η σύγκριση των μοτίβων αντίληψης των ηλικιωμένων όσον αφορά την παρεμβατικότητα των τεχνολογιών που είχαν εγκατεστημένες στο σπίτι τους, απεκάλυψε την αρνητική τους στάση απέναντι στο έξυπνο ρολόι και τον καθρέφτη-κάμερα. Επίσης, καθώς μία από τις 5 κυρίες απεχώρησε οικιοθελώς από την μελέτη, συγκρίθηκαν οι απαντήσεις της με τον μέσο όρο των υπολοίπων ηλικιωμένων γυναικών. ώστε να αποκαλυφθούν οι αιτίες της αποχώρησης από την μελέτη. Βασικοί λόγοι αποδείχθηκαν το έξυπνο ρολόι, ο καθρέφτης με την ενσωματωμένη κάμερα αλλά και η συσκευή Kinect λόγω και της οποίας αναγκάστηκε να αλλάξει την καθημερινή ρουτίνα της. Συμπεράσματα: Η κλινική αξία των ψηφιακών βιοδεικτών έχει αναδειχθεί σε πλήθος δημοσιεύσεων, όμως η χρήση και αξιολόγησή τους σε μακροπρόθεσμες μελέτες απουσιάζει. Μεγάλη πρόκληση αποτελεί στις μέρες μας η εύρωστη λειτουργία τέτοιων τεχνολογιών υπό πραγματικές συνθήκες χωρίς πριορισμούς (in the wild) αλλά και η αποδοχή τους τόσο από τους ηλικιωμένους, όσο και από τους γιατρούς αν κάποια στιγμή στο μέλλον θελήσουμε να χρησιμοποιήσουμε τέτοιου είδους δεδομένα στην κλινική πράξη",Αριστοτέλειο Πανεπιστήμιο Θεσσαλονίκης (ΑΠΘ),Digital biomarkers as ecologically valid measures for the remote and longitudinal assessment of older adults health,,,,core
350021943,2020-09-24T00:00:00,"From the customer's perspective, the appeal of electric vehicles depends on the simplicity and ease of their use, such as flexible access to electric power from the grid to recharge the batteries of their vehicles. Therefore, the expansion of charging infrastructure will be an important part of electric mobility. The related charging infrastructure is a big challenge for the load capacity of the grid connection without additional intelligent charge management: if the control of the charging process is not implemented, it is necessary to ensure the total of the maximum output of all xEVs at the grid connection point, which requires huge costs. This paper proposes to build a prediction module for forecasting dynamic charging load using machine learning (ML) techniques. The module will be integrated into a real charge management concept with optimization procedures for controlling the dynamic load point. The value of load forecasting through practical load data of a car park were taken to illustrate the proposed methods. The prediction performance of different ML methods under the same data condition (e.g., holiday data) are compared and evaluated",'EDP Sciences',Application and machine learning methods for dynamic load point controls of electric vehicles (xEVs),10.1051/e3sconf/202019104003,,,core
389912358,2020-05-15T00:00:00,"Context. The article focuses on the question of automated decision-making analysis made by the operator in ergatic systems of critical infrastructures on the example of marine transport control in difficult navigation conditions. It is evident enough that the main criterion for an adequate perception of input information done by an operator is highly likely to predict the choice of behavioral decision-making strategies in discrete time conditions. However, the difficulty of modeling the operator’s actions is found to be lying in non-linear pattern of taking definite decisions in emergency situations and deviations from the Codes and Rules.Objective. The research purpose strategy of conducted investigation can be defined as the development of the mathematical platform for a decision support system (DSS) module with an aim to identify the class-forming set of atomic elements. In particular this issue determines the fact of distortion of the perception of information about navigation risks predicting the operator’s behavior pattern while having vessel running process. This is possible to have it depicted through formal analysis.Method. To capture the analysis of danger perception by the operator the paper introduces a mathematical model of data collection which identifies the fact of perception distortion in the form of attribute space of metadata obtained by the method of converting information from navigation devices. Besides, the factor of disorientation of the operator can be considered to be a shift on a displaced bridge which significantly affects on the analysis of information for adequate decision making. In addition, taking into account the failure of navigation equipment such as: RADAR, ARPA, AIS, ECDIS, especially while doing exit from the automatic control mode, a dangerous precedent can possibly be created for the operator not ready to perceive the complexity of the situation. To make it work a formal analysis was carried out using the extending risks possibility level tasks during the transition under these conditions. In addition to this item, a probabilistic model of perceiving the situation under the conditions of the error set is reported to have been constructed. So, as the result, the modeling process turned out to show the definite evidence of getting no way possibility to have the degree of criticality of the navigation situation determined without a clear identification of factors affecting the distortion of perception of the operator. Nevertheless, generalized statistical data are sure to be not enough and there is a special need of taking into account an individual information model of each operator for the effective work of DSS as this process faces real challenges. It must be significantly noticed that in order to analyze the perception of information by the operator a special test for defining preferences when choosing a strategy of control actions in the form of maneuvering under difficult navigation conditions purpose was created. Regarding the test results, as well as data on the passage of locations, certain attention is advised to be drawn to the classification analysis of 15 parameters using artificial neural networks having been carried out by our team and, as a consequence, the boundaries of deviations in the perception of navigational danger were found out and clarified. Additional superior item to be spoken about is certainly the introduction of rules and algorithms having been welcomed into the DSS core including the following: interaction field, RADAR and NIS synchronization tools; actual navigational hazard in a given cartographic area; ships trajectories and, as a result, simulations of probable deviations in the information perception of the operator.Results. In order to meet beneficial agreement between the effectiveness of the developed DSS with the proposed formalanalytical approaches an experiment was assumed to be appropriate to be conducted using the Navi Trainer 5000 navigation simulator (NTPRO 5000). Based on the foregoing, due to comprehensive results in experiment metadata for the 2.5 years of operation of navigation simulators and DSS software tools the identification of the deviation probabilities in the information perception of dangers was achieved and export the predicted data to new locations for the operator and cartographic areas was performed. Undoubtedly, the experimental investigation confirmed the hypothesis of the study and reflected completely the feasibility of using this DSS to make predictions of possible risks when control the vessel by analyzing the information model of the operator.Conclusions. Formal-analytical approaches presented in the study combined with the developed DSS software tools and the information itself made it possible to classify the decision-making strategies of the operator when control the vessel and to predict the probability of catastrophic consequences. The feasibility of the proposed models and methods was successfully revealed by carried out experiments. Актуальность. В статье рассматривается задача автоматизированного анализа принятия решений оператором в эргатических системах критических инфраструктур на примере управления морским транспортом в сложных навигационных условиях. Основным критерием адекватного восприятия входной информации оператором является прогнозирование поведенческих стратегий принятия решений в условиях дискретного времени. Однако, сложность моделирования действий оператора состоит в нелинейном формировании решений в условиях внештатных ситуаций и отклонений от Кодексов и Правил. Цель. Целью исследования является разработка математического обеспечения модуля системы поддержки принятия решений (СППР) для идентификации классо-образующего множества атомарных элементов, определяющих факт искажения восприятия информации о навигационных рисках путем формального анализа и прогноза моделей поведения оператора при управлении судном. Метод. С целью автоматизации анализа восприятия опасности оператором, была построена математическая модель, которая идентифицирует факт искажения восприятия в виде признакового пространства метаданных, получаемых посредством преобразования информации навигационных приборов. Фактором дезориентации оператора также может служить несение вахты на смещенном мостике, что существенно влияет на анализ информации для адекватного принятия решений. В связи с нарушением синхронизации навигационных приборов, таких как: РЛС, ARPA, АІС, ECDIS, особенно в случаях выхода из режима автоматического управления, создается опасный прецедент неготовности оператора воспринять сложность ситуации вследствие чего проведен формальный анализ на предмет повышения рисков во время перехода в этих условиях. Также построена вероятностная модель восприятия ситуации в условиях картежа погрешностей. Моделирование показало, что без четкой идентификации факторов, влияющих на искажение восприятия оператора, невозможно определить степень критичности навигационной ситуации, поэтому обобщенных статистических данных недостаточно и для результативной работы СППР необходима индивидуальная информационная модель каждого оператора. С целью анализа восприятия информации оператором был разработан тест, определяющий предпочтения при выборе стратегии управляющих воздействий в виде выполнения маневра при сложных навигационных условиях. Результаты тестирования, а также данные по прохождению локаций позволили выполнить классификационный анализ по 15 параметрам с помощью искусственных нейронных сетей и определить границы отклонений в восприятии навигационной опасности. В ядро СППР внесен ряд правил и алгоритмов, включающие: поле взаимодействия, средства синхронизации РЛС и НИС; фактическая навигационная опасность в данной картографической области; траектории движения судов и, как результат, моделирования вероятного отклонения в информационном восприятии оператора.Результаты. С целью подтверждения результативности разработанной СППР и предложенных формальноаналитических подходов был проведен эксперимент с применением навигационного тренажера Navi Trainer 5000 (NTPRO 5000). Метаданные эксперимента за 2,5 года работы навигационных тренажеров и программные средства СППР позволили идентифицировать вероятность отклонения в информационном восприятии опасностей и экспортировать прогнозированные данные в новые для оператора локации и картографические районы. Проведенный эксперимент подтвердил гипотезу исследования и показал целесообразность применения данной СППР для выполнения прогнозов возможных рисков при управлении судном путем анализа информационной модели оператора. Выводы. Представленные в исследовании информационные и формально-аналитические подходы, а также разработанные программные средства СППР позволили выполнить классификацию стратегий принятия решений оператором при управлении судном и спрогнозировать вероятности катастрофических последствий. Проведенные эксперименты подтвердили целесообразность предложенных моделей и методов. Актуальність. У статті розглядається задача автоматизованого аналізу прийняття рішень оператором в ергатичних системах критичних інфраструктур на прикладі управління морським транспортом в складних навігаційних умовах. Основним критерієм адекватного сприйняття вхідної інформації оператором є прогнозування поведінкових стратегій прийняття рішень в умовах дискретного часу. Однак, складність моделювання дій оператора полягає у нелінійному формуванні рішень в умовах позаштатних ситуацій і відхилень від Кодексів і Правил.Мета. Метою дослідження є розробка математичного забезпечення модуля системи підтримки прийняття рішень (СППР) для ідентифікації класоутворюючих множин атомарних елементів що визначають факт спотворення сприйняття інформації про навігаційні ризики шляхом формального аналізу і прогнозу моделей поведінки оператора при управлінні судном.Метод. З метою автоматизації аналізу сприйняття небезпеки оператором, була побудована математична модель, яка ідентифікує факт спотворення сприйняття у вигляді простору ознак метаданих, що одержуються за допомогою обробки інформації навігаційних приладів. Фактором дезорієнтації оператора також може служити несення вахти на зміщеному містку, що істотно впливає на аналіз інформації для адекватного прийняття рішень. У зв’язку з порушенням синхронізації навігаційних приладів, таких як: РЛС, ARPA, АІС, ECDIS, особливо у випадках виходу з режиму автоматичного управління, виникає небезпечний прецедент що полягає у неготовності оператора сприйняти складність ситуації, внаслідок чого проведено формальний аналіз на предмет підвищення ризиків під час переходу у вказаних умовах. Також побудована імовірнісна модель сприйняття ситуації в умовах картежа похибок. Моделювання показало, що без чіткої ідентифікації факторів, що впливають на спотворення сприйняття оператора, неможливо визначити ступінь критичності навігаційної ситуації, тому узагальнених статистичних даних недостатньо і для результативної роботи СППР, тобто необхідна індивідуальна інформаційна модель кожного оператора. З метою аналізу сприйняття інформації оператором був розроблений тест, що визначає переваги при виборі стратегії керуючих впливів у вигляді виконання маневру при складних навігаційних умовах. Результати тестування, а також дані по проходженню локації дозволили виконати класифікаційний аналіз по 15 параметрам за допомогою штучних нейронних мереж і визначити межі відхилень у сприйнятті навігаційної небезпеки. У ядро СППР внесений ряд правил і алгоритмів, які включають: поле взаємодії, засоби синхронізації РЛС і НІС; фактична навігаційна небезпека в даній картографічній області; траєкторії руху суден і, як результат, моделювання імовірного відхилення у сприйнятті оператора.Результати. З метою підтвердження результативності розробленої СППР і запропонованих формально-аналітичних підходів був проведений експеримент із застосуванням навігаційного тренажера Navi Trainer 5000 (NTPRO 5000). Метадані експерименту за 2,5 року роботи навігаційних тренажерів і програмних засобів СППР дозволили ідентифікувати ймовірність відхилення в інформаційному сприйнятті небезпек і експортувати прогнозовані дані в нові для оператора локації і картографічні райони. Проведений експеримент підтвердив гіпотезу дослідження і показав доцільність трансформаційних змін даної СППР для виконання прогнозів можливих ризиків при управлінні судном шляхом аналізу інформаційної моделі оператора.Висновки. Представлені в дослідженні інформаційні та формально-аналітичні підходи, а також розроблені програмні засоби СППР дозволили виконати класифікацію стратегій прийняття рішень оператором при управлінні судном і спрогнозувати ймовірність катастрофічних наслідків. Проведені експерименти підтвердили доцільність запропонованих моделей і методів.","National University ""Zaporizhzhia Polytechnic""",СИСТЕМА ДІАГНОСТИКИ СПРИЙНЯТТЯ НАВІГАЦІЙНОЇ НЕБЕЗПЕКИ ПІД ЧАС ВИКОНАННЯ СКЛАДНИХ МАНЕВРІВ,,,,core
343451340,2020-01-01T08:00:00,"With smart city infrastructures growing, the Internet of Things (IoT) has been widely used in the intelligent transportation systems (ITS). The traditional adaptive traffic signal control method based on reinforcement learning (RL) has expanded from one intersection to multiple intersections. In this paper, we propose a multi-agent auto communication (MAAC) algorithm, which is an innovative adaptive global traffic light control method based on multi-agent reinforcement learning (MARL) and an auto communication protocol in edge computing architecture. The MAAC algorithm combines multi-agent auto communication protocol with MARL, allowing an agent to communicate the learned strategies with others for achieving global optimization in traffic signal control. In addition, we present a practicable edge computing architecture for industrial deployment on IoT, considering the limitations of the capabilities of network transmission bandwidth. We demonstrate that our algorithm outperforms other methods over 17% in experiments in a real traffic simulation environment",'Sociological Research Online',An Edge Based Multi-Agent Auto Communication Method for Traffic Light Control,,https://core.ac.uk/download/343451340.pdf,,core
297070362,2020-02-10T00:00:00,"Currently, there is a need to improve the systems and control of pumping equipment in the oil and gas production and oil and gas transport industries. Therefore, an adaptive neural network control system for an electric drive of a production well was developed. The task of expanding the functional capabilities of asynchronous electric motors control of the oil and gas production system using the methods of neural networks is solved. We have developed software modules of the well drive control system based on the neural network, an identification system, and a scheme to adapt the control processes to changing load parameters, that is, to dynamic load, to implement the entire system for real-time control of the highspeed process. In this paper, based on a model of an identification block that includes a multilayered neural network of direct propagation, the control of the well system was implemented. The neural network of the proposed system was trained on the basis of the error back-propagation algorithm, and the identification unit works as a forecaster of system operation modes based on the error prediction. In the initial stage of the model adaptation, some fluctuations of the torque are observed at the output of the neural network, which is associated with new operating conditions and underestimated level of learning. However, the identification object and control system is able to maintain an error at minimum values and adapt the control system to a new conditions, which confirms the reliability of the proposed scheme",'EDP Sciences',"Improving the energy efficiency of oil production using identification and prediction of operating modes of production wells based on data analysis methods, machine learning and neural networks",10.1051/e3sconf/201912405031,,,core
322969060,2020-05-15T00:00:00,"In the last four years, the number of distinct autonomous vehicles platforms
deployed in the streets of California increased 6-fold, while the reported
accidents increased 12-fold. This can become a trend with no signs of subsiding
as it is fueled by a constant stream of innovations in hardware sensors and
machine learning software. Meanwhile, if we expect the public and regulators to
trust the autonomous vehicle platforms, we need to find better ways to solve
the problem of adding technological complexity without increasing the risk of
accidents. We studied this problem from the perspective of reliability
engineering in which a given risk of an accident has severity and probability
of occurring. Timely information on accidents is important for engineers to
anticipate and reuse previous failures to approximate the risk of accidents in
a new city. However, this is challenging in the context of autonomous vehicles
because of the sparse nature of data on the operational scenarios (driving
trajectories in a new city). Our approach was to mitigate data sparsity by
reducing the state space through monitoring of multiple-vehicles operations. We
then minimized the risk of accidents by determining proper allocation of tests
for each equivalence class. Our contributions comprise (1) a set of strategies
to monitor the operational data of multiple autonomous vehicles, (2) a Bayesian
model that estimates changes in the risk of accidents, and (3) a feedback
control-loop that minimizes these risks by reallocating test effort. Our
results are promising in the sense that we were able to measure and control
risk for a diversity of changes in the operational scenarios. We evaluated our
models with data from two real cities with distinct traffic patterns and made
the data available for the community.Comment: 12 pages, 14 figures, 15th International Symposium on Software
  Engineering for Adaptive and Self-Managing Systems (SEAMS2020",,"Collective Risk Minimization via a Bayesian Model for Statistical
  Software Testing",,http://arxiv.org/abs/2005.07460,,core
395058120,2020-01-01T00:00:00,"This dissertation’s innovation is concentrated on the introduction and description of a reference network framework for Cognitive Medium Access Control and SDR Services and Abstract Cognitive Medium Access Control and SDR Services integration and deployment on all the OSI layers in a heterogeneous wireless network. The necessity of lower layers services and applications conceptualization within the Cognitive Radio Cycle  is the main issue that this dissertation manifests whilst providing algorithms for  responding to diverse issues within the Cognitive Radio Network and Cognitive Radio Network Cloud. Radio Spectrum is a public good and the recent years due to the vast increase of mobile users and mobile applications as well as the need for Quality of Experience (QoE) of the end user, a spectrum scarcity was a main issue in wireless networks leading to the introduction of a new key technology namely the Cognitive Radio for the underutilized licensed spectrum bands broadening the radio spectrum usage and management. The current challenges in Cognitive Radio Network (CRN) are storing of large amount of data, processing them in real time and the exchanging of nodes’ current status on-the-fly. These challenges are in contrast to the limited storage and processing ability (plus battery lifetime) of Cognitive Devices thus the need for additional capabilities arise. Cognitive Radio Network Cloud (CRNC) is an infra-structure consisting of mobile nodes and the cloud whose primary goal is to keep an up-to-date status of the spectrum availability in the network. Demanding tasks, e.g. signal intelligence, could be off-loaded to powerful nodes locally allowing the local network to be self-organized. By allowing self-organizing networks to be deployed locally, huge heterogeneous wireless networks such as 5G and 6G evolve, which can mitigate their dynamic spectrum access and control to meet the end users and wireless network performance requirements.Self-organizing Cognitive Radio Networks in an immense heterogeneous wireless network along with Dynamic Spectrum Access, Management and Control Mitigation on demand or not on demand to respond to network needs in real time and on the fly can be realized with high level abstraction and conceptualization of Cognitive Medium Access Control and SDR Services and Abstract Cognitive Medium Access Control and SDR Services integration and deployment on all the Open Systems Interconnection (OSI) layers, cross-platform and cross-network, cross-operator. Central coordination would be applicable for triggering local nomad network to be self-organized, as well for hand-off or for meeting QoE, radio network performance, institutional metrics. Network clustering on Cognitive Medium Access Control and SDR Services/Applications and Abstract Cognitive Medium Access Control and SDR Services/Applications Level may be feasible. Artificial Intelligence and other technologies will enable efficient distributed control. Radio Environment Maps are powerful technology to this direction. Off-loading to local powerful nodes increase network performance and QoE which are essential for 5G, 6G network for urban and rural radio environments. A new mathematic method of mathematic game unfolding is introduced i.e. a new mathematic method for generating games without coordination and a corresponding mathematic game model as an application of the proposed mathematic method to for the Cognitive Radio Network and Cognitive Radio Cloud Security was introduced. Other mathematic game reaching Nash Equilibriums also were introduced. Deterministic automata and Machine Learning were also introduced as well as other mathematic formulas applicable to the corresponding network protocols, mathematic models for steady-state- Lyapunov filtering algorithm for enhanced CRN-SDR signal processing and mathematic formulas for Non-Reciprocal Channels in Massive MIMO CRN were also introduced in this dissertation. Demanding tasks, e.g. signal intelligence, could be off-loaded to powerful nodes locally allowing the local network to be self-organized. By allowing self-organizing networks to be deployed locally, huge heterogeneous wireless networks such as 5G and 6G evolve, which can mitigate their dynamic spectrum access and control to meet the end users and wireless network performance requirements. A critical issue in CR Infrastructure-less network deployment on the cloud would be the Standard Interface Operability (SIO) for CR users to connect to the cloud or local powerful nodes. SIO would allow easy access and utilization of powerful wireless and mobile local nodes and infrastructure network front-end. This dissertation and research work presented in the following chapters participated in this effort or the international research community on wireless network  by introducing Cognitive Medium Access Control and SDR Services and Abstract Cognitive Medium Access Control and SDR Services integration to achieve higher degree of conceptualization of Cognitive Medium Access Control and SDR Services utilized in the upper stack layers of OSI and meet a common goal such as smart radio environment utilization locally or globally, QoE, higher wireless network performance. The technologies that may be employed are current technologies such as Virtual Machines and Network Function Virtualization, Network Chaining. The concept of Cognitive Medium Access Control and SDR Services and their conceptualization to an abstract level covering the  seven layers of  the OSI stack and respond to edge requirements of the huge 5G, 6G heterogeneous networks. Cognitive Medium Access Control and SDR Services integration and deployment on all the OSI layers and 5G and 6G huge and heterogeneous network would require high level of scheduling and interaction. High level of scheduling and interaction will trigger eventually adaptation “knobs” within the local and global system would necessiate high level of interaction on all OSI stack layers of Cognitive Medium Access Control and SDR Services and Abstract Cognitive Medium Access Control and SDR Services in this reference network framework for cross-operator, cross-network operability context generation, QoE, radio network demands, cross-operator, cross-border. This dissertation and the subsequent research work introduced a framework of this Cognitive Medium Access Control and SDR Services and application and Abstract Cognitive Medium Access Control and SDR Services integration and deployment framework. In particular in this framework optimal algorithms have been introduced to implement protocols for solving diverse issues such as native Cognitive Medium Access Control, Software Defined Networking coordination, Sustainability, Broadcasting, Security, massive MIMO technology and imperfect CSI for reciprocal channels, Multiscale Decision Making with Machine Learning and Game Theory for Efficient Sensing Scheduling and Spectrum Sensing Control, Enhanced Signal Processing and Radio Environment Maps for  the Cognitive Radio Network and Cognitive Radio Cloud. As huge heterogeneous next generation wireless networks  is  a necessity, proposing such high level design of low layer services and applications cross-layer, cross-network, cross-operator would make feasible 5G and 6G - Space-Air-Ground-Sea integrated communication and wireless tactile network- and allow them to evolve.Η πρωτοτυπία της παρούσας διατριβής έγκειται στο ότι πρότεινε και περιέγραψε ένα πλαίσιο δικτύου αναφοράς για Έλεγχο Πρόσβασης στο Μέσο για Γνωστικά Ασύρματα Δίκτυα και Νέφος και SDR  Υπηρεσίες και για Έλεγχο Πρόσβασης στο Μέσο για Γνωστικά Ασύρματα Δίκτυα και Νέφος και SDR  Αφηρημένες Υπηρεσίες  και ολοκλήρωση τους και ανάπτυξη τους σε όλα κατά OSI επίπεδα για ετερογενή ασύρματα δίκτυα. Η αναγκαιότητα της ύπαρξης και αφαίρεσης  υπηρεσιών χαμηλού επιπέδου λειτουργιών όπως και εφαρμογών τους στα πλαίσια του Γνωστικού Κύκλου είναι κάποια από τα κύρια σημεία που η παρούσα διατριβή διακηρύσσει ενώ παράλληλα προτείνει αλγορίθμους που να ανταποκρίνονται σε ποικίλα θέματα που αφορούν τα Γνωστικά Ασύρματα Δίκτυα και Νέφος. Οι τρέχουσες προκλήσεις στα Γνωστικά Ασύρματα Δίκτυα είναι η αποθήκευση μεγάλου όγκου πληροφοριών και η επεξεργασία τους σε πραγματικό χρόνο και η ανταλλλαγή τους μεταξύ των ασύρματων κόμβων του δικτύου σε αντιδιαστολή του περιορισμένου χώρου αποθήκευσης και της υπολογιστικής ικανότητας των Γνωστικών Συσκευών που απαιτούν πρόσθετες ικανότητες. Το Νέφος Γνωστικού Ασύρματου Δικτύου αποτελεί μια υποδομή που αποτελείται από κινητούς κόμβους και το νέφος των οποίων ο πρωταρχικός στόχος είναι να διατηρούν ενημερωμένη την κατάσταση του δικτύου και της διαθεσιμότητας του φάσματος. Απαιτητικές εργασίες όπως η επεξεργασία σήματος μπορούν να ανατεθούν στο ισχυρούς κόμβους τοπικά επιτρέποντας στο δίκτυο τοπικά να αυτορυθμιστεί. Επιτρέποντας στο δίκτυο να αυτορυθμιστεί και αναπτυχθεί τοπικά, τεράστια ετερογενή ασύρματα δίκτυα όπως τα δίκτυα 5G και 6G  μπορούν να αναπτυχθούν και να μεταφέρουν την δυναμική πρόσβαση στο φάσμα και τον έλεγχο στους τελικούς χρήστες και τις απαιτήσεις της απόδοσης του ασύρματου δικτύου. Αυτορυθμιζόμενα Γνωστικά Ασύρματα Δίκτυα σε ένα τεράστιο ετερογενές ασύρματο δίκτυο μαζί με δυναμική πρόσβαση στο φάσμα και μεταβίβαση του ελέγχου κατά απαίτηση ή μη σε απάντηση των απαιτήσεων του δικτύου σε πραγματικό χρόνο μπορούν να πραγματοποιηθούν με υψηλό ποσοστό αφαίρεσης των Έλεγχο Πρόσβασης στο Μέσο για Γνωστικά Ασύρματα Δίκτυα και Νέφος και SDR Υπηρεσίες και για Έλεγχο Πρόσβασης στο Μέσο για Γνωστικά Ασύρματα Δίκτυα και Νέφος και SDR Αφηρημένες Υπηρεσίες και ολοκλήρωση τους και ανάπτυξη τους σε όλα κατά OSI επίπεδα για ετερογενή ασύρματα δίκτυα και ανάπτυξη τους ανεξάρτητα συστήματος, δικτύου, παρόχου. Κεντρικός συντονισμός εφαρμόζεται για να ενεργοποιήσει τοπικά νομαδικά δίκτυα όπως και προκειμένου να αυτορυθμιστούν και να επιτευχθούν μετρικές ποιότητας υπηρεσίας και δικτύου όπως και του οργανισμού. Η ομαδοποίηση του δικτύου όσον αφορά την Έλεγχο Πρόσβασης στο Μέσο για Γνωστικά Ασύρματα Δίκτυα και Νέφος και SDR  Υπηρεσίες και για Έλεγχο Πρόσβασης στο Μέσο για Γνωστικά Ασύρματα Δίκτυα και Νέφος και SDR  Αφηρημένες Υπηρεσίες προκειμένου να είναι εφικτές. Η Τεχνητή Νοημοσύνη και άλλες τεχνολογίες αποδίδουν και μπορούν να εφαρμοστούν κατανεμημένα. Οι χάρτες του φάσματος συχνοτήτων είναι μια δυναμική τεχνολογία προς αυτή την κατεύθυνση. Η μετατόπιση του φόρτου σε ισχυρούς κόμβους τοπικά αυξάνει την ικανότητα παροχής ποιότητας υπηρεσίας που είναι απαραίτητη για 5G, 6G δίκτυα για αστικά και μη αστικά περιβάλλοντα συχνοτήτων. Μια νέα μαθηματική μέθοδος για δημιουργία μαθηματικών παιγνίων προτάθηκε δηλ. Μια μέθοδος για γένεση παιγνίων χωρίς ανταλλαγή μηνυμάτων μέσω των παικτών και ένα αντίστοιχο μοντέλο παιγνίου ως εφαρμογή στην ασφάλεια των Γνωστικών Ασύρματων Δικτύων και Νέφους. Άλλα παίγνια που επιτυγχάνουν Nash Equilibrium προτάθηκαν. Ντετερμινιστικά Αυτόματα και Μηχανική Μάθηση προτάθηκαν όπως και μαθηματικοί τύποι προκειμένου να εφαρμοστούν σε δικτυακά πρωτόκολλα για μεγάλο ετερογενές δίκτυο, για πρόσβαση στο μέσο, εκπομπές, δίκτυα αισθητήρων, μαθηματικά συστημικά μοντέλα και σταθερής κατάστασης κατά Lyapunov συστήματα, αλγόριθμοι για προηγμένη επεξεργασία σήματος σε CRN-SDR και μαθηματικά μοντέλα για μη αμοιβαία κανάλια μετάδοσης για massive MIMO και αντίστοιχα LMMSE φίλτρα προτάθηκαν στην διατριβή",'National Documentation Centre (EKT)',Αλγόριθμοι για γνωστικά ασύρματα δίκτυα και νέφος γνωστικών ασύρματων δικτύων,10.12681/eadd/47924,,,core
237398591,2020-05-01T00:00:00,"A Digital Twin (DT) refers to a digital replica of physical assets, processes and systems. DTs integrate artificial intelligence, machine learning and data analytics to create living digital simulation models that are able to learn and update from multiple sources, and to represent and predict the current and future conditions of physical counterparts. However, the current activities related to DTs are still at an early stage with respect to buildings and other infrastructure assets from an architectural and engineering/construction point of view. Less attention has been paid to the operation & maintenance (O&M) phase, which is the longest time span in the asset life cycle. A systematic and clear architecture verified with practical use cases for constructing a DT would be the foremost step for effective operation and maintenance of buildings and cities. According to current research about multi-tier architectures, this paper presents a system architecture for DTs which is specifically designed at both the building and city levels. Based on this architecture, a DT demonstrator of the West Cambridge site of the University of Cambridge was developed, which integrates heterogeneous data sources, supports effective data querying and analysing, supports decision-making processes in O&M management, and further bridges the gap between human relationships with buildings/cities. This paper aims at going through the whole process of developing DTs in building and city levels from the technical perspective and sharing lessons learnt and challenges involved in developing DTs in real practices. Through developing this DT demonstrator, the results provide a clear roadmap and present particular DT research efforts for asset management practitioners, policymakers and researchers to promote the implementation and development of DT at the building and city levels.Centre for Digital Built Britain (Innovate UK); Centre for Smart Infrastructure and Construction (Innovate UK/EPSRC",'Organisation for Economic Co-Operation and Development  (OECD)',Developing a dynamic digital twin at building and city levels: A case study of the West Cambridge campus,10.17863/CAM.45198,,,core
326449834,2020-01-01T00:00:00,"Surveillance cameras are widely installed along roadways, and the numbers are steadily increasing. With widely deployed surveillance cameras that monitor the road conditions, it’s feasible to develop a system to analyse the traffic conditions automatically. The aim of this project is to develop an automatic algorithm using Python and deep learning techniques to detect the vehicles from the traffic surveillance video and automatically analyse the traffic flow and congestion. The system is mainly divided into two components: vehicle detection and traffic congestion analysis. For the vehicle detection component of the system, deep learning model RetinaNet  was trained on UA-DETRAC Benchmark which consisted of vehicle images extracted from real life traffic videos. Besides, Cycle Generative Adversarial Network (Cycle-GAN) was adopted to improve the vehicle detection performance at night time. Artificial night time images generated from trained Cycle-GAN models and augmented training dataset were used for RetinaNet vehicle detector. For the traffic congestion analysis component of the system, OpenCV and several Python packages and modules such as Matplotlib, Numpy, Pandas, Tkinter and CSV were used to perform congestion level analysis, display the traffic information in traffic images, generate popping up alert notifications, write log files and report, and generate traffic flow charts and output video. This automatic traffic congestion analysis system can bring many benefits and has great potential in urban traffic management. For example, the information generated from this automated system can be used by the traffic management authorities to monitor the traffic flow in an efficient manner and develop a balanced urban road transportation system. Furthermore, the system-generated alert notifications can help the drivers to improve their awareness of safety driving and reduce the risk of getting into car accidents, especially in traffic congested areas.Bachelor of Engineering (Information Engineering and Media",'Nanyang Technological University',An automatic multi-class vehicle detection and traffic congestion analysis system,,,,core
328847576,2020-02-16T00:00:00,"La crescente attenzione riguardo alle tematiche ambientali sta portando sempre più all’attenzione i problemi legati all’inquinamento, specialmente per quanto riguarda l’emissione dei gas serra. Una delle principali cause dell’aumento di gas serra è senza dubbio l’utilizzo dei combustibili fossili, anche nel campo della mobilità. Per questo motivo, negli ultimi decenni, sono state cercate alternative più ecologiche: la tendenza attuale è senza dubbio quella di muoversi in direzione della trazione elettrica, e in special modo a guida autonoma. Il lavoro oggetto di tesi è appunto inquadrato all’interno del progetto europeo AutoDrive, il quale mira alla progettazione di componenti elettronici e architetture di tipo Fail-Operational, ovvero che continuano a eseguire un insieme definito delle loro funzioni anche in presenza di guasti, che permettano l’introduzione della guida autonoma in autoveicoli di tutte le categorie, con l’intento di contribuire a una mobilità più efficiente e sicura. Infatti, dato il crescente numero di implementazioni software e meccatroniche all’interno delle automobili, sono presenti sempre più rischi dovuti a loro possibili guasti, sia sistematici che aleatori, che devono quindi essere tenuti in considerazione al fine della sicurezza funzionale. Poiché quest’ultima venga garantita è necessario introdurre all’interno del sistema, in modo controllato, un qualche tipo di ridondanza che permetta di mascherare o individuare i guasti che si possono verificare.
La possibilità di realizzare veicoli a trazione elettrica è strettamente legata ai progressi conseguiti nel campo delle batterie, specialmente per quanto riguarda le tecnologie basate sugli ioni di Litio. Infatti, quest’ultime presentano una maggiore densità di energia e di potenza, una tensione di cella più elevata, la mancanza di effetto memoria e una minore corrente di auto scarica se confrontate con le altre chimiche esistenti. Grazie a questa serie di vantaggi, le tecnologie basate sugli ioni di Litio, rappresentano l’unico vero candidato per il futuro della mobilità elettrica. Questa tecnologia, tuttavia, presenta degli svantaggi in quanto è necessario l’inserimento di un sistema elettronico che monitori la batteria, il Battery Management System (BMS), il quale ha, tra gli altri, il compito di garantirne il corretto funzionamento in termini di range operativi di tensione, temperatura e corrente. Difatti, la fuoriuscita di queste grandezze dalla loro Safe Operating Area (SOA), oltre a portare un degradamento delle prestazioni della batteria, può provocare l’innescarsi di condizioni ben più gravi quali fughe termiche interne alle celle o persino l’esplosione delle celle stesse. Qualora il BMS identifichi una situazione critica per l’operatività della batteria, questo sistema interviene attraverso un sistema di feedback sul circuito, andando ad esempio a distaccare il carico. Il BMS misura costantemente tutte le grandezze fisiche delle celle che compongono la batteria, quali corrente, tensione e temperatura. A partire da queste, oltre a verificare se i dati sono all’interno della loro SOA, questo sistema esegue degli algoritmi di stima dello stato della batteria andando a ricavare dei parametri tipici quali lo stato di carica (SoC) e lo stato di salute (SoH) e, qualora fosse richiesto, si occupa di loggare e comunicare queste informazioni agli altri blocchi che compongono il sistema.
Nel caso in cui si voglia realizzare un intero sistema avente una batteria agli ioni litio e che presenti un comportamento di tipo Fail-Operational, come quello in esame nel progetto Autodrive, anche il BMS deve avere questa caratteristica. Per questo motivo, partendo dunque dalla struttura convenzionale del BMS, sono state aggiunte strutture ridondanti al fine di raggiungere l’obbiettivo preposto. La ridondanza può essere classificata in due grandi categorie, spaziale e temporale: la prima involve l’introduzione all’interno del sistema di componenti, o funzioni, che sarebbero inutili in ambienti privi di guasti, mentre la seconda è basata sulla ripetizione delle operazioni eseguite e il confronto con il risultato precedente. Specialmente in applicazioni safety-critical, quali quella automotive, la ridondanza di tipo spaziale è necessaria al fine di raggiungere i massimi livelli di sicurezza stabiliti dagli standard, quali ISO 26262.
Per questo motivo la struttura convenzionale del BMS è stata replicata e un’estensione del BMS stesso, che ricopre un ruolo decisionale, è stata sviluppata su una piattaforma FPGA. La scelta di utilizzare un FPGA invece di un microcontrollore, porta numerosi vantaggi, tra i quali un incremento delle capacità computazionali e una maggiore flessibilità e riconfigurabilità del sistema sviluppato. Nel dettaglio è stato scelto di modificare la struttura del BMS andando a sviluppare un sistema in triplice ridondanza nel quale, come suggerisce il nome, si ha una triplicazione parallela del sistema. I tre flussi di dati ottenuti vengono sottoposti a un sistema di voting a maggioranza, dal quale si ottiene un unico flusso di uscita, permettendo dunque di mascherare un guasto su uno dei tre ingressi. A questo scopo sono state sviluppate delle periferiche hardware su FPGA per eseguire le operazioni di voting sui dati e per stimare la regione di lavoro della batteria. È stata inoltre sviluppata una interfaccia grafica su PC, la quale permette di configurare opportunamente le periferiche implementate e mostra in tempo reale tutte le informazioni riguardo alla batteria, tra le quali i valori di tensione delle celle, le temperature e lo stato del sistema. Il sistema è stato infine testato andando a triplicare virtualmente il BMS convenzionale a nostra disposizione e, manipolando i dati su ciascuna linea in modo controllato, è stato verificato il corretto funzionamento delle periferiche sviluppate e validata l’architettura proposta.
#english version#
The growing awareness about the environmental issues is bringing to the attention the pollution topics, especially the greenhouse gas emission. One of the main causes is fossil fuel consumption for the energy production, even in the automotive systems. For this reason, during the last decades greener alternatives have been sought, and the actual trend is the one that leads to the electrical traction, especially towards the autonomous driving system. The thesis work is part of the European project AutoDrive, which aims at designing Fail-Operational electronic components and system architectures, that enables the introduction of automated driving in all car categories to make future mobility more efficient and safer. It is said that a system presents a Fail-Operational behaviour if it continues to execute a defined set of its function even in presence of faults. Given the increasing number of software and mechatronic implementations within the automotive systems, there are increasing risks from systematic failures and random hardware failures that must be considered within the scope of functional safety. Since this last one must be guaranteed, the introduction within the system of some kind of redundancy is mandatory in order to detect or mask the possible faults.
The possibility of developing electrical traction vehicles is closely related to the progress made in the battery field, especially with regards to the Lithium-ion (Li-ion) based technologies. In fact, these types of cells present a higher energy and power density, an higher cell voltage, no memory effect and a lower auto discharge current compared to the other chemistries. Thanks to these advantages, Li-ion based technologies are the only real candidate for the future electrical mobility. However, this chemistry also brings some disadvantages since a battery monitoring system, the Battery Management System (BMS), is mandatory. This system has to ensure the maintenance of the all cells composing the battery pack within their operative ranges in terms of voltages, temperatures and current. In fact, the coming out of one or more of these measures from its Safe Operating Area (SOA) brings a degradation of the cell’s performance, and could also lead to hazardous conditions, such us thermal runaway within the cell itself or even explosions. If the BMS identifies a critical condition for the battery functionality, it acts on the circuit though a feedback system, for example disconnecting the load. The BMS also uses the acquired measures in order to estimate typical battery parameters, such as State of Charge (SoC) and State of Health (SoH), and, if required, it also provides logging functionality and communicates to the other blocks composing the system.
In the case of a system containing a Li-ion battery with Fail-Operational behaviour has to be developed, such as the project AutoDrive one, even the BMS must show this characteristic. For this reason, starting from a conventional BMS, redundant structures have been added to the system in order to reach the responsible goal.
Redundancy can be classified into two main categories: space redundancy and time redundancy. The former involves the introduction within the system of components, or functions, that would be useless in a fault-free environment, while the latter is based on the repetition of the tasks and the comparison of the results to a stored copy of the previous ones. Especially in safety-critical applications, such as automotive, space redundancy is mandatory in order to ensure the safety level required by standards, such as ISO 26262. Therefore, the conventional structure of BMS has been replicated and an extension of BMS itself, which acts as decisional unit, has been developed on a FPGA platform. The FPGA approach, compared to a microcontroller-based one, brings several advantages, such as an increased computational capability and a higher flexibility and reconfigurability of the developed system. More specifically, the conventional structure of the BMS has been modified by using a Triple Modular Redundancy (TMR) approach. As the name suggests, TMR involves the triplication of the components to perform the same computation in parallel. The three obtained data flows are then subjected to a majority voting unit, which provides a single data flow as output, allowing to mask a fault in one of the inputs. For this purpose, hardware peripherals within the FPGA fabric have been developed in order to execute voting and operating area estimation algorithms. Furthermore, a PC graphical user interface has been developed and it allows to configure the hardware peripherals and to show real-time information about the battery pack, such as cell voltages, temperatures and current. The system has been finally tested by virtually triplicating the conventional BMS at our disposal and, manipulating each data flow in a controlled manner, the proper functioning of the developed peripherals has been verified and the proposed architecture has been validated",'Pisa University Press',FPGA Extension of a Battery Management System for Fail-Operational Control of Lithium-Ion Batteries in Safety-Critical Applications,,,,core
334907160,2020-01-28T00:00:00,"Smart homes, enterprises, and cities are increasingly being equipped with a
plethora of Internet of Things (IoT), ranging from smart-lights to security
cameras. While IoT networks have the potential to benefit our lives, they
create privacy and security challenges not seen with traditional IT networks.
Due to the lack of visibility, operators of such smart environments are not
often aware of their IoT assets, let alone whether each IoT device is
functioning properly safe from cyber-attacks. This thesis is the culmination of
our efforts to develop techniques to profile the network behavioral pattern of
IoTs, automate IoT classification, deduce their operating context, and detect
anomalous behavior indicative of cyber-attacks.
  We begin this thesis by surveying IoT ecosystem, while reviewing current
approaches to vulnerability assessments, intrusion detection, and behavioral
monitoring. For our first contribution, we collect traffic traces and
characterize the network behavior of IoT devices via attributes from traffic
patterns. We develop a robust machine learning-based inference engine trained
with these attributes and demonstrate real-time classification of 28 IoT
devices with over 99% accuracy. Our second contribution enhances the
classification by reducing the cost of attribute extraction while also
identifying IoT device states. Prototype implementation and evaluation
demonstrate the ability of our supervised machine learning method to detect
behavioral changes for five IoT devices. Our third and final contribution
develops a modularized unsupervised inference engine that dynamically
accommodates the addition of new IoT devices and/or updates to existing ones,
without requiring system-wide retraining of the model. We demonstrate via
experiments that our model can automatically detect attacks and firmware
changes in ten IoT devices with over 94% accuracy",,IoT Behavioral Monitoring via Network Traffic Analysis,,http://arxiv.org/abs/2001.10632,,core
325934651,2020-06-08T00:00:00,"Travel time estimation is an important component in modern transportation
applications. The state of the art techniques for travel time estimation use
GPS traces to learn the weights of a road network, often modeled as a directed
graph, then apply Dijkstra-like algorithms to find shortest paths. Travel time
is then computed as the sum of edge weights on the returned path. In order to
enable time-dependency, existing systems compute multiple weighted graphs
corresponding to different time windows. These graphs are often optimized
offline before they are deployed into production routing engines, causing a
serious engineering overhead. In this paper, we present STAD, a system that
adjusts - on the fly - travel time estimates for any trip request expressed in
the form of origin, destination, and departure time. STAD uses machine learning
and sparse trips data to learn the imperfections of any basic routing engine,
before it turns it into a full-fledged time-dependent system capable of
adjusting travel times to real traffic conditions in a city. STAD leverages the
spatio-temporal properties of traffic by combining spatial features such as
departing and destination geographic zones with temporal features such as
departing time and day to significantly improve the travel time estimates of
the basic routing engine. Experiments on real trip datasets from Doha, New York
City, and Porto show a reduction in median absolute errors of 14% in the first
two cities and 29% in the latter. We also show that STAD performs better than
different commercial and research baselines in all three cities.Comment: The 21st IEEE International Conference on Mobile Data Management (MDM
  2020",,"STAD: Spatio-Temporal Adjustment of Traffic-Oblivious Travel-Time
  Estimation",,http://arxiv.org/abs/2006.09892,,core
426867658,2020-10-01T00:00:00,"Acoustic pollution has been associated with adverse effects on the health and life expectancy of people, especially when noise exposure happens during the nighttime. With over half of the world population living in urban areas, acoustic pollution is an important concern for city administrators, especially those focused on transportation and leisure noise. Advances in sensor and network technologies made the deployment of Wireless Acoustic Sensor Networks (WASN) possible in cities, which, combined with artificial intelligence (AI), can enable smart services for their citizens. However, the creation of such services often requires structured environmental audio databases to train AI algorithms. This paper reports on an environmental audio dataset of 363 min and 53 s created in a lively area of the Barcelona city center, which targeted traffic and leisure events. This dataset, which is free and publicly available, can provide researchers with real-world acoustic data to help the development and testing of sound monitoring solutions for urban environments",'MDPI AG',BCNDataset: Description and Analysis of an Annotated Night Urban Leisure Sound Dataset,,,,core
322449666,2020-04-30T00:00:00,"In this white paper we provide a vision for 6G Edge Intelligence. Moving
towards 5G and beyond the future 6G networks, intelligent solutions utilizing
data-driven machine learning and artificial intelligence become crucial for
several real-world applications including but not limited to, more efficient
manufacturing, novel personal smart device environments and experiences, urban
computing and autonomous traffic settings. We present edge computing along with
other 6G enablers as a key component to establish the future 2030 intelligent
Internet technologies as shown in this series of 6G White Papers.
  In this white paper, we focus in the domains of edge computing infrastructure
and platforms, data and edge network management, software development for edge,
and real-time and distributed training of ML/AI algorithms, along with
security, privacy, pricing, and end-user aspects. We discuss the key enablers
and challenges and identify the key research questions for the development of
the Intelligent Edge services. As a main outcome of this white paper, we
envision a transition from Internet of Things to Intelligent Internet of
Intelligent Things and provide a roadmap for development of 6G Intelligent
Edge",,6G White Paper on Edge Intelligence,,http://arxiv.org/abs/2004.14850,,core
389455849,2020-01-01T00:00:00,"Il volume, ripercorrendo quattro casi italiani e catalani, affronta alcune questioni di fondo legate ai sistemi di attori che caratterizzano le politiche locali di rigenerazione urbana temporanea e approfondisce il rapporto tra le imprese e gli altri agenti coinvolti nei processi, arrivando ad ipotizzare che l’orientamento collaborativo – dell’organizzazione, della produzione e della progettazione - dia vita a pratiche di successo in presenza di determinate condizioni e di specifiche figure, qui definite ‘enzimi sociali’, in grado non solo di diffondere, sintetizzare e trasferire informazioni, come tipicamente fanno i broker della conoscenza, ma anche di farle ‘lievitare’ attraverso processi di confronto e scambio in grado di implementarne il valore e le possibilità di impiego.Recently, public policies of urban regeneration have intensified and multiplied. They are being promoted with the aim to start social and economic dynamics within the local context which is subject to intervention. From the empirical analysis, we realise that such activities are mainly implemented by three subjects or by mixed coalitions (public institutions, actors of the third sector and companies). Within them, each player is moved by a multiplicity of interests and goals that go beyond their own nature – public interest, market and mutualism – and tend to redefine themselves, thus becoming hybrid forms of production of value (social, economic, cultural). 
By studying a number Italian and Catalan cases, this essay deals with the theory that, under specific conditions and configurations, a collaborative direction – of organization, production and design – would give life to successful procedures, even without the identification of a one-best-way. 
The collaboration is not simply a choice of operation, but a real production method which mobilises social resources to create hybrid solutions – between state, market and society – to complex issues that could not be faced solely with the use of the rationale of action of one among the three actors. In this framework, the systems of relations and interactions between players and shared capital become an essential condition for the success of every initiative of urban redevelopment, or failure thereof. Such initiatives are brought to life by the strategic role of individuals who foster connections as well as the dissemination of non-redundant information between social networks, and collective and individual actors which would otherwise be separated and barely able to communicate and collaborate with each other. In addition to the functions carried out by knowledge brokers, that have been extensively described in organisational studies and economic sociology, the aforementioned figures act as real social enzymes, that is to say, they handle the available information and function as catalysts of social processes of production of knowledge. 
Moreover, they increase the reaction speed, working on mechanisms which control the spontaneity",PHAIDRA University of Padova,Collaboration Age. Enzimi sociali all’opera in esperienze di rigenerazione urbana temporanea,,,,core
287466012,2020-01-31T00:00:00,"We consider the problem of an operator controlling a fleet of electric vehicles for use in a ridehailing service. The operator, seeking to maximize revenue, must assign vehicles to requests as they arise and recharge and reposition vehicles in anticipation of future requests. To solve this problem, we employ deep reinforcement learning, developing policies whose decision making uses Q-value approximations learned by deep neural networks. We compare these policies against a common taxi dispatching heuristic and against dual bounds on the value of an optimal policy, including the value of an optimal policy with perfect information which we establish using a Benders-based decomposition. We assess performance on instances derived from real data for the island of Manhattan in New York City. We find that, across instances of varying size, our best policy trained with deep reinforcement learning outperforms the taxi dispatching heuristic. We also provide evidence that this policy may be effectively scaled and deployed on larger instances without retraining",HAL CCSD,Dynamic Ridehailing with Electric Vehicles,,https://core.ac.uk/download/287466012.pdf,,core
334866111,2020-02-04T00:00:00,"The smooth operation of largely deployed Internet of Things (IoT)
applications will depend on, among other things, effective infrastructure
failure detection. Access failures in wireless network Base Stations (BSs)
produce a phenomenon called ""sleeping cells"", which can render a cell catatonic
without triggering any alarms or provoking immediate effects on cell
performance, making them difficult to discover. To detect this kind of failure,
we propose a Machine Learning (ML) framework based on the use of Key
Performance Indicator (KPI) statistics from the BS under study, as well as
those of the neighboring BSs with propensity to have their performance affected
by the failure. A simple way to define neighbors is to use adjacency in Voronoi
diagrams. In this paper, we propose a much more realistic approach based on the
nature of radio-propagation and the way devices choose the BS to which they
send access requests. We gather data from large-scale simulators that use real
location data for BSs and IoT devices and pose the detection problem as a
supervised binary classification problem. We measure the effects on the
detection performance by the size of time aggregations of the data, the level
of traffic and the parameters of the neighborhood definition. The Extra Trees
and Naive Bayes classifiers achieve Receiver Operating Characteristic (ROC)
Area Under the Curve (AUC) scores of 0.996 and 0.993, respectively, with False
Positive Rate (FPR) under 5 %. The proposed framework holds potential for other
pattern recognition tasks in smart-city wireless infrastructures, that would
enable the monitoring, prediction and improvement of the Quality of Service
(QoS) experienced by IoT applications.Comment: Submitted to the IEEE Access Journa",,"A Machine Learning framework for Sleeping Cell Detection in a Smart-city
  IoT Telecommunications Infrastructure",,http://arxiv.org/abs/1910.01092,,core
342060224,2020-01-01T00:00:00,"Tensegrity structures are an emergent type of soft-robotics that are compliant, lightweight, and impact-resilient. In collaboration with NASA Ames Research Center, research in the Berkeley Emergent Space Tensegrities Lab at UC Berkeley has largely focused on the design and control of these novel structures as potential surface exploration robots which could act as both landers and rovers. More recently, tensegrity robots have also been proposed for applications closer to home – working as disaster response and emergency co-robots to help first responders obtain situational awareness faster and safer. Constructed using isolated rigid bodies suspended in a tension network of elastic elements, tensegrity structures exhibit unique and advantageous mechanical properties for applications in uncertain and potentially hazardous environments, albeit at the cost of increased complexity for dynamic feedback control.  In addressing these challenges, this work explores possible approaches for feedback control and state estimation for ground-based rolling locomotion with six-bar spherical tensegrities. In this dissertation, we explore problems pertaining to practical implementation – state estimation, modeling, motion planning, and optimal control of tensegrity robots under uncertainty. Leveraging the well-structured dynamics of Class-1 tensegrity robots, we implement and evaluate model-based Model Predictive Control and iterative local quadratic methods for tensegrity motion planning. Additionally, we consider alternative tensegrity topologies and actuator schema which may enable improved performance for task-specific objectives. Due to the many degrees of freedom and compliant nature of tensegrity structures, however, excessive state estimate errors may propagate catastrophically. To evaluate these effects, Bayesian state estimators are applied to tensegrity ground mobility in simulation, evaluating their performance under the additional constraints of low-cost sensors and potentially scarce and noisy sensor data. An imitation learning approach is introduced to achieve directed rolling motion using a contextual neural network policy, combining deep learning and optimal control for real-time feedback control of highly nonlinear tensegrity systems. Finally, a robust minimax control approach is proposed in order to address challenges which arise at the intersection and interaction of state estimation and trajectory optimization for flexible tensegrity robotics.  Combined, these pragmatic research developments help advance the progression of this novel technology towards becoming a viable and more widely adopted robotics paradigm","eScholarship, University of California","Design, Control, and Motion Planning of Cable-Driven Flexible Tensegrity Robots",,,,core
345026560,2020-06-15T19:08:25,"Signal coordination has been widely implemented throughout the world as an effective approach to mitigating traffic congestion, improving operational efficiency on urban arterials, and reducing fuel consumption and emissions. Therefore, developing and implementing efficient signal timing plans is of significant importance. In the U.S., there are around 323,000 traffic signals currently operating in the nation’s roadway system. According to the signal timing manual, more than half of them need signal timing adjustments, and there is still a need for improvements in signal coordination plans. Inadequately implemented signal coordination plans may result in unexpected traffic delays, congestion, and even safety issues. Therefore, it has been a crucial task for transportation agencies to properly implement signal coordination plans. Multiple factors may influence the effects of signal coordination. Among these factors, progression speed is one of the most important, but its effects on signal coordination have not been thoroughly investigated. In this regard, this dissertation aims to develop a systematic approach to determine the optimal progression speeds under different conditions during the timing development stage.  A major task of this research involved the development of an analytical model for determining progression speed. This model served as a foundation for developing a practical guide, as well as conducting field and simulation analyses. To achieve maximum efficiency, it is necessary to associate the progression speed and the traffic flow travel speed, which can be directly obtained through field studies. A significant amount of field data were collected and employed to derive the relationship between the average speed (an estimation to the speed of the platoon, which is also proximate to the optimal progression speed), and its influencing parameters. Three machine learning methods (Scikit-learn, TensorFlow, and Genetic Programming), and a traditional method (Linear Regression), were used to develop the models. Independent parameters that may contribute to average speed were considered, including segment spacing, grade, speed limit, volume, number of lanes, vehicle status at the upstream signal, and vehicle status at the downstream signal. The relationships between average speed and independent parameters were established. The results of the theoretical model are very sensitive when they are used in the field. To produce the results that can be applied in practice, a series of microscopic simulations were designed in VISSIM, a widely used commercial software package in the field of transportation engineering. Various scenarios were carefully designed before conducting the simulation experiment to ensure valid results . A sensitivity test was conducted to capture the impacts of various factors on traffic performance. Data from four signalized arterials in Reno, Nevada were used for the VISSIM simulation. The results of the study showed that the proposed models accurately reflected the relationship between vehicle travel speed and related influencing factors. A general conclusion was that when the traffic volume was low, e.g., volume to capacity ratio less than 0.7, using a progression speed slightly higher than the posted speed limit yielded better performance. When the volume was high, e.g., volume to capacity ratio larger than 0.7, a progression speed set at the posted speed limit was adequate. Deploying a progression speed lower than the posted speed limit was not advised. However, deploying a progression speed higher than the posted speed limit was not allowed since it violated the safety constraint. To avoid safety concerns, the progression speed should be equal to the posted speed limit. As a result, the posted speed limit is recommended as the progression speed as long as the real platoon speed is not significantly different than the posted speed limit, if the traffic platoon speed largely deviates from the posted speed limit, the real platoon speed should be considered to be used. Finally, the recommended progression speed guidance was tested in a real-world setting. New signal coordination plans were generated using the posted speed limit as the progression speed versus before timing generated by the investigated speed. Traffic performance data were collected before and after the implementation of the new signal coordination plans. The before-after comparison of results indicated that the new timing plans had significant improvements over the previous ones, further proving the validity of the proposed progression speed guide for practical applications",,Determination of Progression Speeds for Traffic Signal Coordination,,,,core
387278843,2020-11-15T00:00:00,"The widescale deployment of Autonomous Vehicles (AV) appears to be imminent
despite many safety challenges that are yet to be resolved. It is well-known
that there are no universally agreed Verification and Validation (VV)
methodologies guarantee absolute safety, which is crucial for the acceptance of
this technology. The uncertainties in the behaviour of the traffic participants
and the dynamic world cause stochastic reactions in advanced autonomous
systems. The addition of ML algorithms and probabilistic techniques adds
significant complexity to the process for real-world testing when compared to
traditional methods. Most research in this area focuses on generating
challenging concrete scenarios or test cases to evaluate the system performance
by looking at the frequency distribution of extracted parameters as collected
from the real-world data. These approaches generally employ Monte-Carlo
simulation and importance sampling to generate critical cases. This paper
presents an efficient falsification method to evaluate the System Under Test.
The approach is based on a parameter optimisation problem to search for
challenging scenarios. The optimisation process aims at finding the challenging
case that has maximum return. The method applies policy-gradient reinforcement
learning algorithm to enable the learning. The riskiness of the scenario is
measured by the well established RSS safety metric, euclidean distance, and
instance of a collision. We demonstrate that by using the proposed method, we
can more efficiently search for challenging scenarios which could cause the
system to fail in order to satisfy the safety requirements.Comment: Submitted to IEEE Transactions on Intelligent Transportation System",,"Efficient falsification approach for autonomous vehicle validation using
  a parameter optimisation technique based on reinforcement learning",,http://arxiv.org/abs/2011.07699,,core
387283859,2020-11-22T00:00:00,"Large software systems tune hundreds of 'constants' to optimize their runtime
performance. These values are commonly derived through intuition, lab tests, or
A/B tests. A 'one-size-fits-all' approach is often sub-optimal as the best
value depends on runtime context. In this paper, we provide an experimental
approach to replace constants with learned contextual functions for Skype - a
widely used real-time communication (RTC) application. We present Resonance, a
system based on contextual bandits (CB). We describe experiences from three
real-world experiments: applying it to the audio, video, and transport
components in Skype. We surface a unique and practical challenge of performing
machine learning (ML) inference in large software systems written using
encapsulation principles. Finally, we open-source FeatureBroker, a library to
reduce the friction in adopting ML models in such development environmentsComment: Workshop on ML for Systems at NeurIPS 2020, Accepte",,"Resonance: Replacing Software Constants with Context-Aware Models in
  Real-time Communication",,http://arxiv.org/abs/2011.12715,,core
360327804,2020-10-21T00:00:00,"capacity and the low cost of the Cloud have facilitated the development of new, powerful
algorithms. The efficiency of these algorithms in Big Data processing, Deep Learning and
Convolutional Networks is transforming the way we work and is opening new horizons. Thanks
to them, we can now analyse data and obtain unimaginable solutions to today’s problems.
Nevertheless, our success is not entirely based on algorithms, it also comes from our ability to
follow our “gut” when choosing the best combination of algorithms for an intelligent artefact.
Their development involves the use of both connectionist and symbolic systems, that is to say
data and knowledge. Moreover, it is necessary to work with both historical and real-time data. It
is also important to consider development time, costs and the ability to create systems that will
interact with their environment, will connect with the objects that surround them and will
manage the data they obtain in a reliable manner.
In this keynote, the evolution of intelligent computer systems will be examined, especially that
of convolutional networks. The need for human capital will be discussed, as well as the need to
follow one’s “gut instinct” in problem-solving.
Furthermore, the importance of IoT and Blockchain in the development of intelligent systems
will be analysed and it will be shown how tools like ""Deep Intelligence"" make it possible to create
computer systems efficiently and effectively. ""Smart"" infrastructures need to incorporate all
added-value resources so they can offer useful services to the society, while reducing costs,
ensuring reliability and improving the quality of life of the citizens. The combination of AI with
IoT and with blockchain offers a world of possibilities and opportunities.
The development of transport, smart cities, urbanizations and leisure areas can be improved
through the use of distributed intelligent computer systems. In this regard, edge platforms or fog
computing help increase efficiency, reduce network latency, improve security and bring
intelligence to the edge of the network, the sensors, users and the environment.
Several use cases of intelligent systems will be presented, and it will be analysed how the
processes of implementation and use have been optimized by means of different tools",,Efficiency and Reliability in Bringing AI into Transport and Smart City Solutions,,,,core
344911413,2020-06-30T00:00:00,"Abstract

In this white paper, we provide a vision for 6G edge intelligence. Moving toward 5G and beyond future 6G networks, intelligent solutions utilizing data-driven machine learning and artificial intelligence will become crucial for several real-world applications, including but not limited to more efficient manufacturing, novel personal smart device environments and experiences, urban computing, and autonomous traffic settings. We sent edge computing with other 6G enablers as a key component to establish the future 2030 intelligent Internet technologies shown in this series of 6G white papers.

In this white paper, we focus on the domains of edge-computing infrastructure and platforms, data and edge network management, software development for edge, and real-time and distributed training of ML/AI algorithms, as well as security, privacy, pricing, and end-user aspects. We discuss the key enablers and challenges, and identify the key research questions for the development of intelligent edge services. As the main outcome of this white paper, we envision a transition from the Internet of Things to the Intelligent Internet of Intelligent Things and provide a roadmap for the development of the 6G intelligent edge",Oulun yliopisto,6G white paper on edge intelligence,,,,core
389316132,2020-01-01T00:00:00,"This work aims at unveiling the potential of Transfer Learning (TL) for developing a traffic flow forecasting model in scenarios of absent data. Knowledge transfer from high-quality predictive models becomes feasible under the TL paradigm, enabling the generation of new proper models with few data. In order to explore this capability, we identify three different levels of data absent scenarios, where TL techniques are applied among Deep Learning (DL) methods for traffic forecasting. Then, traditional batch learning is compared against TL based models using real traffic flow data, collected by deployed loops managed by the City Council of Madrid (Spain). In addition, we apply Online Learning (OL) techniques, where model receives an update after each prediction, in order to adapt to traffic flow trend changes and incrementally learn from new incoming traffic data. The obtained experimental results shed light on the advantages of transfer and online learning for traffic flow forecasting, and draw practical insights on their interplay with the amount of available training data at the location of interest",'Institute of Electrical and Electronics Engineers (IEEE)',Transfer Learning and Online Learning for Traffic Forecasting under Different Data Availability Conditions: Alternatives and Pitfalls,10.1109/itsc45102.2020.9294557.,,,core
395096888,2020-02-09T00:00:00,"Freight transportation industry is characterized by several decisional problems that operations managers have to cope with. Not only the routes planning must be realized before their execution, but also other types of decisions must be taken, in order to answer events that may dynamically occur during operations, as for instance road network congestion or vehicle

failures. Each decision can involve different aspects: for instance, the price negotiation of a just-in-time order should take into consideration the current routes status and planning. Off-the-shelf decision support software, although able to independently support the decision makers in each area, tend to keep tasks compartmentalized.

Trans-Cel, a small trucking company in Padova (Italy), has a Research and Development branch developing a cloud-based platform, called Chainment, able to host different decision support tools that can communicate through a data sharing system. These tools rely on an algorithmic engine that includes a routing optimization algorithm and artificial intelligence systems. In particular, the routing problem combines express couriers requirements, generally studied in urban contexts, with routes and vehicle features typical of medium- and long-haul trips, showing interesting characteristics that are worth of study in the Operation Research field.

In this thesis, we focus on the design of an optimization algorithm able to provide a solution to a Vehicle Routing Problem (VRP) inspired by the Trans-Cel scenario, that we name Express Pickup and Delivery in freight Trucking problem (EPDT).

The classical VRP definition includes a set of customers and a fleet of vehicles and aims to define a set of routes such that all customers are visited exactly once while minimizing the overall distance traveled. In the scientific literature, the basic definition of the problem has been generalized in order to consider additional attributes, often rising from real-world scenarios, as for instance capacity of vehicles, time windows and orders with both pickup and delivery operations. Often, in real-world cases, decision makers must simultaneously deal with a large number of attributes, thus defining a class of routing problems called Multi-Attribute VRP (MAVRP), which includes EPDT.

The thesis proposes a meta-heuristic algorithm for the solution of EPDT, with the aim of embedding it in the algorithmic engine of Chainment. In order to comply with the platform  requirements, the algorithm is designed so that a solution is returned within few seconds.

The solution method we propose consists of a two-level heuristic: at the first level, a Tabu Search algorithm hybridized with a Variable Neighborhood Descent explores the order-to-vehicle assignments, while, at the second level, it makes use of a Local Search to determine the sequence of customers visited and obtain an evaluation of routes.

The algorithm efficiency is enhanced by the use of a granular exploration, by procedures for fast evaluation of solutions in the neighborhoods, and parallel implementation of specific algorithmic components. These elements are adapted to the specific attributes of EPDT and represent some of the thesis contributions. The improvement in computational times have been validated by the experimental results, verifying the desired requirements for the platform integration.

The quality of the solutions obtained with the proposed meta-heuristic algorithm has been assessed both on the field, by comparison with Trans-Cel operations managers, and through bounds obtained with mathematical programming methods. To this purpose, the thesis proposes an Integer Linear Programming formulation for EPDT and a solution method for its continuous

relaxation based on Column Generation. In particular, the thesis presents new pricing procedures suitable for the specific EPDT attributes. The available bounds show optimality or near-optimality of solutions provided by the heuristic algorithm for real instances. Moreover, the algorithm has been tested on literature benchmarks related to the Pickup and Delivery

Problem with Time Windows (PDPTW), providing solutions that are competitive with the state-of-the-art.

The thesis also proposes a preliminary study of new approaches for vehicle routing problems in dynamic contexts. In particular, the thesis explores the possibility of taking advantage from the availability of historical data on orders by means of anticipatory strategies. The first strategy is based on clustering methods that are applied to the orders to define space-time

points that aggregate the information on future demand. A second strategy is based on the concept of accessibility, as defined in the discrete choice theory and urban logistic, to represent the route capability of intercepting future orders.

The heuristic algorithm proposed for EPDT has been integrated in the algorithmic engine of the Chainment platform at Trans-Cel. The thesis describes integration and the adaptation of the proposed optimization algorithms for a proper interaction with the different modules in the operational context handled by the platform, as, for instance, initial routes planning, reacting to dynamic events or order price negotiation",,Solving a Multi-Attribute Vehicle Routing Problem in the freight delivery industry,,,,core
369423085,2020-12-23T00:00:00,"Most of the vehicle manufacturers aim to deploy level-5 fully autonomous ground vehicles (FAGVs) on city roads in 2021 by leveraging extensive existing knowledge about sensors, actuators, telematics and Artificial Intelligence (AI) gained from the level-3 and level-4 autonomy. FAGVs by executing non-trivial sequences of events with decimetre-level accuracy live in Smart City (SC) and their integration with all the SC components and domains using real-time data analytics is urgent to establish better swarm intelligent systems and a safer and optimised harmonious smart environment enabling cooperative FAGVs-SC automation systems. The challenges of urbanisation, if unmet urgently, would entail severe economic and environmental impacts. The integration of FAGVs with SC helps improve the sustainability of a city and the functional and efficient deployment of hand over wheels on robotized city roads with behaviour coordination. SC can enable the exploitation of the full potential of FAGVs with embedded centralised systems within SC with highly distributed systems in a concept of Automation of Everything (AoE). This paper proposes a synergistic integrated FAGV-SC holistic framework - FAGVinSCF in which all the components of SC and FAGVs involving recent and impending technological advancements are moulded to make the transformation from today's driving society to future's next-generation driverless society smoother and truly make self-driving technology a harmonious part of our cities with sustainable urban development. Based on FAGVinSCF, a simulation platform is built both to model the varying penetration levels of FAGV into mixed traffic and to perform the optimal self-driving behaviours of FAGV swarms. The results show that FAGVinSCF improves the urban traffic flow significantly without huge changes to the traffic infrastructure. With this framework, the concept of Cooperative Intelligent Transportation Systems (C-ITS) is transformed into the concept of Automated ITS (A-ITS). Cities currently designed for cars can turn into cities developed for citizens using FAGVinSCF enabling more sustainable cities",'Institute of Electrical and Electronics Engineers (IEEE)',A framework for the synergistic integration of fully autonomous ground vehicles with smart city,10.1109/ACCESS.2020.3046999,https://core.ac.uk/download/369423085.pdf,,core
287607770,2020-01-15T00:00:00,"In the fifth-generation (5G) mobile networks, the traffic is estimated to have a fast-changing and imbalance spatial-temporal distribution. It is challenging for a system-level optimisation to deal with while empirically maintaining quality of service. The 5G load balancing aims to address this problem by transferring the extra traffic from a high-load cell to its neighbouring idle cells. In recent literature, controller and machine learning algorithms are applied to assist the self-optimising and proactive schemes in drawing load balancing decisions. However, these algorithms lack the ability of forecasting upcoming high traffic demands, especially during popular events. This shortage leads to cold-start problems because of reacting to the changes in the heterogeneous dense deployment. Notably, the hotspots corresponding with skew load distribution will result in low convergence speed. To address these problems, this paper contributes to three aspects. Firstly, urban event detection is proposed to forecast the changes in cellular hotspots based on Twitter data for enabling context-awareness. Secondly, a proactive 5G load balancing strategy is simulated considering the prediction of the skewed-distributed hotspots in urban areas. Finally, we optimise this context-aware proactive load balancing strategy by forecasting the best activation time. This paper represents one of the first works to couple the real-world urban event detection with proactive load balancing",'Institute of Electrical and Electronics Engineers (IEEE)',Context-aware proactive 5G load balancing and optimization for urban areas,10.1109/access.2020.2964562,https://core.ac.uk/download/287607770.pdf,,core
323446598,2020-01-01T00:00:00,"abstract: Robotic lower limb prostheses provide new opportunities to help transfemoral amputees regain mobility. However, their application is impeded by that the impedance control parameters need to be tuned and optimized manually by prosthetists for each individual user in different task environments. Reinforcement learning (RL) is capable of automatically learning from interacting with the environment. It becomes a natural candidate to replace human prosthetists to customize the control parameters. However, neither traditional RL approaches nor the popular deep RL approaches are readily suitable for learning with limited number of samples and samples with large variations. This dissertation aims to explore new RL based adaptive solutions that are data-efficient for controlling robotic prostheses.

This dissertation begins by proposing a new flexible policy iteration (FPI) framework. To improve sample efficiency, FPI can utilize either on-policy or off-policy learning strategy, can learn from either online or offline data, and can even adopt exiting knowledge of an external critic. Approximate convergence to Bellman optimal solutions are guaranteed under mild conditions. Simulation studies validated that FPI was data efficient compared to several established RL methods. Furthermore, a simplified version of FPI was implemented to learn from offline data, and then the learned policy was successfully tested for tuning the control parameters online on a human subject.

Next, the dissertation discusses RL control with information transfer (RL-IT), or knowledge-guided RL (KG-RL), which is motivated to benefit from transferring knowledge acquired from one subject to another. To explore its feasibility, knowledge was extracted from data measurements of able-bodied (AB) subjects, and transferred to guide Q-learning control for an amputee in OpenSim simulations. This result again demonstrated that data and time efficiency were improved using previous knowledge.

While the present study is new and promising, there are still many open questions to be addressed in future research. To account for human adaption, the learning control objective function may be designed to incorporate human-prosthesis performance feedback such as symmetry, user comfort level and satisfaction, and user energy consumption. To make the RL based control parameter tuning practical in real life, it should be further developed and tested in different use environments, such as from level ground walking to stair ascending or descending, and from walking to running.Dissertation/ThesisDoctoral Dissertation Electrical Engineering 202",,Data-Efficient Reinforcement Learning Control of Robotic Lower-Limb Prosthesis With Human in the Loop,,,,core
334931966,2020-04-13T00:00:00,"Multivariate geo-sensory time series prediction is challenging because of the
complex spatial and temporal correlation. In urban water distribution systems
(WDS), numerous spatial-correlated sensors have been deployed to continuously
collect hydraulic data. Forecasts of monitored flow and pressure time series
are of vital importance for operational decision making, alerts and anomaly
detection. To address this issue, we proposed a hybrid dual-stage
spatial-temporal attention-based recurrent neural networks (hDS-RNN). Our model
consists of two stages: a spatial attention-based encoder and a temporal
attention-based decoder. Specifically, a hybrid spatial attention mechanism
that employs inputs along temporal and spatial axes is proposed. Experiments on
a real-world dataset are conducted and demonstrate that our model outperformed
9 baseline models in flow and pressure series prediction in WDS.Comment: 7 pages, 9 figure",,"Hybrid Attention Networks for Flow and Pressure Forecasting in Water
  Distribution Systems",,http://arxiv.org/abs/2004.05828,,core
305119823,2020-03-01T00:00:00,"The future of urban mobility is expected to be shared and electric. It is not only a more sustainable paradigm that can reduce emissions, but can also bring societal benefits by offering a more affordable on-demand mobility option to the general public. Many car sharing service providers as well as automobile manufacturers are entering the competition by expanding both their EV fleets and renting/returning station networks, aiming to seize a share of the market and to bring car sharing to the zero emissions level. During their fast expansion, one determinant for success is the ability of predicting the demand of stations as the entire system is growing continuously. There are several challenges in this demand prediction problem: First, unlike most of the existing work which predicts demand only for static systems or at few stages of expansion, in the real world we often need to predict the demand as or even before stations are being deployed or closed, to provide information and decision support. Second, for the new stations to be deployed, there is no historical data available to help the prediction of their demand. Finally, the impact of deploying/closing stations on the other stations in the system can be complex. To address these challenges, we formulate the demand prediction problem in the context of fast expanding electric vehicle sharing systems, and propose a data-driven demand prediction approach which aims to model the expansion dynamics directly from the data. We use a local temporal encoding process to handle the historical data for each existing station, and a dynamic spatial encoding process to take correlations between stations into account with Graph Convolutional Neural Networks (GCN). The encoded features are fed to a multi-scale predictor, which forecasts both the long-term expected demand of the stations and their instant demand in the near future. We evaluate the proposed approach with real-world data collected from a major EV sharing platform for one year. Experimental results demonstrate that our approach significantly outperforms the state of the art, showing up to three-fold performance gain in predicting demand for the expanding EV sharing systems",'Association for Computing Machinery (ACM)',D3P : Data-driven demand prediction for fast expanding electric vehicle sharing systems,10.1145/3381005,https://core.ac.uk/download/305119823.pdf,,core
402913130,2020-10-09T08:35:20,"Driver Less Vision examines the tension and reality of AI and humans merging and diverging as they negotiate Seoul's unique urban landscape—challenging us to consider how we can design cities for the future of autonomous vehicles. 

Driver Less Vision aims to generate empathy between humans and non-humans, to construct the trust required for negotiations that will settle how we will live together. By overlapping human and machine’s perceptions, the installation helps to identify the areas of the city that will need to be redesigned in the immediate future.

Driver Less Vision is the immersive experience of becoming an autonomous, self-driving vehicle. It explores the untapped conflicts and disruptive effects on the built environment caused by the deployment of technologies for autonomous mobility. Currently, the visual stimuli that organizes traffic is designed for human perception. The arrival of driverless cars entails the emergence of a omnidirectional gaze that is required to negotiate existing visual codes. To assume that driverless cars will fully adapt to future conditions of the city, however, neglects the history of transformations in urban streetscapes associated with changes in vehicular technologies. Driver Less Vision is an attempt to understand how driverless cars will change the city by immersing the audience in an urban journey through the car’s point of view, seeing the streets of Seoul through overlapping and dissonant perceptions. 

The project was produced for the Seoul Biennale of Architecture and Urbanism in 2017, utilizing an eight meter diameter dome with 360 visuals developed with the generous support of University of Technology Sydney, Rice University and Ocular Robotics",Mediabus,Driver Less Vision,,http://hdl.handle.net/10453/143196,,core
200800387,2020-03-15T00:00:00,"Advances in deep neural networks (DNN) and computer vision (CV) algorithms
have made it feasible to extract meaningful insights from large-scale
deployments of urban cameras. Tracking an object of interest across the camera
network in near real-time is a canonical problem. However, current tracking
platforms have two key limitations: 1) They are monolithic, proprietary and
lack the ability to rapidly incorporate sophisticated tracking models; and 2)
They are less responsive to dynamism across wide-area computing resources that
include edge, fog and cloud abstractions. We address these gaps using Anveshak,
a runtime platform for composing and coordinating distributed tracking
applications. It provides a domain-specific dataflow programming model to
intuitively compose a tracking application, supporting contemporary CV advances
like query fusion and re-identification, and enabling dynamic scoping of the
camera network's search space to avoid wasted computation. We also offer
tunable batching and data-dropping strategies for dataflow blocks deployed on
distributed resources to respond to network and compute variability. These
balance the tracking accuracy, its real-time performance and the active
camera-set size. We illustrate the concise expressiveness of the programming
model for $4$ tracking applications. Our detailed experiments for a network of
1000 camera-feeds on modest resources exhibit the tunable scalability,
performance and quality trade-offs enabled by our dynamic tracking, batching
and dropping strategies",,"A Scalable Platform for Distributed Object Tracking across a Many-camera
  Network",,http://arxiv.org/abs/1902.05577,,core
357554258,2020-04-24T00:00:00,"ABSTRACT In robotics research, perception is one of the most challenging tasks. In contrast to existing approaches that rely only on computer vision, we propose an alternative method for improving perception by learning from human teammates. To evaluate, we apply this idea to a door detection problem. A set of preliminary experiments has been completed using software agents with real vision data. Our results demonstrate that information inferred from teammate observations significantly improves the perception precision. Categories and Subject Descriptors I.2.11 [Distributed Artificial Intelligence]: Intelligent agents General Terms Human Factors Keywords Robot perception, robot-human hybrid teams BACKGROUND Robot perception is generally formulated as a problem of analyzing and interpreting various sensory inputs, e.g., camera feeds. In this paper, we approach robot perception from a completely different direction. Our approach utilizes a team setting where a robot collaborates with human teammates. Motivated by the fact that humans possess superior perception skills relative to their robotic counterparts, we investigate how a robot can take advantage of its teammate&apos;s perfect vision. In general, an agent acquires new information through perception, and in turn, the agent chooses actions based on the information acquired. Let us suppose that a robot has a mental model of its human teammate such that a causal relationship is specified between information and actions. Then, by understanding the human mental model of such decision making (or planning), the robot can infer what the human teammate has seen based on the human&apos;s behavior. In other words, an observation of a human teammate can be * This work was conducted (in part) through collaborative participation in the Robotics Consortium sponsored by the U. used as evidence to infer the information perceived by the human. This, in turn, can be used to reduce uncertainty in robot perception. In this paper, we specifically focus on a motivating problem of door detection in the following scenario. Consider a team consisting of a robot and a human performing a military operation in a hostile environment. According to intelligence, armed insurgents are hiding in an urban street. The team is deployed to cover the buildings in the surrounding area, focusing on doors from which the insurgents may try to egress. This is a stealth operation. We make two specific assumptions that are reasonable in a team context. First, observing a teammate is generally more manageable than perceiving an unfamiliar environment. Second, team members share common objectives in reaching the team&apos;s goals. PERCEPTION USING VISION This section describes a purely camera-based approach. First, we find a likely semantic image segmentation using a computer vision technique called stacked hierarchical labeling  It is not constrained by shape grammars and can model a more general class of objects, but its method of constructing a hierarchical segmentation does not convey semantic meaning at a finer detail, as would be necessary to detect doors on a building. It is, however, reliable in detecting buildings as a whole, significantly reducing the search space for detecting doors in the next step. Once buildings are identified, we can apply a broad feature detector to detect likely openings on the façade of the building. As i",,Enhancing Robot Perception Using Human Teammates * (Extended Abstract),,https://core.ac.uk/download/357554258.pdf,,core
429122025,2020-03-10T00:00:00,"[EN] This article shows a novel geo-visualization method of dynamic spatiotemporal data that allows mobility and concentration of criminal activity to be study. The method was developed using, only and significantly, real data of Santiago de Cali (Colombia), collected by the Colombian National Police (PONAL). This method constitutes a tool that allows criminal influx to be analyzed by concentration, zone, time slot and date. In addition to the field experience of police commanders, it allows patterns of criminal activity to be detected, thereby enabling a better distribution and management of police resources allocated to crime deterrence, prevention and control. Additionally, it may be applied to the concepts of safe city and smart city of the PONAL within the architecture of Command and Control System (C2S) of Command and Control Centers for Public Safety. Furthermore, it contributes to a better situational awareness and improves the future projection, agility, efficiency and decision-making processes of police officers, which are all essential for fulfillment of police missions against crime. Finally, this was developed using an open source software, it can be adapted to any other city, be used with real-time data and be implemented, if necessary, with the geographic software of any other C2S.This work was co-funded by the European Commission as part of H2020 call SEC-12-FCT-2016-thrtopic3 under the project VICTORIA (No. 740754). This publication reflects the views only of the authors, and the Commission cannot be held responsible for any use which may be made of the information contained therein. The authors would like to thank Colombian National Police and its Office of Telematics for their support on development of this project.Salcedo-González, ML.; Suarez-Paez, JE.; Esteve Domingo, M.; Gomez, J.; Palau Salvador, CE. (2020). A Novel Method of Spatiotemporal Dynamic Geo-Visualization of Criminal Data, Applied to Command and Control Centers for Public Safety. ISPRS International Journal of Geo-Information. 9(3):1-17. https://doi.org/10.3390/ijgi9030160S1179",'MDPI AG',"A Novel Method of Spatiotemporal Dynamic Geo-Visualization of Criminal Data, Applied to Command and Control Centers for Public Safety",10.3390/ijgi9030160,http://hdl.handle.net/10251/166659,,core
475286366,2020-08-25T07:00:00,"Autonomous vehicles have the potential to completely upend the way we transport today, however deploying them safely at scale is not an easy task. Any autonomous driving system relies on multiple layers of software to function safely. Among these layers, the Perception layer is the most data intensive and also the most complex layer to get right. Companies need to collect and annotate lots of data to properly train deep learning perception models. Simulation systems have come up as an alternative to the expensive task of data collection and annotation. However, whether simulated data can be used as a proxy for real-world data is an ongoing debate. In this work, we attempt to address the question of whether models trained on simulated data can generalize well to the real-world. We collect datasets based on two different simulators with varying levels of graphics fidelity and use the KITTI dataset as an example of real- world data. We train three separate deep learning based object detection models on each of these datasets, and compare their performance on test sets collected from the same sources. We also add the recently released Waymo Open Dataset as a challenging test set. Performance is evaluated based on the mean average precision (mAP) metric for object detection. We find that training on simulation in general does not translate to generalizability on real-world data and that diversity in the training set is much more important than visual graphics\u27 fidelity",'Institute of Electrical and Electronics Engineers (IEEE)',Evaluating Validity of Synthetic Data in Perception Tasks for Autonomous Vehicles,10.1109/AITEST49225.2020.00018,,,core
201535429,2018-07-01T00:00:00,"Abstract A PubMed query run in June 2018 using the keyword ‘blockchain’ retrieved 40 indexed papers, a reflection of the growing interest in blockchain among the medical and healthcare research and practice communities. Blockchain’s foundations of decentralisation, cryptographic security and immutability make it a strong contender in reshaping the healthcare landscape worldwide. Blockchain solutions are currently being explored for: (1) securing patient and provider identities; (2) managing pharmaceutical and medical device supply chains; (3) clinical research and data monetisation; (4) medical fraud detection; (5) public health surveillance; (6) enabling truly public and open geo-tagged data; (7) powering many Internet of Things-connected autonomous devices, wearables, drones and vehicles, via the distributed peer-to-peer apps they run, to deliver the full vision of smart healthy cities and regions; and (8) blockchain-enabled augmented reality in crisis mapping and recovery scenarios, including mechanisms for validating, crediting and rewarding crowdsourced geo-tagged data, among other emerging use cases. Geospatially-enabled blockchain solutions exist today that use a crypto-spatial coordinate system to add an immutable spatial context that regular blockchains lack. These geospatial blockchains do not just record an entry’s specific time, but also require and validate its associated proof of location, allowing accurate spatiotemporal mapping of physical world events. Blockchain and distributed ledger technology face similar challenges as any other technology threatening to disintermediate legacy processes and commercial interests, namely the challenges of blockchain interoperability, security and privacy, as well as the need to find suitable and sustainable business models of implementation. Nevertheless, we expect blockchain technologies to get increasingly powerful and robust, as they become coupled with artificial intelligence (AI) in various real-word healthcare solutions involving AI-mediated data exchange on blockchains","[{'title': 'International Journal of Health Geographics', 'identifiers': ['1476-072x', 'issn:1476-072X']}]",'Springer Science and Business Media LLC',"Geospatial blockchain: promises, challenges, and scenarios in health and healthcare",10.1186/s12942-018-0144-x,,core
201500559,2018-11-01T00:00:00,"This article develops the design, installation, exploitation, and final utilization of intelligent techniques, hardware, and software for understanding mobility in a modern city. We focus on a smart-campus initiative in the University of Malaga as the scenario for building this cyber&#8315;physical system at a low cost, and then present the details of a new proposed evolutionary algorithm used for better training machine-learning techniques: BiPred. We model and solve the task of reducing the size of the dataset used for learning about campus mobility. Our conclusions show an important reduction of the required data to learn mobility patterns by more than 90%, while improving (at the same time) the precision of the predictions of theapplied machine-learning method (up to 15%). All this was done along with the construction of a real system in a city, which hopefully resulted in a very comprehensive work in smart cities using sensors","[{'title': 'Sensors', 'identifiers': ['issn:1424-8220', '1424-8220']}]",'MDPI AG',BiPred: A Bilevel Evolutionary Algorithm for Prediction in Smart Mobility,10.3390/s18124123,,core
211491110,2018-01-01T00:00:00,"CO2 emissions from road transport have an increasing importance on the discussion about climate change. Today several models exist potentially capable of evaluating the effect of technology and policy on the energy performance of a transportation system, though they either lack the accuracy required, or they require a level of detail on their inputs, which is prohibiting for large-scale simulations and/or integration with other models or software, e.g. traffic simulation models, cost calculation components, etc. In this light, the objective of the present thesis is the combination of standard vehicle emission simulation approaches, used in vehicle simulation models, with limited statistical/empirical relationships provided by measurement data, and advanced data analysis and machine learning technics, to provide a robust and reliable, yet flexible and quick, integrated framework for analyzing and evaluating the influence of technology and policy on passenger cars’ fuel consumption and CO2 emissions. The integrated simulation-based framework is used mainly to support and assess the effects of the transition from the, up-to-recently used, NEDC-based type approval process, to the newly-defined WLTP, which has taken effect in September 2017. Moreover, the tool is used to evaluate the effect of the newly introduced amendments in the type-approval legislation concerning hybrids and other low emission vehicle segments, and, lastly, to replicate real driving emissions gathered through a Portable Emissions Measurement Systems (PEMS) tests campaign.Οι εκπομπές CO2 από την οδική κυκλοφορία έχουν ολοένα μεγαλύτερη σημασία στη συζήτηση για την κλιματική αλλαγή. Διάφορα μοντέλα και πρακτικές εφαρμόζονται σήμερα για την εκτίμηση της επίδρασης τεχνολογικών και πολιτικών επιλογών στην κατανάλωση καυσίμου επιβατικών οχημάτων και στην ενεργειακή αξιολόγηση συστημάτων μεταφοράς, ωστόσο, αυτά, είτε δεν διαθέτουν την απαιτούμενη ακρίβεια στο τελικό αποτέλεσμα, είτε χρειάζονται ένα επίπεδο λεπτομέρειας στις παραμέτρους εισόδου που απαγορεύει τις προσομοιώσεις σε μεγάλη κλίμακα ή/και τον συνδυασμό με άλλα μοντέλα ή λογισμικά, όπως μοντέλα προσομοίωσης κυκλοφορίας, μοντέλα υπολογισμού κόστους κ.α.. Υπό αυτό το πρίσμα, η παρούσα διδακτορική διατριβή στοχεύει στο συνδυασμό προσεγγίσεων προσομοίωσης κατανάλωσης καυσίμου οχημάτων, όπως αυτά χρησιμοποιούνται σε αναλυτικά μοντέλα προσομοίωσης, με εμπειρικές σχέσεις που προκύπτουν από τον συνδυασμό και την ανάλυση δεδομένων μετρήσεων και άλλων διαθέσιμων δεδομένων, σε ένα ολοκληρωμένο, ευέλικτο και ακριβές εργαλείο, το οποίο επιτρέπει την αξιολόγηση της επίπτωσης σύγχρονων τεχνολογιών και πολιτικών επιλογών στην κατανάλωση καυσίμου επιβατικών οχημάτων. Το συγκεκριμένο εργαλείο χρησιμοποιείται κυρίως για την υποστήριξη και την εκτίμηση της επίπτωσης της μετάβασης από το μέχρι πρότινος ισχύον πρωτόκολλο μέτρησης το οποίο εφαρμόζεται στην έγκριση τύπου κατανάλωσης καυσίμου και εκπομπών CO2 επιβατικών οχημάτων στην Ευρώπη, το NEDC, στο νέο, το οποίο αναπτύχθηκε από τα Ηνωμένα Έθνη και ξεκίνησε να εφαρμόζεται στην ευρωπαϊκή νομοθεσία από το Σεπτέμβριο του 2017, το WLTP. Επιπλέον, το εργαλείο εφαρμόζεται για την εκτίμηση της επίπτωσης των αλλαγών της σχετικής νομοθεσίας οι οποίες αφορούν οχήματα χαμηλών εκπομπών (ηλεκτρικά, κ.α.), και, τέλος, για την αναπαραγωγή δεδομένων εκπομπών υπό πραγματικές συνθήκες οδήγησης, τα οποία συλλέχθησαν μέσω φορητών συστημάτων μέτρησης εκπομπών (PEMS)",,Αριστοτέλειο Πανεπιστήμιο Θεσσαλονίκης (ΑΠΘ),Ανάπτυξη ενός εργαλείου ποσοτικοποίησης των εκπομπών CO2 από την οδική κυκλοφορία με σύνδεση λεπτομερούς προσομοίωσης οχήματος και μακροεργαλείων εκπομπών,,,core
299973196,2019-01-01T00:00:00,"This project aims to improve the quality of life of mobility-impaired people by designing and developing an eye-tracking controlled mobile wheelchair. This device enables them to use their eye gaze to control the motorized wheelchair, as a result giving them the freedom to move about freely without any support. The project is classified into two parts: hardware and software. The hardware consists of a headset assembled with a raspberry pi camera towards the eyeball for the real-time video capture and the Peregon EW1200 Wheelchair. The software consists of machine learning and image processing technologies on the real-time camera, to predict the user’s point of gaze.Bachelor of Engineering (Electrical and Electronic Engineering",,,Eye-tracking controlled mobile wheelchair,,,core
429069002,2019-01-01T00:00:00,"© 2019 by the authors. We present a novel convolutional neural network (CNN)-based change detection framework for locating changed building instances as well as changed building pixels from very high resolution (VHR) aerial images. The distinctive advantage of the framework is the self-training ability, which is highly important in deep-learning-based change detection in practice, as high-quality samples of changes are always lacking for training a successful deep learning model. The framework consists two parts: a building extraction network to produce a binary building map and a building change detection network to produce a building change map. The building extraction network is implemented with two widely used structures: a Mask R-CNN for object-based instance segmentation, and a multi-scale full convolutional network for pixel-based semantic segmentation. The building change detection network takes bi-temporal building maps produced from the building extraction network as input and outputs a building change map at the object and pixel levels. By simulating arbitrary building changes and various building parallaxes in the binary building map, the building change detection network is well trained without real-life samples. This greatly lowers the requirements of labeled changed buildings, and guarantees the algorithm's robustness to registration errors caused by parallaxes. To evaluate the proposed method, we chose a wide range of urban areas from an open-source dataset as training and testing areas, and both pixel-based and object-based model evaluation measures were used. Experiments demonstrated our approach was vastly superior: without using any real change samples, it reached 63% average precision (AP) at the object (building instance) level. In contrast, with adequate training samples, other methods-including the most recent CNN-based and generative adversarial network (GAN)-based ones-have only reached 25% AP in their best cases",,'MDPI AG',Building instance change detection from large-scale aerial images using convolutional neural networks and simulated samples,10.3390/rs11111343,,core
270159546,2019-05-27T00:00:00,"International audienceThis work aims to show the new approaches in embedded vision dedicated to object detection and tracking for drone visual control. Object/Pedestrian detection has been carried out through two methods: 1. Classical image processing approach through improved Histogram Oriented Gradient (HOG) and Deformable Part Model (DPM) based detection and pattern recognition methods. In this step, we present our improved HOG/DPM approach allowing the detection of a target object in real time. The developed approach allows us not only to detect the object (pedestrian) but also to estimates the distance between the target and the drone. 2. Object/Pedestrian detection-based Deep Learning approach. The target position estimation has been carried out within image analysis. After this, the system sends instruction to the drone engine in order to correct its position and to track target. For this visual servoing, we have applied 1 This work is carried out as part of the INTERREG VA FMA ADAPT project ""Assistive Devices for empowering disAbled People through robotic Technologies"" http://adapt-project.com/index.php. The Interreg FCE Programme is a European Territorial Cooperation programme that aims to fund high quality our improved HOG approach and implemented two kinds of PID controllers. The platform has been validated under different scenarios by comparing measured data to ground truth data given by the drone GPS. Several tests which were ca 1 rried out at ESIGELEC car park and Rouen city center validate the developed platform",,'University of West Bohemia',Real Time Pedestrian and Object Detection and Tracking-based Deep Learning. Application to Drone Visual Tracking,10.24132/CSRN.2019.2902.2.5,,core
237123451,2019-01-30T00:00:00,"Dissertação (mestrado)—Universidade de Brasília, Faculdade de Tecnologia, Departamento de Engenharia Civil e Ambiental, 2019.A estimativa dos custos de projetos de construção com maior precisão na fase do desenvolvimento do projeto é crucial para estudos de viabilidade e é um fator chave para o seu sucesso. No entanto, na etapa inicial da obra existe pouca definição de projeto e, portanto, as informações são limitadas. Além disso, outros problemas são enfrentados como a falta de dados e de métodos adequados de estimativas, como também o envolvimento das incertezas. Com base nisso, este trabalho apresenta a utilização da Inteligência Artificial para estimar custos de construção de rodovias, a partir dos dados de projetos coletados e analisados no Departamento Nacional de Infraestrutura de Transportes (DNIT), com a finalidade de obter as variáveis que mais contribuem para o custo final da obra. Para aplicação da técnica de Redes Neurais Artificiais foi necessário selecionar uma amostra de 25 projetos. As RNAs foram treinadas e testadas com auxílio do software Matlab®, para a arquitetura de rede perceptron de múltiplas camadas e o algoritmo de backpropagation, utilizando 11, 10 e 5 parâmetros de entrada com diferentes topologias para verificar a robustez da ferramenta. A validação das estimativas foi feita pela comparação entre o valor estimado pelas RNAs e o custo real das obras. Ao final deste trabalho, o erro médio encontrado para a estimativa dos custos de construção de rodovias da melhor rede foi igual a 9%, com 5 parâmetros de entrada, 15 neurônios e função tangente hiperbólica na camada oculta e 1 neurônio na camada de saída, utilizando a função linear. Os resultados confirmam que a RNA é uma ferramenta promissora, especialmente para casos como o Brasil, onde as ferramentas computacionais ainda são pouco exploradas, exigindo um tempo maior na execução das tarefas como estimativas. Espera-se que o estudo apresentado contribua e apoie nas decisões de viabilidade econômica de construção de rodovias, pois uma melhor análise pode ser realizada com as variáveis fornecidos para serem reproduzidos e aplicados a outros projetos, uma vez que são parâmetros facilmente quantificáveis na etapa de anteprojeto.Conselho Nacional de Desenvolvimento Científico e Tecnológico (CNPq).To estimate costs of construction projects more accurately at the project development stage is crucial for feasibility studies and it is a key factor for their success. However, in the initial stage of the work there is little project definition and, therefore, the information is limited. In addition, there are other problems, such as the lack of adequate data and estimation methods, as well as the involvement of uncertainties. Based on this, this work presents the use of Artificial Intelligence to estimate road construction costs, based on data collected and analyzed in the National Department of Infrastructure and Transport (DNIT), in order to obtain the variables that most contribute to the cost end of the construction. For the application of the technique of Artificial Neural Network it was necessary to select a sample of 25 projects. The ANNs were trained and tested using Matlab® software for the multilayer perceptron network architecture and the backpropagation algorithm, using 11, 10 and 5 input parameters with different topologies to verify the robustness of the tool. The validation of the estimates was made by comparing the value estimated by ANNs with the real cost of the work. At the end of this research, the average error found for estimating costs of road construction of the best network was 9%, with 5 input parameters, 15 neurons and hyperbolic tangent function in the hidden layer and 1 neuron in the output layer, using the linear function. The results confirm that ANN is a promising tool, especially for cases such as Brazil, where computational tools are still little explored, requiring a longer time in the execution of tasks as estimates. The presented study is expected to contribute and support the decisions of road construction economic feasibility, as better analysis can be taken with the parameters provided to be reproduced and applied to other projects, since they are easily quantified variables in the preliminary project",,,Application of artificial neural networks in the context of road construction cost estimation,,,core
200820342,2019-04-11T00:00:00,"Pre-trained deep learning models are increasingly being used to offer a
variety of compute-intensive predictive analytics services such as fitness
tracking, speech and image recognition. The stateless and highly parallelizable
nature of deep learning models makes them well-suited for serverless computing
paradigm. However, making effective resource management decisions for these
services is a hard problem due to the dynamic workloads and diverse set of
available resource configurations that have their deployment and management
costs. To address these challenges, we present a distributed and scalable
deep-learning prediction serving system called Barista and make the following
contributions. First, we present a fast and effective methodology for
forecasting workloads by identifying various trends. Second, we formulate an
optimization problem to minimize the total cost incurred while ensuring bounded
prediction latency with reasonable accuracy. Third, we propose an efficient
heuristic to identify suitable compute resource configurations. Fourth, we
propose an intelligent agent to allocate and manage the compute resources by
horizontal and vertical scaling to maintain the required prediction latency.
Finally, using representative real-world workloads for urban transportation
service, we demonstrate and validate the capabilities of Barista",,'Institute of Electrical and Electronics Engineers (IEEE)',"BARISTA: Efficient and Scalable Serverless Serving System for Deep
  Learning Prediction Services",10.1109/IC2E.2019.00-10,http://arxiv.org/abs/1904.01576,core
200850308,2019-05-01T00:00:00,"Rapid transit systems or metros are a popular choice for high-capacity public transport in urban areas due to several advantages including safety, dependability, speed, cost, and lower risk of accidents. Existing studies on metros have not considered appropriate holistic urban transport models and integrated use of cutting-edge technologies. This paper proposes a comprehensive approach toward large-scale and faster prediction of metro system characteristics by employing the integration of four leading-edge technologies: big data, deep learning, in-memory computing, and Graphics Processing Units (GPUs). Using London Metro as a case study, and the Rolling Origin and Destination Survey (RODS) (real) dataset, we predict the number of passengers for six time intervals (a) using various access transport modes to reach the train stations (buses, walking, etc.); (b) using various egress modes to travel from the metro station to their next points of interest (PoIs); (c) traveling between different origin-destination (OD) pairs of stations; and (d) against the distance between the OD stations. The prediction allows better spatiotemporal planning of the whole urban transport system, including the metro subsystem, and its various access and egress modes. The paper contributes novel deep learning models, algorithms, implementation, analytics methodology, and software tool for analysis of metro systems","[{'title': 'Sustainability', 'identifiers': ['2071-1050', 'issn:2071-1050']}]",'MDPI AG',"Rapid Transit Systems: Smarter Urban Planning Using Big Data, In-Memory Computing, Deep Learning, and GPUs",10.3390/su11102736,,core
286458433,2019-01-01T00:00:00,"© 2019 by the authors. We present a novel convolutional neural network (CNN)-based change detection framework for locating changed building instances as well as changed building pixels from very high resolution (VHR) aerial images. The distinctive advantage of the framework is the self-training ability, which is highly important in deep-learning-based change detection in practice, as high-quality samples of changes are always lacking for training a successful deep learning model. The framework consists two parts: a building extraction network to produce a binary building map and a building change detection network to produce a building change map. The building extraction network is implemented with two widely used structures: a Mask R-CNN for object-based instance segmentation, and a multi-scale full convolutional network for pixel-based semantic segmentation. The building change detection network takes bi-temporal building maps produced from the building extraction network as input and outputs a building change map at the object and pixel levels. By simulating arbitrary building changes and various building parallaxes in the binary building map, the building change detection network is well trained without real-life samples. This greatly lowers the requirements of labeled changed buildings, and guarantees the algorithm's robustness to registration errors caused by parallaxes. To evaluate the proposed method, we chose a wide range of urban areas from an open-source dataset as training and testing areas, and both pixel-based and object-based model evaluation measures were used. Experiments demonstrated our approach was vastly superior: without using any real change samples, it reached 63% average precision (AP) at the object (building instance) level. In contrast, with adequate training samples, other methods-including the most recent CNN-based and generative adversarial network (GAN)-based ones-have only reached 25% AP in their best cases",,,Building instance change detection from large-scale aerial images using convolutional neural networks and simulated samples,,,core
478869506,2019-01-01T00:00:00,"2019 annual report for the Blue Waters ProjectNSF OCI-0725070NSF ACI-123899314.	Dinshaw S. Balsara, Simulating Two-Fluid MHD Turbulence and Dynamos in Star-Forming Molecular Clouds and a New Paradigm for Computational Astrophysics for Spherical Systems
16.	Adam Burrows, The Computational Keys to the Supernova Puzzle:  How Multiple 3D Radiation/Hydrodynamic Models Can Unlock the Supernova Mystery
18.	Manuela Campanelli, Shedding Light on Supermassive Binary Black Hole Mergers
20.	Manuela Campanelli, Accretion Dynamics of Supermassive Black Hole Binaries
22.	Matias Carrasco Kind, Achieving Probabilistic Classification of Cosmic Web Particles Using Rapidly Generated Training Data:  A Method for Classifying Galaxies into Their Cosmic Web Structural Groups Using Supervised Machine Learning
24.	Tiziana Di Mattteo, The Epoch of the First Luminous Black Holes:  Evolving the Blue Tides Simulation into the First Billion Years of Cosmic History
26.	Jerry Draayer, Advancing First-Principle Symmetry-Guided Nuclear Modeling for Studies of Nucleosynthesis and Fundamental Symmetries in Nature
28.	Charles F. Gammie, Magnetized Models of Giant Impacts
30.	Nickolayk Gnedin, Cosmic Reionization on Computers
32.	John Hawley, Elucidating the Alignment Mechanism for Black Hole Accretion Disks Subjected to Lense-Thirring Torques
34.	Philip F. Hopkins, Understanding the Origins of the Stars and Galaxies in our Universe
36.	Eliu Huerta, Deep Learning at Scale for the Construction of Galaxy Catalogs with the Dark Energy Survey
38.	Eliu Huerta, Characterization of Numerical Relativity Waveforms of Eccentric Binary Black Hole Mergers
40.	Eiu Huerta, Fusing Numerical Relativity and Deep Learning to Detect Eccentric Binary Black Hole Mergers Using Higher-Order Waveform Multipoles
42.	Athol J. Kemball, Data-and Compute-Intensive Challenges for Observational Astronomy in the Great Survey Era
44.	Deborah A. Levin, Modal Decompositions of Shock Interactions
46.	Deborah A. Levin, Plume Plasma Spacecraft Interactions
48.	Yi-Hsin Liu, The Spreading of Three-Dimensional Magnetic Reconnection in Asymmetric Geometry
50.	Felipe Menanteau, Assembling a Map of the Universe:  Shapes and Mass Distribution for the Dark Energy Survey
52.	Philipp Mosta, Petascale Simulations of Binary Neutron Star Mergers
54.	Michael L. Norman, Development of a Scalable Gravity Solver for Enzo-E
56.	Brian O’Shea, Simulating Galaxy Formation Across Cosmic Time
58.	Donald Petravick, Processing Dark Energy Camera Data to Make the World’s Best Map of the Night Sky
60.	Nikolai Pogorelov, Coupling the Solar Wind and Local Interstellar Medium in the Era of the New Horizons, Interstellar Boundary Explorer, Parker Solar Probe, Ulysses, and Voyager Spacecraft
62.	Jane Pratt, Interior Dynamics of Young Stars Revealed by 3D Hydrodynamic Simulations
64.	Thomas Quinn, Modeling of Galaxy Populations
66.	Paul Ricker, Effects of Active Galaxy Feedback on the Intracluster Medium
68.	Stuart Shapiro, Gravitational and Electromagnetic Signatures from Binary Black Hole-Neutron Star Mergers:  A Jet Engine for Short Gamma-Ray Bursts
70.	Alexander Tchekhovskoy, Feeding Black Holes:  Tilt with a Twist
72.	Gabor Toth, Scaling the BATS-R-US MHD Model to Over 100,000 Cores with Efficient Hybrid OpenMP and MPI Parallelization
74.	Mathew Turk, Numerical Study on the Fragmentation Condition in a Primordial Accretion Disk
75.	Saul A. Teukolsky, Merging Black Holes and Neutron Stars
78.	Jennifer Corcoran, Image Processing to Build a Multitemporal Vegetation Elevation Ecosystem Model of the Great Lakes Basin
80.	Larry Di Girolamo, Petascale Processing of Satellite Earth Observations
82.	Chunyuan Diao, Large-Scale Remote Monitoring of Invasive Species Dynamics Through a Petascale High-Performance Computing System
84.	Francina Domiguez, Deforestation of the Amazon Forest:  Understanding Hydroclimate Impacts by Tracing the Water that Evaporates from the Forest
86.	Patricia M. Gregg, Forecasting Volcanic Unrest and Eruption Potential Using Statistical Data Assimilation
88.	Kaiyu Guan, Monitoring Field-Scale Crop Water Use Using a Satellite Data-Driven Mechanistic Modeling Approach
90.	Kaiyu Guan, Building an Objective Seasonal Forecasting System for U.S. Corn and Soybean Yields
92.	Sonia Lasher-Trapp, Inflow and Outflow from Thunderstorms:  Tracking Their Influence on Precipitation and Further Growth
94.	Paul Morin, Petascale Polar Topography Production
96.	Stephen W. Nesbitt, High-Resolution Numerical Simulations of Convection Initiation over the Sierras de Cordoba Mountains in Argentina
98.	Leigh G. Orf, Simulations of Violently Tornadic Supercells and Damaging Thunderstorms
100.	Nikolaos Pavlis, Prediction of Geomagnetic Secular Variation with Large-Ensemble Geomagnetic Data Assimilation
102.	Nicole Riemer, Machine Learning for Error Quantification in Simulating the Climate Impacts of Atmospheric Aerosols
104.	Clay Tabor, Simulating Hydroclimate Change in Southwest North America at 21,000 Years Ago
106.	Robert J. Trapp, Implementation and Use of a Global Nonhydrostatic Model for Extended-Range Weather Prediction during the RELAMPAGO Field Campaign
108.	John Vidale, Simulating Large California Earthquakes Before They Occur
110.	Renata Wentzcovitch Materials Simulations in Geophysics
112.	Mathew West, Simulating Aerosol Impacts on Climate, One Particle at a Time:  A Regional-Scale, Particle-Resolved Aerosol Model to Quantify and Reduce Uncertainties in Aerosol-Atmosphere Interactions
114.	Donald J. Wuebbles, Evolving Air Quality Under the Changing Climate
116.	Xiangdong Zhang, Sensitivity of Arctic Sea Ice Thickness Distribution to Sea Ice Internal Dynamics in a Changing Climate
120.	Narayana R. Aluru, The Mechanism of Proton Diffusion in ABO3 Perovskite Oxides
122.	Narayana R. Aluru, Identification of Amino Acids with Sensitive Nanoporous MoS2:  Toward Machine Learning-Based Prediction
124.	Narayana R Aluru, Transfer-Learning-Based Course-Graining Method for Simple Fluids:  Toward Deep Inverse Liquid-State Theory
126.	Guillermo Araya, High-End Visualization of Coherent Structures and Turbulent Events in Wall-Bounded Flows with a Passive Scalar
128.	Jerzy Bernholc, Design of Atomically Precise Nanoscale Negative Differential Resistance Devices
130.	Daniel Bodony, Using OpenMP Offloading to Run Code on Blue Waters’ GPU Nodes
132.	Christoph Brehm, Numerical Investigation of Turbulence Suppression in Rotating Flows
134.	Oliver M. F. Browne, An Efficient Method for Hypersonic Laminar-Turbulent Transition Prediction
136.	Carlo Pierleoni, Quantum Simulations:  Properties of Dense Hydrogen
138.	Huck Beng Chew, Role of Interfaces on the Shear Strength and Bending Properties of vander Waals Two-Dimensional Materials
140.	Bryan Clark, Atypically Entangled Phases and New Methods for the Quantum Many-Body Problem
142.	Lian Duan, Direct Numerical Simulation of Pressure Fluctuations Induced by Supersonic Turbulent Boundary Layers
144.	Aida X. El-Khadra, The Anomalous Magnetic Moment of the Muon:  An Improved Ab Initio Calculation of the Hadronic Vacuum Polarization Contribution
146.	Elif Ertekin, Accelerating Thermoelectric Materials Discovery via Dopability Predictions
148.	Jonathan Freund, Machine-Learning Turbulence Models for Simulations of Turbulent Combustion
150.	Marcelo Garcia, Turbulence-Resolving Modeling of Oscillatory Boundary Layer Flows
152.	Benjamin Hooberman, Machine Learning for Particle Physics:  Employing Deep Learning for Particle Identification and Measurement of Colliders
154.	Kathryn Huff, Molten-Salt Reactors and Their Fuel Cycles
156.	Prashant K. Jain, A Novel Crystal Structure with Spin-Protected Surface Electronic Conduction
158.	Eric Johnsen, Inertial Collapse of Individual Bubbles near Solid/Free Boundaries
160.	Harley T. Johnson, Electronic Structure of Microscale Dielectric Barrier Discharges
162.	Seid Koric, Accelerating Virtual Prototyping and Certification in the Aerospace Industry with Scalable Finite-Element Analysis
164.	Jean-Pierre Leburton, Graphene Nanopore Transistor for DNA-Nick Detection
166.	Farzad Mashayek, Compressibility Effects on Spatially Developing Plane Free Shear Layer
168.	Moshe Matalon, Outwardly Propagating Turbulent Flames
170.	Mark Neubauer, Deep Learning for Higgs Boson Identification and Searches for New Physics at the Large Hadron Collider
172.	Rajib Rahman, Designing Quantum Logic Gates on Silicon Chips with Large-Scale Multiphysics Simulations
174.	Venkat Raman, Simulation of Rotating Detonation Engines
176.	Caroline Riedl, Mapping Proton Quark Structure:  Looking Inside the Proton-How do Quarks Spin?
178.	Andre’ Schleife, Electron Dynamics of Ion-Irradiated Two-Dimensional Materials
180.	Andre’ Schleife, Discovery of New Plasmonic Materials via High-Throughput Machine Learning
182.	Brian G. Thomas, Turbulent Multiphase Thermal Flow Modeling of Defect Formation Mechanisms and Electromagnetic Force Effects in Continuous Steel Casting
184.	Rafael Tinoco Lopez, Investigation of Sediment Transport Through Aquatic Vegetation Using Large-Scale High-Fidelity Turbulence Simulations
186.	Kimani Toussaint, Machine Learning-Assisted High-Throughput Computational Design of Solvents for Liquid-Exfoliation
188.	Dallas R. Trinkle, High-Throughput Materials Modeling Optimization
190.	Lela Vukovic, Detecting Neurotransmitters with DNA-Wrapped Nanotube Sensors
192.	Lucas Wagner, Accurate Effective Interactions in Quantum Materials
194.	Zhi Jian Wang, Supersonic Jet Noise Prediction Using High-Order Large-Eddy Simulation
196.	Bin Xu, Spin Spirals in Multiferroic Bismuth Ferrite and at Metal Surface:  From Fully First Principles
198.	Zhen Xu, Numerical Simulations of a Collapsing Cavitation Bubble Near an Elastically Deformable Object
200.	Yonghua Yan, Numerical Study on Shock Wave-Boundary Layer Interaction and Its Control
202.	Jinhui Yan, Free-Surface Flow Modeling of Multiple Tidal Turbines
204.	Pui-Kuen Yeung, New Insights on Intermittency and Circulation Statistics Obtained From a Massive Turbulence Simulation Database
206.	Yang Zhang, Effects of Surface Defects on Hydrophobicity at Rare-Earth Oxide Interfaces Using Molecular Dynamics Simulations Driven by Ab Initio-Based Deep Neural Network Potentials
208.	Phiala Shanahan, Constraining the Properties and Interactions of Dark Matter
212.	Donna Cox, Cinematic Scientific Data Visualization for CADENS
214.	Iwan Duursma, The Structure and Statistics of Reed-Muller Codes
216.	William Gropp, A Parallel Framework for Scaling Phylogeny Estimation Methods to Large Genomic Data Sets
218.	William Gropp, Algorithms for Extreme-Scale Systems
220.	Ravishankar Iyer, Kaleidoscope:  Live Forensics for Large-Scale Data Center Storage Systems
222.	Shantenu Jha, Extensible and Scalable Adaptive Sampling to Fold Proteins on Supercomputers
224.	Luke Olson, Improved Scalability Through Node-Aware Communicators
226.	Luke Olson, Scalable Line and Plane Solvers
228.	Marc Snir, Detection of Silent Data Corruptions Using Machine Learning	
230.	Edgar Solomonik, Pushing the Boundaries of Large-Scale Tensor Computations
232.	Tandy Warnow, Algorithms for Large-Scale Evolutionary Tree Construction:  Improving Scalability and Accuracy through Divide-and-Conquer
234.	Justin Sirigano, HPC Development of Deep Learning Models in Scientific Computing and Finance
235.	David Taflin, Optimization of a Field Data Parallel Output Library
238.	Aleksei Aksimentiev, Resolving the Structure of Bacteriophage HK97 with Atomistic Resolution
240.	Aleksei Aksimentiev, A Nanopore System for Single-Molecule Protein Sequencing
242.	Aleksei Aksimentiev, Dynamic Interactions Between Lipid-Tethered DNA and Phospholipid Membranes
244.	Rommie Amaro, Influence Virulence and Transmissibility Through the Computational Microscope
246.	Rafael C. Bernardi, How Blue Waters Is Aiding the Fight Against Sepsis
248.	Gustavo Caetano-Annoles, A Phylogenomic History of Protein Function and Dynamics
250.	Colleen E. Clancy, Predicting Drug-Induced Cardiac Arrhythmias Using Atomistic Simulations
252.	Julie Dickerson, MRNA Isoform Prediction
254.	Ken Dill, Petascale Integrative Approaches to Protein Structure Prediction
256.	Andrew Ferguson, Discovery of Slow Kinetic Modes from Molecular Simulation Trajectories
258.	Mattia Gazzola, Harnessing Viscous Streaming in Complex Active Systems:  Minibots in Fluids
260.	Jodi A. Hadden-Perilla, Molecular Dynamics Simulations of HBV Capsid
262.	Jodi A. Hadden-Perilla, Molecular Dynamics Simulations of the HBV Capsid as a Drug Target
264.	So Hirata, Toward Predictive Computational Design of Precision Molecular Optoelectronics
266.	Mathew E. Hudson, Impact of Batch Effect and Study Design Biases on Identification of Genetic Risk Factors in Sequencing Data
268.	Tao Jiang, Microscopic Identification of PIP2 Binding Sites on a Ca2+-activated Cl- Channel
270.	Fatemah Khalili-Araghi, Paracellular Ion Transport
272.	David LeBauer, The TERRA Phenotyping Refence Platform:  Open Data and Software for Precision Field Crop Measurement and Analysis
274.	Nancy Makri, Quantum-Classical Path Integral Simulation of Proton Translocation in Biological Channels
276.	Arif Masud, A New Stabilized Fluid-Structure Interaction Method:  Coupled System of Anisotropic Viscoelastic Model for Artery and Non-Newtonian Model for Blood
278.	Jeffery S. Moore, Atomic Scale Simulation of Amyloid Beta with Dismantling Peptide-Based Inhibitors
280.	Mahmoud Moradi, Transport Mechanism of POT Transporters:  Employing Loosely Coupled Molecular Dynamics Simulations to Characterize Protein Structural Dynamics
282.	Mahmoud Moradi, Activation Mechanisms of the Mechanosensitive Channel of Large Conductance:  Employing Loosely Coupled Molecular Dynamics Simulations to Characterize Protein Structural Dynamics
284.	Juan Perilla, Molecular Mechanisms of Infection by Chlamydia
286.	Joseph R. Peterson, Calibrating the SimBioSys TumorScope for the Fight on Cancer:  A Scenario Analysis Engine for Determining Optimal Therapy Choice
288.	Kimberly Prather, Investigating the Climate-Relevant Impacts of Chemical Complexity in Marine Aerosols
290.	Benoit Roux, Molecular Dynamics Binding Free Energy Calculations Offer a Window to Understand Protein-Protein Binding Specificity
292.	Diwakar Shukla, Molecular Basis of the Nitrate Transport Mechanism in Plants
294.	Diwakar Shukla, Simulations Uncover the Mechanism of Serotonin Transport in the Brain
296.	Diwakar Shukla, Elucidating the Ligand Selectivity and Activation Mechanisms of Cannabinoid Receptors
298.	Ivan Soltesz, Full-Scale Biophysical Modeling of Hippocampal Networks During Spatial Navigation
300.	Marcos Sotomayor, Desmosomal Cadherins Beating Under Tension
302.	Ashok Srinivasan, Simulation of Viral Infection Propagation During Air Travel
304.	Brad Sutton, MRI-Based Biomarkers Through High-Performance Computing
306.	Emad Tajkhorshid, Mechanobiology:  Using Blue Waters to Decipher the Physical Principles of Protein Mechanics
308.	Emad Tajkhorshid, Atomistic Simulations of a Protocell
310.	Emad Tajkhorshid, Modeling of a Zika Virus Envelope at Atomic Resolution
312.	Gregory Voth, Multiscale Simulations of Complex Self-Assembling Biomolecules:  Targeting HIV-1
314.	Victor Anisimov, Improving the Agreement of AMBER Simulation of Crystals of Nucleic Acid Bases with Experimental Data
315.	Mohammed El-Kebir, Algorithms for Cancer Phylogenetics
316.	Taras V. Pogorelov, Amphotericin-Driven Sterol Extraction:  Probing the Mechanism
320.	Yongyang Cai, Climate Policy in a Dynamic Stochastic Economy
322.	J. Stephen Downie, Characterizing Descriptivity in Writing through Text Analysis of Books from the HathiTrust Digital Library
324.	Mao Ye, High-Frequency Trading in Nanoseconds:  Analysis, Modeling, and Policy Implications
328.	Wendy K. Tam Cho, A Massively Parallel Evolutionary Markov Chain Monte Carlo Algorithm for Sampling Spatial State Spaces
332.	Eizabeth Agee, The Contributions of Root Systems to Drought Response in the Amazon Rainforest
334.	Elaad Applebaum, Star Formation in Dwarf Galaxies:  Using Simulations to Identify Key Observables to Test Models
336.	Katelyn Barber, Improving Convectively Induced Turbulence Forecast Parameters Through Bulk Numerical Simulations for Aviation Safety
338.	Maureen T. Brooks, Modeling Nonlinear Physical-Biological Interactions:  Inertia and Sargassum in the North Atlantic
340.	Iryna Butsky, Predictions About the Invisible Gas in Galaxy Clusters
342.	Robert Cieri, Computational Fluid Dynamics Investigation into Pulmonary Airflow Patterns in Monitor  Lizards(Varanidae)
344.	Mathew Clement, The Early Instability Scenario for Planet Formation in the Solar System
346.	Salme Cook, The Distribution of Shear Stress and Nutrients in a Tidally Energetic Estuary:  The Role of Numerical Resolution and Vegetation
348.	Andrew Emerick, Exascale Astrophysics with Enzo-E:  Development of Physics Modules for Galaxy-Scale and Cosmology Simulations
350.	Forrest Glines, Magnetohydrodynamic Simulation:  Galaxies
352.	Alexander Gurvich, GPU-Accelerated Interstellar Chemistry with WIND:  A General Ordinary Differential Equation Solver
254.	Jennifer M. Hayes, Using Spectroscopic Data and Molecular Simulations to Estimate Heterogeneous Ensembles:  How to Study Complicated, Flexible Proteins When Experimental Data Are Limited
356.	Joshua Lansford, Electron Density-Based Machine Learning for Accelerating Quantum Calculations
358.	Kara Marsac, Extending the Longevity of Produced Water Disposal Wells:  Evaluation Using Reactive Transport Simulation
360.	Nicole Rosato, Improved Trumpet Initial Lapse and Shift for Binary Black Hole Simulations
361.	Shanna Chu, Understanding the Physical Processes Causing Intermediate-Depth Earthquakes
362.	Micheline Soley, Escaping From an Ultracold Inferno:  The Ultracold KRb Dimer Reaction
364.	Ronald Stenz, The Impacts of Hydrometeor Centrifuging on Tornado Dynamics:  Improving the Realism of Tornado Simulations
366.	Darius Teo, Unraveling Functional Hole Hopping Pathways in The [Fe4S4]-Containing DNA Primase
368.	Walter Torres, The Transport and Dynamics of Wave-Driven Reef Jets Under the Influence of Rotation and Bottom Friction
370.	Samuel Whitman, Simulation of Bluff Body Stabilized Flames with PeleC:  Adaptively Resolving Turbulence-Combustion Interactions in Real-World Engineering ProblemsNot Peer ReviewedOpe",,,Blue Waters 2019 Annual Report,,,core
395096924,2019-11-01T00:00:00,"During these years of my Ph.D. studies the main aim of the research work was to improve the efficiency on energy generation into industrial facilities. 



Novelties are proposed both on the devices used for energy generation and on energy consumption data analytics. In the first part of the thesis, Solid Oxide Fuel Cell (SOFC) and Reversible Solid Oxide Cell (RSOC) are proposed: these technologies have many advantages such as high efficiency on energy generation, heat available at high temperature, and modularity. 



A new heat recovery for a modular micro-cogeneration system based on SOFC is presented with the main goal of improving the efficiency of an air source heat pump with unused heat of fuel cell exhausted gases. The novelty of the system proposed is that exhaust gases after the fuel cell are firstly used to heat water and/or used to produce steam, then they are mixed with the external air to feed the evaporator of the heat pump with the aim of increasing energy efficiency of the latter. This system configuration decreases the possibility of freezing of the evaporator as well, which is one of the drawbacks for air source heat pump in climates where temperature close to 0 °C and high humidity could occur. Results show that the performance of the air source heat pump increases considerably during cold season for climates with high relative humidity and for users with high electric power demand. 



As previously cited, not only SOFC but also RSOC are deeply analysed in the thesis to define innovative energy generation system with the possibility of varying H/P ratio to match energy generation and demand in order to avoid mismatching and, consequently, integration system with a lower system. The aim is to define a modular system where each RSOC module can be switched between energy generation mode (fuel consumption to produce electricity and heat) and energy consumption (electricity and heat are consumed to produce hydrogen, working as Solid Oxide Electrolysis Cells) to vary overall H/P of the overall system. Hydrogen is a sub-product of the system and can be used for many purposes such as fuel and/or for transport sector. Then a re-vamping of the energy generation system of a paper mill by means of RSOCS is proposed and analysed: a real industrial facility, based in Italy with a production capacity of 60000 t/y of paper, is used as case study. Even if the complexity of the system increases, results show that saving between 2% and 6% occurs. Hydrogen generation is assessed, comparing the RSOC integrated system with PEM electrolysis, in terms of both primary energy and economics. Results exhibit significant primary energy and good economic performance on hydrogen production with the novel system proposed. 



In the thesis novelties are proposed not only on energy system “hardware” (component for energy generation) but also on “software”. In the second part of the thesis, artificial intelligence and machine learning methods are analysed to perform analytics on energy consumption data and consequently to improve performances on energy generation and operation strategy.  



A study on how cluster analysis could be applied to analyse energy demand data is depicted. The aim of the method is to design cogeneration systems that suit more efficiently energy demand profiles, choosing the correct type of cogeneration technology, operation strategy and, if they are necessary, energy storages. A case study of a wood industry that requires low temperature heat to dry wood into steam-powered kilns that already uses cogeneration is proposed to apply the methodology in order to design and measure improvements. An alternative cogeneration system is designed and proposed, thermodynamics benchmarks are defined to evaluate differences between as-is and alternative scenarios. Results show that the proposed innovative method allows to choose a more suitable cogeneration technology compared to the adopted one, giving suggestions on the operation strategy in order to decrease energy losses and, consequently, primary energy consumption.  



Finally, clustering is suggested for short-term forecasting of energy demand in industrial facilities. A model based on clustering and kNN is proposed to find similar pattern of consumption, to identify average consumption profiles, and then to use them to forecast consumption data. Novelties on model parameters definition such as data normalisation and clustering hyperparameters are presented to improve its accuracy. The model is then applied to the energy dataset of the wood industry previously cited. Analysis on the parameters and the results of the model are performed, showing a forecast of electricity demand with an error of 3%",,,Energy efficiency in industrial facilities - Improvements on energy transformation and data analysis,,,core
297665105,2019-01-01T00:00:00,"Emergency personnel, such as firefighters, bomb technicians, and urban search and rescue specialists, can be exposed to a variety of extreme hazards during the response to natural and human-made disasters. In many of these scenarios, a risk factor is the presence of hazardous airborne chemicals. The recent and rapid advances in robotics and sensor technologies allow emergency responders to deal with such hazards from relatively safe distances. Mobile robots with gas-sensing capabilities allow to convey useful information such as the possible source positions of different chemicals in the emergency area. However, common gas sampling procedures for laboratory use are not applicable due to the complexity of the environment and the need for fast deployment and analysis. In addition, conventional gas identification approaches, based on supervised learning, cannot handle situations when the number and identities of the present chemicals are unknown. For the purpose of emergency response, all the information concluded from the gas detection events during the robot exploration should be delivered in real time. To address these challenges, we developed an online gas-sensing system using an electronic nose. Our system can automatically perform unsupervised learning and update the discrimination model as the robot is exploring a given environment. The online gas discrimination results are further integrated with geometrical information to derive a multi-compound gas spatial distribution map. The proposed system is deployed on a robot built to operate in harsh environments for supporting fire brigades, and is validated in several different real-world experiments of discriminating and mapping multiple chemical compounds in an indoor open environment. Our results show that the proposed system achieves high accuracy in gas discrimination in an online, unsupervised, and computationally efficient manner. The subsequently created gas distribution maps accurately indicate the presence of different chemicals in the environment, which is of practical significance for emergency response.Funding Agency:European Commission  645101</p",,'MDPI AG',Towards Gas Discrimination and Mapping in Emergency Response Scenarios Using a Mobile Robot with an Electronic Nose,10.3390/s19030685,,core
270121766,2019-12-04T00:00:00,"International audiencePredicting, in the one hand, the time duration that a vehicle remains associated to a cell i.e. Network Attachment Point (NAP) and, on the other hand, the next cell can help anticipating network control decisions to provide services with stringent requirements despite vehicle mobility. In this paper, we propose a machine learning based approach for Software Defined Vehicular Networks that allows a cell to estimate the attachment duration of each newly associated vehicle at the association request time, as well as, a prediction of the upcoming cell, performed at the SDN controller that controls the cells. Our proposed models have been evaluated on a large dataset, which we have generated based on a real mobility trace from the city of Luxembourg, and the evaluation shows promising results in terms of prediction accuracy",,HAL CCSD,Effective Prediction of V2I Link Lifetime and Vehicle's Next Cell for Software Defined Vehicular Networks: A Machine Learning Approach,,,core
333871880,2019-01-01T00:00:00,"A search and rescue drone are an unmanned aircraft used by emergency services, such as police officers, firefighters or volunteer rescue teams, ideal for searching over vast areas for missing persons and crime victims in need of rescue and in any environment. Unmanned aerial vehicles (UAVs) can provide real-time visual information and data in the aftermath of an earthquake or hurricane. They can also become an eye in the sky to locate a lost person in the mountain for example. This report is about the system that can help to automatically detect the human body in lying position. The benefit of this project is we can use drone for search and rescue operation (SAR) and disaster victim identification (DVI). This is because this drone can detect human body in lying position exact location in the disaster area so that the rescuer team can easily find and locate the missing victim. Most of the disaster area is very dangerous for human to enter to give the first quick rescue. The search and rescue team also need to prepare on the suitable attire to wear before enter the disaster area. This drone is very helpful because it provides mobility, easy to setup, real-time visual information and location and suitable for any type of disaster. The existing product in market does not implement artificial intelligence (AI) for the object detection. They just monitor and observe the screen themselves in order to get information about survivor from the disaster. A market survey was conducted and the survey has shown that a majority of the respondent were agree that the existing method for SAR operation consumes a lot of time start from the set-up of the operation until the rescue operation. The survey also collected data of respondent that really hope this system was going to use for the SAR operation. Our target markets are Malaysian Fire Fighter, Malaysian Civil Defense and university researchers. We are now competing with SnakeBot and BEAR (Battlefield Extraction-Assist Robot), as they also provide the services for search and rescue operation. We estimated our profit per month is RM 48,500 and annually is RM 582,000. However, this system is not fully ready yet to be implemented for the SAR operation. There is still a lot room for improvement needed to be done",,,"Drone assisted detection system (deADS) / Muhammad Nur Haziq Abdul Shukor, Mohammad Aidil Shah Sajat and 'Abdurrouuf Anuar",,,core
232687264,2019-01-01T08:00:00,"The current infrastructure in our country will not be able to adequately support the growing demands of an exponentially increasing population. A rise in population contributes to a greater service demand necessary to treat sanitary sewer waste which in many cases contributes to the flow of combined sewers. When it comes to managing these combined sewers, rain and the snowmelt caused from climate changes are major factors that need to be addressed. Some water treatment facilities do not have the ability to treat the capacities or peak flows that a system experiences, resulting in combined sewer overflows which are both bad for the environment and an inconvenience to society. The almost 200-million-dollar project of Akron’s Ohio Canal Interceptor Tunnel (OCIT) sparked interest in the topic of storm and combined sewer management especially the concept in-line storage to significantly reduce peak flows which is discussed later. This Akron sewer project has increased the capacity of Akron’s combined sewer system. Increasing a system’s capacity is just one of the improving the water treatment process. Other procedures include redirecting flows, installing valves to regulate peak flows, and monitoring combined sewer levels with remote devices just to name a few. Initially the focus was to be on the City of Akron’s combined sewer treatment systems alone, but resources were made available that allowed an insight on what storm and combined sewer activities have been happening across the country. The implementation of real-time control systems, a form of artificial intelligence (AI), is becoming more common. This paper will look at multiple case studies of real-time control programs being utilized for a number of scenarios requiring different aspects of real-time control to gain a deeper understanding of their capabilities. This will help to determine whether or not these RTC programs can significantly benefit stormwater management facility operations and potentially eliminate combined sewer overflows",,IdeaExchange@UAkron,Artificial Intelligence of Stormwater Operations,,https://core.ac.uk/download/232687264.pdf,core
297086202,2019-10-15T00:00:00,"Today modern cities tend to grow rapidly. The increased population density brings new challenges in term of public safety. Crime and violence are hard to be detected and managed especially in specific crowd environments like music concerts, sport events or public meetings. To overcome this issue the city administration should implement monitoring systems capable of detecting and analysing such situations. The work presented here combines two approaches that enable implementation of an efficient solution adapted for this purpose. The first one involves sensor networks that prove to be cost effective solution in a smart city environment. They can benefit on the existing surveillance infrastructure and allows rapid deployment. The second approach uses deep learning techniques. They demonstrate outstanding performances in image and actions classification based on a prior learning process. By combining these two approaches we succeed to obtain a real-time and cost-effective solution designed for urban area surveillance networks",,'EDP Sciences',Deep Learning Approach for Violence Detection in Urban Areas,10.1051/itmconf/20192903009,,core
250588403,2019-01-01T00:00:00,"This work aims to show the new approaches in embedded vision dedicated to object detection and tracking for drone visual control. Object/Pedestrian detection has been carried out through two methods: 1. Classical image processing approach through
improved Histogram Oriented Gradient (HOG) and Deformable Part Model (DPM) based detection and pattern recognition methods. In this step, we present our improved HOG/DPM approach allowing the detection of a target object in real time. The developed
approach allows us not only to detect the object (pedestrian) but also to estimates the distance between the target and the drone. 2. Object/Pedestrian detection-based Deep Learning approach. The target position estimation has been carried out within image
analysis. After this, the system sends instruction to the drone engine in order to correct its position and to track target. For this visual servoing, we have applied our improved HOG approach and implemented two kinds of PID controllers. The platform has been
validated under different scenarios by comparing measured data to ground truth data given by the drone GPS. Several tests which were ca1rried out at ESIGELEC car park and Rouen city center validate the developed platform",,'University of West Bohemia',Real Time Pedestrian and Object Detection and Tracking-based Deep Learning. Application to Drone Visual Tracking,10.24132/CSRN.2019.2902.2.5,,core
220130785,2019-03-20T02:02:26,"This thesis investigates the dynamic routing decisions for individual travelers and on-demand service providers (e.g., regular taxis, Uber, Lyft, etc).
For individual travelers, this thesis models and predicts route choice at two time-scales: the day-to-day and within-day. For day-to-day route choice, methodological development and empirical evidences are presented to understand the roles of learning, inertia and real-time travel information on route choices in a highly disrupted network based on data from a laboratory competitive route choice game. The learning of routing policies instead of simple paths is modeled when real-time travel information is available, where a routing policy is defined as a contingency plan that maps realized traffic conditions to path choices. Using data from a competitive laboratory experiment, prediction performance is then measured in terms of both one-step and full trajectory predictions. For within day route choice, a recursive logit model is formulated in a stochastic time-dependent (STD) network without sampling any choice sets. A decomposition algorithm is then proposed so that the model can be estimated in reasonable time. Estimation and prediction results of the proposed model are presented using a data set collected from a subnetwork of Stockholm, Sweden.
Taxis and ride-sourcing vehicles play an important role in providing on-demand mobility in an urban transportation system. Unlike individual travelers, they do not have a clear destination when there\u27s no passenger on board. The optimal routing of a vacant taxi is formulated as a Markov Decision Process (MDP) problem to maximize long-term profit over the full working period. Two approaches are proposed to solve the problem. One is the model-based approach where a model of the state transitions of the environment is obtained from queuing-theory based passenger arrival and competing taxi distribution processes. An enhanced value iteration for solving the MDP problem is then proposed making use of efficient matrix operations. The other is the model-free Reinforcement Learning (RL) approach, which learns the best policy directly from observed trajectory data. Both approaches are implemented and tested in a mega city transportation network with reasonable running time, and a systematic comparison of the two approaches is also provided",,ScholarWorks@UMass Amherst,Modeling and Optimizing Routing Decisions for Travelers and On-demand Service Providers,,https://core.ac.uk/download/220130785.pdf,core
297283436,2019,"In the context of the continuous growth of the worldwide population and the rapid ongoing urbanization around the globe, the need for affordable and effective systems to detect hazardous substances and pathogens in water has gained importance. In order to address this need, multi-disciplinary research efforts from the fields of micro-technology and micro-biology have led to the emergence of microfluidic devices in the form of Lab-on-a-Chip (LoC) and Micro Total Analysis (ÂÂµTAS) devices, which are capable to host analytical and biorecognition assays, previously restricted to laboratory environments. Traditionally, the development of these devices had benefited from the microelectronics fabrication techniques, and afforded the replication of sub-micrometer channels and structures, as well as the implementation of functional materials to integrate diverse types of sensors (e.g. temperature, pressure, etc) in the same microfluidic device. Nevertheless, the reduction of the fabrication cost has become a persistent goal in order to popularize their utilization. This has urged the application of alternative materials like thermoplastics and large batch production techniques such as injection molding and hot embossing, at the same time to have set new challenges to their reliable and reproducible integration as analytical devices. The present report describes the design, development and testing of a microfluidic device for biosensing of bacteria, by means of RNA hybridization and fluorescence detection. The device consits in a fully Cyclo-Olefin Copolymer (COC) microfluidic chip, in size of 25,5 x 37,75 mm, structured by hot embossing. The microfluidic channels and cavities sum up a fluid volume of about 139 ÂÂµL, comprising a heating chamber, temperature sensor chambers, cooling channel and reaction chamber. The device layout includes 7 inlets for the sample fluid and diverse reagents plus 1 outlet. On-chip assay starts with the intake of a volume of 1 mL of water sample. The sample fluid is pumped through the heater chamber, where heat from a screen printed heater is applied to lyse the bacteria and release their RNA content to the running flow. Following the same stream, the fluid with released RNA flows across the cooling channel until the reaction chamber. The reaction chamber bottom surface, previously functionalized with capture oligomers complementary to the RNA target sequences, hosts hybridization reactions to capture the target RNA. The captured RNA is later tagged with a fluorescence molecule in a second hybridization. After washing off unbound analytes, the overall fluorescence emission is collected, filtered and quantified. The net fluorescence intensity measurement is then interpreted as an indicator of the concentration of the viable bacteria presented in the sample. The microfluidic chip was tested in a custom testbench that included particle filtering and pre-concentration of bacteria from raw samples, and fluorescence detection system that performed a limit of detection of 18 fmol, with a sensitivity of 63,08 photon count per fmol. Theoretical evaluation of the microfluidic chip at 0,1 mL/min predicted a mass transport and heat transport efficiency of 68,57% and 67,27%, respectively. Experimentally, the microfluidic device in the biosensing system completed successful detection of bacteria from raw water in less than 1 hour. Fluorescence detection was completed from hybridized bacterial RNA, that was retrieved in the same chip by heat lysis on a dilution of 2x10^8 of E. coli. Theoretical limit of detection of 24,87x10^3 CFU/mL was calculated. The microfluidic chip, integrated with the fluorescence detection system, proved its functionality as biosensing system for on-site applications, as well as its potential as a reference of a low-cost, disposable device for real-time monitoring and control of bacteria pollution",,,"Design of a Biosensor for Detection of Bacteria in Water, by means of a Microfluidic System",,,core
268881594,2019-12-05T02:51:21,"Rapid developments in hardware, software, and communication technologies have facilitated the emergence of Internet-connected sensory devices that provide observations and data measurements from the physical world. By 2020, it is estimated that the total number of Internet-connected devices being used will be between 25 and 50 billion. As these numbers grow and technologies become more mature, the volume of data being published will increase. The technology of Internet-connected devices, referred to as Internet of Things (IoT), continues to extend the current Internet by providing connectivity and interactions between the physical and cyber worlds. In addition to an increased volume, the IoT generates big data characterized by its velocity in terms of time and location dependency, with a variety of multiple modalities and varying data quality. Intelligent processing and analysis of this big data are the key to developing smart IoT applications. This article assesses the various machine learning methods that deal with the challenges presented by IoT data by considering smart cities as the main use case. The key contribution of this study is the presentation of a taxonomy of machine learning algorithms explaining how different techniques are applied to the data in order to extract higher level information. The potential and challenges of machine learning for IoT data analytics will also be discussed. A use case of applying a Support Vector Machine (SVM) to Aarhus smart city traffic data is presented for a more detailed exploration",,SelectedWorks,Machine Learning for Internet of Things Data Analysis: A Survey,,,core
240133824,2019-01-01T00:00:00,"The technological development in the fields of image processing, computer vision and digital photogrammetry and remote sensing, provides new tools and automated solutions for applications in urban studies, cadastre, etc, associated with urban development, identification of illegal constructions, 3D modeling, change detection, etc. Apart from the traditional techniques that utilize purely aerial or satellite imagery, recent approaches exploit 3D point clouds captured from aerial laser scanning systems (LIDAR) or extracted from Dense Image Matching (DIM) methods. Building extraction, change detection and roof segmentation from images and point clouds are topics of high research interest as they are useful for urban city monitoring. These topics still remain challenging due to the increasing demands for accuracy, rapidity and economy. The present Phd thesis provides an integrated, extensive and detailed methodology for building extraction change detection and roof segmentation using images and point clouds and applying sophisticated methods. Several complex urban regions of various types of buildings, pixel resolutions and types of data are examined. To verify the utility and functionality of the proposed methodology, the corresponding 3D models of the buildings are extracted.Initially, the differences between the LIDAR and DIM point clouds were highlighted in terms of robustness and efficiency through direct point cloud comparison as well as by using the corresponding orthoimages. The results shown that the LIDAR point clouds not only describe with fidelity the objects of the scene but also present a global homogenous behavior either at urban or sub-urban areas. However, LIDAR point clouds suffer in some cases from noise and local under-sampling indicating local lack of spatial information at building’s boundaries. On the other side, the DIM is a low cost and flexible solution with promising results. The DIM point clouds and the corresponding orthoimages achieved satisfying results at sub-urban areas. However, weak in urban areas with intense relief or depth discontinuities indicating excessive interpolations and radiometric lesions at building’s boundaries due to occlusions or possible mismatches. The aforementioned issues, both for LIDAR and DIM point clouds, affect the building extraction, change detection and roof segmentation results.Dependent on the data source employed, building extraction, change detection and roof segmentation techniques can be classified into three groups: i) the ones that use radiometric information (airborne or satellite imagery data), ii) the ones that exploit height information (LIDAR or DIM point clouds), and iii) those that combine both of data sources. However, the limitations of using information from multi-modal sources (e.g., LIDAR and imagery data) are the additional cost of acquisition and processing and the co-registration related issues. For this reason, the present doctoral dissertation focuses on the combination of data that are extracted from one sensor, i.e., either from camera devices (aerial images combined with the corresponding DIM point clouds) or from laser scanning systems (LIDAR point clouds combined with the corresponding proper geometric/morphological features). The proposed methodology achieves similar building extraction results with other approaches that utilize multi-modal sources. Also, the building extraction results extracted from the aerial images combined with the corresponding DIM point clouds were similar with those extracted from the LIDAR point clouds combined with the corresponding proper geometric/morphological features. Usually, the approaches of building extraction and change detection are discriminated into the ones that apply supervised machine learning and those that use model-based methods. In this research, both approaches are performed. More specifically, a deep machine learning scheme through CNNs (Convolutional Neural Networks) was developed as well as a new model-based scan line smooth filtering method was proposed. The extracted building detection results from the supervised machine learning and model-based methods can be considered as similar. This was verified by comparing the CNN with other model-based methods at the ISPRS (International Society for Photogrammetry and Remote Sensing) benchmark dataset. Also, in this research, the various factors that indicate the appropriateness of each of the two approaches are discussed. Additionally, the CNN was compared with other shallow and non-linear machine learning schemes such SVMs (Support Vector Machines) και ANN (Artificial Neural Network). The results indicate the clear predominance of the CNN. However, the CNN weak in computational complexity compared to the SVM classifiers. The use of a better CPU and more RAM capacity as well as the implementation of methods that accelerate the learning phase can significantly speed the processing. Concerning the roof segmentation from point clouds, the mostly used data driven plane detection techniques are region growing, RANSAC and Hough methods. The SHT (3D Standard Hough Transform) and its variation, the RHT (3D Randomized Hough Transform), are the most popular Hough methods. In this research, a new accumulator design and novel extensions of the RHT are proposed associated with: 1) additional constraint criteria during the random selection of the 3 points, 2) an additional normal tolerance regarding the detected plane, 3) automatic selection of sub-regions surround adaptive center points, and 4) automatic descent tuning process of parameters. Thus, five variations of the RHT were developed namely iRHT, ΕRHT, eΕRHT, APRHT-C and APRHT-R. A sensitivity analysis of each variation was carried out through extensive experiments using simulated data as well as real life LIDAR and DIM point clouds of several types of buildings. The iRHT led to under-segmented, spurious and over-segmented planes even for typical cases of buildings. The eERHT, APRHT-C and APRHT-R achieved better results compared to the ERHT at complex buildings and building structures of special architecture. The eERHT, APRHT-C and APRHT-R achieved similar results, however, the computational time of the eERHT was significantly higher compared to the two APRHTs. Nevertheless, the ERHT achieves satisfying results for typical cases of buildings after a post processing strategy absorbing spurious and over-segmented planes. The comparison with RANSAC indicated a clear predominance of the ERHT. However, under-segmented planes and local distortions at the boundaries of small planes occurred, which is challenge for further improvement of the refinement process. Connectivity criteria or strict tuning of the APRHT-C and APRHT-R may overcome the under-segmentation problem. The 3D models of the buildings extracted via the ERHT and APRHT-C were qualitative evaluated illustrating their potential for 3D modeling in LoD 1 and LoD 2.Η συνεχής τεχνολογική εξέλιξη στα πεδία της γεωπληροφορικής και ειδικότερα της επεξεργασίας εικόνας (image processing), της όρασης υπολογιστών (computer vision), της Φωτογραμμετρίας (photogrammetry) και της Τηλεπισκόπησης (remote sensing), παρέχει νέα εργαλεία και αυτοματοποιημένες λύσεις για εφαρμογή σε μελέτες πολεοδομίας, χωροταξίας και κτηματολογίου. Εκτός από τις παραδοσιακές τεχνικές, που κάνουν χρήση δεδομένων που περιέχουν αμιγώς ραδιομετρική πληροφορία (εναέριες ή δορυφορικές εικόνες) για την αυτόματη ανίχνευση και κατάτμηση αντικειμένων ενδιαφέροντος ή την ταξινόμηση μίας περιοχής μελέτης, τα τελευταία χρόνια, η επιστημονική κοινότητα έχει στραφεί στη χρήση τρισδιάστατων (3Δ) νεφών σημείων, που προέρχονται είτε από εναέρια συστήματα LIDAR (LIght Detection and Ranging) είτε από τεχνικές πυκνής συνταύτισης εικόνων (Dense Image Matching-DIM). Από την εκτενή ανασκόπηση και διερεύνηση της σχετικής διεθνούς βιβλιογραφίας, προέκυψε πως η ανίχνευση, η κατάτμηση και ο εντοπισμός μεταβολών κτισμάτων από εικόνες και νέφη σημείων αποτελούν θέματα με προοπτική, δυναμική και μεγάλο ερευνητικό ενδιαφέρον, λόγω των ολοένα και μεγαλύτερων απαιτήσεων για ακρίβεια, ταχύτητα και οικονομία. Στην παρούσα διατριβή γίνεται εκτενής και λεπτομερής προσέγγιση των επιστημονικών πεδίων της ανίχνευσης, της κατάτμησης και του εντοπισμού μεταβολών κτισμάτων από εικόνες και νέφη σημείων μέσα από την ανάπτυξη μίας ολοκληρωμένης μεθοδολογίας, με πρωτότυπα στοιχεία στα επιμέρους στάδιά της. Τα αποτελέσματα που προκύπτουν από το κάθε στάδιο αξιοποιούνται στο επόμενο έτσι ώστε τελικά να παραχθούν τα 3Δ μοντέλα κτισμάτων σε διάφορα επίπεδα λεπτομέρειας (Levels of Detail-LoDs). Πιο συγκεκριμένα, πραγματοποιήθηκε ποιοτική και ποσοτική σύγκριση μεταξύ νεφών σημείων από LIDAR και DIM, αλλά και φωτογραμμετρικών προϊόντων (ορθοεικόνες) σε σύνθετες αστικές περιοχές με ιδιαίτερη αρχιτεκτονική και έντονες ασυνέχειες βάθους όπως και σε ημι-αστικές περιοχές. Ανάμεσα στα συμπεράσματα που προέκυψαν αξίζει να σημειωθεί ότι στις ημι-αστικές περιοχές, όπου το ανάγλυφο ήταν πιο ήπιο και χωρίς πολλές ανθρωπογενείς κατασκευές, οι διαφορές μεταξύ των δύο τύπων νεφών σημείων ήταν μικρές. Αυτό είχε ως αποτελέσματα την επίτευξη παρόμοιων ακριβειών στις παραγόμενες ορθοεικόνες. Αντιθέτως, στις αστικές περιοχές όπου παρατηρήθηκαν έντονες ασυνέχειες βάθους και αποκρύψεις, τα νέφη σημείων από DIM παρουσίασαν σημαντικές τοπικές παραμορφώσεις στα όρια των αντικειμένων. Αυτό είχε επίπτωση και στις παραγόμενες ορθοεικόνες παρουσιάζοντας αντίστοιχες αλλοιώσεις. Για την εξαγωγή των βέλτιστων αποτελεσμάτων στη διαδικασία DIM, απαιτείται η κατάλληλη παραμετροποίηση του εκάστοτε εφαρμοζόμενου αλγορίθμου. Επίσης, η μικρή πυκνότητα του νέφους σημείων από LIDAR είχε ως αποτέλεσμα την απουσία σημαντικής πληροφορίας στα όρια κάποιων κτισμάτων ενώ παράλληλα κάποια μικρά κτίσματα περιγράφηκαν μερικώς. Ωστόσο, τα νέφη σημείων από LIDAR παρουσίασαν ομοιογενείς ακρίβειες στις παραγόμενες ορθοεικόνες των αστικών και των ημι-αστικών περιοχών. Γενικά, τα προβλήματα που εμφανίστηκαν στα όρια των κτισμάτων, τόσο στα νέφη σημείων όσο και στις παραγόμενες ορθοεικόνες, επηρεάζουν σημαντικά τις επόμενες διαδικασίες ανίχνευσης, κατάτμησης και εντοπισμού μεταβολών κτισμάτων.Στο πλαίσιο της διατριβής πραγματοποιήθηκε επίσης η διερεύνηση, αξιολόγηση και συλλογή κατάλληλων στοιχείων (κανάλια, δείκτες και χαρακτηριστικά) από τη διεθνή βιβλιογραφία με σκοπό την αξιοποίηση δεδομένων που προέρχονται μόνο από έναν δέκτη, δηλαδή είτε μόνο από φωτογραφική μηχανή (εικόνες ή και νέφη σημείων από DIM) είτε μόνο από εναέριο σαρωτή laser (νέφη σημείων από LIDAR). Δόθηκε έμφαση στα στοιχεία μέσω των οποίων επιτυγχάνεται ο διαχωρισμός των κτισμάτων από άλλα αντικείμενα όπως βλάστηση, έδαφος, δρόμοι κ.ά. Τα στοιχεία αυτά αφορούσαν είτε εναέριες εικόνες υψηλής ανάλυσης (ραδιομετρική πληροφορία) είτε νέφη σημείων από LIDAR ή DIM (υψομετρική πληροφορία). Αποδείχθηκε πως οι προτεινόμενες μεθοδολογίες που αξιοποιούν δεδομένα από ένα δέκτη και κάνουν είτε συνδυασμό ραδιομετρικής και υψομετρικής πληροφορίας (ορθοεικόνες με νέφη σημείων από DIM) είτε συνδυασμό κατάλληλων χαρακτηριστικών υψομετρικής πληροφορίας (για νέφη σημείων από LIDAR), πετυχαίνουν παρόμοιες ακρίβειες μεταξύ τους αλλά και παρόμοιες ακρίβειες με άλλες μεθοδολογίες που χρησιμοποιούν υψηλού κόστους ετερογενή δεδομένα (ορθοεικόνες με νέφη σημείων από LIDAR). Παράλληλα, αναπτύχθηκαν δύο προσεγγίσεις για την ανίχνευση κτισμάτων και τον εντοπισμό των μεταβολών τους οι οποίες εφαρμόζουν: 1) τεχνικές μηχανικής μάθησης (machine learning), και 2) τεχνικές παραμετρικής μοντελοποίησης (model-based). Οι μέθοδοι εφαρμόστηκαν σε πληθώρα περιοχών μελέτης (ελληνικών και μη) και διαφόρων τύπων κτισμάτων, όπως και δεδομένων με διαφορετική ανάλυση προερχόμενα από διάφορους δέκτες. Συγκεκριμένα, αναπτύχθηκαν ένας αλγόριθμος βαθιάς μηχανικής μάθησης μέσω CNN (Convolutional Neural Network ή αλλιώς Συνελικτικό Νευρωνικό Δίκτυο), και μια νέα τεχνική ενίσχυσης χαρακτηριστικών μέσω συνέλιξης ανά γραμμής σάρωσης (scan line smooth filtering). Τόσο τα πειραματικά αποτελέσματα όσο και οι συγκρίσεις με άλλες τεχνικές της διεθνούς βιβλιογραφίας σε σετ δεδομένων αναφοράς της ISPRS (International Society for Photogrammetry and Remote Sensing), έδειξαν πως και οι δύο προσεγγίσεις μπορούν να επιτύχουν παρόμοιες ακρίβειες. Η επιλογή για το ποια από τις δύο προσεγγίσεις θεωρείται καταλληλότερη για την κάθε εφαρμογή εξαρτάται από διάφορους παράγοντες οι οποίοι επισημαίνονται στην παρούσα έρευνα. To CNN συγκρίθηκε και με άλλες ρηχές και τυπικές τεχνικές μηχανικής μάθησης όπως SVMs (Support Vector Machines ή αλλιώς Μηχανές Υποστήριξης Διανυσμάτων) και ANN (Artificial Neural Network ή αλλιώς Τεχνητό Νευρωνικό Δίκτυο) και αναδείχθηκε η σαφής υπεροχή του στις επιτυγχανόμενες ακρίβειες χρησιμοποιώντας μάλιστα μικρά ποσοστά δειγμάτων εκπαίδευσης. Ωστόσο, ο συνολικός υπολογιστικός χρόνος εκτέλεσης του CNN και ειδικότερα ο χρόνος εκπαίδευσης του είναι πολύ μεγαλύτερος από τις τεχνικές SVMs. Το πρόβλημα του μεγάλου υπολογιστικού χρόνου του CNN μπορεί να μειωθεί σημαντικά κάνοντας χρήση ενός σταθμού εργασίας με μεγαλύτερες δυνατότητες. Η χρήση μονάδας GPU ή/και η εφαρμογή εξελιγμένων τεχνικών μάθησης μπορούν επίσης να επιταχύνουν τη διαδικασία μάθησης.Από την άλλη, ο SHT (3D Standard Hough Transform) και η μεταγενέστερη παραλλαγή του, ο RHT (3D Randomized Hough Transform), που μειώνει σημαντικά τον υπολογιστικό χρόνο του πρώτου, είναι δύο δημοφιλείς αλγόριθμοι, που ανήκουν στην κατηγορία των καθοδηγούμενων από τα δεδομένα μεθόδων (data driven) για την κατάτμηση οροφών κτισμάτων από νέφη σημείων. Αναπτύχθηκε νέα αρχιτεκτονική του συσσωρευτή στον παραμετρικό χώρο Hough και προτάθηκαν νέες επεκτάσεις του RHT, όπως: 1) πρόσθετοι γεωμετρικοί περιορισμοί αποκοπής για τη συλλογή της τριάδας σημείων, 2) πρόσθετο κριτήριο επιλογής για τον εντοπισμό των σημείων που ανήκουν στο προσδιορισθέν κυρίαρχο επίπεδο, 3) αυτόματη επιλογή υπο-περιοχών μέσω προσαρμοστικών σημείων, και 4) αυτόματη καθοδική ρύθμιση παραμέτρων. Βάσει των προτεινόμενων επεκτάσεων και της αρχιτεκτονικής του συσσωρευτή, προτάθηκαν πέντε παραλλαγές του RHT (iRHT, ΕRHT, eΕRHT, APRHT-C και APRHT-R) στις οποίες έγινε ανάλυση ευαισθησίας των παραμέτρων τους μέσα από εκτενείς πειραματικές εφαρμογές σε διάφορα είδη δεδομένων και τύπους κτισμάτων. Η εφαρμογή του iRHT σε τυπικές περιπτώσεις κτισμάτων οδήγησε σε υπο-κατατμημένα, πλασματικά και υπερ-κατατμημένα επίπεδα. Οι eERHT, APRHT-C και APRHT-R ανταποκρίθηκαν καλύτερα σε σχέση με τον ERHT σε κατασκευές με αρχιτεκτονικές ιδιαιτερότητες και σε σύνθετες περιπτώσεις κτισμάτων. Παρά το γεγονός πως οι eERHT, APRHT-C και APRHT-R εξαγάγουν παρόμοια αποτελέσματα, οι δύο APRHTs εκτελούνται σε αρκετά μικρότερο υπολογιστικό χρόνο σε σχέση με τον eERHT. Αυτό παρατηρήθηκε τόσο σε κατασκευές με αρχιτεκτονικές ιδιαιτερότητες όσο και σε σύνθετες περιπτώσεις κτισμάτων. Παρόλα αυτά, για τυπικές περιπτώσεις κτισμάτων, ο ERHT εξάγει ικανοποιητικά αποτελέσματα μετά το στάδιο της μετα-επεξεργασίας απορροφώντας κυρίως περιπτώσεις πλασματικών και υπερ-κατατμημένων επιπέδων. Μάλιστα, πέτυχε μεγαλύτερες ακρίβειες από άλλες τεχνικές της διεθνούς βιβλιογραφίας όπως ο RANSAC. Παρόλα αυτά, ακόμα και μετά το στάδιο της μετα-επεξεργασίας, εντοπίστηκαν κάποια τοπικά υπο-κατατμημένα επίπεδα. Αυτά τα επίπεδα μπορούν να απορροφηθούν είτε ενισχύοντας το στάδιο της μετα-επεξεργασίας με πρόσθετες τεχνικές είτε χρησιμοποιώντας αυστηρότερη παραμετροποίηση μέσω της εφαρμογής των APRHT-C και APRHT-R. Αξιολογώντας ποιοτικά τα 3Δ μοντέλα των κτισμάτων που εξήχθησαν με εφαρμογή των ERHT και APRHT-C, προκύπτει ότι μπορούν να αξιοποιηθούν σε πληθώρα εφαρμογών, καθώς καλύπτουν επαρκώς τις προδιαγραφές των LoD 1 και LoD 2",,Εθνικό Μετσόβιο Πολυτεχνείο (ΕΜΠ),Building extraction and change detection from images and point clouds,,,core
186309264,2019-01-16T00:00:00,"The popularity of mobile devices results in the availability of enormous data
and computational resources at the network edge. To leverage the data and
resources, a new machine learning paradigm, called edge learning, has emerged
where learning algorithms are deployed at the edge for providing fast and
intelligent services to mobile users. While computing speeds are advancing
rapidly, the communication latency is becoming the bottleneck of fast edge
learning. To address this issue, this work is focused on designing a low
latency multi-access scheme for edge learning. We consider a popular framework,
federated edge learning (FEEL), where edge-server and on-device learning are
synchronized to train a model without violating user-data privacy. It is
proposed that model updates simultaneously transmitted by devices over
broadband channels should be analog aggregated ""over-the-air"" by exploiting the
superposition property of a multi-access channel. Thereby, ""interference"" is
harnessed to provide fast implementation of the model aggregation. This results
in dramatical latency reduction compared with the traditional orthogonal access
(i.e., OFDMA). In this work, the performance of FEEL is characterized targeting
a single-cell random network. First, due to power alignment between devices as
required for aggregation, a fundamental tradeoff is shown to exist between the
update-reliability and the expected update-truncation ratio. This motivates the
design of an opportunistic scheduling scheme for FEEL that selects devices
within a distance threshold. This scheme is shown using real datasets to yield
satisfactory learning performance in the presence of high mobility. Second,
both the multi-access latency of the proposed analog aggregation and the OFDMA
scheme are analyzed. Their ratio, which quantifies the latency reduction of the
former, is proved to scale almost linearly with device population.Comment: This is an extended version of a submission to IEEE journa",,,"Broadband Analog Aggregation for Low-Latency Federated Edge Learning
  (Extended Version)",,http://arxiv.org/abs/1812.11494,core
201224345,2019-02-01T00:00:00,"Building Automation (BA) is key to encourage the growth of more sustainable cities and smart homes. However, current BA systems are not able to manage new constructions based on Adaptable/Dynamic Building Envelopes (ADBE) achieving near-zero energy-efficiency. The ADBE buildings integrate Renewable Energy Sources (RES) and Envelope Retrofitting (ER) that must be managed by new BA systems based on Artificial Intelligence (AI) and Internet of Things (IoT) through secure protocols. This paper presents the PLUG-N-HARVEST architecture based on cloud AI systems and security-by-design IoT networks to manage near-zero ADBE constructions in both residential and commercial buildings. To demonstrate the PLUG-N-HARVEST architecture, three different real-world pilots have been considered in Germany, Greece and Spain. The paper describes the Spain pilot of residential buildings including the deployment of IoT wireless networks (i.e., sensors and actuators) based on Zwave technology to enable plug-and-play installations. The real-world tests showed the high efficiency of security-by-design Internet communications between building equipment and cloud management systems. Moreover, the results of cloud intelligent management demonstrate the improvements in both energy consumption and comfort conditions","[{'title': 'Sensors', 'identifiers': ['issn:1424-8220', '1424-8220']}]",'MDPI AG',PLUG-N-HARVEST Architecture for Secure and Intelligent Management of Near-Zero Energy Buildings,10.3390/s19040843,,core
429404924,2019-11-14T00:00:00,"A growing trend in smart cities is the use of machine learning techniques to gather city data, formulate learning tasks and models, and use these to develop solutions to city problems. However, although these processes are sufficient for theoretical experiments, they often fail when they meet the reality of city data and processes, which by their very nature are highly distributed, heterogeneous, and exhibit high degrees of spatial and temporal variance. In order to address those problems, we have designed and implemented an integrated development environment called CityFlow that supports developing machine learning applications. With CityFlow, we can develop, deploy, and maintain machine learning applications easily by using an intuitive data flow model. To verify our approach, we conducted two case studies: deploying a road damage detection application to help monitor transport infrastructure and an automatic labeling application in support of a participatory sensing application. These applications show both the generic applicability of our approach, and its ease of use; both critical if we wish to deploy sophisticated ML based applications to smart cities. © 2020, Springer Nature Switzerland AG",,'Springer Science and Business Media LLC',CityFlow:Supporting Spatial-Temporal Edge Computing for Urban Machine Learning Applications,10.1007/978-3-030-28925-6_1,,core
343213519,2019-01-01T00:00:00,"Hybrid electric vehicles powered by fuel cells and batteries have attracted significant attention as they have the potential to eliminate emissions from the transport sector. However, fuel cells and batteries have several operational challenges, which require a power and energy management system (PEMS) to achieve optimal performance. Most of the existing PEMS methods are based on either predefined rules or prediction that are not adaptive to real-time driving conditions and may give solutions that are far from the actual optimal solution for a new drive cycle. Therefore, in this paper, an intelligent PEMS using reinforcement learning is presented, that can autonomously learn the optimal policy in real time through interaction with the onboard hybrid power system. This PEMS is implemented and tested on the simulation model of the onboard hybrid power system. The propulsion load is represented by the new European drive cycle. The results indicate that the PEMS algorithm is able to improve the lifetime of batteries and efficiency of the power system through minimizing the variation of the state of charge of battery",,'Institute of Electrical and Electronics Engineers (IEEE)',An Intelligent Power and Energy Management System for Fuel Cell/Battery Hybrid Electric Vehicle Using Reinforcement Learning,,,core
160762807,2019-10-13T00:00:00,"We consider transport layer approaches for achieving high rate, low delay
communication over edge paths where the bottleneck is an 802.11ac WLAN. We
first show that by regulating send rate so as to maintain a target aggregation
level it is possible to realise high rate, low delay communication over
802.11ac WLANs. We then address two important practical issues arising in
production networks, namely that (i) many client devices are non-rooted mobile
handsets/tablets and (ii) the bottleneck may lie in the backhaul rather than
the WLAN, or indeed vary between the two over time. We show that both these
issues can be resolved by use of simple and robust machine learning techniques.
We present a prototype transport layer implementation of our low delay rate
allocation approach and use this to evaluate performance under real radio
conditions",,,"Quick and Plenty: Achieving Low Delay and High Rate in 802.11ac Edge
  Networks",,http://arxiv.org/abs/1806.07761,core
268906452,2019-01-01T00:00:00,"Efficiency and safety are primary requirements for oil &amp; gas fluid filled transportation system. However, the complexity of the asset makes it challenging to derive a theoretical framework for managing the control parameters. The current frontier for a real time monitoring exploits the ""digital tansformation"", i.e. the acquisition and the analysis of large datasets recorded along the whole asset lifecycle, which are used to infer ""data driven"" relations and to predict the evolution of the asset integrity. This paper presents some results of a research project for the design, implementation and testing of a ""machine learning"" approach to vibroacoustic data recorded continuously by acquisition units installed every 10-20 km along a pipeline. In a fluid transportation system, vibroacoustic signals are generated by the flow regulation equipment (i.e. pumping, valves, metering), by the fluid flowing (i.e. turbulence, cavitation, bubbles), by third party interference (i.e. spillage, sabotage, illegal tapping), by internal inspection using PIGs operations), and by natural hazards (i.e. microseismic, subsidence, landslides). The basic principle of machine learning is to ""observe"", for an appropriate time interval, a series of descriptors, in this stage related to vibroacoustic signals but that can be integrated with other physical data (i.e. temperature, density, viscosity), in order to ""learn"" their safe range of variation or, when properly fed to a classification procedure, to obtain automatically a discrete set of operational status. The classification criteria are then applied to new data, highlighting the presence of system anomalies. The paper considers vibroacoustic signals collected at the flow stations of an oil trunkline in Nigeria. The vibroacoustic signals are the static pressure, the acceleration and the pressure transients recorded at the departure and at the arrival terminals. More than one year of data is available. Derived smart indicators are defined, which are directly linked to the asset parameters: for instance, the cross-correlation of the pressure transients at adjacent measuring locations permits to estimate the fluid channel continuity (correlation value), the sound velocity (time of correlation peak), and the sound attenuation (amplitude versus frequency amplitude decay). A portion of the data during normal operation is used for training and tuning a reference model. After that, new data are compared with the model, and anomalies are automatically detected. Two kind of errors are raised: i) sensors; ii) alerts. Sensor errors are referred to missing or corrupted sensors data. Alerts are raised when the measured physical quantities are not coherent with the functional and known service behaviors of the transport system. The system model is not static over time, and in fact it can be updated by the operators’ feedback, that can tag false alarms and thus, automatically, re-define the set of operational scenarios of the upstream system. The medium-long term construction and update of data driven models is effective for predictive maintenance, automatic anomalies detection, optimization of operational procedures. Moreover, the new policy of data management and the opportunity of gaining awareness by interconnecting the monitoring experience of different assets leverages the introduction of new technologies (cloud, big data), new professional figures (smart data scientist), new operational and business models",,'Society of Petroleum Engineers (SPE)',Data Driven Smart Monitoring for Pipeline Integrity Assessment,10.2118/197327-MS,,core
265350701,04/12/2019,"Predicting, in the one hand, the time duration that a vehicle remains associated to a cell i.e. Network Attachment Point (NAP) and, on the other hand, the next cell can help anticipating network control decisions to provide services with stringent requirements despite vehicle mobility. In this paper, we propose a machine learning based approach for Software Defined Vehicular Networks that allows a cell to estimate the attachment duration of each newly associated vehicle at the association request time, as well as, a prediction of the upcoming cell, performed at the SDN controller that controls the cells. Our proposed models have been evaluated on a large dataset, which we have generated based on a real mobility trace from the city of Luxembourg, and the evaluation shows promising results in terms of prediction accuracy",,HAL CCSD,Effective Prediction of V2I Link Lifetime and Vehicle's Next Cell for Software Defined Vehicular Networks: A Machine Learning Approach,,,core
334862401,2019-09-23T00:00:00,"Our research aims at developing intelligent systems to reduce the
transportation-related energy expenditure of a large city by influencing
individual behavior. We introduce COPTER - an intelligent travel assistant that
evaluates multi-modal travel alternatives to find a plan that is acceptable to
a person given their context and preferences. We propose a formulation for
acceptable planning that brings together ideas from AI, machine learning, and
economics. This formulation has been incorporated in COPTER that produces
acceptable plans in real-time. We adopt a novel empirical evaluation framework
that combines human decision data with a high fidelity multi-modal
transportation simulation to demonstrate a 4\% energy reduction and 20\% delay
reduction in a realistic deployment scenario in Los Angeles, California, USA",,'AI Access Foundation',"Acceptable Planning: Influencing Individual Behavior to Reduce
  Transportation Energy Expenditure of a City",10.1613/jair.1.11352,http://arxiv.org/abs/1909.10614,core
334881123,2019-11-09T00:00:00,"In recent years, artificial intelligence (AI) based on deep learning (DL) has
sparked tremendous global interest. DL is widely used today and has expanded
into various interesting areas. It is becoming more popular in cross-subject
research, such as studies of smart city systems, which combine computer science
with engineering applications. Human action detection is one of these areas.
Human action detection is an interesting challenge due to its stringent
requirements in terms of computing speed and accuracy. High-accuracy real-time
object tracking is also considered a significant challenge. This paper
integrates the YOLO detection network, which is considered a state-of-the-art
tool for real-time object detection, with motion vectors and the Coyote
Optimization Algorithm (COA) to construct a real-time human action localization
and tracking system. The proposed system starts with the extraction of motion
information from a compressed video stream and the extraction of appearance
information from RGB frames using an object detector. Then, a fusion step
between the two streams is performed, and the results are fed into the proposed
action tracking model. The COA is used in object tracking due to its accuracy
and fast convergence. The basic foundation of the proposed model is the
utilization of motion vectors, which already exist in a compressed video bit
stream and provide sufficient information to improve the localization of the
target action without requiring high consumption of computational resources
compared with other popular methods of extracting motion information, such as
optical flows. This advantage allows the proposed approach to be implemented in
challenging environments where the computational resources are limited, such as
Internet of Things (IoT) systems.Comment: SUBMITTED TO IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING
  SYSTEM",,,"A Proposed Artificial intelligence Model for Real-Time Human Action
  Localization and Tracking",,http://arxiv.org/abs/1911.04469,core
200815366,2019-04-05T00:00:00,"Urban traffic optimization using traffic cameras as sensors is driving the
need to advance state-of-the-art multi-target multi-camera (MTMC) tracking.
This work introduces CityFlow, a city-scale traffic camera dataset consisting
of more than 3 hours of synchronized HD videos from 40 cameras across 10
intersections, with the longest distance between two simultaneous cameras being
2.5 km. To the best of our knowledge, CityFlow is the largest-scale dataset in
terms of spatial coverage and the number of cameras/videos in an urban
environment. The dataset contains more than 200K annotated bounding boxes
covering a wide range of scenes, viewing angles, vehicle models, and urban
traffic flow conditions. Camera geometry and calibration information are
provided to aid spatio-temporal analysis. In addition, a subset of the
benchmark is made available for the task of image-based vehicle
re-identification (ReID). We conducted an extensive experimental evaluation of
baselines/state-of-the-art approaches in MTMC tracking, multi-target
single-camera (MTSC) tracking, object detection, and image-based ReID on this
dataset, analyzing the impact of different network architectures, loss
functions, spatio-temporal models and their combinations on task effectiveness.
An evaluation server is launched with the release of our benchmark at the 2019
AI City Challenge (https://www.aicitychallenge.org/) that allows researchers to
compare the performance of their newest techniques. We expect this dataset to
catalyze research in this field, propel the state-of-the-art forward, and lead
to deployed traffic optimization(s) in the real world.Comment: Accepted for oral presentation at CVPR 2019 with review ratings of 2
  strong accepts and 1 accept (work done during an internship at NVIDIA",,,"CityFlow: A City-Scale Benchmark for Multi-Target Multi-Camera Vehicle
  Tracking and Re-Identification",,http://arxiv.org/abs/1903.09254,core
200872247,2019-01-01T00:00:00,"Navigation is a key element influencing fluent, rapid, and safe transport of people and goods. During the last years, special attention was paid to satellite navigation, which is a part of radionavigation where positioning is done thanks to artificial satellites. Issues of application and development of satellite navigation systems in civil aviation are the subject of numerous research and scientific studies in the world. The quality of satellite signal determined by parameters such as accuracy, continuity, availability, and integrity determines possibility of its operational use. Particular attention of scientific research is therefore devoted to the requirements and limitations imposed on satellite systems prior to their implementation in aviation. This extremely important aspect justified undertaking of the aforementioned problem in this article. The paper attempts to answer the question on how to facilitate selection of navigation techniques for the aircraft operator, taking into account factors determining the accuracy, continuity, availability, and integrity of the satellite signal. As a result, the purpose of the work was defined as development of a method for forecasting the values of satellite navigation signal parameters used in air transport by artificial neural networks, taking into account selected atmospheric conditions. Results included in the work indicate further directions of satellite navigation system development. Due to authors’ opinion, the researches should focus especially on the analysis of real-time satellite signal parameter performance or creating applications for UAVs automatically deciding about used techniques of navigation","[{'title': 'International Journal of Aerospace Engineering', 'identifiers': ['issn:1687-5966', 'issn:1687-5974', '1687-5974', '1687-5966']}]",'Hindawi Limited',Forecasting Parameters of Satellite Navigation Signal through Artificial Neural Networks for the Purpose of Civil Aviation,10.1155/2019/7632958,,core
289162380,2019-01-01T00:00:00,"Emergency personnel, such as firefighters, bomb technicians, and urban search and rescue specialists, can be exposed to a variety of extreme hazards during the response to natural and human-made disasters. In many of these scenarios, a risk factor is the presence of hazardous airborne chemicals. The recent and rapid advances in robotics and sensor technologies allow emergency responders to deal with such hazards from relatively safe distances. Mobile robots with gas-sensing capabilities allow to convey useful information such as the possible source positions of different chemicals in the emergency area. However, common gas sampling procedures for laboratory use are not applicable due to the complexity of the environment and the need for fast deployment and analysis. In addition, conventional gas identification approaches, based on supervised learning, cannot handle situations when the number and identities of the present chemicals are unknown. For the purpose of emergency response, all the information concluded from the gas detection events during the robot exploration should be delivered in real time. To address these challenges, we developed an online gas-sensing system using an electronic nose. Our system can automatically perform unsupervised learning and update the discrimination model as the robot is exploring a given environment. The online gas discrimination results are further integrated with geometrical information to derive a multi-compound gas spatial distribution map. The proposed system is deployed on a robot built to operate in harsh environments for supporting fire brigades, and is validated in several different real-world experiments of discriminating and mapping multiple chemical compounds in an indoor open environment. Our results show that the proposed system achieves high accuracy in gas discrimination in an online, unsupervised, and computationally efficient manner. The subsequently created gas distribution maps accurately indicate the presence of different chemicals in the environment, which is of practical significance for emergency response.Funding Agency:European Commission  645101</p",,'MDPI AG',Towards Gas Discrimination and Mapping in Emergency Response Scenarios Using a Mobile Robot with an Electronic Nose,10.3390/s19030685,,core
390020632,2019-06-26T00:00:00,"The economic-legal analysis of the state and trends of the development of technologies of artificial intelligence (AI) has been carried out. The influence of AI on the development of society, economic effect, meth­ods and the field of application, the state of developments in the world and Ukraine are analyzed. In the next decade, AI will become the main market trend and the best business tool. The contribution of intellectual technologies to global GDP is estimated at 15.7 trillion. dollars In the next 5-10 years, China will be the leader in the success­ful operation and adaptation of AI technologies. According to analysts, the most bene­fit from AI technologies will be in the areas of financial services, retail and medicine.The scientific and inventive activity in the sphere of AI, the role of protection of in­tellectual property (patent and copyright), and the maintenance of the balance of com­peting interests are researched. Recently, the number of inventions based on AI has sharply increased. The leaders in the number of such inventions are American compa­nies IBM and Microsoft. This growth is due to the fact that in recent years AI has evolved from the theoretical concept into a real product that gains the world market. Since the advent of AI in the 50’s of the last century, inventors and researchers have applied for almost 340 thousand inventions based on AI (as of the end of 2016) and published more than 1.6 million scientific articles. The transport sector, including au­tonomous vehicles, is one of the sectors with the highest rates of growth in the appli­cation of AI. China has become a global leader in increasing the number of patents in the AI sphere over the past five years.By the number of companies working in the sphere of AI, Ukraine is among the three leaders among the countries of Eastern Europe. There are 57 AI companies in Ukraine and it has 11 investorsGeneralized practice of state regulation of activity in the sphere of AI in industri­alized countries and EU countries. More and more countries are developing national AI strategies. Thus, 17 countries, including Canada, China, Denmark, France, India, South Korea and Taiwan, have already announced their AI strategies. Some of them invest billions of dollars in this area. China, for example, has invested more than $ 10 billion in this technological trend, followed by South Korea — $ 2 billion and France — $ 1.5 billion. Governmental structures from different countries are con­cerned about the need to develop relevant national strategies, programs and regula­tion of AI legislative level. Identified existing problems and suggested ways to solve them. Problems constraining the development of AI in Ukraine: the absence of a strategy for the development of AI, the domestic infrastructure for its work and the weakness of the business about existing fundamental scientific developments in the field of AI, insufficient for the implementation of AI level of digitalization of compa­nies, the lack of a high level of data work, and is also a misunderstanding of the im­plementation guidance in the AI company.В статье представлен экономико-правовой анализ состояния и тенденций развития технологий искусственного интеллекта (ИИ). Проанализировано влияние ИИ на развитие общества, экономический эф­фект, методы и области применения, состояние разработок в мире и Украине. Ис­следована научная и изобретательская активность в сфере ИИ, роль охраны ин­теллектуальной собственности (патентного и авторского права), обеспечение баланса конкурирующих интересов. Обобщена практика государственного регули­рования деятельности в сфере ИИ в промышленно развитых странах и странах ЕС. Выявлены существующие проблемы и предложены пути их решения.У статті подано економіко-правовий аналіз стану і тенденцій розвитку технологій штучного інтелекту (далі — ШІ). Проаналізовано вплив ШІ на розвиток суспільства, еко­номічний ефект, методи і галузі застосування, стан розробок у світі та Україні. Досліджено наукову та винахідницьку активність у сфері ШІ, роль охорони інтелекту­альної власності (патентного і авторського права), забезпечення балансу конкуруючих інтересів. Узагальнено практику державного регулювання діяльності у сфері ШІ в про­мислово розвинених країнах і країнах ЄС. Виявлено проблеми та запропоновано шляхи їх вирішення",,Науково-дослідний інститут інтелектуальної власності НAПрН України,Тенденції розвитку технології штучного інтелекту: економіко-правовий аспект,,,core
286393031,"October 28, 2019","Automated planning is a key Artificial Intelligence technology enabling Unmanned Aerial Systems (UAS) and the eminent reality of Urban Air Mobility (UAM). It produces plans, which formalize procedures often performed by humans. Plans differ from other kinds of computer programs in their ability to react and interact with a dynamically changing environment. Aviation plans must encode the procedural knowledge, reasoning capability, and capacity for multi-tasking held by competent human pilots. Correct execution of these plans (performed by software called an executive) in the dynamic airspace environment is vital to the success of each automated flight, and the safety of the vehicle and all things in its path. In the early 2000s NASA developed a plan representation language and executive called PLEXIL (Plan Execution Interchange Language) that has successfully been applied in several NASA aviation and UAS projects. Autonomy Operating System (AOS), Cockpit Hierarchical Automated Planning and Execution (CHAP-E), and ICAROUS are all projects that have used PLEXIL to help encode and automatically execute flight procedures, some normally performed by human pilots. AOS also automates a subset of pilot/Air Traffic Control communication towards enabling UAS entry into the National Airspace. PLEXIL has been open-source software since 2008 and has seen usage in a wide range of prototypical autonomy applications in academia, government, and industry. In this presentation, we describe PLEXIL and highlight its significant accomplishments in the aviation domain",,,Overview of the PLEXIL Plan Execution Technology and its Applications in Autonomous Piloting Projects at NASA,,https://core.ac.uk/download/pdf/286393031.pdf,core
186301409,2019-02-13T00:00:00,"Recent developments in intelligent transport systems (ITS) based on smart
mobility significantly improves safety and security over roads and highways.
ITS networks are comprised of the Internet-connected vehicles (mobile nodes),
roadside units (RSU), cellular base stations and conventional core network
routers to create a complete data transmission platform that provides real-time
traffic information and enable prediction of future traffic conditions.
However, the heterogeneity and complexity of the underlying ITS networks raise
new challenges in intrusion prevention of mobile network nodes and detection of
security attacks due to such highly vulnerable mobile nodes. In this paper, we
consider a new type of security attack referred to as crossfire attack, which
involves a large number of compromised nodes that generate low-intensity
traffic in a temporally coordinated fashion such that target links or hosts
(victims) are disconnected from the rest of the network. Detection of such
attacks is challenging since the attacking traffic flows are indistinguishable
from the legitimate flows. With the support of software-defined networking that
enables dynamic network monitoring and traffic characteristic extraction, we
develop a machine learning model that can learn the temporal correlation among
traffic flows traversing in the ITS network, thus differentiating legitimate
flows from coordinated attacking flows. We use different deep learning
algorithms to train the model and study the performance using Mininet-WiFi
emulation platform. The results show that our approach achieves a detection
accuracy of at least 80%.Comment: This paper has been accepted for publication in the proceeding of
  IEEE VTC2019-Sprin",,,"Crossfire Attack Detection using Deep Learning in Software Defined ITS
  Networks",,http://arxiv.org/abs/1812.03639,core
186322364,2019-02-04T00:00:00,"Systematic testing of autonomous vehicles operating in complex real-world
scenarios is a difficult and expensive problem. We present Paracosm, a reactive
language for writing test scenarios for autonomous driving systems. Paracosm
allows users to programmatically describe complex driving situations with
specific visual features, e.g., road layout in an urban environment, as well as
reactive temporal behaviors of cars and pedestrians. Paracosm programs are
executed on top of a game engine that provides realistic physics simulation and
visual rendering. The infrastructure allows systematic exploration of the state
space, both for visual features (lighting, shadows, fog) and for reactive
interactions with the environment (pedestrians, other traffic). We define a
notion of test coverage for Paracosm configurations based on combinatorial
testing and low dispersion sequences. Paracosm comes with an automatic test
case generator that uses random sampling for discrete parameters and
deterministic quasi-Monte Carlo generation for continuous parameters. Through
an empirical evaluation, we demonstrate the modeling and testing capabilities
of Paracosm on a suite of autonomous driving systems implemented using deep
neural networks developed in research and education. We show how Paracosm can
expose incorrect behaviors or degraded performance",,,Paracosm: A Language and Tool for Testing Autonomous Driving Systems,,http://arxiv.org/abs/1902.01084,core
186567444,2019,"Modern water system infrastructures are equipped with a large amount of sensors. In recent years machine-learning (ML) algorithms became a promising option for data analysis. However, currently ML algorithms are not frequently used in real-world applications. One reason is the costly and time-consuming integration and maintenance of ML algorithms by data scientists. To overcome this challenge, this paper proposes a generic, adaptable platform for real-time data analysis in water distribution networks. The architecture of the platform allows to connect to different types of data sources, to process its measurements in realtime with and without ML algorithms and finally pushing the results to different sinks, like a database or a web-interface. This is achieved by a modular, plugin based software architecture of the platform. As a use-case, a data-driven anomaly detection algorithm is used to monitor the water quality of several water treatment plants of the city of Berlin",,,Web-based Machine Learning Platform for Condition-Monitoring,10.1007/978-3-662-58485-9_5,,core
218819464,2019-04-04T00:00:00,"The ubiquitous presence of devices with computational resources and connectivity is fostering the diffusion of the Internet of Things (IoT), where smart objects interoperate and react to the available information providing services to the users. The pervasiveness of the IoT across many different areas proves the worldwide interest of researchers from academic and enterprises worlds. This Research has brought to new technologies and protocols addressing different needs of emerging scenarios, making difficult to develop interoperable applications.



The Web of Things is born to address this problem through the standard protocols responsible for the success of the Web. But a greater contribution can be provided by standards of the Semantic Web. Semantic Web protocols grant univocal identification of resources and representation of data in a way that information is machine understandable and computable and such that information from different sources can be easily aggregated. Semantic Web technologies are then interoperability enablers for the IoT.



This Thesis investigates how to employ Semantic Web protocols in the IoT, to realize the Semantic Web of Things (SWoT) vision of an interoperable network of applications. Part I introduces the IoT, Part II investigates the algorithms to efficiently support the publish/subscribe paradigm in semantic brokers for the SWoT and their implementation in Smart-M3 and SEPA. The preliminary work toward the first benchmark for SWoT applications is presented. Part IV describes the Research activity aimed at applying the developed semantic infrastructures in real life scenarios (electro-mobility, home automation, semantic audio and Internet of Musical Things). Part V presents the conclusions.



A lack of effective ways to explore and debug Semantic Web datasets emerged during these activities. Part III describes a second Research aimed at devising of a novel way to visualize semantic datasets, based on graphs and the new concept of Semantic Planes.La presenza massiva di dispositivi dotati di capacità computazionale e connettività sta alimentando la diffusione di un nuovo paradigma nell'ICT, conosciuto come Internet of Things. L'IoT è caratterizzato dai cosiddetti smart object che interagiscono, cooperano e reagiscono alle informazioni a loro disponibili per fornire servizi agli utenti. La diffusione dell'IoT su così tante aree è la testimonianza di un interesse mondiale da parte di ricercatori appartenenti sia al mondo accademico che a quello industriale. La Ricerca ha portato alla nascita di tecnologie e protocolli progettati per rispondere ai diversi bisogni degli scenari emergenti, rendendo difficile sviluppare applicazioni interoperabili.



Il Web of Things (WoT) è nato per rispondere a questi problemi tramite l'adozione degli standard che hanno favorito il successo del Web. Ma un contributo maggiore può venire dal Semantic Web of Things (SWoT). Infatti, i protocolli del Semantic Web permettono identificazione univoca delle risorse e una rappresentazione dei dati tale che le informazioni siano computabili e l'informazione di differenti fonti facilmente aggregabile. Le tecnologie del Semantic Web sono quindi degli interoperability enabler per l'IoT.



Questa Tesi analizza come adottare le tecnologie del Semantic Web nell'IoT per realizzare la visione del SWoT di una rete di applicazioni interoperabile. Part I introduce l'IoT, Part II analizza gli algoritmi per supportare il publish-subscribe nei broker semantici e la loro implementazione in Smart-M3 e SEPA. Inoltre, viene presentato il lavoro preliminare verso il primo benchmark per applicazioni SWoT. Part IV discute l'applicazione dei risultati a diversi domini applicativi (mobilità elettrica, domotica, semantic audio ed Internet of Musical Things). Part V presenta le conclusioni sul lavoro svolto.



La Ricerca su applicazioni semantiche ha evidenziato carenze negli attuali software di visualizzazione. Quindi, Part III presenta un nuovo metodo di rappresentazione delle basi di conoscenza semantiche basato sull’approccio a grafo che introduce il concetto di Semantic Plane",,,"Semantic Web and the Web of Things: concept, platform and applications",,,core
300023006,2019-01-01T00:00:00,"Video Surveillance has been the most important input for the Intelligent Transportation System (ITS). A significant amount of research on the Video Surveillance Intelligent Transportation System is focused on the automatic traffic flow rate counting and vehicle type detection. Artificial Intelligence (AI) with matching learning method had been widely used in this field in recent years. However, machine learning consumes a significant amount of computing power and unpredictable at some incidents.
We are presenting a way to robustly detect the real-time traffic flowrate and classify the vehicle type (bus, car, motorbike) using basic video processing technologies from a surveillance camera video input. We are using video processing open and close operation to detect vehicles from the background as an object. Then we implement Kalman filter predict the movement of this vehicle object. Base on the vehicle object velocity and location from Kalman filter, we using Hungarian algorism to assign the vehicle object to vehicle tracks that detected before. Then we classify all tracks base on the vehicle object existing time and size inside a small region of interest (ROI) to recognize the vehicle object as an actual vehicle and they type of vehicle (bus, car, motorbike). Our major contribution is to find a way that is not consuming too many computational powers as machine learning to detect and classify the vehicle. The key technology is that we find out the width of the vehicle object size in a small region of interest (ROI) on the road can easily classify the type of vehicle (bus, car, motorbike).  We also construct a robust vehicle counting and real-time traffic flow rate computing system based on the previous work. This software can easily be implemented in any traffic surveillance camera as an input of the Intelligent Transportation System.Bachelor of Engineering (Electrical and Electronic Engineering",,,Video surveillance for intelligent transportation system,,,core
395096800,2019-12-01T00:00:00,"The goal of this thesis is to provide algorithms and models for classification, gesture recognition and anomaly detection  with a partial focus on human activity. In applications where humans are involved, it is of paramount importance to provide robust and understandable algorithms and models. A way to accomplish this requirement is to use relatively simple and robust approaches, especially when devices are resource-constrained. The second approach, when a large amount of data is present, is to adopt complex algorithms and models and make them robust and interpretable from a human-like point of view. This motivates our thesis that is divided in two parts.



The first part of this thesis is devoted to the development of parsimonious algorithms for action/gesture recognition in human-centric applications such as sports and anomaly detection for artificial pancreas. The data sources employed for the validation of our approaches consist of a collection of time-series data coming from sensors, such as accelerometers or glycemic. The main challenge in this context is to discard (i.e. being invariant to) many nuisance factors that make the recognition task difficult, especially where many different users are involved. Moreover, in some cases, data cannot be easily labelled, making supervised approaches not viable. Thus, we present the mathematical tools and the background with a focus to the recognition problems and then we derive novel methods for: 

    (i) gesture/action recognition using sparse representations for a sport application;

    (ii) gesture/action recognition using a symbolic representations and its extension to the multivariate case;

    (iii) model-free and unsupervised anomaly detection for detecting faults on artificial pancreas.

These algorithms are well-suited to be deployed in resource constrained devices, such as wearables.



In the second part, we investigate the feasibility of deep learning frameworks where human interpretation is crucial. Standard deep learning models are not robust and, unfortunately, literature approaches that ensure robustness are typically detrimental to accuracy in general. However, in general, real-world applications often require a minimum amount of accuracy to be employed. In view of this, after reviewing some results present in the recent literature, we formulate a new algorithm being able to semantically trade-off between accuracy and robustness, where a cost-sensitive classification problem is provided and a given threshold of accuracy is required. In addition, we provide a link between robustness to input perturbations and interpretability guided by a physical minimum energy principle: in fact, leveraging optimal transport tools, we show that robust training is connected to the optimal transport problem. Thanks to these theoretical insights we develop a new algorithm that provides robust, interpretable and more transferable representations",,,"Learning interpretable representations for classification, anomaly detection, human gesture and action recognition",,,core
297086231,2019-02-01T00:00:00,"WiFi-based passive methods are becoming a common tool to count, estimate, and/or locate people. One area of applicability is the development of intelligent control system for traffic management in urban areas, so that these systems are able to take into account not only vehicles’ behaviors but also pedestrians’, as important actors in the road scenario. In this work, we present the performance evaluation in terms of accuracy of a WiFi-based passive method used to identify pedestrians, classify them as moving pedestrians or static pedestrians, and for the latter, to locate them in a traffic intersection. The proposed algorithm is implemented in a low-cost development board and tested through several experiments in a real outdoor scenario. Our proposal is compared with several classic Machine Learning (ML) algorithms, specifically with Binary Logistic Regression, Support Vector Classification, Gaussian Naive Bayes, Random Forest, and k-Nearest Neighbors. Results show that despite the simplicity of our method, the outcomes are similar or better than most of the ML techniques, without the expected complexity or computational requirements that the latter required",,'EDP Sciences',Counting and locating people in outdoor environments: a comparative experimental study using WiFi-based passive methods,10.1051/itmconf/20192401010,,core
370136624,2019-01-01T00:00:00,"The Internet of Things (IoT) infrastructure forms a gigantic network of interconnected and interacting devices. This infrastructure involves a new generation of service delivery models, more advanced data management and policy schemes, sophisticated data analytics tools, and effective decision making applications. IoT technology brings automation to a new level wherein nodes can communicate and make autonomous decisions in the absence of human interventions. IoT enabled solutions generate and process enormous volumes of heterogeneous data exchanged among billions of nodes. This results in Big Data congestion, data management, storage issues and various inefficiencies. Fog Computing aims at solving the issues with data management as it includes intelligent computational components and storage closer to the data sources. Often, an IoT-enabled infrastructure is shared among many users with various requirements. Sharing resources, sharing operational costs and collective decision making (consensus) among many stakeholders is frequently neglected. This research addresses an essential requirement for adaptive, autonomous and consensus-based Fog computational solutions which are able to support distributed and in-network schemes and policies. These network schemes and policies need to meet the requirements of many users. In this work, innovative consensus-based computational solutions are investigated. These proposed solutions aim to correlate and organise data for effective management and decision making in Fog. Instead of individual decision making, the algorithms aim to aggregate several decisions into a consensus decision representing a collective agreement, benefiting from the individuals variant knowledge and meeting multiple stakeholders requirements. In order to validate the proposed solutions, hybrid research methodology is involved that includes the design of a test-bed and the execution of several experiments. In order to investigate the effectiveness of the paradigm, three experiments were designed and validated. Real-life sensor data and synthetic statistical data was collected, processed and analysed. Bayesian Machine Learning models and Analytics were used to consolidate the design and evaluate the performance of the algorithms. In the Fog environment, the first scenario tests the Aggregation by Distribution algorithm. The solution contribute in achieving a notable efficiency of data delivery obtained with a minimal loss in precision. The second scenario validates the merits of the approach in predicting the activities of high mobility IoT applications. The third scenario tests the applications related to smart home IoT. All proposed Consensus algorithms use statistical analysis to support effective decision making in Fog and enable data aggregation for optimal storage, data transmission, processing and analytics. The final results of all experiments showed that all the implemented consensus approaches surpass the individual ones in different performance terms. Formal results also showed that the paradigm is a good fit in many IoT environments and can be suitable for different scenarios when applying data analysis to correlate data with the design. Finally, the design demonstrates that Fog Computing can compete with Cloud Computing in terms of accuracy with an added preference of locality",,,Consensus-Based Data Management within Fog Computing For the Internet of Things,,https://core.ac.uk/download/370136624.pdf,core
236315312,2019-01-01T08:00:00,"Across all industries, from manufacturing to services, decision-makers must deal day to day with the outcomes from past and current decisions that affect their business. Last-mile delivery is the term used in supply chain management to describe the movement of goods from a hub to final destinations. This research proposes a methodology that supports decision making for the execution of last-mile delivery operations in a supply chain. This methodology offers diverse, hybrid, and complementary techniques (e.g., optimization, simulation, machine learning, and geographic information systems) to understand last-mile delivery operations through data-driven decision-making. The hybrid modeling might create better warning systems and support the delivery stage in a supply chain. The methodology proposes self-learning procedures to iteratively test and adjust the gaps between the expected and real performance. This methodology supports the process of making effective decisions promptly, optimization, simulation, and machine learning models are used to support execution processes and adjust plans according to changes in conditions, circumstances, and critical factors. This research is applied in two case studies. The first one is in maritime logistics, which discusses the decision process to find the type of vessels and routes to deliver petroleum from ships to villages. The second is in city logistics, where a network of stakeholders during the city distribution process is analyzed, showing the potential benefits of this methodology, especially in metropolitan areas. Potential applications of this system will leverage growing technological trends (e.g., machine learning in supply chain management and logistics, internet of things). The main research impact is the design and implementation of a methodology, which can support real-time decisions and adjust last-mile operations depending on the circumstances. The methodology allows taking decisions under conditions of stakeholder behavior patterns like vehicle drivers, customers, locations, and traffic. As the main benefit is the possibility to predict future scenarios and plan strategies for the most likely situations in last-mile delivery. This will help determine and support the accurate calculation of performance indicators. The research brings a unified methodology, where different solution approaches can be used in a synchronized form, which allows researches and other interested people to see the connection between techniques. With this research, it was possible to bring advanced technologies in routing practices and algorithms to decrease operating cost and leverage the use of offline and online information, thanks to connected sensors to support decisions",,'Information Bulletin on Variable Stars (IBVS)',A Methodology for Data-Driven Decision-Making in Last Mile Delivery Operations,,https://core.ac.uk/download/236315312.pdf,core
390020643,2019-07-30T00:00:00,"The economic-legal analysis of the state and trends of the development of technologies of artificial intelligence (AI) has been carried out. The influence of AI on the development of society, eco­nomic effect, methods and the field of application, the state of developments in the world and Ukraine are analyzed. In the next decade, AI will become the main mar­ket trend and the best business tool. The contribution of intellectual technologies to global GDP is estimated at 15.7 trillion. dollars In the next 5-10 years, China will be the leader in the successful operation and adaptation of AI technologies. Ac­cording to analysts, the most benefit from AI technologies will be in the areas of fi­nancial services, retail and medicine.The scientific and inventive activity in the sphere of AI, the role of protection of in­tellectual property (patent and copyright), and the maintenance of the balance of com­peting interests are researched. Recently, the number of inventions based on AI has sharply increased. The leaders in the number of such inventions are American compa­nies IBM and Microsoft. This growth is due to the fact that in recent years AI has evolved from the theoretical concept into a real product that gains the world market. Since the advent of AI in the 50’s of the last century, inventors and researchers have applied for almost 340 thousand inventions based on AI (as of the end of 2016) and published more than 1.6 million scientific articles. The transport sector, including au­tonomous vehicles, is one of the sectors with the highest rates of growth in the appli­cation of AI. China has become a global leader in increasing the number of patents in the AI sphere over the past five years.By the number of companies working in the sphere of AI, Ukraine is among the three leaders among the countries of Eastern Europe. There are 57 AI companies in Ukraine and it has 11 investorsGeneralized practice of state regulation of activity in the sphere of AI in indus­trialized countries and EU countries. More and more countries are developing na­tional AI strategies. Thus, 17 countries, including Canada, China, Denmark, France, India, South Korea and Taiwan, have already announced their AI strate­gies. Some of them invest billions of dollars in this area. China, for example, has invested more than $ 10 billion in this technological trend, followed by South Korea — $ 2 billion and France — $ 1.5 billion. Governmental structures from dif­ferent countries are concerned about the need to develop relevant national strate­gies, programs and regulation of AI legislative level. Identified existing problems and suggested ways to solve them. Problems constraining the development of AI in Ukraine: the absence of a strategy for the development of AI, the domestic infra­structure for its work and the weakness of the business about existing fundamen­tal scientific developments in the field of AI, insufficient for the implementation of AI level of digitalization of companies, the lack of a high level of data work, and is also a misunderstanding of the implementation guidance in the AI company.В статье представлен экономико-правовой анализ состояния и тенденций развития технологий искусственного интеллекта (ИИ). Проанализировано влияние ИИ на развитие общества, экономический эф­фект, методы и области применения, состояние разработок в мире и Украине. Ис­следована научная и изобретательская активность в сфере ИИ, роль охраны ин­теллектуальной собственности (патентного и авторского права), обеспечение ба­ланса конкурирующих интересов. Обобщена практика государственного регулирования деятельности в сфере ИИ в промышленно развитых странах и странах ЕС. Выявлены существующие проблемы и предложены пути их решения.У статті подано економіко-правовий аналіз стану і тенденцій розвитку технологій штучного інтелекту (далі — ШІ). Проаналізовано вплив ШІ на розвиток суспільства, економічний ефект, методи і галузі застосування, стан розробок у світі та Україні. Досліджено наукову та винахідницьку активність у сфері ШІ, роль охорони інтелек­туальної власності (патентного і авторського права), забезпечення балансу конкурую­чих інтересів. Узагальнено практику державного регулювання діяльності у сфері ШІ в промислово розвинених країнах і країнах ЄС. Виявлено проблеми та запропонова­но шляхи їх вирішення",,Науково-дослідний інститут інтелектуальної власності НAПрН України,Тенденції розвитку технологій штучного інтелекту: економіко-правовий аспект (ч. 2),,,core
334892583,2019-12-01T00:00:00,"At high latitudes, many cities adopt a centralized heating system to improve
the energy generation efficiency and to reduce pollution. In multi-tier
systems, so-called district heating, there are a few efficient approaches for
the flow rate control during the heating process. In this paper, we describe
the theoretical methods to solve this problem by deep reinforcement learning
and propose a cloud-based heating control system for implementation. A
real-world case study shows the effectiveness and practicability of the
proposed system controlled by humans, and the simulated experiments for deep
reinforcement learning show about 1985.01 gigajoules of heat quantity and
42276.45 tons of water are saved per hour compared with manual control.Comment: Submitted to Information Processing in Sensor Networks (IPSN 2020",,,"Flow Rate Control in Smart District Heating Systems Using Deep
  Reinforcement Learning",,http://arxiv.org/abs/1912.05313,core
237416954,2019-01-01T00:00:00,"The Cyber-physical Systems (CPS) are a combination of integrated physical processes, networking and computation to be minored and controlled y embedded subsystems via networked systems with feedback loops to change their behaviour when needed. Whilst the increased use of CPS brings more threats to the public, and thus security problems in this area have become a global issue to make it necessary to develop new approaches for securing CPS. The CPS utilise three-level architecture based on the respective functions of each layer: the perception layer, the transmission layer, and the application layer. Security in specific, CPS applications is currently the most important security objective of CPS because it offers the importance of CPS in its improving functionalityThis chapter focuses on the application aspect which is more related to people's daily lives, and will present a real-time system including distributed multi-camera system that integrates computing and communicating capabilities with monitoring on people in the physical world, namely person re-identification in the cyber-physical surveillance systems. The increasing sophistication and diversity of threats to public security have been causing a critical demand for the development and deployment of reliable, secure, and time-efficient visual intelligent surveillance systems in smart cities. For example, visual surveillance for indoor environments, like metro stations, plays an important role both in the assurance of safety conditions for the public and in the management of the transport network. Recent progress in computer vision techniques and related visual analytics offers new prospects for an intelligent surveillance system. A major recent development is the massive success resulting from using deep learning techniques to enable a significant boosting to visual analysis performance and initiate new research directions to understand visual content. For example, convolutional neural networks have demonstrated superiority on modelling high-level visual concepts. It is expected that the development of deep learning and its related visual analytic methodologies would further influence the field of intelligent surveillance systems. In view of the high demand for a prevalent surveillance system by the metropolis communities, this chapter will introduce recent research based on deep neural networks and pipelines to the practitioners and human investigators undertaking forensic and security analysis of large volumes of open-world CCTV video data sourced from a large distributed multi-camera network covering complex urban environments with transport links. This chapter will address the challenges of using deep learning and related techniques to understand and promote the use of ubiquitous intelligent surveillance systems",,'Springer Science and Business Media LLC',Deep Learning in Person Re-identification for Cyber-Physical Surveillance Systems,10.1007/978-3-030-13057-2_3,,core
343824127,2019-01-01T00:00:00,"As the Internet of Things (IoT) continues to expand, there is a growing necessity for improved techniques to authenticate the identity of wireless transmitters to prevent unauthorized network access. In this dissertation, we develop a series of physical-layer authentication, or radio frequency (RF) fingerprinting, techniques which utilize methods from deep learning to train convolutional and recurrent neural network models to verify the identity of wireless transmitters which meet the IEEE 802.15.4 standard.

First, we develop a technique which utilizes a convolutional neural network (CNN) to identify or verify the identity of a transmitter from which a time-domain complex baseband signal was recorded.  This technique relies on an extensive pre-processing sequence to remove sources of potential bias and trivial features from the received waveforms, and derives an estimated error signal from each recording from which the CNN learns discriminatory features.  We demonstrate the effectiveness of the technique on a set of seven off-the-shelf ZigBee devices recorded outside in an urban environment, as well as in a laboratory environment with artificial noise over a wide-range of signal-to-noise ratios (SNRs).

Next, we train a series of models which utilize both convolutional and recurrent elements to improve the performance of the previous technique in the presence of high levels of noise and expand the evaluation to a larger set of twenty-five devices.  We evaluate several realistic scenarios, including the performance in typical multipath environments and the ability to correctly reject previously unseen devices.  In order to justify the proposed pre-processing sequence, we present experimental results that demonstrate weaknesses in fingerprint verification classifiers in which frequency synchronization is not performed.  Finally, we present a simple technique to reduce the amount of memory required for a collection of fingerprint models by up to 95% without loss of performance.

To further enhance the security of the trained fingerprint models, we propose a generative adversarial network (GAN) architecture and training procedure to provide additional training examples for the classifiers.  We show that fingerprint classifiers that are trained exclusively on real devices cannot reliably reject GAN-generated signals.  Furthermore, we illustrate that augmenting the training process of the fingerprint models with GAN-generated signals reduces this vulnerability, even if the GAN used for training and inference are different.

Finally, we assess the practicality of transferring an RF fingerprint model from one receiver to another.  Experimentally, we demonstrate significant degradation in classification performance when a fingerprint model is learned using signals recorded on one receiver and evaluated using signals recorded on another receiver.  First, we show that generalization may be improved by including multiple receivers in the training process.  Then, we develop a calibration procedure whereby models learned on a single receiver can be transferred without alteration to another receiver by learning a transformation function, implemented as a residual neural network, to model the variations between the two receivers.  We perform several experiments with ten commercial receivers to confirm the effectiveness of the technique under realistic constraints",,'Wiley',Deep Neural Networks for Radio Frequency Fingerprinting,10.13016/pv6o-8y0g,,core
322825115,2019-01-01T00:00:00,"Many direct and indirect methods, processes, and sensors available on the market today are used to monitor the occupancy of selected Intelligent Building (IB) premises and the living activities of IB residents. By recognizing the occupancy of individual spaces in IB, IB can be optimally automated in conjunction with energy savings. This article proposes a novel method of indirect occupancy monitoring using CO2, temperature, and relative humidity measured by means of standard operating measurements using the KNX (Konnex (standard EN 50090, ISO/IEC 14543)) technology to monitor laboratory room occupancy in an intelligent building within the Internet of Things (IoT). The article further describes the design and creation of a Software (SW) tool for ensuring connectivity of the KNX technology and the IoT IBM Watson platform in real-time for storing and visualization of the values measured using a Message Queuing Telemetry Transport (MQTT) protocol and data storage into a CouchDB type database. As part of the proposed occupancy determination method, the prediction of the course of CO2 concentration from the measured temperature and relative humidity values were performed using mathematical methods of Linear Regression, Neural Networks, and Random Tree (using IBM SPSS Modeler) with an accuracy higher than 90%. To increase the accuracy of the prediction, the application of suppression of additive noise from the CO2 signal predicted by CO2 using the Least mean squares (LMS) algorithm in adaptive filtering (AF) method was used within the newly designed method. In selected experiments, the prediction accuracy with LMS adaptive filtration was better than 95%.Web of Science1223art. no. 454","[{'title': 'Energies', 'identifiers': ['issn:1996-1073', '1996-1073']}]",'MDPI AG',Novel proposal for prediction of CO2 course and occupancy recognition in Intelligent Buildings within IoT,10.3390/en12234541,https://core.ac.uk/download/322825115.pdf,core
323103028,2019-08-30T00:00:00,"Dissertação (mestrado)—Universidade de Brasília, Faculdade de Tecnologia, Departamento de Engenharia Mecânica, 2019.A previsão e a evolução dos parâmetros climáticos (como radiação solar, índices de precipitações,
temperatura atmosférica e da umidade relativa) são informações importantes para a sociedade e
suas áreas de aplicação. As novas tecnologias (como estações meteorológicas inteligentes, colheitadeiras, robôs e drones) necessitam de informações atuais e futuras sobre as condições de
clima e tempo de sua localidade, para melhorar a eficiência no uso de recursos e permitir o funcionamento sustentável destes dispositivos. Adicionalmente os indicadores de clima e tempo para
a região amazônica apresentam baixa acurácia nas previsões com os modelos disponíveis, devido a fatores como a dinâmica tropical da região, instrumentação escassa e dificuldade logística
da região-acesso e energia elétrica. Este trabalho apresenta uma proposta de aplicação de algoritmos de Inteligência Artificial (IA), para dispositivos inteligentes com sistemas embarcados,
como: drones, robôs e estações meteorológicas, para previsão de classes de chuva (precipitação).
Como classificador foi utilizado o algoritmo SVM, combinados com o algoritmos bio-inspirados
- MOPSO e MODE. Para validação desta proposta foram utilizados dados reais de uma localidade da região amazônica, a cidade de Belém-PA. Essas técnicas foram propostas recentemente
na literatura e apresentaram boa capacidade de previsão de chuva na Europa, China e India. Os
resultados qualitativos e quantitativos, deste trabalho, demostraram que o desempenho destes algoritmos foram bons, e mostrou que seu desempenho depende das reais necessidades do problema
a ser aplicado. Os modelos apresentaram uma boa eficiência computacional, para aplicações em
sistemas embarcados, já que não requerem grandes recursos de hardware e software. Os modelos apresentaram, também, uma boa acurácia, boa precisão e recall eficientes, para as classes de
precipitação utilizadas, podendo ser implementado para previsão de curto prazo com baixo custo
computacional.Prediction and evolution of climate parameters (such as solar radiation, precipitation rates, atmospheric temperature and relative humidity) are important information for society and its application areas. New technologies (such as smart weather stations, harvesters, robots, and drones)
need current and future information on your local weather and weather conditions to improve
resource efficiency and enable these devices to function sustainably. Additionally, climate and
weather indicators for the Amazon region present low accuracy in the predictions with the available models, due to factors such as the region’s tropical dynamics, scarce instrumentation and
logistical difficulty of the access region and electricity. This work presents a proposal for the
application of Artificial Intelligence (AI) algorithms for intelligent devices in embedded systems,
such as: drones, robots and meteorological stations, to predict rainfall classes. For the classifier
the SVM algorithm was used, combined with the bio-inspired algorithms - MOPSO and MODE.
To validate this proposal, real data were used from a locality in the Amazon region, the city
of Belém-PA. These techniques were recently proposed in the literature and showed good rainfall prediction in Europe, China and India. The qualitative and quantitative results of this work
demonstrated that the performance of these algorithms were good and showed that their performance depends on the real needs of the problem to be applied. The models presented a good
computational efficiency, for applications in embedded systems, since they do not require great
hardware and software resources. The models also presented a good accuracy, good precision
and efficient recall for the precipitation classes used, being able to be implemented for short term
prediction with low computational cos",,,Previsão de precipitação usando máquinas de vetores de suporte visando sua implementação em sistemas embarcados,,https://core.ac.uk/download/323103028.pdf,core
286898266,2019-01-01T00:00:00,"Emergency personnel, such as firefighters, bomb technicians, and urban search and rescue specialists, can be exposed to a variety of extreme hazards during the response to natural and human-made disasters. In many of these scenarios, a risk factor is the presence of hazardous airborne chemicals. The recent and rapid advances in robotics and sensor technologies allow emergency responders to deal with such hazards from relatively safe distances. Mobile robots with gas-sensing capabilities allow to convey useful information such as the possible source positions of different chemicals in the emergency area. However, common gas sampling procedures for laboratory use are not applicable due to the complexity of the environment and the need for fast deployment and analysis. In addition, conventional gas identification approaches, based on supervised learning, cannot handle situations when the number and identities of the present chemicals are unknown. For the purpose of emergency response, all the information concluded from the gas detection events during the robot exploration should be delivered in real time. To address these challenges, we developed an online gas-sensing system using an electronic nose. Our system can automatically perform unsupervised learning and update the discrimination model as the robot is exploring a given environment. The online gas discrimination results are further integrated with geometrical information to derive a multi-compound gas spatial distribution map. The proposed system is deployed on a robot built to operate in harsh environments for supporting fire brigades, and is validated in several different real-world experiments of discriminating and mapping multiple chemical compounds in an indoor open environment. Our results show that the proposed system achieves high accuracy in gas discrimination in an online, unsupervised, and computationally efficient manner. The subsequently created gas distribution maps accurately indicate the presence of different chemicals in the environment, which is of practical significance for emergency response.Funding Agency:European Commission  645101</p",,'MDPI AG',Towards Gas Discrimination and Mapping in Emergency Response Scenarios Using a Mobile Robot with an Electronic Nose,10.3390/s19030685,,core
228125010,2019-10-11T00:00:00,"Solar energy is becoming one of the most attractive renewable sources. In many cases, due to a wide range of financial or installation limitations, off-grid small scale micro power panels are favoured as modular systems to power lighting in gardens or to be integrated together to power small devices such as mobile phone chargers and distributed smart city facilities and services. Manufacturers and systems' integrators have a wide range of options of micro-scale photo voltaic panels to choose from. This makes the selection of the right panel a challenging task and risky investment. To address this and to help manufacturers, this paper suggests and evaluates a novel approach based on integrating empirical lab-testing with short-term real data and neural networks to assess the performance of micro-scale photovoltaic panels and their suitability for a specific application in specific environment. The paper outlines the combination of lab testing power output under seasonal and hourly conditions during the year combined with environmental and operating conditions such as temperature, dust accumulation and tilt angle performance. Based on the lab results, a short in-situ experimental work is implemented and the performance over the year in the selected location in Kuwait is evaluated using deep learning neural networks. The findings of this approach are compared with simulation and long-term real data. The results show a maximum error of 23% of the neural network output when compared with the actual data, and a correlation values with previous work within 87.3% and 91.9% which indicate that the proposed approach could provide an experimental rapid and accurate assessment of the expected power output. Hence, supporting the rapid decision-making process for manufacturers and reducing investment risks",,'Elsevier BV',Rapid evaluation of micro-scale photovoltaic solar energy systems using empirical methods combined with deep learning neural networks to support systems' manufacturers,10.1016/j.jclepro.2019.118788,https://core.ac.uk/download/228125010.pdf,core
304653803,2019-09-11T19:12:45,"There is a growing interest in the application of new technologies to assist city management and improve the quality of life of its residents. A Smart-City is composed of several connected Internet of Things electronic devices used to manage the city's assets and resources. To harness the wide variability of coverage, bandwidth, and reliability offered by different technologies, Smart-City network providers are tending more toward the deployment of heterogeneous networks. These heterogeneous networks would be capable of providing different sets of services governed by their corresponding quality–of–service (QoS) capabilities. Different components of the network system need to be upgraded to take in the new functionalities that come with heterogeneous networks. The lower layers must diligently control the access parameters in order to maintain the network connectivity without compromising the QoS. The network layer will have to be aware of the state of the lower layers in order to enhance routing strategies based on several optimization parameters and continuously adapt itself as the system state evolves. Subsequently, the upper application layer must be aware of the conditions of the system and adequately adapt its connectivity requirements. Here, we investigate ways to optimize heterogeneous networks components from multiple points of view. We pay particular attention to routing challenges in multi-radio multi-channel ad hoc networks as well as the challenges the mobility of the network brings.In this dissertation, we propose two methods to balance the traffic load of data flows. First, we design a split-path routing method that assigns multiple paths to data streams given the network state and the flow requirements. Results show that each strategy of path assignment yield different results, and can be used in different situations depending on the network application scenario. Next, we created a distributed routing protocol that uses the past routing decisions to compute the next one by combining multiple metrics such as energy levels and link utilization.Furthermore, we explore the positioning and mobility problems in ad hoc networks. The physical position, as well as the number of nodes in the network, can drastically affect the overall performance of the network since the main medium of transmission is the air. We propose an algorithm to reduce the number of devices necessary to guarantee coverage of a given map. The algorithm runs in linear complexity, which is advantageous for large scale systems. Then, we propose new mobility models for 3D networks to account for urban areas where buildings and other obstacles are presentWe also perform a preliminary study on machine learning (ML) assisted networks. We explore the use of ML models in a cross-layer routing scenario to limit the dissemination of control packets. Finally, we developed a testbed that enables energy-aware prototyping in real IoT devices using reusable code from the simulation phase, bridging the gap between theoretical protocols to the production environment",,,Smart-Cities Enabled by Heterogeneous Networks,,,core
200809994,2019-03-09T00:00:00,"The success of modern ride-sharing platforms crucially depends on the profit
of the ride-sharing fleet operating companies, and how efficiently the
resources are managed. Further, ride-sharing allows sharing costs and, hence,
reduces the congestion and emission by making better use of vehicle capacities.
In this work, we develop a distributed model-free, DeepPool, that uses deep
Q-network (DQN) techniques to learn optimal dispatch policies by interacting
with the environment. Further, DeepPool efficiently incorporates travel demand
statistics and deep learning models to manage dispatching vehicles for improved
ride sharing services. Using real-world dataset of taxi trip records in New
York City, DeepPool performs better than other strategies, proposed in the
literature, that do not consider ride sharing or do not dispatch the vehicles
to regions where the future demand is anticipated. Finally, DeepPool can adapt
rapidly to dynamic environments since it is implemented in a distributed manner
in which each vehicle solves its own DQN individually without coordination",,'Institute of Electrical and Electronics Engineers (IEEE)',"DeepPool: Distributed Model-free Algorithm for Ride-sharing using Deep
  Reinforcement Learning",10.1109/TITS.2019.2931830,http://arxiv.org/abs/1903.03882,core
287621918,2019-01-01T00:00:00,"In order for autonomous systems like robots, drones, and self-driving cars to be reliably introduced into our society, they must have the ability to actively account for safety during their operation. While safety analysis has traditionally been conducted offline for controlled environments like cages on factory floors, the much higher complexity of open, human-populated spaces like our homes, cities, and roads makes it unviable to rely on common design-time assumptions, since these may be violated once the system is deployed. Instead, the next generation of robotic technologies will need to reason about safety online, constructing high-confidence assurances informed by ongoing observations of the environment and other agents, in spite of models of them being necessarily fallible.This dissertation aims to lay down the necessary foundations to enable autonomous systems to ensure their own safety in complex, changing, and uncertain environments, by explicitly reasoning about the gap between their models and the real world. It first introduces a suite of novel robust optimal control formulations and algorithmic tools that permit tractable safety analysis in time-varying, multi-agent systems, as well as safe real-time robotic navigation in partially unknown environments; these approaches are demonstrated on large-scale unmanned air traffic simulation and physical quadrotor platforms. After this, it draws on Bayesian machine learning methods to translate model-based guarantees into high-confidence assurances, monitoring the reliability of predictive models in light of changing evidence about the physical system and surrounding agents. This principle is first applied to a general safety framework allowing the use of learning-based control (e.g. reinforcement learning) for safety-critical robotic systems such as drones, and then combined with insights from cognitive science and dynamic game theory to enable safe human-centered navigation and interaction; these techniques are showcased on physical quadrotors—flying in unmodeled wind and among human pedestrians—and simulated highway driving. The dissertation ends with a discussion of challenges and opportunities ahead, including the bridging of safety analysis and reinforcement learning and the need to ``close the loop'' around learning and adaptation in order to deploy increasingly advanced autonomous systems with confidence",,"eScholarship, University of California",Game-Theoretic Safety Assurance for Human-Centered Robotic Systems,,https://core.ac.uk/download/287621918.pdf,core
328833056,2019-09-01T00:00:00,"WP4 of JERICO-NEXT aims to synthesize the project’s activities in the other WPs and gather the contributions around applied Joint Research Activity Projects (JRAPs) selected to benefit of and highlight JERICO-NEXT activities. In order to fulfil this objective, methodologies developed or improved in WP3 were applied in the JRAPs;the provision of data assembled and distributed was undertaken according to the WP5 recommendations; dedicated topical approaches of the scientific strategy matured jointly with WP1&4 (Deliverable D4.1) were applied within the JRAPs, providing then in return essential input to the future road map of the research infrastructures. Indeed, six JRAPs were implemented to address different key environmental issues and/or policy needs such as those considered by the MSFD, and according to the 6 JERICO-NEXT scientific areas:
1- JRAP#1 on pelagic biodiversity
2- JRAP#2 on benthic biodiversity
3- JRAP#3 on chemical contaminant occurrence and related biological responses
4- JRAP#4 on hydrography and transport
5- JRAP#5 on carbon fluxes and carbonate system
6- JRAP#6 on operational oceanography and forecasting.
These JRAPs were not intending to implement similar actions at each JERICO-RI site but only to a selection of sites/regions according to the consortium interests and requirements from local to regional scales. Consequently, it is paramount to regionally synthesize the preliminary results after deployments in JRAPs, which is the purpose of this document.
The main document is organised according to the following regions and sites:
	- Bay of Biscay (South East Bay of Biscay, Portuguese Margin and Nazaré Canyon,  Girond Mud patch and Bay of Brest)
	- Channel and North Sea
	- Kattegat and Skagerrak Sea
	- Baltic Sea
	- Norwegian Sea
	- Med. Sea: From Liguria to the Ibiza Channel
	- Med. Sea: Northern Adriatic Sea
	- Med. Sea: Cretan Sea.
Results for those regions are presented in chapter 3 after consideration of the regional or site specificities related to the most relevant scientific issues of the area and the most relevant societal and policy needs, respectively. As the project and the JRAPs were not a priori organised to fit with regional to local needs, the reader may identify a weak point in the way JERICO-NEXT is addressing scientific syntheses per region. Nevertheless, it is a preliminary work towards the regional structuration of the RI, as expected, and a significant effort was put forth to identify discrepancies in the level of regional integration, and the recommended way to progress on this issue is presented in Chapter 4.
 
Synthesis of main achievements per region of JERICO-RI
Bay of Biscay: SE BoB
	- Involved JRAPs: JRAPs #1 3 4 6
	- Progress in the study of coastal small scale and mesoscale features from the combined use of multiplatform in-situ and satellite data.
	- Progress in the application of innovative techniques (developed in WP3) in this area for data-gap filling, data-blending, advanced Lagrangian diagnostics and performing observing system experiments (OSEs) and observing system simulation experiments (OSSEs).
	- Success in the gathering of new high-resolution datasets from different surveys and actions in the area (ETOILE with MASTODON-2D moorings deployment and TNA BB-TRANS).
 
Bay of Biscay: Gironde and Bay of Brest
	- Involved JRAPs: JRAP #2
	- Success in gathering biological and biogeochemical observations on a major marine mudpatch located in a high energetic environment. New evidence on the major role of hydrodynamics in controlling benthic diversity and associated biogeochemical processes in this mudpatch.
	- Success in deriving high resolution spatial maps of clam dredging pressure in the Bay of Brest and in using these maps to show the deleterious effect of this activity on benthic diversity hosted by maerl beds.
	- Success in the field testing of several techniques and tools (e.g., Image acquisition via mobile platforms and sediment profiling, image processing via dedicated software, and O2 sediment microprofiling) developed within both JERICO-FP7 and JERICO-NEXT.
 
Bay of Biscay: Nazare Canyon
	- Involved JRAPs: JRAPs #4 & 6
	- Success in the implementation of a high-resolution model with data assimilation able to describe the energetic dynamics and coastal ocean impacts of this long submarine canyon.
	- Progress in understanding the crucial importance of real-time monitoring infrastructures (fixed platforms, HF radars) to the characterization and operational forecasting of coastal ocean areas marked by the presence of submarine canyons, view the energetic and short spatial scale processes that are associated with these topographic features and the impacts canyons promote on larger domains of the coastal ocean. 
	- Progress in understanding the dominant processes of subinertial dynamics in the Nazare Canyon area of influence, contributing to define the main components of a real-time monitoring system for this area.
 
Channel and North Sea
	- Involved JRAPs: JRAPs #1, 4, 5
	- Successful implementation of innovative (semi-)automated techniques for the monitoring of phytoplankton dynamics and C cycle in the English Channel and the North Sea  - an extended shelf system influenced by multiple sources of human pressure and contrasting hydrodynamical conditions
	- Some of these methods were compared with traditional laboratory analysis which helped to better address the added value of innovative techniques in terms of improving the spatial and temporal resolution (both in surface and in the water column), making it possible to consider functional and, sometimes, even taxonomical characterization of phytoplankton communities composition as well as photo-physiology, at high  resolution, in almost real-time.
	- New insights into the seasonality of the spatial distribution of delta partial pressure of carbon dioxide (pCO2) measured continuously on ships of opportunity at a regional scale in the North Sea, and the relation between marine sinks of CO2 with high total chlorophyll a fluorescence.
	- Automated techniques made it possible to characterize the size and functional composition of phytoplankton communities (from pico- to microphytoplankton) through the main bloom episodes including outburst of potential HABs as Pseudo-nitszchia spp. and Phaeocystis globosa (characterized as high or low red fluorescence nano-eukaryotes: Nano high and Low FLR) from the Eastern English Channel (EEC) towards the southern North Sea, in international cross-border (UK, FR, BE, NL) common research cruises, following the spatial and temporal succession of spring blooms.
	- There is a need to increase the combined implementation of innovative and reference techniques both on current monitoring of discrete stations as well as in continuous automated measurements performed on cruises and ships of opportunity (as FerryBoxes), in order to increase the spatial and temporal resolution of the surveys of the different eco-hydrodynamic regions of the area.
 
Kattegat and Skagerrak
	- Involved JRAPs: JRAPs #1, 3, 5, 6
	-  Harmful algae, phytoplankton diversity and abundance were observed in near real time at an aquaculture site on the Swedish west coast. In situ imaging flow cytometry combined with machine learning and wireless communications provided data every 20 minutes.
	- Data from HF radar, FerryBox, research vessels and oceanographic buoys were used together with results from the 3D-NEMO Nordic ocean circulation model and remote sensing to describe the Kattegat-Skagerrak system.
	- Data from a FerryBox system revealed, as expected, the occurrence of strong concentration gradients reflecting progressive dilution along the South-North transect and highlighted harbours areas (Oslo and Kiel) as hotspots for some chemical compounds.
	- Microbial molecular markers representing bacterial species and genes were used to identify hydrocarbon pollution or high nutrient loads.
	- Barcoding of phytoplankton and bacteria revealed previously unknown diversity in the pelagic communities (see deliverable D3.8).
	- Carbon fluxes and carbonate system variability in the Skagerrak/Kattegat region is primarily driven by changes in salinity resulting from the balance of freshwater inputs from riverine and Baltic sources and saline waters from the Atlantic Ocean.
 
Baltic
	- Involved JRAPs: JRAPs #1, 5, 6
	- Different technologies for phytoplankton research have been successfully evaluated in the Baltic Sea. Operational monitoring of phycoerythrin fluorescence started after in-depth study during JRAP1, which identified different origins of this signal. To study filamentous cyanobacteria blooms, various sensors were tested and they were largely complementary. New absorption method seems to provide reliable estimates for Chlorophyll-a concentration, but still lacks automated maintenance procedures. Better understanding was obtained on the range of conversion factor between electron transport rate (measured with fluorescence induction) and carbon fixation rate, as well as of the reasons behind this variability. 
	- Carbonate system components of the Baltic Sea showed large seasonal variability indicating high impact of biological activity for pH and pCO2. Alkalinity of the Baltic Sea is difficult to model from other carbonate system components and online sensors are required to understand its variability.
	- The joint studies between different (multinational) research groups using different technologies provided good know-how exchange and should be encouraged. As well, multidisciplinary research efforts, including physics, chemistry, biology and modelling, should be encouraged, to gain knowledge on the environmental challenges more in detail. 
 
Norwegian Sea
	- Involved JRAPs: JRAPs #1, 3, 6
	- The Norwegian Sea plays a major role as an area where potentially highly polluted waters from the North Sea mix with water transported from the North Atlantic Ocean. This water is then transported into the Arctic region. Analysis of the transport pathway of waterborne contaminants along the Norwegian coast was instrumental for assessing the spatial range of contaminants with different properties and address questions regarding exposure of the Arctic.
	- 42 currently used pesticides in Europe, 5 artificial sweeteners and 11 pharmaceuticals and personal care products were targeted during the study. Several compounds were detected in the Norwegian Sea, including current use pesticides, artificial food additives and some pharmaceuticals. Their presence in this coastal area, and also in high-latitude more open waters, highlight the potential for these contaminants to undergo long range transport with marine currents.
	- Seasonal variability in temperature in the coastal area (up to a 15 ˚C differential between summer and winter depending on latitude) is a large driving force on carbonate system variability, including a decrease in surface water fCO2 due to lower wintertime temperatures as well as the uptake of atmospheric CO2 as water cools during its northward journey from the North Atlantic to the Arctic Ocean.
	- In addition, a focal point was the improvement of systems that provides knowledge for the transport of parasites and harmful algae in the Norwegian Sea coastal area. Here the observations by FerryBoxes, fixed stations as well as repeated transects were used to validate and improve numerical model simulations.
 
Mediterranean Sea: Ligurian to Ibiza channel
	- Involved JRAPs: JRAPs #1, 4, 6
	- Major investigation effort was led in the Liguro-Provencal area and the Catalan Margin to study the variability of the Northern current through the combination of independent and complementary observational platforms. The dynamics of the boundary currents were studied to identify the interplay between various forcings (remote, thermohaline and wind), the generation of mesoscale and submesoscale instabilities, and data blending and assimilation techniques were investigated.
	- Major results have shown a clear correlation between hydrographic changes led by climatic interannual variability and the community composition of phytoplankton and zooplankton. The role of (sub)mesoscale processes have shown to modulate biochemical processes and to locally enhanced marine biomass productions/accumulation.
	- The multiplatform observing system in the NW Mediterranean Sea, combined with growing centralized frameworks of data management and distribution, i.e. Copernicus Marine Environment Monitoring Service and SeaDataNet/SeaDataCloud, will provide the basis for an extended European coastal infrastructure.
 
Mediterranean Sea: Adriatic Sea
	- Involved JRAPs: JRAPs #5 & 6
	- During the project, important steps forward have been made on the development of capabilities to integrate new kinds of experimental data and oceanographic models to support ecosystem management. The implementation of an Adriatic oceanographic model assimilating surface current from coastal radar and temperature profiles from fishing vessels (FOOS fleet) has been developed and tested, providing encouraging results. Surface currents from coastal radars were integrated with results from drifter deployments to investigate zones of recruitment for small pelagic fishes, highlighting the role of remote areas in supporting the ecological role of these environments.
	- One year of high frequency data of sea surface pCO2 was successfully gathered at a fixed station in the northernmost Adriatic. Results highlighted how the biological CO2 uptake during phytoplankton blooms was able to keep the central basin a strong CO2 sink not only in winter, when low temperatures favor CO2 dissolution, but through most of the year, even when temperatures raised above 25°C.
	- The work carried on so far highlighted the potentiality of the area, where historical data and many observational systems are available to support both ecosystem management and advanced marine researches. On the other hand, they pointed out the need for integration of the existing facilities and observational systems also at a trans-border level to address the climate and ecological challenges facing this basin.
 
Mediterranean Sea: Cretan Sea
	- Involved JRAPs: JRAPs #2 & 6
	- An interesting case of how the additional assimilation of glider profiles and FerryBox observations used in the OSE experiment is beneficial for the Aegean Sea forecasting system in terms of reducing the system biases. This improved the sea surface salinity model bias over the south Aegean (north of Crete and south of 37°N).
	- In the Cretan Sea, the vertical migration of mesopelagic organisms (macroplanktonic and micronektonic) was observed by acoustical means for almost 2.5 years in the epipelagic and mesopelagic layers. The observed organisms were categorized into four groups according to their migration patterns which appeared to occur at diel and seasonal scale. The variability of the migration patterns was inspected in relation to the physical and biological environmental conditions of the study area. Stratification of the water column does not act as a barrier for the vertical motion of the strongest migrants that move up to 400 m every day. Instead, changes in light intensity (lunar cycle, daylight duration, cloudiness) and the presence of prey and predators seem to explain the observed daily, monthly and seasonal.
 
This report clearly demonstrates how the consortium of the project is willing to progress on monitoring strategies in several JERICO studies (besides the simulation experiments for transport studies, the analysis of search radius from contaminant sources and the use of covariance in highly relevant low-concentration persistent contaminants, use of multi-functional sensors, etc.); despite the difficulty and time-consuming activity of operating both fixed and mobile platforms working in the highly dynamic complex and densely utilised coastal areas. By progressing on the integration of scientific fields, it also shows the benefit of operating several platforms types (fixed & mobile, at sea, remote, & numerical). For instance, we can emphasize what deployments in JRAPs have proved:
	- JRAP #1: interest in deploying complementary observing systems for algal blooms to get information interoperable at EU level.
	- JRAP #2: success in monitoring highly dynamic benthic ecosystems.
	- JRAP #3: possibility to successfully perform monitoring of contaminants in an interoperable manner.
	- JRAP #4: the highly resolved low-cost sensor and mooring deployments for specific transport or contaminant studies, which shows the intent to be cost-efficient. The complementarity of remote + at sea and numerical systems.
	- JRAP#5: coastal carbon fluxes and biogeochemical cycling: The relatively large variability of conditions keeps being a challenge for sensor developers, with necessary periodic calibration needs that are possible to tackle as shown.
	- JRAP#6: makes a strong case of the need for in-situ data vs models, in particular for coastal processes.
 
A vision: a possible geographical structure of JERICO-RI per region and site
As a consequence, JERICO-RI already proved his capability to gather information and tools to qualify and quantify processes, their scales, related challenges and the possible solution to progress on. As a next step, in agreement with regional stakeholder, JERICO-RI should develop regional forum/center to share information (data and products), expertises, practices, solution and training in line with regional purposes to support scientists and regional stakeholders. This would support application of policies and regulations, based on applied collaborations between scientists and other stakeholders to tackle common societo, environmental and scientific questions from local to regional scales.
According to the monitoring purposes in regions and sites, the need of integration in scientific fields is diverse and JERICO-NEXT presented only a first steps. In the future, JRAPs, TNA & regions should engage with outermost regions where regional projects take place and could be liaised with JERICO RI to better connect these regions in the coastal observing RI landscape. Because of these considerations, the consortium progressed towards regional integrated coastal observatories and preliminary elements are presented in chapter 4.
A main lesson learned is that societal challenges and priorities at the regional level are important elements for the structuring of a coastal observing system. Therefore, a key challenge for the future is to improve regionalisation of the observatories for a better understanding of region-specific processes and an improved fit-for-purpose of the JERICO-RI. Furthermore, the observatories need to be consolidated in terms of performance, reliability and variables to optimally address and answer to key regional and pan-European environmental challenges. The two above-mentioned aspects are the integration challenge that the consortium wishes to tackle by implementing a regional structure of JERICO-RI.
The structuring process will be challenging because coastal observatories are not operated by the same organisations and therefore may have differing objectives and means of operation (financial, logistical, etc.). Based on these differences, we have proposed that the coastal observing systems in JERICO-RI can be structured hierarchically in which all sizes and types of coastal observatories can function in JERICO-RI in an integrated and mutually beneficial way. This will be reported in deliverable D1.4. This way towards a regional structuring of JERICO-RI is included in the proposal for the 3rd project of the JERICO series of projects, JERICO-S3, selected for funding during 4 years and to start in early 2020. With regards to the WP4 of the present project, the final deliverable: D4.5 is in progress to report results of each of the six JRAP activities and will be available by Sept. 2019 on the JERICO-RI website",,,JERICO-NEXT. First valorisation results for each region,,,core
334839736,2019-07-24T00:00:00,"Decision support systems (e.g., for ecological conservation) and autonomous
systems (e.g., adaptive controllers in smart cities) start to be deployed in
real applications. Although their operations often impact many users or
stakeholders, no fairness consideration is generally taken into account in
their design, which could lead to completely unfair outcomes for some users or
stakeholders. To tackle this issue, we advocate for the use of social welfare
functions that encode fairness and present this general novel problem in the
context of (deep) reinforcement learning, although it could possibly be
extended to other machine learning tasks.Comment: Presented at the AI for Social Good Workshop at IJCAI 201",,,Fairness in Reinforcement Learning,,http://arxiv.org/abs/1907.10323,core
369839311,2019-06-19T00:00:00,"The aim of my research was to develop a digital mediation system with urban data for a pedestrian immersed in the city, a link based on digital technologies to design, analyze, represent urban space and access information on this urban space. Augmented Reality is one of the tools allowing this mediation whose critical element is the location of the pedestrian and more precisely the pose calculation of the camera it carries.Thus, the main focus of my work is geolocation on site using spatial data of different dimensions. I was interested in an upstream phase that requires the implementation of data models to keep track of spatial data changes. Finally, I touched on some uses of geolocation and pose calculation. I conclude this report by presenting my research perspectives on digital mediation with urban data for pedestrians.Le but de ma recherche a été de mettre au point un système de médiation numérique avec des données urbaines pour un piéton immergé dans la ville, un lien basé sur des technologies numériques pour concevoir, analyser, représenter l’espace urbain et accéder à des informations sur cet espace urbain. La réalité augmentée est un des outils permettant cette médiation dont l’élément critique est la localisation du piéton et plus précisément le calcul de pose de la caméra qu’il transporte.Ainsi, l’axe principal de mon travail est la géolocalisation sur site à l’aide de données spatiales de différentes dimensions. Je me suis intéressée à une phase amont qui nécessite la mise en place de modèles de données pour garder trace des modifications des données spatiales. J’ai enfin abordé quelques usages de la géolocalisation et du calcul de pose. Je conclus ce mémoire en présentant mes perspectives de recherches vers une médiation numérique avec des données urbaines pour le piéton",,HAL CCSD,Méthodes pour la géolocalisation du piéton sur site - vers une médiation numérique avec les données urbaines,,,core
286618536,2019-01-01T00:00:00,"1.	Abu-Taieh C., Evon J.: Technology Engineering and Management in Aviation: Advancements and Discoveries. Information Science Reference, 2011.
2.	Ajam M, Woolard C, Wiljoen CL. Biomass pyrolysis oil as a renewable feedstock for bio-jet fuel. In: Proceedings of the 13th international conference on stability, handling and use of liquid fuels (IASH2013), Rhodes, Greece; October 2013. p. 6–10.
3.	Аnnual report to Parliament on the renewable transport fuel obligation. Renewable Fuels Agency. The Stationery Office, 2011.
4.	Agarwal S., Chhibber V. K., Bhatnagar A. K.:Tribological behavior of diesel fuels and the effect of anti-wear additives. Fuel. Vol. 106, 2013, p. 21–29, 
5.	Alves S. M., Barros B.S., Trajano M.F.: Tribological behavior of vegetable oil-based lubricants with nanoparticles of oxides in boundary lubrication conditions. Tribology International. Vol. 65, 2013, p. 28–36.
6.	Asgari H., Chen X., Sainudiin R.: Modelling and simulation of gas turbines. International Journalof Modelling, Identification and Control, Vol.25, No.3, 2013, p. 1–15.
7.	Bartis James T. LaTourrette T., Dixon L.: Oil Shale Development in the United States: Prospects and Policy Issues. Santa Monica, Calif.: RAND Corporation, MG-414-NETL, 2005.
8.	Bassam N. El.: Handbook of Bioenergy Crops: A Complete Reference to Species. Development and Applications Earthscan, 2010.
9.	Bazazzadeh M., Badihi H., Shahriari A.: Gas Turbine Engine Control Design Using Fuzzy Logic and Neural Networks. International Journal of Aerospace Engineering. Vol. 1, 2011, p. 1–13. 
10.	Blakey S, Rye L, Wilson C.W.: Aviation gas turbine alternative fuels: A review.  P Combust Inst, No. 33, 2011, p. 2863–2885.
11.	Boichenko S., Iakovlieva A., Vovk O.: Traditional and alternative jet fuels: problems of quality standardization. Journal of Petroleum & Environmental Biotechnology. Vol. 4. Iss. 3, 2013.
12.	Boichenko S., Shkilniuk I., Turchak V.. The problems of biopollution with jet fuels and the way of achieving solution. Transport. 23, 2008; p. 253–257.
13.	Boichenko S., Yakovleva A. Prospects of biofuels introduction into aviation. Transport engineering and management: Proceedings of the 15-th conference for Lithuania Junior researchers. Science – future of Lithuania, 4 May 2012. Vilnius: Technika. p. 90–94.
14.	Boichenko S., Yakovlieva A., Gryshchenko O., Zinchuk A. Prospects of using different generations biofuels for minimizing impact of modern aviation on environment, Энерготехнологии и ресурсосбережение, № 1, 2018, p. 10–20.
15.	Boichenko S., Lejda K., Yakovlieva A., Vovk O. Comparative characteristics of low-temperature properties of jet fuels modified with bio-additives, International Automotive Conference (KONMOT2018). IOP Conf. Series: Materials Science and Engineering 421, 2018.
16.	Breil C., Meullemiestre A., Vian M., Chemat F.: Bio-Based Solvents for Green Extraction of Lipids from Oleaginous Yeast Biomass for Sustainable Aviation Biofuel. Molecules. Iss. 21(196), 2016, p. 1–14.
17.	Carels N., Sujatha M., Bahadur B.: Jatropha, Challenges for a New Energy Crop. Vol. 1: Farming, Economics and Biofuel. Springer Science & Business Media, 2012.
18.	Cavani F., Albonetti S., Basile F., Gandini A.: Chemicals and Fuels from Bio-Based Building Blocks. John Wiley & Sons, 2015.
19.	Cermak S. C., Evangelista R. L., Kenar J. A.: Distillation of Natural Fatty Acids and Their Chemical Derivatives, Distillation - Advances from Modeling to Applications, Dr. Sina Zereshki (Ed.), InTech, 2012. – р. 5. – 140. 
20.	Chai M. Thermal Decomposition of Methyl Esters in Biodiesel Fuel: Kinetics, Mechanisms and Products, Ph.D. Thesis, University оf Cincinnati, 2012.
21.	Chiaramonti D, Bonini M, Fratini E, Tondi G, Gartner K, Bridgwater AV, et al. Development of emulsion from biomass pyrolysis liquid and diesel and their use in engines – Part 1: emulsion production. Biomass Bioenergy, No. 25, 2003, p. 85–99.
22.	Chiaramonti D, Bonini M, Fratini E, Tondi G, Gartner K, Bridgwater AV, et al. Development of emulsion from biomass pyrolysis liquid and diesel and their use in engines – Part 2: tests in diesel engines. Biomass Bioenergy, No. 25, 2003, p. 101–11.
23.	Chuck C.J., Donnelly J.: The compatibility of potential bioderived fuels with Jet A-1 aviation kerosene. Applied Energy. Vol. 118, 2014, p. 83–91.
24.	Cleveland C.J., Morris C. G.: Handbook of energy. Volume II: Cronologies, top ten lists, and words clouds. Elsvier Inc., 2014.
25.	Cushion E., Whiteman A., Dieterle G.: Bioenergy Development: Issues and Impacts for Poverty and Natural Resource Management. World Bank Publications, 2010.
26.	Daggett D. L., Hendricks R.C., Walther R., Corporan E.: Alternative fuels for use in commercial aircrafts. The Boeing Company, 2007.
27.	Dahlquist E.: Biomass as Energy Source. Resources, Systems and Applications. CRC Press, 2013.
28.	Delmon B., Grange P., Froment G.F.: Hydrotreatment and Hydrocracking of Oil Fractions. Elsevier, 1999.
29.	Doc 9889 Airport Air Quality Manual. International Civil Aviation Organization, 2011. 
30.	Doc 9977. Manual on Civil Aviation Jet Fuel Supply, 2012.
31.	Edwards T.: Advancements in Gas Turbine Fuels from 1943 to 2005. J Eng Gas Power, No. 129, 2007, p. 13–20.
32.	Firrisa M. T., Van Duren I., Voinov A.: Energy efficiency for rapeseed biodiesel production in different farming systems. Energy Efficiency, 2013.
33.	Garcia-Anton J., Monzo J., Guninon J.L.: Study of corrosion on copper strips by petroleum naphtha in the ASTM D-130 test by means of electronic microscopy (SEM) and energy dispersive X-ray (EDX). Fresenius Journal of Analytical Chemistry. Iss. 337, 1990, p. 382–388.
34.	Garcia Santander C.M., Gymez Rueda S.M., de Lima da Silva N.: Measurements of normal boiling points of fatty acid esters and triacylglycerols by thermogravimetric analysis, Fuel, Iss. 92, 2012, p. 158–161.
35.	Geller D. P., Goodrum J.: W. Effects of speciﬁc fatty acid methyl esters on diesel fuel lubricity, Fuel, Vol. 83, 2004, p. 2351–2356.
36.	Gupta, K. K, Rehman A, Sarviya R. M.: Bio-fuels for the gas turbine: A review. Renew. Sust. Energ. Rev. No. 14, 2010, p. 2946–2955.
37.	Harvey B. G, Merriman W.W., Koontz T.A.: High-Density Renewable Diesel and Jet Fuels Prepared from Multicyclic Sesquiterpanes and a 1‑Hexene-Derived Synthetic Paraffinic Kerosene, Energy Fuels, 2013.
38.	Hemighaus G., Boval T., Bosley C.: Alternative Jet Fuels. Addendum 1 to Aviation Fuels Technical Review (FTR-3/A1). Chevron Corporation, 2006.
39.	Hileman J.I., Stratton R.W.: Alternative jet fuel feasibility. Transport Policy. Vol. 34, 2014, p. 52–62.
40.	Hileman J.I., Wong H.M., Waitz I.: Near-Term Feasibility of Alternative Jet Fuels. Santa Monica, California: RAND Corporation, 2009.
41.	Hileman, J. Ortiz D., Bartis J.: Near-Term Feasibility of Alternative Jet Fuels. Jointly published by the RAND Corporation (Report No. TR-554-FAA) and the Partnership for Air Transportation Noise and Emissions Reduction, 2009.
42.	Honga T.D., Soerawidjajab T.H., Reksowardojoa I.K.: A study on developing aviation biofuel for the Tropics: Production process – Experimental and theoretical evaluation of their blends with fossil kerosene, Chemical Engineering and Processing: Process Intensification, Vol. 74, 2013, p. 124–130.
43.	Hristova M., Tchaoushev S.: Сalculation of flash points and flammability limits of substances and mixtures. Journal of the University of Chemical Technology and Metallurgy, Iss. 41(3), p. 291–296, 2006.
44.	Hu J., Du Z., Li C., Min E.: Study on the lubrication properties of biodiesel as fuel lubricity enhancers, Fuel. Vol. 84, 2005. p. 1601–1606. 
45.	Iakovlieva A., Boichenko S., Vovk O.: Investigation of the fractional composition of rape oil-derived aviation biofuels. Aviation in the XXI-st century. Safety in aviation and space technologies: the fifth world congress, 25–27 September 2012: abstracts. Kyiv, Vol. 3, 2012, p. 5.41–5.43.
46.	Iakovlieva A.V. Boichenko S.V., Vovk O.O.: Overview of innovative technologies for aviation fuels production. Journal of Chemistry and chemical technology, Vol. 7. Iss. 3, 2013, p. 305–312.
47.	Iakovlieva A., Lejda K., Vovk O., Boichenko S.: Peculiarities of the development and implementation of aviation biofuels in Ukraine. World Congress on Petrochemistry and Chemical Engineering. Journal of Petroleum & Environmental Biotechnology. November 2013, San Antonio. Vol.4. Iss. 6, 2013, p. 47.
48.	Iakovlieva A., Boichenko S., Gay A.: Cause-Effect Analysis of the Modern State in Production of Jet Fuels. Journal of Сhemistry & Chemical Technology. Vol. 8. No 1, 2014, p. 107–116.
49.	Iakovlieva A., Boichenko S., Vovk O., Lejda K.: Potential of jet biofuels production and application in Ukraine and Poland. International Journal of Sustainable Aviation. Vol. 1. No.4, 2015, p. 314–323.
50.	Iakovlieva A., Boichenko S., Lejda K.: Impact of rape oil ethyl esters additives on some characteristics of jet fuel. Проблеми хіммотології. Теорія та практика раціонального використання традиційних і альтернативних паливно -мастильних матеріалів: V міжнар. наук.-техн. конф., 6–10 жовт. 2014. Київ, c. 286 – 289. 
51.	Iakovlieva A., Lejda K., Vovk O., Boichenko S., Skilniuk I.: Vacuum Distillation of Rapeseed Oil Esters for Production of Jet Fuel Bio-Additives, Procedia Engineering, Vol. 187, 2017, p. 363 – 370. 
52.	Iakovlieva A., Lejda K., Vovk O., Boichenko S.: Рotential of jet biofuels production and application in Ukraine and Poland. Proceedings of the 1st International Simposium on Sustainable Aviation.–31 May–03 June 2015, Isntanbul, p. 137.
53.	Iakovlieva A., Boichenko S., Lejda K.: Experimental study on antiwear properties for blends of jet fuel with biocomponents derived from rapeseed oil. Eastern-European journal of enterprise technologies. No. 5/8(77), 2015, p. 20–28.
54.	Iakovlieva A., Vovk O., Boichenko S.: Еxperimental study of rape oil esters influence on physical-chemical properties of jet fuels. Proceedings of the 19th Conference for Junior Researchers ‘Science – Future of Lithuania’ Тransport engineering and management, 6 May 2016, Vilnius. p. 85–89.
55.	Iakovlieva A., Lejda K., Vovk O., Boichenko S., Kuszewski H. Improvement of technological scheme of fatty acids ethyl esters production for use as jet fuels biocomponents. International Journal of Theoretical and Applied Science. Iss. 11(19), 2014, p. 44–55.
56.	International Air Transport organization. Vision 2050. Report. Montreal. Geneva, 2011.
57.	Jansen R. A.: Second Generation Biofuels and Biomass: Essential Guide for Investors, Scientists and Decision Makers. Wiley. 2012.
58.	Jenkins R.W., Munro M., Christopher S.N., Chuck C.: Potential renewable oxygenated biofuels for the aviation and road transport sectors. Fuel, Vol. 103, 2013, p. 593–599.
59.	Jacyna M., Żak J., Jacyna-Gołda I., Merkisz J., Merkisz-Guranowska A., Pielecha J.: Selected aspects of the model of proecological transport system. Journal of KONES Powertrain and Transport, Vol. 20, No. 3, 2013, p. 193 – 202.
60.	Kallio P., Pasztor A., Akhtar M.K., Jones P.R.: Renewable jet fuel. Current Opinion in Biotechnology. Vol. 26, 2014, p. 50–55.
61.	Kandaramath Hari T., Yaakob Z., Binitha N.N.: Aviation biofuel from renewable resources: Routes, opportunities and challenges. Renewable and Sustainable Energy Reviews. Vol. 42, 2015, p. 1234–1244. 
62.	Kinder J. D., Rahmes T.: Evaluation of Bio-Derived Synthetic Paraffinic Kerosene (Bio-SPK). The Boeing Company Sustainable Biofuels Research&Technology Program, 2009.
63.	Kirklin P.W., David. P.: Aviation Fuel: Thermal Stability. ASTM International, 1992.
64.	Lapuerta M., Rodriguez-Fernandeza J., Estevez C., Bayarri N.: Properties of fatty acid glycerol formal ester (FAGE) for use as a component in blends for diesel engines. Biomass and bioenergy. Vol. 76, 2015, p. 130–140. 
65.	Lebedevas S., Vaicekauskas A.: Research into the application of biodiesel in the transport sector of Lithuania. Transport. Vol. 21, Iss. 2, 2006, p. 80–87.
66.	Liu G., Yan B., Chen G.: Technical review on jet fuel production. Renewable and Sustainable Energy Reviews. Vol. 25, 2013, p. 59–70. 
67.	Lu M., Chai M.: Experimental Investigation of the Oxidation of Methyl Oleate: One of the Major Biodiesel Fuel Components Synthetic Liquids Production and Refining. Chapter 13, P. 289–312. American Chemical Society. 2011
68.	Merkisz J., Merkisz-Guranowska, A., Pielecha J., Nowak M., Jacyna M., Lewczuk K., Żak J.: Exhaust emission measurements in the development of sustainable road transport. Journal of KONES Powertrain and Transport, Vol. 20, No. 4 2013, p. 277 – 284.
69.	Maksimuk Yu., Antonova Z., Fes’ko V., Kursevich V.: Diesel biofuel viscosity and heat of combustion. Chemistry and technology of fuels and oils. Iss. 45, 2009, p. 343–346.
70.	Maru M. M., Trommer R.M., Cavalcanti K.F.: The Stribeck curve as a suitable characterization method of the lubricity of biodiesel and diesel blends. Energy. Vol. 69, 2014, p. 673–681.
71.	Maurice L.Q., Lander H., Edwards T., Harrison W.E.: Advanced aviation fuels: a look ahead via a historical perspective. Fuel. Vol. 80, Iss. 5, 2001, p. 747–756.
72.	Merkisz J., Markowski J., Pielecha J. Emission tests of the AI-14RA aircraft engine under real operating conditions of PZL-104"" Wilga"" plane. Silniki Spalinowe. No. 3, 2009, p. 64–70.
73.	Merkisz J., Galant M., Karpiński D., Kubiak, K. Evaluation of possibility to use the LTO cycle for emission test on example of the model turbine engine GTM-120 Journal of Mechanical and Transport Engineering. Vol. 66, No. 2, 2014, p. 25—33.
74.	Murphy D.J., Hall C.A.S.: Year in review—EROI or energy return on (energy) invested. Annals of the New York academy of sciences. Issue: Ecological Economics Reviews. Iss. 1185, 2010, p. 102–118.
75.	Murphy D.J., Hall C.A.S., Powers B.:New perspectives on the energy return on (energy) investment (EROI) of corn ethanol. Environment, Development and Sustainability. Vol. 13, Iss. 1, 2011, p. 179–202.
76.	Naik S.N., Goud V.V., Rout P.K., Dalai A.K.: Production of first and second generation biofuels: A comprehensive review. Renew. Sust. Energ. Rev., No. 14, 2010, p. 578–597.
77.	Nollet Leo M. L.: Handbook of Food Analysis: Physical characterization and nutrient analysis. CRC Press, 2004.
78.	Orszulik S.: Environmental Technology in the Oil Industry. Springer Science & Business Media, 2013.
79.	Pandey A.: Biofuels: Alternative Feedstocks and Conversion Processes. Academic Press, 2011.
80.	Pearlson M.N.: A techno-economic and environmental assessment of hydroprocessed renewable distillate fuels. Master of Science in Technology and Policy. Massachiussets Institute of Technology. June 2011.
81.	Prag P.: Renewable Energy in the Countryside. Taylor & Francis, 2014.
82.	Prussi M, Chiaramonti D, Recchia L, Martelli F, Guidotti F, Pari L.: Alternative feedstock for the biodiesel and energy production: the OVEST project. Energy Journal, No. 58, 2013, p. 2–8.
83.	Rahmes T.F., Kinder J.D., Henry T.M., etc.: Sustainable Bio-Derived Synthetic Paraffinic Kerosene (BioSPK) Jet Fuel Flights and Engine Tests Program Results. American Institute of Aeronautics and Astronautics, 2009.
84.	Rajagopal D., Zilberman D.: Environmental, Economic and Policy Aspects of Biofuels. Nеw Publishers Inc., 2008.
85.	Report on alternative fuels. International Air Transport Association IATA. http://www.iata.org/publications/Documents/2012-report-alternativefuels. pdf; 2012
86.	Rosillo Calle F, Trhan D, Seiffert M, Teeluckingh S. The potential and role of biofuels in commercial air transport – biojetfuels. Task 40 sustainable international bioenergy trade. IEA Bioenergy
87.	Sarin R., Kumar R., Srivastav B., etc.: Biodiesel surrogates: Achieving performance demands. Bioresource Technology. Vol. 100, Iss. 12, 2009, p. 3022–3028. 
88.	Shen Y.. Аn experimental study on thermal stability of FAEE biodiesel fuel with ethanol. Master Thesis, 2015.
89.	Shepherd J.E., Nuyt C.D., Lee J.J.: Flash Point and Chemical Composition of Aviation Kerosene (Jet A). National Transportation Safety Board, 2000.
90.	Singh B.: Biofuel Crops: Production, Physiology and Genetics. CABI, 2013.
91.	Singh B.: Biofuel Crop Sustainability. John Wiley & Sons, 2013.
92.	Sperling D., Cannon J.S.: Reducing Climate Impacts in the Transportation Sector. Springer Science & Business Media, 2011.
93.	Szczerek M., Tuszyсski W. Tribological researches – scuffing. Radom: Institute for Sustainable Technologies – National Research Institute, 2000.
94.	The jet engine. Rolls-Royce plc. Renault Printing Co Ltd., 1996.
95.	T-02U. Aparat czterokulowy – instrukcja obsługi. Radom: Wydawnictwo Instytutu Technologii Eksploatacji, 2011.
96.	Wcisło G.: Determination of the impact of FAME biocomponent on the fractional composition of diesel engine fuels. Combustion Engines. Iss. 154(3), 2013, p. 1098–1103.
97.	Xu Y., Wang Q., Hu X.: Characterization of the lubricity of bio-oil/diesel fuel blends by high frequency reciprocating test rig. Energy. Vol. 35, Iss. 1, 2010, p. 283–287. 
98.	Yakovleva A.V., Boichenko S.V., Lejda K, Vovk O.O., Kuszewski H.: Antiwear Properties of Plant—Mineral-Based Fuels for Airbreathing Jet Engines, Chemistry and Technology of Fuels and Oils, Vol. 53, Iss. 1, 2017, p. 1–9. 
99.	Yakovlieva A.V., Boichenko S.V., Leida K., Vovk O.A., Kuzhevskii Kh.. Influence of Rapeseed Oil Ester Additives on Fuel Quality Index for Air Jet Engines, Chemistry and Technology of Fuels and Oils, Vol. 53, Iss. 3, 2017. p. 308–317.
100.	Yakovlieva A., Boichenko S., Vovk O., Lejda K., Gryshchenko O.. Case Study of Alternative Jet Fuel Production with Bio-additives from Plant Oils in Ukraine and Poland. Advances in Sustainable Aviation. Springer International Publishing, 2018. Chapter 4.
101.	Yakovlieva A., Boshkov V. Experimental study of low-temperature properties of alternative aviation fuels, Proceedings of the 21th Conference for Junior Researchers ‘Science – Future of Lithuania’ Transport Engineering and Management, 4-5 May 2018, Vilnius, Lithuania. 2018. p. 130 – 134.
102.	Yildirim U, Abanteriba S.: Manufacture, qualification and approval of new aviation turbine fuels and additives, proceedia Engineering, No. 49, 2012, p. 310 – 315.
103.	Yutko B. and Hansman J., Approaches to Representing Aircraft Fuel Efficiency Performance for the Purpose of a Commercial Aircraft Certification Standard, MITInternational Center for Air Transportation, Cambridge, Mass, 2011.
104.	Zhu Y.: An Experimental Study on Thermal Stability of Biodiesel Fuel. Master Thesis. – 2012. – 160 p.
105.	Авиационный турбореактивный двигатель РУ 19A-300, руководство по эксплуатации и техническому обслуживанию, ЗАО «АНТЦ Технолог», 2001.
106.	Азев В.С., Середа А.В.: Влияние соединений серы на противоизносные свойства дизельных топлив, Химия и технология топлив и масел. № 3, 2009, c. 23–27.
107.	Андіїшин М.П., Марчук Я.С., Бойченко С.В., Рябоконь Л.А.: Газ природний, палива та оливи. Одеса: Астропринт, 2010.
108.	Бойченко С.В., Спіркін В.Г. Вступ до хіммотології палив та олив: навч. посіб.: у 2-х ч. Одеса: Астропринт, Ч.1., 2009.
109.	Бойченко С.В., Любінін Й.А., Спіркін В.Г.: Вступ до хіммотології палив та олив: навч. посіб.: у 2-х ч. Одеса: Астропринт. Ч.2., 2010.
110.	Бойченко С.В., Черняк Л.М., Яковлєва А.В.: Традиційні технології виробництва палив для повітряно-реактивних двигунів. Вісник Національного авіаційного університету. № 2 (55), 2013, с. 195–209.
111.	Бойченко С. В., Яковлева А. В., Волошинец В. А., Лейда К. Модифицирование эфиров рапсового масла вакуумным фракционированием, Технологии нефти и газа, №5, 2018, c. 15–20
112.	Братичак М.М.: Основи промислової нафтохімії, Львів: Вид-во НУ «Львівська політехніка», 2008.
113.	Васильев И.П.: Влияние топлив растительного происхождения на экологические и экономические показатели дизеля, Луганск: Изд-во ВНУ им. В. Даля, 2009.
114.	Волошинець В.А. Фізична та колоїдна хімія: Фізико-хімія дисперсних систем та полімерів: навч.посіб. Львів : Вид-во Львів. політехніки, 2013. – 200 с.
115.	Голоскоков А.Н. Критерии сравнения эффективности традиционных и альтернативных энергоресурсов. Нефтегазовое дело. № 1, 2011, c. 285–301.
116.	Голоскоков А.Н. Пик добычи нефти и начало мирового энергетического кризиса. Нефтегазовое дело. 2010, c. 1–13.
117.	Данилов А.М., Каминский Э.Ф., Хавкин В.А.: Альтернативные топлива: достоинства и недостатки. Проблемы применения. Российский химический журнал (Журнал Российского химического общества им. Д.И. Менделеева). Т. XLVII. № 6, 2003, c. 4–11.
118.	Дворецкий С.И., Нагорнов С.А., Романцова С.В. и др.: Производство биодизельного топлива из органического сырья. Вопросы современной науки и практики. № 39, 2012, c. 126– 35.
119.	Девянин С.Н., Марков В.А., Семенов В.Г.: Растительные масла и топлива на их основе для дизельных двигателей. Харьков: Новое слово. 2007.
120.	Ергин Д.: Добыча: Всемирная история борьбы за нефть, деньги и власть. Москва,: Альпина Паблишер, 2011.
121.	Запорожець А.О.: Дослідження стехіометричної суміші «повітря ‒ паливо» органічних сполук. Частина 1. Алкани. Наукоємні технології. № 2(22), 2014, c. 163–167.
122.	Кириченко В., Бойченко С., Кириченко В., Нездоровин В.: Комплексная переработка технических растительных масел: концепция, методы и технологи. «Systems and means of motor transport» Seria: Transport. Monografia. № 4, 2013, p. 357–370.
123.	Колодницька Р.В., Семенов В.Г.: Моделювання низькотемпературних властивостей біодизельних палив. Вісник СевНТУ. Серія: Машиноприладобудування та транспорт. № 134, 2012, c. 135–138.
124.	Коллоидная химия нефти и нефтепродуктов: Сборник материалов, посвященных научной деятельности проф. Г.И. Фукса. Москва: Изд-во «Техника». ООО «Тума Групп», 2001.
125.	Крылов И.Ф., Емельянов В.Е.: Альтернативные моторные топлива. Производство, применение",,'National Aviation University',Modification of jet fuels composition with renewable bio-additives,10.18372/37895,https://core.ac.uk/download/286618536.pdf,core
211242274,2019-05-01T00:00:00,"Over the last decade, many interesting route planning problems can be solved by finding the shortest path in a weighted graph that represents a transportation network. Such networks are private transport networks or timetabled public transportation networks. In the shortest path problem, every network type requires different algorithms to compute one or more than one shortest path. However, routing in a public transportation network is completely different and is much more complex than routing in a private transport network, and therefore different algorithms are required.

 

For large networks, the standard shortest path algorithms - Dijkstra's algorithm (1959) and Bellman's algorithm (1958)- are too slow. Consequently, faster algorithms have been designed to speed up the search. However, these algorithms often consider only the simplest scenario of finding an optimal route on a graph with static real edge costs. But real map routing problems are often not that simple – it is often necessary to consider time-dependent edge costs. For example, in public transportation routing, consideration of the time-dependent model of these networks is mandatory.

 

However, there are a number of transportation applications that use informed search algorithms (where the algorithm uses heuristics that guide the search toward the destination), rather than one of the standard static shortest path algorithms. This is primarily due to shortest paths needing to be rapidly identified either because an immediate response is required. For example, the A* algorithm (Nilsson, 1971) is widely used in artificial intelligence. Heuristic information (in the form of estimated distance to the destination) is used to focus the search towards the destination node. This results in finding the shortest path faster than the standard static search algorithms.

 

Road traffic congestion has become an increasingly significant problem in a modern society. In a dynamic traffic environment, traffic conditions are time-dependent. For instance, when travelling from home to the work, although an optimal route can be planned prior to departure based on the traffic conditions at that time, it may be necessary to adjust the route while en route because traffic conditions change all the time. In some cases, it is necessary to modify the travelling route from time to time and re-plan a new route from the current location to the destination, based on the real-time traffic information. The challenge lies in the fact that any modification to the optimal route to adapt to the dynamic environment necessitates speeding up of the search efforts. Among the algorithms suggested for the dynamic shortest path problem is the algorithm of Lifelong Planning A* algorithm (LPA*) (Koenig, Likhachev and Furcy, 2004). This algorithm has been given this name because of its ability to reuse information from previous searches. It is used to adjust a shortest path to adapt to the dynamic transportation network.

 

Search space and fast shortest path queries can be used for finding fastest updated route on road and bus networks. Consequently, the efficient processing of both types of queries is of first-rate significance. However, most search methods focus only on one type of query and do not efficiently support the other. To address this challenge, this research presents the first novel approach; an Optimised Lifelong Planning A* (OLPA*) algorithm. The OLPA* used an appropriate data structure to improve the efficiency of the dynamic algorithms implementation making it capable of improving the search performance of the algorithm to solve the dynamic shortest path problem, which is where the traveller may have to re-compute the shortest path while travelling in a dynamic transportation environment.

 

This research has also proposed bi-directional LPA* (BLPA*) algorithm. The proposed algorithm BLPA* used bi-directional search strategy and the main idea in this strategy is to divide the search problem into two separate problems. One search proceeds forwards from the start node, while the other search proceeds backwards from the end node. The solution requires the two search problems to meet at one middle node. The BLPA* algorithm has the same overall structure as the LPA* algorithm search, with some differences that the BLPA* contains a priority queue for each direction.

 

This research presented another algorithm that designed to adaptively derive the shortest path to the desired destination by making use of previous search results and reducing the total execution time by using the benefits of a bi-directional search strategy . This novel algorithm has been called the bi-directional optimised Lifelong A* algorithm (BiOLPA*). It was originally proposed for road transport networks and later also applied to public transportation networks. For the road transport network, the experimental results demonstrate that the proposed incremental search approach considerably outperforms the original approach method, which recomputed the shortest path from scratch each time without utilization of the previous search results. However, for public transportation, the significant problem is that it is not possible to apply a bi-directional search backwards using estimated arrival time. This has been further investigated and a better understanding of why this technique fails has been documented. While the OLPA* algorithms give an impressive result when applied on bus network compared with original A* algorithms, and our experimental results demonstrate that the BiOLPA* algorithm on road network is significantly faster than the LPA*, OLPA* and the A* algorithms, not only in terms of number of expansion nodes but also in terms of computation time",,,Shortest path algorithms for dynamic transportation networks,,https://core.ac.uk/download/211242274.pdf,core
270202347,2019-01-01T00:00:00,"Cultural Heritage is a testimony of past human activity, and, as such, its objects exhibit great variety in their nature, size and complexity; from small artefacts and museum items to cultural landscapes, from historical building and ancient monuments to city centers and archaeological sites. Cultural Heritage around the globe suffers from wars, natural disasters and human negligence. The importance of digital documentation is well recognized and there is an increasing pressure to document our heritage both nationally and internationally. For this reason, the three-dimensional scanning and modeling of sites and artifacts of cultural heritage have remarkably increased in recent years. The semantic segmentation of point clouds is an essential step of the entire pipeline; in fact, it allows to decompose complex architectures in single elements, which are then enriched with meaningful information within Building Information Modelling software. Notwithstanding, this step is very time consuming and completely entrusted on the manual work of domain experts, far from being automatized. This work describes a method to label and cluster automatically a point cloud based on a supervised Deep Learning approach, using a state-of-the-art Neural Network called PointNet++. Despite other methods are known, we have choose PointNet++ as it reached significant results for classifying and segmenting 3D point clouds. PointNet++ has been tested and improved, by training the network with annotated point clouds coming from a real survey and to evaluate how performance changes according to the input training data. It can result of great interest for the research community dealing with the point cloud semantic segmentation, since it makes public a labelled dataset of CH elements for further tests",,'Copernicus GmbH',Deep learning for semantic segmentation of 3D point cloud.,10.5194/isprs-archives-XLII-2-W15-735-2019,https://core.ac.uk/download/270202347.pdf,core
343942739,2019-11-04T08:00:00,"As the global population soars from today’s 7.3 billion to an estimated 10 billion by 2050, the demand for Food, Energy, and Water (FEW) resources is expected to more than double. Such a sharp increase in demand for FEW resources will undoubtedly be one of the biggest global challenges. The management of food, energy, water for smart, sustainable cities involves a multi-scale problem. The interactions of these three dynamic infrastructures require a robust mathematical framework for analysis. Two critical solutions for this challenge are focused on technology innovation on systems that integrate food-energy-water and computational models that can quantify the FEW nexus. Information Communication Technology (ICT) and the Internet of Things (IoT) technologies are innovations that will play critical roles in addressing the FEW nexus stress in an integrated way. The use of sensors and IoT devices will be essential in moving us to a path of more productivity and sustainability. Recent advancements in IoT, Wireless Sensor Networks (WSN), and ICT are one lever that can address some of the environmental, economic, and technical challenges and opportunities in this sector. This dissertation focuses on quantifying and modeling the nexus by proposing a Leontief input-output model unique to food-energy-water interacting systems. It investigates linkage and interdependency as demand for resource changes based on quantifiable data. The interdependence of FEW components was measured by their direct and indirect linkage magnitude for each interaction. This work contributes to the critical domain required to develop a unique integrated interdependency model of a FEW system shying away from the piece-meal approach. The physical prototype for the integrated FEW system is a smart urban farm that is optimized and built for the experimental portion of this dissertation. The prototype is equipped with an automated smart irrigation system that uses real-time data from wireless sensor networks to schedule irrigation. These wireless sensor nodes are allocated for monitoring soil moisture, temperature, solar radiation, humidity utilizing sensors embedded in the root area of the crops and around the testbed. The system consistently collected data from the three critical sources; energy, water, and food. From this physical model, the data collected was structured into three categories. Food data consists of: physical plant growth, yield productivity, and leaf measurement. Soil and environment parameters include; soil moisture and temperature, ambient temperature, solar radiation. Weather data consists of rainfall, wind direction, and speed. Energy data include voltage, current, watts from both generation and consumption end. Water data include flow rate. The system provides off-grid clean PV energy for all energy demands of farming purposes, such as irrigation and devices in the wireless sensor networks. Future reliability of the off-grid power system is addressed by investigating the state of charge, state of health, and aging mechanism of the backup battery units. The reliability assessment of the lead-acid battery is evaluated using Weibull parametric distribution analysis model to estimate the service life of the battery under different operating parameters and temperatures. Machine learning algorithms are implemented on sensor data acquired from the experimental and physical models to predict crop yield. Further correlation analysis and variable interaction effects on crop yield are investigated",,FIU Digital Commons,"Edge IoT Driven Framework for Experimental Investigation and Computational Modeling of Integrated Food, Energy, and Water System",,,core
200533535,2019-01-30T19:21:52Z,"<p>Three-dimensional Eulerian chemical transport models such as
CMAQ often report a significant model-measurement error due to uncertainties in
the treatment of physical processes and require higher run-time. Machine models
are more computationally efficient and are currently used widely for
forecasting purposes. Deep Neural network (DNN) techniques comprise a popular
class of machine learning methods. Predicting hourly air quality, especially
ozone, is challenging due to its highly varying and complex behavior in the
atmosphere. Here, we used modeled meteorological parameters (by MCIP) along
with selected modeled gaseous species (by CMAQ) as our inputs for predicting
future ozone concentrations. A timely-efficient 1D deep convolutional neural
network (ConvNet 1D), called CMAQ-CNN, was implemented and trained on using
CMAQ outputs as inputs to predict hourly ozone concentration in real-time
across the continental US (1081 AQS stations in 48 states). The CMAQ-CNN model
significantly improved the performance of the CMAQ model in term of both
accuracy (IOA) and bias (maximum daily ozone). IOA improved around 0.06 in
average and up to 0.3 across the United States by using CMAQ-CNN model. The
CMAQ-CNN model shows mediocre performance on capturing very high ozone peaks
(over 90 ppb). This poster was presented at the Earth Science Information Partners (ESIP) Winter Meeting in January 2019.</p",,,Can Deep Learning Improve CMAQ Performance?,10.6084/m9.figshare.7591781.v1,,core
344203551,2019-06-19T00:00:00,"The aim of my research was to develop a digital mediation system with urban data for a pedestrian immersed in the city, a link based on digital technologies to design, analyze, represent urban space and access information on this urban space. Augmented Reality is one of the tools allowing this mediation whose critical element is the location of the pedestrian and more precisely the pose calculation of the camera it carries.Thus, the main focus of my work is geolocation on site using spatial data of different dimensions. I was interested in an upstream phase that requires the implementation of data models to keep track of spatial data changes. Finally, I touched on some uses of geolocation and pose calculation. I conclude this report by presenting my research perspectives on digital mediation with urban data for pedestrians.Le but de ma recherche a été de mettre au point un système de médiation numérique avec des données urbaines pour un piéton immergé dans la ville, un lien basé sur des technologies numériques pour concevoir, analyser, représenter l’espace urbain et accéder à des informations sur cet espace urbain. La réalité augmentée est un des outils permettant cette médiation dont l’élément critique est la localisation du piéton et plus précisément le calcul de pose de la caméra qu’il transporte.Ainsi, l’axe principal de mon travail est la géolocalisation sur site à l’aide de données spatiales de différentes dimensions. Je me suis intéressée à une phase amont qui nécessite la mise en place de modèles de données pour garder trace des modifications des données spatiales. J’ai enfin abordé quelques usages de la géolocalisation et du calcul de pose. Je conclus ce mémoire en présentant mes perspectives de recherches vers une médiation numérique avec des données urbaines pour le piéton",,HAL CCSD,Méthodes pour la géolocalisation du piéton sur site - vers une médiation numérique avec les données urbaines,,,core
224413091,2019-01-01T00:00:00,"With the increase in electric vehicle (EV) adoption in recent years, the impact of EV charging activity to the power grid has become increasingly significant.  Although an EV is considered beneficial to the environment by reducing greenhouse gases, large amounts of un-coordinated EV charging could be detrimental to the power grid and thereby degrade power quality. Recent developments in Vehicle to Grid (V2G) technology has converted an EV to a distributed energy resource (DER). A modern smart grid with intelligent IoT devices, solar generation and battery storage provides additional opportunities but also additional challenges to the grid operator. To alleviate the negative effects of massive EV charging load and turn them into grid assets, the current dissertation performs research in designing and developing optimal EV charging strategies to integrate EVs into the smart power grid. Using the UCLA Smart Grid Energy Research Center (SMERC) smart EV charging network infrastructure as the testbed, data has been collected regarding EV driver charging behavior for five years. Based on historical charging records, both deterministic and generative EV user behavior models are proposed to combine statistical analysis and machine learning to predict day-ahead EV driver itinerary and energy demand. Optimal Vehicle Grid Integration strategy is designed to realize different objectives including EV charging cost minimization, power grid stabilization, computational burden decentralization, increasing convergence speed, mitigating solar over-generation, etc. A distributed optimal bi-directional charging scheduling algorithm with asynchronous converging feature has been designed for load curve flattening; A two-stage optimization and a distributed water-filling algorithm have been developed for aggregating EVs to participate in energy market and demand response program. Both large-scale simulation and real-world implementation are conducted to validate and evaluate the performance of these algorithms. Results show that the proposed distributed optimal bi-directional charging scheduling algorithm is able to flatten power peak load by 35% when implemented in a test-bed located within the parking structure 9 in UCLA. A daily energy cost saving of 18% is achieved when the two-stage optimization algorithm is performed to control the EVs in a parking structure in the Civic Center Garage of the City of Santa Monica to participate in wholesale energy markets. Smart meter data collected in the Santa Monica parking lot shows the proposed charging control algorithm is able to mitigate the solar over-generation in the building by 50% on a daily basis. It can be concluded that our Vehicle Grid Integration strategy is effective in stabilizing power grid load, reducing charging cost and solving solar power over-generation problem. In addition to the development of EV user behavior models and Vehicle Grid Integration strategy, this dissertation also solves practical engineering problems for a scalable, reliable and safe EV bi-directional smart charging infrastructure",,"eScholarship, University of California",Optimal Vehicle Grid Integration,,,core
344908498,2019-01-01T00:00:00,"Abstract

The use of flying platforms such as unmanned aerial vehicles (UAVs), popularly known as drones, is rapidly growing. In particular, with their inherent attributes such as mobility, flexibility, and adaptive altitude, UAVs admit several key potential applications in wireless systems. On the one hand, UAVs can be used as aerial base stations to enhance coverage, capacity, reliability, and energy efficiency of wireless networks. On the other hand, UAVs can operate as flying mobile terminals within a cellular network. Such cellular-connected UAVs can enable several applications ranging from real-time video streaming to item delivery. In this paper, a comprehensive tutorial on the potential benefits and applications of UAVs in wireless communications is presented. Moreover, the important challenges and the fundamental tradeoffs in UAV-enabled wireless networks are thoroughly investigated. In particular, the key UAV challenges such as 3D deployment, performance analysis, channel modeling, and energy efficiency are explored along with representative results. Then, open problems and potential research directions pertaining to UAV communications are introduced. Finally, various analytical frameworks and mathematical tools, such as optimization theory, machine learning, stochastic geometry, transport theory, and game theory are described. The use of such tools for addressing unique UAV problems is also presented. In a nutshell, this tutorial provides key guidelines on how to analyze, optimize, and design UAV-based wireless communication systems",,'Institute of Electrical and Electronics Engineers (IEEE)',"A tutorial on UAVs for wireless networks:applications, challenges, and open problems",,,core
322991098,2019-01-01T00:00:00,"1.	Abu-Taieh C., Evon J.: Technology Engineering and Management in Aviation: Advancements and Discoveries. Information Science Reference, 2011.
2.	Ajam M, Woolard C, Wiljoen CL. Biomass pyrolysis oil as a renewable feedstock for bio-jet fuel. In: Proceedings of the 13th international conference on stability, handling and use of liquid fuels (IASH2013), Rhodes, Greece; October 2013. p. 6–10.
3.	Аnnual report to Parliament on the renewable transport fuel obligation. Renewable Fuels Agency. The Stationery Office, 2011.
4.	Agarwal S., Chhibber V. K., Bhatnagar A. K.:Tribological behavior of diesel fuels and the effect of anti-wear additives. Fuel. Vol. 106, 2013, p. 21–29, 
5.	Alves S. M., Barros B.S., Trajano M.F.: Tribological behavior of vegetable oil-based lubricants with nanoparticles of oxides in boundary lubrication conditions. Tribology International. Vol. 65, 2013, p. 28–36.
6.	Asgari H., Chen X., Sainudiin R.: Modelling and simulation of gas turbines. International Journalof Modelling, Identification and Control, Vol.25, No.3, 2013, p. 1–15.
7.	Bartis James T. LaTourrette T., Dixon L.: Oil Shale Development in the United States: Prospects and Policy Issues. Santa Monica, Calif.: RAND Corporation, MG-414-NETL, 2005.
8.	Bassam N. El.: Handbook of Bioenergy Crops: A Complete Reference to Species. Development and Applications Earthscan, 2010.
9.	Bazazzadeh M., Badihi H., Shahriari A.: Gas Turbine Engine Control Design Using Fuzzy Logic and Neural Networks. International Journal of Aerospace Engineering. Vol. 1, 2011, p. 1–13. 
10.	Blakey S, Rye L, Wilson C.W.: Aviation gas turbine alternative fuels: A review.  P Combust Inst, No. 33, 2011, p. 2863–2885.
11.	Boichenko S., Iakovlieva A., Vovk O.: Traditional and alternative jet fuels: problems of quality standardization. Journal of Petroleum & Environmental Biotechnology. Vol. 4. Iss. 3, 2013.
12.	Boichenko S., Shkilniuk I., Turchak V.. The problems of biopollution with jet fuels and the way of achieving solution. Transport. 23, 2008; p. 253–257.
13.	Boichenko S., Yakovleva A. Prospects of biofuels introduction into aviation. Transport engineering and management: Proceedings of the 15-th conference for Lithuania Junior researchers. Science – future of Lithuania, 4 May 2012. Vilnius: Technika. p. 90–94.
14.	Boichenko S., Yakovlieva A., Gryshchenko O., Zinchuk A. Prospects of using different generations biofuels for minimizing impact of modern aviation on environment, Энерготехнологии и ресурсосбережение, № 1, 2018, p. 10–20.
15.	Boichenko S., Lejda K., Yakovlieva A., Vovk O. Comparative characteristics of low-temperature properties of jet fuels modified with bio-additives, International Automotive Conference (KONMOT2018). IOP Conf. Series: Materials Science and Engineering 421, 2018.
16.	Breil C., Meullemiestre A., Vian M., Chemat F.: Bio-Based Solvents for Green Extraction of Lipids from Oleaginous Yeast Biomass for Sustainable Aviation Biofuel. Molecules. Iss. 21(196), 2016, p. 1–14.
17.	Carels N., Sujatha M., Bahadur B.: Jatropha, Challenges for a New Energy Crop. Vol. 1: Farming, Economics and Biofuel. Springer Science & Business Media, 2012.
18.	Cavani F., Albonetti S., Basile F., Gandini A.: Chemicals and Fuels from Bio-Based Building Blocks. John Wiley & Sons, 2015.
19.	Cermak S. C., Evangelista R. L., Kenar J. A.: Distillation of Natural Fatty Acids and Their Chemical Derivatives, Distillation - Advances from Modeling to Applications, Dr. Sina Zereshki (Ed.), InTech, 2012. – р. 5. – 140. 
20.	Chai M. Thermal Decomposition of Methyl Esters in Biodiesel Fuel: Kinetics, Mechanisms and Products, Ph.D. Thesis, University оf Cincinnati, 2012.
21.	Chiaramonti D, Bonini M, Fratini E, Tondi G, Gartner K, Bridgwater AV, et al. Development of emulsion from biomass pyrolysis liquid and diesel and their use in engines – Part 1: emulsion production. Biomass Bioenergy, No. 25, 2003, p. 85–99.
22.	Chiaramonti D, Bonini M, Fratini E, Tondi G, Gartner K, Bridgwater AV, et al. Development of emulsion from biomass pyrolysis liquid and diesel and their use in engines – Part 2: tests in diesel engines. Biomass Bioenergy, No. 25, 2003, p. 101–11.
23.	Chuck C.J., Donnelly J.: The compatibility of potential bioderived fuels with Jet A-1 aviation kerosene. Applied Energy. Vol. 118, 2014, p. 83–91.
24.	Cleveland C.J., Morris C. G.: Handbook of energy. Volume II: Cronologies, top ten lists, and words clouds. Elsvier Inc., 2014.
25.	Cushion E., Whiteman A., Dieterle G.: Bioenergy Development: Issues and Impacts for Poverty and Natural Resource Management. World Bank Publications, 2010.
26.	Daggett D. L., Hendricks R.C., Walther R., Corporan E.: Alternative fuels for use in commercial aircrafts. The Boeing Company, 2007.
27.	Dahlquist E.: Biomass as Energy Source. Resources, Systems and Applications. CRC Press, 2013.
28.	Delmon B., Grange P., Froment G.F.: Hydrotreatment and Hydrocracking of Oil Fractions. Elsevier, 1999.
29.	Doc 9889 Airport Air Quality Manual. International Civil Aviation Organization, 2011. 
30.	Doc 9977. Manual on Civil Aviation Jet Fuel Supply, 2012.
31.	Edwards T.: Advancements in Gas Turbine Fuels from 1943 to 2005. J Eng Gas Power, No. 129, 2007, p. 13–20.
32.	Firrisa M. T., Van Duren I., Voinov A.: Energy efficiency for rapeseed biodiesel production in different farming systems. Energy Efficiency, 2013.
33.	Garcia-Anton J., Monzo J., Guninon J.L.: Study of corrosion on copper strips by petroleum naphtha in the ASTM D-130 test by means of electronic microscopy (SEM) and energy dispersive X-ray (EDX). Fresenius Journal of Analytical Chemistry. Iss. 337, 1990, p. 382–388.
34.	Garcia Santander C.M., Gymez Rueda S.M., de Lima da Silva N.: Measurements of normal boiling points of fatty acid esters and triacylglycerols by thermogravimetric analysis, Fuel, Iss. 92, 2012, p. 158–161.
35.	Geller D. P., Goodrum J.: W. Effects of speciﬁc fatty acid methyl esters on diesel fuel lubricity, Fuel, Vol. 83, 2004, p. 2351–2356.
36.	Gupta, K. K, Rehman A, Sarviya R. M.: Bio-fuels for the gas turbine: A review. Renew. Sust. Energ. Rev. No. 14, 2010, p. 2946–2955.
37.	Harvey B. G, Merriman W.W., Koontz T.A.: High-Density Renewable Diesel and Jet Fuels Prepared from Multicyclic Sesquiterpanes and a 1‑Hexene-Derived Synthetic Paraffinic Kerosene, Energy Fuels, 2013.
38.	Hemighaus G., Boval T., Bosley C.: Alternative Jet Fuels. Addendum 1 to Aviation Fuels Technical Review (FTR-3/A1). Chevron Corporation, 2006.
39.	Hileman J.I., Stratton R.W.: Alternative jet fuel feasibility. Transport Policy. Vol. 34, 2014, p. 52–62.
40.	Hileman J.I., Wong H.M., Waitz I.: Near-Term Feasibility of Alternative Jet Fuels. Santa Monica, California: RAND Corporation, 2009.
41.	Hileman, J. Ortiz D., Bartis J.: Near-Term Feasibility of Alternative Jet Fuels. Jointly published by the RAND Corporation (Report No. TR-554-FAA) and the Partnership for Air Transportation Noise and Emissions Reduction, 2009.
42.	Honga T.D., Soerawidjajab T.H., Reksowardojoa I.K.: A study on developing aviation biofuel for the Tropics: Production process – Experimental and theoretical evaluation of their blends with fossil kerosene, Chemical Engineering and Processing: Process Intensification, Vol. 74, 2013, p. 124–130.
43.	Hristova M., Tchaoushev S.: Сalculation of flash points and flammability limits of substances and mixtures. Journal of the University of Chemical Technology and Metallurgy, Iss. 41(3), p. 291–296, 2006.
44.	Hu J., Du Z., Li C., Min E.: Study on the lubrication properties of biodiesel as fuel lubricity enhancers, Fuel. Vol. 84, 2005. p. 1601–1606. 
45.	Iakovlieva A., Boichenko S., Vovk O.: Investigation of the fractional composition of rape oil-derived aviation biofuels. Aviation in the XXI-st century. Safety in aviation and space technologies: the fifth world congress, 25–27 September 2012: abstracts. Kyiv, Vol. 3, 2012, p. 5.41–5.43.
46.	Iakovlieva A.V. Boichenko S.V., Vovk O.O.: Overview of innovative technologies for aviation fuels production. Journal of Chemistry and chemical technology, Vol. 7. Iss. 3, 2013, p. 305–312.
47.	Iakovlieva A., Lejda K., Vovk O., Boichenko S.: Peculiarities of the development and implementation of aviation biofuels in Ukraine. World Congress on Petrochemistry and Chemical Engineering. Journal of Petroleum & Environmental Biotechnology. November 2013, San Antonio. Vol.4. Iss. 6, 2013, p. 47.
48.	Iakovlieva A., Boichenko S., Gay A.: Cause-Effect Analysis of the Modern State in Production of Jet Fuels. Journal of Сhemistry & Chemical Technology. Vol. 8. No 1, 2014, p. 107–116.
49.	Iakovlieva A., Boichenko S., Vovk O., Lejda K.: Potential of jet biofuels production and application in Ukraine and Poland. International Journal of Sustainable Aviation. Vol. 1. No.4, 2015, p. 314–323.
50.	Iakovlieva A., Boichenko S., Lejda K.: Impact of rape oil ethyl esters additives on some characteristics of jet fuel. Проблеми хіммотології. Теорія та практика раціонального використання традиційних і альтернативних паливно -мастильних матеріалів: V міжнар. наук.-техн. конф., 6–10 жовт. 2014. Київ, c. 286 – 289. 
51.	Iakovlieva A., Lejda K., Vovk O., Boichenko S., Skilniuk I.: Vacuum Distillation of Rapeseed Oil Esters for Production of Jet Fuel Bio-Additives, Procedia Engineering, Vol. 187, 2017, p. 363 – 370. 
52.	Iakovlieva A., Lejda K., Vovk O., Boichenko S.: Рotential of jet biofuels production and application in Ukraine and Poland. Proceedings of the 1st International Simposium on Sustainable Aviation.–31 May–03 June 2015, Isntanbul, p. 137.
53.	Iakovlieva A., Boichenko S., Lejda K.: Experimental study on antiwear properties for blends of jet fuel with biocomponents derived from rapeseed oil. Eastern-European journal of enterprise technologies. No. 5/8(77), 2015, p. 20–28.
54.	Iakovlieva A., Vovk O., Boichenko S.: Еxperimental study of rape oil esters influence on physical-chemical properties of jet fuels. Proceedings of the 19th Conference for Junior Researchers ‘Science – Future of Lithuania’ Тransport engineering and management, 6 May 2016, Vilnius. p. 85–89.
55.	Iakovlieva A., Lejda K., Vovk O., Boichenko S., Kuszewski H. Improvement of technological scheme of fatty acids ethyl esters production for use as jet fuels biocomponents. International Journal of Theoretical and Applied Science. Iss. 11(19), 2014, p. 44–55.
56.	International Air Transport organization. Vision 2050. Report. Montreal. Geneva, 2011.
57.	Jansen R. A.: Second Generation Biofuels and Biomass: Essential Guide for Investors, Scientists and Decision Makers. Wiley. 2012.
58.	Jenkins R.W., Munro M., Christopher S.N., Chuck C.: Potential renewable oxygenated biofuels for the aviation and road transport sectors. Fuel, Vol. 103, 2013, p. 593–599.
59.	Jacyna M., Żak J., Jacyna-Gołda I., Merkisz J., Merkisz-Guranowska A., Pielecha J.: Selected aspects of the model of proecological transport system. Journal of KONES Powertrain and Transport, Vol. 20, No. 3, 2013, p. 193 – 202.
60.	Kallio P., Pasztor A., Akhtar M.K., Jones P.R.: Renewable jet fuel. Current Opinion in Biotechnology. Vol. 26, 2014, p. 50–55.
61.	Kandaramath Hari T., Yaakob Z., Binitha N.N.: Aviation biofuel from renewable resources: Routes, opportunities and challenges. Renewable and Sustainable Energy Reviews. Vol. 42, 2015, p. 1234–1244. 
62.	Kinder J. D., Rahmes T.: Evaluation of Bio-Derived Synthetic Paraffinic Kerosene (Bio-SPK). The Boeing Company Sustainable Biofuels Research&Technology Program, 2009.
63.	Kirklin P.W., David. P.: Aviation Fuel: Thermal Stability. ASTM International, 1992.
64.	Lapuerta M., Rodriguez-Fernandeza J., Estevez C., Bayarri N.: Properties of fatty acid glycerol formal ester (FAGE) for use as a component in blends for diesel engines. Biomass and bioenergy. Vol. 76, 2015, p. 130–140. 
65.	Lebedevas S., Vaicekauskas A.: Research into the application of biodiesel in the transport sector of Lithuania. Transport. Vol. 21, Iss. 2, 2006, p. 80–87.
66.	Liu G., Yan B., Chen G.: Technical review on jet fuel production. Renewable and Sustainable Energy Reviews. Vol. 25, 2013, p. 59–70. 
67.	Lu M., Chai M.: Experimental Investigation of the Oxidation of Methyl Oleate: One of the Major Biodiesel Fuel Components Synthetic Liquids Production and Refining. Chapter 13, P. 289–312. American Chemical Society. 2011
68.	Merkisz J., Merkisz-Guranowska, A., Pielecha J., Nowak M., Jacyna M., Lewczuk K., Żak J.: Exhaust emission measurements in the development of sustainable road transport. Journal of KONES Powertrain and Transport, Vol. 20, No. 4 2013, p. 277 – 284.
69.	Maksimuk Yu., Antonova Z., Fes’ko V., Kursevich V.: Diesel biofuel viscosity and heat of combustion. Chemistry and technology of fuels and oils. Iss. 45, 2009, p. 343–346.
70.	Maru M. M., Trommer R.M., Cavalcanti K.F.: The Stribeck curve as a suitable characterization method of the lubricity of biodiesel and diesel blends. Energy. Vol. 69, 2014, p. 673–681.
71.	Maurice L.Q., Lander H., Edwards T., Harrison W.E.: Advanced aviation fuels: a look ahead via a historical perspective. Fuel. Vol. 80, Iss. 5, 2001, p. 747–756.
72.	Merkisz J., Markowski J., Pielecha J. Emission tests of the AI-14RA aircraft engine under real operating conditions of PZL-104"" Wilga"" plane. Silniki Spalinowe. No. 3, 2009, p. 64–70.
73.	Merkisz J., Galant M., Karpiński D., Kubiak, K. Evaluation of possibility to use the LTO cycle for emission test on example of the model turbine engine GTM-120 Journal of Mechanical and Transport Engineering. Vol. 66, No. 2, 2014, p. 25—33.
74.	Murphy D.J., Hall C.A.S.: Year in review—EROI or energy return on (energy) invested. Annals of the New York academy of sciences. Issue: Ecological Economics Reviews. Iss. 1185, 2010, p. 102–118.
75.	Murphy D.J., Hall C.A.S., Powers B.:New perspectives on the energy return on (energy) investment (EROI) of corn ethanol. Environment, Development and Sustainability. Vol. 13, Iss. 1, 2011, p. 179–202.
76.	Naik S.N., Goud V.V., Rout P.K., Dalai A.K.: Production of first and second generation biofuels: A comprehensive review. Renew. Sust. Energ. Rev., No. 14, 2010, p. 578–597.
77.	Nollet Leo M. L.: Handbook of Food Analysis: Physical characterization and nutrient analysis. CRC Press, 2004.
78.	Orszulik S.: Environmental Technology in the Oil Industry. Springer Science & Business Media, 2013.
79.	Pandey A.: Biofuels: Alternative Feedstocks and Conversion Processes. Academic Press, 2011.
80.	Pearlson M.N.: A techno-economic and environmental assessment of hydroprocessed renewable distillate fuels. Master of Science in Technology and Policy. Massachiussets Institute of Technology. June 2011.
81.	Prag P.: Renewable Energy in the Countryside. Taylor & Francis, 2014.
82.	Prussi M, Chiaramonti D, Recchia L, Martelli F, Guidotti F, Pari L.: Alternative feedstock for the biodiesel and energy production: the OVEST project. Energy Journal, No. 58, 2013, p. 2–8.
83.	Rahmes T.F., Kinder J.D., Henry T.M., etc.: Sustainable Bio-Derived Synthetic Paraffinic Kerosene (BioSPK) Jet Fuel Flights and Engine Tests Program Results. American Institute of Aeronautics and Astronautics, 2009.
84.	Rajagopal D., Zilberman D.: Environmental, Economic and Policy Aspects of Biofuels. Nеw Publishers Inc., 2008.
85.	Report on alternative fuels. International Air Transport Association IATA. http://www.iata.org/publications/Documents/2012-report-alternativefuels. pdf; 2012
86.	Rosillo Calle F, Trhan D, Seiffert M, Teeluckingh S. The potential and role of biofuels in commercial air transport – biojetfuels. Task 40 sustainable international bioenergy trade. IEA Bioenergy
87.	Sarin R., Kumar R., Srivastav B., etc.: Biodiesel surrogates: Achieving performance demands. Bioresource Technology. Vol. 100, Iss. 12, 2009, p. 3022–3028. 
88.	Shen Y.. Аn experimental study on thermal stability of FAEE biodiesel fuel with ethanol. Master Thesis, 2015.
89.	Shepherd J.E., Nuyt C.D., Lee J.J.: Flash Point and Chemical Composition of Aviation Kerosene (Jet A). National Transportation Safety Board, 2000.
90.	Singh B.: Biofuel Crops: Production, Physiology and Genetics. CABI, 2013.
91.	Singh B.: Biofuel Crop Sustainability. John Wiley & Sons, 2013.
92.	Sperling D., Cannon J.S.: Reducing Climate Impacts in the Transportation Sector. Springer Science & Business Media, 2011.
93.	Szczerek M., Tuszyсski W. Tribological researches – scuffing. Radom: Institute for Sustainable Technologies – National Research Institute, 2000.
94.	The jet engine. Rolls-Royce plc. Renault Printing Co Ltd., 1996.
95.	T-02U. Aparat czterokulowy – instrukcja obsługi. Radom: Wydawnictwo Instytutu Technologii Eksploatacji, 2011.
96.	Wcisło G.: Determination of the impact of FAME biocomponent on the fractional composition of diesel engine fuels. Combustion Engines. Iss. 154(3), 2013, p. 1098–1103.
97.	Xu Y., Wang Q., Hu X.: Characterization of the lubricity of bio-oil/diesel fuel blends by high frequency reciprocating test rig. Energy. Vol. 35, Iss. 1, 2010, p. 283–287. 
98.	Yakovleva A.V., Boichenko S.V., Lejda K, Vovk O.O., Kuszewski H.: Antiwear Properties of Plant—Mineral-Based Fuels for Airbreathing Jet Engines, Chemistry and Technology of Fuels and Oils, Vol. 53, Iss. 1, 2017, p. 1–9. 
99.	Yakovlieva A.V., Boichenko S.V., Leida K., Vovk O.A., Kuzhevskii Kh.. Influence of Rapeseed Oil Ester Additives on Fuel Quality Index for Air Jet Engines, Chemistry and Technology of Fuels and Oils, Vol. 53, Iss. 3, 2017. p. 308–317.
100.	Yakovlieva A., Boichenko S., Vovk O., Lejda K., Gryshchenko O.. Case Study of Alternative Jet Fuel Production with Bio-additives from Plant Oils in Ukraine and Poland. Advances in Sustainable Aviation. Springer International Publishing, 2018. Chapter 4.
101.	Yakovlieva A., Boshkov V. Experimental study of low-temperature properties of alternative aviation fuels, Proceedings of the 21th Conference for Junior Researchers ‘Science – Future of Lithuania’ Transport Engineering and Management, 4-5 May 2018, Vilnius, Lithuania. 2018. p. 130 – 134.
102.	Yildirim U, Abanteriba S.: Manufacture, qualification and approval of new aviation turbine fuels and additives, proceedia Engineering, No. 49, 2012, p. 310 – 315.
103.	Yutko B. and Hansman J., Approaches to Representing Aircraft Fuel Efficiency Performance for the Purpose of a Commercial Aircraft Certification Standard, MITInternational Center for Air Transportation, Cambridge, Mass, 2011.
104.	Zhu Y.: An Experimental Study on Thermal Stability of Biodiesel Fuel. Master Thesis. – 2012. – 160 p.
105.	Авиационный турбореактивный двигатель РУ 19A-300, руководство по эксплуатации и техническому обслуживанию, ЗАО «АНТЦ Технолог», 2001.
106.	Азев В.С., Середа А.В.: Влияние соединений серы на противоизносные свойства дизельных топлив, Химия и технология топлив и масел. № 3, 2009, c. 23–27.
107.	Андіїшин М.П., Марчук Я.С., Бойченко С.В., Рябоконь Л.А.: Газ природний, палива та оливи. Одеса: Астропринт, 2010.
108.	Бойченко С.В., Спіркін В.Г. Вступ до хіммотології палив та олив: навч. посіб.: у 2-х ч. Одеса: Астропринт, Ч.1., 2009.
109.	Бойченко С.В., Любінін Й.А., Спіркін В.Г.: Вступ до хіммотології палив та олив: навч. посіб.: у 2-х ч. Одеса: Астропринт. Ч.2., 2010.
110.	Бойченко С.В., Черняк Л.М., Яковлєва А.В.: Традиційні технології виробництва палив для повітряно-реактивних двигунів. Вісник Національного авіаційного університету. № 2 (55), 2013, с. 195–209.
111.	Бойченко С. В., Яковлева А. В., Волошинец В. А., Лейда К. Модифицирование эфиров рапсового масла вакуумным фракционированием, Технологии нефти и газа, №5, 2018, c. 15–20
112.	Братичак М.М.: Основи промислової нафтохімії, Львів: Вид-во НУ «Львівська політехніка», 2008.
113.	Васильев И.П.: Влияние топлив растительного происхождения на экологические и экономические показатели дизеля, Луганск: Изд-во ВНУ им. В. Даля, 2009.
114.	Волошинець В.А. Фізична та колоїдна хімія: Фізико-хімія дисперсних систем та полімерів: навч.посіб. Львів : Вид-во Львів. політехніки, 2013. – 200 с.
115.	Голоскоков А.Н. Критерии сравнения эффективности традиционных и альтернативных энергоресурсов. Нефтегазовое дело. № 1, 2011, c. 285–301.
116.	Голоскоков А.Н. Пик добычи нефти и начало мирового энергетического кризиса. Нефтегазовое дело. 2010, c. 1–13.
117.	Данилов А.М., Каминский Э.Ф., Хавкин В.А.: Альтернативные топлива: достоинства и недостатки. Проблемы применения. Российский химический журнал (Журнал Российского химического общества им. Д.И. Менделеева). Т. XLVII. № 6, 2003, c. 4–11.
118.	Дворецкий С.И., Нагорнов С.А., Романцова С.В. и др.: Производство биодизельного топлива из органического сырья. Вопросы современной науки и практики. № 39, 2012, c. 126– 35.
119.	Девянин С.Н., Марков В.А., Семенов В.Г.: Растительные масла и топлива на их основе для дизельных двигателей. Харьков: Новое слово. 2007.
120.	Ергин Д.: Добыча: Всемирная история борьбы за нефть, деньги и власть. Москва,: Альпина Паблишер, 2011.
121.	Запорожець А.О.: Дослідження стехіометричної суміші «повітря ‒ паливо» органічних сполук. Частина 1. Алкани. Наукоємні технології. № 2(22), 2014, c. 163–167.
122.	Кириченко В., Бойченко С., Кириченко В., Нездоровин В.: Комплексная переработка технических растительных масел: концепция, методы и технологи. «Systems and means of motor transport» Seria: Transport. Monografia. № 4, 2013, p. 357–370.
123.	Колодницька Р.В., Семенов В.Г.: Моделювання низькотемпературних властивостей біодизельних палив. Вісник СевНТУ. Серія: Машиноприладобудування та транспорт. № 134, 2012, c. 135–138.
124.	Коллоидная химия нефти и нефтепродуктов: Сборник материалов, посвященных научной деятельности проф. Г.И. Фукса. Москва: Изд-во «Техника». ООО «Тума Групп», 2001.
125.	Крылов И.Ф., Емельянов В.Е.: Альтернативные моторные топлива. Производство, применение",,'National Aviation University',Modification of jet fuels composition with renewable bio-additives,10.18372/37895,https://core.ac.uk/download/322991098.pdf,core
429073874,2019-01-01T00:00:00,"The existing Big Data of transport flows and railway operations can be mined through advanced statistical analysis and machine learning methods in order to describe and predict well the train speed, punctuality, track capacity and energy consumption. The accurate modelling of the real spatial and temporal distribution of line and network transport, traffic and performance stimulates a faster construction and implementation of robust and resilient timetables, as well as the development of efficient decision support tools for real-time rescheduling of train schedules. In combination with advanced train control and safety systems even (semi-) automatic piloting of trains on main and regional railway lines will become feasible in near future.</p",,'Springer Science and Business Media LLC',从数据挖掘到智能调度决策支持: 存在问题与实施路径,10.11860/j.issn.1673-0291.2019.01.003,,core
151256677,2019-03-16T00:00:00,"The use of flying platforms such as unmanned aerial vehicles (UAVs),
popularly known as drones, is rapidly growing. In particular, with their
inherent attributes such as mobility, flexibility, and adaptive altitude, UAVs
admit several key potential applications in wireless systems. On the one hand,
UAVs can be used as aerial base stations to enhance coverage, capacity,
reliability, and energy efficiency of wireless networks. On the other hand,
UAVs can operate as flying mobile terminals within a cellular network. Such
cellular-connected UAVs can enable several applications ranging from real-time
video streaming to item delivery. In this paper, a comprehensive tutorial on
the potential benefits and applications of UAVs in wireless communications is
presented. Moreover, the important challenges and the fundamental tradeoffs in
UAV-enabled wireless networks are thoroughly investigated. In particular, the
key UAV challenges such as three-dimensional deployment, performance analysis,
channel modeling, and energy efficiency are explored along with representative
results. Then, open problems and potential research directions pertaining to
UAV communications are introduced. Finally, various analytical frameworks and
mathematical tools such as optimization theory, machine learning, stochastic
geometry, transport theory, and game theory are described. The use of such
tools for addressing unique UAV problems is also presented. In a nutshell, this
tutorial provides key guidelines on how to analyze, optimize, and design
UAV-based wireless communication systems.Comment: Accepted in IEEE Communications Surveys and Tutorials, 201",,,"A Tutorial on UAVs for Wireless Networks: Applications, Challenges, and
  Open Problems",,http://arxiv.org/abs/1803.00680,core
212728120,2019-01-01T00:00:00,"City-scale traffic prediction is an important task for public safety, traffic management, and deployment of intelligent transportation systems. Many approaches have been proposed to address traffic prediction task using machine learning techniques. In this paper, we present a framework to help on addressing the task at hand (density-, traffic flow- and origin-destination flow predictions) considering data type, features, deep learning techniques such as Convolutional Neural Networks (CNNs), e.g., Autoencoder, Recurrent Neural Networks (RNNs), e.g., Long Short Term Memory (LSTM), and Graph Convolutional Networks (GCNs). An autoencoder model is designed in this paper to predict traffic density based on historical data. Experiments on real-world taxi order data demonstrate the effectiveness of the model.QC 20190625</p",,"KTH, Geoinformatik",A Framework for Traffic Prediction Integrated with Deep Learning,,,core
231707630,2019-01-01T00:00:00,"© 2019 by the authors.The intensification of the hydrological cycle because of global warming raises concerns about future floods and their impact on large cities where exposure to these events has also increased. The development of adequate adaptation solutions such as early warning systems is crucial. Here, we used deep learning (DL) for weather-runoff forecasting in región Metropolitana of Chile, a large urban area in a valley at the foot of the Andes Mountains, with more than 7 million inhabitants. The final goal of this research is to develop an effective forecasting system to provide timely information and support in real-time decision making. For this purpose, we implemented a coupled model of a near-future global meteorological forecast with a short-range runoff forecasting system. Starting from a traditional hydrological conceptual model, we defined the hydro-meteorological and geomorphological variables that were used in the data-driven weather-runoff forecast models. The me",,'MDPI AG',Hydrological early warning system based on a deep learning runoff model coupled with a meteorological forecast,10.3390/w11091808,,core
222796530,2019-01-01T00:00:00,"City-scale traffic prediction is an important task for public safety, traffic management, and deployment of intelligent transportation systems. Many approaches have been proposed to address traffic prediction task using machine learning techniques. In this paper, we present a framework to help on addressing the task at hand (density-, traffic flow- and origin-destination flow predictions) considering data type, features, deep learning techniques such as Convolutional Neural Networks (CNNs), e.g., Autoencoder, Recurrent Neural Networks (RNNs), e.g., Long Short Term Memory (LSTM), and Graph Convolutional Networks (GCNs). An autoencoder model is designed in this paper to predict traffic density based on historical data. Experiments on real-world taxi order data demonstrate the effectiveness of the model.QC 20190625</p",,"KTH, Geoinformatik",A Framework for Traffic Prediction Integrated with Deep Learning,,,core
200825699,2019-04-21T00:00:00,"Autonomous navigation is an essential capability of smart mobility for mobile
robots. Traditional methods must have the environment map to plan a
collision-free path in workspace. Deep reinforcement learning (DRL) is a
promising technique to realize the autonomous navigation task without a map,
with which deep neural network can fit the mapping from observation to
reasonable action through explorations. It should not only memorize the trained
target, but more importantly, the planner can reason out the unseen goal. We
proposed a new motion planner based on deep reinforcement learning that can
arrive at new targets that have not been trained before in the indoor
environment with RGB image and odometry only. The model has a structure of
stacked Long Short-Term memory (LSTM). Finally, experiments were implemented in
both simulated and real environments. The source code is available:
https://github.com/marooncn/navbot",,,"Learning to Navigate in Indoor Environments: from Memorizing to
  Reasoning",,http://arxiv.org/abs/1904.06933,core
232192780,2019-05-10T00:00:00,"This is an accepted manuscript of an article published by Elsevier in Cognitive Computation on 10/05/2019, available online:  https://doi.org/10.1007/s12559-019-09653-z 
The accepted version of the publication may differ from the final published version.Objective:
Next-generation audiovisual (AV) hearing-aids stand as a major enabler to realise
more intelligible audio. However, high data rate, low latency, low computational
complexity, and privacy are some of the major bottlenecks to the successful
deployment of such advanced hearing-aids. To address these challenges, we
propose a novel framework based on an integration of 5G Cloud-Radio Access
Network (C-RAN), Internet of Things (IoT), and strong privacy algorithms to
fully benefit from the possibilities these technologies have to offer.
Background:
Existing audio-only hearing-aids are known to perform poorly in noisy situations
where overwhelming noise is present. Current devices make the signal more
audible but remains deficient to restore intelligibility. Thus, we need hearing
aids that can selectively amplify the attended talker or filter out acoustic clutter
Methods:
1
The proposed 5G IoT enabled AV hearing-aid framework transmits the encrypted
compressed AV information and receives encrypted enhanced reconstructed
speech in real-time to address cybersecurity attacks such as location privacy
and eavesdropping. For security implementation, a real-time lightweight AV
encryption is proposed, based on a piece-wise linear chaotic map (PWLSM),
Chebyshev map, and a secure hash and S-Box algorithm. For speech enhancement,
the received secure AV (including lip-reading) information in the cloud is used
to filter noisy audio using both deep learning and analytical acoustic modelling.
To offload the computational complexity and real-time optimization issues,
the framework runs deep learning and big data optimization processes in the
background on the cloud.
Results:
The effectiveness and security of our proposed 5G-IoT-enabled AV hearing-aid
framework are extensively evaluated using widely known security metrics. Our
newly reported, deep learning-driven lip-reading approach for speech enhancement is evaluated under four different dynamic real-world scenarios (cafe, street,
public transport, pedestrian area) using benchmark Grid and ChiME3 corpora.
Comparative critical analysis in terms of both speech enhancement and AV encryption demonstrate the potential of our envisioned technology to deliver high
quality speech reconstruction and secure mobile AV hearing aid communication.
Conclusion:
We believe that the proposed 5G IoT enabled AV hearing aid is an effective
and feasible solution and represents a step change in the development of next
generation multimodal digital hearing aids. The ongoing and future work
includes more extensive evaluation and comparison with benchmark lightweight
encryption algorithms and hardware prototype implementation","[{'title': 'Cognitive Computation', 'identifiers': ['1866-9956', 'issn:1866-9956']}]",'Springer Science and Business Media LLC',"A novel real-time, lightweight chaotic-encryption scheme for next-generation audio-visual hearing-aids",10.1007/s12559-019-09653-z,,core
327015831,2019-10-22T00:00:00,"A Digital Twin (DT) refers to a digital replica of physical assets, processes and systems. DTs integrate artificial intelligence, machine learning and data analytics to create living digital simulation models that are able to learn and update from multiple sources, and to represent and predict the current and future conditions of physical counterparts. However, the current activities related to DTs are still at an early stage with respect to buildings and other infrastructure assets from an architectural and engineering/construction point of view. Less attention has been paid to the operation & maintenance (O&M) phase, which is the longest time span in the asset life cycle. A systematic and clear architecture verified with practical use cases for constructing a DT would be the foremost step for effective operation and maintenance of buildings and cities. According to current research about multi-tier architectures, this paper presents a system architecture for DTs which is specifically designed at both the building and city levels. Based on this architecture, a DT demonstrator of the West Cambridge site of the University of Cambridge was developed, which integrates heterogeneous data sources, supports effective data querying and analysing, supports decision-making processes in O&M management, and further bridges the gap between human relationships with buildings/cities. This paper aims at going through the whole process of developing DTs in building and city levels from the technical perspective and sharing lessons learnt and challenges involved in developing DTs in real practices. Through developing this DT demonstrator, the results provide a clear roadmap and present particular DT research efforts for asset management practitioners, policymakers and researchers to promote the implementation and development of DT at the building and city levels",,'American Society of Civil Engineers (ASCE)',Developing a dynamic digital twin at building and city levels: A case study of the West Cambridge campus,,,core
287207806,2019,"The pervasive and increasing deployment of smart meters allows collecting a huge amount of fine-grained energy data in different urban scenarios. 

The analysis of such data is challenging and opening up a variety of interesting and new research issues across energy and computer science research areas. 

The key role of computer scientists is providing energy researchers and practitioners with cutting-edge and scalable analytics engines to effectively support their daily research activities, hence fostering and leveraging data-driven approaches.

This paper presents SPEC, a scalable and distributed engine to predict building-specific power consumption.

SPEC addresses the full analytic stack and exploits a data stream approach over sliding time windows to train a prediction model tailored to each building. The model allows us to predict the upcoming power consumption at a time instant in the near future. 

SPEC integrates different machine learning approaches, specifically 

ridge regression, artificial neural networks, and random forest regression, to predict fine-grained values of power consumption, and  

a classification model, the random forest classifier, to forecast a coarse consumption level. 

SPEC exploits state-of-the-art distributed computing frameworks to address the big data challenges in harvesting energy data: the current implementation runs on Apache Spark, the most widespread high-performance data-processing platform, and can natively scale to huge datasets.

As a case study, SPEC has been tested on real data {of an heating distribution network and power consumption data} collected in a major Italian city.

Experimental results demonstrate the effectiveness of SPEC to forecast both fine-grained values and coarse levels of power consumption of~buildings",,'MDPI AG',Exploiting scalable machine-learning distributed frameworks to forecast power consumption of buildings,10.3390/en12152933,,core
475180207,2019-04-04T07:00:00,"The recent advancements in computing and sensor technologies, coupled with improvements in embedded system design methodologies, have resulted in the novel paradigm called the Internet of Things (IoT). IoT is essentially a network of small embedded devices enabled with sensing capabilities that can interact with multiple entities to relay information about their environments. This sensing information can also be stored in the cloud for further analysis, thereby reducing storage requirements on the devices themselves. The above factors, coupled with the ever increasing needs of modern society to stay connected at all times, has resulted in IoT technology penetrating all facets of modern life. In fact IoT systems are already seeing widespread applications across multiple industries such as transport, utility, manufacturing, healthcare, home automation, etc.
Although the above developments promise tremendous benefits in terms of productivity and efficiency, they also bring forth a plethora of security challenges. Namely, the current design philosophy of IoT devices, which focuses more on rapid prototyping and usability, results in security often being an afterthought. Furthermore, one needs to remember that unlike traditional computing systems, these devices operate under the assumption of tight resource constraints. As such this makes IoT devices a lucrative target for exploitation by adversaries. This inherent flaw of IoT setups has manifested itself in the form of various distributed denial of service (DDoS) attacks that have achieved massive throughputs without the need for techniques such as amplification, etc. Furthermore, once exploited, an IoT device can also function as a pivot point for adversaries to move laterally across the network and exploit other, potentially more valuable, systems and services. Finally, vulnerable IoT devices operating in industrial control systems and other critical infrastructure setups can cause sizable loss of property and in some cases even lives, a very sobering fact.
In light of the above, this dissertation research presents several novel strategies for identifying known and  zero-day attacks against IoT devices, as well as identifying infected IoT devices present inside a network along with some mitigation strategies. To this end, network telescopes are  leveraged to generate Internet-scale notions of maliciousness in conjunction with signatures that can be used to identify such devices in a network. This strategy is further extended by developing a taxonomy-based methodology which is capable of categorizing unsolicited IoT behavior by leveraging machine learning (ML) techniques, such as ensemble learners, to identify similar threats in near-real time. Furthermore, to overcome the challenge of insufficient (malicious) training data within the IoT realm, a generative adversarial network (GAN) based framework is also developed to identify known and unseen attacks on IoT devices. Finally, a software defined networking (SDN) based solution is proposed to mitigate threats from unsolicited IoT devices",,Scholar Commons,Security Framework for the Internet of Things Leveraging Network Telescopes and Machine Learning,,,core
201044148,2019-01-01T00:00:00,"Tunnel settlement commonly occurs during the tunnel construction processes in large cities. Existing forecasting methods for tunnel settlements include model-based approaches and artificial intelligence (AI) enhanced approaches. Compared with traditional forecasting methods, artificial neural networks can be easily implemented, with high performance efficiency and forecasting accuracy. In this study, an extended machine learning framework is proposed combining particle swarm optimization (PSO) with support vector regression (SVR), back-propagation neural network (BPNN), and extreme learning machine (ELM) to forecast the surface settlement for tunnel construction in two large cities of China P.R. Based on real-world data verification, the PSO-SVR method shows the highest forecasting accuracy among the three proposed forecasting algorithms","[{'title': 'Mathematical Problems in Engineering', 'identifiers': ['issn:1024-123X', '1024-123x', '1563-5147', 'issn:1563-5147']}]",'Hindawi Limited',Modern Machine Learning Techniques for Univariate Tunnel Settlement Forecasting: A Comparative Study,10.1155/2019/7057612,,core
334858232,2019-09-12T00:00:00,"Urban imagery usually serves as forensic analysis and by design is available
for incident mitigation. As more imagery collected, it is harder to narrow down
to certain frames among thousands of video clips to a specific incident. A
real-time, proactive surveillance system is desirable, which could instantly
detect dubious personnel, identify suspicious activities, or raise momentous
alerts. The recent proliferation of the edge computing paradigm allows more
data-intensive tasks to be accomplished by smart edge devices with lightweight
but powerful algorithms. This paper presents a forensic surveillance strategy
by introducing an Instant Suspicious Activity identiFication at the Edge
(I-SAFE) using fuzzy decision making. A fuzzy control system is proposed to
mimic the decision-making process of a security officer. Decisions are made
based on video features extracted by a lightweight Deep Machine Learning (DML)
model. Based on the requirements from the first-line law enforcement officers,
several features are selected and fuzzified to cope with the state of
uncertainty that exists in the officers' decision-making process. Using
features in the edge hierarchy minimizes the communication delay such that
instant alerting is achieved. Additionally, leveraging the Microservices
architecture, the I-SAFE scheme possesses good scalability given the increasing
complexities at the network edge. Implemented as an edge-based application and
tested using exemplary and various labeled dataset surveillance videos, the
I-SAFE scheme raises alerts by identifying the suspicious activity in an
average of 0.002 seconds. Compared to four other state-of-the-art methods over
two other data sets, the experimental study verified the superiority of the
I-SAFE decentralized method.Comment: Manuscript has been accepted and to be presented at the Fourth
  ACM/IEEE Symposium on Edge Computing, Washington DC, November 7-9, 201",,,"I-SAFE: Instant Suspicious Activity identiFication at the Edge using
  Fuzzy Decision Making",,http://arxiv.org/abs/1909.05776,core
297074030,2019-12-04T00:00:00,"The goal of our research was to develop methods based on convolutional neural networks for automatically extracting the locations of buildings from high-resolution aerial images. To analyze the quality of developed deep learning algorithms, there was used Sorensen-Dice coefficient of similarity which compares results of algorithms with real masks. These masks were generated automatically from json files and sliced on smaller parts together with respective aerial photos before the training of developed convolutional neural networks. This approach allows us to cope with the problem of segmentation for high-resolution satellite images. All in all we show how deep neural networks implemented and launched on modern GPUs of high-performance supercomputer NVIDIA DGX-1 can be used to efficiently learn and detect needed objects. The problem of building detection on satellite images can be put into practice for urban planning, building control of some municipal objects, search of the best locations for future outlets etc",,'EDP Sciences',Urban areas analysis using satellite image segmentation and deep neural network,10.1051/e3sconf/201913501064,,core
323232029,2019-08-27T00:00:00,"International audienceA study on the increasing of projects about autonomous vehicles shows that almost of research fields are concerned, from technological domains (improvement of radar and lidar technologies, AI for best choice decisions, etc…) to social sciences (acceptability for example), including economic domain or insurance and legal fields (responsibility cases). For less than 10 years, geographers and land planners have also investigated this thematic, most using simulations software. These simulations, based on theoretical cases (Fagnant and al., 2014) or on real urban territories (Spieser and al., 2014, Bösh and al., 2016), show “predictable” impacts like the decreasing of the number of cars and the growing of daily mobility, but with different rates, depending of the input scenario. Another way of investigation, still not yet explored, need to be presented: the impact on the urban form. For this reason, we purpose to build more complex scenarios combining daily mobility and residential mobility at different time steps: as predicted, the urban sprawl currently found will it continue, extending Newman and Kenworthy’s publication (Newman and Kenworthy, 1996) on the 3 ages of the city? Or the improvement of the traffic conditions provided from automated mobility which is materialized by the disappearance of the congestion and the emissions of pollutants, can it lead to a densification of the central urban spaces",,HAL CCSD,Mobility simulation including autonomous vehicles: What about the urban form ?,,,core
286035359,2019-01-01T00:00:00,"The existing Big Data of transport flows and railway operations can be mined through advanced statistical analysis and machine learning methods in order to describe and predict well the train speed, punctuality, track capacity and energy consumption. The accurate modelling of the real spatial and temporal distribution of line and network transport, traffic and performance stimulates a faster construction and implementation of robust and resilient timetables, as well as the development of efficient decision support tools for real-time rescheduling of train schedules. In combination with advanced train control and safety systems even (semi-) automatic piloting of trains on main and regional railway lines will become feasible in near future.Transport and Plannin",,'Springer Science and Business Media LLC',从数据挖掘到智能调度决策支持: 存在问题与实施路径,10.11860/j.issn.1673-0291.2019.01.003,,core
200862582,2019-04-01T00:00:00,"As the Internet-of-Things (IoT) and edge computing have been major paradigms for distributed data collection, communication, and processing, smart city applications in the real world tend to adopt IoT and edge computing broadly. Today, more and more machine learning algorithms would be deployed into front-end sensors, devices, and edge data centres rather than centralised cloud data centres. However, front-end sensors and devices are usually not so capable as those computing units in huge data centres, and for this sake, in practice, engineers choose to compromise for limited capacity of embedded computing and limited memory, e.g., neural network models being pruned to fit embedded devices. Visual object tracking is one of many important elements of a smart city, and in the IoT and edge computing context, high requirements to computing power and memory space severely prevent massive and accurate tracking. In this paper, we report on our contribution to object tracking on lightweight computing including (1) using limited computing capacity and memory space to realise tracking; (2) proposing a new algorithm region proposal correlation filter fitting for most edge devices. Systematic evaluations show that (1) our techniques can fit most IoT devices; (2) our techniques can keep relatively high accuracy; and (3) the generated model size is much less than others","[{'title': 'Sensors', 'identifiers': ['issn:1424-8220', '1424-8220']}]",'MDPI AG',Object Tracking for a Smart City Using IoT and Edge Computing,10.3390/s19091987,,core
226964524,2019,"Adaptive Human-Machine Interfaces and Interactions (HMI2) are closed-loop cyber-physical systems comprising a network of sensors measuring human, environmental and mission parameters, in conjunction with suitable software for adapting the HMI2 (command, control and display functions) in response to these real-time measurements. Cognitive HMI2 are a particular subclass of these systems, which support dynamic HMI2 adaptations based on the user&#039;s cognitive state. These states are estimated in real-time using various neuro-physiological parameters from gaze, cardiorespiratory and brain signals, which are processed by an Adaptive Neuro-Fuzzy Inference System (ANFIS). However, the accuracy and precision of neuro-physiological measurements are affected by a variety of environmental factors and therefore need to be accurately characterised prior to operational use. This paper describes the characterisation activities performed on two types of eye tracking devices used in the Aerospace Intelligent and Autonomous Systems (AIAS) laboratory of RMIT University to support the development of cognitive human-machine systems. The uncertainty associated with the ANFIS outputs is quantified by propagating the uncertainties in the input data (determined experimentally) through the inference engine. This process is of growing relevance because similar machine learning techniques are now being developed for an increasing number of applications including aerospace, transport, biomedical and defence cyber-physical systems",,Elsevier (Netherlands),Experimental Characterisation of Eye-Tracking Sensors for Adaptive Human-Machine Systems,,,core
250571617,2019-12-04T00:00:00,"International audiencePredicting, in the one hand, the time duration that a vehicle remains associated to a cell i.e. Network Attachment Point (NAP) and, on the other hand, the next cell can help anticipating network control decisions to provide services with stringent requirements despite vehicle mobility. In this paper, we propose a machine learning based approach for Software Defined Vehicular Networks that allows a cell to estimate the attachment duration of each newly associated vehicle at the association request time, as well as, a prediction of the upcoming cell, performed at the SDN controller that controls the cells. Our proposed models have been evaluated on a large dataset, which we have generated based on a real mobility trace from the city of Luxembourg, and the evaluation shows promising results in terms of prediction accuracy",,HAL CCSD,Effective Prediction of V2I Link Lifetime and Vehicle's Next Cell for Software Defined Vehicular Networks: A Machine Learning Approach,,,core
201227050,2019-02-01T00:00:00,"Emergency personnel, such as firefighters, bomb technicians, and urban search and rescue specialists, can be exposed to a variety of extreme hazards during the response to natural and human-made disasters. In many of these scenarios, a risk factor is the presence of hazardous airborne chemicals. The recent and rapid advances in robotics and sensor technologies allow emergency responders to deal with such hazards from relatively safe distances. Mobile robots with gas-sensing capabilities allow to convey useful information such as the possible source positions of different chemicals in the emergency area. However, common gas sampling procedures for laboratory use are not applicable due to the complexity of the environment and the need for fast deployment and analysis. In addition, conventional gas identification approaches, based on supervised learning, cannot handle situations when the number and identities of the present chemicals are unknown. For the purpose of emergency response, all the information concluded from the gas detection events during the robot exploration should be delivered in real time. To address these challenges, we developed an online gas-sensing system using an electronic nose. Our system can automatically perform unsupervised learning and update the discrimination model as the robot is exploring a given environment. The online gas discrimination results are further integrated with geometrical information to derive a multi-compound gas spatial distribution map. The proposed system is deployed on a robot built to operate in harsh environments for supporting fire brigades, and is validated in several different real-world experiments of discriminating and mapping multiple chemical compounds in an indoor open environment. Our results show that the proposed system achieves high accuracy in gas discrimination in an online, unsupervised, and computationally efficient manner. The subsequently created gas distribution maps accurately indicate the presence of different chemicals in the environment, which is of practical significance for emergency response","[{'title': 'Sensors', 'identifiers': ['issn:1424-8220', '1424-8220']}]",'MDPI AG',Towards Gas Discrimination and Mapping in Emergency Response Scenarios Using a Mobile Robot with an Electronic Nose,10.3390/s19030685,,core
232687139,2019-01-01T08:00:00,"Effective management of runoff from rain and snowmelt is critical as increased water flows can negatively affect efficiency and reliability at treatment facilities, as well as potentially damage property or the natural environment. Implementation of artificial intelligence for real-time decision making and support in wet weather infrastructure is a recent technological development; as such, a problem has emerged: experience and knowledge of best practices for successful implementation is limited. Artificial intelligence is being employed to inform operational decisions that are intended to improve the efficiency and reliability of physical wet weather infrastructure. The goal of municipalities and utilities in utilizing artificial intelligence is to maximize use of the existing physical infrastructure and reduce the need for future capital investment. Because artificial intelligence for real-time decision making and support in wet weather infrastructure is a relatively new technology, experience and knowledge of best practices for successful implementation is limited. In addition, staff have been reluctant to embrace or trust the decisions and support made by the AI systems in certain cases. This study approaches the problem through comprehensive review of recent literature and interviews of those responsible for previous implementations of artificial intelligence in Saint Paul, MN, Buffalo, NY, and Kansas City, MO. Best practices include continuous operator input and ongoing training throughout the project, effective and proper maintenance of the “inputs” to the artificially intelligent system, and incorporation of failsafe mechanisms in the design. As artificial intelligence becomes more prevalent in the civil engineering industry and computers are increasingly given real-time control of systems, this study could provide future designers with a framework for successful implementation of artificial intelligence in wet weather infrastructure projects",,IdeaExchange@UAkron,Artificial Intelligence in Wet Weather Infrastructure,,https://core.ac.uk/download/232687139.pdf,core
228326062,01/05/2019,"An emerging urban challenge will be the proliferation of Remotely Piloted Aircraft Systems (RPAS) or drones, as their usage grows and drones fill the urban skies. Urban airspace will include many more Uninhabited Aerial Vehicles (UAVs), and related accidents and mishaps will also increase. Applications of UAVs in urban environments include photography and film-making, security monitoring, real estate, construction, property and infrastructure inspections, leisure, public safety (fires, natural disasters, investigations), traffic, and much more. Emerging UAV applications include last-mile drone delivery services. This paper discusses urban scenarios where drones are more ubiquitous. Monitoring and related safety of these UAVs will be increasingly important. Though multi-modal purpose-built drone tracking and monitoring systems will be the most effective solution for detection and tracking of these RPAS, during the transition to more regular drone use in urban areas and in everyday urban applications, a more rapid-deployment and agile detection system is needed that does not require installation of hardware. In this paper, sensory substitution is presented as an approach to use ambient and pre-existing microphones and AI techniques to detect the presence of RPAS. Preliminary results show promise for this agile IoT method as a key solution to this emerging urban challenge",,,Emerging Urban Challenge: RPAS/UAVs in Cities,10.1109/DCOSS.2019.00103,,core
304995515,2019-08-21T13:54:59,"Complex systems, such as healthcare systems, cities, and information networks, often produce a large volume of time series data, along with ordered event data, which are discrete in time and space, and rich in other features (e.g., markers or texts). We model the asynchronous event data as point processes. It is essential to understand and model the complex dynamics of these time series and event data so that accurate prediction, reliable detection, or smart intervention can be carried out for social goods. Specifically, my thesis focuses on the following aspects: (1) new statistical models and effective learning algorithms for complex dynamics exhibited in event data; (2) new inference algorithms for change-point detection, and temporal logic reasoning involving time series and event data. In Chapter 1, we propose a kernel-based nonparametric change-point detection method for high-dimensional streaming data. Change-point detection is an essential topic in modern complex systems. For example, wearable sensors are nowadays common in healthcare systems, which make it possible to monitor patients' health status in real time. Early event detection of deterioration is helpful and can even save patients' lives. However, it is challenging to aggregate measurements from different sensors to form one indicator, and it is not clear how to define pre- and post- change-point distributions. To tackle this problem, in Chapter 1, we propose a distribution-free and computationally efficient kernel-based nonparametric change-point detection method, which enjoys fewer assumptions on the distributions and can handle high-dimensional streaming data. Theoretical tail probability approximation of the nonparametric statistic is also proposed, which provides a statistically principled way to determine the detection thresholds. The proposed nonparametric method shows excellent performance on real human-activity detection dataset and speech dataset. In Chapter 2, we model networked asynchronous event data as point processes and propose a continuous-time change-point detection framework to detect dynamic changes in networks. We cast the problem into a sequential hypothesis test, and derive the generalized likelihood-ratio (GLR) statistic for networked point processes by considering the network topology. The constructed statistic can achieve weak signal detection by aggregating local statistics over time and networks. We further propose to evaluate the proposed GLR statistic via an efficient EM-like algorithm which can be implemented in a distributed fashion across dimensions. Similarly, we obtain a highly accurate theoretical threshold characterization for the proposed GLR statistic and demonstrate the excellent performance of our method on real social media datasets, such as Twitter and Memetracker. In Chapter 3, we propose an expressive model for the event data and further propose an adversarial learning framework to uncover the temporal dynamics. When modeling event data as point processes, instead of hand-crafting the occurrence intensity function by a parametric form, we leverage recent advances in deep learning and parameterize the intensity function as a recurrent neural network (RNN). RNN is a composition of a series of highly flexible nonlinear functions, which allows the model to capture complex dynamics in event data and make the generative process mimic the real data much better than the prior art. Fitting neural network models for even data is challenging. We develop a novel adversarial learning framework to address this challenge and further avoid model-misspecification. Our method provides a novel connection of such event data fitting method to inverse reinforcement learning, where a stochastic policy and the associated reward function are learned simultaneously. The proposed framework has been evaluated on real crime, social network, and healthcare datasets, and outperforms the state-of-the-art methods in data description. In Chapter 4, we propose a unified framework to integrate first-order temporal logic rules into point process models for event data. The proposed modeling framework excels in small data regime and has the ability to incorporate domain knowledge. The proposed temporal logic point processes model the intensity function of the event starts and ends via a set of first-order temporal logic rules. Using softened representation of temporal relations, and a weighted combination of logic rules, our framework can also deal with uncertainty in event data. Furthermore, many existing point process models can be interpreted as special cases of our framework given simple temporal logic rules. We derive a maximum likelihood estimation procedure for the proposed temporal logic point processes, and show that it can lead to accurate predictions when data are sparse and domain knowledge is critical. The proposed framework has been evaluated on real healthcare datasets, and outperforms the neural network models in event predication on small data and is easy to interpret.Ph.D",,Georgia Institute of Technology,"Statistical inference, modeling, and learning of point processes",,https://core.ac.uk/download/304995515.pdf,core
237487410,2019-01-01T00:00:00,"In recent years it has become common to speak about the republic of letters as a
network. But this was not always the case. Rather, it is the product of a specific set
of conditions: the confluence of readily available digitized documents, computational power to analyse that data, and a ready acceptance of the ‘network perspective’ in the popular consciousness. In our increasingly interconnected world we
encounter networks at every turn. The Internet, public transport networks, and
power grids make our everyday lives possible; our careers are dependent on networking; and social networking sites provide an online account of our professional
and personal capital. Networks have become a metaphor for connectedness, but
also a concrete framework for visualizing and measuring complex systems of
knowledge in the era of big data.

Although scholars working in the humanities might not realize it, the network
turn is due to the emergence of ‘network science’ as a field of interdisciplinary
study. In a series of key publications in the late 1990s and early 2000s, scholars
such as Albert-László Barabási, Reka Albert, Duncan J. Watts, and Steven Strogatz
showed that a huge variety of real-world networks – such as, for example, neural
networks, transport networks, biological regulatory networks, and social networks
– share an underlying order, follow simple laws, and therefore can be analysed using the same mathematical tools and models.1 These publications build on work
from various different disciplines, such as sociology, mathematics, and physics,
which stretches back some decades; but the emergence of network science as a
field in its own right was the product of certain conditions that did not exist before. Barabási and Albert explicitly cite the computerization of data acquisition as
essential to their research. In other words, what they needed was numerous examples of big network data, which they could compare, and the computational power
to analyse that data. In this field, thousands of publications every year describe the
development of new quantitative network analysis methods, and the analysis of
new types of network data.

The advent of large-scale digitization efforts in the humanities has given scholars unprecedented access to their research materials. Perhaps more importantly,
however, it has also put quantitative analysis methods within the reach of this
community. This is particularly true of large collections of metadata, as these represent structured information that is easier to abstract and quantify. Correspondence metadata, such as the data collected by the constituent members of the COST
Action Reassembling the Republic of Letters, lends itself particularly well to quantitative
analysis, as it is exactly the kind of data that network analysis was designed to study
– a set of well-defined relationships, namely letters sent and received, between
well-defined entities, namely individuals. As discussed in chapter II.4, some work
may be necessary to establish the identities of the individuals, but correspondence
is a social relationship that is particularly clearly defined, due to its physical manifestation in the form of the manuscript letter.

The value of the COST Action Reassembling the Republic of Letters additionally relies on a ‘network effect’ – a term employed in the context of modern technology
companies, which means that the value of a software product rises with the number of people using it, as such products typically facilitate interactions between
users in some way. By combining the metadata of a wide range of historical correspondence projects, and by making them compatible with each other, their combined value to the scholarly community is greatly increased. Consistent metadata
allows for much more wide-ranging searches across correspondence collections,
and the power of quantitative network analysis grows rapidly with the size and
scope of the network under study",,Göttingen University Press,Networking the Republic of Letters,,,core
300016020,2019-01-01T00:00:00,"Illegal parking can be a ubiquitous concern faced by urban cities, posing potential traffic impediments and safety risks to other road users. Despite having surveillance systems deployed to monitor traffic offences, the videos recorded are often stored only for post-event forensics. Manually inspecting the videos often involves repetitive human labour, which is tedious and prone to errors.

In this project, a fully automated pipeline to perform end-to-end illegal parking detection with minimal or no human-in-the-loop was proposed. The pipeline first consists of vehicle detection using a deep learning based object detection algorithm, You Only Look Once Version 3 (YOLOv3), to detect vehicles. Next, movement tracking using template matching and Intersection over Union (IoU) are performed to track the time since the violating vehicle has remained stationary. The last step is to extract the license plate, using OpenALPR, of the violating vehicle which has remained stationary for a defined period.

With the fully automated pipeline in place, the dataset can be intelligently leveraged and the analysis can be automated in real-time. Empirical results show high accuracy of vehicle detection and movement tracking module with the license plate detection module achieving a decent performance. However, improvements can be made by retraining its underlying license plate detection and Optical Character Recognition (OCR) engine with the dataset from the location which the system is to be implemented on.Bachelor of Engineering (Computer Science",,,Applications of artificial intelligence in real-time video analytics,,,core
295597015,2019-01-01T00:00:00,"This work aims to show the new approaches in embedded vision dedicated to object detection and tracking for drone visual control. Object/Pedestrian detection has been carried out through two methods: 1. Classical image processing approach through
improved Histogram Oriented Gradient (HOG) and Deformable Part Model (DPM) based detection and pattern recognition methods. In this step, we present our improved HOG/DPM approach allowing the detection of a target object in real time. The developed
approach allows us not only to detect the object (pedestrian) but also to estimates the distance between the target and the drone. 2. Object/Pedestrian detection-based Deep Learning approach. The target position estimation has been carried out within image
analysis. After this, the system sends instruction to the drone engine in order to correct its position and to track target. For this visual servoing, we have applied our improved HOG approach and implemented two kinds of PID controllers. The platform has been
validated under different scenarios by comparing measured data to ground truth data given by the drone GPS. Several tests which were ca1rried out at ESIGELEC car park and Rouen city center validate the developed platform",,'University of West Bohemia',Real Time Pedestrian and Object Detection and Tracking-based Deep Learning. Application to Drone Visual Tracking,10.24132/CSRN.2019.2902.2.5,https://core.ac.uk/download/295597015.pdf,core
200850496,2019-05-01T00:00:00,"Road transportation is the backbone of modern economies, albeit it annually costs     1.25     million deaths and trillions of dollars to the global economy, and damages public health and the environment. Deep learning is among the leading-edge methods used for transportation-related predictions, however, the existing works are in their infancy, and fall short in multiple respects, including the use of datasets with limited sizes and scopes, and insufficient depth of the deep learning studies. This paper provides a novel and comprehensive approach toward large-scale, faster, and real-time traffic prediction by bringing four complementary cutting-edge technologies together: big data, deep learning, in-memory computing, and Graphics Processing Units (GPUs). We trained deep networks using over 11 years of data provided by the California Department of Transportation (Caltrans), the largest dataset that has been used in deep learning studies. Several combinations of the input attributes of the data along with various network configurations of the deep learning models were investigated for training and prediction purposes. The use of the pre-trained model for real-time prediction was explored. The paper contributes novel deep learning models, algorithms, implementation, analytics methodology, and software tool for smart cities, big data, high performance computing, and their convergence","[{'title': 'Sensors', 'identifiers': ['issn:1424-8220', '1424-8220']}]",'MDPI AG',"Smarter Traffic Prediction Using Big Data, In-Memory Computing, Deep Learning and GPUs",10.3390/s19092206,,core
250308286,2019-01-01T00:00:00,"The advances in Internet of Things lead to an increased number of devices generating and streaming data. These devices can be useful data sources for Activity Recognition by using Machine Learning. However, as the set of available sensors may vary over time, e.g. due to mobility of the sensors and technical failures, the feature space might also change over time. Moreover, the labelled data necessary for the training is often costly to acquire. Active Learning is a type of Interactive Machine Learning where the model is given a budget for requesting labels from an oracle, and aims to maximize accuracy by careful selection of what data points to label. It is generally assumed that a query always gets a correct response, but in many real-world scenarios this is not a realistic assumption. In this work we investigate different Proactive Learning strategies, which explore the human factors of the oracle and aspects that might influence a user to provide or withhold labels. We implemented four proactive strategies and hybrid versions of them. They were evaluated on two datasets to examine how a more proactive, or reluctant, user affects performance. The results show that a more proactive user can improve the performance, especially when the user is influenced by the accuracy of earlier predictions. The experiments also highlight challenges related to evaluating performance when the set of classes is changing over time",,'American College of Medical Physics (ACMP)',Interactive Machine Learning for the Internet of Things : A Case Study on Activity Detection,,,core
322371710,2019-01-01T00:00:00,"Unmanned aerial vehicles (UAVs) have recently rapidly grown to facilitate a wide range of innovative applications that can fundamentally change the way cyber-physical systems (CPSs) are designed. CPSs are a modern generation of systems with synergic cooperation between computational and physical potentials that can interact with humans through several new mechanisms. The main advantages of using UAVs in CPS application is their exceptional features, including their mobility, dynamism, effortless deployment, adaptive altitude, agility, adjustability, and effective appraisal of real-world functions anytime and anywhere. Furthermore, from the technology perspective, UAVs are predicted to be a vital element of the development of advanced CPSs. Therefore, in this survey, we aim to pinpoint the most fundamental and important design challenges of multi-UAV systems for CPS applications. We highlight key and versatile aspects that span the coverage and tracking of targets and infrastructure objects, energy-efficient navigation, and image analysis using machine learning for fine-grained CPS applications. Key prototypes and testbeds are also investigated to show how these practical technologies can facilitate CPS applications. We present and propose state-of-the-art algorithms to address design challenges with both quantitative and qualitative methods and map these challenges with important CPS applications to draw insightful conclusions on the challenges of each application. Finally, we summarize potential new directions and ideas that could shape future research in these areas. - 1998-2012 IEEE.Manuscript received October 21, 2018; revised January 18, 2019, March 6, 2019, and May 17, 2019; accepted June 10, 2019. Date of publication June 20, 2019; date of current version November 25, 2019. This work was supported by Qatar University under Internal Grant QUCP-CENG-2018\2019-1. (Corresponding author: Amr Mohamed.) R. Shakeri and M. A. Al-Garadi were with the Department of Computer Science and Engineering, Qatar University, Doha, Qatar (e-mail: shakeri.reza.work@gmail.com; mohammed.g@qu.edu.qa)","[{'title': None, 'identifiers': ['3340-3385', 'issn:3340-3385']}]",'Institute of Electrical and Electronics Engineers (IEEE)',Design Challenges of Multi-UAV Systems in Cyber-Physical Applications: A Comprehensive Survey and Future Directions,10.1109/COMST.2019.2924143,,core
220110766,2019-01-01T00:00:00,"City-scale traffic prediction is an important task for public safety, traffic management, and deployment of intelligent transportation systems. Many approaches have been proposed to address traffic prediction task using machine learning techniques. In this paper, we present a framework to help on addressing the task at hand (density-, traffic flow- and origin-destination flow predictions) considering data type, features, deep learning techniques such as Convolutional Neural Networks (CNNs), e.g., Autoencoder, Recurrent Neural Networks (RNNs), e.g., Long Short Term Memory (LSTM), and Graph Convolutional Networks (GCNs). An autoencoder model is designed in this paper to predict traffic density based on historical data. Experiments on real-world taxi order data demonstrate the effectiveness of the model.QC 20190625</p",,"KTH, Geoinformatik",A Framework for Traffic Prediction Integrated with Deep Learning,,,core
305125115,2019-01-01T00:00:00,"Rapid urbanization and growth in urban populations have forced community-scale infrastructures (e.g., water, power and natural gas distribution systems, and transportation networks) to operate at their limits. Aging (and failing) infrastructures around the world are becoming increasingly vulnerable to operational degradation, extreme weather, natural disasters and cyber attacks/failures. These trends have wide-ranging socioeconomic consequences and raise public safety concerns. In this thesis, we introduce the notion of cyber-physical-human infrastructures (CPHIs) - smart community-scale infrastructures that bridge technologies with physical infrastructures and people. CPHIs are highly dynamic stochastic systems characterized by complex physical models that exhibit regionwide variability and uncertainty under disruptions. Failures in these distributed settings tend to be difficult to predict and estimate, and expensive to repair. Real-time fault identification is crucial to ensure continuity of lifeline services to customers at adequate levels of quality. Emerging smart community technologies have the potential to transform our failing infrastructures into robust and resilient future CPHIs.In this thesis, we explore one such CPHI - community water infrastructures. Current urban water infrastructures, that are decades (sometimes over a 100 years) old, encompass diverse geophysical regimes. Water stress concerns include the scarcity of supply and an increase in demand due to urbanization. Deterioration and damage to the infrastructure can disrupt water service; contamination events can result in economic and public health consequences. Unfortunately, little investment has gone into modernizing this key lifeline.To enhance the resilience of water systems, we propose an integrated middleware framework for quick and accurate identification of failures in complex water networks that exhibit uncertain behavior. Our proposed approach integrates IoT-based sensing, domain-specific models and simulations with machine learning methods to identify failures (pipe breaks, contamination events). The composition of techniques results in cost-accuracy-latency tradeoffs in fault identification, inherent in CPHIs due to the constraints imposed by cyber components, physical mechanics and human operators. Three key resilience problems are addressed in this thesis; isolation of multiple faults under a small number of failures, state estimation of the water systems under extreme events such as earthquakes, and contaminant source identification in water networks using human-in-the-loop based sensing. By working with real world water agencies (WSSC, DC and LADWP, LA), we first develop an understanding of operations of water CPHI systems. We design and implement a sensor-simulation-data integration framework AquaSCALE, and apply it to localize multiple concurrent pipe failures. We use a mixture of infrastructure measurements (i.e., historical and live water pressure/flow), environmental data (i.e., weather) and human inputs (i.e., twitter feeds), combined and enhanced with the domain model and supervised learning techniques to locate multiple failures at fine levels of granularity (individual pipeline level) with detection time reduced by orders of magnitude (from hours/days to minutes). We next consider the resilience of water infrastructures under extreme events (i.e., earthquakes) - the challenge here is the lack of apriori knowledge and the increased number and severity of damages to infrastructures. We present a graphical model based approach for efficient online state estimation, where the offline graph factorization partitions a given network into disjoint subgraphs, and the belief propagation based inference is executed on-the-fly in a distributed manner on those subgraphs. Our proposed approach can isolate 80% broken pipes and 99% loss-of-service to end-users during an earthquake.Finally, we address issues of water quality - today this is a human-in-the-loop process where operators need to gather water samples for lab tests. We incorporate the necessary abstractions with event processing methods into a workflow, which iteratively selects and refines the set of potential failure points via human-driven grab sampling. Our approach utilizes Hidden Markov Model based representations for event inference, along with reinforcement learning methods for further refining event locations and reducing the cost of human efforts.The proposed techniques are integrated into a middleware architecture, which enables components to communicate/collaborate with one another. We validate our approaches through a prototype implementation with multiple real-world water networks, supply-demand patterns from water utilities and policies set by the U.S. EPA. While our focus here is on water infrastructures in a community, the developed end-to-end solution is applicable to other infrastructures and community services which operate in disruptive and resource-constrained environments",,"eScholarship, University of California",Enabling Resilience in Cyber-Physical-Human Water Infrastructures,,https://core.ac.uk/download/305125115.pdf,core
392175949,2019-01-01T00:00:00,"Source at https://doi.org/10.17875/gup2019-1146. In recent years it has become common to speak about the republic of letters as a
network. But this was not always the case. Rather, it is the product of a specific set
of conditions: the confluence of readily available digitized documents, computational power to analyse that data, and a ready acceptance of the ‘network perspective’ in the popular consciousness. In our increasingly interconnected world we
encounter networks at every turn. The Internet, public transport networks, and
power grids make our everyday lives possible; our careers are dependent on networking; and social networking sites provide an online account of our professional
and personal capital. Networks have become a metaphor for connectedness, but
also a concrete framework for visualizing and measuring complex systems of
knowledge in the era of big data.

Although scholars working in the humanities might not realize it, the network
turn is due to the emergence of ‘network science’ as a field of interdisciplinary
study. In a series of key publications in the late 1990s and early 2000s, scholars
such as Albert-László Barabási, Reka Albert, Duncan J. Watts, and Steven Strogatz
showed that a huge variety of real-world networks – such as, for example, neural
networks, transport networks, biological regulatory networks, and social networks
– share an underlying order, follow simple laws, and therefore can be analysed using the same mathematical tools and models.1 These publications build on work
from various different disciplines, such as sociology, mathematics, and physics,
which stretches back some decades; but the emergence of network science as a
field in its own right was the product of certain conditions that did not exist before. Barabási and Albert explicitly cite the computerization of data acquisition as
essential to their research. In other words, what they needed was numerous examples of big network data, which they could compare, and the computational power
to analyse that data. In this field, thousands of publications every year describe the
development of new quantitative network analysis methods, and the analysis of
new types of network data.

The advent of large-scale digitization efforts in the humanities has given scholars unprecedented access to their research materials. Perhaps more importantly,
however, it has also put quantitative analysis methods within the reach of this
community. This is particularly true of large collections of metadata, as these represent structured information that is easier to abstract and quantify. Correspondence metadata, such as the data collected by the constituent members of the COST
Action Reassembling the Republic of Letters, lends itself particularly well to quantitative
analysis, as it is exactly the kind of data that network analysis was designed to study
– a set of well-defined relationships, namely letters sent and received, between
well-defined entities, namely individuals. As discussed in chapter II.4, some work
may be necessary to establish the identities of the individuals, but correspondence
is a social relationship that is particularly clearly defined, due to its physical manifestation in the form of the manuscript letter.

The value of the COST Action Reassembling the Republic of Letters additionally relies on a ‘network effect’ – a term employed in the context of modern technology
companies, which means that the value of a software product rises with the number of people using it, as such products typically facilitate interactions between
users in some way. By combining the metadata of a wide range of historical correspondence projects, and by making them compatible with each other, their combined value to the scholarly community is greatly increased. Consistent metadata
allows for much more wide-ranging searches across correspondence collections,
and the power of quantitative network analysis grows rapidly with the size and
scope of the network under study",,Göttingen University Press,Networking the Republic of Letters,,,core
237395454,2019-10-22T00:00:00,"This paper describes a prototype application that gathers textual data from the microblogging platform Twitter and carries out sentiment analysis to determine the polarity and subjectivity in relation to Brexit, the UK´ s exit from the European Union. The design, implementation and testing of the developed prototype will be discussed and an experimental evaluation of the product described. Specifically we provide insight into how events affect public opinion and how sentiment and public mood may be gathered from textual twitter data and propose this as an alternative to opinion polls. Traditional approaches to opinion polling face growing challenges in capturing the public mood. Small sample response and the time it takes to capture swings in public opinion make it difficult to provide accurate data for the political process. With over 500 million daily messages posted worldwide, the social media platform Twitter is an untapped resource of information. Users post short real time messages views and opinions on many topics, often signed with a ‘#hashtag’ to classify and document the subject matter in discussion. In this paper we apply automated sentiment analysis methods to tweets giving a measure of public support or hostility to a topic (‘Brexit’). The data were collected during several periods to determine changes in opinion. Using machine learning techniques we show that changes in opinion were also related to external events. Limitations of the method are that age, location and education are confounding factors where Twitter users over represent a young, urban public. However, the economic advantage of the method over real-time telephone polling are considerable",,International Conference on Information Society,An Application of Sentiment Analysis Techniques to Determine Public Opinion in Social Media,,https://core.ac.uk/download/237395454.pdf,core
144821760,04/05/2017,"Defra (the Department for the Environment, Food and Rural Affairs) and its network of arm’s length bodies – the Environment Agency, Natural England, the Marine Management Organisation, Kew Gardens, the Centre for Environment, Fisheries and Aquatic Science and the Animal and Plant Health Agency – produce large amounts of data. This has been instrumental in providing evidence to support the development and delivery of key policies. 
 
Historically, expertise has been embedded in subjectspecific areas, resulting in data existing in silos with varying degrees of accessibility for those within the department, wider government and wider society. In 2015, Defra’s Secretary of State declared that more than 8000 of Defra’s datasets would be made freely available for anyone to access, use and share. Doing so creates opportunities for everyone – not just those making their living in food, farming and the environment. Opening access to Defra’s data is intended to both provide opportunities to those who wish to use it to exploit its business potential, and to improve policy delivery by engaging a wider community in solving problems.  
 
The Department has been focused on making datasets available as open data; ensuring future data collection and publication approaches are open by default, taking a transparent approach from the outset. This will make it easier to work more collaboratively with other government bodies, external partners and the public.  
 
Alongside growing support for citizen science activities, new technologies are changing the way in which data flows. This presents opportunities for government as much as for others. Today’s smartphones, for example, have the computing power of the supercomputers of 30 years ago. When coupled with a growing range of accurate sensors, 
smartphones potentially allow the move away from environmental monitoring depending on sparse, fixed, high-accuracy, expensive monitoring stations, replacing them with mobile, cheap, lower accuracy but more densely populated networks, which could be carried on vehicles such as buses or taxis in addition to being supported by the capability of smartphones.   
 
There is already a move towards crowdsourced data for biodiversity measurement (supported by the National Biodiversity Network). Many people, connected together and using mobile phone technology, could revolutionise country-wide data collection that is sufficient to address policy needs. In future, machine learning may allow species recognition by the smartphone in real time when connected to cloud-based software. This kind of completely integrated software and hardware capability is broadly described by the potential of internet-of things-devices. These have the potential to revolutionise the farm: from making comparisons on productivity, to tracing the journey of food from field to fork. 
 
Defra is also explore the use of real-time data from earth observation and remote sensing to respond to incidents such as floods, invasive species, pollution and epidemics affecting livestock. Opportunities form these types of data sources are often balanced by challenges associated with transforming systems to be more data-driven and to ensure that, in areas that are business-critical, the data sources are assured.  
 
Complex decision-making is already supported by modelling that often requires data flows from multiple sources. Machine learning has the potential to increase the automation and speed of data assimilation in to models For example, this could mean using these technologies to inform policies that reward farmers for acting in ways that maximise positive national outcomes. In this particular instance and more widely, however, this comes at the risk of taking human subjectivity and its political and moral weight out of processes.  
 
In future data volumes will be a major challenge. While compression algorithms may improve it is not going to be possible to store all raw data. This is going to require real-time processing of data to provide intermediate outputs. Judgements will need to be made about what data needs to be kept and how to implement intelligent compression focused on the ultimate use.  
 
The main ethical challenges around data concern personal information, including observance of legislation on data protection and more recent legislation on the right to be forgotten. Defra is working both internally, among its network of agencies, and with its data users to ensure these rights are fully respected. However, data ethics extends beyond just consideration of personal data: 

 
 
there are sensitivities around some datasets for which Defra shares custody, and others where there are implications for civic, national or cyber security. Some data are also commercially sensitive. There may be additional sensitivities where data concerns reporting of environmental monitoring and enforcing regulation. 
 
Funding reforms have also been a major driver for change. Finding innovative ways to save money, but deliver a high quality service, remains a departmental mission – new data policies can help to transform some of the department’s ways of working. Adopting an ‘open by default’ approach has impacts beyond data, as working openly allows more efficient collaborative working that avoids duplication, saves money, and allows crosspollination of ideas across the Defra group. 
 
Defra is looking more broadly at how to obtain, analyse and manage data (tools, techniques and storage) to further drive innovation. This has been demonstrated by the recently established Earth Observation Centre of Excellence. This centre aims to ensure satellite data is used to its full potential in policy development and operations across Defra by 2020, via a collaborative group of Defra organisations and external partners. A recent success was during the severe weather in December 2015: the centre facilitated better involvement and communication across departments on flood estimates of non-urban areas using radar satellite system","Sharing more widely: data at the heart of evidence, policy and transformation at Defra",10.5281/zenodo.600753,https://core.ac.uk/download/pdf/144821760.pdf,,,core
156948188,2017-05-01T00:00:00,"International audienceIn this paper, the problem of proactive deployment of cache-enabled unmanned aerial vehicles (UAVs) for optimizing the quality-of-experience (QoE) of wireless devices in a cloud radio access network is studied. In the considered model, the network can leverage human-centric information, such as users' visited locations, requested contents, gender, job, and device type to predict the content request distribution, and mobility pattern of each user. Then, given these behavior predictions, the proposed approach seeks to find the user-UAV associations, the optimal UAVs' locations, and the contents to cache at UAVs. This problem is formulated as an optimization problem whose goal is to maximize the users' QoE while minimizing the transmit power used by the UAVs. To solve this problem, a novel algorithm based on the machine learning framework of conceptor-based echo state networks (ESNs) is proposed. Using ESNs, the network can effectively predict each user's content request distribution and its mobility pattern when limited information on the states of users and the network is available. Based on the predictions of the users' content request distribution and their mobility patterns, we derive the optimal locations of UAVs as well as the content to cache at UAVs. Simulation results using real pedestrian mobility patterns from BUPT and actual content transmission data from Youku show that the proposed algorithm can yield 33.3% and 59.6% gains, respectively, in terms of the average transmit power and the percentage of the users with satisfied QoE compared with a benchmark algorithm without caching and a benchmark solution without UAVs",Caching in the Sky: Proactive Deployment of Cache-Enabled Unmanned Aerial Vehicles for Optimized Quality-of-Experience,10.1109/JSAC.2017.2680898,,'Institute of Electrical and Electronics Engineers (IEEE)',,core
265141728,2017-08-20T00:00:00,"International audienceThis paper describes the evolution of our research from video analytics to a global security system with focus on the video surveillance component. Indeed video surveillance has evolved from a commodity security tool up to the most efficient way of tracking perpetrators when terrorism hits our modern urban centers. As number of cameras soars, one could expect the system to leverage the huge amount of data carried through the video streams to provide fast access to video evidences, actionable intelligence for monitoring real-time events and enabling predictive capacities to assist operators in their surveillance tasks. This research explores a hybrid platform for video intelligence capture, automated data extraction, supervised Machine Learning for intelligently assisted urban video surveillance; Extension to other components of a global security system are discussed. Applying Knowledge Management principles in this research helps with deep problem understanding and facilitates the implementation of efficient information and experience sharing decision support systems providing assistance to people on the field as well as in operations centres. The originality of this work is also the creation of ""common"" human-machine and machine to machine language and a security ontology",VIDEO INTELLIGENCE AS A COMPONENT OF A GLOBAL SECURITY SYSTEM,,https://core.ac.uk/download/265141728.pdf,HAL CCSD,,core
235261716,2017-01-01T00:00:00,"This paper presents the development and implementation of a theoretical mathematical-statistical framework for sequential updating of the grade control model, based on a support vector machine learning algorithm. Utilising the Zambujal orebody within the Neves-Corvo Cu deposit in Portugal, parameters that can be measured in real time, used in visualisation, modelled for resource estimation, and used for process control visualisation and optimisation are considered.
The methodology broadly comprises of three steps. Firstly, the provided dataset is used to develop a virtual asset model (VAM) representing the true 3D grade distribution in order to simulate the mining method. Then ore quality parameters are established simulating real time monitoring sensor installation at: (a) stope development and rock face monitoring (face imaging and drillholes); and (b) transport monitoring (muck pile, LHD/scooptram). Next, the acquired data was assimilated into the models as part of the sequential model update.
Two different mining methods and the monitoring information that can be acquired during the ore extraction are analysed: (a) drift and fill mining and (b) bench and fill mining, which are widely implemented at the Neves-Corvo mine. Selected study zones were chosen such as to contrast mining through the high/low grade zones with different degrees of heterogeneity, which demonstrate the performance of resource estimation and classification models developed in heterogeneous mining stopes.
The grade accuracy and error in the resource model, and high/low grade ore classification accuracy and error are evaluated as performance metrics for the proposed methods.
In drift and fill mining, drillhole and face sampling data collection was simulated in a real-time manner and fed into the support vector machine (SVM) regressor to update the resource estimation model in both a high grade and low grade drift scenarios. In each scenario, six drift and fill mining steps were simulated sequentially and the posterior resource models, after integrating real time mining data, have shown significant improvement of bias correction in both updating planned resources and reconciling extracted ore.
In bench and fill mining, grade classification based on random sampling data from muck pile was demonstrated, considering scoop by scoop derived monitoring data. Three different classifiers (mean, median, and Bayesian) were tested and shown very good performance. In the case study presented here, a sequence of 15 blasting steps was simulated with each step requiring 112 scooping operations to transport the blasted ore. Using the real time monitored information, it was shown that at each blasting step over 85% of the scoops can be labelled correctly using the proposed methods and with an accuracy of over 95%",Development of support vector machine learning algorithm for real time update of resource estimation and grade classification,,,TU Bergakademie Freiberg,,core
200965017,2018-01-01T00:00:00,"We present an open-source accessory for the NAO robot, which enables to test computationally demanding algorithms in an external platform while preserving robot’s autonomy and mobility. The platform has the form of a backpack, which can be 3D printed and replicated, and holds an ODROID XU4 board to process algorithms externally with ROS compatibility. We provide also a software bridge between the B-Human’s framework and ROS to have access to the robot’s sensors close to real-time. We tested the platform in several robotics applications such as data logging, visual SLAM, and robot vision with deep learning techniques. The CAD model, hardware specifications and software are available online for the benefit of the community",The NAO Backpack: An open-hardware add-on for fast software development with the NAO robot,10.1007/978-3-030-00308-1_25,,'Springer Science and Business Media LLC',,core
224458752,2017-01-01T00:00:00,"The ongoing revolution in the mobile networked system (consisting of mobile clients, and the 4G/5G network infrastructure) is reaching a critical stage. On one hand, it has been operational for years, resulting in billions of users and tens of petabytes per month for mobile data traffic. On the other hand, users today regularly complain about network failures, unsatisfactory performance, and security threats with alarming frequency. The impact of such problematic issues may further exaggerate, as the emerging next-generation applications (such as interactive virtual/augmented reality, self-driving cars, telemedicine, and drone-based delivery, etc.) pose more stringent requirements. The convention wisdom seems to attribute most of these issues to the poor wireless channel quality, thus motivating a range of new access technologies to mitigate them.This dissertation demonstrates that, besides the wireless link, the architectural limitation should share equal responsibility. The fundamental problem is that, the entire networked system does not possess sufficient intelligence on what problems may arise, why such issues occur under the given scenarios, and how to react. The mobile clients lack runtime information on the underlying “black-box” network operations, whereas the infrastructure suffers from the complex interplays of protocol functions among distributed nodes. Both again are rooted in the basic design tenet of “smart core, dumb terminal” adopted by the current mobile networked systems. Our study has uncovered a range of real issues incurred by the architectural limitations (rather than the poor wireless link), including handoff instability, suboptimal roaming, and long data access latency perceived by both the client and the infrastructure. Most of the current solutions are piecemeal efforts without looking into the core system architecture designs.This dissertation thus explores a new dimension in mobile networked systems. We seek to augment the system intelligence on verifying the baseline designs a priori, detecting problematic scenarios based on runtime information feedback, understanding their root causes, and taking smart actions. To this end, we propose a new knowledge plane for the system software stack, which offers a novel primitive for the mobile networked system that helps to unleash its architectural limitations. The knowledge plane follows the “smart client, simple infrastructure” principle but leaps one step forward. On one hand, it explores the “data-driven approach to smart clients” by exposing rich network information at runtime and leveraging some recent results on data sciences and machine learning. On the other hand, it seeks to devise simple, yet verifiable protocol solutions with provable properties by applying techniques on network verification and distributed computing. In this way, we enable a simpler and more open networked system for the mobile devices.The main contribution of this dissertation is the design, instantiation, analysis, and validation of the knowledge plane and its benefits on performance and reliability. The concrete results cover on both the client device and the network infrastructure. On the mobile client side, we construct the knowledge plane using the “bottom-up” data-driven system design, by enabling the client-side access to rich network data at runtime. It thus opens access to the typically “closed” network operations without infrastructure changes. We build MobileInsight, the first tool that opens up the runtime, fine-grained cellular network data and offers protocol analytics (using AI-based inference) on commodity phones; it has been used by 247 universities and companies during its first-year release so far. As a showcase application, we develop iCellular, an enhanced client-centric, multi-carrier roaming service. By leveraging low-level network data analytics, iCellular boosts the device with up to 3.74ï¿½ throughput improvement, 1.9ï¿½ latency reduction over the state-of-the-art solution from Google Project Fi.On the infrastructure side, we enable the knowledge plane with the “top-down” approach. We treat the infrastructure as a distributed system, define the structural properties (stability, availability, consistency, etc.) that capture the high- level demands, apply verification and distributed computing techniques to reason about them, and enforce provable reliability and efficiency. At the management plane, we conduct the first study on the stability of the distributed mobility management. We show that, policy/configuration conflicts exist in reality, and force the device to oscillate among base stations permanently. We prove the necessary/sufficient conditions for the stability, and create MMDiag that detects and resolves the policy/configuration conflicts. At the control plane, we build DPCM, the first paradigm in the mobile network that parallelizes the control plane procedures for low-latency data access. It is inspired by the generalized CAP theorem, and leverages the device-side state replica achieve 2.1x–11.5x latency reduction on average in different scenarios.These results show that, augmenting the future mobile networked system with intelligence can benefit both the client and the infrastructure. The knowledge plane presented in this dissertation provides a viable solution to move one step closer toward a future mobile networked system (5G and beyond) with ""Intelligence-as-a-Service""",Augmenting Intelligence in Mobile Networked System,,,"eScholarship, University of California",,core
149302104,2018-01-12T00:00:00Z,"Neuromorphic
or “brain-like” computation is a leading
candidate for efficient, fault-tolerant processing of large-scale
data as well as real-time sensing and transduction of complex multivariate
systems and networks such as self-driving vehicles or Internet of
Things applications. In biology, the synapse serves as an active memory
unit in the neural system and is the component responsible for learning
and memory. Electronically emulating this element <i>via</i> a compact, scalable technology which can be integrated in a three-dimensional
(3-D) architecture is critical for future implementations of neuromorphic
processors. However, present day 3-D transistor implementations of
synapses are typically based on low-mobility semiconductor channels
or technologies that are not scalable. Here, we demonstrate a crystalline
indium phosphide (InP)-based artificial synapse for spiking neural
networks that exhibits elasticity, short-term plasticity, long-term
plasticity, metaplasticity, and spike timing-dependent plasticity,
emulating the critical behaviors exhibited by biological synapses.
Critically, we show that this crystalline InP device can be directly
integrated <i>via</i> back-end processing on a Si wafer
using a SiO<sub>2</sub> buffer <i>without the need for a crystalline
seed</i>, enabling neuromorphic devices that can be implemented
in a scalable and 3-D architecture. Specifically, the device is a
crystalline InP channel field-effect transistor that interacts with
neuron spikes by modification of the population of filled traps in
the MOS structure itself. Unlike other transistor-based implementations,
we show that it is possible to mimic these biological functions without
the use of external factors (<i>e</i>.<i>g</i>., surface adsorption of gas molecules) and without the need for
the high electric fields necessary for traditional flash-based implementations.
Finally, when exposed to neuronal spikes with a waveform similar to
that observed in the brain, these devices exhibit the ability to learn
without the need for any external potentiating/depressing circuits,
mimicking the biological process of Hebbian learning","Mimicking
Biological Synaptic Functionality with an
Indium Phosphide Synaptic Device on Silicon for Scalable Neuromorphic
Computing",10.1021/acsnano.7b08272.s001,,,,core
88219665,2017-08-01T00:00:00Z,"Artificial Neural Network (ANN) is a valuable and well-established inversion technique for the estimation of geophysical parameters from satellite images. After training, ANNs are able to generate very fast products for several types of applications. Satellite remote sensing is an efficient way to detect and map strong earthquake damage for contributing to post-disaster activities during emergency phases. This work aims at presenting an application of the ANN inversion technique addressed to the evaluation of building collapse ratio (CR), defined as the number of collapsed buildings with respect to the total number of buildings in a city block, by employing optical and SAR satellite data. This is done in order to directly relate changes in images with damage that has occurred during strong earthquakes. Furthermore, once they have been trained, neural networks can be used rapidly at application stage. The goal was to obtain a general tool suitable for re-use in different scenarios. An ANN has been implemented in order to emulate a regression model and to estimate the CR as a continuous function. The adopted ANN has been trained using some features obtained from optical and Synthetic Aperture Radar (SAR) images, as inputs, and the corresponding values of collapse ratio obtained from the survey of the 2010 M7 Haiti Earthquake, i.e., as target output. As regards the optical data, we selected three change parameters: the Normalized Difference Index (NDI), the Kullback–Leibler divergence (KLD), and Mutual Information (MI). Concerning the SAR images, the Intensity Correlation Difference (ICD) and the KLD parameters have been considered. Exploiting an object-oriented approach, a segmentation of the study area into several regions has been performed. In particular, damage maps have been generated by considering a set of polygons (in which satellite parameters have been calculated) extracted from the open source Open Street Map (OSM) geo-database. The trained ANN has been proposed for the M6.0 Amatrice earthquake that occurred on 24 August 2016, in central Italy, by using the features extracted from Sentinel-2 and COSMO-SkyMed images as input. The results show that the ANN is able to retrieve a building collapse ratio with good accuracy. In particular, the fusion approach modelled the collapse ratio characterized by high values of CR (more than 0.5) over the historical center that agrees with observed damages. Since the technique is independent from different typologies of input data (i.e., for radiometric or spatial resolution characteristics), the study demonstrated the strength of the proposed approach for estimating damaged areas and its importance in near real time monitoring activities, owing to its fast application",A New Damage Assessment Method by Means of Neural Network and Multi-Sensor Satellite Data,10.3390/app7080781,,MDPI AG,"[{'title': None, 'identifiers': ['2076-3417', 'issn:2076-3417']}]",core
84913689,"June 5, 2017","In recent years, a surge of interest in ""flying cars"" for city commutes has led to rapid development of new technologies to help make them and similar on-demand mobility platforms a reality. To this end, this paper provides analyses of the stakeholders involved, their proposed operational concepts, and the hazards and regulations that must be addressed. Three system architectures emerged from the analyses, ranging from conventional air taxi to revolutionary fully autonomous aircraft operations, each with vehicle safety functions allocated differently between humans and machines. Advancements for enabling technologies such as distributed electric propulsion and artificial intelligence have had major investments and initial experimental success, but may be some years away from being deployed for on-demand passenger air transportation at scale",Exploring Concepts of Operations for On-Demand Passenger Air Transportation,,https://core.ac.uk/download/pdf/84913689.pdf,,,core
84092094,2017-06-20T00:00:00,"We present an open-source accessory for the NAO robot, which enables to test
computationally demanding algorithms in an external platform while preserving
robot's autonomy and mobility. The platform has the form of a backpack, which
can be 3D printed and replicated, and holds an ODROID XU4 board to process
algorithms externally with ROS compatibility. We provide also a software bridge
between the B-Human's framework and ROS to have access to the robot's sensors
close to real-time. We tested the platform in several robotics applications
such as data logging, visual SLAM, and robot vision with deep learning
techniques. The CAD model, hardware specifications and software are available
online for the benefit of the community:
https://github.com/uchile-robotics/nao-backpackComment: Accepted in the RoboCup Symposium 2017. Final version will be
  published at Springe","The NAO Backpack: An Open-hardware Add-on for Fast Software Development
  with the NAO Robot",10.1007/978-3-030-00308-1,http://arxiv.org/abs/1706.06696,'Springer Science and Business Media LLC',,core
286807617,13/12/2018,,"Research presented in this report is based on:1. Problem Submission: Firms submitted corporate challenges relating to digital business models across several industries. Problems were screened and selected.2. Problem Framing: Professor Darwin conducted individual sessions with individual firms to solicit input from both open innovation researchers and practitioners.3. Problem Solving: Input, feedback, and recommendations provided by a community of academic experts and open innovation practitioners across industries who worked deliberated in groups of eight during a one-hour session per challenge.Challenge #1: WIPRO ...................................................................................................................................... 3“Wipro is getting ready to deploy its newly developed digital technology (AR/VR/AI) “do-your-own repairs tool” that will help people repair their white goods in their homes. With what business model could the company deploy this new technology?”Challenge #2: DAIMLER AG ............................................................................................................................. 8As Daimler enters new markets, such as autonomous vehicles, how can a mobility services firm accelerate internal innovation against uncharted territories in the uncertain times of digital transformation?Challenge #3: KANEKA .................................................................................................................................. 12Kaneka wants to improve innovation internally through the following lens: how can Kaneka accelerate internal innovation utilizing a two-sided digital ideation/challenge platform through which it can address company’s internal and external challenges?Challenge #4: ALLERGAN ............................................................................................................................... 16What innovative digitally driven design would you suggest for supporting busy physicians and/or patients, to get them relevant, accurate, reliable and real-time information and analysis quickly?Challenge #5: APPLIED MATERIAL ................................................................................................................. 22How AMAT can leverage its materials engineering capabilities to enter new markets with platform extensions powered by collaborations with external ecosystem partners?Challenge #6: XIAOMI ................................................................................................................................... 26Xiaomi offers High-Value/Low-Cost/Low-Margin products to all customer segments in emerging markets. This demands severe cost curtailment strategies in manufacturing, operation, advertisement, sales, distribution and servicing of its products. Xiaomi cannot deliver this value alone without an ecosystem to sustain and scale the business. How can the government, corporations and other institutions help create a win-win for all? In addition, rural communities lack infrastructure (reliable connectivity, power, healthcare, clean water, accessible roads). What must Xiaomi do to serve and expand the market when this infrastructure is lacking?",,,HAL CCSD,,core
328257691,2018-12-04T00:00:00,"Cultural venues, such as libraries, theatres, cinemas and galleries,contribute to a city’s tourism and economy, and enrich the culturallife of the local residents. In this paper, we propose a novelapproach to automatic site selection of cultural venues in an urbanarea, which requires less expertise in urban planning. The two-stageapproach consists of a learning stage for predicting zones as a priorconstraint, and an optimisation stage for determining the numberof cultural venues and their exact locations according to multiplecriteria. Given an input set of urban data, our approach generatesan optimal configuration of two-dimensional locations for culturalvenues that complies with land use policies and provides easy accessfor the public. We implemented the approach using reliablemethods of deep learning and stochastic optimisation, and the resultsdemonstrate the approach’s effectiveness by a comparison totheir real-world counterparts",SIGGRAPH Asia 2018 Technical Briefs,10.1145/3283254.3283257,http://handle.unsw.edu.au/1959.4/unsworks_52727,'Association for Computing Machinery (ACM)',,core
186299583,2018-12-04T00:00:00,"Machine learning has become an increasingly powerful tool for solving complex
problems, and its application in public health has been underutilized. The
objective of this study is to test the efficacy of a machine-learned model of
foodborne illness detection in a real-world setting. To this end, we built
FINDER, a machine-learned model for real-time detection of foodborne illness
using anonymous and aggregated web search and location data. We computed the
fraction of people who visited a particular restaurant and later searched for
terms indicative of food poisoning to identify potentially unsafe restaurants.
We used this information to focus restaurant inspections in two cities and
demonstrated that FINDER improves the accuracy of health inspections;
restaurants identified by FINDER are 3.1 times as likely to be deemed unsafe
during the inspection as restaurants identified by existing methods.
Additionally, FINDER enables us to ascertain previously intractable
epidemiological information, for example, in 38% of cases the restaurant
potentially causing food poisoning was not the last one visited, which may
explain the lower precision of complaint-based inspections. We found that
FINDER is able to reliably identify restaurants that have an active lapse in
food safety, allowing for implementation of corrective actions that would
prevent the potential spread of foodborne illness","Machine-learned epidemiology: real-time detection of foodborne illness
  at scale",10.1038/s41746-018-0045-1,http://arxiv.org/abs/1812.01813,'Springer Science and Business Media LLC',,core
186296062,2018-11-27T00:00:00,"This work examines the implications of uncoupled intersections with local
real-world topology and sensor setup on traffic light control approaches.
Control approaches are evaluated with respect to: Traffic flow, fuel
consumption and noise emission at intersections.
  The real-world road network of Friedrichshafen is depicted, preprocessed and
the present traffic light controlled intersections are modeled with respect to
state space and action space.
  Different strategies, containing fixed-time, gap-based and time-based control
approaches as well as our deep reinforcement learning based control approach,
are implemented and assessed. Our novel DRL approach allows for modeling the
TLC action space, with respect to phase selection as well as selection of
transition timings. It was found that real-world topologies, and thus
irregularly arranged intersections have an influence on the performance of
traffic light control approaches. This is even to be observed within the same
intersection types (n-arm, m-phases). Moreover we could show, that these
influences can be efficiently dealt with by our deep reinforcement learning
based control approach.Comment: 32nd Conference on Neural Information Processing Systems, within
  Workshop on Machine Learning for Intelligent Transportation System","Distributed traffic light control at uncoupled intersections with
  real-world topology by deep reinforcement learning",,http://arxiv.org/abs/1811.11233,,,core
87904165,2017-06-08T00:00:00,"International audienceSmartphone usage while driving is a dangerous activity directly linked to 17% of deadly accidents in France in 2015. While it can potentially impact every road user, professional drivers are perhaps the most affected collective as they spend a high amount of time in their vehicles on a daily basis, whether they deliver goods or transport people. Detecting smartphone usage is interesting for many reasons, from legal controls to forensics investigation in accidents, without forgetting the possibility of alert the driver of the danger he is engaging in. In this study we evaluate the pertinence of using driver head rotation movements to automatically predict smartphone usage at the wheel. In order to fit the system to the particularities of professional drivers, a naturalistic driving study have been conducted. 15 operational vehicles from two private French transport companies were equipped with near-infrared cameras, embedded real-time computer vision and machine learning techniques. Common behavioural patterns have been revealed. Head rebounds are a constant (letting the driver switch gaze between the road and the device). Additionally, the duration a driver spends looking down from a reference neutral direction can be used as a reliable parameter to predict smartphone usage. On the other hand some divergences between van and bus drivers head movements have been noticed. A real-time smartphone usage detection system has been implemented from the results of this study. Preliminary results are encouraging and a prototype of the system is already being tested by professional drivers in a naturalistic context",Driver Head Movements While Using a Smartphone in a Naturalistic Context,,,HAL CCSD,,core
219456675,2017-01-01T00:00:00,"The development of technologies for autonomous vehicle (AV) have seen rapid achievement in the recent years. Commercial carmakers are actively embedding this system in their production and are undergoing tremendous testing in the real world traffic environment. It is one of today’s most challenging topics in the intelligent transportation system (ITS) field in term of reliability as well as accelerating the world’s transition to a sustainable future. The utilization of current sensor technology however indicates some drawbacks where the complexity is high and the cost is extremely huge. This paper reviews the recent sensor technologies and their contributions in becoming part of the autonomous self-driving vehicle system. The ultimate focus is toward reducing the sensor count to just a single camera based on the single modality model. The capability of the sensor to detect and recognize on-the-road obstacles such as overtaking vehicle, pedestrians, signboards, bicycle, road lane marker and road curvature will be discussed. Different feature extraction approach will be reviewed further with the selection of the recent Artificial Intelligent (AI) methods that are being implemented. At the end of this review, the optimal techniques of processing information from single camera system will be discussed and summarized",Single camera object detection for self-driving vehicle: a review,,https://core.ac.uk/download/219456675.pdf,Society of Automotive Engineers Malaysia,,core
211826997,2018-01-01T00:00:00,"Sprowitz AT, Tuleu A, Ajallooeian M, et al. Oncilla Robot: A Versatile Open-Source Quadruped Research Robot With Compliant Pantograph Legs. FRONTIERS IN ROBOTICS AND AI. 2018;5: 18.We present Oncilla robot, a novel mobile, quadruped legged locomotion machine. This large-cat sized, 5.1 kg robot is one of a kind of a recent, bioinspired legged robot class designed with the capability of model-free locomotion control. Animal legged locomotion in rough terrain is clearly shaped by sensor feedback systems. Results with Oncilla robot show that agile and versatile locomotion is possible without sensory signals to some extend, and tracking becomes robust when feedback control is added (Ajallooeian, 2015). By incorporating mechanical and control blueprints inspired from animals, and by observing the resulting robot locomotion characteristics, we aim to understand the contribution of individual components. Legged robots have a wide mechanical and control design parameter space, and a unique potential as research tools to investigate principles of biomechanics and legged locomotion control. But the hardware and controller design can be a steep initial hurdle for academic research. To facilitate the easy start and development of legged robots, Oncilla-robot's blueprints are available through open-source. The robot's locomotion capabilities are shown in several scenarios. Specifically, its spring-loaded pantographic leg design compensates for overdetermined body and leg postures, i.e., during turning maneuvers, locomotion outdoors, or while going up and down slopes. The robot's active degree of freedom allow tight and swift direction changes, and turns on the spot. Presented hardware experiments are conducted in an open-loop manner, with little control and computational effort. For more versatile locomotion control, Oncilla-robot can sense leg joint rotations, and leg-trunk forces. Additional sensors can be included for feedback control with an open communication protocol interface. The robot's customized actuators are designed for robust actuation, and efficient locomotion. It trots with a cost of transport of 3.2 J/(Nm),at a speed of 0.63 m s(-1) (Froude number 0.25). The robot trots inclined slopes up to 10 degrees, at 0.25 m s(-1). The multi-body Webots model of Oncilla robot, and Oncilla robot's extensive software architecture enables users to design and test scenarios in simulation. Controllers can directly be transferred to the real robot. Oncilla robot's blueprints are open-source published (hardware GLP v3, software LGPL v3)",Oncilla Robot: A Versatile Open-Source Quadruped Research Robot With Compliant Pantograph Legs,10.3389/frobt.2018.00067,,'Frontiers Media SA',,core
154476750,2017-06-22T07:00:00,"Despite the impressive advancements in people detection and tracking, safety is still a key barrier to the deployment of autonomous vehicles in urban environments [1]. For example, in non-autonomous technology, there is an implicit communication between the people crossing the street and the driver to make sure they have communicated their intent to the driver. Therefore, it is crucial for the autonomous car to infer the future intent of the pedestrian quickly. We believe that human body orientation with respect to the camera can help the intelligent unit of the car to anticipate the future movement of the pedestrians. To further improve the safety of pedestrians, it is important to recognize whether they are distracted, carrying a baby, or pushing a shopping cart. Therefore, estimating the fine- grained 3D pose, i.e. (x,y,z)-coordinates of the body joints provides additional information for decision-making units of driverless cars.
In this dissertation, we have proposed a deep learning-based solution to classify the categorized body orientation in still images. We have also proposed an efficient framework based on our body orientation classification scheme to estimate human 3D pose in monocular RGB images.
Furthermore, we have utilized the dynamics of human motion to infer the body orientation in image sequences. To achieve this, we employ a recurrent neural network model to estimate continuous body orientation from the trajectories of body joints in the image plane.
The proposed body orientation and 3D pose estimation framework are tested on the largest 3D pose estimation benchmark, Human3.6m (both in still images and video), and we have proved the efficacy of our approach by benchmarking it against the state-of-the-art approaches.
Another critical feature of self-driving car is to avoid an obstacle. In the current prototypes the car either stops or changes its lane even if it causes other traffic disruptions. However, there are situations when it is preferable to collide with the object, for example a foam box, rather than take an action that could result in a much more serious accident than collision with the object. In this dissertation, for the first time, we have presented a novel method to discriminate between physical properties of these types of objects such as bounciness, elasticity, etc. based on their motion characteristics . The proposed algorithm is tested on synthetic data, and, as a proof of concept, its effectiveness on a limited set of real-world data is demonstrated",Estimation of Human Poses Categories and Physical Object Properties from Motion Trajectories,,https://core.ac.uk/download/154476750.pdf,Scholar Commons,,core
151109022,2018-01-01T00:00:00Z,"Traffic routing is a central challenge in the context of urban areas, with a direct impact on personal mobility, traffic congestion, and air pollution. In the last decade, the possibilities for traffic flow control have improved together with the corresponding management systems. However, the lack of real-time traffic flow information with a city-wide coverage is a major limiting factor for an optimum operation. Smart City concepts seek to tackle these challenges in the future by combining sensing, communications, distributed information, and actuation. This paper presents an integrated approach that combines smart street lamps with traffic sensing technology. More specifically, infrastructure-based ultrasonic sensors, which are deployed together with a street light system, are used for multilane traffic participant detection and classification. Application of these sensors in time-varying reflective environments posed an unresolved problem for many ultrasonic sensing solutions in the past and therefore widely limited the dissemination of this technology. We present a solution using an algorithmic approach that combines statistical standardization with clustering techniques from the field of unsupervised learning. By using a multilevel communication concept, centralized and decentralized traffic information fusion is possible. The evaluation is based on results from automotive test track measurements and several European real-world installations",Density-Based Statistical Clustering: Enabling Sidefire Ultrasonic Traffic Sensing in Smart Cities,10.1155/2018/9317291,,Hindawi-Wiley,"[{'title': None, 'identifiers': ['issn:2042-3195', '2042-3195', 'issn:0197-6729', '0197-6729']}]",core
160784121,2018-08-16T00:00:00,"The Secure Sockets Layer (SSL) and Transport Layer Security (TLS) protocols
are the foundation of network security. The certificate verification in SSL/TLS
implementations is vital and may become the weak link in the whole network
ecosystem. In previous works, some research focused on the automated testing of
certificate verification, and the main approaches rely on generating massive
certificates through randomly combining parts of seed certificates for fuzzing.
Although the generated certificates could meet the semantic constraints, the
cost is quite heavy, and the performance is limited due to the randomness. To
fill this gap, in this paper, we propose DRLGENCERT, the first framework of
applying deep reinforcement learning to the automated testing of certificate
verification in SSL/TLS implementations. DRLGENCERT accepts ordinary
certificates as input and outputs newly generated certificates which could
trigger discrepancies with high efficiency. Benefited by the deep reinforcement
learning, when generating certificates, our framework could choose the best
next action according to the result of a previous modification, instead of
simple random combinations. At the same time, we developed a set of new
techniques to support the overall design, like new feature extraction method
for X.509 certificates, fine-grained differential testing, and so forth. Also,
we implemented a prototype of DRLGENCERT and carried out a series of real-world
experiments. The results show DRLGENCERT is quite efficient, and we obtained
84,661 discrepancy-triggering certificates from 181,900 certificate seeds, say
around 46.5% effectiveness. Also, we evaluated six popular SSL/TLS
implementations, including GnuTLS, MatrixSSL, MbedTLS, NSS, OpenSSL, and
wolfSSL. DRLGENCERT successfully discovered 23 serious certificate verification
flaws, and most of them were previously unknown","DRLGENCERT: Deep Learning-based Automated Testing of Certificate
  Verification in SSL/TLS Implementations",,http://arxiv.org/abs/1808.05444,,,core
296877876,2018-08-26T17:57:25,"Orientadores: Janito Vaqueiro Ferreira, Alessandro Corrêa VictorinoTese (doutorado) - Universidade Estadual de Campinas, Faculdade de Engenharia MecânicaResumo: O desenvolvimento de veículos autônomos capazes de se locomover em ruas urbanas pode proporcionar importantes benefícios na redução de acidentes, no aumentando da qualidade de vida e também na redução de custos. Veículos inteligentes, por exemplo, frequentemente baseiam suas decisões em observações obtidas a partir de vários sensores tais como LIDAR, GPS e câmeras. Atualmente, sensores de câmera têm recebido grande atenção pelo motivo de que eles são de baixo custo, fáceis de utilizar e fornecem dados com rica informação. Ambientes urbanos representam um interessante mas também desafiador cenário neste contexto, onde o traçado das ruas podem ser muito complexos, a presença de objetos tais como árvores, bicicletas, veículos podem gerar observações parciais e também estas observações são muitas vezes ruidosas ou ainda perdidas devido a completas oclusões. Portanto, o processo de percepção por natureza precisa ser capaz de lidar com a incerteza no conhecimento do mundo em torno do veículo. Nesta tese, este problema de percepção é analisado para a condução nos ambientes urbanos associado com a capacidade de realizar um deslocamento seguro baseado no processo de tomada de decisão em navegação autônoma. Projeta-se um sistema de percepção que permita veículos robóticos a trafegar autonomamente nas ruas, sem a necessidade de adaptar a infraestrutura, sem o conhecimento prévio do ambiente e considerando a presença de objetos dinâmicos tais como veículos. Propõe-se um novo método baseado em aprendizado de máquina para extrair o contexto semântico usando um par de imagens estéreo, a qual é vinculada a uma grade de ocupação evidencial que modela as incertezas de um ambiente urbano desconhecido, aplicando a teoria de Dempster-Shafer. Para a tomada de decisão no planejamento do caminho, aplica-se a abordagem dos tentáculos virtuais para gerar possíveis caminhos a partir do centro de referencia do veículo e com base nisto, duas novas estratégias são propostas. Em primeiro, uma nova estratégia para escolher o caminho correto para melhor evitar obstáculos e seguir a tarefa local no contexto da navegação hibrida e, em segundo, um novo controle de malha fechada baseado na odometria visual e o tentáculo virtual é modelado para execução do seguimento de caminho. Finalmente, um completo sistema automotivo integrando os modelos de percepção, planejamento e controle são implementados e validados experimentalmente em condições reais usando um veículo autônomo experimental, onde os resultados mostram que a abordagem desenvolvida realiza com sucesso uma segura navegação local com base em sensores de câmeraAbstract: The development of autonomous vehicles capable of getting around on urban roads can provide important benefits in reducing accidents, in increasing life comfort and also in providing cost savings. Intelligent vehicles for example often base their decisions on observations obtained from various sensors such as LIDAR, GPS and Cameras. Actually, camera sensors have been receiving large attention due to they are cheap, easy to employ and provide rich data information. Inner-city environments represent an interesting but also very challenging scenario in this context, where the road layout may be very complex, the presence of objects such as trees, bicycles, cars might generate partial observations and also these observations are often noisy or even missing due to heavy occlusions. Thus, perception process by nature needs to be able to deal with uncertainties in the knowledge of the world around the car. While highway navigation and autonomous driving using a prior knowledge of the environment have been demonstrating successfully, understanding and navigating general inner-city scenarios with little prior knowledge remains an unsolved problem. In this thesis, this perception problem is analyzed for driving in the inner-city environments associated with the capacity to perform a safe displacement based on decision-making process in autonomous navigation. It is designed a perception system that allows robotic-cars to drive autonomously on roads, without the need to adapt the infrastructure, without requiring previous knowledge of the environment and considering the presence of dynamic objects such as cars. It is proposed a novel method based on machine learning to extract the semantic context using a pair of stereo images, which is merged in an evidential grid to model the uncertainties of an unknown urban environment, applying the Dempster-Shafer theory. To make decisions in path-planning, it is applied the virtual tentacle approach to generate possible paths starting from ego-referenced car and based on it, two news strategies are proposed. First one, a new strategy to select the correct path to better avoid obstacles and to follow the local task in the context of hybrid navigation, and second, a new closed loop control based on visual odometry and virtual tentacle is modeled to path-following execution. Finally, a complete automotive system integrating the perception, path-planning and control modules are implemented and experimentally validated in real situations using an experimental autonomous car, where the results show that the developed approach successfully performs a safe local navigation based on camera sensorsDoutoradoMecanica dos Sólidos e Projeto MecanicoDoutor em Engenharia Mecânic",Percepção do ambiente urbano e navegação usando visão robótica : concepção e implementação aplicado à veículo autônomo,,https://core.ac.uk/download/296877876.pdf,[s.n.],,core
427372947,2017-10-12T00:00:00,"Rapid developments in hardware, software, and communication technologies

have allowed the emergence of Internet-connected sensory devices that provide

observation and data measurement from the physical world. By 2020, it is

estimated that the total number of Internet-connected devices being used will

be between 25-50 billion. As the numbers grow and technologies become more

mature, the volume of data published will increase. Internet-connected devices

technology, referred to as Internet of Things (IoT), continues to extend the

current Internet by providing connectivity and interaction between the physical

and cyber worlds. In addition to increased volume, the IoT generates Big Data

characterized by velocity in terms of time and location dependency, with a

variety of multiple modalities and varying data quality. Intelligent processing

and analysis of this Big Data is the key to developing smart IoT applications.

This article assesses the different machine learning methods that deal with the

challenges in IoT data by considering smart cities as the main use case. The

key contribution of this study is presentation of a taxonomy of machine learning

algorithms explaining how different techniques are applied to the data in order

to extract higher level information. The potential and challenges of machine learning for IoT data analytics will also be discussed. A use case of applying

Support Vector Machine (SVM) on Aarhus Smart City traffic data is presented

for a more detailed exploration",Machine Learning for Internet of Things Data Analysis: A Survey,10.1016/j.dcan.2017.10.002,,'Elsevier BV',,core
267156304,2018-02-02T00:00:00,"This article proposes a new general approach in short-term water demand forecasting based on a two-stage learning process that couples time-series clustering with gene expression programming (GEP). The approach was tested on the real life water demand data of the city of Milan, in Italy. Moreover, multi-scale modeling using a series of head-time was deployed to investigate the optimum temporal resolution under study. Multi-scale modeling was performed based on rearranging hourly based patterns of water demand into 3, 6, 12, and 24 h lead times. Results showed that GEP should receive more attention among the emerging nonlinear modelling techniques if coupled with unsupervised learning algorithms in detailed spherical k-means.Applied Science, Faculty ofNon UBCEngineering, School of (Okanagan)ReviewedFacult","Gene Expression Programming Coupled with Unsupervised Learning: A Two-Stage Learning Process in Multi-Scale, Short-Term Water Demand Forecasts",10.3390/w10020142,,'MDPI AG',,core
475168165,2018-03-30T07:00:00,"The World Health Organization claims that there are more than 285 million blind and visually impaired people in the world. In the US, 25 million Americans suffer from total or partial vision loss. As a result of their impairment, they struggle with mobility problems, especially the risk of falling. According to the National Council On Aging, falls are among the primary causes for fatal injury and they are the most common cause of non-fatal trauma-related hospital admissions among older adults. Visibility, an organization that helps visually impaired people, reports that people with visual impairments are twice as likely to fall as their sighted counterparts.
The Centers for Disease Control and Prevention reported that 2.5 million American adults were treated for fall-related injuries in 2013, leading to over 800,000 hospitalizations and over 27,000 deaths. The total cost of fall injuries in the United States in 2013 was $31 billion, and the financial total is expected to rise to $67.7 billion by 2020. Reducing the amount of these unexpected hospital visits saves money and expands the quality of life for the affected population.
Technology has completely revolutionized how nowadays activities are conducted and how var- ious tasks are accomplished, and mobile devices are at the center of this paradigm shift. According to the Pew Research Center, 64% of American adults own a smartphone currently, and this number is trending upward. Mobile computing devices have evolved to include a plethora of data sensors that can be manipulated to create solutions for humanity, including fall prevention.
Fall prevention is an area of research that focuses on strengthening safety in order to prevent falls from occurring. Many fall prevention systems use sensing devices to measure the likelihood of a fall. Sensor data are usually processed using computer vision, data mining, and machine learning techniques.
This work pertains to the implementation of a smartphone-based fall prevention system for the elderly and visually impaired. The system consists of two modules: fall prevention and fall detection. Fall prevention is in charge of identifying tripping hazards in the user’s surroundings. Fall detection is in charge of detecting when falls happen and alerting a person of interest. The proposed system is challenged by multiple problems: it has to run in near real time, it has to run efficiently in a smartphone hardware, it has to process structured and unstructured environments, and many more related to image analysis (occlusion, motion blur, computational complexity, etc).
The fall prevention module is divided into three parts, floor detection, object-on-floor detection, and distance estimation. The evaluation process of the best approach for floor detection achieved an accuracy of 92%, a precision of 88%, and a recall of 92%. The evaluation process of the best approach for object-on-floor detection achieved an accuracy of 90%, a precision of 56%, and a recall of 78%. The evaluation process of the best approach for distance estimation achieved a MSE error of 0.45 meters.
The fall detection module is approached from two perspectives, using inertial measuring units (IMU) embedded in today’s smartphones, and using a 2D camera. The evaluation process of the solution using IMUs achieved an accuracy of 83%, a precision of 89%, and a recall of 58.2%. The evaluation process of the solution that uses a 2D camera achieved an accuracy of 85.37% and a recall of 70.97%",A Fall Prevention System for the Elderly and Visually Impaired,,,Scholar Commons,,core
224412113,2018-01-01T00:00:00,"Traffic congestion is a major concern, especially in large cities. To relieve a city of congestion impacts, transportation authorities typically base controls such as toll or congestion prices on expected congestion patterns. However, such strategies can be sub-optimal and may also lead to unintended negative consequences for two main reasons: (i) estimates of congestion state may be inaccurate if the proposed model inputs are too limiting or the assumptions are too restrictive and (ii) predictions may be too late to help. This creates the need for accurate predictions of traffic congestion well ahead of time to avoid delays and gridlocks. Traffic congestion is analyzed as a network-wide phenomenon. Large-scale spatial correlation and long-term temporal correlation govern traffic congestion propagation across the regional traffic network. Such correlations may be exploited to develop congestion prediction algorithms that are more effective than purely local predictions for the purpose of dynamic controls. Moreover, real-time information that is generated at locations across the network may signal the future congestion state, making it possible to take control measures in advance of worsening traffic situations. However, this approach also presents several new challenges, addressed in this research.  Microscopic models of congestion do not produce realistic representations of network-wide congestion generation and its propagation. This motivates the analysis of congestion at neighborhood scale through macroscopic analysis. Recent literature has shown that Macroscopic Fundamental Diagrams (MFDs) are effective for developing neighborhood-wide congestion controls by controlling inflows from immediately surrounding areas or managing signal timings, in combination with flow conservation laws. To further enhance the use of MFDs for neighborhood-wide congestion management, traffic prediction over much larger geographic scales is incorporated. To that end, a numerically well-behaved score function, called Macroscopic Congestion Level (MCL), is proposed. The score is defined to be the ratio of the neighborhood’s vehicle accumulation, to its trip completion rate. Future values of this score are then predicted through a model using network state characteristics over the larger, region-wide network as input. These characteristics are represented as a vector of Origin-Destination (O-D) demands, link accumulations, link travel times and observed MCL values. The predicted score can be used to describe the likely congestion state in a neighborhood in the near future. It is challenging to develop congestion prediction algorithms that incorporate both spatial and temporal dependence at a network scale, adapt to sudden changes in demand patterns and forecast accurately over sufficiently long periods to implement controls. Deep learning is used to build sufficiently complex models for this task. Predictions are made using a deep learning model based on Long Short-Term Memory (LSTM) neural network architecture. The ideas are tested using simulation on a simple, hypothetical city-street network. A battery of simulation tests suggests that the model inputs are sensitive to different queue propagation scenarios, within and across days. MCL predictions made by the proposed deep learning model outperform a simple, yet competent, baseline model that assumes congestion on the next day mimics congestion today (referred to as the 1-Nearest Neighbor or 1-NN model). The prediction accuracy of the deep learning model is compared to that of the baseline 1-NN model in three situations that either aid the baseline model or adversely impact the deep learning model: (i) correlated O-D demands across multiple days, (ii) noisy observations, and (iii) partially observed network state. The limitations to the prediction accuracy in these situations are studied in simulated scenarios. Methods are suggested to improve accuracy in these situations either by modifying certain model hyper-parameters or by understanding the importance of various inputs. A Neural Attention Model-based framework is developed to extract the importance of various inputs and to better understand the inner workings of the deep learning model. Simulation experiments suggest that such a framework allows identification of major congestion-causing factors that may be targeted during control. Model predictions and the importance of various inputs are then used to test a dynamic control strategy, namely an app-based dynamic congestion toll at the beginning of a trip. Individuals are charged a toll at the beginning of each trip based on their predicted congestion impact. This dissuades them from traveling during the time and along the routes where they are likely to cause major harm to the performance of the network. An optimization problem is formulated to minimize cumulative MCL across a day in a target neighborhood while maintaining overall travel demand. The conditions for optimal tolls are obtained and approximations for these conditions are proposed through learned deep learning model parameters. Simulations indicate that a deep learning model-based dynamic toll reduces delays and charges lower toll than other tolling strategies that depend on predictions of demand. Possible improvements to the architecture of the deep learning model are discussed for congestion prediction in large networks. Signals in large networks are assumed to propagate through hypothetical graphs, such as graphs representing the road network or graphs representing the similarities in route-choices made by various individuals. The inputs are transformed to impose a graphical structure on them. A Graph Convolutional Neural Network (Graph-CNN) architecture is implemented for extracting relevant spatial features from this graphical input. An LSTM model makes real-time predictions based on these extracted features. Simulations of commuter trips on a pared-down freeway/highway network as well as full-scale network representing the San Francisco Bay Area suggest that the model accuracy is superior to that of the 1-NN model, the LSTM-only model, and a Graph-CNN + LSTM model without any road network or route-choice information","A Deep Learning, Model-Predictive Approach to Neighborhood Congestion Prediction and Control",,,"eScholarship, University of California",,core
200684591,2018-09-26T10:46:13Z,"In this work, teams of small mobile robots are used to test hypotheses about cooperative transport by ants. This study attempts to explain a decrease in steady-state transport speed with increasing team size that was previously observed in the ant <i>Novomessor cockerelli</i>. Two models of one-dimensional collective towing are compared: one in which transporters with different maximum speeds pull the payload with continuous, variable forces, and another in which transporters with identical speeds pull with intermittent, unsynchronized forces. A statistical analysis of ant data supports the hypothesis that ants behave according to the first model, in which the steady-state transport speed is the maximum speed of the slowest teammate. By contrast, the ant data are not consistent with the second model, which predicts constant speed regardless of team size. To verify these predictions, the ant behaviours in each model are translated into decentralized controllers and implemented on teams of two to four robots. The controller for the first model incorporates a real-time reinforcement learning algorithm that successfully reproduces the observed relationship between ant team size and transport speed. The controller for the second model yields the predicted invariance of transport speed with team size. These results show the value of robotic swarms for testing mechanistic hypotheses about biological collectives",Explanation of the assumption the ant data is normally distributed for proper order statistic analysis. from Multi-robot replication of ant collective towing behaviours,10.6084/m9.figshare.7133237.v1,,,,core
158379559,2018-06-13T00:00:00,"This paper presents preliminary work on learning the search heuristic for the
optimal motion planning for automated driving in urban traffic. Previous work
considered search-based optimal motion planning framework (SBOMP) that utilized
numerical or model-based heuristics that did not consider dynamic obstacles.
Optimal solution was still guaranteed since dynamic obstacles can only increase
the cost. However, significant variations in the search efficiency are observed
depending whether dynamic obstacles are present or not. This paper introduces
machine learning (ML) based heuristic that takes into account dynamic
obstacles, thus adding to the performance consistency for achieving real-time
implementation.Comment: 3 pages, 1 figure, 1 pseudocode, extended abstract accepted to ICML /
  IJCAI / AAMAS 2018 Workshop on Planning and Learning (PAL-18",Safe learning-based optimal motion planning for automated driving,,http://arxiv.org/abs/1805.09994,,,core
161546101,2018-01-01T00:00:00,"As a result of the digitalization of the power business in Norway and Europa, a lot of new possibilities and challenges arise. In 2014 an expert committee one outlined a proposal for the future grid company structure in Norway (Reiten, 2014). In addition, new technologies are being implemented in the system. Wind power, solar power, un-regulated small hydro power production, battery storage domestic and industrial and electrification of transport. Transmission System Operators (TSOs) have a responsibility to supply industry and communities with reliable electric power. However, the operators have been virtually blind to slowly occurring changes in the load profile that reduce the expected regularity of the power supply. This paper will focus on the possibilities and challenges the power business are facing. The paper will describe what technologies is needed i.e Real time probabilistic risk calculations, artificial intelligence, machine learning and smart grid technology. The main question is: can the power business and the introduction of new system tools manage without probabilistic risk calculation for making use of the digitalization and the corresponding big data",Digitalization of the power business: How to make this work?,,,Taylor&Francis Group,,core
216392458,2018-08-01T07:00:00,"Security is a critical concern around the world, whether it is the challenge of protecting ports, airports, and other critical infrastructure; interdicting the illegal flow of drugs, weapons, and money; protecting endangered wildlife, forests, and fisheries; or suppressing urban crime or security in cyberspace. Unfortunately, limited security resources prevent full security coverage at all × instead, we must optimize the use of limited security resources. To that end, we founded a new  security games  framework that has led to building of decision aids for security agencies around the world. Security games are a novel area of research that is based on computational and behavioral game theory while also incorporating elements of AI planning under uncertainty and machine learning. Today security-games-based decision aids for infrastructure security are deployed in the US and internationally; examples include deployments at ports and ferry traffic with the US Coast Guard, for security of air traffic with the US Federal Air Marshals, and for security of university campuses, airports, and metro trains with police agencies in the US and other countries. Moreover, recent work on  green security games  has led our decision aids to be deployed, assisting NGOs in protection of wildlife; and  opportunistic crime security games  have focused on suppressing urban crime. In cyber-security domain, the interaction between the defender and adversary is quite complicated with high degree of incomplete information and uncertainty. Recently, applications of game theory to provide quantitative and analytical tools to network administrators through defensive algorithm development and adversary behavior prediction to protect cyber infrastructures has also received significant attention. This chapter provides an overview of use-inspired research in security games including algorithms for scaling up security games to real-world sized problems, handling multiple types of uncertainty, and dealing with bounded rationality and bounded surveillance of human adversaries",Trends and Applications in Stackelberg Security Games,10.1007/978-3-319-44374-4_27,,'Springer Science and Business Media LLC',,core
151300019,2018-03-07T00:00:00,"Este trabalho apresenta a metodologia, o desenvolvimento e testes de um sistema de automação independente, baseado em Redes Neurais Artificiais, para redução de perdas técnicas em redes de distribuição subterrâneas reticuladas por meio do controle ótimo dos bancos de capacitores presentes na rede. A metodologia proposta contempla funcionalidades típicas de Redes Inteligentes, incluindo soluções práticas para o posicionamento de sensores de corrente em redes subterrâneas, coleta de medições de campo e transmissão para o Centro de Operação da Distribuição e controle em tempo real dos equipamentos de campo (bancos de capacitores). Portanto este trabalho consiste na implementação da solução através de baixo custo de investimento na mitigação do controle do fator de potência nos pontos de entrega ao consumidor, sendo que com isto ocorrem melhorias nos indicadores de qualidade e confiabilidade atendendo aos requisitos regulamentares e contratuais de fornecimento das distribuidoras. Para validação da metodologia proposta, foram utilizados os dados da concessionária de energia AES Eletropaulo sobre a Rede de Distribuição Subterrânea Reticulada do centro da cidade de São Paulo. As etapas da metodologia proposta e os principais aspectos do desenvolvimento do sistema são também descritos, bem como os testes realizados para comprovação dos resultados e validação do sistema.This work presents the methodology, development and testing of an independent automation system, based on Artificial Neural Networks, to reduce technical losses in reticulated underground distribution networks by means of the optimal control of the capacitor banks present in the network. The proposed methodology includes typical functionalities of Intelligent Networks, including practical solutions for the positioning of current sensors in underground networks, collection of field measurements and transmission to the Distribution Operation Center and real-time control of field equipment (capacitors banks). Therefore, this work consists in the implementation of the solution through a low cost of investment in the mitigation of the control of the power factor in the points of delivery to the consumer, and with this there are improvements in the indicators of quality and reliability taking into account the regulatory and contractual requirements of supply of the distributors. The energy concessionaire AES Eletropaulo had great participation in this research project, providing the necessary data of the Reticulated Underground Distribution Network of the city center of São Paulo. The steps of the proposed methodology and the main aspects of system development are also described, as well as the tests performed to prove the results and validate the system","Automation of the reduction of technical in reticulated distribution systems using artificial neural netwarks, te4chnical losses power factor.",10.11606/T.3.2018.tde-05032018-102829,,"'Universidade de Sao Paulo, Agencia USP de Gestao da Informacao Academica (AGUIA)'",,core
215548348,2018-07-01T07:00:00,"Aerosol jet printing (AJP)—a direct-write, additive manufacturing technique—has emerged as the process of choice particularly for the fabrication of flexible and hybrid electronics. AJP has paved the way for high-resolution device fabrication with high placement accuracy, edge definition, and adhesion. In addition, AJP accommodates a broad range of ink viscosity, and allows for printing on non-planer surfaces. Despite the unique advantages and host of strategic applications, AJP is a highly unstable and complex process, prone to gradual drifts in machine behavior and deposited material. Hence, real-time monitoring and control of AJP process is a burgeoning need. In pursuit of this goal, the objectives of the work are, as follows: (i) In situ image acquisition from the traces/lines of printed electronic devices right after deposition. To realize this objective, the AJP experimental setup was instrumented with a high-resolution charge-coupled device (CCD) camera, mounted on a variable-magnification lens (in addition to the standard imaging system, already installed on the AJ printer). (ii) In situ image processing and quantification of the trace morphology. In this regard, several customized image processing algorithms were devised to quantify/extract various aspects of the trace morphology from online images. In addition, based on the concept of shape-from-shading (SfS), several other algorithms were introduced, allowing for not only reconstruction of the 3D profile of the AJ-printed electronic traces, but also quantification of 3D morphology traits, such as thickness, cross-sectional area, and surface roughness, among others. (iii) Development of a supervised multiple-input, single-output (MISO) machine learning model—based on sparse representation for classification (SRC)—with the aim to estimate the device functional properties (e.g., resistance) in near real-time with an accuracy of ≥ 90%. (iv) Forwarding a computational fluid dynamics (CFD) model to explain the underlying aerodynamic phenomena behind aerosol transport and deposition in AJP process, observed experimentally.
Overall, this doctoral dissertation paves the way for: (i) implementation of physics-based real-time monitoring and control of AJP process toward conformal material deposition and device fabrication; and (ii) optimal design of direct-write components, such as nozzles, deposition heads, virtual impactors, atomizers, etc","COMPUTATIONAL FLUID DYNAMICS MODELING AND IN SITU PHYSICSBASED MONITORING OF AEROSOL JET PRINTING TOWARD FUNCTIONAL ASSURANCE OF ADDITIVELY-MANUFACTURED, FLEXIBLE AND HYBRID ELECTRONICS",,https://core.ac.uk/download/215548348.pdf,The Open Repository @ Binghamton (The ORB),,core
199174625,2018-12-06T00:00:00,"The theme of public safety, in light of the latest dramatic events, has recently become a relevant aspect, especially in large urban areas. The possibility to act promptly in case of dangers or alarms can be of fundamental importance in determining the favorable outcome of the interventions. For example, identifying and tracking an individual or a suspicious vehicle that moves in an urban context may require significant deployment of forces, with costs that can sometimes render the interventions ineffective.
The intelligent camera represents a technology that can help manage this problem and that in recent years has seen a rapid development in many directions. The most interesting aspect is certainly its low cost. Furthermore, the advent of the 5G network and its integration with drone technology could provide a synergy for the development of innovative and low-cost public security applications. The visual information detected by the camera (photo and video) can be effectively exploited in the phase of identification on the ground, through the use of artificial intelligence technologies based on deep learning. The development of these technologies has been enormous as well. Thanks to the power of graphics cards equipped with Graphics Processing Units (GPU), recognizing in real time, in a video, a face of a person or a specific object in motion is now within the reach of a home computer",Design and Implementation of a System Based on Deep Convolutional Networks for Intelligent Visual Surveillance,,,'Pisa University Press',,core
231786118,2017-09-01T00:00:00,"Seoul, Republic of Korea, September, 2017 — Commissioned by the curators of the inaugural Seoul Biennale of Architecture and Urbanism, open September 2, 2017 through December ??, 2017 at the Donuimun Museum Village, Seoul, SK. Driver Less Vision presents the immersive experience of becoming an autonomous, self-driving vehicle. Created by Urtzi Grau (Fake Industries Architectural Agonism), Guillermo Fernandez-Abascal with Perlin Studios, the project was produced with virtual reality video and architectural design by the New York-based teams, and installed in a 25’ dome at the Seoul Biennale. Driver Less Vision examines the tension and reality of AI and humans merging and diverging as they negotiate Seoul's unique urban landscape—challenging us to consider how we can design cities for the future of autonomous vehicles. Driver Less Vision aims to generate empathy between humans and non-humans, to construct the trust required for negotiations that will settle how we will live together. By overlapping human and machine’s perceptions, the installation helps to identify the areas of the city that will need to be redesigned in the immediate future. Driver Less Vision is the immersive experience of becoming an autonomous, self-driving vehicle. It explores the untapped conflicts and disruptive effects on the built environment caused by the deployment of technologies for autonomous mobility. Currently, the visual stimuli that organizes traffic is designed for human perception. The arrival of driverless cars entails the emergence of a omnidirectional gaze that is required to negotiate existing visual codes. To assume that driverless cars will fully adapt to future conditions of the city, however, neglects the history of transformations in urban streetscapes associated with changes in vehicular technologies. Driver Less Vision is an attempt to understand how driverless cars will change the city by immersing the audience in an urban journey through the car’s point of view, seeing the streets of Seoul through overlapping and dissonant perceptions. The project was produced for the Seoul Biennale of Architecture and Urbanism in 2017, utilizing an eight meter diameter dome with 360 visuals developed with the generous support of University of Technology Sydney, Rice University and Ocular Robotics",Driver Less Vision,,http://hdl.handle.net/10453/133482,Actar,,core
299440355,06/11/2018,"We present an automatic aerosol classification
method based solely on the European Aerosol Research Lidar
Network (EARLINET) intensive optical parameters with
the aim of building a network-wide classification tool that
could provide near-real-time aerosol typing information. The
presented method depends on a supervised learning technique
and makes use of the Mahalanobis distance function
that relates each unclassified measurement to a predefined
aerosol type. As a first step (training phase), a reference
dataset is set up consisting of already classified EARLINET
data. Using this dataset, we defined 8 aerosol classes: clean
continental, polluted continental, dust, mixed dust, polluted
dust, mixed marine, smoke, and volcanic ash. The effect of
the number of aerosol classes has been explored, as well as
the optimal set of intensive parameters to separate different
aerosol types. Furthermore, the algorithm is trained with literature
particle linear depolarization ratio values. As a second
step (testing phase), we apply the method to an already
classified EARLINET dataset and analyze the results of the
comparison to this classified dataset. The predictive accuracy
of the automatic classification varies between 59% (minimum)
and 90% (maximum) from 8 to 4 aerosol classes, respectively,
when evaluated against pre-classified EARLINET
lidar. This indicates the potential use of the automatic classification
to all network lidar data. Furthermore, the training of
the algorithm with particle linear depolarization values found
in the literature further improves the accuracy with values for
all the aerosol classes around 80 %. Additionally, the algorithm
has proven to be highly versatile as it adapts to changes
in the size of the training dataset and the number of aerosol
classes and classifying parameters. Finally, the low computational
time and demand for resources make the algorithm extremely suitable for the implementation within the single
calculus chain (SCC), the EARLINET centralized processing
suite.The research
leading to these results has received funding from the European
Union’s Horizon 2020 research and innovation program under
grant agreement no. 602014 (project ECARS – East European
Centre for Atmospheric Remote Sensing) and from the European
Union’s Horizon 2020 research program for societal challenges
– smart, green and integrated transport under grant agreement
no. 723986 (project EUNADICS-AV – European Natural Disaster
Coordination and Information System for Aviation)",An automatic observation-based aerosol typing method for EARLINET,10.5194/acp-18-15879-2018,,European Geosciences Union,,core
237101206,2018-11-06T00:00:00,"We present an automatic aerosol classification
method based solely on the European Aerosol Research Lidar
Network (EARLINET) intensive optical parameters with
the aim of building a network-wide classification tool that
could provide near-real-time aerosol typing information. The
presented method depends on a supervised learning technique
and makes use of the Mahalanobis distance function
that relates each unclassified measurement to a predefined
aerosol type. As a first step (training phase), a reference
dataset is set up consisting of already classified EARLINET
data. Using this dataset, we defined 8 aerosol classes: clean
continental, polluted continental, dust, mixed dust, polluted
dust, mixed marine, smoke, and volcanic ash. The effect of
the number of aerosol classes has been explored, as well as
the optimal set of intensive parameters to separate different
aerosol types. Furthermore, the algorithm is trained with literature
particle linear depolarization ratio values. As a second
step (testing phase), we apply the method to an already
classified EARLINET dataset and analyze the results of the
comparison to this classified dataset. The predictive accuracy
of the automatic classification varies between 59% (minimum)
and 90% (maximum) from 8 to 4 aerosol classes, respectively,
when evaluated against pre-classified EARLINET
lidar. This indicates the potential use of the automatic classification
to all network lidar data. Furthermore, the training of
the algorithm with particle linear depolarization values found
in the literature further improves the accuracy with values for
all the aerosol classes around 80 %. Additionally, the algorithm
has proven to be highly versatile as it adapts to changes
in the size of the training dataset and the number of aerosol
classes and classifying parameters. Finally, the low computational
time and demand for resources make the algorithm extremely suitable for the implementation within the single
calculus chain (SCC), the EARLINET centralized processing
suite.The research
leading to these results has received funding from the European
Union’s Horizon 2020 research and innovation program under
grant agreement no. 602014 (project ECARS – East European
Centre for Atmospheric Remote Sensing) and from the European
Union’s Horizon 2020 research program for societal challenges
– smart, green and integrated transport under grant agreement
no. 723986 (project EUNADICS-AV – European Natural Disaster
Coordination and Information System for Aviation)",An automatic observation-based aerosol typing method for EARLINET,10.5194/acp-18-15879-2018,,'Copernicus GmbH',,core
234915328,2017,"Today a large volume of energy-related data have been continuously collected. Extracting actionable knowledge from such data is a multi-step process that opens up a variety of interesting and novel research issues across two domains: energy and computer science. The computer science aim is to provide energy scientists with cutting-edge and scalable engines to effectively support them in their daily research activities. This paper presents SPEC, a scalable and distributed predictor of fine grain energy consumption in buildings. SPEC exploits a data stream methodology analysis over a sliding time window to train a prediction model tailored to each building. The building model is then exploited to predict the upcoming energy consumption at a time instant in the near future. SPEC currently integrates the artificial neural networks technique and the random forest regression algorithm. The SPEC methodology exploits the computational advantages of distributed computing frameworks as the current implementation runs on Spark. As a case study, real data of thermal energy consumption collected in a major city have been exploited to preliminarily assess the SPEC accuracy. The initial results are promising and represent a first step towards predicting fine grain energy consumption over a sliding time window",Predicting large scale fine grain energy consumption,10.1016/j.egypro.2017.03.271,https://core.ac.uk/download/pdf/234915328.pdf,'Elsevier BV',,core
84251632,2017,"Today a large volume of energy-related data have been continuously collected. Extracting actionable knowledge from such data is a multi-step process that opens up a variety of interesting and novel research issues across two domains: energy and computer science. The computer science aim is to provide energy scientists with cutting-edge and scalable engines to effectively support them in their daily research activities. This paper presents SPEC, a scalable and distributed predictor of fine grain energy consumption in buildings. SPEC exploits a data stream methodology analysis over a sliding time window to train a prediction model tailored to each building. The building model is then exploited to predict the upcoming energy consumption at a time instant in the near future. SPEC currently integrates the artificial neural networks technique and the random forest regression algorithm. The SPEC methodology exploits the computational advantages of distributed computing frameworks as the current implementation runs on Spark. As a case study, real data of thermal energy consumption collected in a major city have been exploited to preliminarily assess the SPEC accuracy. The initial results are promising and represent a first step towards predicting fine grain energy consumption over a sliding time window",Predicting large scale fine grain energy consumption,10.1016/j.egypro.2017.03.271,https://core.ac.uk/download/pdf/84251632.pdf,Elsevier,,core
84583768,2017-06-08T00:00:00,"International audienceSmartphone usage while driving is a dangerous activity directly linked to 17% of deadly accidents in France in 2015. While it can potentially impact every road user, professional drivers are perhaps the most affected collective as they spend a high amount of time in their vehicles on a daily basis, whether they deliver goods or transport people. Detecting smartphone usage is interesting for many reasons, from legal controls to forensics investigation in accidents, without forgetting the possibility of alert the driver of the danger he is engaging in. In this study we evaluate the pertinence of using driver head rotation movements to automatically predict smartphone usage at the wheel. In order to fit the system to the particularities of professional drivers, a naturalistic driving study have been conducted. 15 operational vehicles from two private French transport companies were equipped with near-infrared cameras, embedded real-time computer vision and machine learning techniques. Common behavioural patterns have been revealed. Head rebounds are a constant (letting the driver switch gaze between the road and the device). Additionally, the duration a driver spends looking down from a reference neutral direction can be used as a reliable parameter to predict smartphone usage. On the other hand some divergences between van and bus drivers head movements have been noticed. A real-time smartphone usage detection system has been implemented from the results of this study. Preliminary results are encouraging and a prototype of the system is already being tested by professional drivers in a naturalistic context",Driver Head Movements While Using a Smartphone in a Naturalistic Context,,,HAL CCSD,,core
160631705,2017-01-01T00:00:00,"This paper presents the development and implementation of a theoretical mathematical-statistical framework for sequential updating of the grade control model, based on a support vector machine learning algorithm. Utilising the Zambujal orebody within the Neves-Corvo Cu deposit in Portugal, parameters that can be measured in real time, used in visualisation, modelled for resource estimation, and used for process control visualisation and optimisation are considered.
The methodology broadly comprises of three steps. Firstly, the provided dataset is used to develop a virtual asset model (VAM) representing the true 3D grade distribution in order to simulate the mining method. Then ore quality parameters are established simulating real time monitoring sensor installation at: (a) stope development and rock face monitoring (face imaging and drillholes); and (b) transport monitoring (muck pile, LHD/scooptram). Next, the acquired data was assimilated into the models as part of the sequential model update.
Two different mining methods and the monitoring information that can be acquired during the ore extraction are analysed: (a) drift and fill mining and (b) bench and fill mining, which are widely implemented at the Neves-Corvo mine. Selected study zones were chosen such as to contrast mining through the high/low grade zones with different degrees of heterogeneity, which demonstrate the performance of resource estimation and classification models developed in heterogeneous mining stopes.
The grade accuracy and error in the resource model, and high/low grade ore classification accuracy and error are evaluated as performance metrics for the proposed methods.
In drift and fill mining, drillhole and face sampling data collection was simulated in a real-time manner and fed into the support vector machine (SVM) regressor to update the resource estimation model in both a high grade and low grade drift scenarios. In each scenario, six drift and fill mining steps were simulated sequentially and the posterior resource models, after integrating real time mining data, have shown significant improvement of bias correction in both updating planned resources and reconciling extracted ore.
In bench and fill mining, grade classification based on random sampling data from muck pile was demonstrated, considering scoop by scoop derived monitoring data. Three different classifiers (mean, median, and Bayesian) were tested and shown very good performance. In the case study presented here, a sequence of 15 blasting steps was simulated with each step requiring 112 scooping operations to transport the blasted ore. Using the real time monitored information, it was shown that at each blasting step over 85% of the scoops can be labelled correctly using the proposed methods and with an accuracy of over 95%",Development of support vector machine learning algorithm for real time update of resource estimation and grade classification,,,TU Bergakademie Freiberg,,core
390006579,2018-11-28T00:00:00,"According to statistics, every fifth married couple is faced with the inability to conceive a child. Male germ cells are very vulnerable, and the growing number of cases of male infertility confirms that in today's world there are many factors that affect the activity of spermatozoa and their number. But the important thing is not so much their quantity, but quality. The spermogram is an objective method of laboratory diagnosis, which allows  to accurately assess the man’s ability to fertilize by analyzing ejaculate for a number of key parameters. Only a spermogram can answer the question of a possible male infertility and the presence of urological diseases. When constructing spermograms, it is important to determine not only the number of good spermatozoa, but also their morphology and mobility. Therefore, research and improvement of some stages of spermogramm is the purpose of the study. This article addresses the problem of classification of spermatozoa in good and bad ones, taking into account their mobility and morphology, using methods of machine learning. In order to implement the first stage of machine learning (with a teacher) in the graphic editor, educational specimens (training sample) were created. The training was implemented by three methods: the method of support vector machine, the logistic regression and the method of K - the nearest neighbors. As a result of testing, the method K - the nearest neighbors is chosen. At the testing stage, a sample of 15 different spermatozoa was used in different variations of rotation around their axis. The test sample did not contain specimens from the training sample and was formed taking into account the morphological characteristics of the spermatozoa, but did not copy them from the training sample. At the final stage of study, the program's functioning was tested on real data.По статистике, каждая пятая супружеская пара сталкивается с невозможностью зачатия ребенка. Мужские половые клетки очень уязвимы, растущее число случаев мужского бесплодия подтверждает, что в современном мире очень много факторов, которые влияют и на активность сперматозоидов и на их количество.  И важно не столько их количество, сколько качество. Спермограмма является объективным методом лабораторной диагностики, что позволяет максимально точно оценить способность к оплодотворению человека, проанализировав эякулят по ряду важнейших параметров. Только спермограмма способна ответить на вопрос о возможном мужском бесплодии и о наличии урологических заболеваний. При построении спермограммы, важно определять не только количество хороших сперматозоидов, но и их морфологию и подвижность. Поэтому исследования и совершенствования некоторых этапов спермограммы и является целью исследования. В данной статье решается задача классификации сперматозоидов на добрые и плохие, с учетом их подвижности и морфологии, с применением методов машинного обучения. Для реализации первого этапа машинного обучения (с учителем) в графическом редакторе были созданы учебные экземпляры (тренировочная выборка). Обучение было реализована тремя методами: методом опорных векторов, логистическая регрессия и метод К - ближайших соседей. По результатам тестирования выбран метод К - ближайших соседей. На этапе тестирования использовалась выборка из 15 различных сперматозоидов в различных вариациях вращения вокруг своей оси. Тестовая выборка не содержала экземпляров с тренировочной выборки и была сформирована с учетом морфологических особенностей сперматозоидов, но не копировала их с тренировочной выборки. На завершающем этапе обучения работе программы были протестированы на реальных данных.За статистикою, кожна п'ята подружня пара стикається з неможливістю зачаття дитини. Чоловічі статеві клітини дуже вразливі, зростаюче число випадків чоловічого безпліддя підтверджує, що в сучасному світі дуже багато чинників, які впливають і на активність сперматозоїдів і на їх кількість. Та важливою є  не стільки їх кількість, скільки якість. Спермограма є об'єктивним методом лабораторної діагностики, що дозволяє максимально точно оцінити здатність до запліднення чоловіка, проаналізувавши еякулят за рядом найважливіших параметрів. Тільки спермограма здатна відповісти на питання про можливе чоловіче безпліддя та про наявність урологічних захворювань. При побудові спермограми, важливо визначати не тільки кількість добрих сперматозоїдів, але й їх морфологію та рухливість. Тому дослідження та вдосконалення деяких етапів спермограми і є метою дослідження. У даній статті вирішується задача класифікації сперматозоїдів на добрі та погані, з урахуванням їх рухливості та морфології, із застосуванням методів машинного навчання. Для реалізації першого етапу машинного навчання (з вчителем) у графічному редакторі були створені навчальні екземпляри (тренувальна вибірка). Навчання було реалізована трьома методами: методом опорних векторів, логістична регресія та метод К – найближчих сусідів. За результатами тестування обрано  метод К – найближчих сусідів. На етапі тестування використовувалася вибірка з 15 різних сперматозоїдів в різних варіаціях обертання навколо своєї осі. Тестова вибірка не містила примірників з тренувальної вибірки і була сформована з урахуванням морфологічних особливостей сперматозоїдів, але не копіювала їх з тренувальної вибірки. На завершальному етапі навчання роботу програми було протестовано на реальних даних",Застосування методів машинного навчання для вирішення задачі аналізу біологічних даних,,,"NTU ""KhPI""",,core
234579428,14/02/2018,"The subject of the research in this paper is the artificial intelligence models through a modern methodological approach to solving complex water management problems. Fuzzy logic as new mathematics, neural networks and fuzzy expert systems open up great opportunities in simulating and solving real problems related to the division of water and water resources. When decision-makers seek optimum, in addition to technical and economic criteria, sometimes the decisive influence belongs to the low, ecology, political, territorial, social, religious or cultural criteria. The developed theoretical model was tested and confirmed by the development of an expert system for the selection of an optimal solution for the management and distribution of the water potential of the Drina River, in the zone of different entities, between the cities of Foča and Goražde. Holistic approach and synergy appreciation of all relevant criteria resulted in the fact that the solution found by the expert model was also implemented in practice, which is confirmation for the expert system developed",CONTEMPORARY METHODS OF ARTIFICIAL INTELLIGENCE  IN THE FUNCTION OF WATER RESOURCES MANAGEMENT,10.7251/cm.v2i8.4408,,Academy of Sciences and Arts of the Republic of Srpska,,core
324084887,2017-08-01T00:00:00,"Artificial Neural Network (ANN) is a valuable and well-established inversion technique
for the estimation of geophysical parameters from satellite images. After training, ANNs are able to generate very fast products for several types of applications. Satellite remote sensing is an efficient way to detect and map strong earthquake damage for contributing to post-disaster activities during emergency phases. This work aims at presenting an application of the ANN inversion technique addressed to the evaluation of building collapse ratio (CR), defined as the number of collapsed buildings with respect to the total number of buildings in a city block, by employing optical and SAR satellite data. This is done in order to directly relate changes in images with damage that has occurred during strong earthquakes. Furthermore, once they have been trained, neural networks can be used rapidly at application stage. The goal was to obtain a general tool suitable for re-use in different scenarios. An ANN has been implemented in order to emulate a regression model and to estimate the CR as a continuous function. The adopted ANN has been trained using some features obtained from optical and Synthetic Aperture Radar (SAR) images, as inputs, and the corresponding values of collapse ratio obtained from the survey of the 2010 M7 Haiti Earthquake, i.e., as target output. As regards the optical data, we selected three change parameters: the Normalized Difference Index (NDI), the Kullback–Leibler divergence (KLD), and Mutual Information (MI). Concerning the SAR images, the Intensity Correlation Difference (ICD) and the KLD parameters have been considered. Exploiting an object-oriented approach, a segmentation of the study area into several regions has been performed. In particular, damage maps have been generated by considering a set of polygons (in which satellite parameters have been calculated) extracted from the open source Open Street Map (OSM) geo-database. The trained ANN has been proposed for the M6.0 Amatrice earthquake that occurred on 24 August 2016, in central Italy, by using the features extracted from Sentinel-2 and COSMO-SkyMed images as input. The results show that the ANN is able to retrieve a building collapse ratio with good accuracy. In particular, the fusion approach modelled the collapse ratio characterized by high values of CR (more than 0.5) over the historical center that agrees with observed damages. Since the technique is independent from different typologies of input data (i.e., for radiometric or spatial resolution characteristics), the study demonstrated the strength of the proposed approach for estimating damaged areas and its importance in near real time monitoring activities, owing to its fast application.Published7814T. Sismologia, geofisica e geologia per l'ingegneria sismicaN/A or not JC",A New Damage Assessment Method by Means of Neural Network and Multi-Sensor Satellite Data,10.3390/app7080781,,'MDPI AG',,core
129584156,2017-06-22T07:00:00,"Despite the impressive advancements in people detection and tracking, safety is still a key barrier to the deployment of autonomous vehicles in urban environments [1]. For example, in non-autonomous technology, there is an implicit communication between the people crossing the street and the driver to make sure they have communicated their intent to the driver. Therefore, it is crucial for the autonomous car to infer the future intent of the pedestrian quickly. We believe that human body orientation with respect to the camera can help the intelligent unit of the car to anticipate the future movement of the pedestrians. To further improve the safety of pedestrians, it is important to recognize whether they are distracted, carrying a baby, or pushing a shopping cart. Therefore, estimating the fine- grained 3D pose, i.e. (x,y,z)-coordinates of the body joints provides additional information for decision-making units of driverless cars.
In this dissertation, we have proposed a deep learning-based solution to classify the categorized body orientation in still images. We have also proposed an efficient framework based on our body orientation classification scheme to estimate human 3D pose in monocular RGB images.
Furthermore, we have utilized the dynamics of human motion to infer the body orientation in image sequences. To achieve this, we employ a recurrent neural network model to estimate continuous body orientation from the trajectories of body joints in the image plane.
The proposed body orientation and 3D pose estimation framework are tested on the largest 3D pose estimation benchmark, Human3.6m (both in still images and video), and we have proved the efficacy of our approach by benchmarking it against the state-of-the-art approaches.
Another critical feature of self-driving car is to avoid an obstacle. In the current prototypes the car either stops or changes its lane even if it causes other traffic disruptions. However, there are situations when it is preferable to collide with the object, for example a foam box, rather than take an action that could result in a much more serious accident than collision with the object. In this dissertation, for the first time, we have presented a novel method to discriminate between physical properties of these types of objects such as bounciness, elasticity, etc. based on their motion characteristics . The proposed algorithm is tested on synthetic data, and, as a proof of concept, its effectiveness on a limited set of real-world data is demonstrated",Estimation of Human Poses Categories and Physical Object Properties from Motion Trajectories,,https://core.ac.uk/download/129584156.pdf,Scholar Commons,,core
188681324,2018,"As a result of the digitalization of the power business in Norway and Europa, a lot of new possibilities and challenges arise. In 2014 an expert committee one outlined a proposal for the future grid company structure in Norway (Reiten, 2014). In addition, new technologies are being implemented in the system. Wind power, solar power, un-regulated small hydro power production, battery storage domestic and industrial and electrification of transport. Transmission System Operators (TSOs) have a responsibility to supply industry and communities with reliable electric power. However, the operators have been virtually blind to slowly occurring changes in the load profile that reduce the expected regularity of the power supply. This paper will focus on the possibilities and challenges the power business are facing. The paper will describe what technologies is needed i.e Real time probabilistic risk calculations, artificial intelligence, machine learning and smart grid technology. The main question is: can the power business and the introduction of new system tools manage without probabilistic risk calculation for making use of the digitalization and the corresponding big data?publishedVersionPublished by Taylor & Francis. Made available under the CC-BY-NC-ND 4.0 license http://creativecommons.org/licenses/by-nc-nd/4.0",Digitalization of the power business: How to make this work?,10.1201/9781351174664,,Taylor & Francis,,core
148027074,2017-01-18T14:14:18,"Targeting the problem of generating high-resolution air quality maps for cities, we leverage four different sources of data: (i) in-situ air quality measurements produced by our mobile sensor network deployed on public transportation vehicles, (ii) explanatory air-quality and meteorological variables obtained from two static monitoring stations, (iii) land-use data of the city, and (iv) traffic statistics. We propose two novel approaches for estimating the targeted pollutant level at desired time-location pairs, extending also to areas of the city that are beyond the coverage of our mobile sensor network. The first is a log-linear regression model which is built over a virtual dependency graph based on land-use data. The second is a deep learning framework that automatically captures the dependencies of the data based on autoencoders. We have evaluated the two proposed approaches against three canonical modeling techniques considering metrics of coefficient of determination (R-squared), root mean square error (RMSE), and the fraction of predictions within a factor of two of observations (FAC2). Using more than 45 million real measurements in the models, the results show consistently superior performance in respect to the canonical techniques","Extending Urban Air Quality Maps Beyond the Coverage of a Mobile Sensor Network: Data Sources, Methods, and Performance Evaluation",,https://core.ac.uk/download/148027074.pdf,,,core
201478044,2017-11-01T00:00:00,"Monitoring of mechanical structures is a Big Data challenge and includes Structural Health Monitoring (SHM) and Non-destructive Testing (NDT). The sensor data produced by common measuring techniques, e.g., guided wave propagation analysis, is characterized by a high dimensionality in the temporal and spatial domain. There are off- and on-line methods applied at maintenance- or run-time, respectively. On-line methods (SHM) usually are constrained by low-resource processing platforms, sensor noise, unreliability, and real-time operation requiring advanced and efficient sensor data processing. Commonly, structural monitoring is a task that maps high-dimensional input data on low-dimensional output data (information, which is feature extraction), e.g., in the simplest case a Boolean output variable “Damaged”. Machine Learning (ML), e.g., supervised learning, can be used to derive such a mapping function. But ML quality and performance depends strongly on the input data size. Therefore, adaptive and reliable input data reduction (that is feature selection) is required at the first layer of an automatic structural monitoring system. Assuming some kind of two-dimensional sensor data (or n-dimensional data in general), image segmentation can be used to identify Regions of Interest (ROI), e.g., of wave propagation fields. Wave propagation in materials underlie reflections that must be distinguished, especially in hybrid materials (e.g., combining metal and fibre-plastic composites) there are complex wave propagation fields. The image segmentation is one of the most crucial parts of image processing. Major difficulties in image segmentation are noise and the differing homogeneity (fuzziness and signal gradients) of regions, complicating the definition of suitable threshold conditions for the edge detection or region splitting/clustering. Many traditional image segmentation algorithms are constrained by this issue. Artificial Intelligence can aid to overcome this limitation by using autonomous agents as an adaptive and self-organizing software architecture, presented in this work. Using a collection of co-operating agents decomposes a large and complex problem in smaller and simpler problems with a Divide-and-Conquer approach. Related to the image segmentation scenario, agents are working mostly autonomous (de-coupled) on dynamically bounded data from different regions of a signal or an image (i.e., distributed with simulated mobility), adapted to the locality, being reliable and less sensitive to noisy sensor data. In this work, self-organizing agents perform segmentation. They are evaluated with measured high-dimensional data from piezo-electric acusto-ultrasonic sensors recording the wave propagation in plate-like structures. Commonly, SHM deploys only a small set of sensors and actuators at static positions delivering only a few temporal resolved sensor signals (1D), whereas NDT methods additionally can use spatial scanning to create images of wave signals (2D). Both one-dimensional temporal and two-dimensional spatial segmentation are considered to find characteristic ROI",Robust and Adaptive Signal Segmentation for Structural Monitoring Using Autonomous Agents,10.3390/ecsa-4-04917,,'MDPI AG',"[{'title': 'Proceedings', 'identifiers': ['issn:2504-3900', '2504-3900']}]",core
211797442,2017-08-27T00:00:00,"Copyright © 2017 Hasi Bagan et al. This is an open access article distributed under the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited.Remote sensing data have already proven useful for environmental monitoring in a timely, detailed, and cost-effective manner to assist various planning and management activities. Remotely sensed data collected over a span of years can be used to identify and characterize both natural and anthropogenic changes over large areas of land at a variety of spatial and temporal scales [1–3]. As climate change and population growth place increasing pressures on many parts of the world, improved methods for monitoring urban growth across a range of spatial and temporal scales will be vital for understanding and addressing the impacts of urbanization on our natural resources [4, 5]. With the advance of machine learning algorithms and computing facilities, many investigations on their real applications are taking place. Combining remote sensing data and mathematics techniques to quantitatively analyze environmental change is a topic growing in importance [6]. The meaningful interpretation of remote sensing data and in situ observations require implementation and analysis using advanced mathematics and statistical techniques.

The objective of this special issue is to provide a snapshot of status, potentials, challenges, and achievements of mathematical application in using remote sensing data to address environmental issues. This special issue includes thirteen papers that cover four major topics: image processing methods, land use/land cover change analysis, land degradation, urbanization, and vegetation cover. A brief description of these 13 works is detailed below",Mathematics in Utilizing Remote Sensing Data for Investigating and Modelling Environmental Problems,10.1155/2017/7430658,https://core.ac.uk/download/211797442.pdf,'Hindawi Limited',"[{'title': 'Mathematical Problems in Engineering', 'identifiers': ['1563-5147', 'issn:1563-5147']}]",core
303787830,2018-01-01T00:00:00,"Irrigation management is a considerable time investment for many sugarcane farmers. Better irrigation practices can lead to improved yields through less water stress and reduced water usage to deliver economic benefits for farmers. In some cases reduced runoff and deep drainage from excess irrigation can also deliver benefits to the environment. The Internet of Things (IoT) is about allowing things to sense, to communicate, and thus create opportunities for more direct integration between the physical world and computer-based systems. IoT has been transforming all spheres of life into smart homes, smart cities, and smart healthcare. Today’s farms can leverage IoT to remotely monitor sensors, manage and control harvesters and irrigation equipment, and utilise artificial intelligence based analytics to quickly analyse operational data combined with third party information, to provide new insights and improve decision-making. This project focuses on improving irrigation management by integrating the auto-irrigation system (e.g., WiSA) and IrrigWeb (a sugarcane irrigation scheduling tool) to provide a smarter irrigation solution using IoT. The system generates a two-way communication channel between these two platforms, which allows them to share data. Specifically, the uplink program (WiSA to IrrigWeb) was developed and deployed on a Burdekin farm. It connects the farmer’s WiSA to IrrigWeb, by uploading irrigation data automatically. The farmer’s irrigation records are automatically loaded into IrrigWeb. This saves the farmer time and makes irrigation scheduling more efficient. Another benefit is that the farmer can now see the exact amount of water being applied to each field and make modifications to the irrigation management, if required. Moreover, automating the data transfer from WiSA to IrrigWeb will greatly improve the potential for uptake and use of technologies like IrrigWeb. On the other hand, the downlink program (IrrigWeb to WiSA) will be developed to automatically apply scheduling from IrrigWeb to WiSA. Combining the uplink and downlink programs, a smarter irrigation management system can automatically control sugarcane irrigation, and ultimately make sugarcane irrigation fully autonomous",Internet of things for smarter irrigation in Australian sugarcane,,,Informa Agra,,core
224433859,2018-01-01T00:00:00,"The field of optimal transportation is a broad area of theory pertaining to the computation of a mapping to transform one probability distribution into another.  Many computational strategies solving the transport problem spanning decades of research have led to several interesting applications in areas such as statistics, economics, machine learning and computer science.  However, there also exist many limitations to these algorithms, including the ever-present difficulty of scaling to larger datasets, both with respect to dimension and number of available samples from which we would like to extract useful information.  Furthermore, although we have seen a dramatic increase in the availability of powerful distributed computational resources throughout the past few decades, such as publicly available CPU clusters and GPU compute nodes, many modern algorithmic approaches to the transport problem are not specifically designed with parallelism in mind to accommodate such frameworks.  In a world where the continuous interaction between human users and distributed computational resources drives our technologically modern lives, this capability is critical, especially with applications that are expected to work in real-time.Building upon previous research from our group, this work investigates a parallelized, computational problem to create optimal transport maps that is designed with the notion of remote scalability in mind; this work focuses on a CUDA-based implementation of the framework that utilizes GPU computational resources to accommodate analysis of data in higher dimensions not often seen in the field of computational optimal transportation, and that furthermore scale with the availability of additional hardware.  We will also present applications using this framework in several different areas of machine learning with an emphasis on human-computer interfaces, including a novel multi-user brain-computer interface, a classification problem pertaining to automated sleep-staging based on electroencephalographical (EEG) data, and an example of generative modeling using the MNIST dataset.  Finally, we'll discuss a future direction for application of this framework to preference-based deep reinforcement learning",Scalable Measure Transportation and Applications in Machine Learning and Human Computer Interfaces,,,"eScholarship, University of California",,core
201454620,2018-12-01T00:00:00,"Flood disasters have had a great impact on city development. Early flood warning systems (EFWS) are promising countermeasures against flood hazards and losses. Machine learning (ML) is the kernel for building a satisfactory EFWS. This paper first summarizes the ML methods proposed in this special issue for flood forecasts and their significant advantages. Then, it develops an intelligent hydroinformatics integration platform (IHIP) to derive a user-friendly web interface system through the state-of-the-art machine learning, visualization and system developing techniques for improving online forecast capability and flood risk management. The holistic framework of the IHIP includes five layers (data access, data integration, servicer, functional subsystem, and end-user application) and one database for effectively dealing with flood disasters. The IHIP provides real-time flood-related data, such as rainfall and multi-step-ahead regional flood inundation maps. The interface of Google Maps fused into the IHIP significantly removes the obstacles for users to access this system, helps communities in making better-informed decisions about the occurrence of floods, and alerts communities in advance. The IHIP has been implemented in the Tainan City of Taiwan as the study case. The modular design and adaptive structure of the IHIP could be applied with similar efforts to other cities of interest for assisting the authorities in flood risk management",Building an Intelligent Hydroinformatics Integration Platform for Regional Flood Inundation Warning Systems,10.3390/w11010009,,'MDPI AG',"[{'title': 'Water', 'identifiers': ['issn:2073-4441', '2073-4441']}]",core
195779112,2018-01-01T00:00:00,"According to statistics, every fifth married couple is faced with the inability to conceive a child. Male germ cells are very vulnerable, and the growing number of cases of male infertility confirms that in today's world there are many factors that affect the activity of spermatozoa and their number. But the important thing is not so much their quantity, but quality. The spermogram is an objective method of laboratory diagnosis, which allows to accurately assess the man’s ability to fertilize by analyzing ejaculate for a number of key parameters. Only a spermogram can answer the question of a possible male infertility and the presence of urological diseases. When constructing spermograms, it is important to determine not only the number of good spermatozoa, but also their morphology and mobility. Therefore, research and improvement of some stages of spermogramm is the purpose of the study. This article addresses the problem of classification of spermatozoa in good and bad ones, taking into account their mobility and morphology, using methods of machine learning. In order to implement the first stage of machine learning (with a teacher) in the graphic editor, educational specimens (training sample) were created. The training was implemented by three methods: the method of support vector machine, the logistic regression and the method of K - the nearest neighbors. As a result of testing, the method K - the nearest neighbors is chosen. At the testing stage, a sample of 15 different spermatozoa was used in different variations of rotation around their axis. The test sample did not contain specimens from the training sample and was formed taking into account the morphological characteristics of the spermatozoa, but did not copy them from the training sample. At the final stage of study, the program's functioningwas tested on real data.За статистикою, кожна п'ята подружня пара стикається з неможливістю зачаття дитини. Чоловічі статеві клітини дуже вразливі, зростаюче число випадків чоловічого безпліддя підтверджує, що в сучасному світі дуже багато чинників, які впливають і на активність сперматозоїдів і на їх кількість. Та важливою є не стільки їх кількість, скільки якість. Спермограма є об'єктивним методом лабораторної діагностики, що дозволяє максимально точно оцінити здатність до запліднення чоловіка, проаналізувавши еякулят за рядом найважливіших параметрів. Тільки спермограма здатна відповісти на питання про можливе чоловіче безпліддя та про наявність урологічних захворювань. При побудові спермограми, важливо визначати не тільки кількість добрих сперматозоїдів, але й їх морфологію та рухливість. Тому дослідження та вдосконалення деяких етапів спермограми і є метою дослідження. У даній статті вирішується задача класифікації сперматозоїдів на добрі та погані, з урахуванням їх рухливості та морфології, із застосуванням методів машинного навчання. Для реалізації першого етапу машинного навчання (з вчителем) у графічному редакторі були створені навчальні екземпляри (тренувальна вибірка). Навчання було реалізована трьома методами: методом опорних векторів, логістична регресія та метод К - найближчих сусідів. За результатами тестування обрано метод К - найближчих сусідів. На етапі тестування використовувалася вибірка з 15 різних сперматозоїдів в різних варіаціях обертання навколо своєї осі. Тестова вибірка не містила примірників з тренувальної вибірки і була сформована з урахуванням морфологічних особливостей сперматозоїдів, але не копіювала їх з тренувальної вибірки. На завершальному етапі навчання роботу програми було протестовано на реальних даних",Застосування методів машинного навчання для вирішення задачі аналізу біологічних даних,10.20998/2522-9052.2018.3.01,,'National Technical University Kharkiv Polytechnic Institute',,core
200670767,2018-10-05T08:46:28Z,"In this work, teams of small mobile robots are used to test hypotheses about cooperative transport by ants. This study attempts to explain a decrease in steady-state transport speed with increasing team size that was previously observed in the ant <i>Novomessor cockerelli</i>. Two models of one-dimensional collective towing are compared: one in which transporters with different maximum speeds pull the payload with continuous, variable forces, and another in which transporters with identical speeds pull with intermittent, unsynchronized forces. A statistical analysis of ant data supports the hypothesis that ants behave according to the first model, in which the steady-state transport speed is the maximum speed of the slowest teammate. By contrast, the ant data are not consistent with the second model, which predicts constant speed regardless of team size. To verify these predictions, the ant behaviours in each model are translated into decentralized controllers and implemented on teams of two to four robots. The controller for the first model incorporates a real-time reinforcement learning algorithm that successfully reproduces the observed relationship between ant team size and transport speed. The controller for the second model yields the predicted invariance of transport speed with team size. These results show the value of robotic swarms for testing mechanistic hypotheses about biological collectives",Explanation of the assumption the ant data is normally distributed for proper order statistic analysis. from Multi-robot replication of ant collective towing behaviours,10.6084/m9.figshare.7171844.v1,,,,core
190155923,2018,"A recently started joint activity under International Federation of Surveyors (FIG) Commission 9 (Valuation and the Management of Real Estate) and FIG Commission 7 (Cadastre and Land Management) has started development of an information model for the specification of valuation information maintained b y public authorities especially for property taxation. In this initiative, ISO 19152:2012 Land Administration Domain Model (LADM) has been taken as the basis for the development of a Valuation Information Model. A first version of the LADM Valuation Information Model was created based on standards, literature survey and data gained from questionnaires replied by the national delegates of FIG Commission 9 and FIG Commission 7. The conceptual model was represented through class diagrams of the Unified Modeling Language (UML). This paper describes the development of a prototype for the implementation of the conceptual model in terms of a Turkish case study. The main ai m of this paper is to assess and improve the proposed conceptual LADM Valuation Information Model. In the development part, the classes, attributes, constraints, cardinalities and relations between classes of the conceptual model were converted to technical (physical) model, namely the Oracle Spatial 11g database schema has been generated from the conceptual model definitions. The conceptual schema definitions were implemented into the database, whi ch next was loaded with sample datasets related to property valuation and taxation in Turkey. The sample data includes valuation units that are the subjects of recurrently levied property taxes in Turkey, such as unimproved urban parcel and parcel and improvements together as condominium property, valuation and taxation information of the valuation units in different years, and as well as geometries of valuation units. The technical model of LADM Valuation Information Model Turkish Country Profile has been tested in evaluation phase through SQL queries and visualization tools, respectively. In this phase, it is investigated that whether the both conceptual and technical models fulfill the needs of information management aspects of valuation activities for property taxation",A Database Implementation of LADM Valuation Information Model in Turkish Case Study,,,,,core
201392715,2018-11-01T00:00:00Z,"Abstract Machine learning has become an increasingly powerful tool for solving complex problems, and its application in public health has been underutilized. The objective of this study is to test the efficacy of a machine-learned model of foodborne illness detection in a real-world setting. To this end, we built FINDER, a machine-learned model for real-time detection of foodborne illness using anonymous and aggregated web search and location data. We computed the fraction of people who visited a particular restaurant and later searched for terms indicative of food poisoning to identify potentially unsafe restaurants. We used this information to focus restaurant inspections in two cities and demonstrated that FINDER improves the accuracy of health inspections; restaurants identified by FINDER are 3.1 times as likely to be deemed unsafe during the inspection as restaurants identified by existing methods. Additionally, FINDER enables us to ascertain previously intractable epidemiological information, for example, in 38% of cases the restaurant potentially causing food poisoning was not the last one visited, which may explain the lower precision of complaint-based inspections. We found that FINDER is able to reliably identify restaurants that have an active lapse in food safety, allowing for implementation of corrective actions that would prevent the potential spread of foodborne illness",Machine-learned epidemiology: real-time detection of foodborne illness at scale,10.1038/s41746-018-0045-1,,Nature Publishing Group,"[{'title': None, 'identifiers': ['2398-6352', 'issn:2398-6352']}]",core
211494876,2018-01-01T00:00:00,"The objective of this research is to develop more accurate, robust and reliable microscopic models. Anintegrated methodological framework based on non–parametric approaches is proposed for estimationof data–driven microscopic traffic simulation models. The methodology is implemented using differentma-chine learning techniques such as clustering, classification, locally weighted regression, splinefitting, Gaussian processes, Kernel support vector machines and neural networks. The methodology isdemonstrated using real trajectory data from three different sources and specifically an experimentfrom Naples, NGSIM data and non–lane disciplinary trajectory data from India. The focus is given oncar–following models and Gipps’ model, one of the most extensively used car–following models, iscalibrated against the same data in order to be used as a reference benchmark. Many parametersaffect driving behavior and it is explored how the performance of the models is improved by includingmore explanatory variables. Then, a practical and simple approach is developed and motivated for theonline calibration of microscopic traffic simulation models, which considers dynamic parameters forindividual drivers, in time and space. The model adapts to driving behavior in a rolling horizon andleads to less than 10% error in speed prediction even for ten steps into the future. This research alsoexamines the feasibility and the benefits of using data–driven models on mixed traffic trajectory data,including non–lane discipline and heterogeneity in vehicle types, common characteristics in cities indeveloping countries. Although typical car–following models are theoretically justified, data–drivenapproaches are more flexible and allow the easy incorporation of additional information to the processof speed estimation. The results indicate that data–driven models could ensure reliability andimprovement in estimation of microscopic models.Στόχος της έρευνας είναι η ανάπτυξη πιο αξιόπιστων μικροσκοπικών κυκλοφοριακών προτύπων. Αναπτύσσεται μια ολοκληρωμένη μεθοδολογία για την εκτίμηση προτύπων κυκλοφοριακής προσομοίωσης με τη χρήση καινοτόμων και ευέλικτων μεθόδων μηχανικής μάθησης, όπως η ταξινόμηση, η ομαδοποίηση, η τοπικά σταθμισμένη παλινδρόμηση (loess), οι καμπύλες splines, οι Gaussian διαδικασίες, οι διανυσματικές μηχανές υποστήριξης και τα νευρωνικά δίκτυα. Τα δεδομένα που χρησιμοποιήθηκαν στην έρευνα αυτή περιλαμβάνουν δεδομένα από τρεις διαφορετικές πηγές, δεδομένα από τη Νάπολη, τα NGSIM δεδομένα και δεδομένα από την Ινδία. Δίνεται έμφαση στα πρότυπα ακολουθίας οχημάτων και για τα ίδια δεδομένα εφαρμόζεται το μοντέλο του Gipps, ένα γνωστό μοντέλο ακολουθίας οχημάτων που χρησιμοποιείται ως μοντέλο αναφοράς στην παρούσα έρευνα. Επειδή πολλοί παράγοντες επηρεάζουν τη συμπεριφορά του οδηγού, εξετάζεται κατά πόσο βελτιώνεται το μοντέλο ενσωματώνοντας περισσότερες μεταβλητές. Επιπλέον, εξετάζεται η δυναμική βαθμονόμηση κυκλοφοριακών προτύπων λαμβάνοντας υπόψη τη δυναμική μεταβολή των παραμέτρων για κάθε οδηγό, στον χρόνο και το χώρο. Οι παράμετροι μεταβάλλονται σε έναν κυλιόμενο χρονικό ορίζοντα και επιτυγχάνεται πρόβλεψη της ταχύτητας έως 10% για δέκα βήματα στο μέλλον. Διερευνάται η χρήση μοντέλων καθοδηγούμενων από τα δεδομένα σε συνθήκες μεικτής κυκλοφορίας χωρίς τήρηση των λωρίδων κυκλοφορίας και με μεγάλη ποικιλία ως προς τον τύπο των οχημάτων, κοινά χαρακτηριστικά των αναπτυσσόμενων χωρών. Αν και τα κλασσικά πρότυπα ακολουθίας οχημάτων είναι θεωρητικά τεκμηριωμένα, τα πρότυπα βασισμένα σε δεδομένα προσφέρουν μεγαλύτερη ευελιξία και επιτρέπουν την εύκολη ενσωμάτωση νέων μεταβλητών. Τα αποτελέσματα υποδεικνύουν ότι τα πρότυπα που βασίζονται σε δεδομένα μπορούν να συμβάλλουν στην εκτίμηση πιο αξιόπιστων μικροσκοπικών προτύπων",Προς την ανάπτυξη ευέλικτων μικροσκοπικών κυκλοφοριακών προτύπων βασισμένων σε δεδομένα,,,Εθνικό Μετσόβιο Πολυτεχνείο (ΕΜΠ),,core
159314970,2018-06-07T00:00:00,"While the idea of a city built for people is gaining more and more acceptance today, many of our urban environments remain focused and built around the car. Currently, day-to-day life in the city means the frequent interaction between pedestrians and drivers, a situation which can be dangerous, or, at worst, deadly.


This project, a collaboration between the Complex Systems group at IN3 (CoSIN3) of the Universitat Oberta de Catalunya (UOC), the Dirección General de Tráfico (DGT) and the Guàrdia Urbana de Barcelona, aims to quantify the issue of car-pedestrian collisions by characterising specific street areas with an indicator of pedestrian safety based on the structural properties of the street. Concretely, the project will generate this safety index for Spain’s two largest cities, Madrid and Barcelona, but the methodology and pipeline are applicable theoretically to any urban setting.


As a base unit for measuring pedestrian safety over space, the total pedestrian area of the city (sidewalks and crossings) will be tessellated into small, regular segments. Each of these segments will be assigned various indicator values, from simple geometric properties (distance to the closest pedestrian crossing; width of sidewalk) to more complex measures such as driver visibility.


Geometric operations to arrive at these values are performed on a PostGIS-build geo-database, over a variety of data, including street, sidewalk and block geometries, from diverse sources of open GIS data (Instituto Geográfico Nacional, Institut Cartogràfic i Geològic de Catalunya, OpenStreetMap, municipal data sources).Visibility values will be derived from a combination of deep learning technologies with GIS. A deep learning architecture will deliver computer-segmented street-scene images from Google Streetview. Each labeled image will be paired with an image from a simplified 3-dimensional model of the city, replicating its point of view (rendered with open-source 3D mapping software). The model will be clean of all street features (parked cars, trees, etc.) besides sidewalks and buildings. Comparison between the real and simplified images will thus permit the identification of sidewalk areas invisible to drivers due to visual obstructions.


The results of the project will be presented as online “heatmap” visualisations of safety indexes for the focus cities, open to the public for browsing and research. Additionally, a purpose-built API (Application Programming Interface) will provide public and private organisations working in the area of traffic safety access to the results for integration in their own internal or public application",Espacio persona: Big data to make urban streets safer,,,Universitat de Girona. Servei de Sistemes d'Informació Geogràfica i Teledetecció,,core
157860604,2018-05-10T00:00:00,"With rapid development of computer vision and artificial intelligence, cities are becoming more and more intelligent. Recently, since intelligent surveillance was applied in all kind of smart city services, object tracking attracted more attention. However, two serious problems blocked development of visual tracking in real applications. The first problem is its lower performance under intense illumination variation while the second issue is its slow speed. This paper addressed these two problems by proposing a correlation filter based tracker. Fog computing platform was deployed to accelerate the proposed tracking approach. The tracker was constructed by multiple positions&#x0027; detections and alternate templates (MPAT). The detection position was repositioned according to the estimated speed of target by optical flow method, and the alternate template was stored with a template update mechanism, which were all computed at the edge. Experimental results on large-scale public benchmark datasets showed the effectiveness of the proposed method in comparison with state-of-the-art methods",Object Tracking in Vary Lighting Conditions for Fog based Intelligent Surveillance of Public Spaces,10.1109/ACCESS.2018.2834916,https://core.ac.uk/download/157860604.pdf,'Institute of Electrical and Electronics Engineers (IEEE)',,core
212997673,2018-04-24T07:00:00,"Intelligent vehicle technologies are growing  rapidly that can enhance road safety, improve  transport efficiency, and aid driver operations  through sensors and intelligence. Advanced driver  assistance system (ADAS) is a common platform of  intelligent vehicle technologies. Many sensors  like LiDAR, radar, cameras have been deployed on  intelligent vehicles. Among these sensors,  optical cameras are most widely used due to their  low costs and easy installation. However, most  computer vision algorithms are complicated and  computationally slow, making them difficult to be  deployed on power constraint systems. This  dissertation investigates several mainstream ADAS  applications, and proposes corresponding  efficient digital circuits implementations for  these applications. This dissertation presents  three ways of software / hardware algorithm  division for three ADAS applications: lane  detection, traffic sign classification, and  traffic light detection. Using FPGA to offload  critical parts of the algorithm, the entire  computer vision system is able to run in real  time while maintaining a low power consumption  and a high detection rate. Catching up with the  advent of deep learning in the field of computer  vision, we also present two deep learning based  hardware implementations on application specific  integrated circuits (ASIC) to achieve even lower  power consumption and higher accuracy.
The real time lane detection system is  implemented on Xilinx Zynq platform, which has a  dual core ARM processor and FPGA fabric. The  Xilinx Zynq platform integrates the software  programmability of an ARM processor with the  hardware programmability of an FPGA. For the lane  detection task, the FPGA handles the majority of  the task: region-of-interest extraction, edge  detection, image binarization, and hough  transform. After then, the ARM processor takes in  hough transform results and highlights lanes  using the hough peaks algorithm. The entire  system is able to process 1080P video stream at a  constant speed of 69.4 frames per second,  realizing real time capability.
An efficient system-on-chip (SOC) design which  classifies up to 48 traffic signs in real time is  presented in this dissertation. The traditional  histogram of oriented gradients (HoG) and support  vector machine (SVM) are proven to be very  effective on traffic sign classification with an  average accuracy rate of 93.77%. For traffic sign  classification, the biggest challenge comes from  the low execution efficiency of the HoG on  embedded processors. By dividing the HoG  algorithm into three fully pipelined stages, as  well as leveraging extra on-chip memory to store  intermediate results, we successfully achieved a  throughput of 115.7 frames per second at 1080P  resolution. The proposed generic HoG hardware  implementation could also be used as an  individual IP core by other computer vision  systems.
A real time traffic signal detection system is  implemented to present an efficient hardware  implementation of the traditional grass-fire blob  detection. The traditional grass-fire blob  detection method iterates the input image  multiple times to calculate connected blobs. In  digital circuits, five extra on-chip block  memories are utilized to save intermediate  results. By using additional memories, all  connected blob information could be obtained  through one-pass image traverse. The proposed  hardware friendly blob detection can run at 72.4  frames per second with 1080P video input.  Applying HoG + SVM as feature extractor and  classifier, 92.11% recall rate and 99.29%  precision rate are obtained on red lights, and  94.44% recall rate and 98.27% precision rate on  green lights.
Nowadays, convolutional neural network (CNN)  is revolutionizing computer vision due to  learnable layer by layer feature extraction.  However, when coming into inference, CNNs are  usually slow to train and slow to execute. In  this dissertation, we studied the implementation  of principal component analysis based network  (PCANet), which strikes a balance between  algorithm robustness and computational  complexity. Compared to a regular CNN, the PCANet  only needs one iteration training, and typically  at most has a few tens convolutions on a single  layer. Compared to hand-crafted features  extraction methods, the PCANet algorithm well  reflects the variance in the training dataset and  can better adapt to difficult conditions. The  PCANet algorithm achieves accuracy rates of 96.8%  and 93.1% on road marking detection and traffic  light detection, respectively. Implementing in  Synopsys 32nm process technology, the proposed  chip can classify 724,743 32-by-32 image  candidates in one second, with only 0.5 watt  power consumption.
In this dissertation, binary neural network  (BNN) is adopted as a potential detector for  intelligent vehicles. The BNN constrains all  activations and weights to be +1 or -1. Compared  to a CNN with the same network configuration, the  BNN achieves 50 times better resource usage with  only 1% - 2% accuracy loss. Taking car detection  and pedestrian detection as examples, the BNN  achieves an average accuracy rate of over 95%.  Furthermore, a BNN accelerator implemented in  Synopsys 32nm process technology is presented in  our work. The elastic architecture of the BNN  accelerator makes it able to process any number  of convolutional layers with high throughput. The  BNN accelerator only consumes 0.6 watt and  doesn\u27t rely on external memory for storage",Computer Vision System-On-Chip Designs  for Intelligent Vehicles,,https://core.ac.uk/download/212997673.pdf,Digital WPI,,core
201556333,2018-12-01T00:00:00,"Authentication systems based on biometrics characteristics and data represents one of the most important trend in the evolution of the society, e.g., Smart City, Internet-of-Things (IoT), Cloud Computing, Big Data. In the near future, biometrics systems will be everywhere in the society, such as government, education, smart cities, banks etc. Due to its uniqueness, characteristic, biometrics systems will become more and more vulnerable, privacy being one of the most important challenges. The classic cryptographic primitives are not sufficient to assure a strong level of secureness for privacy. The current paper has several objectives. The main objective consists in creating a framework based on cryptographic modules which can be applied in systems with biometric authentication methods. The technologies used in creating the framework are: C#, Java, C++, Python, and Haskell. The wide range of technologies for developing the algorithms give the readers the possibility and not only, to choose the proper modules for their own research or business direction. The cryptographic modules contain algorithms based on machine learning and modern cryptographic algorithms: AES (Advanced Encryption System), SHA-256, RC4, RC5, RC6, MARS, BLOWFISH, TWOFISH, THREEFISH, RSA (Rivest-Shamir-Adleman), Elliptic Curve, and Diffie Hellman. As methods for implementing with success the cryptographic modules, we will propose a methodology which can be used as a how-to guide. The article will focus only on the first category, machine learning, and data clustering, algorithms with applicability in the cloud computing environment. For tests we have used a virtual machine (Virtual Box) with Apache Hadoop and a Biometric Analysis Tool. The weakness of the algorithms and methods implemented within the framework will be evaluated and presented in order for the reader to acknowledge the latest status of the security analysis and the vulnerabilities founded in the mentioned algorithms. Another important result of the authors consists in creating a scheme for biometric enrollment (in Results). The purpose of the scheme is to give a big overview on how to use it, step by step, in real life, and how to use the algorithms. In the end, as a conclusion, the current work paper gives a comprehensive background on the most important and challenging aspects on how to design and implement an authentication system based on biometrics characteristics",Security and Cryptographic Challenges for Authentication Based on Biometrics Data,10.3390/cryptography2040039,,'MDPI AG',"[{'title': 'Cryptography', 'identifiers': ['2410-387x', 'issn:2410-387X']}]",core
215548120,2018-04-20T07:00:00,"There is ever increasing disparity between number of organs needed for transplantation and numbers available for donation to save lives. As a result, thousands of people die every year waiting for organs. Therefore, it is now more important than ever before to take serious actions to decrease this disparity. One way to bridge gap between organ demand and supply is to increase family consent for organ donation. This research studied the factors associated with family consent. Machine Learning approach had been used in very few literature to understand factors related to family consent. This study uses six Ensemble Machine Learning models to accurately predict family consent outcome (yes/no). All family approaches data between January 2016 and March 2018 from an Organ Procurement Organization (OPO) based in New York city is used to build the family consent prediction model. The experimental results reveals that eXtreme Gradient Boosting (XGB) Machine Learning model performs better than other ensemble models with AUC of 0.8946 and accuracy of 81.7% after normalizing features and using LDA for dimension reduction and then tuning parameters using grid search method. 24 out of 29 features are identied as important features by XGB model. The model is used to calculate probability of consent before approaching family as the values for dierent features are available real-time after patient is referred to OPO for medical evaluation and suitability. The experimental result shows that the accuracy of the model increases from 77.6% to 91.5% as value for factors are added real-time. This model is also used for selecting the best sta for a particular case to approach family based on their past experience. Sta work schedule is incorporated with the model to select the top three sta based on likelihood of getting consent from family for organ donation. This recommendation system can be used as a potential sta dispatch model for OPO to further improve the consent from family for organ donation and save more lives by customizing the sta deployment procedure based on the characteristics of donor referral",Ensemble Machine Learning to Predict Family Consent for Organ Donation,,https://core.ac.uk/download/215548120.pdf,The Open Repository @ Binghamton (The ORB),,core
151251709,2018-02-17T00:00:00,"Rapid developments in hardware, software, and communication technologies have
allowed the emergence of Internet-connected sensory devices that provide
observation and data measurement from the physical world. By 2020, it is
estimated that the total number of Internet-connected devices being used will
be between 25 and 50 billion. As the numbers grow and technologies become more
mature, the volume of data published will increase. Internet-connected devices
technology, referred to as Internet of Things (IoT), continues to extend the
current Internet by providing connectivity and interaction between the physical
and cyber worlds. In addition to increased volume, the IoT generates Big Data
characterized by velocity in terms of time and location dependency, with a
variety of multiple modalities and varying data quality. Intelligent processing
and analysis of this Big Data is the key to developing smart IoT applications.
This article assesses the different machine learning methods that deal with the
challenges in IoT data by considering smart cities as the main use case. The
key contribution of this study is presentation of a taxonomy of machine
learning algorithms explaining how different techniques are applied to the data
in order to extract higher level information. The potential and challenges of
machine learning for IoT data analytics will also be discussed. A use case of
applying Support Vector Machine (SVM) on Aarhus Smart City traffic data is
presented for a more detailed exploration.Comment: Digital Communications and Networks (2017",Machine learning for Internet of Things data analysis: A survey,10.1016/j.dcan.2017.10.002,http://arxiv.org/abs/1802.06305,'Elsevier BV',,core
151121414,2018-02-01T00:00:00Z,"This article proposes a new general approach in short-term water demand forecasting based on a two-stage learning process that couples time-series clustering with gene expression programming (GEP). The approach was tested on the real life water demand data of the city of Milan, in Italy. Moreover, multi-scale modeling using a series of head-time was deployed to investigate the optimum temporal resolution under study. Multi-scale modeling was performed based on rearranging hourly based patterns of water demand into 3, 6, 12, and 24 h lead times. Results showed that GEP should receive more attention among the emerging nonlinear modelling techniques if coupled with unsupervised learning algorithms in detailed spherical k-means","Gene Expression Programming Coupled with Unsupervised Learning: A Two-Stage Learning Process in Multi-Scale, Short-Term Water Demand Forecasts",10.3390/w10020142,,MDPI AG,"[{'title': None, 'identifiers': ['issn:2073-4441', '2073-4441']}]",core
392173855,2018-01-01T00:00:00,"Open Access. Link to publishers version: https://www.taylorfrancis.com/books/e/9781351174657/chapters/10.1201%2F9781351174664-316As a result of the digitalization of the power business in Norway and Europa, a lot of new possibilities and challenges arise. In 2014 an expert committee one outlined a proposal for the future grid company structure in Norway (Reiten, 2014). In addition, new technologies are being implemented in the system. Wind power, solar power, un-regulated small hydro power production, battery storage domestic and industrial and electrification of transport. Transmission System Operators (TSOs) have a responsibility to supply industry and communities with reliable electric power. However, the operators have been virtually blind to slowly occurring changes in the load profile that reduce the expected regularity of the power supply. This paper will focus on the possibilities and challenges the power business are facing. The paper will describe what technologies is needed i.e Real time probabilistic risk calculations, artificial intelligence, machine learning and smart grid technology. The main question is: can the power business and the introduction of new system tools manage without probabilistic risk calculation for making use of the digitalization and the corresponding big data",Digitalization of the power business: How to make this work?,,,Taylor&Francis Group,,core
301379098,2018-12-06T08:00:00,"Smart tourism destination as: an innovative tourist destination, built on an infrastructure of state-of-the-art technology guaranteeing the sustainable development of tourist areas, accessible to everyone, which facilitates the visitor’s interaction with and integration into his or her surroundings, increases the quality of the experience at the destination, and improves residents’ quality of life. Lopez de Avila (2015). Smart tourism involves multiple components and layers of “smart” include (1) Smart Destinations which was special cases of smart cities integration of ICT’s into physical infrastructure, (2) Smart experience which specifically focus on technology-mediated tourism experience and their engagement through personalization, context-awareness and real-time monitoring, (3) Smart business refer to the complex business ecosystem that creates and supports the exchange of touristic resource and the co-creation of tourism experience. Gretzel et al, (2015). Smart tourism also clearly relies on the ability to not only collect enormous of data but to intelligently store, process, combine, analyze and use big data to inform business innovation, operations and services by artificial intelligence and big data technique. The rapid development of information communication technology (ICT) such as artificial intelligent, cloud computing, mobile device, big data mining and social media cause computing, storage and communication relevant software and hardware popular. Facebook, Amazon, Apple, Microsoft and Google have risen rapidly since 2000. In recent years, Emerging technologies such as Artificial Intelligence, Internet of Thing, Robotic, Cyber Security, 3D printer and Block chain also accelerate the development of industry toward digital transformation trend such as Fintech, e-commerce, smart cities, smart tourism, smart healthcare, smart manufacturing... This study proposes a conceptual framework that integrates (1) artificial intelligence/machine learning, (2) institution/organizational and (3) business processes to assist smart tourism stake holder to leverage artificial intelligence to integrate cross-departmental business and streamline key performance metrics to build a business-level IT Strategy. Artificial intelligence as long as the function includes (1) Cognitive engagement to (voice/pattern recognition function) (2) Cognitive process automation (Robotic Process Automation) (3) Cognitive insight (forecast, recommendation)",Artificial Intelligence in Smart Tourism: A Conceptual Framework,,https://core.ac.uk/download/301379098.pdf,AIS Electronic Library (AISeL),,core
270138430,2018-08-01T07:00:00,"Rapid developments in hardware, software, and communication technologies have facilitated the emergence of Internet-connected sensory devices that provide observations and data measurements from the physical world. By 2020, it is estimated that the total number of Internet-connected devices being used will be between 25 and 50 billion. As these numbers grow and technologies become more mature, the volume of data being published will increase. The technology of Internet-connected devices, referred to as Internet of Things (IoT), continues to extend the current Internet by providing connectivity and interactions between the physical and cyber worlds. In addition to an increased volume, the IoT generates big data characterized by its velocity in terms of time and location dependency, with a variety of multiple modalities and varying data quality. Intelligent processing and analysis of this big data are the key to developing smart IoT applications. This article assesses the various machine learning methods that deal with the challenges presented by IoT data by considering smart cities as the main use case. The key contribution of this study is the presentation of a taxonomy of machine learning algorithms explaining how different techniques are applied to the data in order to extract higher level information. The potential and challenges of machine learning for IoT data analytics will also be discussed. A use case of applying a Support Vector Machine (SVM) to Aarhus smart city traffic data is presented for a more detailed exploration",Machine Learning for Internet of Things Data Analysis: A Survey,,https://core.ac.uk/download/270138430.pdf,Scholar Commons,,core
334485034,2018-03-01T00:00:00,"The use of flying platforms such as unmanned aerial vehicles (UAVs), popularly known as drones, is rapidly growing in a wide range of wireless networking applications. In particular, with their inherent attributes such as mobility, flexibility, and adaptive altitude, UAVs admit several key potential applications in wireless systems. On the one hand, UAVs can be used as aerial base stations to enhance coverage, capacity, reliability, and energy efficiency of wireless networks. For instance, UAVs can be deployed to complement existing cellular systems by providing additional capacity to hotspot areas as well as to provide network coverage in emergency and public safety situations. On the other hand, UAVs can operate as flying mobile terminals within the cellular networks. Such cellular-connected UAVs can enable a wide range of key applications expanding from real-time video streaming to item delivery. Despite the several benefits and practical applications of using UAVs as aerial wireless devices, one must address many technical challenges. In this paper, a comprehensive tutorial on the potential benefits and applications of UAVs in wireless communications is presented. Moreover, the important challenges and the fundamental tradeoffs in UAV-enabled wireless networks are thoroughly investigated. In particular, the key UAV challenges such as three-dimensional (3D) deployment, performance analysis, air-to-ground channel model-ing, and energy efficiency are explored along with representative results. Then, fundamental open problems and potential research directions pertaining to wireless communications and networking with UAVs are introduced. To cope with the open research problems , various analytical frameworks and mathematical tools such as optimization theory, machine learning, stochastic geometry, transport theory, and game theory are described. The use of such tools for addressing unique UAV problems is also presented. In a nutshell, this tutorial provides key guidelines on how to analyze, optimize, and design UAV-based wireless communication systems","A Tutorial on UAVs for Wireless Networks: Applications, Challenges, and Open Problems",,,HAL CCSD,,core
201844826,2018-08-01T00:00:00,"Rapid developments in hardware, software, and communication technologies have facilitated the emergence of Internet-connected sensory devices that provide observations and data measurements from the physical world. By 2020, it is estimated that the total number of Internet-connected devices being used will be between 25 and 50 billion. As these numbers grow and technologies become more mature, the volume of data being published will increase. The technology of Internet-connected devices, referred to as Internet of Things (IoT), continues to extend the current Internet by providing connectivity and interactions between the physical and cyber worlds. In addition to an increased volume, the IoT generates big data characterized by its velocity in terms of time and location dependency, with a variety of multiple modalities and varying data quality. Intelligent processing and analysis of this big data are the key to developing smart IoT applications. This article assesses the various machine learning methods that deal with the challenges presented by IoT data by considering smart cities as the main use case. The key contribution of this study is the presentation of a taxonomy of machine learning algorithms explaining how different techniques are applied to the data in order to extract higher level information. The potential and challenges of machine learning for IoT data analytics will also be discussed. A use case of applying a Support Vector Machine (SVM) to Aarhus smart city traffic data is presented for a more detailed exploration. Keywords: Machine learning, Internet of Things, Smart data, Smart Cit",Machine learning for internet of things data analysis: a survey,10.1016/j.dcan.2017.10.002,,'Elsevier BV',"[{'title': 'Digital Communications and Networks', 'identifiers': ['2352-8648', 'issn:2352-8648']}]",core
186275490,2018-10-09T00:00:00,"The development of smart cities and their fast-paced deployment is resulting
in the generation of large quantities of data at unprecedented rates.
Unfortunately, most of the generated data is wasted without extracting
potentially useful information and knowledge because of the lack of established
mechanisms and standards that benefit from the availability of such data.
Moreover, the high dynamical nature of smart cities calls for new generation of
machine learning approaches that are flexible and adaptable to cope with the
dynamicity of data to perform analytics and learn from real-time data. In this
article, we shed the light on the challenge of under utilizing the big data
generated by smart cities from a machine learning perspective. Especially, we
present the phenomenon of wasting unlabeled data. We argue that
semi-supervision is a must for smart city to address this challenge. We also
propose a three-level learning framework for smart cities that matches the
hierarchical nature of big data generated by smart cities with a goal of
providing different levels of knowledge abstractions. The proposed framework is
scalable to meet the needs of smart city services. Fundamentally, the framework
benefits from semi-supervised deep reinforcement learning where a small amount
of data that has users' feedback serves as labeled data while a larger amount
is without such users' feedback serves as unlabeled data. This paper also
explores how deep reinforcement learning and its shift toward semi-supervision
can handle the cognitive side of smart city services and improve their
performance by providing several use cases spanning the different domains of
smart cities. We also highlight several challenges as well as promising future
research directions for incorporating machine learning and high-level
intelligence into smart city services.Comment: 7 pages, 5 figures and 1 table. Final version is published in IEEE
  Communications Magazin","Enabling Cognitive Smart Cities Using Big Data and Machine Learning:
  Approaches and Challenges",10.1109/MCOM.2018.1700298,http://arxiv.org/abs/1810.04107,'Institute of Electrical and Electronics Engineers (IEEE)',,core
162631921,2018-01-01T00:00:00,"Quantifying the effects of competition for natural resources between different sectors and interests is a key part of natural resource management globally. A major form of land use conflict in natural forests is between water production and timber production. Here we explore trade-offs in water yield resulting from logging in the forested water catchments north-east of Melbourne – the second largest urban settlement in Australia with a current population of five million. It has long been understood that logging significantly decreases water yields in Melbourne’s water catchments. However, the extent of losses of water yield from past logging have rarely been documented. Here, we model changes in water yield in Melbourne’s largest single catchment, the Thomson Catchment, resulting from: (1) past forest management activities (especially clearfell logging), and (2) future forest management scenarios. Our particular focus was on the effects of logging on water yields from ash-type eucalypt forests. This is because these areas have the greatest impact on water runoff due to them receiving the most rainfall and being the forest types subject to the most intensive and extensive industrial logging. We modelled four key scenarios: 
	Scenario (1) Historical logging of the Thomson Catchment with continued logging in the future (current reality/status quo); 
	Scenario (2) If there had been no logging and none was planned (past, present or future) in the Thomson Catchment; 
	Scenario (3) Logging ceasing in 1967 (as specified under the first Wood Pulp Agreement Act 1936 – but which never occurred); and
	Scenario (4) Impacts of the past logging, but with cessation of logging in 2018. 
Our initial spatial analysis revealed that 42% of the ash-type eucalypt forests in the Thomson Catchment have been logged. Moreover, there are 4,000 hectares of Ash forest assigned for logging in the next 5 years under the existing Timber Release Plan for the Central Highlands region. Our analyses revealed that the current (in 2018) reduction in water yield due to historical logging of the ash forests across the Thomson Catchment exceeds 15,000 ML annually. This loss is projected to increase to nearly 35,156 ML by 2050. Under Scenario (3), where logging would have ceased in 1967 if the first Wood Pulp Agreement 1936 was implemented, the loss in water yield by 2018 was projected to be 1,079 ML, annually. This loss is a result of logging occurring prior to 1967. This was modelled to remain constant through to 2050. Under Scenario (4), where logging ceases in 2018, we projected that approximately 20,149 ML would have been returned to the Thomson Catchment by 2050 compared with Scenario (2) of no historical logging. Losses in water yield as a result of logging correspond to 9%-20% of the ash forest catchment water yield for 2018 and 2050, respectively. Based on an estimated consumption of 161 litres of water per person per day, the loss in water yield resulting from logging would equate to the lost water for nearly 600,000 people by 2050. 
Given the strategic importance of water from the Thomson Catchment, our analyses suggest that native forest logging should be excluded from this catchment, particularly in the context of increasing human consumption of water and decreasing stream inflows from the catchments. Previous work has shown that the economic value of the water across all of Melbourne’s Water Catchments, including the Thomson Catchment, is 25.5 times greater than the economic value of the timber produced from the all native forests, based on integrated economic and environmental accounting (e.g. under the System of Environmental and Economic Accounting [SEEA] developed by the United Nations). It is not the difference in value between water and timber that is important, it is the change due to the use of an ecosystem service, resulting in the reduction of water yield. Therefore, we suggest that ongoing logging of the Thomson Catchment, when it is known to reduce water yields, is a questionable natural resource management policy.This research was partly supported by the National Environmental Science Program Threatened Species Recovery Hub",Resource Conflict Across Melbourne’s Largest Domestic Water Supply Catchment,10.25911/5beb630e45d35,https://core.ac.uk/download/162631921.pdf,The Australian National University. Fenner School of Environment and Society,,core
157733738,2018-12-13T00:00:00,"End-to-end approaches to autonomous driving have high sample complexity and
are difficult to scale to realistic urban driving. Simulation can help
end-to-end driving systems by providing a cheap, safe, and diverse training
environment. Yet training driving policies in simulation brings up the problem
of transferring such policies to the real world. We present an approach to
transferring driving policies from simulation to reality via modularity and
abstraction. Our approach is inspired by classic driving systems and aims to
combine the benefits of modular architectures and end-to-end deep learning
approaches. The key idea is to encapsulate the driving policy such that it is
not directly exposed to raw perceptual input or low-level vehicle dynamics. We
evaluate the presented approach in simulated urban environments and in the real
world. In particular, we transfer a driving policy trained in simulation to a
1/5-scale robotic truck that is deployed in a variety of conditions, with no
finetuning, on two continents. The supplementary video can be viewed at
https://youtu.be/BrMDJqI6H5UComment: Accepted at Conference on Robotic Learning (CoRL'18)
  http://proceedings.mlr.press/v87/mueller18a.htm",Driving Policy Transfer via Modularity and Abstraction,,http://arxiv.org/abs/1804.09364,,,core
234970498,2018-03-14T00:00:00,"The Reporter is a publication produced by Western Carolina University featuring news, events, and campus community updates for faculty and staff. The publication began in August of 1970 and continues digitally today. Click on the link in the “Related Materials” field to access recent issues.THE REPORTER A Weekly Newsletter for the Faculty and Staff of Western Carolina University * Cullowhee, N.C. * November 1, 1976
WCU TEACHER CORPS PIONEERS IN USE OF COMPUTER TO TEACH MATH TO MIDDLE-GRADE CHILDREN
EdLLtoA'A no te,: The WCU TeacheA
ColpA this ifQjxA. kaA IntAodaced
computeA-aA&iAtexl InAtAuction at Log
Cabin ElementaAy School. AI BloeAeA,
TeacheA Coip6 aA&l6tant diAectoA, haA
pAcpaAed thl& Aepoit o& the pAoject.
By Alfred H. Bloeser
Western Carolina University
Teacher Corps
Gone are the days of dull, old
math drill because ""Charlie Computer""
now teaches the class. No more mul­tiplication
tables to recite, or
choral responses to flash cards
because ""Charlie"" has taken it all
on.
You can see the excitement in
the students' eyes at Log Cabin Ele­mentary
School whenever ""Charlie""
begins his daily routine, because,
although ""Charlie"" is a computer
terminal and not a real person, he
does ""talk"" to the boys and girls
in Miss Pennington's and Mrs. Fore's
classes.
""Charlie"" begins a typical day
by being ""logged-in"" by his proctor,
Mrs. Toni Brooks. Through a compli­cated
system of telephone lines, a
computer, located at the Research
Triangle Park, midway between Raleigh,
Durham, and Chapel Hill, is contacted
and ""Charlie"" is readied for a full
day of work.
He asks his first charge of the
day to identify himself and then
calling on his ""memory and record
bank"" produces the appropriate math
lesson with a cheery, ""Hello, Sally.
Today's lesson will be on fractions.
Here we go.""
""Charlie"" has identified the
student, tested her to find the appro­priate
level of study, and monitored
her work so that today's problem will
be just what Sally needs.
Sally, who is fascinated by see­ing
her work on an electronic screen,
concentrates with all her effort to
catch ""Charlie"" asleep, if she can,
for the next ten minutes, and without
being aware of it learns a great deal
about adding and subtracting fractions
It all seems so simple when each step
is monitored and ""Charlie"" is right
there to show her where she makes a
mistake. One of these days she is
going to catch ""Charlie"" in a mistake,
but she needs a great deal of prac­tice
first.
At the end of the week ""Charlie""
reports that Sally has improved a
great deal but that she still makes
errors and needs more work. Perhaps
her teacher can help her with some
personalized instruction.
""Charlie"" is the talk of the Log
Cabin Elementary School, one of the
schools in the Jackson County school
system. ""Computer-assisted instruc­tion""
via the Hewlett-Packard Math
program, has been introduced to this
school through the Eleventh Cycle
Teacher Corps Project at Western
Carolina University.
This program is the only one of
its kind being used in North Carolina
elementary schools, according to Miss
Betsy Little of the North Carolina
Education Computing Service. The
students in the fourth, fifth, and
sixth grades meet ""Charlie"" every day
in order to give them the necessary
experience to raise their.grade ievel
scores in math and are advancing
rapidly.
""Computer-assisted instruction
will be a very challenging and reward­ing
experience,"" Miss Doris Pennington,
fifth- and sixth-grade teacher at Log
Cabin School, recently commented. Her
students are excited and intensely in­terested
in this new method of learn­ing.
Mrs. Patricia Fore, who teaches
fourth and fifth grades at the same
school, stated that she was very ex­cited
about the program, both as a
parent and as a teacher. ""Any child
is fortunate to participate in such a
program and I would love to have my
own boys involved in the math program,""
she said.
""Charlie"" is the special care of
Mrs. Toni Brooks at Log Cabin Elemen­tary
School. She is readily available
to assist any student who may have
some difficulty with the computer ter­minal,
or who may not understand what
is expected of him. She also is re­sponsible
for daily and weekly
reports on each student and keeping in
close contact with the student's tea­cher
so that weaknesses and need for
additional help can be noted.
""Charlie"" is capable of many
things. He can play games with the
students, give them more advanced work
to do, or make available to them many
types of educational programs. He can
be of special help to teachers, too,
by giving them the opportunity to
secure complete lessons, test, etc.,
in advance, as well as furnishing them
with complete records of their stu­dents
.
When students demonstrate compe­tence
in course material, they are
automatically skipped to a higher
level of difficulty or on to new mate­rial.
On the other hand, when a stu­dent
needs drill, appropriate review
lessons are given automatically. The
A REMINDER
The Computer Center is offering an
elementary short course covering enough
of the language and the system commands
to enable the novice user to run BASIC
programs on the Xerox 560. No previous
programming experience is required for
the course, which is being taught by
C. J. Duckenfield.
The course will meet Nov. 1, 3, 5,
and 8 from 3 to 4 p.m. in room 117 of
Forsyth Building.
teacher is kept informed of each
student's progress by a variety of
reports that are supplied on demand.
The Teacher Corps at Western Caro­lina
University, the Jackson County
School System, and the principal and
teachers at Log Cabin School are all
intensely excited over the possibili­ties
of computer-assisted education.
Although this is a pilot program,
there is every indication that it will
prove to be very beneficial to both
students and educators, and may very
well be the forerunner of a ""new""
approach to educating students on
the elementary level.
METROPOLITAN OPERA SOPRANO
TO SING HERE
Soprano Annette Parker will pre­sent
a recital with Lawrence Skrobacs,
pianist, and Lucy Cross, lutenist,
Thursday (Nov. 4) at 8:15 p.m. in
Hoey Auditorium.
The program will include songs
from the ""Italienisches Liederbuch""
of Hugo Wolf, ""Zaide"" by Hector
Berlioz, and ""Fiancailles Pour Rire"" .
by Poulenc. Also on the program are
songs by Dowland, Morley, Philip
Rosseter, Peter Mathews, and Fernando
Obradors.
A native of Charleston, S.C.,
Miss Parker went to New York as a
regional winner of the 1970 Metropoli­tan
Opera Auditions. She was immedi­ately
engaged for two nationwide tours
with the Columbia Operatic Trio, and
was later invited to join the Metro­politan
Opera Studio.
Since then she has performed with
many orchestras, including the Cleve­land
Orchestra, the New Orleans Sym­phony,
and the Opera Orchestra of New
York.
She has sung in operas in
Brussels, St. Paul, New Orleans, Fort
Worth, and at several summer festivals.
Last season she returned to Europe to
sing Eurydice in ""Orfeo"" with the
Netherlands Opera.
Miss Parker received her early
training at Converse College, and did
graduate work at the Villa Schifanoia
in Florence, Italy.
In addition to study grants from
the Metropolitan Opera National Coun­cil
and the Metropolitan Opera Studio,
Miss Parker has received three William
Matheus Sullivan awards and been a
finalist in several national competi­tions.
She made her Carnegie Recital
Hall debut in February of this year,
as the only singer chosen for a Con­cert
Artists Guild Award in the 1975
auditions.
Her performance here is sponsored
by the Lectures, Concerts, and Exhibi­tions
Committee. Admission will be
free to WCU students and subscription
series members of the LCE, $1 for
other adults, and 50 cents for
children.
ARTS AND SCIENCES
ADVISORY GROUP NAMED
John McCrone (dean, Arts and
Sciences) has appointed a Dean's
Student Advisory Committee within the
School of Arts and Sciences. The
Student Advisory Committee consists
of one student selected from each de­partment
in the School. The duty of
this committee is to advise the Dean
on matters of common interest and con­cern
to the students and student body.
The following students were
selected for Arts and Sciences Stu­dent
Advisory Committee for 1976-77:
Bryan McDowell, Art; Alice Bowers,
Biology; Orvilla Smith, Chemistry;
Peter Palmer, Earth Sciences; Susan
Allman, English; Mary Church, History;
Denise Lilley, Mathematics; Vincent
Clark, Physics; Stacy Rae Place,
Military Science; Sandra Crews, Mod­ern
Foreign Languages; Jacqueline
Culpepper, Music; Nancy Hope,
Political Science; Wanda Mull,
Social Work; Rob Daves, Sociology and
Anthropology; and Sam Gray, Speech
and Theatre Arts.
The committee already has held
its first meeting and will meet as
convenient during the quarter to
discuss items of interest and con­cern
to the students.
PIEDMONT BRASS QUINTET TO PERFORM
The Piedmont Brass Quintet, WCU's
resident instrumental ensemble, will
present its first home performance of
the season Nov. 11 at 8:15 p.m. in Hoey
Auditorium.
The program will include six se­lections
representing four centuries.
""Die Bankelsangerlieder"" is an anony­mous
work written around 1684, Hein-rich
Finck's ""Greiner Zanner"" dates
from the late 15th or early 16th cen­tury,
and ""Contrapunctus I"" from J. S.
Bach's ""Art of the Fugue"" is an early
18th century work.
Three 20th century works will
complete the program—Morley Calvert's
""Suite from the Monteregian Hills,""
Ingolf Dahl's ""Music for Brass In­struments""
(1944), and ""Sonatine""
(1951) by Eugene Bozza.
The members of the Piedmont Brass
Quintet are Mary Lazarus and Ned Gard­ner,
trumpet; Gordon Campbell, French
horn; John Woolley, trombone; and John
Sizemore. Each of them is a visiting
artist-in-residence at WCU, and serves
as a brass instructor in the Depart­ment
of Music.
The group's musical director is
Mary Lazarus.
The quintet was organized three
years ago in Winston-Salem, and chose
the name ""Piedmont"" to honor the peo­ple
of the region whose support
nurtured the group during its early
years.
The performance here is sponsored
by the Department of Music. There
will be no admission charge.
""HOT L BALTIMORE"" TO RUN HERE
""Hot 1 Baltimore,"" the Lanford
Wilson comedy that won the New York
Critics Circle Award, will be the Uni­versity
Players' next production. It
will be performed at 7:30 p.m. Novem­ber
11-13 and 15-18 in the Little
Theatre in Stillwell Science Building.
Stephen L. Carr, instructor in
speech and theatre arts, will direct
the production with the assistance of
Mike Gundy of High Point.
The play is set in the lobby of
the run-down Hotel Baltimore, so seedy
that the letter ""e"" is missing from
its marquee. The everyday encounters
of the hotel residents during the
course of one day form a mosaic of
human comedy.
Members of the cast include Beth
Thomas, Cathy Dixon, and Dennis West
of Cullowhee, David Dorsey of Sylva,
Carolyn Fulton of Franklin, Vanessa
Drake of Hendersonville, Tom Caudle of
Waynesville, Ken Stikeleather of Wax-haw,
Frank Joyner and Susan Davis of
Winston-Salem, Dan Spence of Roanoke
Rapids, Diana Marshall of Pfafftown,
Tonya Lamm of Wilson,' Tim Dickenson of
Greensboro, and Armando Erba of New
Haven, Conn.
Tickets are $2 for adults, $1 for
students, and 50 cents for children.
All seats are reserved, and tickets
are available from the Department of
Speech and Theatre Arts, telephone
293-7491.
MADRIGAL DINNER TICKETS
ON SALE MONDAY
The Madrigal Christmas Dinners,
a popular holiday entertainment at
WCU from 1970 through 1973, will be
resumed this year. Dinners will be
held in the Grandroom of Hinds Uni­versity
Center Nov. 30 and Dec. 1.
Admission will be $5 and will include
the cost of dinner and the musical
entertainment.
Tickets for this year's dinners
will go on sale Monday (Nov. 8) at 9
9 a.m. at the information desk of
Hinds University Centfer. Seating is
limited, and no reservations will be
accepted before Monday at 9 a.m. The
dinners have traditionally been sell­outs.
The Grandroom will accommodate
340 patrons each night. As in pre­vious
years, there will be 10 seats
to a table, and individuals may
purchase tickets singly or in groups.
For purposes of the madrigal
dinners, the Grandroom becomes an
Elizabethan banquet hall. The dinners
are served with wassail bowl, boar's
head, trumpet fanfares, and other
pageantry in the manner of the 16th
century.
The dinners have been cancelled
for the past two years because final
examinations were held the week after
Thanksgiving, when the dinners would
normally have been scheduled.
i/
KOINONIA HOUSE LUNCHEON
A ""Soup and Sandwich"" Luncheon
will be held from 12 until 12:5fr*p.m.
each Tuesday through November 16 at
the Koinonia House (Baptist Student
Union). A period of informal dia­logue
and sharing between faculty and
students will be held along with the
meal. Persons wishing to attend may
call 293-9030 from 1:00 p.m. Tuesday
until 5:30 p.m. the following Monday
to make reservations at the rate of
$1 per person.
The speaker for next Tuesday
(Nov. 9) will be Dr. Lewis Sutton,
and the luncheon will feature beef
or pork roast.
WITH THE FACULTY
Thomas B. Westcott (CAP Center)
recently attended the Southeastern
Conference of Counseling Center Per­sonnel
at Appalachian State University.
Or. Westcott presented a paper
entitled ""New Relationships for Ser­vice
to the Campus Community: The
Counseling, Advising and Placement
Center at Western Carolina University.""
The Department of Information
Systems hosted the North Carolina
Association for Educational Data Sys­tems
at A-B Tech in Asheville on
October 18. Drs. Beegle, Hunter,
Morton, and Woods attended the
conference. Dr. Hunter was elected
to a three-year term with the Board
of Directors. Dr. John Bennett
(Philosophy and Religion) spoke at
the luncheon on ""Colonel Elliott
White Springs: Industrial and
Literary Maverick.""
Bob Mason (School of Nursing and
Health Sciences) participated in a
workshop at the University of Kentucky
in Lexington during the week of
October 17. The workshop was designed
to instruct participants on the
development and implementation of
""Interdisciplinary Continuing Educa­tion
Programs in Allied Health.""
Wilma B. Cosper (head, Home
Economics) was elected Vice-Chairperson
of the Southwestern Child Development
Commission Board at a recent meeting
in Franklin.
Tom O'Toole (History) attended a
Guided Designs System Approach Work­shop
sponsored by the Exxon Education
Foundation Impact Program October 13-
15 at West Virginia University.
Bob Mason (Nursing and Health
Sciences) attended the statewide
meeting of the North Carolina Area
Health Education centers in Boone on
October 27 and 28 where he presented
a paper entitled ""AHEC and the Commu­nity
College.""
Donald Josif (Earth Sciences)
was coordinator for the North Carolina
Geographical Society's annual meeting,
October 22-23, in Asheville. WCU and
ASU were co-hosts for the meeting.
THE REPORTER A Weekly Newsletter for the Faculty and Staff of Western Carolina University * Cullowhee, N.C. * November 10, 1976
PRESENTATION OF DISTINGUISHED SERVICE, OTHER AWARDS MARKS 1976 HOMECOMING
The vice president of IBM's
European marketing and services opera­tions
and the president-elect of the
North Carolina Association of Educa­tors
won Western Carolina University's
Alumni Distinguished Service awards
Saturday night.
Raymond H. Fentriss of Paris,
France, a 1956 graduate, received the
award for service outside the field
of education and Mrs. Linda Israel
Rader of Gastonia, a Canton native
and a 1958 graduate, received the
award for service in the field of
education.
The presentations were made by
Dr. H. F. Robinson, chancellor, at
the Alumni Association annual
awards banquet in the Waynesville
Country Club.
The banquet climaxed the univer­sity's
1976 Homecoming celebrations
that included the selection of Miss
Melanie Ann Shore of Peachtree City,
Ga., as homecoming queen. She was
crowned by Chancellor Robinson at the
half-time of the WCU-Lenoir-Rhyne
football game. Other members of the
court afe Kimber'ly Cockman of Brown
Summit, Andrea Zaher of Pembroke
Pines, Fla., Allison Miller of Rock­well,
Patricia Mitchum of Monroe and
LeAnne West of Greenville (N.C.).
Bob Terrell, associate editor of
the Citizen-Times and well-known
columnist and author, a member of the
WCU class of 1951, spoke at the
evening banquet, recounting many of
the humorous experiences he has had
as a newspaperman.
Two other members of the 1951
class shared the podium with Terrell—
Herbert L. Hyde, Asheville attorney,
as master of ceremonies, and Charles
West of CBS news in New York, who was
in charge of presenting other special
awards.
Fentriss, who with his wife,
Jane, also a WCU graduate, came from
Paris for the occasion, joined IBM
the year he graduated and moved up
rapidly in the corporation. Four
years after starting with IBM as a
sales trainee, he was singled out by
A. K. Watson, president of IBM World
Trade Corporation to become his
administrative assistant.
In a series of appointments,
Fentriss became regional manager of
IBM operations in Germany, Switzer­land
and Austria, and was picked to
manage IBM's entry into the Eastern
European Socialist countries,
tice first.
In 1972, when the United States
and Russia embarked on ""detente,""
Fentriss began a study of the USSR
market and the next year was named to
head up the corporation's business
efforts in the Soviet Union. About a
year ago, he moved back to Paris to
assume the vice presidency in Europe.
Mrs. Rader, who earned both her
bachelor's and master's degrees at WCU,
has taught in the Gaston County and
Gastonia school systems since her
graduation. As president-elect of the
state's major educational asso­ciation,
she will take over its lead­ership
in April, 1977.
In 1962, she was a Fulbright
Exchange Program teacher in England
and later was chosen by the National
Education Association as one of the
outstanding women in education in the
United States, receiving that honor
for her role in elevating the stand­ards
of the teaching profession.
She has been a member of the
NCAE board of directors, member of
the General Assembly's special com­mittee
to study the teacher's retire­ment
system, a member of the Governor's
Study Committee on Relationships
Between Boards of Education and
Professional Educators, a member of the
State Board of Education Committee to
Study Sick Leave and Substitute
Teacher Regulations, a member and vice
chairman of the NCAE Professional
Rights and Responsibilities Commission,
vice president of the North Carolina
Association of Classroom Teachers,
president of the Gastonia Association
of Childhood Education, and a delegate
to the NEA national convention.
Two special awards made at the
banquet went to the alumni chapter
with the most members present from
the farthest distance—Florida
chapter—and the individual alumnus
traveling the greatest distance (other
than Fentriss) to attend the meeting—
Dr. Robert Failing of Santa Barbara,
Calif.
An additional ""Unsung Hero"" award
was presented to Doug Reed, WCU
Director of Public Information, for
selfless and dedicated"" service to
the alumni association.
Theme for this year's on-campus
homecoming parade and other activities
was ""Catstruction '76,"" in recognition
of several multi-million dollar build­ing
projects currently under way.
Professor Sue Fields was chairman of
the homecoming committee and alumni
activities were planned under the
direction of James A. Ballard, director
of development and alumni affairs, and
two staff members, Mrs. Jean Robinson
and Miss Toni Knott.
WCU'S HONORS PROGRAM—A FIRST REPORT
Most people, especially students,
would agree that it's hard to get pro­fessors
and students to agree.
But in a few special classes at
Western Carolina University, the stu­dents
are doing more talking, the
professors are doing more listening,
and both agree that the students are
coming away with more knowledge than
is usually gleaned in traditionally
structured courses.
These honors courses are five
beginning classes for freshmen in
chemistry, biology, philosophy, anth­ropology,
and history.
Dr. Ellerd M. Hulbert, interim
chairman of the honors faculty who is
teaching the history honors class
this quarter, said that one of the
reasons the honors program was begun
at WCU was the quality of students.
""Western Carolina is getting a
substantial number of high quality
students from high schools,"" the
history professor said, ""and we have
to insure that they are working at
their capacity.""
The honors students were invited
to take any of the courses on the
basis of their high SAT scores or
class rank.
Although Dr. Hulbert said it was
too early to appraise the effects of
the program, the faculty is encour­aged
and plans are being made to offer
upper level honors courses in subse­quent
years. Next year the honors
courses will be offered as the
present freshmen become sophomores,
so that in four years honors pro­grams
will be available during all
four years of undergraduate work.
Three of the five courses have a
second part—chemistry, biology, and
history—and the second part of the
sequence will be offered during the
winter quarter. Instead of anthro­pology,
an introductory sociology
honors course will be offered in the
winter. In the spring quarter,
honors courses in other departments
will be offered.
Dr. Hulbert said that WCU is
exploring the possibility of offer­ing
honors courses not only in the
School of Arts and Sciences but in
the economics and psychology depart­ments.
What makes the honors programs
different from other classes?
Dr. Patrick Morris, head of the
Department of Sociology and Anthro­pology,
said his class differs in at
least three ways.
""The class is smaller, for one
thing,"" he explained, ""and most of
the classroom time is spent in dis­cussion
rather than lecture.""
Dr. Morris also said there is
much more work for students in his
honors class to do.
He said his main objective in
the course i","The Reporter, November 1976",,,"Hunter Library Digital Collections, Western Carolina University, Cullowhee, NC 28723;",,core
478685821,2018-01-01T00:00:00,"As a result of the digitalization of the power business in Norway and Europa, a lot of new possibilities and challenges arise. In 2014 an expert committee one outlined a proposal for the future grid company structure in Norway (Reiten, 2014). In addition, new technologies are being implemented in the system. Wind power, solar power, un-regulated small hydro power production, battery storage domestic and industrial and electrification of transport. Transmission System Operators (TSOs) have a responsibility to supply industry and communities with reliable electric power. However, the operators have been virtually blind to slowly occurring changes in the load profile that reduce the expected regularity of the power supply. This paper will focus on the possibilities and challenges the power business are facing. The paper will describe what technologies is needed i.e Real time probabilistic risk calculations, artificial intelligence, machine learning and smart grid technology. The main question is: can the power business and the introduction of new system tools manage without probabilistic risk calculation for making use of the digitalization and the corresponding big data",Digitalization of the power business: How to make this work?,,,'Informa UK Limited',,core
304151969,2018-08-01T00:00:00,"Soil moisture modifies the state of the atmosphere and thus plays a major role in the climate system. Its spatial distribution is strongly modulated by the underlying orography. Yet the vertical transport of soil water and especially the generation of groundwater runoff at the bottom of the soil column are currently treated in a crude way in most atmospheric and climate models. This potentially leads to large biases in near‐surface temperatures during midlatitude summertime conditions, when the soils may dry out. Here we present a new formulation for groundwater and runoff formation. It is based on Richards equation, allows for saturated aquifers, includes a slope‐dependent groundwater discharge, and enables a subgrid‐scale treatment of the underlying orography. The proposed numerical implementation ensures a physically consistent treatment of the water fluxes in the soil column, using ideas from flux‐corrected transport methodologies. An implementation of this formulation into TERRA_ML, the land surface model of the regional climate model of the COnsortium for Small‐scale MOdeling (COSMO) in CLimate Mode (CCLM), is validated both in idealized and real‐case simulations. Idealized simulations demonstrate the important role of the lower boundary condition at the bottom of the soil column and display a physically meaningful recharge and discharge of the saturated zone. Validation against measurements at selected stations shows an improved seasonal evolution of soil water content. Finally, decade‐long climate simulations over Europe exhibit a realistic representation of the groundwater distribution across continental scales and mountainous areas, an improved annual cycle of surface latent heat fluxes, and as a consequence reductions of long‐standing biases in near‐surface temperatures in semiarid regions.ISSN:1942-246",A Groundwater and Runoff Formulation for Weather and Climate Models,10.1029/2017MS001260,,'American Geophysical Union (AGU)',,core
160783172,2018-08-13T00:00:00,"We present the design, implementation and evaluation of a system, called
MATRIX, developed to protect the privacy of mobile device users from location
inference and sensor side-channel attacks. MATRIX gives users control and
visibility over location and sensor (e.g., Accelerometers and Gyroscopes)
accesses by mobile apps. It implements a PrivoScope service that audits all
location and sensor accesses by apps on the device and generates real-time
notifications and graphs for visualizing these accesses; and a Synthetic
Location service to enable users to provide obfuscated or synthetic location
trajectories or sensor traces to apps they find useful, but do not trust with
their private information. The services are designed to be extensible and easy
for users, hiding all of the underlying complexity from them. MATRIX also
implements a Location Provider component that generates realistic
privacy-preserving synthetic identities and trajectories for users by
incorporating traffic information using historical data from Google Maps
Directions API, and accelerations using statistical information from user
driving experiments. The random traffic patterns are generated by
modeling/solving user schedule using a randomized linear program and
modeling/solving for user driving behavior using a quadratic program. We
extensively evaluated MATRIX using user studies, popular location-driven apps
and machine learning techniques, and demonstrate that it is portable to most
Android devices globally, is reliable, has low-overhead, and generates
synthetic trajectories that are difficult to differentiate from real mobility
trajectories by an adversary","Mitigating Location Privacy Attacks on Mobile Devices using Dynamic App
  Sandboxing",,http://arxiv.org/abs/1808.04490,,,core
185549869,2018,"The characterization of the melting layer (ML) is an important task for operational radar meteorology. Melting layer identification may be used to establish distances at which radar rainfall estimates become affected by melting hydrometeors, with benefits for operational hydrometeor classification, as snowfall, freezing rain and liquid precipitation. Consequently, this yields a significant performance improvement in radar-based quantitative precipitation estimation (QPE) on ground. Furthermore, knowledge of the ML location is also important for microphysical cloud characterization and the evaluation of icing potential. Several studies and algorithms exist in literature for characterizing ML through radar measurement signature, the most popular being the increase in the radar reflectivity factor known as bright. For dual polarization radar, ML detection can be pursued using specific signatures of dual polarization measurements, such as differential reflectivity ZDR, co-polar correlation coefficient \rhoHV , specific differential phase KDP, and linear depolarization ratio LDR that exhibit well-pronounced ML signatures both in stratiform and even in convective situations. Moreover, ML and rainfall have a strong influence on satellite links signals, as they are attenuated due to the presence of hydrometeors along the propagation path. In stratiform precipitation systems, the ML is the upper limit of the rainfall column height that is responsible of the attenuation of radio signals. Therefore, ML climatology is proficiently used for designing satellites links relative to a specific area. Very recently, a new X-band Doppler, dual-polarization weather radar system has been installed in Florence funded by the Tuscany Region Government within the NEFOCAST project. The latter investigates a new concept system that aims at providing real time precipitation maps trough the attenuation measurements collected by a dense population of new-generation interactive satellite terminals (called SmartLNB, Smart Low-Noise Block converter). A number of SmartLNB has been deployed in the Tuscany region and a test bed has been established in cooperation with the schools of the Florence Metropolitan city. In the present study, the potential of the new weather radar system is investigated for characterizing the ML in terms of height and thickness under different meteorological conditions and cloud systems. Measurements collected in the RangeHeight Indicator (RHI) scan mode along the direction to the Eutelsat 10A satellite (used for the experimental campaign of the NEFOCAST project) have been analysed with the simultaneous attenuation estimation obtained by the SmartLNBs with radar coverage during selected precipitative events. Statistical analyses have been performed based on both weather radar and SmartLNB measurements, supplemented by ancillary observations from some rain gauges (both impact and tipping-bucket types) co-located with SmartLNBs. In addition, a C-band radar system (Polar 55C) located in Rome has also been used for this work. Polar 55C dual polarization measurements have been analysed with respect to co-located SmartLNBs and laser disdrometer. The results of this analysis highlight the effects of ML on radio signal attenuation, as the total attenuation of signal increases also with the increase of ML vertical thickness. Therefore, the characterization of the vertical profile of precipitation is mandatory for implementing accurate QPE on ground, both from radars and satellite links",Studies of the melting layer of precipitative systems using X-band dual polarization weather radar and SmartLNB network,10.18174/454537,,'Wageningen University and Research',,core
186281112,2018-10-23T00:00:00,"Unmanned Aerial Vehicles (UAVs) have recently rapidly grown to facilitate a
wide range of innovative applications that can fundamentally change the way
cyber-physical systems (CPSs) are designed. CPSs are a modern generation of
systems with synergic cooperation between computational and physical potentials
that can interact with humans through several new mechanisms. The main
advantages of using UAVs in CPS application is their exceptional features,
including their mobility, dynamism, effortless deployment, adaptive altitude,
agility, adjustability, and effective appraisal of real-world functions anytime
and anywhere. Furthermore, from the technology perspective, UAVs are predicted
to be a vital element of the development of advanced CPSs. Therefore, in this
survey, we aim to pinpoint the most fundamental and important design challenges
of multi-UAV systems for CPS applications. We highlight key and versatile
aspects that span the coverage and tracking of targets and infrastructure
objects, energy-efficient navigation, and image analysis using machine learning
for fine-grained CPS applications. Key prototypes and testbeds are also
investigated to show how these practical technologies can facilitate CPS
applications. We present and propose state-of-the-art algorithms to address
design challenges with both quantitative and qualitative methods and map these
challenges with important CPS applications to draw insightful conclusions on
the challenges of each application. Finally, we summarize potential new
directions and ideas that could shape future research in these areas","Design Challenges of Multi-UAV Systems in Cyber-Physical Applications: A
  Comprehensive Survey, and Future Directions",,http://arxiv.org/abs/1810.09729,,,core
187494467,2018-01-01T00:00:00,"Chatbots as a new information, communication and transaction channel enable businesses to reach their target audience through messenger apps like Facebook, WhatsApp or WeChat. Compared to traditional chats, chatbots are not handled by human persons, but software is leading through conversations. Latest chatbots developments in customer services and sales are remarkable. However, in the field of public transport, little research has been published on chatbots so far. With chatbots, passengers find out timetables, buy tickets and have a personal, digital travel advisor providing real-time and context-relevant information about trips. Chatbots collect and provide different data about users and their journey in public transportation systems. They include travel, product, service and content preferences, usage patterns, demographic and location-based data. Chatbots have many advantages for both companies and mobile users. They enable new user touch points, improve convenience, reduce service, sales and support costs, one-to-one marketing, new data collections and deep learning. Using chatbots, smartphone users can reach a company anytime and anywhere. The questioned users of an investigated prototype are remarkably open to new mobile services and they quickly adapt to this technology",Chatbots : an interactive technology for personalized communication and transaction,,,'IADIS - International Association for the Development of the Information Society',,core
188831342,2018-01-01T00:00:00,"Contemporary sensing devices provide reliable mechanisms for continuous process monitoring, accommodating use cases related to mHealth and smart mobility, by generating real-time data streams of numerous physiological and vital parameters. Such data streams can be later utilized by machine learning algorithms and decision support systems to predict critical clinical states and motivate users to adopt behaviours that improve the quality of their life and the society as a whole. However, in many cases, even when deployed over highly sophisticated, cutting-edge network infrastructure and deployment paradigms, data may exhibit missing values and non-uniformities due to various reasons, including device malfunction, deliberate data reduction for efficient processing, or data loss due to sensing and communication failures. This work proposes a novel approach to deal with missing entries in heart rate measurements. Benefiting from the low-rank property of the generated data matrices and the proximity of neighbouring measurements, we provide a novel method that combines classical matrix completion approaches with weighted Laplacian interpolation offering high reconstruction accuracy at fast execution times. Extensive evaluation studies carried out with real measurements show that the proposed methods could be effectively deployed by modern wristband-cloud computing systems increasing the robustness, the reliability and the energy efficiency of these systems",Uncertainty Management for Wearable IoT Wristband Sensors Using Laplacian-Based Matrix Completion,10.1109/CAMAD.2018.8515001,,'Institute of Electrical and Electronics Engineers (IEEE)',,core
217169845,2018-04-13T17:05:52,"Traditional remotely operated vehicles (ROV’s) require extensive setup and unnatural control systems. Integrating wearable devices as a control system, operators gain mobility and situational awareness to execute additional tasks. Analysis is conducted to understand if wearable devices connected by Internet of Things (IoT) allows for a more natural control system. A gesture recognition armband is worn around the operator’s forearm reading surface electromyography (sEMG) signals produced by their muscles to recognize hand gestures. An Augmented Reality (AR) headset overlays supplemental information on a heads-up display (HUD). IoT enables each component of the system to transmit and receive data over a network. The AR headset serves as the central processing unit, processing sEMG signals and transmitting respective commands to a ROV. The ROV acts on the received commands and transmits data, describing its actions and environment, to be displayed. A library of electrical signals that relate to hand gestures defined in US Army Publication TC3-21.60 are developed as a control set of commands. Signal processing and machine learning methods are implemented to reduce cross-talk and interference of weak sEMG signals for accurate gesture recognition. Results provide insight on the effectiveness of neuromuscular control compared to human-to-human instruction, and how wearable control systems can increase operator situational awareness",Integration of Augmented Reality and Neuromuscular Control Systems for Remote Vehicle Operations,,https://core.ac.uk/download/217169845.pdf,Scholarly Commons,,core
225895087,2018-01-01T00:00:00,"As a result of the digitalization of the power business in Norway and Europa, a lot of new possibilities and challenges arise. In 2014 an expert committee one outlined a proposal for the future grid company structure in Norway (Reiten, 2014). In addition, new technologies are being implemented in the system. Wind power, solar power, un-regulated small hydro power production, battery storage domestic and industrial and electrification of transport. Transmission System Operators (TSOs) have a responsibility to supply industry and communities with reliable electric power. However, the operators have been virtually blind to slowly occurring changes in the load profile that reduce the expected regularity of the power supply. This paper will focus on the possibilities and challenges the power business are facing. The paper will describe what technologies is needed i.e Real time probabilistic risk calculations, artificial intelligence, machine learning and smart grid technology. The main question is: can the power business and the introduction of new system tools manage without probabilistic risk calculation for making use of the digitalization and the corresponding big data?Digitalization of the power business: How to make this work?publishedVersionNivå",Digitalization of the power business: How to make this work?,,https://core.ac.uk/download/225895087.pdf,,,core
299953422,2018-01-01T00:00:00,"The fast advancements in sensor data acquisition and vehicle telematics facilitate data collection from taxis and thus, enable building a system to monitor and analyze the citywide taxi service. In this paper, we present a novel and practical system for taxi service analytics and visualization. By utilizing both real time and historical taxi data, the system conducts the estimation on region based passenger wait time for taxi, where recurrent neural network (RNN) and deep learning algorithms are used to build a predictive model. The built RNN-based predictive model achieves 73.3% overall accuracy, which is significantly higher than other classic models. Meanwhile, the system conducts the analytics on the taxi pickup hotspots and trip distributions. The experimental results show that around 97% trips are accurately identified and more than 200 hotspots in the city are successfully detected. Moreover, a novel three dimensional (3D) visualization together with the informative user interface is designed and implemented to ease the information access, and to help system users to understand the characteristics and gain insights of the taxi service","An intelligent system for taxi service : analysis, prediction and visualization",10.3233/AIC-170747,,'IOS Press',"[{'title': 'AI Communications', 'identifiers': ['0921-7126', 'issn:0921-7126']}]",core
159637925,07/06/2018,"While the idea of a city built for people is gaining more and more acceptance today, many of our urban environments remain focused and built around the car. Currently, day-to-day life in the city means the frequent interaction between pedestrians and drivers, a situation which can be dangerous, or, at worst, deadly.
 
 
 This project, a collaboration between the Complex Systems group at IN3 (CoSIN3) of the Universitat Oberta de Catalunya (UOC), the Dirección General de Tráfico (DGT) and the Guàrdia Urbana de Barcelona, aims to quantify the issue of car-pedestrian collisions by characterising specific street areas with an indicator of pedestrian safety based on the structural properties of the street. Concretely, the project will generate this safety index for Spain’s two largest cities, Madrid and Barcelona, but the methodology and pipeline are applicable theoretically to any urban setting.
 
 
 As a base unit for measuring pedestrian safety over space, the total pedestrian area of the city (sidewalks and crossings) will be tessellated into small, regular segments. Each of these segments will be assigned various indicator values, from simple geometric properties (distance to the closest pedestrian crossing; width of sidewalk) to more complex measures such as driver visibility.
 
 
 Geometric operations to arrive at these values are performed on a PostGIS-build geo-database, over a variety of data, including street, sidewalk and block geometries, from diverse sources of open GIS data (Instituto Geográfico Nacional, Institut Cartogràfic i Geològic de Catalunya, OpenStreetMap, municipal data sources).Visibility values will be derived from a combination of deep learning technologies with GIS. A deep learning architecture will deliver computer-segmented street-scene images from Google Streetview. Each labeled image will be paired with an image from a simplified 3-dimensional model of the city, replicating its point of view (rendered with open-source 3D mapping software). The model will be clean of all street features (parked cars, trees, etc.) besides sidewalks and buildings. Comparison between the real and simplified images will thus permit the identification of sidewalk areas invisible to drivers due to visual obstructions.
 
 
 The results of the project will be presented as online “heatmap” visualisations of safety indexes for the focus cities, open to the public for browsing and research. Additionally, a purpose-built API (Application Programming Interface) will provide public and private organisations working in the area of traffic safety access to the results for integration in their own internal or public application",Espacio persona: Big data to make urban streets safer,,,Universitat de Girona. Servei de Sistemes d'Informació Geogràfica i Teledetecció,,core
212848228,2018-01-01T08:00:00,"Cyber-Physical Systems (CPSs) are combinations of physical processes and network computation. Modern CPSs such as smart buildings, power plants, transportation networks, and power-grids have shown tremendous potential for increased efficiency, robustness, and resilience. However, such modern CPSs encounter a large variety of physical faults and cyber anomalies, and in many cases are vulnerable to catastrophic fault propagation scenarios due to strong connectivity among their sub-systems. To address these issues, this study proposes a graphical modeling framework to monitor and predict the performance of CPSs in a scalable and robust way.
This thesis investigates on two critical CPS applications to evaluate the effectiveness of this proposed framework, namely (i) health monitoring of highway traffic sensors and (ii) building energy consumption prediction. In highway traffic sensor networks, accurate traffic sensor data is essential for traffic operation management systems and acquisition of real-time traffic surveillance data depends heavily on the reliability of the physical systems. Therefore, detecting the health status of the sensors in a traffic sensor network is critical for the departments of transportation as well as other public and private entities, especially in the circumstances where real-time decision making is required. With the purpose of efficiently determining the traffic network status and identifying failed sensor(s), this study proposes a cost-effective spatiotemporal graphical modeling approach called spatiotemporal pattern network (STPN). Traffic speed and volume measurement sensors are used in this work to formulate and analyze the proposed sensor health monitoring system. The historical time-series data from the networked traffic sensors on the Interstate 35 (I-35) within the state of Iowa is used for validation. Based on the validation results, this study demonstrates that the proposed graphical modeling approach can: (i) extract spatiotemporal dependencies among the different sensors which lead to an efficient graphical representation of the sensor network in the information space, and (ii) distinguish and quantify a sensor issue by leveraging the extracted spatiotemporal relationship of the candidate sensor(s) to the other sensors in the network.
In the building energy consumption prediction case, we consider the fact that energy performance of buildings is primarily affected by the heat exchange with the building outer skin and the surrounding environment. In addition, it is a common practice in building energy simulation (BES) to predict energy usage with a variable degree of accuracy. Therefore, to account for accurate building energy consumption, especially in urban environments with a lot of anthropogenic heat sources, it is necessary to consider the microclimate conditions around the building. These conditions are influenced by the immediate environment, such as surrounding buildings, hard surfaces, and trees. Moreover, deployment of sensors to monitor the microclimate information of a building can be quite challenging and therefore, not scalable. Instead of applying local weather data directly on building energy simulation (BES) tools, this work proposes a spatiotemporal pattern network (STPN) based machine learning framework to predict the microclimate information based on the local weather station, which leads to better energy consumption prediction in buildings",Spatiotemporal graphical modeling for cyber-physical systems,,https://core.ac.uk/download/212848228.pdf,Iowa State University Digital Repository,,core
132585739,2018-03-15T00:00:00,"Over the years, collaborative mobility proved to be an important but challenging component of the smart
cities paradigm. One of the biggest challenges in the smart mobility domain is the use of data science as an enabler for the implementation of large scale transportation sharing solutions. In particular, the next generation of Intelligent Transportation Systems (ITS) requires the combination of artificial intelligence and discrete simulations when exploring the effects of whatif decisions in complex scenarios with millions of users. In this paper, we address this challenge by presenting an innovative data modelling framework that can be used for ITS related problems. We demonstrate that the use of graphs and time series in multi-dimensional data models can satisfy the requirements of descriptive and predictive analytics in real-world case studies with massive amounts of continuously changing data. The features of the framework are explained in a case study of a complex collaborative mobility system that combines carpooling, carsharing and shared parking. The performance of the framework is tested with a large-scale dataset, performing machine learning tasks and interactive realtime data visualization. The outcome is a fast, efficient and complete architecture that can be easily deployed, tested and used for research as well in an industrial environment",A New Modelling Framework over Temporal Graphs for Collaborative Mobility Recommendation Systems,10.1109/itsc.2017.8317664,,,,core
161097511,2018-09-16T00:00:00,"Public transport users are increasingly expecting better service and up to date information, in pursuit of a seamless journey

experience. In order to meet these expectations, many transport operators are already offering free mobile apps to help customers

better plan their journeys and access real-time travel information. Leveraging the spatio-temporal data that such apps can produce

at scale (i.e. timestamped GPS traces), opens an opportunity to bridge the gap between passenger expectations and capabilities of

the operators by providing a real-time 360-degree view of the transport network based on the ‘Apps as infrastructure’ paradigm.

The first step towards fulfilling this vision is to understand which routes and services the passengers are travelling on at any given

time. Mapping a GPS trace onto a particular transport network is known as ‘network matching’. In this paper, the problem is

formulated as a supervised sequence classification task, where sequences are made of geographic coordinates, time, and line and

direction of travel as the label. We present and compare two data-driven approaches to this problem: (i) a heuristic algorithm, which

looks for nearby stops and makes an estimation based on their timetables — used as a baseline — and (ii) a deep learning approach

using a recurrent neural network (RNN). Since RNNs require considerable amounts of data to train a good model, and collecting

and labelling this data from real users is a challenging task (e.g. asking too often can be overwhelming; privacy concerns on

providing GPS location; unreliable labels due to mistakes or misuse), one of our contributions is a synthetic journey data generator.

The datasets that we generated have been made as realistic as possible by querying real timetables and adding position and temporal

noise to simulate variable GPS accuracy and vehicle delays, sampled from empirical distributions estimated using thousands of

real location reports. To validate our approach we have used a separate dataset made of hundreds of real user journeys provided by

a UK-based bus operator. Our experimental results are very promising and our next step is to deploy the solution in a production

environment. From the operator’s point of view, this will enable multiple smart applications like account-based ticketing,

identification of disruptions, real-time passenger counting, and network analysi",Automatic Transport Network Matching Using Deep Learning,10.1016/j.trpro.2018.09.053,https://core.ac.uk/download/161097511.pdf,'Elsevier BV',,core
275635397,2018-01-10T00:00:00,"[EN] Road safety applications envisaged for vehicular ad hoc networks (VANETs) depend largely on the exchange of messages to deliver information to concerned vehicles. Safety applications as well as inherent VANET characteristics make data dissemination an essential service and a challenging task. We are developing a decentralized efficient solution for broadcast data dissemination through two game-theoretical mechanisms. Besides, VANETs can also include autonomous vehicles (AVs). AVs might represent a revolutionary new paradigm that can be a reality in our cities in the next few years. AVs do not need a driver to work; instead, they should copy a proper human behavior to adapt the driving according to the current circumstances, such as speed limit, pedestrian crossing street or wheather conditions. We will develop an AV software module including artificial intelligence (AI) techniques so that AVs can interact with the dynamic scenario throughout time. Finally, we also will include electrical vehicles (EV) in the VANET, so that special services such as finding and reserving an EV charging station place will be welcome. In addition, we are developing a multimetric geographic routing protocol for VANETs to transmit H.265 video (traffic accident, traffic state, commercial….) over VANETs.This work was partly supported by the Spanish Government through the project TEC2014-54335-C4-
1-R INcident monitoRing In Smart COmmunities, QoS and Privacy (INRISCO). Cristian Iza is recipient of a grant from Secretaria Nacional de Educación Superior, Ciencia y Tecnología SENESCYT. Ahmad Mohamad Mezher is a postdoctoral researcher with the Information Security Group (ISG) at the Universitat Politècnica de Catalunya (UPC).Iza Paredes, C.; Uribe Ramírez, JA.; López Márquez, N.; Lemus, L.; Mezher, A.; Aguilar Igartua, M. (2018). Multimedia communications in vehicular adhoc networks for several applications in the smart cities. Editorial Universitat Politècnica de València. 212-215. https://doi.org/10.4995/JITEL2017.2017.6584OCS21221",Multimedia communications in vehicular adhoc networks for several applications in the smart cities,10.4995/JITEL2017.2017.6584,https://riunet.upv.es/bitstream/10251/102992/1/6584-18904-1-PB.pdf,'Universitat Politecnica de Valencia',,core
162620817,2018-01-01T00:00:00,"This thesis presents a novel synthetic environment for supporting advanced explorations of user interfaces and interaction modalities for future transport systems. The main goal of the work is the definition of novel interfaces solutions designed for increasing trust in self-driving vehicles. The basic idea is to provide insights to the passengers concerning the information available to the Artificial Intelligence (AI) modules on-board of the car, including the driving behaviour of the vehicle and its decision making. Most of currently existing academic and industrial testbeds and vehicular simulators are designed to reproduce with high fidelity the ergonomic aspects associated with the driving experience. However, they have very low degrees of realism for what concerns the digital components of the various traffic scenarios. These includes the visuals of the driving simulator and the behaviours of both other vehicles on the road and pedestrians.  High visual testbed fidelity becomes an important pre-requisite for supporting the design and evaluation of future on-board interfaces. An innovative experimental testbed based on the hyper-realistic video game GTA V, has been developed to satisfy this need. To showcase its experimental flexibility, a set of selected user studies, presenting novel self-driving interfaces and associated user experience results, are described. These explore the capabilities of inducing trust in autonomous vehicles and explore Heads-Up Diplays (HUDs), Augmented Reality (ARs) and directional audio solutions. The work includes three core phases focusing on the development of software for the testbed, the definition of relevant interfaces and experiments and focused testing with panels comprising different user demographics. Specific investigations will focus on the design and exploration of a set of alternative visual feedback mechanisms (adopting AR visualizations) to gather information about the surrounding environment and AI decision making. The performances of these will be assessed with real users in respect of their capability to foster trust in the vehicle and on the level of understandability of the provided signals. Moreover, additional accessory studies will focus on the exploration of different designs for triggering driving handover, i.e. the transfer vehicle control from AI to human drivers, which is a central problem in current embodiments of self-driving vehicles.QC 20181010</p",Novel synthetic environment to design and validate future onboard interfaces for self-driving vehicles,,,,,core
286982314,2018-12-13T00:00:00,"Research presented in this report is based on:1. Problem Submission: Firms submitted corporate challenges relating to digital business models across several industries. Problems were screened and selected.2. Problem Framing: Professor Darwin conducted individual sessions with individual firms to solicit input from both open innovation researchers and practitioners.3. Problem Solving: Input, feedback, and recommendations provided by a community of academic experts and open innovation practitioners across industries who worked deliberated in groups of eight during a one-hour session per challenge.Challenge #1: WIPRO : “Wipro is getting ready to deploy its newly developed digital technology (AR/VR/AI) “do-your-own repairs tool” that will help people repair their white goods in their homes. With what business model could the company deploy this new technology?”;Challenge #2: DAIMLER AG : ""As Daimler enters new markets, such as autonomous vehicles, how can a mobility services firm accelerate internal innovation against uncharted territories in the uncertain times of digital transformation?""; Challenge #3: KANEKA : ""Kaneka wants to improve innovation internally through the following lens: how can Kaneka accelerate internal innovation utilizing a two-sided digital ideation/challenge platform through which it can address company’s internal and external challenges?"";Challenge #4: ALLERGAN ""What innovative digitally driven design would you suggest for supporting busy physicians and/or patients, to get them relevant, accurate, reliable and real-time information and analysis quickly?"";Challenge #5: APPLIED MATERIAL ""How AMAT can leverage its materials engineering capabilities to enter new markets with platform extensions powered by collaborations with external ecosystem partners?"";Challenge #6: XIAOMI "" Xiaomi offers High-Value/Low-Cost/Low-Margin products to all customer segments in emerging markets. This demands severe cost curtailment strategies in manufacturing, operation, advertisement, sales, distribution and servicing of its products. Xiaomi cannot deliver this value alone without an ecosystem to sustain and scale the business. How can the government, corporations and other institutions help create a win-win for all? In addition, rural communities lack infrastructure (reliable connectivity, power, healthcare, clean water, accessible roads). What must Xiaomi do to serve and expand the market when this infrastructure is lacking?",Digital Transformation for sustainability,,,HAL CCSD,,core
156873021,2018-01-01T00:00:00,"On the occasion of the recent editions of the WSIS Forum MEDICI organised different workshops to showcase on the one side the richness of applications and services provided

by ICTs in the field of safety, security and disaster recovery and management and to

contribute to provide a reference point for all those working in these sectors and those

who may take advantage from their outcomes. This year we continued this path selecting additional international case study both to approach new sectors and enrich the platform

of skills and competences involved.

Safety and security are integral part of human rights; we must provide all the efforts in

order to guarantee such rights (as stated in art 3, 22, 25 - The Universal Declaration of

Human Rights). In addition, a number of SDGs are tightly connected or rely on safety and security: SDG 2, SDG3, SDG6, SGD6, SDG7, SDG8, SDG9, SDG11, SDG16, SDG17. Some of the specific fields are: food & water security, human security, safety, critical infrastructure resilience, drugs security and more.

Nowadays the demand for ""safety & security"" in all its forms has increased, especially

quantitatively and qualitatively, making clear the need for new approaches to enable the

entire sector to ensure better results.

Looking from a different perspective: we outline the role of ICTs in risks assessment and

management. They are playing key roles in a number of “risky” scenarios from health and

children abuse to homeland security and law enforcement, crimes, trafficking (humans,

drugs, weapons, artefacts, etc.) and even safety on working places and mobility.

Of course, technology it is not enough to solve problems, it is well known and

demonstrated that a holistic, interdisciplinary approach and a culture of ""safety & security"" taking adequately into account human factors are the basis in order to obtain

good results in this area.

We must promote an interdisciplinary approach and a “culture” of safety & security, they

are the basis in order to obtain good results in this area; foster the exchange of

experiences and best practices among countries and promote research thanks to the

WSIS.

On the occasion of previous editions of the WSIS Forum (e.g. 2014, 2015, 2016) some

eminent speakers underlined the key-role played by ICTs on the occasion of natural

disasters and other critical events, they said that cyber technologies have fuelled the

hope of people affected by the natural disaster. The availability of low price high

performance devices and the proactive activity of clever developers have boosted the

production of a number of smart solutions spread in different countries all-over the

world. Due to the actual “silos” segmenting these sectors it is quite difficult to have a

comprehensive vision on these resources and success stories, there is a need for a holistic

approach and best practice sharing.

Internet of things, machine learning, grids, network of sensors, remote sensing as well as

near field communication and, why not, unmanned vehicles glued by networking are

some of the building blocks of safety and security in different fields.

The nine case study presented by the distinguished speakers on the occasion of the ICT

for Safety and Security led to the following outcomes: there is a need to improve the

visibility of ICT applications devoted to safety and security raising the same level of

awareness actually limited to cyber security. The case study presented on the occasion

of the workshop this year and the one already presented in the last editions of the WSIS

have strengthened this need. Achievements in these fields positively impact the human

rights and must be shared among researchers and countries. The WSIS Forum is the key

forum for discussing the role of ICTs as a means of implementation of the Sustainable

Development Goals, if we consider ICTs as powerful means to implement SDGs we must

include and adequately take into account ICTs applied to safety and security in a broad

sense, they are relevant part of SDGs as outlined many times both within the UNGA

Overall WSIS Review and the UNDP 2030 Agenda for Sustainable Development SDGs.

An additional relevant issue emerged on the occasion of the workshop, as sometimes

happens after revolutions, revolutionaries wonder if what they achieved is actually what

they were hoping for. The original idea of computer scientists in the “hippies” counterculture era was aimed to empower citizens and provide them much freedom. Almost fifty years later, after the chimera of the “happy cyber-world”, some of us have started thinking that the foreseen Orwellian “1984” has simply come true ten, fifteen

years later: globalisation, always on devices, position tracking systems, CRMs and users’ profiles, CCTVs and IoT; are those technologies framing citizens? Thoughts for some time have circled around how the speed of the new information revolution renders us less capable develop a critical approach able to foresee the social, ethic, economic impact of such revolution in a long-term perspective. So, in recent times we started facing a wave

of criticism about the evolutionary path of the information and knowledge society, for quite a long time ICT gurus and humanists didn’t interact too much, the true power of cyber technology was largely unexpressed, there were some alerts as Artificial Intelligence, Virtual Reality, Robots often seen from humanists as potential danger for the mankind, but nothing concrete happened. The turning point was probably the exploitation of the Internet and the dissemination of information. Information is built on top of single or aggregation of data, for quite a long-time people use to think that cyberspace is a “black hole” without memory where you pour data without any side effect. Young generations shared on line sensitive information in order to access a videogame or chat with friends and more recently posted images and clips about their private life; does this mean “goodbye privacy?” As a consequence of a lack of “culture” in the use of emerging technologies now we have to deal with serious problems related to information ownership, use, abuse and misuse, not mentioning cybercrimes. An additional drawback is due to the deep technological intrusion affecting our daily life, we feel framed by cyber devices more than supported.

Some evident outcomes of this feeling are the “right to disconnect” - controversial reform of French labour law by the labour minister Myriam El Khomri back in May 2016 and the “right to obsolescence” or the “right to be forgotten” due to Viktor Mayer-Schonberger, the author of “Delete: The Virtue of Forgetting in the Digital Age”. All these to do not mention the cultural, social and economic impacts not always positive especially in a long-term perspective.

Technologies originally conceived by idealists to provide much more freedom and

wellness to humans took then a wrong path framing humans due to all the constraints

placed upon us with new technologies. For instance, as liberating as they are - by

providing flexibility and instant connectivity - we have become enslaved to our devices,

fearful of losing out information and access in an increasingly competitive and fast-paced

world. Consequently, our bodies have suffered, as have our minds (due to information overload), what of our work-life balance -- and this is just to begin with! Ranjit Makkuni’s

paper “Betrayed IT Revolution” and presentation outlines a vision for new design of devices, clutter-free access to web documents to create deeper learning experiences. At the implication level, the project rethinks implications for new design of web mark-up languages that support the creating of ‘privacy’ based secure browsing.

In conclusion we would like to stress the positive effects due to the WSIS process and its outcomes, panellists suggest to establish in the WSIS framework a global observatory on ICTs for safety, security and disaster recovery and to include and promote a wider range of “security” topics under the WSIS umbrella endorsing a holistic approach to the “Safety, Security, Disaster Recovery and Management” sector","Thematic Workshop: ICTs for Safety, Security and Disaster Recovery",,,International Telecommunication Union ITU,,core
234970564,2018-04-11T00:00:00,"The Reporter is a publication produced by Western Carolina University featuring news, events, and campus community updates for faculty and staff. The publication began in August of 1970 and continues digitally today. Click on the link in the “Related Materials” field to access recent issues.The Reporter
News for the Faculty and Staff of Western Carolina University
June 24, 1998
Food Services to
Switch July 1
See you at Starbucks!
Cullowhee, North Carolina
Western's Learning Communities
Helping Freshmen Find a Niche
Jr Dining in takes on
a new dimension
July 1 at Western
with a changeover4
in food services. ARAMARK
Corp. has been selected to operate
the university's campus dining
services, including those at
Dodson and Brown cafeterias, the
food court areas at Hinds
University Center and Dodson,
concessions at the Ramsey
Regional Activity Center, and
catering services throughout
the campus.
Among the changes expected
under ARAMARKs management
are new menus and formats
providing a wide variety of student
dining options. These include addi­tional
nationally known franchises,
such as Starbucks Coffee and Little
Ceasars Pizza, as well as demon­stration
cooking areas where
customers can watch as their food
is prepared, and expansion of full
menu cafeteria-style service and
""grab-and-go"" items.
Clete Myers, operations
manager of food services at
Clemson University, will become
general manager of campus dining
services at Western. ARAMARK
hopes to retain personnel currently
employed in WClFs food service
when the management change
occurs.
Retention Services Director Susan Clarke Smith (far right) and Peer Mentor
Bryan Dodge (center), meet entering LC students.
""An institution's capacity to retain students is directly related to its ability to reach out and make
contact with students and integrate them into the social and intellectual fabric of institutional life.
It hinges on the establishment of a healthy, caring environment which enables individuals to find a
niche in the social and intellectual communities of the institution."" —VINCENT TINTO IN LEAVING COLLEGE
a ±JL good many of us came of age
when linking the notion of caring
with any institution would have been
greeted with skepticism—if not out­right
scorn. But Western, changing
with, and in many instances ahead of,
the times, will set the standard for
high tech and high touch this fall.
By embracing the idea of learning
communities as part of its freshman
year experience program, Western
makes use of a proven method for
meeting both the individual's need
to belong and the institution's need
to retain.
Randomly selected as participants
in the Learning Community (or LC
Pilot) project beginning this fall, some
170 of Western's estimated 1,200
first-year students will live together
and learn together in classes linked to
their common interests and conduct­ed
by a core group of instructors—all
in an effort to address the problems
presented by the transition from high
school to college. Meanwhile, the
institution will look closely to see if
the initiative addresses its own prob­lems
associated with an unusually
high rate of student attrition,
particularly after the first year.
Frank Prochaska, associate vice
chancellor for academic affairs and
chief architect of the LC Pilot project,
believes that establishing learning
communities at Western, even on a
limited basis initially, will deliver on
both counts. The numbers would
appear to bear him out, with
universities where LCs have become
a standard for first-year programs
reporting significantly higher GPAs
and retention rates among students
who participate.
Elizabeth Shelly coordinates the
Freshman Year Experience Program
through Student Affairs and is herself
a product of an undergraduate LC.
Shelly describes it as setting out in
college life with an ""instant group of
friends"" to go to at any time for
support. She explains that Western's
approach to the idea conforms to a
pretty standard model but incorporates
some new features made possible by
our unique environment.
The pilot group is divided into eight
communities of about twenty students
each, with several of the communities
grouped according to a declared major
or according to undergraduate college.
Three LCs are made up of freshmen
undecided upon majors. Each LC will
be housed in a suite-style layout in
Walker Hall with common areas set
aside for studying and group
activities. In addition to a resident
assistant on the floor, each commu­nity
will be assigned an upperclass
peer mentor who will live with the
group and work with the USI130
class designed for it.
A revamped USI course is key to
the LC Pilot, according to Prochaska.
The USI 130 course for the individual
communities will be co-taught by a
faculty member and a student affairs
professional, both trained in freshman
issues. Themes introduced for
discussion in this course will carry
over to the other General Education
courses linked for the purposes of each
community and taught by specially
selected instructors. For example,
Instructor Nory Prochaska's USI 130
section, and its linked English,
continued on page 2
Learning, continued
computer science, and math courses,
will examine the advantages and
disadvantages of the ""virtual
university"" idea, from the perspective
of LC students who have a particular
interest in technology and a close
comfort with using electronic media.
Susan Clarke Smith, director of
Retention Services, sees the linked
courses as an effective means of
addressing the ""intimidation factor,""
identified by most retention experts
such as Vincent Tinto as one of the
main reasons students leave college.
Smith asserts that by establishing
opportunities for interaction in and
out of the classroom, students and
faculty can begin to ""break down
barriers."" Smith says, ""Students can
see that their professors are human,
and faculty can feel less hesitation, as
part of a wider network of support, to
communicate concern on a more
personal level.""
To some extent, we need look no
farther than our own campus for an
example of a learning community
beginning to deliver on its promise.
Now entering its second year, The
Honors College combines an academic
emphasis with a residential and social
component. Brian Railsback, acting
dean of the college, calculates a 92
percent fall-spring retention rate for
honors freshmen for 1997-98. (The
university's rate for freshmen was
83 percent.) ""That figure,"" Railsback
says, ""obviously points to a good bond
and a real desire for these students to
stay together.""
Perhaps the most ambitious and
the most advantageous aspects of
initiating a learning community
program now come to a confluence
with the computer implementation
project, also spearheaded by
Prochaska. Student Affairs Vice
Chancellor Robert Caruso sees the
efforts as quite extraordinary. ""The
added component of new technologies
in the classroom and the residence
halls will revolutionize our whole
notion of community,"" Caruso says.
""With something on the order of only
forty higher education institutions
out of about 2,800 nationwide that
have a computer requirement, I think
everyone is going to be looking to
Western as a model for creating the
learning community of the twenty-first
century.""
Michael Dougherty
Named Dean of
Education, Allied
Professions
Carolina
Board of
Governors.
Associate
dean of the
College of
Education
and Allied
Professions
since 1996, Dougherty is a professor
in the Department of Human
Services and is a former head of the
department. He earned his bachelor's
degree from the University of Notre
Dame in 1968, master's degrees from
Oakland University in 1970 and
Notre Dame in 1971, and doctoral
degree from Indiana State University
in 1974. He has been a member of the
human services faculty since 1976.
The 1988 recipient of Western's
Paul A. Reid Distinguished Service
Award for Faculty, he also has been
nominated for the Chancellor's
Distinguished Teaching Award and
the Taft Botner Award for Superior
Teaching. Prior to coming to Western,
he was a teacher and counselor in
public schools in Detroit; Mattoon, 111.;
and Taylor County, Fla.
Dougherty is a member of several
professional organizations, including
the American Counseling Association
and the Association for Educational
and Psychological Consultation. His
research activities have focused on
study skills and locus of control, the
effects of counseling techniques on
incarcerates, and consultation styles.
Dougherty's appointment will be
effective July 1, upon the retirement
of Chambers, who served seventeen
years as the college's dean.
Highlighting the summer's cultural
events on campus are this weekend's
concerts by the Atlanta Ballet and the
Cassatt String Quartet.
Hailed as one of the nation's
outstanding young ensembles, the
Cassatt String Quartet will perform in
recital at 8 p.m. this Friday and will
also perform as part of the Atlanta
Ballet programs at 2 p.m. and 8 p.m.
on Saturday. All performances are in
Hoey Auditorium.
The oldest continually operating
ballet company in the United States,
the acclaimed Atlanta Ballet travels to
the mountains each year for a summer
residency. The two performances on
Saturday come in conjunction with
Western's hosting the second annual
Atlanta Ballet Centre for Dance Educa­tion
summer dance camp. Selected
participants in the June 14-July 4
camp, which attracts more than fifty of
the top ballet students in the Southeast,
will share the stage with the
professional dancers for one piece.
The ballet programs on Saturday
include ""Intermezzo,"" featuring three
couples in an intricate series of
dances set to music by Johannes
Brahms; ""Prisma,"" featuring music
by Charles Ives and Atlanta Ballet
executive/music director Robert
Chumbley performed by the Cassatt
String Quartet; and ""II Distrato,"" an
abstract ballet in five movements
demonstrating how the different
parts of the dancer's body work
separately and as a unit.
The Cassatt String Quartet's
Friday program features music for
strings by Beethoven and Ravel.
Admission for the quartet's recital
is $10 for adults, $5 for WCU
students and children. Admission for
the ballet's performances is $20 for
adults and $5 for WCU students and
children. For tickets, call 227-7397.
Architects Tapped for Construction Projects
A pair of major construction projects planned for Western moved closer to reality
recently with the board of trustees' selection of two Charlotte architecture firms
to design an expansion of the university center and a federally funded workforce
development facility.
The trustees named Lee Nichols Hepler Architecture of Charlotte to design
the expansion of the Hinds University Center and Jenkins-Peer Architects of
Charlotte to design the proposed new Western North Carolina laborforce high-technology
education and training center.
The Hinds University Center project is the second in a three-phased
expansion effort designed to enhance the quality of student life at WCU. Plans
call for the construction of approximately 31,000 additional square feet to add a
retail shopping area, a movie theater, increased meeting and office space for
student organizations, and a multicultural center. Preliminary estimates for the
expansion set the cost at about $4.5 million.
The regional high-tech workforce training center, announced last fall by U.S.
Rep. Charles Taylor, is designed to help raise the economic potential of the
region by improving the availability of high-technology education to the
mountains' workforce. The center could include such high-tech training tools as
an industrial laser lab, artificial intelligence lab, geographic information lab,
robotics training, and sound and video production facilities complete with digital
editing capabilities. The facility, pending funding from Congress and the federal
Economic Development Administration, is expected to be built adjacent to the
Belk Building.
Michael Dougherty, associate dean of
the College of Education and Allied
Professions, has been named by
WClFs board of trustees to succeed
Gurney E. Chambers as dean.
Appointment of Dougherty to the
dean's post was approved by the
board Wednesday, June 10, at its
quarterly meeting. The appointment
is subject to the approval of the
University
I of North
WCU Campus
Plays Host to
Weekend of CI
Music and
June 24,1998 • T he Reporter • p age 2
University Awards Top Honors for Teaching, Research, and Service
Western's top faculty and staff
awards for teaching, research, and
service for the 1997-98 academic
year were presented at the annual
spring General Faculty and Awards
Convocation in May.
Mary C. ""Katie"" Ray, assistant
professor of elementary and middle
grades education, won the
Chancellor's Distinguished Teaching
Award. The Paul A. Reid Distin­guished
Service Award for faculty
went to Gordon Mercer, professor
of political science and public
affairs, and the Paul A. Reid
Distinguished Service Award for
administrative staff went to
Stephen White, sports information
director. David J. Butcher,
associate professor of chemistry,
received the University Scholar
Award for distinguished scholarly
achievement.
The honors, presented by
Chancellor John Bardo, carry $1,000
cash awards and engraved plaques
for each recipient. Bardo also
presented the Academic Award of
Excellence to the Department of
English. The award provides
$10,000 for program and staff
development.
Chancellor's Distinguished
Teaching Award
Katie Ray joined WCU's faculty
in 1994 after seven years of teaching
in elementary and middle schools in
New York City. In presenting the
award, Bardo quoted from Ray's
comments to the awards selection
committee.
""For students to be great
teachers, they must be passionate
about living and learning,"" Ray said.
""Consequently, I must show
students by my own passion. The
challenge I face every day is not to
know about good teaching, but to
demonstrate it in every interaction I
have with my students.""
Paul A. Reid
Distinguished Service
Award—Faculty
Gordon Mercer has been
a member of the WCU
faculty since 1980. Among
his accomplishments are the
creation of the annual
Undergraduate Research
| Conference and the
organization of faculty
forum assemblies to foster
communication about
athletic director. He received twenty-six
publication awards from the
College Sports Information Directors of
America, and eight Football Writers
Association of America awards for
""Outstanding Press Box Service.""
Following his retirement on June 30,
integrate computers and new
technology with writing and research
in a way that reflects ""real-world""
practices. The department has become
a primary user of the electronic
classrooms, and during the 1997-98
academic year, every freshman
years of continuous service leave
from usual work commitments to
pursue concentrated scholarly work.
Recipients are chosen on a competi­tive
basis by a faculty committee.
important campus issues.
He has held leadership
positions in WCU's Faculty
Senate and the University of
North Carolina's Faculty Assembly.
Mercer also has been described as ""a
superior teacher"" by his students and
has been nominated frequently for
campus teaching awards.
Paul A. Reid Distinguished Service
Award—Staff
Steve White will retire at the end of
June as WCU's sports information
director. A 1967 graduate of WCU,
White has also served as associate
Distinguished Teacher Katie Ray, University Scholar David Butcher, and Reid Service honoree
Gordon Mercer receive their awards from Chancellor John Bardo.
White will head the Catamount Sports
Network, Inc., the radio broadcaster of
WCU's football and basketball games.
University Scholar Award
David Butcher joined WCU's faculty
in 1990. An analytical chemist, he has
received several external grants and
has published numerous articles on his
scientific research, which includes the
atomic absorption spectroscopy, atomic
fluorescence spectrometry, and the
search for potential
chemical causes for
Sports Information Director Steve White receives the Reid
Service Award for Staff from Chancellor John Bardo.
of Excellence, Bardo
praised the
Department of
English for its
innovative work to
studied composition in the electronic
environment.
Other major awards recognized at
the convocation were the Beyond the
Classroom Teaching Award and the
Scholarly Development Assignment
Program Awards.
The Beyond the Classroom
Teaching Award is given to an
academic teaching unit that excels
in enhancing students' learning
through such activities outside the
classroom as mentoring programs,
effective academic advising or
cooperative learning experience. The
1998 winner of the award, which is
funded by the UNC Board of
Governors, is the Department of
Health Sciences.
Recipients in the Scholarly
Development Assignment
Program are Richard Boyer
(English); Barbara Lovin (head,
Health Sciences); and Dan Pittillo
(Biology). The Scholarly Develop­ment
Assignment Program awards
provide full-time tenured faculty
members who have a minimum of six
the decline of Fraser
fir trees in the
Southern Appala­chian
Mountains.
Academic Award
of Exc ellence
In presenting
the Academic Award
June 24,1998 • The Reporter • page 3
Bruce Henderson Receives UNC System Teaching Award
Bruce B. Henderson, professor
of psychology, was among
sixteen recipients of the fourth
annual Awards for Excellence
in Teaching, presented by the
UNC Board of Governors.
Henderson accepted the award
at a special academic convoca­tion
held at N.C. Central
University in conjunction with
the inauguration of Molly
Corbett Broad as president of the
University of North Carolina.
Winners from
each campus
received a
bronze medallion
and a $7,500
cash prize.
Recipients were
nominated by
special commit­tees
from each of
the sixteen UNC
campuses and selected by the Board
of Governors Committee on Teach­ing
Awards.
Established by the Board of
Governors in 1994 to underscore
the importance of teaching and to
reward good teaching across the
university system, the awards are
given annually to a tenured faculty
member from each UNC campus.
Winners must have taught at their
present institutions at least seven
years, and no one may receive the
award more than once.
Henderson, on WCU's faculty
since 1978 and former head of the
psychology department, received
Western's Botner Superior
Teaching Award in 1988. He co-edited
the book Curiosity and
Exploration, focusing on how
intrinsic rewards affect behavior
in children. Henderson received
his bachelor's and master's degrees
from Bucknell University and
his doctorate from the University
of Minnesota.
WCU Colleges Present Awards
Awards for teaching, service, and scholarship were presented on a college-level
at the end of spring semester. The following is a listing of award
winners by college:
College of Arts and Sciences
• Curtis Wood (History) received
the Creighton Sossomon Professor­ship
for outstanding teacher-scholars
in American, English or European
history. Appointment to the
professorship is for a three-year term.
• Richard Bruce (Biology and
director, Highlands Biological
Station) received the H.F. and
Katherine P. Robinson Professorship.
• Robert Holquist (Music and
director of choral activities) received
the James Dooley Excellence in
Music Teaching Award, which carries
a $500 cash stipend.
• Betty Farmer (Communication
and Theatre Arts) received the Board
of Governors College of Arts and
Sciences Teaching Award, which
carries a $1,000 prize.
• Faculty members in the College of
Arts and Sciences also presented
acting dean J.C. Alexander Jr. with
a ""lifetime achievement award"" in
appreciation for service as acting
dean of the college.
The College of Business
• Roger Lirely (Accounting and
Information Systems) received the
Jay I. Kneedler Professor of
Excellence Award, which includes a
$1,000 cash prize and a plaque.
• Board of Governors Creative and
Innovative Teaching Awards went to
Julie Johnson (Business Adminis­tration,
Law and Marketing); Susan
Kask (Economics); Reagan
McLaurin (Business Administration,
Law and Marketing); and Max
Schreiber (Economics, Finance and
International Business). Each award
carries a $250 stipend.
The College of Education and
Allied Professions
• Carol Burton (director, Teaching
Fellows) received the annual Taft B.
Botner Award for Superior Teaching,
which includes a $750 cash prize and
a plaque.
• Board of Governors Awards for
Superior Teaching and $250 stipends
went to Barbara Bell (Elementary
and Middle Grades Education and
director, Reading Center); Cindy
Cavanaugh (Health and Human
Performance); Richard Haynes
(Administration, Curriculum and
Instruction and director of field
experiences and teacher education);
and Hedy White (Psychology).
The College of Applied Sciences
• The Board of Governors Innovation
in Teaching Award, which carries a
$1,000 stipend, went to Walter
Floreani (Health Sciences).
Trustees Approve Appointments and
Campus Name Changes
WCU's board of trustees approved a number of administrative appoint­ments
for the coming year at its quarterly meeting June 10.
• Terry L. Ballman, assistant professor of Hispanic studies at the
University of Northern Colorado, as associate professor and head of the
Department of Modern Foreign Languages.
• Paul F. Brandt as head of the Department of Chemistry and Physics.
• James A. Lewis as head of the Department of History.
• Carol C. Stephens as director of the Master of Science in Nursing ~
Program.
• Paul Wright as head of the Department of Biology.
• Kathleen S. Wright as head of the Department of Communication
and Theatre Arts.
• Jerry L. Kinard to continue as head of the Department of Manage­ment
through spring 1999.
• John A. Wade III to continue as head of the Department of Econom­ics,
Finance and International Business through spring 2000.
The trustees also approved several administrative and departmental
changes within the College of Business. The department ","The Reporter, June 1998",,,"Hunter Library Digital Collections, Western Carolina University, Cullowhee, NC 28723;",,core
227007466,2018-01-01T00:00:00,"Internet of Things: Technologies and Applications for a New Age of Intelligence outlines the background and overall vision for the Internet of Things (IoT) and Cyber-Physical Systems (CPS), as well as associated emerging technologies. Key technologies are described including device communication and interactions, connectivity of devices to cloud-based infrastructures, distributed and edge computing, data collection, and methods to derive information and knowledge from connected devices and systems using artificial intelligence and machine learning. Also included are system architectures and ways to integrate these with enterprise architectures, and considerations on potential business impacts and regulatory requirements.  New to this edition: • Updated material on current market situation and outlook. • A description of the latest developments of standards, alliances, and consortia. More specifically the creation of the    Industrial Internet Consortium (IIC) and its architecture and reference documents, the creation of the Reference   Architectural Model for Industrie 4.0 (RAMI 4.0), the exponential growth of the number of working groups in the Internet   Engineering Task Force (IETF), the transformation of the Open Mobile Alliance (OMA) to OMA SpecWorks and the introduction of OMA LightweightM2M device management and service enablement protocol, the initial steps in the specification of the architecture of Web of Things (WoT) by World Wide Consortium (W3C), the GS1 architecture and    standards,  the transformation of ETSI-M2M to oneM2M, and a few key facts about the Open Connectivity Forum (OCF),   IEEE, IEC/ISO, AIOTI, and NIST CPS. • The emergence of new technologies such as distributed ledgers, distributed cloud and edge computing, and the use of   machine learning and artificial intelligence for IoT. • A chapter on security, outlining the basic principles for secure IoT installations. • New use case description material on Logistics, Autonomous Vehicles, and Systems of CPS      Standards organizations covered: IEEE, 3GPP, IETF, IEC/ISO, Industrial Internet Consortium (IIC), ITU-T, GS1, Open    Geospatial Consortium (OGC), Open Mobile Alliance (OMA, e.g. LightweightM2M), Object Management Group (OMG,   e.g. Business Process Modelling Notation (BPMN)), oneM2M, Open Connectivity Forum (OCF), W3C     Key technologies for IoT covered: Embedded systems hardware and software, devices and gateways, capillary networks, local and wide area networking, IoT data management and data warehousing, data analytics and big data, complex event processing and stream analytics, control systems, machine learning and artificial intelligence, distributed   cloud and edge computing, and business process and enterprise integration.     In-depth security solutions for IoT systems     Technical explanations combined with design features of IoT and use cases, which help the development of real-world solutions     Detailed descriptions of the architectures and technologies that form the basis of IoT     Clear examples of IoT use cases from real-world implementations such as Smart Grid, Smart Buildings, Smart Cities,    Logistics and Participatory Sensing, Industrial Automation, and Systems of CPS     Market perspectives, IoT evolution, and future outloo",Internet of Things: technologies and applications for a new age of intelligence,,,'Cambridge University Press (CUP)',,core
268674228,2018-08-01T07:00:00,"Cyber-Physical Systems (CPS) seamlessly integrate computation, networking and physical devices. A Connected and Autonomous Vehicle (CAV) system in which each vehicle can wirelessly communicate and share data with other vehicles or infrastructures (e.g., traffic signal, roadside unit), requires a Transportation Cyber-Physical System (TCPS) for improving safety and mobility, and reducing greenhouse gas emissions.  Unfortunately, a typical TCPS with a centralized computing service cannot support real-time CAV applications due to the often unpredictable network latency, high data loss rate and expensive communication bandwidth, especially in a mobile network, such as a CAV environment. Edge computing, a new concept for the CPS, distributes the resources for communication, computation, control, and storage at different edges of the systems. TCPS with edge computing strategy forms an edge-centric TCPS. This edge-centric TCPS system can reduce data loss and data delivery delay, and fulfill the high bandwidth requirements.
Within the edge-centric TCPS, Vehicle-to-X (V2X) communication, along with the in-vehicle sensors, provides a 360-degree view for CAVs that enables autonomous vehiclesâ€™ operation beyond the sensor range. The addition of wireless connectivity would improve the operational efficiency of CAVs by providing real-time roadway information, such as traffic signal phasing and timing information, downstream traffic incident alerts, and predicting future traffic queue information. In addition, temporal variation of roadway traffic can be captured by sharing Basic Safety Messages (BSMs) from each vehicle through the communication between vehicles as well as with roadside infrastructures (e.g., traffic signal, roadside unit) and traffic management centers. In the early days of CAVs, data will be collected only from a limited number of CAVs due to a low CAV penetration rate and not from other non-connected vehicles.  This will result in noise in the traffic data because of low penetration rate of CAVs. This lack of data combined with the data loss rate in the wireless CAV environment makes it challenging to predict traffic behavior, which is dynamic over time. To address this challenge, it is important to develop and evaluate a machine learning technique to capture stochastic variation in traffic patterns over time.
This dissertation focuses on the development and evaluation of various connected and autonomous vehicles applications in an edge-centric TCPS. It includes adaptive queue prediction, traffic data prediction, dynamic routing and Cooperative Adaptive Cruise Control (CACC) applications. An adaptive queue prediction algorithm is described in Chapter 2 for predicting real-time traffic queue status in an edge-centric TCPS. Chapter 3 presents noise reduction models to reduce the noise from the traffic data generated from the BSMs at different penetration of CAVs and evaluate the performance of the Long Short-Term Memory (LSTM) prediction model for predicting traffic data using the resulting filtered data set.  The development and evaluation of a dynamic routing application in a CV environment is detailed in Chapter 4 to reduce incident recovery time and increase safety on a freeway. The development of an evaluation framework is detailed in Chapter 5 to evaluate car-following models for CACC controller design in terms of vehicle dynamics and string stability to ensure user acceptance is detailed in Chapter 5.
Innovative methods presented in this dissertation were proven to be providing positive improvements in transportation mobility. These research will lead to the real-world deployment of these applications in an edge-centric TCPS as the dissertation focuses on the edge-centric TCPS deployment strategy. In addition, as multiple CAV applications as presented in this dissertation can be supported simultaneously by the same TCPS, public investments will only include infrastructure investments, such as investments in roadside infrastructure and back-end computing infrastructure. These connected and autonomous vehicle applications can potentially provide significant economic benefits compared to its cost",Connected and Autonomous Vehicles Applications Development and Evaluation for Transportation Cyber-Physical Systems,,,Clemson University Libraries,,core
250088997,2018-08-24T11:49:53,"Nos aterros sanitários, a camada de base usada para a impermeabilização deve garantir que não haja migração de poluentes para o meio ambiente, sendo denominada de liner. A avaliação dos mecanismos que regem o transporte de contaminantes nos materiais geológicos pode ser realizada em laboratório por meio de ensaios de equilíbrio em lote, que consiste em uma forma rápida de estimar a adsorção de cátions metálicos pelos solos. O objetivo desse trabalho foi avaliar a adsorção de metais tóxicos em dois liners de solos compactados, com composições diferentes, utilizando os ensaios de equilíbrio em lote. Os solos estudados compreendem ao liner da base de uma célula experimental, que simula um aterro sanitário, e um liner da base de um aterro em escala real, localizado no município de Campina Grande-PB. O liner usada na célula experimental é composta por solo natural compactado, enquanto que no aterro sanitário, foi realizada a adição de bentonita ao solo local, sendo essa mistura, posteriormente, compactada. As amostras de solos do liner da célula experimental foram coletadas em uma jazida localizada no município de Boa Vista-PB, a qual encontra-se situado a 50 km do município de Campina Grande-PB. O solo do liner usado no aterro sanitário, foi coletado na área do próprio aterro, que se localiza em Catolé de Boa Vista, distrito de Campina Grande-PB. As amostras dos respectivos liners foram coletadas, armazenadas no Laboratório de Geotecnia Ambiental, do Departamento de Engenharia Civil da Universidade Federal de Campina Grande, e preparadas para a caracterização geotécnica, físico-química e mineralógica. No ensaio de equilíbrio em lote foram utilizadas soluções com proporção solo-solução 1: 12,5 (4g de solo seco para 50 mL de solução). As isotermas experimentais obtidas foram ajustadas estatisticamente aos modelos físico-químicos de adsorção linear, de Langmuir e Freundlich. Os ajustes das curvas experimentais foram realizados pelo método interativo de Gauss-Newton. A ferramenta utilizada foi o software STATISTICA 8.0, utilizando modelos não-lineares e empregando o método dos mínimos quadrados, considerando como critério o maior coeficiente de determinação (R2). O modelo de Langmuir e de Freundlich se ajustaram melhor às isotermas do solo do liner da célula experimental, enquanto que a adsorção ocorrida no liner do aterro sanitário foi melhor ajustada ao modelo de Langmuir. Os diferentes solos estudados neste trabalho apresentaram propriedades adsortivas para todos os metais, o que o viabiliza para ser empregado em camadas de base de solo compactado para aterros sanitário, além de outras aplicações em geotecnia ambiental. Dentre os metais pesquisados, o chumbo se destacou por apresentar a maior adsorção em relação aos demais metais em ambos os solos. A caracterização geotécnica, físico-química e mineralógica, bem como o comportamento adsortivo dos solos, verificado por meio de ensaio de equilíbrio em lote, indica que os materiais estudados apresentam um relevante potencial para o uso em liners para aterros sanitários.In sanitary landfills, the base layer used for waterproofing should ensure that there is no migration of pollutants into the environment and is called a liner. The evaluation of the mechanisms governing the transport of contaminants in the geological materials can be carried out in the laboratory by means of equilibrium tests in lot, which consists of a quick way of estimating the adsorption of metallic cations by the soils. The objective of this work was to evaluate the adsorption of toxic metals in two liners of compacted soils, with different compositions, using the equilibrium assays in lot. The studied soils comprise the base liner of an experimental cell, which simulates a sanitary landfill, and a liner from the base of a real-scale landfill, located in the municipality of Campina Grande-PB. The liner used in the experimental cell is composed of compacted natural soil, while in the sanitary landfill, the addition of bentonite was carried out to the local soil, and this mixture was later compacted. Soil samples from the liner of the experimental cell were collected in a field located in the municipality of Boa Vista-PB, which is located at 50 km from the city of Campina Grande-PB. The soil of the liner used in the sanitary landfill was collected in the landfill site, which is located in Catolé de Boa Vista, a district in Campina Grande-PB. The samples of the respective liners were collected, stored in the Laboratory of Environmental Geotechnics, from the Department of Civil Engineering in the Federal University of Campina Grande, and prepared for the geotechnical, physical-chemical and mineralogical characterization. In the equilibrium assay in lot, solutions with 1: 12,5 soil-solution ratio (4g of dry soil for 50 ml solution) were used. The experimental isotherms obtained were statistically adjusted to the physical-chemical models of linear adsorption, Langmuir and Freundlich. The adjustments of the experimental curves to the models were performed using the STATISTICA 8.0 software, applying the nonlinear regression of the least squares method, using the Gauss-Newton algorithm. Considering the R2 ≥ 0.7 criterion, the Langmuir and Freundlich model were better fitted to the liner soil isotherms of the experimental cell, while the adsorption occurred in the liner of the sanitary landfill was better fitted to the Langmuir model. The different soils studied in this work presented adsorptive properties for all metals, which makes it feasible to be used in compacted soil base layers for sanitary landfills, as well as other applications in environmental geotechnics. Among the metals studied, the Pb was distinguished by the higher adsorption in relation to the other metals in both soils. The geotechnical, physical-chemical and mineralogical characterization, as well as the adsorptive behavior of the soils, verified by means of a balance test in lot, indicates that the studied materials present a relevant potential for the use in liners for sanitary landfills",Adsortion of toxic metals in compacted soil liners in sanitary lands.,,,UFCG,,core
303786250,2018-01-01T00:00:00,"Irrigation management is a considerable time investment for many sugarcane farmers. Better irrigation practices can lead to improved yields through less water stress, and reduce water usage to deliver economic benefits for farmers. The reduced runoff and deep drainage from excess irrigation can also deliver benefits to the environment.



The Internet of Things (IoT) is about allowing things to sense, to communicate, and thus creating opportunities for more direct integration between the physical world and computer-based systems. IoT has been transforming all spheres of life into smart homes, smart cities, and smart healthcare. Today’s farms can leverage IoT to remotely monitor sensors, manage and control harvesters and irrigation equipment, and utilise artificial intelligence based analytics to quickly analyse operational data combined with third party information, to provide new insights and improve decision-making.



This project focuses on improving irrigation management by integrating the auto-irrigation system (e.g., WiSA) and IrrigWeb (a sugarcane irrigation scheduling tool) to provide a smarter irrigation solution using IoT. The system generates a two-way communication channel between these two platforms, which allows them to share data. Specifically, the uplink program (WiSA to IrrigWeb) was developed and deployed in a Burdekin farm. It connects the farmer’s WiSA to IrrigWeb, by uploading irrigation data automatically. The farmer’s irrigation records are automatically loaded into IrrigWeb. This saves the farmer time and makes the scheduling more efficient. Another benefit is that the farmer can now see the exact amount being applied to each field, and make modifications to the irrigation management, if required. Moreover, automating the data transfer from WiSA to IrrigWeb will greatly improve the potential for uptake and use of technologies like IrrigWeb. On the other hand, the downlink program (IrrigWeb to WiSA) will be developed to automatically apply scheduling from IrrigWeb to WiSA. Combining the uplink and downlink programs, a smarter irrigation management system can automatically control sugarcane irrigation, and ultimately make sugarcane irrigation fully autonomous",Smarter irrigation management in the sugarcane farming system using internet of things,,,Australian Society of Sugar Cane Technologists,,core
212471168,2018-01-01T00:00:00,"Autonomous cooperative driving systems require the integration of research activities in the field of embedded systems, robotics, communication, control and artificial intelligence in order to create a secure and intelligent autonomous drivers behaviour patterns in the traffic. Beside autonomous vehicle management, an important research focus is on the cooperation behaviour management. In this paper, we propose hybrid automaton modelling to emulate flexible vehicle Platoon and vehicles cooperation interactions. We introduce novel coding function for Platoon cooperation behaviour profile generation in time, which depends of vehicles number in Platoon and behaviour types. As the behaviour prediction of transportation systems, one of the primarily used methods of artificial intelligence in Intelligent Transport Systems, we propose an approach towards NARX neural network prediction of Platoon cooperation behaviour profile. With incorporation of Platoon manoeuvres dynamic prediction, which is capable of analysing traffic behaviour, this approach would be useful for secure implementation of real autonomous vehicles cooperation",Hybrid Automaton Based Vehicle Platoon Modelling and Cooperation Behaviour Profile Prediction,10.17559/TV-20170308230100,https://core.ac.uk/download/212471168.pdf,'Mechanical Engineering Faculty in Slavonski Brod',,core
168394807,2018-01-01T00:00:00,"This thesis presents a novel synthetic environment for supporting advanced explorations of user interfaces and interaction modalities for future transport systems. The main goal of the work is the definition of novel interfaces solutions designed for increasing trust in self-driving vehicles. The basic idea is to provide insights to the passengers concerning the information available to the Artificial Intelligence (AI) modules on-board of the car, including the driving behaviour of the vehicle and its decision making. Most of currently existing academic and industrial testbeds and vehicular simulators are designed to reproduce with high fidelity the ergonomic aspects associated with the driving experience. However, they have very low degrees of realism for what concerns the digital components of the various traffic scenarios. These includes the visuals of the driving simulator and the behaviours of both other vehicles on the road and pedestrians.  High visual testbed fidelity becomes an important pre-requisite for supporting the design and evaluation of future on-board interfaces. An innovative experimental testbed based on the hyper-realistic video game GTA V, has been developed to satisfy this need. To showcase its experimental flexibility, a set of selected user studies, presenting novel self-driving interfaces and associated user experience results, are described. These explore the capabilities of inducing trust in autonomous vehicles and explore Heads-Up Diplays (HUDs), Augmented Reality (ARs) and directional audio solutions. The work includes three core phases focusing on the development of software for the testbed, the definition of relevant interfaces and experiments and focused testing with panels comprising different user demographics. Specific investigations will focus on the design and exploration of a set of alternative visual feedback mechanisms (adopting AR visualizations) to gather information about the surrounding environment and AI decision making. The performances of these will be assessed with real users in respect of their capability to foster trust in the vehicle and on the level of understandability of the provided signals. Moreover, additional accessory studies will focus on the exploration of different designs for triggering driving handover, i.e. the transfer vehicle control from AI to human drivers, which is a central problem in current embodiments of self-driving vehicles.QC 20181010</p",Novel synthetic environment to design and validate future onboard interfaces for self-driving vehicles,,,,,core
216957097,2018-03-30T07:00:00,"The World Health Organization claims that there are more than 285 million blind and visually impaired people in the world. In the US, 25 million Americans suffer from total or partial vision loss. As a result of their impairment, they struggle with mobility problems, especially the risk of falling. According to the National Council On Aging, falls are among the primary causes for fatal injury and they are the most common cause of non-fatal trauma-related hospital admissions among older adults. Visibility, an organization that helps visually impaired people, reports that people with visual impairments are twice as likely to fall as their sighted counterparts.
The Centers for Disease Control and Prevention reported that 2.5 million American adults were treated for fall-related injuries in 2013, leading to over 800,000 hospitalizations and over 27,000 deaths. The total cost of fall injuries in the United States in 2013 was $31 billion, and the financial total is expected to rise to $67.7 billion by 2020. Reducing the amount of these unexpected hospital visits saves money and expands the quality of life for the affected population.
Technology has completely revolutionized how nowadays activities are conducted and how var- ious tasks are accomplished, and mobile devices are at the center of this paradigm shift. According to the Pew Research Center, 64% of American adults own a smartphone currently, and this number is trending upward. Mobile computing devices have evolved to include a plethora of data sensors that can be manipulated to create solutions for humanity, including fall prevention.
Fall prevention is an area of research that focuses on strengthening safety in order to prevent falls from occurring. Many fall prevention systems use sensing devices to measure the likelihood of a fall. Sensor data are usually processed using computer vision, data mining, and machine learning techniques.
This work pertains to the implementation of a smartphone-based fall prevention system for the elderly and visually impaired. The system consists of two modules: fall prevention and fall detection. Fall prevention is in charge of identifying tripping hazards in the user’s surroundings. Fall detection is in charge of detecting when falls happen and alerting a person of interest. The proposed system is challenged by multiple problems: it has to run in near real time, it has to run efficiently in a smartphone hardware, it has to process structured and unstructured environments, and many more related to image analysis (occlusion, motion blur, computational complexity, etc).
The fall prevention module is divided into three parts, floor detection, object-on-floor detection, and distance estimation. The evaluation process of the best approach for floor detection achieved an accuracy of 92%, a precision of 88%, and a recall of 92%. The evaluation process of the best approach for object-on-floor detection achieved an accuracy of 90%, a precision of 56%, and a recall of 78%. The evaluation process of the best approach for distance estimation achieved a MSE error of 0.45 meters.
The fall detection module is approached from two perspectives, using inertial measuring units (IMU) embedded in today’s smartphones, and using a 2D camera. The evaluation process of the solution using IMUs achieved an accuracy of 83%, a precision of 89%, and a recall of 58.2%. The evaluation process of the solution that uses a 2D camera achieved an accuracy of 85.37% and a recall of 70.97%",A Fall Prevention System for the Elderly and Visually Impaired,,https://core.ac.uk/download/216957097.pdf,Scholar Commons,,core
196141017,2018,"According to statistics, every fifth married couple is faced with the inability to conceive a child. Male germ cells are very vulnerable, and the growing number of cases of male infertility confirms that in today's world there are many factors that affect the activity of spermatozoa and their number. But the important thing is not so much their quantity, but quality. The spermogram is an objective method of laboratory diagnosis, which allows to accurately assess the man’s ability to fertilize by analyzing ejaculate for a number of key parameters. Only a spermogram can answer the question of a possible male infertility and the presence of urological diseases. When constructing spermograms, it is important to determine not only the number of good spermatozoa, but also their morphology and mobility. Therefore, research and improvement of some stages of spermogramm is the purpose of the study. This article addresses the problem of classification of spermatozoa in good and bad ones, taking into account their mobility and morphology, using methods of machine learning. In order to implement the first stage of machine learning (with a teacher) in the graphic editor, educational specimens (training sample) were created. The training was implemented by three methods: the method of support vector machine, the logistic regression and the method of K - the nearest neighbors. As a result of testing, the method K - the nearest neighbors is chosen. At the testing stage, a sample of 15 different spermatozoa was used in different variations of rotation around their axis. The test sample did not contain specimens from the training sample and was formed taking into account the morphological characteristics of the spermatozoa, but did not copy them from the training sample. At the final stage of study, the program's functioningwas tested on real data.За статистикою, кожна п'ята подружня пара стикається з неможливістю зачаття дитини. Чоловічі статеві клітини дуже вразливі, зростаюче число випадків чоловічого безпліддя підтверджує, що в сучасному світі дуже багато чинників, які впливають і на активність сперматозоїдів і на їх кількість. Та важливою є не стільки їх кількість, скільки якість. Спермограма є об'єктивним методом лабораторної діагностики, що дозволяє максимально точно оцінити здатність до запліднення чоловіка, проаналізувавши еякулят за рядом найважливіших параметрів. Тільки спермограма здатна відповісти на питання про можливе чоловіче безпліддя та про наявність урологічних захворювань. При побудові спермограми, важливо визначати не тільки кількість добрих сперматозоїдів, але й їх морфологію та рухливість. Тому дослідження та вдосконалення деяких етапів спермограми і є метою дослідження. У даній статті вирішується задача класифікації сперматозоїдів на добрі та погані, з урахуванням їх рухливості та морфології, із застосуванням методів машинного навчання. Для реалізації першого етапу машинного навчання (з вчителем) у графічному редакторі були створені навчальні екземпляри (тренувальна вибірка). Навчання було реалізована трьома методами: методом опорних векторів, логістична регресія та метод К - найближчих сусідів. За результатами тестування обрано метод К - найближчих сусідів. На етапі тестування використовувалася вибірка з 15 різних сперматозоїдів в різних варіаціях обертання навколо своєї осі. Тестова вибірка не містила примірників з тренувальної вибірки і була сформована з урахуванням морфологічних особливостей сперматозоїдів, але не копіювала їх з тренувальної вибірки. На завершальному етапі навчання роботу програми було протестовано на реальних даних",Застосування методів машинного навчання для вирішення задачі аналізу біологічних даних,10.20998/2522-9052.2018.3.01,,"Національний технічний університет ""Харківський політехнічний інститут""",,core
199298351,2018-01-01T00:00:00,"As a result of the digitalization of the power business in Norway and Europa, a lot of new possibilities and challenges arise. In 2014 an expert committee one outlined a proposal for the future grid company structure in Norway (Reiten, 2014). In addition, new technologies are being implemented in the system. Wind power, solar power, un-regulated small hydro power production, battery storage domestic and industrial and electrification of transport. Transmission System Operators (TSOs) have a responsibility to supply industry and communities with reliable electric power. However, the operators have been virtually blind to slowly occurring changes in the load profile that reduce the expected regularity of the power supply. This paper will focus on the possibilities and challenges the power business are facing. The paper will describe what technologies is needed i.e Real time probabilistic risk calculations, artificial intelligence, machine learning and smart grid technology. The main question is: can the power business and the introduction of new system tools manage without probabilistic risk calculation for making use of the digitalization and the corresponding big data",Digitalization of the power business: How to make this work?,,,,,core
162306906,2018-01-01T00:00:00,"This thesis presents a novel synthetic environment for supporting advanced explorations of user interfaces and interaction modalities for future transport systems. The main goal of the work is the definition of novel interfaces solutions designed for increasing trust in self-driving vehicles. The basic idea is to provide insights to the passengers concerning the information available to the Artificial Intelligence (AI) modules on-board of the car, including the driving behaviour of the vehicle and its decision making. Most of currently existing academic and industrial testbeds and vehicular simulators are designed to reproduce with high fidelity the ergonomic aspects associated with the driving experience. However, they have very low degrees of realism for what concerns the digital components of the various traffic scenarios. These includes the visuals of the driving simulator and the behaviours of both other vehicles on the road and pedestrians.  High visual testbed fidelity becomes an important pre-requisite for supporting the design and evaluation of future on-board interfaces. An innovative experimental testbed based on the hyper-realistic video game GTA V, has been developed to satisfy this need. To showcase its experimental flexibility, a set of selected user studies, presenting novel self-driving interfaces and associated user experience results, are described. These explore the capabilities of inducing trust in autonomous vehicles and explore Heads-Up Diplays (HUDs), Augmented Reality (ARs) and directional audio solutions. The work includes three core phases focusing on the development of software for the testbed, the definition of relevant interfaces and experiments and focused testing with panels comprising different user demographics. Specific investigations will focus on the design and exploration of a set of alternative visual feedback mechanisms (adopting AR visualizations) to gather information about the surrounding environment and AI decision making. The performances of these will be assessed with real users in respect of their capability to foster trust in the vehicle and on the level of understandability of the provided signals. Moreover, additional accessory studies will focus on the exploration of different designs for triggering driving handover, i.e. the transfer vehicle control from AI to human drivers, which is a central problem in current embodiments of self-driving vehicles.QC 20181010</p",Novel synthetic environment to design and validate future onboard interfaces for self-driving vehicles,,,,,core
211490163,2015-01-01T00:00:00,"Terrestrial isopods (Oniscidea) represent a monophyletic suborder of the order Isopoda (Crustacea, Malacostraca, Peracarida). They are the only group of crustaceans fully adapted to live on land and occur in all the ecosystems from sea level to high mountains, from forests to deserts, excluding the polar areas. The majority of terrestrial isopods need a high degree of air humidity for survival, due to their limited ability to preserve water. The genus Trachelipus comprises of relatively stenoecious animals living in habitats generally threatened by human activities, such as humid forest sites and riparian habitats. It includes some 50 species distributed around the Palaearctic, with 8 species recorded from Greece, 4 endemic to the country (i.e. Greek endemics: T. palustris, T. aegaeus, T. cavaticus, T. n. sp.; European species: T. camerani, T. squamuliger, T. ratzauti, T. arcuatus). The distribution of species is discontinuous due to the increasing fragmentation of its habitats and the expansion of agricultural land and dry woodland. Projected climatic change will restrict further gene flow between Trachelipus populations, as dry habitats are expected to expand in Greece. The phylogenetic relationships among these animals are still largely unknown because robust analyses have started to appear only relatively recently. Species-level taxonomy has been based mainly on a few secondary sexual characters of males, although recent analyses based on molecular markers have indicated that species definitions based on morphology may underestimate the true levels of divergence among populations. Furthermore, within several genera or species groups, morphological characters do not provide clear-cut taxonomic resolution, so that many changes in the interpretation of nominal species have appeared in the literature. In my PhD Thesis I performed a phylogenetic analysis of Trachelipus spp. distributed in Greece using a population by population approach. I have sampled 123 populations from several sites in mainland and insular Greece and I have also incorporated samples of European origin in order to check both for the morphological and the genetic homology. My main goal was to study the phylogeny of these species, to reveal any possible geographical and/or palaeogeographical pattern and to particularize the distribution of each species within the Greek territory in order to better estimate possible geographic structure in the patterns of divergence among populations, and to throw new light in their systematics. After total DNA extraction, I employed three DNA markers; the mitochondrial 16S rRNA and the Cytochrome Oxidase subunit I, that are widely used for the phylogenetic studies at the species and the population level, and the nuclear sodium-potassium ATPase a-subunit, which is a P-type ATPase ion co-transporter responsible for maintaining electrochemical potential differences across cell membranes, and is essential for cell signalling and secondary transport. Due to the poor preservation of several specimens, I propose a modified DNA extraction protocol, which returned highly positive results in terms of the quality of the total extracted DNA. After PCR amplification of the gene markers I calculated the genetic divergence within and among the populations and species studied, as well as their phylogenetic relationships. The methods for phylogenetic reconstruction that I used were Maximum Parsimony (MP), Maximum Likelihood and Bayesian Inference (BI) for each sequence data and the concatenated datasets. In order to evaluate the biogeographic history and to estimate the chronology of diversification events among the species and their populations I have implement a calibrated molecular clock using 12 alternative scenarios in BEAST [(i.e. palaeogeographical events such as the formation of the Mid- Aegean Trench 12-9 mya and the Messinian Salinity Crisis 5,971 - 5,33 mya), different tree priors (i.e. birth-death process; Yule process), different clocks (strict, Uncorrelated Exponential Relaxed Clock and Uncorrelated Lognormal relaxed clock) and different rates of substitution for COI and 16S rRNA markers (COI = 1.56%-1.72% after Poulakakis & Sfenthourakis 2008; 16Sr RNA = 0,14% after Held 2001)]. The different scenarios implemented where tested and ranked using the Maximum Likelihood Estimators as they were calculated using the path sampling and stepping stone analyses. Αll phylogenetic methods (MP, ML, BI) - produced trees with quite congruent topologies that reveal incongruence with current taxonomy. Some populations that are considered conspecific exhibit large genetic distances and cluster in different clades. The most prominent example is that of T. palustris’ populations that cluster into three different clades that exhibit very large genetic divergence (Kimura 2-p) in all three genetic markers used (COI= 27.2%; 16S rRNA=15.2%; NaK ATPase=4.3% ). In general, it can be argued that the genetic distances recorded in the present study are quite large compared with those reported for different species and even genera in other studies of terrestrial isopods. On the other hand, the Cretan populations that belong to T. cavaticus and T. aegaeus are clustered together and exhibit low genetic distances (COI: 12.3%, 16S rRNA: 6.4%, NaK: 2.9%). Under the light of these results, a revision of the Greek Trachelipus species taxonomical status is in need. The results of my Thesis corroborate the possibility of probably two cryptic species for the “T. palustris” populations; one would include the populations distributed to N. Evia and the peninsula of Magnisia and the second would apply to the populations distributed to the N-NW Peloponnese and Western Greece area. On the other hand the results of my Thesis point to the merger of Cretan populations of Τ. cavaticus and T. aegaeus to a single species (this would be T. aegaeus since it has priority). The highly-structured phylogenetic trees agree with the palaeogeographic history of the Greek territory since the Miocene (23,03 mya). The distribution of the species seem to follow the tectonic and kinematic reconstruction of the Aegean region of Greece and western Turkey depicting the linkage between the subduction-related mountain building, continental extension, and the resulting formation and exhumation of metamorphic rocks to plate motions and mantle dynamics in the area. The chronology results of the clades also corroborate to the geological events and the reconstruction of the Greek territory that account for the today’s geography and geology of the region. My Thesis comprises the first comprehensive phylogenetic study of terrestrial crustacean populations in Greece with positive results concerning the delineation of both the distributional range and the phylogeography of these animals with respect to the palaeogeographic history and the current geography of insular and mainland Greece. Both the phylogeny presented in my Thesis and the genetic distances separating the populations appear to justify the necessity of further investigation into the systematics of the Greek Trachelipus species. My study brought evidence that morphology inadequately describes the real variation inside and among species. Hence, diagnoses based on the morphological characters used so far for the delineation of Trachelipus species should be reconsidered under the light of these more extensive molecular phylogenetic analyses.Τα χερσόβια ισόποδα (Oniscidea) αντιπροσωπεύουν έναν μονοφυλετικό κλάδο της τάξης ισοπόδων (Crustacea, Malacostraca, Peracarida). Είναι η μόνη ομάδα καρκινοειδών που έχουν πλήρως προσαρμοστεί να ζουν στη στεριά, με εξάπλωση σε όλα τα ενδιαιτήματα. Εμφανίζονται από το επίπεδο της θάλασσας έως και σε ψηλά υψόμετρα, από τα δάση μέχρι και στις ερήμους, με εξαίρεση τις πολικές περιοχές. Η πλειοψηφία των χερσόβιων ισοπόδων χρειάζεται ένα υψηλό βαθμό υγρασίας του αέρα για την επιβίωσή της, εξαιτίας της περιορισμένης δυνατότητάς τους να διατηρήσουν το νερό. Το γένος Trachelipus περιλαμβάνει σχετικά στενόοικους οργανισμούς οι οποίοι διαβιούν σε οικοτόπους που υπόκεινται σε απειλή λόγω των ανθρώπινων δραστηριοτήτων, όπως τα υγρά δάση και οι παρόχθιες διαπλάσεις. Περιλαμβάνει περί τα 50 είδη που εξαπλώνονται σε όλη την Παλαιαρκτική, με 8 είδη να έχουν καταγραφεί στην Ελλάδα, 4 εκ των οποίων είναι ενδημικά στη χώρα (ελληνικά ενδημικά: Τ. palustris, Τ. aegaeus, Τ. cavaticus, Τ. n. sp., ευρωπαϊκά είδη: Τ. camerani Τ, Τ. squamuliger, Τ. razzauti, Τ. arcuatus). Η κατανομή των ειδών είναι ασυνεχής λόγω του αυξανόμενου κατακερματισμού των ενδιαιτημάτων τους και της επέκτασης της γεωργικής γης και της αποψίλωσης των δασών. Η επίδραση της προϊούσας κλιματικής αλλαγής ενδέχεται να περιορίσει περαιτέρω τη ροή γονιδίων μεταξύ των πληθυσμών Trachelipus, καθώς τα ξηρά ενδιαιτήματα αναμένεται να αυξηθούν στην Ελλάδα. Οι φυλογενετικές σχέσεις μεταξύ αυτών των οργανισμών είναι ακόμη σε μεγάλο βαθμό άγνωστες καθώς πιο συστηματικές αναλύσεις έχουν αρχίσει να διενεργούνται σχετικά πρόσφατα. Η διάκριση μεταξύ των ειδών έχει γίνει βάσει περιορισμένου αριθμού δευτερευόντων φυλετικών μορφολογικών χαρακτήρων των αρσενικών ατόμων, ωστόσο οι πρόσφατες αναλύσεις με μοριακούς δείκτες καταδεικνύουν ότι η ταξινόμηση των ειδών με βάση τη μορφολογία μπορεί να υποτιμά τα πραγματικά επίπεδα της απόκλισης μεταξύ των πληθυσμών. Επιπλέον, μέσα σε διάφορα γένη ή ομάδες ειδών, οι χρησιμοποιούμενοι μορφολογικοί χαρακτήρες δεν παρέχουν σαφή ταξινομική ανάλυση, κι έτσι έχουν παρουσιαστεί πολλές αλλαγές στην ονοματολογία των περιγεγραμμένων ειδών στη βιβλιογραφία. Ο κύριος στόχος της διατριβής ήταν η μελέτη των φυλογενετικών σχέσεων μεταξύ των ειδών, η διαπίστωση κάποιου ενδεχόμενου γεωγραφικού ή/και παλαιογεωγραφικού προτύπου και η αποσαφήνιση της κατανομής κάθε είδους εντός της ελληνικής επικράτειας, προκειμένου να εκτιμηθεί καλύτερα μια πιθανή γεωγραφική δομή στα πρότυπα της απόκλισης μεταξύ των πληθυσμών, και να επανεξεταστεί η συστηματική τους. Μετά την απομόνωση ολικού DNA, χρησιμοποίησα τρεις μοριακούς δείκτες: τους μιτοχονδριακούς δείκτες 16S rRNA και την υπομονάδα Ι της κυτοχρωμικής οξειδάσης, οι οποίοι χρησιμοποιούνται ευρέως για τις φυλογενετικές μελέτες στο διαειδικό και ενδοειδικό επίπεδο, και τον πυρηνικό γενετικό τόπο της α-υπομονάδας της αντλίας NaK (NaK ATPase a-subunit), η οποία είναι μία Ρ-τύπου αντλία, συμμεταφορέας ιόντων για τη διατήρηση της διαφοράς του ηλεκτροχημικού δυναμικού μεταξύ των κυτταρικών μεμβρανών, και είναι απαραίτητη για την κυτταρική σηματοδότηση και τη δευτερογενή μεταφορά. Λόγω της κακής συντήρησης αρκετών δειγμάτων, προτείνω ένα τροποποιημένο πρωτόκολλο απομόνωσης DNA, το οποίο έδωσε πολύ θετικά αποτελέσματα αναφορικά με την ποιότητα του απομονωμένου DNA. Μετά τον πολλαπλασιασμό των τριών γενετικών δεικτών με τη μέθοδο της PCR, υπολόγισα τις γενετικές αποστάσεις εντός και μεταξύ των μελετηθέντων πληθυσμών και ειδών, καθώς και τις μεταξύ τους φυλογενετικές σχέσεις. Για τη φυλογενετική ανασύσταση χρησιμοποίησα τις μεθόδους της Μέγιστης Φειδωλότητας (MP), της Μέγιστης Πιθανοφάνειας (ML) και της Μπεϊεσιανής Συμπερασματολογίας (BI) για κάθε γενετικό τόπο και για τις συνδυασμένες αναλύσεις. Προκειμένου να αξιολογηθεί η βιογεωγραφική ιστορία και να εκτιμηθεί η χρονολόγηση των γεγονότων διαφοροποίησης μεταξύ των ειδών και των πληθυσμών, εφάρμοσα ένα βαθμονομημένο μοριακό ρολόι εξετάζοντας 12 εναλλακτικές υποθέσεις με το πρόγραμμα BEAST [α: παλαιογεωγραφικά γεγονότα (όπως η διαμόρφωση της Μεσαιγαιακής Τάφρου 12-9 εκ. χρόνια πριν και η κρίση αλατότητας κατά το Μεσσήνιο 5,971 - 5,33 εκ. χρόνια πριν), β: διαφορετικές παραμέτρους δέντρων (tree priors: διαδικασία της γέννησης-θανάτου, διαδικασία Yule), γ: διαφορετικά ρολόγια (σταθερό μοριακό ρολόι, μη συσχετιζόμενο λογαριθμικό μοντέλο μη σταθερού ρυθμού εξέλιξης, μη συσχετιζόμενο εκθετικό μοντέλο μη σταθερού ρυθμού εξέλιξης) και δ: διαφορετικούς ρυθμούς υποκατάστασης για τoυς μιτοχονδριακούς δείκτες COI και 16S rRNA (COI = 1,56% -1,72% Poulakakis & Sfenthourakis 2008, 16Sr RNA = 0,14% Held 2001). Οι διαφορετικές υποθέσεις που δοκιμάστηκαν κατατάχθηκαν με βάση τους οριακούς εκτιμητές πιθανοφάνειας που υπολογίστηκαν με τις μεθόδους Path Sampling και Stepping Stone. Όλες οι φυλογενετικές μέθοδοι (MP, ML, BI) - κατέληξαν σε δέντρα με κοινές τοπολογίες κλάδων και καταδεικνύουν ασυμφωνία με την ισχύουσα ταξινόμηση. Ορισμένοι πληθυσμοί που θεωρούνται πως ανήκουν στο ίδιο είδος παρουσιάζουν μεγάλες γενετικές αποστάσεις και ομαδοποιούνται σε διακριτούς κλάδους. Το πιο χαρακτηριστικό παράδειγμα είναι αυτό των πληθυσμών του είδους Τ. palustris που ομαδοποιούνται σε τρεις διαφορετικούς κλάδους με πολύ μεγάλη γενετική απόσταση (Kimura 2-P) και στους τρεις γενετικούς δείκτες που χρησιμοποιήθηκαν (COI = 27,2%, 16S rRNA = 15,2%, NaK ΑΤΡ = 4,3%). Σε γενικές γραμμές, οι γενετικές αποστάσεις που καταγράφονται στην παρούσα μελέτη είναι αρκετά μεγάλες σε σύγκριση με εκείνες που έχουν αναφερθεί για τα διάφορα είδη και γένη, ακόμη και σε άλλες μελέτες χερσόβιων ισοπόδων. Από την άλλη πλευρά, διαφορετικοί πληθυσμοί από την Κρήτη που ανήκουν στα είδη Τ. cavaticus και Τ. aegaeus, ομαδοποιούνται μαζί και παρουσιάζουν μικρές γενετικές αποστάσεις (COI: 12,3%, 16S rRNA: 6,4%, NaK: 2,9%). Υπό το φως αυτών των αποτελεσμάτων, κρίνεται απαραίτητη η αναθεώρηση της ταξινόμησης των ειδών Trachelipus που απαντώνται στην Ελλάδα. Τα αποτελέσματα της διατριβής μου προτείνουν την ύπαρξη δύο ενδεχομένων κρυπτικών ειδών αφενός για τους πληθυσμούς “T. palustris” που ομαδοποιούνται στον κλάδο της δυτικής ηπειρωτικής Ελλάδας και της βόρειας-βορειοδυτικής Πελοποννήσου και αφετέρου για του πληθυσμούς που ομαδοποιούνται στον κλάδο της βόρειας Εύβοιας και της χερσονήσου της Μαγνησίας Αντίθετα, με βάση τα αποτελέσματα της διατριβής προτείνω τη συγχώνευση των κρητικών πληθυσμών Τ. cavaticus και Τ. aegaeus σε ένα μόνο είδος (αυτό θα ήταν Τ. aegaeus δεδομένου ότι έχει προτεραιότητα). Τα φυλογενετικά δέντρα με την ισχυρή στατιστική υποστήριξη συμφωνούν με την παλαιογεωγραφική ιστορία της ελληνικής περιοχής από την εποχή του Μειόκαινου (23,03 εκ. χρόνια πριν). Οι γεωγραφικές κατανομές των ειδών φαίνεται να ακολουθούν την τεκτονική και κινηματική ανασυγκρότηση της περιοχής της ηπειρωτικής Ελλάδας, του Αιγαίου και της δυτικής Τουρκίας απεικονίζοντας τη σχέση μεταξύ των ορογενετικών διεργασιών, της ηπειρωτικής επέκτασης και της ανύψωσης των μεταμορφωμένων πετρωμάτων στις κινήσεις των τεκτονικών πλακών στην περιοχή. Τα αποτελέσματα της χρονολόγησης των κλάδων επιβεβαιώνουν τόσο τα γεωλογικά γεγονότα όσο και την ανασυγκρότηση του ελλαδικού χώρου που ευθύνονται για την σημερινή γεωγραφία και γεωλογία της περιοχής. Η Διατριβή μου αποτελεί την πρώτη ολοκληρωμένη μελέτη των φυλογενετικών σχέσεων σε πληθυσμούς χερσόβιων καρκινοειδών στην Ελλάδα. Τα αποτελέσματα κατέληξαν στην οριοθέτηση της γεωγραφικής εξάπλωσης των ειδών και την ανασύσταση των φυλογενετικών τους σχέσεων υπό το πρίσμα της παλαιογεωγραφικής ιστορίας και της σύγχρονης γεωγραφίας της νησιωτικής και ηπειρωτικής περιοχής. Τόσο οι φυλογενετικές σχέσεις όσο και οι γενετικές αποστάσεις μεταξύ των πληθυσμών φαίνεται να συνηγορούν στην περαιτέρω διερεύνηση της συστηματικής των ειδών του γένους. Η Διατριβή μου καταδεικνύει ότι η μορφολογία περιγράφει ανεπαρκώς την πραγματική ποικιλότητα μεταξύ των ειδών. Ως εκ τούτου, η ταξινόμηση των ειδών με βάση τους διαγνωστικούς χαρακτήρες και τα μορφολογικά χαρακτηριστικά που χρησιμοποιούνται μέχρι σήμερα για τα είδη Trachelipus θα πρέπει να επανεξεταστεί υπό το φως των νέων αυτών μοριακών φυλογενετικών αναλύσεων",Πανεπιστήμιο Πατρών,Geographic differentiation and genetic diversity of terrestrial isopods populations,,,,core
270073900,2016-10-19T00:00:00,"Two different methodologies for hydraulic head simulation were compared in this study. The first methodology is a classic numerical groundwater flow simulation model, Princeton Transport Code (PTC), while the second one is a black-box approach that uses Artificial Neural Networks (ANNs). Both methodologies were implemented in the Bavaria region in Germany at thirty observation wells. When using PTC, meteorological and geological data are used in order to compute the simulated hydraulic head following the calibration of the appropriate model parameters. The ANNs use meteorological and hydrological data as input parameters. Different input parameters and ANN architectures were tested and the ANN with the best performance was compared with the PTC model simulation results. One ANN was trained for every observation well and the hydraulic head change was simulated on a daily time step. The performance of the two models was then compared based on the real field data from the study area. The cases in which one model outperforms the other were summarized, while the use of one instead of the other depends on the application and further use of the model",'University of the Aegean',Comparison of a black-box model to a traditional numerical model for hydraulic head prediction,https://core.ac.uk/download/270073900.pdf,10.30955/gnj.002002,,core
35147195,2015-01-01T00:00:00,"This paper presents an architecture designed for serious simulation games to automatically generate game scenarios adapted to player's level and knowledge. We detail two central modules of the architecture: (1) the player model and (2) the adaptation module. The player model estimates the current knowledge of the player using a Bayesian Network (BN). The evidence variables in the BN are assigned through the observation of player's actions and the current state of the simulation. Considering the estimated player's knowledge and skills, the adaptation module uses automated planning algorithms to dynamically adjust the parameters of the simulation, in order to generate scenarios that will be well suited to improve player's knowledge and skills. We implemented our proposed game architecture in a simulation serious game named Game of Homes. The purpose of this game is to teach the basis of real estate. The player is a virtual real estate broker in a city who has to seek for brokerage contracts, estimate the value of houses, fix asked prices, perform visits, and close the deals. The player competes with other brokers driven by artificial intelligence (AI). We conducted a pilot experiment with human participants (N=10) to validate our architecture in Game of Homes. On day 1, participants were asked to take a pre-test about real estate skills taught in our game. On day 2, participants played Game of Homes for approximately 90 minutes and then filled up a motivation questionnaire. On day 3, participants took a post-test. Preliminary results show that in addition to induce strong motivation among the players, Game of Homes significantly improved real estate skills between pre-tests and post-tests. Results suggest that our serious game architecture allows (a) to induce learning process by providing content adapted to the player progression and (b) to keep the player motivated and interested during the game by adapting the challenge and providing new content",Academic Conferences and Publishing International Limited,An empirical evaluation of a serious simulation game architecture for automatic adaptation,https://core.ac.uk/download/35147195.pdf,,,core
224992044,2015-01-01T00:00:00,"In the current economic situation, characterized by a high uncertainty

in the appraisal of property values, the need of “slender” models able to operate

even on limited data, to automatically capture the causal relations between explanatory

variables and selling prices and to predict property values in the short

term, is increasingly widespread. In addition to Artificial Neural Networks

(ANN), that satisfy these prerogatives, recently, in some fields of Civil Engineering

an hybrid data-driven technique has been implemented, called Evolutionary

Polynomial Regression (EPR), that combines the effectiveness of Genetic

Programming with the advantage of classical numerical regression. In the

present paper, ANN methods and the EPR procedure are compared for the construction

of estimation models of real estate market values. With reference to a

sample of residential apartments recently sold in a district of the city of Bari (Italy),

two estimation models of market value are implemented, one based on

ANN and another using EPR, in order to test the respective performance. The

analysis has highlighted the preferability of the EPR model in terms of statistical

accuracy, empirical verification of results obtained and reduction of the

complexity of the mathematical expression",'Springer Science and Business Media LLC',Property valuations in times of crisis Artificial neural networks and evolutionary algorithms in comparison,,10.1007/978-3-319-21470-2_14,,core
53809287,2015-01-01T00:00:00,"Received 1 September 2014 Received in revised form

31 March 2015

Accepted 20 April 2015 Available online 3 July 2015

Keywords:

Photovoltaic systems Maximum power production Dynamical reconfiguration

1. Introduction

The efficiency of a photovoltaic (PV) panel is actually about 15%, but in real applications this figure is even lower because of many reasons. One of the reasons occurring especially in urban context is the mismatched operating conditions at which the panels forming a PV plant work [10]. As some panels are connected in series in order to reach a voltage level that fits with the input specifications of the commercial inverters, the presence of a partial shadowing affecting some cells or other inhomogeneity among the parameters of the cells, e.g., due to aging, failures or manufactur- ing tolerances, might cause a significant drop in the power production [24]. Producers usually install bypass diodes in the panels for mitigating the power loss in case of mismatching, but these diodes greatly change the voltage vs. current ðV  IÞ char- acteristic of the PV array. When the diodes enter into conduction for compensating a current mismatching among the cells of the string, the voltage vs. power (V  P) characteristic of the array shows more than one Maximum Power Point (MPP).

To obtain the MPP, the inverter control system is usually equipped with a Maximum Power Point Tracking (MPPT) algorithm and many methods have been proposed in the literature with the aim to find the MPP, including Fuzzy Logic and Neural Networks

n Corresponding author.

E-mail addresses: pcarotenuto@unisa.it (P.L. Carotenuto),

adellacioppa@unisa.it (A. Della Cioppa), amarcelli@unisa.it (A. Marcelli), gspagnuolo@unisa.it (G. Spagnuolo).

http://dx.doi.org/10.1016/j.neucom.2015.04.094

0925-2312/& 2015 Elsevier B.V. All rights reserved.

abstract

The dynamical reconfiguration of photovoltaic panels is a useful approach for fighting the detrimental effects of mismatching on their power production. The practical implementation of the method has been recently optimized by means of efficient and reliable relays. However, two problems remain still open. The first is to determine the optimal electrical connection among the panels that ensures the maximum power produced at the actual irradiance conditions, while the latter is to constrain the computation time of such optimal configuration to fit the need of real time applications.

We present an evolutionary approach to the first problem. It is designed for allowing a straightforward porting to an embedded system and it is aimed at reconfiguring photovoltaic panels, thus not modules like some other approaches do in literature. Simulation results confirm the reliability and convergence capabilities of the proposed method and encourage further work for the adoption of the algorithm in real time applications. The problem of minimizing the computation time is also addressed",'Elsevier BV',An Evolutionary Approach to the Dynamical Reconfiguration of PhotoVoltaic Panels,,10.1016/j.neucom.2015.04.094,,core
103633044,09/01/2016,"Abstract — In this paper we report on some of the current advances in the development of Hywacoss (Hyperspectral waterway control and security system). The objective of the Hywacoss project is to produce a real time small, light and easy to transport visible and near infrared hyperspectral detection and recognition system that autonomously monitors waterways, especially port and bay areas, and detects and classifies all the traffic, producing alerts when previously unknown objects or behavior patterns arise. Obviously, Hywacoss involves dedicated hardware and software modules, some of them based on computational intelligence methods. Here we will provide a global description of the system and a detailed analysis of some of its modules, in particular those related to hyperspectral image segmentation and the Artificial Neural Network based spectral-geometrical identification and profiling subsystems. Keywords- Hyperspectral image processing, Waterway control and security, Artificial Neural Networks",,An ANN based Hyperspectral Waterway Control and Security System,,,,core
54611646,2015,"Obiettivi: l’integrazione tra servizi sanitari e servizi socio-assistenziali è uno dei temi più rilevanti per l’amministrazione comunale di Cagliari che sta realizzando alcuni interventi di recupero funzionale di immobili dismessi in quartieri con particolari problematiche socio-economiche per creare Strutture Sanitarie Integrate denominate “Centri di salute”, rivolte in particolare modo agli anziani, ma capaci di avere ricadute sull’intera struttura sociale di riferimento.

Metodi: gli strumenti utilizzati sono quelli della pianificazione strategica e degli investimenti territoriali integrati, secondo una logica di intervento sulla struttura fisica della città coerente con la struttura demografica dei residenti e quindi capace di rispondere ai bisogni reali degli abitanti. I quartieri scelti per la sperimentazione sono “zone di margine” caratterizzati da una condizione di segregazione sociale e spaziale. Il modello di gestione è quello della medicina di iniziativa con intervento proattivo basato sulla partecipazione e coinvolgimento delle persone.

Risultati: i primi risultati mettono in evidenza che l’intervento messo in atto si presta particolarmente al coinvolgimento della popolazione anziana emarginata, che per presenza di barriere sociali, economiche, culturali ed ambientali, sottoutilizza o non si rivolge ai servizi di cura. Inoltre, alle azioni specifiche su target predefinito (ultra 70enni) vengono affiancate attività di promozione sociale del valore della salute e della cultura della responsabilità all’auto-cura individuale e collettiva.Objectives: the integration of health and social services is one of the most important issues for the municipal amministration of Cagliari which is building, restoration of functional abandoned buildings in neighborhoods with particular socio-economic problems to create Integrated Medical Information referred to as “health centers”, aimed particularly so for the elderly, but capable of having repercussions on the entire social structure of reference.

Methods: the instruments used are the strategic planning and investment territorial integrated, according to a logic of intervention on the physical structure of the city in line with the demographic structure of the residents and therefore able to respond to the real needs of the inhabitants. The districts chosen for testing are “marginal areas” characterized by a state of social and spatial segregation. The management model is one of medicine’s initiative with proactive intervention based on parteciapzione and involvement of people.

Results: the first results show that the intervention implemented is particularly the involvement of the elderly marginalized, that the presence of barriers to social, economic, and environmental culturalie, does not use care services. In addition, specific actions on target default (over 70years) are flanked promotion of social avlore health and culture of responsibility to self-care individual and collective",,Strategie e progettualità per Cagliari Città per la salute = Planning and strategies and for Cagliari Healthy City,,,,core
78940132,Jul-16,"By taking advantage of complementary communication technologies, distinct sensing functionalities and varied motion dynamics present in a heterogeneous multi-robotic network, it is possible to accomplish a main mission objective by assigning specialized sub-tasks to specific members of a robotic team. An adequate selection of the team members and an effective coordination are some of the challenges to fully exploit the unique capabilities that these types of systems can offer. Motivated by real world applications, we focus on a multi-robotic network consisting off aerial and ground agents which has the potential to provide critical support to humans in complex settings. For instance, aerial robotic relays are capable of transporting small ground mobile sensors to expand the communication range and the situational awareness of first responders in hazardous environments.

In the first part of this dissertation, we extend work on manipulation of cable-suspended loads using aerial robots by solving the problem of lifting the cable-suspended load from the ground before proceeding to transport it. Since the suspended load-quadrotor system experiences switching conditions during this critical maneuver, we define a hybrid system and show that it is differentially-flat. This property facilitates the design of a nonlinear controller which tracks a waypoint-based trajectory associated with the discrete states of the hybrid system. In addition, we address the case of unknown payload mass by combining a least-squares estimation method with the designed controller.

Second, we focus on the coordination of a heterogeneous team formed by a group of ground mobile sensors and a flying communication router which is deployed to sense areas of interest in a cluttered environment. Using potential field methods, we propose a controller for the coordinated mobility of the team to guarantee inter-robot and obstacle collision avoidance as well as connectivity maintenance among the ground agents while the main goal of sensing is carried out. For the case of the aerial communications relays, we combine antenna diversity with reinforcement learning to dynamically re-locate these relays so that the received signal strength is maintained above a desired threshold.

Motivated by the recent interest of combining radio frequency and optical wireless communications, we envision the implementation of an optical link between micro-scale aerial and ground robots. This type of link requires maintaining a sufficient relative transmitter-receiver position for reliable communications. In the third part of this thesis, we tackle this problem. Based on the link model, we define a connectivity cone where a minimum transmission rate is guaranteed. For example, the aerial robot has to track the ground vehicle to stay inside this cone. The control must be robust to noisy measurements. Thus, we use particle filters to obtain a better estimation of the receiver position and we design a control algorithm for the flying robot to enhance the transmission rate. Also, we consider the problem of pairing a ground sensor with an aerial vehicle, both equipped with a hybrid radio-frequency/optical wireless communication system. A challenge is positioning the flying robot within optical range when the sensor location is unknown. Thus, we take advantage of the hybrid communication scheme by developing a control strategy that uses the radio signal to guide the aerial platform to the ground sensor. Once the optical-based signal strength has achieved a certain threshold, the robot hovers within optical range.

Finally, we investigate the problem of building an alliance of agents with different skills in order to satisfy the requirements imposed by a given task. We find this alliance, known also as a coalition, by using a bipartite graph in which edges represent the relation between agent capabilities and required resources for task execution. Using this graph, we build a coalition whose total capability resources can satisfy the task resource requirements. Also, we study the heterogeneity of the formed coalition to analyze how it is affected for instance by the amount of capability resources present in the agents.Electrical EngineeringDoctoralUniversity of New Mexico.  Dept. of Electrical and Computer EngineeringFierro, RafaelOishi, MeekoTapia, LydiaSadler, Bria",,Exploiting Heterogeneity in Networks of Aerial and Ground Robotic Agents,,,,core
347692253,2016-11-14T00:00:00,"Nowadays, monitoring of people and events is a common matter in the street, in the industry or at home, and acoustic event detection is commonly used. This increases the knowledge of what is happening in the soundscape, and this information encourages any monitoring system to take decisions depending on the measured events. Our research in this field includes, on one hand, smart city applications, which aim is to develop a low cost sensor network for real time noise mapping in the cities, and on the other hand, ambient assisted living applications through audio event recognition at home. This requires acoustic signal processing for event recognition, which is a challenging problem applying feature extraction techniques and machine learning methods. Furthermore, when the techniques come closer to implementation, a complete study of the most suitable platform is needed, taking into account computational complexity of the algorithms and commercial platforms price. In this work, the comparative study of several platforms serving to implement this sensing application is detailed. An FPGA platform is chosen as the optimum proposal considering the application requirements and taking into account time restrictions of the signal processing algorithms. Furthermore, we describe the first approach to the real-time implementation of the feature extraction algorithm on the chosen platform",'MDPI AG',An FPGA Platform Proposal for Real-Time Acoustic Event Detection: Optimum Platform Implementation for Audio Recognition with Time Restrictions,,,,core
215178970,2016-12-16T00:00:00,"This thesis presents a “Novel Small Cell Planning Solution using Machine learning”. The Telecom service providers are interested in estimating various trends in order to plan future upgrades and deployments driven by real data. Fundamentally, the service provider landscape is changing. The numbers of devices are increasing in the network such as small cells to cater the growing demands. Also, the increasing amount of data has caused a big data revolution that is having an impact on telecom.
	With the advance big data analytics solutions and with fine grained analytics in real time, needs in bandwidth change from one place to another throughout the day, week, month, etc, becomes predictable. Hence, big data analytics solutions can help in deciding footprint of small cells and efficiently deployment of small cells.
	In this thesis, I have used the open big data that is published at the site: https://dandelion.eu/datamine/open-big-data/ under Open Data Commons Open Database License (ODbL) license. This dataset provides information about the telecommunication activities over the city of Milano. The dataset is the result of a computation over the Call Detail Records (CDRs) generated by the Telecom Italia cellular network over the city of Milano.
	Data mining is the technique to find concealed and fascinating pattern from dataset, which can be used in decision making and future prediction. In this thesis, data preprocessing has been performed on hadoop framework using hive with Cloudera's open source platform, CDH cloudera-quickstart-vm-5.3.0-0-vmware.
	In this thesis, the (Eps, MinPts) DBSCAN density based spatial clustering algorithm is used clustering the geospatial data. DBSCAN clusters a spatial data set based on two parameters namely physical distance from each point and a minimum cluster size. This method is best fit for spatial latitude-longitude data.
	In this thesis, the scikit-leran machine learning platform is used to implement the solution, scikit-learn in python is one of the widely used machine learning platform, it provides a wide range of supervised and unsupervised learning algorithms via a consistent interface in Python.
	 For the validation of the clustering results, the data mining tool WEKA 3.6.11 is used. For benchmarking of the proposed solution, the DBSCAN algorithms clustering result is compared with the WEKA cluster’s results. The final results show that the solution produces very promising results. The three promising results are , it  is  able  to  reveal  all  the objects  from  the  datasets  on  the  basis  of  user  defined  algorithm  input  parameters.  The input parameters have a decisive impact on the cluster result. It can extract spatial, temporal and semantically separated clusters. 
 	The detected clusters are visualized using Matplotlib plotting library for the Python, WEKA and geojson.io online tool",,BIG DATA ANALYTICS SOLUTION FOR SMALL CELLS DEPLOYMENT USING MACHINE LEARNING TECHNIQUES,https://core.ac.uk/download/215178970.pdf,,,core
87903086,2015-10-01T00:00:00Z,"Background

Depressive disorders do not only affect the personal life of individuals and their social circles but it has also a strongly negative economic impact. For example, according to a recent study by the European Depression Association, workers in the United Kingdom suffer higher levels of depression than those anywhere else in Europe. The survey found that 1 in 10 employees had taken time off at some point in their working lives because of depression problems. We believe that the support provided by new mobile technologies can help to tackle this problem providing new ways for supporting both patients and healthcare officers, possibly through the automatic delivery of behaviour interventions.


Aims

Existing interview-based studies in the literature have shown that depression leads to a reduction of mobility and activity levels. The goal of our project is to investigate how mobile phones can be used to collect and analyse mobility patterns of individuals in order to quantitatively understand how mental health problems affect their daily routines and behaviour and how potential changes can be automatically detected. More specifically,  we investigate the design, implementation and evaluation of analytical techniques for studying the correlation between patterns of human mobility and emotional states. 

More specifically, the aims of our project is to provide answers to the following questions: 
a) is there any correlation between mobility patterns extracted from GPS traces and depressive mood?; b) is it possible to devise unobtrusive smartphone applications that collect and exploit only mobility data in order to automatically infer a potential depressed mood of the user over time?; c) if this is possible, can we devise behaviour interventions based on the inferences we can derive from the mobile phones?; d) what are the ethical and practical implications related to the automatic delivery of behaviour intervention in the case of depressive mood disorders without the involvement of mental health professionals?


Methods

During our project, we designed an energy-efficient Android application to collect mobility data and assess the presence of depressed mood disorders by analysing mobility traces. We deployed the application and we collected data from 28 users.

We also introduced a set of mobility metrics that can be extracted from the mobility traces of the users and, using the ground truth data collected by means of the Android application.

More specifically, we considered the following mobility traces:

We extracted the following mobility metrics:
1) The total distance covered; 
2) The maximum distance between two locations; 
3) The radius of gyration; 
4) The standard deviation of the displacements; 
5) The maximum distance from home;
6) The number of different places visited; 
7) The number of different significant places visited;
8) The routine index.

We trained and evaluated personalised and general machine learning models to predict PHQ score changes from mobility metrics variations. 


Results 

During our experiments, we identified a significant correlation between the changes of such metrics and the variations in the PHQ score. The correlation ranges from 0.336 (p-value: 0.181) to 0.432 (p-value: 0.069) when the mobility metrics are computed over a period of 14 days.

With respect to the personalised prediction models, we obtained very good prediction accuracies.  For example, when the mobility metrics are computed over a period of 14 days, the general model achieves sensitivity and specificity values of 0.74 and 0.78, respectively, whereas the average sensitivity and specificity values of the personalized models are 0.71 and 0.87, respectively. 

 We are currently exploring how to exploit these findings. We are developing models for taking decisions based on the machine learning models we investigated. We are developing solutions for the automatic delivery of information based on the observed behaviour and inferred depressive states. In other words, we are designing feedback based systems, where the actual effectiveness of the intervention, measured by the change in the mood of the individual, is constantly monitored in order to take the next intervention decision. An open question is how to establish whether the observed mood change is actually the result of the intervention or external causes contribute to it.
  

 Conclusions

We have demonstrated that it is possible to observe a significant correlation between mobility patterns and depressive mood using data collected by means of smartphones.
We have also shown that it is possible to develop inference algorithms as a basis for unobtrusive monitoring and prediction of depressive mood disorders.

The key open question is how to exploit the correlations between mobility metrics and depressive states we observe in the data. We are currently exploring a variety of possible solutions for enabling automatic delivery of behaviour intervention through real-time analysis of the sensed data.

The focus of this initial work is on a specific modality, i.e., GPS location, but the results of this work can be indeed exploited to build more complex system based on the analysis of data extracted by means of other sensors, such as accelerometers, and other sources of information, such as call and SMS logs. We indeed plan to use the application in future studies that will focus on specific populations, such as clinically-diagnosed depressed individuals.

Ethical considerations are also an important part of our investigation: we believe that the potential risks associated to the delivery of incorrect behaviour interventions should be analysed in depth. A possible solution might consist in mixed intervention methods, based on the automatic delivery of behaviour interventions by means of mobile phones with the involvement of mental healthcare officers and clinicians, at least in case of mild and severe depressive cases",Frontiers Media S.A.,From Mobile Phone Monitoring of Depressive States using GPS Traces Analysis to Data-Driven Behaviour Change Interventions,,10.3389/conf.FPUBH.2016.01.00050/full,"[{'title': None, 'identifiers': ['2296-2565', 'issn:2296-2565']}]",core
102580124,26/08/2015,"The developing regions of the world contain most of the human population and the planet&apos;s natural resources, and hence are particularly important to the study of sustainability. Despite some difficult problems in such places, a period of enormous technology-driven change has created new opportunities to address poor management of resources and improve human well-being. It might be thought that artificial intelligence techniques or other types of computational methods are irrelevant in countries with few technological resources. As just one example of the possibilities, however, take road traffic in cities. The chaotic and spectacular road congestion which is characteristic of developing-world cities is a microcosm of opportunities for applying AI methods. The problems are mainly caused by inadequate infrastructure (e.g., road layouts which have not changed significantly despite decades of economic growth, unsealed or pothole-strewn roads), and a lack of resources to monitor or control traffic (e.g., scarce and possibly corrupt traffic police, rolling blackouts affecting traffic lights). Computational solutions might come in the form of ways to cheaply gather real-time data, to advise individuals or emergency vehicles on optimal routes, to dynamically re-deploy a limited number of traffic police, or t",,AI Magazine Special Issue on Computational Sustainability Computational Sustainability and Artificial Intelligence in the Developing World,,,,core
224463063,2015-01-01T00:00:00,"Smartphones are becoming an increasingly interesting survey medium for behavioral research due to their value for collecting long-term panel observations and supplementary data on the choice environment. Thanks to the sensor data, it becomes possible to survey participants based on whether or not a certain activity has been carried out. By fusing the phone-generated sensor data and survey responses with data from outside sources, substantial data sets can be generated which can be used to investigate choices in complex environments. Computational systems for behavior research take advantage of automation and scalability opportunities, thereby building also on pertinent bodies of literature regarding machine learning on large data sets and crowdsourcing. The importance of comprehensive, long-term data sets in understanding behavior has been highlighted in the choice theory literature, specifically with respect to capturing an individual decision-maker’s history of choices and personal experiences with those choices. To date, however, relatively few studies have capitalized on emerging technologies to create or analyze such data sets.Rich data sets which combine panel information on the decision-maker with information on the choice environment can support the study of dynamic phenomena, which is especially important in a rapidly changing world where behavioral adaptation can take place on a relatively small time scale and, once habits are formed, have long-lasting effects. Some examples of pressing questions in the field of transportation involve understanding how travelers are responding to the emerging sharing economy, to new ride sharing services and new information systems, how time use and travel patterns will change due to automated vehicles, and how more sustainable travel behavior can be promoted through incentive or pricing strategies. This dissertation aims to support the adoption of smartphone-based survey technology in travel behavior research in order to lay the groundwork for research aimed at answering the above questions. It describes the design and implementation of a smartphone-based study, presents a system for fusing smartphone data with externally acquired data, and demonstrates how these ample data sets can be leveraged to generate new behavioral insights. The problem chosen for study is the link between transit service quality, rider satisfaction and ridership retention on public transit. This is motivated by the fact that many transit agencies in the United States continue to see large rates of ridership turnover, and that to date, very little is known about what drives transit use cessation.The six-week San Francisco Travel Quality Study (SFTQS) was conducted in autumn 2013. It collected a data set that included high-resolution phone locations, a number of daily mobile surveys on specific trip experiences, responses to online entry and exit surveys, and transit vehicle locations.  By fusing the phone location data with transit vehicle locations, individual-level automatic transit travel diaries could be created without the need to ask participants. The reduced respondent burden, in turn, facilitated a longer term data collection. Initial recruitment proved to be challenging, with response rates to some of the email and direct mailing lists around 1%, and response rates to in-person recruiting between 8 and 15%. On the other hand, attrition was lower than expected, considering the length of the study: The initial enrollment was 856 participants, of which 555 (65%) participants completed all required surveys and 637 (74%) completed the entry and exit survey as well as at least one daily mobile survey. Interestingly, 36% of participants later stated they would have preferred to fill out mobile surveys more frequently (e.g., one per trip rather than one per day) than what was required in the study.A central part of the computational infrastructure used to collect the data was the system of integrated methods to reconstruct and track travelers’ usage of transit at a detailed level by matching location data from smartphones to automatic transit vehicle location (AVL) data and by identifying all out-of-vehicle and in-vehicle portions of the passengers’ trips. This system is presented in detail in this dissertation, where it is shown how high-resolution travel times and their relationships with the timetable are derived. Approaches are presented for processing relatively sparse smartphone location data in dense transit networks with many overlapping bus routes, distinguishing waits and transfers from non-travel related activities, and tracking underground travel in a metro network. While transit agencies have increasingly adopted systems for collecting data on passengers and vehicles, the ability to derive high-resolution passenger trajectories and directly associate them with vehicles has remained a challenge. The system presented in this dissertation is intended to remedy this situation, and it enables a range of different analyses and applications. Results are presented from an implementation and deployment of the system during the SFTQS. An analysis of out-of-vehicle travel times shows that (a) longer overall travel times in trips involving a transfer are strongly driven by transfer times, and (b) median wait times at the origin stops are consistently low regardless of the headway. The latter can be seen as an effect of real-time information, as it appears that wait times are increasingly spent at locations other than the stop and that passengers time their arrivals at the stop. Given these shifts, the traditional assumption that the average wait time at a transit stop of a high-frequency route is half the headway due to random arrivals may need to be revisited.This dissertation presents two applications to derive new behavioral insights from the SFTQS data set and to demonstrate the power and value of these new types of data. The analyses were based on participants’ individual history of transit usage and experiences with service quality. The first analysis used the data from the daily mobile surveys to model the link between participants' reported satisfaction with travel times on specific trips (i.e., their subjective assessment) and objective measures of those travel times. Thanks to the tracking data, it was possible to decompose observed travel times into their in-vehicle and out-of-vehicle components, and to compare the observed in-vehicle travel times to scheduled in-vehicle travel times to identify delays suffered while the participant was on board. The estimation results show that on average, a minute of delay on board a vehicle contributed more to passenger dissatisfaction than a minute of waiting time either at the origin stop or at a transfer stop, and that delays on board metro trains are perceived as more onerous than delays on board buses. Furthermore, the models included participants' baseline satisfaction levels as reported in the entry survey and a daily measure of their subjective well-being. Both variables are relatively new elements in travel surveys, and both are seen to be significant in the estimation results. These results indicate that satisfaction with travel times may be composed of a baseline satisfaction level and a variable component that depends on daily experiences, and that there may be non-negligible interactions between subjective well-being and travel satisfaction. Therefore, it is recommended that future survey designs should include measures for both these variables.The second application builds on the results of the first to empirically investigate the causes for cessation of transit use, with a specific focus on the influence of personal experiences that users have had in the past, on resulting levels of satisfaction, and subsequent behavioral intentions. A latent variable choice model is developed to explain the influence of satisfaction with travel times, including wait times at the origin stop, in-vehicle travel times, transfer times and overall reliability, and satisfaction with the travel environment on behavioral intentions. The group of variables summarized as ``travel environment'' includes crowding, cleanliness, the pleasantness of other passengers, and safety. Satisfaction is modeled as a latent variable, and the choice consists of participants’ stated desire and intention to continue using public transportation in the future. In addition to the delay types captured in the first analysis, a set of negative critical incidents is included, namely being left behind at stops and arriving late to work, school or a leisure activity. The results of the model and descriptive analysis show that operational problems resulting in delays and crowding are much stronger drivers of overall dissatisfaction and cessation than variables related to the travel environment. The importance of baseline satisfaction, mood and the relatively larger impact of in-vehicle delays are confirmed by this model. Thanks to the framework, the critical incidents can be expressed in terms of equivalent delay minutes. For instance, being left behind at a bus stop is found to cause the same amount of dissatisfaction as approximately 18 minutes of wait time. Furthermore, the effect of delays or incidents on ridership can be quantified, as is demonstrated in a set of simulations using the San Francisco transit network (Muni) as a basis. It is shown that if all passengers were subjected to one hypothetical on-board delay of 10 minutes per person, the resulting loss of riders would account for approximately 9.5% of Muni's yearly ridership turnover.In summary, the contributions and impact of this dissertation are as follows: It presents a framework and system that allows the researcher to gather detailed information on an individual and on the decision environment through phone-based survey apps in combination with sensor data from the phone and from external sources. In the public transit context, an innovative system is presented to match AVL data with smartphone location data in order to measure the personal experiences of travelers with respect to travel times. With repeated measurements, these data can be used to calculate personalized reliability metrics for individual travelers, reflecting the sum of their travel experiences, or they can be used to derive aggregate travel time distributions across all travelers by time of day, origin-destination pair or location on the network. These metrics can capture the true door-to-door travel times experienced by travelers and can serve as the basis for user-centric performance metrics to supplement system-level performance metrics commonly used by transit agencies. The long-term nature of this data collection and low respondent burden facilitate the observation of behavioral dynamics such as habit formation or lifestyle adjustments in response to changes of the choice set. The low cost and scalability of these data collection methods permits relatively short lead times for studies and frequent data collection. Thus, these systems can be deployed at the early stages of a new product's or technology's emergence (e.g., new ride sharing services or traveler information systems) to gain insights into how they affect consumer choices in a real-world setting, both on short-term and longer-term time scales. Moreover, personalized interactions between the researcher and the participant via the smartphone facilitate behavioral interventions and allow for targeted incentives to change behavior.By elaborating on the researchers’ experiences in designing and implementing the SFTQS, this dissertation provides a powerful example of how these new types of data sets can be harnessed and supports the development of future studies. The application of the methodology and framework to the transit context were motivated by a desire to learn more about the drivers of satisfaction among transit riders and the causes of transit use cessation. The model estimation results underscore the importance of investments into run time stability measures such as transit signal priority systems and dedicated rights of way. Furthermore, thanks to the modeling framework, the cost of holding vehicles with passengers on board and the cost of vehicles not stopping at stops due to overcrowding can be quantified; this can directly impact operating and control policies. On the other hand, the model results provide evidence that investments into stop amenities may be becoming less important as passengers spend less time at stops (except in the case of transfers), but that conversely, the benefits of transit-oriented development may be increasing as passengers can choose to spend their wait times elsewhere thanks to real-time information. The data and modeling framework allow transit agencies to create service quality metrics that can appropriately capture individual users' experiences in real-time, and improve researchers' ability to compare the level of service on public transportation with the level of service of private modes of transportation. Together, this set of methods and results is an important step toward aligning the service provided by transit agencies more closely with the needs of customers.The framework and methodologies described in this dissertation are useful beyond the specific transit application presented as a case study. Thanks to the flexibility and scalability of the smartphone-based data collection and automated post-processing, they can be used by researchers to quickly gather insights on emerging trends and on travelers' adaptation to new services and new technologies, and to study the dynamics of behavior change over longer time periods. In particular, they can be applied to understand how flexible, shared-ride transportation systems and future automated mobility on demand systems will shape travel demand and how users will interact with those systems. This, in turn, will be a critical input to policy-making and system design","eScholarship, University of California",Traveler Satisfaction Surveys meet Mobile Phone and Vehicle Tracking: Linking Individual Experiences to Travel Habit Changes with Panel Data,,,,core
74313351,2016-01-01T00:00:00,"The continuous expansion of urban areas worldwide is expected to highly increase residential water demand over the next few years, ultimately challenging the distribution and supply of drinking water. Several studies have recently demonstrated that actions focused only on the water supply side of the problem (e.g., augmenting existing water supply infrastructure) will likely fail to meet future demands, thus calling for the concurrent deployment of effective water demand management strategies (WDMS) to pursue water savings and conservation. However, to be effective WDMS do require a substantial understanding of water consumers' behaviors and consumption patterns at different spatial and temporal resolutions. Retrieving information on users' behaviors, as well as their explanatory and/or causal factors, is key to spot potential areas for targeting water saving efforts and to design user-tailored WDMS, such as education campaigns and personalized recommendations. In this work, we contribute a data-driven approach to identify household water users' consumption behavioural profiles and model their water use habits. State-of-the-art clustering methods are coupled with big data machine learning techniques with the aim of extracting dominant behaviors from a set of water consumption data collected at the household scale. This allows identifying heterogeneous groups of consumers from the studied sample and characterizing them with respect to several consumption features. Our approach is validated onto a real-world household water consumption dataset associated with a variety of demographic and psychographic user data and household attributes, collected in nine towns of the Pilbara and Kimberley Regions of Western Australia. Results show the effectiveness of the proposed method in capturing the influence of candidate determinants on residential water consumption profiles and in attaining sufficiently accurate predictions of users' consumption behaviors, ultimately providing valuable information to water utilities and managers",,Data-driven behavioural modelling of residential water consumption to inform water demand management strategies,,,,core
56698369,2016-01-01T08:00:00,"Yearly increases in computer performance have diminished as of late, mostly due to the inability of transistors, the building blocks of computers, to deliver the same rate of performance seen in the 1980’s and 90’s. Shifting away from traditional CPU design, accelerator architectures have been shown to offer a potentially untapped solution. These architectures implement unique, custom hardware to increase the speed of certain tasking, such as graphics processing. The studies undertaken for this dissertation examine the ability of unique accelerator hardware to provide improved power and speed performance over traditional means, with an emphasis on classification tasking.
In the first study, the compression algorithm Lempel-Ziv-Oberhumer (LZO) 1x-1-15 is analyzed and documented. This algorithm family has seen widespread use and can be found in the NASA mars space rover and the B-tree Linux file system. A thorough analysis of the algorithm is seen to yield x86 vector and other CPU parallelization improvements that can be utilized for acceleration. Real-world datasets are used to concretely benchmark the improved performance.
The second study shifts the focus from CPU instruction acceleration to optimized hardware acceleration. A real-world embedded application of machine learning involving Support Vector Machine (SVM) accelerated hardware is examined. Prior work developed by URI’s Biomedical Engineering department investigated the use of a state-of-the-art SVM-based algorithm to control an artificial limb in real-time. Evaluation of the algorithm was performed using general processing means, using a Core i7 CPU and an Intel ATOM mobile CPU. This study builds on the prior work, investigating the performance advantages imparted by implementing the SVM decision function in hardware and combining it with a hardware-based feature extractor on a Field Programmable Gate Array (FPGA). The design is evaluated for both accuracy and real-time response to determine if the FPGA implementation is a better choice for implementation in a power-limited cyber physical system.
The third study examines the SVM classification portion of the FPGA design that was constructed for use in the artificial limb in further detail. A general purpose hardware architecture for fast, accurate SVM classification, R2SVM, is proposed. While several similar architectures have been published, our architecture is shown to be superior in a several ways. To prove the performance, accuracy, and power consumption of the architecture, a prototype is constructed and multiple machine learning datasets are run and analyzed.
The final study takes a look at the creation of a smart city architecture. A novel multi-tiered hierarchical architecture, Reflex Tree, is proposed as a solution to automated city management in the future. The four layers of the architecture are able to perform massive parallel sensing, pattern recognition, spatial-temporal association, and system-wide behavioral analysis. Like the human nervous system, each layer in the hierarchy is able to detect specific events and inject feedback without the need for higher level intervention. Simulations of the architecture are performed in two scenarios: a gas pipeline and a city power supply network",DigitalCommons@URI,Research Into Computer Hardware Acceleration of Data Reduction and SVMS,https://core.ac.uk/download/56698369.pdf,,,core
103131457,01/11/2015,"The aim of this study was modeling of ambient air pollutants through ANN in industrial area of Ujjain city in India and the study was carried out on modeling of air pollutants like SPM and RSPM using Artificial Neural Network. Artificial neural networks (ANN), whose performances to deal with pattern recognition problems is well known, are proposed to identify air pollution sources. The ANN system was run by giving the inputs of meteorological data’s and giving the outputs of concentration of various pollutants and accordingly the estimation of Errors was done by this study. The monthly data’s in year from 2009-2012 of meteorological data’s like Temperature, Humidity, wind pressure and rainfall and the pollutants concentration were collected from the State Pollution Control Board. The ANN system used, as shown in figure 1, analyses all these data’s and finds the error coming during the experiment. The study estimated the Mean Square Error (MSE) from the inputs and outputs which were given to ANN in the industrial area of Ujjain City in India was found satisfactory being in the range of 0.001-0.003. The results shown here indicate that the neural network techniques can be useful tool in the hands of practitioners of air quality management and prediction. The models studied in this study are easily implemented, and they can deliver prediction in real time, unlike other modeling techniques",,Modeling of Ambient Air Pollutats for RSPM and SPM Through Artifical Neural Network In Industrial Area of Ujjain City,,,,core
43095130,2015-10-08T12:54:10,"WRAH 2011: Weather Radar and Hydrology International Symposium, 18-21 April 2011, University of Exeter, UKThis paper describes the application of ANNs (Artificial Neural Networks) as DDMs (Data Driven Models) to predict urban flooding in real-time based on BADC weather radar and/or rainfall data. A 123-manhole combined sewer sub-network from Keighley, West Yorkshire, UK is used to demonstrate the methodology. An ANN is configured for prediction of flooding at manholes based on rainfall. In the absence of actual flood data, the 3DNet / SIPSON simulator, which uses a conventional fluid-dynamic approach to predict flooding surcharge levels in sewer networks, is employed to provide the target data for training the ANN. Artificial rainfall profiles derived from observed data provide the input. Both flood-level analogue and flood-severity classification schemes are implemented. We also investigate the use of an ANN for nowcasting of rainfall based on the relationship between radar data and recorded rainfall history. This allows the two ANNs to be cascaded to predict flooding in real-time based on weather radar",International Association of Hydrological Sciences,Urban flood prediction in real-time from weather radar and rainfall data using artificial neural networks,https://core.ac.uk/download/43095130.pdf,,,core
213619981,2015-12-01T00:00:00,"RÉSUMÉ Les simulateurs de vol sont des systèmes composés d’un système logiciel et d’actuateurs mécaniques qui recréés la dynamique d’un vrai vol avec un aéronef et qui sont notamment utilisés par les compagnies de transport aérien pour former leurs futurs pilotes. Ces simulateurs doivent recréer des scénarios de vol permettant d’exécuter des itinéraires réguliers de vol ou de confronter les pilotes à différentes situations d’urgence qui peuvent subvenir lors d’un vrai vol (p. ex., une tempête violente). Puisque les simulateurs de vol ont un impact direct sur la qualité de formation des pilotes, une batterie exhaustive de tests s’impose à chaque fois qu’une nouvelle version d’un simulateur doit être mise en opération. Une bonne partie de ces tests requièrent l’intervention d’un pilote qui stimule le système en réalisant une série d’opérations de contrôle provenant de scénarios de vol préparés par des experts en aéronautique, ce qui est gourmand en temps, en ressources humaines et en ressources financières. La raison de la nécessité de réaliser tous ces tests est que le logiciel en soit est constitué de plusieurs composants de type boîte noire dont le code source n’est pas disponible, ils sont fournis tels quels par les fournisseurs du composant réel d’origine devant être simulé (p. ex., un composant hydraulique d’un aéronef que nous voulons simuler). Puisque nous n’avons pas pas accès au code source, il n’est pas possible lors de la mise-à-jour de l’un de ces composants de savoir quel sera le changement dans le comportement du simulateur. Dans le meilleur des cas, un défaut dans un composant aura un impact local et, dans le pire des cas, il peut nuire à tous les autres composants du simulateur. Il est dans ce cas nécessaire d’utiliser une stratégie de test d’une large granularité couvrant à la fois le fonctionnement des composants eux-mêmes et l’ensemble des fonctionnalités de vol du simulateur afin d’en assurer sa qualité. Dans le cadre de ces tests, afin de réduire le temps requis pour vérifier le bon comportement d’une nouvelle version d’un simulateur, nous proposons dans ce mémoire d’automatiser l’analyse des résultats de test en modélisant le comportement normal du système logiciel en utilisant de l’apprentissage automatique. Un tel modèle tente de recréer le comportement stable du système en bâtissant des règles reliant ses entrées à chacune de ses sorties. Les entrées sont constituées de métriques provenant des contrôles qui stimulent le système et les sorties sont constituées de métriques que nous pouvons observer. Nous produisons donc un modèle initial d’une version fiable du simulateur et nous validons par la suite le bon comportement des versions subséquentes en comparant leur comportement avec celui du modèle initial. Nous cherchons à voir s’il y a un écart trop important avec le modèle initial, ce nous considérons comme étant une déviation. Le but final de notre recherche est de pouvoir appliquer notre méthodologie d’analyse des résultats de test dans l’industrie de la simulation. Afin d’avoir plus de flexibilité dans nos expérimentations, nous commençons d’abord par valider l’approche avec un simulateur plus simple et ouvert. Nous utilisons donc le simulateur de vol à source libre FlightGear comme cas d’étude pour évaluer notre approche de test. Ce simulateur utilise l’engin de vol très populaire JSBsim et la librairie SimGear pour modéliser les aéronefs et simuler leur comportement dynamique dans un environnement de vol. JSBsim est notamment utilisé par la Nasa à des fins de recherche. Nous réalisons d’abord un vol manuel en utilisant des périphériques de contrôle d’un ordinateur tout en enregistrant les données contenues dans le simulateur. Le vol doit être composé de toutes les opérations régulières de pilotage afin que le modèle que l’on en extrait représente fidèlement le comportement normal du simulateur. Nous réalisons par la suite plusieurs répétitions du même vol afin d’identifier des niveaux de seuils robustes pour déterminer au delà de quels écarts un métrique d’une nouvelle version doit être considéré comme déviant par rapport au modèle. Nous élaborons à cet effet cinq scénarios de mutation du comportement de la version initiale de notre simulateur afin de générer différentes versions ayant des métriques qui respectent le comportement normal du système ou présentant une déviation du comportement normal reliée à un problème fonctionnel. En sachant d’avance quelles mutations présentent des déviations et avec l’aide d’experts identifiant précisément les métriques qui dévient de leur comportement normal, nous pouvons construire un oracle avec lequel nous évaluons la précision et le rappel de notre approche de détection. En particulier, dans le cadre de notre étude empirique sur FlightGear, nous modifions une version initiale du simulateur en y injectant cinq différentes mutations qui n’ont pas d’impact grave ou qui engendrent des problème fonctionnels avec lesquels nous évaluons notre approche. Les résultats montrent qu’en choisissant des niveaux de seuil initiaux lors d’une première calibration, notre approche peut détecter les métriques ayant des déviations avec un rappel majoritairement de 100% (un seul cas de mutation à 50%) et une précision d’au moins 40%. En ce qui a trait aux mutations ne changeant pas le comportement normal du simulateur, notre approche a un taux de fausses alarmes de 60%. Nous montrons également qu’avec une base de données plus large, il est possible de calibrer notre approche afin d’améliorer sa performance. Avec des niveaux de seuil optimaux qui sont calibrés pour chacune des mutations, nous obtenons un taux de rappel de 100% pour tous les cas, une précision d’au moins 50% pour les versions ayant des problèmes fonctionnels et un taux de fausses alarmes de 0% pour les versions n’ayant pas de problèmes fonctionnels. Afin d’ouvrir la voie pour des améliorations futures, nous utilisons notre méthodologie pour faire une petite expérimentation connexe sur JSBsim afin de détecter les déviations dans les transitions entre les états stables de chacun des métriques. Nous utilisons la même modélisation du simulateur. Les résultats montrent que nous pouvons battre une classification aléatoire des déviations. Cette nouvelle approche n’en n’est qu’à ces débuts et nous sommes convaincu que nous pouvons obtenir des résultats plus significatifs avec de futurs travaux sur le sujet. Nous proposons également dans les améliorations futures de descendre le niveau de granularité de notre modélisation du simulateur au niveau de chacun de ses composants. Cela permettrait d’isoler exactement qu’elle composant présente une déviation, et ainsi trouver la source du problème.----------ABSTRACT Flight simulators are systems composed of software and mechanical actuators used to train crews for real flights. The software is emulating flight dynamics and control systems using components off-the-shelf developed by third parties. For each new version of these components, we do not know what parts of the system is impacted by the change and hence how the change would impact the behaviour of the entire simulator. Hence, given the safety critical nature of flight simulators, an exhaustive set of manual system tests must be performed by a professional pilot. Test experts then rely on the test pilot’s feedback and on the analysis of output metrics to assess the correctness of a simulator’s behaviour, which consumes time and money, even more these tests must be repeated each time one of the components is updated by its vendor. To automate the analysis of test results from new simulator versions, we propose a machine learning-based approach that first builds a model that mimics the normal behaviour of a simulator by creating rules between its input and output metrics, where input metrics are control data stimulating the system and output metrics are data used to assess the behaviour of the system. We then build a behavioural model of the last-known correctly behaving version of the simulator, to which we compare data from new versions to detect metric deviations. The goal of our research is to first build a model of the simulator, then detect deviations of new simulator versions using that model, and finally develop an automatic approach to flag deviations when there are functional problems. Our case study uses the open-source flight simulator Flight Gear, based on the well-known JSBsim flight dynamic engine currently used by the Nasa. Applying our approach, we first drive a manual basic flight using computer peripherals while recording metrics in the simulator. We then use the recorded metrics to build a model of this basic execution scenario. After repeating several times our flight we have enough data to identify robust metric deviation thresholds to determine when a metric in a new version is deviating too far from the model. We generate five different versions of FlightGear by injecting harmless and harmful modifications into the the simulator, then perform again our initial flight multiple times for each of those versions while recording input and output metrics. We rely on experts identifying which output metric should be deviating in each scenario to build an oracle. Using an initial threshold, our approach can detect most of the deviations in problematic versions with a near perfect recall (only one case at 50%) and a reasonable precision of at least 40%. For the versions without harmfull deviations we get a false alarm rate of 60%. By optimizing the threshold for each version, we get a perfect recall of 100%, a precision of at least 50%, and a false alarm rate of 0% for the good behaving versions. To open the path for future work, we perform a case study on JSBsim using a variant of our basic deviation detection approach to detect transient deviations using the same model. Our results show that we can beat a random classification for one kind of deviation, however for other deviations our models do not perform as well. This new approach is just a first step towards transient deviation detection, and we are convinced that we can get better results in future work. We also propose to use finer-grained models for each component to be able to isolate the exact component causing a functional problem",,Identification automatique des problèmes fonctionnels dans les simulateurs de vol,https://core.ac.uk/download/213619981.pdf,,,core
290064301,2015,"Emancipare significa rendere liberi, concedere autonomia.
Nella città moderna, che si è sviluppata a misura dei cittadini adulti e delle
loro automobili, i bambini hanno perso la possibilità di muoversi autonomamente
e non vengono riconosciuti come cittadini, ma solo come futuri cittadini,
bisognosi di cura e di protezione. Di fronte alle inadempienze degli adulti
e ai «guai» che stanno producendo nel mondo contemporaneo, il progetto «La
città delle bambine e dei bambini» propone una reale emancipazione infantile
chiamando i bambini a partecipare al governo delle città e restituendo loro la
possibilità di muoversi autonomamente negli spazi pubblici della città. Il progetto
intende dare piena attuazione alla Convenzione dei diritti dell’Infanzia,
approvata dalle Nazioni Unite nel 1989 e finora poco conosciuta e sostanzialmente
inattuata. A conclusione del contributo si riferiscono alcuni esempi di
come concrete proposte infantili possano modificare sostanzialmente la politica
delle nostre città.Emancipar significa liberar, conceder autonomía. La ciudad
moderna, se ha desarrollado a medida de los ciudadanos adultos y de sus automóviles,
los niños han perdido la posibilidad de moverse de forma autónoma y no se les reconoce como ciudadanos, sino sólo como futuros ciudadanos, necesitados
de cuidado y protección. Frente a los incumplimientos de los adultos
y a los «líos» que están provocando en el mundo contemporáneo, el proyecto
La Ciudad de las Niñas y los Niños propone una emancipación infantil real
llamando a los niños a participar en el gobierno de las ciudades y devolviéndoles
la posibilidad de moverse de manera autónoma en los espacios públicos
de la ciudad. El proyecto pretende aplicar en su totalidad la Convención de los
derechos de la Infancia, aprobada por las Naciones Unidas en 1989 y hasta el
momento poco conocida y prácticamente sin aplicar. Al final del artículo se
incluyen algunos ejemplos de cómo propuestas infantiles concretas podrían
modificar sustancialmente la política de nuestras ciudadesEmancipar significa alliberar, concedir autonomia. La ciutat moderna
s’ha desenvolupat a mida de la ciutadania adulta i dels automòbils, els xiquets
i les xiquetes han perdut la possibilitat de moure’s per la ciutat de manera
autònoma i no se’ls reconeix com a ciutadans, sinó només com a futurs ciutadans
que necessiten cura i protecció. Davant dels incompliments dels adults i
els «embolics» que provoquen en el món contemporani, el projecte La Ciutat
de les Xiquetes i dels Xiquets» proposa una emancipació infantil real i crida
els xiquets a participar en el govern de les ciutats, per a retornar-los la possibilitat
de moure’s de manera autònoma als espais públics de la ciutat. El projecte
pretén aplicar-hi totalment la Convenció dels Drets de la Infantesa, aprovada
per les Nacions Unides, en 1989, i fins ara poc coneguda i pràcticament sense
aplicar. Al final de l’article s’inclouen alguns exemples de com propostes
infantils concretes podrien modificar substancialment la política urbana.Emancipating means freeing, granting autonomy. In the modern
city, which has developed to accommodate adult citizens and their cars, children
have lost the ability to move autonomously and they are not recognised as citizens, but rather only as future citizens, in need of care and protection. In
light of adults’ inadequacies and the ‘mess’ they are making in today’s world,
the City of Girls and Boys project calls for real emancipation for children,
inviting them to participate in governing their cities and giving them back
the ability to move autonomously in the city’s public spaces. The aim of the
project is to apply in full the Convention on the Rights of the Child approved
by the United Nations in 1989 and to date, unfamiliar to most and yet to be
implemented practically everywhere. The article concludes with some examples
of how children’s specific proposals can substantially alter the politics of
our citie",Universitat Jaume I. Servei d'Activitats Socioculturals (SASC),A proposito di emancipare: “Se gli adulti non ascoltano i bambini vanno incontro a guai grossi” La città dei bambini: una nuova filosofia di governo delle città.,,10.6035/Kult-ur.2015.2.3.3,"[{'title': None, 'identifiers': ['issn:2386-5458', '2386-5458']}]",core
73423435,2016-11-28T00:00:00,"Intelligent Transportation Systems (ITS) rely on connected vehicle
applications to address real-world problems. Research is currently being
conducted to support safety, mobility and environmental applications. This
paper presents the DrivingStyles architecture, which adopts data mining
techniques and neural networks to analyze and generate a classification of
driving styles and fuel consumption based on driver characterization. In
particular, we have implemented an algorithm that is able to characterize the
degree of aggressiveness of each driver. We have also developed a methodology
to calculate, in real-time, the consumption and environmental impact of spark
ignition and diesel vehicles from a set of variables obtained from the
vehicle's Electronic Control Unit (ECU). In this paper, we demonstrate the
impact of the driving style on fuel consumption, as well as its correlation
with the greenhouse gas emissions generated by each vehicle. Overall, our
platform is able to assist drivers in correcting their bad driving habits,
while offering helpful tips to improve fuel economy and driving safety.Comment: Journal of Communications and Network",,"DrivingStyles: A mobile platform for driving styles and fuel consumption
  characterization",http://arxiv.org/abs/1611.09065,,,core
87486272,2016-11-01T00:00:00Z,"In recent years, the strong pace of construction is increasing in big cities. With their growth becomes a question of the deployment of firefighters and the number of fire stations. The most effective solution is the problem of finding the optimum route of fire departments, taking into account the information transport logistics systems within the city that will allow us to arrive at the scene at any time, regardless of the degree of congestion of city roads. Prompt arrival of fire units provides the most successful fire fighting. The main objective of the study is to develop a preliminary route and the route in case of unforeseen factors affecting the time fire engine arrived. To construct the routes used to develop actively in the current methods of machine learning artificial neural networks. To construct the optimal route requires a correct prediction of the future behavior of a complex system of urban traffic based on its past behavior. Within the framework of statistical machine learning theory considered the problem of classification and regression. The learning process is to select a classification or a regression function of a predetermined broad class of such functions. After determining the prediction scheme, it is necessary to evaluate the quality of its forecasts, which are measured not on the basis of observations, and on the basis of an improved stochastic process, the result of the construction of the prediction rules. The model is verified on the basis of data collected in real departures real fire brigades, which made it possible to obtain a minimum time of arrival of fire units",Voronezh state university of engineering technologies,Mathematical model of optimizing the arrival of fire units with the use of information systems for monitoring transport logistics of Voronezh city,,10.20914/2310-1202-2016-3-122,"[{'title': None, 'identifiers': ['2226-910x', '2310-1202', 'issn:2310-1202', 'issn:2226-910X']}]",core
30788733,2015-02-01T00:00:00,"The prediction of environmental noise in urban environments requires the solution of a complex and non-linear problem, since there are complex relationships among the multitude of variables involved in the characterization and modelling of environmental noise and environmental-noise magnitudes. Moreover, the inclusion of the great spatial heterogeneity characteristic of urban environments seems to be essential in order to achieve an accurate environmental-noise prediction in cities. This problem is addressed in this paper, where a procedure based on feature-selection techniques and machine-learning regression methods is proposed and applied to this environmental problem. Three machine-learning regression methods, which are considered very robust in solving non-linear problems, are used to estimate the energy-equivalent sound-pressure level descriptor (LAeq). These three methods are: (i) multilayer perceptron (MLP), (ii) sequential minimal optimisation (SMO), and (iii) Gaussian processes for regression (GPR). In addition, because of the high number of input variables involved in environmental-noise modelling and estimation in urban environments, which make LAeq prediction models quite complex and costly in terms of time and resources for application to real situations, three different techniques are used to approach feature selection or data reduction. The feature-selection techniques used are: (i) correlation-based feature-subset selection (CFS), (ii) wrapper for feature-subset selection (WFS), and the data reduction technique is principal-component analysis (PCA). The subsequent analysis leads to a proposal of different schemes, depending on the needs regarding data collection and accuracy. The use of WFS as the feature-selection technique with the implementation of SMO or GPR as regression algorithm provides the best LAeq estimation (R2 = 0.94 and mean absolute error (MAE) = 1.14–1.16 dB(A))",'Elsevier BV',A general procedure to generate models for urban environmental-noise pollution using feature selection and machine learning methods,,10.1016/j.scitotenv.2014.08.060,,core
402086028,2015-01-01T00:00:00,"Tez Başlığı: Bitki Kökenli Antimikrobiyal (-)-Roemerin Alkaloidine Karşı Bakteriyel Cevabın Biyosistem Mühendisliği Araçları Kullanılarak Anlaşılması
Çoklu-ilaçlara direnç gösteren bakterilerin hızla artmasından ötürü antibiyotik direnciyle savaşmak gitgide güçleşmektedir. Bu savaşta, bakteriyel direnç mekanizmalarını zayıflatan yeni antibakteriyel arayışı, araştırmaları biyoaktif moleküller için alternatif bir kaynak olan bitkilere yöneltmiştir. Bu nedenle, bu çalışmada, bitki alkaloidi                             (-)-roemerine’nin E. coli TB1 hücreleri üzerindeki antibakteriyel mekanizması araştırılmıştır. (-)-Roemerine’in minimum inhibitor konsantrasyonu (MİK) 100 µg/ml olarak bulunmuştur. Bu bulgu doğrultusunda, E. coli hücrelerindeki değişim, 1× MİK           (-)-roemerine’e maruz bırakıldıktan bir saat sonra incelenmiştir. İlaç adayı ile   muamelenin ardınan hücrelerin sadece % 33’ünün koloni oluşturduğu bulunmuştur. Ekspresyonu farklı olan proteinleri belirlemek için iki boyutlu jel elektroforezi ve kütle spektrometrisi kullanılmıştır. Daha sonra, Ludesi REDFIN yazılımı ile (BIO-RAD), kontrol ve (-)-roemerine uygulanmış örneklere ait jellerdeki protein spot hacimleri karşılaştırılmıştır. Ekspresyonu farklı olan proteinlerden 16 tanesinin (eşik değeri ≥ 1.8 kat) karbonhidrat taşınmasında, D-glukarat katabolik prosesinde, amino asit biyosentezinde, amino asit taşınmasında, amino asit metabolizmasında, protein biyosentezinde, bakteriosin taşınmasında, dış membran bileşiminde, hücre redoks homoestosizinde ve  hücre adezyonunda rol aldığı bulunmuştur. Ayrıca, gerçek-zamanlı kantitatif PZR ile yapılan transkripsiyonel farklılıklarının analizi, (-)-roemerine’nin dış membranda, antibiyotik direnç mekanizmalarında ve karbonhidrat taşınmasında rol oynayan proteinleri kodlayan genleri etkilediğini göstermiştir. STRING 9.1 veritabanı kullanılarak çizilen fonksiyonel etkileşim ağı, etkilenen tüm ekspresyonu farklı   proteinler arasında güçlü bir etkileşim olduğunu göstermiştir. Bu tez, bitkilerden elde edilecek alternatif ilaç adaylarının keşfi açısından önemli bir çalışmayı ortaya koymaktadır.treated samples. 16 of the differentially expressed proteins with a cut-off value ≥ 1.8 fold, were found to be involved in carbohydrate transport, D-glucarate catabolic process, amino acid biosynthesis, amino acid transport, amino acid metabolism, protein biosynthesis, bacteriosin transport, outer membrane formation, cell redox homeostosis and cell adhesion. Furthermore, analysis of transcriptional differences with real-time quantitative PCR revealed that the genes encoding proteins related to the outer membrane, antibiotic resistance mechanisms, and carbohydrate transport were affected with (-)-roemerine treatment. The construction of the functional interaction network of all differentially expressed proteins by the      STRING 9.1 database showed a strong interaction between them. This thesis work presents a novel work in the discovery of alternative drug leads from plants.  

ABSTRACT
Thesis Title: Understanding the Response of Bacteria to Plant Origin Antimicrobial Alkaloid (-)-Roemerine Using Biosystem Engineering Tools
Fighting with antibiotic resistance is a challenging task due to the exponential increase     in multi-drug resistant bacteria. In this fight, the search for novel antibacterials that    impair bacterial resistance mechanism(s), has led researches towards plants as      alternative sources of bioactive molecules. Hence, in the current study, the antibacterial mechanism of the plant alkaloid (-)-roemerine against E. coli TB1 cells was      investigated. The minimum inhibitory concentration (MIC) of (-)-roemerine was found                                as 100 µg/ml. Based on this finding, the changes in E. coli cells were investigated following one hour of exposure to 1×MIC (-)-roemerine. Only 33% of the cells were  found to form colonies after this treatment. Two-dimensional gel electrophoresis  followed by mass spectrometry were used to identify the differentially expressed    proteins. Ludesi REDFIN software (BIO-RAD) was then used to compare the spot volumes from gels of control and (-)-roemerine treated samples. 16 of the differentially expressed proteins with a cut-off value ≥ 1.8 fold, were found to be involved in carbohydrate transport, D-glucarate catabolic process, amino acid biosynthesis, amino acid transport, amino acid metabolism, protein biosynthesis, bacteriosin transport, outer membrane formation, cell redox homeostosis and cell adhesion. Furthermore, analysis of transcriptional differences with real-time quantitative PCR revealed that the genes encoding proteins related to the outer membrane, antibiotic resistance mechanisms, and carbohydrate transport were affected with (-)-roemerine treatment. The construction of the functional interaction network of all differentially expressed proteins by the      STRING 9.1 database showed a strong interaction between them. This thesis work presents a novel work in the discovery of alternative drug leads from plants. ",'Marmara Universitesi Ilahiyat Fakultesi Dergisi',Understandıng the response of bacterıa to plant orıgın antımıcrobıal alkaloıd (-)-roemerıne usıng bıosystem engıneerıng tools,,,,core
61480129,2015-01-01T00:00:00,"Emancipare significa rendere liberi, concedere autonomia.
Nella città moderna, che si è sviluppata a misura dei cittadini adulti e delle
loro automobili, i bambini hanno perso la possibilità di muoversi autonomamente
e non vengono riconosciuti come cittadini, ma solo come futuri cittadini,
bisognosi di cura e di protezione. Di fronte alle inadempienze degli adulti
e ai «guai» che stanno producendo nel mondo contemporaneo, il progetto «La
città delle bambine e dei bambini» propone una reale emancipazione infantile
chiamando i bambini a partecipare al governo delle città e restituendo loro la
possibilità di muoversi autonomamente negli spazi pubblici della città. Il progetto
intende dare piena attuazione alla Convenzione dei diritti dell’Infanzia,
approvata dalle Nazioni Unite nel 1989 e finora poco conosciuta e sostanzialmente
inattuata. A conclusione del contributo si riferiscono alcuni esempi di
come concrete proposte infantili possano modificare sostanzialmente la politica
delle nostre città.Emancipar significa liberar, conceder autonomía. La ciudad
moderna, se ha desarrollado a medida de los ciudadanos adultos y de sus automóviles,
los niños han perdido la posibilidad de moverse de forma autónoma y no se les reconoce como ciudadanos, sino sólo como futuros ciudadanos, necesitados
de cuidado y protección. Frente a los incumplimientos de los adultos
y a los «líos» que están provocando en el mundo contemporáneo, el proyecto
La Ciudad de las Niñas y los Niños propone una emancipación infantil real
llamando a los niños a participar en el gobierno de las ciudades y devolviéndoles
la posibilidad de moverse de manera autónoma en los espacios públicos
de la ciudad. El proyecto pretende aplicar en su totalidad la Convención de los
derechos de la Infancia, aprobada por las Naciones Unidas en 1989 y hasta el
momento poco conocida y prácticamente sin aplicar. Al final del artículo se
incluyen algunos ejemplos de cómo propuestas infantiles concretas podrían
modificar sustancialmente la política de nuestras ciudadesEmancipar significa alliberar, concedir autonomia. La ciutat moderna
s’ha desenvolupat a mida de la ciutadania adulta i dels automòbils, els xiquets
i les xiquetes han perdut la possibilitat de moure’s per la ciutat de manera
autònoma i no se’ls reconeix com a ciutadans, sinó només com a futurs ciutadans
que necessiten cura i protecció. Davant dels incompliments dels adults i
els «embolics» que provoquen en el món contemporani, el projecte La Ciutat
de les Xiquetes i dels Xiquets» proposa una emancipació infantil real i crida
els xiquets a participar en el govern de les ciutats, per a retornar-los la possibilitat
de moure’s de manera autònoma als espais públics de la ciutat. El projecte
pretén aplicar-hi totalment la Convenció dels Drets de la Infantesa, aprovada
per les Nacions Unides, en 1989, i fins ara poc coneguda i pràcticament sense
aplicar. Al final de l’article s’inclouen alguns exemples de com propostes
infantils concretes podrien modificar substancialment la política urbana.Emancipating means freeing, granting autonomy. In the modern
city, which has developed to accommodate adult citizens and their cars, children
have lost the ability to move autonomously and they are not recognised as citizens, but rather only as future citizens, in need of care and protection. In
light of adults’ inadequacies and the ‘mess’ they are making in today’s world,
the City of Girls and Boys project calls for real emancipation for children,
inviting them to participate in governing their cities and giving them back
the ability to move autonomously in the city’s public spaces. The aim of the
project is to apply in full the Convention on the Rights of the Child approved
by the United Nations in 1989 and to date, unfamiliar to most and yet to be
implemented practically everywhere. The article concludes with some examples
of how children’s specific proposals can substantially alter the politics of
our citie",'Universitat Jaume I',A proposito di emancipare: “Se gli adulti non ascoltano i bambini vanno incontro a guai grossi” La città dei bambini: una nuova filosofia di governo delle città.,https://core.ac.uk/download/61480129.pdf,10.6035/Kult-ur.2015.2.3.3,"[{'title': 'kult-ur revista interdisciplinària sobre la cultura de la ciutat', 'identifiers': ['issn:2386-5458', '2386-5458']}]",core
86633917,2015-01-01T00:00:00,"The iMagiMat Smart Carpet is a tomography-based polymer optical fibre (POF) sensor system that images the real-time deformation exerted by human footsteps, to access characteristics of individual gait and mobility. Measurements of changes in POF transmission, due to small deformations of individual optical fibres embedded within the carpet surface, are mapped using a sensing grid of Toray POF sensor elements of 1mm diameter (multimode, 980μm inner diameter, 10μm cladding) and fed into an inverse tomography imaging problem solver. We report recent progress; including the implementation of fast footprint ""centre of mass"" calculation, related to ""centre of pressure"" and ground reaction force, the suitability of machine learning techniques for the extraction of gait parameters; and evaluation of iMagiMat performance against more conventional technology such as GAITrite, force plates and inertial motion sensors",International Conference on Plastic Optical Fibers,Imagimat smart carpet: POF layer to detect gait and mobility,,,,core
220620863,2015-01-01T00:00:00,"Emancipar significa alliberar, concedir autonomia. La ciutat moderna s’ha desenvolupat a mida de la ciutadania adulta i dels automòbils, els xiquets
i les xiquetes han perdut la possibilitat de moure’s per la ciutat de manera
autònoma i no se’ls reconeix com a ciutadans, sinó només com a futurs ciutadans
que necessiten cura i protecció. Davant dels incompliments dels adults i
els «embolics» que provoquen en el món contemporani, el projecte La Ciutat
de les Xiquetes i dels Xiquets» proposa una emancipació infantil real i crida
els xiquets a participar en el govern de les ciutats, per a retornar-los la possibilitat
de moure’s de manera autònoma als espais públics de la ciutat. El projecte
pretén aplicar-hi totalment la Convenció dels Drets de la Infantesa, aprovada
per les Nacions Unides, en 1989, i fins ara poc coneguda i pràcticament sense
aplicar. Al final de l’article s’inclouen alguns exemples de com propostes
infantils concretes podrien modificar substancialment la política urbana.Emancipating means freeing, granting autonomy. In the modern city, which has developed to accommodate adult citizens and their cars, children have lost the ability to move autonomously and they are not recognisedas citizens, but rather only as future citizens, in need of care and protection. In light of adults’ inadequacies and the ‘mess’ they are making in today’s world, the City of Girls and Boys project calls for real emancipation for children, inviting them to participate in governing their cities and giving them back the ability to move autonomously in the city’s public spaces. The aim of the project is to apply in full the Convention on the Rights of the Child approved
by the United Nations in 1989 and to date, unfamiliar to most and yet to be
implemented practically everywhere. The article concludes with some examples
of how children’s specific proposals can substantially alter the politics of our cities.Emancipare significa rendere liberi, concedere autonomia. Nella città moderna, che si è sviluppata a misura dei cittadini adulti e delle loro automobili, i bambini hanno perso la possibilità di muoversi autonomamente e non vengono riconosciuti come cittadini, ma solo come futuri cittadini, bisognosi di cura e di protezione. Di fronte alle inadempienze degli adulti e ai “guai” che stanno producendo nel mondo contemporaneo, il progetto “La città delle bambine e dei bambini” propone una reale emancipazione infantile chiamando i bambini a partecipare al governo delle città e restituendo loro la possibilità di muoversi autonomamente negli spazi pubblici della città. Il progetto intende dare piena attuazione alla Convenzione dei diritti dell’Infanzia, approvata dalle Nazioni Unite nel 1989 e finora poco conosciuta e sostanzialmente inattuata. A conclusione del contributo si riferiscono alcuni esempi di come concrete proposte infantili possano modificare sostanzialmente la politica delle nostre città.Emancipar significa liberar, conceder autonomía. La ciudad moderna, se ha desarrollado a medida de los ciudadanos adultos y de sus automóviles, los niños han perdido la posibilidad de moverse de forma autónoma yno se les reconoce como ciudadanos, sino sólo como futuros ciudadanos, necesitados de cuidado y protección. Frente a los incumplimientos de los adultos y a los «líos» que están provocando en el mundo contemporáneo, el proyecto La Ciudad de las Niñas y los Niños propone una emancipación infantil real llamando a los niños a participar en el gobierno de las ciudades y devolviéndoles la posibilidad de moverse de manera autónoma en los espacios públicos de la ciudad. El proyecto pretende aplicar en su totalidad la Convención de los derechos de la Infancia, aprobada por las Naciones Unidas en 1989 y hasta el momento poco conocida y prácticamente sin aplicar. Al final del artículo se incluyen algunos ejemplos de cómo propuestas infantiles concretas podrían modificar sustancialmente la política de nuestras ciudades",,A proposito di emancipare: “Se gli adulti non ascoltano i bambini vanno incontro a guai grossi”  La città dei bambini: una nuova filosofia di governo delle città,,,,core
196618218,2015-01-01T00:00:00,"Complex real problems increasingly require intelligent systems that combine knowledge, techniques and methodologies from various sources. Intelligent systems based on artificial intelligence techniques that are associated with the behavior of people can perform the processes of learning, reasoning and solving all kinds of problems. Such systems, which automatically can perform tasks set by the user or other software, today thankfully called intelligent agents. Independent, intelligent agents on the Internet can be very successful to perform some search work on behalf of and for the needs of different users. For efficient collection, manipulation and management of data, such software can be very interesting from the standpoint of intelligent data analysis in many areas the police. Analysis of the data collected by an intelligent agent (a software robot-bot) can be successfully utilized, among many jobs in the police, and in the field of crime and in particular manifestation of cyber­crime, traffic safety, emergencies, etc. To make the collection and analysis of data from criminal activities on the Internet effective, it is necessary to examine the existing artificial intelligence techniques to be used for the conclusion of the intelligent agents. On the other hand, using of methods of artificial intelligence in finding data along with intelligent data analysis (data mining) should be used, which has found wide use in the area of business, economics, mechanics, medicine, genetics, transport etc.Kompleksni realni problemi sve češće zahtevaju inteligentne sisteme koji kombinuju znanje, tehnike i metodologije iz različitih izvora. Inteligentni sistemi bazirani na tehnikama veštačke inteligencije koje asociraju na ponašanje ljudi mogu da obavljaju procese učenja, zaključivanja i rešavanje raznovrsnih problema. Ovakvi sistemi, koji automatski mogu da izvrše zadatke zadate od strane korisnika ili drugih softvera, danas se sreću pod imenom inteligentni agenti. Samostalno, inteligentni agenti na Internetu mogu veoma uspešno da izvode neki pretraživački posao u ime i za potrebe raznih korisnika. Zbog efikasnog sakupljanja, manipulisanja i upravljanja podacima, ovakvi softveri mogu biti veoma interesantni sa stanovišta inteligentne analize podataka u mnogim oblastima policije. Analiza podataka sakupljenih od strane inteligentnog agenta (softverskog robota - bota) može se uspešno iskoristiti, između mnogih poslova u policiji, i na polju kriminala i naročito pojavnog oblika sajber kriminala, bezbednosti saobraćaja, vanrednih situacija itd. Kako bi sakupljanje i analiza podataka iz kriminalnih aktivnosti na Internetu bila efikasna, neophodno je sagledati postojeće tehnike veštačke inteligencije koje se koriste za zaključivanje u inteligentnim agentima. S druge strane, treba iskoristiti metode veštačke inteligencije u pronalaženju podataka pri inteligentnoj analizi podataka (data mining-u) koja je našla široku primenu u oblasti poslovanja preduzeća, ekonomije, mehanike, medicine, genetike, saobraćaja i sl",'Centre for Evaluation in Education and Science (CEON/CEES)',Veštačka intelegencija u prikupljanju i analizi podataka u policiji,https://core.ac.uk/download/196618218.pdf,10.5937/NBP1503131K,"[{'title': 'Nauka bezbednost policija', 'identifiers': ['issn:0354-8872', '0354-8872']}]",core
296622416,2015-11-26T14:16:33Z,"[No abstract available]312563Arai, T., Pagello, E., Parker, L., Guest editorial, advances in multi-robot systems (2002) IEEE Transactions on Robotics and Automation, 18 (5), pp. 655-661Bonabeau, E., Dorigo, M., Theraulaz, G., (1999) Swarm Intelligence: From Natural to Artificial SystemsBalch, T., Arkin, R.C., Communication in reactive multiagent robotic systems (1994) Autonomous Robots, 1 (1), pp. 27-52Grasse, P., La reconstruction du nid et les coordinations inter-individuelle chez bellicoitermes natalenis et cubitermes sp la theorie de la stigmergie: Essai d'interpretation des termites constructeurs Insectes Sociaux, 6, p. 1959Camazine, S., Franks, N.R., Sneyd, J., Bonabeau, E., Deneubourg, J.-L., Theraula, G., (2001) Self-Organization in Biological Systems, , Princeton University PressHolland, O., Melhuish, C., Stimergy, self-organization, and sorting in collective robotics (1999) Artificial Life, 5 (2), pp. 173-202Cazangi, R.R., Von Zuben, F.J., Figueiredo, M.F., A classifier system in real applications for robot navigation (2003) Proceedings of the 2003 Congress on Evolutionary Computation, 1, pp. 574-580. , Canberra, Australia IEEE PressCazangi, R.R., Uma proposta evolutiva para controle inteligente em navegação autônoma de robôs (2004), Master's thesis, Faculdade de Engenharia Elétrica e de Computação, Universidade Estadual de CampinasHolland, J., Escaping brittleness: The possibilities of general purpose learning algorithms applied to parallel rule-based systems (1986) Machine Intelligence II, , In R. Michalsky, J. Carbonell, and T. Mitchell, editors Morgan KaufmannBrooks, R.A., Intelligence without reason (1991) Proceedings of the 1991 International Joint Conference on Artificial Intelligence, pp. 569-595Kube, C.R., Parker, C., Wang, T., Zhang, H., Biologically inspired collective robotics (2004) Recent Developments in Biologically In-spired Computing, , In L. N. de Castro and F. J. Von Zuben, editors, Idea Group IncNolfi, S., Floriano, D., (2000) Evolutionary Robotics, , The MIT PressEdelen, M., Swarm intelligence and stigmergy: Robotic implementation of foraging behavior (2003), Master's thesis, University of MarylandBonabeau, E., Theraulaz, G., Deneubourg, J.-L., Aron, S., Camazine, S., Self-organization in social insects (1997) Trends in Ecology and Evolution, 12, pp. 188-193de Castro, L.N., Timmis, J., (2002) Artificial Immune Systems: A New Computational Intelligence Paradigm, , SpringerVerlagResnick, M., Turtles, Termites, and Traffic Jams: Explorations in Massively Parallel Microworlds, , Bradford Books/MIT PressHolldobler, B., Wilson, E., (1990) The Ants, , Belknap Press of Harvard University PressCaetano, F.H., Klaus, J., Zara, F.J., (2002) Formigas: Biologia E Anatomia, , Editora da UNESPDeneubourg, J.L., Pasteels, J.M., Verhaeghe, J.C., Probabilistic behaviour in ants: A strategy of errors? (1983) Journal of Theoretical Biology, 105, pp. 259-271Pasteels, J., Deneubourg, J.L., Goss, S., Self-organization mechanisms in ant societies (i): Trail recruitment to newly discovered food sources (1987) Experientia Supplementum, 54, pp. 155-175Cao, Y., Fukunaga, A., Kahng, A., Cooperative mobile robotics: Antecedents and directions (1997) Autonomous Robots, 4 (1), pp. 7-27Tambe, M., Adabi, J., Al-Onaizan, Y., Erden, A., Kaminka, G., Marsella, S.C., Muslea, I., Building agent teams using an explicit teamwork model and learning (1999) Artificial Intelligence, 110 (2), pp. 215-239Wagner, I.A., Bruckstein, A.M., Cooperative cleaners: A study in ant-robotics (1997) Communications, Computation, Control, and Signal Processing, pp. 298-308Ding, Y., He, Y., Jiang, J., Multi-robot cooperation method based on the ant algorithm (2003) Proceedings of the 2003 IEEE Swarm Intelligence Symposium, pp. 14-18. , Indianapolis, USA IEEEDrogoul, A., Ferber, J., From tom thumb to the dockers: Some experiments with foraging robots (1992) Proceedings of the Second International Conference on Simulation of Adaptive Behavior, pp. 451-459. , Honolulu, USASugawara, K., Watanabe, T., Swarming robots - Foraging behavior of simple multirobot system (2002) Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 2702-2707. , Lausanne, SwitzerlandNicolis, S.C., Deneubourg, J.L., Emerging patterns and food recruitment in ants: An analytical study (1999) Journal of Theoretical Biology, 198, pp. 575-592Deneubourg ans, J.L., Aron, S., Goss, S., Pasteels, J.M., The self-organizing exploratory pattern of the argentine ant (1990) Journal of Insect Behavior, 3, pp. 159-168Ermentrout, G., Edelstein-Keshet, L., Cellular automata approaches to biological modeling (1993) Journal of Theoretical Biology, 160, pp. 97-133Vaughan, R., Stoy, K., Sukhatme, G., Mataric, M., Lost: Localization-space trails for robot teams IEEE Transactions on Robotics and Automation, 18 (5)Sauter, J., Matthews, R., Parunak, H., Brueckner, S., Evolving adaptive pheromone path planning mechanisms (2002) Proceedings of the First International Joint Conference on Autonomous Agents and Multi-Agent Systems, pp. 434-440Deneubourg, J.L., Goss, S., Franks, N., Sendova-Franks, A., Detrain, C., Christien, L., The dynamics of collective sorting robot-like ants and ant-like robots (1991) Proceedings of the First International Conference on Simulation of Adaptive Behavior on From Animals to Animats, pp. 356-363. , Paris, France MIT PressBeckers, R., Holland, O.E., Deneubourg, J.L., From Local Actions to Global Tasks: Stigmergy and Collective Robotics (1994) Proceedings of the 4th International Workshop on the Synthesis and Simulation of Living Systems, pp. 181-189. , In R. A. Brooks and P. Maes, editors, Cambridge, USA MIT PressKube, C.R., Bonabeau, E., Cooperative transport by ants and robots (2000) Robotics and Autonomous Systems, 1, pp. 85-101Rybski, P., Larson, A., Veeraraghavan, H., LaPoint, M., Gini, M., Communication strategies in multi-robot search and retrieval (2004) Proceedings of the 7th International Symposium on Distributed Autonomous Robotic Systems, pp. 301-310. , Toulouse, FranceWurr, A., Robotic team navigation in complex environments using stigmergic clues (2003), Master's thesis, University of ManitobaCazangi, R.R., Von Zuben, F.J., Figueiredo, M.F., Autonomous navigation system applied to collective robotics with ant-inspired communication (2005) Proceedings of the 2005 Conference on Genetic and Evolutionary Computation, (1), pp. 121-128. , Washington DC, USA ACM PressSvennebring, J., Koenig, S., Towards building terrain-covering ant robots (2002) In Ant Algorithms, pp. 202-215Ziemke, T., On the role of robot simulations in embodied cognitive science (2003) Artificial Intelligence and Simulation of Behaviour, 1 (4), pp. 1-11Dorigo, M., Stützle, T., (2004) Ant Colony Optimization, , MIT Press/Bradford Book",,Stigmergic Autonomous Navigation In Collective Robotics,,10.1007/978-3-540-34690-6_2,,core
296644543,2015-11-26T15:04:41Z,"The frequent growth of visual data, either by countless monitoring video cameras wherever we go or the popularization of mobile devices that allow each person to create and edit their own images and videos have contributed enormously to the so-called big-data revolution. This shear amount of visual data gives rise to a Pandora box of new visual classification problems never imagined before. Image and video classification tasks have been inserted in different and complex applications and the use of machine learning-based solutions has become the most popular approach for several applications. Notwithstanding, there is no silver bullet that solves all the problems, i.e., it is not possible to characterize all images of different domains with the same description method nor is it possible to use the same learning method to achieve good results in any kind of application. In this work, we aim at proposing a framework for classifier selection and fusion. Our method seeks to combine image characterization and learning methods by means of a meta-learning approach responsible for assessing which methods contribute more towards the solution of a given problem. The framework uses a strategy of classifier selection which pinpoints the less correlated, yet effective, classifiers through a series of diversity measures analysis. The experiments show that the proposed approach achieves comparable results to well-known algorithms from the literature on four different applications but using less learning and description methods as well as not incurring in the curse of dimensionality and normalization problems common to some fusion techniques. Furthermore, our approach is able to achieve effective classification results using very reduced training sets. The proposed method is also amenable to continuous learning and flexible enough for implementation in highly-parallel architectures. © 2013 Elsevier B.V. All rights reserved.3915264Antonie, M.-L., Zaïane, O.R., Coman, A., Application of data mining techniques for medical image classification (2001) Workshop on Multimedia Data Mining at ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD), pp. 94-101Benediktsson, J.A., Sveinsson, J.R., Ersoy, O.K., Swain, P.H., Parallel consensual neural networks (1997) IEEE Transactions on Neural Networks, 8 (1), pp. 54-64. , PII S1045922797002385Bishop, C.M., (2006) Pattern Recognition and Machine Learning (Information Science and Statistics), , first ed. Springer Verlag New York, Inc., Secaucus, NJ, USABoser, B.E., Guyon, I.M., Vapnik, V.N., A training algorithm for optimal margin classifiers (1992) Workshop on Computational Learning Theory, pp. 144-152Breiman, L., Bagging predictors (1996) Machine Learning, 24 (2), pp. 123-140Breiman, L., Random forests (2001) Machine Learning, 45 (1), pp. 5-32. , DOI 10.1023/A:1010933404324Brennan, R.L., Prediger, D.J., Coefficient kappa: Some uses, misuses, and alternatives (1981) Educ. Psychol. Meas., 41 (3), pp. 687-699Brooks, D. 2013. The philosophy of data. In: The New York Times, A23, February, 5thChang, C.-C., Lin, C.-J., LIBSVM: A library for support vector machines (2011) ACM Trans. Intell. Syst. Technol., 2 (3), pp. 271-2727Cristianini, N., Shawe-Taylor, J., (2000) An Introduction to Support Vector Machines and Other Kernel-based Learning Methods, , Cambridge University PressDeng, J., Berg, A.C., Li, K., Fei-Fei, L., What does classifying more than 10,000 image categories tell us (2010) European Conf. on Computer Vision, , Springer-Verlag Berlin, Heidelberg 71-84Dos Santos, J.A., Penatti, O.A.B., Torres, R., Da, S., Evaluating the potential of texture and color descriptors for remote sensing image retrieval and classification (2010) Int. Conf. on Computer Vision Theory and Applications, pp. 203-208Dos Santos, J.A., Faria, F.A., Torres, R., Da, S., Rocha, A., Gosselin, P.-H., Philipp-Foliguet, S., Falcao, A., Descriptor correlation analysis for remote sensing image multi-scale classification (2012) 21st International Conference On, Pattern Recognition (ICPR) 2012, pp. 3078-3081Dos Santos, J.A., Gosselin, P.-H., Philipp-Foliguet, S., Torres, R.D.S., Falcão, A.X., Interactive multiscale classification of high-resolution remote sensing images, selected topics in applied earth observations and remote sensing (2013) IEEE J. PP, (99), pp. 1-15Duin, R.P.W., Pekalska, E., Open issues in pattern recognition (2005) Int. Conf. on Computer Recognition Systems (CORE), 30, pp. 27-42Džeroski, S., Ženko, B., Is combining classifiers with stacking better than selecting the best one (2004) Mach. Learn., 54 (3), pp. 255-273Faria, F.A., Calumby, R.T., Torres, R., Da, S., RECOD at ImageCLEF 2011: Medical modality classification using genetic programming (2011) CLEF (Notebook Papers/Labs/Workshop), pp. 1-8Faria, F.A., Dos Santos, J.A., Torres, R., Da, S., Rocha, A., Falcão, A.X., Automatic fusion of region-based classifiers for coffee crop recognition (2012) IEEE Geoscience and Remote Sensing Symposium, pp. 2221-2224Faria, F.A., Dos Santos, J.A., Rocha, A., Torres, R., Da, S., Automatic Classifier Fusion for Produce Recognition (2012) Conf. on Graphics, Patterns and Images, pp. 252-259Fei-Fei, L., Fergus, R., Perona, P., Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories (2004) IEEE Int. Conf. on Computer Vision and Pattern Recognition Workshop, p. 178Fernando, B., Fromont, E., Muselet, D., Sebban, M., Discriminative feature fusion for image classification (2012) IEEE Int. Conf. on Computer Vision and Pattern Recognition, pp. 3434-3441Freund, Y., Schapire, R.E., Experiments with a new boosting algorithm (1996) Int. Conf. on Machine Learning, pp. 148-156Friedman, J., Hastie, T., Tibshirani, R., (2001) The Elements of Statistical Learning, , first ed. SpringerGehler, P., Nowozin, S., On feature combination for multiclass object classification (2009) Intl. Conf. on Computer Vision, pp. 221-228Guigues, L., Cocquerez, J.P., Le Men, H., Scale-sets image analysis (2006) International Journal of Computer Vision, 68 (3), pp. 289-317. , DOI 10.1007/s11263-005-6299-0Hampshire, I.B.J., Waibel, A., The Meta-Pi network: Building distributed knowledge representations for robust multisource pattern recognition (1992) IEEE Trans. Pattern Anal. Mach. Intell., 14 (7), pp. 751-769Harchaoui, Z., Douze, M., Paulin, M., Dudik, M., Malick, J., Large-scale image classification with trace-norm regularization (2012) IEEE Intl. Conf. on Computer Vision and Pattern Recognition, pp. 3386-3393. , 2012Ho, T.K., Hull, J., Srihari, S., Decision combination in multiple classifier systems (1994) IEEE Trans. Pattern Anal. Mach. Intell., 16 (1), pp. 66-75Hong, J.-H., Min, J.-K., Cho, U.-K., Cho, S.-B., Fingerprint classification using one-vs-all support vector machines dynamically ordered with nai{dotless}ve Bayes classifiers (2008) Pattern Recognition, 41 (2), pp. 662-671. , DOI 10.1016/j.patcog.2007.07.004, PII S0031320307003299Hou, J., Zhang, B.-P., Qi, N.-M., Yang, Y., Evaluating feature combination in object classification Int. Conf. on Advances in Visual Computing - Volume Part II, pp. 597-606Huang, C., Liu, Q., An orientation independent texture descriptor for image retrieval (2007) Int. Conf. on Computational Science, pp. 772-776Huang, J., Kumar, R., Mitra, V., Zhu, W., Zabih, R., Image indexing using color correlograms (1997) IEEE Int. Conf. on Computer Vision and Pattern Recognition, pp. 762-768Jurek, A., Bi, Y., Wu, S., Nugent, C., Classification by cluster analysis: A new meta-learning based approach (2011) Int. Conf. on Multiple Classifier Systems, pp. 259-268Kendall, M.G., A new measure of rank correlation (1938) Biometrika, 30 (12), pp. 81-93Kim, S.D., Baek, Y.-M., Kim, W.-Y., Reducing overfitting of AdaBoost by clustering-based pruning of hard examples (2013) Proceedings of the 7th International Conference on Ubiquitous Information Management and Communication, p. 90Ko, B., Gim, J., Nam, J., Cell image classification based on ensemble features and random forest (2011) Electron. Lett., 47 (11), pp. 638-639Kuncheva, L.I., Whitaker, C.J., Measures of diversity in classifier ensembles and their relationship with the ensemble accuracy (2003) Mach. Learn., 51 (2), pp. 181-207Lin, Y., Lv, F., Zhu, S., Yang, M., Cour, T., Yu, K., Cao, L., Huang, T., Large-scale image classification: Fast feature extraction and SVM trainingA (2011) IEEE Intl. Conf. on Computer Vision and Pattern Recognition, pp. 1689-1696Ma, Z., Redmond, R., Tau coefficients for accuracy assessment of classification of remote sensing data (1995) Photogram. Eng. Remote Sens., 61 (4), pp. 439-453Mahmoudi, F., Shanbehzadeh, J., Eftekhari-Moghadam, A.-M., Soltanian-Zadeh, H., Image retrieval based on shape similarity by edge orientation autocorrelogram (2003) Pattern Recognition, 36 (8), pp. 1725-1736. , DOI 10.1016/S0031-3203(03)00010-4Mansano, V., Matsuoka, J.A., Afonso, L.C.S., Papa, J.P., Faria, F., Torres, R., Da, S., Improving image classification through descriptor combination (2012) Conf. on Graphics, Patterns and Images, pp. 324-329Morais, E., Goldenstein, S., Ferreira, A., Rocha, A., Automatic tracking of indoor soccer players using videos from multiple cameras (2012) Conf. on Graphics, Patterns and Images, pp. 174-181Nakamura, E.F., Loureiro, A.A.F., Frery, A.C., Information fusion for wireless sensor networks: Methods, models, and classifications (2007) ACM Comput Surv., 39 (3)Oza, N.C., Tumer, K., Classifier ensembles: Select real-world applications (2008) Information Fusion, 9 (1), pp. 4-20. , DOI 10.1016/j.inffus.2007.07.002, PII S1566253507000620, Applications of Ensemble MethodsPasserini, A., Pontil, M., Frasconi, P., New results on error correcting output codes of kernel machines (2004) IEEE Trans. Neural Networks, 15 (1), pp. 45-54Pass, G., Zabih, R., Miller, J., Comparing images using color coherence vectors (1996) ACM Multimedia, pp. 65-73Pedronette, D.C.G., Torres, R., Da, S., Exploiting contextual spaces for image re-ranking and rank aggregation (2011) ACM Int. Conf. on Multimedia Retrieval, pp. 131-138Pedronette, D.C.G., Torres, R., Da, S., Image re-ranking and rank aggregation based on similarity of ranked lists (2011) Int. Conf. on Computer Analysis of Images and Patterns - Volume Part i, pp. 369-376Penatti, O.A.B., Valle, E., Torres, R.D.S., Comparative study of global color and texture descriptors for web image retrieval (2012) J. Visual Commun. Image Represent., 23 (2), pp. 359-380Perrone, M.P., Cooper, L.N., (1993) When Networks Disagree: Ensemble Methods for Hybrid Neural Networks, , Chapman and HallRamos, C.C.O., Souza, A.N., Chiachia, G., Falcão, A.X., Papa, J.P., A novel algorithm for feature selection using Harmony search and its application for non-technical losses detection (2011) Comput. Electric. Eng., 37 (6), pp. 886-894Rocha, A., Hauagge, D.C., Wainer, J., Goldenstein, S., Automatic fruit and vegetable classification from images (2010) Elsevier Comput. Electron. Agric., 70 (1), pp. 96-104Rocha, A., Papa, J.P., Meira, L.A.A., How far do we get using machine learning black-boxes (2012) Int. J. Pattern Recogn. Artif. Intell., 26 (2), pp. 12610011-126100123Rokach, L., Ensemble-based classifiers (2010) J. Artif. Intell. Rev., 33 (12), pp. 1-39Ross, A.A., Nandakumar, K., Jain, A.K., (2006) Handbook of Multibiometrics (Int. Series on Biometrics), , Springer-Verlag New York, Inc., Secaucus, NJ, USA 0387222960Schapire, R.E., A Brief Introduction to Boosting (1999) Int. Joint Conf. on Artificial Intelligence, pp. 1401-1406Segata, N., Blanzieri, E., Fast and scalable local kernel machines (2010) J. Mach. Learn. Res., 99, pp. 1883-1926Shipp, C.A., Kuncheva, L.I., An investigation into how AdaBoost affects classifier diversity (2009) IPMU, pp. 203-208. , http://hdl.handle.net/10242/41889Stehling, R., Nascimento, M., Falcão, A.X., A compact and efficient image retrieval approach based on border/interior pixel classification (2002) Int. Conf. on Information and Knowledge Management, pp. 102-109Sun, Y., Todorovic, S., Li, J., Reducing the overfitting of adaboost by controlling its data distribution skewness (2006) International Journal of Pattern Recognition and Artificial Intelligence, 20 (7), pp. 1093-1116. , DOI 10.1142/S0218001406005137, PII S0218001406005137Sun, B., Luo, J., Shu, S., Yu, N., Introduce randomness into AdaBoost for robust performance on noisy data (2010) Fuzzy Systems and Knowledge Discovery (FSKD), 2010 Seventh International Conference on, 4, pp. 1858-1861Suzuki, C.T.N., Gomes, J.F., Falcão, A.X., Papa, J.P., Shimizu, S.H., Automatic segmentation and classification of human intestinal parasites from microscopy images (2012) IEEE Trans. Biomed. Eng., 60 (3), pp. 803-812Swain Michael, J., Ballard Dana, H., Color indexing (1991) International Journal of Computer Vision, 7 (1), pp. 11-32Tao, B., Dickinson, B., Texture recognition and image retrieval using gradient indexing (2000) J. Visual Commun. Image Represent., 11 (3), pp. 327-342Unser Michael, Sum and difference histograms for texture classification (1986) IEEE Transactions on Pattern Analysis and Machine Intelligence, PAMI-8 (1), pp. 118-125Viola, P., Jones, M., Rapid object detection using a boosted cascade of simple features (2001) IEEE Int. Conf. on Computer Vision and Pattern Recognition, pp. 511-518Weber, R., Schek, H.-J., Blott, S., A quantitative analysis and performance study for similarity-search methods in high-dimensional spaces (1998) Proceedings of the International Conference on Very Large Data Bases, pp. 194-205Xia, X., O'Gorman, L., Innovations in fingerprint capture devices (2002) Pattern Recognition, 36 (2), pp. 361-369. , DOI 10.1016/S0031-3203(02)00036-5, PII S0031320302000365Xiao, J., Hays, J., Ehinger, K., Oliva, A., Torralba, A., SUN database: Large-scale scene recognition from abbey to zoo (2010) IEEE Intl. Conf. on Computer Vision and Pattern Recognition, pp. 3485-3492Zegarra, J., Leite, N., Torres, R.D.S., Wavelet-based feature extraction for fingerprint image retrieval (2008) J. Comput. Appl. Math., 227 (2), pp. 294-307Zhou, W., Huang, G., Troy, A., Cadenasso, M.L., Object-based land cover classification of shaded areas in high spatial resolution imagery of urban areas: A comparison study (2009) Remote Sens. Environ., 113, pp. 1769-177",,A Framework For Selection And Fusion Of Pattern Classifiers In Multimedia Recognition,,10.1023/A:1010933404324,,core
44523753,2016-01-01T00:00:00,"This monograph opens up new horizons for engineers and researchers in academia and in industry dealing with or interested in new developments in the field of system identification and control. It emphasizes guidelines for working solutions and practical advice for their implementation rather than the theoretical background of Gaussian process (GP) models. The book demonstrates the potential of this recent development in probabilistic machine-learning methods and gives the reader an intuitive understanding of the topic. The current state of the art is treated along with possible future directions for research. Systems control design relies on mathematical models and these may be developed from measurement data. This process of system identification, when based on GP models, can play an integral part of control design in data-based control and its description as such is an essential aspect of the text. The background of GP regression is introduced first with system identification and incorporation of prior knowledge then leading into full-blown control. The book is illustrated by extensive use of examples, line drawings, and graphical presentation of computer-simulation results and plant measurements. The research results presented are applied in real-life case studies drawn from successful applications including: a gas–liquid separator control; urban-traffic signal modelling and reconstruction; and prediction of atmospheric ozone concentration. A MATLAB® toolbox, for identification and simulation of dynamic GP models is provided for download. Advances in Industrial Control aims to report and encourage the transfer of technology in control engineering. The rapid development of control technology has an impact on all areas of the control discipline. The series offers an opportunity for researchers to present an extended exposition of new work in all aspects of industrial control",'Springer Science and Business Media LLC',Modelling and control of dynamic systems using gaussian process models,,10.1007/978-3-319-21021-6,,core
237666522,2017-01-01T08:00:00,"Web image analysis has witnessed an AI renaissance. The ILSVRC benchmark has been instrumental in providing a corpus and standardized evaluation. The NVIDIA AI City Challenge is envisioned to provide similar impetus to the analysis of image and video data that helps make cities smarter and safer. In its first year, this Challenge has focused on traffic video data. While millions of traffic video cameras around the world capture data, albeit low-quality, very little automated analysis and value creation results. Lack of labeled data, and trained models that can be deployed at the edge of the city fabric, ensure that most traffic video data goes through little or no automated analysis. Real-time and batch analysis of this data can provide vital breakthroughs in real-time traffic management as well as pedestrian safety. The NVIDIA AI City Challenge brought together 29 teams from universities in 4 continents to collaboratively annotate a 125 hour data set and then compete on detection, localization and classification tasks as well as traffic and safety application analytics tasks. The result is the largest high quality annotated data set, a set of models trained using NVIDIA AI City Edge to Cloud platform and ready to be deployed at the edge solving traffic and safety problems for cities worldwide",Iowa State University Digital Repository,The NVIDIA AI City Challenge,https://core.ac.uk/download/237666522.pdf,,,core
73382112,2017-02-03T00:00:00,"Many model based scientific and engineering methodologies, such as system
identification, sensitivity analysis, optimization and control, require a large
number of model evaluations. In particular, model based real-time control of
urban water infrastructures and online flood alarm systems require fast
prediction of the network response at different actuation and/or parameter
values. General purpose urban drainage simulators are too slow for this
application. Fast surrogate models, so-called emulators, provide a solution to
this efficiency demand. Emulators are attractive, because they sacrifice
unneeded accuracy in favor of speed. However, they have to be fine-tuned to
predict the system behavior satisfactorily. Also, some emulators fail to
extrapolate the system behavior beyond the training set. Although, there are
many strategies for developing emulators, up until now the selection of the
emulation strategy remains subjective. In this paper, we therefore compare the
performance of two families of emulators for open channel flows in the context
of urban drainage simulators. We compare emulators that explicitly use
knowledge of the simulator's equations, i.e. mechanistic emulators based on
Gaussian Processes, with purely data-driven emulators using matrix
factorization. Our results suggest that in many urban applications, naive
data-driven emulation outperforms mechanistic emulation. Nevertheless, we
discuss scenarios in which we think that mechanistic emulation might be
favorable for i) extrapolation in time and ii) dealing with sparse and unevenly
sampled data. We also provide many references to advances in the field of
Machine Learning that have not yet permeated into the Bayesian environmental
science community.Comment: This article was published in Environmental Modelling and Software, 7
  figures, 4 table",'Elsevier BV',"Appraisal of data-driven and mechanistic emulators of nonlinear
  hydrodynamic urban drainage simulators",http://arxiv.org/abs/1609.08395,10.1016/j.envsoft.2017.02.006,,core
131988905,2016-09-16T00:00:00,"Part 4: Environmental AI Modeling (ENAIM)International audienceAir pollution with suspended particles from PM2.5 fraction represents an important factor to increasing atmospheric pollution degree in urban areas, with a significant potential effect on the health of vulnerable people such as children and elderly. PM2.5 air pollutant concentration continuous monitoring represents an efficient solution for the environment management if it is implemented as a real time forecasting system which can detect the PM2.5 air pollution trends and provide early warning or alerting to persons whose health might be affected by PM2.5 air pollution episodes. The forecasting methods for PM concentration use mainly statistical and artificial intelligence-based models. This paper presents a model based protocol, MBP – PM2.5 forecasting protocol, for the selection of the best ANN model and a case study with two artificial neural network (ANN) models for real time short-term PM2.5 forecasting",'Springer Science and Business Media LLC',Applying Artificial Neural Networks to Short-Term PM2.5 Forecasting Modeling,,10.1007/978-3-319-44944-9_18,,core
275634298,2017-01-01T00:00:00,"[EN] Intelligent transportation systems (ITS) rely on connected vehicle applications to address real-world problems. Research is currently being conducted to support safety, mobility and environmental applications. This paper presents the DrivingStyles architecture, which adopts data mining techniques and neural networks to analyze and generate a classification of driving styles and fuel consumption based on driver characterization. In particular, we have implemented an algorithm that is able to characterize the degree of aggressiveness of each driver. We have also developed a methodology to calculate, in real-time, the consumption and environmental impact of spark ignition and diesel vehicles from a set of variables obtained from the vehicle's electronic control unit (ECU). In this paper, we demonstrate the impact of the driving style on fuel consumption, as well as its correlation with the greenhouse gas emissions generated by each vehicle. Overall, our platform is able to assist drivers in correcting their bad driving habits, while offering helpful tips to improve fuel economy and driving safety.This work was partially supported by the Ministerio de Economía y Competitividad, Programa Estatal de Investigación, Desarrollo e Innovación Orientada a los Retos de la Sociedad, Proyectos I+D+I 2014, Spain, under Grant TEC2014-52690-R.Meseguer Anastasio, JE.; Toh, CK.; Tavares De Araujo Cesariny Calafate, CM.; Cano, J.; Manzoni, P. (2017). DrivingStyles: A Mobile Platform for Driving Styles and Fuel Consumption Characterization. Journal of Communications and Networks. 19(2):162-168. doi:10.1109/JCN.2017.000025S16216819",'Institute of Electrical and Electronics Engineers (IEEE)',DrivingStyles: A Mobile Platform for Driving Styles and Fuel Consumption Characterization,http://hdl.handle.net/10251/99369,10.1109/JCN.2017.000025,,core
144826390,22/06/2017,"TIMON is an European research project under the Horizon 2020 programme. The main objective of
this project is to provide real-time services through a web based platform and a mobile APP for drivers,
Vulnerable Road Users (VRUs) and businesses. These services will contribute to increasing drivers
and VRUs assistance and safety. To provide these services, one of the core technologies developed
inside TIMON will be the design and development of Artificial Intelligence (AI) techniques for traffic
prediction and route planning. The DeustoTech-Mobility research group is in charge of this part of the
project. The objective of this technical paper is to describe the approach followed in TIMON to
develop traffic congestion prediction and route planning services based on AI techniques and the
progress done so far. Additionally, the deployment and the result obtained in the first test done is also
detailed in this study",,"Application of Artificial Intelligence Techniques to Traffic Prediction and Route Planning, the vision of TIMON project",,10.5281/zenodo.894075,,core
30412261,2015-01-01T00:00:00,"Tese (mestrado) - Universidade Federal de Santa Catarina, Centro Tecnológico, Programa de Pós-Graduação em Engenharia de Automação e Sistemas, Florianópolis, 2015.As Redes Tolerantes a Atrasos e Interrupções (DTN) foram concebidas para operar considerando interrupções e grandes atrasos na comunicação. O roteamento se torna uma tarefa mais desafiadora em contextos com alta frequência de mudança da topologia e poucas informações a respeito da topologia futura. Em uma rede formada somente por ônibus do sistema de transporte público, os contatos entre os ônibus acontecem de forma quase-oportunista devido à regularidade não estritamente seguida nos itinerários. Uma forma de melhorar o roteamento nas DTNs é explorar informações históricas e do cenário para aumentar a taxa de entrega de mensagens e diminuir o atraso na entrega e o consumo de recursos. Com poucas informações a serem exploradas no cenário, o roteamento fica mais difícil de ser tratado. Neste contexto, esta tese propõe uma nova abordagem de roteamento baseado em Redes Neurais Artificiais (RNA) que apresenta vantagens em relação as outras estratégias aplicáveis nas mesmas condições, tal como a estratégia do caminho de probabilidade máxima (MaxProp). Um mecanismo de predição de contatos baseado em RNA foi desenvolvido para permitir a obtenção de contatos futuros que então são utilizados em um mecanismo de construção de jornadas, permitindo estimar a melhor jornada até o destino. Um procedimento para projetar as RNAs é apresentado. Um simulador de troca de mensagens foi desenvolvido para testar as estratégias avaliadas. Os resultados obtidos demonstram que a abordagem desenvolvida atinge um maior número de mensagens entregues, menor atraso e menor custo de uso da rede. Esses resultados foram obtidos nas versões com ou sem replicação de mensagens utilizando dados reais ou sintéticos. Uma modelagem para a implementação da estratégia de roteamento proposta projetada para funcionar na arquitetura da Internet Research Task Force (IRTF) é apresentada.Abstract : Delay/disruption Tolerant Networks (DTN) are designed to operate considering interruptions and high delays in communication. The routing becomes a more challenging task in a context of a high frequency of topology changes in which little information are available. In a network formed just by buses of a bus transportation system, the contacts are quasi-opportunistic due the regularity in the itineraries not strictly followed by the bus. A manner to improve the routing in DTN is to exploit historical information to increase the delivery rate and decrease the delivery delay and resources consumption. In this context, this thesis describes a new routing approach based on Artificial Neural Networks (ANN) presenting advantages over other applicable strategies in the same conditions, such as the maximum probability path (Max- Prop). A contact predictor based on ANN was developed to achieve future contacts. The predicted contacts are used in a journey predictor, aiming to obtain the best journey to the destination. A procedure for designing of ANNs is presented. A message forwarding simulator was developed to test the evaluated strategies. The obtained results demonstrate that the developed approach increases the number of delivered messages, decreases the delivery delay and decreases the delivery cost. These results are verified both in the version with message replication, as without message replication, using synthetic or real data. A modeling for the implementation of the proposed routing strategy designed to work in the Internet Research Task Force (IRTF) architecture is presented",,Roteamento em redes tolerantes a atrasos e interrupções: uma abordagem baseada em redes neurais,https://core.ac.uk/download/30412261.pdf,,,core
389906354,2016-10-25T00:00:00,"Purpose.The article is aimed to develop the means and methods of forming a plurality of real and potential structural diagrams for zones of energy recovery and different locations of trains for further training neuro-fuzzy networks on the basis of expert solutions and also for the formation of good control. Methodology. Methodology of mathematical and algorithmic constructivism for modeling the structural diagrams of the electric supply system and modes of traction power consumption and the train’s locations in zones of energy recovery was applied. This approach involves the development of constructive-synthesizing structures (CSS) with transformation by specialization, interpretation, specification and implementation. Development CSS provides an extensible definition media, relations and the signature of operations and constructive axiomatic. The most complex and essential part of the axioms is the set formed by the substitution rules defining the process of withdrawal of the corresponding structures. Findings. A specialized and specified CSS, which allows considering all the possibilities and features, that supply power traction systems with modern equipment, stations and trains location was designed. Its feature: the semantic content of the terminal alphabet images of electrical traction network and power consumers with relevant attributes. A special case of the formation of the structural diagram shows the possibilities CSS in relation to this problem. Originality. A new approach to solving the problem of rational use of energy recovery, which consists in application of the methods and means of artificial neural networks, expert systems, fuzzy logic and mathematical and algorithmic constructivism. This paper presents the methods of constructive simulation of a production-distribution of energy recovery zone structure in the system of the DC traction. Practical value. The tasks decision of the rational use of energy recovery can significantly save energy, contribute to the technical re-equipment of a railway transportation of Ukraine through the introduction of modern means and capabilities. The developed model can be used to solve other energy-saving tasks in different systems of electric transport.Цель. Научная работа посвящена разработке средств и методов формирования множества реальных и потенциальных структурных схем зон рекуперации с различными поездными ситуациями для дальнейшего обучения нейро-нечетких сетей на основе экспертных решений и формирования рационального управления. Методика. Для моделирования структурной схемы системы тягового электроснабжения, режимов тягового электропотребления и поездной ситуации в зоне распределения энергии рекуперации применена методология математико-алгоритмического конструктивизма. Данный подход предусматривает разработку конструктивно-продукционных структур (КПС) с преобразованиями специализации, интерпретации, конкретизации и реализации. Разработка КПС предусматривает определение расширяемого носителя, сигнатуры отношений и операций и конструктивной аксиоматики. Наиболее сложной и существенной частью аксиоматики является множество формируемых правил подстановки, определяющих процесс вывода соответствующих конструкций. Результаты. Разработана специализированная и конкретизированная КПС, которая позволяет учесть все возможности и особенности современного оборудования систем тягового электроснабжения, участков тяговой сети и поездной ситуации. Ее особенность: семантическое наполнение терминального алфавита образами электрооборудования, тяговой сети и потребителей электроэнергии, обладающими соответствующей атрибутикой. Приведенный частный случай формирования структурной схемы демонстрирует возможности КПС применительно к данной задаче. Научная новизна. Авторами предложен новый подход к решению задачи рационального использования энергии рекуперации, который заключается в применении методов и средств искусственных нейронных сетей, экспертных систем и нечеткой логики и математико-алгоритмического конструктивизма. В данной работе представлены методы конструктивно-продукционного моделирования структуры зоны распределения энергии рекуперации в системе тяги постоянного тока. Практическая значимость. Решение задачи рационального использования энергии рекуперации позволяет значительно экономить энергоресурсы, способствовать техническому переоснащению железнодорожного транспорта Украины путем внедрения современных средств и возможностей. Разработанные модели могут применяться для решения и других задач энергосбережения в различных системах электрического транспорта.Мета. Наукова робота присвячена розробці методів та засобів формування множини реальних і потенційних структурних схем зон рекуперації з різними поїзними ситуаціями для подальшого навчання нейро-нечітких мереж на основі експертних рішень для формування раціонального управління. Методика. Для моделювання структурної схеми системи тягового електропостачання, режимів тягового електроспоживання та поїзної ситуації у зоні розподілу енергії рекуперації застосована методологія математико-алгоритмічного конструктивізму. Даний підхід передбачає розробку конструктивно-продукційних структур (КПС) з перетвореннями спеціалізації, інтерпретації, конкретизації та реалізації. Розробка КПС передбачає визначення розширюваного носія, сигнатури відносин і операцій та конструктивної аксіоматики. Найбільш складною та істотною частиною аксіоматики є множина сформованих правил підстановки, що визначають процес виведення відповідних конструкцій. Результати. Розроблено спеціалізовану і конкретизовану КПС, яка дозволяє врахувати всі можливості та особливості сучасного обладнання систем тягового електропостачання, ділянок тягової мережі й поїзної ситуації. Її особливість: семантичне наповнення термінального алфавіту образами електрообладнання, тягової мережі та споживачів електроенергії, що володіють відповідною атрибутикою. Наведений окремий випадок формування структурної схеми демонструє можливості КПС стосовно даної задачі. Наукова новизна. Авторами запропоновано новий підхід до вирішення задачі раціонального використання енергії рекуперації, який полягає в застосуванні методів та засобів штучних нейронних мереж, експертних систем нечіткої логіки й математико-алгоритмічного конструктивізму. У даній роботі представлені методи конструктивно-продукційного моделювання структури зони розподілу енергії рекуперації у системі тяги постійного струму. Практична значимість. Рішення задачі раціонального використання енергії рекуперації дозволяє значно економити енергоресурси, сприяти технічному переоснащенню залізничного транспорту України шляхом впровадження сучасних засобів і можливостей. Розроблені моделі можуть застосовуватися для вирішення й інших задач енергозбереження в різних системах електричного транспорту",Dnipro National University of Railway Transport named after Academician V. Lazaryan,КОНСТРУКТИВНЕ МОДЕЛЮВАННЯ ЗОНИ РОЗПОДІЛУ ЕНЕРГІЇ РЕКУПЕРАЦІЇ ТЯГИ ПОСТІЙНОГО СТРУМУ,,,,core
268448721,2016-12-01T08:00:00,"With foresight into the state of the wireless channel, a robot can make various optimization decisions with regards to routing packets, planning mobility paths, or switching between diverse radios. However, the process of predicting link quality (LQ) is nontrivial due to the streaming and dynamic nature of radio wave propagation, which is complicated by robot mobility. Due to robot movement, the wireless propagation environment can change considerably in terms of distance, obstacles, noise, and interference. Therefore, LQ must be learned and regularly updated while the robot is online. However, the existing fuzzy-based models for assessing LQ are non-adaptable due to the absence of any learning mechanism. To address this issue, we introduce a fuzzy-based prediction model designed for the efficient online and incremental learning of LQ. The unique approach uses fuzzy logic to infer LQ based on the collective output from a series of offset classifiers and their posterior probabilities. In essence, the proposed model leverages machine learning for extracting the underlying functional relationship between the input and output variables, but deeper inferences are made from the output of the learning algorithms using fuzzy logic. Wireless link data from a real-world robot network was used to compare the model with the traditional linear regression approach. The results show statistically significant improvements in three out of the six real-world indoor and outdoor environments where the robot operated. Additionally, the novel approach offers a number of other benefits, including the flexibility to use fuzzy logic for model tuning, as well as the ability to make implementation efficiencies in terms of parallelization and the conservation of labeling resources",USMA Digital Commons,A Fuzzy-Based Machine Learning Model for Robot Prediction of Link Quality,,,,core
217770316,2016-01-01T00:00:00,"International audienceSingapore's vision of a Smart Nation encompasses the development of effective and efficient means of transportation. The government's target is to leverage new technologies to create services for a demand-driven intelligent transportation model including personal vehicles, public transport, and taxis. Singapore's government is strongly encouraging and supporting research and development of technologies for autonomous vehicles in general and autonomous taxis in particular. The design and implementation of intelligent routing algorithms is one of the keys to the deployment of autonomous taxis. In this paper we demonstrate that a reinforcement learning algorithm of the Q-learning family, based on a customized exploration and exploitation strategy, is able to learn optimal actions for the routing autonomous taxis in a real scenario at the scale of the city of Singapore with pick-up and drop-off events for a fleet of one thousand taxis",'Association for Computing Machinery (ACM)',Routing an Autonomous Taxi with Reinforcement Learning,,10.1145/2983323.2983379,,core
103617735,08/01/2016,"Abstract—We present an approach to the generation of realistic synthetic workloads for use in benchmarking of (massively) multiplayer online gaming infrastructures. Existing techniques are either too simple to be realistic or are too specific to a particular network structure to be used for comparing different networks with each other. Desirable properties of a workload are reproducibility, realism and scalability to any number of players. We achieve this by simulating a gaming session with AI players that are based on behavior trees. The requirements for the AI as well as its parameters are derived from a real gaming session with 16 players. We implemented the evaluation platform including the prototype game Planet PI4. A novel metric is used to measure the similarity between real and synthetic traces with respect to neighborhood characteristics. In our experiments, we compare real trace files, workload generated by two mobility models and two versions of our AI player. We found that our AI players recreate the real workload characteristics more accurately than the mobility models. I",,†Databases and Distributed Systems,,,,core
299434479,2015,"Passenger flow modeling and station dwelling time estimation are significant elements for railway mass transit planning, but system operators usually have limited information to model the passenger flow. In this paper, an artificial-intelligence technique known as fuzzy logic is applied for the estimation of the elements of the origin-destination matrix and the dwelling time of stations in a railway transport system. The fuzzy inference engine used in the algorithm is based in the principle of maximum entropy. The approach considers passengers’ preferences to assign a level of congestion in each car of the train in function of the properties of the station platforms. This approach is implemented to estimate the passenger flow and dwelling times of the recently opened Line 1 of the Panama Metro. The dwelling times obtained from the simulation are compared to real measurements to validate the approach.The authors of this paper want to express their gratitude to the National Secretary of Science and Technology (SENACYT) of the Government of the Republic of Panama for funding this study through the R & D project (MDEPRB09-001). Additionally, they want to thank the support received from
Technological University of Panama (UTP), the University of Granada, the Fundación Carolina and the Secretaría del Metro de Panamá (SMP)",MDPI,A Fuzzy Logic-Based Approach for Estimation of Dwelling Times of Panama Metro Stations,,10.3390/e17052688,"[{'title': None, 'identifiers': ['1099-4300', 'issn:1099-4300']}]",core
301274186,2017-01-01T00:00:00,"Road safety applications envisaged for vehicular ad hoc networks (VANETs) depend largely on the exchange of messages to deliver information to concerned vehicles. Safety applications as well as inherent VANET characteristics make data dissemination an essential service and a challenging task. We are developing a decentralized efficient solution for broadcast data dissemination through two game-theoretical mechanisms. Besides, VANETs can also include autonomous vehicles (AVs). AVs might represent a revolutionary new paradigm that can be a reality in our cities in the next few years. AVs do not need a driver to work; instead, they should copy a proper human behavior to adapt the driving according to the current circumstances, such as speed limit, pedestrian crossing street or wheather conditions. We will develop an AV software module including artificial intelligence (AI) techniques so that AVs can interact with the dynamic scenario throughout time. Finally, we also will include electrical vehicles (EV) in the VANET, so that special services such as finding and reserving an EV charging station place will be welcome. In addition, we are developing a multimetric geographic routing protocol for VANETs to transmit H.265 video (traffic accident, traffic state, commercial….) over VANETsPeer Reviewe",'Universitat Politecnica de Valencia',Multimedia communications in vehicular adhoc networks for several applications in the smart cities,,10.4995/JITEL2017.2017.6584,,core
141489661,2017-08-01T00:00:00,"Carbon fiber reinforced plastic (CFRP) parts for airplane components can be so huge that a single industrial robot is no longer able to handle them, and cooperating robots are required. Manual programming of cooperating robots is difficult, but with large numbers of different sized and shaped cut-pieces, it is almost impossible. This paper presents an automated production system consisting of a camera for the precise detection of the position of each cut-piece and a collision-free path planner which can dynamically react to different positions for the transfer motions. The path is planned for multiple robots adhering to motion constrains, such as the requirement that the textile cut-piece must form a catenary which can change during transport. Different existing path planning algorithms are evaluated and compared. Additionally a technique based on machine learning has been implemented which correctly resolves redundancy for a linear axis during planning. Finally, all components are tested on a real robot system in industrial scale",,Full Automatic Path Planning of Cooperating Robots in Industrial Applications,,10.1109/coase.2017.8256157,,core
43097967,2016-03-31T08:38:01,"This Presentation is brought to you for free and open access by the City College of New York at CUNY Academic Works. It has been accepted for
inclusion in International Conference on Hydroinformatics by an authorized administrator of CUNY Academic Works. For more information, please
contact AcademicWorks@cuny.edu.11
th International Conference on Hydroinformatics
HIC 2014, New York City, USAModelling rainfall-runoff processes enables hydrologists to plan their response to flooding events. Urban drainage catchment modelling requires rainfall-runoff models as a prerequisite. In the UK, one of the main software tools used for drainage modelling is InfoWorks CS, based on relatively simple methods which are relatively robust in predicting runoff. This paper presents an alternative approach to modelling runoff that will allow for the complex inter-relation of runoff that occurs from impermeable areas, permeable areas, local surface storage and variation in rainfall induced infiltration. Apart from the uncertainties associated with the measurement of connected surfaces to the drainage system, the physical processes involved in runoff are nonlinear, making artificial neural networks (ANNs) an ideal candidate for modelling them. ANNs have been used for runoff prediction in natural catchments, and recently on a study for predicting the performance of urban drainage systems. This study seeks to determine an input set that predicts sewerage flow in urban catchments where the runoff is dominated by infiltration, a major issue for the water industry. A framework is proposed in which an ANN is trained by an evolutionary algorithm, which optimises ANN weights; results are assessed using the Nash-Sutcliffe Efficiency Coefficient. The model is demonstrated on a real-world case study site for which rainfall, flow, air temperature and groundwater levels in three boreholes have been measured. Various combinations of these data are used as model inputs, examining a mixture of daily and sub-daily timesteps. The best predictions are generated from daily linearly combined antecedent rainfall and air temperature, although sub-daily information improves the worst-case performance of the model. Although infiltration is affected by groundwater levels, incorporating groundwater into the model does not improve predictions. The proposed ANN model is capable of producing acceptable predictions, thus avoiding many of the uncertainties involved in traditional infiltration modelling",City University of New York (CUNY): CUNY Academic Works,An artificial neural network-based rainfall runoff model for improved drainage network modelling,https://core.ac.uk/download/43097967.pdf,,,core
74210977,2016-01-01T00:00:00,"The work on Autonomic Road Transport Support (ARTS) presented here aims at

meeting the challenge of engineering autonomic behavior in Intelligent Transportation

Systems (ITS) by fusing research from the disciplines of traffic engineering

and autonomic computing. Ideas and techniques from leading edge artificial intelligence

research have been adapted for ITS over the last  years. Examples include

adaptive control embedded in real time traffic control systems, heuristic algorithms

(e.g. in SAT-NAV systems), image processing and computer vision (e.g. in automated

surveillance interpretation). Autonomic computing which is inspired from the

biological example of the body’s autonomic nervous system is a more recent development.

It allows for a more efficient management of heterogeneous distributed

computing systems. In the area of computing, autonomic systems are endowed

with a number of properties that are generally referred to as self-X properties,

including self-configuration, self-healing, self-optimization, self-protection and more

generally self-management. Some isolated examples of autonomic properties such

as self-adaptation have found their way into ITS technology and have already proved

beneficial. This edited volume provides a comprehensive introduction to Autonomic

Road Transport Support (ARTS) and describes the development of ARTS systems. It

starts out with the visions, opportunities and challenges, then presents the foundations

of ARTS and the platforms and methods used and it closes with experiences

from real-world applications and prototypes of emerging applications. This makes

it suitable for researchers and practitioners in the fields of autonomic computing,

traffic and transport management and engineering, AI, and software engineering.

Graduate students will benefit from state-of-the-art description, the study of novel

methods and the case studies provided",'Springer Fachmedien Wiesbaden GmbH',Autonomic Road Transport Support Systems,https://core.ac.uk/download/74210977.pdf,,,core
296771721,2016-10-20T00:00:00,"The Liane River is a small costal river, famous for its floods, which can affect the city of Boulogne-sur-Mer. Due to the complexity of land cover and hydrologic processes, a black-box non-linear modelling was chosen using neural networks. The multilayer perceptron model, known for its property of universal approximation is thus chosen. Four models were designed, each one for one forecasting horizon using rainfall forecasts: 24h, 12h, 6h, 3h. The desired output of the model is original: it represents the maximal value of the water level respectively 24h, 12h, 6h, 3h ahead. Working with best forecasts of rain (the observed ones during the event in the past), on the major flood of the database in test set, the model provides excellent forecasts. Nash criteria calculated for the four lead times are 0.98 (3h), 0.97 (6h), 0.91 (12h), 0.89 (24h). Designed models were thus estimated as efficient enough to be implemented in a specific tool devoted to real time operational use. The software tool is described hereafter: designed in Java, it presents a friendly interface allowing applying various scenarios of future rainfalls, and a graphical visualization of the predicted maximum water levels and their associated real time observed values",'EDP Sciences',Neural networks-based operational prototype for flash flood forecasting: application to Liane flash floods (France),,10.1051/e3sconf/20160718025,,core
354416839,2017-01-01T00:00:00,"2017 Fall.Includes bibliographical references.Rainfall estimation based on satellite measurements has proven to be very useful for various applications. A number of precipitation products at multiple time and space scales have been developed based on satellite observations. For example, the National Oceanic and Atmospheric Administration (NOAA) Climate Prediction Center has developed a morphing technique (i.e., CMORPH) to produce global precipitation products by combining existing space-based observations and retrievals. The CMORPH products are derived using infrared (IR) brightness temperature information observed by geostationary satellites and passive microwave-(PMW) based precipitation retrievals from low earth orbit satellites. Although space-based precipitation products provide an excellent tool for regional, local, and global hydrologic and climate studies as well as improved situational awareness for operational forecasts, their accuracy is limited due to restrictions of spatial and temporal sampling and the applied parametric retrieval algorithms, particularly for light precipitation or extreme events such as heavy rain. In contrast, ground-based radar is an excellent tool for quantitative precipitation estimation (QPE) at finer space-time scales compared to satellites. This is especially true after the implementation of dual-polarization upgrades and further enhancement by urban scale X-band radar networks. As a result, ground radars are often critical for local scale rainfall estimation and for enabling forecasters to issue severe weather watches and warnings. Ground-based radars are also used for validation of various space measurements and products. In this study, a new S-band dual-polarization radar rainfall algorithm (DROPS2.0) is developed that can be applied to the National Weather Service (NWS) operational Weather Surveillance Radar-1988 Doppler (WSR-88DP) network. In addition, a real-time high-resolution QPE system is developed for the Engineering Research Center for Collaborative Adaptive Sensing of the Atmosphere (CASA) Dallas-Fort Worth (DFW) dense radar network, which is deployed for urban hydrometeorological applications via high-resolution observations of the lower atmosphere. The CASA/DFW QPE system is based on the combination of a standard WSR-88DP (i.e., KFWS radar) and a high-resolution dual-polarization X-band radar network. The specific radar rainfall methodologies at Sand X-band frequencies, as well as the fusion methodology merging radar observations at different temporal resolutions are investigated. Comparisons between rainfall products from the DFW radar network and rainfall measurements from rain gauges are conducted for a large number of precipitation events over several years of operation, demonstrating the excellent performance of this urban QPE system. The real-time DFW QPE products are extensively used for flood warning operations and hydrological modelling. The high-resolution DFW QPE products also serve as a reliable dataset for validation of Global Precipitation Measurement (GPM) satellite precipitation products. This study also introduces a machine learning-based data fusion system termed deep multi-layer perceptron (DMLP) to improve satellite-based precipitation estimation through incorporating ground radar-derived rainfall products. In particular, the CMORPH technique is applied first to derive combined PMW-based rainfall retrievals and IR data from multiple satellites. The combined PMW and IR data then serve as input to the proposed DMLP model. The high-quality rainfall products from ground radars are used as targets to train the DMLP model. In this dissertation, the prototype architecture of the DMLP model is detailed. The urban scale application over the DFW metroplex is presented. The DMLP-based rainfall products are evaluated using currently operational CMORPH products and surface rainfall measurements from gauge networks",Colorado State University. Libraries,"Radar and satellite observations of precipitation: space time variability, cross-validation, and fusion",https://core.ac.uk/download/354416839.pdf,,,core
355104730,2015-07-22T00:00:00,"This paper investigates the development of an urban crowd simulation for the purposes of psychophysical experimentation. Whilst artificial intelligence (AI) is advancing to produce more concise and interesting crowd behaviours, the number or sophistication of the algorithms implemented within a system does not necessarily guarantee its perceptual realism. Human perception is highly subjective and does not always conform to the reality of the situation. Therefore it is important to consider this aspect when dealing with AI implementations within a crowd system aimed at humans. In this research an initial two alternative forced choice (2AFC) with constant stimuli psychophysical experiment is presented. The purpose of the experiment is to assess whether human participants perceive crowd behaviour with a social forces model to be more realistic. Results from the experiment suggest that participants do consider crowd behaviour with social forces to be more realistic. This research could inform the development of crowd-based systems, especially those that consider viewer perception to be important, such as for example video games and other media",'Institute of Electrical and Electronics Engineers (IEEE)',Perceived Realism of Crowd Behaviour with Social Forces,https://core.ac.uk/download/355104730.pdf,10.1109/iV.2015.88,"[{'title': None, 'identifiers': ['issn:1550-6037', '1550-6037', 'issn:2375-0138', '2375-0138']}]",core
188017671,2017-01-01T00:00:00,"Chatbots as a new information, communication and transaction channel enable businesses to reach their target audience through messenger apps like Facebook, WhatsApp or WeChat. Compared to traditional chats, bots are not handled by human persons, but software is leading through conversations. Latest chatbots developments in customer services and sales are remarkable. However, in the field of public transport, little research has been published on chatbots so far. With chatbots, train customers find out timetables, buy tickets and have a personal, digital travel advisor providing real-time and context-relevant information about trips. Chatbots collect and provide different data about users and their journey in public transportation systems. They include travel, product, service and content preferences, usage patterns, demographic and location-based data. Chatbots have different advantages and high potential for both companies and mobile users. They enable new user touch points, reduce service, sales and support costs, one-to-one marketing, new data collections and deep learning. Using chatbots, smartphone users can reach a company anytime and anywhere. The questioned users of a chatbot prototype are remarkably open to new mobile services and they quickly adapt to this innovative technology",'IADIS - International Association for the Development of the Information Society',Communicating and transacting with chatbots : insights from public transport,,,,core
216856349,2017-08-01T07:00:00,"Web image analysis has witnessed an AI renaissance. The ILSVRC benchmark has been instrumental in providing a corpus and standardized evaluation. The NVIDIA AI City Challenge is envisioned to provide similar impetus to the analysis of image and video data that helps make cities smarter and safer. In its first year, this Challenge has focused on traffic video data. While millions of traffic video cameras around the world capture data, albeit low-quality, very little automated analysis and value creation results. Lack of labeled data, and trained models that can be deployed at the edge of the city fabric, ensure that most traffic video data goes through little or no automated analysis. Real-time and batch analysis of this data can provide vital breakthroughs in real-time traffic management as well as pedestrian safety. The NVIDIA AI City Challenge brought together 29 teams from universities in 4 continents to collaboratively annotate a 125 hour data set and then compete on detection, localization and classification tasks as well as traffic and safety application analytics tasks. The result is the largest high quality annotated data set, a set of models trained using NVIDIA AI City Edge to Cloud platform and ready to be deployed at the edge solving traffic and safety problems for cities worldwide",SJSU ScholarWorks,The NVIDIA AI City Challenge,,,,core
211489448,2017-01-01T00:00:00,"Specifying and measuring the properties that a system provides, plays an important role for risk analysis during the development and management processes. Large organizations, like NASA and the FORD motor company, apply formal methodologies to guarantee that the developed systems fulfil the design requirements and accomplish the desired mission critical goals.The effectiveness of cyber-attacks raises security as a main system concern. Privacy also becomes important, as high volumes of personal data are processed by modern systems. Still, simple establishment of security and privacy defence mechanisms do not guarantee protection. The dependability of the solution must be also verified. Safety management is crucial, as any incident can lead to potential damage or even personal injury.Tackling the overall security, privacy, and dependability (SPD) calculation in a practical and systematic manner is difficult. The problem hardens when we deal with a composed system. In the era of pervasive and ubiquitous computing several systems are dynamically composed, with high volumes of heterogeneous embedded and mobile devices exchanging information.This thesis examines SPD and safety-related issues on Internet-of-Things (IoT) in corporation with Artificial Intelligence (AI). Formal methods are applied for system composition and SPD validation on evaluating systems. Then, the AI process can manage the system in real-time to protect the system itself and the users of the ambient environment. For example, a smart campus setting can assist living conditions during normal operation and counter cyber-attacks. In case of emergency, like fire or earthquake, the AI manages the surrounding smart equipment to assist the timely and safe evacuation of all evacuees. In other smart city scenarios, the system can detect physical tampering of critical railway infrastructure or car accidents and inform the involving authorities to take actions. Information regarding the incident and the passengers’ health condition are distributed. The goal is to achieve fast response with adequate rescue means.The main achievements of this work include a formal framework which describes the SPD properties of a composed system and its sub-components, and how these features are affected by changes in the state-architecture. The implementation can be used for composition verification, security validation, comparison of different system settings, and evaluation of the impact of a change in a system. Moreover, the proposed framework can be used as a middleware for real-time monitoring and management of a system. Technically, the framework is modelled as the reasoning process of JADE agents and ported in the OSGi middleware platform. The network layer is further fortified by a novel trust-based secure routing protocol that provides enhanced security and performance, surpassing the current solutions. Lightweight cryptographic primitives are implemented at the device end to ensure authentication, confidentiality, and integrity.The deployment is applied and demonstrated in five main scenarios:•A smart home application to evaluate and manage embedded devices with ambient intelligence capabilities for assisting living.•An IoT system for precision agriculture deployments with wireless sensor networks (WSNs) for monitoring olive groves or forests while detecting and countering cyber-attacks.•A smart campus setting for disaster mitigation planning that manages the surrounding smart equipment and assist the timely and safe evacuation of all evacuees in case of emergency, like fire.•A railway cyber-physical system (CPS) for smart transportation with in-carriage and on-route WSNs that continuously monitor the critical infrastructure for safety-related incidents while providing protection against cyber-attacks.•A smart vehicle fleet management where the system monitors the underlying vehicles at runtime, protecting against cyber-attacks. The system can also detect car accidents and inform the involving authorities to take actions.Ο προσδιορισμός και η μέτρηση των ιδιοτήτων που παρέχει ένα σύστημα διαδραματίζει σημαντικό ρόλο στην ανάλυση κινδύνου κατά τη διάρκεια των διαδικασιών ανάπτυξης και διαχείρισης. Μεγάλοι οργανισμοί, όπως η NASA και η αυτοκινητοβιομηχανία FORD, εφαρμόζουν τυπικές μεθόδους για να διασφαλίσουν ότι τα παραγόμενα συστήματα πληρούν τις απαιτήσεις σχεδίασης και επιτυγχάνουν τους επιθυμητούς στόχους που είναι κρίσιμης σημασίας για την επίτευξη της αποστολή.Η αποτελεσματικότητα των επιθέσεων στον κυβερνοχώρο αναδεικνύει την ασφάλεια ως κύρια ιδιότητα ενός συστήματος. Η ιδιωτικότητα γίνεται επίσης σημαντική, καθώς τα σύγχρονα συστήματα επεξεργάζονται μεγάλους όγκους προσωπικών δεδομένων. Ωστόσο, η απλή δημιουργία μηχανισμών ασφάλειας και προστασίας της ιδιωτικής ζωής δεν εγγυάται προστασία. Πρέπει επίσης να επαληθευτεί η αξιοπιστία της λύσης. Η διαχείριση της φυσικής ασφάλειας είναι ζωτικής σημασίας, καθώς κάθε περιστατικό μπορεί να οδηγήσει σε πιθανή ζημιά ή ακόμα και σε τραυματισμό.Η αντιμετώπιση του υπολογισμού της συνολικής ασφάλειας, της ιδιωτικότητας, και της αξιοπιστίας (ΑΙΑ) με πρακτικό και συστηματικό τρόπο είναι δύσκολη. Το πρόβλημα δυσκολεύει όταν αντιμετωπίζουμε ένα σύνθετο σύστημα. Στην εποχή της διάχυτης και πανταχού παρουσίας υπολογιστικής, πολλά συστήματα είναι δυναμικά συντεθειμένα, με μεγάλους όγκους ετερογενών ενσωματωμένων και κινητών συσκευών να ανταλλάσσουν πληροφορίες.Αυτή η εργασία εξετάζει θέματα ΑΙΑ και φυσικής ασφάλειας που σχετίζονται με το Διαδίκτυο των Πραγμάτων (ΔτΠ) σε συνεργασία με την Τεχνητή Νοημοσύνη (ΤΝ). Τυπικές μέθοδοι εφαρμόζονται για τη σύνθεση του συστήματος και την επικύρωση του επιπέδου ΑΙΑ στα συστήματα αξιολόγησης. Στη συνέχεια, η διαδικασία ΤΝ μπορεί να διαχειριστεί το σύστημα σε πραγματικό χρόνο για να προστατεύσει το ίδιο το σύστημα και τους χρήστες του περιβάλλοντα χώρου. Για παράδειγμα, μια εφαρμογή έξυπνης πανεπιστημιούπολης μπορεί να βοηθήσει τις συνθήκες διαβίωσης κατά τη διάρκεια της κανονικής λειτουργίας και να καταπολεμήσει τις κυβερνο-επιθέσεις. Σε περίπτωση έκτακτης ανάγκης, όπως η πυρκαγιά ή ο σεισμός, η ΤΝ διαχειρίζεται τον περιβάλλοντα έξυπνο εξοπλισμό για να βοηθήσει στην έγκαιρη και ασφαλή εκκένωση όλων των ανθρώπων. Σε άλλα σενάρια έξυπνης πόλης, το σύστημα μπορεί να ανιχνεύσει την φυσική παραβίαση της κρίσιμης σιδηροδρομικής υποδομής ή τροχαία ατυχήματα και να ενημερώσει τις εμπλεκόμενες αρχές για τη λήψη μέτρων. Πληροφορίες σχετικά με το περιστατικό και την κατάσταση της υγείας των επιβατών διανέμονται στις αρχές. Ο στόχος είναι να επιτευχθεί γρήγορη ανταπόκριση με τα κατάλληλα μέσα διάσωσης.Τα κύρια επιτεύγματα αυτής της μελέτης περιλαμβάνουν ένα τυπικό πλαίσιο που περιγράφει τις ιδιότητες ΑΙΑ ενός σύνθετου συστήματος και των υποσυστημάτων του και τον τρόπο με τον οποίο αυτά τα χαρακτηριστικά επηρεάζονται από τις αλλαγές στην αρχιτεκτονική δομή. Η εφαρμογή μπορεί να χρησιμοποιηθεί για την επαλήθευση της σύνθεσης, την επικύρωση της ασφάλειας, τη σύγκριση των διαφορετικών ρυθμίσεων του συστήματος και την αξιολόγηση των επιπτώσεων μιας αλλαγής σε ένα σύστημα. Επιπλέον, το προτεινόμενο πλαίσιο μπορεί να χρησιμοποιηθεί ως ενδιάμεσο λογισμικό για την παρακολούθηση και διαχείριση σε πραγματικό χρόνο ενός συστήματος. Από τεχνικής πλευράς, το πλαίσιο διαμορφώνεται ως διαδικασία συλλογιστικής των πρακτόρων JADE και μεταφέρεται στην πλατφόρμα middleware OSGi. Το επίπεδο δικτύου προστατεύεται επιπλέον από ένα καινοτόμο ασφαλές πρωτόκολλο δρομολόγησης βασισμένο σε εμπιστευτικότητα το οποίο  παρέχει ενισχυμένη ασφάλεια και επίδοση, ξεπερνώντας τις  σημερινές λύσεις. Ελαφριά δομικά στοιχεία κρυπτογραφίας υλοποιήθηκαν σε επίπεδο συσκευής ώστε να εξασφαλιστούν η αυθεντικότητα, η εμπιστευτικότητα, και η ακεραιότητα.Η ανάπτυξη εφαρμόζεται και επιδεικνύεται σε πέντε κύρια σενάρια:•Μια εφαρμογή για έξυπνο σπίτι για την αξιολόγηση και τη διαχείριση ενσωματωμένων συσκευών με δυνατότητες περιβαλλοντικής ευφυΐας για την παροχή βοήθειας στις συνθήκες διαβίωσης.•Ένα σύστημα ΔτΠ για την ανάπτυξη γεωργικών μηχανισμών ακριβείας με ασύρματα δίκτυα αισθητήρων (ΑΔΑ) για την παρακολούθηση των ελαιώνων ή των δασών καθώς και τον εντοπισμό και την αντιμετώπιση των κυβερνο-επιθέσεων.•Μια εφαρμογή έξυπνης πανεπιστημιούπολης για τον σχεδιασμό μετριασμού καταστροφών που διαχειρίζεται τον περιβάλλοντα έξυπνο εξοπλισμό και βοηθά στην έγκαιρη και ασφαλή εκκένωση όλων των εκτοπισμένων σε περίπτωση έκτακτης ανάγκης, όπως η πυρκαγιά.•Ένα κυβερνο-φυσικό σύστημα (ΚΦΣ) σιδηροδρόμων για έξυπνες μεταφορές με ΑΔΑ στα μέσα μεταφοράς και κατά μήκος της διαδρομής που παρακολουθούν συνεχώς την υποδομή για συμβάντα που σχετίζονται με την φυσική ασφάλεια, παρέχοντας ταυτόχρονα προστασία από επιθέσεις στον κυβερνοχώρο.•Μια διαχείριση στόλου έξυπνων οχημάτων, όπου το σύστημα παρακολουθεί τα υποκείμενα οχήματα κατά το χρόνο εκτέλεσης, προστατεύοντας από επιθέσεις στον κυβερνοχώρο. Το σύστημα μπορεί επίσης να ανιχνεύσει τροχαία ατυχήματα και να ενημερώσει τις εμπλεκόμενες αρχές για την ανάληψη ενεργειών",'National Documentation Centre (EKT)',"SPD-Safe: διαχείριση ασφάλειας, ιδιωτικότητας και αξιοπιστίας σε ενσωματωμένα συστήματα που χρησιμοποιούνται σε εφαρμογές υψηλής ασφάλειας",,10.12681/eadd/42140,,core
149369523,2017-10-27T17:38:23Z,"<p><i>ISG15</i><sup>+/+</sup> or <i>ISG15</i><sup>-/-</sup> BMDM pretreated or not with IFN (500 units/ml, 16 hours) were infected (1 PFU/cell) with VACV at the times indicated. <b>(A-B)</b> Basal and maximal OCR rates were monitored using the Seahorse Biosciences extracellular flux analyzer. Results represent the mean ± the standard deviation of 4 biological replicates. <b>(C)</b> Mitochondrial ATP production was measured as indicated in materials and methods. <b>(D)</b> ECAR rates were monitored using the Seahorse Biosciences extracellular flux analyzer. <b>(E)</b> Variation of the mtDNA levels (MitoF) relative to nuclear DNA (B2) after IFN treatment in <i>ISG15</i><sup><i>+/+</i></sup> or <i>ISG15</i><sup><i>-/-</i></sup> BMDM was quantified by real time PCR. For each condition, the data represent the ratio of mitochondrial DNA in untreated <i>vs</i> those after IFN treatment. Each point represents 3 independent samples measured in duplicate <b>(F)</b> Citrate synthase activity after IFN treatment in <i>ISG15</i><sup><i>+/+</i></sup> or <i>ISG15</i><sup><i>-/-</i></sup>. In total cell extracts from <i>ISG15</i><sup><i>+/+</i></sup> or <i>ISG15</i><sup><i>-/-</i></sup> the citrate synthase activity was measured by spectrophotometric procedure. Each point represents 3 independent samples measured in duplicate. <b>(G)</b> ROS production was analyzed by fluorescence microscopy using MitoSOX Red in Mock or VACV-infected <i>ISG15</i><sup>+/+</sup> or <i>ISG15</i><sup>-/-</sup> IFN-treated or not BMDM. At the indicated times post-infection, relative ROS production was quantified using ImageJ software and represented as the relative fluorescence value in relation to that in non-infected cells. Significance was tested using a two-tailed t test assuming non-equal variance. In all cases p < 0.01. <b>(H)</b> Analysis of the electronic transport chain (ETC) complex. Isolated mitochondria from <i>ISG15</i><sup>+/+</sup> or <i>ISG15</i><sup>-/-</sup> BMDM pretreated or not with IFN (500 units/ml, 16 hours) were subjected to a blue native gel and the presence of the chain complex were analyzed using the following specific antibodies: anti-CORE2 for the complex III, anti- NDUFA9, for complex I and ant-SDHA for complex II.</p",,Characterization of the energy metabolism of VACV-infected <i>ISG15</i><sup>+/+</sup> or <i>ISG15</i><sup>-/-</sup> BMDM.,,10.1371/journal.ppat.1006651.g003,,core
85223885,2017-08-02T19:49:19,"Geothermal energy is becoming an important clean energy source, however, the stimulation of a reservoir for an Enhanced Geothermal System (EGS) is associated with seismic risk due to induced seismicity. Seismicity occurring due to the water injection at depth have to be well recorded and monitored. To mitigate the seismic risk of a damaging event, an appropriate alarm system needs to be in place for each individual experiment. In recent experiments, the so-called traffic-light alarm system, based on public response, local magnitude and peak ground velocity, was used. We aim to improve the pre-defined alarm system by introducing a probability-based approach; we retrospectively model the ongoing seismicity in real time with multiple statistical forecast models and then translate the forecast to seismic hazard in terms of probabilities of exceeding a ground motion intensity level. One class of models accounts for the water injection rate, the main parameter that can be controlled by the operators during an experiment. By translating the models into time-varying probabilities of exceeding various intensity levels, we provide tools which are well understood by the decision makers and can be used to determine thresholds non-exceedance during a reservoir stimulation; this, however, remains an entrepreneurial or political decision of the responsible project coordinators. We introduce forecast models based on the data set of an EGS experiment in the city of Basel. Between 2006 December 2 and 8, approximately 11 500 m3 of water was injected into a 5-km-deep well at high pressures. A six-sensor borehole array, was installed by the company Geothermal Explorers Limited (GEL) at depths between 300 and 2700 m around the well to monitor the induced seismicity. The network recorded approximately 11 200 events during the injection phase, more than 3500 of which were located. With the traffic-light system, actions where implemented after an ML 2.7 event, the water injection was reduced and then stopped after another ML 2.5 event. A few hours later, an earthquake with ML 3.4, felt within the city, occurred, which led to bleed-off of the well. A risk study was later issued with the outcome that the experiment could not be resumed. We analyse the statistical features of the sequence and show that the sequence is well modelled with the Omori-Utsu law following the termination of water injection. Based on this model, the sequence will last 31+29/−14 years to reach the background level. We introduce statistical models based on Reasenberg and Jones and Epidemic Type Aftershock Sequence (ETAS) models, commonly used to model aftershock sequences. We compare and test different model setups to simulate the sequences, varying the number of fixed and free parameters. For one class of the ETAS models, we account for the flow rate at the injection borehole. We test the models against the observed data with standard likelihood tests and find the ETAS model accounting for the on flow rate to perform best. Such a model may in future serve as a valuable tool for designing probabilistic alarm systems for EGS experiment",,Statistical analysis of the induced Basel 2006 earthquake sequence: introducing a probability-based monitoring approach for Enhanced Geothermal Systems,https://core.ac.uk/download/85223885.pdf,,,core
156902162,2015-01-01T00:00:00,"Advances in technology in recent years have changed the learning
behaviours of learners and reshaped teaching methods and learning
environments. This paper overviews a foundational framework and provides
models for planning and implementing smart learning environments.

Gartner’s 2015 Hype Cycle for Emerging Technologies identifies the computing
innovations such as Internet of Things, Advanced Analytics, Machine Learning,
Wearables, etc., that organisations should monitor. Learners and students,
being the future drivers of these industries, are the main human resource to
fulfil the vacancies of these work forces. Constant improvements and re-evaluation
of the curriculum has to be done regularly to keep the learners up-to-date in
meeting the requirements of these industries and corporations.

Universities benefit from these thinking-outside-the-box practices by equipping
students with work force experience that involves more hands-on tasks with
real-life infrastructures. The introduction is focused on analysis of emerging
industries and new types of jobs that require future

personnel to be well equipped to meet the expansion requirements of these
industries and keep up with their development needs. Section 2 looks at
the future Internet domain landscape that comprises a great diversity of
technology related topics involved in the implementation of Smart Learning Environments.

The purpose of section 3 is to overview a foundational framework and major
considerations for the planning and implementation of smart learning environments,
behind which is the convergence of advances and developments in social constructivism,
psychology, and technology. Section 4 introduces the smart learning models which
are developed to reflect the dynamic knowledge conversion processes in technology
enabled smart learning environments.

The last section presents a case study of a learning scenario entitled
“Monitoring the environmental parameters in a Smart City” as an illustration of
experimental learning on Internet of Things, which proofs the power of the FORGE
(Forging Online Education through FIRE) FP7 project methodology and infrastructure
for building remote labs and delivering them to students.

ACM Computing Classification System (1998): K.3.2",Institute of Mathematics and Informatics Bulgarian Academy of Sciences,On Learning in a Smart City Environment,,,"[{'title': None, 'identifiers': ['issn:1312-6555', '1312-6555']}]",core
215412336,2017-08-01T07:00:00,"Web image analysis has witnessed an AI renaissance. The ILSVRC benchmark has been instrumental in providing a corpus and standardized evaluation. The NVIDIA AI City Challenge is envisioned to provide similar impetus to the analysis of image and video data that helps make cities smarter and safer. In its first year, this Challenge has focused on traffic video data. While millions of traffic video cameras around the world capture data, albeit low-quality, very little automated analysis and value creation results. Lack of labeled data, and trained models that can be deployed at the edge of the city fabric, ensure that most traffic video data goes through little or no automated analysis. Real-time and batch analysis of this data can provide vital breakthroughs in real-time traffic management as well as pedestrian safety. The NVIDIA AI City Challenge brought together 29 teams from universities in 4 continents to collaboratively annotate a 125 hour data set and then compete on detection, localization and classification tasks as well as traffic and safety application analytics tasks. The result is the largest high quality annotated data set, a set of models trained using NVIDIA AI City Edge to Cloud platform and ready to be deployed at the edge solving traffic and safety problems for cities worldwide",SJSU ScholarWorks,The NVIDIA AI City Challenge,,,,core
201967742,31/12/2016,"AbstractThe advances in Information Technologies have led to more complex road safety applications. These systems provide multiple possibilities for improving road transport. The integrated system that this paper presents deals with two aspects that have been identified as key topics: safety and efficiency. To this end, the development and implementation of an integrated advanced driver assistance system (ADAS) for rural and intercity environments is proposed. The system focuses mainly on single-carriageways roads, given the complexity of these environments compared to motorways and the high number of severe and fatal accidents on them. The proposed system is based on advanced perception techniques, vehicle automation and communications between vehicles (V2V) and with the infrastructure (V2I). Sensor fusion architecture based on computer vision and laser scanner technologies are developed. It allows real time detection and classification of obstacles, and the identification of potential risks. The driver receives this information and some warnings generated by the system. In case, he does not react in a proper way, the vehicle could perform autonomous actions (both on speed control or steering maneuvers) to improve safety and/or efficiency. Furthermore, a multimodal V2V and V2I communication system, based on GeoNetworking, facilitates the flow of information between vehicles and assists in the detection and information broadcasting processes. All this, combined with vehicle positioning, detailed digital maps and advanced map-matching algorithms, establish the decision algorithms of different ADAS systems.The applications developed include: adaptive cruise control with consumption optimization, overtaking assistance system in single-carriageways roads that takes into account appropriate speed evolution and identifies most suitable road stretches for the maneuver; assistance system in intersections with speed control during approximation maneuvers, and collision avoidance system with the possibility of evasive maneuvers. To this end, mathematical vehicle dynamics models have been used to ensure the stability, and propulsion system models are used to establish efficient patterns, Artificial Intelligence and simulation are used for experimentation and evaluation of algorithms to be implemented in the control unit. Finally, the system is designed to warn the driver if a risk is detected and, if necessary, to take control of the vehicle. The system has been implemented on a passenger car and has been tested in specific scenarios on a test track with satisfactory results",The Author(s). Published by Elsevier B.V.,Advanced Driver Assistance System for Road Environments to Improve Safety and Efficiency ,,10.1016/j.trpro.2016.05.240,,core
402035888,2016-01-01T00:00:00,"Tez Başlığı: (-)-Roemerin muamelesi altındaki Escherichia coli’de transkripsiyonel değişimler
1928’de Fleming tarafından ilk antibiyotik olan Penisilin’in keşfedilmesinden sonra, bakteriyel direnç mekanizmalarındaki hızlı artış sebebiyle yeni etki mekanizmalarına sahip antibakteriyellerin geliştirilmesi önemli bir süreç haline gelmiştir. Bu bağlamda, yeni antibakteriyel ajanların araştırılmasında doğal kaynak olan bitkiler dikkatleri üzerlerine çekmektedir. Bu doğrultuda, mevcut çalışmada, bitki kökenli bir alkaloit olan (-)-roemerin’in Escherichia coli TB1 hücreleri üzerindeki antibakteriyel etkisi transkriptom düzeyinde incelenmiştir. E. coli TB1 hücrelerindeki transkripsiyonel değişimler, 1 saat boyunca 1xMİK (Minimum inhibitör Konsantrasyonu) (100 mg/ml) (-
)-roemerin’e maruz bırakıldıktan sonra incelenmiştir. Karşılaştırma amacı ile ilaç ile muameleden hemen sonra (0. dakikada) ve muameleden bir saat sonra (60. dakikada) alınan ilaçlı (R0 ve R60) ve kontrol (D0 ve D60) hücre örneklerinden elde edilen toplam RNA kullanılmıştır. İzole edilen RNA örnekleri hem mikroarray için hem de kantitatif gerçek zamanlı polimeraz zincir reaksiyonu (PZR) ile validasyon için kullanılmıştır. 60 dakikalık ilaç muamelesi sonrasında bakterinin tepkisi transkiptom seviyesinde önemli değişiklikler olduğunu göstermiştir. İlacın etkisinin net olarak görüldüğü D60-R60 grubunda 213 gende (p değeri<0.05) bu değişimler gözlenmiştir. Protein kodlayan gen setlerinin işlevsel zenginleştirme analizleri Clusters of Orthologous Groups (COGs) veritabanı, The Database for Annotation, Visualization and Integrated Discovery (DAVID) biyoinformatik aracı ve zenginleştirme sonuçlarını moleküler yolaklar ve Gen Ontoloji (GO) terimleri olarak gösteren AmiGO 2 kullanılarak yapılmıştır. Hücrelerin (-
)-roemerin’e karşı verdiği ilk an tepkisi olarak çoklu antibiyotik direnç genlerindeki  artış gözlemlenmiştir. Bir saatlik muamelenin ardından ise, dış hücre zarı proteinlerindeki değişikliklere bağlı olarak karbonhidrat taşınımı ve enerji metabolizmasındaki azalmanın hücrelerde besin kıtlığına yol açtığı bulunmuştur. (-)- Roemerine muamelesi altında meydana gelen değişikliklerin çoğunluğunun ise flagella sentezinin durdurulması ve biyofilm oluşumundaki eksiklikler olarak bulunmuş ancak bunların hücrelerin kendilerini (-)-roemerin alkaloidine karşı korumasında yetersiz kaldığı tahmin edilmiştir.
ABSTRACT

Thesis Title: Transcriptional Changes in Escherichia coli Upon Treatment with (-)- Roemerine
The development of antibacterials with novel action mechanisms has become a challenging task due to the rapid increase in the bacterial resistance mechanisms since the introduction of the first antibiotic, penicillin, by Fleming in 1928. In this sense,plants have drawn attention as natural sources in the search for new antibacterials. Thus, in the current study, the antibacterial mechanism of the plant derived alkaloid (-)- roemerine against Escherichia coli TB1 cells were investigated on a transcriptomic level. Transcriptional changes in E. coli TB1 cells were investigated upon one hour treatment with 1xMIC (minimum inhibitory concentration) (100 mg/ml) (-)-roemerine. Total RNA extracts of cell samples taken just after treatment (0 min) and one hour after treatment (60 min) from drug treated (defined as R0 and R60) and control cells (defined  as D0 and D60) were comparatively analyzed. The RNA samples were used for both microarray analysis and quantitative Real-Time Polymerase Chain Reaction validation. The response to treatment after 60 min pointed out significant alterations on the transcriptome level. 213 of the altered genes (p-value<0.05) were found in the D60-R60 group, which explicitly represent the effect of treatment after 60 minutes. The functional enrichment analyses of the protein encoding gene sets were achieved through Clusters of Orthologous Groups (COGs) database, The Database for Annotation, Visualization and Integrated Discovery (DAVID) bioinformatics tool and AmiGO 2, which present  the enrichment results for molecular pathways and Gene Ontology (GO) terms. The topological analysis of the networks was performed via CytoHubba plugin of Cytoscape software. Clustering of functionally related genes via COGs database and DAVID enrichment tool pointed out significantly altered biological processes and molecular pathways. As an instant response to (-)-roemerine the multiple antibiotic resistance genes were up-regulated. Following one-hour treatment with (-)-roemerine, it was clear that the alterations in outer membrane proteins resulted in a decrease in the rate of carbohydrate transport and energy metabolisms leading to nutrient limitation. The majority of the alterations upon (-)-roemerine treatment resulted in the deficiency in flagellar motility and loss of ability for biofilm formation leading to insufficient protection",'Marmara Universitesi Ilahiyat Fakultesi Dergisi',Transcrıptıonal Changes In Escherıchıa Colı Upon Treatment Wıth (-)- Roemerıne,,,,core
148025514,2016-11-09T08:27:53,"In cities, urban places provide a socio-cultural habitat for people to counterbalance the daily grind of urban life, an environment away from home and work. Places provide an environment for people to communicate, share perspectives, and in the process form new social connections. Due to the active role of places to the social fabric of city life, it is important to understand how people perceive and experience places. One fundamental construct that relates place and experience is ambiance, i.e., the impressions we ubiquitously form when we go out. Young people are key actors of urban life, specially at night, and as such play an equal role in co-creating and appropriating the urban space. Understanding how places and their youth inhabitants interact at night is a relevant urban issue. Until recently, our ability to assess the visual and perceptual qualities of urban spaces and to study the dynamics surrounding youth experiences in those spaces have been limited partly due to the lack of quantitative data. However, the growth of computational methods and tools including sensor-rich mobile devices, social multimedia platforms, and crowdsourcing tools have opened ways to measure urban perception at scale, and to deepen our understanding of nightlife as experienced by young people. In this thesis, as a first contribution, we present the design, implementation and computational analysis of four mobile crowdsensing studies involving youth populations from various countries to understand and infer phenomena related to urban places and people. We gathered a variety of explicit and implicit crowdsourced data including mobile sensor data and logs, survey responses, and multimedia content (images and videos) from hundreds of crowdworkers and thousands of users of mobile social networks. Second, we showed how crowdsensed images can be used for the computational characterization and analysis of urban perception in indoor and outdoor places. For both place types, urban perception impressions were elicited for several physical and psychological constructs using online crowdsourcing. Using low-level and deep learning features extracted from images, we automatically inferred crowdsourced judgments of indoor ambiance with a maximum R2 of 0.53 and outdoor perception with a maximum R2 of 0.49. Third, we demonstrated the feasibility to collect rich contextual data to study the physical mobility, activities, ambiance context, and social patterns of youth nightlife behavior. Fourth, using supervised machine learning techniques, we automatically classified drinking behavior of young people in an urban, real nightlife setting. Using features extracted from mobile sensor data and application logs, we obtained an overall accuracy of 76.7%. While this thesis contributes towards understanding urban perception and youth nightlife patterns in specific contexts, our research also contributes towards the computational understanding of urban places at scale with high spatial and temporal resolution, using a combination of mobile crowdsensing, social media, machine learning, multimedia analysis, and online crowdsourcing","Lausanne, EPFL",Computational Analysis of Urban Places Using Mobile Crowdsensing,https://core.ac.uk/download/148025514.pdf,10.5075/epfl-thesis-7243,,core
228051070,2017-05-01T00:00:00,"International audienceIn this paper, the problem of proactive deployment of cache-enabled unmanned aerial vehicles (UAVs) for optimizing the quality-of-experience (QoE) of wireless devices in a cloud radio access network is studied. In the considered model, the network can leverage human-centric information, such as users' visited locations, requested contents, gender, job, and device type to predict the content request distribution, and mobility pattern of each user. Then, given these behavior predictions, the proposed approach seeks to find the user-UAV associations, the optimal UAVs' locations, and the contents to cache at UAVs. This problem is formulated as an optimization problem whose goal is to maximize the users' QoE while minimizing the transmit power used by the UAVs. To solve this problem, a novel algorithm based on the machine learning framework of conceptor-based echo state networks (ESNs) is proposed. Using ESNs, the network can effectively predict each user's content request distribution and its mobility pattern when limited information on the states of users and the network is available. Based on the predictions of the users' content request distribution and their mobility patterns, we derive the optimal locations of UAVs as well as the content to cache at UAVs. Simulation results using real pedestrian mobility patterns from BUPT and actual content transmission data from Youku show that the proposed algorithm can yield 33.3% and 59.6% gains, respectively, in terms of the average transmit power and the percentage of the users with satisfied QoE compared with a benchmark algorithm without caching and a benchmark solution without UAVs",'Institute of Electrical and Electronics Engineers (IEEE)',Caching in the Sky: Proactive Deployment of Cache-Enabled Unmanned Aerial Vehicles for Optimized Quality-of-Experience,,10.1109/JSAC.2017.2680898,,core
84145810,2016-09-21T00:00:00,"Recent work has attempted to deliver optimized distributed energy resource management, including the use of demand side management through smart homes. This aims to reduce power transmission losses, increase the generation share of renewable energy sources and create new markets through peak shaving and flexibility markets. Further, this leverages the development of product models at the device, building, and network level within the operational lifecycle stage, beyond the conventional role of BIM between design and construction stages. However, the management of heterogeneous software entities, incompatible data models and domain perspectives, across systems of systems of significant complexity, represent critical barriers to sustainable urban energy solutions and leads to a highly challenging problem space. The presented work describes a systemic approach based on the concept of holonic systems, which exemplify the role of autonomy, belonging, connectivity, diversity and emergence across entities. This reduces the decision complexity of the problem and facilitates the implementation of optimized solutions in real power systems in a scalable and robust manner. Further, the concept of a flexibility market is introduced, whereby smart appliance owners are able to sell load curtailment and deferment to a local aggregator, which interfaces between a small number of homes and a distribution system operator. Artificial intelligence is present at each of the entities in order to express constraints, trade energy and flexibility, and optimize the network management decisions within that entity’s scope. Specifically, this paper focuses on enabling interoperability between system entities such as smart homes, local load aggregators, and last mile network operators. This interoperability is achieved through ontological modelling of the domain, based on the existing standards of CIM, OpenADR, and energy@home. The produced ontology utilizes description logic to formalize the concepts, relationships and properties of the domain. A use case is presented of applying the ontology within a multi-agent system, which enables the optimization of day-ahead markets, load balancing, and stochastic renewable generation, and closely aligns with the holonic approach to deliver a holonic multi-agent system. The use case assumes a scenario in line with the emerging energy landscape of a district of domestic prosumers, with a high penetration of micro-generation, energy storage and electric vehicles. Initial results demonstrate interoperability between heterogeneous agents through ontological modelling based on an integration and extension of existing standards, which acts as a proof of concept for the approach",'Informa UK Limited',Semantic interoperability for holonic energy optimization of connected smart homes and distributed energy resources,,10.1201/9781315386904,,core
90623808,2016-09-01T00:00:00Z,"In recent years, the advancement of sensor technology has led to the generation of heterogeneous Internet-of-Things (IoT) data by smart cities. Thus, the development and deployment of various aspects of IoT-based applications are necessary to mine the potential value of data to the benefit of people and their lives. However, the variety, volume, heterogeneity, and real-time nature of data obtained from smart cities pose considerable challenges. In this paper, we propose a semantic framework that integrates the IoT with machine learning for smart cities. The proposed framework retrieves and models urban data for certain kinds of IoT applications based on semantic and machine-learning technologies. Moreover, we propose two case studies: pollution detection from vehicles and traffic pattern detection. The experimental results show that our system is scalable and capable of accommodating a large number of urban regions with different types of IoT applications",MDPI AG,Semantic Framework of Internet of Things for Smart Cities: Case Studies,,10.3390/s16091501,"[{'title': None, 'identifiers': ['issn:1424-8220', '1424-8220']}]",core
78395286,2016-01-31T00:00:00,"Background: Ethiopia is a country with 9 ethnically-based administrative regions and 2 city administrations, often cited, among other things, with high fertility rates and rapid population growth rate. Despite the country’s effort in their reduction, they still remain high, especially at regional-level. To this end, the study of fertility in Ethiopia, particularly on its regions, where fertility variation and its repercussion are at boiling point, is paramount important. An easy way of finding different characteristics of a fertility distribution is to build a suitable model of fertility pattern through different mathematical curves. ASFR is worthwhile in this regard. In general, the age-specific fertility pattern is said to have a typical shape common to all human populations through years though many countries some from Africa has already started showing a deviation from this classical bell shaped curve. Some of existing models are therefore inadequate to describe patterns of many of the African countries including Ethiopia. In order to describe this shape (ASF curve), a number of parametric and non-parametric functions have been exploited in the developed world though fitting these models to curves of Africa in general and that of Ethiopian in particular data has not been undertaken yet. To accurately model fertility

patterns in Ethiopia, a new mathematical model that is both easily used, and provides good fit for the data is required. Objective: The principal goals of this thesis are therefore fourfold: (1). to examine the pattern of ASFRs at country and regional level,in Ethiopia; (2). to propose a model that best captures various shapes of ASFRs at both country and regional level, and then compare the performance of the model with some existing ones; (3). to fit the proposed model using Hierarchical Bayesian techniques and show that this method is flexible enough for local estimates vis-´a-vis traditional formula, where the estimates might be very imprecise, due to low sample size; and (4). to compare the resulting estimates obtained with the non-hierarchical procedures, such as Bayesian and Maximum likelihood counterparts.

Methodology: In this study, we proposed a four parametric parametric model, Skew Normal model, to fit the fertility schedules, and showed that it is flexible enough in capturing fertility patterns shown at country level and most regions of Ethiopia. In order to determine the performance of this proposed model, we conducted a preliminary analysis along with ten other commonly used parametric and non-parametric models in demographic literature, namely: Quadratic Spline function, Cubic Splines, Coale-Trussell function, Beta, Gamma, Hadwiger distribution, Polynomial models, the Adjusted Error Model, Gompertz curve, Skew Normal, and Peristera & Kostaki Model. The criterion followed in fitting these models was Nonlinear Regression with nonlinear least squares (nls) estimation. We used Akaike Information Criterion (AIC) as model selecction criterion. For many demographers, however, estimating regional-specific ASFR model and the associated uncertainty introduced due those factors can be difficult, especially in a situation where we have extremely varying sample size among different regions. Recently, it has been proposed that Hierarchical procedures might provide more reliable parameter estimates than Non-Hierarchical procedures, such as complete pooling and independence to make local/regional-level analyses. In this study, a Hierarchical Bayesian procedure was, therefore, formulated to explore the posterior distribution of model parameters (for

generation of region-specific ASFR point estimates and uncertainty bound). Besides, other non-hierarchical approaches, namely Bayesian and the maximum likelihood methods, were also instrumented to estimate parameters and compare the result obtained using these approaches with Hierarchical Bayesian counterparts. Gibbs sampling along with MetropolisHastings argorithm in R (Development Core Team, 2005) was applied to draw the posterior samples for each parameter. Data augmentation method was also implemented to ease the sampling process. Sensitivity analysis, convergence diagnosis and model checking were also thoroughly conducted to ensure how robust our results are. In all cases, non-informative prior distributions for all regional vectors (parameters) were used in order to real the lack

of knowledge about these random variables. Result: The results obtained from this preliminary analysis testified that the values of the Akaike Information  criterion(AIC) for the proposed model, Skew Normal (SN), is lowest:

in the capital, Addis Ababa, Dire Dawa, Harari, Affar, Gambela, Benshangul-Gumuz, and country level data as well. On the contrary, its value was also higher some of the models and lower the rest on the remain regions, namely: Tigray, Oromiya, Amhara, Somali and SNNP. This tells us that the proposed model was able to capturing the pattern of fertility at the empirical fertility data of Ethiopia and its regions better than the other existing models considered in 6 of the 11 regions. The result from the HBA indicates that most of the posterior means were much closer to the true fixed fertility values. They were also more precise and have lower uncertainty with narrower credible interval vis-´a-vis the other approaches, ML and Bayesian estimate analogues. Conclusion: From the preliminary analysis, it can be concluded that the proposed model was better to capture ASFR pattern at national level and its regions than the other existing common models considered. Following this result, we conducted inference and prediction on the model parameters using these three approaches: HBA, BA and ML methods. The overall result suggested several points. One such is that HBA was the best approach to implement for such a data as it gave more consistent, precise (the low uncertainty) than the other approaches. Generally, both ML method and Bayesian method can be used to analyze our model, but they can be applicable to different conditions. ML method can be applied when precise values of model parameters have been known, large sample size can be obtained in the test; and similarly, Bayesian method can be applied when uncertainties

on the model parameters exist, prior knowledge on the model parameters are available, and few data is available in the study",,Implementing hierarchical bayesian model to fertility data: the case of Ethiopia,https://core.ac.uk/download/78395286.pdf,,,core
215284497,2016-11-01T00:00:00,"Reliable, real-time traffic surveillance is an integral and crucial function of the 21st century intelligent transportation systems (ITS) network. This technology facilitates instantaneous decision-making, improves roadway efficiency, and maximizes existing transportation infrastructure capacity, making transportation systems safe, efficient, and more reliable. Given the rapidly approaching era of smart cities, the work detailed in this dissertation is timely in that it reports on the design, development, and implementation of a novel, fully-autonomous, self-powered intelligent wireless sensor for real-time traffic surveillance. Multi-disciplinary, innovative integration of state-of-the-art, ultra-low-power embedded systems, smart physical sensors, and the wireless sensor network—powered by intelligent algorithms—are the basis of the developed Intelligent Vehicle Counting and Classification Sensor (iVCCS) platform. The sensor combines an energy-harvesting subsystem to extract energy from multiple sources and enable sensor node self-powering aimed at potentially indefinite life. A wireless power receiver was also integrated to remotely charge the sensor’s primary battery. Reliable and computationally efficient intelligent algorithms for vehicle detection, speed and length estimation, vehicle classification, vehicle re-identification, travel-time estimation, time-synchronization, and drift compensation were fully developed, integrated, and evaluated. Several length-based vehicle classification schemes particular to the state of Oklahoma were developed, implemented, and evaluated using machine learning algorithms and probabilistic modeling of vehicle magnetic length. A feature extraction employing different techniques was developed to determine suitable and efficient features for magnetic signature-based vehicle re-identification. Additionally, two vehicle re-identification models based on matching vehicle magnetic signature from a single magnetometer were developed. Comprehensive system evaluation and extensive data analyses were performed to fine-tune and validate the sensor, ensuring reliable and robust operation. Several field studies were conducted under various scenarios and traffic conditions on a number of highways and urban roads and resulted in 99.98% detection accuracy, 97.4782% speed estimation accuracy, and 97.6951% classification rate when binning vehicles into four groups based on their magnetic length. Threshold-based, re-identification results revealed 65.25%~100% identification rate for a window of 25~500 vehicles. Voting-based, re-identification evaluation resulted in 90~100% identification rate for a window of 25~500 vehicles. The developed platform is portable and cost-effective. A single sensor node costs only $30 and can be installed for short-term use (e.g., work zone safety, traffic flow studies, roadway and bridge design, traffic management in atypical situations), as well as long-term use (e.g., collision avoidance at intersections, traffic monitoring) on highways, roadways, or roadside surfaces. The power consumption assessment showed that the sensor is operational for several years. The iVCCS platform is expected to significantly supplement other data collection methods used for traffic monitoring throughout the United States. The technology is poised to play a vital role in tomorrow’s smart cities",,FULLY AUTONOMOUS SELF-POWERED INTELLIGENT WIRELESS SENSOR FOR REAL-TIME TRAFFIC SURVEILLANCE IN SMART CITIES,https://core.ac.uk/download/215284497.pdf,,,core
79584970,2016-01-01T00:00:00,"abstract: Microblogging services such as Twitter, Sina Weibo, and Tumblr have been emerging and deeply embedded into people's daily lives. Used by hundreds of millions of users to connect the people worldwide and share and access information in real-time, the microblogging service has also became the target of malicious attackers due to its massive user engagement and structural openness. Although existed, little is still known in the community about new types of vulnerabilities in current microblogging services which could be leveraged by the intelligence-evolving attackers, and more importantly, the corresponding defenses that could prevent both the users and the microblogging service providers from being attacked. This dissertation aims to uncover a number of challenging security and privacy issues in microblogging services and also propose corresponding defenses.

This dissertation makes fivefold contributions. The first part presents the social botnet, a group of collaborative social bots under the control of a single botmaster, demonstrate the effectiveness and advantages of exploiting a social botnet for spam distribution and digital-influence manipulation, and propose the corresponding countermeasures and evaluate their effectiveness. Inspired by Pagerank, the second part describes TrueTop, the first sybil-resilient system to find the top-K influential users in microblogging services with very accurate results and strong resilience to sybil attacks. TrueTop has been implemented to handle millions of nodes and 100 times more edges on commodity computers. The third and fourth part demonstrate that microblogging systems' structural openness and users' carelessness could disclose the later's sensitive information such as home city and age. LocInfer, a novel and lightweight system, is presented to uncover the majority of the users in any metropolitan area; the dissertation also proposes MAIF, a novel machine learning framework that leverages public content and interaction information in microblogging services to infer users' hidden ages. Finally, the dissertation proposes the first privacy-preserving social media publishing framework to let the microblogging service providers publish their data to any third-party without disclosing users' privacy and meanwhile meeting the data's commercial utilities. This dissertation sheds the light on the state-of-the-art security and privacy issues in the microblogging services.Dissertation/ThesisDoctoral Dissertation Electrical Engineering 201",,Secure and Privacy-Preserving Microblogging Services: Attacks and Defenses,https://core.ac.uk/download/79584970.pdf,,,core
87474816,2016-01-01T00:00:00Z,"Two years ago, 63 people died and more than 150 were seriously injured in Beijing (China) because of damage to a hydrocarbon pipeline. Urban networks are invisible because usually buried between 1 and 1,5 meters underground. They should be identified to prevent such accidents which involve workers as well as the public. Rural and urban districts, network concessionaries and contractors: everyone could benefit from their networks becoming safer. To prevent such accidents and protect workers and the public as well, some new regulations propose to identify and secure the buried networks. That’s why it is important to develop a software which deals with the risk management process and also about the risk visualization. This work is structured around three major sections:<br />– the utility of the Geographical Information to determine the minimal distances and the topological relations between the networks themselves, and also with the other element in their vicinity;<br />– the use of some Artificial Intelligence tools, and more particularly of Expert System, to take the current regulation into account and determine the accident risk probability;<br />– the contribution of virtual reality to perceive the underground world","Laboratory of Complex Mapping, Faculty of Geography, MSU","THE INTEREST OF GEOGRAPHICAL INFORMATION, ARTIFICIAL INTELLIGENCE AND VIRTUAL REALITY FOR THE UNDERGROUND NETWORK REPRESENTATION",,10.24057/2414-9179-2016-1-22-189-198,"[{'title': None, 'identifiers': ['issn:2414-9209', '2414-9209', 'issn:2414-9179', '2414-9179']}]",core
88286731,2017-01-01T00:00:00Z,"Modern broadband hybrid optical-wireless access networks have gained the attention of academia and industry due to their strategic advantages (cost-efficiency, huge bandwidth, flexibility, and mobility). At the same time, the proliferation of Software Defined Networking (SDN) enables the efficient reconfiguration of the underlying network components dynamically using SDN controllers. Hence, effective traffic-aware schemes are feasible in dynamically determining suitable configuration parameters for advancing the network performance. To this end, a novel machine learning mechanism is proposed for an SDN-enabled hybrid optical-wireless network. The proposed architecture consists of a 10-gigabit-capable passive optical network (XG-PON) in the network backhaul and multiple Long Term Evolution (LTE) radio access networks in the fronthaul. The proposed mechanism receives traffic-aware knowledge from the SDN controllers and applies an adjustment on the uplink-downlink configuration in the LTE radio communication. This traffic-aware mechanism is capable of determining the most suitable configuration based on the traffic dynamics in the whole hybrid network. The introduced scheme is evaluated in a realistic environment using real traffic traces such as Voice over IP (VoIP), real-time video, and streaming video. According to the obtained numerical results, the proposed mechanism offers significant improvements in the network performance in terms of latency and jitter",Hindawi Limited,DIANA: A Machine Learning Mechanism for Adjusting the TDD Uplink-Downlink Configuration in XG-PON-LTE Systems,,10.1155/2017/8198017,"[{'title': None, 'identifiers': ['1574-017x', '1875-905x', 'issn:1574-017X', 'issn:1875-905X']}]",core
132513871,2017,"Over the last few years, standardisation efforts are consolidating the role of the Routing Protocol for Low-Power and Lossy Networks (RPL) as the standard routing protocol for IPv6-based Wireless Sensor Networks (WSNs). Although many core functionalities are well defined, others are left implementation dependent. Among them, the definition of an efficient link-quality estimation (LQE) strategy is of paramount importance, as it influences significantly both the quality of the selected network routes and nodesâ\u80\u99 energy consumption. In this paper, we present RL-Probe, a novel strategy for link quality monitoring in RPL, which accurately measures link quality with minimal overhead and energy waste. To achieve this goal, RL-Probe leverages both synchronous and asynchronous monitoring schemes to maintain up-to-date information on link quality and to promptly react to sudden topology changes, e.g. due to mobility. Our solution relies on a reinforcement learning model to drive the monitoring procedures in order to minimise the overhead caused by active probing operations. The performance of the proposed solution is assessed by means of simulations and real experiments. Results demonstrated that RL-Probe helps in effectively improving packet loss rates, allowing nodes to promptly react to link quality variations as well as to link failures due to node mobility",'Elsevier BV',A reinforcement learning-based link quality estimation strategy for RPL and its impact on topology management,https://core.ac.uk/download/pdf/132513871.pdf,10.1016/j.comcom.2017.08.005,,core
77229041,2016-04-01T00:00:00,"Optical and radar satellite remote sensing have proven to provide essential crisis information in case of natural disasters, humanitarian relief activities and civil security issues in a growing number of cases through mechanisms such as the Copernicus Emergency Management Service (EMS) of the European Commission or the International Charter ‘Space and Major Disasters’.

The aforementioned programs and initiatives make use of satellite-based rapid mapping services aimed at delivering reliable and accurate crisis information after natural hazards.

Although these services are increasingly operational, they need to be continuously updated and improved through research and development (R&D) activities. The principal objective of ASAPTERRA (Advancing SAR and Optical Methods for Rapid Mapping), the ESA-funded R&D project being described here, is to improve, automate and, hence, speed-up geo-information extraction procedures in the context of natural hazards response. This is performed through the development, implementation, testing and validation of novel image processing methods using optical and Synthetic Aperture Radar (SAR) data. The methods are mainly developed based on data of the German radar satellites TerraSAR-X and TanDEM-X, the French satellite missions Pléiades-1A/1B as well as the ESA missions Sentinel-1/2 with the aim to better characterize the potential and limitations of these sensors and their synergy. The resulting algorithms and techniques are evaluated in real case applications during rapid mapping activities. 

The project is focussed on three types of natural hazards: floods, landslides and fires. Within this presentation an overview of the main methodological developments in each topic is given and demonstrated in selected test areas. The following developments are presented in the context of flood mapping: a fully automated Sentinel-1 based processing chain for detecting open flood surfaces, a method for the improved detection of flooded vegetation in Sentinel-1data using Entropy/Alpha decomposition, unsupervised Wishart Classification, and object-based post-classification as well as semi-automatic approaches for extracting inundated areas and flood traces in rural and urban areas from VHR and HR optical imagery using machine learning techniques. Methodological developments related to fires are the implementation of fast and robust methods for mapping burnt scars using change detection procedures using SAR (Sentinel-1, TerraSAR-X) and HR optical (e.g. SPOT, Sentinel-2) data as well as the extraction of 3D surface and volume change information from Pléiades stereo-pairs. In the context of landslides, fast and transferable change detection procedures based on SAR (TerraSAR-X) and optical (SPOT) data as well methods for extracting the extent of landslides only based on polarimetric VHR SAR (TerraSAR-X) data are presented",,"Improving the extraction of crisis information in the context of flood, fire, and landslide rapid mapping using SAR and optical remote sensing data",,,,core
478869500,2017-01-01T00:00:00,"2017 annual report for the Blue Waters ProjectNSF OCI-0725070NSF ACI-12389932017 Blue Waters Annual Report Table of Contents
18.	Dinshaw S. Balsara, Simulating Two-Fluid MHD Turbulence in Star-Forming Molecular Clouds on the Blue Waters System
20.	Tiziana Di Matteo, Supermassive Black Holes at the Cosmic Frontier
22.	Gilbert Holder, Theoretical Astrophysics and Data Analysis
24.	Eliu Huerta, Detection of Gravitational Wave Sources in Dense Stellar Environments
26.	Eliu Huerta, Deep Neural Networks to Enable Real-Time Multimessenger Astrophysics
28.	Thomas W. Jones, Toward Robust Magnetohydrodynamic Simulations of Galaxy Cluster Formation
30.	Eric J. Lentz, Exploring the Nature of Exploding Massive Stars with High Resolution
32.	Deborah Levin, Modeling Plasma Flows with Kinetic Approaches using Hybrid CPU-GPU Computing
34.	Yi-Hsin Liu, Three-Dimensional Nature of Collisionless Magnetic Reconnection at Earth’s Magnetopause
36.	Warren B. Mori, Transformative Petascale Particle-in-Cell Simulations
38.	Scott C. Noble, Mini-disk Dynamics about Supermassive Black Holes Binaries 
40.	Michael L. Norman, Realistic Simulations of the Intergalactic Medium:  The Search for Missing Physics
42.	Brian W. O’Shea, Simulating Galaxy Formation Across Cosmic Time
44.	Christian D. Ott, 3D General-Relativistic Radiation-Hydrodynamic Simulations of Core-Collapse Supernovae
46.	Nikolai Pogorelov, Modeling Physical Processes in the Solar Wind and Local Interstellar Medium with a Multiscale Fluid-Kinetic Simulations Suite
48.	Thomas Quinn, Unified Modeling of Galaxy Populations in Clusters
50.	Vadim Roytershteyn, Kinetic Simulations of Large-Scale Plasma Turbulence
52.	Hsi-Yu Schive, GPU-Accelerated Adaptive Mesh Refinement
54.	Stuart L. Shapiro, Magnetorotational Collapse of Supermassive Stars:  Black Hole Formation, Gravitational Waves, and Jets
56.	Alexander Tchekhovskoy, GPU-Accelerated Simulations:  Black Holes, Spaghettified Stars, and Tilted Disks
58.	Gabor Toth, Advanced Space Weather Modeling
60.	Paul R. Woodward, 3D Simulations of i-Process Nucleosynthesis
64.	Marin Clark, High-Resolution Digital Surface Models of the 2016 Mw7.8 Kaikoura Earthquake, New Zealand
66.	Jennifer Corcoran, Image Processing to Build a Multi-Temporal Vegetation Elevation Model(MTVEM) of the Great Lakes Basin(GLB)
68.	Larry Di-Girolamo, The Terra Data Fusion Project
70.	Marcelo H. Garcia, Large-Eddy Simulation of Sediment Transport and Hydrodynamics at River Bifurcations:  Using a Highly Scalable Spectral Element-Based CFD Solver
72.	Ian Howat, The Reference Elevation Model of Antarctica
74.	Sonia Lasher-Trapp, Untangling Entrainment and Precipitation in Convective Clouds
76.	Lijun Liu, Understanding the 4-D Evolution of the Solid Earth Using Geodynamic Models with Data Assimilation
78.	Philip J. Maechling, Physics-Based Modeling of High-Frequency Ground Motions and Probabilistic Seismic Hazard Analysis
80.	Paul Morin, Enhanced Digital Elevation Model for the Artic
82.	Leigh Orf, Simulating the Most Devastating Tornadoes Embedded Within Supercell Thunderstorms
84.	Robert Rauber, Don Wuebbles, High-Resolution Earth System Modeling Using Blue Waters’ Capabilities
86.	Jamesina J. Simpson, Location-Specific Space Weather Hazards to Electric Power Grids Calculated on a Global Scale
88.	Ryan L. Sriver, Impact of Ocean Coupling on Simulated Tropical Cyclone Activity in the High-Resolution Community Earth System Mode
90.	Robert J. Trapp, Petascale Modeling of Convective Storms Under Climate Change and Variability
92.	Junshik Um, Impacts of Orientation and Morphology of Small Atmospheric Ice Crystals on in-situ Aircraft Measurements:  Scattering Calculations
94.	Albert J. Valocchi, Pore-Scale Simulation of Multiphase Flow in Porous Media with Applications to Geological Sequestration of Carbon Dioxide
96.	Matthew West, 3D Particle-Resolved Aerosol Model to Quantify and Reduce Uncertainties in Aerosol-Atmosphere Interactions
98.	Donald J. Wuebbles, Particulate Matter Prediction and Source Attribution for U.S. Air Quality Management in a Changing World
102.	David Ackerman, Exploring Confinement vs. Orientation Effects in Rigid and Semi-Flexible Polymers Using a Massively Parallel Framework
104.	Ange-Therese Akono, Multi-Scale and Multi-Physics Modeling of the Strength of Geopolymer Composites
106.	Jean Paul Allain, Harnessing Petascale Computing to Explain Fundamental Mechanisms Driving Nanopatterning of Multicomponent Surfaces by Directed Irradiation Synthesis
108.	Narayana R. Aluru, Study of DIBs with Functional Channels
110.	Jerzy Bernholc, Petaflops Simulation and Design of Nanoscale Materials and Devices
112.	Daniel Bodony, Reducing Jet Aircraft Noise
114.	David Ceperley, Properties of Dense Hydrogen
116.	Huck Beng Chew, Scalable Nanopatterning of Graphene by Hydrogen-Plasma Etching
118.	Davide Curreli, hPIC:  A Scalable Electrostatic Particle-In-Cell for Plasma-Material Interactions
120.	J. P. Draayer, Innovative Ab Initio Symmetry-Adapted No-Core Shell Model for Advancing Fundamental Physics and Astrophysics
122.	Lian Duan, DNS of Pressure Fluctuations Induced by Supersonic Turbulent Boundary Layers
124.	Said Elghobashi, Dispersion of Fully Resolved Liquid Droplets in Isotropic Turbulent Flow
126.	Elif Ertekin, QMCBD:  A Living Database to Accelerate Worldwide Development and use of Quantum Monte Carlo Methods
128.	Paul Fischer, Numerical Methods and Software for Computational Fluid Dynamics, NEK5000
130.	Marcelo H. Garcia, Direct Numerical Simulation of Turbulence and Sediment Transport in Oscillatory Boundary Layer Flows
132.	Paolo Gardoni, 3D Probabilistic Physics-Based Seismic Hazard Maps for Regional Risk Analysis
134.	Mattia Gazzola, Optimal Bio-Locomotion Strategies in Fluids
136.	Kathryn Huff, Coupled Multi-Physics of Advanced Molten Salt Nuclear Reactors
138.	Sohrab Ismail-Beigi, Understanding Hydrogen Storage in Metal Organic Frameworks using Massively-Parallel Electronic Structure Calculations
140.	Prashant K. Jain, Atomistic Modeling of Transformations in Nanocrystals
142.	Eric Johnsen, Numerical Simulations of Collapsing Cavitation Bubbles on Blue Waters
144.	Gerhard Klimeck, Leading Future Electronics into the Nano Regime Using Quantum Atomistic Simulations in NEMO5
146.	Deborah A. Levin, Kinetic Simulations of Unsteady Shock-Boundary Layer Interactions
148.	Paul Mackenzie, High Energy Physics on Blue Waters
150.	Burkhard Militzer, First-Principles Computer Simulations of Hydrocarbons Under Fusion Conditions
152.	Sarma L. Rani, Direct Numerical Simulations of the Relative Motion of High-Inertia Particles in Isotropic Turbulence
154.	Caroline Riedl, Mapping Proton Quark Structure Using Petabytes of COMPASS Data
156.	Andre Schleife, Optical Determination of Crystal Phase in Semiconductor Nanocrystals
158.	Ahmed Taha, Advanced Digital Technology for Materials and Manufacturing
160.	Brian G. Thomas, Transient Multiphase Flow Phenomena and Defect Formation in Steel Continuous Casting
162.	Rafael Tinoco Lopez, High Resolution Numerical Simulation of Oscillatory Flow and Sediment Transport through Aquatic Vegetation
164.	Lucas K. Wagner, Quantum Monte Carlo Simulations of Magnetism and Models in Condensed Matter
166.	P.K. Yeung, Intermittency, Resolution Effects and High Schmidt Number Mixing in Turbulence
170.	Donna Cox, CADENS NSF Project:  Digital Literacy, Data Visualization, and the Cinematic Presentation of Science
172.	William Gropp, Algorithms for Extreme-Scale Systems
174.	Levent Gurel, Parallelization of the Multilevel Fast Multipole Algorithm(MLFMA) on Heterogeneous CPU-GPU Architectures
176.	Ravishankar K. Iyer, Predicting Performance Degradation and Failure of Applications through System Activity Monitoring
178.	Rakesh Nagi, Parallel Algorithms for Solving Large Assignment Problems
180.	Luke Olson, Localizing Communication in Sparse Matrix Operations
182.	Edgar Solomonik, Performance Evaluation of New Algebraic Algorithms and Libraries
184.	Tandy Warnow, Parallel Algorithms for Big Data Phylogenomics, Proteomics, and Metagenomics
186.	Tao Xie, Hardware Acceleration of Deep Learning
188.	Kevin Olson, A Critical Evaluation of the OP2/OPS Parallel Meshing and Code Generation Software
192.	Aleksei Aksimentiev, DNA Origami Membrane Channels
194.	Aleksei Aksimentiev, Molecular Mechanism of Nuclear Transports
196.	Gregory Bauer, Improving NWChem Scalability Using the DataSpaces Framework
198.	Gustavo Caetano-Anolles, How Function Shapes Dynamics in Protein Evolution
200.	Isaac Cann, Cellulosome Structure Determination by Atomistic Simulations Combined with Experimental Assays
202.	Vincenzo Carnevale, Mechanism of Temperature Sensitivity in TRPV1 Channel
204.	Thomas E. Cheatham III, Exploring the Structure and Dynamics of Converged Ensembles of DNA and RNA Through Molecular Dynamics Simulations
206.	Christina C.H. Cheng, Structural Basis for Extreme Cold Tolerance in the Eye Lenses of Teleost Fishes
208.	Ken A. Dill, Predicting Protein Structures with Physical Petascale Molecular Simulations
210.	Ahmed Elbanna, Multiscale Modeling of Biofilm Dynamics in Drinking Water Distribution Systems:  Toward Predictive Modeling of Pathogen Outbreaks
212.	Peter Freddolino, Comprehensive in silico Mapping of DNA-Binding Protein Affinity Landscapes
214.	Sharon Hammes-Schiffer, Non-Born-Oppenheimer Effects Between Electrons and Protons
216.	So Hirata, Brueckner-Goldstone Quantum Monte Carlo
218.	Peter Kasson, How Membrane Organization Controls Influenza Infections
220.	Zaida Luthey-Schulten, A Hybrid Stochastic-Deterministic Simulation Method Enables Fast Simulation of Cellular Processes in Eukaryotes
222.	Nancy Makri, Quantum-Classical Path Integral Simulation of Charge Transfer Reactions
224.	Arif Masud, Patient-Specific HPC Models and Simulation-Based Imaging for Cardiovascular Surgical Planning
226.	James W. Mazzuca, Quantum Effects of Proton Transfer in Biological Systems
228.	Mahmoud Moradi, Thermodynamic Characterization of Conformational Landscape in Proton-Coupled Oligopeptide Transporters
230.	Vijay Pande, Machine Learning Reveals Ligand-Directed Conformational Change of u Opioid Receptor
232.	Benoit Roux, Elucidating the Molecular Mechanism of C-type Inactivation in Potassium Channels
234.	Klaus Schulten, Studying Cellular Processes through the Computational Microscope
236.	Diwakar Shukla, Understanding the Protein Allostery in Kinanses and GPCRs
238.	Ivan Soltesz, Data-Driven, Biologically Constrained Computational Model of the Hippocampal Network at Full Scale
240.	Marcos Sotomayor, Stretching the Cadherin Molecular Velcro of Cell-Cell Junctions
242.	Ahsok Srinivasan, Simulation of Viral Infection Propagation through Air Travel
244. 	Brad Sutton, High-Resolution Magnetic Resonance Imaging of Mechanical Properties of the Brain
246.	Ilias Tagkopoulos, A Crystal Ball of Bacterial Behavior:  from Data to Prediction using Genome-Scale Models
248.	Greg A. Voth, Large-Scale Coarse-Grained Molecular Simulations of the Viral Lifecycle of HIV-1
252.	Lars Hansen, Yongyang Cai, Policy Responses to Climate Change in a Dynamic Stochastic Economy
254.	Wendy K. Tam Cho, Enabling Redistricting Reform:  A Computational Study of Zoning Optimization
258.	Elizabeth Agee, Resolving Plant Functional Biodiversity to Quantify Forest Drought Resistance in the Amazon
260.	Maureen T. Brooks, Modeling Nonlinear Physical-Biological Interactions:  Eddies and Sargassum in the North Atlantic
262.	Iryna Butsky, The Role of Cosmic Rays in Isolated Disk Galaxies
264.	Jon Calhoun, Analyzing the Propagation of Soft Error Corruption in HPC Applications
266.	Justin Drake, Toward Developing a Thermodynamic Model of Binding-Induced Conformational Transitions in Short, Disordered Protein Regions
268. 	Paul Hime, Genomic Perspectives on the Amphibian Tree of Life
270.	Michael P. Howard, Multiscale Simulations of Complex Fluid Rheology
272.	Alexandra L. Jones, High Accuracy Radiative Transfer in Cloudy Atmospheres
274.	Andrew Kirby, High-Fidelity Blade-Resolved Wind Farm Simulations
276.	Sara Kokkila Schumacher, Reducing the Computational Cost of Coupled Clustery Theory
278.	Larisa Reames, Simulated Effects of Urban Environments on the Dynamics of a Supercell Thunderstorm
280.	Sherwood Richers, Monte Carlo Neutrino Closures in 3D GRMHD Simulations of Core-Collapse Supernovae and Neutron Star Mergers
282.	Sean L. Seyler, Understanding the Role of Hydrodynamic Fluctuations in Biomacromolecular Dynamics through the Development of Hybrid Atomistic-Continuum Simulation
284.	Ronald Stenz, The Impacts of Hydrometeor Centrifuging on Tornado Dynamics
286.	Erin Teich, Glassy Dynamics and Identity Crises in Hard-Particle Systems
288.	Samuel Totorica, Magnetic Reconnection in Laser-Driven Plasmas:  from Astrophysics to the Laboratory in silico






2017 Blue Waters Annual Report Table of Contents
18.	Dinshaw S. Balsara, Simulating Two-Fluid MHD Turbulence in Star-Forming Molecular Clouds on the Blue Waters System
20.	Tiziana Di Matteo, Supermassive Black Holes at the Cosmic Frontier
22.	Gilbert Holder, Theoretical Astrophysics and Data Analysis
24.	Eliu Huerta, Detection of Gravitational Wave Sources in Dense Stellar Environments
26.	Eliu Huerta, Deep Neural Networks to Enable Real-Time Multimessenger Astrophysics
28.	Thomas W. Jones, Toward Robust Magnetohydrodynamic Simulations of Galaxy Cluster Formation
30.	Eric J. Lentz, Exploring the Nature of Exploding Massive Stars with High Resolution
32.	Deborah Levin, Modeling Plasma Flows with Kinetic Approaches using Hybrid CPU-GPU Computing
34.	Yi-Hsin Liu, Three-Dimensional Nature of Collisionless Magnetic Reconnection at Earth’s Magnetopause
36.	Warren B. Mori, Transformative Petascale Particle-in-Cell Simulations
38.	Scott C. Noble, Mini-disk Dynamics about Supermassive Black Holes Binaries 
40.	Michael L. Norman, Realistic Simulations of the Intergalactic Medium:  The Search for Missing Physics
42.	Brian W. O’Shea, Simulating Galaxy Formation Across Cosmic Time
44.	Christian D. Ott, 3D General-Relativistic Radiation-Hydrodynamic Simulations of Core-Collapse Supernovae
46.	Nikolai Pogorelov, Modeling Physical Processes in the Solar Wind and Local Interstellar Medium with a Multiscale Fluid-Kinetic Simulations Suite
48.	Thomas Quinn, Unified Modeling of Galaxy Populations in Clusters
50.	Vadim Roytershteyn, Kinetic Simulations of Large-Scale Plasma Turbulence
52.	Hsi-Yu Schive, GPU-Accelerated Adaptive Mesh Refinement
54.	Stuart L. Shapiro, Magnetorotational Collapse of Supermassive Stars:  Black Hole Formation, Gravitational Waves, and Jets
56.	Alexander Tchekhovskoy, GPU-Accelerated Simulations:  Black Holes, Spaghettified Stars, and Tilted Disks
58.	Gabor Toth, Advanced Space Weather Modeling
60.	Paul R. Woodward, 3D Simulations of i-Process Nucleosynthesis
64.	Marin Clark, High-Resolution Digital Surface Models of the 2016 Mw7.8 Kaikoura Earthquake, New Zealand
66.	Jennifer Corcoran, Image Processing to Build a Multi-Temporal Vegetation Elevation Model(MTVEM) of the Great Lakes Basin(GLB)
68.	Larry Di-Girolamo, The Terra Data Fusion Project
70.	Marcelo H. Garcia, Large-Eddy Simulation of Sediment Transport and Hydrodynamics at River Bifurcations:  Using a Highly Scalable Spectral Element-Based CFD Solver
72.	Ian Howat, The Reference Elevation Model of Antarctica
74.	Sonia Lasher-Trapp, Untangling Entrainment and Precipitation in Convective Clouds
76.	Lijun Liu, Understanding the 4-D Evolution of the Solid Earth Using Geodynamic Models with Data Assimilation
78.	Philip J. Maechling, Physics-Based Modeling of High-Frequency Ground Motions and Probabilistic Seismic Hazard Analysis
80.	Paul Morin, Enhanced Digital Elevation Model for the Artic
82.	Leigh Orf, Simulating the Most Devastating Tornadoes Embedded Within Supercell Thunderstorms
84.	Robert Rauber, Don Wuebbles, High-Resolution Earth System Modeling Using Blue Waters’ Capabilities
86.	Jamesina J. Simpson, Location-Specific Space Weather Hazards to Electric Power Grids Calculated on a Global Scale
88.	Ryan L. Sriver, Impact of Ocean Coupling on Simulated Tropical Cyclone Activity in the High-Resolution Community Earth System Mode
90.	Robert J. Trapp, Petascale Modeling of Convective Storms Under Climate Change and Variability
92.	Junshik Um, Impacts of Orientation and Morphology of Small Atmospheric Ice Crystals on in-situ Aircraft Measurements:  Scattering Calculations
94.	Albert J. Valocchi, Pore-Scale Simulation of Multiphase Flow in Porous Media with Applications to Geological Sequestration of Carbon Dioxide
96.	Matthew West, 3D Particle-Resolved Aerosol Model to Quantify and Reduce Uncertainties in Aerosol-Atmosphere Interactions
98.	Donald J. Wuebbles, Particulate Matter Prediction and Source Attribution for U.S. Air Quality Management in a Changing World
102.	David Ackerman, Exploring Confinement vs. Orientation Effects in Rigid and Semi-Flexible Polymers Using a Massively Parallel Framework
104.	Ange-Therese Akono, Multi-Scale and Multi-Physics Modeling of the Strength of Geopolymer Composites
106.	Jean Paul Allain, Harnessing Petascale Computing to Explain Fundamental Mechanisms Driving Nanopatterning of Multicomponent Surfaces by Directed Irradiation Synthesis
108.	Narayana R. Aluru, Study of DIBs with Functional Channels
110.	Jerzy Bernholc, Petaflops Simulation and Design of Nanoscale Materials and Devices
112.	Daniel Bodony, Reducing Jet Aircraft Noise
114.	David Ceperley, Properties of Dense Hydrogen
116.	Huck Beng Chew, Scalable Nanopatterning of Graphene by Hydrogen-Plasma Etching
118.	Davide Curreli, hPIC:  A Scalable Electrostatic Particle-In-Cell for Plasma-Material Interactions
120.	J. P. Draayer, Innovative Ab Initio Symmetry-Adapted No-Core Shell Model for Advancing Fundamental Physics and Astrophysics
122.	Lian Duan, DNS of Pressure Fluctuations Induced by Supersonic Turbulent Boundary Layers
124.	Said Elghobashi, Dispersion of Fully Resolved Liquid Droplets in Isotropic Turbulent Flow
126.	Elif Ertekin, QMCBD:  A Living Database to Accelerate Worldwide Development and use of Quantum Monte Carlo Methods
128.	Paul Fischer, Numerical Methods and Software for Computational Fluid Dynamics, NEK5000
130.	Marcelo H. Garcia, Direct Numerical Simulation of Turbulence and Sediment Transport in Oscillatory Boundary Layer Flows
132.	Paolo Gardoni, 3D Probabilistic Physics-Based Seismic Hazard Maps for Regional Risk Analysis
134.	Mattia Gazzola, Optimal Bio-Locomotion Strategies in Fluids
136.	Kathryn Huff, Coupled Multi-Physics of Advanced Molten Salt Nuclear Reactors
138.	Sohrab Ismail-Beigi, Understanding Hydrogen Storage in Metal Organic Frameworks using Massively-Parallel Electronic Structure Calculations
140.	Prashant K. Jain, Atomistic Modeling of Transformations in Nanocrystals
142.	Eric Johnsen, Numerical Simulations of Collapsing Cavitation Bubbles on Blue Waters
144.	Gerhard Klimeck, Leading Future Electronics into the Nano Regime Using Quantum Atomistic Simulations in NEMO5
146.	Deborah A. Levin, Kinetic Simulations of Unsteady Shock-Boundary Layer Interactions
148.	Paul Mackenzie, High Energy Physics on Blue Waters
150.	Burkhard Militzer, First-Principles Computer Simulations of Hydrocarbons Under Fusion Conditions
152.	Sarma L. Rani, Direct Numerical Simulations of the Relative Motion of High-Inertia Particles in Isotropic Turbulence
154.	Caroline Riedl, Mapping Proton Quark Structure Using Petabytes of COMPASS Data
156.	Andre Schleife, Optical Determination of Crystal Phase in Semiconductor Nanocrystals
158.	Ahmed Taha, Advanced Digital Technology for Materials and Manufacturing
160.	Brian G. Thomas, Transient Multiphase Flow Phenomena and Defect Formation in Steel Continuous Casting
162.	Rafael Tinoco Lopez, High Resolution Numerical Simulation of Oscillatory Flow and Sediment Transport through Aquatic Vegetation
164.	Lucas K. Wagner, Quantum Monte Carlo Simulations of Magnetism and Models in Condensed Matter
166.	P.K. Yeung, Intermittency, Resolution Effects and High Schmidt Number Mixing in Turbulence
170.	Donna Cox, CADENS NSF Project:  Digital Literacy, Data Visualization, and the Cinematic Presentation of Science
172.	William Gropp, Algorithms for Extreme-Scale Systems
174.	Levent Gurel, Parallelization of the Multilevel Fast Multipole Algorithm(MLFMA) on Heterogeneous CPU-GPU Architectures
176.	Ravishankar K. Iyer, Predicting Performance Degradation and Failure of Applications through System Activity Monitoring
178.	Rakesh Nagi, Parallel Algorithms for Solving Large Assignment Problems
180.	Luke Olson, Localizing Communication in Sparse Matrix Operations
182.	Edgar Solomonik, Performance Evaluation of New Algebraic Algorithms and Libraries
184.	Tandy Warnow, Parallel Algorithms for Big Data Phylogenomics, Proteomics, and Metagenomics
186.	Tao Xie, Hardware Acceleration of Deep Learning
188.	Kevin Olson, A Critical Evaluation of the OP2/OPS Parallel Meshing and Code Generation Software
192.	Aleksei Aksimentiev, DNA Origami Membrane Channels
194.	Aleksei Aksimentiev, Molecular Mechanism of Nuclear Transports
196.	Gregory Bauer, Improving NWChem Scalability Using the DataSpaces Framework
198.	Gustavo Caetano-Anolles, How Function Shapes Dynamics in Protein Evolution
200.	Isaac Cann, Cellulosome Structure Determination by Atomistic Simulations Combined with Experimental Assays
202.	Vincenzo Carnevale, Mechanism of Temperature Sensitivity in TRPV1 Channel
204.	Thomas E. Cheatham III, Exploring the Structure and Dynamics of Converged Ensembles of DNA and RNA Through Molecular Dynamics Simulations
206.	Christina C.H. Cheng, Structural Basis for Extreme Cold",,Blue Waters 2017 Annual Report,,,,core
157595638,2017-05-01T00:00:00,"International audienceIn this paper, the problem of proactive deployment of cache-enabled unmanned aerial vehicles (UAVs) for optimizing the quality-of-experience (QoE) of wireless devices in a cloud radio access network is studied. In the considered model, the network can leverage human-centric information, such as users' visited locations, requested contents, gender, job, and device type to predict the content request distribution, and mobility pattern of each user. Then, given these behavior predictions, the proposed approach seeks to find the user-UAV associations, the optimal UAVs' locations, and the contents to cache at UAVs. This problem is formulated as an optimization problem whose goal is to maximize the users' QoE while minimizing the transmit power used by the UAVs. To solve this problem, a novel algorithm based on the machine learning framework of conceptor-based echo state networks (ESNs) is proposed. Using ESNs, the network can effectively predict each user's content request distribution and its mobility pattern when limited information on the states of users and the network is available. Based on the predictions of the users' content request distribution and their mobility patterns, we derive the optimal locations of UAVs as well as the content to cache at UAVs. Simulation results using real pedestrian mobility patterns from BUPT and actual content transmission data from Youku show that the proposed algorithm can yield 33.3% and 59.6% gains, respectively, in terms of the average transmit power and the percentage of the users with satisfied QoE compared with a benchmark algorithm without caching and a benchmark solution without UAVs",'Institute of Electrical and Electronics Engineers (IEEE)',Caching in the Sky: Proactive Deployment of Cache-Enabled Unmanned Aerial Vehicles for Optimized Quality-of-Experience,https://core.ac.uk/download/157595638.pdf,10.1109/JSAC.2017.2680898,,core
214366808,2017-01-01T08:00:00,"Falls are common and often dangerous for groups with impaired mobility, like the elderly or people with lower limb amputations. Finding ways of minimizing the frequency or impact of a fall can improve quality of life dramatically. When someone does fall, real-time detection of the fall and a long-lie can trigger fast medical assistance. Such a system can also collect reliable data on the nature of real-world falls that can be used to better understand the circumstances, to aid in prevention efforts. This work has been to develop a real-time fall tracking system specifically for subjects with lower limb amputations.
In this study 17 subjects (10 healthy controls and 7 amputees) were asked to simulate 4 types of falls (trip, slip, right and left lateral) 3 times each with a mobile phone placed at 3 different locations on the body (pouch, pocket, and hand). Signals were collected from the accelerometer, gyroscope and barometer sensors using the Android mobile phone application Purple Robot. We compared 5 different machine learning classifiers for fall detection: logistic regression (L1 and L2 norm), support vector machines, K-nearest neighbors, decision trees, and random forest. Logistic regression (L1 regularized  lasso ) and random forest yielded the best results on the test set (98.8% and 98.4%, respectively). There was no significant difference between amputee and healthy control falls in terms of classifier accuracy. When testing on real world data with no recorded falls, the false positive rate was only 0.07%.
In addition to offline algorithmic development, the detection system was implemented for real-time application on a mobile platform. The previously-trained logistic regression model was implemented on the mobile platform for real-time detection. This platform will be used in an upcoming amputee population falls study. The completed system will gather data on the current conditions leading to the fall (weather, GPS location, etc.) and classify the type of the fall. The system will follow up with notifications requesting a response from the user, or automatically notify emergency contacts or 911 as needed. The steps taken in creating this system bring us closer to real-time intervention strategies to minimize the impact of falls, and enable us to collect accurate falls-related data to improve fall prevention strategies and prosthesis design",Loyola eCommons,Real-Time Fall Detection and Response on Mobile Phones Using Machine Learning,https://core.ac.uk/download/214366808.pdf,,,core
216795010,Issued on: 2016,"Summarization: Two different methodologies for hydraulic head simulation were compared in this study. The first methodology is a classic numerical groundwater flow simulation model, Princeton Transport Code (PTC), while the second one is a black-box approach that uses Artificial Neural Networks (ANNs). Both methodologies were implemented in the Bavaria region in Germany at thirty observation wells. When using PTC, meteorological and geological data are used in order to compute the simulated hydraulic head following the calibration of the appropriate model parameters. The ANNs use meteorological and hydrological data as input parameters. Different input parameters and ANN architectures were tested and the ANN with the best performance was compared with the PTC model simulation results. One ANN was trained for every observation well and the hydraulic head change was simulated on a daily time step. The performance of the two models was then compared based on the real field data from the study area. The cases in which one model outperforms the other were summarized, while the use of one instead of the other depends on the application and further use of the model.Παρουσιάστηκε στο: Global Nest Journa",'University of the Aegean',Comparison of a black-box model to a traditional numerical model for hydraulic head prediction,,10.30955/gnj.002002,,core
80311895,2016-01-01T00:00:00,"The need for evaluation models capable of returning ‘slender’ and

reliable mass appraisals of properties belonging to different market segments

has been made mandatory by the events that are covering the global real estate

finance, because of the emergence of non-performing loans in the banks’

balance sheets. In Italy, the non-performing loans have been estimated by the

Italian Banking Association equal to about 300 billion euro in 2014. In the

present paper, three approaches of data-driven techniques (hedonic price

model, artificial neural networks and evolutionary polynomial regression) have

been applied to a sample of residential apartments recently sold in a district of

the city of Bari (Italy), in order to test the respective performance for mass

appraisals. The models obtained by the implementation of the three procedures

have been compared in terms of statistical accuracy, empirical compliance of

the results and complexity of the functional relationships",,Data-driven techniques for mass appraisals. Applications to the residential market of the city of Bari (Italy),,10.1504/ijbidm.2016.081604,,core
141533423,2017-11-29T00:00:00,"The combination of Artificial Intelligence (AI) and Internet-of-Things (IoT),
which is denoted as AI-powered Internet-of-Things (AIoT), is capable of
processing huge amount of data generated from a large number of devices and
handling complex problems in social infrastructures. As AI and IoT technologies
are becoming mature, in this paper, we propose to apply AIoT technologies for
traffic light control, which is an essential component for intelligent
transportation system, to improve the efficiency of smart city's road system.
Specifically, various sensors such as surveillance cameras provide real-time
information for intelligent traffic light control system to observe the states
of both motorized traffic and non-motorized traffic. In this paper, we propose
an intelligent traffic light control solution by using distributed multi-agent
Q learning, considering the traffic information at the neighboring
intersections as well as local motorized and non-motorized traffic, to improve
the overall performance of the entire control system. By using the proposed
multi-agent Q learning algorithm, our solution is targeting to optimize both
the motorized and non-motorized traffic. In addition, we considered many
constraints/rules for traffic light control in the real world, and integrate
these constraints in the learning algorithm, which can facilitate the proposed
solution to be deployed in real operational scenarios. We conducted numerical
simulations for a real-world map with real-world traffic data. The simulation
results show that our proposed solution outperforms existing solutions in terms
of vehicle and pedestrian queue lengths, waiting time at intersections, and
many other key performance metrics",,"Intelligent Traffic Light Control Using Distributed Multi-agent Q
  Learning",http://arxiv.org/abs/1711.10941,,,core
151576746,2016-07-01T07:00:00,"By taking advantage of complementary communication technologies, distinct sensing functionalities and varied motion dynamics present in a heterogeneous multi-robotic network, it is possible to accomplish a main mission objective by assigning specialized sub-tasks to specific members of a robotic team. An adequate selection of the team members and an effective coordination are some of the challenges to fully exploit the unique capabilities that these types of systems can offer. Motivated by real world applications, we focus on a multi-robotic network consisting off aerial and ground agents which has the potential to provide critical support to humans in complex settings. For instance, aerial robotic relays are capable of transporting small ground mobile sensors to expand the communication range and the situational awareness of first responders in hazardous environments. In the first part of this dissertation, we extend work on manipulation of cable-suspended loads using aerial robots by solving the problem of lifting the cable-suspended load from the ground before proceeding to transport it. Since the suspended load-quadrotor system experiences switching conditions during this critical maneuver, we define a hybrid system and show that it is differentially-flat. This property facilitates the design of a nonlinear controller which tracks a waypoint-based trajectory associated with the discrete states of the hybrid system. In addition, we address the case of unknown payload mass by combining a least-squares estimation method with the designed controller. Second, we focus on the coordination of a heterogeneous team formed by a group of ground mobile sensors and a flying communication router which is deployed to sense areas of interest in a cluttered environment. Using potential field methods, we propose a controller for the coordinated mobility of the team to guarantee inter-robot and obstacle collision avoidance as well as connectivity maintenance among the ground agents while the main goal of sensing is carried out. For the case of the aerial communications relays, we combine antenna diversity with reinforcement learning to dynamically re-locate these relays so that the received signal strength is maintained above a desired threshold. Motivated by the recent interest of combining radio frequency and optical wireless communications, we envision the implementation of an optical link between micro-scale aerial and ground robots. This type of link requires maintaining a sufficient relative transmitter-receiver position for reliable communications. In the third part of this thesis, we tackle this problem. Based on the link model, we define a connectivity cone where a minimum transmission rate is guaranteed. For example, the aerial robot has to track the ground vehicle to stay inside this cone. The control must be robust to noisy measurements. Thus, we use particle filters to obtain a better estimation of the receiver position and we design a control algorithm for the flying robot to enhance the transmission rate. Also, we consider the problem of pairing a ground sensor with an aerial vehicle, both equipped with a hybrid radio-frequency/optical wireless communication system. A challenge is positioning the flying robot within optical range when the sensor location is unknown. Thus, we take advantage of the hybrid communication scheme by developing a control strategy that uses the radio signal to guide the aerial platform to the ground sensor. Once the optical-based signal strength has achieved a certain threshold, the robot hovers within optical range. Finally, we investigate the problem of building an alliance of agents with different skills in order to satisfy the requirements imposed by a given task. We find this alliance, known also as a coalition, by using a bipartite graph in which edges represent the relation between agent capabilities and required resources for task execution. Using this graph, we build a coalition whose total capability resources can satisfy the task resource requirements. Also, we study the heterogeneity of the formed coalition to analyze how it is affected for instance by the amount of capability resources present in the agents",UNM Digital Repository,Exploiting Heterogeneity in Networks of Aerial and Ground Robotic Agents,https://core.ac.uk/download/151576746.pdf,,,core
199344602,2017-01-01T00:00:00,"Road safety applications envisaged for vehicular ad hoc networks (VANETs) depend largely on the exchange of messages to deliver information to concerned vehicles. Safety applications as well as inherent VANET characteristics make data dissemination an essential service and a challenging task. We are developing a decentralized efficient solution for broadcast data dissemination through two game-theoretical mechanisms. Besides, VANETs can also include autonomous vehicles (AVs). AVs might represent a revolutionary new paradigm that can be a reality in our cities in the next few years. AVs do not need a driver to work; instead, they should copy a proper human behavior to adapt the driving according to the current circumstances, such as speed limit, pedestrian crossing street or wheather conditions. We will develop an AV software module including artificial intelligence (AI) techniques so that AVs can interact with the dynamic scenario throughout time. Finally, we also will include electrical vehicles (EV) in the VANET, so that special services such as finding and reserving an EV charging station place will be welcome. In addition, we are developing a multimetric geographic routing protocol for VANETs to transmit H.265 video (traffic accident, traffic state, commercial….) over VANETsPeer ReviewedPostprint (published version",'Universitat Politecnica de Valencia',Multimedia communications in vehicular adhoc networks for several applications in the smart cities,https://core.ac.uk/download/199344602.pdf,10.4995/JITEL2017.2017.6584,,core
146472430,2017-12-17T00:00:00,"With the railway transportation Industry moving actively towards automation,
accurate location and inventory of wayside track assets like traffic signals,
crossings, switches, mileposts, etc. is of extreme importance. With the new
Positive Train Control (PTC) regulation coming into effect, many railway safety
rules will be tied directly to location of assets like mileposts and signals.
Newer speed regulations will be enforced based on location of the Train with
respect to a wayside asset. Hence it is essential for the railroads to have an
accurate database of the types and locations of these assets. This paper talks
about a real-world use-case of detecting railway signals from a camera mounted
on a moving locomotive and tracking their locations. The camera is engineered
to withstand the environment factors on a moving train and provide a consistent
steady image at around 30 frames per second. Using advanced image analysis and
deep learning techniques, signals are detected in these camera images and a
database of their locations is created. Railway signals differ a lot from road
signals in terms of shapes and rules for placement with respect to track. Due
to space constraint and traffic densities in urban areas signals are not placed
on the same side of the track and multiple lines can run in parallel. Hence
there is need to associate signal detected with the track on which the train
runs. We present a method to associate the signals to the specific track they
belong to using a video feed from the front facing camera mounted on the lead
locomotive. A pipeline of track detection, region of interest selection, signal
detection has been implemented which gives an overall accuracy of 94.7% on a
route covering 150km with 247 signals",,Railway Track Specific Traffic Signal Selection Using Deep Learning,http://arxiv.org/abs/1712.06107,,,core
299986734,2017-01-01T00:00:00,"Since the advancement of technology in the area of artificial intelligence over the years, engineers had started tapping on these resources to implement in the transportation sector. Selfdriving cars and autonomous vehicles (AV) are the outcome of the implementation of Artificial Intelligence resources in the field of transportation. In addition, with the deployment of autonomous vehicles on the ground by various countries, there is an increase in popularity and acceptance by the public to use AV as a tool to increase efficiency of the traffic. In Singapore, ST engineering (STE) had taken this initiative to test on a fleet of AVs to transport passengers from one location to another in Sentosa. In this project, PTV Vissim had been selected to model the network of Sentosa, the behaviour of the visitors and the routes of the existing public transport lines using real-time data to simulate realistic situations. Furthermore, to increase the relationship between visitors and the existing Sentosa public transportation, MATLAB had been selected as the programming language to program COM which acted as a “control centre” to manage the number of passengers waiting to travel from one area to another. With both the software and simulation platform in place, the next step would be to develop an on-demand algorithm to activate fleet of AVs/buses to meet the passengers’ needs during peak period to reduce waiting time.Bachelor of Engineerin",,Optimization of transportation in a resort park,,,,core
475164299,2017-06-22T07:00:00,"Despite the impressive advancements in people detection and tracking, safety is still a key barrier to the deployment of autonomous vehicles in urban environments [1]. For example, in non-autonomous technology, there is an implicit communication between the people crossing the street and the driver to make sure they have communicated their intent to the driver. Therefore, it is crucial for the autonomous car to infer the future intent of the pedestrian quickly. We believe that human body orientation with respect to the camera can help the intelligent unit of the car to anticipate the future movement of the pedestrians. To further improve the safety of pedestrians, it is important to recognize whether they are distracted, carrying a baby, or pushing a shopping cart. Therefore, estimating the fine- grained 3D pose, i.e. (x,y,z)-coordinates of the body joints provides additional information for decision-making units of driverless cars.
In this dissertation, we have proposed a deep learning-based solution to classify the categorized body orientation in still images. We have also proposed an efficient framework based on our body orientation classification scheme to estimate human 3D pose in monocular RGB images.
Furthermore, we have utilized the dynamics of human motion to infer the body orientation in image sequences. To achieve this, we employ a recurrent neural network model to estimate continuous body orientation from the trajectories of body joints in the image plane.
The proposed body orientation and 3D pose estimation framework are tested on the largest 3D pose estimation benchmark, Human3.6m (both in still images and video), and we have proved the efficacy of our approach by benchmarking it against the state-of-the-art approaches.
Another critical feature of self-driving car is to avoid an obstacle. In the current prototypes the car either stops or changes its lane even if it causes other traffic disruptions. However, there are situations when it is preferable to collide with the object, for example a foam box, rather than take an action that could result in a much more serious accident than collision with the object. In this dissertation, for the first time, we have presented a novel method to discriminate between physical properties of these types of objects such as bounciness, elasticity, etc. based on their motion characteristics . The proposed algorithm is tested on synthetic data, and, as a proof of concept, its effectiveness on a limited set of real-world data is demonstrated",Scholar Commons,Estimation of Human Poses Categories and Physical Object Properties from Motion Trajectories,,,,core
55284815,2016-06-24T00:00:00,"Cities are the nucleus for creativity and ideas, as it has all the potentials for people to work, explore and live. People always come to cities because they want to be part of something, this magnet in the cities created the problem of population (Ericsson: Thinking Cities in the Networked Society, 2012). Approximately 50% of world’s population lives in urban areas, a number which is expected to increase to nearly 60% by 2030. (Mutizwa-Mangiza ND, Arimah B C, Jensen I, Yemeru EA, Kinyanjui MK, 2011). 



According to the rapid change in cities’ population there exists a need to utilize intelligent prediction tools to deliver a better way of living. Smart cities provide an opportunity to connect people and places using innovative technologies that help in better city planning and management ( Khan, Anjum, Soomro, & Tahir, 2015).



Data is never a new thing, but data sources are always in change. The internet made everything easier and more reachable. This wide range of technologies such as IOT (internet of things) and M2M (machine to machine) (Gartner, 2015), is believed to offer a new potential to deliver an analytical framework for urban optimization. The real value of such data is gained by new knowledge acquired by performing data analytics using various data mining, machine learning or statistical methods. 



According to this technologically mutated, data comes from weather channels, street security cameras, Facebook, Twitter, sensor networks, in-car devices, location-based smartphone apps, RFID tags, smart meters, among other sources (Hinssen, 2012). This massive amount of information that comes from real-time based tools, made the world in front of a new era of data called ‘Big Data’. However, turning an ocean of messy data into knowledge and wisdom is an extremely challenging task.



The proposed paper will discuss the IOT developed frameworks which are used to improve cities infrastructure and their vital systems. Analyzing these frameworks will help developing a conceptual proposal of data visualizing software; with the aim of helping urban planners get a better and easier way to comprehend the usage of multi-data sources for city planning and management.



The full control of data is an open challenge, however proposing the fundamental bases of framework with the ability to extend and having an application layer above would be very helpful for urban process shifting. The Egyptian case is our main scope to have a smarter city that provides an opportunity to connect people and places using innovative technologies",CORP – Competence Center of Urban and Regional Planning,The influences of user generated ‘Big data’ on urban development,https://core.ac.uk/download/55284815.pdf,,,core
88565588,2017-01-01T00:00:00Z,"Outdoor air pollution costs millions of premature deaths annually, mostly due to anthropogenic fine particulate matter (or PM2.5). Quito, the capital city of Ecuador, is no exception in exceeding the healthy levels of pollution. In addition to the impact of urbanization, motorization, and rapid population growth, particulate pollution is modulated by meteorological factors and geophysical characteristics, which complicate the implementation of the most advanced models of weather forecast. Thus, this paper proposes a machine learning approach based on six years of meteorological and pollution data analyses to predict the concentrations of PM2.5 from wind (speed and direction) and precipitation levels. The results of the classification model show a high reliability in the classification of low (<10 µg/m3) versus high (>25 µg/m3) and low (<10 µg/m3) versus moderate (10–25 µg/m3) concentrations of PM2.5. A regression analysis suggests a better prediction of PM2.5 when the climatic conditions are getting more extreme (strong winds or high levels of precipitation). The high correlation between estimated and real data for a time series analysis during the wet season confirms this finding. The study demonstrates that the use of statistical models based on machine learning is relevant to predict PM2.5 concentrations from meteorological data",Hindawi Limited,Modeling PM2.5 Urban Pollution Using Machine Learning and Selected Meteorological Parameters,,10.1155/2017/5106045,"[{'title': None, 'identifiers': ['2090-0155', 'issn:2090-0147', '2090-0147', 'issn:2090-0155']}]",core
201576170,2017-12-01T00:00:00,"Transportation planning and solutions have an enormous impact on city life. To minimize the transport duration, urban planners should understand and elaborate the mobility of a city. Thus, researchers look toward monitoring people’s daily activities including transportation types and duration by taking advantage of individual’s smartphones. This paper introduces a novel segment-based transport mode detection architecture in order to improve the results of traditional classification algorithms in the literature. The proposed post-processing algorithm, namely the Healing algorithm, aims to correct the misclassification results of machine learning-based solutions. Our real-life test results show that the Healing algorithm could achieve up to 40% improvement of the classification results. As a result, the implemented mobile application could predict eight classes including stationary, walking, car, bus, tram, train, metro and ferry with a success rate of 95% thanks to the proposed multi-tier architecture and Healing algorithm",'MDPI AG',A Novel Segment-Based Approach for Improving Classification Performance of Transport Mode Detection,,10.3390/s18010087,"[{'title': 'Sensors', 'identifiers': ['issn:1424-8220', '1424-8220']}]",core
161678999,2017,"This paper presents the development and implementation of a theoretical mathematical-statistical framework for sequential updating of the grade control model, based on a support vector machine learning algorithm. Utilising the Zambujal orebody within the Neves-Corvo Cu deposit in Portugal, parameters that can be measured in real time, used in visualisation, modelled for resource estimation, and used for process control visualisation and optimisation are considered.
The methodology broadly comprises of three steps. Firstly, the provided dataset is used to develop a virtual asset model (VAM) representing the true 3D grade distribution in order to simulate the mining method. Then ore quality parameters are established simulating real time monitoring sensor installation at: (a) stope development and rock face monitoring (face imaging and drillholes); and (b) transport monitoring (muck pile, LHD/scooptram). Next, the acquired data was assimilated into the models as part of the sequential model update.
Two different mining methods and the monitoring information that can be acquired during the ore extraction are analysed: (a) drift and fill mining and (b) bench and fill mining, which are widely implemented at the Neves-Corvo mine. Selected study zones were chosen such as to contrast mining through the high/low grade zones with different degrees of heterogeneity, which demonstrate the performance of resource estimation and classification models developed in heterogeneous mining stopes.
The grade accuracy and error in the resource model, and high/low grade ore classification accuracy and error are evaluated as performance metrics for the proposed methods.
In drift and fill mining, drillhole and face sampling data collection was simulated in a real-time manner and fed into the support vector machine (SVM) regressor to update the resource estimation model in both a high grade and low grade drift scenarios. In each scenario, six drift and fill mining steps were simulated sequentially and the posterior resource models, after integrating real time mining data, have shown significant improvement of bias correction in both updating planned resources and reconciling extracted ore.
In bench and fill mining, grade classification based on random sampling data from muck pile was demonstrated, considering scoop by scoop derived monitoring data. Three different classifiers (mean, median, and Bayesian) were tested and shown very good performance. In the case study presented here, a sequence of 15 blasting steps was simulated with each step requiring 112 scooping operations to transport the blasted ore. Using the real time monitored information, it was shown that at each blasting step over 85% of the scoops can be labelled correctly using the proposed methods and with an accuracy of over 95%",TU Bergakademie Freiberg,Development of support vector machine learning algorithm for real time update of resource estimation and grade classification,,,,core
158781578,2016-01-01T00:00:00,"Recent advances in telecommunications created new opportunities for monitoring public transport operations in real-time. This paper presents an automatic control framework to mitigate the Bus Bunching phenomenon in real-time. The framework depicts a powerful combination of distinct Machine Learning principles and methods to extract valuable information from raw location-based data. State-of-the-art tools and methodologies such as Regression Analysis, Probabilistic Reasoning and Perceptron's learning with Stochastic Gradient Descent constitute building blocks of this predictive methodology. The prediction's output is then used to select and deploy a corrective action to automatically prevent Bus Bunching. The performance of the proposed method is evaluated using data collected from 18 bus routes in Porto, Portugal over a period of one year. Simulation results demonstrate that the proposed method can potentially reduce bunching by 68% and decrease average passenger waiting times by 4.5%, without prolonging in-vehicle times. The proposed system could be embedded in a decision support system to improve control room operations",'Elsevier BV',An Online Learning Approach to Eliminate Bus Bunching in Real-time,,10.1016/j.asoc.2016.06.031,,core
201690721,2017-06-01T00:00:00,"Abstract Background Respiratory syncytial virus (RSV) is responsible for severe respiratory infections and higher costs in medical care. The two aims of this work were to assess the performances of SOFIA®RSV tests in “real-life-laboratory” conditions (study 1) and implemented at point-of-care testing in a pediatric emergency department (ED, study 2), during two consecutive winter seasons. Methods In study 1, fresh nasopharyngeal swabs from patients of all ages were sampled in 1.5 ml of Universal virological Transport Medium (UTM) and prospectively tested using SOFIA®RSV tests. In study 2, conducted in a pediatric ED, nasopharyngeal swabs were placed in 3 ml of UTM. All SOFIA®RSV tests were confirmed by molecular testing, considered as reference method. The epidemiological and clinical features of tested patients, as well as the care of these patients after obtaining quick results were evaluated. Results The sensitivities of SOFIA®RSV in infants (aged under 24 months) performed in the laboratory and in the pediatric ED were respectively 95% (95% CI: 86.8–98.1) and 74.8% (95% CI: 68.0–80.9) compared to PCR. In study 1, the sensitivity among children (from 2 to 15 years old) and adults (above 15 years old) dropped to 45% (95% CI: 23.1–68.5) and 59% (95% CI: 32.9–81.6), respectively. In study 2, there were some differences in bed-management of SOFIA®RSV positive compared to SOFIA®RSV negative infants. Conclusions SOFIA®RSV tests performed in the laboratory and in the pediatric ED show high and satisfactory sensitivities among young children under 24 months, which supports its robustness and reliability. However, the impact of these tests on patient care at point-of-care cannot be clearly assessed when considering the limits of the study 2 design",'Springer Science and Business Media LLC',SOFIA®RSV: prospective laboratory evaluation and implementation of a rapid diagnostic test in a pediatric emergency ward,,10.1186/s12879-017-2557-8,"[{'title': 'BMC Infectious Diseases', 'identifiers': ['issn:1471-2334', '1471-2334']}]",core
151207769,2017-01-01T00:00:00,"La sécurité de plusieurs systèmes électriques est fortement dépendante de la fiabilité de leur
bloc-batterie à base de piles aux ions lithium (Li-ion). Par conséquent, ces batteries doivent
être suivis et contrôlés par un système de gestion des batteries (BMS). Le BMS interagit
avec toutes les composantes du bloc-batterie de façon à maintenir leur intégrité. La
principale composante d’un BMS est un modèle représentant le comportement des piles Liion et capable de prédire ses différents points d’opération. Dans les industries de
l’électronique et de l’automobile, le BMS repose habituellement sur des modèles empiriques
simples. Ceux-ci ne sont cependant pas capables de prédire les paramètres de la batterie
lorsqu’elle vieillit. De plus, ils ne sont applicables que pour des piles spécifiques. D’un autre
côté, les modèles électrochimiques sont plus sophistiqués et plus précis puisqu’ils sont basés
sur la résolution des équations de transport et de cinétique électrochimique. Ils peuvent être
utilisés pour simuler les caractéristiques et les réactions à l’intérieur des piles aux ions
lithium.
Pour résoudre les équations des modèles électrochimiques, il faut connaître les différents
paramètres électrochimiques et thermo-physiques de la pile. Les variables les plus
significatives des piles Li-ion peuvent être divisées en 3 catégories : les paramètres
géométriques, ceux définissant les matériaux et les paramètres d’opération. Les paramètres
géométriques et de matériaux peuvent être facilement obtenus à partir de mesures directes
ou à partir des spécifications du manufacturier. Par contre, les paramètres d’opération ne
sont pas faciles à identifier. De plus, certains d’entre eux peuvent dépendre de la technique
de mesure utilisée et de l’âge. Finalement, la mesure de certains paramètres requiert le
démantèlement de la pile, une procédure risquée et destructive.
Plusieurs recherches ont été réalisées afin d’identifier les paramètres opérationnels des piles
aux ions lithium. Toutefois, la plupart de ces études ont porté sur l’estimation d’un nombre
limité de paramètres et se sont attardées sur un seul type de matériau pour l’électrode positive
utilisé dans la fabrication des piles Li-ion. De plus, le couplage qui existe entre les
paramètres électrochimiques et thermo-physiques est complètement ignoré. Le but principal
de cette thèse est de développer une méthode générale pour identifier simultanément
différents paramètres électrochimiques et thermo-physiques et de prédire la performance des
piles Li-ion à base de différents matériaux d’électrodes positives. Pour atteindre ce but, une
méthode inverse efficace a été introduite. Des modèles directs représentatifs des piles Li-ion
à base de différents matériaux d’électrodes positives ont également été développés. Un modèle rapide et précis simulant la performance de piles Li-ion avec des électrodes positives
à base de LiMn2O4 ou de LiCoO2 est présenté. Également, deux modèles ont été développés
pour prédire la performance des piles Li-ion avec une électrode positive de LiFePO4. Le
premier, appelé modèle mosaïque modifié (MM), est basé sur une approche macroscopique
alors que le deuxième, appelé le modèle mésoscopique, est plutôt basé sur une approche
microscopique. Des études d’estimation de paramètres ont été conduites en utilisant les
modèles développés et des données expérimentales fournies par Hydro-Québec. Tous les
paramètres électrochimiques et thermo-physiques des piles Li-ions ont été simultanément
identifiés et appliqués à la prédiction de la performance des piles. Finalement, une technique
en temps réel reposant sur des réseaux de neurones est introduite dans la méthode
d’estimation des paramètres intrinsèques au piles Li-ion.Abstract : The safety of many electrical systems is strongly dependent on the reliable operation of their
 lithium-ion (Li-ion) battery packs. As a result, the battery packs must be monitored by a
 battery management system (BMS). The BMS interacts with all the components of the system
 so as to maintain the integrity of the batteries. The main part of a BMS is a Li-ion battery
 model that simulates and predicts its different operating points. In the electronics and in the
 automobile industries, the BMS usually rests on simple empirical models. They are however
 unable to predict the battery parameters as it ages. Furthermore, they are only applicable to a
 specific cell. Electrochemical-based models are, on the other hand, more sophisticated and
 more precise. These models are based on chemical/electrochemical kinetics and transport
 equations. They may be used to simulate the Li-ion battery characteristics and reactions.
 In order to run the electrochemical-based mathematical models, it is imperative to know the
 different electrochemical and thermophysical parameters of the battery. The significant
 variables of the Li-ion battery can be classified into three groups: geometric, material and
 operational parameters. The geometric and material parameters can be easily obtained from
 direct measurements or from the datasheets provided by the manufacturer. The operational
 properties are, on the other hand, not easily available. Furthermore, some of them may vary
 according to the measurement techniques or the battery age. Sometimes, the measurement of
 these parameters requires the dismantling of the battery itself, which is a risky and destructive
 procedure.
 Many investigations have been conducted to identify the operational parameters of Li-ion
 batteries. However, most of these studies focused on the estimation of limited parameters, or
 considered only one type of the positive electrode materials used in Li-ion batteries. Moreover,
 the coupling of the thermophysical parameters to the electrochemical variables is ignored in
 all of them. The main goal of this thesis is to develop a general method to simultaneously
 identify different electrochemical and thermophysical parameters and to predict the
 performance of Li-ion batteries with different positive electrode materials. To achieve this
 goal, an effective inverse method is introduced. Also, direct models representative of Li-ion
 batteries are developed, applicable for all of the positive electrode materials. A fast and
 accurate model is presented for simulating the performance of the Li-ion batteries with the
 LiMn2O4 and LiCoO2 positive electrodes. Moreover, two macro- and micro-based models are
 developed for predicting the performance of Li-ion battery with the LiFePO4 positive
 electrode, namely the Modified Mosaic (MM) and the mesoscopic-based models. The
 parameter estimation studies are then implemented by means of the developed direct models
 and experimental data provided by Hydro-Québec. All electrochemical and thermophysical
 parameters of the Li-ion batteries are simultaneously identified and applied for the prediction
 of the battery performance. Finally, a real-time technique resting on neural networks is used
 for the estimation of the Li-ion batteries intrinsic parameters",'Universite de Sherbrooke',Méthode inverse pour estimer les paramètres électrochimiques et thermophysiques des batteries aux ions lithium composées de différents matériaux pour l’électrode positive,https://core.ac.uk/download/151207769.pdf,,,core
88584388,2016-05-01T00:00:00Z,"Background : Molecular diagnostic methods are among major tools in management of hepatitis C virus (HCV) in infected patients. Many studies have shown that viral load is associated with stage of infection and response to treatment. Therefore, the evaluation and quantification of viral load is very important. The goal of this study is implementation of inexpensive, yet accurate method for quantitative assessment of viral load in plasma samples of infected patients.

Materials and Methods: After development and validation of the assay, quantification of HCV RNA on 200 chronic patients the start of therapy was performed using an In-House Real-time PCR assay. Measuring the concentration of viral RNA was performed using an external standard curve. It should be noted that the validation and standardization of all procedures in this study were performed using RNA standard panel. The results of this method were compared with results obtained from Artus commercial kit.

Results: Detection limit of the assay was 50 IU/ml. The mean viral load measured on a logarithmic scale (5/81&plusmn; 0/22, p<0/05). Parallel analysis of samples with commercial kit showed that there is a good correlation between these two methods (R2 = 0.988 p< 0.05).

Conclusion: Viral load detection of HCV was reported for the first time in Khorramabad city.&nbsp; According to the results, this method has a good sensitivity and specificity for HCV quantification in large-scale. It can be a good replacement for commercial kits especially for clinical evaluation of therapy",Lorestan University of Medical Science,The quantification of hepatitis C viral load using an In-House Real-Time PCR assay in HCV infected patients in Khorramabad city,,,"[{'title': None, 'identifiers': ['1563-0773', 'issn:1563-0773']}]",core
89127865,2017-01-01T00:00:00Z,"This paper presents some of the results of a project that aimed at the design and implementation of a system for the spatial mapping and forecasting the temporal evolution of air pollution from dust transport from the Sahara Desert into the eastern Mediterranean and secondarily from anthropogenic sources, focusing over Cyprus. Monitoring air pollution (aerosols) in near real-time is accomplished by using spaceborne and in situ platforms. The results of the development of a system for forecasting pollution levels in terms of particulate matter concentrations are presented. The aim of the present study is to utilize the recorded PM10 (particulate matter with aerodynamic diameter less than 10 μm) ground measurements, Aerosol Optical Depth retrievals from satellite, and the prevailing synoptic conditions established by Artificial Neural Networks, in order to develop regression models that will be able to predict the spatial and temporal variability of PM10 in Cyprus. The core of the forecasting system comprises an appropriately designed neural classification system which clusters synoptic maps, Aerosol Optical Depth data from the Aqua satellite, and ground measurements of particulate matter. By exploiting the above resources, statistical models for forecasting pollution levels were developed",Hindawi Limited,"Monitoring and Forecasting Air Pollution Levels by Exploiting Satellite, Ground-Based, and Synoptic Data, Elaborated with Regression Models",,10.1155/2017/2954010,"[{'title': None, 'identifiers': ['issn:1687-9309', '1687-9309', 'issn:1687-9317', '1687-9317']}]",core
90397160,2016-01-01T00:00:00Z,"The Liane River is a small costal river, famous for its floods, which can affect the city of Boulogne-sur-Mer. Due to the complexity of land cover and hydrologic processes, a black-box non-linear modelling was chosen using neural networks. The multilayer perceptron model, known for its property of universal approximation is thus chosen. Four models were designed, each one for one forecasting horizon using rainfall forecasts: 24h, 12h, 6h, 3h. The desired output of the model is original: it represents the maximal value of the water level respectively 24h, 12h, 6h, 3h ahead. Working with best forecasts of rain (the observed ones during the event in the past), on the major flood of the database in test set, the model provides excellent forecasts. Nash criteria calculated for the four lead times are 0.98 (3h), 0.97 (6h), 0.91 (12h), 0.89 (24h). Designed models were thus estimated as efficient enough to be implemented in a specific tool devoted to real time operational use. The software tool is described hereafter: designed in Java, it presents a friendly interface allowing applying various scenarios of future rainfalls, and a graphical visualization of the predicted maximum water levels and their associated real time observed values",EDP Sciences,Neural networks-based operational prototype for flash flood forecasting: application to Liane flash floods (France),,10.1051/e3sconf/20160718025,"[{'title': None, 'identifiers': ['issn:2267-1242', '2267-1242']}]",core
127587458,2017-01-01T00:00:00,"Il ruolo della cultura tecnologica del progetto rispetto alle tre grandi sfide poste dalle questioni ambientali ed energetiche del nostro tempo: cambiamenti climatici, limitatezza delle risorse e eccessivo consumo di energia, è centrale e imprescindibile per affrontare consapevolmente la ricerca di una rinnovata dimensione delle condizioni dell'Abitare che nei differenti contesti europei e mondiali porti con sé i concetti - diversi terminologicamente ma affini nelle ampie accezioni che essi racchiudono e negli obiettivi che sottendono - di Sustainable Development, Nachhaltige Entwicklung, Dévéloppement Durable.

Nelle tre differenti aggettivazioni è racchiuso uno dei mandati della Tecnologia e della relativa Cultura tecnologica progettuale da sviluppare nel prossimo futuro: la richiesta di sostenibilità del fare umano per le generazioni future (Sustainable), di solidità e affidabilità dei comportamenti e delle prestazioni (Nachhaltige), di durabilità nel tempo dei prodotti delle trasformazioni (Durable).

L'altro grande mandato della Cultura tecnologica nella nostra epoca è quello di offrire non solo risposte 'dinamiche' nella dimensione temporale di medio-lungo termine alle crescenti esigenze di sostenibilità/affidabilità/durabilità, ma anche risposte 'dinamiche' nello spazio reale e nel tempo presente e di breve termine supportando l'Architettura nella sua altrettanto assoluta necessità di essere 'adattiva' e 'resiliente' ai cambiamenti già in atto sul piano climatico e ambientale.

Cultura tecnologica significa dunque profonda consapevolezza degli obiettivi da perseguire progettualmente, indissolubilmente legata ad un'attitudine alla visione sistemica dei problemi, ad un'impostazione metodologica delle strategie da tracciare e promuovere, ad un'intima sapienza degli aspetti di fattibilità e realizzabilità delle azioni da sperimentare, monitorare, consolidare nel tempo.

D'altra parte tutti i maggiori centri di ricerca e sperimentazione dell'area dell'Architettura nei Paesi avanzati pongono quale nodo focale dei processi di concezione, progettazione e realizzazione di qualsiasi tipo di intervento trasformativo delle nostre realtà quello caratterizzato dall'approccio tecnologico, in cui nei diversi contesti la Architectural Technology, la Baukonstruktion, la Technologie de l'Architecture, la Construcción en Arquitectura non rappresentano solo un ambito disciplinare (pealtro da sempre a vocazione fortemente interdisciplinare) ma, di più, la dimensione logica e culturale nella quale si coordinano e ruotano le complesse declinazioni e i differenti caratteri del progetto.

Nel rinvenire gli elementi di innovazione propri della Cultura tecnologica nell'approcciare e sviluppare ricerca e sperimentazione sui temi dell'ambiente e dell'energia vi è in primis la capacità di perseguire al contempo tre categorie di obiettivi ""alti"" prestazionali: efficienza delle azioni nel controllarne rendimenti e risultati; efficacia complessiva delle strategie nel verificarne il rapporto tra efficienza conseguita e quantità di risorse coinvolte e impiegate nei processi per raggiungere quei risultati; soddisfazione degli utenti nel vivere quelle condizioni di efficienza e nel percepire o addirittura esser coinvolti in quelle dimensioni di efficacia.

La concezione innovativa di scenario che è aperta dall'evoluzione del significato di efficacia pone in primo piano la questione della 'scarsità di risorse' e tiene in massima considerazione -fino al punto di farla sua nei recenti sviluppi a più ancora in quelli a venire nel prossimo futuro - l'accezione di 'Economia circolare' (asse portante della visione della Green Economy) che, secondo il recentissimo Piano di Azione Globale della Comunità Europea, attiene ad un sistema complesso in cui ""il valore dei prodotti, dei materiali, dell'energia e delle risorse è mantenuto quanto più a lungo possibile e la produzione di emissioni, inquinamento, scarti e rifiuti è ridotta al massimo"" (European Commission, 2017) e in cui, secondo il report della Agenzia Europea per l'Ambiente, centrale è ""contrastare il depauperamento delle risorse naturali, re-immettere nel mercato le risorse in dismissione ed agevolare il recupero delle risorse di valore"" (European Environment Agency, 2016).

In questo senso protagonista assoluto di tale innovazione è quell'approccio ecosistemico ai problemi, alle questioni in gioco e alle strategie ed azioni per risolverli, che fa fondamentalmente riferimento alla visione di tipo 'Life Cycle' strutturante il senso stesso di 'sostenibilità' di uno sviluppo capace di sostanziare la 'circolarità' del sistema economico, la 'mitigabilità' della crisi climatica, l''efficientabilità' della questione energetica, la 'capitalizzabilità' del patrimonio naturale, la 'inclusività' del benessere e la 'rigenerabilità' delle città; e di includere il concetto di 'Costo ambientale' in tutte le strategie e azioni da questi assi sottese.The role of technological culture in planning, seen in light of the three major challenges raised by the environmental and energy-related issues of our time, meaning climate change, limited resources and excessive energy consumption, is a key role, and one that we ignore at our peril when it comes to seeking out, in informed fashion, a renewal in the conditions of inhabiting the contexts of Europe and the rest of the world, a quest that necessarily brings into play concepts which differ with respect to their terminology, but yet prove quite similar when it comes to the board range of concepts covered and the wealth of their underlying objectives: Sustainable Development, Nachhaltige Entwicklung and Dévéloppement Durable.

Ensconced within each of these descriptors is one of the vital tasks that technology and the related cultural of technology must fulfil in the near future: responding to the request for sustainability in human  endeavours in future generations (Sustainable), as well as for integrity and reliability in terms of conduct and performance (Nachhaltige), together with the durability over time of the products of transformations (Durable).

The other major task of cultural technology in our time is to provide responses that prove 'dynamic' not only over the medium-long term, with respect to meeting the growing need for sustainability/reliability /durability, but that are also 'dynamic' within real space and at the present time, in the short term, supporting architecture’s equally absolute need to be 'adaptable' and 'resilient' in response to changes currently affecting the climate and the environment.

Technological culture, therefore, means possessing a thoroughgoing knowledge of the objectives to be pursued through planning, irrevocably linked to a leaning towards a systemic vision of issues, as well as a methodological approach to the strategies to be outline and promoted, plus an intimate familiarity with the considerations of feasibility and practicality tied to the initiatives to be tested, monitored and built up over time.

For that matter, all the major centres of research and experimentation in the architectural sectors of the developed countries posit the technological approach as a vital factor in the processes of conceptualisation, planning and implementation of any type of initiative meant to transform our existing realties, with the result that the various contexts of Architectural Technology, Baukonstruktion, Technologie de l'Architecture and Construcción en Arquitectura represent not only a disciplinary realm (albeit one with a marked interdisciplinary bent) but, even more to the point, the logical and cultural dimension within which the various manifestations and characteristics of planning are coordinated and brought into focus.

In identifying the innovative features displayed by technological culture as it addresses and develops research and experimentation on environmental and energy-related topics, the first observation to be made is its ability to simultaneously pursue three different categories of ""advanced"" objectives involving performance: the efficiency of initiatives in terms of controlling yields and results; the overall effectiveness of strategies in light of assessments of the ratio between the efficiency achieved and the quantities of resources involved and utilised in the processes enacted to reach those results; the satisfaction of the users of those conditions of efficiency as they perceive, or even become involved in, the new dimensions of effectiveness.

The innovative approach to conceptualisation of scenarios brought about by the evolution in the meaning of effectiveness highlights the issue of 'scarce resources' while placing the utmost importance – to the point of absorbing it in recent developments, a trend that shall become even more pronounced in the near future – on the framework of the 'Circular Economy' outlook (a cornerstone of the vision of a Green Economy), an outlook that, according to the recent Global Action Plan of the European  Community, points to a complex system in which, ""The value of products, materials, energy and resources is maintained for as long as possible, while the production of emissions, pollution, scrap and waste is reduced to the greatest possible extent"" (European Commission, 2017), with key importance placed on, ""Contrasting the impoverishment of natural resources, reintroducing discarded resources onto the market and facilitating the recovery of resources of value "" (European Environment Agency, 2016).

Seen in this light, the key ingredient to a similar innovation is the eco-systemic approach taken to the problems and issues addressed, as well as to the strategies and initiatives enacted to resolve them, an outlook that essentially consists of a 'Life Cycle' vision able to provide the structure for the fundamental 'sustainability' of a mode of development that succeeds in supporting the 'circularity' of the economic system, together with the potential for mitigating the climate crisis, for increasing energy efficiency, for capitalising on natural resources, for making wellbeing more inclusive while regenerating cities, with the concept of 'environmental cost' becoming an integral part of all the strategies and actions underlying such efforts",'Firenze University Press',"Cultura tecnologica, ambiente, energia: prospettive della ricerca e della sperimentazione | Technological Culture, the Environment and Energy: the outlook for research and experimentation",,,,core
147686560,2017,"Goals

Over the past decade there has been a growing public fascination with the complex ""connectedness"" of modern society. This connectedness is found in many contexts: in the rapid growth of the Internet and the Web, in the ease with which global communication now takes place, and in the ability of news and information as well as epidemics and financial crises to spread around the world with surprising speed and intensity. These are phenomena that involve networks and the aggregate behavior of groups of people; they are based on the links that connect us and the ways in which each of our decisions can have subtle consequences for the outcomes of everyone else.

This crash course is an introduction to the analysis of complex networks, made possible by the availability of big data, with a special focus on the social network and its structure and function. Drawing on ideas from computing and information science, complex systems, mathematic and statistical modelling, economics and sociology, this lecture sketchily describes the emerging field of study that is growing at the interface of all these areas, addressing fundamental questions about how the social, economic, and technological worlds are connected.

Syllabus

•Big graph data and social, information, biological and technological networks

•The architecture of complexity and how real networks differ from random networks: node degree and long tails, social distance and small worlds, clustering and triadic closure. Comparing real networks and random graphs. The main models of network science: small world and preferential attachment.

•Strong and weak ties, community structure and long-range bridges. Robustness of networks to failures and attacks. Cascades and spreading. Network models for diffusion and epidemics. The strength of weak ties for the diffusion of information. The strength of strong ties for the diffusion of innovation.

•Practical network analytics with Cytoscape and Gephi. Simulation of network processes with NetLogo.

Reference Textbooks

David Easley, Jon Kleinberg: Networks, Crowds, and Markets (2010)

http://www.cs.cornell.edu/home/kleinber/networks-book/

Albert-Laszlo Barabasi. Network Science (2016)

http://barabasi.com/book/network-science

Network Analytics Software

Visual Analytics: Cytoscape http://www.cytoscape.org/

Gephi http://gephi.github.io/

Network Simulation: NetLogo

https://ccl.northwestern.edu/netlogo/

Data science ethics & privacy-preserving analytics

Data science created unprecedented opportunities but also new risks. Data science techniques might expose sensitive traits of individuals and invade their privacy, this information could be used to discriminate people based on their presumed characteristics, or profiles. Sophisticated data driven machine learning algorithms yield classification and prediction models of behavioral traits of individuals, such as credit score, insurance risk, health status, personal preferences and orientations, on the basis of personal data disseminated in the digital environment by users, with or

sometimes without their awareness. Such automated decision-making systems are often ""black boxes"", mapping user's features into a class label or a ranking value without exposing the reasons .

This is worrying not only for the lack of transparency, which undermines the trust of stakeholders, but also for possible social biases and prejudices hidden in the training data and learned by the algorithms, which may bring to discriminatory decisions or unfair actions. Gartner says that, within 2018, half of business ethics violations will occur through improper use of Big Data analytics .

Often, the achievements of data science are the result of re-interpreting available data for analysis goals that differ from the original reasons motivating data collection. Examples include mobile phone call records, originally collected by telecom operators for billing and operations, used for accurate and timely demography and human mobility analysis at country orregional scale. This re-purposing of data clearly shows the importance of legal compliance and data ethics technologies and safeguards to protect privacy and anonymity, secure data, engage users, avoid discrimination and misuse, account for transparency and fair use - to the purpose of seizing the opportunities of data science while controlling the associated risks. This is the focus of my lecture.

Syllabus

• Fairness, Accountability, Confidentiality, Accuracy: the ethical challenges of data science • Privacy-preserving data mining • Privacy-by-design and data-driven risk assessment • Democratizing data science: centralised vs. user-centric analytics • Personal data analytics, collective awareness • Algorithmic bias and ethical challenges of machine learning • Discrimination-aware data minin",'Association for Computing Machinery (ACM)',"Social network analytics, data science ethics & privacy-preserving analytics",,10.1145/3168836.3168841,,core
156876048,2017-10-04T00:00:00,"Public transport users are increasingly expecting better service and up to date information, in pursuit of a seamless journey experience. In order to meet these expectations, many transport operators are already offering free mobile apps to help customers better plan their journeys and access real-time travel information. Leveraging the spatio-temporal data that such apps can produce at scale (i.e. timestamped GPS traces), opens an opportunity to bridge the gap between passenger expectations and capabilities of the operators by providing a real-time 360-degree view of the transport network based on the ‘Apps as infrastructure’ paradigm. The first step towards fulfilling this vision is to understand which routes and services the passengers are travelling on at any given time. Mapping a GPS trace onto a particular transport network is known as ‘network matching’. In this paper, the problem is formulated as a supervised sequence classification task, where sequences are made of geographic coordinates, time, and line and direction of travel as a label. We present and compare two data-driven approaches to this problem: (i) a heuristic algorithm, which looks for nearby stops and makes an estimation based on their timetables -- used as a baseline -- and (ii) a deep learning approach using a recurrent neural network (RNN). Since RNNs require considerable amounts of data to train a good model, and collecting and labelling this data from real users is a challenging task (e.g. asking too often can be overwhelming; privacy concerns on providing GPS location; not reliable labels due to mistakes or misuse), one of our contributions is a synthetic journey data generator. The datasets that we generated have been made as realistic as possible by querying real timetables and adding position and temporal noise to simulate variable GPS accuracy and vehicle delays, sampled from empirical distributions estimated using thousands of real location reports. To validate our approach we have used a separate dataset made of hundreds of real user journeys provided by a UK-based bus operator. Our experimental results are promising and our next step is to deploy a solution in a production environment. From the operator’s point of view, this will enable multiple smart applications like account based ticketing, identification of disruptions, real-time passenger counting, and network analysis. Passengers will also, therefore, benefit from a better service and an increase in the quality of information due to leveraging such big data processing",,Automatic Transport Network Matching Using Deep Learning,https://core.ac.uk/download/156876048.pdf,,,core
89199363,2016-10-01T00:00:00Z,"Purpose.The article is aimed to develop the means and methods of forming a plurality of real and potential structural diagrams for zones of energy recovery and different locations of trains for further training neuro-fuzzy networks on the basis of expert solutions and also for the formation of good control. Methodology. Methodology of mathematical and algorithmic constructivism for modeling the structural diagrams of the electric supply system and modes of traction power consumption and the train’s locations in zones of energy recovery was applied. This approach involves the development of constructive-synthesizing structures (CSS) with transformation by specialization, interpretation, specification and implementation. Development CSS provides an extensible definition media, relations and the signature of operations and constructive axiomatic. The most complex and essential part of the axioms is the set formed by the substitution rules defining the process of withdrawal of the corresponding structures. Findings. A specialized and specified CSS, which allows considering all the possibilities and features, that supply power traction systems with modern equipment, stations and trains location was designed. Its feature: the semantic content of the terminal alphabet images of electrical traction network and power consumers with relevant attributes. A special case of the formation of the structural diagram shows the possibilities CSS in relation to this problem. Originality. A new approach to solving the problem of rational use of energy recovery, which consists in application of the methods and means of artificial neural networks, expert systems, fuzzy logic and mathematical and algorithmic constructivism. This paper presents the methods of constructive simulation of a production-distribution of energy recovery zone structure in the system of the DC traction. Practical value. The tasks decision of the rational use of energy recovery can significantly save energy, contribute to the technical re-equipment of a railway transportation of Ukraine through the introduction of modern means and capabilities. The developed model can be used to solve other energy-saving tasks in different systems of electric transport",Dnipropetrovsk National University of Railway Transport named after Academician V. Lazaryan,CONSTRUCTIVE MODELLING FOR ZONE OF RECOVERY ENERGY DISTRIBUTION OF DC TRACTION,,,"[{'title': None, 'identifiers': ['2307-3489', '2307-6666', 'issn:2307-3489', 'issn:2307-6666']}]",core
158593422,2016-01-01T00:00:00,"Recent advances in telecommunications created new opportunities for monitoring public transport operations in real-time. This paper presents an automatic control framework to mitigate the Bus Bunching phenomenon in real-time. The framework depicts a powerful combination of distinct Machine Learning principles and methods to extract valuable information from raw location-based data. State-of-the-art tools and methodologies such as Regression Analysis, Probabilistic Reasoning and Perceptron's learning with Stochastic Gradient Descent constitute building blocks of this predictive methodology. The prediction's output is then used to select and deploy a corrective action to automatically prevent Bus Bunching. The performance of the proposed method is evaluated using data collected from 18 bus routes in Porto, Portugal over a period of one year. Simulation results demonstrate that the proposed method can potentially reduce bunching by 68% and decrease average passenger waiting times by 4.5%, without prolonging in-vehicle times. The proposed system could be embedded in a decision support system to improve control room operations.Transport and Plannin",'Elsevier BV',An Online Learning Approach to Eliminate Bus Bunching in Real-time,,10.1016/j.asoc.2016.06.031,,core
227378041,2017-09-17T18:45:17.153+02:00,"This research data file contains the necessary software and the dataset for estimating the missing prices of house units. This approach combines several machine learning techniques (linear regression, support vector regression, the k-nearest neighbors and a multi-layer perceptron neural network) with several dimensionality reduction techniques (non-negative factorization, recursive feature elimination and feature selection with a variance threshold). It includes the input dataset formed with the available house prices in the center of Teruel city (Spain) in December 30, 2016 from Idealista website.

This dataset supports the research of the authors in the improvement of the setup of agent-based simulations about real-estate market. The work about this dataset has been submitted for consideration for publication to a scientific journal.

The open source python code is composed of all the files with the “.py” extension. The main program can be executed from the “main.py” file. The “boxplotErrors.eps” is a chart generated from the execution of the code, and compares the results of the different combinations of machine learning techniques and dimensionality reduction methods.

The dataset is in the “data” folder. The input raw data of the house prices are in the “dataRaw.csv” file. These were shuffled into the “dataShuffled.csv” file. We used cross-validation to obtain the estimations of house prices. The outputted estimations alongside the real values are stored in different files of the “data” folder, in which each filename is composed by the machine learning technique abbreviation and the dimensionality reduction method abbreviation",,Python code for the estimation of missing prices in real-estate market with a dataset of house prices from the center of Teruel city,,10.17632/mxpgf54czz.1,,core
385434021,2017-01-01T00:00:00,"Chatbots as a new information, communication and transaction channel enable businesses to reach their target audience through messenger apps like Facebook, WhatsApp or WeChat. Compared to traditional chats, bots are not handled by human persons, but software is leading through conversations. Latest chatbots developments in customer services and sales are remarkable. However, in the field of public transport, little research has been published on chatbots so far. With chatbots, train customers find out timetables, buy tickets and have a personal, digital travel advisor providing real-time and context-relevant information about trips. Chatbots collect and provide different data about users and their journey in public transportation systems. They include travel, product, service and content preferences, usage patterns, demographic and location-based data. Chatbots have different advantages and high potential for both companies and mobile users. They enable new user touch points, reduce service, sales and support costs, one-to-one marketing, new data collections and deep learning. Using chatbots, smartphone users can reach a company anytime and anywhere. The questioned users of a chatbot prototype are remarkably open to new mobile services and they quickly adapt to this innovative technology",'IADIS - International Association for the Development of the Information Society',Communicating and transacting with chatbots : insights from public transport,,,,core
73388074,2016-10-05T00:00:00,"In this paper, the problem of proactive deployment of cache-enabled unmanned
aerial vehicles (UAVs) for optimizing the quality-of-experience (QoE) of
wireless devices in a cloud radio access network (CRAN) is studied. In the
considered model, the network can leverage human-centric information such as
users' visited locations, requested contents, gender, job, and device type to
predict the content request distribution and mobility pattern of each user.
Then, given these behavior predictions, the proposed approach seeks to find the
user-UAV associations, the optimal UAVs' locations, and the contents to cache
at UAVs. This problem is formulated as an optimization problem whose goal is to
maximize the users' QoE while minimizing the transmit power used by the UAVs.
To solve this problem, a novel algorithm based on the machine learning
framework of conceptor-based echo state networks (ESNs) is proposed. Using
ESNs, the network can effectively predict each user's content request
distribution and its mobility pattern when limited information on the states of
users and the network is available. Based on the predictions of the users'
content request distribution and their mobility patterns, we derive the optimal
user-UAV association, optimal locations of the UAVs as well as the content to
cache at UAVs. Simulation results using real pedestrian mobility patterns from
BUPT and actual content transmission data from Youku show that the proposed
algorithm can yield 40% and 61% gains, respectively, in terms of the average
transmit power and the percentage of the users with satisfied QoE compared to a
benchmark algorithm without caching and a benchmark solution without UAVs",,"Caching in the Sky: Proactive Deployment of Cache-Enabled Unmanned
  Aerial Vehicles for Optimized Quality-of-Experience",http://arxiv.org/abs/1610.01585,,,core
227380180,2017-12-12T13:32:34.662+01:00,"This research data file contains the necessary software and the dataset for estimating the missing prices of house units. This approach combines several machine learning techniques (linear regression, support vector regression, the k-nearest neighbors and a multi-layer perceptron neural network) with several dimensionality reduction techniques (non-negative factorization, recursive feature elimination and feature selection with a variance threshold). It includes the input dataset formed with the available house prices in two neighborhoods of Teruel city (Spain) in November 13, 2017 from Idealista website. These two neighborhoods are the center of the city and “Ensanche”.

This dataset supports the research of the authors in the improvement of the setup of agent-based simulations about real-estate market. The work about this dataset has been submitted for consideration for publication to a scientific journal.

The open source python code is composed of all the files with the “.py” extension. The main program can be executed from the “main.py” file. The “boxplotErrors.eps” is a chart generated from the execution of the code, and compares the results of the different combinations of machine learning techniques and dimensionality reduction methods.

The dataset is in the “data” folder. The input raw data of the house prices are in the “dataRaw.csv” file. These were shuffled into the “dataShuffled.csv” file. We used cross-validation to obtain the estimations of house prices. The outputted estimations alongside the real values are stored in different files of the “data” folder, in which each filename is composed by the machine learning technique abbreviation and the dimensionality reduction method abbreviation",,Python code for the estimation of missing prices in real-estate market with a dataset of house prices from Teruel city,,10.17632/mxpgf54czz.2,,core
228185802,2016-06-14T00:00:00,"Cities are living organisms, 24h / 7day, with demands on resources and outputs.  Water is a key resource whose management has not kept pace with modern urban life. Demand for clean water and loads on waste water no longer fit diurnal patterns; and they are impacted by events that are outside the normal range of parameters that are taken account of in water management.
This feasibility study will determine how the application of computational intelligence can be used to analyse a mix of data inputs to produce credible predictions for clean water demand and foul water outputs in urban areas. The data inputs will be social-media and gas and electricity usage, combined with meteorological and traffic movement data. These will deliver predictions of population density and activity over a subsequent 8 hours period, thus providing inputs to the water supply services on the future demand of fresh water supplies, and the subsequent load on waste water and sewerage systems.  The innovation of this concept is the aggregation of social-media data with transport related data to deliver a toolkit that predicts population density in an urban area over the next 8 hours. The toolkit will output the predictions in an open-source manner to support interoperability; thus enabling the development of new applications. 
For the sake of feasibility study the obtained data sets are localised to Leicester city in United Kingdom. The created online database contains mix of historic and real-time data. Data sources which are monitored and collected in real-time are localised Twitter feeds, current gas and electricity usage on regional level, traffic information from in-situ sensors and from traffic monitoring institutions, weather forecast and rainfall data.  To ease the work with such large dataset a graphical user interface was developed in Matlab software and employed capabilities its specialised toolboxes.  The online database is based on the Microsoft Azure solution.
The computational intelligence model currently developed consist of various topologies of artificial neural networks and support vector machine regression. Note that the ﬁnal model will comprise at least two models with weighted outputs as initial studies suggested that one model may not capture all the possible trends that characterises the training data for artificial neural network. The created toolkit includes a sensitivity test unit to evaluate the importance or contribution of each of the input variable on the prediction accuracy of the model, and also as a means of comparing our approach with traditional methods of population and water prediction. The toolkit aims to provide predictions for different time intervals, e.g. hourly, daily, monthly and yearly. Embedded within the tool are variants of differential evolutionary and swarm intelligence optimisation algorithms for optimising the meta-parameters of the computational intelligence models and the weights of the combined model.
To test the functionality of the developed tool along with appropriateness of the proposed approach for the water demand prediction, data obtained from the SmartSpaces website (http://smartspaces.dmu.ac.uk) were utilised. This website shows the energy performance of a selection of public buildings in Leicester such as De Montfort University campus buildings, Leicester City Council buildings, schools, libraries, leisure centres and others buildings in Leicester. The SmartSpaces website monitors at 30 min intervals temperature, usage of electricity, gas and water within the buildings on the list. While the number of monitored buildings on the SmartSpaces website is limited, it provided a convenient access to the data and thereby enabled development of initial models. For testing the functionality of the toolkit using historic data from the SmartSpace project, the inputs of the artificial neural network and support vector machine models include electricity, gas, temperature and two recent past water demand. The output is the predicted current water demand. The outcomes from the initial study seem promising as the water usage was predicted with an average mean square error of 0.119 in terms of cubic meters","CCWI2016, Amsterdam",Water advisory demand evaluation and resource toolkit,,,,core
228552379,2016-10-23T00:00:00,"This paper introduces a singular IIS that mixes Internet of products (IoT), Cloud-computing, Geoinformatics, and e-Science for ecological monitoring and management, having a situation study regional global warming and it is environmental effects. The important thing technologies and tools include real-time operational database (RODB) extraction-transformation-loading (ETL) on-line analytical processing (OLAP) and relational OLAP (ROLAP) naming, addressing, and profile server (NAPS) application gateway (AG) software for various platforms and tasks (APPs) IoT application infrastructure (IoT-AI) GIS and e-Science platforms and representational condition transfer/Java database connectivity. Global warming and ecological monitoring and management have obtained much attention lately, as well as an integrated information system (IIS) is recognized as highly valuable. Multi-sensors and web services were utilized to gather data along with other information for that perception layer both public systems and systems were utilized to gain access to and transport mass data along with other information within the network layer. Application Program Interfaces (APIs) were implemented within the middleware layer from the IIS. In addition, in the correlation between environmental indicators and meteorological elements, water resource availability may be the decisive factor regarding the terrestrial ecosystem in the region. The applying layer offers the functions of storing, organizing, processing, and discussing of information along with other information, along with the functions of applications in ecological monitoring and management. The research implies that the study jobs are greatly taken advantage of this kind of IIS, not just in data collection based on IoT, but additionally in Web services and applications according to cloud-computing and e-Science platforms, and the potency of monitoring processes and decision-making could be clearly improved. This paper supplies a prototype IIS for ecological monitoring and management, and in addition it supplies a new paradigm for future years research and exercise mainly in the era of massive data and IoT",International Journal of Innovative Technology and Research,DESIGNING A GEO INFORMATICS SERVER TO SHARE ECOLOGICAL STATISTICS,https://core.ac.uk/download/228552379.pdf,,,core
217336850,2014-08-01T07:00:00,"Artificial neural network (ANN) is widely applied as data-driven modeling tool in hydroinformatics due to its broad applicability of handing implicit and nonlinear relationships between the input and output data. To obtain a reliable ANN model, training ANN using the data is essential, but the training is usually taking many hours for large data set and/or for large systems with many variants. This may not be a concern when ANN is trained for offline applications, but it is of great importance when ANN is trained or retrained for real-time and near real-time applications, which are becoming an increasingly interested research theme while the hydroinformatics tools will be an integral part of smart city operation system. Based on author’s previous research projects, which proved that GPU-based ANN is more than 10X efficient than CPU-based ANN for constructing the meta-model (fast simulation), applied as a surrogate of the physics-based model (slow simulation). This paper presents the latest development of GPU-based ANN computing kernels that is implemented with OpenCL an Open Compute Language. The generalized ANN can be used an efficient machine learning library for data-driven modeling. The performance of the implemented library has been tested with the benchmark example and compared with the previous results",CUNY Academic Works,Portable GPU-Based Artificial Neural Networks For Data-Driven Modeling,https://academicworks.cuny.edu/cgi/viewcontent.cgi?article=1091&context=cc_conf_hic,,,core
33219198,,"[[abstract]]本計畫原訂期程為三年，第一年已獲得執行，執行半年至今已經獲得豐碩之成果並發表於國際期刊獲得接受，本次申請依據評審意見修正(謹附於表C012-2 計畫內容中說明)，並將過去半年已完成成果以及已經為未來兩年所作之準備進行詳盡說明，以期研究成果能獲得延續執行。 IEEE 802.16 WiMax 無線網路的發展非常迅速，許多國家列為重要基礎建設之一(台灣也不例外)，就連標準的制定與更新也非常快，新的標準不斷出現，包含802.16e-2005 定義了Mobile WiMax，草擬中的802.16j (draft) 和802.16m (pre-draft)更嘗試在原來的高 throughput 下提昇relay 以及高mobility 的能力，在這些完稿或未確立的標準中，都是在媒體存取層(Media Access Control; MAC)以及實體層(Physical layer; PHY)中進行定義。其中並有對於服務品質(Quality of Service; QoS)的規範，足見QoS 不只是網路層以上的協定必需研究的問題，特別針對WiMax，在MAC 層實作QoS 的重要性更受到許多研究學者之重視。本計劃承接過計畫經驗，這些經驗包含使用跨協定控制方式完成之IEEE 802.11e EDCA QoS 之增進、IEEE 802.11e HCCA QoS 之增進、以及去年成功地完成IEEE 802.16e/j QoS 之增進，欲進行下列的研究，以將研究深入到IEEE 802.16e/j/m 以至於廣義Scalable OFDMA-based 的無線網路的QoS 控制： 1. 階層式跨協定模糊控制(Hierarchical Cross-Layer Control; HCLC) 使用於 802.16e/j/m Mobile 以及Relaying 之QoS 2. 定義於Lattice 上高階模糊宇集(高type 以及高Level 之fuzzy universe)之普適化模糊自動機(Generalized Fuzzy Automata; GFA) 來模仿(mathematically realize)WiMax 無線網路的行為，了解QoS 參數的特性。 3. 根據GFA 自動機理論設計HCLC 控制器，使用高階模糊宇集進行多參數以及多目標控制，以在相容於標準之條件下調整不同QoS 參數。 4. HCLC 之軟硬體共同設計與實作以及MAC 排程器矽智財之電子系統層級 (Electronic System Level; ESL)驗證，開發現今儀器無法量測以及測試的WiMax 驗證平台。 5. 通用於OFDM/Scalable OFDMA-based 之跨協定控制，以利未來延伸到其他之無線網路標準，例如IEEE 802.22 Cognitive Radio，IEEE 802.11n MIMO 之QoS 排程器等。本計畫從過去的研究成果來延伸，在跨協定QoS 控制中，在標準已經定義的Power Sleep Behaviors (Listen/Active)以及Ranging 以外，另加入路由控制、PDU 長度、排程器計算等造成能源消耗的考量，這是IEEE 802.16 標準中缺乏定義的部份。不同於使用最佳化理論之硬式計算(hard computing)演算法，我們說明使用軟式計算 (soft computing)的原因。除了IEEE Computational Intelligence Society (CIS)在CIS 雜誌中所揭出的Emergent Applications 包含了網路控制之外，我們定義了WiMax 問題，說明無線網路無法根據固定的機率模型假設以及固定的環境因素來進行推導，因此無法獲得以 constraints 與 controls 來表示 的objective function 的close-form。無線網路的與變數狀態非常多且動態，無法根據專家知識逐一定義模糊規則或進行最佳化之推導，本計畫使用簡單控制規則，在不同通訊協定層進行控制，每一協定層的模糊規則展開之後，規則數可以隨著所跨協定層數獲得exponential 成長，我們以Homomorphism 來說明並規範上下層之間狀態的關係。針對這樣的網路控制架構，我們稱之為階層式跨協定控制 (Hierarchical Cross-Layer Control; HCLC)。本計畫分析WiMax 在動態無線網路環境下的延遲與QoS 參數的關係，以廣義模糊自動機當作其行為模型，並整合能量消耗的模型。自動機之行為實現(output function)利用過去國科會計畫成果—單一維度HCLC控制架構來延伸。因為WiMax 之OFDMA 與802.16m 的 MIMO 中之特性，跨協定所必須完成的是多目標與多參數之控制，方法是延伸到高階宇集(universe)的HCLC。我們並發現這樣的一個架構是一個比傳統Hierarchical Tagaki-Sugeno Control 還要廣義的Paradigm，本計劃除針對這樣的架構進行穩定性以及效率的理論分析之外，並設計實驗證明這樣的架構即時有效地進行IEEE 802.16e/j 的QoS 控制，本計畫在第一年已經證明HCLC 應用於802.16e/j 的兩種重要的公平性：inter-class fairness 與intra-class fairness，這是大多文獻中未考慮到的部份。在以嵌入式系統以及矽智財設計、實作、驗證WiMax 跨協定排程器方面，我們考慮SoC 設計流程來說明，以往任何通訊網路之矽智財驗證平台無法驗證出在不明確網路下之可行性(特別是無線網路)，必須在花費大筆經費和時間整合到SoC 並且下線(tape out) 之後，再花許多時間進行embedded software 之porting，然後才發現所設計之SIP 是否有瑕疵(bug)，若發現瑕疵，現有之積體電路設計或模擬驗證軟體並無法針對通訊網路的部份進行除錯，即使是昂貴的SoC 驗證工具(例如具有AMBA SoC bus 輸出入介面之 ARM based 平台)也無法立即進行驗證晶片在通訊網路上之行為，僅能提供訊號波形、指令週期等層級之觀察與控制。而對於通訊網路，還必須花費成本複製多台才能夠架構出基本網路拓樸(topology)，也僅能進行基本實驗。即便最後成功，市場之優勢早已不再，對於許多design houses 來說，這是一個難關。本計畫所產出之Electronic System Level (ESL)驗證平台，可以大幅縮短Design House 在嵌入式系統設計驗證之時程，並大幅縮小開發成本。由於我們已經完成WLAN MANET 之ESL 架構，此架構基於HCLC 控制模型，成功驗證了FPGA 以及HCLC 跨協定演算法於MANET 下的行為，因此這樣的一個HCLC ESL Verification 平台， 最適用於Architecture/Algorithm 以及 Hardware/Software Co-design，對於無線網路矽智財以及嵌入式系統初期之開發最具幫助。綜合以上目標以及原因之說明，本計畫擬定接下來兩年之研究方法、步驟以及個別目標，本著研究應該持續且深入的觀念，沿著過去研究成果的脈絡，來規劃未來兩年的研究。第一年已經完成原訂第一年之目標： 高階宇集HCLC 控制模型以及 OFDMA-based 802.16e-2005 Mobile WiMax 以及802.16j (MMR)的跨協定排程器軟體元件；第二年將擴充到OFDMA-based 的802.16j/m，整合PHY 與MIMO 能量消耗模型(特別是mobile node 上行部份)，完成具Relaying 功能且支援MIMO 的IEEE 802.16 e/j/m Mobile WiMax 跨協定控制器；第三年依據WLAN 之ESL 經驗以及第一年與第二年之研究成果，實作OFDMA-based WiMax 排程器矽智財，並進行軟硬體系統整合，以及完成ESL 驗證平台。總結本計畫的創意以及特點即是：1. 所完成之系統以及QoS 矽智財能夠滿足QoS 之外，在透過跨協定的架構下還能同時inter-與intra-class 的公平性；2. 達到節能的目標；3. 完成支援Cross-Layer Design 的WiMAX 通訊網路晶片矽智財；4. 結合網路模擬器與Hierarchical Cross-Layer 控制模型完成低成本之ESL 平台。以未來之擴充性來說，ESL 驗證平台之應用，不僅用於矽智財之設計，也不僅用於MAC 層之協定驗證，對於跨協定設計中任何演算法之驗證、嵌入式系統之演算法驗證、…、等等，均能發揮其功能。而模糊自動機可以應用之範圍甚廣，例如機器學習，以智慧型的方式學習網路的狀態空間，朝實體層之跨協定調變控制以產生最佳頻寬利用等。對於本計畫利用人工智慧與軟式計算理論為工具，應用到通訊網路，並實現在SoC 領域，本著過去計劃之成功經驗，向系統整合方向邁進，成果將非常豐碩。[[abstract]]This project was planned in three-year execution. The first year was granted and till now very good results has been obtained and published in an international journal (SCIE indexed). In this proposal, we revise according to the reviewers’ comments (cf. Table C012-2). We also include the results descriptions of the first year execution and present how/what we have prepared for the following two years wishing that the research can be continued. Since IEEE 802.16 WiMax wireless networks are developed, it becomes the future infrastructure of many developed and developing countries. New related standards and their amendments are fervidly discussed. The IEEE 802.16e-2005 has defined Mobile WiMax and the latest drafting 802.16j and the “pre-drafting” 802.16m even try to respectively promote multi-hop relay and mobility beyond the original throughput. In these finalized, drafting, and pre-drafting standards, most are about Media Access Control (MAC) and PHYsical layers (PHY). What insights are with Quality of Service (QoS) and we see that QoS is a problem relating not only upper layers but also MAC and PHY. Especially for WiMax, QoS realization in MAC and PHY are emphasized by many researches. Inheriting the experiences of previous projects since IEEE 802.11e EDCA, HCCA, and currently the general TDMA-based QoS controls, we propose the following researches to strike into IEEE 802.16e/j Wave 2 and so as into general Scalable OFDMA (SOFDMA) based QoS control: 1. Define Hierarchical Cross-Layer Control (HCLC) for 802.16e/j/m mobile and multi-hop relay QoS 2. Study Generalized Fuzzy Automata (GFA) defined with high-type and high-level fuzzy universes over lattices to mathematically realize WiMax behaviors and characteristics 3. Design HCLC scheduler and allocator (together called controller) according to the GFA theory where high-type fuzzy universes are used for multi-parameter and multi-objective control. In this way, we can adjust QoS parameters while the wireless communications are still compatible with the standards.4. Perform HCLC software-hardware co-design, implement the MAC QoS controller Silicon Intellectual Property (SIP), and its Electronic System Level (ESL) verifications. Develop WiMax verification platform with cross-layer features that modern instruments cannot measure. 5. Research generic cross-layer control for the OFDM/Scalable OFDMA-based wireless such that we can extend the WiMax QoS control research into future standards such as IEEE 802.22 Cognitive Radio and IEEE 802.11n MIMO. This project extends the valuable results of previous executed/executing projects. Beside cross-layer, and beside defined power sleep behaviors (Listen/Active) and ranging, we also consider energy consumption about routing control, PDU length, and scheduling computation. These are what the IEEE 802.16 does not define. Unlike those hard computation algorithms using optimization theory, we explain the feasibility using soft computing. Beside the reason explained in the Computational Intelligence Magazine published by IEEE Computational Intelligence Society (CIS) that one of the emergent applications of soft computing is network control, we additionally and in detail defined problems of WiMax QoS control. In this proposal we explain why wireless networks cannot be modeled by fixed probability model and why their performances cannot be derived using fixed environmental factors. Consequently, optimization using hard computing algorithms can not express objective functions in terms of constraints and control parameters. However, using soft computing we still suffer difficulties. The variables and states of wireless networks forms a large and dynamic space such at that no expertise can define detail fuzzy rules. Therefore, in this project, we try to use simple fuzzy rules distributed in different protocol layers. Expanding the rules in the protocol layers, we found that the number of fuzzy rules exponentially grows along the number of layers. We adopt homomorphism concept to explain and constrain inter-layer states and variables. We called this network control architecture the Hierarchical Cross-Layer Control (HCLC). For WiMax, this project analyses the relationship between delay and QoS parameters when network is highly dynamic. Then, we use generalized fuzzy automata to realize the behavior of the WiMax network and integrate energy consumption model. The realization of automata (output function) inherits previous projects’ results – the one dimensional HCLC control to extend new SOFDMA-based control with multiple objectives and multiple parameters. The method used is adopting high-type and high-level fuzzy universes in the HCLC. We even find out that such HCLC architecture is a more generic paradigm than defined in so-called Hierarchical Tagaki-Sugeno fuzzy control. This project will analyze the stability and efficiency of this fuzzy control architecture. In the first half year, we design the experiment to prove that HCLC efficiently and effectively perform IEEE 802.16e/j QoS control while at the same time keep the fairness that few articles concern. Considering the embedded system design, silicon intellectual property implementation, and verification of the WiMax QoS controller, this project provides a low cost solution. According to known SoC design flow, no SIP verification platform can perform verification of SIP behavior under realistic wireless network environment before accomplishment of SoC integration, tapering, and embedded software porting. It’s a long journey to accomplish all these tasks. If a bug is found in the SIP, no EDA tool can remove it dedicatedly for network communication. Even using costly SoC verification tool such as ARM-based platform with AMBA SoC bus on the PCB, we still cannot observe the SIP behavior under wireless networks. Using costly SoC verification tool, what only we can do is signal wave measurement, observation and control at the instruction and cycle accuracy. For basic networking experiments, we still even have to replicate platforms to construct the basic network topology. After we success eventually, the market opportunity is gone. This is a tuff work for design houses. The verification platform of this project will be superior over traditional ones and obviously shorten the development of networking SIPs. Thus reduce the cost effectively. In previous project, we had accomplished WLAN MANET ESL verification where HCLC control model successfully cooperates with FPGA hardware. This is the ever first verification platform that verify behaviours of real hardware and cross-layer algorithm under MANET. The platform will be extended for WiMAX MMR since it optimally match hardware architecture and cross-layer algorithms and is the best tool for hardware/software co-design. According to the above descriptions, we project three-year research methods, steps, and objectives. Based on the concept that research should be always continuing and burrowing to the deep, we follow the direction of previous research projects. In the first half year, we already accomplished high-type HCLC control model and its cross-layer scheduler software component for OFDMA-based 802.16e/j Mobile Multi-hop Relay (MMR) WiMax. In the second year, we will extend the HCLC controller to 802.16j/m and integrate energy consumption model to accomplish embedded cross-layer scheduler software component for multi-hop relaying and mobile WiMax. In the third year, we will implement the OFDMA-based WiMax scheduler and its electronic system level (ESL) verification platform. Summarizing the originality and features, we have: first, the system and QoS SIP not only guarantee QoS but also intra- and inter-class fairness; sencond, the QoS and fairness guarantees are energy-aware; third, we accomplish HCLC SIP for general WiMax; fourth, we produce low cost ESL verification platform integrating the network simulator and hierarchical cross-layer control. For future extension, fuzzy automata theory applies in many areas such as machine learning, intelligent network state space learning, and cross-layer physical layer modulation for spectrum optimization. The resulting SIP verification platform is also useful in many cases in addition to SIP and MAC layer control. For example, it should be also useful in any cross-layer algorithms, embedded system algorithms, …, etc. This project covers several research fields including artificial intelligence, computing theory, communication networks, and VLSI. With the basis of previous successful projects, we progress toward system integration and the results will also be very plentiful and valuable.[[note]]NSC98-2221-E327-02",行政院國家科學委員會,Hierarchical Cross-Layer Soft Computation Model for WiMAX Networks and Its Electronic Sytem Level Verification(II),,,,core
236403589,2011-11-01T00:00:00,"The computer gaming industry has begun to export powerful products and technologies from its initial entertainment roots to a number of ""serious"" industries. Game technologies are being adapted to defense, medicine, architecture, education, city planning, and government applications. Each of these industries is already served by an established group of companies that typically do not use computer games to serve their customers. The rapid growth in the power of game technologies and the growing social acceptance of these technologies has created an environment in which these are displacing other industry-specific computer hardware and software tools.

This dissertation proposes four hypotheses concerning the impact and acceptance of virtual reality and computer game technologies in education and training for laparoscopic surgery. It focuses on laparoscopic surgery because of the similarities between that form of surgery and virtual reality systems. The research indicates that the following four hypotheses are supported by the literature published in the field. Hypothesis 1: Training in laparoscopic surgery can be accomplished at a lower cost using virtual reality and game technology-based tools than through existing methods of training. Hypothesis 2: Virtual reality and game technology-based training environments provide better access to representative patient symptoms and allow more repetitive practice than existing forms of training. Hypothesis 3: Virtual reality and game technology-based training environments can reduce the training time required to achieve proficiency in laparoscopic procedures. Hypothesis 4: Virtual reality and game technology-based training can reduce the number of medical errors caused by residents and surgeons learning to perform laparoscopic procedures.

I also proposed a model of medical education in which virtual reality, including game technology, is the next major addition to or transformation of the medical education curriculum. The strong evidence collected in this study indicates that these systems are becoming much more accepted in medical education and that the technical limitations that existed when these devices were first introduced are already being overcome.ABSTRACT
Title of Dissertation: INVESTIGATING THE DISRUPTIVE
EFFECT OF COMPUTER GAME
TECHNOLOGIES ON MEDICAL
EDUCATION AND TRAINING
Roger D. Smith, Doctor of Management, 2008
Dissertation Directed By: Dr. Michael Evanchik, Graduate School of
Management and Technology,
University of Maryland University College
The computer gaming industry has begun to export powerful products and technologies
from its initial entertainment roots to a number of “serious” industries. Game technologies are
being adapted to defense, medicine, architecture, education, city planning, and government
applications. Each of these industries is already served by an established group of companies that
typically do not use computer games to serve their customers. The rapid growth in the power of
game technologies and the growing social acceptance of these technologies has created an
environment in which these are displacing other industry-specific computer hardware and
software tools.
This dissertation proposes four hypotheses concerning the impact and acceptance of
virtual reality and computer game technologies in education and training for laparoscopic
surgery. It focuses on laparoscopic surgery because of the similarities between that form of
surgery and virtual reality systems. The research indicates that the following four hypotheses are
supported by the literature published in the field.
• Hypothesis 1: Training in laparoscopic surgery can be accomplished at a lower cost
using virtual reality and game technology-based tools than through existing methods
of training.
• Hypothesis 2: Virtual reality and game technology-based training environments
provide better access to representative patient symptoms and allow more repetitive
practice than existing forms of training.
• Hypothesis 3: Virtual reality and game technology-based training environments can
reduce the training time required to achieve proficiency in laparoscopic procedures.
• Hypothesis 4: Virtual reality and game technology-based training can reduce the
number of medical errors caused by residents and surgeons learning to perform
laparoscopic procedures.
I also proposed a model of medical education in which virtual reality, including game
technology, is the next major addition to or transformation of the medical education curriculum.
The strong evidence collected in this study indicates that these systems are becoming much more
accepted in medical education and that the technical limitations that existed when these devices
were first introduced are already being overcome.
INVESTIGATING THE DISRUPTIVE EFFECT OF COMPUTER GAME TECHNOLOGIES
ON MEDICAL EDUCATION AND TRAINING
By
Roger D. Smith.
Dissertation submitted to the Faculty of the Graduate School of the
University of Maryland University College, in partial fulfillment
of the requirements for the degree of
Doctor of Management
2008
Advisory Committee:
Dr. Michael Evanchik, Chair
Dr. Monica Bolesta, Member
Dr. Joseph D’Mello, Member
© Copyright by
Roger D. Smith
2008
ii
Acknowledgements
I would like to thank the members of my dissertation committee: Dr. Michael Evanchik,
Dr. Monica Bolesta, and Dr. Joseph D’Mello for their guidance and help in structuring this
research so that it makes a valuable contribution toward understanding current and future
changes in the business of medical education. I also appreciate the opportunity that this degree
program and its faculty have given me to expand my understanding of business, management,
and innovation. This knowledge has been tremendously helpful to me in contributing to the
companies and government organizations with which I have been associated.
I am indebted to my mother and father for the foundations that they established in my
early years and for always believing that I could accomplish great things. I only wish that my
father had lived to see this newest accomplishment. Finally, my gratitude to my wife and
children for allowing me to spend years shut in my office working toward this goal. Your
understanding and support have been greatly appreciated.
iii
Table of Contents
Acknowledgements............................................................................................................. ii
Table of Contents............................................................................................................... iii
List of Tables ...................................................................................................................... v
List of Figures .................................................................................................................... vi
Chapter 1: Introduction and Research Problem.................................................................. 1
Virtual Reality................................................................................................................. 2
Computer Game Technologies ....................................................................................... 3
3D Engine ................................................................................................................... 5
Graphical User Interface ............................................................................................. 7
Physical Models.......................................................................................................... 8
Artificial Intelligence .................................................................................................. 8
Networking ................................................................................................................. 9
Persistence................................................................................................................. 10
Terminology Issues....................................................................................................... 11
Surgical Practice and Education ................................................................................... 12
Research Problem ......................................................................................................... 16
Chapter 2: Literature Review............................................................................................ 20
Medical Education with Virtual Reality ....................................................................... 20
Pioneers in Medical Simulation ................................................................................ 21
Simulation as a Tool for Education .......................................................................... 23
Cost Factors in Medical Education........................................................................... 26
Access to Patient Symptoms and Virtual Reality ..................................................... 28
Simulation and VR Impact on Training Time .......................................................... 31
Potential to Reduce Medical Errors .......................................................................... 33
Game Technology for Non-Entertainment Applications.............................................. 35
Historical Applications ............................................................................................. 35
Educational Applications .......................................................................................... 36
Business Aspects of Games ...................................................................................... 40
Games as Technology Products ................................................................................ 44
Social Acceptance of Games .................................................................................... 45
Dissertations on Computer Games................................................................................ 48
Social Impacts........................................................................................................... 48
Educational Applications .......................................................................................... 51
Business Aspects of Games ...................................................................................... 51
Technology in Games ............................................................................................... 52
Simulation ..................................................................................................................... 53
History of Technology .................................................................................................. 55
Disruptive Innovation and Creative Destruction .......................................................... 59
Chapter 3: Conceptual Framework and Research Method ............................................... 64
Conceptual Framework................................................................................................. 66
Rationale ....................................................................................................................... 72
The Hypotheses............................................................................................................. 74
iv
Research Method .......................................................................................................... 76
Reference Coding.......................................................................................................... 82
Chapter 4: Data Analysis, Results, and Conclusions........................................................ 86
Data Analysis ................................................................................................................ 86
Hypothesis 1: Lower Cost......................................................................................... 86
Hypothesis 2: Better Access ..................................................................................... 95
Hypothesis 3: Reduced Training Time ................................................................... 101
Hypothesis 4: Reduced Errors ................................................................................ 106
Results........................................................................................................................ 112
H1 Lower Cost: Supported ..................................................................................... 113
H2 Better Access: Supported.................................................................................. 113
H3 Reduced Training Time: Supported.................................................................. 114
H4 Reduced Errors: Supported ............................................................................... 115
Model of Medical Education: Supported................................................................ 116
Misleading Domain Assumptions............................................................................... 117
Assumption 1: Didactic Education is Effective ...................................................... 117
Assumption 2: Cost of Systems is Not an Issue ..................................................... 118
Assumption 3: Sufficient Access to Faculty and Patients is Possible .................... 119
Assumption 4: Practicing on Live Patients is Acceptable ...................................... 120
Discussion................................................................................................................... 121
Conclusion .................................................................................................................. 122
Chapter 5: Recommendations for Future Work............................................................. 125
Appendix 1. Medical VR Reference Coding Matrix ...................................................... 127
Appendix 2. Medical VR and Simulation Vendors in the Literature Reviewed ............ 154
Appendix 3. Personal Communication in Support of Dissertation Topic ...................... 158
References...................................................................................................................... 170
VR for Laparoscopic Surgical Education ................................................................... 170
General Medical Education and Virtual Reality......................................................... 176
Game Technology for Non-Entertainment Applications............................................ 185
Dissertations on Computer Games.............................................................................. 188
Simulation ................................................................................................................... 190
History of Technology ................................................................................................ 190
Disruptive Innovation and Creative Destruction ........................................................ 191
v
List of Tables
Table 1. MIST-VR implementations used in the literature............................................... 81
Table 2. Cost/benefit of an AccuTouch laparoscopic simulator....................................... 91
Table 3. Cost categories associated with each method of psychomotor training. ............ 92
Table 4. MIST-VR training program for laparoscopic instrument proficiency.............. 103
vi
List of Figures
Figure 1. Sim One computerized training mannequin in 1967........................................... 2
Figure 2. Six core game technologies that are disruptive to other industries. .................... 5
Figure 3. Visual comparison of 3D scenes from 1992 and 2005........................................ 6
Figure 4. Unique domains of simulations, virtual environments, and computer games..... 7
Figure 5. Denson (left) and Hoffman (right) demonstrate Sim One in 1967.................... 22
Figure 6. Medical education model .................................................................................. 66
Figure 7. Medical education model by example............................................................... 67
Figure 8. Minimally Invasive Surgical Trainer – Virtual Reality (MIST-VR) system .... 81
Figure 9. Medical VR coding matrix. ............................................................................... 83
Figure 10. Medical VR reference coding items. ............................................................... 83
Figure 11. Exercises in MIST-VR skills course ............................................................. 104
1
Chapter 1: Introduction and Research Problem
Medical education has traditionally been conducted on live patients, cadavers, live
animals, collections of tissue and organs, and inanimate mannequins. The “gold standard” for
perfecting operations has been the use of porcine subjects in place of humans. But for over 40
years researchers, surgeons, and scientists have been introducing computerized devices to
augment or replace many of the traditional tools for training. The “Sim One” computerized
mannequin is considered one of the first applications of computers to medical training. This
system was conceived at an aerospace company in 1964, funded with a $272,000 grant from the
Department of Education, and first demonstrated on March 17, 1967. Sim One delivered a
mechanically animated, computer controlled mannequin that could receive and respond to two
forms of gaseous anesthesia and four forms of injection. The “patient” breathed, had a heart beat,
presented temporal and carotid pulse, and maintained blood pressure. The mannequin opened
and closed its mouth, blinked its eyes, and changed these behaviors in response to anesthesia
administered through a mask or a tube (Abrahamson, 1997). The device was enhanced in 1971 to
deliver training in respirator application, endotrachael intubation, intramuscular injection,
recovery room care, and the measurement of pulse and respiration (Hoffman & Abrahamson,
1975). Figure 1 shows the system in a classroom as it would be used for education.
2
Figure 1. Sim One computerized training mannequin in 1967.
Source: Abrahamson, 1997
Hoffman and Abrahamson (1975) summarized the results of 15 different studies into the
effectiveness of the device in improving performance in medical practice. These studies
demonstrated improvements in “learning gain per unit of time, amount of student time required
to reach criterion levels of performance, and investment of faculty time necessary for student
learning.” The educational improvements that were achieved using what would today be
considered primitive computers and animatronics were very impressive and suggest that further
development of these devices could grow these advantages and add others that were not
achievable forty years ago.
Virtual Reality
With Sim One and many later computerized mannequins as a foundation, new computer
technologies have been introduced into medical training with the hope of carrying improvements
3
deeper into the educational curriculum. One group of these technologies includes virtual reality
and the software being created for modern computer games. The first medical virtual reality
system based on a head mounted display and a data glove was introduced by Richard Satava and
Jaron Lanier in 1991 (Satava, 1993). Lanier had coined the term “virtual reality” around 1984 to
refer to the use of electronic devices for immersing humans into a computer generated world (the
head mounted display) and to provide a tool with which to interact with that world (the data
glove). Satava applied these to medical training and demonstrated how such a system might be
employed to teach surgery. Satava’s assessment of that system was that the technology was no
where near good enough to be used in real training. He felt that it would take at least ten more
years for the technology to reach a useful state (R. Satava, personal communication, January 10,
2008).
Early definitions of “virtual reality” required that a system must immerse at least one of
the senses by cutting off access to the outside world and replacing it with a computer generated
stimuli. However, a less strict definition often allows that the visual, audible, or tactile stimuli
can be presented without totally eliminating external, non-computerized stimuli. This latter view
has proven to be more practical and less expensive to develop and to sell to customers. In
medical education, the term virtual reality is usually applied to any system where 3D computer
images are being presented and manipulated. This categorization leads to computer games being
referred to as virtual reality in most of the medical literature.
Computer Game Technologies
The computer gaming industry has begun to export powerful products and technologies
from its initial entertainment roots to a number of “serious” industries. Game technologies are
being adapted in defense, medical, architectural, educational, social, and governmental
4
applications. Each of these industries is already served by an established group of companies that
typically do not use computer games to serve their customers. The rapid growth in the power of
game technologies and the growing social acceptance of these technologies has created an
environment in which these are displacing other industry-specific computer hardware and
software tools.
Computer games provide a rich environment in which to train a wide variety of tasks.
The availability of the necessary computer hardware and game-based software technologies
makes these an attractive alternative to existing methods of training (Lane, 1995; Mayo, 2007).
This attraction is motivated by lower costs, higher effectiveness, and the in",University of Maryland University College (UMUC),Investigating the disruptive effect of computer game technologies on medical education and training,,,,core
213007942,2011,"Burkholderia pseudomallei is a saprophytic bacterium which is the causative agent of melioidosis, a common cause of fatal bacterial pneumonia and sepsis in the tropics. The incidence of melioidosis is clustered spatially and temporally and is heavily linked to rainfall and extreme weather events. Clinical case clustering has recently been reported in Townsville, Australia, and has implicated Castle Hill, a granite monolith in the city center, as a potential reservoir of infection. Topsoil and water from seasonal groundwater seeps were collected around the base of Castle Hill and analyzed by quantitative real-time PCR targeting the type III secretion system genes for the presence of B. pseudomallei. The organism was identified in 65% (95% confidence interval [CI], 49.5 to 80.4) of soil samples (n =40) and 92.5% (95% CI, 83.9 to 100) of seasonal groundwater samples (n =40). Further sampling of water collected from roads and gutters in nearby residential areas after an intense rainfall event found that 88.2% (95% CI, 72.9 to 100) of samples (n =16) contained viable B. pseudomallei at concentrations up to 113 CFU/ml. Comparison of isolates using multilocus sequence typing demonstrated clinical matches and close associations between environmental isolates and isolates derived from clinical samples from patients in Townsville. This study demonstrated that waterborne B. pseudomallei from groundwater seeps around Castle Hill may facilitate exposure to B. pseudomallei and contribute to the clinical clustering at this site. Access to this type of information will advise the development and implementation of public health measures to reduce the incidence of melioidosis. © 2011, American Society for Microbiology",American Society for Microbiology,Groundwater seeps facilitate exposure to Burkholderia pseudomallei,https://core.ac.uk/download/pdf/213007942.pdf,10.1128/AEM.05048-11,,core
29490120,2014-11-28T00:00:00,"In-memory (transactional) data stores are recognized as a first-class data
management technology for cloud platforms, thanks to their ability to match the
elasticity requirements imposed by the pay-as-you-go cost model. On the other
hand, defining the well-suited amount of cache servers to be deployed, and the
degree of in-memory replication of slices of data, in order to optimize
reliability/availability and performance tradeoffs, is far from being a trivial
task. Yet, it is an essential aspect of the provisioning process of cloud
platforms, given that it has an impact on how well cloud resources are actually
exploited. To cope with the issue of determining optimized configurations of
cloud in-memory data stores, in this article we present a flexible simulation
framework offering skeleton simulation models that can be easily specialized in
order to capture the dynamics of diverse data grid systems, such as those
related to the specific protocol used to provide data consistency and/or
transactional guarantees. Besides its flexibility, another peculiar aspect of
the framework lies in that it integrates simulation and machine-learning
(black-box) techniques, the latter being essentially used to capture the
dynamics of the data-exchange layer (e.g. the message passing layer) across the
cache servers. This is a relevant aspect when considering that the actual
data-transport/networking infrastructure on top of which the data grid is
deployed might be unknown, hence being not feasible to be modeled via white-box
(namely purely simulative) approaches. We also provide an extended experimental
study aimed at validating instances of simulation models supported by our
framework against execution dynamics of real data grid systems deployed on top
of either private or public cloud infrastructures.Comment: 34 page",,"A Flexible Framework for Accurate Simulation of Cloud In-Memory Data
  Stores",http://arxiv.org/abs/1411.7910,,,core
55628774,2014-03-01T00:00:00,"Education is being revolutionized by the introduction, of mobile technologies in the teaching and learning process. However, studies that focus in the application of mobile technologies to informal
learning environments is scarce and not systematized [1]. This is the reason for conducting a research
project that involved a urban game MobiGeo, designed in to take better advantage of the flexibility and
ubiquity offered by the Mobile Learning (ML) but also taking into account the importance of motivation
and interaction to enhance students learning.
The definition of ML has been a complicated task for researchers, but there are assumptions that can
not be neglected: the mobility, portability and ubiquity [2], these are features that will drive new learning spaces and thus motivate students. This idea is supported by [2] that introduces the concepts
of ""just in time"", ""just enough"" and ""just-for-me"" and [4] that speaks of the triad ""location
independence”, ""independence time"" and ""meaningful content”.
These principles of ""anytime"" and ""anywhere"" consolidated by mobile technologies came to renew the
variety of educational activities available to teachers and in this context arises the concept of mobile
location-based games. According to [5] ""these games are played in physical space, but at the same
time, they are supported by actions and events in an interconnected virtual space"", which can be
classified into three categories: ludic, pedagogic and hybrid. By being in direct contact with the
contents to assimilate and move in a real context, students will have a more significant learning [6]
and this will result in the mobilization of knowledge in different contexts. To make the connection
between the physical and the virtual world, our research has made use of Qr codes as these devices provide information in real time and in a dynamically way.
For this research was idealized an urban game called “MobiGeo”, that respect the principles
suggested by [7] and that has as common thread the history of the European Union. To measure
results the researchers developed a questionnaire that was adapted from a proposal of [8] which
created a ""Model to evaluate Educational Games”, so our proposal was built taking into account the
motivational model of Kirkpatrick (level1) and encompassing three major dimensions:
Motivation/Interest, Interaction and Perceived Learning. To assess the Motivation/Interest was used
the Model ARCS (Relevance, Confidence and Satisfaction) and items of Fun, Immersion and
Challenge of “Game User Experience”. On the other hand the interaction was evaluated by items of
the Social Interaction dimension of the “Game of User Experience”, the Learning Perceptions were
evaluated by Bloom's Taxonomy (Knowledge category).
In this paper we present the design and implementation of the MobiGeo outdoor learning activity with
a group of 173 students from the 7th grade of a basic school in the north of Portugal. Initial results
show that this urban game with Qr codes was an adequate activity to use in informal learning
environments that could engage students in gaming with high degrees of motivation and interaction in order to solve the tasks presented to them and so consolidate and acquired new knowledge about the European Union.CIEC – Research Centre on Child Studies, UM (FCT R&D 317","'Associated Management Consultants,  PVT., Ltd.'",The implementation of mobile location based-games and Qr codes : the case of MobiGeo,https://core.ac.uk/download/55628774.pdf,,"[{'title': None, 'identifiers': ['2340-1079', 'issn:2340-1079']}]",core
217337005,2014-08-01T07:00:00,"Modelling rainfall-runoff processes enables hydrologists to plan their response to flooding events. Urban drainage catchment modelling requires rainfall-runoff models as a prerequisite. In the UK, one of the main software tools used for drainage modelling is InfoWorks CS, based on relatively simple methods which are relatively robust in predicting runoff. This paper presents an alternative approach to modelling runoff that will allow for the complex inter-relation of runoff that occurs from impermeable areas, permeable areas, local surface storage and variation in rainfall induced infiltration. Apart from the uncertainties associated with the measurement of connected surfaces to the drainage system, the physical processes involved in runoff are nonlinear, making artificial neural networks (ANNs) an ideal candidate for modelling them. ANNs have been used for runoff prediction in natural catchments, and recently on a study for predicting the performance of urban drainage systems. This study seeks to determine an input set that predicts sewerage flow in urban catchments where the runoff is dominated by infiltration, a major issue for the water industry. A framework is proposed in which an ANN is trained by an evolutionary algorithm, which optimises ANN weights; results are assessed using the Nash-Sutcliffe Efficiency Coefficient. The model is demonstrated on a real-world case study site for which rainfall, flow, air temperature and groundwater levels in three boreholes have been measured. Various combinations of these data are used as model inputs, examining a mixture of daily and sub-daily timesteps. The best predictions are generated from daily linearly combined antecedent rainfall and air temperature, although sub-daily information improves the worst-case performance of the model. Although infiltration is affected by groundwater levels, incorporating groundwater into the model does not improve predictions. The proposed ANN model is capable of producing acceptable predictions, thus avoiding many of the uncertainties involved in traditional infiltration modelling",CUNY Academic Works,An Artificial Neural Network-Based Rainfall Runoff Model For Improved Drainage Network Modelling,https://academicworks.cuny.edu/cgi/viewcontent.cgi?article=1333&context=cc_conf_hic,,,core
250625842,2012-10-01T00:00:00,"Road traffic has a heavy impact on the urban sound environment, constituting the main source of noise and

widely dominating its spectral composition. In this context, our research investigates the use of recorded

sound spectra as input data for the development of real-time short-term road traffic flow estimation models.

For this, a series of models based on the use of Multilayer Perceptron Neural Networks, multiple linear regression,

and the Fisher linear discriminant were implemented to estimate road traffic flow as well as to classify it

according to the composition of heavy vehicles and motorcycles/mopeds. In view of the results, the use of the

50–400 Hz and 1–2.5 kHz frequency ranges as input variables in multilayer perceptron-based models

successfully estimated urban road traffic flow with an average percentage of explained variance equal to

86%, while the classification of the urban road traffic flow gave an average success rate of 96.1%",'Elsevier BV',Using recorded sound spectra profile as input data for real-time short-term urban road-traffic-flow estimation,,10.1016/j.scitotenv.2012.07.014,,core
22547267,13/07/2013,"Abstract. Enterprise distributed real-time and embedded (DRE) publish/subscribe (pub/sub) systems manage resources and data that are vital to users. Cloud computing—where computing resources are provisioned elastically and leased as a service—is an increasingly popular deployment paradigm. Enterprise DRE pub/sub systems can leverage cloud computing provisioning services to execute needed functionality when on-site computing resources are not available. Although cloud computing provides flexible on-demand computing and networking resources, enterprise DRE pub/sub systems often cannot accurately characterize their behavior a priori for the variety of resource configurations cloud computing supplies (e.g., CPU and network bandwidth), which makes it hard for DRE systems to leverage conventional cloud computing platforms. This paper provides two contributions to the study of how autonomic configuration of DRE pub/sub middleware can provision and use on-demand cloud resources effectively. We first describe how supervised machine learning can configure DRE pub/sub middleware services and transport protocols autonomically to support end-to-end quality-of-service (QoS) requirements based on cloud computing resources. We then present results that empirically validate how computing and networking resources affect enterprise DRE pub/sub system QoS. These results show how supervised machine learning can configure DRE pub/sub middleware adaptively in &lt; 10 μsec with bounded time complexity to support key QoS reliability and latency requirements",,Adapting Distributed Real-time and Embedded Pub/Sub Middleware,,,,core
23855741,29/01/2014,"The BD MAX ™ GBS Assay as implemented on the BD MAX ™ System is a qualitative in vitro diagnostic test designed to detect Group B Streptococcus (GBS) DNA in Lim Broth cultures, after incubation for greater than or equal to (≥)18 hours, obtained from vaginal-rectal swab specimens from antepartum pregnant women. The test incorporates automated DNA extraction to isolate the target nucleic acid from the specimen and real-time polymerase chain reaction (PCR) to detect a 124 bp region of the cfb gene sequence of the Streptococcus agalactiae chromosome. Results from the BD MAX GBS Assay can be used as an aid in determining colonization status in antepartum women. The BD MAX GBS Assay does not provide susceptibility results. Cultured isolates are needed for performing susceptibility testing as recommended for penicillin-allergic women. Subculture to solid media for additional testing when indicated. SUMMARY AND EXPLANATION OF THE PROCEDURE A vaginal-rectal swab is collected and transported to the laboratory using standard bacterial swab transport systems containing a non-nutritive transport medium (e.g. Amies or Stuart). In the lab, the swab is removed from the transport medium and placed into selective Lim Broth [Todd-Hewitt Broth supplemented with colistin (10µg/mL) and nalidixic acid (15µg/mL)]. After incubation o",,INTENDED USE,,,,core
328256817,2012-01-01T00:00:00,"Wireless sensor networks (WSNs) are a rapidly emerging technology with a great potential in many ubiquitous applications. Although these sensors can be inexpensive, they are often relatively unreliable when deployed in harsh environments characterized by a vast amount of noisy and uncertain data, such as urban traffic control, earthquake zones, and battlefields. The data gathered by distributed sensors—which serve as the eyes and ears of the system—are delivered to a decision center or a gateway sensor node that interprets situational information from the data streams. Although many other machine learning techniques have been extensively studied, real-time data mining of high-speed and nonstationary data streams represents one of the most promising WSN solutions. This paper proposes a novel stream mining algorithm with a programmable mechanism for handling missing data. Experimental results from both synthetic and real-life data show that the new model is superior to standard algorithms",'Hindawi Limited',A very fast decision tree algorithm for real-time data mining of imperfect data streams in a distributed wireless sensor network,,10.1155/2012/863545,,core
360545829,2011-04-07T00:00:00,"The work reported here has been motivated by the need for a generic spatial model to overcome the limitations of Cellular Automata (CA) regarding the rigid square-cell structure and limited neighbourhood configurations. A novel approach for spatial modelling technique is developed: the “vector-agent” in which the individual entity is represented by their real geometric boundaries (which can change over time) beneath an agent modelling structure. We show in this paper how the theory behind CA and agents can be combined to produce a generic and dynamic agent based on the vector data structure. This new paradigm has extended capabilities over the Geographic Automata (Torrens and Benenson, 2005) in terms of CA disunity and the abstraction of non-fixed-objects. Through computer simulation, different techniques and algorithms have been derived achieving a high degree of representational realism for a variety of phenomenaPublishedNon Peer ReviewedBarros, J. (2003) Simulating Urban Dynamics in Latin American Cities, Proceedings of the 7th International
Conference on Geocomputation, University of Southampton, UK.
Batty, M. (2000) Geocomputation Using Cellular Automata, In Openshaw, I. S. and Abrahart, R. J. (Eds.)
Geocomputation, Taylor & Francis, London, UK, pp. 95-126.
Batty, M. (2001) Cellular Dynamics: Modelling Urban Growth as a Spatial Epidemic, In Fischer, I. M. M. and
Leung, Y. (Eds.) Geocomputational Modelling: Techniques and Applications, Advances in Spatial Science,
Springer, Berlin, pp. 109-141.
Batty, M., Desyllas, J. and Duxbury, E. (2003) The Discrete Dynamics of Small-Scale Spatial Events: Agent-
Based Models of Mobility in Carnivals and Street Parades, International Journal of Geographic Information
Science, 17:7, pp. 673-697.
Batty, M. and Longley, P. (1994) Fractal Cities, Academic press, London.
Benenson, I. and Torrens, P. (2004a) Geosimulation Object-Based Modelling of Urban Phenomena, Editorial,
Computers, Environment and Urban Systems, 28, pp. 1-8.
Benenson, I. and Torrens, P. M. (2004b) Geosimulation: Automata-Based Modelling of Urban Phenomena,
Wiley, England.
Egenhofer, M. J. and Franzosa, R. D. (1991) Point-Set Toplogical Spatial Relations, International Journal of
Geographic Information Science, 5:2, pp. 161-174.
Egenhofer, M. J. and Franzosa, R. D. (1994) On the Equivalence of Topological Relations, International Journal
of Geographic Information Science, 8:6, pp. 133-152.
Goodchild, M. F. and Mark, D. M. (1987) The Fractal Nature of Geographic Phenomena, Annals of the
Association of American Geographers, 77, pp. 265-278.
Haklay, M., O'Sullivan, D. and Thurstain-Goodwin, M. (2001) So Go Downtown: Simulating Pedestrian
Movement in Town Centres, Environment and Planning B: Planning and Design, 28, pp. 343-359.
Hammam, Y., Moore, A., Whigham, P. and Freeman, C. (2004) Irregular Vector-Agent Based Simulation for
Land-Use Modelling, Proceedings of the 16th Annual Colloquium of the Spatial Information Research Centre,
University of Otago, Dunedin, New Zealand.
Hayes-Roth, B. (1995) An Architecture for Adaptive Intelligent Systems, Artificial Intelligence, 72, pp. 329-365.
Kenkel, N. C. and Walker, D. J. (1996) Fractals in the Biological Sciences, COENOSES, 11, pp. 77-100.
Laurini, R. and Thompson, D. (1992) Fundamentals of Spatial Information Systems, Academic Press, London.
Liu, Y. (2001) Modelling Urban Development with Geographical Information Systems and Cellular Automata:
A Case Study of Sydney since 1971, Unpblished PhD Thesis, University of Queensland, Australia.
Luck, M., D'inverno, M. and Munroe, S. (2003) Autonomy: Variable and Generative, In Hexmoor, H.,
Castelfranchi, C. and Falcone, R. (Eds.) Agent Autonomy, Kluwer Academic Publishers, USA.
O'Sullivan, D. (2000) Graph-Based Cellular Automata Models of Urban Spatial Processes, Unpublished PhD
Thesis, Barlett School of Architecture and Planning, University College London, University of London, London.
O'Sullivan, D. (2001) Graph-Cellular Automata: A Generalised Discrete Urban and Regional Model,
Environment and Planning B: Planning and Design, 28, pp. 687-707.
Peitgen, H., Jurgens, H. and Saupe, D. (1992) Chaos and Fractals: New Frontiers of Science, Springer-Verlag,
New York.
Rasmussen, S. and Barrett, C. L. (1995) Elements of a Theory of Simulation, Proceedings of ECAL'95, Lecture
Notes in Computer Science, Springer-Verlag, Berlin.
Rodrigues, A., Grueau, C., Raper, J. and Neves, N. (1998) Environmental Planning Using Spatial Agents, In
Carver, S. (Ed.) Innovations in GIS 5, Selected Papers from the 5th National Conference on GIS Research UK
(GISUK), Taylor & Francis Ltd., London.
Rodrigues, M. A. S. (1999) The Development of Spatial Intelligent Agents with Geographic Information
Systems, Unpublished PhD Thesis, City University, UK.
Rucc, J. C. (1994) Fractal Surfaces, Plenum Press, New York.
Russell, S. and Norvig, P. (1995) Artificial Intelligence: A Modern Approach, Prentice Hall International Inc.,
NJ, USA.
Russell, S. and Norvig, P. (2003) Artificial Intelligence: A Modern Approach, Second Edition, Prentice Hall
Series in Artificial Intelligence, New Jersey, USA.
Semboloni, F. (2000) The Growth of an Urban Cluster into a Dynamic Self-Modifying Spatial Pattern,
Environment and Planning B: Planning and Design, 27, pp. 549-564.
Shi, W. and Pang, M. Y. C. (2000) Development of Voronoi-Based Cellular Automata: An Integrated Dynamic
Model for Geographical Information Systems, International Journal of Geographical Information Science, 14:5,
pp. 455-474.
Torrens, P. and O'Sullivan, D. (2000a) Cities, Cells, and Complexity: Developing a Research Agenda for Urban
Geocomputation, Proceedings of the 5th International Conference on Geocomputation, University of
Greenwich, London.
Torrens, P. M. and Benenson, I. (2005) Geographic Automata Systems, International Journal of Geographic
Information Sciences, 10:4, pp. 385-412.
Torrens, P. M. and O'Sullivan, D. (2000b) Cellular Models of Urban Systems, The Centre for Advanced Spatial
Analysis (CASA), University College London, UK., Paper No. 22.
Voss, R. F. (1988) Fractals in Nature: From Characterization to Simulation, In Peitgen, H. O. and Saupe, D.
(Eds.) The Science of Fractal Images, Springer-Verlag, New York.
White, R. and Engelen, G. (2000) High Resolution Integrated Modelling of the Spatial Dynamics of Urban and
Regional Systems, Computers, Environment and Urban Systems, 24, pp. 383-400.
Wooldridge, M. (1997) Agent-Based Software Engineering, Proceedings on Software Engineering, 144:1, pp.
26-37.
Worboys, M. and Duckham, M. (2004) GIS: A Computing Perspective, CRC Press New York",,Generic vector-agents,,,,core
303744834,2011-10-01T00:00:00,"Burkholderia pseudomallei is a saprophytic bacterium which is the causative agent of melioidosis, a common cause of fatal bacterial pneumonia and sepsis in the tropics. The incidence of melioidosis is clustered spatially and temporally and is heavily linked to rainfall and extreme weather events. Clinical case clustering has recently been reported in Townsville, Australia, and has implicated Castle Hill, a granite monolith in the city center, as a potential reservoir of infection. Topsoil and water from seasonal groundwater seeps were collected around the base of Castle Hill and analyzed by quantitative real-time PCR targeting the type III secretion system genes for the presence of B. pseudomallei. The organism was identified in 65% (95% confidence interval [CI], 49.5 to 80.4) of soil samples (n = 40) and 92.5% (95% CI, 83.9 to 100) of seasonal groundwater samples (n = 40). Further sampling of water collected from roads and gutters in nearby residential areas after an intense rainfall event found that 88.2% (95% CI, 72.9 to 100) of samples (n = 16) contained viable B. pseudomallei at concentrations up to 113 CFU/ml. Comparison of isolates using multilocus sequence typing demonstrated clinical matches and close associations between environmental isolates and isolates derived from clinical samples from patients in Townsville. This study demonstrated that waterborne B. pseudomallei from groundwater seeps around Castle Hill may facilitate exposure to B. pseudomallei and contribute to the clinical clustering at this site. Access to this type of information will advise the development and implementation of public health measures to reduce the incidence of melioidosis",'American Society for Microbiology',Groundwater seeps facilitate exposure to Burkholderia pseudomallei,,10.1128/AEM.05048-11,,core
195652143,2012-01-01T00:00:00,"This paper presents an innovative algorithm integrated with particle swarm optimization and artificial neural networks to develop short-term traffic flow predictors, which are intended to provide traffic flow forecasting information for traffic management in order to reduce traffic congestion and improve mobility of transportation. The proposed algorithm aims to address the issues of development of short-term traffic flow predictors which have not been addressed fully in the current literature namely that: a) strongly non-linear characteristics are unavoidable in traffic flow data; b) memory space for implementation of short-term traffic flow predictors is limited; c) specification of model structures for short-term traffic flow predictors which do not involve trial and error methods based on human expertise; d) adaptation to newly-captured, traffic flow data is required. The proposed algorithm was applied to forecast traffic flow conditions on a section of freeway in Western Australia, whose traffic flow information is newly-captured. These results clearly demonstrate the effectiveness of using the proposed algorithm for real-time traffic flow forecasting",'Institute of Electrical and Electronics Engineers (IEEE)',Prediction of Short-term Traffic Variables using Intelligent Swarm-based Neural Networks,https://core.ac.uk/download/195652143.pdf,10.1109/TCST.2011.2180386,,core
23600021,2013,"Generalised wide are search and surveillance is a common-place tasking for multi-sensory equipped autonomous systems. Here we present on a key supporting topic to this task- the automatic interpretation, fusion and detected target reporting from multi-modal sensor information received from multiple autonomous platforms deployed for wide-area environment search. We detail the realization of a real-time methodology for the automated detection of people and vehicles using combined visible-band (EO), thermal-band (IR) and radar sensing from a deployed network of multiple autonomous platforms (ground and aerial). This facilities real-time target detection, reported with varying levels of confidence, using information from both multiple sensors and multiple sensor platforms to provide environment-wide situational awareness. A range of automatic classification approaches are proposed, driven by underlying machine learning techniques, that facilitate the automatic detection of either target type with cross-modal target confirmation. Extended results are presented that show both the detection of people and vehicles under varying conditions in both isolated rural and cluttered urban environments with minimal false positive detection. Performance evaluation is presented at an episodic level with individual classifiers optimized for maximal each object of interest (vehicle/person) detection over a given search path/pattern of the environment, across all sensors and modalities, rather than on a per sensor sample basis. Episodic target detection, evaluated over a number of wide-area environment search and reporting tasks, generally exceeds 90%+ for the targets considered here. 1",,Multi-modal target detection for autonomous wide area search and surveillance,,10.1117/12.2028340,,core
236621381,2011-01-01T08:00:00,"Sensor exploitation (SE) is the crucial step in surveillance applications such as airport security and search and rescue operations. It allows localization and identification of movement in urban settings and can significantly boost knowledge gathering, interpretation and action. Data mining techniques offer the promise of precise and accurate knowledge acquisition techniques in high-dimensional data domains (and diminishing the “curse of dimensionality” prevalent in such datasets), coupled by algorithmic design in feature extraction, discriminative ranking, feature fusion and supervised learning (classification). Consequently, data mining techniques and algorithms can be used to refine and process captured data and to detect, recognize, classify, and track objects with predictable high degrees of specificity and sensitivity.
Automatic object detection and tracking algorithms face several obstacles, such as large and incomplete datasets, ill-defined regions of interest (ROIs), variable scalability, lack of compactness, angular regions, partial occlusions, environmental variables, and unknown potential object classes, which work against their ability to achieve accurate real-time results. Methods must produce fast and accurate results by streamlining image processing, data compression and reduction, feature extraction, classification, and tracking algorithms. Data mining techniques can sufficiently address these challenges by implementing efficient and accurate dimensionality reduction with feature extraction to refine incomplete (ill-partitioning) data-space and addressing challenges related to object classification, intra-class variability, and inter-class dependencies.
A series of methods have been developed to combat many of the challenges for the purpose of creating a sensor exploitation and tracking framework for real time image sensor inputs. The framework has been broken down into a series of sub-routines, which work in both series and parallel to accomplish tasks such as image pre-processing, data reduction, segmentation, object detection, tracking, and classification. These methods can be implemented either independently or together to form a synergistic solution to object detection and tracking.
The main contributions to the SE field include novel feature extraction methods for highly discriminative object detection, classification, and tracking. Also, a new supervised classification scheme is presented for detecting objects in urban environments. This scheme incorporates both novel features and non-maximal suppression to reduce false alarms, which can be abundant in cluttered environments such as cities. Lastly, a performance evaluation of Graphical Processing Unit (GPU) implementations of the subtask algorithms is presented, which provides insight into speed-up gains throughout the SE framework to improve design for real time applications.
The overall framework provides a comprehensive SE system, which can be tailored for integration into a layered sensing scheme to provide the war fighter with automated assistance and support. As more sensor technology and integration continues to advance, this SE framework can provide faster and more accurate decision support for both intelligence and civilian applications",Louisiana Tech Digital Commons,Data mining based learning algorithms for semi-supervised object identification and tracking,https://core.ac.uk/download/236621381.pdf,,,core
228638600,2014-01-01T08:00:00Z,"The following paper reports on the efforts made to assist in the overall implementation of one specific household water treatment (HWT) for improving water quality for people in developing countries, biosand filters (BSFs). It is recognized that BSFs are not applicable for every situation or community. When BSFs were first developed for household applications, the minimum sand bed depth was determined to be 50 cm, based on existing Canadian regulations for water treatment through large-scale, high-capacity sand filters. We questioned this basic assumption, and investigated whether smaller, lighter, and cheaper BSFs (with a shorter sand bed depth) are as effective as the traditional large, concrete filter. The overall project objective was to assess the efficacy, effectiveness, and acceptability of a smaller biosand filter, both in the laboratory and in the field, with the overall goal of demonstrating successful performance and acceptability of the smaller BSFs to reduce implementation costs, allowing more households to be reached. Hopefully, the results presented herein will provide additional insight and quantified data on the operational considerations and removal capabilities of various types of full-scale BSFs to aid in the justification and support for future implementation efforts. In section one, the background and scope of the problem of water access and quality in developing countries is reviewed, including a brief overview of several household water treatment technologies that are currently used. The introduction, section two, provides a detailed description of the biosand filter and the experimental setup that was the focus of the laboratory research. Sections three through six contain the manuscript style descriptions of the four studies conducted, including the results and conclusions. The last and final section, section seven, is a summary of conclusions including findings and lessons learned gained in from the execution and evaluation of this research. The research conducted and reported herein tested the general hypothesis that biosand filtration can be effective on a smaller, cheaper scale than currently practiced with the concrete BSF. In particular, we investigated how the efficacy of the CAWST BSF compared to smaller bucket-sized BSFs with respect to removal of turbidity, total coliforms, E. coli, MS2 coliphage, and Cryptosporidium parvum oocysts from raw drinking water supplies. Specifically, the research attempted to answer the following questions regarding BSF performance: (1) Are the removal efficiencies of smaller BSFs significantly different from the concrete BSF? (2) Is removal efficiency impacted by the turbidity of the source water? (3) To what extent do slight disturbances affect the performance of the bucket BSFs? (4) Can the BSF be modified (i.e., by the addition of rusty nails in the diffuser basin) to significantly improve the removal of viruses in the BSF? (5) How is the removal efficiency impacted by the length of the pause period? (6) If smaller sized BSFs can offer an acceptable level of removal (based on the laboratory results), how will a smaller BSF perform in the field and will it be acceptable to end-users?Four separate studies (Sections 3.0 - 7.0 and summarized below), were conducted to answer the questions outlined above. Effect of sand bed depth and media age on bacteria and turbidity removal The main objective of the first study was to build several full-scale BSFs, simulate real-world usage conditions, and assess the long-term efficacy (9-month study period) for particulate and bacteria removal. Four replicates of three different filter designs were built: the traditional concrete BSF, and two scaled-down versions that use a 5-gal and 2-gal bucket, respectively, as the casing material. The major difference among the three BSF designs was the depth of the sand layer: approximately 54, 15, and 10 cm for the concrete, 5-gal bucket, and 2-gal bucket BSFs, respectively. This study investigated (1) how the efficacy of the CAWST (Centre for Affordable Water and Sanitation Technology version 10) BSF performed with respect to removal of turbidity and E. coli from raw drinking water supplies, (2) whether biosand filtration could be effective with scaled-down 5-gal and 2-gal bucket BSFs, (3) the effects of low and high turbidity feed water on filter performance and maintenance, and (4) the effects of filter maintenance (i.e., cleaning) on filter performance. All bucket-sized filters, and two of the concrete filters, had hydraulic loading rates (HLRs) in the range of 0.2-0.3 m3/(m2*hr) for the majority of the testing period. The smaller sand bed depths in the bucket-sized filters did not impact filter performance with respect to turbidity and E. coli removal or the effluent levels of turbidity and E. coli. All filters produced effluents with a mean turbidity of \u3c0.6 NTU. In addition, 78%, 74%, and 72% percent of effluent samples for the concrete, 5-gal, and 2-gal filters, respectively, had E. coli concentrations \u3c1 CFU/100 mL. Based on the data collected in this study, the CAWST v10 concrete filter was able to achieve 98.1 - 98.4% turbidity removal and 3.8 - 4.0 log E. coli removal. The scaled-down BSFs, constructed in 5-gal (15cm bed depth) and 2-gal (10cm bed depth) buckets, were shown to be as effective (p-values \u3e0.05) as the CAWST v10 concrete (54cm bed depth) configuration for both turbidity and E. coli removal. Alternating the influent turbidity between periods of high and low turbidity (~50 and ~5 NTU, respectively) did not influence either turbidity removal or E. coli removal. Periodic filter maintenance (i.e., cleaning the top of the sand bed) exhibited no correlation to either removal values or effluent levels of either E. coli or turbidity (p\u3c0.05 and |r|\u3c0.4). The smaller bucket-sized filters were found to be a viable alternative to the concrete BSFs for the removal of bacteria and turbidity from drinking water.Transport effects on hydraulic loading rate and removal performance BSFs designed using smaller and/or lighter casing material can result in reduced logistical requirements and implementation costs. However, the increased portability of a smaller, lighter design presents a potential negative consequence: the ability to move the installed/operational filter by the homeowner and potentially disturb the system. This study investigated the effects of moving and agitation on filter performance, using mature BSFs which had been in use for over nine months prior to the move. Data were analyzed for four replicate filters of three different filter types: the traditional concrete BSF and two plastic bucket (5-gal and 2-gal, respectively) BSFs. Filters were moved approximately 1 km and monitored for hydraulic loading rates (HLRs) and E. coli removal for eight weeks following the move. Moving the filters resulted in reduced HLRs, likely due to sand compaction, but E. coli removal remained high (log10 removal ≥ 2.8 for all sizes) and increased significantly as compared to data collected prior to the move. The resulting operational implications of moving BSFs are discussed.Influence of sand depth and pause period on microbial removal in traditional and modified BSFsThe results of the first study showed that small biosand filters (sand bed depths of 10-15 cm) were effective at removing bacteria and turbidity. However, the impact of shorter bed depths on removal rates for smaller, sub-micron particles (such as viruses), as well as the impact of shorter pause periods on filter performance, remained unknown. For the third study, biosand filters with three different sand bed depths were modified with the addition of iron nails in the diffuser basin and evaluated for bacterial, protozoal, and virus removal over six different pause periods (1, 3, 6, 12, 24, and 72 hours). The BSF configurations tested proved effective at removing the microbial contaminants over a range of pause periods. Removal of bacteria and protozoan cysts for all filter types and sizes ranged from 3 log10 to 4 log10. The addition of nails resulted in significantly better bacteria removal for all filter sizes, while only the smallest filters exhibited significantly better protozoan removal with the addition of nails. Virus removal for all filter types and sizes ranged from \u3c1 log10 to 6 log10. Both the pause period and filter type (size/configuration) influenced virus removal, and the addition of nails to the filter significantly improved virus removal at the shorter pause periods. Field evaluation of plastic-cased filters in NicaraguaThe fourth study was a field investigation to assess 1) the effectiveness of plastic-cased BSFs for improving water quality, 2) user acceptability and use, and 3) operational performance of the units. Two types of household BSFs were built, installed, and monitored over a three month period in four rural communities near San Juan del Sur, specifically a large filter made from (10in diameter) PVC pipe and a small filter made from a 5-gallon plastic bucket. The filters were designed based on the proportions of the CAWST v10 concrete BSF, that is there were proportionally designed with respect to filter media layers (i.e., sand, rock, and gravel) with the major differences between the types being the sand bed depths and reservoir volumes, which were 54cm and 15cm, and 12L and 3.6L for the large (PVC pipe) and small (5-gal) filters, respectively. From the results of this study, the 5-gal bucket and PVC BSFs performed similarly with respect to E. coli removal. After approximately 6 months of use, the median log reduction values (LRVs) for the bucket and PVC BSFs were 1.73 and 0.95, respectively",Lehigh Preserve,Investigation on the effects of design and operational variables on the efficacy of biosand filters,https://core.ac.uk/download/pdf/228638600.pdf,,,core
236257923,2012-01-01T08:00:00,"High speed facilities are considered the backbone of any successful transportation system; Interstates, freeways, and expressways carry the majority of daily trips on the transportation network. Although these types of roads are relatively considered the safest among other types of roads, they still experience many crashes, many of which are severe, which not only affect human lives but also can have tremendous economical and social impacts. These facts signify the necessity of enhancing the safety of these high speed facilities to ensure better and efficient operation. Safety problems could be assessed through several approaches that can help in mitigating the crash risk on long and short term basis. Therefore, the main focus of the research in this dissertation is to provide a framework of risk assessment to promote safety and enhance mobility on freeways and expressways. Multi-level Safety Performance Functions (SPFs) were developed at the aggregate level using historical crash data and the corresponding exposure and risk factors to identify and rank sites with promise (hot-spots). Additionally, SPFs were developed at the disaggregate level utilizing real-time weather data collected from meteorological stations located at the freeway section as well as traffic flow parameters collected from different detection systems such as Automatic Vehicle Identification (AVI) and Remote Traffic Microwave Sensors (RTMS). These disaggregate SPFs can identify real-time risks due to turbulent traffic conditions and their interactions with other risk factors. In this study, two main datasets were obtained from two different regions. Those datasets comprise historical crash data, roadway geometrical characteristics, aggregate weather and traffic parameters as well as real-time weather and traffic data. iii At the aggregate level, Bayesian hierarchical models with spatial and random effects were compared to Poisson models to examine the safety effects of roadway geometrics on crash occurrence along freeway sections that feature mountainous terrain and adverse weather. At the disaggregate level; a main framework of a proactive safety management system using traffic data collected from AVI and RTMS, real-time weather and geometrical characteristics was provided. Different statistical techniques were implemented. These techniques ranged from classical frequentist classification approaches to explain the relationship between an event (crash) occurring at a given time and a set of risk factors in real time to other more advanced models. Bayesian statistics with updating approach to update beliefs about the behavior of the parameter with prior knowledge in order to achieve more reliable estimation was implemented. Also a relatively recent and promising Machine Learning technique (Stochastic Gradient Boosting) was utilized to calibrate several models utilizing different datasets collected from mixed detection systems as well as real-time meteorological stations. The results from this study suggest that both levels of analyses are important, the aggregate level helps in providing good understanding of different safety problems, and developing policies and countermeasures to reduce the number of crashes in total. At the disaggregate level, real-time safety functions help toward more proactive traffic management system that will not only enhance the performance of the high speed facilities and the whole traffic network but also provide safer mobility for people and goods. In general, the proposed multi-level analyses are useful in providing roadway authorities with detailed information on where countermeasures must be implemented and when resources should be devoted. The study also proves that traffic data collected from different detection systems could be a useful asset that should be utilized iv appropriately not only to alleviate traffic congestion but also to mitigate increased safety risks. The overall proposed framework can maximize the benefit of the existing archived data for freeway authorities as well as for road users",'Information Bulletin on Variable Stars (IBVS)',Multi-level Safety Performance Functions For High Speed Facilities,https://core.ac.uk/download/236257923.pdf,,,core
293487289,2011-01-01T00:00:00,"O presente estudo vislumbra analisar a importância da participação dos produtores no desenvolvimento da fruticultura e do Condomínio Frutícola Diamante Ltda, localizado no município de Quaraí. Para atingir o objetivo proposto foi necessário a utilização de pesquisa exploratória para obter um maior conhecimento da problemática a ser estuda, promovendo um levantamento de informações a respeito da participação dos atores sociais no desenvolvimento da fruticultura e na implantação desta forma peculiar de produção em condomínio. Fez-se necessário uma aproximação da realidade da fruticultura no município de Quaraí, que surgiu com incentivos governamentais para reverter o quadro de pouco desenvolvimento da região da Campanha, portanto surge esta produção como alternativa de diversificação e desenvolvimento devido às qualidades naturais da região para esta produção. Com o incentivo disponível os produtores do município de Quaraí se organizaram e desenvolveram a Associação Quaraiense de Fruticultores com o intuito de receber as verbas destinadas a produção, para tanto o pêssego foi selecionado para inovar a produção do município. Com o desenvolvimento da produção surgiram inúmeros erros, a maioria, decorrentes da falta de planejamento prévio, frente a estas limitações os produtores foram relaxando nos cuidados com a produção e a padronização da matéria-prima foi diminuindo. Devido a qualidades naturais da região e aos índices positivos da fruticultura surgiu uma nova idéia para reverter o quadro da produção de pêssego, a produção em forma de condomínio. Surge ai o Condomínio Frutícola Diamante, motivado e seguindo o exemplo do programa estadual da fruticultura que visa uma fruticultura moderna, sustentável e padronizada.  Baseado nos conceitos teóricos do desenvolvimento rural e de organização social, e utilizando a análise da constituição da Associação Quaraiense de Fruticultores e do Condomínio Frutícola Diamante, tornou-se possível compreender que a falta de envolvimento dos atores sociais é uma das principais causas do fracasso de políticas, programas e projetos.This study envisions analyze the importance of farmer participation in the development of fruit and Condominium Diamond Fruit Ltd., located in the municipality of Quaraí. To reach that goal it was necessary to use exploratory research to gain a better understanding of the issues to be investigated, providing a survey of information regarding the participation of stakeholders in the development of fruit production and deployment of this peculiar form of production in a condominium. It was necessary to approach the reality of a fruit in the city of Quaraí who came up with government incentives to reverse the underdevelopment of the region of the Campaign, so this comes as an alternative production diversification and development due to the natural qualities of the region for this production. With the incentive available to producers in the municipality of Quaraí organized and developed the Association of Fruit Growers Quaraiense in order to receive funding for production, for both the peach was selected to innovate the production of the municipality. With the development of production appeared numerous errors, most from the lack of advance planning, given these limitations producers were relaxing in the care of the production and standardization of raw materials declined. Due to the region's natural qualities and positive indices of fruit was a new idea to reverse the production of peach production in the form of condominium. There arises the Condominium Diamond Fruit, motivated, and following the example of the fruit of the state program aimed at a modern fruit production, sustainable and standardized. Based on the theoretical concepts of rural development and social organization, and using the analysis of the constitution of the Association of Fruit Growers and Quaraiense Condominium Diamond Fruit has become possible to understand that the lack of involvement of social actors is a major cause of failure policies, programs and projects",,"A participação social, a fruticultura, e o desenvolvimento rural : o caso do condomínio frutícola diamante no município de Quaraí (RS)",,,,core
357535926,2013-01-01T00:00:00,"ABSTRACT HSCT, to identify the risk factors related to potential infertility after HSCT and to provide data on longitudinal sperm recovery after HSCT. Design and Methods For this retrospective multicenter study of the Late Effects Working Party of the EBMT all centers were asked if they had performed seminal analysis (SFA) in male patients before and after allogeneic HSCT, and if they would agree to provide information on all patients who had. Five hundred and forty-three centers were contacted of which 93 responded. Twenty-three Transplant Centers reported having data on SFA, and 19 of them finally contributed reports on a total of 259 patients; 224 of 259 were treated with allogeneic HSCT. Overall, 224 patients were included in this study. The EBMT is a voluntary group of transplant centers each of which is required to provide transplant-related information on each patient using a specific anonymous data collection form. Patients provide written informed consent to have their data on disease, treatment and outcome, including late complications, reported to the registry. Clinical surveillance of HSCT recipients was approved by the local institutional review boards. Patients&apos; characteristics, HSCT conditioning regimens and clinical outcome data were collected prospectively and stored in the EBMT database. Details requested on seminal fluid parameters included the number and date of collections performed per patient, the sperm concentration, the motility and the morphology of the spermatozoa. Gonodotropic hormone and testosterone levels (if performed) were requested. Results of SFA were assessed according to the World Health Organization (WHO) guidelines. 15  Patients were considered to be normozoospermic when the sperm concentration exceeded 20x10 6 /mL, oligozoospermic when the sperm count was between 5 and 20x10 6 /mL, severely oligozoospermic with a sperm count below 5x10 6 /mL, and cryptospermic when spermatozoa were detected only after careful analysis of the concentrated sample. When no spermatozoa were detected patients were considered azoospermic. Statistical analysis Variables significantly associated with the risk of infertility after allogeneic HSCT were assessed by univariate and multivariate analysis. Any presence of spermatozoa in SFA was considered as existence of spermatogenesis. Patients with sperm detectable in the SFA were compared to patients with no evidence of spermatogenesis, using the c 2 test for categorical data and the MannWhitney U test for continuous variables. Variables considered for infertility risk analysis were age at HSCT, disease, type of conditioning regimen (TBI≥7.5 Gray vs. busulfan containing regimen vs. regimen without TBI and without busulfan), occurrence of acute and chronic GVHD, persistence of chronic GVHD at time of SFA, continuous treatment with immunosuppression, and time interval between HSCT and SFA. For multivariate analysis, logistical regression with 2-sided significance levels was used to assess the impact of risk factors with infertility. A backward stepwise procedure was used to eliminate non-significant variables. Since conditioning with TBI presented the strongest impact on the risk of infertility after HSCT, a multivariate analysis including and notincluding TBI as a variable was performed. Furthermore, a separate subgroup analysis was performed on patients with and without TBI. To predict infertility risk after allogeneic HSCT a score system was set up and applied to all patients. Significant variables in univariate analysis were included into the score calculation and weighted according to their impact. As a result, TBI counted for 2 points, age over 25 years at HSCT and ongoing chronic GVHD at time of SFA for 1 point each, and time interval between HSCT and SFA under eight years for 0.5 point. Patients with a score of 0 to 1.0 point were considered as low-risk, those with a score of 1.5 to 3 points as intermediate-risk, and those with a score of 3.5 points or more as high-risk for presenting infertility after allogeneic HSCT. We included in this calculation only patients where all 4 variables were available. Finally, subgroup analysis was performed on male patients for whom results on SFA before and after HSCT were available to assess the role of pre-existing spermatogenesis defects, and in those patients in which more than one SFA after allogeneic HSCT was collected, the longitudinal recovery spermatogenesis capacity was evaluated. In all statistical procedures, P&lt;0.05 was considered as the level of significance. Statistical analysis was performed using SPSS statistical software (IBM SPSS Statistics, version 19, IBM Co.). Patients&apos; characteristics From the 224 patients included in the present study, 166 (74%) had data on one SFA and 58 (26%) on two or more SFA after allogeneic HSCT. Data on SFA collected before and after HSCT were available from 17 (8%) patients. For the longitudinal subgroup analysis on 58 patients who had more than one SFA post-transplant, data of the first and last SFA collected after HSCT were compared. Characteristics of these 224 patients are presented in  Results Results of seminal fluid analysis In the last SFA, presence of any degree of spermatozoa was reported in 70 (31%), and complete azoospermia in 154 (69%) patients. Among those with spermatogenesis, 22 (10%) patients had normozoospermia, 13 (6%) oligozoospermia, 28 (13%) severe oligozoospermia and 7 (3%) cryptospermia. Details on SFA, including data on sperm concentration, motility and morphology are shown in  Risk factors analysis associated with azoospermia In the univariate analysis, having undergone conditioning with TBI of 7.5 Gy or over (P&lt;0.0001), ongoing chronic GVHD at time of last SFA (P=0.004), and age over 25 years at HSCT (P=0.01) were significant factors associated with azoospermia. When comparing the impact of the three main conditioning regimens on spermatogenesis, there was a clear statistical difference (P&lt;0.0001;  In multivariate backward stepwise logistical regression analysis, being conditioned with TBI (RR 7.1; 95% CI: 3.4-14.8) and age over 25 years at transplantation (RR 2.4; 95% CI: 1.09-5.2) were significantly associated with higher risk for azoospermia, whereas the presence of ongoing chronic GVHD at SFA showed a trend to remain azoospermic  Risk score for azoospermia Out of 224 patients, 188 were included in the risk score analysis. The risk score for azoospermia was highly predictive: 36% (10 of 28) of the long-term survivors with a low risk score (0-1), 67% (67 of 100) of the long-term survivors with an intermediate score (1.5-3), and 92% (55 of 60) of the long-term survivors with a high risk score (3.5-4.5) had azoospermia after allogeneic HSCT (P=0.0001)  Longitudinal sperm recovery analysis Data from 58 patients were available for a longitudinal analysis of the sperm recovery. The median time interval between HSCT and first or last SFA was 49 months (range 1-269), and 87 months (range 28-275), respectively. The median time interval between first and last SFA was 24 months (range 1.5-140 months). At first SFA, 7 patients (12%) were normozoospermic, one (2%) was oligozoospermic, 4 (7%) severely oligozoospermic, 3 (5%) cryptospermic, and 43 (74%) azoospermic. At last SFA, 9 (15%) were normozoospermic, 3 (5%) oligozoospermic, 6 (10%) severely oligozoospermic, 4 (7%) cryptospermic, and 36 (62%) azoospermic (P&gt;0.0001). In 12 (21%) out of 58 patients, there was an increase in sperm counts. We compared Ongoing GVHD is a risk factor for azoospermia after HSCT haematologica | 2013; 98  Analysis of patients with sperm analysis before and after HSCT In 28 patients, seminal fluid had been collected before HSCT and was available for the analysis. Sperm could be detected in 27 (96%): 20 (71%) of them presented normozoospermia, 4 (14%) oligozoospermia, 3 (11%) severe oligozoospermia, and one (4%) azoospermia. All patients with decreased sperm counts in the pre-transplant SFA had malignant disease (AML n=3; CML n=2; lymphoma n=2; MDS/MPN n=1), and most of them were heavily pretreated. Data from 17 of these 28 patients were also available post-transplant. After HSCT, 14 of 17 patients (82%) had azoospermia (10 of them conditioned with TBI), one oligozoospermia and 2 severe oligozoospermia. Paternity After HSCT, 29 of 211 (14%) patients with a median age at HSCT of 21 (2-55) years, became fathers (total number of children 44). The time interval between HSCT and the birth of the first child was a median of 7.2 (1-21.6) years. Among the patients who fathered a child after HSCT, 11 fathered naturally, and 11 were assisted conceptions (cryopreserved sperms); there is no information on conception for 7 patients. Among the 11 patients who fathered a child naturally, none was in the group of high-risk score. A. Rovó et al. 342 haematologica | 2013; 98(3)   Discussion To our knowledge, this is the largest cohort of male patients in which spermatogenesis has been evaluated following allogeneic HSCT. In this study, one-third of all patients had some degree of spermatogenesis post-transplant, and 10% of patients had a normal sperm count. As previously reported,  Patients transplanted with a busulfan containing conditioning regimen have some degree of spermatogenesis post-transplant, as well as a real chance to become father of a child.  To accurately interpret the impact of treatment on posttransplant infertility, we need to know the degree of gonadal dysfunction before HSCT. Decreased sperm concentration before HSCT can be due to the underlying disease itself. Indeed, oligozoospermia has been reported in 25% of patients with Hodgkin&apos;s lymphoma and in 57% of patients with leukemia before starting any treatment. 20 Decreased sperm concentration can also be the consequence of treatment-induced gonadal dysfunction before HSCT.  This high proportion of normozoospermic patients observed here before transplantation could be an overestimation. Indeed, sperm concentration does not decrease immediately after cytotoxic treatment. During the first 1-2 months sperm counts may remain normal and then diminish later during treatment. After radiation with a dose between 0.35 and 0.5 Gy, the nadir of sperm count occurs after 4-6 months only. Spermatogenic stem cells are more sensitive to chemotherapy and radiation, while later stage germ cells continue to mature after chemotherapy and radiation therapy. Therefore, recovery of spermatogenesis depends on the degree of destruction of the early sperm stem cells. 26 There are few data on longitudinal recovery of fertility Ongoing GVHD is a risk factor for azoospermia after HSCT 28 This is not surprising, since patients with aplastic anemia do not require chemotherapy before HSCT, and are usually conditioned with a reduced intensity regimen. In a subgroup analysis, we show that sperm recovery is possible, and is more likely the longer the time interval between HSCT and SFA. With a new scoring system, based on the four predictors for decreased sperm production, which are TBI, age over 25 years old at HSCT, ongoing chronic GVHD, and time interval between HSCT and SFA under eight years, we were able to detect long-term survivors with a probability of more than 90% of being azoospermic. This risk score does not replace the SFA for fertility assessment after HSCT, but can be used as a tool for patient counseling. The findings of this study have direct repercussions on long-term male survivors and their respective partners. These results may also support health care providers in counseling patients before and after HSCT. Because of the high probability of infertility after HSCT, fertility preservation by cryopreservation of spermatozoa early during the course of the disease before HSCT has to be advised.  Our study has some limitations. First, this is a retrospective analysis with SFA performed at different centers, at different time points after HSCT, and for various reasons. The centers reported only on patients for whom an SFA had been performed. We have data on hormonal status such as FSH 10,31,32 LH and testosterone levels, but not on inhibin B, a parameter that seems to be more closely related to sperm activity.  Acknowledgments Appendix of EBMT participating centers (CIC, country, city, investigator/s, number of patients) 169, Turkey, Ankara, Gülsan Sucak, 15; 202, Switzerland,  Basel, Alicia Rovó, 39; 205, United Kingdom, London (Imperial  College), Nina Salooja, Authorship and Disclosure",,2009,,,,core
71384695,2013-06-11T00:00:00,"The adoption of digital sound broadcasting systems, which are under testing in the country, allows new studies aimed a better planning for the implementation of new stations, which means to reassess the major existing radio propagation models or propose new alternatives to meet demands inherent in digital systems. The current models, as Recommendations ITU-R P. 1546 and ITU-R P. 1812, do not match closely with the reality of some regions of Brazil, especially in the tropical regions, such as the Amazon Region, due to the high rainfall and the vast existing flora. Using models suited to the propagation channel, it becomes feasible to develop planning tools covering most accurate and efficient. The use of these tools is applicable both to ANATEL, for the elaboration of the basic plans, as distribution channels for broadcasters. 
This paper presents a methodology using a computational intelligence based in Bayesian Networks for prediction of electric field intensity, which can be applied to planning or expanding coverage areas in broadcasting systems for frequencies in the range of medium wave (300 kHz to 3 MHz). This methodology generates electric field values estimated from the values of terrain altitude (through analysis of conditional probability tables) and provides a comparison of these values with the measured electric field. 
The data used in this study were collected in Brazil’s central region, nearby the city of Brasilia. The transmitted signal was an AM radio signal transmitted at a frequency of 980 kHz. With the data collected during the measurement campaigns, simulations were performed using conditional probability tables generated by Bayesian Networks. 
Thus, it’s proposed a method for predicting values of electric field based on the correlation between the measured electric field and the altitude through the use of artificial intelligence. Compared to numerous studies in the literature that have the same goal, the results found in this study validate the use of the methodology to determine the electric field in medium wave radio broadcasting using Bayesian Networks.A adoção de sistemas digitais de radiodifusão sonora, que estão em fase de testes no país, permite realizar novos estudos visando um melhor planejamento para a implementação dessas novas emissoras. O que significa reavaliar os principais modelos de radiopropagação existentes ou propor novas alternativas para atender as demandas inerentes dos sistemas digitais. Os modelos atuais, conforme Recomendações ITU-R P. 1546 e ITU-R P. 1812, não condizem fielmente com a realidade de algumas regiões do Brasil, principalmente com as regiões de clima tropical, como a Região Amazônica, seja pelo elevado índice pluviométrico seja pela vasta flora existente. A partir dos modelos adequados ao canal de propagação, torna-se viável desenvolver ferramentas de planejamento de cobertura mais precisas e eficientes. A utilização destas ferramentas é cabível tanto para a ANATEL, para a elaboração dos planos básicos de distribuição de canais quanto para os radiodifusores. No presente trabalho é apresentada uma metodologia utilizando a inteligência computacional, baseada em Inferênciass Baysianas, para predição da intensidade de campo elétrico, a qual pode ser aplicada ao planejamento ou expansão de áreas de cobertura em sistemas de radiodifusão para frequências na faixa de ondas médias (de 300 kHz a 3MHz). Esta metodologia gera valores de campo elétrico estimados a partir dos valores de altitude do terreno (através de análises de tabelas de probabilidade condicional) e estabelece a comparação destes com valores de campo elétrico medidos. Os dados utilizados neste trabalho foram coletados na região central do Brasil, próximo à cidade de Brasília. O sinal transmitido era um sinal de rádio AM transmitido na frequência de 980 kHz. De posse dos dados coletados durante as campanhas de medição, foram realizadas simulações utilizando tabelas de probabilidade condicional geradas por Inferências Bayesianas. 
Assim, é proposto um método para predizer valores de campo elétrico com base na correlação entre o campo elétrico medido e altitude, através da utilização de inteligência computacional. Se comparados a inúmeros trabalhos existentes na literatura que têm o mesmo objetivo, os resultados encontrados neste trabalho validam o uso da metodologia para determinar o campo elétrico de radiodifusão sonora em ondas médias utilizando Inferências Bayesianas",Programa de Pós-Graduação em Engenharia Elétrica,Uma metodologia para predição do campo elétrico de radiodifusão sonora em ondas médias utilizando inferências bayesianas,,,,core
196694373,01/09/2012,"Traffic accidents are still one of the main health problems in the World. A number of measures have been applied in order to reduce the number of injuries and fatalities in roads, i.e., implementation of Advanced Driver Assistance Systems (ADAS) based on image processing. In this paper, a real time speed supervisor based on road sign recognition that can work both in urban and non-urban environments is presented. The system is able to recognize 135 road signs, belonging to the danger, yield, prohibition obligation and indication types, and sends warning messages to the driver upon the combination of two pieces of information: the current speed of the car and the road sign symbol. The core of this paper is the comparison between the two main methods which have been traditionally used for detection and recognition of road signs: template matching (TM) and neural networks (NN). The advantages and disadvantages of the two approaches will be shown and commented. Additionally we will show how the use of well-known algorithms to avoid illumination issues reduces the amount of images needed to train a neural network.The work reported in this article has been partly funded by the Spanish Government by the grants
FEDORA (TRA2010-20225-C03-01) and D3System (TRA2011-29454-C03-02)",MDPI Publishing,Recognition Stage for a Speed Supervisor Based on Road Sign Detection,,10.3390/s120912153,"[{'title': None, 'identifiers': ['issn:1424-8220', '1424-8220']}]",core
148452645,2011-01-01T00:00:00,"In critical domains such as urban search and rescue (USAR), and bomb disposal, the deployment of teleoperated robots is essential to reduce the risk of first responder personnel. Teleoperation is a difficult task, particularly when controlling robots from an isolated safety zone. In general, the operator has to solve simultaneously the problems of mission planning, target identification, robot navigation, and robot control. We introduce a system to support teleoperated navigation with real-time mapping consisting of a two-step scan matching method that re-considers data associations during the search. The algorithm processes data from laser range finder and gyroscope only, thereby it is independent from the robot platform. Furthermore, we introduce a user-guided procedure for improving the global consistency of maps generated by the scan matcher. Globally consistent maps are computed by a graph-based maximum likelihood method that is biased by localizing crucial parts of the scan matcher trajectory on a prior given geo-tiff image. The approach has been implemented as an embedded system and extensively tested on robot platforms designed for teleoperation in critical situations, such as bomb disposal. Furthermore, the system was evaluated in a test maze by first responders during the Disaster City event in Texas 2008.Artificial Intelligence & Integrated Computer System",'Springer Science and Business Media LLC',Mapping for the Support of First Responders in Critical Domains,,10.1007/s10846-010-9520-x,,core
79814016,2012-10-29T00:00:00,"International audienceThe Artificial Intelligence entities, capable to communicate with their vicinity, such as PDAs, Smartphones, sensors, robots, software, middleware, etc. are increasingly introduced in our environment. The conventional topography of our everyday space introduces Distributed Artificial Intelligence, Ambient Intelligence and Ubiquitous Computing concepts. For example, when traveling to a new city, information about sights in the city, hotels, or even bus timetable are needed. Thanks to ubiquitous computing, citizens as well as tourists can communicate with their environment and get all these information in real time right to their terminals. In this paper, we present a large package of ubiquitous services that might be requested in a ubiquitous space (in this case, the ubiquitous space is the Grand Stade Lille Métropole) and we have designed a multi-agents architecture which will support these services and optimize their quality as response time and information cost.Les entités de l'Intelligence Artificielle, communiquantes avec leur entourage, telles que les PDAs, les téléphones, les capteurs, les robots, les logiciels, les middlewares, etc. sont de plus en plus présentes dans notre environnement. La nouvelle topographie de nos espaces quotidiens introduit les notions de l'Intelligence Artificielle Distribuée, l'Intelligence Ambiante et l'Informatique Ubiquitaire. Notre objectif est d'avoir un accès en temps réel, via un support mobile, à différents types d'information issus d'un environnement ubiquitaire, tels que les lieux touristiques et les horaires de bus. Dans ce papier, nous proposons une architecture efficiente d'un système ubiquitaire ciblé basé sur une approche multi-agent. Nous nous focalisons dans notre étude sur le Grand Stade Lille Métropole et les services qui peuvent y être demandés. L'architecture proposée va devoir supporter les services et optimiser leur qualité par rapport au temps de réponse et au coût de l'information",HAL CCSD,Architecture à base d'agents pour optimiser les services d'aide à la mobilité : vers une conception d'un espace ubiquitaire,,,,core
57744432,11/06/2013,"Introduction: 
A sufficient regional myocardial flood flow (MBF) is essential for obtaining cardiac function.  Otherwise, ischemic heart disease or even myocardial infarction may occur. Therefore, MBF measurement using  non-invasive techniques such as contrast-enhanced dynamic magnetic resonance imaging (MRI) is of particular clinical interest. MRI-based quantification of MBF (in mL /min/g tissue) relies on an estimation of the contrast agent inflow into the particular tissue of interest, i.e. the arterial input function (AIF). Since the true AIF at the vascular inlet to the individual voxel of the MRI image cannot be measured, it is usually estimated in the left ventricular cavity of the heart. However, due to complex flow profiles in the coronary arteries, a distortion of the spatiotemporal shape of the CA bolus may occur before the bolus reaches the myocardium. This distortion would then misleadingly be attributed to dispersion in the tissue of interest. Thus, MBF values may be subject to an unknown amount of systematic error which potentially may lead to an erroneous diagnosis. It was the aim of this study to predict this immeasurable bolus dispersion and the systematic error of the MRI-based MBF measurements by CFD simulations.

Materials and Methods:  
To obtain a general understanding of the influence of the systematic error on the MBF measurements, simulations of the influence of contrast agent bolus dispersion on quantification of MBF have been made first using an exponential residue function model (time constant 1-6s). Subsequently, CFD simulations using Fluent software on multicore (n=8) Windows PC or at the High-Performing Computing Cluster âElwetrischâ at Kaiserslautern University were made based on the incompressible Navier-Stokes equations assuming Newtonian rheology. Mass transport of the contrast agent particles was considered by solving the diffusion-convection equation. The diffusion coefficient of the Gd-DOTA MRI contrast agent, which was required for mass transport calculations, was determined from direct MRI diffusion measurements of the Gd-DOTA molecule.  Simulations were performed assuming stationary and pulsatile coronary blood flow. To obtain a basic understanding of CA transport, different vessel geometries (linear, curved, different degrees of stenosis) were analyzed first. Subsequently, a realistic model of a vessel bifurcation was used as well. After the CFD simulations, the calculated CA bolus shape was used to generate data of a synthetic MRI perfusion measurement. The result of this synthetic data set was subsequently analyzed using our established heart perfusion analysis protocol based on pharmacological analysis of the CA concentration timeâcurves using a two-compartment model in XSIM software (Univ. of Washington, Seattle). From a comparison between the simulated MBF value with that calculated using XSIM, the systematic error induced by the complex flow profile in the coronary vessel on the MBF quantification was calculated.

Results and Discussion: 
Based on an exponential residue function model, up to 60% underestimation in MBF were observed depending on the time constant of the residue function. With the CFD approach, a more detailed and realistic analysis was feasible. Differences in bolus dispersion between stationary and pulsatile blood flow were small. Normal vessel geometries may lead to a MBF error of up to 20% if a laminar flow profile develops. Surprisingly however, stenosis results in less bolus dispersion (10% MBF error) because the lateral distribution of the CA in the coronary vessel is reduced behind the stenosis. Moreover, we found that a vessel bifurcation also leads to an unexpected reduction of the dispersion of the contrast agent. Here we found a systematic underestimation of the MBF values of 16%.

Conclusion:
In conclusion, MBF measurements using MRI depend strongly on the spatiotemporal flow profile in the coronary arteries. In patients with coronary artery disease, MBF measurements appear to be more realistic than those in patients with normal blood flow. However, the real amount of bolus dispersion and, thus, MBF error, is still unclear. Therefore, future work is underway towards more complex geometries as well as including analysis of non-Newtonian flow and elasticity of the vessel walls. Latest developments of MRI hardware will permit a validation of the predictions obtained with CFD with in-vivo data in patients","The MATHCARD EU Project - www.mathcard.eu, Chia Laguna, Domus de Maria, Cagliari, Sardinia Island, Italy http://www2.mate.polimi.it/ocs/",CFD Analysis of MRI-based Myocardial Perfusion Measurements,,,,core
17354658,2013-05-01T00:00:00,"Continuous increases in traffic volume and limited available capacity in the roadway system have created a need for improved traffic control. From traditional pre-timed isolated signals to actuated and coordinated corridors, traffic control for urban networks has evolved into more complex adaptive signal control systems. However, unexpected traffic fluctuations, rapid changes in traffic demands, oversaturation, the occurrence of incidents, and adverse weather conditions, among others, significantly impact the traffic network operation in ways that current control systems cannot always cope with.
On the other hand, strategies for traffic control based on developments from the field of machine learning can provide promising alternative solutions, particularly those that make use of unsupervised learning such as reinforcement learning (RL) - also referred as approximate dynamic programming (ADP) in some research communities. For the traffic control problem, examples of convenient RL algorithms are the off-policy Q-learning and the ADP using a post decision state variable, since they address processes with sequential decision making, do not need to compute transition probabilities, and are well suited for high dimensional spaces. 
A series of benefits are expected from these algorithms in the traffic control domain: 1) no need of prediction models to transition traffic over time and estimate the best actions; 2) availability of cost-to-go estimates at any time (appropriate for real-time applications); 3) self-evolving policies; and 4) flexibility to make use of new sources of information part of emergent Intelligent Transportation Systems (ITS) such as mobile vehicle detectors (Bluetooth and GPS vehicle locators).   
Given the potential benefits of these strategies, this research proposes MASTraf: a decentralized Multi-Agent System for network-wide Traffic signal control with dynamic coordination. MASTraf is designed to capture the behavior of the environment and take decisions based on situations directly observed by RL agents. Also, agents can communicate with each other, exploring the effects of temporary coalitions or subgroups of intersections as a mechanism for coordination. 
Separate MASTraf implementations with similar state and reward functions using Q-learning and ADP were tested using a microscopic traffic simulator (VISSIM) and real-time manipulation of the traffic signals through the software’s COM interface. Testing was conducted to determine the performance of the agents in scenarios with increasing complexity, from a single intersection, to arterials and networks, both in undersaturated and oversaturated conditions. 
Results show that the multi-agent system provided by MASTraf improves its performance as the agents accumulate experience, and the system was able to efficiently manage the traffic signals of simple and complex scenarios. Exploration of the policies generated by MASTraf showed that the agents followed expected behavior by providing green to greater vehicle demands and accounting for the effects of blockages and lost time. The performance of MASTraf was on par with current state of practice tools for finding signal control settings, but MASTraf can also adapt to changes in demands and driver behavior by adjusting the signal timings in real-time, thus improving coordination and preventing queue spillbacks and green starvation.  
A strategy for signal coordination was also tested in one of the MASTraf implementations, showing increased throughput and reduced number of stops, as expected. The coordination employed a version of the max-plus algorithm embedded in the reward structure, acting as a bias towards improved coordination. The response of the system using imprecise detector data, in the form of coarse aggregation, showed that the system was able to handle oversaturation under such conditions. Even when the data had only 25% of the resolution of the original implementation, the system throughput was only reduced by 5% and the number of stops per vehicle was increased by 8%. 
The state and reward formulations allowed for a simple function approximation method in order to reduce the memory requirements for storing the state space, and also to create a form of generalization for states that have not been visited or that have not been experienced enough. Given the discontinuities in the reward function generated by penalties for blockages and lost times, the value approximation was conducted through a series of functions for each action and each of the conditions before and after a discontinuity. 
The policies generated using MASTraf with a function approximation were analyzed for different intersections in the network, showing agent behavior that reflected the principles formulated in the original problem using lookup tables, including right of way assignment based on expected rewards with consideration of penalties such as lost time. In terms of system performance, MASTraf with function approximation resulted in average reductions of 1% in the total system throughput and 3.6% increases in the number of stops per vehicle, when compared to the implementation using lookup tables on a congested network of 20 intersections",,MASTraf: a decentralized multi-agent system for network-wide traffic signal control with dynamic coordination,https://core.ac.uk/download/17354658.pdf,,,core
100051638,28/11/2014,"Mobile learning or m-learning (ML) is a new form of e-learning where students use wireless networks to exchange data (questions, notes, courses, demos …) with their teachers. It is accomplished with the use of mobile devices such as palmtops, PDA, smart phones, tablet PCs or any other handheld devices. The main contribution of ML is the large mobility it offers to the different actors of the training. We believe that the comprehension of this mobility is crucial in order to better design, deploy, and manage future wireless networks in a training context. In this article we propose an approach to more understand and analyze learners &apos; behavior in wireless environments. Our idea is to integrate and visualize statically (e.g. infrastructure, Access Points position) and dynamically (e.g. users &apos; itineraries, accessed data) extracted data in unique dashboard interface. This approach makes it possible to track real learners by providing, in addition to their graph of mobility, various views on their space-time behaviors. Moreover, it allows the comparison of behaviors evolution over several periods of time",,TOWARD A MODELIZATION OF MOBILE LEARNERS BEHAVIOR FOR THE DESIGN AND THE EVALUATION OF ADVANCED TRAINING SYSTEMS,,,,core
230481165,2012-12-01T08:00:00,"Real-time identification of human activities in urban environments is increasingly becoming important in the context of public safety and national security. Distributed camera networks that provide multiple views of a scene are ideally suited for real-time action recognition. However, deployments of multi-camera based real-time action recognition systems have thus far been inhibited because of several practical issues and restrictive assumptions that are typically made such as the knowledge of a subjects orientation with respect to the cameras, the duration of each action and the conformation of a network deployment during the testing phase to that of a training deployment. In reality, action recognition involves classification of continuously streaming data from multiple views which consists of an interleaved sequence of various human actions. While there has been extensive research on machine learning techniques for action recognition from a single view, the issues arising in the fusion of data from multiple views for reliable action recognition have not received as much attention. In this thesis, I have developed a fusion framework for human action recognition using a multi-camera network that addresses these practical issues of unknown subject orientation, unknown view configurations, action interleaving and variable duration actions.;The proposed framework consists of two components: (1) a score-fusion technique that utilizes underlying view-specific supervised learning classifiers to classify an action within a given set of frames and (2) a sliding window technique that is used to parse a sequence of frames into multiple actions. The use of a score-fusion technique as opposed to a feature-level fusion of data from multiple views allows us to robustly classify actions even when camera configurations are arbitrary and different from training phase and at the same time reduces the required network bandwidth for data transmission permitting wireless deployments. Moreover, the proposed framework is independent of the underlying classifier that is used to generate scores for each action snippet and thus offers more flexibility compared to sequential approaches like Hidden Markov Models. The amount of training and parameterization is also significantly lower compared to HMM-based approaches. This Real-Time recognition system has been tested on 4 classifiers which are Linear Discriminant Analysis, Multinomial Naive Bayes, Logistic Regression and Support Vector Machines. Over 90% accuracy has been achieved by this system in Real-Time recognizing variable duration actions performed by the subject. The performance of the system is also shown to be robust to camera failures",The Research Repository @ WVU,Robust Real-Time Recognition of Action Sequences Using a Multi-Camera Network,https://core.ac.uk/download/230481165.pdf,,,core
88838648,2014-12-04T00:00:00,"In the past decades, embedded systems become more and more present in our daily life. They are present in our wallet, in our car, in our house appliances and the current tendency is to control more and more things by using them. Major companies already started to envision the future of internet which become known as the Internet of Things. In the next years, more and more of our things will be smart, connected... and subject to software flaws.Micro-controllers, very small devices with a few hundreds bytes of memory, are at the heart of this revolution. They become cheaper, smaller and more powerful. Yet, contrary to the technologies used in our desktop computers or our server farms, the goal of the industry that creates these devices is not power. Indeed, controlling temperature in your home or humidity in your wine cellar does not need Gigahertz core. It even does not need multiple cores. What these devices need is to be cheap, to be able to be produced in very high volumes and to use small amount of electrical energy. In this context, the moore's law does not help us to have more cores in one device, but it allows to produce more devices with the same amount of silicon. The research I made in the past years try to reduce the gap between cheap software and robust and efficient software for these devices. I think that, by providing smart tools and production toolchains we can help junior or non specialized developers so that they can produce good embedded software for a reasonable development cost. I also focus my work on optimizing and providing efficient and secure software and network protocols, so that the Internet of Things can become a reality while respecting user privacy and being sustainable from a energy point of view.I focused my research on three aspects. First, I focused on how to allow larger softwares do be developped in embedded systems. In particular, we proposed techniques to allow Java/JavaCard code to be executed from a non-addressable memory using a full software cache.  I also looked at how to use domain specific languages to ease the implementation of a collaborative network intrusions detection system. Thanks to adapted tools, the software, writen in the DSL, can be compiled for a wide range of probes while offering garanties on the produced software. Last, I focused on energy aware network protocols in the context of smart cities.Au cours des dernières années, les systèmes embarqués sont de plus en plus présents dans notre vie de tous les jours. Ils sont dans notre portefeuille, dans notre voiture ou dans nos appareils ménagers. La tendance actuelle est à contrôler de plus en plus d'objets à l'aide de ces systèmes. Les grands acteurs de l'industrie ont déjà commencé à envisager le futur de l'internet, l'internet des objets. Dans les années à venir, de plus en plus de nos objets seront « intelligents », connectés... et sujets à des fautes logicielles.Les micro contrôleurs, de minuscules ordinateurs possédant quelques centaines d'octets de mémoire, sont au coeur de cette révolution. Ils deviennent de moins en moins cher et de plus en plus puissant. Néanmoins, à la différence de nos ordinateurs de bureau ou des serveurs, le but de l'industrie des micro contrôleurs n'est pas la puissance. En effet, contrôler la température de notre maison et le taux d'humidité de notre cave à vin ne nécessite pas des processeurs cadencés à plusieurs gigahertz. Cela ne nécessite même pas plusieurs coeurs de calculs. Le véritable besoin de ces équipements est d'être bons marché, d'être produits en très grands volumes, d'êtres petits, facilement intégrables et d'utiliser une faible quantité d'énergie électrique. Le meilleur exemple de cette tendance est très certainement la carte à puce. Depuis le début des années 90, de plus en plus de cartes à puce sont produites, vendues et utilisées dans le monde. Cependant, elles n'en restent pas moins de tout petits équipements d'une puissance inférieure de plusieurs ordres de grandeur de nos ordinateurs de bureau. Quand il s'agit de développer du logiciel pour ces équipements, un développeur débutant n'est généralement pas suffisamment préparé et ne sera pas capable d'écrire du logiciel efficace pour ces cibles avant de longue années passées à gagner de l'expérience et de l'expertise. Si le logiciel que l'ont souhaite produire doit être efficace, sûr et correct au sens le plus strict du terme, l'industrie doit compter sur des développeurs très spécialisés, expérimentés et donc chers. Les acteurs industriels ont donc deux alternatives : produire du logiciel bon marché, mais assez inefficaceet défectueux ou dépenser une somme importante pour le développement et fournir un logiciel de bonne qualité. On peu aisément supposer que la tendance actuelle est au logiciel bon marché, et que la probabilité que le logiciel qui équipera les millions d'équipements formant l'internet des objets sera de piètre qualité et offrira quantité de failles que des acteurs malveillants se feront une joie d'exploiter si rien n'est fait pour rendre bon marché le développement correct de logiciels embarqués.Mes recherches de ces dernières années vont dans le sens de la réduction du fossé séparant logiciels bon marché et logiciels sûrs et performants. Je suis convaincu qu'en offrant des outils « intelligents » et des chaînes de production logicielle efficaces, nous pouvons aider les développeurs débutants, ou non spécialisés, à produire du logiciel embarqué de bonne qualité pour un coût de développement raisonnable. J'ai également porté mon attention sur l'optimisation et la sécurisation des logiciels et des protocoles réseau afin que l'internet des objets puisse devenir une réalité tout en respectant la vie privée des utilisateurs et en offrant une alternative durable sur le plan énergétique.Je me suis principalement intéressé à trois champs de recherche. Tout d'abord, j'ai cherché à permettre l'utilisation de techniques de développement standard (objets, composants, programmation en Java…) à la faible capacité mémoire des équipements embarqués. A l'inverse, je me suis également intéressé à l'utilisation de langages dédiés à une application afin de permettre à des spécialistes du domaine de la sécurité réseau d'exprimer des algorithmes de détection d'intrusions. A l'aide d'une suite d'outils dédiée, ces algorithmes sont compilés et optimisés automatiquement pour être utilisés dans une architecture distribuée de sondes embarquées. Enfin, je me suis intéressé aux protocoles réseau économes en énergie pour interconnecter les équipements embarqués dans le cadre des villes intelligentes",HAL CCSD,Élements de conception de systèmes embarqués fortement contraints,,,,core
236531365,2012-05,"Issue 44.5 of the Review for Religious, September/October 1985.REVIEW FOR RELIGIOUS (ISSN 0034-639X), published every two months, is edited in collaboration with
the faculty members of the Department of Theological Studies of St. Louis University. The editorial
offices are located at Room 428; 3601 Lindell Blvd., St. Louis, M O 63108-3393. REVIEW FOR RELIGIOUS
is owned by the Missouri Province Educational Institute of the Society of Jesus, St. Louis, MO. © 1985
by REVIEW FOR R ELIGIOOS. Composed, printed and manufactured in U.S.A. Second class postage paid
at St. Louis, MO. Single copies: $2.50. Subscription U.S.A. $10.00 a year; $19.00 for two years. Other
countries: add $2.00 per year (postage). For subscription orders or change of address, write REVIEW FOR
RELIGIOUg P.O. Box 6070; Duluth, MN 55806.
Daniel F. X. Meenan, S.J.
Dolores Greeley, R.S.M.
Iris Ann Ledden, S.S.N.D.
Richard A. Hill, S.J.
Jean Read
Editor
Associate Editor
Review Editor
Contributing Editor
Assistant Editor
Sept./Oct., 1985
Volume 44
Number 5
Manuscripts, books for review and correspondence with the editor should be sent to REVIEW FOR
RELIGIOUS; Room 428; 3601 Lindell Blvd.; St. Louis, MO 63108-3393.
Correspondence about the department ""Canonical Counsel"" should be addressed to Richard A.
Hill, S.J.; J.S.T.B.; 1735 LeRoy Ave.; Berkeley, CA 94709.
Back issues and reprints should be ordered from R Evmw FOrt R ELIGIOUS; Room 428; 3601 Lindell
Blvd.; St. Louis, MO 63108-3393.
""Out of print"" issues and articles not published as reprints are available from University Microf’dms
International; 300 N. Zeeb Rd.; Ann Arbor, MI 48106.
The Primordial Mystery of Consecration
John R. Sheets, S.J. ~
Father Sheets has been a frequent contributor to our. pages. The substance of the
present article; the first of the ""’Mother Xavier Ross Lecture"" series, was given at the
Motherhouse of the Sisters of Charity of Leavenworth in June, 1985. Father Sheets
continues to teach in die Theology Department of Creighton University; Omaha, NE
68178.
It seems that among people .with religious sensitivities, there is reawakening
of the sense of mystery. This~is taking place on every level of the Church,
among laity, religious, priests. In spite of the technological milieu in which
we live, there breaks through what Peter Berger’once called ""the rumor of
angels."" This is a sense of what lies beyond; beneath, and around the
""manufactured"" world that seems to dry up and suck out that deep source
of life which overflows from the fountain of living water, through the
mysteries of nature and of grace.
It is the sense of what Teilhard de Chardin called ""The Divine Milieu.""
""The perception of the divine omnipresence is essentially a seeing, a taste,
that is to say a sort of intuition bearing upon certain superior qualities in
things. It cannot,, therefore; be. attained directly by any process of reasoning,
nor by any human artifice"" (Theo Divine Mileiu, p. 131).
Perhaps this deepening sense of mystery is taking place not only :in spite
of the technologizing of our world, but because of it. When windows ~are
shut and there is no fresh air, our Jungs cry out for this freshness in a
stronger way than we. felt when fresh air was part of our normal life.
This milieu of the:.’freshness, at the heart of reality is the world of the
sacred. It is the world where two worlds join and compenetrate, the sacred
world, which is the ""milieu"" of the divine, and our created, spatial-temporal
~2 / Review for Religious, Sept.-Oct., 1985
world. The overflow of the sacred into our world is con-secration. Etymo-logically
the word means a ""with-sacredness,"" or a ""co-sacredness."" In a
manner that is pure gift, what belongs to God alone, his milieu, so to speak,°
becomes our milieu.
In what follows, I would like to take this notion of milieu as a way of
speaking about consecration. In particular, I want to show how religious
consecration, a life committed to Christ through commitment to the evan-gelical
counsels of chastity, poverty and obedience, is a particular florescence
of the sacred md~eu ~nto wh~cfi we are drawn through baptism. Baptism is
not only the ""door of the Church,"" as .it has often been called. It is also the
way in which Christ’s own consecration eniers into us, and we are drawn
into his. It is what has been described as the ""admirabile commercium,"" the
wonderful exchange. His milieu becomes mine, and mine becomes his.
What Jacob said about the place where he had wrestled with the angel
applies to the sacrament of baptism: ""This. is indeed the house of God and
the gate of heaven,"" It is out of this sacred milieu into which we are drawn
through baptism that religious consecration arises as a particular ""art
form"" of the. mystery of consecration.
The Milieus in Which We Live
Since the notion Of milieu is central to the way that I want to speak of
religious consecration, | shall begin with a brief description of its meaning:
then, comment on the different milieus which shape our lives.
A milieu, acco.rding to the etymology, is a ""middle place."" I am not sure
of all that is implied in that derivation. :But in someway, everything, and
every person in a. milieu is always in the middle of it, no matter where the
thing or person is. It is th~ mystery of the interpenetration of two levels of
reality: that by which we belong to what is greater than we. Yet what is
greater than any individual enters into the individual as though each indi-vidual
is a center of convergence of all that is in the milieu. Undoubtedly
one could speculate on this interdependence of individual and milieu at
length. But this is not the place to do that. It is enough to call attention to
the way that the whole exists in the part, and the part in the whole. ~he
milieu is not merely something external to the individuals, but works to
shape individuals, groups, nations, marking them with an identity which
gives them.a sameness even in their individuality.
For our purposes we can speak of three different milieus~ The first is
that of the,world of things as they exist.within the interdependence of the
whole. Today we call that milieu, and the way that iridividual things interact
with it, the ecosystem;
There is also the milieu in which we exist, not simply as things, but as
7he Mystery of Consecration
spirit-embodied in the world of things. It is the world created by.lspirit-in-the-
flesh, the world of culture, It is the world which is our home as persons,
a world created by the power of the spirit--the world of language, art,
literature, and the world of human relationships.
In the third place, there is the world of the sacred. By its very nature,
every°milieu is found in time and space, but has no limits or boundaries.
Also, every milieu compenetrates, in a greater or lesser degree, everything
that is within it. But in the realm of the sacred milieu this is even more
profound. There is in every heart, as well as in communities of mankind, a
sense of the more, the depth, and the beyond that.surrounds, encompasses,
sustains,’ every other aspect of our existence.
This sense of’the sacred is found in the heart Of individuals and in the
collective awareness of all peoples. Augustine describes it as the restlessness
of the human heart that thirsts for the fullness which cannot be satisfied by
any limited good. ""Our hearts are made for thee, O God, and they are
restless, until they find their rest in thee."" Rudolf Otto speaks of it as the
sense ~of the numinous (The Idea of the Holy). Paul Tillich as ""ultimate
concern."" Rabbi Heschel describes it as the ""sense of wonder coming from
the ineffable depths of reality."" No matter how it is described, all the
descriptions point to the milieuwhich is the source and sustaining power of
the whole of created existence. St. Paul, in his speech to the Athenians, will
speak of God as this milieu: ""... he is not far from any of us, for in him we
live, and move and have our being""~(Ac 17:27). ~
It. is this sense of the sacred that is at the heart of all the :searching for
meaning of life, the attempt to make some sense out of the problem of evil,
suffering, death.’~It is the basis of all religious practice, in the attempt to
enter,into communion with this absolute reality, or to propitiate it.
It is here, then, for the first time that we find a new reality. It is the
mystery of con-secration, a ""with-sacredness."" This boundless mystery,
which is out of time and space, is concretized in time and space. Certain
persons, places, things are set apart to embody this mystery, to be the
""sacrament"" which inserts the mystery into our lives, so that it can touch
us, and we can touch it. For this reason, persons, places, things are set
apart and assume the specialness that belongsto the mystery of the sacred.
In other words, they are consecrat6d.
We have been speaking of the mystery of the sacred, and the consecra-tion
by which it takes on a certain sacramental presence in the world. This
is the realm of what is called natural religion.
But .with God’s entering into the history of Israel through the call of
Moses, and the whole of the Exodus experience, there is a new sense of the
sacred. We are now in the realm of dialogue. In an’ incomprehensible way,
644 / Review for Religious, Sept.-Oct., 1985
the sacred is revealed as a person,,who calls, chooses, sends. He has a
name, ~""Yahweh."" He has designs for the whole people, and the whole of
history. He invites the people to enter into his own holiness, to share ’it.
This is an entirely new dimension in the mystery of the saci’ed, as well as in
the meaning of con-secration.
The mystery Of the holy is not only the mystery of the numinou~, but
the revelation of a God who is also will. His will is that we might live. But
his will, as well as our lives, are inseparable from keeping his word. ""From
this you know that now, if you obey my voice and hold fast to my
covenant, yoti of all the nations shall be my very own; for all the earth is
mine. I will count you a kingdom of priests, a consecrated .nation (Ex 19:5).
As we saw above, in the realm of natural religion, the’sense of the sacred
is incarnated in the world of persons, actions and things, which are conse-crated,
to be the meeting place of the sacred and the human. The same is
true in Israel, but with a richer meaning that comes from revelation. Now,
the consecration of persons, places, things serves to put the people in touch
with what took place in the saving events of their history, whose meaning is
revealed through the .prophetic word.
Even. within the consecrated people, the tribe of Levi has a ~pecial
consecration to be the tribe to serve as priests. They would receive no part
of the land’allotted to the others, because God himself was to be their lot.
However, all these aspects of the sacred, from natural religion, to the
historical religion of Israel, are only stages to the revelation wefind in Christ..
The Letter to the Hebrews describes the whole pattern of consecrating
activities in the Old Testament as shadows and figures of what is to come,
""For the Law contains but a shadow, and no true image of the good things
which were’ to come"" (Heb 10:1). ""These are no more than a shadow of
what was to come; the solid reality is Christ"" (Col 2:17).
The entrance of.God into history in a unique and unforeseeable way in
the Incarnation is described as an act of consecration. Mary is to be
overshadowed by the power of the Holy Spirit, and the one to be born of
her will be called the Holy One. The terminology is reminiscent of the
description of the consecration of the temple of Solomon..A cloud, sym-bolizing
the consecrating presence, of Yahweh, filled the temple (IK 8:10).
Jesus himself is the one who is consecrated, the one in whom and through
the Father’s redemptive love reaches out to us, and through whom we
touch the Father. He is the fulfillment of the meaning of the ladder stretching
from heaven to earth seen in Jacob’s dream (Gn 28:17). ""You shall see
greater things than that. In. truth, in’ very truth I tell you all, you shall see
heaven wide open, and God’s angels ascending and descending upon the
Son of Man"" (Jn 1:51). ,.
7he Mystery. of Consecration / 645
.~ The theme of consecration is in particular central to the Johannine
thought. Jesus is at one and the same time the one consecrated by the
Father, as well as the one who consecrates’ the wbrld. He is lamb and priest.
""I have been consecrated and sent into the world by the Father (Jn 10:36).
He is the fullness Of the Father’s h~llowing act, that takes place in hi~ Hour,
the Hour in which he is glorified. But this hallowing act overflows to
consecrate the world, in particular the Church. ""For their sake I now
consecrate mYself that they may be consecrated by the truth,’ (Jn 17:19). In
the blood and water flowing from his open side, together with the gift of
the Spirit, are symbolized the .consecration, in the first place of Christ, and
then the manner in which his own consecration reaches out to touch the
world.
In Paul in particular, there is the sense of what we cancall the Christic
milieu. To be a Christian is to be put into Christ, incorporated in him. This
is an inse.rtion into Christ’s ownconsecration. One of the favorite phrases
he used to express the whole mystery of the faith is ""in Christ Jesus,"" or ""in
the Lord."" The Christian finds his identity as the new creation by being
taken up in Christ’s consecration.
This theme is central to the thought of Paul: I shall give only a sampling
of the texts. In his farewell to.the elders at Miletus, Paul told them: ""And
now I commend you to God and to his gracious word, which has power to
build you up and give you your heritage among all who are consecrated to
him"" (Ac 20:32). The Corinthians ""have been consecrated in Christ Jesus""
(1 Co 1:2), ""washed and consecrated"" (6:11)o His own ministry is described
as a liturgical act: ’,It falls to me to offer the gentiles to him as an acceptable
sacrifice, consecrated by the Holy Spirit"" (Rm 15:!6)
Hebrews stresses the identity which is established between Christ ai~d
the Christian through being consecrated by Christ: ""For a consecrating
priest and those whom he consecrates are all of one stock"" (Heb 2:11).
Peter speaks of the faithful as ""consecrated by the Spirit to a life of obedience
to Jesus Christ"" (1 P 1:2).
A.person who is consecratedby Christ and in Christ:should live the
kind of life that flows from consec~:ation and brings it to fulfillment. ""Let us
therefore cleanse ourselves from 511 that can defile flesh or spirit, and ifi the
fear of God complete ’our consecration"" (2 Co 7: !).
The consecration of a believing spouse has inner power.to draw the
unbelieving wife or husband into the consecration of the believing spouse.
""For the heathen husband now belongs to God through his Christian wife,
and the heathen wife through her Christian husband. Otherwise your
children would not belong to God, whereas in fact they do"" (1 Co 7:14).
Paul parallels the sacrificial love by which Christ consecrated th~ Church
~a46 / Review for Religious, Sept.-Oct., 1985
with the way that.a husband should,love his wife: ""Husbands, love your
wives as Christ also loved the Church and gave himself up for it, to
consecrate it, cleansing it by water and word, so that he might present the
Church to .himself all glorious, with no stain or wrinkle or anything of the
sort, bht holy and Without blemish"" (Ep 5:25~27). ’~
The extension.of Christ’s power to consecrate is transmitted in the
mysterious power by which his own power to consecrate is sacramentalized
in his apostles. They are told to ""do this in memory of me,"" that is, to repeat
sacramentally the act by which Christ consecrated the world. They are
empowered by being given the gift of the consecrating, or rather, re-conse-crating
Spirit.""’Peace’be with you. As the Father sent me, so 1 send you.’
He then breathed on them, saying: ’Receive the Holy Spirit. If you forgive
any man’s, sins, they stand forgiven; if you pronounce them unforgiven,
unforgiven they remain’"" (Jn 20:21-23).
It is this sacred power sacramentalized in his apostles and their succes-sors
which is described by the word hierarchy. In popular understanding,
the word is identified with power and bureaucracy. ~But its original meaning,
coming from the word ""hieros’"" and ""archia,"" ""holy principle,"" is the sacra-mentalization
of Christ’s power to consecrate the faithful.
Before proceeding on to the topic of religious consecration, I would like
to sum up what l have said. At first it might seem as though I have a very
long staircase, by way of introduction, to reach the place where I am going,
’~ As I said above, it is important to recapturethe importance of the
various milieus in which weqive, Each in its own way, on different levels;
contributes to the shaping of individuals and societies. This is especially
true of the milieu of thesacred. There is an analogy between the compre-hensive
force of the power of gravity in the ecosystem with the power of the
sacred to sustain and give meaning to the whole of reality.
This sense of the ~way that the sacred permeates the whole of created
reality is at the root of the. symbolism in the Book of Revelation, ch. 4,
where created reality acclaims the One who is on the throne, singing,
""Holy, holy, holy is God the sovereign Lord of all, who was, and is, and is
to come’~ (v, 8).
0 .In ~the Old Testament, this milieu is described through images such as
covenanted people, people of God. In the New Testament, the images
abound: kingdom, city, temple, body, vine and branches, the New Creation.
We must,then, recapture the radical or primordial meaning of conse-cration,
It is’. not something which touches us merely externally. To be ""in
the Lord,"" or ""in Christ Jesus,"" means to be in a milieu which tranSforms
the inner.,person into a new creature, while at the same time it draws him
into a consecrated community.
The Mystery of Consecration
The followiiag passage, then, will serve as a summary of what I have
said, as well as a bridge to the next section. ""Let us t.hen establish ourselves
in th( .divine milieu. There we shall be within the inmost depths of souls
and the greatest consistency of matter. There, at the confluence of all the
forms of beauty, we shall discover the ultra-vital, ultra-perceptible, ultra-active
point of t.hb universe; and, at the same time, we shall experience in
the depths, of our own being the effortless deployment of’the plenitude of
all our powers of action and of adoration; For it is not merely that .at that
privileged point all the external springs of the world are coordinated and
harmonized: there is the further, complementary marvel that the man who
surrenders himself to the divine, milieu feels his own inward powers
directed and enlarged by it with a sureness which enables him effortlessly to
avoid the all too numerous reefs on which mystical quests have so often
foundere~d"" (Hymn of the Universe, Pierre Teilhard de Chardin, p. 141).’
Religious Consecration: The Florescence of the Divine Milieu
I would like then to apply what I have said above, about the way we are
consecrated by being drawn into the milieu of Christ throu’gh baptism and
the Church, to the consecrated life of the counseis.~l shall do this by
commenting on Pope John ’Paul’s letter on the religious life, Redemptionis
Donum ( The Gift of Redemption). It was addressed to religious throughout
the world at the close of the Jubilee Year of the Redemption. It is dated
March 25, 1984. But first I would like to give some of my own reflections
about the letter.
In my mind it is most consistenL the most profound presentation of the
theology of the religious life ever written. That is a bold statement. But I
think it is true. In the first place it is a coherent theology taking in all the
mysteries of our faith to bring them to ’converge on the meaning of the
religious life: Trinity, incarnation, Church, sacraments, grace, Mary. What
we have in Vatican II and other official statements ~abo~t religious life
brings out the meaning but not in the context of a coherent theology.
There is a depth° to the treatment which undercuts the traditional
dichotomies which often prevent us from getting to the central meaning of
religious life.~ Such, for example, are the contrasts between ""Pre-Vatican
and Post-Vatican,"" ""monastic and apostolic,"" ""conservative and liberal~""
""American and R6man,"" ""male and female.""
The letter goes to what is permanent beneath all of the changes. It
shows the principle of identity that marks the religious life wherever and
whenever it is found. Perhaps one of the main reasons for confusion today
among religious, and in the formation programs of so many congregations,
is the lack of any permanent base which acts as a constant among the many
6ttlt / Review for Religious, Sept.-Oct., 1985
¯variables which-affect religious life as it emerges in different cultures
throughout, history, responding to new needs of the Church as these develop.
How many things have been written in the past couple of decades on
the ""religious life of the future,"" ""changing religigus life today,"" and more.
Most of these ’are projections from a view of the religious life which is
simply-a recombination of variables, without any sense of a constant which
gives them consistency. They are like the skywriting messages we see in the
sky, which are there, lose their shape, disappear, to be succeeded by more
skywriting. In this sense, to be current is to be always out-of-date.
To come then to the letter itself. There are seven.sections to it. I want to
call special attention to sections three and four. Section Three is entitled
""Consecration,"" and Section Four, ""Evangelical Counsels.""
In Section One, ""Greeting,"" the Holy Father describes the purpose of
the letter, which is, fir",Missouri Province of the Society of Jesus,Review for Religious - Issue 44.5 (September/October 1985),,,,core
49881571,2012-01-14T00:00:00,"International audienceVideo services are being adopted widely in both mobile and fixed networks. For their successful deployment, the content providers are increasingly becoming interested in evaluating the performance of such traffic from the final users' perspective, that is, their Quality of Experience (QoE). For this purpose, subjective quality assessmentmethods are costly and can not be used in real time. Therefore, automatic estimation of QoE is highly desired. In this paper, we propose a noreference QoE monitoringmodule for adaptive HTTP streaming using TCP and the H.264 video codec. HTTP streaming using TCP is the popular choice of many web based and IPTV applications due to the intrinsic advantages of the protocol. Moreover, these applications do not suffer from video data loss due to the reliable nature of the transport layer. However, there can be playout interruptions and if adaptive bitrate video streaming is used then the quality of video can vary due to lossy compression. Our QoE estimation module, based on Random Neural Networks, models the impact of both factors. The results presented in this paper show that our model accurately captures the relation between them and QoE",HAL CCSD,Quality of Experience estimation for Adaptive HTTP/TCP video streaming using H.264/AVC,https://core.ac.uk/download/49881571.pdf,,,core
20366312,2014-04-09T00:00:00,"Obiettivo di questa tesi è la costruzione di un modello cinematico di risalita delle lave etnee basato sull’analisi congiunta delle CSD, dei dati sulle inclusioni magmatiche e dei calcoli geotermobarometrici. Un modello in grado di correlare le modalità di risalita con i processi ad essa associati, in particolare, il degassamento del fuso e i suoi effetti sulla cinetica di cristallizzazione registrati nelle strutture dei magmi eruttati.
Per questo lavoro, sono state prese in esame diverse lave etnee relative al periodo 2011-2013. Su i vari campioni sono stati condotti studi di analisi d’immagine al fine di ricostruire le Crystal Size Distribution (CSD) dei plagioclasi. Inoltre, sono state eseguite analisi chimiche al SEM sui clinopirosseni, in modo da poter ottenere informazioni su condizioni P-T di cristallizzazione, utilizzando il geotermobarometro di Putirka (Putirka et al., 2003).
Utilizzando dati di letterature sulle inclusioni magmatiche (Métrich et al., 2003), è stato possibile valutare il contenuto in H2O alle varie pressioni, ricostruendo così l’evoluzione del processo di essoluzione durante la risalita. La perdita di volatili, in particolare di H2O, tra tutti quello più abbondante, si riflette in una variazione della temperatura di liquidus, portando a un aumento della velocità di crescita dei cristalli e del loro tasso di nucleazione (Armienti et al., 2004); allo stesso modo, la variazione della temperatura del magma durante la risalita è strettamente connessa alle dinamiche della stessa, raffreddandosi con diversi tassi secondo la tipologia del moto di ascesa. Risulta quindi evidente come la cinematica di risalita controlli in maniera importante le leggi di crescita e nucleazione dei cristalli, portando a lave caratterizzate da diverse curve di CSD.
Una legge di crescita per i plagioclasi è stata stimata attraverso la regressione di dati sperimentali presenti in bibliografia (Carroll et al., 2003).
I tempi e la tipologia del moto di risalita sono le incognite di questo modello. Per questo motivo è necessario formulare delle ipotesi sulla tipologia del moto, vincolando questa al processo di cristallizzazione; in questo modo devono coincidere le tempistiche che permettono ai cristalli di raggiungere il numero e la taglia osservabile nelle rocce, con quelle del moto di ascesa del fuso verso la superficie. I tempi così ottenuti devono essere congrui con quelli che sono registrati nel contesto vulcanico reale: tempi di ritorno dell’attività esplosiva e del record sismico profondo, durata e frequenza dell’attività stromboliana antecedente le fasi più esplosive di fontane di lava.
Abstract
The aim of this thesis is to construct a kinematic model of the ascent typology of Etnean lavas based on the combined analysis of the CSDs, data on magmatic inclusions and geothermobarometric calculations. This model must be able to correlate the ascent modalities with the processes associated with them, in particular, melt degassing and its effects on the crystallization kinetics, recorded in the structures of erupted magmas.
The lavas analyzed in this study refer to the eruptive period 2011-2013.
By using of software for image analysis, it was possible to reconstruct the CSDs for plagioclase crystals. Furthermore, chemical analyzes were conducted on the pyroxene crystals, in order to obtain information about P and T of crystallization, through the use of Putirka’s geothermobarometer (Putirka et al., 2003).
Using data on melt inclusion published by Métrich et al., 2003, it was possible to reconstruct the exsolution process during the ascent of magma, knowing the water content at different pressures.
Water is the most abundant volatile component in magmas; its exsolution induces a dramatic change in the liquidus temperature, thus producing observable effects on the rates of nucleation and growth of minerals (Armienti et al., 1994). Similarly, the variation of magma temperature during its ascent, is closely linked to its dynamics, cooling with different rates according to the typology of ascent motion. For these reasons, the laws of nucleation and growth of crystals are controlled by the kinetics of the ascent, through the variation of the undercooling that can be linked to the shape of the CSDs.
The law of Crystal growth rate has been calculated through the regression of literature experimental data (Carroll et al., 2003).
Transport time and the typology of motion appear to be the unknown quantities of this model. For this reason it is necessary to make certain assumptions about the type of motion, linking this to the crystallization process; in this way, the timing which allow the crystals to reach the number and size observed in the rocks must coincide with those of the ascent of the melt to the surface. Thus, transport time obtained must be congruent with those that are recorded in the real volcanic environment: return periods of explosive activity and the deep seismic records, duration and frequency of Strombolian activity preceding the most explosive stages of lava fountains",'Pisa University Press',Relazione tra modalità di risalita e cinetica di cristallizzazione nelle lave dell'Etna: un esempio di interpretazione cinetica della CSD,https://core.ac.uk/download/20366312.pdf,,,core
302346536,2012-01-01T00:00:00,"Abstract



Purpose: To quantify and compare conjunctival epithelial TNF mRNA expression in Sjogren’s syndrome (SS), non-Sjogren’s syndrome aqueous deficient dry eye (non-SS DE), and non-dry eye (NDE) control subjects. 



Methods: 76 subjects were recruited for this study: 25 SS (confirmed via American-European Consensus Criteria 2002), 25 non-SS DE (confirmed by symptoms and Schirmer scores ≤ 10mm) and 26 NDE. Superior and temporal bulbar conjunctival epithelial cells were collected via impression cytology. Epithelial RNA was extracted and TNF gene expression was quantified by real time qPCR. 



Results: TNF gene expression was found to be significantly higher in the SS group (2.48±1.79) compared to both non-SS DE (0.95±1.18; p<0.05) and NDE (0.84±0.51; p<0.05) groups. No difference in gene expression was found between the non-SS DE and NDE groups (p=NS).



Conclusions: These results demonstrate that SS-associated aqueous deficient dry eye is associated with a significant up-regulation of TNF, which is considered one of the primary mediators of inflammation. The degree to which TNF is up-regulated may contribute to the severe ocular surface damage observed in Sjogren’s syndrome.







Introduction:



Dry eye (DE) is defined as “a multifactorial disease of the tears and ocular surface that results in symptoms of discomfort, visual disturbance and tear film instability, with potential damage to the ocular surface. It is accompanied by increased osmolarity of the tear film and inflammation of the ocular surface”.(1)  Two major subtypes of dry eye have been defined as “aqueous deficient” and “evaporative” (1),  and these subtypes can be found clinically in isolation and in combination. Aqueous deficient dry eye is further divided into Sjogren’s syndrome (SS) and non-SS subsets.  In both cases there is insufficient volume of tear secretion from the lacrimal gland, which is most often determined by a Schirmer 1 test, with the SS criterion for objective dry eye being ≤5mm of wetting of the Schirmer strip in 5 minutes. (2)  Non-SS aqueous deficient dry eye is considered moderate with Schirmer 1 scores of ≤10mm in 5 minutes and severe at ≤5mm in 5 minutes. (1) Sjogren’s syndrome, especially in its primary form, is considered to be the most severe form of aqueous deficient dry eye due to the severity of symptoms and signs that usually accompany the DE, in addition to the associated systemic nature of the disease. 



The major causative mechanisms underlying the reduced tear secretion from the lacrimal glands in aqueous deficient DE are not fully understood, although inflammation is believed to be a key factor, along with various hormonal and neurological influences. (3-6) To-date, relatively little is known regarding the specific inflammatory processes occurring at and within the lacrimal glands. However, data does suggest that in SS, specific infiltrative events occur in the lacrimal and salivary glands that are distinct from those in non-SS groups. (2) Whether such distinctions translate into differential disease severity, pathological mediators and/or unique targets for aqueous deficient DE treatment strategies remain unknown. 



As with the lacrimal gland, the cycle of ocular surface inflammation associated with DE disease is multifactorial, complex and poorly understood.(1) As the lacrimal gland becomes inflamed, it is presumed that secreted tear fluid would contain various inflammatory mediators that could elicit an inflammatory response from the epithelia and vasculature of the ocular surface. A cascade of inflammatory events within the epithelia may also be triggered through exposure to high osmolarity, shear stress from the lids, and environmental exposure. Whether some or all of these responses are present in all forms of aqueous deficient DE remains unclear. 



Characterizing both shared and unique inflammatory signalling pathways would be of benefit to better understand etiological and disease-driving mechanisms in various subsets of aqueous deficient DE, in addition to identifying appropriate targets for disease diagnosis and management. As such, significant research has taken place over the last decade to elucidate various inflammatory mediators associated with DE. 



Human tear film analysis has revealed that IL-1 (7),  IL-6 (8), MMP3 (7) and TNFα (9) proteins are elevated in SS samples relative to controls. IL-6 protein (9) also appears elevated in SS relative to non-SS DE tears, whereas no difference in tear TNFα protein expression between the two DE subtypes has been reported. (9). Lastly, non-SS DE tears compared to tears from healthy controls appear to contain elevated concentrations of IL-6 (9), MMP3 (7) and TNFα (9) proteins. 



In addition to tears, significant insight into ocular surface inflammatory mediators has been gained from direct study of conjunctival epithelial cells collected using impression cytology. With respect to protein expression, HLA-DR has been reported to be elevated in SS compared to healthy controls (10, 11), whereas variable findings ranging from elevated to no difference have been reported from SS compared to non-SS DE.  (12) (13, 14). Three previous studies have reported an elevation of HLA-DR protein expression in non-SS DE compared to healthy human controls. (11, 15, 16) Further investigation of conjunctival cells has determined that the expression of epithelial ICAM-1 protein (10), IL-6 protein (10, 17, 18), TNFα mRNA and TGF-β1 mRNA (17, 18) is elevated in SS versus control populations. In addition, expression of HLA-DR mRNA was found to be greater in an SS population compared to a non-SS DE population [18]. Taken together, biomarker data collected to date has provided evidence that inflammation does indeed play a role in various forms of DE and that distinct difference in the identity and magnitude of at least some inflammatory mediators may exist between subgroups. 



To continue to add to the knowledge base in this area, our study examined the expression of conjunctival epithelial TNFα mRNA in three diverse populations:  SS, non-SS aqueous deficient DE and healthy, age-matched controls. As noted above, there is some question with regards to TNFα protein expression in DE, as one study has reported higher concentrations in the tears of SS (n=8) and non-SS (n=10) DE subjects compared to control subjects (n=14), whereas no difference in SS versus non-SS DE TNFα tear concentration has also been reported (9). As quantitation of mRNA is free of the variables that confound tear protein quantitation, including lack of tear reflex interference, tear matrix effects and insufficient sample volume, we chose to place our initial focus on quantitation of mRNA. This research will add to the current published data which only includes information on 26 samples. (17, 18). 



 

METHODS



Study Design and Subjects

Before the start of the study, ethics approval was attained from the Office of Research Ethics at the University of Waterloo and University of Toronto. All procedures adhered to the Declaration of Helsinki. Seventy-six (76) subjects were enrolled in the study: 26 control non-DE subjects, (NDE), 25 Sjogren’s subjects (SS) and 25 non-SS moderate aqueous deficient DE (non-SS DE).  All participants underwent a clinical evaluation visit to determine entry eligibility, prior to a second visit in which ocular samples were collected. 



SS patients were recruited from the Multidisciplinary Sjogren’s Syndrome Clinic of the University Health Network in Toronto. All SS participants had been diagnosed with primary SS using the American-European consensus criteria of 2002 (2). Thus, each of these subjects had four or more of the following criteria: symptoms and signs of dry eye and dry mouth and either a positive minor salivary gland biopsy or the presence of antibodies to Ro and/or La. No further preliminary screening was performed on this group, as all had confirmed Sjogren’s syndrome using these criteria. 



The non-SS DE and NDE subjects were recruited through the SS clinic and a private practice. Subjects who answered “yes” to the question:  “If you have dry eyes, have they been dry for at least 3 months?” were asked to rate their eye dryness on a visual analogue scale that is used routinely in the SS clinic. The horizontal line of the scale was marked from 0 to 10. At the 0 point the phrase “not dry at all” was written and at the 10 point “as dry as the desert”. Those subjects who scored their dryness as ≥6/10 then underwent a Schirmer I test. If their results were ≤ 10mm wetting in 5 minutes in at least one eye, they were classified as non-SS DE KCS. Non-dry eye subjects (NDE) were enrolled if they stated that they did not have dry eyes and had Schirmer I scores of >10mm in both eyes. Exclusion criteria included: a history of ocular allergy, any ocular surface disease not related to dry eye and untreated blepharitis. Subjects previously diagnosed with blepharitis were allowed to use lid scrubs and hot soaks, but were not allowed to use topical antibiotics or topical anti-inflammatories. The screening assessment for non-SS DE and NDE subjects was performed within two months of the actual clinic visit for collection of conjunctival epithelial samples. Participants were required to confirm that their dry eye status had not changed at the collection visit. 



Conjunctival Impression Cytology (CIC)

Sterile Millipore, MF membranes (pore size 0.45 uM) were used in the collection of conjunctival epithelial cells. Two drops of a topical anaesthetic (Alcaine, Alcon), dosed 60 seconds apart, were applied to the right eye (only right eyes were sampled for this study). Fifteen seconds after the second drop of anaesthetic, the subject was instructed to look down, to expose the superior conjunctiva. The investigator, using sterile gloves, held the upper lid up and one piece of filter paper was placed on the superior region of the conjunctiva for five to seven seconds, then removed with blunt forceps and placed in a sterile pre-labelled 2 ml capped polypropylene centrifuge tube containing 1 mL of RLT RNA Isolation Buffer (Qiagen, Maryland, USA) with 0.01% -mercaptoethanol. A second piece of filter paper was immediately applied to the temporal conjunctiva and, after removal was placed in the same sterile tube as the first sample. All samples were immediately placed on dry ice and then transferred to -80oC for storage until processing.



RNA Isolation From CIC Samples and Reverse Transcription

Tubes containing 1 mL of RLT buffer (Qiagen) and the two impression cytology samples were allowed to thaw at room temperature and then vortexed for 30 seconds. Membranes were removed using a 21 gauge needle and samples were vortexed again and then passed through a 21 gauge needle 10 times. Extraction of total RNA proceeded according to manufacturer’s directions (RNeasy Minikit, Qiagen). The DNase step was performed as recommended. The final isolation step was conducted with 40 µL of RNAse free water and samples were stored at -80ºC. 



RNA quantity and quality were assessed by measuring the optical density using a Beckman DU530 Life Science UV/Visible Spectrophotometer at 260 nm and 280 nm. cDNA was synthesized from 16 µL of RNA sample using random hexamer primers with Superscript™ III First-Strand Synthesis System for RT-PCR (Invitrogen, Carlsbad, CA), according to the manufacturer’s instructions. 





Real Time qPCR

Amplification of cDNA was performed in multiplex real-time PCR reactions containing target and endogenous control oligonucleotide primers in the presence of gene-specific dye-labeled Taqman probes (Table I). Two microliters of cDNA was used for amplification in a 50-µL PCR reaction containing target (300 nM) and endogenous control (100 nM) oligonucleotide primers, control and target Taqman probes (100 nM), and Taqman® Universal PCR Master Mix (Applied Biosystems, Foster City, CA, 4304437, lot L00765, exp 01/31/10). Duplicate samples were analyzed in a 7500 Real Time PCR System (Applied Biosystems, Asset No. 518030). Conditions used for amplification were as follows: 50°C for 2 minutes, followed by an initial 10 minute denaturing step at 95°C. This was followed by 40 cycles of denaturing at 95°C for 45 seconds, annealing at 60°C for 45 seconds, and extension at 72°C for 60 seconds. Reporter dye fluorescence (Rn) data exceeding a manually-set critical threshold (Ct) was collected after the extension step of each cycle in real time using a 7500 Real Time PCR System with SDS software v1.3.1 (Applied Biosystems, Foster City, CA). 



A sample from a single, healthy volunteer was used to calculate the normalizing Ct. Target (TNF α) Ct data was subtracted from endogenous control (GAPDH) Ct data. This Ct value was subtracted from the Ct values calculated from study participants to generate the study statistic (Ct). 













Table 1

Oligonucleotide Primers and Probes Used For Relative Expression Analysis



Gene	Forward Primer	Reverse Primer	Taqman Probe

TNF	CCCCAGGGACCTCTCTCTAA	CAGCTTGAGGGTTTGCTACA	6FAM-GAGTGACAAGCCTGTAGCCC

GAPDH	GAAGGTGAAGGTCGGAGTCA	GACAAGCTTCCCGTTCTGAG	VIC-CAATGACCCCTTCATTGACC

     





Data analysis

Outlying data were identified by Dixon’s Q test prior to further statistical analysis in Statistica Ver7.1 (StatSoft Inc., Tulsa, OK, USA) and Microsoft Excel™ XLfit software. All data are reported as mean ± standard deviation. Statistical differences between groups were identified by using one-way ANOVA, Dunnett’s comparison of means and by Tukey’s test. Significance was identified at p<0.05 (α = 0.05). 





RESULTS



Demographics

A total of 76 subjects were enrolled into this study, and the subject demographics are displayed in Table 2. The mean age of the SS group was found to be statistically higher than the NDE group (p=0.024), but not different from the KCS group (p>0.05). Mean Schirmer I scores from both eyes collected without anaesthesia for five minutes revealed a significantly reduced (p<0.0001) tear flow in both SS (5.12 ± 5.96 mm) and KCS subjects (7.84 ± 7.35 mm), relative to NDE (23.83 ± 7.85 mm). There was no difference in mean Schirmer I scores between the KCS and SS groups (p = 0.19). 



Table 2: Summary of Demographic Information for Study Groups

Group	Mean Age (years)	Number of Female Subjects	Number of Male Subjects	Total Subjects

Control Group of Non-Dry-Eyed	52.4  ± 11.4	24	2	26

KCS

	59.3 ± 9.1	21	4	25

Sjogren’s Syndrome	60 ± 11.8*	21	4	25



The age of the SS group was higher than the NDE controls (*) (p=0.024)



Quantification of TNF mRNA Expression

TNF gene expression was found to be significantly higher in the SS group (2.48±1.79) compared to both non-SS DE (0.95±1.18; p<0.05) and NDE (0.84±0.51; p<0.05) groups (Figure 1). No difference in gene expression was found between the non-SS DE and NDE groups (p=NS).

    

Figure 1: Mean TNF mRNA expression in two distinct populations of aqueous deficient 

	   dry eye compared to control. 





Relationship of tear flow and TNF-α mRNA expression



No correlation was found between gene expression and Schirmer scores within any of the three study groups (Figures 2a to 2c). Schirmer scores from right eyes were used in correlation analysis as RNA was harvested only from right eyes. 



Figure 2: Correlation between TNF mRNA expression (RQ) and Schirmer Score (mm)





        (2A): Non-SS DE (Mean OD Schirmer Score = 6.7±4.8) 



 

       (2B) Non-Dry Eye Control (Mean OD Schirmer Score = 23.3±9.0)

 

       (2C) SS Dry Eye (Mean OD Schirmer Score = 5.0±5.5).

 

DISCUSSION:



Our results demonstrate that expression of conjunctival epithelial TNFα mRNA expression is significantly increased in SS subjects compared to both moderate non-SS aqueous deficient dry eye and control sub-populations. No difference in TNFα mRNA expression was found when non-SS dry eye and control groups were compared. To our knowledge, this is the first time that these two distinct subgroups of aqueous deficient dry eye have been studied simultaneously with a control group.  TNFα is regarded as a central mediator in human inflammatory responses (19) and has been implicated in the pathophysiology of dry eye. Our data is consistent with two previous studies that demonstrated that TNFα mRNA expression is significantly elevated in SS relative to control. (17, 18) Given the relatively abundant sample size in each of our three study groups, we can infer that at least at the genetic level, a distinctly different response occurs in SS relative to moderate DE and SS. Whether this difference translates to the protein level is unclear. 



Our findings are in conflict with one study in which tear film TNFα protein was found to be similar in SS and non-SS DE. (9) Numerous explanations could account for this finding relative to our genetic findings, including differential post transcriptional modulation and the contribution from additional sources for TNF, including the lacrimal gland and leakage from conjunctival vasculature. Also of note is that our non-SS dry eye subjects do fall in the moderate dry eye category with Schirmer scores of ≤10mm. It may be that a severe form of aqueous deficient non-SS dry eye would show similarities with SS dry eye from a genetic and protein level. Ideally, a study that simultaneously quantifies mRNA and tear protein expression in each subgroup and severity category would shed light on the relative roles of gene versus protein expression of various inflammatory mediators thought to be involved in dry eye.



No correlation was found between the degree of TNFα mRNA expression and tear flow as measured through the Schirmer 1 test. Desiccation is thought to be an important factor in driving an inflammatory response at the ocular surface. As reduced volume presumably could lead to less ocular surface protection and thus greater desiccation damage, it was interesting to find no correlation. As has been noted previously, significant correlation between any two signs or symptoms of dry eye has not yet been conclusively demonstrated, owing in part to the multifactorial nature of the disease. (20) Thus, although our results clearly support the notion that TNFis a key inflammatory mediator associated with SS-dry eye, the path linking stimulus versus signs of inflammation remains elusive. 



It is noteworthy that the epithelial samples analysed in this work were also used to quantify the expression of mucin genes. We have reported that clear differences in MUC1 (21) and MUC16 (22) gene expression exist in SS relative to both non-SS DE and control populations, whereas distinctions between non-SS and DE mucin expression are either absent or much reduced. Whether the sum total of regulated gene expression that occurs in SS patients reflects the increased magnitude of dry eye severity and/or different pathophysiological pathways remains to be determined.



It should be noted that the mean age of the SS group in our study was statistically higher than the NDE group. It is known that tear volume, production, stability and /or quality is reduced in the older population. (23, 24) However, whether the difference between our two study groups (60 vs 52) is relevant is questionable, as the age comparison for such changes in tear stability generally refers to populations <30 years of age relative to those older than 50. In addition, the impact of age on mRNA expression is not known. With respect to dry eye status, tear secretion data collected from Schirmer I suggests that both the SS and non-SS DE groups in our study were aqueous deficient dry eye, lending to the validity of our comparison. In addition, a critical comparison in this work was between the two sub-populations of dry eye, which were age and sex matched. 



In summary, our results conclude that expression of conjunctival epithelial TNFα mRNA is significantly elevated in a SS population relative to both non-SS and control populations and that no difference in expression was found between non-SS DE versus control. These data support the severe clinical presentation of dry eye in the SS population. 



References





1.	Lemp M, Baudouin C, Baum J, Dogru M, Foulks G, Kinoshita S, et al. The definition and classification of dry eye disease: Report of the Definition and Classification Subcommittee of the international Dry Eye Workshop (2007). Ocul Surf 2007;5:75-92.

2.	Vitali C, Bombardieri S, Jonsson R, Moutsopoulos H, Alexander E, Carsons S, et al. Classification criteria for Sjogren's syndrome: a revised version of the European criteria proposed by the American-European Consensus Group. Ann Rheum Dis 2002;61:554-558.

3.	Williamson J, Gibson A, Wilson T, Forrester J, Whaley K, Dick W. Histology of the lacrimal gland in keratoconjunctivitis sicca. British Journal of Ophthalmology 1973;57:852.

4.	Konttinen Y, Sorsa T, Hukkanen M, Segerberg M, Kuhlefelt-Sundstrom M, Malmstrom M, et al. Topology of innervation of the salivary glands by protein gene product 9.5 and synaptophysin immunoreactive nerves in patients with Sjogren's syndrome. J Rheumatol 1992;19:30-37.

5.	Walcott B, Brink P. Age-related decrease in the innervation density of the lacrimal gland in mouse models of the Sjogren's syndrome. In: Sullivan DA DD, Meneray MA, editor. Lacrimal Gland, Tear Film and Dry Eye Syndrome 2. New York: Plennum; 1998. p. 917-923.

6.	Andoh Y, Shimura S, Sawai T, Sasaki H, Takashimi T, Shirato K. Morphometric analysis of airways in Sjogren's syndrome. Am Rev Respir Dis 1993;148:1358-1362.

7.	Solomon A, Dursun D, Liu Z, Xie Y, Macri A, Pflugfelder S. Pro- and anti-inflammatory forms of interleukin-1 in the tear fluid and conjunctiva of patients with dry-eye disease. Invest Ophthalmol Vis Sci 2001;42:2283-2292.

8.	Tishler M, Yaron I, Geyer O, Shirazi I, Naftalie",,Quantification of Conjunctival TNFα mRNA Expression in Sub Populations of Human Aqueous Deficient Dry Eye,,,,core
78933374,07/07/2014,"Purpose The purpose of this study is to: 1) investigate and examine the Indian Health Service (IHS) and Bureau of Indian Affairs (BIA) programs providing services to American Indians and Alaska Natives (AI/ANs) whom have alcohol and drug problems; 2) focus upon information and budget systems concerning the monitoring of services provided and funds expended; and 3) research the scope of the alcohol and drug problems, financial costs, and human costs among AI/ANs. Methods For operational purposes, this study was split into two major components. The first component involved the examination of current IHS and BIA program information systems with regard to the collection of alcohol and substance abuse (ASA) data. This component culminated with the development and field testing of a prototype, community-based information system to collect data associated with the services being provided by IHS, BIA, and tribal programs. The second component was labeled as the Cost Analysis Component. This component involves description of the scope of the problem of ASA including an economic cost-of-illness analysis to estimate the economic costs of alcohol and drug use and abuse for the AI/AN sub-population and comparisons with the all race data for the base year 1985. Results Some study findings include: 1) the economic burden in 1985 was $900 million nation-wide for Indian communities; 2) both BIA and IHS program information systems were not meeting the requirements of P.L. 99-750; 3) a mechanism like the alcohol and drug community-based information systems prototype should be implemented to assist tribal action committees and communities to make informed decisions; and 4) IHS, BIA, tribal and urban programs have not sufficiently evaluated ASA treatment and prevention programs. Conclusion Although alcoholism, drug abuse, and mental health can be statistically separated, in reality they co-exist, and that future research should include all 3 categories","Indian Health Service, Staff Office of Planning, Evaluation and Research, Rockville, MD 20857.",Scope of the Problem of Alcohol and Substance Abuse Among American Indian and Alaska Native Communities,,,,core
211486899,2014-01-01T00:00:00,"Along the last decades persistent research endeavors in the areas of robotics andartificial intelligence revealed the great challenge of robot navigation, an areathat combines robot mobility with perception of the environment. Moreover,in modern human societies it is of great importance to build up machines thatcan be operated by non specialists or even by technologically illiterate people,such as youngsters or elderly. Therefore, the mobile robots to be released intothe market in the near future should possess, among others, the potential ofproducing meaningful internal perceptual representations of their own environment,capacitating them to cope a range of real-life situations and tasks. Tomake matters even more challenging, when it comes for mapping and navigation,robots should comprehend human concepts about places and objects,to skillfully deploy in human frequented environments. In response to thischallenge, intense research efforts, to build cognitive robots apt to competentlyperceive and understand their surroundings and to cooperate with humans,take place. With this goal in view, semantic mapping with mobile robots can constituteto a holistic solution in response to the aforementioned challenges. Thesemantic mapping is an augmented representation of the robot’s environmentthat -supplementary to the geometrical knowledge- encapsulates characteristicscompatible with human understanding. It provides several algorithmic opportunitiesfor innovative development of applications that will eventually lead tothe human robot interaction. The main objective of the PhD dissertation in handis the construction of accurate and consistent semantic maps facilitating amplerobot deployment in domestic environments.The motivation behind this PhD dissertation has been the observation thatthe plethora of the existing mapping and navigation algorithms are not ableto provide a sufficient representation of the environment in terms of humans’concepts. This is due to the fact that the mapping methods developed sofar focus on the construction of geometrical maps. Although some of thesesolutions proved to be capable of driving robots into specific target positions,they lack of high level cognition attributes, which would allow them to bring the human-robot interaction one step beyond. Aiming to remove this barrier, thisdissertation is oriented towards the direction of developing semantic mappingalgorithms for high level robot navigation. Therefore, this doctoral researchidentified the basic components of the semantic mapping and developed aninnovative solution for each one of them. Due to the fact that the semanticmapping requires an integrated system that comprises several subordinatemodules, a wide range of a algorithms that serve different tasks had to bedesigned and implemented. Within the context of this thesis several algorithmicmodules have been developed including a competent localization algorithm,novel simultaneously localization and mapping strategies, breakthrough place andobject recognition tactics, as well as the integration of all these methods undera time supervised framework able to produce consistent semantic maps. Dueto the fact that each subordinate module comprises an innovative solutionto the respective field, the resulting semantic mapping system constitutesa state of the art solution in the area of conceptual mapping with mobilerobots. The introduced algorithms exploit solely visual and depth sensors,while by combining basic tools from three different scientific areas such asrobotics, computer vision and machine learning the final objective is accomplished.Overall, the main contribution of this thesis to the advancement the stateof the art is the introduction of a stacked map hierarchy of four differenttype of maps, namely a metric, a topological, a labeled sparse topological andan augmented navigation one. Each of these representations accomplishes aunique purpose: (i) the metric is the physical (lower) layer; (ii) the topologicalone contains abstract geometrical information of the environment, i.e. pointclouds registered in a graph of nodes; (iii) the labeled sparse topological oneestablishes the spatiotemporal coherence by associating the respective nodes inthe topometric maps via place labels and geometrical transformations, enablingbidirectional exchange of information among the conceptual and metric mapsand, last, (iv) the augmented navigation map inheres the significance of thedetected places as well as their connectivity relationships, expressed in termsof their transition probability.The thesis in hand has been developed in a hierarchical fashion and can bedivided into four main chapters. The first one comprises a literature survey ofthe existing semantic mapping methods in which an explicit analysis of the sofar developed methods is sought. The insights of the semantic mapping arereviewed, the distinct components encompassing, to give a categorization of therelated literature, are studied the possible applications in mobile robotics areexamined and, lastly, the methods and databases available for benchmarkingare referred. Furthermore, a quality-based taxonomy of the existing semanticmapping methods highlights the dominant attributes such methods retain.More precisely, according to the scale, to which each method is expanded, the metric map could be either a single scene or a progressively created map.Another important attribute a typical semantic mapping method possessesis the existence of the respective topological map, that is an abstraction ofthe explored environment in terms of a graph. The nodes of such a graph areorganized in a geometrical manner, so as to simultaneously preserve conceptualknowledge about the explored places. Moreover, the modalities (single ormultiple visual cues) utilized to reason about the observed scene constitute anelement apt to distinguish the abundance of different methods. An additionalfeature in many recent semantic mapping techniques is the temporal coherencesuch a map reveals, which renders it useful for high-level activities, viz. taskplanning or human robot interaction.The second chapter refers to the description of the developed technologicalbackground required to build a consistent semantic map. Therefore, a majorcontribution of the first part is the development of an innovative visual odometryalgorithm able to operate in real time. This algorithm receives as input successivestereo pair of images from a stereoscopic camera mounted on a mobilerobot. It involves the detection of the salient landmarks between successiveimages. A depth estimation of these features is then obtained and a novelnon-iterative outlier detection and discarding methodology able to remove boththe mismatches between the features and the inserted errors due to the 3Dreconstruction procedure. A hierarchical motion estimation technique, whichproduces robust estimations for the movement of the robot is then adopted,thus providing refinements to the robot’s global position and orientation. Anadditional 3D reconstruction algorithm that operates on stereo images hasbeen developed providing accurate reconstruction of the area observed bythe robot. Moreover, the localization algorithm comprises the cornerstone forthe development of a simultaneously localization and mapping system suitablefor the 3D geometrical mapping of the explored environment. This 3D metricmapping system is based solely on an RGB-D sensor, where in course of robot’slocomotion 3D point clouds are merged with respect to the visual odometry.The resulted 3D map is refined by exploiting a random sample consensus planedetection algorithm accompanied by an iterative closest point registration step,among the dominant planes of the consecutive time instances, resulting thusin a very consistent geometrical 3D map. All the aforementioned developedalgorithms have been evaluated on a custom made robot platform bearing twostereoscopic cameras with different baselines and a RGB-D sensor.The third chapter encloses all the semantic mapping methods based onvisual cues that have been developed within this PhD dissertation. The firstone examines the overall traversability of the observed scene taking into considerationthe robot’s embodiment. This knowledge constitutes a cornerstone forthe autonomous robot navigation. The developed system utilizes an algorithm to retrieve specific characteristics of the environment using a stereo cameraand to produce a disparity map of the scene. Then, the v-disparity image iscalculated based on the disparity map. The v-disparity is then exploited by afeature extraction procedure to provide the system with respective conceptualvectors, which are used to train support vector machines in order to assess theoverall traversability of the scene. The traversable classified scenes are furtherprocessed and the likelihood distribution of the collision risk assessment, whenthe robot moves towards any direction, is calculated. The second part involvesthe description of a novel object recognition algorithm based on hierarchical temporalmemory networks. This constitutes a supervised learning method usedto recognize objects in different orientations. It introduces specific alternativerules for the design of each building block of a hierarchical temporal memorynetwork. These rules expand both the spatial and the temporal module ofthe network. Various type of input layers have been tested such as logppolarand saliency detection ones in order to find the solution that fits better in applicationsthat concern cluttered environments. The third part of this chaptercomprises the description of an innovative place classification algorithm. Withinthis work, in course of robot’s locomotion, salient visual features are detectedand they shape a bag-of-features problem, quantized by a neural gas to codethe spatial information for each scene. Each input image is transformed intoan appearance based histogram representation that abstracts the place in avery compatible and consistent manner. The learning procedure is performedby support vector machines able to accurately recognize multiple dissimilarplaces. The innovative appearance based algorithm produces semantic inferencessuitable for labeling unexplored environments. In the rest of this chaptera long range semantic mapping framework is described, where geometricalmapping, with place and object recognition is combined in order to construct ahuman oriented semasiological map. This framework features geometrical andsemasiological attributes capable to reveal relationships between objects andplaces in a real-life environment. The geometrical component consists of a 3Dmap, onto which a topological map is deployed, representing the explored areaas a graph of nodes. The semasiological part is realized by putting together aplace recognition algorithm and an object recognition one. The categorizationof the different places relies on the resolution of appearance-based consistencyhistograms, while for the recognition of objects in the scene we make use of thehierarchical temporal memory network boosted by a saliency attentional model.These semantical attributes are then deposited on each node of the topologicalmap, in order to augment it with the belief distributions regarding the visitedplaces, thus resulting in a 3D object map embodying geometrical and semanticalproperties.Finally, in the fourth chapter of this thesis a novel semantic mapping method suitable for robot navigation in a human compatible manner is illustrated. Themain objective of this chapter is to take into account the time proximity of therobot acquired frames within the semantic map. The goal of this method is; (i)to introduce a semasiological mapping method for robot exploration and (ii) tomake use of the constructed semantic map as a means to provide a hierarchicalnavigation solution. The semantic map is formed during the robot’s course,relying on the memorization of abstract place representations. It integrates thespace quantization, the time proximity of the acquired frames and the spatialcoherence in terms of a novel labeled sparse topological map. A time evolvingaugmented navigation graph is shaped determining the semantic topology ofthe explored environment and the physical connectivity among the recognizedplaces expressed by the inter-place transition probability. Concerning therobot navigation part, a human-robot interaction methodology is illustratedcapable of competently addressing go-to commands. As a product of thismodule, an augmented navigation graph is formed, which handles the high levelrobot navigation, forwarding the presumable sequence of places the robotshould traverse to reach its target location. Accordingly, for the low levellocal navigation the topometric data of the semantic map are made use of.Additionally, to assist the human robot interaction a graphical user interfacehas been developed providing an overall supervision of the mapping andnavigation procedure.The last chapter of this dissertation concludes the doctoral research conductedhereby. It discusses the achievements of this work, while it also highlightsthe open issues and the questions revealed during the developmentof the several algorithmic solutions. Additionally, some future trends of thesemantic mapping are outlined establishing the potential descendants of thisdissertation",'National Documentation Centre (EKT)',Χωροχρονικά συναφείς σημασιολογικοί χάρτες με κινούμενα ρομπότ,,10.12681/eadd/40861,,core
82980580,2013-06-18T00:00:00,"NanoBioTouch is an FP7 funded project that has an overall aim of developing NEMS tactile sensors for integration in an articulated robotic finger. The design of the sensors and signal processing are based on a multidisciplinary approach to improving the current understanding of the human mechano-transduction system. A range of NEMS arrays and bio-NEMS sensor technologies are being designed and fabricated in order to discriminate textures and assess their pleasantness with a resolution that is comparable to that of human subjects. They are being incorporated into a multiphalangeal biorobotic finger with artificial intelligence for enabling discriminative and affective touch. Silicone elastomer is used as the artificial skin with a fingerprint texture and it was found that their spacing relative to the individual sensors was important in generating discriminative textural signals. The current NEMS sensors enable discrimination among surfaces having spatial periods differing down to 40 μm, both under passive-touch and under human-like active-touch tasks. In the case of gratings, this corresponded to an accuracy of > 97.6%. A range of machine learning strategies are being adopted for interpreting the data that includes spatiotemporal phase analysis and a neuromorphic approach to translate the analogue signals into spikes that are similar to those produced by the mechanoreceptors in the human finger pad. In addition, signal processing software has been developed that autonomously learns tactile skills on the robotic finger using a curiosity-driven learning algorithm and that allows real-time motor control and sensor readout. Such curiosity-driven exploration enables the robotic finger to develop tactile skills, by rewarding the finger as when it explores novel methods for recognizing and learning about tactile sensations that it has not previously learnt. Interestingly, this leads to the sequential development of tactics, from the use of tapping motions to more complex sliding motions.  Significant progress has also been achieved for the bio-NEMS sensors, which involves the development of the equivalent of the subcutaneous tissue in the human finger pad by using alginate gels. Acellular gels exhibited a strong capacitance change with amplitude that depended on the imposed strain. When a population of live fibroblast cells was encapsulated in such gels there was an additional spiked response with a characteristic time that was believed to be associated with the transport of ions across the cell membranes. This behaviour has some analogies with the action potentials emitted by the mechanoreceptors",,NEMS based tactile sensing in an artificial finger,,,,core
76692113,2014-02-28T17:00:00,"Please join us for a panel covering all aspects of patent assertion and non-practicing entities and their effect on the patent industry. Our distinguished panelists are as follows:
Michael D. Friedman is Managing Director at Ocean Tomo, overseeing its Investments practice, which is composed of Investment Banking, Asset Management and Investment Research.
Ocean Tomo’s Investment Banking practice brings IP financing, monetization and capital markets solutions to corporations and other intellectual property owners. Recent notable transactions include the leveraged buyout of Mosaid Technologies and the sale of MIPS Technologies’ IP portfolio. Ocean Tomo Asset Management, where Mr. Friedman serves as Chief Investment Officer, engages in public equity, special situations and private equity investing where intellectual property insight drives alpha creation. Investment Research works in parallel with institutional investors, hedge funds and private equity funds advising them on capital allocations to IP-themed investments.
Mr. Friedman holds a JD from the University of Chicago Law School, where he worked as Research Editor of the University of Chicago Legal Forum. He also holds a BS in marine engineering and nautical science from the U.S. Merchant Marine Academy. Mr. Friedman is a member of the board of directors of the Intellectual Property Exchange International, the world’s first IP-focused financial exchange, and a Lecturer in Law at the University of Chicago Law School.
Jay P. Kesan is a Professor at the University of Illinois, College of Lawwhere he is H. Ross & Helen Workman Research Scholar and Director of the Program in Intellectual Property and Technology Law. Professor Kesan received his J.D. summa cum laude from Georgetown University, where he received several awards including Order of the Coif and served as associate editor of the Georgetown Law Journal. After graduation, he clerked for Judge Patrick E. Higginbotham of the United States Court of Appeals for the Fifth Circuit. Prior to attending law school, Jay Kesan – who also holds a Ph.D. in electrical and computer engineering from the University of Texas at Austin – worked as a research scientist at the IBM T.J. Watson Research Center in New York. He is a registered patent attorney and practiced at the former firm of Pennie & Edmonds LLP in the areas of patent litigation and patent prosecution. In addition, he has published numerous scientific papers, and he has obtained several patents in the U.S. and abroad. His recent publications can be found on SSRN (Social Science Research Network) at http://www.ssrn.com. At the University of Illinois, Professor Kesan is appointed in the College of Law, the Institute of Genomic Biology, the Department of Electrical & Computer Engineering, the Information Trust Institute, the Coordinated Science Laboratory, the College of Business, and the Department of Agricultural & Consumer Economics. Professor Kesan continues to be professionally active in the areas of patent litigation and technology entrepreneurship. He has served as a special master in patent litigations, and has served as a technical and legal expert and/or counsel in patent matters. He also serves on the boards of directors/advisors of start-up technology companies.
Matthew Levy is Patent Counsel at the Computer and Communications Industry Association, where he handles legal, policy advocacy, and regulatory matters related to patents and is lead blogger for CCIA’s Patent Progress.
Matt joined the CCIA in 2013 from the IP boutique Cloudigy Law, PLLC. He has also been an associate at Finnegan, Henderson, Farabow, Garrett, & Dunner, LLP and at Hogan & Hartson LLP. He got first-hand experience in both patent prosecution and patent litigation, including defending clients against patent trolls.
Matt graduated from the Georgetown University Law Center magna cum laude with the Order of the Coif, winning the ABA/BNA Award for Excellence in Intellectual Property. He received a Master’s in Computer Science from the University of Kentucky, where he won the Presidential Fellowship twice. His research at UK was in computational complexity theory and artificial intelligence. He received a Bachelor’s degree in Computer Science from the University of Southern Maine.
Before law school, Matt was a software engineer at IBM in Lexington, KY, as part of the team that developed and maintained Lotus Sametime, IBM’s real-time messaging and conferencing product. He is co-inventor on U.S. Patent No. 8,521,830.
Matt is still a software developer in his spare time. He developed an app for the iPad, Federal Local Rules, which is available on the App Store.
Laura Beth Miller is a shareholder at Brinks Gilson & Lione, where she co-chairs the firm’s practice before the U.S. International Trade Commission (“ITC”). With over two decades of trial and arbitration experience, Ms. Miller has handled substantial first and second chair responsibilities. She focuses her practice on patent, trade secret and trademark issues, as well as client counseling on complex commercial issues, including licensing, anti-trust and contract issues affecting business operations, product services and technology. In addition to representing major Fortune 500 companies, she is an adjunct professor at The John Marshall Law School, in Chicago, Illinois. She is a frequent speaker on intellectual property issues both in the United States and abroad, and has written a number of articles on intellectual property topics.
Ms. Miller received her B.A. from the University of Virginia and her J.D. from The College of William and Mary Marshall Wythe School of Law. She is licensed to practice before the United States Supreme Court, the Supreme Court of Illinois, the United States Patent and Trademark Office, and numerous federal courts. She has been recognized as one of Illinois\u27 leading intellectual property lawyers by Chambers USA, and has been named a Leading Intellectual Property Lawyer and one of the Top 50 Women Business Litigation Lawyers in Illinois by the Leading Lawyers Network. She also serves on the management teams at Brinks Gilson & Lione.
K. McNeill Taylor, Jr., is General Counsel at Round Rock Research, LLC. Neill Taylor joined Round Rock in 2012 as Vice President Law and General Counsel responsible for supervising and administering all legal affairs for the company.
Before joining Round Rock, Neill was Corporate Vice President and Chief IP Counsel of Motorola Mobility, Inc., responsible for the intellectual property law and litigation functions. He managed the offensive and defensive patent litigation in support of MMI’s Android smart phones, and the preparation, prosecution and legal support for MMI’s patent portfolio of approximately 24,000 patents and applications worldwide. Neill had a leading role in setting the strategy for and negotiating MMI’s $12.5B acquisition agreement with Google in August 2011.
After joining Motorola, Inc. in 2002, Neill led its efforts to protect intellectual property for a number of Motorola businesses through patent operations, licensing, in-business counseling, defensive matters and litigation. He also served as general counsel in the integration of Symbol Technologies, Inc. after its $4B acquisition by Motorola in January 2007.
Prior to joining Motorola, Neill served as vice president, general counsel and assistant secretary of Corning Cable Systems and Siecor Corp. Before that he held patent counsel positions with Corning Inc. and Schlumberger Ltd., and began his patent law career as an associate with Fish & Neave, a patent litigation firm in New York.
Neill received his law degree from the University of Chicago and a bachelor’s degree in physics and philosophy from Duke University, where he graduated magna cum laude and was an Angier B. Duke scholar.
Andrew W. Williams is a partner with McDonnell Boehnen Hulbert & Berghoff LLP. Dr. Williams\u27 practice primarily consists of patent litigation, prosecution, and opinion work in the areas of biotechnology, pharmaceuticals, and chemistry. Dr. Williams is a contributing author to the Patent Docs weblog, a site focusing on biotechnology and pharmaceutical patent law. Dr. Williams earned his Ph.D. in Molecular Biophysics and Biochemistry at Yale University. He earned his J.D. from George Washington University Law School with highest honors, and was Managing Editor of the law review",Northwestern Pritzker School of Law Scholarly Commons,Patent Assertion and Non-Practicing Entities Panel,,,,core
380812268,2014-03-01T00:00:00,"Education is being revolutionized by the introduction, of mobile technologies in the teaching and learning process. However, studies that focus in the application of mobile technologies to informal
learning environments is scarce and not systematized [1]. This is the reason for conducting a research
project that involved a urban game MobiGeo, designed in to take better advantage of the flexibility and
ubiquity offered by the Mobile Learning (ML) but also taking into account the importance of motivation
and interaction to enhance students learning.
The definition of ML has been a complicated task for researchers, but there are assumptions that can
not be neglected: the mobility, portability and ubiquity [2], these are features that will drive new learning spaces and thus motivate students. This idea is supported by [2] that introduces the concepts
of ""just in time"", ""just enough"" and ""just-for-me"" and [4] that speaks of the triad ""location
independence”, ""independence time"" and ""meaningful content”.
These principles of ""anytime"" and ""anywhere"" consolidated by mobile technologies came to renew the
variety of educational activities available to teachers and in this context arises the concept of mobile
location-based games. According to [5] ""these games are played in physical space, but at the same
time, they are supported by actions and events in an interconnected virtual space"", which can be
classified into three categories: ludic, pedagogic and hybrid. By being in direct contact with the
contents to assimilate and move in a real context, students will have a more significant learning [6]
and this will result in the mobilization of knowledge in different contexts. To make the connection
between the physical and the virtual world, our research has made use of Qr codes as these devices provide information in real time and in a dynamically way.
For this research was idealized an urban game called “MobiGeo”, that respect the principles
suggested by [7] and that has as common thread the history of the European Union. To measure
results the researchers developed a questionnaire that was adapted from a proposal of [8] which
created a ""Model to evaluate Educational Games”, so our proposal was built taking into account the
motivational model of Kirkpatrick (level1) and encompassing three major dimensions:
Motivation/Interest, Interaction and Perceived Learning. To assess the Motivation/Interest was used
the Model ARCS (Relevance, Confidence and Satisfaction) and items of Fun, Immersion and
Challenge of “Game User Experience”. On the other hand the interaction was evaluated by items of
the Social Interaction dimension of the “Game of User Experience”, the Learning Perceptions were
evaluated by Bloom's Taxonomy (Knowledge category).
In this paper we present the design and implementation of the MobiGeo outdoor learning activity with
a group of 173 students from the 7th grade of a basic school in the north of Portugal. Initial results
show that this urban game with Qr codes was an adequate activity to use in informal learning
environments that could engage students in gaming with high degrees of motivation and interaction in order to solve the tasks presented to them and so consolidate and acquired new knowledge about the European Union.CIEC – Research Centre on Child Studies, UM (FCT R&D 317","'Associated Management Consultants,  PVT., Ltd.'",The implementation of mobile location based-games and Qr codes : the case of MobiGeo,,,"[{'title': None, 'identifiers': ['2340-1079', 'issn:2340-1079']}]",core
22728786,02/08/2013,"Abstract. Enterprise distributed real-time and embedded (DRE) publish/subscribe (pub/sub) systems manage resources and data that are vital to users. Cloud computing—where computing resources are provisioned elastically and leased as a service—is an increasingly popular deployment paradigm. Enterprise DRE pub/sub systems can leverage cloud computing provisioning services to execute needed functionality when on-site computing resources are not available. Although cloud computing provides flexible on-demand computing and networking resources, enterprise DRE pub/sub systems often cannot accurately characterize their behavior a priori for the variety of resource configurations cloud computing supplies (e.g., CPU and network bandwidth), which makes it hard for DRE systems to leverage conventional cloud computing platforms. This paper provides two contributions to the study of how autonomic configuration of DRE pub/sub middleware can provision and use on-demand cloud resources effectively. We first describe how supervised machine learning can configure DRE pub/sub middleware services and transport protocols autonomically to support end-to-end quality-of-service (QoS) requirements based on cloud computing resources. We then present results that empirically validate how computing and networking resources affect enterprise DRE pub/sub system QoS. These results show how supervised machine learning can configure DRE pub/sub middleware adaptively in &lt; 10 μsec with bounded time complexity to support key QoS reliability and latency requirements",,Adapting Distributed Real-time and Embedded Pub/Sub Middleware,,,,core
23824179,28/01/2014,"Massively multiplayer online games (MMOGs) have become increasingly popular in the recent years, particularly in the form of online role-playing games (MMORPGs). These games support up to several ten thousand players interacting in a virtual game world. The current commercially successful games are client-server based, which is feasible for relatively slow role-playing games. Those have modest bandwidth and latency requirements and are paid for by their customers. For MMOGs with higher realtime requirements and/or a smaller number of customers willing to pay, peer-to-peer networking seems to be a serious alternative. This work analyzes the implementation of both a client-server and a peer-to-peer networking model for the prototype shooter game Planet π4. Initially, a survey introduces recent academic approaches to peer-to-peer systems specifically designed for games. Of those, one system is selected for implementation with Planet π4. Planet π4 is improved in several aspects for the purpose of analyzing various network implementations. First, its architecture is restructured and cleaned to allow for an easy replacement of the networking component. Second, its core is modified to work in a completely event-based mode, supporting the execution of the game in an discrete-event-based network simulator. Third, a simple artificial intelligence player is developed for workload generation in large (and possibly simulated) networks. Furthermore, the newly developed transport protocol CUSP is applied for the network implementation, thus the game is the first real application using CUSP. Finally the game Planet π4 is integrated with the CUSP network simulator, allowing to run the whole game in a simulated network without the need for modification of its core components. Kurzfassung Massen-Mehrspieler-Onlinespiele (Massively multiplayer online games; MMOGs) wurden in den vergangenen Jahren zunehmend populär, insbesondere in Form von Online-Rollenspielen (MMORPGs). I",,22.10.2009 Implementation of a Peer-to-Peer Multiplayer Game with Realtime Requirements,,,,core
324119383,13/12/2013,"A Hipertensão Arterial Sistêmica (HAS), condição crônica altamente prevalente, é um dos principais fatores de risco para o desenvolvimento e progressão da Doença Renal Crônica (DRC), que é considerada um problema de saúde pública em todo o mundo. A prevalência de DRC vem aumentando mundialmente, com incremento anual de 8 a 16%, que é maior do que o crescimento populacional geral. A DRC é definida como a presença de lesão renal, associada ou não à diminuição da Taxa de Filtração Glomerular (TFG) < 60 mL/min/1,73m2 por um período ≥ 3 meses. O diagnóstico da DRC baseia-se na identificação de grupos de risco (idosos, obesos, portadores de HAS ou diabetes), na presença de lesão renal e na redução da TFG. Na prática clínica, a função renal pode ser avaliada pela dosagem da creatinina, pela depuração da creatinina e pela estimativa da TFG por meio de fórmulas. No entanto, os estudos não mostram um consenso sobre o método de detecção da DRC a ser utilizado. Neste contexto, a DRC, em geral, tem sido subdiagnosticada e tratada inadequadamente, culminando em um alto custo de tratamento, sobretudo pela não implementação de medidas preventivas. Assim, destaca- se a importância da Atenção Primária à Saúde (APS), por meio da Estratégia Saúde da Família (ESF), no diagnóstico precoce e encaminhamento dos pacientes a especialistas. Sendo assim, o objetivo do presente estudo é avaliar os diferentes métodos de diagnóstico da DRC e sua prevalência oculta em portadores de HAS cadastrados na ESF do município de Porto Firme-MG, com ênfase no papel estratégico da APS na prevenção de agravos e enfermidades. Trata-se de um estudo transversal com os portadores de HAS acompanhados pela ESF do município, cuja coleta dos dados se deu por meio de entrevistas, avaliações antropométrica, clínica e bioquímica. O diagnóstico de DRC foi realizado por meio da dosagem de creatinina sérica, depuração de creatinina e avaliação da TFG estimada a partir das fórmulas CG, CG-corrigido, MDRD-6, MDRD-4, CKD-EPI. Uma vez identificada alguma alteração em um dos testes realizados, os mesmos foram repetidos após três meses para confirmação do diagnóstico. Para a análise dos dados utilizou-se o software SPSS. O nível de significância adotado para rejeição da hipótese de nulidade foi α = 0,05. O projeto de pesquisa foi submetido à análise e aprovado pelo Comitê de Ética em Pesquisa com Seres Humanos da Universidade Federal de Viçosa (UFV), protocolo 044/2012. Foram avaliados 293 indivíduos, com média de idade de 65,8 anos; 76,0% eram do sexo feminino; 60,8% eram casados; 74,1% eram aposentados; 90,0% possuíam baixa renda e baixa escolaridade e 66,9% apresentavam sobrepeso. A maior concordância com a depuração real, segundo o kappa ponderado, se deu pela fórmula CKD-EPI (kappa = 0,362) e a menor concordância pela fórmula CG (kappa = 0,311). Observou-se uma correlação significante (p<0,001), porém moderada entre a depuração real e todas as fórmulas avaliadas, sendo a maior correlação com a fórmula CKD-EPI (r = 0,612). A fórmula CKD-EPI apresentou o menor número de indivíduos abaixo do limite inferior de concordância pela análise de Bland-Altman. Em relação à prevalência, 38,6% (IC 95%: 33,0 - 44,2) da população estudada apresentavam DRC, sendo que aproximadamente 14% estavam em estágio mais avançado da doença (TFG < 45 mL/min/1,73m2). Quando as variáveis foram comparadas nos diferentes estágios, observou-se relação estatisticamente significante da DRC com idade, escolaridade, uso de álcool, sobrepeso, risco cardiovascular, creatinina e microalbuminúria. Quando avaliada a razão de prevalência, observou-se associação estatisticamente significante com idade e creatinina. Visto tais resultados, recomenda-se que, juntamente com o valor da creatinina sérica, os serviços de patologia clínica forneçam a estimativa da TFG, o que permitirá a visualização imediata da DRC. Sugere-se também a divulgação e utilização das tabelas que fornecem a TFG, sendo esta, uma ferramenta de baixo custo e fácil utilização, que poderá ser aplicada também como rotina na APS. O estudo reforça o papel estratégico da APS no diagnóstico e prevenção de agravos e enfermidades, visto que esta é a porta de entrada dos indivíduos nos serviços de saúde. Por fim, os achados deste estudo ratificam a necessidade de capacitação da equipe de saúde envolvida no tratamento desses pacientes, fomentando a prevenção e diagnóstico da DRC nos estágios iniciais.High Blood Pressure (HBP) is a highly prevalent chronic disease and is one of the main risk factors for the development and progression of Chronic Kidney Disease (CKD), which is a worldwide health problem. The prevalence of CKD has been increasing on a global scale, with an annual growth of between 8% and 16%, which is higher than the growth of the general population. CKD is defined as the presence of a kidney injury, associated or not with a decrease in the glomerular filtration rate (GFR) < 60 mL/min/1.73m2 for a period ≥ 3 months. A diagnosis of CKD is based on identifying risk groups (the elderly and obese, individuals with HBP or diabetes) in the presence of the kidney injury and a reduction of the GFR. In clinical practice, the kidney function can be assessed by measuring creatinine or creatinine clearance and by estimating the GFR through formulae. However, there is no consensus among previous studies about the method that should be used to detect CKD. In this context, CKD has generally been underdiagnosed and inadequately treated, culminating in high treatment costs, mainly due to the non-implementation of preventative measures. Thus, the importance of primary health care (PHC), through the family health strategy (FHS), is significant, in terms of an early diagnosis and sending patients to see specialists. The aim of the present study was to study and assess the different diagnosis methods of CKD and its hidden prevalence in individuals with HBP registered in the FHS of the municipality of Porto Firme-MG, with an emphasis on the strategic role of the PHC in the prevention of injuries and illnesses. This is a cross-sectional study using individuals with HBP who are monitored by the FHS of the city, collecting data through interviews, as well as anthropometric, clinical and biochemical assessments. CKD was diagnosed by measuring creatinine serum, creatinine clearing and estimating the GFR through the CG, CG-corrected, MDRD-6, MDRD-4 and CKD-EPI formulae. When an abnormality was identified in one of the tests carried out, the test was repeated three months later to confirm the diagnosis. Data analysis was conducted using SPSS software. The level of significance adopted to reject the null hypothesis was α = 0.05. The present study was approved by the Human Rights Ethics Committee of the Universidade Federal de Viçosa (UFV) under protocol number 044/2012. In total, 293 individuals were assessed, with a mean age of 65.8 years; 76.0% were female; 60.8% were married; 74.1% were retired; 90.0% had a low income and low level of education and 66.9% were overweight. The greatest agreement with real clearance, according to the weighted Kappa, was achieved using the formula CKD-EPI (kappa = 0.362) and the lowest agreement was achieved with the formula CG (kappa = 0.311). A significant, although moderate, correlation (p<0.001) was found between real clearing and all of the formulae assessed, with the greatest correlation recorded for the formula CKD-EPI (r = 0.612). The formula CKD-EPI exhibited the lowest level of individuals under the lowest limit of agreement in the Bland-Altman analysis. With regards to prevalence, 38.6% (CI 95%: 33.0 – 44.2) of the population studied exhibited CKD and approximately 14% were in the most advanced stage of the disease (GFR < 45 mL/min/1.73m2). When the variables were compared in the different stages, a statistically significant association was found between CKD and age, education, alcohol consumption, overweight individuals, cardiovascular risk, creatinine and microalbuminuria. When the prevalence ratio was assessed, a statistically significant association was found between age and creatinine. Based on these results, it is recommended that, together with the value of creatinine serum, clinical pathology services should provide an estimate of the GFR, which will enable an immediate visualization of the CKD. It is also suggested that tables that show the GFR should be used and distributed as a low-cost tool that is easy to use and could also become routine in PHC. The present study reinforces the strategic role of PHC in the diagnosis and prevention of injuries and illnesses since it is the entrance to health services for individuals. The findings of the present study confirm the need to train health staff involved in the treatment of these patients, promoting the prevention and diagnosis of CKD in its early stages.Coordenação de Aperfeiçoamento de Pessoal de Nível Superio",Universidade Federal de Viçosa,Diagnosis and hidden prevalence of chronic kidney disease in individuals with hypertension: the strategic role of primary health care in the prevention of injuries and illnesses,,,,core
296651874,2015-11-26T15:16:45Z,"Environment perception is a major research issue which is very important in the field of robotic system. In order to identify the horizon line and the drivable region, we have proposed a visual-perception system based on an automatic image discarding method as a simple solution to improve the system performance. In this paper, all these previous methods are organized in a visual-perception layer which also includes a method for estimating the risk-of-collision based on Pearson's Correlation Coefficient and an evolution of the Threshold and Horizon Finder (TH Finder). These methods were successfully evaluated from real data. © 2012 IEEE.16IEEE Latin American Robotics CouncilBonin-Font, F., Ortiz, A., Oliver, G., Visual Navigation for Mobile Robots: A Survey (2008) Journal of Intelligent and Robotic SystemsKim, B., Hubbard, P., Necsulescu, D., (2003) Swarming Unmanned Aerial Vehicles: Concept Development and Experimentation, A State of the Art Review on Flight and Mission Control, , DRDC-OTTAWATM-2003-176Technical MemorandumThrun, S., Stanley, the robot that won the DARPA Grand Challenge (2006) Journal of Robotic Systems, 23 (9), pp. 661-692. , DARPA Grand Challenge(2007) Spirit of Berlin: An Autonomous Car for the DARPA Urban Challenge Hardware and Software Architecture, , http://www.darpa.mil/grandchallenge/TechPapers/Team_Berlin.pdf, Team Berlin retrieved 02 Dec. 2010Gietelink, O., Ploeg, J., De Schutter, B., Verhaegen, M., Development of advanced driver assistance systems with vehicle hardware-in-the-loop simulations (2006) Vehicle System Dynamics, 44 (7), pp. 569-590Miranda Neto, A., Rittner, L., A Simple and Efficient Road Detection Algorithm for Real Time Autonomous Navigation based on Monocular Vision (2006) Proceedings of the 2006 IEEE 3rd Latin American Robotics SymposiumUlrich, I., Nourbakhsh, I., Appearance-Based Obstacle Detection with Monocular Color Vision (2000) Proceedings of the AAAI National Conference on Artificial Intelligence, July/August 2000, pp. 866-871Bertozzi, M., Broggi, A., Fascioli, A., Vision-based intelligent vehicles: State of the art and perspectives (2000) Robotics and Autonomous Systems, 32, pp. 1-16Miranda Neto, A., Rittner, L., Leite, N., Zampieri, D.E., Lotufo, R., Mendeleck, A., Pearson's Correlation Coefficient for Discarding Redundant Information in Real Time Autonomous Navigation System (2007) IEEE International Conference on Control Applications Part of IEEE Multi-conference on Systems and Control (CCA - MSC 2007), SingapuraMiranda Neto, A., Rittner, L., Leite, N., Zampieri, D.E., Victorino, A.C., Nondeterministic Criteria to Discard Redundant Information in Real Time Autonomous Navigation Systems based on Monocular Vision (2008) IEEE Multi-conference on Systems and Control (ISIC - MSC 2008), San Antonio, Texas, US, , ISIC Invited PaperMiranda Neto, A., Victorino, A.C., Fantoni, I., Zampieri, D.E., Real-Time Dynamic Power Management based on Pearson's Correlation Coefficient (2011) IEEE International Conference on Advanced Robotics (ICAR 2011), Tallinn, EstoniaRodgers, J.L., Nicewander, W.A., Thirteen Ways to Look at the Correlation Coefficient (1988) The American Statistician, 42, pp. 59-66Pearson, K., (1895) Royal Society Proceedings, 58, p. 241Eugene, Y.K., Johnston, R.G., (1996) The Ineffectiveness of the Correlation Coefficient for Image Comparisons, , Technical Report LA-UR-96-2474, Los Alamos(2005) DARPA Grand Challenge, , http://www.darpa.mil/grandchallenge05/, DARPA(2006) Stanford Racing Team's Entry in the 2005 DARPA Grand Challenge, , http://www.stanfordracing.org/, June 10Gonzalez, C.R., Woods, E.R., (2000) Digital Image Processing, , Ed. Edgard Blücher, S.Paulo, BrazilSahoo, P.K., Soltani, S., Wong, A.K.C., A survey of thresholding techniques (1988) Comput. Vision Graphics Image Processing, 41, pp. 233-260Otsu, N., A threshold selection method from gray-level histogram (1978) IEEETransactions on Systems, Man, and CyberneticsLee, U.S., Chung, Y.S., Park, H.R., A Comparative Performance Study of Several Global Thresholding Techniques for Segmentation (1990) Computer Vision, Graphics, and Image ProcessingLim, K.H., Vision-based Lane-Vehicle Detection and Tracking (2009) Chapter 13 of IAENG Transactions on Engineering Technologies, 3, pp. 157-171. , Special Edition', American Institute of PhysicsDahlkamp, H., Self-Supervised Monocular Road Detection in Desert Terrain (2006) Proceedings of the Robotics Science and Systems ConferenceEttinger, S., Vision-Guided Flight Stability and Control for Micro Air Vehicles (2003) Advanced Robotics, 17, pp. 617-640Sezgin, M., Sankur, B., Survey over image thresholding techniques and quantitative performance evaluation (2004) Journal of Electronic Imaging, 13, pp. 146-165Miranda Neto, A., Victorino, A.C., Fantoni, I., Zampieri, D.E., Robust Horizon Finding Algorithm for Real Time Autonomous Navigation based on Monocular Vision (2011) IEEE International Conference on Intelligent Transportation Systems (ITSC 2011), Washinton DC, USRauskolb, F.W., Caroline: An autonomously driving vehicle for urban environments (2008) Journal of Field Robotics, 25 (9), pp. 674-724http://www.youtube.com/user/kingdombr, April, 25 201",,A Visual-perception Layer Applied To Reactive Navigation,,10.1109/SBR-LARS.2012.8,,core
102670227,03/09/2015,"Abstract. Off-road autonomous navigation is one of the most difficult automation challenges from the point of view of constraints on mobility, speed of motion, lack of environmental structure, density of hazards, and typical lack of prior information. This paper describes an autonomous navigation software system for outdoor vehicles which includes perception, mapping, obstacle detection and avoidance, and goal seeking. It has been used on sev-eral vehicle testbeds including autonomous HMMWV’s and planetary rover prototypes. To date, it has achieved speeds of 15 km/hr and excursions of 15 km. We introduce algorithms for optimal processing and computational stabilization of range imagery for terrain map-ping purposes. We formulate the problem of trajectory generation as one of predictive control searching trajectories in command space. We also formulate the problem of goal arbitration in local autonomous mobility as an optimal control problem. We emphasize the modeling of vehicles in state space form. The resulting high fidelity models sta-bilize coordinated control of a high speed vehicle for both obstacle avoidance and goal seeking purposes. An intermediate predictive control layer is introduced between the typical high-level strategic or artificial intelli-gence layer and the typical low-level servo control layer. This layer incorporates some deliberation, and some envi-ronmental mapping as do deliberative AI planners, yet it also emphasizes the real-time aspects of the problem as d",,An Approach to Rough Terrain Autonomous Mobility,,,,core
102728677,21/10/2015,"Abstract — This paper details the research, development, and demonstrations of real-world systems intended to assist the driver in urban environments, as part of the Urban Intelligent Assist (UIA) research initiative. A 3-year collaboration between Audi AG, Volkswagen Group of America Electronics Research Laboratory, and UC San Diego, the driver assistance portion of the UIA project focuses on two main use cases of vital importance in urban driving. The first, Driver Attention Guard, applies novel computer vision and machine learning research for accurately tracking the driver’s head position and rotation using an array of cameras. The system then infers the driver’s focus of attention, alerting the driver and engaging safety systems in case of extended driver inattention. The second application, Merge and Lane Change Assist, applies a novel probabilistic compact representation of the on-road environ-ment, fusing data from a variety of sensor modalities. The system then computes safe and low-cost merge and lane-change maneuver recommendations. It communicates desired speeds to the driver via Head-up Display, when the driver touches the blinker, indicating his desired lane. The fully-implemented systems, complete with HMI, were demonstrated to the public and press in San Francisco in January of 2014. I",,Looking-in and Looking-out Vision for Urban Intelligent Assistance: Estimation of Driver Attentive State and Dynamic Surround for Safe Merging and Braking,,,,core
103588049,2014,"Traffic prediction lies at the core of many intelligent transport systems (ITS). Commonly deployed prediction methods such as support vector regression and neural networks achieve good performance by explicitly predicting the traffic variables (e.g., traffic speed or volume) at each road segment in the network. For large traffic networks, predicting traffic variable at each road segment may be unwieldy, especially in the setting of real-time prediction. To tackle this problem, we propose an alternative approach in this paper. We first generate low-dimensional representation of the network, leveraging on the column-based (CX) decomposition of matrices. The low-dimensional model represents the large network in terms of a small subset of road segments. The future state of the low-dimensional network is predicted by standard procedures, i.e., support vector regression. The future state of the entire network is then inferred by extrapolating the predictions of the subnetwork, using the CX decomposition. Numerical results for a large-scale road network in Singapore demonstrate the efficiency and accuracy of the proposed algorithm. Index Terms — Prediction in large networks, low-dimensional model",,Compressed prediction of large-scale urban traffic,,10.1109/icassp.2014.6854752,,core
102027787,2012,"Video services are being adopted widely in both mobile and fixed networks. For their successful deployment, the con-tent providers are increasingly becoming interested in evalu-ating the performance of such traffic from the final users ’ per-spective, that is, their Quality of Experience (QoE). For this purpose, subjective quality assessment methods are costly and can not be used in real time. Therefore, automatic estimation of QoE is highly desired. In this paper, we propose a no-reference QoE monitoring module for adaptive HTTP stream-ing using TCP and the H.264 video codec. HTTP stream-ing using TCP is the popular choice of many web based and IPTV applications due to the intrinsic advantages of the pro-tocol. Moreover, these applications do not suffer from video data loss due to the reliable nature of the transport layer. How-ever, there can be playout interruptions and if adaptive bitrate video streaming is used then the quality of video can vary due to lossy compression. Our QoE estimation module, based on Random Neural Networks, models the impact of both factors. The results presented in this paper show that our model accu-rately captures the relation between them and QoE",,Quality of Experience estimation for Adaptive HTTP/TCP video streaming using H.264/AVC,,,,core
100071853,30/11/2014,"Abstract. This paper presents a semi-automatic visualization method for the evaluation of urban environments that is based on artificial intelligence. It proposes the use of agent-based crowd simulation software on a mid-scale urban planning level for design evaluation. The information on agents ’ movements is noted in standard raster images. The results are maps that are easy to understand. These maps show movement paths of the agents and density and give further conclusion on bottlenecks in planning contexts. Key measures, like the occupant movement in a given district, until now relied greatly on empirical knowledge or data that could be only gathered after an urban design had become built reality. Our method focuses on the adaptation of common software technology that is originally situated in film and TV productions. A practical workflow shows how our method can be easily integrated in daily design tasks",,eCAADe 26 493-Section 12: Prediction and Evaluation 2 Crowd Simulation for Urban Planning,,,,core
236295509,2014-01-01T08:00:00,"The aim of this thesis is to examine and develop new techniques in stormwater Best Management Practices (BMP) for nutrient and erosion reduction and monitoring by incorporation of low impact green technologies and sensor networks.  Previous research has found excessive nutrient loading of nitrogen and phosphorus species from urban stormwater runoff can lead to ecological degradation and eutrophication of receiving lakes and rivers (Fareed and Abid, 2005).  In response, the Florida Department of Environmental Protection (FDEP) has set forth reduction goals as established in Total Maximum Daily Load (TMDL) reports to reduce nutrient loading and restore, or maintain, Florida water bodies to reasonable conditions.  Often times current stormwater management practices are not sufficient to attain these goals and further improvements in system design are required.  In order to reach these goals, affordable technologies designed for both nutrient reduction and monitoring of system performance to deepen and improve our understanding of stormwater processes are required.  Firstly this thesis examines the performance of three types of continuous-cycle Media Bed Reactors (MBRs) using Bio-activated Adsorptive Media (BAM) for nutrient reduction in three retention ponds located throughout the Central Florida region.  Chapter 2 examines the use of a Sloped and Horizontal MBRs arranged in a baffling configuration, whereas Chapter 3 examines the field performance of a Floating MBR arranged in an upflow configuration.  Each MBR was analyzed for performance in reducing total phosphorus, soluble reactive phosphorus, total nitrogen, organic nitrogen, ammonia, nitrates + nitrites, turbidity and chlorophyll a species as measured from the influent to effluent ends of the MBR.  The results of the experiments indicate that MBRs may be combined with retention ponds to provide  green technology  alternatives for inter-event treatment of nutrient species in urban stormwater runoff by use of recyclable sorption media and solar powered submersible pumps.    Secondly the thesis focusses on three new devices for BMP monitoring which may be integrated into wireless networks, including a Groundwater Variable Probe (GVP) for velocity, hydraulic conductivity and dispersion measurements in a retention pond bank (Chapter 4), an affordable Wireless Automated Sampling Network (WASN) for sampling and analysis of nutrient flux gradients in retention ponds (Chapter 5), and finally an Arc-Type Automated Pulse Tracer Velocimeter (APTV) for low velocity and direction surface water measurements in retention ponds and constructed wetlands (Chapter 6).   The GVP was integrated with other environmental sensing probes to create a remote sensing station, capable of real-time data analysis of sub-surface conditions including soil moisture, water table stage.  Such abilities, when synced with user control capabilities, may help to increase methods of monitoring for applications including erosion control, bank stability predictions, monitoring of groundwater pollutant plume migration, and establishing hydraulic residence times through subsurface BMPs such as permeable reactive barriers.  Advancement of this technology may be used by establishing additional sub-stations, thereby creating sensing networks covering broader areas on the kilometer scale.  Two methods for velocity calculation were developed for the GVP for low flow (Pe \u3c 0.2) and high flow (Pe \u3e 0.6) conditions.  The GVP was found to operate from a 26-505 cmd-1 range in the laboratory to within ±26% of expected velocities for high-flow conditions and effectively measure directional flow angles to within ±14° of expected.  Hydraulic conductivity measurements made by the GVP were confirmed to within ±12% as compared to laboratory measurements.  The GVP was found capable of measuring the dispersion coefficient in the laboratory, however turbulent interferences caused during injection was found to occur.  Further advancement of the technology may be merited to improve dispersion coefficient measurements.   Automated water sampling can provide valuable information of the spatial and temporal distribution of pollutant loading in surface water environments.  This ability is expanded with the development of the WASN, providing an affordable, ease-of-use method compared to conventional automated water samplers currently on the market.  The WASN was found to effectively operate by text activation via GSM cellular networks to an activation module.  Propagation of the signal was distributed to collection units via XBee modules operating on point-to-point star communication using an IEEE 802.15.4 protocol.  Signal communications effectively transmitted in the field during a storm event to within a range of 200 feet and collected 50 ±4 ml samples at synced timed increments.   A tracer study confirmed that no mixing of samples occurs when a factor of safety of 2 is applied to flush times.  This technology provides similar abilities to current market devices at down to 10% of the cost, thereby allowing much more sampling locations for a similar budget.   The Arc-Type APTV is useful in establishing both low range horizontal velocity fields and expanding low range velocity measurements below detection ranges of mechanical velocity meters.  Installation of a field station showed system functionality, which may be integrated with other environmental sensing probes for surface water testing.  This may assist in nutrient distribution analysis and understanding the complex behavior of hydraulic retention times within wetland systems.   The device was found to work effectively in both lab and field environments from a 0.02 – 5.0 cms-1 range and measure velocity within approximately ±10% of an acoustic Doppler velocimeter and within an average of ±10° of directional measurements.  A drop in accuracy was measured for velocity ranges \u3e4.5 cms-1.  The field station operated on 3G CDMA cellular network two-way communication by installation of a Raven cellular modem.  Use of LoggerNet software allowed control and data acquisition from anywhere with an internet connection.   This thesis also introduces brief discussions on expanding these  point  measurement technologies into sensing networks.  Installation of sub-stations with communication protocols to one central master node station may broaden the sensing system into much larger kilometer-scale ranges, thus allowing large spatial analysis of environmental conditions.  Such an integration into controllable sensing networks may help bridge the gap and add calibration and verification abilities between fine-resolution  point  measurements and large scale technologies such as Electrical Resistivity Tomography and satellite remote sensing.  Furthermore, application of sensing networks may assist in calibration and verification of surface and groundwater models such as ModFlow, SVFlux and FEHM",'Information Bulletin on Variable Stars (IBVS)',Green Technologies and Sensor Networks for BMP Evaluation in Stormwater Retention Ponds and Wetlands.,,,,core
213619955,2015-12-01T00:00:00,"RÉSUMÉ
Les simulateurs de vol sont des systèmes composés d'un système logiciel et d'actuateurs mécaniques qui recréés la dynamique d'un vrai vol avec un aéronef et qui sont notamment utilisés par les compagnies de transport aérien pour former leurs futurs pilotes. Ces simulateurs doivent recréer des scénarios de vol permettant d'exécuter des itinéraires réguliers de vol ou de confronter les pilotes à différentes situations d'urgence qui peuvent subvenir lors d'un vrai vol (p. ex., une tempête violente). Puisque les simulateurs de vol ont un impact direct sur la qualité de formation des pilotes, une batterie exhaustive de tests s'impose à chaque fois qu'une nouvelle version d'un simulateur doit être mise en opération. Une bonne partie de ces tests requièrent l'intervention d'un pilote qui stimule le système en réalisant une série d'opérations de contrôle provenant de scénarios de vol préparés par des experts en aéronautique, ce qui est gourmand en temps, en ressources humaines et en ressources financières. 
La raison de la nécessité de réaliser tous ces tests est que le logiciel en soit est constitué de plusieurs composants de type boîte noire dont le code source n'est pas disponible, ils sont fournis tels quels par les fournisseurs du composant réel d'origine devant être simulé (p. ex., un composant hydraulique d'un aéronef que nous voulons simuler). Puisque nous n'avons pas pas accès au code source, il n'est pas possible lors de la mise-à-jour de l'un de ces composants de savoir quel sera le changement dans le comportement du simulateur. Dans le meilleur des cas, un défaut dans un composant aura un impact local et, dans le pire des cas, il peut nuire à tous les autres composants du simulateur. Il est dans ce cas nécessaire d'utiliser une stratégie de test d'une large granularité couvrant à la fois le fonctionnement des composants eux-mêmes et  l'ensemble des fonctionnalités de vol du simulateur afin d'en assurer sa qualité. 
Dans le cadre de ces tests, afin de réduire le temps requis pour vérifier le bon comportement d'une nouvelle version d'un simulateur, nous proposons dans ce mémoire d'automatiser l'analyse des résultats de test en modélisant le comportement normal du système logiciel en utilisant de l'apprentissage automatique. Un tel modèle tente de recréer le comportement stable du système en bâtissant des règles reliant ses entrées à chacune de ses sorties. Les entrées sont constituées de métriques provenant des contrôles qui stimulent le système et les sorties sont constituées de métriques que nous pouvons observer. Nous produisons donc un modèle initial d'une version fiable du simulateur et nous validons par la suite le bon comportement des versions subséquentes en comparant leur comportement avec celui du modèle initial. Nous cherchons à voir s'il y a un écart trop important avec le modèle initial, ce nous considérons comme étant une déviation.
Le but final de notre recherche est de pouvoir appliquer notre méthodologie d'analyse des résultats de test dans l'industrie de la simulation. Afin d'avoir plus de flexibilité dans nos expérimentations, nous commençons d'abord par valider l'approche avec un simulateur plus simple et ouvert. Nous utilisons donc le simulateur de vol à source libre FlightGear comme cas d'étude pour évaluer notre approche de test. Ce simulateur utilise l'engin de vol très populaire JSBsim et la librairie SimGear pour modéliser les aéronefs et simuler leur comportement dynamique dans un environnement de vol. JSBsim est notamment utilisé par la Nasa à des fins de recherche. Nous réalisons d'abord un vol manuel en utilisant des périphériques de contrôle d'un ordinateur tout en enregistrant les données contenues dans le simulateur. Le vol doit être composé de toutes les opérations régulières de pilotage afin que le modèle que l'on en extrait représente fidèlement le comportement normal du simulateur. Nous réalisons par la suite plusieurs répétitions du même vol afin d'identifier des niveaux de seuils robustes pour déterminer au delà de quels écarts un métrique d'une nouvelle version doit être considéré comme déviant par rapport au modèle. Nous élaborons à cet effet cinq scénarios de mutation du comportement de la version initiale de notre simulateur afin de générer différentes versions ayant des métriques qui respectent le comportement normal du sytème ou présentant une déviation du comportement normal reliée à un problème fonctionnel. En sachant d'avance quelles mutations présentent des déviations et avec l'aide d'experts identifiant précisément les métriques qui dévient de leur comportement normal, nous pouvons construire un oracle avec lequel nous évaluons la précision et le rappel de notre approche de détection.
En particulier, dans le cadre de notre étude empirique sur FlightGear, nous modifions une version initiale du simulateur en y injectant cinq différentes mutations qui n'ont pas d'impact grave ou qui engendrent des problème fonctionnels avec lesquels nous évaluons notre approche. Les résultats montrent qu'en choisissant des niveaux de seuil initiaux lors d'une première calibration, notre approche peut détecter les métriques ayant des déviations avec un rappel majoritairement de 100% (un seul cas de mutation à 50%) et une précision d'au moins 40%. En ce qui a trait aux mutations ne changeant pas le comportement normal du simulateur, notre approche a un taux de fausses alarmes de 60%. Nous montrons également qu'avec une base de données plus large, il est possible de calibrer notre approche afin d'améliorer sa performance. Avec des niveaux de seuil optimaux qui sont calibrés pour chacune des mutations, nous obtenons un taux de rappel de 100% pour tous les cas, une précision d'au moins 50% pour les versions ayant des problèmes fonctionnels et un taux de fausses alarmes de 0% pour les versions n'ayant pas de problèmes fonctionnels.
Afin d'ouvrir la voie pour des améliorations futures, nous utilisons notre méthodologie pour faire une petite expérimentation connexe sur JSBsim afin de détecter les déviations dans les transitions entre les états stables de chacun des métriques. Nous utilisons la même modélisation du simulateur. Les résultats montrent que nous pouvons battre une classification aléatoire des déviations. Cette nouvelle approche n'en n'est qu'à ces débuts et nous sommes convaincu que nous pouvons obtenir des résultats plus significatifs avec de futurs travaux sur le sujet.
Nous proposons également dans les améliorations futures de descendre le niveau de granularité de notre modélisation du simulateur au niveau de chacun de ses composants. Cela permettrait d'isoler exactement qu'elle composant présente une déviation, et ainsi trouver la source du problème.----------ABSTRACT
Flight simulators are systems composed of software and mechanical actuators used to train crews for real flights. The software is emulating flight dynamics and control systems using components off-the-shelf developed by third parties. For each new version of these components, we do not know what parts of the system is impacted by the change and hence how the change would impact the behaviour of the entire simulator. Hence, given the safety critical nature of flight simulators, an exhaustive set of manual system tests must be performed by a professional pilot. Test experts then rely on the test pilot's feedback and on the analysis of output metrics to assess the correctness of a simulator's behaviour, which consumes time and money, even more these tests must be repeated each time one of the components is updated by its vendor. 
To automate the analysis of test results from new simulator versions, we propose a machine learning-based approach that first builds a model that mimics the normal behaviour of a simulator by creating rules between its input and output metrics, where input metrics are control data stimulating the system and output metrics are data used to assess the behaviour of the system. We then build a behavioural model of the last-known correctly behaving version of the simulator, to which we compare data from new versions to detect metric deviations. The goal of our research is to first build a model of the simulator, then detect deviations of new simulator versions using that model, and finally develop an automatic approach to flag deviations when there are functional problems
Our case study uses the open-source flight simulator Flight Gear, based on the well-known JSBsim flight dynamic engine currently used by the Nasa. Applying our approach, we first drive a manual basic flight using computer peripherals while recording metrics in the simulator. We then use the recorded metrics to build a model of this basic execution scenario. After repeating several times our flight we have enough data to identify robust metric deviation thresholds to determine when a metric in a new version is deviating too far from the model. 
We generate five different versions of FlightGear by injecting harmless and harmful modifications into the the simulator, then perform again our initial flight multiple times for each of those versions while recording input and output metrics. We rely on experts identifying which output metric should be deviating in each scenario to build an oracle. Using an initial threshold, our approach can detect most of the deviations in problematic versions with a near perfect recall (only one case at 50%) and a reasonable precision of at least 40%. For the versions without harmfull deviations we get a false alarm rate of 60%. By optimizing the threshold for each version, we get a perfect recall of 100%, a precision of at least 50%, and a false alarm rate of 0% for the good behaving versions.
To open the path for future work, we perform a case study on JSBsim using a variant of our basic deviation detection approach to detect transient deviations using the same model. Our results show that we can beat a random classification for one kind of deviation, however for other deviations our models do not perform as well. This new approach is just a first step towards transient deviation detection, and we are convinced that we can get better results in future work. We also propose to use finer-grained models for each component to be able to isolate the exact component causing a functional problem",,Conception d'un outil de modélisation intégré pour l'indexation et l'analyse de trace,https://core.ac.uk/download/213619955.pdf,,,core
60782735,2014-01-01T00:00:00,"Nowadays, there are many different types of mobility aids for elderly people. Nevertheless, these devices may lead to accidents, depending on the terrain where they are being used. In this context, the goal of the EyeWalker project is to develop a ultralight computer vision device for users with mobility problems. One of the main objective of this work is to develop a ground change detection module that will warn the user before entering dangerous terrains or hostile situations. This software component integrated on a ”smart” walker will be able to react in real time, to operate both indoor and outdoor, as well as in familiar or unfamiliar environments. Specifically, we propose a classification algorithm using colour and texture as a descriptor to detect ground changes. In our classifier, the distributions of HSV colours and Local Edge Patterns are used to compare the similarity between the current frame and the average of the k previous frames. To compare similarities, we used four different techniques (Histogram Intersection, Kolmogorov-Smirnov, Cumulative Integral and Artificial Neural Networks) with outdoor training images. Preliminary results reveal that artificial neural networks achieved the best performances",,"A robust, real-time ground change detector for a ""smart"" walker",,,,core
22576950,18/07/2013,"Abstract. Smartphones represent powerful mobile computing devices enabling a wide variety of new applications and opportunities for human interaction, sensing and communications. Because smartphones come with front-facing cameras, it is now possible for users to interact and drive applications based on their facial responses to enable participatory and opportunistic face-aware applications. This paper presents the design, implementation and evaluation of a robust, real-time face interpretation engine for smartphones, called Visage, thatenablesanewclassof face-aware applications for smartphones. Visage fuses data streams from the phone’s front-facing camera and built-in motion sensors to infer, in an energy-e cient manner, the user’s 3D head poses (i.e., the pitch, roll and yaw of user’s heads with respect to the phone) and facial expressions (e.g., happy, sad, angry, etc.). Visage supports a set of novel sensing, tracking, and machine learning algorithms on the phone, which are specifically designed to deal with challenges presented by user mobility, varying phone contexts, and resource limitations. Results demonstrate that Visage is e↵ective in di↵erent real-world scenarios. Furthermore, we developed two distinct proof-of-concept applications, Streetview+ and Mood Profiler driven by Visage. Key words: face-aware mobile application, face interpretation engin",,Visage: A Face Interpretation Engine for Smartphone Applications,,10.1007/978-3-642-36632-1_9,,core
46819344,2014-12-04T00:00:00,"In the past decades, embedded systems become more and more present in our daily life. They are present in our wallet, in our car, in our house appliances and the current tendency is to control more and more things by using them. Major companies already started to envision the future of internet which become known as the Internet of Things. In the next years, more and more of our things will be smart, connected... and subject to software flaws.Micro-controllers, very small devices with a few hundreds bytes of memory, are at the heart of this revolution. They become cheaper, smaller and more powerful. Yet, contrary to the technologies used in our desktop computers or our server farms, the goal of the industry that creates these devices is not power. Indeed, controlling temperature in your home or humidity in your wine cellar does not need Gigahertz core. It even does not need multiple cores. What these devices need is to be cheap, to be able to be produced in very high volumes and to use small amount of electrical energy. In this context, the moore's law does not help us to have more cores in one device, but it allows to produce more devices with the same amount of silicon. The research I made in the past years try to reduce the gap between cheap software and robust and efficient software for these devices. I think that, by providing smart tools and production toolchains we can help junior or non specialized developers so that they can produce good embedded software for a reasonable development cost. I also focus my work on optimizing and providing efficient and secure software and network protocols, so that the Internet of Things can become a reality while respecting user privacy and being sustainable from a energy point of view.I focused my research on three aspects. First, I focused on how to allow larger softwares do be developped in embedded systems. In particular, we proposed techniques to allow Java/JavaCard code to be executed from a non-addressable memory using a full software cache.  I also looked at how to use domain specific languages to ease the implementation of a collaborative network intrusions detection system. Thanks to adapted tools, the software, writen in the DSL, can be compiled for a wide range of probes while offering garanties on the produced software. Last, I focused on energy aware network protocols in the context of smart cities.Au cours des dernières années, les systèmes embarqués sont de plus en plus présents dans notre vie de tous les jours. Ils sont dans notre portefeuille, dans notre voiture ou dans nos appareils ménagers. La tendance actuelle est à contrôler de plus en plus d'objets à l'aide de ces systèmes. Les grands acteurs de l'industrie ont déjà commencé à envisager le futur de l'internet, l'internet des objets. Dans les années à venir, de plus en plus de nos objets seront « intelligents », connectés... et sujets à des fautes logicielles.Les micro contrôleurs, de minuscules ordinateurs possédant quelques centaines d'octets de mémoire, sont au coeur de cette révolution. Ils deviennent de moins en moins cher et de plus en plus puissant. Néanmoins, à la différence de nos ordinateurs de bureau ou des serveurs, le but de l'industrie des micro contrôleurs n'est pas la puissance. En effet, contrôler la température de notre maison et le taux d'humidité de notre cave à vin ne nécessite pas des processeurs cadencés à plusieurs gigahertz. Cela ne nécessite même pas plusieurs coeurs de calculs. Le véritable besoin de ces équipements est d'être bons marché, d'être produits en très grands volumes, d'êtres petits, facilement intégrables et d'utiliser une faible quantité d'énergie électrique. Le meilleur exemple de cette tendance est très certainement la carte à puce. Depuis le début des années 90, de plus en plus de cartes à puce sont produites, vendues et utilisées dans le monde. Cependant, elles n'en restent pas moins de tout petits équipements d'une puissance inférieure de plusieurs ordres de grandeur de nos ordinateurs de bureau. Quand il s'agit de développer du logiciel pour ces équipements, un développeur débutant n'est généralement pas suffisamment préparé et ne sera pas capable d'écrire du logiciel efficace pour ces cibles avant de longue années passées à gagner de l'expérience et de l'expertise. Si le logiciel que l'ont souhaite produire doit être efficace, sûr et correct au sens le plus strict du terme, l'industrie doit compter sur des développeurs très spécialisés, expérimentés et donc chers. Les acteurs industriels ont donc deux alternatives : produire du logiciel bon marché, mais assez inefficaceet défectueux ou dépenser une somme importante pour le développement et fournir un logiciel de bonne qualité. On peu aisément supposer que la tendance actuelle est au logiciel bon marché, et que la probabilité que le logiciel qui équipera les millions d'équipements formant l'internet des objets sera de piètre qualité et offrira quantité de failles que des acteurs malveillants se feront une joie d'exploiter si rien n'est fait pour rendre bon marché le développement correct de logiciels embarqués.Mes recherches de ces dernières années vont dans le sens de la réduction du fossé séparant logiciels bon marché et logiciels sûrs et performants. Je suis convaincu qu'en offrant des outils « intelligents » et des chaînes de production logicielle efficaces, nous pouvons aider les développeurs débutants, ou non spécialisés, à produire du logiciel embarqué de bonne qualité pour un coût de développement raisonnable. J'ai également porté mon attention sur l'optimisation et la sécurisation des logiciels et des protocoles réseau afin que l'internet des objets puisse devenir une réalité tout en respectant la vie privée des utilisateurs et en offrant une alternative durable sur le plan énergétique.Je me suis principalement intéressé à trois champs de recherche. Tout d'abord, j'ai cherché à permettre l'utilisation de techniques de développement standard (objets, composants, programmation en Java…) à la faible capacité mémoire des équipements embarqués. A l'inverse, je me suis également intéressé à l'utilisation de langages dédiés à une application afin de permettre à des spécialistes du domaine de la sécurité réseau d'exprimer des algorithmes de détection d'intrusions. A l'aide d'une suite d'outils dédiée, ces algorithmes sont compilés et optimisés automatiquement pour être utilisés dans une architecture distribuée de sondes embarquées. Enfin, je me suis intéressé aux protocoles réseau économes en énergie pour interconnecter les équipements embarqués dans le cadre des villes intelligentes",HAL CCSD,Élements de conception de systèmes embarqués fortement contraints,,,,core
296623047,2015-11-26T14:18:44Z,"This work presents the development of dynamic models based in the fuzzy systems for polymerization processes. These reactions present a highly non linear dynamic behavior, thus making difficult the attainment of mathematical models for conventional methods. The solution copolymerization of methyl methacrylate and vinyl acetate in a continuous stirred tank reactor is used to illustrate the generation of the proposed model. Factorial planning was used to discriminate the process variables with higher impact on the system performance (effects) and they are used to built up a dynamic model based on the functional fuzzy relationship of Takagi-Sugeno type. These models present an excellent capacity to represent dynamic data. Moreover, they allow the inclusion of qualitative or operational information of the process. Gaussian membership functions are used for the fuzzy sets and model determination (rules number and model parameters) is obtained from the process database. The treatment of these data for the fuzzy model determination is carried out by means of algorithms of subtractive clustering and least squares. The kinetic parameters and reactor operating conditions are obtained from the literature and a mathematical model is considered as plant for generation of identification data. Fuzzy dynamic models showed satisfactory predictive capabilities, and may be an interesting alternative to attack problems of modeling in chemical processes.ABDELAZIM, T., MALIK, O.P., Identification of nonlinear systems by Takagi-Sugeno logic grey box modeling for real-time control (2005) Control Engineering Practice, 13, pp. 1489-1498ALEXANDRIDIS, A.P., SIETTOS, C.I., SARIMVEIS, H.K., BOUDOUVIS, A.G., BAFAS, G.V., Modelling of nonlinear process dynamics using Kohonen's neural networks, fuzzy systems and Chebyshev series (2002) Computers and Chemical Engineering, 26, pp. 479-486CERRADA, M., AGUILAR, J., COLINA, E., TITLI, A., Dynamical membership functions: An approach for adaptive fuzzy modeling (2005) Fuzzy Sets and Systems, 152, pp. 513-533CHEN, B., LIU, X., Reliable control design of fuzzy dynamic systems with time-varying delay (2003) Fuzzy Sets and Systems, pp. 1-26CHEN, Y., LIU, X., Modeling mass transport of propylene polymerization on Ziegler-Natta catalyst (2005) Polymer, 46, pp. 9434-9442CHIU, S., A cluster estimation method with extension to fuzzy model identification (1994) IEEE, pp. 1240-1245CHIU, S., Method and software for extracting fuzzy classification rules by subtractive clustering (1996) IEEE, pp. 461-465CONGALIDIS, J.P., RICHARDS, J.R., RAY, W.H., Feedforward and feedback control of a solution copolymerization reactor (1989) AIChe Journal, 35 (6), pp. 891-907. , JuneHABBI, H., ZELMAT, M., BOUAMAMA, B.O., A dynamic fuzzy model for a drum-boiler-turbine system (2003) Automatica, 39, pp. 1213-1219MANER, B.R., DOYLE III, F.J., Polymerization reactor control using autoregressiveplus volterra-based MPC (1997) AIChe Journal, 43 (7), pp. 1763-1784. , JulyPASSINO, K. M. and YURKOVICH, S. Fuzzy Control. Addison-Wesley- Longman, Menlo Park, CA,1998SALA, A., GUERRA, T.M., BABUSKA, R., Perspectives of fuzzy systems and control (2005) Fuzzy Sets and Systems, 156, pp. 432-444TAKAGI, T., SUGENO, M., Fuzzy identification of systems and its applications to modeling and Control (1985) IEEE Transactions on Systems, Man, and Cybernetics, 15, pp. 116-133ZADEH, L., Outline of a new approach to the analysis of complex systems and decision process (1973) IEEE Transactions on Systems, Man, and Cybernetics, 1, pp. 28-4",,Development Of Fuzzy Dynamic Models: Application To Polymerization Systems,,,,core
296648423,2015-11-26T15:11:39Z,"The perception of the environment is a major issue in autonomous robots. In our previous works, we have proposed a visual perception system based on an automatic image discarding method as a simple solution to improve the performance of a real-time navigation system. In this paper, we take place in the obstacle avoidance context for vehicles in dynamic and unknown environments, and we propose a new method for Collision Risk Estimation based on Pearson's Correlation Coefficient (PCC). Applying the PCC to real-time CRE has not been done yet, making the concept unique. This paper provides a novel way of calculating collision risk and applying it for object avoidance using the PCC. This real-time perception system has been evaluated from real data obtained by our intelligent vehicle. © 2013 IEEE.4045ANR; French National Research AgencyThrun, S., Stanley, the robot that won the DARPA Grand Challenge (2006) Journal of Robotic Systems, 23 (9), pp. 661-692. , DARPA Grand Challenge(2007) Spirit of Berlin: An Autonomous Car for the DARPA Urban Challenge Hardware and Software Architecture, , http://www.darpa.mil/grandchallenge/TechPapers/Team_Berlin.pdf, Team Berlin retrieved 02 Dec. 2010Gietelink, O., Ploeg, J., De Schutter, B., Verhaegen, M., Development of advanced driver assistance systems with vehicle hardware-in-The-loop simulations (2006) Vehicle System Dynamics, 44 (7), pp. 569-590Ulrich, I., Nourbakhsh, I., Appearance-based obstacle detection with monocular color vision (2000) Proceedings of the AAAI National Conference on Artificial Intelligence, pp. 866-871. , July/AugustBonin-Font, F., Ortiz, A., Oliver, G., Visual navigation for mobile robots: A survey (2008) Journal of Intelligent and Robotic SystemsKim, B., Hubbard, P., Necsulescu, D., Swarming unmanned aerial vehicles: Concept development and experimentation, a state of the art review on flight and mission control (2003) Technical MemorandumDugarry, A., (2004) Advanced Driver Assistance Systems Information Management and Presentation, , PhD Thesis, Cranfield University School of Engineering Applied Mathematics and Computing GroupBeauchemin, S., Barron, J.L., The computation of optical flow (1995) ACM Computing Surveys, 27, pp. 433-467State of the art report and requirement specifications (2009) Hybrid Intelligent Virtual Actors, Integrating Research in Interactive Storytelling, , http://www.irisa.fr/, INRIA Report retrieved Feb. 03, 2011Pearson, K., (1895) Royal Society Proceedings, 58, p. 241Eugene, Y.K., Johnston, R.G., (1996) The Ineffectiveness of the Correlation Coefficient for Image Comparisons, , Technical Report LA-UR-96-2474, Los Alamos, 1996Miranda Neto, A., Rittner, L., Leite, N., Zampieri, D.E., Victorino, A.C., Nondeterministic criteria to discard redundant information in real time autonomous navigation systems based on monocular vision (2008) ISIC Invited Paper, 2008 IEEE Multi-conference on Systems and ControlMiranda Neto, A., Rittner, L., Leite, N., Zampieri, D.E., Lotufo, R., Mendeleck, A., Pearson's correlation coefficient for discarding redundant information in real time autonomous navigation systems (2007) Proceedings of the 2007 IEEE Multi-conference on Systems and ControlGreco, C.R., (2008) Real-Time Forward Urban Environment Perception for An Autonomous Ground Vehicle Using Computer Vision and Lidar, , Master of Science (thesis), Brigham Young UniversityBertozzi, M., Broggi, A., Fascioli, A., Vision-based intelligent vehicles: State of the art and perspectives (2000) Robotics and Autonomous Systems, 32, pp. 1-16Dahlkamp, H., Self-supervised monocular road detection in desert terrain (2006) Proceedings of the Robotics Science and Systems ConferenceRadio Spectrum Committee, European Commission, , http://ec.europa.eu/information_society/policy/ecomm/radio_spectrum /_document_storage/rsc/rsc32_public_docs/rscom10_35.pdf, Public Document, Brussels, 5 July 2010, RSCOM10-35 [retrieved Dec. 02, 2010]Gietelink, O.J., Ploeg, J., Schutter, B., Verhaegen, M., Development of a driver information and warningsystem with vehicle hardware-in-theloop simulations (2009) Mechatronics, 19, pp. 1091-1104Müller, D., Pauli, J., Nunn, C., Görmer, S., Müller-Schneiders, S., Time to contact estimation using interest points (2009) IEEE Proceedings of the International Conference on Intelligent Transportation Systems (ITSC 2009), , St.Louis, USAAlenya, G., Negre, A., Crowley, J.L., A comparison of three methods for measure of time to contact (2009) IEEE Proceedings of the International Conference on Intelligent Transportation Systems (ITSC 2009), , St.Louis, USADagan, E., Mano, O., Stein, G.P., Shashua, A., Forward collision warning with a single camera (2004) Intelligent Vehicles Symposium, IEEE, pp. 37-42Negre, A., Braillon, C., Crowley, J., Laugier, C., Real-time time-to-collision from variation of intrinsic scale (2006) INRIA Base, Proc. of the Int. Symp. on Experimental RoboticsHorn, B.K.P., (1986) Robot Vision, , The MIT PressWu, S., Decker, S., Chang, P., Camus, T., Eledath, J., Collision sensing by stereo vision and radar sensor fusion (2009) IEEE Transactions on Intelligent Transportation Systems, 10 (4)Beyeler, A., Zufferey, J.C., Floreano, D., Vision-based control of near-obstacle flight (2009) Autonomous Robots, 27 (3), pp. 201-219Ruffier, F., Franceschini, N., Optic flow regulation: The key to aircraft automatic guidance (2005) Robotics Autonomous Systems, 50, pp. 177-194Mesbah, M., Gradient-based optical flow: A critical review (1999) Proc. of the Fifth Int. Symp. on Signal Processing and Its Applications. ISSPA '99, 1, pp. 467-470. , 1999Otsu, N., A threshold selection method from graylevel histogram (1978) IEEE Transactions on Systems, Man, and CyberneticsRodgers, J.L., Nicewander, W.A., Thirteen ways to look at the correlation coefficient (1988) The American Statistician, 42, pp. 59-66Yanqing, W., Deyun, C., Chaoxia, S., Peidong, W., Vision-based road detection by monte carlo method (2010) Information Technology Journal, 9, pp. 481-487Sahoo, P.K., Soltani, S., Wong, A.K.C., A survey of thresholding techniques (1988) Comput. Vision Graphics Image Processing, 41, pp. 233-260Lee, U.S., Chung, Y.S., Park, H.R., A comparative performance study of several global thresholding techniques for segmentation (1990) Computer Vision, Graphics, and Image ProcessingSezgin, M., Sankur, B., Survey over image thresholding techniques and quantitative performance evaluation (2004) Journal of Electronic Imaging, 13, pp. 146-165Gonzalez, C.R., Woods, E.R., (1991) Digital Image Processing, , Addison-Wesley Publishing CompanyCanny, J.F., A computational approach to edge detection (1986) IEEE Trans. Pattern Anal. Machine Intell., 8 (6), pp. 679-698(2005) DARPA Grand Challenge Rulebook, , http://www.darpa.mil/grandchallenge05/, DARPA(2006) Stanford Racing Team's Entry in the 2005 DARPA Grand Challenge, , http://www.stanfordracing.org, June 10http://youtu.be/J8YuZlJFEx",,Real-time Collision Risk Estimation Based On Pearson's Correlation Coefficient,,10.1109/WORV.2013.6521911,,core
55632960,2014-01-01T00:00:00,"Education is being revolutionized by the introduction, of mobile technologies in the teaching and
learning process. However, studies that focus in the application of mobile technologies to informal
learning environments is scarce and not systematized [1]. This is the reason for conducting a research
project that involved a urban game MobiGeo, designed in to take better advantage of the flexibility and
ubiquity offered by the Mobile Learning (ML) but also taking into account the importance of motivation
and interaction to enhance students learning.
The definition of ML has been a complicated task for researchers, but there are assumptions that can
not be neglected: the mobility, portability and ubiquity [2], these are features that will drive new
learning spaces and thus motivate students. This idea is supported by [2] that introduces the concepts
of ""just in time"", ""just enough"" and ""just-for-me"" and [4] that speaks of the triad ""location
independence”, ""independence time"" and ""meaningful content”.
These principles of ""anytime"" and ""anywhere"" consolidated by mobile technologies came to renew the
variety of educational activities available to teachers and in this context arises the concept of mobile
location-based games. According to [5] ""these games are played in physical space, but at the same
time, they are supported by actions and events in an interconnected virtual space"", which can be
classified into three categories: ludic, pedagogic and hybrid. By being in direct contact with the
contents to assimilate and move in a real context, students will have a more significant learning [6]
and this will result in the mobilization of knowledge in different contexts. To make the connection
between the physical and the virtual world, our research has made use of Qr codes as these devices
provide information in real time and in a dynamically way.
For this research was idealized an urban game called “MobiGeo”, that respect the principles
suggested by [7] and that has as common thread the history of the European Union. To measure
results the researchers developed a questionnaire that was adapted from a proposal of [8] which
created a ""Model to evaluate Educational Games”, so our proposal was built taking into account the
motivational model of Kirkpatrick (level1) and encompassing three major dimensions:
Motivation/Interest, Interaction and Perceived Learning. To assess the Motivation/Interest was used
the Model ARCS (Relevance, Confidence and Satisfaction) and items of Fun, Immersion and
Challenge of “Game User Experience”. On the other hand the interaction was evaluated by items of
the Social Interaction dimension of the “Game of User Experience”, the Learning Perceptions were
evaluated by Bloom's Taxonomy (Knowledge category).
In this paper we present the design and implementation of the MobiGeo outdoor learning activity with
a group of 173 students from the 7th grade of a basic school in the north of Portugal. Initial results
show that this urban game with Qr codes was an adequate activity to use in informal learning
environments that could engage students in gaming with high degrees of motivation and interaction in
order to solve the tasks presented to them and so consolidate and acquired new knowledge about the
European Union.CIEC – Research Centre on Child Studies, IE, UMinho (FCT R&D unit 317), Portuga","'Associated Management Consultants,  PVT., Ltd.'",The implementation of mobile location based-games and QR-codes: the case of MobiGeo,https://core.ac.uk/download/55632960.pdf,,"[{'title': None, 'identifiers': ['2340-1079', 'issn:2340-1079']}]",core
25864196,2012-01-01T00:00:00Z,"A large amount of effort is spent on forecasting the outcome of sporting events. Moreover, there are large quantities of data regarding the outcomes of sporting events and the factors which are assumed to contribute to those outcomes. In this paper we tried to predict the success of nations at the Asian Games through macro-economic, political, social and cultural variables. we used the information of variables include urban population, Education Expenditures, Age Structure, GDP Real Growth Rate, GDP Per Capita, Unemployment Rate, Population, Inflation Average, current account balance, life expectancy at birth and Merchandise Trade for all of the participating countries in Asian Games from 1970 to 2006 in order to build the model and then this model was tested by the information of variables in 2010. The prediction is based on the number of total medals acquired each country. In this research we used WEKA software that is a popular suite of machine learning software written in Java. The value of correlation coefficient between the predicted and original ranks is 90.42%. Neural Network Model, between 28 countries mentioned, predicts their ranks according to the maximum difference between predicted and original ranks of 19 countries (67.85%) is 3, the maximum difference between predicted and original ranks of 8 countries (28.57%) is between 4 to 6 and the difference between predicted and original ranks of 1 countries (3.57%) is more than 6",Greek Center for Sport Science Research,Are Macro variables good predictors? A prediction based on the number of total medals acquired,,,"[{'title': None, 'identifiers': ['issn:1791-4027', '1791-4027']}]",core
43491761,2015-01-01T00:00:00,"Passenger flow modeling and station dwelling time estimation are significant elements for railway mass transit planning, but system operators usually have limited information to model the passenger flow. In this paper, an artificial-intelligence technique known as fuzzy logic is applied for the estimation of the elements of the origin-destination matrix and the dwelling time of stations in a railway transport system. The fuzzy inference engine used in the algorithm is based in the principle of maximum entropy. The approach considers passengers’ preferences to assign a level of congestion in each car of the train in function of the properties of the station platforms. This approach is implemented to estimate the passenger flow and dwelling times of the recently opened Line 1 of the Panama Metro. The dwelling times obtained from the simulation are compared to real measurements to validate the approach.The authors of this paper want to express their gratitude to the National Secretary of Science and Technology (SENACYT) of the Government of the Republic of Panama for funding this study through the R & D project (MDEPRB09-001). Additionally, they want to thank the support received from
Technological University of Panama (UTP), the University of Granada, the Fundación Carolina and the Secretaría del Metro de Panamá (SMP)",'MDPI AG',A Fuzzy Logic-Based Approach for Estimation of Dwelling Times of Panama Metro Stations,,10.3390/e17052688,"[{'title': 'Entropy', 'identifiers': ['1099-4300', 'issn:1099-4300']}]",core
36193778,2015-04-27T08:39:35,"Special issue on SPURS: Salinity Processes in the Upper-ocean Regional Study.-- 10 pages, 5 figures, 2 tablesThe Global Drifter Program deployed a total of 144 Lagrangian drifters drogued at 15 m depth, including 88 equipped with salinity sensors, in support of the first Salinity Processes in the Upper-ocean Regional Study (SPURS-1) in the subtropical North Atlantic Ocean with the goal of measuring salt fluxes associated with surface currents. The quality-controlled data set consists of 996,583 salinity observations collected between August 2012 and April 2014. A comparison of the drifter salinities with Aquarius satellite sea surface salinity (SSS) data shows that the lifespan of the salinity sensor fitted to the drifters is of the order of one year. The salinity and velocity data from the drifters were used to validate salt transport divergence computed with satellite products, with satellite salinity taken from the standard Aquarius v3.0 data set. The results indicate good agreement between the two independent methods, and also demonstrate that the effect of the eddy field combined with SSS variability at the surface dominates the signal. SSS variability within spatial bins as compared to Aquarius-beam footprints measured by drifters can be in excess of 0.1 psu. This result suggests that careful evaluation of the representation error is required when single-point in situ measurements, such as those collected by Argo floats, are used to validate spatially averaged Aquarius salinity data. © 2015 by The Oceanography Society. All rights reservedLance Braasch is gratefully acknowledged for implementing and managing real-time distribution of SVP and SVPS data to the SPURS-1 investigators and for data analysis support. SVP and SVPS drifters described in this study were provided by the GDP of NOAA at SIO, grant #NA10OAR4320156. LC, VH, and DL were supported by NASA grant #NNX12AI67G, and NOAA grant #NA10OAR4320156. The altimeter products F. Wentz, S. Yueh, C. Ruf, J. Lilly, J. Gunn, were produced by Ssalto/Duacs and distributed by AVISO, with support from CNES (http://www.aviso.altimetry.fr/duacs). GR thanks the SMOS TOSCA/CNES and STRASSE/SPURS LEFE/INSU projects for funding. YC was funded by the Aquarius project office at the Jet Propulsion Laboratory and the NASA Physical Oceanography program. JF was supported by the Spanish national R+D plan (project AYA2010-22062-C05)Peer Reviewe",'The Oceanography Society',"Sea Surface Salinity Observations with Lagrangian Drifters in the Tropical North Atlantic During SPURS: Circulation, Fluxes, and Comparisons with Remotely Sensed Salinity from Aquarius",https://core.ac.uk/download/36193778.pdf,10.5670/oceanog.2015.08,"[{'title': None, 'identifiers': ['issn: 1042-8275', ' 1042-8275']}]",core
43094040,2014-12-18T15:07:04,"Machine Learning in Water Systems symposium: part of AISB Annual Convention 2013, University of Exeter, UK, 3-5 April 2013Convention organised by the Society for the Study of Artificial Intelligence and Simulation of Behaviour (AISB), www.aisb.org.uk/This paper describes the application of Artificial Neural Networks (ANNs) as Data Driven Models (DDMs) to predict urban flooding in real-time based on weather radar and/or raingauge rainfall data. A time-lagged ANN is configured for prediction of flooding at sewerage nodes and outfalls based on input parameters including rainfall. In the absence of observed flood data, a hydrodynamic simulator may be used to predict flooding surcharge levels at nodes of interest in sewer networks and thus provide the target data for training and testing the ANN. The model, once trained, acts as a rapid surrogate for the hydrodynamic simulator and can thus be used as part of an urban flooding Early Warning System (EWS). Predicted rainfall over the catchment is required as input, to extend prediction times to operationally useful levels. Both flood-level analogue and flood-severity classification schemes are implemented. An initial case study using Keighley, W Yorks, UK demonstrated proof-of-concept. Three further case studies for UK cities of different sizes explore issues of soil-moisture, early operation of pumps as flood-mitigation/prevention strategy and spatially variable rainfall. We investigate the use of ANNs for nowcasting of rainfall based on the relationship between radar data and recorded rainfall history; a feature extraction scheme is described. This would allow the two ANNs to be cascaded to predict flooding in real-time based on current weather radar Quantitative Precipitation Estimates (QPE). We also briefly describe the extension of this methodology to Bathing Water Quality (BWQ) prediction",,RAPIDS: Early Warning System for Urban Flooding and Water Quality Hazards,https://core.ac.uk/download/43094040.pdf,,,core
37030730,2015-01-01T08:00:00,"Transportation infrastructure takes a primary role in urban development planning. To better facilitate or understand the infrastructure status and demands, a huge amount of transportation data such as traffic flow counts has been collected from numerous transportation monitoring systems. Making full use of harvested data samples to discover important patterns has become an increasingly appealing research topic, in which a sophisticated and uncertainty processing framework is required. In this paper, a big-data processing framework is introduced to analyse the transportation data, particularly taking the classification problem of the parking occupation status as an illustrative example. Three modules are implemented to crawl the raw records, generate high-level features, and apply the machine learning algorithm for classification. In addition, the fuzzification algorithm is also introduced to quantify the key attributes of the data, which helps in removing the data redundancy and inconsistency. The proposed framework then is evaluated using a real-world dataset collected from twelve car parks in a university. Simulation results show that the proposed framework performs well with a convincing classification accuracy",'Sociological Research Online',A big-data processing framework for uncertainties in transportation data,https://core.ac.uk/download/37030730.pdf,10.1109/fuzz-ieee.2015.7337843,,core
295544501,12/05/2014,"Apesar das transformações econômicas e sociais, a hanseníase persiste como um problema de saúde pública. Pernambuco apresenta 60% dos casos concentrado na região metropolitana e o Recife é responsável por 30% dos casos do estado sendo considerado prioritário para o controle da hanseníase. Sua distribuição espacial com identificação de grupos de risco e o estudo de novas tecnologias que colaborem no diagnóstico e tratamento dos casos são importantes para auxiliar no planejamento, implementação e monitoramento de ações de prevenção e controle dessa doença. A utilização de marcadores sorológicos e moleculares surge como uma opção que podem auxiliar na elaboração de futuras estratégias de quimioprofilaxia de contactantes, como medidas de controle da doença. Estudos genéticos sugerem que variações na susceptibilidade dos seres humanos a hanseníase envolvem características complexas de vários alelos polimórficos e o polimorfismo do gene da lectina ligadora de manose (MBL2) tem sido associado à proteção para forma lepromatosa da hanseníase. A presente pesquisa teve como objetivo verificar a associação do polimorfismo do gene MBL2 com classe operacional e formas clínicas da Hanseníase e a sorologia anti-PGL-1 em contatos intradomiciliares nas áreas de maior risco no município Recife no período de 2006 a 2013. A construção do Indicador de Carência Social (ICS) foi realizada a partir de dados do censo demográfico de 2010. Para detecção do anti-PGL-1 nos contatos utilizou-se o teste ML Flow desenvolvido no Laboratório de Imunologia da AIDS e da Hanseníase do Instituto de Patologia Tropical e Saúde Pública (IPTSP) da Universidade Federal de Goiás (UFG). A genotipagem da região promotora do gene MBL2 foi realizada pelo sistema de sondas fluorescentes Taqman através da tecnologia de PCR em tempo real. A comparação do Indicador de Carência Social com o coeficiente de detecção da hanseníase mostrou existir uma relação direta entre eles. O coeficiente de correlação de Spearman apresentou-se positiva, porém fraca, provavelmente devido à composição heterogênea da cidade. A soroprevalencia do anti-PGL 1 foi de 36,80% sendo maior nos contatos do sexo feminino, na faixa etária de maiores de 15 anos e em contatos de pacientes multibacilares. Na análise univariada o alelo O e o genótipo OO foram associados a classificação operacional Paucibacilar, no sexo masculino e com mais de 40 anos (p = 0,034 e 0,003, respectivamente). Na análise multivariada o genótipo OO também foi associado a Paucibacilar (p = 0,023). O alelo O e os haplótipos HYO e LYO foram associadas à sorologia positiva para anti-PGL-1 (p = 0,046 e 0,032). Portanto, os polimorfismos do gene MBL2 podem influenciar no desenvolvimento das formas clínicas mais graves da hanseniase com baixa ativação do complemento, assim como na modulação de anti-PGL-1Despite the economic and social transformations, leprosy remains a public health problem. Pernambuco has 60 % of cases concentrated in the Recife metropolitan area and is responsible for 30% of the cases in the state, being considered a priority for leprosy control. The spatial distribution and identification of risk groups and the study of new technologies which may collaborate in the diagnosis and treatment of the cases is important to assist in the planning, implementation and monitoring of prevention and control measures of this disease. The use of serological and molecular markers appears as an option that can help to develop future strategies for contacting chemoprophylaxis, as measures of disease control. Genetic studies suggest that variations in the susceptibility of humans to leprosy involve complex characteristics of various polymorphic alleles, moreover the polymorphism of the mannose binding lectin (MBL2 ) gene have been associated with the protection against lepromatous leprosy .The present study aimed to investigate the association of polymorphism of MBL2 gene with operating class and clinical forms of leprosy and anti - PGL - 1 serology in household contacts in the highest risk areas in the city Recife in the period 2006-2013. The construction of the Social Need Indicator (ICS) was performed using data from the 2010 demographic census. For the detection of anti- PGL – 1 in the contacts, the ML Flow test was used. This test was developed in the Laboratory of Immunology of AIDS and Leprosy Institute of Tropical Pathology and Public Health (IPTSP) at the Federal University of Goiás (UFG). Genotyping of the promoting region was performed using Taqman fluorescent probes by PCR technology in real time. The comparison of the Social Need indicator with the detection rate of leprosy showed a direct relationship between them. The Spearman correlation coefficient was positive, but weak, probably due to the heterogeneous composition of the city. The seroprevalence of anti -PGL- 1 was 36.80 % being greater in female contacts, aged over 15, and in contacts of multibacillary patients MBL2 structural variantallele O andgenotype OO were associated to PB in univariate (p= 0.034 and 0.003) in manand cases olderthan 40 years; and in multivariate analysis the genotype OO also was independently associated to PB (p= 0.023). Interestingly, the O allele and HYO and LYO haplotype were associated to IgM anti-PGL-1 positive serology (p= 0.046 and 0.032). MBL2 polymorphism may influence leprosy development by anefficient modulation of anti-PGL-1 response and low complement activation",Programa de Pós-Graduação em Biotecnologia (Renorbio),"MBL2 gene polymorphism in patients and household contacts of leprosy and its association with operational class, clinical forms and Anti - PGL1 in risk areas in Recife -PE",,,,core
291726010,2012-05,"Current analyses of groundwater flow and transport typically rely on a physically-based model (PBM), which is inherently subject to error and uncertainty from multiple sources including model structural error, parameter error and data error. The model uncertainty can be difficult to quantify, and is propagated to the prediction. In this study, complementary data-driven models (DDMs) are used to improve prediction of groundwater flow models. The DDMs, trained with the historical residual of the PBM, have the capability to compensate for the defects of PBM. Five machine learning techniques, instance-based weighting (IBW), locally weighted regression (\textit{loess}), decision trees (DT), artificial neural networks (ANN) and support vector regression (SVR) are employed to construct the DDMs, and their performance of enhancing the prediction of the PBM is compared. Before the DDMs updating, cluster analysis is implemented on the dataset to improve the robustness and efficiency of the framework. The framework is tested in two real-world case studies based on the Republic River Compact Association (RRCA) model and the Spokane Valley Rathdrum Prairie (SVRP) model. The DDMs reduce the root-mean-square errors (RMSE) of the temporal, spatial and temporal plus spatial head prediction of the RRCA model by 82\%, 60\% and 48\% respectively. In the SVRP case study, the DDMs reduces the temporal head forecast of the PBM by 79\%. Localized DDMs that are conditioned on each cluster outperform global DDMs without clustering. It is also demonstrated that clustering significantly reduces the computational cost of training and cross validation of the DDMs. After clustering, the run-time of DDMs is negligible comparing with the PBM, which makes the framework very computationally efficient",,Use of data-driven models to improve prediction of physically based groundwater models,,,,core
291738743,2013-05,"Continuous increases in traffic volume and limited available capacity in the roadway system have created a need for improved traffic control. From traditional pre-timed isolated signals to actuated and coordinated corridors, traffic control for urban networks has evolved into more complex adaptive signal control systems. However, unexpected traffic fluctuations, rapid changes in traffic demands, oversaturation, the occurrence of incidents, and adverse weather conditions, among others, significantly impact the traffic network operation in ways that current control systems cannot always cope with.
On the other hand, strategies for traffic control based on developments from the field of machine learning can provide promising alternative solutions, particularly those that make use of unsupervised learning such as reinforcement learning (RL) - also referred as approximate dynamic programming (ADP) in some research communities. For the traffic control problem, examples of convenient RL algorithms are the off-policy Q-learning and the ADP using a post decision state variable, since they address processes with sequential decision making, do not need to compute transition probabilities, and are well suited for high dimensional spaces. 
A series of benefits are expected from these algorithms in the traffic control domain: 1) no need of prediction models to transition traffic over time and estimate the best actions; 2) availability of cost-to-go estimates at any time (appropriate for real-time applications); 3) self-evolving policies; and 4) flexibility to make use of new sources of information part of emergent Intelligent Transportation Systems (ITS) such as mobile vehicle detectors (Bluetooth and GPS vehicle locators).   
Given the potential benefits of these strategies, this research proposes MASTraf: a decentralized Multi-Agent System for network-wide Traffic signal control with dynamic coordination. MASTraf is designed to capture the behavior of the environment and take decisions based on situations directly observed by RL agents. Also, agents can communicate with each other, exploring the effects of temporary coalitions or subgroups of intersections as a mechanism for coordination. 
Separate MASTraf implementations with similar state and reward functions using Q-learning and ADP were tested using a microscopic traffic simulator (VISSIM) and real-time manipulation of the traffic signals through the software’s COM interface. Testing was conducted to determine the performance of the agents in scenarios with increasing complexity, from a single intersection, to arterials and networks, both in undersaturated and oversaturated conditions. 
Results show that the multi-agent system provided by MASTraf improves its performance as the agents accumulate experience, and the system was able to efficiently manage the traffic signals of simple and complex scenarios. Exploration of the policies generated by MASTraf showed that the agents followed expected behavior by providing green to greater vehicle demands and accounting for the effects of blockages and lost time. The performance of MASTraf was on par with current state of practice tools for finding signal control settings, but MASTraf can also adapt to changes in demands and driver behavior by adjusting the signal timings in real-time, thus improving coordination and preventing queue spillbacks and green starvation.  
A strategy for signal coordination was also tested in one of the MASTraf implementations, showing increased throughput and reduced number of stops, as expected. The coordination employed a version of the max-plus algorithm embedded in the reward structure, acting as a bias towards improved coordination. The response of the system using imprecise detector data, in the form of coarse aggregation, showed that the system was able to handle oversaturation under such conditions. Even when the data had only 25% of the resolution of the original implementation, the system throughput was only reduced by 5% and the number of stops per vehicle was increased by 8%. 
The state and reward formulations allowed for a simple function approximation method in order to reduce the memory requirements for storing the state space, and also to create a form of generalization for states that have not been visited or that have not been experienced enough. Given the discontinuities in the reward function generated by penalties for blockages and lost times, the value approximation was conducted through a series of functions for each action and each of the conditions before and after a discontinuity. 
The policies generated using MASTraf with a function approximation were analyzed for different intersections in the network, showing agent behavior that reflected the principles formulated in the original problem using lookup tables, including right of way assignment based on expected rewards with consideration of penalties such as lost time. In terms of system performance, MASTraf with function approximation resulted in average reductions of 1% in the total system throughput and 3.6% increases in the number of stops per vehicle, when compared to the implementation using lookup tables on a congested network of 20 intersections",,MASTraf: a decentralized multi-agent system for network-wide traffic signal control with dynamic coordination,,,,core
389907284,2015-12-17T00:00:00,"Objective. This paper presents the selective particle swarm optimization and results of it applying to solve a problem of the optimal capacitor placement and sizing, configuration and conductor sizing simultaneously for a real distribution system with 3 substations, 37 feeders, 274 buses, 284 branches and 11 normally open switches located in the eastern part of Mariupol city. Technique. Selective particle swarm optimization as a simple modification of the binary particle swarm optimization to search in a selected decision space was used for solving the aforesaid problem formulated as a multi-objective mixed-integer combinatorial nonlinear optimization problem with equality and inequality constraints. To verify the effectiveness of the proposed algorithm, a plenty of test systems and real schemes with the different number of buses were used and selective particle swarm optimization was compared to another modern heuristic and artificial intelligence techniques (e.g., simulated annealing, genetic algorithms, ant colony search algorithm, fuzzy-reasoning approach, etc.). MATLAB R2010a was applied as a computational tool to realize the proposed algorithm for modeling of the real large-scale distribution system. Before starting simulation, the long-term measurements (during one week) of the active and reactive power, power factor, voltage distortion and unbalance were made by Fluke 435 power quality analyzer on the four feeders with the highest active power losses and voltage drop. Results. Simulation results before and after optimization demonstrated an effectiveness of the selective particle swarm optimization in reducing the both active power and energy losses, improving the voltage quality and receiving the total annual saving. A comparative study of the three abovementioned techniques (capacitor placement, reconfiguration and reconductoring) to optimize a performance of the large-scale distribution system was executed. Scientific novelty. The proposed algorithm has been implemented for an optimization procedure of the real large-scale distribution system to find the optimal capacitor placement and sizing, configuration and conductor sizing simultaneously accounting for technical constraints (maximum permissible branch current, maximum and minimum voltage limits, maximum total harmonic distortion limit and maximum permissible total size of capacitors) and operational constraints (load connectivity and radial network structure). Practical value. For investigated distribution system, applying of all three optimization techniques simultaneously with THD constraint would be reduce the active power losses from 7.4 % to 3.5 % and energy losses – from 5 % to 2.38 % (approximately twice). In this case, the total annual saving would be about $700000 and the total saving for remaining period – about $4.8 million.Цель. В работе приведены селективный метод роя частиц и результаты его применения для решения проблемы совместной оптимизации мест установки и мощности батарей конденсаторов, конфигурации и сечений проводников реальной распределительной сети, которая содержит 3 подстанции, 37 фидеров, 274 узла, 284 ветви и 11 нормально разомкнутых коммутационных аппаратов, и расположена в восточной части города Мариуполь. Методика. Селективный метод роя частиц, представляющий собой простую модификацию бинарного метода роя частиц, предназначенную для поиска в выбранном пространстве решений, был использован для решения вышеуказанной проблемы, сформулированной как проблема многокритериальной частично целочисленной комбинаторной нелинейной оптимизации с ограничениями в виде равенств и неравенств. Для подтверждения эффективности предложенного алгоритма были использованы многочисленные тестовые и реальные схемы с различным количеством узлов, а селективный метод роя частиц сравнили с другими современными эвристическими методами и методами искусственного интеллекта (например, имитация отжига, генетические алгоритмы, алгоритм колонии муравьёв, нечёткая логика и др.). Чтобы реализовать предложенный алгоритм для моделирования реальной разветвлённой сети, в качестве вычислительного средства была использована среда MATLAB R2010a. Перед началом моделирования с помощью анализатора качества электроэнергии Fluke 435 на четырёх фидерах с наибольшими потерями активной мощности и падением напряжения были проведены длительные (в течение одной недели) измерения активной и реактивной мощности, коэффициента мощности, несинусоидальности и несимметрии напряжений. Результаты. Результаты моделирования до и после оптимизации продемонстрировали эффективность применения селективного метода роя частиц для снижения потерь активной мощности и электроэнергии, улучшения качества напряжения и получения суммарной годовой экономии. Выполнено сравнительное исследование трёх приведенных выше методов оптимизации параметров разветвлённой распределительной сети (установка батарей конденсаторов, реконфигурация и замена сечений проводников). Научная новизна. Предложенный алгоритм был реализован для оптимизации параметров реальной разветвлённой распределительной сети с целью совместного определения оптимальных мест установки и мощности батарей конденсаторов, конфигурации и сечений проводников, учитывая технические ограничения (максимально допустимый ток, максимально и минимально допустимые напряжения, максимальное значение суммарного коэффициента гармонических составляющих по напряжению и максимально допустимое значение суммарной установленной мощности батарей конденсаторов) и эксплуатационные ограничения (отсутствие отключенных нагрузок и радиальная структура сети). Практическая значимость. Применение для исследуемой распределительной сети всех трёх методов оптимизации одновременно с учётом ограничения на максимальное значение суммарного коэффициента гармонических составляющих по напряжению позволило бы снизить потери активной мощности с 7,4 % до 3,5 %, а потери электроэнергии – с 5 % до 2,38 % (примерно в 2 раза). В этом случае суммарная годовая экономия составила бы около 700000 $, а суммарная экономия за остаточный период – около 4,8 млн. $.Мета. В роботі наведені селективний метод рою частинок та результати його застосування щодо вирішення проблеми спільної оптимізації місць встановлення та потужності батарей конденсаторів, конфігурації та перерізів провідників реальної розподільної мережі, яка містить 3 підстанції, 37 фідерів, 274 вузли, 284 гілки та 11 нормально розімкнених комутаційних апаратів і розташована у східній частині міста Маріуполь.Методика. Селективний метод рою частинок, який є простою модифікацією бінарного методу рою частинок, призначеною для пошуку у обраному просторі розв'язань, був застосований щодо вирішення зазначеної вище проблеми, сформульованої як проблема багатокритеріальної частково цілочислової комбінаторної нелінійноїоптимізації з обмеженнями у вигляді рівностей та нерівностей. Для підтвердження ефективності запропонованого алгоритму були використані численні тестові та реальні схеми з різною кількістю вузлів, а селективний метод рою частинок був порівняний з іншими сучасними евристичними методами та методами штучного інтелекту (наприклад, імітація відпалу, генетичні алгоритми, алгоритм колонії мурашів, нечітка логіка та ін.). Щоби реалізувати запропонований алгоритм для моделювання реальної розгалуженої розподільної мережі, у якості обчислювального засобу було застосовано середовище MATLAB R2010a. Перед початком моделювання за допомогою аналізатору якості електроенергії Fluke 435 на чотирьох фідерах з найбільшими втратами активної потужності та спадом напруги були виконані тривалі (протягом одного тижня) вимірювання активної та реактивної потужності, коефіцієнту потужності, несинусоїдності та несиметрії напруг. Результати. Результати моделювання до та після оптимізації продемонстрували ефективність застосування селективного методу рою частинок для зниження втрат активної потужності та електроенергії, покращення якості напруги та отримання сумарної річної економії. Виконано порівняльне дослідження трьох зазначених вище методів оптимізації параметрів розгалуженої розподільної мережі (встановлення батарей конденсаторів, реконфігурація та заміна перерізів провідників). Наукова новизна. Запропонований алгоритм був реалізований для оптимізації параметрів реальної розгалуженої розподільної мережі з метою спільного знаходження оптимальних місць встановлення та потужності батарей конденсаторів, конфігурації та перерізів провідників, враховуючи технічні обмеження (максимально припустимий струм, максимально та мінімально припустимі напруги, максимальне значення сумарного коефіцієнту гармонічних складових за напругою та максимально припустиме значення сумарної встановленої потужності батарей конденсаторів) та експлуатаційні обмеження (відсутність вимкнених навантажень та радіальна структура мережі). Практична значимість. Застосування для досліджуваної розподільної мережі всіх трьох методів оптимізації разом з урахуванням обмеження на максимальне значення сумарного коефіцієнту гармонічних складових за напругою дозволило би знизити втрати активної потужності з 7,4 % до 3,5 %, а втрати електроенергії – з 5 % до 2,38 % (приблизно у 2 рази). У цьому випадку сумарна щорічна економія склала би приблизно 700000 $, а сумарна економія за залишковий період – приблизно 4,8 млн. $",Dnipropetrovsk National University of Railway Transport named after Acad. V.  Lazaryan,Застосування селективного методу рою частинок для оптимізації режимів та структури реальної розгалуженої розподільної мережі з метою зниження втрат електроенергії та покращення якості напруги,,,,core
48220435,2012-01-14T00:00:00,"International audienceVideo services are being adopted widely in both mobile and fixed networks. For their successful deployment, the content providers are increasingly becoming interested in evaluating the performance of such traffic from the final users' perspective, that is, their Quality of Experience (QoE). For this purpose, subjective quality assessmentmethods are costly and can not be used in real time. Therefore, automatic estimation of QoE is highly desired. In this paper, we propose a noreference QoE monitoringmodule for adaptive HTTP streaming using TCP and the H.264 video codec. HTTP streaming using TCP is the popular choice of many web based and IPTV applications due to the intrinsic advantages of the protocol. Moreover, these applications do not suffer from video data loss due to the reliable nature of the transport layer. However, there can be playout interruptions and if adaptive bitrate video streaming is used then the quality of video can vary due to lossy compression. Our QoE estimation module, based on Random Neural Networks, models the impact of both factors. The results presented in this paper show that our model accurately captures the relation between them and QoE",HAL CCSD,Quality of Experience estimation for Adaptive HTTP/TCP video streaming using H.264/AVC,,,,core
216542904,2013-01-01T00:00:00,"O problema de mobilidade urbana tem crescido a níveis cada vez mais desgastantes para a população das cidades. Acompanhamos diariamente notícias sobre: os enormes engarrafamentos, a falta de transporte público eficiente e de qualidade, o desrespeito as regras de trânsito, os acidentes e as mortes desnecessárias. Nem todos os problemas podem ser resolvidos apenas com a melhoria da educação no trânsito, alguns requerem altos investimentos, como na expansão e duplicação de vias, construção de viadutos e pontes, e que permitem suprir a demanda cada vez maior de veículos. Com menor custo de implantação podemos incluir a sinalização de trânsito e semafórica, que ajuda no controle das vias e aumenta a segurança para as pessoas. Neste trabalho é estudado um sistema de controle de trânsito simulado com a perspectiva de melhorar o sistema semáforico real. Visto que desta forma é possível causar um grande impacto aos sistemas de transporte em geral com um baixo custo de implantação quando comparado as modificações na infraestrutura das vias. Serão aplicados três algoritmos de inteligência artificial ao sistema semafórico experimental, de forma a torná-lo adaptativo, e assim permitir uma melhoria do fluxo de veículos através de um sistema que não necessita de um controle centralizado. Os resultados serão apresentados através da comparação dos algoritmos utilizados, levando-se em conta questões como tempo de simulação e tempo de viagem dos veículos.The problem of urban mobility has grown to increasingly stressful levels for the cities population. We see daily reports about: huge traffic jams, the lack of a quality and efficient public transportation, the disregard of the traffic rules, the accidents and the unnecessary deaths. All problems can not be solved only with the improvement of traffic education, some require high investments, such as the expansion and duplication of streets, construction of viaducts and bridges, which allow to meet the increasing demand of vehicles. With lower cost of deployment we can include traffic signs and traffic lights, which helps in streets control and increases the people safety. In this work is presented a study about a simulated traffic control system with the prospect of improving the actual traffic system. In this way is possible to generate a major impact on transport systems, in general with a low implementation cost if compared with changes in the infrastructure of roads. Will be applied three artificial intelligence algorithms to an experimental traffic lights system, in order to make it adaptive, and thus allow an improvement in the traffic flow through a system which does not require a centralized control. The results are presented through a comparison of the algorithms used, taking into account issues such as simulation time and vehicles travel time",,"Adaptive traffic signals, a swarm-based approach for the urban mobility",,,,core
333585435,2013-01-01T00:00:00,"Background: B-chronic lymphocytic leukemia (B-CLL) is characterized by the accumulation of mature-like monoclonal B lymphocytes, defective apoptosis, progressive hypogammaglobulinemia with immunodeficiency, and high prevalence of autoimmune phenomena.Toll-like receptors (TLRs) are a pattern of type I integral membrane glycoproteins expressed on cells of the immune system and represent major agents of innate immunity and initiators of adaptive immunity. Aims: To evaluate TLR4 and TLR9 genes and protein expression in B-lymphocytes from B-CLL patients, and its relationship with stage, therapy, and known prognostic markers (IgVH region mutational status, cytogenetic alterations,CD38 and ZAP70). Further aim was to correlate TLR4 and 9 expression with infections, autoimmunity and disease progressionMethods: TLR4 and TLR9 gene expression was evaluated in total RNA of B cell from 95 B-CLL patients and controls (pool of 10 healthy donors) by TaqMan\uae Gene Expression Assays on the model 7300 real-time PCR system (Applied BioSystems-Life Technology, Foster City, CA, USA). The results of relative quantitation (RQ) assays were analyzed using SDS Software (Applied BioSystems-Life Technology). Surface TLR4 and TLR9 expression on B-lyphocytes was determined by standard flow cytometry. TLR4 expression was evaluated in cultured B cells, obtained with RosetteSep (purity &gt;97%) either unstimulated or stimulated with 1mg/mL lypopolysaccharide (LPS). Culture supernatants were assayed for IL-10 and TGF-\u3b2 production using commercially available ELISA kits. Results: Patients (41 females and 56 males, median age 74 years, range 41-
89) were 78.9% in Binet stage A, 11.6% in B, and 9.5% in C and were followed for a median of 27.6 months, range 19-44. Quantitative real-time PCR revealed that TLR4 gene expression was decreased (RQ=16.1+1.56) and TLR9increased (RQ=2725+165) in all B-CLL patients versus controls (RQ=100). Consistently, the percentage of CD19+ cells expressing TLR4 by cytofluorimetric analysis was lower (1.70+0.2% versus 3.93+0.68%, P=0.004) and TLR9 greater (5.5+0.6% versus 1.39+0.31%, P=0.04) in patients compared to controls. TLR4 reduction was more pronounced in advanced and multi-treated disease, and in patients with unmutated IgVH status and unfavorable cytogenetic abnormalities. Univariate Cox regression model showed that patients with reduced TLR4 gene expression had an increased risk of disease progression
(HR 4.6, 95% CI 1.8-11.6; P=0.001) (Figure1, upper panel), suggesting that an impaired innate immunity identifies a subset of B-CLL patients with poor prognosis. Interestingly, patients with reduced TLR4 gene expression had an increased risk to develop autoimmune complications (P=0.02) (Figure1, lower panel). Finally, TGF-\u3b2 production by B cells was increased in patients who thereafter developed autoimmune complications, compared with those that did not (1092+158 versus 793+18 pg/mL, mean+SE, P=0.07). Summary / Conclusion: Reduced expression of TLR4 in B-CLL patients correlates with an advanced and multi-treated disease and is a risk factor for disease progression and development of autoimmune complications. Moreover, B-lymphocytes from patients with autoimmunity produced high levels of TGF\u3b2. These findings suggest that interactions between the B cell receptor and TLRs signaling, and an abnormal cytokine pattern play a critical role in the maintenance of self-tolerance",,Reduced expression of toll-like receptor 4 in B-chronic lymphocytic leukemia patients with autoimmune complications,,,,core
58800040,01/09/2012,"Traffic accidents are still one of the main health problems in the World. A number of measures have been applied in order to reduce the number of injuries and fatalities in roads, i.e., implementation of Advanced Driver Assistance Systems (ADAS) based on image processing. In this paper, a real time speed supervisor based on road sign recognition that can work both in urban and non-urban environments is presented. The system is able to recognize 135 road signs, belonging to the danger, yield, prohibition obligation and indication types, and sends warning messages to the driver upon the combination of two pieces of information: the current speed of the car and the road sign symbol. The core of this paper is the comparison between the two main methods which have been traditionally used for detection and recognition of road signs: template matching (TM) and neural networks (NN). The advantages and disadvantages of the two approaches will be shown and commented. Additionally we will show how the use of well-known algorithms to avoid illumination issues reduces the amount of images needed to train a neural network.The work reported in this article has been partly funded by the Spanish Government by the grants
FEDORA (TRA2010-20225-C03-01) and D3System (TRA2011-29454-C03-02)",MDPI Publishing,Recognition Stage for a Speed Supervisor Based on Road Sign Detection,,10.3390/s120912153,"[{'title': None, 'identifiers': ['issn:1424-8220', '1424-8220']}]",core
296648505,2015-11-26T15:11:44Z,"Camera-based estimation of drivable image areas is still in evolution. These systems have been developed for improved safety and convenience, without the need to adapt itself to the environment. Machine Vision is an important tool to identify the region that includes the road in images. Road detection is the major task of autonomous vehicle guidance. In this way, this work proposes a drivable region detection algorithm that generates the region of interest from a dynamic threshold search method and from a drag process (DP). Applying the DP to estimation of drivable image areas has not been done yet, making the concept unique. Our system was has been evaluated from real data obtained by intelligent platforms and tested in different types of image texture, which include occlusion case, obstacle detection and reactive navigation. © 2013 IEEE.6368Australian Dedicated Short Range Communication (AusDSRC),Griffith University Intelligent Control Systems Laboratory,Intelligent Transport Systems AustraliaBonin-Font, F., Ortiz, A., Oliver, G., Visual navigation for mobile robots: A survey (2008) Journal of Intelligent and Robotic Systems, 53 (3), pp. 263-296Rodríguez Flórez, S.A., (2010) Contributions by Vision Systems to Multi-sensor Object Localization and Tracking for Intelligent Vehicles, , Thesis, UTC, FranceThrun, S., Stanley, the robot that won the darpa grand challenge (2006) Journal of Robotic Systems, 23 (9), pp. 661-692. , 2006, ISSN: 0741-2223(2007) Spirit of Berlin: An Autonomous Car for the DARPA Urban Challenge Hardware and Software Architecture, , retrieved Jan 05, 2010]Gietelink, O., Ploeg, J., De Schutter, B., Verhaegen, M., Development of advanced driver assistance systems with vehicle hardware-in-The-loop simulations (2006) Vehicle System Dynamics, 44 (7)Ulrich, I., Nourbakhsh, I., Appearance-based obstacle detection with monocular color vision (2000) Proceedings of the AAAI National Conference on Artificial Intelligence, pp. 866-871Kim, B., Hubbard, P., Necsulescu, D., (2003) Swarming Unmanned Aerial Vehicles: Concept Development and Experimentation, A State of the Art Review on Flight and Mission Control, , Technical MemorandumAviña-Cervantes, G., Devy, M., Marín, A., Lane extraction and tracking for robot navigation in agricultural applications (2003) Proceedings of the IEEE ICAR 2003Dahlkamp, H., Self-supervised monocular road detection in desert terrain (2006) Proceedings of the Robotics Science and Systems ConferenceDiego, F., Álvarez, J.M., Serrat, J., López, A.M., Vision based road detection via on line video registration (2010) Proceedings of the IEEE ITSC 2010Chetan, J., Madhava, K., Jawahar, C.V., An adaptive outdoor terrain classification methodology using monocular camera (2010) Proceedings of the IEEE IROS 2010Yanqing, W., Deyun, C., Chaoxia, S., Peidong, W., Vision-based road detection by monte carlo method (2010) Information Technology Journal, 9, pp. 481-487Yamaguchi, K., Watanabe, A., Naito, T., Ninomiya, Y., Road region estimation using a sequence of monocular images (2008) Proceedings of the International Conference on Pattern Recognition, 2008Otsu, N., A threshold selection method from graylevel histogram (1978) IEEE Transactions on Systems, Man, and Cybernetics, 9, pp. 62-66Bertozzi Broggi, A.M., Fascioli, A., Vision-based intelligent vehicles: State of the art and perspectives (2000) Robotics and Autonomous Systems, 32, pp. 1-16Miranda Neto, A., Rittner, L., Leite, N., Zampieri, D.E., Lotufo, R., Mendeleck, A., Pearson's correlation coefficient for discarding redundant information in real time autonomous navigation systems (2007) Proceedings of the IEEE MSC 2007Benini, L., Bogliolo, A., Micheli, G.D., A survey of design techniques for system-level dynamic power management (2000) IEEE Transactions on Very Large Scale Integration Systems, 8 (3), pp. 299-316Miranda Neto, A., Victorino, A.C., Fantoni, I., Zampieri, D.E., Real-time dynamic power management based on pearson's correlation coefficient (2011) Proceedings of the IEEE ICAR 2011King, H.L., Vision-based lane-vehicle detection and tracking (2009) IAENG Transactions on Engineering Technologies Volume 3-Special Edition, pp. 157-171. , American Institute of PhysicsEttinger, S., Vision-guided flight stability and control for micro air vehicles (2003) Adv. Robotics, pp. 617-640Neto, A.M., Victorino, A.C., Fantoni, I., Zampieri, D.E., Robust horizon finding algorithm for real-time autonomous navigation based on monocular vision (2011) Proceedings of the IEEE ITSC 2011Miranda Neto, A., Rittner, L., A simple and efficient road detection algorithm for real time autonomous navigation based on monocular vision (2006) Proceedings of the IEEE 3rd LARS 2006Gonzalez, C.R., Woods, E.R., (1991) Digital Image Processing, , Addison-Wesley Publishing CompanySahoo, P.K., Soltani, S., Wong, A.K.C., A survey of thresholding techniques (1988) Comput. Vision Graphics Image Processing, 41, pp. 233-260Lee, U.S., Chung, Y.S., Park, H.R., A comparative performance study of several global thresholding techniques for segmentation (1990) Computer Vision, Graphics, and Image ProcessingSezgin, M., Sankur, B., Survey over image thresholding techniques and quantitative performance evaluation (2004) Journal of Electronic Imaging, 13, pp. 146-165Rauskolb, F.W., Caroline: An autonomously driving vehicle for urban environments (2008) Journal of Field Robotics, 25 (9), pp. 674-724Canny, J.F., (1986) A Computational Approach to Edge Detection, , IEEE Trans. Pattern Anal. Machine IntellBallard, D., Generalized hough transform to detect arbitrary shapes (1981) IEEE Trans. Pattern Anal. Machine Intell, 13 (2), pp. 111-122White, F.M., (1986) Fluid Mechanics, , 2nd Ed. McGraw HillDARPA 2005. DARPA Grand ChallengeNeto, A.M., Victorino, A.C., Fantoni, I., Zampieri, D.E., Ferreira, J.V., Visual-perception layer applied to reactive navigation (2012) Proceedings of the IEEE 9th LARS 2012http://youtu.be/ZpEbRo32pY8, retrieved Jan 31, 201",,Real-time Estimation Of Drivable Image Area Based On Monocular Vision,,10.1109/IVS.2013.6629448,,core
90587106,2015-04-01T00:00:00Z,"Passenger flow modeling and station dwelling time estimation are significant elements for railway mass transit planning, but system operators usually have limited information to model the passenger flow. In this paper, an artificial-intelligence technique known as fuzzy logic is applied for the estimation of the elements of the origin-destination matrix and the dwelling time of stations in a railway transport system. The fuzzy inference engine used in the algorithm is based in the principle of maximum entropy. The approach considers passengers’ preferences to assign a level of congestion in each car of the train in function of the properties of the station platforms. This approach is implemented to estimate the passenger flow and dwelling times of the recently opened Line 1 of the Panama Metro. The dwelling times obtained from the simulation are compared to real measurements to validate the approach",MDPI AG,A Fuzzy Logic-Based Approach for Estimation of Dwelling Times of Panama Metro Stations,,10.3390/e17052688,"[{'title': None, 'identifiers': ['1099-4300', 'issn:1099-4300']}]",core
275620737,2013-11-06T00:00:00,"[EN] The Science of Artificial Intelligence provides us with techniques to improve our understanding and characterization of the coherences and patterns which constitute reality. Among these, artificial neural networks and more specifically Self Organizing Maps (SOM) stand out because of their ability to map reality in such a way that their objectives are represented distributed and structured two-dimensionally, with their properties as a single starting point. In this way an entire series of topological relations is generated, which in their turn enable the grouping and characterization of reality. In this research these representations are explored as a valid method to obtain information and to interpret reality. By means of experimentation this kind of methods are implemented to further understanding of diverse exemplary residential fabrics, while obtaining a typological grouping which enables the characterization of urban forms starting from their defining variables[ES] Las ciencias de la Inteligencia Artificial proporcionan técnicas para la comprensión y caracterización de las coherencias y de los patrones que constituyen la realidad. Entre ellas destacan las redes neuronales artificiales y concretamente los Mapas Auto-organizados (SOM) por su capacidad de cartografiar la realidad, representando sus objetivos distribuidos estructurados bidimensionalmente, a partir únicamente de sus propiedades. Se generan así toda una serie de relaciones topológicas que permiten a su vez la agrupación y caracterización de la realidad. En la investigación se exploran estas representaciones como método válido de obtención de información e interpretación de la realidad. Como experimentación se implementan tales técnicas para la comprensión de diversos tejidos residenciales ejemplares, obteniéndose un agrupamiento tipológico que permite caracterizar las formas urbanas a partir de sus variables definidoras.Abarca-Alvarez, FJ.; Osuna Pérez, F. (2013). Cartografías semánticas mediante redes neuronales: los mapas auto-organizados (SOM) como representación de patrones y campos. EGA. Revista de Expresión Gráfica Arquitectónica. 18(22):154-163. doi:10.4995/ega.2013.1692.SWORD154163182",'Universitat Politecnica de Valencia',SEMANTIC MAPPING THOUGH NEURAL NETWORKS: THE SELF-ORGANIZING MAPS (SOM) AS REPRESENTATION OF PATTERNS AND FIELDS,https://riunet.upv.es/bitstream/handle/10251/75710/1692-5743-1-PB.pdf?sequence=1&isAllowed=y,10.4995/ega.2013.1692,,core
214020501,2014-05-21T21:30:00,"This presentation discusses the pedagogical outcomes of using Geographic Information Systems (GIS) technology in an intermediate-level French language and culture course. The course is based around an interactive, web-based map of the spaces, both real and imaginary, featured in various literary and cultural documents that have evoked Paris throughout its history. Through targeted online activities done outside of the classroom, students review the course material and reconstruct the multiple layers of the city, navigating through the cultural, urban, architectural and social history of Paris in the process. The digital mapping interface enables them to be more active in the creation of knowledge (data collection) and fosters critical thinking in order to better understand the urban evolution of Paris (or a specific neighborhood) and how the past explains the modern city. Learning historical and cultural knowledge through geo-visualization offers an efficient way to foster visual and spatial thinking, as well as critical and analytical skills within an interdisciplinary framework. The blended learning approach of the course frees up time for deeper class discussions and face-to-face interaction for practice of linguistic skills at a higher level. In brief, the use of a digital mapping interface provides a framework for implementation of blended learning in a foreign language and culture course, allowing new opportunities generating deep learning outcomes and high motivational results","Scholarship, Research, and Creative Work at Bryn Mawr College",“Mapping Paris”: Using GIS Technology for a Blended Learning Approach in a Foreign Language and Culture Course,,,,core
300347872,2015-01-01T00:00:00,"Complex real problems increasingly require intelligent systems that combine knowledge, techniques and methodologies from various sources. Intelligent systems based on artificial intelligence techniques that are associated with the behavior of people can perform the processes of learning, reasoning and solving all kinds of problems. Such systems, which automatically can perform tasks set by the user or other software, today thankfully called intelligent agents. Independent, intelligent agents on the Internet can be very successful to perform some search work on behalf of and for the needs of different users. For efficient collection, manipulation and management of data, such software can be very interesting from the standpoint of intelligent data analysis in many areas the police. Analysis of the data collected by an intelligent agent (a software robot-bot) can be successfully utilized, among many jobs in the police, and in the field of crime and in particular manifestation of cyber­crime, traffic safety, emergencies, etc. To make the collection and analysis of data from criminal activities on the Internet effective, it is necessary to examine the existing artificial intelligence techniques to be used for the conclusion of the intelligent agents. On the other hand, using of methods of artificial intelligence in finding data along with intelligent data analysis (data mining) should be used, which has found wide use in the area of business, economics, mechanics, medicine, genetics, transport etc.Kompleksni realni problemi sve češće zahtevaju inteligentne sisteme koji kombinuju znanje, tehnike i metodologije iz različitih izvora. Inteligentni sistemi bazirani na tehnikama veštačke inteligencije koje asociraju na ponašanje ljudi mogu da obavljaju procese učenja, zaključivanja i rešavanje raznovrsnih problema. Ovakvi sistemi, koji automatski mogu da izvrše zadatke zadate od strane korisnika ili drugih softvera, danas se sreću pod imenom inteligentni agenti. Samostalno, inteligentni agenti na Internetu mogu veoma uspešno da izvode neki pretraživački posao u ime i za potrebe raznih korisnika. Zbog efikasnog sakupljanja, manipulisanja i upravljanja podacima, ovakvi softveri mogu biti veoma interesantni sa stanovišta inteligentne analize podataka u mnogim oblastima policije. Analiza podataka sakupljenih od strane inteligentnog agenta (softverskog robota - bota) može se uspešno iskoristiti, između mnogih poslova u policiji, i na polju kriminala i naročito pojavnog oblika sajber kriminala, bezbednosti saobraćaja, vanrednih situacija itd. Kako bi sakupljanje i analiza podataka iz kriminalnih aktivnosti na Internetu bila efikasna, neophodno je sagledati postojeće tehnike veštačke inteligencije koje se koriste za zaključivanje u inteligentnim agentima. S druge strane, treba iskoristiti metode veštačke inteligencije u pronalaženju podataka pri inteligentnoj analizi podataka (data mining-u) koja je našla široku primenu u oblasti poslovanja preduzeća, ekonomije, mehanike, medicine, genetike, saobraćaja i sl",'Centre for Evaluation in Education and Science (CEON/CEES)',Veštačka intelegencija u prikupljanju i analizi podataka u policiji,https://core.ac.uk/download/pdf/300347872.pdf,10.5937/NBP1503131K,"[{'title': 'Nauka bezbednost policija', 'identifiers': ['issn:0354-8872', '0354-8872']}]",core
87930708,2012-09-27T00:00:00,"Part 7: Multi Agent SystemsInternational audienceThis paper describes the design and implementation of a modular hybrid intelligent model and system, for monitoring and forecasting of air pollution in major urban centers. It is based on Multiagent technologies, Artificial Neural Networks (ANN), Fuzzy Rule Based sub-systems and it uses a Reinforcement learning approach. A multi level architecture with a high number of agent types was employed. Multiagent’s System modular and distributed nature, allows it’s interconnection with existing systems and it reduces its functional cost, allowing its extension by incorporating decision functions and real time imposing actions capabilities",'Springer Science and Business Media LLC',Hybrid and Reinforcement Multi Agent Technology for Real Time Air Pollution Monitoring,,10.1007/978-3-642-33409-2_29,,core
295402756,2013-06-30T00:00:00,"The “Cooperativa Andria” has been among operators in the social housing sector the one who have taken promptly and accurately the spirit of innovation that characterized the regional planning program in the first half of the Nineties, following the promulgation of the law 179/1992, which called a focus on the housing problems of particular social categories who in those years were highlighted in most cities and urban areas of the country: the elderly, young couples, immigrants. In response to announcements for subsidized housing developed by the Emilia-Romagna Region, Andria has implemented pilot-actions as real best practises.La Cooperativa Andria è stata tra gli operatori del settore dell’edilizia abitativa sociale che hanno colto con tempestività e precisione lo spirito di innovazione che caratterizzava la programmazione regionale nella prima metà degli anni Novanta, a seguito della promulgazione della legge 179 del 1992, che chiedeva una particolare attenzione per i problemi abitativi di particolari categorie sociali che in quegli anni si evidenziavano nelle maggiori città ed aree urbane del paese: gli anziani, le giovani coppie, gli immigrati. In risposta ai bandi per l’edilizia agevolata elaborati dalla Regione Emilia-Romagna, Andria ha realizzato interventi-pilota che sono stati dei veri prototipi","Dipartimento di Architettura, Alma Mater Studiorum – Università di Bologna",Esperienze di housing sociale nel reggiano,,10.6092/issn.2036-1602/3946,,core
4838711,2012-05-01T00:00:00,"Current analyses of groundwater flow and transport typically rely on a physically-based model (PBM), which is inherently subject to error and uncertainty from multiple sources including model structural error, parameter error and data error. The model uncertainty can be difficult to quantify, and is propagated to the prediction. In this study, complementary data-driven models (DDMs) are used to improve prediction of groundwater flow models. The DDMs, trained with the historical residual of the PBM, have the capability to compensate for the defects of PBM. Five machine learning techniques, instance-based weighting (IBW), locally weighted regression (\textit{loess}), decision trees (DT), artificial neural networks (ANN) and support vector regression (SVR) are employed to construct the DDMs, and their performance of enhancing the prediction of the PBM is compared. Before the DDMs updating, cluster analysis is implemented on the dataset to improve the robustness and efficiency of the framework. The framework is tested in two real-world case studies based on the Republic River Compact Association (RRCA) model and the Spokane Valley Rathdrum Prairie (SVRP) model. The DDMs reduce the root-mean-square errors (RMSE) of the temporal, spatial and temporal plus spatial head prediction of the RRCA model by 82\%, 60\% and 48\% respectively. In the SVRP case study, the DDMs reduces the temporal head forecast of the PBM by 79\%. Localized DDMs that are conditioned on each cluster outperform global DDMs without clustering. It is also demonstrated that clustering significantly reduces the computational cost of training and cross validation of the DDMs. After clustering, the run-time of DDMs is negligible comparing with the PBM, which makes the framework very computationally efficient",,Use of data-driven models to improve prediction of physically based groundwater models,https://core.ac.uk/download/4838711.pdf,,,core
303067668,2014-05-21T21:30:00,"This presentation discusses the pedagogical outcomes of using Geographic Information Systems (GIS) technology in an intermediate-level French language and culture course. The course is based around an interactive, web-based map of the spaces, both real and imaginary, featured in various literary and cultural documents that have evoked Paris throughout its history. Through targeted online activities done outside of the classroom, students review the course material and reconstruct the multiple layers of the city, navigating through the cultural, urban, architectural and social history of Paris in the process. The digital mapping interface enables them to be more active in the creation of knowledge (data collection) and fosters critical thinking in order to better understand the urban evolution of Paris (or a specific neighborhood) and how the past explains the modern city. Learning historical and cultural knowledge through geo-visualization offers an efficient way to foster visual and spatial thinking, as well as critical and analytical skills within an interdisciplinary framework. The blended learning approach of the course frees up time for deeper class discussions and face-to-face interaction for practice of linguistic skills at a higher level. In brief, the use of a digital mapping interface provides a framework for implementation of blended learning in a foreign language and culture course, allowing new opportunities generating deep learning outcomes and high motivational results","Scholarship, Research, and Creative Work at Bryn Mawr College",“Mapping Paris”: Using GIS Technology for a Blended Learning Approach in a Foreign Language and Culture Course,,,,core
54527048,2015-01-01T00:00:00,"In-memory (transactional) data stores, also referred to as data grids, are recognized as a first-class data management technology for cloud platforms, thanks to their ability to match the elasticity requirements imposed by the pay-as-you-go cost model.

On the other hand, determining how performance and reliability/availability of these systems vary as a function of configuration parameters, such as the amount of cache servers to be deployed, and the degree of in-memory replication of slices of data, is far from being a trivial task. Yet, it is an essential aspect of the provisioning process of cloud platforms, given that it has an impact on the amount of cloud resources that are planned for usage.

To cope with the issue of predicting/analysing the behavior of different configurations of cloud in-memory  data stores, in this article we present a flexible simulation framework offering skeleton simulation models that can be easily  specialized in order to capture the dynamics of diverse data grid systems, such as those related to the specific (distributed) protocol used to provide data consistency and/or transactional guarantees. Besides its flexibility, another peculiar aspect of the framework lies in that it integrates simulation and machine-learning (black-box) techniques, the latter being used to capture the dynamics of the data-exchange layer (e.g. the message passing layer) across the cache servers. This is a relevant aspect when considering that the actual data-transport/networking infrastructure on top of which the data grid is deployed might be unknown, hence being  not feasible to be modeled via white-box (namely purely simulative) approaches.

 We also provide an extended experimental study aimed at validating instances of simulation models supported by our framework against execution dynamics of real data grid systems deployed on top of either private or public cloud infrastructures. Particularly,   our validation test-bed has been based on an industrial-grade open-source data grid, namely Infinispan by JBoss/Red-Hat, and a de-facto standard benchmark for NoSQL platforms, namely YCSB by Yahoo.

The validation study has been conducted by relying on both public and private cloud systems, scaling the underlying infrastructure up to 100 (resp. 140) Virtual Machines for the public (resp. private) cloud case. Further, we provide some experimental data related to a scenario where our framework is used for on-line capacity planning and reconfiguration of the data grid system",'Elsevier BV',A flexible framework for accurate simulation of cloud in-memory data stores,https://core.ac.uk/download/54527048.pdf,10.1016/j.simpat.2015.05.011,,core
231037332,2014-02-28T17:00:00,"Please join us for a panel covering all aspects of patent assertion and non-practicing entities and their effect on the patent industry. Our distinguished panelists are as follows:
Michael D. Friedman is Managing Director at Ocean Tomo, overseeing its Investments practice, which is composed of Investment Banking, Asset Management and Investment Research.
Ocean Tomo’s Investment Banking practice brings IP financing, monetization and capital markets solutions to corporations and other intellectual property owners. Recent notable transactions include the leveraged buyout of Mosaid Technologies and the sale of MIPS Technologies’ IP portfolio. Ocean Tomo Asset Management, where Mr. Friedman serves as Chief Investment Officer, engages in public equity, special situations and private equity investing where intellectual property insight drives alpha creation. Investment Research works in parallel with institutional investors, hedge funds and private equity funds advising them on capital allocations to IP-themed investments.
Mr. Friedman holds a JD from the University of Chicago Law School, where he worked as Research Editor of the University of Chicago Legal Forum. He also holds a BS in marine engineering and nautical science from the U.S. Merchant Marine Academy. Mr. Friedman is a member of the board of directors of the Intellectual Property Exchange International, the world’s first IP-focused financial exchange, and a Lecturer in Law at the University of Chicago Law School.
Jay P. Kesan is a Professor at the University of Illinois, College of Lawwhere he is H. Ross & Helen Workman Research Scholar and Director of the Program in Intellectual Property and Technology Law. Professor Kesan received his J.D. summa cum laude from Georgetown University, where he received several awards including Order of the Coif and served as associate editor of the Georgetown Law Journal. After graduation, he clerked for Judge Patrick E. Higginbotham of the United States Court of Appeals for the Fifth Circuit. Prior to attending law school, Jay Kesan – who also holds a Ph.D. in electrical and computer engineering from the University of Texas at Austin – worked as a research scientist at the IBM T.J. Watson Research Center in New York. He is a registered patent attorney and practiced at the former firm of Pennie & Edmonds LLP in the areas of patent litigation and patent prosecution. In addition, he has published numerous scientific papers, and he has obtained several patents in the U.S. and abroad. His recent publications can be found on SSRN (Social Science Research Network) at http://www.ssrn.com. At the University of Illinois, Professor Kesan is appointed in the College of Law, the Institute of Genomic Biology, the Department of Electrical & Computer Engineering, the Information Trust Institute, the Coordinated Science Laboratory, the College of Business, and the Department of Agricultural & Consumer Economics. Professor Kesan continues to be professionally active in the areas of patent litigation and technology entrepreneurship. He has served as a special master in patent litigations, and has served as a technical and legal expert and/or counsel in patent matters. He also serves on the boards of directors/advisors of start-up technology companies.
Matthew Levy is Patent Counsel at the Computer and Communications Industry Association, where he handles legal, policy advocacy, and regulatory matters related to patents and is lead blogger for CCIA’s Patent Progress.
Matt joined the CCIA in 2013 from the IP boutique Cloudigy Law, PLLC. He has also been an associate at Finnegan, Henderson, Farabow, Garrett, & Dunner, LLP and at Hogan & Hartson LLP. He got first-hand experience in both patent prosecution and patent litigation, including defending clients against patent trolls.
Matt graduated from the Georgetown University Law Center magna cum laude with the Order of the Coif, winning the ABA/BNA Award for Excellence in Intellectual Property. He received a Master’s in Computer Science from the University of Kentucky, where he won the Presidential Fellowship twice. His research at UK was in computational complexity theory and artificial intelligence. He received a Bachelor’s degree in Computer Science from the University of Southern Maine.
Before law school, Matt was a software engineer at IBM in Lexington, KY, as part of the team that developed and maintained Lotus Sametime, IBM’s real-time messaging and conferencing product. He is co-inventor on U.S. Patent No. 8,521,830.
Matt is still a software developer in his spare time. He developed an app for the iPad, Federal Local Rules, which is available on the App Store.
Laura Beth Miller is a shareholder at Brinks Gilson & Lione, where she co-chairs the firm’s practice before the U.S. International Trade Commission (“ITC”). With over two decades of trial and arbitration experience, Ms. Miller has handled substantial first and second chair responsibilities. She focuses her practice on patent, trade secret and trademark issues, as well as client counseling on complex commercial issues, including licensing, anti-trust and contract issues affecting business operations, product services and technology. In addition to representing major Fortune 500 companies, she is an adjunct professor at The John Marshall Law School, in Chicago, Illinois. She is a frequent speaker on intellectual property issues both in the United States and abroad, and has written a number of articles on intellectual property topics.
Ms. Miller received her B.A. from the University of Virginia and her J.D. from The College of William and Mary Marshall Wythe School of Law. She is licensed to practice before the United States Supreme Court, the Supreme Court of Illinois, the United States Patent and Trademark Office, and numerous federal courts. She has been recognized as one of Illinois\u27 leading intellectual property lawyers by Chambers USA, and has been named a Leading Intellectual Property Lawyer and one of the Top 50 Women Business Litigation Lawyers in Illinois by the Leading Lawyers Network. She also serves on the management teams at Brinks Gilson & Lione.
K. McNeill Taylor, Jr., is General Counsel at Round Rock Research, LLC. Neill Taylor joined Round Rock in 2012 as Vice President Law and General Counsel responsible for supervising and administering all legal affairs for the company.
Before joining Round Rock, Neill was Corporate Vice President and Chief IP Counsel of Motorola Mobility, Inc., responsible for the intellectual property law and litigation functions. He managed the offensive and defensive patent litigation in support of MMI’s Android smart phones, and the preparation, prosecution and legal support for MMI’s patent portfolio of approximately 24,000 patents and applications worldwide. Neill had a leading role in setting the strategy for and negotiating MMI’s $12.5B acquisition agreement with Google in August 2011.
After joining Motorola, Inc. in 2002, Neill led its efforts to protect intellectual property for a number of Motorola businesses through patent operations, licensing, in-business counseling, defensive matters and litigation. He also served as general counsel in the integration of Symbol Technologies, Inc. after its $4B acquisition by Motorola in January 2007.
Prior to joining Motorola, Neill served as vice president, general counsel and assistant secretary of Corning Cable Systems and Siecor Corp. Before that he held patent counsel positions with Corning Inc. and Schlumberger Ltd., and began his patent law career as an associate with Fish & Neave, a patent litigation firm in New York.
Neill received his law degree from the University of Chicago and a bachelor’s degree in physics and philosophy from Duke University, where he graduated magna cum laude and was an Angier B. Duke scholar.
Andrew W. Williams is a partner with McDonnell Boehnen Hulbert & Berghoff LLP. Dr. Williams\u27 practice primarily consists of patent litigation, prosecution, and opinion work in the areas of biotechnology, pharmaceuticals, and chemistry. Dr. Williams is a contributing author to the Patent Docs weblog, a site focusing on biotechnology and pharmaceutical patent law. Dr. Williams earned his Ph.D. in Molecular Biophysics and Biochemistry at Yale University. He earned his J.D. from George Washington University Law School with highest honors, and was Managing Editor of the law review",Northwestern Pritzker School of Law Scholarly Commons,Patent Assertion and Non-Practicing Entities Panel,,,,core
101896791,01/04/2015,"Abstract. Even though intelligent agent has proven itself to be a promising branch of artificial intelligence (AI), its mobility capacity has yet been paid enough attention to match the pervasive trend of networks. This paper proposes to inject intelligence into mobile agent of current literature by introducing rule-driven mobile agent so as to maintain both intelligence and mobility of current agent. Particularly, this methodology is fully exemplified in the context of real-time IP network configuration through intelligent mobile agent based network management architecture, policy specification language and policy information model. A case study for inter-domain IP VPN configuration demonstrates the design and implementation of this management system based on the test-bed developed in the context of European Union IST Project CONTEXT. 1 Background and Rationale After years of recession, Artificial Intelligence (AI) regained it vitality relatively thanks to the inception of Intelligent Agent (IA). Agent was even highlighted as another approach of AI by S. Russell et al. [1]. Intelligent agent usually is a kind o",,Rule-driven Mobile Intelligent Agents for Realtime Configuration of IP Networks,,,,core
102702449,04/09/2015,"Abstract—In this paper, we introduce vehicle detection by in-dependent parts (VDIP) for urban driver assistance. In urban environments, vehicles appear in a variety of orientations, i.e., oncoming, preceding, and sideview. Additionally, partial vehicle occlusions are common at intersections, during entry and exit from the camera’s field of view, or due to scene clutter. VDIP provides a lightweight robust framework for detecting oncoming, preceding, sideview, and partially occluded vehicles in urban driving. In this paper, we use active learning to train independent-part detectors. A semisupervised approach is used for training part-matching classification, which forms sideview vehicles from independently detected parts. The hierarchical learning process yields VDIP, featuring efficient evaluation and robust performance. Parts and vehicles are tracked using Kalman filtering. The fully implemented system is lightweight and runs in real time. Extensive quantitative analysis on real-world on-road data sets is provided. Index Terms—Active learning, active safety, computer vision, detection by parts, machine learning, occlusions, vehicle detection. I",,Vehicle Detection by Independent Parts for Urban Driver Assistance,,,,core
226687115,2015-01-01T00:00:00,"In smart-cities, computer vision has the potential to dramatically improve the quality of life of people suffering of visual impairments. In this field, we have been working on a wearable mobility aid aimed at detecting in real-time obstacles in front of a visually impaired. Our approach relies on a custom RGBD camera, with FPGA on-board processing, worn as traditional eyeglasses and effective point-cloud processing implemented on a compact and lightweight embedded computer. This latter device also provides feedback to the user by means of an haptic interface as well as audio messages. In this paper we address crosswalk recognition that, as pointed out by several visually impaired users involved in the evaluation of our system, is a crucial requirement in the design of an effective mobility aid. Specifically, we propose a reliable methodology to detect and categorize crosswalks by leveraging on point-cloud processing and deep-learning techniques. The experimental results reported, on 10000+ frames, confirm that the proposed approach is invariant to head/camera pose and extremely effective even when dealing with large occlusions typically found in urban environments",'Springer Science and Business Media LLC',Crosswalk Recognition Through Point-Cloud Processing and Deep-Learning Suited to a Wearable Mobility Aid for the Visually Impaired,,10.1007/978-3-319-23222-5_35,,core
13631844,2012-01-01T08:00:00,"The Internet is an example of a successful and scalable decentralized system capable of connecting millions of systems and transporting data seamlessly between them. It has been so successful that today, it is impossible to imagine entertainment, education, communication, business and other services without the Internet. In fact, the Internet is widely considered to be just another utility service. Additionally, the diversity of the end-systems ranging from high-end servers to mobile phones and sensors is only adding to the rate of its growth and value. This success is largely the result of careful thought put into the design philosophy of the Internet (globally deployed network layer, isolation of protocol layers). This has resulted in a digital information explosion with recent studies predicting a ten fold increase in the amount of digital content over the next five years. Factors such as information replication, increasingly affordable and heterogeneous end-systems, cheap storage and numerous services are cited to be a few of the reasons for this rapid growth. However, this fixed network layer also poses a barrier to introduction of innovations to support increasingly diverse end-systems and new communication paradigms. Moreover, the inherent issues in security, mobility, performance and reliability cannot be completely resolved by merely changing functionality in the end-systems, and will require addition of functionality in the core of the network as well. Service-centric networking is a new paradigm that seeks to introduce functionality into the network by deploying customized in-network services on-demand. Different compositions of services are used to customize connections to satisfy various user communication requirements. This work addresses four challenges in the context of service-centric networks: (1) automated service composition (2) combined service composition and routing, (3) support for inter-domain data-plane policies in such networks, and (4) end-system support for services through abstractions. Automated service composition deals with the challenge of finding an optimal sequence of services to satisfy communication requirements of a connection. This composed sequence of services is applied to the connection in the data-path. A semantic tree is used to describe communication characteristics and the problem is solved by reducing it to a planning problem. Service composition is typically followed by “service routing”, where the connection is set up such that the services are applied in order. This is not always optimal as we show through experiments. Combined service composition and routing tries to solve both problems in a single stage by reducing it to a planning problem. We further explore the issues of inter-domain data-plane policies in next-generation networks and discuss a system that uses the semantic tree to specify such policies. The system translates these policies into planning rules and determines the right way to set up the connection such that all policies are met. We also discuss the design and implementation of a novel “service socket” API that allows end-system applications to access services in a service-centric networking context. Another key aspect of next-generation networking is virtualization of the physical network infrastructure. Network virtualization allows multiple networks with different protocol stacks to share the same physical infrastructure. A key problem for virtual network providers is the need to efficiently allocate their customers\u27 virtual network requests to the underlying network infrastructure. This problem is known to be computationally intractable and heuristic solutions continue to be developed. Most existing heuristics use a two-stage approach in which virtual nodes are first placed on physical nodes and virtual links are subsequently mapped. We present VHub, a novel single-stage approach that formulates this problem as a p-hub median problem. Our results show that VHub outperforms the state of the art algorithms by mapping 23% more virtual networks in lesser time (26% to 96%). Overall, this dissertation discusses techniques through which data-path customization can be achieved in next-generation networks. To solve some of the technical challenges, this work follows a cross-disciplinary approach exploring ideas from computer networking, distributed systems and algorithms to graph theory, mathematical optimization and artificial intelligence. The solutions are also tested through simulations using real and synthetically generated workloads to validate the design",ScholarWorks@UMass Amherst,On data-path customization in next-generation networks,,,,core
4765372,2012-01-19T00:00:00,"Tropospheric aerosol information from NASA satellites in space has reached the milestone of ten years of continuous measurements. These higher resolution satellite aerosol records allow for a broader regional perspective than can be gained using only sparsely located ground based monitoring sites. Decadal satellite aerosol data have the potential to advance knowledge of the climatic impacts of aerosols through better understanding of solar dimming/brightening and radiative forcings on regional scales, as well as aid in air quality applications. The goal of this thesis is to develop and implement methodologies for using satellite remotely sensed data in conjunction with ground based observations and modeling for characterization of regional aerosol variations with applications to air quality and climate studies in the Southeastern U. S. This region is of special interest because of distinct aerosol types, less warming climate trends compared to the rest of U.S., and growing population.
To support this primary goal, a technique is developed that exploits the statistical relationship between PM2.5 (particulate matter that has an aerodynamic radius of 2.5 µm or less) and satellite AOD (Aerosol Optical Depth) from MODIS (Moderate resolution Imaging Spectroradiometer) where a probabilistic approach is used for air quality assessments in the metropolitan Atlanta area. The metropolitan Atlanta area experiences the poorest air quality during the warmer seasons. We found that satellite AODs capture a significant portion of PM2.5 concentration variability during the warmer months of the year with correlation values above 0.5 for a majority of co-located (in time and space) ground based PM2.5 monitors, which is significant at the 95% confidence interval. The developed probabilistic approach uses five years of satellite AOD, PM2.5 and their related AQI (Air Quality Index) to predict future AQI based solely on AOD retrievals through the use of AOD thresholds, e.g., 80% of Code Green AQI days have AOD below 0.3. This approach has broad applicability for concerned stakeholders in that it allows for quick dissemination of pertinent air quality data in near-real time around a satellite overpass.
Examination of the use of multiple satellite sensors to aid in investigating the impacts of biomass burning in the region is performed. The utility of data fusion is evaluated in understanding the effects of the large wildfire that burned in May 2007. 
This wildfire caused PM2.5 in the metropolitan Atlanta area to exceed healthy levels with some measurements surpassing 150 µg/m3 during the month. OMI (Ozone Monitoring Instrument) AI (Aerosol Index), which qualitatively measures absorbing aerosols, have high values of more than 1.5 during May 26 - 31, 2007. CALIPSO (Cloud-Aerosol Lidar and Infrared Pathfinder Satellite Observations) a space based lidar was used to determine the vertical structure of the atmosphere across the region during the active fire period. CALIPSO was able to identify wildfire aerosols both within the planetary boundary layer (likely affects local air quality) and aloft where aerosol transport occurs. This has important implications for climatic studies specifically aerosol radiative effects.
In-depth analysis of the satellite and ground based aerosol data records over the past decade (2000 - 2009) are performed from a climatic perspective. The long temporal scale allowed for better characterization of seasonality, interannual variability, and trends. Spatial analysis of ten years of AOD from both MODIS and MISR (Multi-angle Imaging Spectroradiometer) showed little variability of AOD during the winter with mean AOD below 0.1 for the entire region, while the summer had decidedly more variability with mean AOD around 0.33 for MODIS and 0.3 for MISR. Seasonal analysis of the PM2.5 revealed that summer means are twice as high as winter means for PM2.5. All of the datasets show interannual variability that suggests with time AOD and PM2.5 are decreasing, but seasonal variability obscured the detection of any appreciable trends in AOD; however, once the seasonal influence was removed through the creation of monthly anomalies there were decreasing trends in AOD, but only MODIS had a trend of -0.00434 (per month) that statistically significant at the 95% confidence level. 
Satellite and ground-based data are used to assess the radiative impacts of aerosols in the region. The regional TOA (Top Of the Atmosphere) direct radiative forcing is estimated by utilizing satellite AOD from MODIS and MISR both on Terra, along with satellite derived cloud fraction, surface albedo (both from MODIS), and single scattering albedo (SSA) from MISR data from 2000 - 2009. Estimated TOA forcing varied from between -6 to -3 W/m2 during the winter, and during the warmer months there is more variation with ΔF varying between -28 to -12.6 W/m2 for MODIS and -26 to -11 W/m2 for MISR. The results suggest that when AOD, cloud fraction and surface albedo are all consider they add an additional 6 W/m2 of TOA forcing compared to TOA forcing due to aerosol effects only. Varying SSA can create changes in TOA forcing of about 5 W/m2. With removal of the seasonal variability timeseries anomaly trend analysis revealed that estimated TOA forcing is decreasing (becoming less negative) with MODIS based estimates statistically significant at the 95% confidence level. 
Optical and radiative 1-D radiative transfer modeling is performed to assess the daily mean TOA forcing and forcing at the surface for representative urban and background aerosol mixtures for summer and winter. During the winter, modeled TOA forcing is -2.8 and -5 W/m2 for the WB and WU cases, and the modeled summer TOA forcings (SB = -13.3 W/m2) also generally agree with earlier estimates. While surface forcings varied from -3 to -210 W/m2. The radiative forcing efficiency at the TOA (amount of forcing per unit of AOD at 550 nm) varied from -9 to -72 W/m2 τ-1, and RFE at the surface varied from -50 to -410 W/m2 τ-1. It was found that the forcing efficiency for biomass burning aerosols are similar to the forcing efficiency of background aerosols during the summer that highlights the importance of possible increased biomass burning activity. Ultimately, the methodologies developed in this work can be implemented by the remote sensing community and have direct applicability for society as a whole.PhDCommittee Chair: Sokolik, Irina; Committee Member: Bergin, Michael; Committee Member: Curry, Judith; Committee Member: Doddridge, Bruce; Committee Member: Tatarskii, Viatchesla",Georgia Institute of Technology,"Aerosol characterization in the  Southeastern U. S. using
satellite data for applications to air quality and climate",https://core.ac.uk/download/4765372.pdf,,,core
30788723,2012-10-01T00:00:00,"Road traffic has a heavy impact on the urban sound environment, constituting the main source of noise and widely dominating its spectral composition. In this context, our research investigates the use of recorded sound spectra as input data for the development of real-time short-term road traffic flow estimation models. For this, a series of models based on the use of Multilayer Perceptron Neural Networks, multiple linear regression, and the Fisher linear discriminant were implemented to estimate road traffic flow as well as to classify it according to the composition of heavy vehicles and motorcycles/mopeds. In view of the results, the use of the 50–400 Hz and 1–2.5 kHz frequency ranges as input variables in multilayer perceptron-based models successfully estimated urban road traffic flow with an average percentage of explained variance equal to 86%, while the classification of the urban road traffic flow gave an average success rate of 96.1%",'Elsevier BV',Using recorded sound spectra profile as input data for real-time short-term urban road-traffic-flow estimation,,10.1016/j.scitotenv.2012.07.014,,core
293519777,2013-01-01T00:00:00,"O problema de mobilidade urbana tem crescido a níveis cada vez mais desgastantes para a população das cidades. Acompanhamos diariamente notícias sobre: os enormes engarrafamentos, a falta de transporte público eficiente e de qualidade, o desrespeito as regras de trânsito, os acidentes e as mortes desnecessárias. Nem todos os problemas podem ser resolvidos apenas com a melhoria da educação no trânsito, alguns requerem altos investimentos, como na expansão e duplicação de vias, construção de viadutos e pontes, e que permitem suprir a demanda cada vez maior de veículos. Com menor custo de implantação podemos incluir a sinalização de trânsito e semafórica, que ajuda no controle das vias e aumenta a segurança para as pessoas. Neste trabalho é estudado um sistema de controle de trânsito simulado com a perspectiva de melhorar o sistema semáforico real. Visto que desta forma é possível causar um grande impacto aos sistemas de transporte em geral com um baixo custo de implantação quando comparado as modificações na infraestrutura das vias. Serão aplicados três algoritmos de inteligência artificial ao sistema semafórico experimental, de forma a torná-lo adaptativo, e assim permitir uma melhoria do fluxo de veículos através de um sistema que não necessita de um controle centralizado. Os resultados serão apresentados através da comparação dos algoritmos utilizados, levando-se em conta questões como tempo de simulação e tempo de viagem dos veículos.The problem of urban mobility has grown to increasingly stressful levels for the cities population. We see daily reports about: huge traffic jams, the lack of a quality and efficient public transportation, the disregard of the traffic rules, the accidents and the unnecessary deaths. All problems can not be solved only with the improvement of traffic education, some require high investments, such as the expansion and duplication of streets, construction of viaducts and bridges, which allow to meet the increasing demand of vehicles. With lower cost of deployment we can include traffic signs and traffic lights, which helps in streets control and increases the people safety. In this work is presented a study about a simulated traffic control system with the prospect of improving the actual traffic system. In this way is possible to generate a major impact on transport systems, in general with a low implementation cost if compared with changes in the infrastructure of roads. Will be applied three artificial intelligence algorithms to an experimental traffic lights system, in order to make it adaptive, and thus allow an improvement in the traffic flow through a system which does not require a centralized control. The results are presented through a comparison of the algorithms used, taking into account issues such as simulation time and vehicles travel time",,"Adaptive traffic signals, a swarm-based approach for the urban mobility",,,,core
250625828,2015-02-01T00:00:00,"The prediction of environmental noise in urban environments requires the solution of a complex and non-linear

problem, since there are complex relationships among themultitude of variables involved in the characterization

andmodelling of environmental noise and environmental-noisemagnitudes.Moreover, the inclusion of the great

spatial heterogeneity characteristic of urban environments seems to be essential in order to achieve an accurate

environmental-noise prediction in cities. This problem is addressed in this paper, where a procedure based on

feature-selection techniques and machine-learning regression methods is proposed and applied to this environmental

problem. Threemachine-learning regression methods, which are considered very robust in solving nonlinear

problems, are used to estimate the energy-equivalent sound-pressure level descriptor (LAeq). These three

methods are: (i) multilayer perceptron (MLP), (ii) sequential minimal optimisation (SMO), and (iii) Gaussian

processes for regression (GPR). In addition, because of the high number of input variables involved in

environmental-noise modelling and estimation in urban environments, which make LAeq prediction models

quite complex and costly in terms of time and resources for application to real situations, three different

techniques are used to approach feature selection or data reduction. The feature-selection techniques used are:

(i) correlation-based feature-subset selection (CFS), (ii) wrapper for feature-subset selection (WFS), and the

data reduction technique is principal-component analysis (PCA). The subsequent analysis leads to a proposal

of different schemes, depending on the needs regarding data collection and accuracy. The use of WFS as the

feature-selection technique with the implementation of SMO or GPR as regression algorithm provides the best

LAeq estimation (R2 = 0.94 and mean absolute error (MAE) = 1.14–1.16 dB(A))",'Elsevier BV',A general procedure to generate models for urban environmental-noise pollution using feature selection and machine learning methods,,10.1016/j.scitotenv.2014.08.060,,core
53344724,2014-01-01T00:00:00,"Computer generated images have always drawn the attention of millions of people in their different roles: students, professors, researchers, designers, movie directors, etc. It is not only the magic of generating synthetic, static and/or dynamic images, emulations or simulations of reality, but of capturing all the creative abilities of the human being to narrate events, from the memory of the past, descriptions of current reality and projections towards the future. In other words, they may be timeless images and malleable towards the infinite.

In them rests the freedom of expression in a chromatic and three-dimensional way where sometimes it is difficult to see whether we are in the face of something real or virtual. It is the charm of the viewing power of communication where an image is always worth more than a thousand words. Not for nothing when the first graphics appeared to depict the numerical information in the offices, it marked the beginning of the use of computers in the businesses with professional purposes until they reached the home. It was these synthetic images which collaborated to change the meaning of the PC initials, that is, from professional computer to personal computer.

The risk lies in the fact that those who manage those images respect the final viewer, at the moment they are presented to his/her eyes. That is, a transparent communication avoiding manipulation through them. If this doesn\u2019t happen, the written word takes again a prevailing place in the communicative process, and one goes back to the time of the appearance of print. It is in this regressive period where the human and social factors may be inserted, which generate different types of structures which do not change over time.

Although we are in one of the most dynamic sectors of the current interactive systems, we may come across in some places of the Old Continent with dogmas and their supporters, who slow down the whole creative process which entails working with these computer-made images. Now if the human and social factors are positive, in view of the constant advances of the graphic hardware and software, the only limit that exists is the imagination or originality at the moment of generating static and/or dynamic images in 2D and 3D.

Computer graphics and computer animation along with text, audio and video have been key in the momentum of the Internet phenomenon of the 20th century. In the new millennium they have been the main means of drawing the attention of the users to the screens of a great variety of devices of classical computing.

There are millions of users who decide to purchase computer equipment because of the images that they can see on the screens. These images gain in quality thanks to their generation from scratch and thanks also to myriad techniques, methods, algorithms, etc. related to computer graphics down to the access to the information stored in the database, such as decompressed files with multimedia information available via the interaction of the user through the various peripherals.

Virtual images are generated in 2D and/or 3D, thanks to democratization of the pixels in 1990 \u2013 2000. Users are now capable of generating images in movement for 3D starting from photographs with digital cameras or the commercial applications to generate films with computer animations.

In this whole productive process, whether it is at a personal or an industrial level the cost factor is always present, in the equation of maximum quality in the least possible time. To reach that goal it is necessary to resort to several quality attributes that have an influence from the design stage right until the programming of the applications used in the context of graphic informatics.

Our intention in the current handbook is to show an essential part of those strategies and also the latest breakthroughs in all that which is directly or indirectly related to computer graphics, computer animation, database, software quality, hypermedia, design, communicability, interfaces, cloud computing, augmented reality, human-computer interaction, computer-aided design, mixed reality, models, techniques and methods of computer science, etc.

Some of the works that make up the current compendium have been presented orally by their authors in the following international conference in Venice, Italy: SETECEC 2012 \u2013First International Conference on Software and Emerging Technologies for Education, Culture, Entertainment, and Commerce: New Directions in Multimedia Mobile Computing, Social Networks, Human-Computer Interaction and Communicability, and international symposium in Valle d'Aosta, Italy: CCGIDIS 2012 \u2013Second International Symposium on Communicability, Computer Graphics and Innovative Design for Interactive Systems.

The innovation and originality of those proposals has been the reason for which the authors have been invited to enlarge their works submitting again the new versions to an assessment process of said works by the members of the scientific committee. Consequently, these are works that have gone satisfactorily through a double process of international selection. In the next section a short introductory presentation of the research works that make up the current compendium is given:

The authors Luigi Barazzetti and  Marco Scaioni present an interesting work \u201cTransforming Images and Laser Scans into 3D Models\u201d where theory and practice converge in the context of 3D images. They introduce all details of the use of a laser scanner to obtain high quality three-dimensional images and accuracy in the maintenance and the virtual reconstruction of cultural heritage, for instance. Besides, they comprehensively describe computer vision and photogrammetric 3D modeling techniques that are mainly based on images or laser scans. The description further covers the integration of global navigation satellite system (GNSS) or theodolite data allowing precise geo-referencing. The main goal they have set themselves is to indicate the reliability and robustness of their combined use for real surveys. Simultaneously a set of real examples serves to illustrate each one of the advantages and disadvantages of the used methods and techniques. In these examples the resolution of complex problems and with reduced costs can be verified. Each one of the issues is approached in a didactic way which facilitates the understanding even of the diverse devices used for the measurements.

Chih-Fang Huang, Chih-Hsiang Liang, and En-Ju Lin are the authors of the chapter \u201cSound-Color Synaesthetic Effect Using Algorithmic Composition with Image for Emotional Release.\u201d In said research they present the qualities of music from an emotive point of view. This is one of the motivations why they spread their study to the field of colors. In this sense, they develop an emotional model for music based on studies of color theory. Another main goal is to present the emotions of the users of a special software starting from the color and the electronic sounds that these represent from the psychological point of view, an approach informed by studies belonging to the set of the music color synesthesis and emotion releasing effect. Throughout the research work the authors explain in a gradual, simple and thorough way each one of the presented concepts with their matching theoretical examples and also the use of a special software for the experiments they have carried out.

The research work \u201cUser-created Interior Design Service Concepts, Interfaces and Outlines for Augmented Reality\u201d has been made by the following authors Tiina Kym\ue4l\ue4inen and Sanni Siltanen. This research work presents us masterfully a design process and the implementation requirements of an interactive interior design system. In said system the use of two focus groups can be detected where the knowledge and the experiences of designers, bloggers and serious amateurs in the field of interior design interact transversally. In that transversal interaction of diverse professionals the presence of user-driven innovation can be seen. The strategies of the conformation of the professionals and the transversal knowledge to the different areas of knowledge are explained with examples. Examples are given, which describe all the aspects of the new technologies applied to augmented reality, the results of tests with real users, the realization of a special software for the three-dimensional visualization of interiors, etc. Each one of the stages of the research project is accompanied by its corresponding textual and graphical explanations. Finally the authors make a wide-ranging reflection about the learned lessons signaling the positive and negative aspects of the used technology and also the future lines of research before their conclusions.

In the context of artificial intelligence, Damijan Novak and Domen Verber show us in their research \u201cNew Generation of Artificial Intelligence for Real-time Strategy Games\u201d the importance of applying it to computer games. The text starts with a complete state of the art overview, where the main notions to which the current research work refers are defined. In it is a constant interrelation among real-time strategy, games and artificial intelligence. The authors signal the importance of these issues within the academic field since they have detected new fields for research, whether in the present or the immediate future. Besides, in this work an excellent comparison example in the open-source real time strategy game development tools can be seen. The main and secondary goals of the text are gradually developed answering to a set of rhetorical questions with practical examples. All of this makes the reading and understanding easier to the potential readers. Their proposal of an enhanced combat artificial intelligence algorithm is striking in a very positive way.

The authors of the chapter \u201cTowards Smart Mobile System for Public Bus Transportation\u201d are Mitja Krajnc, Vili Podgorelec, and Marjan Heri\u10dko. In it converge the use of multimedia mobile phones and the geographical information systems (GPS satellite) for the localization of public buses in the town of Maribor, Slovenia. The system allows the user to see the movement of the buses in the road network through a real-time visualization on a mobile phone. The digital information which replaces the analogical one such as the timetables of the buses on paper support increases the available options of those users to access the public transportation system such as the combination of buses to reach a given destination. Besides, ease of use of the system can be seen since it is based on Windows Phone. Finally a quality attribute of the interactive systems such as the prediction has been incorporated to the system. The goal pursued by its authors is to increase the alternatives the user has at the moment of accessing the interactive information without this having negative repercussions on the visualization of the information inside the interface of the mobile phone.

Under the title \u201cA Learning Environment based on Movement and Sound Interaction\u201d its authors Serena Zanolla, Sergio Canazza, Antonio Rod\ue0, Giovanni De Poli, and Gian Luca Foresti present an Interactive Multimodal Environment for E-learning called \u201cStanza Logo-Motoria.\u201d The experiments carried out with children for the learning of a second language such as English have allowed the authors to carry out several experiments with positive results. A special module has been developed and tested in this sense called English as a Second Language (ESL). The motivation for learning languages in an environment of these characteristics is presented in a detailed way with a special stress on other aspects such as the socialization of early age users through new interactive technologies. Along with this the authors introduce technological aspects of interactive design which have been evolving as the experiments were carried out.

Michele Argiolas, Claudia Loggia, Vittorio Tramontin, and Cristina Trois are the authors of the chapter called \u201cA Web Application to Support Decision Making Process Based on a Bottom-up Approach in Public Buildings\u2019 Green Retrofit.\u201d They introduce a GIS web application related to such topics as green retrofit, public buildings, cost management just to mention three examples. The proposal starts with a comprehensive state of the art in legislation issues as technique of the main and secondary topics approached by its authors. The work shows how the implemented system online may serve to curtail the costs of management of green retrofitting public buildings. The study shows a constant interrelation among several disciplines of the sciences. Each of the issues is very well referenced in the bibliography section and they are developed in a pleasant way for the reader non-specialized in the issues approached at the start. Besides, there is a comparative study of real cases which entail additional advantages to the work developed by each one of the authors of the current text. Finally we can state that we are in front of an excellent triadic example of interrelations among databases, access to the stored information and the management of public administration.

The wide range of the phenomenon of the social networks requires in many of the offered services a set of recommendations. In this sense the authors Sa\u161o Karakati\u10d, Vili Podgorelec, and Marjan Heri\u10dko show in their research work \u201cMerging Social Networks and Semantic Databases to Build Recommendation Service\u201d the potential of linking semantic databases and social networks. Starting from Facebook and Freebase they have fashioned a prototype that uses Java technology. The prototype takes into consideration each one of the key elements for the recommendation of an online service to the potential users of contents in TV format, videogames, music, etc. In this sense we see a novel low cost solution is little by little brought forward by the authors along the pages. Besides, they have made a series of tests with users at the moment of interacting with the software and the hardware used. The results obtained back up the transcendence of the current proposal, and also the future developments they will make in a short time.

The three-dimensional reconstruction of the religious-historical heritage in the province of Avila (Spain) is the main objective of \u201cComputer Graphics to Encourage the Participation in the Virtual Reconstruction of Cultural Heritage: The Example of the Monastery of  Nuestra Se\uf1ora del Risco\u201d author of the chapter whose title is Gonzalo Mart\uedn S\ue1nchez. Starting from a reality which reveals the passing of time and the scarce remains of the walls of the monastery, the author resorts to several techniques using static images in 2D and 3D, such as historical cartography, digital photography, commercial software and open source, etc., for three-dimensional reconstruction. Now the author points out that these techniques from the field of computer graphics must be accompanied by a previous historical study of that is intended to be rebuilt. In this sense the illustrations, as well as the used methodology can serve as a guide for other analogous works.

In \u201cDeveloping Design Guidelines for E-Learning Environments: A War Story\u201d its authors Laura Benvenuti, Maria Menendez Blanco, and Gerrit C. van der Veer, denote the importance of the design, development, evaluation of a system oriented at university e-learning. A comprehensive study of the educational system to be developed with Moodle highlights the intersection between pedagogical theory, interactive design computer programming, among other areas of knowledge when it refers to blended academic education. A course on webculture has allowed the authors to carry out analyses of the web design. The university students belonging to the social sciences (culture, psychology, etc.) have taken part in several processes of heuristic evaluation to reach the goals proposed by the authors of the current research work. The learned lessons, future lines of research and the conclusions are interesting sections to understand the acquired experiences and for new research in the context of the blended education, distance learning, Internet based learning environments to mention a few examples.

The author presents a heuristic study of the evolution of human factors with their educational and social consequences in the context of the pixel of Southern Europe during the years 1990 \u2013 2012, under the title \u201cPixels: Educational Structures and Power Systems.\u201d Simultaneously, it is used for the first time a special language where diachronism interacts with the synchronism of the real examples that are presented, related to the graphics software. Besides, in this first research work are combined different techniques of the social sciences to establish the nodes and links which make up the human and functional structure of all that related to the commercial and educational pixel. Lastly, a series of strategies is put forward to analyse the educational quality of some training centers with regard to computer graphics and its derivations",place:Bergamo,"A Learning Environment
Based on Movement and Sound Interaction",,,,core
99993417,2012,"Abstract—This paper deals with the implementation details and results of simulating a city populated by a large number of pedestrians. The goal of the simulation was to, as realistically as possible, simulate large numbers of people going about their daily lives, interacting with each other and the city environment, in real-time. We also simulated dense crowds and realistic collision avoidance techniques, and tried to replicate some observations of previous studies. Simulators based on the mechanics of human interaction can easily become inconveniently complex and/or resource expensive. As this has been the main risk during the project we’ve been careful in the implementation to keep coupling as low as possible and to construct interfaces that allow for scaling and adding of new behaviour in “isolation”, without having to modify prior code. The concern for performance was just as real – the simulation, after all, was to support thousands of interacting pedestrians walking about in real-time. In the end, the resulting simulation turned out to be a good and efficient representation of inner-city pedestrians, and was mostly fine in handling the issues of denser crowds. This may potentially be extended for use in city and public transport planning, producing large amounts of data for data mining or as a basis for further development into city life dynamics and the artificial intelligence of individuals in a populated environment",,1 Large-Scale Agent-based Pedestrian and Crowd Simulation in Real-Time,,,,core
226063288,2015,"In lab-on-chip for food analysis, dealing with large samples with complex composition (i.e. the matrix) and target analytes potentially in very low concentrations is one of major issues that needs to be tackled. In order to increase the analytical accuracy of a method, it is often required to process the sample before quantification of the chemical specie under examination. In milk case, the presence of fat, proteins and many other components in a complex phase equilibrium poses particular challenges for analysis. Caseins, in particular, are known as blocking agent for surfaces, and therefore may interact with sensor surfaces. Many methods are available for microfluidic processing [1], but they are usually developed for small samples (often blood). Scaling up processing rate to deal with 1-100ml samples in minutes is not practical in most cases. In milk, casein is usually found in micelles the size of about 50-300nm [2], formed by hydrophobic caseins complexed by calcium ions and enclosed by partially hydrophilic k-casein. Since isoelectric point of casein is 4.6, at milk pH there is an intrinsic negative charge, which can be used to separate it from the sample. Other options include size and density separation in microfluidic structure as for instance acoustic separation.

In this paper, we present the development of a high-throughput electrophoretic separation system for milk proteins allowing the separation of casein at flow rates in the range of mls/min, by exploiting the intrinsic charge of casein at milk pH (i.e. 6.6). The device is intended as a step for sample preparation to increase the accuracy of detection of contaminants in milk samples. Design, estimated performances and implementation of the device for the experimental evaluation of the approach are reported. We preliminary focused on BSA, lactoglobulins and caseins as model molecules. An electrophoretic mobility of -1.39µm cm V-1 s-1 was measured for casein in conditions as close as possible to real sample (typical ionic strength of milk 0.08M, pH 6.6). The range of flow rate suitable for separation can be estimated by the velocity of electrophoretic drag and flow velocity. Modelling of device provided the evaluation of optimal working conditions and design (potential and flow rate); with a maximum applied potential of 1.4V, the expected fractionation of proteins between the two outlets is reported in Fig. 1, where 0.5 is equivalent to no separation.

The device was implemented with platinum electrodes lithographed on silica glass wafers with semi-transparent grid pattern for inspection. In order to increase the throughput, we designed a separation chamber with large cross section (Fig. 2), where electrodes are facing on top and bottom walls of a chamber with width 4mm and channel height 510µm. Electrodes are 28 mm long. Electric field applied drags charged particle toward the upper or lower walls depending on charge. The fluidic chamber was fabricated by 3D printing in PVC and assembled using two shells machined to house the electrode chips (Fig. 3). Experimental demonstration and optimization of the separation procedure is ongoing",,Evaluation of microfluidic sample preparation methods for the analysis of milk contaminants,,,,core
27024496,2012-09-01T00:00:00Z,"Traffic accidents are still one of the main health problems in the World. A number of measures have been applied in order to reduce the number of injuries and fatalities in roads, &lt;em&gt;i.e.&lt;/em&gt;, implementation of Advanced Driver Assistance Systems (ADAS) based on image processing. In this paper, a real time speed supervisor based on road sign recognition that can work both in urban and non-urban environments is presented. The system is able to recognize 135 road signs, belonging to the danger, yield, prohibition obligation and indication types, and sends warning messages to the driver upon the combination of two pieces of information: the current speed of the car and the road sign symbol. The core of this paper is the comparison between the two main methods which have been traditionally used for detection and recognition of road signs: template matching (TM) and neural networks (NN). The advantages and disadvantages of the two approaches will be shown and commented. Additionally we will show how the use of well-known algorithms to avoid illumination issues reduces the amount of images needed to train a neural network",MDPI AG,Recognition Stage for a Speed Supervisor Based on Road Sign Detection,,10.3390/s120912153,"[{'title': None, 'identifiers': ['issn:1424-8220', '1424-8220']}]",core
71618851,2014,"Climate change and environmental monitoring and management have received much attention recently, and an integrated information system (IIS) is considered highly valuable. This paper introduces a novel IIS that combines Internet of Things (IoT), Cloud Computing, Geoinformatics [remote sensing (RS), geographical information system (GIS), and global positioning system (GPS)], and e-Science for environmental monitoring and management, with a case study on regional climate change and its ecological effects. Multi-sensors and web services were used to collect data and other information for the perception layer; both public networks and private networks were used to access and transport mass data and other information in the network layer. The key technologies and tools include real-time operational database (RODB); extraction-transformation-loading (ETL); on-line analytical processing (OLAP) and relational OLAP (ROLAP); naming, addressing, and profile server (NAPS); application gateway (AG); application software for different platforms and tasks (APPs); IoT application infrastructure (IoT-AI); GIS and e-Science platforms; and representational state transfer/Java database connectivity (RESTful/JDBC). Application Program Interfaces (APIs) were implemented in the middleware layer of the IIS. The application layer provides the functions of storing, organizing, processing, and sharing of data and other information, as well as the functions of applications in environmental monitoring and management. The results from the case study show that there is a visible increasing trend of the air temperature in Xinjiang over the last 50 years 1962-2011) and an apparent increasing trend of the precipitation since the early 1980s. Furthermore, from the correlation between ecological indicators [gross primary production (GPP), net primary production (NPP), and leaf area index (LAI)] and meteorological elements (air temperature and precipitation), water resource availability is the decisive factor with regard to the terrestrial ecosystem in the area. The study shows that the research work is greatly benefited from such an IIS, not only in data collection supported by IoT, but also in Web services and applications based on cloud computing and e-Science platforms, and the effectiveness of monitoring processes and decision-making can be obviously improved. This paper provides a prototype IIS for environmental monitoring and management, and it also provides a new paradigm for the future research and practice; especially in the era of big data and IoT",,An Integrated System for Regional Environmental Monitoring and Management Based on Internet of Things,,10.1109/tii.2014.2302638,,core
103157672,03/11/2015,"Abstract—One of the important challenges in Real Time Strategy games is to construct GameAI. Using classic GameAI techniques for this game genre is not possible because of large amount of complexity and magnitude of these games ’ worlds. There are large numbers of areas in RTS games which need to be controlled by an AI. One of the main components of a NPC in RTS games is attack tactic controller. In this paper, we will suggest a system for controlling the army during the invasion to an enemy city. The suggested system includes two main modules: Attack Tactic Manager and Attack Tactic Designer. ATM determines preferable times for a change in routine of the army operation. In such times, ATD makes appropriate decisions for any of the defined circumstances of the game. An implementation of the purposed system is achieved and tested using StarCraft game. The results of the experiments illustrate optimized performance and intelligent behavior during attacks",,Intelligent Decision Making Process for an Attack in,,,,core
152470566,2013-02-20T11:31:57Z,"<p>(<b>A-E</b>), (<b>A</b>) Normal or Nod2 knockdown macrophages were treated as indicated. Culture supernatants were collected after 12 hours and the level of secreted IL1β was measured by ELISA. Each well contained 1×10<sup>6</sup> macrophages. Statistical significance was checked by one way ANOVA. P value was found to be significant (<0.0001). (<b>B</b>) Real time RT-PCR analysis of IL1β mRNA. Normal or Nod2 knockdown Macrophages were treated as indicated. Total RNA was isolated used for checking IL1β transcripts. Each bar represents fold expression relative to untreated macrophages. Statistical significance was checked by one way ANOVA. P value was found to be significant (<0.0001). (<b>C</b>) Western blot analysis of pre-IL1β. Peritoneal macrophages were treated as indicated. Cell extracts were collected after 4 hours and analysed for the presence of pre-IL1β. ATP stimulation (1 mM) was given for 20 minutes after primary stimulation (MDP or PGN) was over. Lane1: untreated macrophages (123); lane2: MDP treated macrophages (127); lane3: PGN treated macrophages (140); lane4: ATP stimulated macrophages (124); lane5: MDP+PGN treated macrophages (132); lane6: Macrophages treated with MDP followed by ATP stimulation (108). Values in brackets indicate average integrated density values of spot densitometric analysis using software Alpha Imager (<b>D</b>) Electrophoretic mobility shift Assay for NF-κB. Normal or TLR2 knockdown macrophages or macrophages pre-treated (for 20 minutes) with TLR2 specific blocking antibody (1 µg/ml) were stimulated as indicated. Nuclear extracts were prepared and incubated with 100 fmol of biotinylated NF-κB binding sequence. EMSA was performed as described. Lane1: untreated macrophages; lane2: MDP treated macrophages; lane3: PGN treated macrophages; lane4: MDP+PGN treated macrophages; lane5: PGN treated TLR2 knockdown macrophages; lane6: Macrophages treated with TLR2 blocking antibody for 20 min and then stimulated with PGN; lane7: Nuclear extract of PGN stimulated macrophages is incubated with 100 fmol biotinylated NF-κB binding DNA and 100 pmol of unlabelled NF-κB binding DNA. Macrophages were treated for 40 minutes. (<b>E</b>) Electrophoretic mobility shift Assay for AP1. Macrophages were treated for 40 minutes as indicated. Nuclear extracts were prepared and EMSA was performed as described. Lane1: untreated macrophages (25); lane2: MDP treated macrophages (45); lane3: PGN treated macrophages (106); lane4: MDP+PGN treated macrophages (87); lane5: Pam3CSK4 treated macrophages (82); lane6: Pam3CSK4+MDP treated macrophages (69); lane7: Nuclear extract of PGN stimulated macrophages is incubated with 100 fmol biotinylated NF-κB binding DNA and 100 pmol of unlabelled AP1 binding DNA (12). Values in brackets indicate average integrated density values of spot densitometric analysis using software Alpha Imager. Western blot and EMSA correspond to one representative experiment of three and two independent experiments respectively.</p",,MDP stimulation downregulates PGN mediated IL1β transcription by downregulating NF-κB and AP1 activation.,,10.1371/journal.pone.0027828.g003,,core
32435423,2012-02-01T08:00:00,"The Internet is an example of a successful and scalable decentralized system capable of connecting millions of systems and transporting data seamlessly between them. It has been so successful that today, it is impossible to imagine entertainment, education, communication, business and other services without the Internet. In fact, the Internet is widely considered to be just another utility service. Additionally, the diversity of the end-systems ranging from high-end servers to mobile phones and sensors is only adding to the rate of its growth and value. This success is largely the result of careful thought put into the design philosophy of the Internet (globally deployed network layer, isolation of protocol layers). This has resulted in a digital information explosion with recent studies predicting a ten fold increase in the amount of digital content over the next five years. Factors such as information replication, increasingly affordable and heterogeneous end-systems, cheap storage and numerous services are cited to be a few of the reasons for this rapid growth.
However, this fixed network layer also poses a barrier to introduction of innovations to support increasingly diverse end-systems and new communication paradigms. Moreover, the inherent issues in security, mobility, performance and reliability cannot be completely resolved by merely changing functionality in the end-systems, and will require addition of functionality in the core of the network as well.
Service-centric networking is a new paradigm that seeks to introduce functionality into the network by deploying customized in-network services on-demand. Different compositions of services are used to customize connections to satisfy various user communication requirements. This work addresses four challenges in the context of service-centric networks: (1) automated service composition (2) combined service composition and routing, (3) support for inter-domain data-plane policies in such networks, and (4) end-system support for services through abstractions. Automated service composition deals with the challenge of finding an optimal sequence of services to satisfy communication requirements of a connection. This composed sequence of services is applied to the connection in the data-path. A semantic tree is used to describe communication characteristics and the problem is solved by reducing it to a planning problem. Service composition is typically followed by  service routing , where the connection is set up such that the services are applied in order. This is not always optimal as we show through experiments. Combined service composition and routing tries to solve both problems in a single stage by reducing it to a planning problem. We further explore the issues of inter-domain data-plane policies in next-generation networks and discuss a system that uses the semantic tree to specify such policies. The system translates these policies into planning rules and determines the right way to set up the connection such that all policies are met. We also discuss the design and implementation of a novel  service socket  API that allows end-system applications to access services in a service-centric networking context.
Another key aspect of next-generation networking is virtualization of the physical network infrastructure. Network virtualization allows multiple networks with different protocol stacks to share the same physical infrastructure. A key problem for virtual network providers is the need to efficiently allocate their customers\u27 virtual network requests to the underlying network infrastructure. This problem is known to be computationally intractable and heuristic solutions continue to be developed. Most existing heuristics use a two-stage approach in which virtual nodes are first placed on physical nodes and virtual links are subsequently mapped. We present VHub, a novel single-stage approach that formulates this problem as a p-hub median problem. Our results show that VHub outperforms the state of the art algorithms by mapping 23% more virtual networks in lesser time (26% to 96%).
Overall, this dissertation discusses techniques through which data-path customization can be achieved in next-generation networks. To solve some of the technical challenges, this work follows a cross-disciplinary approach exploring ideas from computer networking, distributed systems and algorithms to graph theory, mathematical optimization and artificial intelligence. The solutions are also tested through simulations using real and synthetically generated workloads to validate the design",ScholarWorks@UMass Amherst,On Data-Path Customization In Next-Generation Networks,,,,core
79176563,2012-09-01T00:00:00,"Traffic accidents are still one of the main health problems in the World. A number of measures have been applied in order to reduce the number of injuries and fatalities in roads, i.e., implementation of Advanced Driver Assistance Systems (ADAS) based on image processing. In this paper, a real time speed supervisor based on road sign recognition that can work both in urban and non-urban environments is presented. The system is able to recognize 135 road signs, belonging to the danger, yield, prohibition obligation and indication types, and sends warning messages to the driver upon the combination of two pieces of information: the current speed of the car and the road sign symbol. The core of this paper is the comparison between the two main methods which have been traditionally used for detection and recognition of road signs: template matching (TM) and neural networks (NN). The advantages and disadvantages of the two approaches will be shown and commented. Additionally we will show how the use of well-known algorithms to avoid illumination issues reduces the amount of images needed to train a neural network.The work reported in this article has been partly funded by the Spanish Government by the grants
FEDORA (TRA2010-20225-C03-01) and D3System (TRA2011-29454-C03-02)",'MDPI AG',Recognition Stage for a Speed Supervisor Based on Road Sign Detection,https://core.ac.uk/download/79176563.pdf,10.3390/s120912153,"[{'title': 'Sensors', 'identifiers': ['issn:1424-8220', '1424-8220']}]",core
432177305,2012-01-14T00:00:00,"International audienceVideo services are being adopted widely in both mobile and fixed networks. For their successful deployment, the content providers are increasingly becoming interested in evaluating the performance of such traffic from the final users' perspective, that is, their Quality of Experience (QoE). For this purpose, subjective quality assessmentmethods are costly and can not be used in real time. Therefore, automatic estimation of QoE is highly desired. In this paper, we propose a noreference QoE monitoringmodule for adaptive HTTP streaming using TCP and the H.264 video codec. HTTP streaming using TCP is the popular choice of many web based and IPTV applications due to the intrinsic advantages of the protocol. Moreover, these applications do not suffer from video data loss due to the reliable nature of the transport layer. However, there can be playout interruptions and if adaptive bitrate video streaming is used then the quality of video can vary due to lossy compression. Our QoE estimation module, based on Random Neural Networks, models the impact of both factors. The results presented in this paper show that our model accurately captures the relation between them and QoE",HAL CCSD,Quality of Experience estimation for Adaptive HTTP/TCP video streaming using H.264/AVC,,,,core
326233150,2013-01-01T00:00:00,"Il lavoro ha tratto ispirazione dalla condizione nella quale versa il centro storico del capoluogo abruzzese a quattro anni di distanza dalla catastrofe del 6 aprile 2009 e propone un approccio alla gestione complessiva della situazione ricorrendo a uno strumento di georeferenziazione open source in grado di fornire risposte a più scale spaziali (e/o temporali) di intervento attraverso procedure idonee a garantire la gestione dei processi dell’emergenza e della ricostruzione nei loro aspetti evolutivi.

Principali finalità del lavoro sono da un lato l’elaborazione di strategie miranti alla ‘riattivazione urbana’, dall’altro la produzione di un sistema di monitoraggio del costruito costantemente aggiornato e utile sia ai professionisti incaricati del restauro dei singoli manufatti sia alle amministrazioni che vogliano intraprendere politiche di prevenzione sismica a lungo termine mediante l’elaborazione di mappe del rischio ed altri documenti per il governo del territorio urbano sotto il profilo della sicurezza sismica.

Una prima fase del lavoro di tesi ha affrontato la ricerca delle cause e dell’entità del danneggiamento notevolmente esteso, incrociando i vari aspetti rilevanti, come l’indagine del sottosuolo, la vulnerabilità del costruito - studiata anche attraverso il censimento e l’indagine delle tipologie murarie e della loro qualità, ed altri.

A ciò ha fatto seguito la creazione di un vasto database in cui sono stati immagazzinati i dati relativi ai 505 aggregati presenti all’interno della zona A (il centro storico ovvero l’area di studio), dalle caratteristiche anagrafico/funzionali degli stessi alla loro agibilità, passando per il danno registrato secondo la scala EMS, le informazioni relative alla loro costituzione strutturale, la collocazione o meno in zona rossa ecc. 

Strati informativi di particolare interesse sono quelli relativi al rilievo georiferito di ogni singola opera di messa in sicurezza presente su ogni aggregato e agli esiti di agibilità.

La base informativa costruita, estesa attraverso una cospicua quantità di dati fotografici e abachi anch’essi georiferiti, ha avuto una duplice utilità: la creazione di grafici di analisi e bilancio per una comprensione a più livelli di complessità della condizione del centro storico (aggregati in zona rossa, livelli di danneggiamento, funzioni compromesse ecc.) oltre che l’elaborazione di strategie per la ridefinizione e il monitoraggio della zona rossa nonché di supporto alla viabilità e alla cantierizzazione delle aree della città.

Il lavoro ha in nuce molteplici possibili sviluppi futuri: da un più efficace monitoraggio delle opere di presidio (che mostrano a ormai 4 anni dal sisma situazioni ricorrenti di degrado) a una auspicabile gestione informatizzata dei dati per una loro fruibilità da parte della cittadinanza e un loro costante sviluppo e aggiornamento nel tempo; ciò nella convinzione che solo elevando sempre più il livello di conoscenza sul costruito si possano elaborare strategie operative sia globali che mirate aventi alla base un utilizzo sempre più efficace e trasparente delle risorse.

Le metodologie elaborate mostrano la loro validità nella constatazione che il caso aquilano è soltanto uno degli innumerevoli possibili campi d’applicazione, laddove esse risultano applicabili a qualunque realtà, urbana o territoriale, interessata da calamità e/o tale da richiedere la gestione di una molteplicità di dati tra loro interconnessi per controllarne e migliorarne le dinamiche evolutive.The work was inspired by the condition in which is the heart of the city of L’Aquila (centre Italy) 4 years after the seism of April 6, 2009. It proposes an approach to the complex situation management using an open source georeferencing instrument, able to give solutions at several spatial/temporal intervention scale thanks to methods apt to guarantee the control of the emergency and of the reconstruction.

Main purposes of the work are the processing of strategies oriented to the “urban reactivation” on the one hand, the realization of an always updated buildings monitoring system advantageous at multiple levels on the other: this may be interesting not only for all freelance professionals projecting single buildings restoration, but also for local administrations starting seismic-prevention policies. 

The first phase dealt with the research of the damage causes and importance intersecting many significant aspects, such as the subsoil survey, the buildings vulnerability, also examined through the typology and quality masonry census, and so over.

The results obtained were the basis for the creation of a database storing the attributes of the 505 aggregates consituting the inner city of L’Aquila (“A zone” according to the town plan). It is divided into 5 sections containing respectively geographic-functional, structural technology, damage, practicability, provisional measures data. 

This platform was implemented by a huge quantity of photographic references and georeferenced indexations: in particular, after a diffused urban survey, it was possible to classify the most common masonry works and all kind of provisional measures employed and to place them on the map through photos.

This had a double utility: at first the elaboration of analysis diagrams for understanding the complexity of urban reality at multiple levels (aggregates belonging to the red zone, damage conditions, compromized functions ecc.), then the planning of “seismic-urban” strategies to redefine and monitor the gradual extension changing of the red zone or to support the viability for the next construction sites intallation.

This method shows a lot of development potentialities: for istance a more efficacious provisional measures screening (showing evident decay signs, after 4 years), but also an advantageous computerized data management in view of their fruition from the citizens; all this is realized in the certainty that, only through a continued elevation of the “knowledge level” of the buildings it is possible to study operational strategies both global and focused, starting from a more efficacious and effective use of resources.

The methodologies show also that the case of L’Aquila is only one of the innumerable possible application field, because they could be tested on whichever urban or territorial reality, interested by natural calamity or needing a multiplicity data management to control and improve certain evolving dynamics.

For its characteristics of originality and in-depth analysis, this study was graded with highest honours and  publication recommended by the academic board",,"Interventi di riabilitazione post-sisma nel centro storico di L'Aquila: un approccio GIS, metodologia e prospettive applicative",,,,core
100083237,30/11/2014,"Current analyses of groundwater flow and transport typically rely on a physically-based model (PBM), which is inherently subject to error and uncertainty from multiple sources including model structural error, parameter error and data error. The model uncertainty can be difficult to quantify, and is propagated to the prediction. In this study, comple-mentary data-driven models (DDMs) are used to improve prediction of groundwater flow models. The DDMs, trained with the historical residual of the PBM, have the capability to compensate for the defects of PBM. Five machine learning techniques, instance-based weighting (IBW), locally weighted regression (loess), decision trees (DT), artificial neural networks (ANN) and support vector regression (SVR) are employed to construct the DDMs, and their performance of enhancing the prediction of the PBM is compared. Before the DDMs updating, cluster analysis is implemented on the dataset to improve the robustness and efficiency of the framework. The framework is tested in two real-world case studies based on the Republic River Compact Association (RRCA) model and the Spokane Valley Rathdrum Prairie (SVRP) model. The DDMs reduce the root-mean-square errors (RMSE) of the temporal, spatial and temporal plus spatial head prediction of the RRCA model by 82%, 60 % and 48 % respectively. In the SVRP case study, the DDMs reduces the temporal head forecast of the PBM by 79%. Localized DDMs that are conditioned on each cluster outperform global DDMs without clustering. It is also demonstrated that clustering signifi-cantly reduces the computational cost of training and cross validation of the DDMs. After clustering, the run-time of DDMs is negligible comparing with the PBM, which makes the framework very computationally efficient. i",,Adviser:,,,,core
147989598,2012-08-15T12:28:04,"Locomotion is one of the most important abilities of humans. Actually, gait locomotion provides mobility, and symbolizes freedom and independence. However, gait can be affected by several pathologies, due to aging, neurodegenerative disease, or trauma. The evaluation and treatment of mobility diseases thus requires clinical gait assessment, which is commonly done by using either qualitative analysis based on subjective observations and questionnaires, or expensive analysis established in complex motion laboratories settings. This thesis presents a new wearable system and algorithmic methods for gait assessment in natural conditions, addressing the limitations of existing methods. The proposed system provides quantitative assessment of gait performance through simple and precise outcome measures. The system includes wireless inertial sensors worn on the foot, that record data unobtrusively over long periods of time without interfering with subject's walking. Signal processing algorithms are presented for the automatic calibration and online virtual alignment of sensor signals, the detection of temporal parameters and gait phases, and the estimation of 3D foot kinematics during gait based on fusion methods and biomechanical assumptions. The resulting 3D foot trajectory during one gait cycle is defined as Foot Signature, by analogy with hand-written signature. Spatio-temporal parameters of interest in clinical assessment are derived from foot signature, including commonly parameters, such as stride velocity and gait cycle time, as well as original parameters describing inner-stance phases of gait, foot clearance, and turning. Algorithms based on expert and machine learning methods have been also adapted and implemented in real-time to provide input features to recognize locomotion activities including level walking, stairs, and ramp locomotion. Technical validation of the presented methods against gold standard systems was carried out using experimental protocols on subjects with normal and abnormal gait. Temporal aspects and quantitative estimation of foot-flat were evaluated against pressure insoles in subjects with ankle treatments during long-term gait. Furthermore, spatial parameters and foot clearance were compared in young and elderly persons to data obtained from an optical motion capture system during forward gait trials at various speeds. Finally, turning was evaluated in children with cerebral palsy and people with Parkinson's disease against optical motion capture data captured during timed up and go and figure-of-8 tests. Overall, the results demonstrated that the presently proposed system and methods were precise and accurate, and showed agreement with reference systems as well as with clinical evaluations of subjects' mobility disease using classical scores. Currently, no other methods based on wearable sensors have been validated with such precision to measure foot signature and subsequent parameters during unconstrained walking. Finally, we have used the proposed system in a large-scale clinical application involving more than 1800 subjects from age 7 to 77. This analysis provides reference data of common and original gait parameters, as well as their relationship with walking speed, and allows comparisons between different groups of subjects with normal and abnormal gait. Since the presented methods can be used with any foot-worn inertial sensors, or even combined with other systems, we believe our work to open the door to objective and quantitative routine gait evaluations in clinical settings for supporting diagnosis. Furthermore, the present studies have high potential for further research related to rehabilitation based on real-time devices, the investigation of new parameters' significance and their association with various mobility diseases, as well as for the evaluation of clinical interventions","Lausanne, EPFL",Assessment of Foot Signature Using Wearable Sensors for Clinical Gait Analysis and Real-Time Activity Recognition,https://core.ac.uk/download/147989598.pdf,10.5075/epfl-thesis-5434,,core
21207801,2008,"This paper investigates an application of mobile sensing: detecting and reporting the surface conditions of roads. We describe a system and associated algorithms to monitor this important civil infrastructure using a collection of sensor-equipped vehicles. This system, which we call the Pothole Patrol (P 2), uses the inherent mobility of the participating vehicles, opportunistically gathering data from vibration and GPS sensors, and processing the data to assess road surface conditions. We have deployed P 2 on 7 taxis running in the Boston area. Using a simple machine-learning approach, we show that we are able to identify potholes and other severe road surface anomalies from accelerometer data. Via careful selection of training data and signal features, we have been able to build a detector that misidentifies good road segments as having potholes less than 0.2 % of the time. We evaluate our system on data from thousands of kilometers of taxi drives, and show that it can successfully detect a number of real potholes in and around the Boston area. After clustering to further reduce spurious detections, manual inspection of reported potholes shows that over 90 % contain road anomalies in need of repair",The Pothole Patrol: Using a Mobile Sensor Network for Road Surface Monitoring,,,,,core
21052594,06/08/2009,"Abstract Computational trust has been developed as a novel means of coping with uncertainty within collaborative communities of interacting peers. The idea now offers enourmous potential for use in pervasive mobile environments; however, to date there is little agreement about what computational trust itself means, and what the limitations that emerge from its use are. In this work, we project the idea of computational trust into machine learning terms, showing that trust is a metaphor that helps system designers reason about and exploit the intended deployment scenario to achieve their goals. Viewing a trust model as a strategy to confront a learning problem thus allows us to explore the effect that constraints, such as mobility and user participation, will have on the quantity of information available to learn from; in this work, we demonstrate this idea with a set of experiments on the Reality Mining Dataset. The results highlight that the most successful trust models will be based on strong contextual information about the environment they are to be deployed in. ",Learning to Trust on the Move,,,,,core
15349363,2008,"This paper presents a semi-automatic visualization method for the evaluation of urban environments that is based on artificial intelligence. It proposes the use of agent-based crowd simulation software on a mid-scale urban planning level for design evaluation. The information on agentsi movements is noted in standard raster images. The results are maps that are easy to understand. These maps show movement paths of the agents and density and give further conclusion on bottlenecks in planning contexts. Key measures, like the occupant movement in a given district, until now relied greatly on empirical knowledge or data that could be only gathered after an urban design had become built reality. Our method focuses on the adaptation of common software technology that is originally situated in film and TV productions. A practical workflow shows how our method can be easily integrated in daily design tasks",Crowd Simulation for Urban Planning,,,,,core
429704714,2007-08-31T00:00:00,"The University of Edinburgh and research sponsors are authorised to reproduce and distribute reprints and on-line copies for their purposes notwithstanding any copyright annotation hereon.  The views and conclusions contained herein are the author’s and shouldn’t be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of other parties.The Collaborative Operations for Personnel Recovery (Co-OPR) project sought to
provide collaborative task support for a Search and Rescue coordination center. The
project aimed to create a prototype “Personnel Recovery (Experimental) Pack” (PREP) and to demonstrate its use.

A number of requirements capture, knowledge gathering and transition workshops and
meetings were held. This included an initial requirements setting workshop early in 2005,
meetings at the USJFCOM Joint Personnel Recovery Agency’s (JPRA) Personnel
Recovery Education and Training Center (PRETC) in Fredericksburg, Virginia in June
2005, a review meeting in Edinburgh in August 2005, and attendance at a Command Post
Exercise (CPX) at the PRETC in November 2005. These initially established the potential
areas for use of Co-OPR and I-X tools in training exercises. In the second project year such tools were developed and tested using a training exercise as held at the PRETC, using observers from PRETC and USJFCOM and an evaluation expert from USJFCOM/J9.

The project was provided with a rich set of urban and rural scenarios by JPRA/PRETC
which together are unclassified versions of scenarios used within the PRETC training
courses and Command Post Exercises. At the time, these stretched the capabilities of the
current and envisaged technologies within Co-OPR/I-X. Refinement of the scenarios
alongside PRETC, and knowledge engineering to capture information on standard operating procedures and responses were a key part of making the work relevant to the potential real use of Co-OPR/I-X for Personnel Recovery.

The core I-X technology was packaged into a number of checkpoint releases to make
available the features required to meet the application needs. I-X version 4.3 released in November 2005 to checkpoint the results achieved on the first 12 months of work with
the PRETC. It formed a basis for the work on really using the technology at the PRETC.
New “white cell” aids for training were made available in an initial version. A new I-Sim
simulation capability, and advanced option exploration tools have all been improved
significantly to make them more usable, including the features of the I-Plan AI planner and its capability for plan repair after failures. The features of the Domain Editor (I-DE) and its ability to browse and update or augment standard operating procedure knowledge dynamically during missions were enhanced. The final release of I-X that includes all the new developments achieved in the Co-OPR project is version 4.5.

The results of the work were packaged, along with Personnel Recovery domain specific
models, as a web site and/or CD which could be considered as a prototype “Personnel
Recovery (Experimental) Pack” of tools to assist a Joint Personnel Recovery Center
(JPRC) and associated operational staff in performing their operations. The versions of
PREP produced were used in one workshop or Command Post Exercise at the PRETC
under guidance from Dr. Jeff Hansberger at training related workshops already organized
by USJFCOM/J9 Expt. and Fred Kleibacker, the (now former) Director of the PRETC in
Fredericksburg, Virginia. Co-OPR team members were engaged with these workshops to
to show the tools in realistic settings, to assist with training where possible, and to gather experimental feedback.

Realistic use of tools for Personnel Recovery requires that the systems can work with
emerging technology for geo-positioning, survival radios, evasion aids, robotic or semiautomated rescue aids or robots, and doctrine or tactics, techniques and procedures for Personnel Recovery. A number of short studies of these “complementary technologies” were made which explored deployment and inter-working aspects of these with the Co-OPR/I-X technology",Collaborative Operations for Personnel Recovery Final Report on DARPA/AFRL,,Artificial Intelligence Applications Institute,,,core
235570187,2003-04-01T00:00:00,"An archive of the Milo Canopener.The University of Lethbridge Library received permission from the Archives at Milo Library to digitize and display this content.Milo Can Opener
Box 12, Milo, AB T0L1L0
Canada Post Agmt. # 40607518
April, 2003
I 1
J^a^GO UO 0\)iAd Rates
Subscription Rates
Business Directory
$ 5.00
Milo $ 16.00
Quarter Page
6.00
(- pickup,delivery or mailed)
Half Page
8.00
Mailed (outside Milo) 24.00
Full Pages
15.00
Single Copies 2.00
Classifieds
2.00
Please note that those who paid for a Milo subscription by mail ($24) on our previous rates
have payed $8 too much. Your subscription
The following items are free of charge
renewal date will be extended by 6 months.
Notices Please sign them, no letters will be
Announcements printed if not signed.
( Wedding, Anniversary, Births, Showers, etc.) Requests to remain anonymous
Cards of Thanks will not be honoured.
News items Articles
Please send items to the following volunteer staff
Layout Editors - Barb Godkin - 599
-2213, 485-8389
Iris Gough - ..........................
599-2377
Production - Colleen Deitz
599-2306
Betty Armstrong
Zola Webber
Subscriptions -Georgina Ully -........................
..... 599 - 2424
Notices - Julie Nelson -.................................
.... 599-2175
Charlotte Nelson -..........................
599-2253
Cartoons & “Kids Say” - Marina Vannatta -...
..... 381 - 6389
Milo Can Opener
Box 12, Milo, Alberta, TOL 1L0
f Fax# 599 - 2457
? (fax shares line with phone so you will get the answering machine sometimes. You can also fax to Milo Municipal Library at 599-3850) Email: igodkin@telusplanet.net
Items may be left at Jamie’s Foods in the Can Opener box at back of store
or at Milo Municipal Library.
Please note the new fax number!
Please Note: The deadline for articles that need typing, etc. to be submitted is the Monday before the last Friday of each month. If your article is ready for press, we can accept it until Wednesday or Thursday.GOODS & SERVICES
Donald W. Kinney
Manager
Box 150
Milo, Alberta TOL 1LO
i
Tel: (403) 599-4101 Fax: (403) 599-2409
Scotiabank
BUSINESS HOURS:
Mon - Thurs 10:00 - 12:00 1:00 - 3:00 Friday 9:30 -12:00 1:00- 5:30
GRANT, KRYSTALOWICH & BENNETT
CERTIFIED GENERAL ACCOUNTANTS
FULL ACCOUNTING SERVICES AND CONSULTING
P.O. Box 239 Vulcan, Alberta TOL 2B0
Phone: 485-2996 485-2681
■
\^luVenture
Travel
Val Umscheid
travel consultant/tour guide
|J§g&
Box 88
Milo, AB mil T0L1L0 ^^1
e-mail: valuventure@telusplanet.net www.valuventure.com
tel: 1(403)599-2406 fax: 1(403)599-2247 toll free: 1(877)599-2499
*;■ •: ! Tift.
MILO
SEED CLEANING ASSOCIATION LTD.
599-2150
Cleaner Seed is Sown Cleaner Crops are Grown
Ed Posein - Manager
Box 7 Milo, AB T0L1L0
Doug Marks
PRESIDENT
Office: (403) 599-0003 Fax: (403) 599-3990 Mobile: (403) 362-1764
Marks
Oilfield Services Inc.
Trucking, Gravel
Oilfield Maintenance and Construction Pipelining
Pressure Washing and Steaming
(£sso)
VULCAN VILLAGE GAS BAR
P.O. BOX 425 VULCAN. ALTA. TOL 200 PHONE: 485-6000
FOOD TO CO
Garry &
Bernardine Nelson 485-2519
CORNER STORE & GARAGE
OIL - GAS - DIESEL - REPAIRS - WELDING A.M.A. TOWING
MERV & FRANCES GOLDTHORPE 485 - 6671We would like to thank our advertisers for their continued support.
Without them, we would not be able to print this newsletter for the enjoyment of the readers.
Dor, (Go Mo L3s®®imlb®
(403)485-6005 ■' P.O. Box 87, Vulcan, Alberta, Canada
FAIRBANKS DENTURE CLINIC
125 Centre Street, Vulcan, Alberta TOL 2B0
485-2368
Scott D. Fairbanks - Denturist
OFFICE HOURS:
Wednesday 9:00 am - 5 00 pm Friday 1.00 pm. - 5:00 p m
_________' ""__________ '_
Lori Vooys, CIM, FCSI Financial Planner
lori_vooys©scotiamcleoc).com
Suite 1800, Scotia Centre 700 - 2nd Street SW Calgary, AB T2P 2W1
Tel: (403) 298-7823 Fax: (403) 298-4054 Toll Free: 1-800-372-9274 Cell: (403) 815-6002
% ScotiaMcLeod
ScotiaMcLeod is a division of Scotia Capital Inc, a member of the Scotiabank Group.
V.->
Xr. B. X Drump
OPTOMETRIST
BOX 972
VULCAN, ALBERTA TGL280
telephones
485-2177
485-2886
Jamie’s H Foods
Carol and Jams Robertson Box 38 Milo, AB.TOLILO
Ph. 403-599-3922 Fax 599-3835
^LMARy KA>4
Donna Bennett Deiti
independent Beauty Consultant
P.O.. Box 37,
Milo, Alta. TOL 110 (403) 599-2140
A. P. C. S.
AARDVARK PEST CONTROL SERVICES®
JERRY GAUTREAU
P.C.T. Diploma, AIB Certified & ASI Certified
SUITE 213, 204 - 1440 52nd STREET N.E. CALGARY, ALBERTA T2A 4T8
Tel: (403) 273-MICE (6423) Fax: (403) 204-2125
^ ' -
Sc ©lean Sc pimple
INTERNATIONAL RECORDING ARTISTS
Cell: (403) 512-9066 Fax: (403) 599-2398
Keepln' the Country In music
LAH-MAR PROMO Ph: (403) 381-6389 Fax: (403) 381-6341& <5 £ Gsiecdiue ^beilc^t
for a&bMasitf*
599-2466
Milo, Alberta
Open - Monday, Wednesday, Friday, Saturday
RENO BEXTE
Weed Co ntrof (Centre
P & H GRAIN LTD. AGENT FOR ALTA HAIL INS.
TELEPHONE:
(403) 534-3461 ANYTIME FAX: (103) 534-2182
MOSSLE3GH. AB TOL IPO
■■ ■ / • ■■ .
MILO CAFE
CHINESE A'WESTERN TAKE OUT ORDERS
599-3832
Closed Monday
Monday-Sunday 8:30 am - 800 pm
Beer&. Wine with meals
_.......................
Next Canopener Deadline April 28""Organized for Savings Not fo r P rofit”
t
COOP
ARROWWOOD CO-OPERATIVE ASSOCIATION LIMITED
P.O. BOX 120 ARROWWOOD, Alberta TOL 0B0
(403) 534-3803 Store (403) 534-3804 Tire & Lube Center Fax (403) 534-3330
Your Suppliers of:
Petroleum Products - Fuel & Lubricants Tires - On and Off Road - New/Used/Repair Services Lube Center - Most vehicles - including 1 ton trucks Hardware /Lumber / Plumbing / Electrical Filters - Complete line of oil / air / fuel Belts / Bearings / Hydraulic Hoses Paint - interior/exterior - mixed to your color specifications Batteries - automotive/flashlight / watch / etc.
Automotive - lights /fuses /accessories Housewares / Sporting Goods Feed and Animal Health Supplies
Hours: Mon-Fri 8am-12 noon 1pm-5:30pm Saturday 8am-12 noon
DR. JAMES L. POTVIN
B.Sc (Hons), D.D.S.
General Dentistry
Patient Services
Nitrous gas for anxious patients Electronic freezing (no needle) Televisions & Walkmans Highest standard of sterilization for your protection Financing and payment plans . available 0A.C
Quality Dental Services
- Comprehensive preventative exams including periodontal and oral cancer screening
• Gentle hygiene care
• Toothcolored & silver fillings
• Cosmetic Bonding and Veneers to cover chips, cracks and stains
• Tooth whitening to brighten your smile
• Crown and Bridge
• Complete and Partial Dentures
Alt members of our Cavity Free Club are entered into a monMy draw for a S25 Gif Certificate Redeemable at Wolfe's Hardware Toy and Sporting Goods Department
Snake Valley Drop-In News
GENERAL MEETING Friday, April 4 at 2:30 p.m.
Everyone Welcome
Games
Bridge - every Tuesday at 1:30 p.m.
Crib - Monday, April 7 at 7:30 p.m. Monday, April 21 at 7:30 p.m.
NEW PATIENTS ARE ALWAYS WELCOME
J H, 3rd Avenue North, Vulcan
Health Nurse
The public health nurse will be at the drop-in Wednesday, April 9 from 1:00 to 2:00 p.m.VILLAGE OF MILO MINUTES
The regular meeting of the Village of Milo was held on Monday, February 17, 2003 at 7:00 p.m. at the Village Office.
Present were Mayor Vooys, Councillor Whaley, Councillor Phillips, Municipal Administrator Dorothy Way, John and Bea Kuzma, and Bob Phair.
The minutes of the meeting held on January 20, 2003 were read. Councillor Phillips moved the adoption of the minutes. Councillor Whaley seconded. CARRIED.
Mayor Vooys attended a FCSS meeting. The FCSS will be running a courtesy vehicle in this area. Councillor Whaley attended a VBDC meeting and they will be hosting an economic development training course. The VBDC will be joining up with Tourism. Mayor Vooys thanked the Council for the flowers.
Town man Victor Crowe reported that the tractor is leaking power steering fluid. The tractor is to be repaired before we start cutting the grass in the spring. It was suggested that a barrel be put up at the water treatment plant for the water to drain into after the trucks fill up. The water could also be drained into the sewer pipe on the south side of the water treatment plant. The culvert at the end of Center Street needs to be flushed out. Mayor Vooys will talk to Doug Marks and see if we can have an oilfield hydro truck flush out this culvert. The other end of this culvert will have to be cleaned out before this can be done.
Mr. Bob Phair attended the meeting to gather information on the water reservoirs so the company he works for KENECO can put together a proposal for this project. Mr. Bob Phair and a construction personnel will take a look at the reservoirs. The Municipal Administrator to look into funding from Alta. Transportation & Utilities as well as Infrastructure funding. A letter to be sent to Alpine Engineering Ltd. stating that we will not require his services.
A letter has been sent to JRD Contracting confirming that he has the contract for doing the sidewalk repairs. The Village has received the funding for this project.
Mr. & Mrs. John Kuzma attended the meeting in regards to the sale of the lots in BLOCK C PLAN 1403JK. The 2 trailers that are on this lot will be moved. There is no gas line on the property. The trailers that were there previously used propane.
MOVED by Councillor Phillips that the Village apply for the STEP grant this year. CARRIED.
We received 2 quotes one from Trinus Technologies Inc. and one from the Management Information Group on Municipal software. The Municipal Administrator to talk to Delanoy and Ziel and see if they have any information on what programs we should be using.
AXIA was here to take measurements of where the wire and the box will be installed for the Supemet. Mayor Vooys and Councillor Phillips want to be contacted when they return.
Letters were received from the Oldman River Intermunicipal Service Agency and the Vulcan County on the proposed subdivision for a petroleum card lock facility. Letter to be sent to the Vulcan County and the Oldman River Intermunicipal Service Agency stating that the Village of Milo has no objection to this proposed subdivision and petroleum card lock facility. MOVED by Councillor Phillips that the Village of Milo approve the petroleum card lock facility. Mayor Vooys seconded. CARRIED. Councillor Whaley is opposed to this proposal. Councillor Phillips to contact the Arrowwood Co-op and get a copy of the letter from Alta. Transportation and Utilities for Councillor Whaley.
MOVED by Councillor Whaley that the Village of Milo approve the proposed resolution for membership in the Oldman River Regional Services Commission. The proposed resolution is as follows: THAT the Village of Milo join the Oldman River Regional services Commission. CARRIED.
No one has come forward to organize the Communities in Bloom for the Village.
The next Council meeting of the Village of Milo was set for Monday, March 17, 2003 at 7:00p.m. at the Village Office.
The meeting adjourned at 8:55 p.m.DUSTING
M A house becomes a home when you can write “I love you” on the furniture!”
I can’t tell you how many countless hours that I have spent CLEANING! I used to spend at least 8 hours every weekend making sure things were just perfect—“in case someone came over.” Then I realized one day that no one came over; they were all out living life
and having fun!
Now when people visit, I find no need to explain the ‘condition’ of my home. They are more interested in hearing about the things I’ve been doing while I was away living life and having fun! If you haven’t figured this out yet, Please heed this advice!!!
LIFE IS SHORT! ENJOY IT! Dust if you must, but wouldn’t it be better to paint a picture or write a letter, bake a cake or plant a seed,—ponder the difference between
want and need?
Dust if you must, but there’s not much time, with rivers to swim and mountains to climb, music to hear and books to read, —friends to cherish and life to lead.
Dust if you must, but the world’s out there, with the sun in your eyes, the wind in your hair, the flutter of snow, a shower of rain,—this day will not come around again!
Dust if you must, but bear in mind, old age will come, and it’s not kind.
And when you go—and go you must—you, yourself, will make more dust!
4
“It’s not what you gather, but what you scatter, that tells what kind of life you have
lived!!”
• Aerator covers • Screens • Covers • Tarps • Tents
• Security pouches for radios
Bug sweeps
• Boat covers
• Patio swings
• Sprayer covers
• Tire covers
• Winter fronts
• Cargo nets
• Outfitter's tents
• Fertilizer & chemical spreader covers
N-l Upholstery & Tarp Manufacturing — 120 Main Street, Neville
&
Irene
Champion, flB P0 Box 206 TOL 0R0
Phone: 1-888-227-0170
Please phone before you corr>e if you're coming any distance to ensure we are here to ouoid disappointment.
• Portable generator covers
• Hopper covers
• Auger extension spouts
• Air conditioner covers
• Tonneau covers
• Windshield covers
• Bug screens
• Asphalt tarps 600°F rating
• Pack covers
• Truck tarps : flat & roll
• Recreational tents-
NAME
ADDRESS
Milo Minor Soccer Program 2003 Registration Form
______BIRTHD ATE: (d/m/y)__/__/__/ AGE(as at deadline)_
PHONE___________________________________SEX.
HEALTHCARE NUMBER
DOCTOR,
Please indicate any illness and physical limitations
Please indicate any illness or injury occurred over the last year,
PARENT/GUARDIAN(please print)__________________________
AGE CATEGORIES: (please check appropriate box)
(age categories may be combined if there are not enough registrations)
□
U-6 Mini-Soccer
4 to 5 years
□
U-8 Mini-Soccer
6 to 7 years
□
. U-10 Mini-Soccer
8to 9 years
□
U-12 Mini-Soccer
10 to 11 years
Important Information
All players must supply their own certified shin guards, soccer socks, and water bottles When: Practice to begin as weather permits(note will be sent home)
Parents will be responsible for ensuring their child has a ride for games.
Cost: $20/ participant (covers insurance and equipment costs)
Please make check payable to Milo Community Soccer Return form/payment by: April 10,2003
Shane Cranston Milo Community School
I the undersigned accept responsibility for the above registered player during activities associated with Milo Minor Soccer Program during the 2003 season in such that Milo Soccer Board and/or any of its agents cannot be held liable for any accidents which occur incidental thereto, and the player is accepted on this condition.
I would be interested in coaching.___________________________
Signature of Parent/Guardian:________________________________________Frank Mclnenly Auctions Ltd.
Vulcan, AB
. „ . - • , ..jss-**,.*,— .**»$*.*•; •' -
Serving The Agriculture Industry
Since 1967
(403)485-2440
Frank Mclnenly Stacey Mclnenly Les McIntyre
Foothills Ljvestock Auction
(403) 549-2120
Regular sales every Friday Special Calf Sales Bred Sales as announced
For up to date marketing call:
Frank Mclnenly (403) 485-2440 cell: (403) 485-8123
Marvin Fowler (403) 646 -2334 cell: (403) 625-6070
F M Trailer World
Located at Foothills Livestock Auction
Stavely. AB
SoulZern A/lerta's Exclusive JVorlerl Dealer
N0RBERT DEX TRAILTECH
Stock, Horse, Flatdecks SNew & Used
1-877-205-1999
Call StaceyGOALS:
* To provide affordable transportation where needed.
* To assist the Senior shut-in population and persons with special needs to be able to access health care, shopping and socialization.
* To make the transportation available every second week in each small community.
PARTNERS:
* Headwaters Health Authority
* Family and Community Support Services
* Vulcan Lions Club
* Friends of Little Bow
NOTE:
People who can not access the service without assistance would need to bring a companion along.
Pick up from residences will be made in each small town. Efforts will be made to accommodate stops in Vulcan
Headwafers Health Authority Regional Health Authority #3
Three Month Pilot Project
Start Date:
April 02, 2003
Pick up. Carmangay 9:00 am
Pickup Champion 9:30am
Pick up Milo 9:00 am
Arrive
Vulcan Community Health Centre
10:00 am
Downtown Vulcan - Mainstreet
10:30 am
Pick up VCHC 11:00 am
Arrive downtown 11:30 am
Pick up at designated point at 1:30 pm
Leave Vulcan for home 2:00 pm
April 09, 2003
Pick up Arrowwood 9:00 am
Pick up Mossleigh 9:30 am
Pick up Lomond 9:00 am
Amve
Vulcan Community Health Centre
10:00 am
Downtown Vulcan - Mainstreet
'l0:30am
Pick up VCHC 11:00 am
Arrive downtown T1:30 am
Pick up at designated point at 1:30 pm
Leave Vulcan for home 2:00 pm
Suggested Donation:
$6.00 per round trip
BUS SERVICE COMING TO THE COMMUNITIES OF THE VULCAN COUNTY
HANDI-GO BUS VULCAN COUNTY
ANSWERING A NEED IN OUR COMMUNITIES
Phone 485-2285 For information or bookingsif
caf
*'0**+' Are You A Poet and Don’t Know It?
4 Your talents could pay off
Enter the Milo Library Poetry Contest And you could win a “Chapters” gift certificate
3 Categories:
Grade 3 and Under Grades 4 to 6 Grades 7 and up
Your poem should have something to do with the history of Milo and/or the Milo Library Any type of poem is acceptable Submit to the Library by Thursday May 29th.
isrfbjfcrrH & comjpany
BARRISTERS and SOLJCnXJRS
SERVICING ALL YOUR LEGAL NEEDS
Dr. Robert J. (Bob) Langrldge will be In attendance at the Village Office in Milo the first Friday of each month from 1:00 p.m. to 3:00 p.m. Appointments may be made by calling 485-2070
Brian J. Murray and Robert J. (Bob) Langrldge servicing our Vulcan office 104 Centre Street Vulcan, Alberta Phone: (403) 485-2070
Areas of Law: Real Estate, Personal Injury, Divorce and Family Law, Wills and Estates. Dependent Adutts, Employment Law. Criminal Law, Business and Corporate Law. Mediation, Uhgatlon and Tax Law.
LETHBRIDGE OFFICE
#600, 220 - 4m Street South Phone: 403) 278-7781 Fax. (403) 320-8958 Toll Free: 1-800-552-8022
SOUTHERN ALBERTA'S REGIONAL LAW FIRMMilo Municipal Library
NEWS
a member of the Chinook Arch Regional Library System www.chinookarch.ab.ca
NASA arfti the “Association of Library Services to Children” have partnered to offer online and discovery-based activities focusing on space science and technology. http://www.ala.orq/alsc/spaceplace/clubspace.ht
ml
Search for current affairs radio and t.v.clips. http://archives.cbc.ca
Our next Library Board Meeting will be held on Thursday May 1st 2003.
THANK YOU
A HUGE THANK YOU goes out to David Healy for donating an incredible oak bookcase that he built. The “Friends of the Library” will be selling raffle tickets on it as a fund raiser. Check next months’ Can Opener for further details.
MARCH BESTSELLERS
“2nd Chance” by James Patterson “Everything’s Eventual” by Stephen King “Portrait in Death” by J.D. Robb “Body of Lies” by Iris Johansen “Midnight Voices” by John Saul “City of Bones” by Michael Connelly “Hunting Season” by Nevada Barr “The English Assassin” by Daniel Silva “Gone For Good” by Harlan Coben “Little Altars Everywhere” by Rebecca Wells
New Books
“The Academy Awards A Complete History of Oscar”
“Red Mafia” by Robert Friedman “Rush Hour Recipes” by Jean Pare “He Sees You When You’re Sleeping” by Mary Higgins Clark
“Take a Thief’ by Mercedes Lackey “Fortress Draconis” by Michael Stackpole “The Diamond Hunters” by Wilbur Smith “Shout at the Devil” by Wilbur Smith “Star by Star” by Troy Jenning
“Fit Not Fat at 40 Plus”
New Videos
“Cats and Dogs”
Junior Books
“Pokemon Jr. The Wobuffet Village” “Mathamazing” by Raymond Blum “Under The Stars” by Ben Baglio “Trouble With Girls” by Ted Stanton “Stinky” by Ted Stanton “Breakout at the Bug Lab” by Ruth Horowitz “Franklin’s Music Lessons” “Franklin and the Magic Show”
New CDs
“Scooby Doo Showdown in Ghost Town” “Kid Pix 3”
“Ocean Voyager”
“JumpStart Explorers”
“Improving Classroom Behavior” “Compton’s Encyclopedia 2000”
LIBRARY HOURS
Tuesdays 9:30 am -12:30 pm
.............1:30 pm -5:00 pm
Thursdays.........9:30 am - 12:30 pm
1:30 pm - 5:00 pm 6:30 pm - 8:00 pm Phone and Fax: 599-3850 email messages to libmil@chinookarch.ab.ca
FAMILY HAIRSTYLING
X 599-2491 X
MILO
HOURS TUES ■ FRI 9:00-5:00 SAT 10:00-2:00
WED. Mens walk in 9:00-12:00
NOTICE: Trends will be closed from March 11 to the 29th, and also on April 10th and 11th.
We apologize for any inconvenience.
Waxing, Eyelash & Eyebrow TintingSHARE the NEWS
MILO CORRESPONDENT for NEWS only
L. STUMPF 599-3748
NEW PUBLISHING DATE: THURSDAYS Deadline remains the same: Friday at 4:00 p.m.
CALL WANDA - 485-2036
COUNTY CALENDAR
Coming Events for non-profit groups Call Economic Development - 485-2992
CLASSIFIED AD RATES
$7.42 for 20 words + ,10p each additional word 2nd week half price (Minimum $4.45)
Phone. 485-2036 • Fax: 485-6938 Web site: www.vulcanadvocate.com
Customer Service
CHECK OUR WEBSITE!!
See the Classifieds, News and Photos on-line! www.vulcanadvocate.com
rnwr
the (rAP
and SHARE the NEWS
Bernice Finlay
main§vulHnadvocale.com
SUBSCRIPTION RATE
$25.00 per year (within county)For Those Who Grieve the Loss of a Loved One
It is hard to remain in the world and not feel a part of it. To watch others rush about like nothing has changed \$hen everything has.
Grief makes you feel alone.
Yet you do have a place in the world And others who care for you.
There is still beauty and meaning in life And you will find it again,
For now, it is enough to rest, to mourn,
And wrap yourself in your memories.
The world can wait.
A MOTHER’S WISH,
Oh, give me patience when the little hands Tug at me with their ceaseless demands.
Oh, give me gentle lips and smiling eyes And keep my mouth from hasty, sharp replies. Oh, let me not in weariness, confusion or noise Obscure life’s vision from its fleeting joys.
And when in years to come my house is still No bitter memories its rooms may fill.
Cl
-b_
a,
|h
5
P
p
\
1In
Jbu
.JL-
._U
?
T
X
v
X
\K
ID-
□
r
s
V
11
V
t]
£-
n 1 j
IX-
S-
o
p
UV
0
X
r
A_
nr
c
b-
p
a
P
In
31
JL
\ L
0
_y_
X-
b
m
a
^ r
Oh
hQ-
o
■P
P
<°
n
_s_
V 1
p
b_
_V_j
P
t
y
-* i
n
r
n
SI
Ul
a
JZj
b_
cv
q
^r
r
X
P
V |
V-
b
b
u
i.
_C_
i
Q
Z
c -
A
AJ
X.
\
b
lb
g
o_
_o__
c
\
T-
4
10
q
P
b.
a
b
$
-b
Q_
\
","Milo Canopener (April 1, 2003)",,Milo Community Volunteers,,,core
199742415,2007-11-27T00:00:00,"背景
高膽固醇血症會使冠狀動脈疾病、動脈硬化及中風的相對危險性大幅增加，而異型接合子家族性高膽固醇血症(heterozygous familial hypercholesterolemia，FH)是導致高膽固醇血症其中一個常見而重要的原因。家族性高膽固醇血症主要肇因於低密度脂蛋白受體(LDL receptor)或其配體(LDL receptor ligand) Apo-B 100，以及一新近發現之PCSK9基因變異，使低密度脂蛋白膽固醇(LDL cholesterol)代謝異常，臨床上常以高膽固醇血症、黃瘤(xanthoma)及早發性心肌梗塞來表現。家族性高膽固醇血症是屬於體染色體顯性(autosomal dominant)的遺傳性疾病，同型接合子家族性高膽固醇血症(homozygous FH)相當罕見，發生率約為百萬分之一，多半於年幼時即因併發心血管疾病而死亡；而異型接合子家族性高膽固醇血症則較為常見，據估計約每五百人中就有一人帶有相關的基因突變，其中約有一半在50歲之前就可能發生心血管疾病。因此，早期診斷家族性高膽固醇血症，並給予適當飲食及藥物治療，可以降低該族群日後發生心血管病變的機會，對於疾病預防及減少社會醫療成本上有相當大的幫助。目前關於台灣人家族性高膽固醇血症基因突變的研究仍然十分缺乏，本研究的目的在找出台灣地區華人家族性高膽固醇血症基因變異型式，並評估其心血管功能受到的影響。

目標，材料及方法
第一部份是關於家族性高膽固醇血症患者的基因變異研究，我們從台大醫院高血脂門診中篩選出符合家族性高膽固醇血症的患者作為指引個案，萃取指引個案及其一等親以內家屬之血液中淋巴球之DNA進行分析。先對包含低密度脂蛋白受體基因(LDLR)，Apo-B 100及PCSK9基因在內共33 DNA片段進行聚合酶鏈反應（polymerase chain reaction, PCR），以變性高效液相層析技術（DHPLC, denaturing high performance liquid chromatography）分析PCR產物中可能發生的相關突變或單核苷酸基因多形性（SNP, single nucleotide polymorphism），並對可能發生突變的DNA片段進行序列分析，來確認其基因變異的型式。
第二部份是評估家族性高膽固醇血症對人體心血管功能的影響。我們為所有收案之FH患者於臨床診斷時執行心血管功能檢查，包含頸動脈之內膜中膜層厚度(IMT, intima-media thickness)，肱動脈之內皮依賴性血管舒張功能(FMD, flow-mediated dilation)，心臟超音波之Tei指標，都普勒組織成像 (DTI, Doppler tissue imaging)及左心室舒張功能評估(LV diastolic function)，並與非高膽固醇血症之對照組個案比較，以評估家族性高膽固醇血症對其心血管功能之影響。
結果及結論
我們一共完成18個高膽固醇血症家族的基因及心血管功能分析，其中有11個家族(61.1%)確定帶有LDLR，APOB或PCSK9 gene的突變，包含了LDLR的6種突變(其中delAT1954及G&gt;C1586+5兩者為novel mutation)及APOB的兩個點突變(其中T3540M為novel mutation)。LDLR的基因變異方面，雖有delAT1954及G&gt;C1586+5兩個novel mutation分別在兩個不同家族被發現，但因家族數目仍佔少數，不能證明此即為台灣家族性高脂血症家族的主要突變。另外，我們找到的APOB突變，其中R3500W即存在於兩個家族，而另一個西方最常被報告的R3500Q反而完全未發現，此結果與過去師大篩檢台灣一般高血脂病患中發現R3500W的發生率遠高於R3500Q (2.4% vs 0.3%)的結果相吻合。
心血管功能分析方面，我們從18個家族中一共選出60位FH患者與另外年齡姓別配對之60位非高膽固醇血症對照組進行比較分析，在排除罹患有冠心症的患者之後，兩組之間的低密度膽固醇有顯著差異(198.8±46.8 vs 115.4±29.0 mg/dl, P&lt;0.001)，其餘心血管危險因子並無差異，而IMT平均值在FH患者明顯比對照組增厚(62.0±14.1 vs 54.1±10.7μm, P=0.009)，FH患者也比控制組有較高比例發生心肌舒張期功能異常(29.8 vs 10.5%, OR=3.6, P=0.013)。FH患者比控制組雖然平均FMD值較低，但未達顯著差異(16.7±15.3 vs 18.3±10.1%, P=0.623)；另外，兩組之間的Tei index也沒有明顯差異(0.40±0.11 vs 0.38±0.11, P=0.567)。
總結我們的研究，台灣的家族性高膽固醇血症患者，約有六成帶有LDLR、APOB或PCSK9基因之突變，目前並無證據顯示LDLR mutation在台灣人中有founder effect；另外，台灣人的APOB突變，R3500W遠較R3500Q常見，這一點與西方人有相當大的不同。高膽固醇血症會使頸動脈內膜中層厚度顯著增厚，也代表未來罹患心血管疾病的危險性增加；另外，高膽固醇血症患者相較一般人有較高的危險性罹患心肌舒張功能異常，這也是第一個前瞻性研究證明高膽固醇血症對人體心肌舒張功\能具有不良影響。【Background】
Genetic Characterizations of ADH patients 
Autosomal dominant hypercholesterolemia (ADH) is an inherited disorder of cholesterol metabolism characterized by a high concentration of plasma LDL-C, deposition of cholesterol in tendons and skin, and increased risk of premature coronary heart disease (CHD). ADH is most commonly caused by mutations in the LDL receptor (LDLR) gene, which can lead to reduced hepatic clearance of LDL from the blood. The estimated prevalence of LDLR gene mutation is 1 per 500 in its heterozygous form. To date, more than 800 mutations for the LDLR gene have been reported. ADH can also be caused by certain mutations in apolipoprotein B (APOB) gene, which encodes the ligand for LDLR, named familial defective apolipoprotein B (FDB). FDB occurs with a prevalence of 1 per 1000 in most populations. Until recently, a third locus responsible for ADH (FH3) was identified at 1p34.1-p32 in several large ADH kindreds without mutations in LDLR or APOB genes. The proprotein convertase subtilisin/kexin type 9 (PCSK9) gene, localized to the third FH locus, has been proposed to be the third gene with pathogenic mutations accounting for ADH. PCSK9 encodes neural-apoptosis -regulated convertase (NARC-1), a novel protein that may play a crucial role in cholesterol homeostasis, though the exact molecular mechanisms are still obscure. 
Although heterozygous ADH is presumed to be a common disorder resulting in atherosclerosis in Asians, there is very limited epidemiologic and genetic data with regard to Taiwanese ADH patients. To establish the molecular basis of ADH in Taiwan, we investigated the genes of LDLR, APOB and PCSK9 for mutations in Taiwanese ADH patients.
Hypercholesterolemia has been recognized as a major risk factor for the development of atherosclerosis and coronary heart disease. There have been abundant evidences suggesting hypercholesterolemia may impair endothelial function, accelerate the progression of atherosclerosis, and ultimately increase the risk of ischemia in myocardium and many other end organs. In this study, we would like to evaluate the effect of hypercholesterolemia on cardiovascular system, with particular interest on endothelial function, intima-media thickness and myocardial function, in patients with ADH 
【Aims, Materials and Methods】
Patients 
Patients attending the Lipid Clinic at National Taiwan University Hospital (NTUH), diagnosed as ADH were recruited in our study. The diagnostic criteria of ADH included (1)fasting plasma total cholesterol and LDL-C levels above 95th percentiles for adult Taiwanese after adjust with age and gender and triglycerides&lt;220 mg/dl (2.5 mmol/l), and (2) presence of tendon xanthomata/xantholesma/corneal arcus or premature CHD in index case or first degree relative, or a family history of hypercholesterolemia consistent with an autosomal dominant inheritance. Patients with secondary causes for hypercholesterolemia, such as hypothyroidism, renal or hepatic disease, were excluded. 
Lipid measurements
Blood samples from fasting patients without concurrent lipid-lowering therapy were obtained for measurements. The concentration of plasma total cholesterol, high-density lipoprotein cholesterol (HDL-C), and triglycerides (TG) were determined with commertially available kits (Boehringer Mannheim). LDL-C was estimated with the aid of the Friedewald formula . 
DNA Preparation
Genomic DNA was isolated from EDTA whole blood with the Puregene DNA Isolation Kit (Gentra Systems, Inc., Minneapolis, USA) according to the manufacturer’s instructions.
PCR 
PCR amplification of the LDLR gene (including the promoter, 18 coding exons and flanking intron regions), APOB gene (the exon 26 of APOB gene containing condons 3473-3561), and the PCSK9 gene (the 12 exons and flanking intron regions) were performed with primers provided in Table 1. Each PCR mixture, with total volume of 25 mL, contained 50 ng of genomic DNA, 0.12 mM of each primer, 100 mM dNTPs, 0.5 units of AmpliTaq GoldTM enzyme (PE Applied Biosystems, Foster City, USA), and 2.5 mL of GeneAmp 10X buffer II (10 mM Tris-HCl, pH = 8.3, 50 mM KCl), in 2 mM MgCl2 as provided by the manufacturer. Amplification was performed in a multiblock system (MBS) thermocycler (ThermoHybaid, Ashford, UK). PCR amplification was performed with an initial denaturation step at 95℃ for 10 min, followed by 35 cycles consisting of denaturation at 94℃ for 30 sec, annealing at 55-57℃ for 60 sec (specific annealing temperature for each PCR product are listed in Table 1), extension at 72℃ for 30 sec, and then a final extension step at 72℃ for 10 min. 
DHPLC Analysis 
Mutation analysis was performed on a Transgenomic Wave Nucleic Acid Fragment Analysis System (Transgenomic Inc., San Jose, USA). Denaturing high performance liquid chromatography (DHPLC) was carried out on automated HPLC instrumentation equipped with a DNASep column (Transgenomic Inc.). DHPLC-grade acetonitrile (9017-03, JT Baker, Phillipsburg, NJ) and triethylammonium acetate (TEAA, Transgenomic Inc.,Crewe, UK) were used to constitute the mobile phase. The mobile phases comprised 0.05% acetonitrile in 0.1 M TEAA (eluent A) and 25% acetonitrile in 0.1 M TEAA (eluent B). For heteroduplex detection of crude PCR products, subjected to an additional 3-min 95℃ denaturing step followed by gradual reannealing from 95℃ to 65℃ over a period of 30 min prior to analysis, were eluted at a flow rate of 0.9 mL/min. The start- and end-points of the gradient obtained by mixing eluents A and B and the temperature required for successful resolution of heteroduplex molecules, were deduced from the WAVEmaker system control software version 4.1.42 (Transgenomic Inc.). Eight microliters of PCR product was injected for analysis in each running. The DHPLC temperatures for each PCR product are listed in Table 1. Heterozygous profiles were identified by visual inspection of the chromatograms on the basis of the appearance of additional earlier eluting peaks. Corresponding homozygous profiles show as only one peak. 
Direct Sequencing Analysis 
PCR products from index cases of ADH who showed abnormal DHPLC heteroduplex pattern compared with controls were sequenced. Amplicons were purified by solid-phase extraction and bidirectionally sequenced with the PE Biosystems Taq DyeDeoxy terminator cycle sequencing kit (PE Biosystems) according to the manufacturer’s instructions. Sequencing reactions were separated on a PE Biosystems 373A/3100 sequencer.
Carotid Artery Intima-Media Thickness (IMT)
With the use of a high-resolution B-mode ultrasonography (7.5MHz real-time B-mode scanner with HP SONO 1000 ultrasound system), we obtained 2 measurement of IMT on the far wall of both R’t and L’t CCA along a 1 cm section proximal to the carotid bulb. Carotid IMT was defined as the distance from the leading edge of the first echogenic (bright line) to the the leading edge of the second echogeinc line. Two measurements were done on each side of the CCA among 60 ADH patients and another 60 age-gender matched normocholesterolemic conrols. 
FMD (flow-mediated dilatation) of Brachial Artery
Using a high-resolution B-mode ultrasonography (7.5MHz real-time B-mode 
scanner with HP SONO 1000 ultrasound system), we obtained two measurement of brachial a. diameter in right antecubital fossa at baseline. Then we inflated blood pressure cuff up to 200mmHg or 50mmHg above baseline systolic BP to compress brachial artery for 5 minutes. After then, we release the cuff and measure brachial a. diameter again. FMD was defined as the percentage change of the post-compression brachial artery diameter from baseline. 
Echocardiography Evaluation of the Myocardial Function
All studies were performed with a Hewlett-Packard Sonos 1000 system, with a 3.5 mHz duplex probe. Standard 2-dimensional and color flow Doppler images were obtained in all patients in the parasternal short-axis and apical views. Each patient underwent LV myocardial function assessment by conventional Doppler, tissue Doppler imaging (TDI), and calculated myocardial performance index (Tei index).
In conclusion, the genetic background of Taiwanese ADH patients is highly heterogeneous, consisting of a variety of different mutations in LDLR and APOB genes. However, there may exist some common mutations responsible for a significant portion of ADH population in Taiwan. The mutations of the PCSK9 gene seem not to play a significant role to cause ADH in Taiwanese. These observations reflect the heterogeneous ethnic origins of Taiwanese and a characterized mutation pattern that is different from those in other countries. A larger screening program is required to clarify the epidemiological features of ADH in Taiwan. In vitro expression study is also needed to confirm the functional implication of the newly identified mutations in ADH patients. We also demonstrated that hypercholesterolemia in FH subjects can lead to an increased carotid artery IMT and a higher risk of myocardial diastolic dysfunction.封面					 P 	01
口試通過證明				 P 	02
致謝					 P 	03
目錄					 P 	04
中英對照表				 P 	05
一、中文摘要				 P 	07
二、緒論					 P 	09
第一部分：家族性高膽固醇血症簡介　　		 P	10
第二部分：體顯性遺傳高膽固醇血症　　　　　　	 P 	11
第三部分：高膽固醇血症與心血管功能變化　	 P 	13
第四部分：本實驗研究目的			 P 	14
三、研究方法與材料				 P 	16
四、結果					 P 	20
第一部分：家族性高膽固醇血症的基因變異分析　　 P 	21
第二部分：家族性高膽固醇血症的心血管功能分析	 P 	23
五、討論					 P 	24
六、展望					 P 	29
七、論文英文簡述				 P 	30
八、參考文獻				 P 	38
九、圖表					 P 	4",Familial Hypercholesterolemia Genetic and Functional Studies in Taiwanese Population,,,,,core
53777447,2010-01-01T00:00:00,"In the paper, a rule-based (RB) control strategy is proposed to optimize on-board energy management on a Hybrid Solar Vehicle (HSV) with series structure. Previous studies have shown the promising benefits of such vehicles in urban driving in terms of fuel economy and carbon dioxide reduction, and that economic feasibility could be achieved in a near future. 

The control architecture consists of two main loops: one external, which determines final battery state of charge (SOC) as function of expected solar contribution during next parking phase, and the second internal, whose aim is to define optimal ICE-EG power trajectory and SOC oscillation around the final value, as addressed by the first loop. 

In order to maximize the fuel savings achievable by a series architecture, an intermittent ICE scheduling is adopted for HSV. Therefore, the second loop yields the average power at which the ICE is operated as function of the average values of traction power demand and solar power. Expected solar contribution can be estimated starting from widely available solar databases and by processing past solar energy data measured on the vehicle. Neural Networks predictors, previously stored data and/or GPS derived information are suitable to estimate average power requested for vehicle traction. 

Extensive simulation analyses were carried out to test the performance of the RB algorithm, also comparing it to Genetic Algorithms-based optimization strategies previously developed by the authors. The results confirm the high potentialities offered by the proposed RB control strategy to perform real-time energy management on hybrid solar vehicles. 

The proposed rule-based optimization is currently under-implementation in an NI® cRIO control unit, thus allowing to perform experimental tests on a real HSV prototype developed at University of Salerno",Rule-Based Optimization of Intermittent ICE Scheduling on a Hybrid Solar Vehicle,,'SAE International',10.4271/2009-24-0067,,core
4752017,2002-01-01T00:00:00,"©2002. American Association for Artificial Intelligence. The original publication is available at: www.aaai.orgPresented at the AAAI Mobile Robot Competition Workshop (AAAI-02), 28 July-1 August 2002 in Edmonton, Alberta, Canada.We describe our entry in the AAAI 2002 Urban Search and
Rescue (USAR) competition, a marsupial team consisting of
a larger wheeled robot and several small legged robots, carried
around by the larger robot. This setup exploits complimentary
strengths of each robot type in a challenging domain.
We describe both the hardware and software architecture, and
the on-board real-time mapping which forms the basis of accurate
victim-localization crucial to the USAR domain. We
also evaluate what challenges remain to be resolved in order
to deploy search and rescue robots in realistic scenarios",The Georgia Tech Yellow Jackets: A Marsupial Team for Urban Search and Rescue,https://core.ac.uk/download/4752017.pdf,AAAI Press,,,core
20642380,2008-03-31T09:54:28,"Ad hoc wireless multi-hop networks (AHWMNs) are communication networks that consist entirely of wireless nodes, placed together in an ad hoc manner, i.e. with minimal prior planning. All nodes have routing capabilities, and forward data packets for other nodes in multi-hop fashion. Nodes can enter or leave the network at any time, and may be mobile, so that the network topology continuously experiences alterations during deployment. AHWMNs pose substantially different challenges to networking protocols than more traditional wired networks. These challenges arise from the dynamic and unplanned nature of these networks, from the inherent unreliability of wireless communication, from the limited resources available in terms of bandwidth, processing capacity, etc., and from the possibly large scale of these networks. Due to these different challenges, new algorithms are needed at all layers of the network protocol stack. We investigate the issue of adaptive routing in AHWMNs, using ideas from artificial intelligence (AI). Our main source of inspiration is the field of Ant Colony Optimization (ACO). This is a branch of AI that takes its inspiration from the behavior of ants in nature. ACO has been applied to a wide range of different problems, often giving state-of-the-art results. The application of ACO to the problem of routing in AHWMNs is interesting because ACO algorithms tend to provide properties such as adaptivity and robustness, which are needed to deal with the challenges present in AHWMNs. On the other hand, the field of AHWMNs forms an interesting new application domain in which the ideas of ACO can be tested and improved. In particular, we investigate the combination of ACO mechanisms with other techniques from AI to get a powerful algorithm for the problem at hand. We present the AntHocNet routing algorithm, which combines ideas from ACO routing with techniques from dynamic programming and other mechanisms taken from more traditional routing algorithms. The algorithm has a hybrid architecture, combining both reactive and proactive mechanisms. Through a series of simulation tests, we show that for a wide range of different environments and performance metrics, AntHocNet can outperform important reference algorithms in the research area. We provide an extensive investigation of the internal working of the algorithm, and we also carry out a detailed simulation study in a realistic urban environment. Finally, we discuss the implementation of ACO routing algorithms in a real world testbed",Adaptive routing in ad hoc wireless multi-hop networks,https://core.ac.uk/download/20642380.pdf,,,,core
11339876,2003-09-01T00:00:00,"The development of the Virtual Reality Modelling Language (VRML) for the Internet has resulted in the emergence of a multiplicity of 3D web sites.  The metaphor used by these sites varies enormously from virtual galleries to virtual cities and style varies from abstract to reality.  Additionally these worlds are populated by virtual objects, some having reactive or interactive properties, including movement, audio, video, databases, artificial intelligence etc.  Perhaps the most stimulating embodiment of these new environments are those that offer the participant the opportunity to meet and communicate with other visitors exploring the same virtual space/world.  The Glasgow Directory is an established 3D web space,

with around 10,000 visitors per year.  The model represents approximately 10,000 properties in the city and is populated by contextual information on its culture and socio-economic topography.  This paper describes the background to this VR space, and suggests a set of design criteria for successfully deploying multi-user software within this and similar

environments. These criteria take into account lessons learned by ‘observing’ and analysing how participants interact with the existing system under different conditions and also what benefits they perceive on entering the environment via the multi-user interface.  These recommendations will hopefully be applicable to a wide spectrum of internet virtual environment builders and users",Visit VR Glasgow : Welcoming Multiple Visitors to the Virtual City,,,,,core
20841623,03/12/2008,"Traffic congestion is one of the leading causes of lost productivity and decreased standard of living in urban settings. Recent advances in artificial intelligence suggest vehicle navigation by autonomous agents will be possible in the near future. In a previous paper, we proposed a reservation-based system for alleviating traffic congestion, specifically at intersections. This paper extends our prototype implementation in several ways with the aim of making it more implementable in the real world. In particular, we 1) add the ability of vehicles to turn, 2) enable them to accelerate while in the intersection, and 3) augment their interaction capabilities with a detailed protocol such that the vehicles do not need to know anything about the intersection control policy. The use of this protocol limits the interaction of the driver agent and the intersection manager to the extent that it is a reasonable approximation of reliable wireless communication. Finally, we describe how different intersection control policies can be expressed with this protocol and limited exchange of information. All three improvements are fully implemented and tested, and we present detailed empirical results validating their effectiveness. 1",Multiagent Traffic Management: An Improved Intersection Control Mechanism,,,,,core
61870513,2007,"This paper focuses on the results of different consumer surveys conducted between 2004 and 2006 with regard to consumers’ perceptions and reactions concerning AI in Vietnam, (mainly in Hanoi). The main results observed are as follows: A high proportion of consumers consider AI to be a food-related risk. However, over time,there has been a slight shift from a fear of consuming poultry to a fear of preparing it (slaughtering it). AI has had a profound effect on poultry consumption, even outside peak crisis times, more in terms of the quantity consumed (approximately a third less in 2006) than in terms of the number of consumers (6% less). Blood and internal organs are considered particularly risky, while eggs are viewed as being safer. Poultry from industrial farms is considered to be more risky than poultry from small farms. Purchasing practices have also been affected by AI: in Hanoi, consumers declare that they prefer to buy poultry directly from producers that they know, or from supermarkets in the case of the wealthiest consumers. A high proportion still buy live poultry from market traders, but more consumers now ask sellers to slaughter it for them. With a view to lessening market shocks in the wake of the crisis while maintaining the priority of consumer safety, a number of measures should nevertheless be implemented: Risk communication should not over-emphasize AI as a food-related risk. Reliable safe distribution channels should be promoted (with reliable quality signs and controls) in order to encourage safe production and poultry consumption. Otherwise, a market recovery will only benefit supermarkets and large-scale farmers capable of supplying supermarkets. As numerous live birds are still slaughtered in urban market places, facilities should be provided for safe slaughter. At the same time, more attention should be paid to the provision of a real “cold chain” with a view to promoting the sale of slaughtered poultry",Consumer perceptions and reactions concerning AI,,,,,core
15351700,2001,"The development of the Virtual Reality Modelling Language (VRML) for the Internet has resulted in the emergence of a multiplicity of 3D web sites. The metaphor used by these sites varies enormously from virtual galleries to virtual cities and style varies from abstract to reality. Additionally these worlds are populated by virtual objects, some having reactive or interactive properties, including movement, audio, video, databases, artificial intelligence etc. Perhaps the most stimulating embodiment of these new environments are those that offer the participant the opportunity to meet and communicate with other visitors exploring the same virtual space/world. The Glasgow Directory is an established 3D web space, with around 10,000 visitors per year. The model represents approximayely 10,000 properties in the city and is populated by contextual information on its culture and socio-economic topography. This paper will describe the background to this VR space, and suggest a set of design criteria for successfully deploying multi-user software within this and similar environments. These criteria will take into account lessons learned by'observing'and analysing how participants interact with the existing system under different conditions and also what benefits they perceive on entering the environment via the multi-user interface. These recommendations will hopefully be applicable to a wide spectrum of internet virtual environment builders and users.",VRGLASGOW.CO.UK implementation of internet multi-user functionality to Glasgow\u27s virtual city,,,,,core
15357530,2005,"1.1. The themes in this theses 16 1.1.1. Mind the mind gap 16 1.1.2. Prologue: The World Center for Human Concerns 17 1.1.3. Creative computer use 26 1.1.4. Design strategies and techniques 31 1.2. Overview 33 1.2.1. Main issues 34 1.2.2. The material 36 1.2.3. The framework of this thesis 37 2. CURRENT STATE AND BACKGROUND 39 2.1. New tools, old thoughts. 39 2.1.1. A misuse strategy 44 2.1.2. Emergence in design 47 2.1.3. Programming and design 50 2.1.4. Artificial intelligence 53 2.1.5. Human intelligence and artificial representations 53 2.2. Electronic dreams 54 2.2.1. The dream of intuitive software 55 2.2.2. The dream of the designing machine 60 2.2.3. The dream of self-emerging architecture; genetic algorithms in design 61 2.2.4. A cultural lag 62 2.3. Ideas and ideology 64 2.3.1. A personal perspective on the theories of the 1990s 65 2.3.2. ""The suffering of diagrams"" 68 2.3.3. Architectural theory and design methodology 69 2.4. Ideas on creativity 72 2.4.1. What is creativity? 73 2.4.2. Creativity, a cultural phenomenon. 75 2.4.3. Creativity in the information age 79 2.4.4. Creativity-enhancing techniques 81 2.4.5. Crucial fiicro-cultures 82 2.4.6. A proposal for a practitioner approach to creativity 83 2.5. Summary and conclusion of part 2 84 3. NEW DESIGN TECHNIQUES 86 3.1. Introduction 86 3.2. New technology - new strategy 87 3.3. Thinking through design practice: the inspirational playful design approach 88 3.4. A Corner stone: emergence 89 3.4.1. The source material 94 3.5. Recoding, translation and interpretation 95 A case: Tidsrom 97 3.6. Reconfiguring schemata 109 3.7. Rules and games 113 3.8. Virtuality and virtual models 118 3.8.1. What is ""The Virtual""? 118 3.8.2. Virtual reality 119 Investigating ""the virtual"" 120 3.8.3. Analysing the virtual 126 3.9. Visual thinking (diagrams and visual thinking) 130 3.9.1. Visual Thinking and Abstraction. 130 3.9.2. A heuristic process 132 3.9.3. Visual thinking, skills and tacit knowledge 132 3.9.4. Media for visual thinking 133 3.10. Diagrammatic thinking 138 3.10.1. Descriptive diagrams 142 3.10.2. Generative diagrams 144 3.10.3. Versioning 149 3.10.4. Finding 153 3.10.5. Translation and interpretation 158 3.10.6. From generative diagram to program 168 3.10.7. Dynamic generative diagrams 171 3.11. The question of selection 175 3.12. Summary and conclusion of part 3 178 4. WAYS OF WORKING: FROM DESIGN PRACTICE TOWARDS THEORY AND DIGITAL DESIGN METHODS 179 4.1. Introduction 179 4.1.1. Practice-based research 180 4.1.2. Visual material is central. 180 4.1.3. Two investigation paths 180 4.1.4. Achievements 180 4.2. Methods 181 4.2.1. Explorative and generative research 182 4.2.2. A first-person approach 183 4.2.3. Analysis 184 4.2.4. The Material 185 4.3. Systematising creative computer use. Ways of working; techniques in creative computer use. 186 4.3.1. Categorization 186 4.3.2. Mapping the field of design computing. 187 4.3.3. Generic techniques 190 4.3.4. Specific techniques 192 4.3.5. Table of techniques 193 4.3.6. Examples of techniques 200 4.3.7. Traces of technology. 213 4.4. The further use of the generated material 219 4.4.1. Realisation strategies 221 4.4.2. Templates and scaffolds 223 4.5. Summary of Part 4 240 PART 5. WAYS OF THINKING: INTENTIONS IN CREATIVE COMPUTER USE. 241 5.1. Intentions 241 5.1.1. Categorising intentions 242 5.2. Intention themes 243 5.2.1. Cases and samples from Group one: Formal, phenomenal, spatial and geometrical themes 244 5.2.2. Intentions of response to the complexity of urban systems 297 5.3. The Hybrid Process 317 5.3.1. Hybridization strategies 319 5.3.2. The hybrid process and its elements. 328 6. SUMMARY AND CONCLUSIONS 344 6.1. Principles, concepts and methods for creative design computing 344 6.2. A new type of creativity? 348 6.3. A practice as the field for an investigation 349 6.4. Suggestions for further studies 34",Developing Digital Design Techniques Investigations on Creative Design Computing,,,,,core
20806435,14/08/2008,"Abstract- This paper describes a method of teaching Agent Technology and Ad-hoc Networks using a novel, simple agent framework developed specifically for the purposes of teaching introductory Artificial Intelligence (AI) to undergraduate students. The agent framework in question is Java-implemented and it embodies the concepts of concurrency, multi-agency, persistency, and mobility. The introductory AI coursework in question is a set of assignments that requires the students to use intelligent agents to route SMS messages through an ad-hoc network. While many AI courses teach basic AI concepts such as heuristic search, rule-based reasoning, and neural networks, by means of programming assignments, we were not able to find any AI course introducing students to relatively new AI concepts such as distribution, mobility and ad-hoc networks by similar means. The coursework presented in this paper represents a synthesis of the traditional objectivist approach and a real-world oriented, constructivist approach to introducing students to these “hot topics”",Teaching Ad-hoc Networks Using a Simple Agent Framework,,,,,core
20930941,30/12/2008,"The Remus project aims at conceiving a simulation tool for both architectural and urban morphology, building a computer system using artificial intelligence tools, and computer graphics. Remus is made of a base of architectural knowledge, an expert system, and an interactive graphical environment for generating and displaying architectural objects. In this paper are presented new developments concerning evolution toward virtual reality models. 1- Context of the Remus project Urban simulation, either for historical or for forecast purposes, needs many different pieces of information to produce good quality realistic images. Often, this information came from heterogeneous software environments and are of different types. Some are not directly known but must be estimated or generated for simulation, in particular information concerning buildings and urban furniture. The problems we have to solve are the conflicting objectives of the system:- best rendering to display images giving satisfactory impression,- easiness of virtual walking- global realistic view because modeling of quarters, or even of whole a town, needs that views which are presented look like aerial photographs or perspectives found in architectural books.- faithfulness of architectonic elements which would be in agreement with reality a",Simulation of Architectural and Urban Morphology From automated 3D Modeling to virtual reality: problems and challenges,,,,,core
41213248,2009-01-01T00:00:00,"Teleoperation is a difficult task, particularly when controlling robots from an isolated operator station. In general, the operator has to solve nearly blindly the problems of mission planning, target identification, robot navigation, and robot control at the same time. The goal of the proposed system is to support teleoperated navigation with real-time mapping. We present a novel scan matching technique that re-considers data associations during the search, enabling robust pose estimation even under varying roll and pitch angle of the robot enabling mapping on rough terrain. The approach has been implemented as an embedded system and extensively tested on robot platforms designed for teleoperation in critical situations, such as bomb disposal. Furthermore, the system has been evaluated in a test maze by first responders during the Disaster City event in Texas 2008. Finally, experiments conducted within different environments show that the system yields comparably accurate maps in real-time when compared to higher sophisticated offline methods, such as Rao-Blackwellized SLAM.(Best Paper Award Finalist)Artificial Intelligence & Integrated Computer System",Operator-Assistive Mapping in Harsh Environments,,'Institute of Electrical and Electronics Engineers (IEEE)',10.1109/SSRR.2009.5424159,,core
235571392,2002-03-27T00:00:00,"An archive of the Magrath Trading Store News.The University of Lethbridge Library received permission from the Wes Balderson to digitize and display this content.MAGRATH
Published Weekly Since 1932 by
The Magrath Trading Company
50e 'J March 27,2002
Kiwanis Festival Results
The 72nd Annual Kiwanis Music and Speech Arts Festival held in Lethbridge each year
has a number of participants from Magrath. Individuals, choirs and bands from the
school participate each year. The following are results from the past week:
Saturday, March 16 - VOCAL SOLO ~ MUSICAL THEATRE 12 yrs. & under -
Jason Ragan - 88; Dallin Tagg - 86
Monday, March 18 - VOCAL SOLO ~ BOYS PRESCRIBED 16yrs. & under­Aaron
Ragan - 84
VOCAL SOLO - BOYS OWN CHOICE, 16 yrs. & under ­Aaron
Ragan - 83
Tuesday, March 19 - VOCAL SOLO ~ BOYS PRESCRIBED 12 yrs. & under -
Jason Ragan - 90
VOCAL SOLO - BOYS OWN CHOICE 12 yrs. & under -
Jason Ragan - 87
Wednesday, March 20 - VOCAL SOLO ~ FOLK SONG Boys 14 yrs. & under­Shawn
Ragan - 85
Thursday, March 21 - SCHOOL CHORUS ~ CONTEMPORARY Grades 4-6 -
Magrath Elementary School Senior Choir, Kathy Cosgrove - Excellent
SCHOOL CHORUS ~ FOLK SONG Grades 2-3 unison -
Magrath Elementary School Junior Choir, Kathy Cosgrove - Excellent
SCHOOL CHORUS ~ FOLK SONG Grades 4-6 -
Magrath Elementary School Senior Choir, Kathy Cosgrove - Excellent
VIOLINCELLO SOLO ~ OWN CHOICE 16 yrs. & under-
Jacqueline Tagg - 91
Points of Interest^
* Employment
* Seniors News
* Grocery Specials
Home Hardware j
Inside this Issue:
Community 1-3
5,7,9
Sports 4
Classified Ads 6
Calendar 8
Grocery Specials 10-11
^Hardware Specials 12>
VOCAL TRIO - Jason, Shawn and Aaron Ragan - 88
VOCAL TRIO ~ BALLAD/MOVIE/TV/POP
Jason, Shawn and Aaron Ragan - 90
VOCAL DUET ~ MUSICAL THEATRE 16 yrs. & under
Sarah-Jane Kulak/Craig Watson - 84
ALTO SAXOPHONE C SOLO Grade 6-7 Standard -
Aaron Ragan - 90
Friday, March 22
BRASS SOLO - GENERAL 14 yrs. & under -
Shawn Ragan - 85
SNARE DRUM A SOLO Grade 1-2 Standard -
Jason Ragan - 86
MALLETS A SOLO Grade 1-2 Standard -
Jason Ragan - 87
CELLO AND KEYBOARD SONATA 16 yrs. & under -
Jacqueline Tagg - 87 VOCAL SOLO ~
FOLK SONG Boys 12 yrs. & under -
Jason Ragan - 90
VOCAL SOLO ~ SACRED ~ CLASSICAL 12 yrs. &
under - Jason Ragan - 88
VOCAL SOLO ~ SACRED ~ CONTEMPORARY 6 yrs
& under - Karalynn Tagg - 85
VOCAL TRIO ~ SACRED -
Jason, Shawn and Aaron Ragan - 95
VOCAL SOLO - BALLAD/MOVIE/TV/POP 16 yrs. &
under - Craig Watson - 89
WOODWIND SOLO ~ GENERAL 16 yrs. & under -
Aaron Ragan - 87
! X
MISSIONARY CORNER [
Elder Michael Balderson, son of John & Ireta, is 1
returning from the Canada Halifax Mission on Friday,1
March 22. He will report on his mission on Sunday, *
March 31 at 9:00 a.m. in the Garden Place Chapel!
(1st Ward). t
PIES! PIES! PIES!
Senior's are making >
Delicious Turkey Pies
Each 4"" pie is $1.50
Orders must be placed by April 24th
The pies will be made on April 29* A 30*.
To order call: Mary Baker @ 758-3207
LaVaun Thompson @ 758-6672^/
□= ■ci
Garden City Greenhouses
Starter.
Begonias
=E
'''-•• Mix and matcri your colors to complete a flat!
$1.09
Starter
Geraniums
$1.50 '’’fr
•/
_______Share with a friend or neighbor.
Geranium Starter colors:
Red* White * Salmon
Pink * Violet * Pink Star
Order _
Mo th er'S Da ÿ Baskets Now
Starting at $12
Custom Made in 12, 16 or 20 in
baskets.
Great Gifts for mother &
Grandmother
Begonias, Impatiens, Geraniums, Petunias,
and more.
Jason and oathy Beck,
! 758-6566 | =□
foothills brass
from Calgary
will be performing in conjunction with
the
MAGRATH HIGH SCHOOL
COHORT BAH©
Wednesday, March 27th
7:00 p.m.
Tom Karren Gym
Tickets are:
$5/person $20/family
and can be purchased from any band
member or call Sandi McClung @ 758-378
I
(Ï)Dr-Alan w- Dudlev is pleased to announce
the opening of his
Chiropractic Office
2 S. 1st St. W.
(formerly Dr. Smart’s office)
758-3200 r.’
S
*•;». -r
. • *■*. \
ROJu .«
, • .... ? . *
3
Chad A Suzanne Thomson
are pleased to announce the arrival
of their baby son
William McGillivray Thomson
Born March 18, 2002
Weighing 6 lbs. 13 oz.
20 long
Lethbridge Regional Hospital
Proud Grandparents are:
Merrill & Lise Thomson
Anne-Marie Sera
Allan & Dorothy Sera of Coaldale
Great-Grandparents are:
Hanne Kristensen of Aalborg Denmark
Yvonne McGillivray of Coaldale
Edna Thomson of Raymond
Alex A Elizabeth Sera of Lethbridge
0=
3
f CARDS OF THANKS A
A heartfelt thank you is extended to all who have been so kind and thoughtful to Fern Cook and to us, her family.
We appreciated all the calls, cards, visits, the flowers and the generous donations of food.
To the 4th Ward - The Bishopric and the Relief Society - who took charge of the funeral and prepared and served the hot luncheon. To all those who gave of their time and talents. To all the extended family and friends who gave us comfort.
Thank You - Fern Cook’s family Norma, Martin, John, Floyd, Donna, Keith and families
On behalf of the Raymond Judo Club, I would like to thank those who made donations in support of the Katsuta Kup held in Raymond on March 23. The Magrath Trading Company, THE Store and Dr. Dan Anderson.
SENIOR’S NEWS
Potluck Friday, March 29 ~ 5:00 p.m. with Reagan Boys Entertainment after supper.
Our April suppers will be held at the Senior's Centre at 5:00 p.m.
April 3rd, IO*. & 17th Potluck Friday, April 26th
*******
Contact Hazel Rasmussen for rental of the Senior’s Centre. Fee $75.00 Phone 758-3545
‘c *******
BINGO NEWS
Next Bingo will be held on Thursday, April 4, 2002 Doors open at 6:30 p.m.
Bingo - 7:00 p.m. Public Welcome!
;>£ *******
Making Meat Pies ~ April 29-30 Come & HelplI
a
I B
In Your Town
MOBILE
Special thanks to the Magrath Lions Club for their generous donation. All of the support was helpful and very much appreciated.
Thanks again, Monica Zaugg President - Raymond Judo Club^
EMWOYBIEOT OPPORTUniTiet I
¡1 INLINE OVALS
Applications are now being accepted for the following foil-time positions at InLine Ovals:
, Air-operated Painting
Telephone Sales
Experienced applicants may phone Barb Smith or Shannon Sheridan @ 758-6278 or drop off applications to the InLine Ovals office.
George
Torre Alba
Denturist
*
Complete denture service
*
Soft Liners & stabilizing wings for problems, sore lowers
APRIL 8 - 6:00 p.m.
Diamond Willow Terrace
DOWN HOME SOCIAL
sponsored b/ibe MAGRATH HISTORY & MUSEUM ASSOC.
Dining & Entertainment $15 / plate Friday, June 7,,2002 ~ 7:00 p.m. Passey’s Covered Wagon R.V. Park
Seating is limited! For tickets call: Andrea Burrows 758-3843 ' ; Donna Lybbert 758-3896
Melody Johnston 758-6268
All proceeds to the Magrath Museum.r - SPORTS
L JUDD SWIMMING ____1
Katsuta Kup
The Raymond Judo Club hosted the Katsuta
Kup on March 23rd and some Magrath
Judokins participated.
Amberlea Zaugg - Gold (Juvenile Women
. • ;• -40kg)
Katlin Zaugg - Bronze (Girls -38kg)
Jordan Zaugg - Silver (Boys -34 kg)
Tanner Zaugg - 4th (Boys -28 kg)
Corbin Stringam - Gold (Boys -30 kg)
All of the above are from the Raymond Judo
Club.
Also, from the Lethbridge Judo Club:
Kathryn Sindlinger - Silver (-34kg)
Congratulations to all!
[ SOFTBALL
MAGRATH CO-ED SOFTBALL
Grades 4-7
$15
Registration forms will be available in the
school office after Easter.
Practices - Mondays, Games - Wednesdays
3:30 - 5:00 p.m.
Call Laurel Bennett for more info 758-6222.
TOWN OF MAGRATH
“THE GARDEN CITY""
2002 POOL STAFF
Applications are being accepted by the Town of
Magrath for the following swimming pool staff
positions for 2002.
Senior Guards and Junior Guards.
Qualifications:
Senior Guards: 18 years of age or over, Bronze
Cross, N.L.S., Senior Resuscitation, St. Johns
Ambulance First Aid or Aquatic Emergency Care or
equivalent, Aquaquest Water Safety Instructor.
Junior Guards: 16 years of age or over, Aquatic
Emergency Care or St. Johns Ambulance First Aid
or equivalent, Aquaquest Water Safety Instructor,
Senior Resuscitation, N.L.S., Bronze Cross.
Proof of current qualifications and age must
accompany all applications to be considered.
Closing date for all applications is Friday, April 26,
2002.
Town of Magrath
Box 520
Magrath, AB T0K1J0
Phone (403) 758-3212
F T-BALL :□
T-BALL REGISTRATION
Saturday, March 30
10:00 a.m. -12:00 p.m.
Pool Foyer
Boys & girls ages 5-9 years
$20
Call Leanne Sabey before April 6 @ 758-6846.
((
real estate - magrath
Call
John
Latham
az-ioGiaís. ír’ioé.E’L
y Phone 758-6060 Fax 758-9090 j >
Please submit your sports scores and schedules by dropping off at the Trading Company Office,
faxing to 758-6888, or e-mailing to tidmarsh@telusplanet.net
5
I
i
Pregnant?
Aching Back?
Headaches?
OR
Just uuant to relax? j
Coll Kim Aanczak - RMT j
libra Massage Therapy
758-3210
to book your appointment todoy I
185 € 1 RwNMaptth i
use east side entrance I
********t|^Qj********* L
Pregnancy massage cushions j
to moke your massage a more l
relaxing and enjoyable experience!! i
Deep tissue massage and
Relaxation massage L
also available
covered by most health care plans |
HomeLife
Higher Standards
M.L.S.
Jim Anderson
agent
Residential - Farm
Acreage ~ Commercial
in Magrath and Area
Mobile Home for Sale
68’ x 14’ - Needs to be moved from the property.
$12,000.
Comparative Market Analysis
(No Charge) - For people interested in getting an
evaluation of marketability of your property.
Phone: 758-6725 (leave message)
331-8882 (cellular)
We specialize in:
»Scanning: slides, photos, negatives, text...
»Photo repair, retouching, restoration &
photo collages
> Memory videos (weddings, reunions, sports...
All of the above can be preserved
on VHS, CD and w w v w
The Magrath Hospital Auxiliary would like
to report our total for
HEART & STROKE CANVAS
was $3558.95
We again would like to thank all our
volunteer workers and all who made
donations. We are very pleased with our
town & districts’ support for this worthy
cause.
Again our very sincere thanks to one & all.
Magrath Hospital Auxiliary
(phone for details)
We also do private tutoring, troubleshooting,
desktop publishing & more!
Phone Bonny/Brenda; 758-3844 eve./voicemail (day)
Now Open!!
Wakeup 'With ^take-up!
'Permanent Cosmetics Clinic
Tina D. Reid epet
Phone: (403) 758-3930
Book your Free Consultation
between March 1^ & March and
set your procedure done for % price.
CLASSIFIED ADS
FREE MARKET
SNOWMOBILES-Magrah 4“ Ward Young Men would like to beg, borrow or rent your snowmobile for a Father and Son outing this Saturday, March 30. Please phone Wally Holt at 758-3201.
❖
FOUND last Monday night after the Minor Hockey Banquet at the exit door by the Tom Karren Gym - Silver Necklace. Call Shainne Harker at 758-3311.
❖
UNCLAIMED Cookie Sheet - from the Family Dance at the school on February 11. To claim please call Lachelle @.758-3313.
❖
WWn BIOGRAPHY-Brook Harker is doing a WWB biography on Blair M. Harker. Anyone having photos, memorabilia, scrapbooks, stories of Blair and other Magrath veterans please contact Mildred' Harker at 758­3883, Lance or Mark Harker, or Brook Harker at 306­924-4743 or email harkerb@em.agr.ca
❖
TAKEN BY MISTAKE from the Alston reception. Medium dark green ladies trench coat. Please call 758­6438 to return. Yours is still at die church. Thank you.
REALESTATE
*******
FOR RENT
*♦* House for rent. 2 bedroom home in Welling. No smoking unit for abstainers. No pets. Fricke & stove included in rent Call 752-3848.
2 bedroom home with laundry room for rent $450.°° plus utilities. Call Tom 758-3490.
**’ 2 bedroom downstairs apartment for rent $385.°° plus half electric. Abstainers only. Call Faye 758-3822.
FOR SALE
* HOME FOR SALE-1360 sq. ft. bi-level.
4 bedrooms, 2 Vi baths, 2 fireplaces, recent pine flooring and linoleum. Double garage, greenhouses. Totally fenced, landscaped yard $112,000. Phone 758-6880.
HOME FOR SALE - 4 bedroom, 2 bath, developed basement landscaped, mature trees, underground sprinklers, concrete drive, 2 car garage, next to parks and school. Phone Cooks @ 758-3639.
AUTOMOTIVE
*******
❖ FOR SALE-1993 Honda Accord LX
4 door, 5 speed, CD player. Excellent condition, low mileage. Phone 758-6243 Jim & Dayna Blumel
BUSINESS
❖
Looking for day home for 4 year old boy. Must be able to take him to and from school (in Magrath). Please provide references. Call Tracy @ 758-6277.
❖
If your child needs help in Math or Reading the Kumon
Method might help. For more information call Rusty or Martine Rollingsan at 758-3648. .
❖
DO YOU KNOW WHERE YOUR KEYS ARE? Just moved in or lost a key? Re-keying a lock is inexpensive prevention. Call Canadian Security Systems 758-3945.
❖
For all your cleaning needs from hospital clean to a touch-up, carpet to ceiling & everything in between. No job too big or too small.
Call Wayne’s Carpet & Upholstery Cleaning 758-6414.
NORMAN HENRY LARSON
beloved husband of Mrs. Gladys Larson of Magrath, passed away at the Lethbridge Regional Hospital on Tuesday, March 26, 2002 at the age of 74 years.
Besides his loving wife Gladys, he is survived by his children Debbie (Barry) Cherniawski, Patti (John) Kloppenburg, Norm Jr. (Terry) Larson, Ron (Lavonne) Larson, Terri (Jeff) Phillips and Rick Larson, his stepchildren Chuck (Laura) Smith, Dan (Tami) Smith, and Nathan (Karen) Smith, and 16 grandchildren. He is also survived by 3 brothers as well as numerous other relatives, and friends.
He was predeceased by his parents and 1 sister.
Relatives and friends are invited to meet with the family at Cornerstone Funeral Home, 2800 Mayor Magrath Drive South, Lethbridge, on Saturday, March 30,2002 from 1:00 to 3:00 p.m.
At Norman’s request no service will be held. Cremation.7 .
Magrath Elementary Schools’ 5TUDENT OF THE (ONTH AWARDS (“larch 2002
GRADE 1 - MRS. COURT
Shenoa Lowry, Emily Kenney
GRADE 1 * MRS. HATCH
Phonenix Riggin, Mackenzie Beres, James Bennett, Tyler Ralph
GRADE 2 - MISS COLEMAN
Clayton Goodsell, Andrew Gast, Erica McMahon, Tanner Murphy, Jasmine Walburger, Colton Steed
GRADE 2 - MR. REDD
Cianna Eyre, Mia Harris, Zack McAllister, Matthew Larson
GRADE 3 - MRS. BOLT
Trevor Bevers, Christopher Gast, Robert Karren, Katlin Zaugg
GRADE 3 - MRS. TOLY
C.J. Murray, Evan Bowen, Teina Lowry, Kathryn Sindlinger, Taylor Evans
GRADE 3 - MRS. VAN MAARION
Shaelene Ascione, Kinzee Chipman, Kimberly Clifton, Madison Hatch, Quincee Hoy, Dallin Redd, Kyle Schad, Dillon Toone, Alysa Lybbert
GRADE 4 - MRS. KARREN
Darin Christensen, Chloe Cosgrove, Shane Eyre, Mikaela Fisher, Carson Keeler, Mandi Meldrum, Keaton Moore, Tyson Tondevold, Tannis Wilde, Seth Harris, Broghan Seear, Olivia Boyes, Casidy Helgeson, Christina Williams
GRADE 4 - MISS NEILSON
Christian McLaughlin, Cash Carlton, Ammon Rasmussen
GRADE 5 - MRS. COLEMAN AND MRS. COSGROVE Caleb Jackson, Taylor Boivie, T.J. Hatch, Julie Hatch
GRADE 5 - MR. DEWINTER
Indie Riggin, Russell Tricsll, Alora Bennett, Josh Bennett, Jenna Perks, Derek Clifton GRADE 5 - MRS. DORNER
Sarah Balderson, Max Beazer, Morgan Sorpold, Daylen Tidmarsh
GRADE 6 - MR. ALTSTON
Celestina Alston, Karisa Bingley, Natalie Boyes, Jeremy Charlesworth, Wade Bullock, Kennedy Russell, Natalie Wolsey
GRADE 6 - MRS. COPPIETERS
Gregg Karren, Reagan Rasmussen, Denton Henry, Trinda Sheridan, Bobby Yerxa Gemmell, Kristen Atwood, Jade Blumel -
The following was written by Audrey Hepburn I who was asked to share ""beauty tips.""
*
For attractive lips, speak words of kindness.
*
For lovely eyes, seek out the good in people. ,1
*
For a slim figure, share your food with the i
hungry. ■
*
For beautiful hair, let a child run his or her | fingers through in once a day.
*
For poise, walk with the knowledge that you ! never walk alone.
*
People, even more than things, have to be restored, renewed, revived, reclaimed and redeemed; never throw out anyone.
*
Remember, if you ever need a helping hand, you'll find one at the end of each of your arms. • As you grow older, you will discover that you have two hands, one for helping yourself, the : other for helping others.
*
The beauty of a woman is not in the clothes j she wears, the figure she carries, or the way ! she combs her hair. The beauty of a woman must be seen from in her eyes, because that is ! the doorway to her heart, the place where love j resides.
| * The beauty of a woman is not in a facial
; mode, but the true beauty in a woman is
| reflected in her soul. It is the caring that she ; lovingly gives and the passion that she shows. .
* The beauty of a woman grows with the passing years.
I
78 JU6&AW ATU’S
In Our Cmumuuity... Jlurch - April 2002
Sunday
Monday
Tuesday
Wednesday
Thursday
Friday
Saturday
24
■r ■
25
Mobile Denturist 6:00 Diamond Willow Terrace
26
27
Band Concert 7:00 p.m. Tom Karren
28
29
GOOD FRIDAY
Senior’s Pot Luck 5:00 p.m.
30
T-Ball Registration TOa.m.-noon Pool Foyer
31
EASTER SUNDAY
/
EASTER MONDAY
2
3
Senior's Dinner 5:00 p.m.
4
Bingo 7:00
5
6
SPRING BREAK-NO SCHOOL
-
■
LIBRARY NEWS
We have some books and a video that have been dropped in the book bin by mistake. If you are missing them please check with us.
If you are wanting to learn to use computers we are still offering computer tutoring. It is taught on a one to one ratio and you go at your own pace. There is no charge for these lessons. To find out more please check at the library. 758-6498
Adult Book
Videos
Snow Days
The BigjFreeze The Mexican Hanging Up .
The Oath by John Lescroart
The Hearing by John Lescroart
The Stone Monkey by Jeffrey Deaver
Lone Eagle by Danielle Steel
Final Target by Iris Johansen
Heart of a Warrior by Johanna Lindsey
Fade to Black by Wendy Corsi Staub Petrushevskaya The Time: Night by Ludmilla Petrushevskaya
- "" ~ —-=---------
Library Closed for the Holiday Friday, March 29 to
Monday, April 1
k» fi >
' •••*•; -Reopens on Tuesday, April 2 @ 3:30 p.m.
Adult Non-Fiction
The New Canada by Preston Manning Leduc, Manning & the Age of Prosperity 1946-1963
Garfield Bigger Than Life by Jim Davis “Herman, dinner’s served., .as soon as the smoke clears!” by Jim Unger
Absolutely Australia by Keith Garvey
The Beef Book by Jean Pare
Juvenile Books
Bunny’s Noisy Book by Margaret Wise Brown
(board book)
Sticks and Stones and Doggie Bones
by Debbie Dadey
The Case of Mall Mystery by Alice Leonhardt Ghosts Don’t Ride Wild Horses by Debbie Dadey That Bad, Bad Cat by Claire Masurel First He Made The Sun by Harriet Ziefert
Please note that the deadline for submissions to the paper is MONDAY at 6:00 p.m. Entries submitted after the deadline will be published the following week. Phone 758-6377, email to tidmarsh@telusplanet.net, fax 758­6888 or drop off your submissions at the Magrath Trading Company Office.ADULT EDUCATTON
>■
/ •>
-i
iI
Cardston & Magrath Area Computer Workshops
Note. To request additional workshop topics, please phone
Kathy Richards, Adult Ed. Coordinator at 653-4991
Instructors for the following courses: Bonny West and Brenda Beck
CARDSTON: :
Beyond Internet Basics-April
This 4-week course covers managing bookmarks and email, refining search techniques
customizing web browsers, downloading files, using plug-ins, Internet safety ’
issues/concems, etc. Participants should have basic familiarity with the Internet
Dates: Tuesdays, April. 9-30*, 2002 Time: 7-9:30 p.m.
Location: Cardston High School (John S. Smith Bldg.) Cost: $50
Intermediate Word Processing-Microsoft Word-May
This 4 week follow-up course to Word Basics will cover creating and editing tables
using columns, tabs, & style sheets, using Word’s drawing tools, adding headers, footers,
footnotes, working with dip art, using templates, mailing labels, indents, bullets &
special characters, adding web links, etc.
(Note. Microsoft Word Basics is NOT a prerequisite to this course. You may enroll if
you already have some facility with Word and want to extend your skills )
Dates: Tuesdays, May 7-28*, 2002 Time: 7-9:30 p.m.
Location: Cardston High School (John S. Smith Bldg.) Tuition: $50
MAGRATH:
Magrath: Scanning & Photo Retouching Basics-April
Participants will work with their own photos and learn proper scanning techninnps
for best results. Then participants will learn the basics of re-touching/enhancing photos
using photo-editing software. Photo retouching will cover: color enhancements, mending
tears in old photos, removing unwanted items-such as a power pole behind someone’s head
blemishes, tears and stains in heirloom or newer photos, etc. We will also look at photo
printing techniques, archiving photos on CD & scanner purchase tips.
Participants need to bring their own photos to work. (Practice ones can be provided)
Dates: Wednesdays, April 10-May 1st (4 weeks) Time: 7:00-10i00 p.m.
Location: Magrath Elementary School lab Tuition: $60
Make Your Own Desktop Movie-Mav
Using photos, slides, video clips, scanners, digital cameras and easy-to-use video­editing
software participants will make their own home video movie complete with
background music, transitions, special effects, & voice annotations. Participants will take
home a completed digital video “movie” on VHS tape. (These movies may also be uploaded
to the Web Or stored on CD.) A great idea for gift-giving, weddings, sporting highlight»;
video baby book, family reunions, etc.
Participants need to bring their own photographs, slides, video clips, blank videotape
and choice of background music on CD.
■j
i...
Dates: Wednesdays, May 8-June 12 (6 weeks) Time: 7:00-10:00 p.m.
Locations: Magrath Elementary School Lab Tuition: $90
10
Magrath. Trading Company
GROCERY SPECIALS
“From Our Family To Yours.
»
• ■
Dairy Delights and Frozen Favorites!
Green Giant Vegetables, frozen-select varieties
750 g-1kg
$3.48
Western Family PerOgies, frozen -select varieties
Ikg
2 for $5.00
Dairyland Yogurt -select varieties
•75 g '
3 for $1.98
Daiiyland Whipping Cream
500 ml
$1.98
McCain Cream Pie,Jrozen -select varieties
340 g
2 for $5.00
Breyer's Classic Ice Cream-select varieties
2 litre
$3.98
Groceries...
Western Family Pineapple-select varieties
398 ml
.88
Western Family Cranberiy Cocktail-select varieties
1.89 litre
$2.98
Western Fami","Magrath Store News (March 27, 2002)",,J. A. Ririe,,,core
224592847,2001-01-01T00:00:00,"A recent US DOT plan guiding IVHS research correctly notes that, ""Over the next 20 years, a nationalIVHS program could have a greater societal impact than even the Interstate Highway System"".  But what will those impacts be? What could they be?The primary thrust of current IVHS initiatives is to accommodate more vehicles more safely using existing roadspace. The principal focus is on two sets of technologies: 1) real-time information to manage traffic flows better; and 2) automated controls to pack vehicles closer together. A variety of other applications are also being pursued, including transit and goods movement, but are receiving much less attention and government resources. The benefits of current IVHS initiatives are coming under increasing scrutiny. It appears unlikely that deployment of IVHS technologies, other than automated vehicle controls, will lead to major congestion reductions or road capacity expansions (e.g., Hall, 1993; AI-Deek et al, 1989). Highway automation could provide large capacity improvements, but perhaps at a huge economic, environmental, and social cost (Burwell, 1993, Gordon, 1992, Johnston and Ceerla, 1994).The current thrust of IVHS activities, as indicated above, has its historical origins in the highway engineering community, it is described in detail in the 1993 Draft National Program Plan for IVHS prepared by IVHS AMERICA. One might extrapolate these unfolding IVHS initiatives into the future and treat them as one potential IVHS scenario. It is a scenario that could be described as a pragmatic attempt to guide the development and deployment of information and control technologies or, less charitably, as a reductionist engineering approach to the problem of congestion and safety.An alternative IVHS vision is proposed here. The overarching goal inspiring this vision is increased accessibility -- not mobility, that is, improved access to goods and services, but with little or no increase in vehicle travel. Three complementary goals, suppressed or ignored in current IVHS activities, are also fundamental to this alternative vision greater consideration of the less privileged, enhanced environmental quality, and community liability.Pursuit of these goals would lead to a very different transportation future than in the first scenario. Many of the same IVHS products would be commercialized and promoted in both scenarios, with the difference being that in this second scenarios government more actively supports products and activities that benefit lower income classes and the environment. Government marshals its R&amp;D resources, infrastructure investments, and rulemaking authority in such a way that goals of accessibility, equity, and environmental quality dominate the design of the overall system architecture. The many effects of IVHS technologies on travel behavior, land use patterns, vehicle acquisition decisions of households and businesses, and corporate logistical and facility location decisions are treated as primary impacts. The power of IVHS technologies to transform the urban and social landscape, similar to that of the Interstate Highway System, is acknowledged and harnessed",Intelligent and Environmentally-Sensible Transportation System: An Alternative Vision,,"eScholarship, University of California",,,core
102671191,1998,"Abstract. Off-road autonomous navigation is one of the most difficult automation challenges from the point of view of constraints on mobility, speed of motion, lack of environmental structure, density of hazards, and typical lack of prior information. This paper describes an autonomous navigation software system for outdoor vehicles which includes perception, mapping, obstacle detection and avoidance, and goal seeking. It has been used on sev-eral vehicle testbeds including autonomous HMMWV’s and planetary rover prototypes. To date, it has achieved speeds of 15 km/hr and excursions of 15 km. We introduce algorithms for optimal processing and computational stabilization of range imagery for terrain map-ping purposes. We formulate the problem of trajectory generation as one of predictive control searching trajectories expressed in command space. We also formulate the problem of goal arbitration in local autonomous mobility as an optimal control problem. We emphasize the modeling of vehicles in state space form. The resulting high fidelity models stabilize coordinated control of a high speed vehicle for both obstacle avoidance and goal seeking purposes. An intermediate predictive control layer is introduced between the typical high-level strategic or artificial intelli-gence layer and the typical low-level servo control layer. This layer incorporates some deliberation, and some envi-ronmental mapping as do deliberative AI planners, yet it also emphasizes the real-time aspects of the problem as do minimalist reactive architectures",Terrain Autonomous Mobility - Part 2: An Active Vision,,,,,core
24439433,06/02/2008,"Traffic congestion and automobile accidents are two of the leading causes of decreased standard of living and lost productivity in urban settings. Recent advances in artificial intelligence and, specifically, intelligent vehicle technology suggest that vehicles driven entirely by autonomous agents will be possible in the near future. In previous work, we presented a novel reservation-based approach for governing interactions of multiple autonomous vehicles, specifically at intersections. This approach alleviated many traditional problems associated with intersections, in terms of both safety and efficiency. However, such a system relies on all vehicles being equipped with the requisite technology — a restriction that would make implementing such a system in the real world extremely difficult. In this paper, we augment the system such that it is able to accomodate traditional human-operated vehicles using existing infrastructure. Furthermore, we show that as the number of autonomous vehicles on the road increases, traffic delays decrease monotonically toward the levels exhibited in the system involving only autonomous vehicles. Additionally, we demonstrate how the system can be extended to allow high-priority vehicles such as ambulances, police cars, or fire trucks through more quickly without placing undue burden on other vehicles. Both augmentations are fully implemented and tested in our custom simulator, and we present detailed experimental results attesting to their effectiveness. 1",HumanUsable and Emergency Vehicle–Aware Control Policies for Autonomous Intersection Management,,,,,core
24570193,07/02/2008,"Traffic congestion is one of the leading causes of lost productivity and decreased standard of living in urban settings. Recent advances in artificial intelligence suggest vehicle navigation by autonomous agents will be possible in the near future. In a previous paper, we proposed a reservation-based system for alleviating traffic congestion, specifically at intersections. This paper extends our prototype implementation in several ways with the aim of making it more implementable in the real world. In particular, we 1) add the ability of vehicles to turn, 2) enable them to accelerate while in the intersection, and 3) augment their interaction capabilities with a detailed protocol such that the vehicles do not need to know anything about the intersection control policy. The use of this protocol limits the interaction of the driver agent and the intersection manager to the extent that it is a reasonable approximation of reliable wireless communication. Finally, we describe how different intersection control policies can be expressed with this protocol and limited exchange of information. All three improvements are fully implemented and tested, and we present detailed empirical results validating their effectiveness. 1",Multiagent Traffic Management: An Improved Intersection Control Mechanism,,,,,core
148440386,2009-01-01T00:00:00,"Urban Search And Rescue (USAR) is a time critical task since all survivors have to be rescued within the first 72 hours. One goal in Rescue Robotics is to support emergency response by mixed-initiative teams consisting of humans and robots. Their task is to explore the disaster area rapidly while reporting victim locations and hazardous areas to a central station, which then can be utilized for planning rescue missions. To fulfill this task efficiently, humans and robots have to map disaster areas jointly while co- ordinating their search at the same time. Additionally, robots have to perform subproblems, such as victim detection and navigation, autonomously. In disaster areas these problems are extraordinarily challenging due to the unstructured environment and rough terrain. Furthermore, when communication fails, methods that are deployed under such conditions have to be decentralized, i.e. operational without a central station. In this thesis a unified approach joining human and robot resources for solving these problems is contributed. Following the vision of combined multi-robot and multi-human teamwork, core problems, such as position tracking on rough terrain, mapping by mixed teams, and decentralized team coordination with limited radio communication, are directly addressed. More specific, RFID-SLAM, a novel method for robust and efficient loop closure in large-scale environments that utilizes RFID technology for data association, is contributed. The method is capable of jointly improving multiple maps from humans and robots in a centralized and decentralized manner without requiring team members to perform loops on their routes. Thereby positions of humans are tracked by PDR (Pedestrian Dead Reckoning), and robot positions by slippage- sensitive odometry, respectively. The joint-graph emerging from these trajectories serves as an input for an iterative map optimization procedure. The introduced map representation is further utilized for solving the centralized and decentralized coordination of large rescue teams. On the one hand, a deliberate method for combined task assignment and multi-agent path planning, and on the other hand, a local search method using the memory of RFIDs for coordination, are proposed. For autonomous robot navigation on rough terrain and real-time victim detection in disaster areas an efficient method for elevation map building and a novel approach to genetic MRF (Markov Random Field) model optimization are contributed. Finally, a human in the loop architecture is presented that integrates data collected by first responders into a multi-agent system via wearable computing. In this context, the support and coordination of disaster mitigation in large-scale environments from a central-command-post-perspective are described. Methods introduced in this thesis were extensively evaluated in outdoor environments and official USAR testing arenas designed by the National Institute of Standards and Technology (NIST). Furthermore, they were an integral part of systems that won in total more than 10 times the first prize at international competitions, such as the RoboCup world championships.This is a Ph.D. thesis originally defended at University of Freiburg.Artificial Intelligence & Integrated Computer System",Mapping and Exploration for Search and Rescue with Humans and Mobile Robots,,Freiburg : University of Freiburg,,,core
33706694,2008,"English translation of chapters: 1. Knowledge management in enterprises functioning in the new economy - 2. Innovativeness as one of the knowledge-based economy pillars - 3. The approach to information management in small and medium sized enterprise - 4. Knowledge outsourcing III. The fluctuation of the knowledge and decision centers - 5. Implementation of declarative framework for decision support in scheduling problems - 6. Small-size and multi-product production flow planning - 7. Artificial intelligence methods in prediction of stock index values with usage of newspaper articles - 8. Information system supporting utilization of knowledge on movement and transport control - 9. The role of modern manager in knowledge management process - 10. Developing an organisational culture supporting knowledge management - 11. A method for evaluating organizational structure on the basis of social network analysis - 12. Developing sets of experience knowledge structure: Toward decisional DNA - 13. The language of communication in a multi-agent system for network monitoring - 14. A concept study of a multiagent system for maintaining the quality of service in future mobile ad hoc networks - 15. Reconstruction of attack propagation tree in multi-agent IDS system - 16. Data storage management: outline of practical methodology foreffectiveness assessment - 17. Applications of rough classification method in e-learning systems - 18. Description logic as software modeling language - 19. The concept of IT system for knowledge and experience management - 20. Knowledge acquisition for workflow systems - 21. Knowledge management embedded in software engineering processes - 22. E-document technology in the e-administration - 23. The application of PHP scripts for information extraction from newspaper announcements - 24. Knowledge management in an EU project on the example of MAYDAY project - 25. Outline of the contemporary trends of the net and their implications for ebusiness - 26. Providing learning components with learning schemata by means of the UDDI registry - 27. Pomeranian firms and the new technology - 28. Linear model approach to the computerization strategy of an organization - 29. The analysis of organization models and the choice of information management system for commercial and manufacturing enterprises - 30. Role of project management office in IT project management - 31. An intelligent platform for communication and control as well as management of modern companies - 32. Internet-based polling system as company's competitiveness improvement tool - 33. The UML model of an intelligent system for the management of industrial like processes in real-time - 34. Formal foundations of a knowledge management system supporting business process optimization - 35. Examples of tools used for business process modeling - 36. A quality model for UML tools - 37. Evaluation of the business processes modeling methods used in enterprises - 38. Fuzzy logic and logic-algebraic method for constraint programming-driven project prototyping - 39. Use of analytical and simulation methods for modeling of discrete and stochastic systems - 40. Architecture of distributed system for teletraffic monitoring - 41. Needle Desktop Search: A search engine for local internet documents - 42. An approach to composite web services evaluation for service oriented architectures - 43. Methodological converters for the evaluation of internet computer shops’ websites - 44. Knowledge management in IT project management - 45. Studies of the stage of the IT projects realization as the project management adjusting factor - 46. Evaluation of information technology as a part of R&D process optimization - 47. IT organization transformation modelling. SITAR – the mimplified model - 48. A proposal of metodology for testing clients requirements against employed information technology - 49. Knowledge resources management model in the information technology evaluation environment - 50. Ontology management model in an information technology evaluation environment - 51. IT evaluation using a functional prototype of multiagent systems - 52. Estimation of the IT technologies, using Mind Map techniques, modeling, and expert estimation",Zarządzanie wiedzą i technologiami informatycznymi,,Pomorskie Wydawnictwo Naukowo-Techniczne,,,core
235573821,2001-02-21T00:00:00,"An archive of the Magrath Trading Store News.The University of Lethbridge Library received permission from the Wes Balderson to digitize and display this content.MAGRATH NEWS ! i
Published Weekly Since 1932 by
The Magrath Trading Company
35t February 21,2001
Zeniths win Luther Tournament
The Zeniths traveled to Regina over the weekend and won the
prestigious *49 annual Luther Invitational Basketball Tournament,
beating Regina Winston Knoll Wolverines 87-84 in the championship
game. Jimmy Balderson led the points for the Zeniths in the final game
with 34 points and Allen Tollestrup added 21.
Magrath has only won the Luther Tournament twice in its
history. The other was in 1963.
Jimmy was named tournament most valuable player with a total
of 107 points in three games. This is the second highest point total ever
recorded over three games with the highest being 112 made by
Raymonds, Richard Bohne in 1989.
Magrath advanced to the final by opening with a 78-60 win over
the host Luther Lions. The dub then posted a 72-61 triumph over
Shddon-Williams Spartans of Regina in the semifinal game.
The Zeniths have had a great season this year with a lot of
exciting games. There is another exciting game coming up this Friday,
March 23 in Cardston. Also, you wont want to miss the playoffs to be
held at the Enmax Centre in Lethbridge on March 7-10. Come out and
support the Zeniths.
Points of Interest
* Sports
* Case Lot Sale
* Grocery Specials
* Home Hardware
Sales
Make your dream come true
have your home built by
JENSEN ENTERPRISES
New construction:
Additions:
Patio Decks:
403-758-3669
Ken Jensen, Owner
r i
Inside this issue:
Community Interests 1-3
Sports 4
Classified Ads 6-7
Community Calendar 8
Grocery Specials 9-10
Meat & Produce 11
Hardware Specials 12
k
Finewoodwork:
Hardwood furniture:
Renovations:
Box 93
Magrath, AB TOK 1J0
f LADIES '
| The Croft Niqht that was formerly held |
•at the Lutheran Church will now be held at.
I Maria Passey's the third Wednesday of each |
jmonth. ‘
■ »
| Any info, you need phone 758-3240 -Betty j
■ or758-3569-Darlene .
V. ""EVERYONE WELCOME!"" /
Mag rath United Church
Pancake Supper
Tuesday, February 27th
5:00 - 7:00 p.m.
Adults & Children $4.00 each
6 & under eat free
or $15.00 per family
Come enjoy the food & fellowship!
Proceeds go to the Magrath United Church Sunday School.
Cardston Lutheran School
A Community Forum meeting to discuss the
feasibility of a Lutheran School in Cardston will
be held Thursday, February 22 - 7:30 p.m. -
Cardston Faith Lutheran Church basement.
Welcoming enrollment
from all cultures and denominations.
The South Country Jamboree Society
will be holding their regular jam at the
702 Wing on
Sunday, February 25th at 2:00 p.m.
Meeting at 12:30 p.m. Everyone welcome.
Phone 327-8477 or 330-4098.
to- Gfaputa,
The parents of Chynna Johnston wish to say
how proud they are of her being named
Student of the Month for January at Robert
Warren Junior High School in Calgary. Chynna
was awarded this honour after being runner-up
in November. The school has 164 students in
Grade 7 and the judging criteria includes the
areas of supporting school spirit, positive peer
relationships, leadership, attitude, helpfulness,
and academic achievement. Chynna is the
only student in Grade 7 asked to work in the
school’s store and she has a regular shift in the
office where she answers the phones and fulfils
other receptionist duties. She ran on the cross
country team last fall and plays flute in the
junior band. She is also an original member of
the Homework Club. Well done Chynna! Keep
doing your best and remember we both love
you very much!
Mifemtm
patterai mi
ttelJHBfitf
Lance Miller
Agent
Bub. 3284411
Fax. 3204427
• You are invited to a fun evening...
! Late Night Shopping
in Del Bonita!
: Tupperware, Candle lights,
I Pampered Chefs, Mary Kaye,
= Ufana (health), Jewellry, underthings,
I New Balance clothes.
: Thursday, February 22nd
from 4 p.m. til closing
l at Kelly Newton’s (across from the
L.D.S. Church in Dei Bonita).
i Hosted by Kelly & Syd
♦
I■■
I
■■
I■■
I□■
I■
■
I■
I
We specialize in:
• Video slide shows (weddings,
reunions, sports highlights,
the Web (Output format:
VHS tapes CD)
• Hi-Res Photo scanning/Repair s
Retouching/Printing (colors B/W),
OCR (text)
• Banners, Posters, Ads, etc.
• Private tutoring
• Computer troubleshooting
• Desktop Publishing
Phone Bonny/Brenda
758-3844 (evenings)/
leave a message (day)
Cards of Thanks
Norman Christensen’s family would like to express thanks to - families, friends & neighbors for
-
prayers, calls, cards, food, flowers and donations we received during this difficult time. They were greatly appreciated.
To Dr. Gentlemen. Tollestrup, Dr. Ken Dahl and the staff at the Magrath Hospital we thankyou for the kind loving careyou gave Norm while he was there.
A special thanks to Joe Keeler for his tribute
-
to our family for honoring a Great Guy by doing such a great Job with their parts in the Service.
To the honorary pallbearers and to the many people who took time to attend the Service - We so appreciateyou.
A gracious thankyou to Max and staff of Christensen Funeral Home for the caring, professional manner they took care of all our needs.
Melva, Joan, Barrie, Kim, Joy and Families
SENIOR CENTRE NEWS
Suppers will be held Februay 28th Pot Luck - Februay 23~ All at 5.00 p.m. Senior Centre Your Hostesses for Februay are Bernice Coleman, May Tanner & Evie Hillmer.
*******
Contact Jean or Jack Butlin for rental of the Seniors Centre. Fee $75.00 Phone 758-3030
*******
BINGO*BINGO
Bingo is Thursday, March Ist at the Seniors Centre
Doors open at 6:30 p.m. Bingo at 7:00 p.m. Public Welcome!!
GEORGE BURGER
We would like to express our sincere gratitude to our friends for the many cards, food, visits, flowers, prayers and memorial donations we received during this difficult time.
Also special thanks to:
-
the Doctors (especially Dr. Regehr), nurses, caregivers and staff at the Magrath Hospital & Clinic and the Magrath Ambulance Crew for their excellent care.
-
Pastor Brian Amison, Organist Joy Johnson, Soloist Cheyl Walburger for their part in the service.
-
All those who attended George’s Memorial Service and the U.C.W. Ladies and friends who helped out and supplied the luncheon.
Your thoughtfulness and kindness will always be remembered.
Sincerely,
Lila & Family
SENIOR NEWS
We will be going to Stage West in Calgay on Wednesday, March 21st.
Matinee for luncheon & stage play “OVER THE RIVER & THROUGH THE WOODS"" Members - $45.00 Non-members - $55.00
ALSO
“THE BLUE CASTLE”
is a play adapted from L.M. Montgomey’s book. March 22-31
Thursday, March 29th has been reserved for the Magrath Seniors.
Cost: $25/person includes dinner & theatre. Bus cost will be extra.
Call now to make your reservations. Phone: Levaun Thompson - 758-6672 Hazel Rasmussen - 758-3545 Norma Owens - 758-3560SPORTS I
SOCCER ~l
SOCCER REGISTRATION .
Ages 4-U13 play in Magrath
$35 per player, $105 familyof3 or more
U15 & U19 boys play in Lethbridge
$70 per player
(teams playing in Lethbridge must be submitted by April 1)
Coaches and referee registration
Place: Magrath Pool Foyer
Dates: Thurs. March 15 7:00pm - 9:00pm Sat. March 17 9:00am- 12:00pm
Thurs. March 22 7:00pm-9:00pm _____|
; CURLING
Magrath Ladies Open Bonspiel
February 22 to February 28, 2001
■ All teams eligible for the Investors Group, Ken Leavitt & Mike Holt “Draw to the Button” Challenge Possible Prize: $5,000.00
Entry Deadline: February 20,2001 Phone entries to:
Sandy Meldrum: 758-3697 :
Lindy Oliver: 758-6521
Rod & Gun Club Annual Banquet & Awards Night March 3,2001
United Church Tickets $10.00 each - Juniors $5.00 each Tickets at Trading Co. Hardware EVERYONE WELCOME!
SKATING
The Skating Rink is closing, with the last day being February 25*.
BASKETBALL
Pandas and Cubs
Last Tuesday the Pandas upended Raymond Comettes 54-42 with Joan Leishman scoring 14 points and Cassy Foggin 10.
The Cubs also came out on top with a score of 47-29 with Lauren Balderson and Bonnie Wilde scoring 12 and 11 respectively.
On Friday the girls went to Winston Churchill where the Pandas suffered a defeat of 64-57 against the Griffins. Danielle Wilde made 18 points for the Pandas with Kelsey Helgeson and Joan Leishman adding 10 points each.
The Cubs, on the other hand, were able to come out victorious with a score of68-27 as Lauren Balderson hit for 16 points. Debbie Balderson made 13 points and Bonnie Wilde 12.
Saturday afternoon the Pandas and Cubs hosted Cardston Cougars. The Pandas put up a good fight but were not able to defeat the Cougars as the Cougars won 78-67. Kelsey Helgeson made 24 points while Joan Leishman and Cassy Foggin added 11 and 10 points respectively.
The Cubs earned a 70-56 victory over Cardston with Debbie Balderson scoring 26 points. Elaine Woolf added 12 points and Lauren Balderson 10.
The girls next game is Saturday, March 3 in Medicine Hat.
Please submit your sports scores and schedules by dropping off at the Trading company office, faxing to 758-6888, or emailing to tidmarsh@telusplanet.net._________Adult Education Computer Courses
January-March 2001
g Instructors: Bonny West/Brenda Beck
I MARCH
' Cardston: Microsoft Word-Intermediate
This 3 week will cover creating and editing tables, using Word’s drawing tools, adding headers, footnotes,
and endnotes, importing files, modifying templates, mailing labels, customizing bullets and special characters,
adding web links, and additional editing features.
Dates: Tuesdays, March 6-20 (3 weeks) Time: 7:00-9:30 p.m.
Location: Cardston High School Tuition: $35
Magrath: Make Your Own Memory Video
. Using slides, photos, scanners, video dips, background music, voice annotations, transitions, and easy to use
Software, it is now possible to create “living” video scrapbooks for home movies, weddings, family reunions, baby
books, gifts, sports highlights, etc. Participants will create their own personal memory video output to VHS tape.
(These movies may also be stored on CD, on the Web, o r sent via email.)
Participants need to bring their own photographs & video clips and choice of badcground music (preferably
CD). Additional photos and music and blank tapes will be provided.
Dates: Wednesdays, March 7-28 (4 weeks) Time: 7:00-10:00 p.m.
Location: Magrath Elementary School Tuition: $65
To register call Kathy Richards @ 653-4991
IDA MAY COX PILLING
passed away in Cardston of February ,*19 2001 at
the age of 94 years. She was predeceased by her
husband Owen Leonard Pilling in 1990 and two
infant children: Freeman in 1935, and Kay LaRene
in 1938. She is survived by six children - Maxine
(Ronald) Rodgers of Cardston, Gaelynd (Norma)
Pilling, and Merna (Reed) Coleman of Magrath,
Dale (Karen) Pilling, and Curtiss (Janice) Pilling of
Leavitt, and Norma (Al) Hatch of Valleyview, AB; 30
grandchildren; and 59 great grandchildren. Also
surviving is her sister - Birdie Lybbert of Spokane,
WA; brother - Myron (Virgie) Cox of Buhl, ID; sisters-in-
law - Essie Cox of Blairmore, AB and Dulcie Cox
of Brooks, AB.
The Funeral Service will be held at The
Church of Jesus Christ of Latter-Day Saints, Temple
Street Chapel, Cardston on Thursday, February
22nd at T.00 p.m. With Bishop Michael Nelson
officiating. Friends may meet the family at the
Church from 11:30 a.m. to 12:40 p.m. prior to the
service. Interment in the Leavitt Cemetery.
WWW
FEBRUARY Is
HEART & STROKE
MONTH
To Magrath & District Residents:
The canvas by the Magrath Hospital
Auxiliary is now under way with a total
of 48 volunteers involved.
We ask that when called on,
you would please welcome them.
Every dollar will help.
Please consider a donation.
Thanks,
Magrath Hospital Auxiliary
RE^tlX real estate
John Latham
Associate Broker
- magrath 758-6060
TOP CLASS ACREAGE
1444 SQ.FT. BUNGALOW - $89,900
Don’t wait around, this one won’t wait. -
Call John immediately, mls#210241
This 25 year
bungalow is
definitely priced
right. Owners
are moving to
another city.
Large Luxury Bungalow (2195 sq.ft.) on 5 acres
nestled in mature trees. 5 acres of irrigation water
rights. On town water. This is a must to see. For
details call John at 758-6060. mls #210402
$79,900
4 Bedroom Bungalow with Front Drive Garage
Beautiful
character home
fully renovated
from top to
bottom. Only
asking $99,900.
For more details
call John at 758­6060.
MLS #202461
Excellent starter home. Priced to sell. Large flat lot in
nice location. Call John at 758-^60^^1-521^77^^^^
BUY & SELL
■'i'
* Want to Lower your Natural Gas or Electric Bills?
Invest in a “Magic Cooking Box”! $35.°° each.
Just bring water to a boil for a few minutes. Turn
your stove off & place your pot in your “Magic
Cooking Box“. Your food will finish cooking in
your Box. Phone Pat Harrison - 758-3714.
* FOR SALE - 9 cu. ft. Chest Freezer - $85- Over
Head Garage Door Opener - $45. New motorized
Pea Podder - $100. Interior Mahogany doors plus
hardware, very good-$10 each. Garden Potting &
Trays. Ph. 758-6339.
* FOR SALE - Ice House - 4 man. Used twice.
$250. 758-6360, space #17 Trailer Park.
BUSINESS
* TUPPERWARE
Have defectives? Need replacements?
Call Heather MacKay at 758-3662.
All orders receive discounts, no pressure, last
order. Call before March 7*.
* FOR SALE - Home raised, grain fed beef. Ready
now for slaughter. Split one with a friend.
Thomson Livestock Ltd.
Merrill Thomson - 758-3209.
* REMEMBER TO CALL TAI
Heating, Air conditioning, Refrigeration, and
Appliance Service, Gas & Electrical Service and
Trenching. 752-3866.
* If your child needs help in Math or Reading the
Kumon Method might help. For more
information call Rusty or Martine Rollingson
at 758-3648.
CLASSIFIEDS
* Jeanie’s Hair Fashion
136 S. 1 St. W. - four doors south of the Trading Company. 758-3379. Open Tues. - Fri. Professional Hair Care at pleasing prices.
*
CANADIAN SECURITY SYSTEMS
We sell, install & service alarms, safes, camera systems, deadbolts & key locks. Call Ross Moore at 758-3945. Free estimate.
*
For all your cleaning needs from hospital clean to a touch up, carpet to ceiling & everything in between. No job too big or too small.
Call Waynes Carpet & Upholstery Cleaning 758-6414.
*
Will Tutor, grade 1-12 Math or Chemistry.
Call Martine Rollingson at 758-3648.
USED VEHICLES
ÇJ 1-^5 A-;'* A-;1 JA-: eT-P J L—
* 2 bedroom apartment. For information call 758-3876.
k FOR SALE - 1999 Ford XLT loaded short box 4x4. 30,000 km. Offers. Call Codey - 758-3395 days or 758-3767 evenings.
REALESTATE
FOR RENT
*
Storage space & Commercial space for rent. For information call 758-3876.
*
2 Bedroom House - including washer/dryer, stove & refrigerator. 122 Civic Ave. W. Abstainers. Phone 758-3082.
*
2 bedroom, furnished apartment. No pets. Abstainers. Call Ty Alston - 758-3322
*
Small 2 bedroom home. Private. Treed lot under the hill. Inexpensive. 758-3700. Available now!!
*
4 bedroom, 2 story home. Close to school and LDS stake center. 1 Vi bath, living & family room, utility room. Fridge & stove included. Abstainers. No pets. 758-3700. Leave message.
FOR SALE
*
Character Home in Magrath. 1300 sq. ft. Fully renovated. Loads of oak throughout main floor. 9 ft. ceilings. Large country kitchen with oak cabinets. Finished basement. 25’x24’ heated garage with finished loft. Landscaped lot with mature trees. A must to see. Call collect for Viewing and more details - (403) 227-2611.
*
3 bedroom, 3 bathrooms. 1250 sq. Ft. on main floor. Treed lot - Fully developed basement. Call 758-6991.
*
5 bedroom, 3000 sq. Ft. Home on 1 acre.
2 bath, 2 family rms (one with wood burning stove), living rm, dining rm, oak in kitchen.
A steal at $132,000. Phone 758-6789.
*
House for sale - all brand new inside when finished renovations. For complete details See “www.retailcontrols.com/house” Asking $49,000 when complete.
*
New 1230 sq. ft. home. 3 bedrooms, 2 bath, garage, fridge, stove. No GST.
Phone 758-6835 or 758-3446fMGMTHNEWS
In Our Community... February2001
Sunday Monday Tuesday Vvfednesday Thursday
-- —------- -r
Friday ! Saturday
18
19 H
FAMILY DAY
HOLIDAY
20 21 22 23
Seniors Ftot Luck
5:00
BASKETBALL
Boys at
Cardston
24
NO SCHOOL FOR STUDENTS THIS WEEK!
25 26 27
I
§8
Seniors Supper
5:00
1
BINGO
2
BASKETBALL
Boys host Kate
Andrews
3
BASKETBALL
Boys at
Raymond
LIBRARY NEWS
We are still offering introductory computer skills, covering word processing, internet and
email. There is no charge and you do not have to be a library member. For further information
phone us @ 758-6498. . . .
Besides pre-school books, the library has special pre-school reading and activity kits that can
be checked out.
We have a used Heintzman piano for sale.
Please note that the deadline for submissions to the paper is MONDAY at 6:00 p,m. Entries submitted
after the deadline will be published the following week. Phone 758-6377, Fax 758-6888 or drop off your
submissions at the Magrath Trading Company Office.
Jtagrath trading Company
GBOCCBB SPUIAZS
“3rom Our Janfly to tymin...”
9
Dairy Delights and frozen Jawrites!
Western Family Margarine, parchment - 454 g
Case of 12
$8.98
Good Humor Ice Cream - select varieties
4 litre pail
$3.98
Dairyland Sour Cream
750 mL
$2.88
Cheemo Perogy, frozen - select varieties •
2 kg
$5.68
Pillsbury Pizza Pops, frozen - select varieties
1 -2 kg
$8.98
Five Alive Chilled Citrus Beverage - select variety s 1.89 litre
$2.48
Dairyland Multi-pack Yogurt - select varieties
8x 125g
$2.98
Dairyland Milk 2 GO - select varieties
500 mL
$1.08
Tampico Chilled Punch - select varieties
4 litre
$2.88
Western Family Vegetables, frozen - select varieties
1 kg '
3 for $5.88
Niagara Orange Juice - frozen, 341 rm
Case of 12
$10.88
Kellogg’S Eggo Waffles - select varieties
48 pack
$8.98
Groceries
McCain Punch - select varieties - 200 mL
Case of 24
$5.98
Green Giant Vegetables, select varieties- 341-398 rm
Case of 12
$7.98
western Family Flour - an purpose
10kg
$3.98
Ocean’s Chunk Light Tuna, in water -170 g;
pack of 10
$6.98
Kraft Dinner - 225 g
Case of 12
$7.98
western Family Coffee - select varieties
300 g
6 for $12.98
Alpha or Pacific Evaporated Milk - 385 mL
Case of 12
$ 1 1.98
Western Farpily Apple Blends -1 litre
Case of 12
$8.98
Kellogg’s Mini Wheats, jumbo
13OOg
$7.98
Del Monte Peach Fruit Cups
16 pack
$10.98
Western Family Coffee - select varieties
300 g
6 for $ 12.98
Dasani Water
6 x 500 mL
$5.48
Heinz Tomato Juice, 284 mL
Case of 12
$8.98I kJ
Jlore Grocery Specials...
Betty Crocker Frosting - select varieties
340-450 g
2 for $4.00
Betty Crocker Cake Mix - select varieties
510g
3 for $3.99
Dare Simple Pleasures Cookies - select varieties
300-350 g
$2.28
Hunt’s Pudding Snacks - select varieties
12 pack
$3.98
Kraft Handi Snacks - select varieties
696 g
$10.98
Hostess Super Size Potato Chips - select varieties
410-475 g
$2.98
Old Dutch Potato Chips -select varieties
300 g
2 for $4.00
Old Dutch Variety Pack Potato Chips - select vari< ties3O x 32 g
$9.98
Hunt’s Tomatoes - select varieties, 398 mL
Pack of 6
$4.98
Hunt’s Tomato Sauce - 398 mL
Case of 12
$8.98
Hunt’s Tomato Paste - select varieties, 156 mL
Case of 12
$7.98
Hunt’s Thick & Rich Pasta Sauce - 68o mL
Case of 12
$ 19.98
Campbell’s Vegetable or Chicken Noodle soup
Case of 12
$8.98
Heinz Beans - select varieties, also kidney beans
Case of 12
$8.98
Heinz Pastas - select varieties, 398 mL
Case of 12
$9.98
Value Priced Noodle Cups - select varieties, 64 g
case of 12
$7.98
Catelli Pasta - select varieties
4-kg
$6.98
Western Family Mushrooms - pieces & stems, 284 n l Case of 12
$7.98
Dempster’s Sliced Bread - sesame white or 100% whc e 680 g
$1.98
Household Items...
Scotties Facial Tissue, iso’s-muitt-pack
Pack of 6
$5.88
Viva Paper Towels, white - 2 roll pack
Case of 12
$ 1 1.98
Purex Bathroom Tissue - regular 24 roll
Case of 2
$ 14.98
ABC Laundry Detergent - select varieties, 34 37 uses
Case of 4
$23.88
Western Family Winshield Washer Fluid - 4 litre
Case of 4
$7.88
Electrasol for Dishwashers
4 kg pail
$8.98
Pears Shampoo or Conditioner - select varieties
300 mL
$1.8811
JHore Household Items...
Western Family Dog Food - select varieties, 624 g
Case of 12
$ 1 1.98
Mainstay Dog Food
18kg
$ 1 1.98
Friskies cat Food - select varieties, I56g
Case of 24
$10.98
Mainstay Cat Food
8kg
$8.98
Value Priced Cat Litter
10kg
$3.98
JlABKtt JBSSB PBODUCC!
Sunkist Navel Oranges - California grown
8 lb bag
$3.98
Texas Sweet Red Grapefruit - Texas grown
5 lb bag
$3.48
Russet Potatoes, Canadian grown
20 lb bag
$3:98
Medium Carrots, u.s. grown, #i grade
5 lb bag
$2.48
Medium Onions. U.S. grown, #1 grade
5 lb bag
$2.48
BMCCHCB’S BUBS 07 ZBS WttK!
Lean Ground Beef, value pack 4-e ibs
$4.37/kg
$1.98/lb
Pork Loin Chops, rib or tenderloin ends - value pack 3-4
bs $5.47/kg
$2.48/lb
Boneless Chicken Breasts, skinless - iqf
1 kg
$8.78
Western Family Sliced Meats - select varieties
175 g
$1.28
Burn’s Wieners
450 g
$2.28
Burn’s Cooked Ham
375 g
$3.28
Burn’s Sliced Bacon
500 g
$3.48
Burn’S Breakfast Grill - select varieties
375 g
$3.48
Burn’s Bologna
500 g
$2.98
Western Family Beef Burgers, frozen
4.4 kg box
$19.98""Home of the Handyman""_________
Unisex Digital Wrist Watch
Only $3.49
Deluxe Pocket Watch with Fob
$29.99
Cattlemen!
Please order special number
ear tags now.
50 cc Roux
Automatic Syringe
for the Cattlemen
Reg. $54.99
Special $39.99
Crosley Gas 30 inch - white
Gas Range ..
Sealed burners - 2 high speed burners
Regular $999.00
Save $50 - $949.00
Old fashioned Big Ben Radio
Black in color with alarm
Reg. $49.99
Save $10 - Special $39.99
Easy chair Magazine & remote
control Holder........
Regular $19.99
Sale Price $13.99
Ford Motorcraft
Anti-freeze
Reg. $8.95/gallon
Sale Price $7.95
\Ne have two
Garden Seed
racks come in.
We have several
Microwaves
to choose from.
We have several boxes of
Valentines left.
Now V2 Price!
We have some
Toboggans & GT racers
left
Orville Redenbacher's / ■ —
blip Microwave Popcorn 1
v—.J Low Fat or Buttery J
Regular $449 SncSHre
Special $3","Magrath Store News (February 21, 2001)",,J. A. Ririe,,,core
357319816,2009-01-01T00:00:00,"Abstract Nuclear factor-erythroid 2 p45-related factor 2 (Nrf2) is the primary transcription factor protecting cells from oxidative stress by regulating cytoprotective genes, including the antioxidant glutathione (GSH) pathway. GSH maintains cellular redox status and affects redox signaling, cell proliferation, and death. GSH homeostasis is regulated by de novo synthesis as well as GSH redox state; previous studies have demonstrated that Nrf2 regulates GSH homeostasis by affecting de novo synthesis. We report that Nrf2 modulates the GSH redox state by regulating glutathione reductase (GSR). In response to oxidants, lungs and embryonic fibroblasts (MEFs) from Nrf2-deficient (Nrf2 −/− ) mice showed lower levels of GSR mRNA, protein, and enzyme activity relative to wild type (Nrf2 +/+ ). Nrf2 −/− MEFs exhibited greater accumulation of glutathione disulfide and cytotoxicity compared to Nrf2 +/+ MEFs in response to t-butylhydroquinone, which was rescued by restoring GSR. Microinjection of glutathione disulfide induced greater apoptosis in Nrf2 −/− MEFs compared to Nrf2 +/+ MEFs. In silico promoter analysis of the GSR gene revealed three putative antioxidantresponse elements (ARE1, −44; ARE2, −813; ARE3, −1041). Reporter analysis, site-directed mutagenesis, and chromatin immunoprecipitation assays demonstrated binding of Nrf2 to two AREs distal to the transcription start site. Overall, Nrf2 is critical for maintaining the GSH redox state via transcriptional regulation of GSR and protecting cells against oxidative stress. Keywords Nrf2; Oxidative stress; Glutathione; Glutathione reductase; Cigarette smoke; COPD; Emphysema; Free radicals Because of their function (gas exchange) and structure (large surface area), the lungs are constantly challenged with oxidative insults caused by exogenous air pollutants. Oxidative stress is involved in the pathogenesis of several chronic lung inflammatory diseases, including chronic obstructive pulmonary disease (COPD), asthma, acute respiratory distress syndrome (ARDS), and pulmonary fibrosis. Glutathione (GSH) is a vital antioxidant that regulates the cellular redox status and protects airway epithelial cells from oxidant-induced lung injury and inflammation  NIH Public Access Glutathione reductase (GSR), a homodimeric flavoprotein (50-kDa subunits), regulates cellular GSH homeostasis by catalyzing the reduction of GSSG to GSH using NADPH as a reducing cofactor  The basic leucine-zipper transcription factor nuclear factor-erythroid 2 p45-related factor 2 (Nrf2) has been shown to play a vital role in protecting cells from oxidative stress  Materials and methods Animals and care Nrf2 −/− CD-1 (ICR) mice were generated as described  Mouse exposure to cigarette smoke Eight-week-old mice were divided into four groups (n=3 per group): I, air control Nrf2 +/+ mice; II, experimental Nrf2 +/+ mice; III, air control Nrf2 −/− mice; and IV, experimental Nrf2 −/− mice. Groups I and III were kept in a filtered-air environment, and groups II and IV were subjected to CS according to a previously published protocol  Cell culture Mouse embryonic fibroblasts (MEFs) were isolated from Nrf2 +/+ , Nrf2 −/− , and Keap1 −/− mice as previously described  Knockdown of GSR by short interfering RNA (siRNA) Four GSR siRNA duplexes and siControl nontargeting siRNA 1 (SS siRNA) were obtained from Dharmacon Research (Lafayette, CO, USA). First, we identified an siRNA duplex that exhibits selective and maximal silencing of the GSR gene compared to SS siRNA. Briefly, Nrf2 +/+ MEFs at 80% confluency were transfected with 20 pmol of siRNA duplexes using Lipofectamine 2000 and OPTI-MEM I reduced-serum medium (Invitrogen) according to the Lipofectamine protocol. Concentrations of siRNAs were chosen on the basis of dose-response studies. Knockdown of the GSR gene was quantified by real-time reverse transcriptasepolymerase chain reaction (RT-PCR) 48 h after transfection. The sequences of the sense strands of the siRNA duplexes were as follows: duplex 1, AGACGAAGCUGUUCAUAAGUU; duplex 2, GACCAUGAUUCCAGAUGUUUU; duplex 3, GACGGGACCCAAAUUCUAAUU; and duplex 4, CGUGAAUGUUGGAUGUGUAUU. Duplex 1 was shown to have the greatest knockdown efficiency (data not shown). Overexpression of GSR The vector for GSR overexpression was obtained from Open Biosystems (Huntsville, AL, USA) (Clone ID 6813295). The cDNA insert was digested out of the parent plasmid and ligated into the pUB6-V5-His mammalian expression vector (Invitrogen) with a ubiquitin promoter. After amplification in Escherichia coli and plasmid purification (Qiagen Sciences, Gaithersburg, MD, USA), the plasmid was transfected into Nrf2 −/− MEFs using Lipofectamine 2000 (Invitrogen) as described above. Cytoprotective role of GSR Microinjection of GSSG and measurement of apoptosis-Nrf2 +/+ and Nrf2 −/− MEFs were plated at a density of 1×10 6 cells/ml in Mattek dishes. Approximately 100 cells per dish were injected with vehicle, phosphate-buffered saline (PBS), GSSG, or GSH (Sigma-Aldrich, St. Louis, MO, USA) at a concentration of 50 mM using a FemtoJet microinjector (Eppendorf North America, Westbury, NY, USA). As a control for injection efficiency, cells were also coinjected with Alexa fluorhydrazine 596 (Invitrogen). After the 4-h incubation at 37°C in 5% CO 2 , the number of apoptotic cells was determined by annexin V Alexa fluor 488 (Invitrogen) staining following the manufacturer&apos;s protocol. Data are expressed as the percentage of apoptotic cells compared to total number of cells microinjected. tert-Butylhydroquinone-induced cell death-tert-Butylhydroquinone (tBHQ)-induced cytotoxicity was evaluated in (i) Nrf2 +/+ and Nrf2 −/− MEFs; (ii) Nrf2 +/+ MEFs transfected with GSR siRNA or SS siRNA; and (iii) Nrf2 −/− MEFs overexpressing GSR or plasmid vector. Briefly, MEFs were seeded in 96-well plates at a density of 1×10 5 cells/ml and grown overnight. Cells were treated with tBHQ (150 μM) for 4 h and cell death was evaluated using a 3-(4,5-dimethylthiazol-2-yl)-2,5-diphenyltetrazolium bromide (MTT; Sigma) reduction conversion assay  Measurement of oxidized and reduced glutathione Nrf2 +/+ and Nrf2 −/− MEFs were seeded in 96-well plates at a density of 1×10 5 cells/ml and were treated with or without BCNU (25 μM), an irreversible inhibitor of GSR, for 16 h. Intracellular levels of total glutathione, GSH, and GSSG were determined using an enzymatic recycling assay as described previously  GSR mRNA expression by quantitative real-time RT-PCR Total RNA was extracted from lung tissues or cells with TRIzol reagent (Invitrogen) and reverse transcribed using the Superscript First Strand Synthesis system (Invitrogen) as per the manufacturer&apos;s instructions. Quantitative real-time RT-PCR analyses of murine GSR was performed by using Assay on Demand (Cat. No. Mm00439151_m1) primers and probe sets from Applied Biosystems (Foster City, CA, USA). The assay was performed using the ABI 7000 TaqMan system (Applied Biosystems). β-Actin was used for normalization. Immunoblot analysis of GSR For immunoblot analysis, 40 μg of total protein was separated by 4-12% sodium dodecyl sulfate-polyacrylamide gel electrophoresis and transferred to PVDF membrane by semidry blotting. The PVDF membrane was blocked and incubated with polyclonal rabbit anti-GSR antibody (Santa Cruz Biotechnology, Santa Cruz, CA, USA) (1:1000) or rabbit anti-GAPDH (1:1000), followed by incubation with horseradish peroxidase-conjugated horse anti-rabbit secondary antibody (Jackson ImmunoResearch Laboratories, West Grove, PA, USA), and developed using an ECL chemiluminescence detection system (Amersham Biosciences, Piscataway, NJ, USA). Immunohistochemical analysis of GSR in mouse lungs Nrf2 +/+ and Nrf2 −/− mice were exposed to CS for 5 h. After 24 h, the mice were sacrificed, and the lungs were processed for immunohistochemical analysis as described previously  GSR enzyme activity To measure the enzyme activity of GSR, mice were exposed to CS for 5 h and sacrificed 24 h later. The lungs were excised (n=5 per group) and processed as described previously  Identification of AREs in the promoter of GSR To identify the presence and location of AREs in the GSR (Accession No. 14782) promoter, the 2-kb upstream region from the translation start site was downloaded from the NCBI database (www.ncbi.nlm.nih.gov). The 2-kb sequence was used to search for AREs with the help of Genamics Expression 1.1 software (Hamilton, New Zealand) using the primary ARE core sequence (RTGAYNNNGCR) as the probe. The location of the transcription start site for the mouse GSR gene was determined using Mouse Genome build 33, version 1, of the NCBI database. The promoter sequences of mouse and human homologs of GSR were downloaded from the NCBI database and scanned for the presence of AREs. Plasmids and mutagenesis The 5′ flanking region of the mouse GSR promoter region was PCR amplified from genomic DNA isolated from murine blood with high-fidelity Taq polymerase (Applied Biosystems). The isolated PCR product was ligated into pCR2.1 (Invitrogen), and a KpnI-XhoI fragment from this construct was cloned into the pGL3 Basic vector (Promega, Madison, WI, USA). Three deletion constructs (− 1253 to +624, −978 to +624, and −755 to +624) were generated. The primers used for amplification were as follows: AAGTCAAAGTAACGCTGGTGTTGG (ARE1-3 forward), CCCTTTTGTGATCAAGTAGGAGTT (ARE1-2 forward), ACAGGGTACTCTACCAGAAGGAAA (ARE1 forward), and CTGTTACCTAGCACTTTGCCCTTG (reverse primer for all constructs). Individual AREs identified in the mouse GSR promoter region were PCR amplified from the ARE1-3 constructs in the pGL3 Basic vector using high-fidelity Taq polymerase (Applied Biosystems). The isolated PCR product was ligated into pCR2.1 (Invitrogen), and a KpnIXhoI fragment from this construct was cloned into the pTAL luciferase reporter vector (BD Biosciences, San Jose, CA, USA). The forward primers used for amplification were as follows: ARE3 forward (−1073), ACTGGTTATTGCCTCACAACGGCA; ARE2 forward (− 857), TTAACTCGGTGATCTTAGCAATCA; and ARE1 forward (− 143), TGCAGGAATCCGAGAAGC. The reverse primers used for amplification were as follows: ARE3 reverse (−997), CCGGCAGAAAAGTTGGTTTCCCTT; ARE2 reverse (−725), CCTTCTTCCTTGATGATCTTGAAC; and ARE1 reverse (+14), ACTTCCGCGCATGGCGCT. Mutated (mu) ARE sequences were generated by using a sitedirected mutagenesis kit from Stratagene (La Jolla, CA, USA). Primers containing the mu-ARE sequences (mu-ARE3, TTCTTATGACTTATTAGTATTTAA; mu-ARE2, AAGAAAGCCTTTTGGTCACTGTGA; mu-ARE1, GGCGCGGTTTTTAGTCACGGCGAC) were used for PCR amplification of the mutated GSR promoter, and PCR products were digested with DpnI for 1 h to cleave the wild-type promoter. The mutation in each promoter was verified by sequencing. The NQO1 ARE reporter construct contains a 41-bp rQR-ARE/EpRE inserted into a minimal promoter vector containing a TATA box and an initiator element  DNA transfection and luciferase activity Cells were transfected at 80% confluence using Lipofectamine 2000 (Invitrogen). Briefly, cells were seeded in 24-well plates at a density of 2×10 5 cells/ml and grown overnight. The subsequent day, cells were transfected with 200 ng of plasmid DNA and 1 ng of pRL-TK plasmid (Promega). After 48 h, cells were lysed, and Renilla and firefly luciferase activities were measured using the dual luciferase assay kit (Promega) with a luminometer (EG&amp;G Wallac, Gaithersburg, MD, USA). For transfection efficiency, luciferase activity was normalized to Renilla luciferase activity. Chromatin immunoprecipitation assay MEFs(Keap −/− , Nrf2 +/+ , and Nrf2 −/− )were harvested and the chromatin immunoprecipitation (CHIP)assay was performed using a commercially available kit (Upstate Biotechnology, Lake Placid, NY, USA). Immunoprecipitates and total chromatin input were reverse cross-linked, DNA was isolated, and 1μl of DNA was used for PCR(35cycles)with primers specific for the individual GSR AREs. The GSR primer sequences were as follows: (1) ARE2 forward, CCATCAAACTTTAACTCGGTGA; reverse GACTTGGGAGATAGAAGGAACG; (2) ARE3 forward, TGAGATTGACTGACACAATGGA; reverse, GATCACAAAAGGGAAACCAACT. Statistical analysis Results are presented as the means±SE. Statistical comparisons were performed by paired Student t tests. A value of p &lt; 0.05 was considered statistically significant. Results Nrf2-dependent expression of glutathione reductase in mouse lung after acute exposure to CS The basal levels of mRNA expression of GSR in the lungs were similar in Nrf2 +/+ and Nrf2 −/− mice as determined by real-time RT-PCR analysis. In response to CS exposure, the mRNA expression of GSR showed an approximately fourfold induction in the lungs of Nrf2 +/+ mice compared to air, whereas the lungs of Nrf2 −/− mice showed no induction of GSR  tBHQ treatment induces GSR expression only in Nrf2 +/+ MEFs In support of in vivo data on CS exposure, we measured GSR expression in Nrf2 +/+ and Nrf2 −/− MEFs after tBHQ (50 μM) treatment. Nrf2 +/+ MEFs showed an ~2-fold increase in GSR mRNA and protein expression 16 h after tBHQ treatment compared to vehicle control (DMSO) (  Oxidized glutathione induces greater apoptosis in Nrf2-deficient cells To determine the role of Nrf2 in conferring cytoprotection against intracellular GSSG, Nrf2 +/+ and Nrf2 −/− MEFs were microinjected with GSSG (50 μM), reduced glutathione (50 μM), or PBS. Cells with an approximate intracellular volume of 1 pl (picoliter) were injected with 1 fl (femtoliter) of each treatment. Approximately 100 cells were injected with each treatment, and apoptosis was determined by annexin V binding 1.5 h after injection. MEFs injected with the three treatments on a single plate are shown in  Restoration of GSR rescues Nrf2 −/− MEFs from tBHQ-induced cell death To determine whether restoration of GSR protects Nrf2 −/− MEFs from tBHQ-induced cytotoxicity, Nrf2 −/− MEFs were transfected with the GSR overexpression vector. After 48 h of transfection, an ~10-fold increase in the GSR mRNA expression in Nrf2 −/− MEFs compared to vector alone was noted  Glutathione reductase maintains GSH redox state and is critical for cell survival Next, we investigated whether knockdown of GSR in Nrf2 +/+ MEFs sensitizes cells to tBHQinduced cytotoxicity. MEFs transfected with GSR siRNA showed &gt;90 and 65% reduction mRNA levels and enzyme activity of GSR, respectively, compared to mock and scrambled siRNA (ssRNA)  GSR regulates GSH homeostasis To determine the role of GSR in the maintenance of GSH homeostasis, wild-type MEFs were treated with BCNU, a selective inhibitor of GSR  Nrf2 −/− cells show enhanced accumulation of GSSG during oxidative stress Constitutively, Nrf2 +/+ MEFs showed significantly higher levels of total glutathione compared to Nrf2 −/− MEFs  Nrf2 regulates glutathione gene expression through the ARE Detailed in silico analysis of the 2-kb promoter region 5′ upstream of the translational start site of the mouse GSR genomic locus was performed using the core ARE sequence RTGAYNNNGCR with the help of Genamics Expression 1.1 software as previously described  All three putative AREs in the GSR promoter showed high sequence homology to the NQO1 ARE  To investigate the functionality of these putative AREs in GSR promoter and Nrf2-dependent regulation, we cloned the 2-kb portion 5′ upstream of the translation start site of the GSR gene into the pGL3 Basic luciferase reporter vector. Reporter constructs containing all three putative AREs, two AREs, and one ARE were cloned into the pGL3 Basic vector. Two deletion constructs (−1253 to +624 (ARE1-3), −978 to +624 (ARE1-2)) exhibited ~160-fold higher luciferase activity compared with the pGL3 Basic vector. The smallest clone containing the ARE1 sequence (−755 to +624 (ARE1)) overlapping with the promoter demonstrated ~40-fold higher reporter activity  To dissect the functionalities of individual ARE enhancer sequences, we cloned specific regions of the promoter-containing individual AREs into the pTAL luciferase vector with minimal promoter (ARE3, −1703 to −997; ARE2 −857 to −725; and ARE1, −143 to +14) and transiently transfected them into Nrf2 −/− and Keap −/− MEFs. The two distal ARE reporter constructs (ARE3 and ARE2) showed significant induction of luciferase activity in Keap −/− MEFs, whereas no induction was observed in Nrf2 −/− MEFs. ARE3 showed the highest luciferase activity, followed by ARE2, whereas ARE1 showed no significant activity  On the other hand, transfection of individual mu-ARE reporter plasmids in which the consensus ARE was mutated failed to induce luciferase activity in Keap −/− MEFs, and it was comparable to that in Nrf2 −/− cells and or vector alone, indicating the specificity of the &quot;ARE motifs&quot; in the induction of reporter activity  Nrf2 binds to ARE3 and ARE2 in the promoter of the GSR gene Next, we analyzed the constitutive Nrf2 binding to ARE3 and ARE2 by CHIP assay in Nrf2 +/+ , Nrf2 −/− , and Keap −/− MEFs. A CHIP assay was performed using a commercially available kit (Upstate Biotechnology). Immunoprecipitates and total chromatin input were reverse cross-linked, DNA was isolated, and 1 μl of DNA was used for PCR (35 cycles) with primers specific for the individual GSR AREs present in the promoter. An anti-Nrf2 antibody was used to confirm the binding of Nrf2 to the GSR promoter. Activation of Nrf2 by genetic deletion of Keap1 enhanced the recruitment of Nrf2 to AREs in the GSR promoter as demonstrated by enhanced amplification, with ARE3 showing the highest level of binding  Multiple inflammatory disorders including COPD, ARDS, Alzheimer disease, Parkinson disease, liver disease, heart attack, stroke, and diabetes, as well as HIV infection and AIDS, are associated with elevated GSSG and reduced GSH levels. GSR is the key enzyme that maintains the GSH redox state by converting GSSG to GSH and thus may play a vital role in protecting against oxidative pathologies in these diseases. Chemical inhibition of GSR has been demonstrated to significantly impair the survival of mice in response to hyperoxia  Recent studies have indicated that oxidant-induced alterations in the cellular GSH-to-GSSG ratio in favor of GSSG trigger apoptosis independent of ROS and GSH levels, suggesting that GSSG alone is capable of inducing apoptosis  Recently we reported that Nrf2 regulates GPX2, one of the major isoforms of glutathione peroxidase, in lung cells as a response to oxidant or phytochemical treatment  Nrf2 regulates its target gene transcripts by directly binding to a cis-enhancer element referred to as an antioxidant response element that functions in both forward and reverse orientations. In silico analysis of the GSR promoter revealed three potential AREs within 2 kb upstream of the translation start point. The sequences of all three AREs in the GSR promoter were similar to the prototypical AREs identified in Nrf2-regulated antioxidants such as NQO1 and HO-1. A luciferase reporter construct with two distal AREs (ARE2 and ARE3) showed significant induction of luciferase activity only in Keap −/− and not in Nrf2 −/− MEFs. Similarly, individual AREs cloned in luciferase reporter constructs with a minimal flanking promoter also showed significant induction of luciferase activity in Keap −/− MEFs, whereas activity in Nrf2 −/− MEFs was equivalent to vector control. Mutation in the core sequence of the individual AREs completely abolished the luciferase activity in Keap −/− and Nrf2 +/+ MEFs. Similarly, CHIP analysis revealed greater Nrf2 binding activity at ARE2 and ARE3 in Keap −/− MEFs. However, nuclear extracts from Nrf2 −/− and Nrf2 +/+ showed poor Nrf2 DNA binding activity. These results corroborate the findings that Nrf2 upregulates expression of GSR (mRNA and protein) under inducible but not basal conditions, with induction of GSR being observed in Nrf2 +/+ lungs and MEFs only in response to oxidative stress. Taken together, these results suggest that the two distal AREs in the GSR promoter are functional to a variable degree, and Nrf2 regulates inducible but not basal expression of GSR via these AREs. In conclusion, we report Nrf2-ARE-mediated regulation of GSR induction during oxidative stress. In addition to GSH biosynthesis, Nrf2-dependent regulation of the cellular GSH redox state through GSR is vital for protecting against oxidative stress-induced cellular redox signal perturbations and cell death in lungs and other organs",Nrf2-regulated glutathione recycling independent of biosynthesis is critical for cell survival during oxidative stress. Free Radic,,,,,core
323902917,2004-08-01T00:00:00,"In this paper, the results obtained by inter-comparing several statistical techniques for modelling SO2 concentration at a point such as neural networks, fuzzy logic, generalised additive techniques and other recently proposed statistical approaches are reported. The results of the inter-comparison are the fruits of collaboration between some of the partners of the APPETISE project funded under the Framework V Information Societies and Technologies (IST) programme. Two different cases for study were selected: the Siracusa industrial area, in Italy, where the pollution is dominated by industrial emissions and the Belfast urban area, in the UK, where domestic heating makes an important contribution. The different kinds of pollution (industrial/urban) and different locations of the areas considered make the results more general and interesting. In order to make the inter-comparison more objective, all the modellers considered the same datasets. Missing data in the original time series was filled by using appropriate techniques. The inter-comparison work was carried out on a rigorous basis according to the performance indices recommended by the European Topic Centre on Air and Climate Change (ETC/ACC). The targets for the implemented prediction models were defined according to the EC normative relating to limit values for sulphur dioxide. According to this normative, three different kinds of targets were considered namely daily mean values, daily maximum values and hourly mean values. The inter-compared models were tested on real cases of poor air quality. In the paper, the inter-compared techniques are ranked in terms of their capability to predict critical episodes. A ranking in terms of their predictability of the three different targets considered is also proposed. Several key issues are illustrated and discussed such as the role of input variable selection, the use of meteorological data, and the use of interpolated time series. Moreover, a novel approach referred to as the technique of balancing the training pattern set, which was successfully applied to improve the capability of ANN models to predict exceedences is introduced. The results show that there is no single modelling approach, which generates optimum results in terms of the full range of performance indices considered. In view of the implementation of a warning system for air quality control, approaches that are able to work better in the prediction of critical episodes must be preferred. Therefore, the artificial neural network prediction models can be recommended for this purpose. The best forecasts were achieved for daily averages of SO2 while daily maximum and hourly mean values are difficult to predict with acceptable accuracy. © 2003 Elsevier Ltd. All rights reserved",Modelling SO2 concentration at a point with statistical approaches,,'Elsevier BV',10.1016/j.envsoft.2003.10.003,,core
21218395,2008,"This paper investigates an application of mobile sensing: detecting and reporting the surface conditions of roads. We describe a system and associated algorithms to monitor this important civil infrastructure using a collection of sensor-equipped vehicles. This system, which we call the Pothole Patrol (P 2), uses the inherent mobility of the participating vehicles, opportunistically gathering data from vibration and GPS sensors, and processing the data to assess road surface conditions. We have deployed P 2 on 7 taxis running in the Boston area. Using a simple machine-learning approach, we show that we are able to identify potholes and other severe road surface anomalies from accelerometer data. Via careful selection of training data and signal features, we have been able to build a detector that misidentifies good road segments as having potholes less than 0.2 % of the time. We evaluate our system on data from thousands of kilometers of taxi drives, and show that it can successfully detect a number of real potholes in and around the Boston area. After clustering to further reduce spurious detections, manual inspection of reported potholes shows that over 90 % contain road anomalies in need of repair",The pothole patrol: using a mobile sensor network for road surface monitoring,,ACM,,,core
21083696,2005,"Traffic congestion is one of the leading causes of lost productivity and decreased standard of living in urban settings. Recent advances in artificial intelligence suggest vehicle navigation by autonomous agents will be possible in the near future. In a previous paper, we proposed a reservation-based system for alleviating traffic congestion, specifically at intersections. This paper extends our prototype implementation in several ways with the aim of making it more implementable in the real world. In particular, we add the ability of vehicles to turn, enable them to accelerate while in the intersection, improve the efficiency and sensor model of the driver agents, and augment their interaction capabilities with a detailed protocol such that the vehicles do not need to know anything about the intersection control policy. The use of this protocol limits the interaction of the driver agent and the intersection manager to the extent that it is a reasonable approximation of reliable wireless communication. Finally, we describe how different intersection control policies can be expressed with this protocol and limited exchange of information. All improvements are fully implemented and tested, and we present detailed empirical results validating their effectiveness",Multiagent Traffic Management: Driver Agent Improvements And A Protocol for Intersection Control,,,,,core
20714824,03/04/2008,"Abstract. We are using machine learning to construct a failure-susceptibility ranking of feeders that supply electricity to the boroughs of New York City. The electricity system is inherently dynamic and driven by environmental conditions and other unpredictable factors, and thus the ability to cope with concept drift in real time is central to our solution. Our approach builds on the ensemble-based notion of learning from expert advice as formulated in the continuous version of the Weighted Majority algorithm [16]. Our method is able to adapt to a changing environment by periodically building and adding new machine learning models (or “experts”) based on the latest data, and letting the online learning framework choose what experts to use as predictors based on recent performance. Our system is currently deployed and being tested by New York City’s electricity distribution company",Real-time Ranking of Electrical Feeders using Expert Advice ⋆,,,,,core
144002198,2004-03-16T00:00:00,"Um dos problemas que afetam as cidades brasileiras é a falta de uma metodologia adequada para a implantação de equipamentos coletivos públicos (escolas, postos de saúde, entre outros), e da utilização de modo racional desses equipamentos. Esse foi o ponto de partida para esse trabalho, cujo objetivo principal foi desenvolver uma ferramenta de análise espacial para auxiliar o poder público no planejamento e na gestão dos serviços públicos de educação e saúde, no que concerne basicamente à melhor localização das unidades básicas de atendimento e à melhor distribuição dos usuários por essas unidades, buscando reduzir os custos de transporte.  Após uma avaliação da atual política de distribuição e utilização dos equipamentos coletivos de educação e saúde no Brasil e no mundo, foi realizada uma análise dos conceitos relativos às ferramentas de apoio a problemas de decisão de caráter espacial, particularmente os Sistemas de Apoio à Decisão Espacial (SADE). De posse dessa base conceitual, buscou-se formular as bases de um sistema (ou uma metodologia) que apoiasse a implantação de novos equipamentos coletivos e uma utilização eficiente dos equipamentos já existentes, tendo como parâmetro principal de comparação o custo de deslocamento dos usuários. Esse sistema deveria ser adequado à realidade das cidades médias brasileiras, de maneira geral hoje inseridas num cenário de falta de planejamento e de ausência de bases de dados estruturadas (e atualizadas). Ainda, como proposta adicional, procurou-se incorporar como ferramentas de apoio ao sistema algumas técnicas emergentes que, embora relativamente pouco utilizadas no planejamento urbano até o presente momento, apresentam grande potencial para tal. São elas: os Autômatos Celulares (ou CA, Cellular Automata) e as Redes Neurais Artificiais (RNA). Os fundamentos do sistema concebido foram transpostos para uma aplicação prática desenvolvida em um Sistema de Informações Geográficas (SIG) através de um estudo de caso conduzido na cidade de São Carlos (SP), cujos resultados demonstraram que quando se pensa em otimizar os custos de deslocamento, a principal ação a ser empreendida é a redistribuição da demanda às unidades de oferta, antes de se pensar na abertura de novas unidades. Mais importante do que os resultados numéricos obtidos, foi a confirmação de que é possível se utilizar as diversas ferramentas de planejamento e gestão de modo integrado. A partir dessa metodologia, um investimento em desenvolvimento de software pode levar à construção de um efetivo Sistema de Apoio à Decisão Espacial. De maneira mais geral, pode-se afirmar que a obtenção de dados é o grande obstáculo para pesquisas dessa natureza. A montagem de uma base de dados sólida e confiável é, sem dúvida, o ponto crucial para a execução de projetos potencialmente bem sucedidosOne of the main problems faced by brazilian cities is the lack of adequate methodologies for the implementation and rational use of public service facilities (such as schools and health care centers). That was the starting point in the definition of the main objective of the present work, which is the development of a spatial analysis tool for seeking an optimal arrangement of primary health and education facilities, in order to reduce transportation costs. The use of such a tool in the public administration is important not only for planning but also for management purposes.   The study starts with an investigation of approaches that have been used in real cases, in cities of different countries, to define health and education facility types and their distribution. A careful analysis of location-allocation concepts and analysis tools for the solution of spatial problems, with particular emphasis on the Spatial Decision Support Systems (SDSS), was also conducted. The system (or methodology) proposed, which was meant to support the process of location of new facilities and also an efficient use of the existing ones while reducing transportation costs, was based on that conceptual framework. The system was directed to the context of brazilian medium-sized cities, where planning concepts are not widely applied and even the databases available are sometimes very limited and frequently outdated. The system design looked for both traditional and innovative approaches and computer-based techniques, such as Cellular Automata (CA) and Artificial Neural Networks (ANN). These techniques, although not yet extensively applied in urban planning and management, seem to be promising alternatives for those activities.  The integration of the different tools and techniques was tested in a practical application carried out in the city of São Carlos (SP) and developed in a Geographic Information System (GIS) environment. The results drawn from the application are clear: the main action to reduce the transportation costs in the case studied was a re-distribution of the demand. It was considerable, despite the fact that not a single facility has been added. Another important outcome of the research was the confirmation of the hypothesis that the distinct planning and management tools tested could be used in an integrated approach. Therefore, the methodology discussed here could effectively become a Spatial Decision Support System only by means of software development for building the system interfaces. As a general conclusion, the data gathering problems still seem to be the main barrier for research development in this area. Consequently, the construction of solid and reliable databases is undoubtedly a key point to start any potentially successful projec",Bases for a decision support methodology for education and health services from a transportation perspective,,"'Universidade de Sao Paulo, Agencia USP de Gestao da Informacao Academica (AGUIA)'",10.11606/T.18.2003.tde-17122003-145512,,core
357195381,2007-01-01T00:00:00,"Here we report the identification of a novel human leukocyte antigen (HLA)-B44-restricted minor histocompatibility antigen (mHA) with expression limited to hematopoietic cells. cDNA expression cloning studies demonstrated that the cytotoxic T lymphocyte (CTL) epitope of interest was encoded by a novel allelic splice variant of HMSD, hereafter designated as HMSD-v. The immunogenicity of the epitope was generated by differential protein expression due to alternative splicing, which was completely controlled by 1 intronic single-nucleotide polymorphism located in the consensus 5 splice site adjacent to an exon. Both HMSD-v and HMSD transcripts were selectively expressed at higher levels in mature dendritic cells and primary leukemia cells, especially those of myeloid lineage. Engraftment of mHA ؉ myeloid leukemia stem cells in nonobese diabetic/severe combined immunodeficient (NOD/SCID)/␥c null mice was completely inhibited by in vitro preincubation with the mHA-specific CTL clone, suggesting that this mHA is expressed on leukemic stem cells. The patient from whom the CTL clone was isolated demonstrated a significant increase of the mHA-specific T cells in posttransplantation peripheral blood, whereas mHA-specific T cells were undetectable in pretransplantation peripheral blood and in peripheral blood from his donor. These findings suggest that the HMSD-v-encoded mHA (designated ACC  - Introduction Minor histocompatibility antigens (mHAs) are major histocompatibility complex (MHC)-bound peptides derived from cellular proteins encoded by polymorphic genes. Following human leukocyte antigen (HLA)-matched allogeneic hematopoietic cell transplantation (HCT), donor-recipient disparities in mHAs can induce a favorable graft-versus-leukemia (GVL) effect that is often associated with graft-versus-host disease (GVHD). 1-3 Significant efforts have been made to identify mHAs, particularly those specific for hematopoietic cells, since such mHAs are speculated to contribute to the GVL effect. The first report on the identification of a hematopoietic lineage-specific mHA, HA-1, was generated by the Goulmy group in 1998 (den Haan et al 4 ) as a result of biochemical analysis of peptides eluted from HLA-A*0201 molecules. The only other mHAs with selective expression in hematopoietic cells described to date are HA-2 5 ; ACC-1 and ACC-2 6 ; and DRN-7, 7 HB-1,  Immunogenicity of most autosomal mHAs identified to date results from single-nucleotide polymorphisms (SNPs) that cause amino-acid substitutions within epitopes, leading to the differential display/recognition of peptides between HCT donor and recipient via several mechanisms: peptide binding to MHC observed in HA-1/A2-, 4 HA-2-, 5 and CTSH-encoded mHAs 11 ; proteasomal cleavage in HA-3 12 ; peptide transport in HA-8 13 ; and altered recognition of MHC-peptide complex by cognate T cells in  In this study, we report the identification of a novel gene encoding an HLA-B44-restricted mHA that is recognized by the 2A12 cytotoxic T lymphocyte (CTL) clone and selectively expressed in primary hematologic malignant cells, especially those of myeloid lineage, multiple myeloma (MM) cells, and normal mature dendritic cells (DCs). The antigenic peptide recognized by 2A12-CTL was encoded by a novel allelic splice variant of HMSD, Submitted February 26, 2007;accepted April 2, 2007. Prepublished online as Blood First Edition paper, April 4, 2007; DOI 10.1182 DOI 10. /blood-2007 The online version of this article contains a data supplement. The publication costs of this article were defrayed in part by page charge payment. Therefore, and solely to indicate this fact, this article is hereby marked &apos;&apos;advertisement&apos;&apos; in accordance with 18 USC section 1734. For personal use only. on October 5, 2016. by guest www.bloodjournal.org From hereafter designated as HMSD-v, due to an intronic SNP located in the consensus 5Ј splice site adjacent to an exon. The leukemic stem cell (LSC) engraftment assay using severely immunodeficient mice demonstrated that the engraftment of primary acute myeloid leukemia (AML) cells was completely abolished by coincubation with the CTL clone before injection. These findings suggest that this novel mHA epitope may be an attractive therapeutic target for immunotherapy. Patients, materials, and methods Cell isolation and cell cultures This study was approved by the Institutional Review Board of Aichi Cancer Center. All blood or tissue samples were collected after written informed consent was obtained in accordance with the Declaration of Helsinki. B-lymphoid cell lines (B-LCLs) were derived from donors, recipients, and healthy volunteers. B-LCLs and all cell lines of hematologic malignancy were maintained in RPMI 1640 medium supplemented with 10% fetal calf serum (FCS), 2 mM L-glutamine, and 1 mM sodium pyruvate (referred to as complete medium). CD40 ligand-activated B (CD40-B) cells were generated as previously described.  cDNA library construction The cDNA library used in the present study was the same one that had been used to identify HLA-A31-and HLA-A33-restricted cathepsin H-encoded mHAs (ACC-4 and ACC-5) previously.  Transfection of 293T cells and ELISA Twenty thousand 293T cells retrovirally transduced with HLA-B*4403 were plated in each well of 96-well flat-bottomed plates, cultured overnight at 37°C, then transfected with 0.12 g of plasmid containing a pool of the cDNA library using Trans  Madison, WI). Ten thousand CTL-2A12 cells were added to each well 20 hours after transfection. After overnight incubation at 37°C, 50 L of supernatant was collected and IFN-␥ was measured by enzyme-linked immunosorbent assay (ELISA). Genotyping of polymorphisms Genomic DNA was isolated from each B-LCL with a QIAamp DNA blood kit (Qiagen). Total RNA was extracted using an RNeasy Mini Kit (Qiagen), and cDNA was synthesized by standard methods. Genomic DNA or cDNA was amplified using KOD-plus-DNA polymerase (Toyobo, Osaka, Japan) according to the manufacturer&apos;s instructions. The polymerase chain reaction (PCR) temperature profile was 30 cycles of 94°C for 15 seconds, 58°C for 20 seconds, and 68°C for 40 seconds on a GeneAmp PCR system 9700 (Applied Biosystems, Foster City, CA). The primer sequences used to amplify from exon 1 to exon 4 of HMSD cDNA were as follows: sense, 5Ј-CCTCTCCGACCCGGTCTC-3Ј; antisense, 5Ј-GGGAAAAGCTAAAGCTAGAGAAAA-3Ј. Exonic sequence and intronic sequence adjacent to HMSD exon 1 and 2 were amplified with primers as follows: exon 1 sense, 5Ј-GACTGAAAACTCCCGGACAG-3Ј; exon 1 antisense, 5Ј-GAAAGGTCTGGAGCAACAGG-3Ј; exon 2 sense, 5Ј-GCAGACATTCACTCACAGCA-3Ј; exon 2 antisense, 5Ј-AAGCAC-CCACATGAGTGACC-3Ј. PCR products were purified and directly sequenced with the same primer. Construction of minigenes and truncated genes for HMSD-v Mammalian expression plasmids containing the full-length or truncated forms of the HMSD-v cDNA were constructed by reverse transcriptase (RT)-PCR using the isolated cDNA clone as a template. The constructs all encoded a Kozak sequence and initiator methionine (CCACC-ATG) and a stop codon (TAA). All products were ligated into HindIII-NotI-cut pEAK10 vector (Edge Bio Systems, Gaithersburg, MD) and verified by sequencing. Epitope reconstitution assay The candidate HMSD-encoded epitopes were synthesized by standard Fmoc chemistry. 51  Cr-labeled donor B-LCLs were incubated for 30 minutes in complete medium containing 10-fold serial dilutions of the peptides and then used as targets in standard cytotoxicity assays. Real-time PCR assay for HMSD and HMSD-v expression cDNAs were prepared from various hematologic malignant cell lines, primary cell cultures, freshly isolated CD34 ϩ bone marrow (BM) and peripheral-blood hematopoietic cells and their subpopulations, immature and mature DCs, activated B and T cells, CD34 ϩ subsets of primary leukemic cells, and CD138 ϩ subsets of primary MM cells. Cell sorting was performed using magneticactivated cell separation (MACS) immunomagnetic beads (Miltenyi Biotec, Bergish Gladbach, Germany). A panel of cDNA made from different human adult and fetal tissues was purchased (MTC panels human I and II; BD Biosciences, San Diego, CA). Real-time PCR analysis was performed using the TaqMan assay as described previously. 11 Because of uncertainty of which allele(s) were included in each cDNA pool from the MTC panels, quantitative PCR primers and a probe were designed to detect the exon 3-4 boundary, which is shared by both alleles. The following sequences spanning the exon 3-4 boundary were used as primers with TaqMan probe to detect both HMSD and HMSD-v transcripts simultaneously: sense, 5Ј-AGAACTGCCAACGGGCTCTT-3Ј; antisense, 5Ј-TTGGTAGAATTTGCCACAGGAAT-3Ј; probe, 5Ј-(FAM)-CTTAT-GATTTCCTCACAGGTT-(MGB)-3Ј. To selectively detect HMSD-v transcripts, the following oligonucleotides specific for the exon 1-3 boundary were used: sense, 5Ј-CTCCGACCCGGTCTCACTT-3Ј; antisense, 5Ј-TCTCCATCTTCAC-CTCCGATTT-3Ј; probe, 5Ј-(FAM)-CAAAGTGCCCCAGTTC-(MGB)-3Ј. For personal use only. on October 5, 2016. by guest www.bloodjournal.org From CD45 mRNA expression was detected as described previously.  LSC engraftment assay of AML cells in immunodeficient NOG mice BM cells were obtained from patients with AML at diagnosis and then positively selected for CD34 ϩ subsets using MACS immunomagnetic beads (Miltenyi). NOD/Shi-scid, IL-2R␥c null (NOG) mice 22 were purchased from the Central Institute for Experimental Animals (Kanagawa, Japan). All mice were maintained under specific pathogen-free conditions in the Aichi Cancer Center Research Institute. The Ethical Review Committee of the Institute approved the experimental protocol. The ACC-2 D mHA-specific CTL clone 3B5 6 restricted by the same HLA-B*4403 allele as CTL-2A12 was used as a control CTL clone for this assay. AML cells (7.0 ϫ 10 6 ) were preincubated for 16 hours in CTL medium supplemented with 25 units/mL recombinant human IL-2 at 37°C with 5% CO 2 either alone or in the presence of CTL-2A12 or CTL-3B5 at a T-cell/AML cell ratio of 5:1. Thereafter, the cultures were harvested and resuspended in a total volume of 300 L and were inoculated via the tail vein of 8-to 12-week-old NOG mice (3 mice per group). Five weeks after inoculation, mice were killed, peripheral blood was aspirated from the heart, and BM cells were obtained by flushing the femora with complete medium. Nucleated cells were prepared for flow cytometry by incubation at 4°C for 20 minutes in PBS and 2% FCS with antihuman CD45 and CD34 (all from BD Biosciences) and were analyzed with a FACSCalibur flow cytometer and CellQuest 3.3 software (BD Biosciences). Percentage of engraftment was examined by 1-way analysis of variance (ANOVA) test. Real-time PCR assay for detecting CTLs specific for ACC-6, a newly identified mHA Complementary DNAs for a standard curve were prepared from mixtures of ACC-6-specific CTL clone (CTL-2A12) at various ratios with CD3 ϩ cells from healthy donors, and cDNAs of peripheral blood CD3 ϩ cells from the donor and patient before and after HCT were prepared from the AML patient (UPN-027). Real-time PCR analysis was performed using a TaqMan assay as described in &quot;Real-time PCR assay for HMSD and HMSD-v expression.&quot; The primers and fluorogenic probe sequences spanning the CTL-2A12 complementarity-determining region 3 (CDR3) were used to detect T cells carrying the CDR3 sequences identical to that of CTL-2A12. Samples were quantified with the comparative cycle threshold (C T ) method. The delta C T value was determined by subtracting the average GAPDH C T value from the average CTL-2A12 CDR3 C T value. The standard curve for the proportion of CTL-2A12 among CD3 ϩ cells (  Results Characterization of a CTL clone The CD8 ϩ CTL clone 2A12 (CTL-2A12) was 1 of 24 putative CTL clones isolated from day-197 post-HCT PBMCs of a male with refractory AML with multilineage dysplasia (UPN-027) receiving an HLA-identical HCT from his brother (A*2402, A*3303, B75, B*4403, Cw3, DR4, DR6). 11 The patient developed grade 1 acute GVHD in the first 2 years after transplantation and then suffered from glomerular IgG deposition and mild bronchiolitis obliterans organizing pneumonia. He is alive and in good condition and has been disease free for more than 3 years. Cytotoxicity assays revealed that CTL-2A12 lysed the recipient B-LCL and less efficiently phytohemagglutinin (PHA)-stimulated T-cell blasts but not donor B-LCL or natural killer (NK)-sensitive K562 cells (  Identification of the gene encoding the mHA and elucidation of the mechanism of antigenicity cDNA expression cloning using a cDNA library was conducted as described in &quot;Patients, materials, and methods, cDNA library construction.&quot; In the first round of screening, 1 of 96 plasmid pools induced IFN-␥ production by CTL-2A12. Two-step subclonings (ϳ5 cDNAs and 1 cDNA) of this pool finally resulted in the isolation of a cDNA clone (data not shown). The cDNA clone was sequenced and a BLAST search 23 revealed that this cDNA clone was previously unreported, but partially identical to XM_209104. XM_209104 was designated histocompatibility (minor) serpin domain containing (HMSD) by   MINOR ANTIGEN GENERATED BY INTRONIC SNP 1057 BLOOD, 1 AUGUST 2007 ⅐ VOLUME 110, NUMBER 3 For personal use only. on October 5, 2016. by guest www.bloodjournal.org From the Human Genome organization Nomenclature Committee Identification of an HLA-B*4403-restricted epitope of HMSD-v and epitope reconstitution assay To identify the epitope recognized by CTL-2A12, HMSD-v cDNA was divided into 3 minigenes overlapping each other by around 100 bp (  Subsequently, a peptide reconstitution assay was conducted. Undecameric peptide (MEIFIEVFSHF), its C-terminal deleted decameric peptide (MEIFIEVFSH), and N-terminal deleted decameric peptide (EIFIEVFSHF) were synthesized and titrated by adding to the mHA Ϫ donor B-LCL, and among these, only undecameric peptide showed dose-dependent cytolysis with a half-maximal lysis at 20 nM (  HMSD and HMSD-v mRNA expression in various hematopoietic and nonhematopoietic cells To determine the expression of HMSD and HMSD-v mRNA in a more comprehensive manner, real-time PCR was performed. Individual real-time PCR analysis specific for the HMSD-v transcript and for both HMSD and HMSD-v transcripts revealed that both were equally present in cDNA samples from B-LCLs heterozygous for the defined mHA (data not shown). Thus, further real-time PCR analysis was performed to quantify the total expression of both transcripts partly because mHA allelic status of commercial Due to the lack of exon 2, the mHA ϩ allele produced a smaller PCR product. Genotyping of the 2 SNPs mentioned above and cytolysis of B-LCLs by CTL-2A12 are summarized below the results of electrophoresis. The correlation between the genotyping results of SNPs at IVS2ϩ5, CTL-2A12 cytolysis, and the bands of electrophoresis produced by mHA ϩ and mHA Ϫ allele showed complete concordance. (C) Schematic representation of HMSD and HMSD-v and mapping of the region encoding the CTL-2A12 mHA epitope by minigenes. The HMSD-v cDNA was divided into 3 minigenes, and mammalian expression plasmids containing individual minigenes were constructed. 293T/B*4403 cells were transfected with individual plasmids and cocultured with CTL-2A12. Supernatants were then harvested and assayed for IFN-␥ production by ELISA. Release of IFN-␥ is expressed in arbitrary units (AUs) corresponding to optical density at 630 nm. 1058 KAWASE et al BLOOD, 1 AUGUST 2007 ⅐ VOLUME 110, NUMBER 3 For personal use only. on October 5, 2016. by guest www.bloodjournal.org From tissue cDNAs was unknown. High levels of expression were observed in primary AML and MM cells, mature DCs, CD40-B cells and PHA blasts (  It is possible that HMSD-v is differentially expressed from HMSD in cell types other than B-LCLs, where both transcripts were generated at similar levels. Thus, we examined both total HMSD and HMSD-v transcripts in various primary cells that were heterozygous for the ACC-6 allele. As shown in  Inhibition of human AML-cell engraftment in severely immunodeficient NOG mice by CTL-2A12 We first confirmed that the positively selected CD34 ϩ fraction of primary AML cells positive for HLA-B*4403 and the ACC-6 ϩ allele (all heterozygous) by genotyping was efficiently lysed by   To determine the mHA epitope, a minigene encoding 16 amino acids, which stimulated CTL-2A12, was serially deleted from its C terminus and then tested by ELISA. An undecameric but not decameric peptide was sufficient to induce IFN-␥ production from the CTL-2A12. indicates the presumed HMSD-v transcript region encoding a 53-mer polypeptide starting with an ATG codon and including the CTL-2A12 epitope. The location of the identified 2A12 epitope is shown below the HMSD-v cDNA. These 2 polypeptides have no homology because they are translated from different reading frames. MINOR ANTIGEN GENERATED BY INTRONIC SNP 1059 BLOOD, 1 AUGUST 2007 ⅐ VOLUME 110, NUMBER 3 For personal use only. on October 5, 2016. by guest www.bloodjournal.org From CTL-2A12 (  Next, to determine whether the ACC-6 mHA recognized by CTL-2A12 is indeed expressed on LSCs and thus might have been involved in a GVL effect in AML patient UPN-027, we performed the LSC engraftment assay as previously reported 27 but substituted the significantly immunodeficient NOG mice because the absence of NK activity in NOG mice has been shown to facilitate the engraftment level of xenogenic human hematopoietic cells.  Follow-up of ACC-6-specific CTLs in peripheral blood from an AML patient (UPN-027) To detect ACC-6-specific CTLs in peripheral blood from AML patient UPN-027 and from his donor, we performed real-time quantitative PCR (  Discussion Antigenicity of the majority of previously identified human mHAs is generated by differences in amino-acid sequence between donor  1060 KAWASE et al BLOOD, 1 AUGUST 2007 ⅐ VOLUME 110, NUMBER 3 For personal use only. on October 5, 2016. by guest www.bloodjournal.org From and recipient due to nonsynonymous SNPs. In this study, we identified a novel HLA-B44-restricted mHA epitope (ACC-6) encoded by an allelic splice variant of HMSD (HMSD-v) in which exclusion of exon 2 due to alternative splicing was completely controlled by an intronic SNP at IVS2ϩ5. Indeed, by RT-PCR, the novel HMSD-v was not detected in cDNA samples from mHA Ϫ B-LCLs, whereas it was detectable in mHA ϩ B-LCLs. An interesting question is why the splicing of exon 2 was completely controlled by the intronic SNP. In general, during intron splicing reactions, U1snRNA first binds the 5Ј splice site of an intron, spliceosome assembly starts, lariat formation is made with several other factors, and thereafter the intron is spliced out (reviewed in Valadkhan 28 ). Here U1snRNA is an important initiator of the cascade. It has been shown that aberrant splicing can result from mutations that either destroy or create splice-site consensus sequences at the 5Ј splice site such that approximately half of the observed aberrant splicing is exon skipping while intron retention is rarely observed.  The novel epitope was located on exon 3 and was transcribed from a reading frame different from the HMSD transcripts (  LSCs, which are present at very low frequencies, have a particularly strong capacity for proliferation, differentiation, and self-renewal 34 and likely play an important role in disease refractoriness or relapse after chemotherapy and transplantation. Thus, complete eradication of such stem cells is critical for cure in any treatment modalities. The LSC engraftment assay of AML cells in  For personal use only. on October 5, 2016. by guest www.bloodjournal.org From immunodeficient mice has been shown to be a powerful method for testing the effect of treatment, here mHA-specific CTLs, on LSCs. In addition, preliminary analysis has shown that CTL-2A12 lysed the CD34 ϩ CD38 Ϫ fraction of AML cells (  We performed quantitative RT-PCR analyses for HMSD transcripts in various tissues with great interest because cytotoxicity assays suggested its limited expression in hematopoietic cells. Notably, HMSD showed selective expression in several hematopoietic primary tumor cells (especially those of myeloid lineage), mature DCs, and activated B and T cells. Since high expression was observed in mature DCs as in the case of HMHA1 encoding HA-1 mHA, 36 immune responses to HMSD-derived mHAs may induce not only a GVL effect 37 against hematopoietic tumor cells but also GVHD, 38 since recipient DCs are responsible for initiating GVHD after HCT. Collectively, our data suggest that this novel mHA, ACC-6, might be a good target for immunotherapy inducing GVL if potential GVHD induction can be managed until recipient DCs have been eliminated early after HCT. Finally, relatively high expression of HMSD in the CD138 ϩ fraction of MM cells and their susceptibility to 2A12-CTL (  It is of interest to correlate clinical outcomes with ACC-6-specific T-cell kinetics after HCT using reagents such as tetramers. The preparation of HLA-B44 tetramer, however, is known to be very difficult, 39 so we used real-time quantitative RT-PCR using CTL-2A12 CDR3 sequence-specific primers/probe, because Yee et al  The therapeutic applicability of particular mHAs, calculated from the disparity rate and restricting HLA allele frequency, is an issue of interest",Alternative splicing due to an intronic SNP in HMSD generates a novel minor histocompatibility antigen,https://core.ac.uk/download/357195381.pdf,,,,core
24444175,06/02/2008,"Traffic congestion is one of the leading causes of lost productivity and decreased standard of living in urban settings. Recent advances in artificial intelligence suggest vehicle navigation by autonomous agents will be possible in the near future. In a previous paper, we proposed a reservation-based system for alleviating traffic congestion, specifically at intersections. This paper extends our prototype implementation in several ways with the aim of making it more implementable in the real world. In particular, we 1) add the ability of vehicles to turn, 2) enable them to accelerate while in the intersection, and 3) augment their interaction capabilities with a detailed protocol such that the vehicles do not need to know anything about the intersection control policy. The use of this protocol limits the interaction of the driver agent and the intersection manager to the extent that it is a reasonable approximation of reliable wireless communication. Finally, we describe how different intersection control policies can be expressed with this protocol and limited exchange of information. All three improvements are fully implemented and tested, and we present detailed empirical results validating their effectiveness. 1",Multiagent Traffic Management: An Improved Intersection Control Mechanism,,,,,core
20689063,02/04/2008,"This paper presents an on-going research focused on providing artificial intelligence to a traffic light controller. RACE intends to use this intelligent traffic light in its small educative traffic safety village, in which students exercise practices using traffic signs (vertical and horizontal) and traffic lights. One intersection in this village is controlled by traffic lights and the University Rey Juan Carlos group is designing an artificial vision system to control it. Control will be done with two CCD cameras and an automaton. One computer will process the images and provide orders to the automaton (therefore to the traffic light). One CCD will control vehicle speed and the other will be dedicated to pedestrians. Use of computer and cameras will result in an adaptive control, in which time and sequence of lights will depend on the real conditions of zebra crossing. In this stage of the control, risky situations will be avoided. Control will be done to give enough time for pedestrian and to enforce vehicles to moderate and limit speed. Several sequences of lights will be implemented and each one will be dedicated to avoid specific situations: pedestrian in zebra crossing, excessive vehicle speed, etc. System will be tested in real conditions in RACE premises, with pedestrian and vehicles (in this case: bicycles). Feedback obtained in this test will be valuable to improve the system and to design a full-scale experiment in an urban environment. Also, the system will be fully parameterized, so it is possible to change values and to observe any effects on traffic conditions or in driver or pedestrian behavior",SUMMARY,,,,,core
82714363,31/10/2002,"AbstractThis volume contains the Proceedings of the 9th Workshop on Logic, Language, Information and Computation (WoLLIC'2002). The Workshop was held in Rio de Janeiro, Brazil from July 30 to August 2, 2002, in the campus of the Pontifícia Universidade Católica do Rio de Janeiro (PUC-Rio).WoLLIC is a series of workshops which started in 1994 with the aim of fostering interdisciplinary research in pure and applied logic. The idea is to have a forum which is large enough in the number of possible interactions between logic and the sciences related to information and computation, and yet is small enough to allow for concrete and useful interaction among participants. Previous versions were held at: Recife (Pernambuco, Brazil) in 1994 and 1995; Salvador (Bahia, Brazil) in 1996; Fortaleza (Ceará, Brazil) in 1997; São Paulo (Brazil) in 1998; Itatiaia (Rio de Janeiro, Brazil) in 1999; Natal (Rio Grande do Norte) in 2000; Brasília (Distrito Federal, Brazil) in 2001. [new] Scientific sponsorship comes from the Interest Group in Pure and Applied Logics (IGPL), the European Association for Logic, Language and Information (FoLLI), the Association for Symbolic Logic (ASL), the Sociedade Brasileira de Computação (SBC), and the Sociedade Brasileira de Lógica (SBL).[end new] Scientific sponsorship comes from the Interest Group in Pure and Applied Logics (IGPL), the European Association for Logic, Language and Information (FoLLI), the Association for Symbolic Logic (ASL), the Sociedade Brasileira de Computação (SBC), and the Sociedade Brasileira de Lógica (SBL). Funding was kindly given by: (i) CNPq (Conselho Nacional de Desenvolvimento Científico e Tecnológico, the scientific and technological development council of the Brazilian Ministério da Ciência e Tecnologia) (grant 451491/2002-5); (ii) CAPES (Fundação Coordenação de Apoio ao Aperfeiçoamento de Pessoal de Nível Superior, a Foundation for the Development of Higher-Education under the Brazilian Ministério da Educação e do Desporto) (grant PAEP0169/02-1); (iii) PUC-Rio (Pontifícia Universidade Católica do Rio de Janeiro). Contributions were received in the form of short papers in all areas related to logic, language, information and computation, including: pure logical systems, proof theory, model theory, algebraic logic, type theory, category theory, constructive mathematics, lambda and combinatorial calculi, program logic and program semantics, logics and models of concurrency, logic and complexity theory, proof complexity, foundations of cryptography (zero-knowledge proofs), descriptive complexity, nonclassical logics, nonmonotonic logic, logic and language, discourse representation, logic and artificial intelligence, automated deduction, foundations of logic programming, logic and computation, and logic engineering.Apart from the contributed papers (14), and the invited talks (6), the programme includes 6 tutorial lectures:
				•Some model theory of ordered structures by Ricardo Bianconi (Departamento de Matemática Pura, Instituto de Matemática e Estatística, Universidade de São Paulo, Brazil)•Computing with Real Numbers by Felipe Cucker (Department of Mathematics, City University of Hong Kong, People's Republic of China)•Model Checking Games by Erich Grädel (Mathematische Grundlagen der Informatik, RWTH Aachen, Germany)•The Metalanguage Lambda Prolog and its Implementation by Gopalan Nadathur (University of Minnesota, USA)•Automata theory and logic by Igor Walukiewicz (Bordeaux University, France)•States of Knowledge (Tutorial) by Rohit Parikh (Department of Computer and Information Science, Brooklyn College, City University of New York, USA)All papers in the volume were reviewed by the program committee consisting, besides editor, of
				Mauricio Ayala-Rincón(Department of Mathematics, Universidade de Brasília, Brazil)Mario Benevides(Institute of Mathematics, Universidade Federal do Rio de Janeiro, Brazil)Anuj Dawar(Computer Laboratory, Cambridge University, England)Philippe de Groote(LORIA, France)Roger Maddux(Department of Mathematics, Iowa State University, USA)Toniann Pitassi(Department of Computer Science, Toronto University, Canada)Bruno Poizat(Institut Girard Desargues, Université Claude Bernard Lyon I, France)Alberto Policriti(Department of Mathematics and Informatics, Università di Udine, Italy)Glynn Winskel(Computer Laboratory, Cambridge University, England)The organising committee consisted, besides editor, of
				Edward Hermann Haeusler(Department of Informatics, Pontifícia Universidade Católica do Rio de Janeiro, Brazil)Claus Akira Matsushigue(Institute of Mathematics and Statistics, Universidade de São Paulo, Brazil)Anjolina Grisi de Oliveira(Center of Informatics, Universidade Federal de Pernambuco, Brazil)Luiz Carlos Pereira(Department of Philosophy, Pontifícia Universidade Católica do Rio de Janeiro, Brazil)Ruy de Queiroz(Center of Informatics, Universidade Federal de Pernambuco, Brazil)Jorge Petrúcio Viana(Coordination of Postgraduate Programmes in Engineering and Systems, Universidade Federal do Rio de Janeiro, Brazil)The volume will be published as volume 67 in the series Electronic Notes in Theoretical Computer Science (ENTCS). This series is published electronically through the facilities of Elsevier Science B.V. and its auspices. The volumes in the ENTCS series can be accessed at the URL http://www.elsevier.nl/locate/entcsA printed version of the current volume is distributed to the participants at the workshop in Rio de Janeiro.We are very grateful to the following persons, whose help has been crucial for the success of WoLLIC'2002: Mike Mislove, one of the Managing Editors of the ENTCS series, for his assistance with the use of the ENTCS style files; Thanks are also due to the Departments of Philosophy and Informatics of the Pontifícia Universidade Católica of Rio de Janeiro, which has provided the logistic support to the organising committee.July 26, 2002 Ruy de Queiro",Preface Volume 67,https://core.ac.uk/download/pdf/82714363.pdf,Elsevier B.V.,10.1016/S1571-0661(05)80556-0,,core
36204603,2009-01-01T00:00:00,"Teleoperation is a difficult task, particularly when controlling robots from an isolated operator station. In general, the operator has to solve nearly blindly the problems of mission planning, target identification, robot navigation, and robot control at the same time. The goal of the proposed system is to support teleoperated navigation with real-time mapping. We present a novel scan matching technique that re-considers data associations during the search, enabling robust pose estimation even under varying roll and pitch angle of the robot enabling mapping on rough terrain. The approach has been implemented as an embedded system and extensively tested on robot platforms designed for teleoperation in critical situations, such as bomb disposal. Furthermore, the system has been evaluated in a test maze by first responders during the Disaster City event in Texas 2008. Finally, experiments conducted within different environments show that the system yields comparably accurate maps in real-time when compared to higher sophisticated offline methods, such as Rao-Blackwellized SLAM.(Best Paper Award Finalist)Artificial Intelligence & Integrated Computer System",Operator-Assistive Mapping in Harsh Environments,,'Institute of Electrical and Electronics Engineers (IEEE)',10.1109/SSRR.2009.5424159,,core
53833657,2005,"Negli ultimi decenni la Pubblica Amministrazione italiana è stata al centro di un profondo e continuo processo di modernizzazione, volto a promuovere l’introduzione di modelli gestionali più efficienti ed efficaci, più simili e più vicini a quelli adottati nelle imprese private. Sulla scia di questi cambiamenti, le amministrazioni sono state chiamate ad adottare “nuovi” sistemi di gestione delle risorse umane caratterizzati da mobilità, da stipendi basati sul merito e da un sistema di incentivi ancorato ai risultati conseguiti e non più e non solo alla volontà di qualcuno.

Le buone pratiche di gestione delle risorse umane potrebbero diventare essenziali per sviluppare una forza lavoro pubblica sempre più qualificata, per favorire la crescita e lo sviluppo delle competenze del personale, per garantire la flessibilità organizzativa e, da ultimo, per migliorare tanto la produttività, quanto l’efficienza del settore pubblico. 

Si tratta di una importante innovazione, ancora tutta da realizzare. Per farlo, diventa necessario compiere un salto di qualità che coinvolga non solo le competenze tecniche ma anche gli aspetti e le resistenze culturali che caratterizzano, da sempre le organizzazioni pubbliche.

Tuttavia, nonostante le sue possibili potenzialità, ci sono diversi problemi relativi all’adozione di un sistema di valutazione delle prestazione: spesso le persone hanno paura della valutazione, solo perché questa viene percepita come un controllo. Questo è ancora più vero nella pubblica amministrazione italiana, dove sembra che il solo scopo per il quale i sistemi di valutazione della prestazione siano stati utilizzati è quello relativo alla remunerazione: si rileva il contributo fornito dal singolo solo al fine di valutarne la coerenza con le aspettative dell'organizzazione e identificare, di conseguenza, la “giusta” remunerazione. Ma non è tutto. Un altro problema che caratterizza la Pubblica Amministrazione è quello della scarsità delle risorse economiche da poter distribuire, la remunerazione e, nello specifico, gli incentivi perdono così sempre più spesso la loro funzione d’uso tradizionale che consiste nel motivare i dipendenti pubblici. 

Per queste ragioni sembra fondamentale che non solo il management pubblico, ma anche i politici comprendano le possibili e reali finalità dei sistemi di valutazione della prestazione. Obiettivo del presente lavoro è, dunque, quello di fornire una panoramica sullo stato dell'arte, in termini di finalità, metodi e risultati, dell’adozione di sistemi di valutazione della prestazione nelle pubbliche amministrazioni italiane.Over the last twenty years the Italian Public Administration was at the core of a deep and continuous process of modernization, looking for introducing the more efficient and effective management models of private enterprises. Administrations were called upon to implement human resource management systems characterized both by mobility and a salary based on merit and professional incentives linked to results.

Effective human resource management practices could became essential to develop a skilled workforce, to favour the growth and development of the competence of the personnel, to guarantee the organizational flexibility and to address public sector productivity and efficiency. In order to realize this innovation a qualitative leap is required touching the technical competencies but also the cultural aspects that characterize the Italian public administrations.

However, there are different problems related to the performance appraisal system: people are often afraid of the appraisal, because they frequently perceive it as a control. In Italian Public Administration, the only purpose for which the performance appraisal systems have been used up to now and, consequently, people can see, is that related to pay, where the contribution produced by each person is detected, in order to assess their consistency with the expectations of the organization and identify the incentives needed to motivate. But there is another problem: in Public Administration there are often few economic resources to distribute, so this purpose is not motivating and inconsistent. For these reasons it should be very important that both governors and civil servants understand the possible and real purposes of the performance appraisal systems. 

This work intends to give an overview of the state of art of implementation of the performance appraisal systems in Italian public organizations and the quality of implementation, in terms of purposes, use, method and results",La valutazione della prestazione: costoso rito politico o strumento di convenienza organizzativa?,,McGraw-Hill,,,core
199610301,2001-01-01T00:00:00,"This dissertation investigates methods of real-time adaptive traffic signal control in the context of single isolated intersection and coordinated urban network applications. A primary goal in this dissertation is to identify and address scenarios where real-time optimized controllers do not maintain competitive performance with off-line calibrated, vehicle-actuated control techniques. An extensive literature review is supplemented by subsequent simulation experiments. Several strategies were implemented and evaluated, including OPAC, PRODYN, COP, ALLONS-D, Webster's optimized fixed-time control, and vehicle-actuated control. In particular, evaluation is based on simulation of a single, isolated intersection, where all algorithms are required to adopt the exact, deterministic traffic model used by the simulation. This approach eliminates confounding factors in comparison of algorithms, such as detector placement and disparate traffic models, focusing evaluation on the efficiency of the algorithms and their ultimate performance in terms of vehicle delay. A new algorithm is developed, employing neuro-dynamic programming techniques, also known as reinforcement learning techniques. Several very effective pruning strategies are also constructed. The final product is a very efficient algorithm capable of solving problems up to 2000 times faster than the most efficient previously published algorithm tested, with an 8% decrease in delay. This algorithm is then extended to a generalized, multi-ring control formulation. Simulation results with a standard dual-ring, eight-phase controller demonstrate that efficient, real-time solutions are achieved with a corresponding 12--22% reduction in delay relative to dual-ring, vehicle-actuated control. The real-time optimized, multi-ring controller is finally extended for urban network applications, expanding the objective function to consider downstream performance measures, and adopt standard, vehicle-actuated type coordination constraints. Control on an 8-intersection arterial is evaluated using a CORSIM simulation over a range of traffic conditions. Results are compared with TRANSYT optimized fixed-time control, coordinated vehicle-actuated control, and RHODES. Two regimes of control are revealed, where cyclic coordination constraints provide a significant benefit, and where they prevent more effective control. An adaptive coordination layer is prescribed as a unifying architecture with the potential of obtaining effective control under both regimes. The adaptive control layer specification is explicitly distinguished from existing algorithms, such as SCOOT, SCATS, and VFC-OPAC",Design and evaluation of real-time adaptive traffic signal control algorithms,,The University of Arizona.,,,core
22935353,2005,"Traffic congestion is one of the leading causes of lost productivity and decreased standard of living in urban settings. Recent advances in artificial intelligence suggest vehicle navigation by autonomous agents will be possible in the near future. In a previous paper, we proposed a reservation-based system for alleviating traffic congestion, specifically at intersections. This paper extends our prototype implementation in several ways with the aim of making it more implementable in the real world. In particular, we 1) add the ability of vehicles to turn, 2) enable them to accelerate while in the intersection, and 3) augment their interaction capabilities with a detailed protocol such that the vehicles do not need to know anything about the intersection control policy. The use of this protocol limits the interaction of the driver agent and the intersection manager to the extent that it is a reasonable approximation of reliable wireless communication. Finally, we describe how different intersection control policies can be expressed with this protocol and limited exchange of information. All three improvements are fully implemented and tested, and we present detailed empirical results validating their effectiveness. 1",Multiagent traffic management: an improved intersection control mechanism,,ACM,,,core
24530409,2005,"Abstract — Designing robots that are capable of interacting with humans in real life settings is a challenging task. One key issue is the integration of multiple modalities (e.g., mobility, physical structure, navigation, vision, audition, dialogue, reasoning) into a coherent framework. Taking the AAAI Mobile Robot Challenge (making a robot attend the National conference on Artificial Intelligence) as the experimental context, we are currently addressing hardware, software and computation integration issues involved in designing a robot capable of sophisticated interaction with humans. This paper reports on our design solutions and the current status of the work, along with the potential impacts this design will have on human-robot interaction research. Index Terms — Socially interactive mobile robot, Embodied interaction and communication, Multi-modal communication",Modularity and integration in the design of a socially interactive robot,,,,,core
24458220,2007,"In modern urban settings, automobile traffic and collisions lead to endless frustration as well as significant loss of life, property, and productivity. Recent advances in artificial intelligence suggest that autonomous vehicle navigation may soon be a reality. In previous work, we have demonstrated that a reservation-based approach can efficiently and safely govern interactions of multiple autonomous vehicles at intersections. Such an approach alleviates many traditional problems associated with intersections, in terms of both safety and efficiency. However, the system relies on all vehicles being equipped with the requisite technology — a restriction that would make implementing such a system in the real world extremely difficult. In this paper, we extend this system to allow for incremental deployability. The modified system is able to accommodate traditional human-operated vehicles using existing infrastructure. Furthermore, we show that as the number of autonomous vehicles on the road increases, traffic delays decrease monotonically toward the levels exhibited in our previous work. Finally, we develop a method for switching between various human-usable configurations while the system is running, in order to facilitate an even smoother transition. The work is fully implemented and tested in our custom simulator, and we present detailed experimental results attesting to its effectiveness. ",Sharing the road: Autonomous vehicles meet human drivers,,,,,core
68891087,30/11/2009,"UnrestrictedC5/C6 tetraplegic patients and transhumeral amputees may be able to use voluntary shoulder motion as command signals for a functional electrical stimulation (FES) system or a transhumeral prosthesis.  Such prostheses require the control of endpoint position in three-dimensions, hand orientation, and grasp.  Stereotyped relationships, termed “postural synergies,” exist between the shoulder, forearm, and wrist joints emerge during goal-oriented reaching and transport movements as performed by able bodied subjects.  Thus, the posture of the shoulder can potentially be used to infer the posture of the elbow and forearm joints during reaching and transport movements.  To fit these synergies we utilized three-layer artificial neural networks (ANNs).  In contrast to previous work in this field, we initially trained ANNs with three rotational angles at the shoulder to predict the elbow angle during reaches in a horizontal plane.  We found that the ANNs could predict elbow angle remarkably well across the entire horizontal workspace during offline and online analysis.  In the subsequent works, we extended this paradigm to include shoulder translation movements in addition the shoulder rotational angles to predict forearm angle and to control grasping in 3D extrapersonal space.; The complete inferential command system (ICS) was deployed for use in a virtual reality reach and grasp task.  In order to examine whether the ANNs generalized across subjects, we alternated the use of ANNs trained on the subject’s own data and ANNs trained with a novel subject’s data.  Furthermore, we compared the performance of subjects using the ICS with subjects operating the simulated prosthesis in virtual reality according to complete motion tracking of their actual arm and hand movements.  Subjects using the ICS were able to complete the task at a high percentage and with low spatial variability across the workspace.  Mean task completion times of subjects using the ICS compared favorably to subjects using full motion capture regardless of which set of ANNs were used.  Inferring the desired movement of distal joints from voluntary shoulder movements appears to be a relatively simple, intuitive and non-invasive approach to obtaining command signals for prostheses to restore distal arm and hand function",A non-invasive and intuitive command source for upper limb prostheses,,University of Southern California. Libraries,,,core
378581175,2003-09-01T00:00:00,"The development of the Virtual Reality Modelling Language (VRML) for the Internet has resulted in the emergence of a multiplicity of 3D web sites.  The metaphor used by these sites varies enormously from virtual galleries to virtual cities and style varies from abstract to reality.  Additionally these worlds are populated by virtual objects, some having reactive or interactive properties, including movement, audio, video, databases, artificial intelligence etc.  Perhaps the most stimulating embodiment of these new environments are those that offer the participant the opportunity to meet and communicate with other visitors exploring the same virtual space/world.  The Glasgow Directory is an established 3D web space,

with around 10,000 visitors per year.  The model represents approximately 10,000 properties in the city and is populated by contextual information on its culture and socio-economic topography.  This paper describes the background to this VR space, and suggests a set of design criteria for successfully deploying multi-user software within this and similar

environments. These criteria take into account lessons learned by ‘observing’ and analysing how participants interact with the existing system under different conditions and also what benefits they perceive on entering the environment via the multi-user interface.  These recommendations will hopefully be applicable to a wide spectrum of internet virtual environment builders and users",Visit VR Glasgow : Welcoming Multiple Visitors to the Virtual City,,,,,core
235573972,2001-02-28T00:00:00,"An archive of the Magrath Trading Store News.The University of Lethbridge Library received permission from the Wes Balderson to digitize and display this content.MAGRATH NEWS
Published Weekly Since 1932 by
The Magrath Trading Company
?------------------------------------- <
Points of Interest
35* February 28, 2001
T^e Rd^monò playhouse Society
Proudly Presents
The Blue Castle
A dramatic play based on tfe book by L.M. Montgomery
Stage Adaptation by Kjiye Merrill
Performance Dates:
March 22 - 31st
8:00 p.m. Raymond Broadway Theatre
(Sunday & Wednesday Dark)
Directed by Kaye Merrill
One of L.M. Montgomery’s most endearing classics.
Starring:
Beth Nielson as Valancy Melanie Rudd as Mrs. Stirling
Mike Holt as Barney Snaith Robin Steed as Cousin Stickles
Stuart Fowler as Uncle Benjamin
Tickets available at Raymond Broadway Theatre
Beginning March 5,2001 Weekdays 1-5 p.m.
or by nailing 752-2336 Ticket Price $10.00
Dinner Theatre Tickets call 752-4036 or 752-3572
Monday 26th Family Night - Tickets $8.00
Tuesday 27th Youth Night - Tickets $8.00
* Sports
* Rod & Gun Banquet
* Grocery Specials
* Home Hardware
Sales
- -
—
Inside this issue:
Community Interests 1-3
Sports 4
Classified Ads 6-7
Community Calendar 8
Grocery Specials 9-10
Meat & Produce 11
Hardware Specials 12
i____________________
?
■Ï Make your dream come true
have your home built by
JENSEN ENTERPRISES
New construction:
Additions:
Patio Decks:
403-758-3669
Ken Jensen, Owner
Fine woodwork:
Hardwood furniture:
Renovations:
Box 93
Magrath, AB TOK 1 JO
The March meeting of
St. Joseph’s C.W.L.
will be held on Tuesday, March 13th
at 7:00 p.m. at the home of Elizabeth Stanek.
Reminder that memberships are due.
Jtagrnth United Church
is having a fund raiser.
They are selling
Avon
Skin-So-Soft Candles in a Can
3 y4"" x 3 72""
Cost: $10.00 each - $5 of
which goes to the church.
ATTENTION!!
Members, Parents, Friends
of the
Magrath Concert Bands
1972 to 1975
Kennedy Jenson, has compiled a number
of Band selections on to
a 40 minute CD
which will be available for sale as soon
as the covering Jacket is made.
We are asking you to search your photos for
pictures of the band or of individual members -
humorous or sedate!!
These she plans to use in a collage for the
Jacket.
If you have any pictures please contact:
Ruth Jenson 653-3461
Mary Yvonne Hohm 758-6608
VINNIE S KITCHEN
j Preparing all Kinds of Food For Special Events
Full Service Catering or We Deliver For Yon to Serve
Over 25 Varieties of Our Own Soup
No a restaurant - a large commercial kitchen with
Alberta Health Food Establishment Permit
For great comfort food» Call 758-3292
To order you candle call:
Betty Twitchen at 758-3240
or
Pinky Coleman at 758-6899
MISSIONARY CORNER :
ai
Wayne Kinder, son of James & Debera, has been;
called to the Idaho, Boise Mission. His farewell j
will be held on March 18th at 1:00 p.m. in the!
Garden Place Chapel. He reports to the M.T.C. on •
March 28th. i
WHY WE LOVE CHILDREN
A small boy is sent to bed by his father.
Five minutes later:
""Da-ad...""
""What?'' h
""I'm thirsty. Can you bring me a drink of water?“
""No. You had your chance. Lights out.“
Five minutes later: ""Da-aaaad... I'm THIRSTY.
Can I have a drink of water??“
""I told you NO! If you ask again, I'll have to
spank you!!“ ?
Five minutes later... ""Dciaaa-aaaad..:
""WHAT??!!""
""When you come in to spank me, can you bring
me a drink of water?""
3
Cards of Thanks
Our families would like to express our
appreciation to all those who helped with the
wedding in anyway. People were so generous with
help and also with gifts. We are grateful for such a
wonderful community. Thankyou so much.
Brad & K.D. Wolsey
Rod & Sharon Gibb
Scott & Falene Wolsey
Thankyou to all who gave us condolences
for the loss of our daughter, sister and best friend,
Gail Drab.
Muriel Sabey
Sharon Owens
SENIOR CENTRE NEWS
Supper will be held tonight, February 28th
at 5:00 p.m. Senior Centre
Your Hostesses for Februaiy are Bernice
Coleman, Mary Tanner & Evie Hillmer.
*******
Contact Jean or Jack Butlin for rental of the
Seniors Centre. Fee $75.00
Phone 758-3030
*******
BINGO*BINGO*BINGO
Bingo is tomorrow, Thursday, March 1st
at the Seniors Centre
Doors open at 6:30 p.m.
Bingo at 7:00 p.m.
Public Welcome!!
Magrath Co-op Annual Meeting
Wednesday, March 21
2:00 p.m.
at the Magrath Co-op
All members welcome.
Lance Miller
Agent
Bub. 320-6411
Fax. 320-6427
SENIOR NEWS
We will be going to Stage West in Calgaiy on
Wednesday, March 2l5t.
Matinee for luncheon & stage play
OVER THE RIVER & THROUGH THE WOODS”
Members - $45.00 Non-members - $55.00
ALSO
Raymond Playhouse Society presents
“THE BLUE CASTLE""
is a play adapted from L.M. Montgomeiy’s book.
March 22-31
Thursday, March 29th has been reserved for the
Magrath Seniors.
Cost: $25/person includes dinner & theatre.
Bus cost will be extra.
Call now to make your reservations.
Phone: Levaun Thompson - 758-6672
Hazel Rasmussen - 758-3545
Norma Owens - 758-3560
SPORTS
BASKETBALL
CURLING
the Zenith scoring with 31 points. Brooks Blacki and Allan Tollestrup added 22 and 19 respectively.
Zeniths have 14-0 Record
Last Friday night, February 23, the Zeniths and Eagles travelled to Cardston for what turned out to be a very exciting game. The Zeniths beat the Cardston Cougars with a score of93-88 to make this years league record 14-0.
Jimmy Balderson came off his sick bed to lead
The Eagles didn’t fair as well as Cardston came out on top with a score of68-64. Dylan Alston scored 22 points and Kyle Clifton added 14.
j SOCCElT ...
SOCCER REGISTRATION
Ages 4-U13 play in Magrath $35 perplayer, $105 family of3 ormore
U16 & U18 boys play in Lethbridge $70 per player
(teams playing in Lethbridge must be submitted by April 1)
(players born in 1982 not eligible for U18 team)
Coaches and referee registration
Place: Magrath Pool Foyer
Dates: Th urs. March 15 7:00pm - 9:00pm
Sat. March 17 9:00am-12:00pm
Thurs. March 22 7:00pm - 9:00pm
Rod & Gun Cluh Annual Banquet & Awards Night
Saturday, March 3, 2001 6:30 p.m.
Magrath United Church Lots of Door Prizes - Excellent Food Guest Speaker
Adults $10.00 each - Under 16 $5.00 each Tickets at Trading Co. Hardware EVERYONE WELCOME!
Magrath Curling Club News
The Magrath Curling Club held their Annual Ladies Bonspiel, February 22 to February 25 with ten teams participating. Results of the bonspiel were:
Carly Quigley and her Lethbridge team won over Sandy Meldrums Magrath team to capture the first event sponsored by Kenneth C. Long Seeds Ltd.
The Sherri Kowalchuk rink of Lethbridge won over Janet Walter s Lethbridge/Cardston rink to win the second event sponsored by Alberta Treasury Branch of Magrath. The Jean Malmberg rink from Magrath placed third with the Sally Bond team of Coaldale placing fourth.
Saturday evening, Ken Leavitt, Mike Holt and Investor’s Group sponsored a “Draw to the Button” challenge. Each rink selected a team mate to enter the shoot-out. No one claimed the $5000.°° possible prize. Kristen Simpson, representing the Janet Walter rink drew her rock within 15 inches of the button and Ken Leavitt presented each member of her team with a vest.
The Magrath Curling Club would like to Thank everyone who made donations to make our bonspiel a success! ' „ .
Magrath Mixed Bonspiel
March 13 - 18,2001
Banquet and Dance: Saturday, March 17, 2001
A, .
To Enter, Contact:
Maurice Bevers at 758-6726
or
Ron Balderson at 758-6521
Please submit your sports scores and schedules by dropping off at the Trading Company office, faxing to 758-6888, or emailing to tidmarsh@telusplanet.net.MARCH
Adult Education Computer Courses
January-March 2001
Instructors: Bonny West/Brenda Beck
A
Cardston: Microsoft Word-Intermediate
This 3 week will cover creating and editing tables, using Word’s drawing tools, adding headers, footnotes,
and endnotes, importing files, modifying templates, mailing labels, customizing bullets and special characters,
adding web links, and additional editing features.
Dates: Tuesdays, March 6-20 (3 weeks)
Location: Cardston High School
Time: 7:00-9:30 p.m.
Tuition: $35
Magrath: MakeYour Own Memory Video
Using slides, photos, scanners, video clips, background music, voice annotations, transitions, and easy to use
software, it is now possible to create “living” video scrapbooks for home movies, weddings, family reunions, baby
books, gifts, sports highlights, etc. Participants will create their own personal memory video output to VHS tape.
(These movies may also be stored on CD, on the Web, or sent via email.)
Participants need to bring their own photographs & video clips and choice of background music (preferably
CD). Additional photos and music and blank tapes will be provided.
Dates: Wednesdays, March 7-28 (4 weeks) lime: 7:00-10:00 p.m.
Location: Magrath Elementary School Tuition: $65
To register call Kathy Richards @ 653-4991
We specialize in:
• Video slide shows (weddings,
reunions, sports highlights,
the Web (Output format:
VHS tape & CD)
• Hi-Res Photo scanning/Repair &
Retouching/Printing (color & B/W),
OCR (text)
• Banners, Posters, Ads, etc.
• Private tutoring
• Computer troubleshooting
• Desktop Publishing
Phone Bonny/Brenda
758-3844 (evenings)/
leave a message (day)
WWW
FEBRUARY
HEART & STROKE
MONTH
is quickly drawing to a close. We still
have 20 Volunteer Canvassers out there
and ask that they get their districts
finished and turn their books in as soon
as possible.
If you have been missed & would like to
give, call 758-3030 a pick up could be
arranged.
A sincere thanks to all that have given.
A hearty note of thanks to all our 48
volunteers. We appreciate you all. You
are what makes our canvass a success
each year.
Magrath Hospital Awdliaiy
6
Zola Woolf
Passed away in Magrath on Tuesday,
February 27*\ 2001 at the age of 68 years,
beloved sister of Verna Leishman Peterson
ofCardston.
Friends may meet the family at
Salmon Funeral Home, Cardston on
Thursday, March 1st from 7:00 to 8:00 p.m.
or at the Church from 10:00 to 10:45 a.m.
prior to the service. The Funeral Service will
be held at the Church of Jesus Christ of
Latter-day Saints, Magrath Stake Center
on Friday, March 2nd at 11:00 a.m. with
Bishop Burns Alston officiating. Interment in
the Hill Spring Cemetery at 2:30 p.m.
<- ' 4.,
*2Vcti
V,
NON taxable
S/LEni
1 TCM'A4* '
^coST
_______CLASSIFIEDS____
FREE MARKET
* MISSING - 6 month old gray Llama.
Went missing from my corral sometime on
Saturday, March 24. If you have any information
Please call 758-6051.
BUY & SELL
* Bumper Bale Handler for sale.
In good condition. Phone 758-3226.
BUSINESS
* TUPPERWARE
Have defectives? Need replacements?
Call Heather MacKay at 758-3662.
All orders receive discounts, no pressure, last
order. Call before March 7*.
* If your child needs help in Math or Reading the
Kumon Method might help. For more
information call Rusty or Martine Rollingson
at 758-3648.
CLASSIFIEDS
*
Jeanie s Hair Fashion
136 S. 1 St. W. - four doors south of the Trading Company. 758-3379. Open Tues. - Fri. Professional Hair Care at pleasing prices.
*
CANADIAN SECURITY SYSTEMS
We sell, install & service alarms, safes, camera systems, deadbolts & key locks. Call Ross Moore at 758-3945. Free estimate.
*
For all your cleaning needs from hospital clean to a touch up, carpet to ceiling & everything in between. No job too big or too small.
Call Wayne’s Carpet & Upholstery Cleaning 758-6414.
*
Will Tutor, grade 1-12 Math or Chemistry. Call Martine Rollingson at 758-3648.
USED VEHICLES
* FOR SALE - 1999 Ford XLT loaded short box 4x4. 30,000 km. Offers. Call Codey - 758-3395 days or 758-3767 evenings.
REAL ESTATE
*******
FOR RENT
*
Storage space & Commercial space for rent. For information call 758-3876.
*
Rent or Buy! Newly decorated 2 bedroom home, large garage on beautiful double lot. Magrath. Phone 758-3789.
*
2 bedroom apartment. For information call 758-3876.
*
2 bedroom, furnished apartment. No pets. Abstainers. Call Ty Alston - 758-3322
*
Small 2 bedroom home. Private. Treed lot under the hill. Inexpensive. 758-3700. Available now!!
*
4 bedroom, 2 story home. Close to school and LDS stake center. 1 16 bath, living & family room, utility room. Fridge & stove included. Abstainers. No pets. 758-3700. Leave message.
FOR SALE
*
A 1340 sq. foot home near schools for sale.
3 Bedrooms, 2 bathrooms, main floor family room with fireplace, kitchen has new cupboards. New carpet and ceramic tile upstairs throughout, new furnace, two care garage attached and shop in backyard. Selling price $118,000.
Phone 758-3992 - Wolseys. Address is #50 - 1A Street East.
*
Character Home in Magrath. 1300 sq. ft. Fully renovated. Loads of oak throughout main floor.
9 14 ft. ceilings. Large country kitchen with oak cabinets. Finished basement. 25’x24’ heated garage with finished loft. Landscaped lot with mature trees. A must to see. Call collect for Viewing and more details - (403) 227-2611.
*
3 bedroom, 3 bathrooms. 1250 sq. Ft. on main floor. Treed lot - Fully developed basement. Call 758-6991.
*
5 bedroom, 3000 sq. Ft. Home on 1 acre.
2 bath, 2 family rms (one with wood burning stove), living rm, dining rm, oak in kitchen.
A steal at $132,000. Phone 758-6789.
*
House for sale - all brand new inside when
. finished renovations. For complete details See “www.retailcontrols.com/house” Asking $49,000 when complete.
*
New 1230 sq. ft. home. 3 bedrooms, 2 bath, garage, fridge, stove. No GST.
Phone 758-6835 or 758-34468 MaGROTHNEWS
In Our Community ... March2001
SUMÒay Monday Tuesday Wednesday T^ursttay Friday Saturday
25 ~ 26 27 28
Seniors Supper
5:00
I
BINGO
2
BASKETBALL
Boys host Kate
Andrews
3
BASKETBALL
Boys at
Raymond
. -
4 5 6 7
Seniors Supper
5:00
8 9 I0
Southern Alberta E
Lethbridge Er
lasketball Playoffs
imax Centre
758-6060
1444 sq. ft. Bungalow. This 25
year bungalow is definitely priced
right. Owners are moving to
another city. Don’t wait around,
this one won’t wait, mls
Large Luxury Bungalow (2195 sq.ft.) on 5
acres nestled in mature trees. 5 acres of
irrigation water rights. On town water. This
is a must to see. mls
4 bedroom bungalow with front
drive garage. Excellent starter
home. Priced to sell. Large flat lot in
nice location, mls
OLDIEBUTAGOODI Call
John
Latham
aiioalalz inokvi
EXCLUSIVE
Beautiful character home. Fully
renovated from top to bottom.
^Only asking $99I,.900. MLS
Phone 758-6060
Fax 758-3008
Price $39,900. Close to schools.
Woodburning fireplace. 2
Bedrooms.
Please note that the deadline for submissions to the paper is MONDAY at 6:00 p.m. Entries submitted
after the deadline will be published the following week Phone 758-6377, Fax 758-6888 or drop off your
submissions at the Magradi Trading Company Office.
Jlagrath trading Company 9
GBOCSRU $P£CIA£S
“3rom Our Jamily lo Hours...”
Dairy Delights »nd frozen favorites!
Parkay Margarine - select varieties, quarters or soft tub
1.36 kg
$2.98
Western Family Single Slices - select varieties
500 g
$2.98
Dairyland Cottage Cheese - select varieties
500 mL
$2.18
Dairyland Sour Cream - select varieties
500 mL
2 for $2.00
Western Family Cheddar Cheese - select varieties
750 g
$6.98
Western Family Aged Cheddar
750 g
$7.98
Minute Maid Chilled Orange Juice - select varieties
2.84 litre
$4.48
Minute Maid Lemonade Punch, Limeade or Nestea iced 1
;a 355 mL
.88
Sunny Delight Chilled Punch - select varieties
3.78 litre
$3.98
McCain Superfries, frozen - select varieties
1.5 kg
$3.48
Stouffer's Lean Cuisine Dinners, frozen - select
196-340 g
$2.98
Swanson Dinners, frozen - select varieties
280-376 g
$2.98
Breyer’s Classic ice Cream - select varieties
2 litre
$4.48
McCain Deep *N Delicious or Triple Chill Cake
510-530 g
$3.48
Groceries
Western Family Chicken or Beef Broth
284 mL
3 for $ 1.98
Sunburst Noodle Cups - select varieties
^... 64 g< - ,,
4 for $3.00
Western Family Chicken Noodle Soup, dry soup
JK?4 patrk
^$1.48
Western Family Sliced Bread, white or 60% whole v
heat-.» 567 g
.88
Western Family Egg Noodles - select varieties
375 g
.98
PregO Pasta Sauce - select varieties
750 g
$2.48
Western Family Flakes of Ham
184 g
.98
MJB Coffee - select varieties
250-300 g
2 for $5.00
Sun-Rype Pure Apple Juice
1 litre
4 for $5.00
Western Family Peaches - select varieties
398 mL
3 for $3.99jnore grocery specials...
Kellogg’s Rice Krispies or Raisin Bran
700-775 g
$3.98
Post Cereals - select varieties
375-620 g
2 for $5.98
Kraft Peanut Butter - select varieties
500 g
$2.98
Western Family Liquid Honey - unpasteurized
500 mL
$2.48
Robin Hood Quick Oats
3kg
$3.98
Western Family Jam - select varieties
375 mL
$2.28
Kellogg’S Pop TUrtS - select varieties
300 g
2 for $3.98
Western Family Granola Bars - select varieties
187-240 g
$1.88
Western Family Ketchup - squeeze
1 litre
$1.98
Western Family Pickles - select varieties
1 litre
$1.88
Western Family Mayonnaise or whipped salad dres<
ing 950 mL
$2.88
Western Family Pure Canola Oil
1 litre
$1.98
Western Family Pink Salmon
213g
$1.48
McCain Punches or Blends - select varieties
1 litre
.98
Minute Maid Juice, Punch or Five Alive Citrus
10x200mL
$3.28
Ruffles Potato Chips - select varieties
270 g
3 for $4.98
Old Dutch Crunchys or Corn Chips - select varied
!S 400-430 g
2 for $5.00
Old Dutch Arriba Tortilla Chips - select varieties
190 g
3 for $3.00
Old Dutch Restaurante Tortilla Chips or Salsa
312-4O(Jg,
4.7^ ml
2 for $5.00
Western Family Jelly Powders - select varieties
9-85 g
2 for .88
Western Family Pudding Snacks - select varieties
4 pack
4 for $6.00
Western Family Cookies - select varieties
400 g
$2.28
Pepsi, 7-up and Pepsi Products - select varieties
2 litre
2 for $3.00
Household Items...
Western Family Reclosable Sandwich Bags
50 pack
$ 1.48
Sifto Aqua Magic Crystal Plus Water Conditior
¡er 20 kg
$3.88
Value Priced Fabric Softener
3.6 litre
$1.8811
More Household Items...
Pedigree Mealtime Dog Food - select varieties
8 kg
$13.98
AlpO Dog Food - select varieties
624-630 g
2 for $3.00
Alpo Balanced Diet Dog Food - select varieties
7.2 kg
$11.98
Western Family Cat Food - select varieties
156 g
2 for .88
value Priced cat Food - complete, dry
8kg
$8.88
Alley Cat Cat Food
1 kg box
$1.98
Kitty Kit Clay Cat Litter
10 kg
$5.98
MARKtt 3R£SH PBOBUC£!
Green Seedless Grapes - Chilean grown, # i grade
$3.04/kg
$ 1.38/lb
Fresh Broccoli - U.S. grown
$2.16/kg
.98/lb
Mini Carrots - U.S. grown, # 1 grade
2 lb bag
$2.98
Premium Bananas - product of Ecuador
$1.28/kg
.58/lb
Kiwifruit - California grown
3 for .88
BUtUttR'S BURS 03 £H£ W££K!
Whole Frying Chicken - 2 pack
$3.48/kg
$1.58/lb
Chicken Legs, back attached - value pack 3-4 lbs
$ 1.94/kg
.88/lb
Chicken Breasts, back attached - value pack 3-4 lbs
$5.47/kg
$2.48/lb
Schneider’s Bologna - regular or beef
500 g
$3.28
Schneider’s Bacon - select varieties
500 g
$4.48
Fleetwood Rings - select varieties
300 g
$2.78
Fleetwood Bavarian Smokies
1 kg
$6.98
Schneider’s Wieners - select varieties
450 g
$2,98
Western Family Sliced Meats - select varieties
175 g
$1.18""Home of the Handyman”
Ryobi Power Tool Sale!
7 '/4"" Circular Saw -13 amp
Special Buy $139.99
13/4 HP Router
Special Buy $119.99
8 inch Drill Press
Special Buy $99.99
7.2 Volt Cordless Drill
w/ 2 batteries
Special BuY S69?9______
4 pack - Home Hardware
Light Bulbs - 40, 60, 100 W
Regular $1.09/pack
Special 67t/pack
Check our price on our Home Pack
Electrical Devices.
Prices are comparable to
Wholesale Prices!_______
Home Manual Food Chopper
Regular $15.99
Special $11.97
6 Outlet Electrical Tap
Regular $3.19
Special $2.25
Duracell Batteries are on
Special!
AM - Reg. $4.49 - Special $3.47
C - Reg. $4.49 - Special $3.97
9V - Reg. $4.49 - Special $3.97
M - Reg. $2.99 - Special $2.57
D - Reg. $4.49 - Special $3.97
Unival
10/30 Engine Oil - 4 litre
Regular Low Price $6.49
Sale Price $5.32
40 Watt Appliance Bulb
Regular $1.49
Special $1.00 each
Home Plumber Iron Remover
567 G - Regular $6.49
Special $5.00 each
Fire Sentry Smoke Detector
9V Battery included
Regular $5.99
Special $4.97
We have three
Seed Displays out!
Home b-OKtwaie
Century S 244E Wood Burning Stove
High Efficiency - Heats up to 1000 sq. Ft.
Only $469.9","Magrath Store News (February 28, 2001)",,J. A. Ririe,,,core
216475866,2011-01-01T00:00:00,"O presente estudo vislumbra analisar a importância da participação dos produtores no desenvolvimento da fruticultura e do Condomínio Frutícola Diamante Ltda, localizado no município de Quaraí. Para atingir o objetivo proposto foi necessário a utilização de pesquisa exploratória para obter um maior conhecimento da problemática a ser estuda, promovendo um levantamento de informações a respeito da participação dos atores sociais no desenvolvimento da fruticultura e na implantação desta forma peculiar de produção em condomínio. Fez-se necessário uma aproximação da realidade da fruticultura no município de Quaraí, que surgiu com incentivos governamentais para reverter o quadro de pouco desenvolvimento da região da Campanha, portanto surge esta produção como alternativa de diversificação e desenvolvimento devido às qualidades naturais da região para esta produção. Com o incentivo disponível os produtores do município de Quaraí se organizaram e desenvolveram a Associação Quaraiense de Fruticultores com o intuito de receber as verbas destinadas a produção, para tanto o pêssego foi selecionado para inovar a produção do município. Com o desenvolvimento da produção surgiram inúmeros erros, a maioria, decorrentes da falta de planejamento prévio, frente a estas limitações os produtores foram relaxando nos cuidados com a produção e a padronização da matéria-prima foi diminuindo. Devido a qualidades naturais da região e aos índices positivos da fruticultura surgiu uma nova idéia para reverter o quadro da produção de pêssego, a produção em forma de condomínio. Surge ai o Condomínio Frutícola Diamante, motivado e seguindo o exemplo do programa estadual da fruticultura que visa uma fruticultura moderna, sustentável e padronizada.  Baseado nos conceitos teóricos do desenvolvimento rural e de organização social, e utilizando a análise da constituição da Associação Quaraiense de Fruticultores e do Condomínio Frutícola Diamante, tornou-se possível compreender que a falta de envolvimento dos atores sociais é uma das principais causas do fracasso de políticas, programas e projetos.This study envisions analyze the importance of farmer participation in the development of fruit and Condominium Diamond Fruit Ltd., located in the municipality of Quaraí. To reach that goal it was necessary to use exploratory research to gain a better understanding of the issues to be investigated, providing a survey of information regarding the participation of stakeholders in the development of fruit production and deployment of this peculiar form of production in a condominium. It was necessary to approach the reality of a fruit in the city of Quaraí who came up with government incentives to reverse the underdevelopment of the region of the Campaign, so this comes as an alternative production diversification and development due to the natural qualities of the region for this production. With the incentive available to producers in the municipality of Quaraí organized and developed the Association of Fruit Growers Quaraiense in order to receive funding for production, for both the peach was selected to innovate the production of the municipality. With the development of production appeared numerous errors, most from the lack of advance planning, given these limitations producers were relaxing in the care of the production and standardization of raw materials declined. Due to the region's natural qualities and positive indices of fruit was a new idea to reverse the production of peach production in the form of condominium. There arises the Condominium Diamond Fruit, motivated, and following the example of the fruit of the state program aimed at a modern fruit production, sustainable and standardized. Based on the theoretical concepts of rural development and social organization, and using the analysis of the constitution of the Association of Fruit Growers and Quaraiense Condominium Diamond Fruit has become possible to understand that the lack of involvement of social actors is a major cause of failure policies, programs and projects","A participação social, a fruticultura, e o desenvolvimento rural : o caso do condomínio frutícola diamante no município de Quaraí (RS)",,,,,core
15351702,2001,"The development of the Virtual Reality Modelling Language (VRML) for the Internet has resulted in the emergence of a multiplicity of 3D web sites. The metaphor used by these sites varies enormously from virtual galleries to virtual cities and style varies from abstract to reality. Additionally these worlds are populated by virtual objects, some having reactive or interactive properties, including movement, audio, video, databases, artificial intelligence etc. Perhaps the most stimulating embodiment of these new environments are those that offer the participant the opportunity to meet and communicate with other visitors exploring the same virtual space/world. The Glasgow Directory is an established 3D web space, with around 10,000 visitors per year. The model represents approximately 10,000 properties in the city and is populated by contextual information on its culture and socio-economic topography. This paper describes the background to this VR space, and suggests a set of design criteria for successfully deploying multi-user software within this and similar environments. These criteria take into account lessons learned by “observingi and analysing how participants interact with the existing system under different conditions and also what benefits they perceive on entering the environment via the multi-user interface. These recommendations will hopefully be applicable to a wide spectrum of internet virtual environment builders and users",Visit VR Glasgow - Welcoming multiple visitors to the Virtual City,,,,,core
20747450,03/04/2008,"Figure 1 shows the Honeypot system deploying diagram. As mentioned above, A Honeypot consists of real service network (100) and virtual network (103). The ingress point, router(101) transport input traffic to both network and if the honey sensor(102) detects a sign of cracking, the input traffic is transported to only Honeypot network and Honeypot system stores every forensic evidences. 3. P2P Honeypot In this paper we adopt the idea of honeypot and design a method to trace users who spread illegal or harmful contents in P2P network and store forensic evidences. To do this, we operate several P2P service on bait P2P server farm. To gather illegal/harmful contents list from these bait P2P server, we need to analyze each P2P service protocol and hook and reassemble the P2P packets and extract file lists. And then, we store these file list in central DB. From this information we can figure out the trend of distribution of illegal/harmful contents. Each file on file list can be classified automatically to illegal/harmful using machine learning. In chapter 4, we show these machine learning algorithms. To gather files in file list, we use the P2P client function. So our bait P2P function can execute server function and client function. If our classification module judges the file illegal/harmful, we store the forensic evidences that are source/target IP, request time, target file and user etc. In the case that we need to prevent from spreading a certain file, we can make hash code of that file. With this hash code, we can trace the spreading trend of that file. Figure 2. P2P Honeypot deploy diagram Figure 2 shows the deploy diagram of P2P honeypot. In Figure 1 we install honeypot sensor(103) to protect a service server farm(100). P2P honeypot is not for protecting service server but for pretending that it is normal P2P server and alluring the users who spread illegal/harmful files. The illegal/harmful file list is stored in function DB(130). We have several bait P2P server for supporting different P2P service and arranging a number of servers for each P2P service. All illegal/harmful file lists are collected by central DB(150). The system consists of five modules. Figure 3 depicts honeypot system(120, 130) more precisely. � P2P server/client function module(123, 125) � Packet hooking and data collecting module(121",,,,,,core
20700486,2005,"Traffic congestion is one of the leading causes of lost productivity and decreased standard of living in urban settings. Recent advances in artificial intelligence suggest vehicle navigation by autonomous agents will be possible in the near future. In a previous paper, we proposed a reservation-based system for alleviating traffic congestion, specifically at intersections. This paper extends our prototype implementation in several ways with the aim of making it more implementable in the real world. In particular, we add the ability of vehicles to turn, enable them to accelerate while in the intersection, improve the efficiency and sensor model of the driver agents, and augment their interaction capabilities with a detailed protocol such that the vehicles do not need to know anything about the intersection control policy. The use of this protocol limits the interaction of the driver agent and the intersection manager to the extent that it is a reasonable approximation of reliable wireless communication. Finally, we describe how different intersection control policies can be expressed with this protocol and limited exchange of information. All improvements are fully implemented and tested, and we present detailed empirical results validating their effectiveness",Multiagent Traffic Management: Driver Agent Improvements And A Protocol for Intersection Control,,,,,core
20992002,02/02/2009,"This document 1 describes our ideas for organizing a video game AI competition at AIIDE. We discuss our goals, software, issues, and tournament organization, and propose to hold the first such event focusing on real-time strategy games in 2006. MOTIVATION AND GOALS Complex video games are popular pastimes. They form a multi-billion dollar market and have begun to attract interest from the AI research community and developers of military training and simulation technology. (Wikipedia 2005) provides the following list of major video game genres: fighting, racing, role-playing, simulators, sports, strategy, first and third-person shooters, and city-building games. A common element of these game genres is fast-paced action that serious challenges to AI systems designed for playing thes",ABSTRACT Complex Video Game AI Competitions at AIIDE’06,,,,,core
235572555,2003-08-20T00:00:00,"An archive of the Magrath Trading Store News.The University of Lethbridge Library received permission from the Wes Balderson to digitize and display this content.Vol 1-22 PEOPLE CONNECTING WITH PEOPLE August 20, 2003
Why Buy Local?
by Ron Williams
There are economic forces at
work right now that threaten local
businesses in Magrath. When you buy
your groceries, your gas, or your home
improvement fixtures, how do you
decide where to buy? Most of us focus
on two questions: Which stores have
the lowest prices? And which stores are
most conveniently located? Many of us
then decide to save the bulk of our
purchases for when we visit one of the
super-discount, big-box chain stores
like Wal-Mart, Costco, or Home Depot.
These are well managed stores with
plenty of virtues but they tend to siphon
money out of towns like ours and put
traditional local retailers out of busi­ness.
Local retailers often can’t com­pete
with the big chain prices but they
can offer better service, convenient
locations, unique atmospheres, friend­liness,
and community investment.
There are at least three com­Top:
Some of the staffat the Magrath
Trading Co., Howard West (Manager), Joan
Bly, Heidi Grtininger, Kristi Dudley.
Left: Arlen Bennett, Pharmacist and owner
of the Magrath Pharmacy with one of his
staff, Roger Davies.
family members, who in turn use
their paycheques to buy at other
local stores. All these transactions
reinforce one another and pump up
the local economic multiplier, the
basic building block for community
prosperity.
SECOND, local ownership mini-pelling
economic rea­sons
to buy from
locally-owned
Magrath businesses:
FIRST, a locally
owned business is
likely to produce
income, jobs, and charitable donations
for a community over several genera­tions.
They employ our neighbors and
^Oc^^ire[ess
High-Speed Internet
is now available in Magrath and surrounding area!
320-2600 or visit our website at www.shockware.com.
Once we have 20 users the rate for all will go down to $39.95
Top: Albert andJasmine Lee
owners of ""The Store"",
Right: Jessica Schneyder, Ted
Holland owner of ""Hollands
Insurance”, Kathy Heninger.
mizes the chances of
calamity. Too often we see
local companies sell their
interests to outsiders only
to have the hometown
plant close its doors with­in
a year. Tragic conse­quences
always follow.
Taxpayers thrown out of work become tax drainers through
welfare and unemployment payments. When the tax base
shrinks, vital services like education, police, fire, and roads
must be cut. Property values plummet and, like so many
steel, auto and mining towns in the 1970s, 1980s, and
1990s, the community descends into an economic death
spiral. Local stores have no plans to pull up stakes and
move to Taiwan.
THIRD, local companies are interested in the quality
of life in the community. Today, many communities are held
hostage by their largest companies. Many communities have
large employers that are perennial polluters. They routinely
dump waste in local lakes and streams. They take the atti­tude:
regulate us and we’ll move to more lax cities that want
to have us.
Locally owned companies never practice this kind of
extortion. They take a long term approach to the communi­ty.
It’s their community and they gradually improve it with
investments and service over time. Look at professional
sports franchises. How often do we hear owners threatening
to leave town if their demands for new stadiums, tax conces­sions,
and other booty are not met. One exception is the
Green Bay Packers, a team that is owned primarily by the
citizens of Wisconsin. Local ownership effectively prevents
the Packers from ever threatening to leave town. There will
never be the San Diego Packers.
So the next time you need to purchase fight
bulbs, go to the local hardware store instead of a
national big box store. Next time you want to go
out for dinner, support the many locally owned
restaurants versus the national chains. Next time
you pass a lemonade stand, stop, buy a glass, and
support tomorrow’s entrepreneurs.
Now is the time to support your neighbors
and local businesses. Imagine how inconvenient
it would be if the Trading Company closed its
doors. What if you couldn’t buy gas in Magrath?
What if the closest drug store were in Lethbridge?
Let’s take pride in the quality and character
of our community. Put your money where your
house is. Buy Local! >
Devonshire Realty Inc.
Jim Anderson agent
RESIDENTIAL - FARM ACREAGE -
COMMERCIAL IN MAGRATH AND AREA
2 Houses for Sale in Del Bonita
House for Sale
Harker Ave. & 2nd West St.
<num
Published weekly on Wednesdays by Keyline Communications
Box 179, Magrath, AB TOK 1J0 Ph: 758-6911 • Fx: 758-3.661
email magrathnews@telus.net
Ad deadline is Friday at 5pm and may be dropped off at the
Magrath Pharmacy or at Keyline Communications’
office at 14 Centennial Place, Duane & Carma Thomson’s home.
For permission to reprint any
material found in this publication please
contact Keyline Communications.
2 story home - suite in upstairs with separate
gas and electrical for billing - $82,000
2 Br„ 1600 sq.f^iyl^Oith attached 1
REDUCED!! 3 Br. walkt^k^J^jQattached garage. $153,000
1st Ave North & 1st St. East
2 bedroom, single bungalow - $34,900
car garage $129,000
Comparative Market Analysis
(No Charge) - For people interested in getting an
evaluation of marketability of your property
Phone 758-6725 (leave message)
331-8882 (cellular)
The Family of
Kenneth S. Harker
&
Eva Marie Harker
Request the Pleasure
of Your Company at
Their 50th Wedding Anniversary Celebration
Saturday
August 23, 2003
7:00 - 8:30 pm
Magrath Seniors Center
Obituaries?
GERALD LEISHMAN passed away in Lethbridge at the
Lethbridge Regional Hospital on Friday, August 15, 2003 at
83 years of age.
Gerry was born December 27,1919 in Magrath,
Alberta, where he lived most of his life. He married
Margaret Dahl and had four children.
Funeral Services were held on Monday, August 18,
2003 at the CHURCH OF JESUS CHRIST OF LATTER-DAY
SAINTS, Garden Place Chapel, Magrath, AB, with Bishop
Shane Gurney officiating. Interment in the Magrath Cemetery.
Kyle Harker, son of Vince and Heather, will be
speaking tins Sunday, August 24, at 11:00 am in 4th Ward at
the Garden Place Chapel. He will be leaving for the
California Roseville Mission September 3rd.
Jared Olsen of Welling, son of Wes and Dian,
returned from the California Fresno Mission on August 20.
He will be speaking August 31 at 10:00 am in the
Welling Ward.
SAVINGSSAVINGS
White Glue - 225 ml, reg $2.69 $1.69
Index Dividers - reg $1.29 79*
Math or Geometry Sets - reg $2.99 $2.19
Pencils - 10 pk, reg $1.29 59*
Trilok Covers - 4 pk, reg $1.99 $1.29
Papermate ball pens -10 pk, reg $2.29 $ 1.80
Glue Sticks - reg $2.39 $1.49
Tape - Home brand, reg $ 1.99 $1.19
Crayons - 96 pk, reg $6.99 $5.77
nil Magrath Trading Company
Phone: 758-3065
“BLACK” ONCE
International Tae Kwon Do Federation offers
congratulations to Mr. Tim Moore on his successful
completion of 2nd Degree Black Belt testing requirements.
He is now a formally recognized
ITF Second Degree (Dan) Black Belt.
The objectives of Tae Kwon Do are to:
• cultivate character
• trim and slim the body
• display graceful techniques
• bring out one’s strengths
• cultivate the mind
Tae Kwon Do has spread World Wide since its inception 50 years
ago, even in the town of Magrath. A small but motivated group
of individuals now trains on a regular basis and has proven
very successful over the past 4 years.
Following summer break, classes will begin in
September, Tuesday and Thursday evenings.
If you have any questions please call 758-6835.
Help Defend Traditional Marriage in Canada!
Sign The National Marriage Petition - Write a Letter
We are entering a critical time for our nation. The
government is taking steps to legalize same sex marriage,
something that would undermine traditional marriage and
the natural family. Preserving these vital institutions is
critical to Canada’s future.
Canadian Citizens to Defend Marriage (CCDM), is a
nationwide, grass roots, citizen-based effort, headquartered
in Toronto and organized to defend traditional marriage in
Canada at this important time. CCDM invites you to join with
Canadians across Canada in showing our support for tradi­tional
marriage by signing the online National Marriage
Petition. Please go to www.marriagepetition.ca to sign the
petition and learn more.
Sponsored by Canadian Citizens To Defend Marriage
www.marriagenctition.ca
Canadians of ALL ages can sign
We need your help to spread the word.
For further information or to volunteer to help, please
call Lorraine Balderson at 758-6380.
Take a look at us now!
Now offering
Soft Serve Ice Cream
Assorted
Candy Bars 3/$l-40
Excel Gum 2/$1.40
Milk $385
With min, purchase of 15 litre of gas.
Blue
Goose
277N 1st Street West • 758-3322
Writing letters to your local government representa­tives
is more beneficial than you might realize. A possible
place to start is by writing to Premier Ralph Klein expressing
your views on the legal definition of marriage.
Address letters to:
The Honourable Ralph Klein MLA
Premier of Alberta
307 Legislature Building
10800 97th Ave.
Edmonton AB T5K 2B7
Using your own words you may wish to express the
following items.
/ support for the Premier’s stand on Marriage between
male and female
/ concurrence that the basic family unit consists of a
father and mother; that children born to that union is
a core value of our society.
/ opposition to same sex marriages, feeling that this
would erode the God-given and time tested family
unit as defined above.
/ support for the use of the not-withstanding clause
should it become necessary to defeat same-sex
marriages.
/ express appreciation for his courage and commitment
in defending our family unit in Alberta. ♦
If you ever find happiness by hunting for it, you will find it,
as the old woman did her lost spectacles, safe on her own
nose all the time. - Josh Billings
(Happiness) always looks small while you hold it in your
hands, but let it go, and you learn at once how big and
precious it is. - Maksim Gorky
Rockport Flour Mill Inc.
Phone: 758-3077
Fax: 758-3340
8miles south and 2
miles east of Magrath
--------- ---------
Coyote Pancake & Waffle Mix
Made from pure natural whole wheat flour.
Four flavors: regular, buttermilk, blueberry
and chocolate chip. Use for family reunions,
pancake breakfasts, etc.
Also available: Coyote Pure Natural Whole
Wheat Wheatlets - Germade Breakfast Cereal.
Available at all stores or buy direct
CM
O
IZ1
CU
DO
Jñ
CD
CU
DO
o
CU
E
o
CD
1-0 o
CU\
CD
“O
CP
-a
=3
O
O
CD
IX) u—
cu
CD
ÖO
S
CU
OO
""O
CD
en
e o
o
cu
CQÄK
CU 5
cu
tí
£
•Ö
<u
Q-E
o
o
""O
cu
Eoo
Q¿
DO-C
C
c
cu
E
c<uo <O
CQ
<U
O
en
ZJ
o
cd on
o
o
Raw Water Irrigation
Pumping Station
The Town’s irrigation pumping station is now operational.
Magrath has a rich heritage of irrigation and so it is
fitting that we have a state of the art system to water our
lawns and gardens.
In order to make the most efficient use of our grid of underground pipes,
we are changing our water use policy:
EFFECTIVE IMMEDIATELY:
Residents with EVEN HOUSE NUMBERS will water on EVEN DAYS.
Residents with ODD NUMBERED HOUSES will water on ODD DAYS.
This change in watering policy will allow all residents to appreciate even
. better water pressure and flow to their properties.
The Town requests that all residents discontinue the use of potable water
(that is, water that comes from a tap on your house) for watering lawns
and gardens. The potable water is treated for drinking and household use. This treatment is
expensive. We can reduce our overall Town taxes by not using potable/treated water outside.
Remember, the raw water irrigation should not be drunk. It is not treated. However, the raw
water is healthier for lawns than the potable water.
Connections to the Raw Water Irrigation System:
If you do not have a valve and spigot on your property that connects you to the Raw Water Irrigation
System, please call the Town Office at 758-3212. We will look at how close our pipes are to your property
and, if feasible, see that you are connected in a timely manner.
Watch for Leaks
We are pressurizing the irrigation system at about 40 pounds per square inch (p.s.i.). We may
increase this in time. But for now, we expect to spend some time monitoring the system and fixing leaks in
lines that are unaccustomed to this higher pressure. We ask that all residents be aware of the
irrigation lines on or around their property and inform the Town of all leaks,
accumulations of water, or Old Faithful-like geysers.
Town of Magrath
Telephone: 758-3212 Fax: 758-6333
E-mail: magrathl@telusplanet.net
Demolition Day
One of the last remaining historical icons of
Magrath met its fate in June of this year. The 103 year
old Ririe Barn on the east end of town was bulldozed to
the ground, burned, and buried. There are now few
buildings left in Magrath that are over 100 years old.
James and Elizabeth Ann Ririe were among the
first settlers of Magrath arriving in 1899- They began
building their beautiful home first, followed shortly by
the big livestock barn capable of housing dozens of cows
and horses. A bam which withstood the rigors of 100
years of Alberta weather took just one day to be com­pletely
demofished. The only evidence left of the barn is
a freshly leveled area of dirt. ♦ Value Priced Diapers
- For Boys or Girls .
Super absorbent contoured
shape is ultra thin and
comfortable.
24 Small-Medium 10-22 lbs
20 Large 22-271bs
$6.86 ea.
MY ROOTS
T-SHIRTS.
Logo reads:
“My ROOTS are in the Garden City
Magrath, Alberta, Canada”
Assorted colors and sizes YM to 2XL.
- Front Only
- Front and Back
- Golf Shirts
$12.95
$14.95
$19.95
Magrath
2000 ltd.
80 South 1 st Street West
Mon-Fri 9:00am - 6:00pm
Ph: 758-3001 • Fax 758-3505
After Hours: 758-6222 • 382-0749
New On-ramp to the Information Superhighway
by Rick Humphreys
High-speed internet has finally arrived in Magrath. Not
through your phone line or cablevision, but through radio
waves via a wireless network.
In late July, Shockware Wireless Inc. installed a wire­less
base unit on the top of the P&H elevator, making high­speed
internet a reality for Magrath businesses and resi­dents.
Shockware, owned by Dean Shock, has already
installed wireless internet for a number of small communi­ties
including Vauxhall, Barnwell, Seven Persons, Taber and
Stirling. They also service Lethbridge and Medicine Hat,
competing directly with Telus and Shaw Cable.
The basic range of the base station is about 15 miles.
Wireless networks are restricted to “line of sight” connec­tions.
In other words, the main base station and the dish
antenna at the residence must be in sight of each other. For
homes within a mile or so of the base station, the signal may
pass through trees, but that is not guaranteed. In order to
determine whether there is sufficient signal strength to a
prospective customer, a Shockware technician from
Lethbridge makes a personal visit and tests the signal before
setting up the connection.
Todd Johnson, the first customer signed up in Magrath,
is very happy with this new service. He says his download
speeds are at least 15-20 times faster than dial-up.
Something that took 10-15 minutes to download with a
modem is now taking him around 20 seconds.
The Town of Magrath has signed up for the service.
Ron Williams, the Town Administrator, said that there were
some initial problems with connectivity, but Shockware
remedied the problems for them. He said web pages come
up “almost instantaneously.”
The cost of this wireless internet service includes:
* $50 - Survey Travel Fee: to determine if a proper signal
can be established. Mr. Shock indicated that this fee
can be shared by customers by arranging for the
Lethbridge technician to come out and test multiple
homes in the same visit.
* $75 installation fee to mount the antenna on the roof,
install the wireless radio card in the computer, set up
the software and train the customer.
* $49-95 monthly fee rental of the equipment and con­nection
to the internet. This rate will drop to $39-95
for all users once 20 users are signed up, and to
$34.95 once 30 users are signed up.
This is the only high-speed internet option available for
Magrath at this time. Both Telus and Monarch Cable have
indicated that there is no schedule yet for installing their
high-speed internet services. ♦
r■II
III
I■I
>
EM
Start in September
Cost is $20 per Month.
Classes will be one day a week
Ages 5-11 and 12-17
An adult class considered if enough interest.
Call Leanne Sabey at 758-6846 to
register or for more information.
Limited Space Available
1 Tl
I
IIIIIII■
■
PAVING &
MAINTENANCE LTD.
""YOUR LOCAL SPECIALISTS""
Asphalt Paving & Repairs
Pot Holes & Pathways
Driveways
Parking Lots
• Seal Coating
• Sand Slurry Seal
• Hot & Cold Crack Sealing
• Line Painting
ROB @ Ph/Fax: 329-1381 Cell: 330-6956
s
CARPET AND
UPHOLSTERY CLEANING
* Walls * Windows * General Cleaning *
758 - 6414 Cell 308-6580 Magrath
Fop relief f pom all ®f youp Imposing and difficult yobs.
I am the person you need fop all of youp light construc­tion
and renovation needs. Fences and yapd clean-up ape
a snap when I do it fop you.
lODie Town Handymai
Fop mope Inf opmatlon about my skills and the reasonable
pates I ask. call Home Town Handyman at 738-b7b5
HOW TO STAY YOUNG
1. Throw out nonessential numbers. This includes age,
weight and height. Let the doctor worry about them. That
is why you pay him/her.
2. Keep only cheerful friends. The grouches pull you down.
3. Keep learning. Learn more about the computer, crafts, gar­dening,
whatever. Never let the brain idle. “An idle mind is
the devil’s workshop.” And the devil’s name is Alzheimer’s.
4. Enjoy the simple things.
5. Laugh often, long and loud. Laugh until you gasp for breath.
6. The tears happen. Endure, grieve, and move on. The only
person who is with us our entire life, is ourselves.
Be ALIVE while you are alive.
7. Surround yourself with what you love, whether it’s family,
pets, keepsakes, music, plants, hobbies, whatever. Your
home is your refuge.
8. Cherish your health: If it is good, preserve it. If it is
unstable, improve it. If it is beyond what you can improve,
get help.
9. Don’t take guilt trips. Take a trip to the mall, to the next
county, to a foreign country, but NOT to where the guilt is.
10. Tell the people you love that you love them, at every
opportunity.
AND ALWAYS REMEMBER: Life is not measured by the
number of breaths we take, but by the moments that take our
breath away. Have a Great Day! ♦
MAXWELL REALTY - LETHBRIDGE
Come See The New Look
FOR SALE — 153N - 2St East, Magrath
We have a conditional offer pending on this beautiful 5 bed­room,
3 bathroom family home. The kitchen / dining room
comes complete with wall-to-wall shelving. Absolutely beau­tiful
park-like yard. The near double lot is surrounded by a
newer cedar fence. The single attached garage is “topped
off” with a huge deck surrounded by maintenance free
aluminum railing. Nearly 2900 sq. ft. of immaculate family
home priced at $132,900. Call Carole Paquette for viewing
or more information.
Holland Insurance
(Magrath) LTD.
____________________AGENTS FOR___________________
INGitò MaiteH*
Vehicle vandalized and caller is not able to pay his
deductible. Can this be extended until he can afford
the full amount or can he negotiate a payment plan?
ANSWER: Deductibles are part of the loss and has to
be paid if caller wants his vehicle repaired.
jll___ Driver's Licenses, Plate Renewals, Driver's Testing, •/QlDdTCl Pay Fines- B>rth Certificate, Marriage License, Death
rc&strcs Certificate, Annual Returns, Corporate Searches, Etc
WE SELL TRAVEL INSURANCE, $1 .OO/doy (restrictions apply)
Phone: Ted, Kathy, Jewelene or Mike at
1-403-758-3391 Fax:1-403-758-6607
My Way Broadway My Way
My Way My Way
21
ns
Broadway
Thcri
Tickets Prices
Adult $15 Youth $13 Family $49
- Group rates available -
WESTWIND SCHOOL DIVISION 171
Westwind School Division #74
2003-2004 School Calendar
August September October
M
1
8
15
T
2
9
16
o-> • no • ->4 •
29 • 30 • 31 *
November
§ = PD Day (School Based)
= Easter Holidays
* = Christmas Holidays
oo = Diploma Exams
■ = 30 First day of Second Semester
§ = PD Day (School Based)
* = Family Day
• = Teacher's Convention
I 18 00
21 « 22 oo 23 x, 24 oo 25 oc
28 29 30
* = Remembrance Day
M T W T F M 1 T W T F M T W T F MTW T F
1 1 * 2 • 3 4 5 1 2 3 3 4 5 6 7
4 5 6 7 8 8 9 10 11 12 6 7 8 9 10 . 10 § Il • 12 13 14
11 12 13 14 15 15 16 17 18 19 13 * 14 15 16 17 17 18 19 20 21
18 19 20 21 22 22 23 24 25 26 § 20 21 22 23 24 24 25 26 27 28
25 26 27 28 29 29 30 27 28 29 30 31
* = Labour Day * = Thanksgiving Day § = PD Day (Divisional Based)
• = First Day of School Y ear
§ = PD Day (School Based)
December January
M T W T F
2 3 4 5 6
9 10 11 12 13
16 • 17 18 19 • 20 •
23 24 25 26 27 '
March
= Christmas Holidays
* = Victoria Day
M T W T F
3 4 5 6 7
10 11 12 13 14
17 18 19 20 21
24 ». 25 26 27 28
31
oo = Diploma Exams
• = Last Day of School
Achievement Examinations
Timed Number Facts Gr. 3 18-May
Language Arts Gr. 3 Part A 19-May
Mathematics Gr. 6 Part A 19-May
Language Arts Gr. 6 & 9 Part A 20-May
Language Arts Gr. 3 (Part B) 17-Jun
Science Gr. 6 17-Jun
Social Studies Gr. 6 18-Jun
Language Arts Gr. 6 Part B 21-Jun
Mathematics Gr. 3 22-Jun
Mathematics Gr. 6 (Part B) 22-Jun
Mathematics Gr. 9 22-Jun
Social Studies Gr. 9 23-Jun
Language Arts Gr. 9 (Part B) 24-Jun
Science Gr. 9 25-Jun
Diploma Examinations
Subject January' June
Eng. 30-1/30-2 Part A 14-Jan 8-Jun
Social Studies 30/33 Part A 16-Jan 10-Jun
Eng. 30-1/30-2 PartB 22-Jan 18-Jun
Mathematics 30 P/30 A 23-Jan 21-Jun
Biology 30 26-Jan 22-Jun
Social Studies 30/33 Part B 27-Jan 23-Jun
Chemistry 30 28-Jan 24-Jun
Physics 30 29-Jan 25-Jun
Instructional Days 188
P.D. Days (No Students) 6
Parent/Teacher Evenings 2
Total Days 196
* For further in","Magrath Store News (August 20, 2003)",,J. A. Ririe,,,core
203430599,31/10/2002,"AbstractThis volume contains the Proceedings of the 9th Workshop on Logic, Language, Information and Computation (WoLLIC'2002). The Workshop was held in Rio de Janeiro, Brazil from July 30 to August 2, 2002, in the campus of the Pontifícia Universidade Católica do Rio de Janeiro (PUC-Rio).WoLLIC is a series of workshops which started in 1994 with the aim of fostering interdisciplinary research in pure and applied logic. The idea is to have a forum which is large enough in the number of possible interactions between logic and the sciences related to information and computation, and yet is small enough to allow for concrete and useful interaction among participants. Previous versions were held at: Recife (Pernambuco, Brazil) in 1994 and 1995; Salvador (Bahia, Brazil) in 1996; Fortaleza (Ceará, Brazil) in 1997; São Paulo (Brazil) in 1998; Itatiaia (Rio de Janeiro, Brazil) in 1999; Natal (Rio Grande do Norte) in 2000; Brasília (Distrito Federal, Brazil) in 2001. [new] Scientific sponsorship comes from the Interest Group in Pure and Applied Logics (IGPL), the European Association for Logic, Language and Information (FoLLI), the Association for Symbolic Logic (ASL), the Sociedade Brasileira de Computação (SBC), and the Sociedade Brasileira de Lógica (SBL).[end new] Scientific sponsorship comes from the Interest Group in Pure and Applied Logics (IGPL), the European Association for Logic, Language and Information (FoLLI), the Association for Symbolic Logic (ASL), the Sociedade Brasileira de Computação (SBC), and the Sociedade Brasileira de Lógica (SBL). Funding was kindly given by: (i) CNPq (Conselho Nacional de Desenvolvimento Científico e Tecnológico, the scientific and technological development council of the Brazilian Ministério da Ciência e Tecnologia) (grant 451491/2002-5); (ii) CAPES (Fundação Coordenação de Apoio ao Aperfeiçoamento de Pessoal de Nível Superior, a Foundation for the Development of Higher-Education under the Brazilian Ministério da Educação e do Desporto) (grant PAEP0169/02-1); (iii) PUC-Rio (Pontifícia Universidade Católica do Rio de Janeiro). Contributions were received in the form of short papers in all areas related to logic, language, information and computation, including: pure logical systems, proof theory, model theory, algebraic logic, type theory, category theory, constructive mathematics, lambda and combinatorial calculi, program logic and program semantics, logics and models of concurrency, logic and complexity theory, proof complexity, foundations of cryptography (zero-knowledge proofs), descriptive complexity, nonclassical logics, nonmonotonic logic, logic and language, discourse representation, logic and artificial intelligence, automated deduction, foundations of logic programming, logic and computation, and logic engineering.Apart from the contributed papers (14), and the invited talks (6), the programme includes 6 tutorial lectures:
				•Some model theory of ordered structures by Ricardo Bianconi (Departamento de Matemática Pura, Instituto de Matemática e Estatística, Universidade de São Paulo, Brazil)•Computing with Real Numbers by Felipe Cucker (Department of Mathematics, City University of Hong Kong, People's Republic of China)•Model Checking Games by Erich Grädel (Mathematische Grundlagen der Informatik, RWTH Aachen, Germany)•The Metalanguage Lambda Prolog and its Implementation by Gopalan Nadathur (University of Minnesota, USA)•Automata theory and logic by Igor Walukiewicz (Bordeaux University, France)•States of Knowledge (Tutorial) by Rohit Parikh (Department of Computer and Information Science, Brooklyn College, City University of New York, USA)All papers in the volume were reviewed by the program committee consisting, besides editor, of
				Mauricio Ayala-Rincón(Department of Mathematics, Universidade de Brasília, Brazil)Mario Benevides(Institute of Mathematics, Universidade Federal do Rio de Janeiro, Brazil)Anuj Dawar(Computer Laboratory, Cambridge University, England)Philippe de Groote(LORIA, France)Roger Maddux(Department of Mathematics, Iowa State University, USA)Toniann Pitassi(Department of Computer Science, Toronto University, Canada)Bruno Poizat(Institut Girard Desargues, Université Claude Bernard Lyon I, France)Alberto Policriti(Department of Mathematics and Informatics, Università di Udine, Italy)Glynn Winskel(Computer Laboratory, Cambridge University, England)The organising committee consisted, besides editor, of
				Edward Hermann Haeusler(Department of Informatics, Pontifícia Universidade Católica do Rio de Janeiro, Brazil)Claus Akira Matsushigue(Institute of Mathematics and Statistics, Universidade de São Paulo, Brazil)Anjolina Grisi de Oliveira(Center of Informatics, Universidade Federal de Pernambuco, Brazil)Luiz Carlos Pereira(Department of Philosophy, Pontifícia Universidade Católica do Rio de Janeiro, Brazil)Ruy de Queiroz(Center of Informatics, Universidade Federal de Pernambuco, Brazil)Jorge Petrúcio Viana(Coordination of Postgraduate Programmes in Engineering and Systems, Universidade Federal do Rio de Janeiro, Brazil)The volume will be published as volume 67 in the series Electronic Notes in Theoretical Computer Science (ENTCS). This series is published electronically through the facilities of Elsevier Science B.V. and its auspices. The volumes in the ENTCS series can be accessed at the URL http://www.elsevier.nl/locate/entcsA printed version of the current volume is distributed to the participants at the workshop in Rio de Janeiro.We are very grateful to the following persons, whose help has been crucial for the success of WoLLIC'2002: Mike Mislove, one of the Managing Editors of the ENTCS series, for his assistance with the use of the ENTCS style files; Thanks are also due to the Departments of Philosophy and Informatics of the Pontifícia Universidade Católica of Rio de Janeiro, which has provided the logistic support to the organising committee.July 26, 2002 Ruy de Queiro",Preface Volume 67,,Elsevier B.V.,10.1016/S1571-0661(05)80556-0,,core
288531199,2005,"Negli ultimi decenni la Pubblica Amministrazione italiana è stata al centro di un profondo e continuo processo di modernizzazione, volto a promuovere l’introduzione di modelli gestionali più efficienti ed efficaci, più simili e più vicini a quelli adottati nelle imprese private. Sulla scia di questi cambiamenti, le amministrazioni sono state chiamate ad adottare “nuovi” sistemi di gestione delle risorse umane caratterizzati da mobilità, da stipendi basati sul merito e da un sistema di incentivi ancorato ai risultati conseguiti e non più e non solo alla volontà di qualcuno.

Le buone pratiche di gestione delle risorse umane potrebbero diventare essenziali per sviluppare una forza lavoro pubblica sempre più qualificata, per favorire la crescita e lo sviluppo delle competenze del personale, per garantire la flessibilità organizzativa e, da ultimo, per migliorare tanto la produttività, quanto l’efficienza del settore pubblico. 

Si tratta di una importante innovazione, ancora tutta da realizzare. Per farlo, diventa necessario compiere un salto di qualità che coinvolga non solo le competenze tecniche ma anche gli aspetti e le resistenze culturali che caratterizzano, da sempre le organizzazioni pubbliche.

Tuttavia, nonostante le sue possibili potenzialità, ci sono diversi problemi relativi all’adozione di un sistema di valutazione delle prestazione: spesso le persone hanno paura della valutazione, solo perché questa viene percepita come un controllo. Questo è ancora più vero nella pubblica amministrazione italiana, dove sembra che il solo scopo per il quale i sistemi di valutazione della prestazione siano stati utilizzati è quello relativo alla remunerazione: si rileva il contributo fornito dal singolo solo al fine di valutarne la coerenza con le aspettative dell'organizzazione e identificare, di conseguenza, la “giusta” remunerazione. Ma non è tutto. Un altro problema che caratterizza la Pubblica Amministrazione è quello della scarsità delle risorse economiche da poter distribuire, la remunerazione e, nello specifico, gli incentivi perdono così sempre più spesso la loro funzione d’uso tradizionale che consiste nel motivare i dipendenti pubblici. 

Per queste ragioni sembra fondamentale che non solo il management pubblico, ma anche i politici comprendano le possibili e reali finalità dei sistemi di valutazione della prestazione. Obiettivo del presente lavoro è, dunque, quello di fornire una panoramica sullo stato dell'arte, in termini di finalità, metodi e risultati, dell’adozione di sistemi di valutazione della prestazione nelle pubbliche amministrazioni italiane.Over the last twenty years the Italian Public Administration was at the core of a deep and continuous process of modernization, looking for introducing the more efficient and effective management models of private enterprises. Administrations were called upon to implement human resource management systems characterized both by mobility and a salary based on merit and professional incentives linked to results.

Effective human resource management practices could became essential to develop a skilled workforce, to favour the growth and development of the competence of the personnel, to guarantee the organizational flexibility and to address public sector productivity and efficiency. In order to realize this innovation a qualitative leap is required touching the technical competencies but also the cultural aspects that characterize the Italian public administrations.

However, there are different problems related to the performance appraisal system: people are often afraid of the appraisal, because they frequently perceive it as a control. In Italian Public Administration, the only purpose for which the performance appraisal systems have been used up to now and, consequently, people can see, is that related to pay, where the contribution produced by each person is detected, in order to assess their consistency with the expectations of the organization and identify the incentives needed to motivate. But there is another problem: in Public Administration there are often few economic resources to distribute, so this purpose is not motivating and inconsistent. For these reasons it should be very important that both governors and civil servants understand the possible and real purposes of the performance appraisal systems. 

This work intends to give an overview of the state of art of implementation of the performance appraisal systems in Italian public organizations and the quality of implementation, in terms of purposes, use, method and results",La valutazione della prestazione: costoso rito politico o strumento di convenienza organizzativa?,,McGraw-Hill,,,core
91350266,2002,"Information and communication technology (ICT) is widely accepted as a potentially favourable set of instruments, which may improve the welfare and competitiveness of nations and cities. Nowadays, both public and private actors aim to exploit the expected benefits of ICT developments. The authors seek to investigate the potential of ICT use at an urban level and, in particular, to shed more light on various factors that influence urban ICT policies in the public domain. First, a conceptual framework, designed to improve understanding of the driving forces of urban ICT policies, is outlined. It focuses on the way decisionmakers perceive their city, and shape their opinions about ICT; it addresses in particular the way these decisionmakers evaluate the importance of ICT for their city. Next, interviews with urban decisionmakers in different European cities in three countries (Austria, Spain, and the Netherlands) are used to analyse the complex relationship between perceived urban characteristics (for example, nature of problems and urban image), personal attitudes towards ICT, administrative features of the cities concerned, and perceptions of the relevance of ICT to the cities. The authors' main focus is on the identification of a possible systematic relationship between the aforementioned explanatory factors and urban decisionmakers' attitudes towards ICT policies. Understanding the decisionmakers' perceptions is an important step towards grasping the nature and substance of the policy itself, and may explain some of the variance among different cities. Because the 'urban ICT' discourse is still relatively new, an open-interview method is used to capture a variety of different views and perceptions on ICT and on the information age in the city. With the aid of qualitative content analysis, the interview results are transformed into a more systematic and comparable form. The results suggest that even interviewees from the same city may have a different understanding of their urban reality whereas, on the other hand, cities with different characteristics may appear to suffer from similar problems. Moreover, the authors found a wide range of attitudes toward ICT and its expected social impacts, although most of the interviewees appeared to be more sceptical than had been expected. The authors identified a clear need for a more thorough investigation of background factors and, therefore an approach originating from the field of artificial intelligence-rough-set analysis-was deployed to offer a more rigorous analysis. This approach helped in the characterisation and understanding of perceptions and attitudes regarding urban policies, problems, and images",Information and Communication Technology Policy in European Cities: A Comparative Approach,,,10.1068/b12843,,core
100193646,2009,"Copyright © 2009 SAE International In the paper, a rule-based (RB) control strategy is proposed to optimize on-board energy management on a Hybrid Solar Vehicle (HSV) with series structure. Previous studies have shown the promising benefits of such vehicles in urban driving in terms of fuel economy and carbon dioxide reduction, and that economic feasibility could be achieved in a near future. The control architecture consists of two main loops: one external, which determines final battery state of charge (SOC) as function of expected solar contribution during next parking phase, and the second internal, whose aim is to define optimal ICE-EG power trajectory and SOC oscillation around the final value, as addressed by the first loop. In order to maximize the fuel savings achievable by a series architecture, an intermittent ICE scheduling is adopted for HSV. Therefore, the second loop yields the average power at which the ICE is operated as function of the average values of traction power demand and solar power. Expected solar contribution can be estimated starting from widely available solar databases and by processing past solar energy data measured on the vehicle. Neural Networks predictors, previously stored data and/or GPS derived information are suitable to estimate average power requested for vehicle traction. Extensive simulation analyses were carried out to test the performance of the RB algorithm, also comparing it to Genetic Algorithms-based optimization strategies previously developed by the authors. The results confirm the high potentialities offered by the proposed RB control strategy to perform real-time energy management on hybrid solar vehicles. The proposed rule-based optimization is currently under-implementation in an NI ® cRIO control unit, thus allowing to perform experimental tests on a real HSV prototype developed at University of Salerno","Rule-Based Optimization of Intermittent ICE Scheduling on a Hybrid Solar Vehicle”, SAE Paper 2009-24-0067",,,,,core
295401132,30/06/2010,"Representation of the city, according to dictates of classical literature and manuals, takes shape from studies about morphology and expressive language of the urban plot, in a deep connection between spaces, monuments and significant buildings. In the contemporary digital era, city can be described as a linkage of elements converging toward built environment's generation, following gathering procedures detectable by parameters. Present territorial planning can take advantage of these criteria, in order to simulate the evolving of urban realities and reach predictable or desirable development frontiers, using models made by computer software aimed to favour choices in managing and transforming the city. Parametric technologies make possible to embed information into urban digital models, intended as sort of data collectors browsable in real time.La rappresentazione della città, secondo i dettami della manualistica e della letteratura classiche, si sviluppa attraverso lo studio della morfologia e del linguaggio espressivo della trama urbana, in stretta relazione con i monumenti e gli edifici significativi. Nell’era digitale contemporanea, la città può essere intesa come connessione di elementi concorrenti nella generazione dello spazio costruito, secondo logiche aggregative individuabili da parametri. L’attuale pianificazione del territorio può avvalersi di tali criteri, simulando realtà in divenire per seguire frontiere di sviluppo prevedibili od auspicabili, formalizzando modelli prodotti attraverso programmi di calcolo che coadiuvano le scelte progettuali di trasformazione e gestione della città. Le possibilità fornite dalle tecnologie parametriche consentono di associare informazione ai modelli digitali che possono così assumere la valenza di contenitori informativi consultabili in tempo reale",La rappresentazione parametrica della città,,"Alma Mater Studiorum, Università di Bologna",10.6092/issn.1828-5961/1946,,core
14694880,2008-02-16T00:00:00,"La professione giornalistica vive nel terzo millennio un’evoluzione che non puo’ prescindere dagli sviluppi delle nuove tecnologie entrate a far parte in modo pervasivo del vivere comune della nostra societa’. Quello che in particolare si modifica non e’ tanto la professione in quanto tale, che mantiene la principale caratteristica di mediazione dell’informazione, ma l’organizzazione stessa del lavoro dei professionisti della notizia. La scelta dell’argomento della tesi parte dalla considerazione di questo dato di fatto, e la possibilita’ di aver svolto uno stage presso la redazione della free press torinese “Shop in the city”, e' stata l’occasione per indagare direttamente sul campo i meccanismi di fare informazione nell’era digitale.
Partendo da questa prospettiva si e’ cercato innanzitutto di offrire una panoramica dei cambiamenti della professione e dei professionisti all’interno della societa’; un’analisi degli sviluppi tecnologici che hanno contribuito in modo determinante al cambiamento del lavoro di giornalista; un’analisi delle nuove forme di giornalismo che si sviluppano nell’era digitale. Parallelamente, un capitolo-intermezzo viene dedicato a due autori, Richard Florida e McKenzie Wark, i quali, partendo da prospettive diametralmente opposte, arrivano entrambi alla considerazione che nell’era digitale cio’ che rappresenta una vera produttrice di valore per la societa’ e’ la creatività degli individui, che si colloca proprio nell’intreccio tra rete e metropoli, tra virtuale e reale: l’informale diventa la sorgente cui la società attinge per trovare i suoi nuovi contesti produttivi.
Queste considerazioni vengono applicate nella seconda parte del lavoro di tesi in cui viene analizzata la free press torinese “Shop in the city”, sia nella sua realta’ cartacea che soprattutto nella sua forma web. Ciò permette di applicare tutte le considerazioni fatte in fase teorica. In particolare l’analisi dei punti di forza e di debolezza della parte online servono come spunto per presentare un progetto di implementazione del sito seguendo i pilastri della filosofia del web 2.0 e di un maggior coinvolgimento dell’utente, volendo dimostrare che oggi non conta più essere a tutti i costi online con un sito vetrina: occorre, nel terzo millennio che viviamo, fornire un vero servizio agli utenti di modo che essi effettivamente utilizzino il sito e siano fidelizzati alla navigazione.
Lo scopo finale e’ quindi quello di fornire in prima istanza una mappatura cognitiva dello scenario mediale degli ultimi anni, che sia forse utile ai vari dibattiti che alimentano la scena informativa riguardo al futuro dei giornali e ai possibili sviluppi del campo giornalistico attraverso lo sfruttamento delle potenzialita’ dei new media; in seconda istanza, preso il caso pratico di una free press, si propone di offrire un esempio di come migliorare lo sfruttamento delle potenzialità di internet per fare informazione nell’era digitale.
In the third millennium the journalistic profession is going through an evolution and cannot ignore the new technological developments that have become an important and pervasive part of the shared lives of our society. It is not so much the profession itself which is changing, in the sense that it maintains it’s principal role of the mediation and distribution of information, but instead the way in which the work of news professionals is organised.
Based on this assumption, this work begins by offering an overview of the changes to the profession and to the professionals themselves; an analysis of the technological developments that have contributed to determining the changes in the work of a journalist and an analysis of the new forms of journalism that are developing in the digital era. In parallel, a part is dedicated to two authors, Richard Florida and McKenzie Wark, who both arrive at the conclusion that in the digital era that which represents a real increase in value for an organisation is the creativity of individuals, which can be seen as being placed in the crossover of network and metropolis,: the informal becomes the source on which the organisation draws to find it’s new production contexts.
These considerations are applied and discussed in the second part of the thesis in which the Turin-based free press ‘Shop in the city’ is analysed, both in it’s paper and web-based forms. This analysis allows the application of the theories discussed in the relevant part of the thesis and offers a practical example of the reality of journalism in the digital era. In particular, the analyses of the strengths and weaknesses of the online format provide a good basis for the development and presentation of an implementation project for the site, following the philosophies of web 2.0 and of increased user involvement, demonstrating that it is no longer enough simply to be online at all costs, or to only have a ‘window display site’: in this third millennium it is fundamental to provide a real service to users so that they can effectively navigate and make use of the site.
The final aim, therefore, is to provide in the first instance a cognitive map of the field over the past few years, something which may be of use in the various debates that rise in the sector about the future of newspapers and the possible future developments in the field of journalism through the use of new media techniques and methods. In the second instance, with regard to the practical case of a free paper, this thesis proposes an example of how to best make use of the power of the internet to provide information in the digital era",UNA PROFESSIONE IN EVOLUZIONE. FORME E PROGETTI DI GIORNALISMO NELL'ERA DIGITALE.,https://core.ac.uk/download/14694880.pdf,'Pisa University Press',,,core
36728889,2011-01-01T00:00:00,"Proceedings of the 25th USENIX Large Installation Systems Administration Conference (LISA 2011), Boston, MA, December 2011.Botnets are a significant source of abusive messaging (spam, phishing, etc) and other types of malicious traffic. A promising approach to help mitigate botnet-generated traffic is signal analysis of transport-layer (\ie TCP/IP) characteristics, \eg timing, packet reordering, congestion, and flow-control. Prior work~\cite{spamflow-ceas08} shows that machine learning analysis of such traffic features on an SMTP MTA can accurately differentiate between botnet and legitimate sources. We make two contributions toward the \emph{real-world} deployment of such techniques: i) an architecture for real-time on-line operation; and ii) auto-learning of the unsupervised model across different environments without human labeling (\ie training). We present a ``SpamFlow'' SpamAssassin plugin and the requisite auxiliary daemons to integrate transport-layer signal analysis with a popular open-source spam filter. Using our system, we detail results from a production deployment where our auto-learning technique achieves better than $95$ percent accuracy, precision, and recall after reception of $\approx$ 1,000 emails",Auto-learning of SMTP TCP Transport-Layer Features for Spam and Abusive Message Detection,https://core.ac.uk/download/36728889.pdf,,,,core
78782276,2007-10-26T00:00:00,"Since about twenty years, the field of computer graphics has been striving towards increased realism. The goal is to achieve photorealistic outputs, which actually involves to precisely describe lighting phenomena, their intrinsic models and the light transport equations and to finally propose numerical schemes to solve them. In the field of off-line rendering, efficient and aggressive numerical methods have already been explored and set up. Therefore, even if there is still room for improvement, the most challenging and appealing problems are today related to real-time rendering which consists in ensuring to display at least 20 frames per second with some interactivity offered to the user. In this Ph.D. thesis, I therefore focus on the real-time or, if not possible, interactive simulation of light transport phenomena. During the three years while I tried to propose new ideas, I thus mainly aimed at efficiently using current existing hardware systems, GPUs or CPUs and designing new numerical schemes which may be easily implemented in such systems. It is finally the most common role of numericians: making a subtle mixture of mathematical and algorithmic methods and efficiently combining themDepuis maintenant vingt ans, la synthèse d'image par ordinateur s'est tournée vers un désir toujours plus grand de représenter fidèlement la réalité. Le but est donc devenu de calculer des images plus réalistes visuellement et plus précisément, physiquement réalistes: ceci a donc impliqué de décrire avec précision les phénomènes lumineux impliqués dans l'impression de toute photographie, leurs modèles intrinsèques et les équations sous-jacentes qui les régissent. Dans le domaine du rendu hors-ligne, des schémas numériques et des algorithmes de rendu efficaces et performants ont déjà été explorés et mis au point. Par conséquent, même s'il est encore possible de les améliorer, les problèmes les plus attrayants sont aujourd'hui ceux liés à l'affichage en temps réel d'images photo-réalistes qui consiste assurer un taux de rafraîchissement de 20 images par seconde tout en offrant à l'utilisateur un certain degré d'interactivité. Dans le cadre de cette thèse, je me suis donc concentré sur le calcul et la représentation en temps réel ou à défaut, interactive de la simulation des phénomènes physiques liés au transport de la lumière. Plus précisément, j'ai d'une part essayé d'utiliser efficacement le matériel informatique couramment utilisé aujourd'hui, comme les processeurs graphiques spécialisés ou encore les jeux instructions et les possibilités offertes par les processeurs centraux. D'autre part, j'ai proposé de nouvelles méthodes numériques qui peuvent être aisément mises en oeuvre dans de tels systèmes. C'est finalement le rôle de tout numéricien: combiner subtilement et efficacement schémas numériques et algorithme",Simulation interactive de la lumière à l'aide des sources virtuelles ponctuelles,,HAL CCSD,,,core
232274958,2006-01-31T08:00:00,"The emerging wireless technologies has made ubiquitous wireless access a reality and enabled wireless systems to support a large variety of applications. Since the wireless self-configuring networks do not require infrastructure and promise greater flexibility and better coverage, wireless ad hoc and sensor networks have been under intensive research. It is believed that wireless ad hoc and sensor networks can become as important as the Internet. Just as the Internet allows access to digital information anywhere, ad hoc and sensor networks will provide remote interaction with the physical world.
Dynamics of the object distribution is one of the most important features of the wireless ad hoc and sensor networks. This dissertation deals with several interesting estimation and optimization problems on the dynamical features of ad hoc and sensor networks. Many demands in application, such as reliability, power efficiency and sensor deployment, of wireless ad hoc and sensor network can be improved by mobility estimation and/or prediction. In this dissertation, we study several random mobility models, present a mobility prediction methodology, which relies on the analysis of the moving patterns of the mobile objects. Through estimating the future movement of objects and analyzing the tradeoff between the estimation cost and the quality of reliability, the optimization of tracking interval for sensor networks is presented. Based on the observation on the location and movement of objects, an optimal sensor placement algorithm is proposed by adaptively learn the dynamical object distribution. Moreover, dynamical boundary of mass objects monitored in a sensor network can be estimated based on the unsupervised learning of the distribution density of objects.
In order to provide an accurate estimation of mobile objects, we first study several popular mobility models. Based on these models, we present some mobility prediction algorithms accordingly, which are capable of predicting the moving trajectory of objects in the future. In wireless self-configuring networks, an accurate estimation algorithm allows for improving the link reliability, power efficiency, reducing the traffic delay and optimizing the sensor deployment. The effects of estimation accuracy on the reliability and the power consumption have been studied and analyzed. A new methodology is proposed to optimize the reliability and power efficiency by balancing the trade-off between the quality of performance and estimation cost. By estimating and predicting the mass objects\u27 location and movement, the proposed sensor placement algorithm demonstrates a siguificant improvement on the detection of mass objects with nearmaximal detection accuracy. Quantitative analysis on the effects of mobility estimation and prediction on the accuracy of detection by sensor networks can be conducted with recursive EM algorithms. The future work includes the deployment of the proposed concepts and algorithms into real-world ad hoc and sensor networks",Probabilistic approaches to the design of wireless ad hoc and sensor networks,https://core.ac.uk/download/232274958.pdf,Digital Commons @ NJIT,,,core
14449359,2010-01-01T00:00:00,"An appropriately designed motorway access control can decrease the total travel time spent in the system up to 30% and consequently increase the merging operations safety. To date, implemented traffic responsive motorway access control systems have been of local or regulatory type and not truly adaptive in the real sense of the meaning. Hence, traffic flow can be influenced positively by numerous intelligent transportation system (ITS) techniques. In this paper a contemporary approach is presented. It considers the design philosophy of an optimal and adaptive closed-loop multiple motorway access control strategy. The methodology proposed uses the artificial intelligence technique - known as reinforcement learning (RL) with multiple agents, and applies the Q-learning algorithm. One segment of the motorway network with three lanes in each direction and three motorway entries was designed. The detectors and traffic signals were placed at the entries (ramps). Traffic flows and traffic occupancy on the main line as well as the traffic demand on the motorway entries were taken as input model variables. The output variables referred to the travel speed on the corridor, the total travel time, and the total stop time. VISSIM micro-simulator and direct programming of the simulator functions were used in order to implement the RL technique. The peak hour was chosen for the time of simulation.
The model was tested in two phases. Its effectiveness was compared to ALINEA. It was observed that the proposed strategy was capable of responding both to dynamic sensory inputs from the environment and to dynamically changing environment. The model of the environment and supervision were not required. The control policy changed as response to the inherent system characteristic changes. It was confirmed that the strategy was truly adaptive and real-time responsive to the traffic demand on the corridor.
KEY WORDS: motorway access, traffic flows, control, strategy, artificial intelligence, Q-Learning, simulatio",Reinforcement Learning Technique in Multiple Motorway Access Control Strategy Design,https://core.ac.uk/download/14449359.pdf,'Faculty of Transport and Traffic Sciences',10.7307/ptt.v22i2.170,,core
41213247,2011-01-01T00:00:00,"In critical domains such as urban search and rescue (USAR), and bomb disposal, the deployment of teleoperated robots is essential to reduce the risk of first responder personnel. Teleoperation is a difficult task, particularly when controlling robots from an isolated safety zone. In general, the operator has to solve simultaneously the problems of mission planning, target identification, robot navigation, and robot control. We introduce a system to support teleoperated navigation with real-time mapping consisting of a two-step scan matching method that re-considers data associations during the search. The algorithm processes data from laser range finder and gyroscope only, thereby it is independent from the robot platform. Furthermore, we introduce a user-guided procedure for improving the global consistency of maps generated by the scan matcher. Globally consistent maps are computed by a graph-based maximum likelihood method that is biased by localizing crucial parts of the scan matcher trajectory on a prior given geo-tiff image. The approach has been implemented as an embedded system and extensively tested on robot platforms designed for teleoperation in critical situations, such as bomb disposal. Furthermore, the system was evaluated in a test maze by first responders during the Disaster City event in Texas 2008.Artificial Intelligence & Integrated Computer System",Mapping for the Support of First Responders in Critical Domains,,'Springer Science and Business Media LLC',10.1007/s10846-010-9520-x,,core
21045282,23/07/2009,"Abstract—Real-time traffic signal control is an integral part of the urban traffic control system, and providing effective real-time traffic signal control for a large complex traffic network is an extremely challenging distributed control problem. This paper adopts the multiagent system approach to develop distributed unsupervised traffic responsive signal control models, where each agent in the system is a local traffic signal controller for one intersection in the traffic network. The first multiagent system is developed using hybrid computational intelligent techniques. Each agent employs a multistage online learning process to update and adapt its knowledge base and decision-making mechanism. The second multiagent system is developed by integrating the simultaneous perturbation stochastic approximation theorem in fuzzy neural networks (NN). The problem of real-time traffic signal control is especially challenging if the agents are used for an infinite horizon problem, where online learning has to take place continuously once the agent-based traffic signal controllers are implemented into the traffic network. A comprehensive simulation model of a section of the Central Business District of Singapore has been developed using PARAMICS microscopic simulation program. Simulation results show that the hybrid multiagent system provides significant improvement in traffic conditions when evaluated against an existing traffic signal control algorithm as well as the SPSA-NN-based multiagent system as the complexity of the simulation scenario increases. Using the hybrid NN-based multiagent system, the mean delay of each vehicle was reduced by 78 % and the mean stoppage time, by 85 % compared to the existing traffic signal control algorithm. The promising results demonstrate the efficacy of the hybrid NN-based multiagent system in solving large-scale traffic signal control problems in a distributed manner. Index Terms—Distributed control, hybrid model, neural control, online learning, traffic signal control. I",Neural Networks for Real-Time Traffic Signal Control,,,,,core
213389987,2009-01-01T00:00:00,"This dissertation focuses on the collaboration of multiple heterogeneous, intelligent agents (hardware or software) which collaborate to learn a task and are capable of sharing knowledge. The concept of collaborative learning in multi-agent and multi-robot systems is largely under studied, and represents an area where further research is needed to gain a deeper understanding of team learning. This work presents experimental results which illustrate the importance of heterogeneous teams of collaborative learning agents, as well as outlines heuristics which govern successful construction of teams of classifiers. A number of application domains are studied in this dissertation. One approach is focused on the effects of sharing knowledge and collaboration of multiple heterogeneous, intelligent agents (hardware or software) which work together to learn a task. As each agent employs a different machine learning technique, the system consists of multiple knowledge sources and their respective heterogeneous knowledge representations. Collaboration between agents involves sharing knowledge to both speed up team learning, as well as to refine the team's overall performance and group behavior. Experiments have been performed that vary the team composition in terms of machine learning algorithms, learning strategies employed by the agents, and sharing frequency for a predator-prey cooperative pursuit task. For lifelong learning, heterogeneous learning teams were more successful compared to homogeneous learning counterparts. Interestingly, sharing increased the learning rate, but sharing with higher frequency showed diminishing results. Lastly, knowledge conflicts are reduced over time, as more sharing takes place. These results support further investigation of the merits of heterogeneous learning. This dissertation also focuses on discovering heuristics for constructing successful teams of heterogeneous classifiers, including many aspects of team learning and collaboration. In one application, multi-agent machine learning and classifier combination are utilized to learn rock facies sequences from wireline well log data. Gas and oil reservoirs have been the focus of modeling efforts for many years as an attempt to locate zones with high volumes. Certain subsurface layers and layer sequences, such as those containing shale, are known to be impermeable to gas and/or liquid. Oil and natural gas then become trapped by these layers, making it possible to drill wells to reach the supply, and extract for use. The drilling of these wells, however, is costly. Here, the focus is on how to construct a successful set of classifiers, which periodically collaborate, to increase the classification accuracy. Utilizing multiple, heterogeneous collaborative learning agents is shown to be successful for this classification problem. We were able to obtain 84.5% absolute accuracy using the Multi-Agent Collaborative Learning Architecture, an improvement of about 6.5% over the best results achieved by Kansas Geological Survey with the same data set. Several heuristics are presented for constructing teams of multiple collaborative classifiers for predicting rock facies. Another application utilizes multi-agent machine learning and classifier combination to learn water presence using airborne polar radar data acquired from Greenland in 1999 and 2007. Ground and airborne depth-soundings of the Greenland and Antarctic ice sheets have been used for many years to determine characteristics such as ice thickness, subglacial topography, and mass balance of large bodies of ice. Ice coring efforts have supported these radar data to provide ground truth for validation of the state (wet or frozen) of the interface between the bottom of the ice sheet and the underlying bedrock. Subglacial state governs the friction, flow speed, transport of material, and overall change of the ice sheet. In this dissertation, we focus on how to construct a successful set of classifiers which periodically collaborate to increase classification accuracy. The underlying method results in radar independence, allowing model transfer from 1999 to 2007 to produce water presence maps of the Greenland ice sheet with differing radars. We were able to obtain 86% accuracy using the Multi-Agent Collaborative Learning Architecture with this data set. Utilizing multiple, heterogeneous collaborative learning agents is shown to be successful for this classification problem as well. Several heuristics, some of which agree with those found in the other applications, are presented for constructing teams of multiple collaborative classifiers for predicting subglacial water presence. General findings from these different experiments suggest that constructing a team of classifiers using a heterogeneous mixture of homogeneous teams is preferred. Larger teams generally perform better, as decisions from multiple learners can be combined to arrive at a consensus decision. Employing heterogeneous learning algorithms integrates different error models to arrive at higher accuracy classification from complementary knowledge bases. Collaboration, although not found to be universally useful, offers certain team configurations an advantage. Collaboration with low to medium frequency was found to be beneficial, while high frequency collaboration was found to be detrimental to team classification accuracy. Full mode learning, where each learner receives the entire training set for the learning phase, consistently outperforms independent mode learning, where the training set is distributed to all learners in a team in a non-overlapping fashion. Results presented in this dissertation support the application of multi-agent machine learning and collaboration to current challenging, real-world classification problems",Collective Machine Learning: Team Learning and Classification in Multi-Agent Systems,https://core.ac.uk/download/213389987.pdf,'Paleontological Institute at The University of Kansas',,,core
102679009,2008,"Abstract Computational trust has been developed as a novel means of coping with uncertainty within collaborative communities of interacting peers. The idea now of-fers enourmous potential for use in pervasive mobile environments; however, to date there is little agreement about what computational trust itself means, and what the limitations that emerge from its use are. In this work, we project the idea of com-putational trust into machine learning terms, showing that trust is a metaphor that helps system designers reason about and exploit the intended deployment scenario to achieve their goals. Viewing a trust model as a strategy to confront a learning problem thus allows us to explore the effect that constraints, such as mobility and user participation, will have on the quantity of information available to learn from; in this work, we demonstrate this idea with a set of experiments on the Reality Min-ing Dataset. The results highlight that the most successful trust models will be based on strong contextual information about the environment they are to be deployed in. ",Learning to Trust on the Move,,,,,core
24519352,07/02/2008,"The paper is focused on the analysis and design of multivariate time series prediction systems. It addresses mainly practical issues, the main contribution is the developed and implemented conceptual predictive methodology. It is based on designed data management structures that define basic data flow. Despite the fact that the methodology is inspired by problems common for utility companies that distribute and control the transport of their applicable commodity, it may be considered as a general methodology. Currently, the predictive methodology combines several prediction techniques, such as regression by means of singular value decomposition, support vector machines and neural networks. Data management structures are open to other predictive algorithms as well. The methodology is implemented in the form of a software tool. It is verified on a real-life prediction task— prediction of the daily gas consumption of regional gas utility companies",,,,,,core
36204601,2011-01-01T00:00:00,"In critical domains such as urban search and rescue (USAR), and bomb disposal, the deployment of teleoperated robots is essential to reduce the risk of first responder personnel. Teleoperation is a difficult task, particularly when controlling robots from an isolated safety zone. In general, the operator has to solve simultaneously the problems of mission planning, target identification, robot navigation, and robot control. We introduce a system to support teleoperated navigation with real-time mapping consisting of a two-step scan matching method that re-considers data associations during the search. The algorithm processes data from laser range finder and gyroscope only, thereby it is independent from the robot platform. Furthermore, we introduce a user-guided procedure for improving the global consistency of maps generated by the scan matcher. Globally consistent maps are computed by a graph-based maximum likelihood method that is biased by localizing crucial parts of the scan matcher trajectory on a prior given geo-tiff image. The approach has been implemented as an embedded system and extensively tested on robot platforms designed for teleoperation in critical situations, such as bomb disposal. Furthermore, the system was evaluated in a test maze by first responders during the Disaster City event in Texas 2008.Artificial Intelligence & Integrated Computer System",Mapping for the Support of First Responders in Critical Domains,,'Springer Science and Business Media LLC',10.1007/s10846-010-9520-x,,core
41213264,2009-01-01T00:00:00,"Urban Search And Rescue (USAR) is a time critical task since all survivors have to be rescued within the first 72 hours. One goal in Rescue Robotics is to support emergency response by mixed-initiative teams consisting of humans and robots. Their task is to explore the disaster area rapidly while reporting victim locations and hazardous areas to a central station, which then can be utilized for planning rescue missions. To fulfill this task efficiently, humans and robots have to map disaster areas jointly while co- ordinating their search at the same time. Additionally, robots have to perform subproblems, such as victim detection and navigation, autonomously. In disaster areas these problems are extraordinarily challenging due to the unstructured environment and rough terrain. Furthermore, when communication fails, methods that are deployed under such conditions have to be decentralized, i.e. operational without a central station. In this thesis a unified approach joining human and robot resources for solving these problems is contributed. Following the vision of combined multi-robot and multi-human teamwork, core problems, such as position tracking on rough terrain, mapping by mixed teams, and decentralized team coordination with limited radio communication, are directly addressed. More specific, RFID-SLAM, a novel method for robust and efficient loop closure in large-scale environments that utilizes RFID technology for data association, is contributed. The method is capable of jointly improving multiple maps from humans and robots in a centralized and decentralized manner without requiring team members to perform loops on their routes. Thereby positions of humans are tracked by PDR (Pedestrian Dead Reckoning), and robot positions by slippage- sensitive odometry, respectively. The joint-graph emerging from these trajectories serves as an input for an iterative map optimization procedure. The introduced map representation is further utilized for solving the centralized and decentralized coordination of large rescue teams. On the one hand, a deliberate method for combined task assignment and multi-agent path planning, and on the other hand, a local search method using the memory of RFIDs for coordination, are proposed. For autonomous robot navigation on rough terrain and real-time victim detection in disaster areas an efficient method for elevation map building and a novel approach to genetic MRF (Markov Random Field) model optimization are contributed. Finally, a human in the loop architecture is presented that integrates data collected by first responders into a multi-agent system via wearable computing. In this context, the support and coordination of disaster mitigation in large-scale environments from a central-command-post-perspective are described. Methods introduced in this thesis were extensively evaluated in outdoor environments and official USAR testing arenas designed by the National Institute of Standards and Technology (NIST). Furthermore, they were an integral part of systems that won in total more than 10 times the first prize at international competitions, such as the RoboCup world championships.This is a Ph.D. thesis originally defended at University of Freiburg.Artificial Intelligence & Integrated Computer System",Mapping and Exploration for Search and Rescue with Humans and Mobile Robots,,Freiburg : University of Freiburg,,,core
148440413,2009-01-01T00:00:00,"Teleoperation is a difficult task, particularly when controlling robots from an isolated operator station. In general, the operator has to solve nearly blindly the problems of mission planning, target identification, robot navigation, and robot control at the same time. The goal of the proposed system is to support teleoperated navigation with real-time mapping. We present a novel scan matching technique that re-considers data associations during the search, enabling robust pose estimation even under varying roll and pitch angle of the robot enabling mapping on rough terrain. The approach has been implemented as an embedded system and extensively tested on robot platforms designed for teleoperation in critical situations, such as bomb disposal. Furthermore, the system has been evaluated in a test maze by first responders during the Disaster City event in Texas 2008. Finally, experiments conducted within different environments show that the system yields comparably accurate maps in real-time when compared to higher sophisticated offline methods, such as Rao-Blackwellized SLAM.(Best Paper Award Finalist)Artificial Intelligence & Integrated Computer System",Operator-Assistive Mapping in Harsh Environments,,'Institute of Electrical and Electronics Engineers (IEEE)',10.1109/SSRR.2009.5424159,,core
62874453,2006-01-01T08:00:00,"Integrating human knowledge with modeling tools, an intelligent decision support system (DSS) is developed to assist decision makers during different phases of flood management. The DSS is developed as a virtual planning tool and can address both engineering and non-engineering issues related to flood management. Different models (hydrodynamic, forecasting, and economic) that are part of the DSS share data and communicate with each other by providing feedback. The DSS is able to assist in: selecting suitable flood damage reduction options (using an expert system approach); forecasting floods (using artificial neural networks approach); modeling the operation of flood control structures; and describing the impacts (area flooded and damage) of floods in time and space. The proposed DSS is implemented for the Red River Basin in Manitoba, Canada. The results from the test application of DSS for 1997 flood in the Red River Basin are very promising. The DSS is able to predict the peak flows with 2% error and reveals that with revised operating rules the contribution of Assiniboine River to the flooding of Winnipeg city can be significantly reduced. The decision support environment allows a number of “what-if” type questions to be asked and answered, thus, multiple decisions can be tried without having to deal with the real life consequences",An intelligent decision support system for management of floods,https://digitalscholarship.unlv.edu/fac_articles/108,Digital Scholarship@UNLV,10.1007/s11269-006-0326-3,,core
24598743,2007,"In modern urban settings, automobile traffic and collisions lead to endless frustration as well as significant loss of life, property, and productivity. Recent advances in artificial intelligence suggest that autonomous vehicle navigation may soon be a reality. In previous work, we have demonstrated that a reservation-based approach can efficiently and safely govern interactions of multiple autonomous vehicles at intersections. Such an approach alleviates many traditional problems associated with intersections, in terms of both safety and efficiency. However, the system relies on all vehicles being equipped with the requisite technology — a restriction that would make implementing such a system in the real world extremely difficult. In this paper, we extend this system to allow for incremental deployability. The modified system is able to accommodate traditional human-operated vehicles using existing infrastructure. Furthermore, we show that as the number of autonomous vehicles on the road increases, traffic delays decrease monotonically toward the levels exhibited in our previous work. Finally, we develop a method for switching between various human-usable configurations while the system is running, in order to facilitate an even smoother transition. The work is fully implemented and tested in our custom simulator, and we present detailed experimental results attesting to its effectiveness. ",Sharing the road: Autonomous vehicles meet human drivers,,,,,core
22682061,2011,"Botnets are a significant source of abusive messaging (spam, phishing, etc) and other types of malicious traffic. A promising approach to help mitigate botnet-generated traffic is signal analysis of transport-layer (i.e. TCP/IP) characteristics, e.g. timing, packet reordering, congestion, and flow-control. Prior work [4] shows that machine learning analysis of such traffic features on an SMTP MTA can accurately differentiate between botnet and legitimate sources. We make two contributions toward the real-world deployment of such techniques: i) an architecture for real-time on-line operation; and ii) auto-learning of the unsupervised model across different environments without human labeling (i.e. training). We present a “SpamFlow ” SpamAssassin plugin and the requisite auxiliary daemons to integrate transport-layer signal analysis with a popular open-source spam filter. Using our system, we detail results from a production deployment where our auto-learning technique achieves better than 95 percent accuracy, precision, and recall after reception of ≈ 1,000 emails. ",Auto-learning of SMTP TCP Transport-Layer features for spam and abusive message detection,,USENIX Association,,,core
360545798,2011-04-07T00:00:00,"Urban structures exhibit complex patterns made of heterogeneous and irregular objects. Few works in the computational urban modelling literature have considered and examined the real geometric boundary of the city’s objects. However, most of these works are driven by Cellular Automata (CA) as a spatial modelling vehicle. This model has had success, but also has its limitations regarding the study of urban dynamics in computer simulation. Extensive modification of CA or use of a different modelling paradigm should be considered. We argue here that, representational realism must be achieved in urban complexity. This paper is an attempt to fill this gap to address the rigid structure of CA: we present a novel technique called the “vector-agent based simulation”, which uses discrete irregular objects as an autonomous spatial entity beneath an agent modelling structure. Through computer simulation, this new technique has been applied to von Thunen’s theory of agricultural land use as a hypothetical environment for model verification. The findings demonstrate that our proposal can be a new paradigm for urban simulationPublishedNon Peer ReviewedBarros, J. (2003), ""Simulating Urban Dynamics in Latin American Cities"", Proceedings of the 7th International Conference on GeoComputation, University of Southampton, UK.

Batty, M. (1997), ""Editorial, Urban Systems as Cellular Automata"", Environment and Planning B, Vol. 27, pp.

Batty, M. and Longley, P. (1994), ""Fractal Cities"", Academic press, London.

Batty, M. and Xie, Y. (1994), ""From Cells to Cities"", Environment and Planning B, Vol. 21, pp. 31-48.

Batty, M. and Xie, Y. (1997), ""Possible Urban Automata"", Environment and Planning B, Vol. 24, pp. 175-192.

Batty, M., Xie, Y. and Sun, Z. (1999), ""Modelling Urban Dynamics through GIS-Based Cellular Automata"", Computers, Environment and Urban Systems, Vol. 23, pp. 205-233.

Benenson, I., Omer, I. and Hatna, E. (2002), ""Entity-Based Modelling of Urban Residential Dynamics: The Case of Yaffo, Tel Aviv"", Environment and Planning B: Planning and Design, Vol. 29, pp. 491-512.

Collier, N. (2003), ""Repast: An Extensible Framework for Agent Simulation"", Social Science Research Computing at the university of Chicago, Chicago, IL, USA, Available online at: http://repast.sorceforge.net, last accessed: 09-02-04.

Colonna, A., Stefano, V., Lombardo, S., Papini, L. and Rabino, G. A. (1998), ""Learning Cellular Automata: Modelling Urban Modelling"", Proceedings of the 3rd International Conference on GeoComputation, University of Bristol, UK.

Couclelis, H. (1985), ""Cellular Worlds: A Framework for Modelling Micro-Macro Dynamics"", Environment and Planning A, Vol. 17, pp. 585-596.

Flache, A. and Hegselmann, R. (2001), ""Do Irregular Grids Make Difference? Relaxing the Spatial Regularity Assumption in Cellular Models of Social Dynamics"", Journal of Artificial Societies and Social Simulation, Vol. 4(4).

Goldstein, N. C. (2003), ""Brains Vs. Brawn-Comparative Strategies for the Calibration of a Cellular Automata-Based Urban Growth Model"", Proceedings of the 7th International Conference on GeoComputation, University of Southampton, UK.

Haklay, M., O’Sullivan, D. and Thurstain-Goodwin, M. (2001), ""So Go Downtown: Simulating Pedestrian Movement in Town Centres"", Environment and Planning B: Planning and Design, Vol. 28, pp. 343-359.

Jiang, B. and Gimblett, H. R. (2002), ""An Agent-Based Approach to Environmental and Urban Systems within Geographic Information Systems"", In Gimblett, H. R. (Ed.) Integrating Geographic Information Systems and Agent-Based Modelling Techniques for Simulating Social and Ecological Processes, a Volume in the Santa Fee Institute in the Science of Complexity, Oxford Investment Press, Inc., New York.

Li, X. and Yeh, A. G. (2001), ""Calibration of Cellular Automata by Using Neural Networks for the Simulation of Complex Urban Systems"", Environment and Planning A, Vol. 33, pp. 1445-1462.

Minar, N., Burkhart, R., Langton, C. and Askenazi, M. (1996) ""The Swarm Simulation System: A Toolkit for Building Multi-Agent Simulations"", Available online at: http://wiki.swarm.org/wiki/Papers_on_Swarm, last accessed: 12-01-04.

Okabe, A., Boots, B. and Sugihara, K. (1992), ""Spatial Tessellations: Concepts and Applications of Voronoi Diagram"", John Wiley & Sons Ltd., England.

O’Sullivan, D. (2000), ""Graph-Based Cellular Automata Models of Urban Spatial Processes"", Unpublished PhD Thesis, Bartlett School of Architecture and Planning, University College London, University of London, London, UK.

O’Sullivan, D. (2001), ""Graph-Cellular Automata: A Generalised Discrete Urban and Regional Model"", Environment and Planning B: Planning and Design, Vol. 28, pp. 687-707.

O’Sullivan, D., MacGill, J. R. and Yu, C. (2003), ""Agent-Based Residential Segregation: A Hierarchically Structured Spatial Model"", Presented at Agent 2003: Challenges in social simulation, University of Chicago and Argonne National Laboratory, IL, USA.

Portugali, J. (2000), ""Self Organization and the City"", Springer, Verlag, Berlin.

Rodrigue, J. P. (2003), ""Transport Geography on the Web: The Free Educational Materials"", Available online at: http://people.hofstra.edu/geotrans/eng/ch6en/conc6en/vonthunen.html, last access: 21/04/04.

Rodrigues, A., Grueau, C., Raper, J. and Neves, N. (1998), ""Environmental Planning Using Spatial Agents"", In Carver, S. (Ed.) Innovations in GIS 5, Selected Papers from the 5th National Conference on GIS Research UK (GISUK), Taylor & Francis Ltd., London.

Russell, S. and Norvig, P. (1995), ""Artificial Intelligence: A Modern Approach"", Printice Hall International Inc., NJ, USA.

Sasaki, Y. and Box, P. (2003), ""Agent-Based Verification of von Thunen's Location Theory"", Journal of Artificial Societies and Social Simulation, Vol. 6(2).

Schelling, T. C. (1969), ""Models of Segregation"", American Economic Association Papers and Proceedings, Vol. 59(2), pp. 488-493.

Semboloni, F. (2000), ""The Growth of an Urban Cluster into a Dynamic Self-Modifying Spatial Pattern"", Environment and Planning B: Planning and Design, Vol. 27(4), pp. 549-564.

Shi, W. and Pang, M. Y. C. (2000), ""Development of Voronoi-Based Cellular Automata: An Integrated Dynamic Model for Geographical Information Systems"", International Journal of Geographical Information Science, Vol. 14(5), pp. 455-474.

Takeyama, M. and Couclelis, H. (1997), ""Map Dynamics: Integrating Cellular Automata and GIS through Geo-Algebra"", International Journal of Geographical Information Science, Vol. 11(1), pp. 73-91.

Torrens, P. and O’Sullivan, D. (2000), ""Cities, Cells, and Complexity: Developing a Research Agenda for Urban GeoComputation"", Proceedings of the 5th International Conference on GeoComputation, University of Greenwich, London.

Torrens, P. M. (2000), ""How Cellular Models of Urban System Work (1. Theory)"", Centre for Advanced Spatial Analysis (CASA), University College London, London, CASA working paper 28.

Turton, I. (2003), ""Modelling Landuse Development Using Multi-Agent Systems"", Proceedings of the 7th International Conference on GeoComputation, University of Southampton, UK.

Weiss, G. (1999), ""Multiagent Systems: A Modern Approach to Distributed Artificial Intelligence"", the MIT Press, England.

White, R. and Engelen, G. (2000), ""High-Resolution Integrated Modelling of the Spatial Dynamics of Urban and Regional Systems"", Computers, Environment and Urban Systems, Vol. 24, pp. 383-400.

Wilson, A. G. (2000), ""Complex Spatial Systems"", Pearson Education Limited, England.

Wolfram, S. (2002), ""A New Kind of Science"", Wolfram Media, Champaign, IL.

Worboys, M. F. (1995), ""GIS a Computing Perspective"", Taylor & Francis, UK.

Wu, F. (1998), ""An Experiment on the Generic Polycentricity of Urban Growth in a Cellular Automatic City"", Environment and Planning B: Planning and Design, Vol. 25, pp. 731-752.

Xu, H. (2003), ""A Model-Based Approach for Development of Multi-Agent Software Systems"", Unpublished PhD Thesis, University of Illinois at Chicago, Chicago, USA.

Yeh, A. G. and Li, X. (2001), ""A Constrained CA Model for the Simulation and Planning of Sustainable Urban Forms by Using GIS"", Environment and Planning B: Planning and Design, Vol. 28, pp. 733-753.

Yeh, A. G. and Li, X. (2002), ""A Cellular Automata Model to Simulate Development Density for Urban Planning"", Environment and Planning B: Planning and Design, Vol. 29, pp. 431-450",Irregular vector-agent based simulation for land-use modelling,,,,,core
357314651,2009-01-01T00:00:00,"Green tea catechins (GTC) have been shown to inhibit the activities of enzymes involved in folate uptake. Hence, regular green tea drinkers may be at risk of impaired folate status. The present experiments aimed at studying the impact of dietary GTC on folate concentrations and metabolism. In a human pilot study (parallel design) healthy men consumed for 3 weeks 6 capsules (~670 mg GTC) per day (2 capsules with each principal meal) containing aqueous extracts of the leaves of Camellia sinensis (n=17) or placebo (n=16). No differences in plasma folate concentrations were observed between treatments. We further fed groups of 10 male rats diets fortified with 0, 0.05, 0.5, 1, or 5 g GTC/kg for 6 weeks. Only at the highest intake, GTC significantly decreased serum 5-methyl-tetrahydrofolate concentrations in rats, while mRNA concentrations of reduced folate carrier, proton-coupled folate transporter/heme carrier protein 1, and dihydrofolate reductase (DHFR) remained unchanged in intestinal mucosa. Using an in vitro enzyme activity assay, we observed a time-and dose-dependent inhibition of DHFR activity by epigallocatechin gallate and a green tea extract. Our data suggest that regular green tea consumption is unlikely to impair folate status in healthy males, despite the DHFR inhibitory activity of GTC. K e y w o r d s : folates, catechins, bioavailability, human, rat MATERIAL AND METHODS Dihydrofolate reductase activity The inhibition of human dihydrofolate reductase (DHFR) activity by (-) epigallocatechin gallate (EGCG) and a standardized green tea extract (Polyphenon 60 (P60); Sigma Chemical Co., St Louis, MO, USA) was measured using a commercial dihydrofolate reductase assay kit (Sigma-Aldrich) according to the manufacturer&apos;s protocol. Methotrexate, a well-known competitive DHFR inhibitor was used as a positive control. EGCG and P60 were dissolved in ultra pure-water (containing 1% ascorbic acid (w/v) (Merck KGaA, Darmstadt, Germany) to stabilize the catechins) on the day of the experiments. DHFR was used at a final activity of 1.5 x 10 -3 units per reaction. Final concentrations of EGCG and methotrexate were 1000, 100 and 10 nmol/L per reaction. P60 was used at final concentrations of 1428 .57, 142.86, and 14.29 µg/L and, thus, contained 1060 , 106,  and 10.6 nmol/L EGCG and 1427.3 nmol/L of the gallated catechins (EGCG, ECG and gallocatechin gallate), respectively. Rat study Fifty male Wistar rats (Harlan Winkelmann GmbH, Borchen, Germany) with an initial body weight of 99.8 ± 2.0 g (mean ± SEM) were randomized into 5 groups of 10 animals each and housed pair-wise with sawdust bedding under controlled environmental conditions (23 ± 2°C and 65 ± 5% relative humidity, 12 h dark-light cycle). The rats were kept for 5 days on a folate-adjusted rat diet for growing animals containing 2 mg of folic acid/kg (C1027; Altromin GmbH, Lage, Germany) and thereafter received their respective experimental diets consisting of the standard diet supplemented with 0, 0.05, 0.5, 1, or 5 g green tea catechins per kg diet using P60 as the source of catechins (see  The animal experiment was conducted in accordance with the German Guidelines and Regulations on Animal Care (Deutsches Tierschutzgesetz, 2006)  and was approved by the University of Kiel Ethics Committee on Animal Care. Human pilot study Healthy males were recruited by advertisement at the University and local community of Reading (United Kingdom) and amongst volunteers who previously participated in nutritional trials at the Hugh Sinclair Human Nutrition Unit. Inclusion criteria were: male gender, 18-55 y of age, and a BMI in the range of 22-32 kg/m 2 . Subjects were excluded from the trial if they were diagnosed with any illness or on long-term medication, used dietary supplements, participated in &gt;5 h of aerobic exercise activity per week, or were involved in a clinical trial within 3 months prior to the study. The study protocol was approved by the University of Reading ethics committee and all subjects gave written informed consent before participation. A standardized aqueous green tea extract prepared from the leaves of Camellia sinensis L. (a kind gift of Cognis Deutschland GmbH &amp; Co KG, Monheim am Rhein, Germany) was used to make the green tea extract (GTE) capsules. The composition of the GTE is given in  The trial was designed as a double-blind placebo-controlled parallel study. Thirty-one volunteers were randomly assigned to one of two treatment groups (GTE, n=16 or placebo, n=15) with similar BMI and age (data not shown). Subjects took 6 capsules per day, two with each principal meal, for 3 weeks and were instructed to limit their daily tea and coffee consumption to ≤ 3 cups, but to otherwise maintain their normal diet and exercise patterns. Compliance was determined by counting of the returned capsules at the end of the trial and was high (&gt;98%). Blood samples (20 ml) were drawn into tubes containing 0.05 mL 15% K 3 EDTA (Vacutainer; Becton Dickinson UK Ltd., Oxford, UK) after an overnight fast on the first and last day of the intervention period. Plasma was immediately obtained by centrifugation (1,000 x g, 10 min) and 3 ml aliquots were stored at -80°C until analysis. Folate quantification by HPLC Procedures for extraction and purification of folates from human plasma and rat serum and liver samples by strong anion exchange solid-phase extraction were described previously by Witthoft et al. (18). Dialysed rat serum (500 µl/g) was used to ensure complete deconjugation of folate polyglutamates in liver samples; modified from Patring et al. (19). Analyses were performed using an HPLC system (Agilent 1100) consisting of a 104 10-formyltetrahydrofolate (10-HCO-H 4 folate), and 5,10-methenyltetrahydrofolate (5,10-CH + -H 4 folate) (a gift of Merck Eprova AG, Schaffhausen, Switzerland, except 10-HCO-H 4 folate, which was purchased from Schircks Laboratories, Jona, Switzerland). Quantification was based on a multilevel (n=7) external calibration curve with a linear range over 1.2-118.0 ng/mL for H 4 folate, 0.6-93.1 ng/mL for 5-CH 3 -H 4 folate, 0.9-184.1 ng/mL for 10-HCO-H 4 folate and 9.3-184.5 ng/mL for 5,10-CH + -H 4 folate. mRNA quantification RNA was isolated from rat duodenal mucosa using the RNeasy Lipid Tissue Kit (Qiagen GmbH, Hilden, Germany) according to the manufacturer&apos;s protocol. DNA digestion was performed with RNase-Free DNase Set (Qiagen). RNA integrity was checked by electrophoresis on a denaturing agarose gel and ethidium bromide staining. The concentration and purity of isolated RNA was determined by measuring the absorbance (AB) at 260 and 280 nm in a spectrophotometer (DU800, Beckmann Instruments; Munich, Germany). A ratio of &gt;1.8 between AB 260nm and AB 280nm was considered as acceptable. RNA aliquots were stored at -80°C until analysis. Primer pairs of β-actin, reduced folate carrier (RFC) and proton-coupled folate transporter/heme carrier protein-1 (PCFT/HCP1) were designed to the corresponding sequences of Rattus norvegicus mRNA with Primer3 software (http://frodo.wi.mit.edu/cgi-bin/primer3/ primer3_www.cgi; 03.05.2007) and purchased from MWGBiotech AG (Ebersberg, Germany). The sequences of primers used in this study were as follows: Sense primer for β-actin, 5´-GGGGTGTTGAAGGTCTCAAA-3´, antisense primer for β-actin, 5´-TGTCACCAACTGGGACGATA-3´; sense primer for RFC, 5´-GGCTCGTGTTCTACCTCTGC-3´, antisense primer for RFC, 5´-GGTAGTCGGTGAGCAGGAAG-3´; sense primer for PCFT/HCP1, 5´-TGAGCTAAGCACACCCCTCT-3´, antisense primer for PCFT/HCP1, 5´-TCCGTACCCTGTGAACATGA-3´. The product size was 90 base pair (bp) for β-actin; 183 bp for RFC and 217 bp for PCFT/HCP1. QuantiTect ® Primer Assay (Qiagen) was used for DHFR mRNA amplification, with a product size of 88 bp. For one-step quantitative reverse transcriptase polymerase chain reaction (one-step qRT-PCR) two aliquots of RNA were amplified. External relative standard curves of total RNA were determined with each run. Data was normalized by dividing the concentrations of RFC, PCFT/HCP1 or DHFR by the concentrations of β-actin mRNA. Each PCR reaction (final volume 20 µl) contained 0.5 µmol/L of each primer, 10 µl of 2x QuantiTect ® SYBR ® Green RT-PCR Master Mix (Qiagen), 0.2 µl QuantiTect RT-Mix (Qiagen), 8 µl of RNA dilution and 1.4 µl water. Real-time cycler conditions were set according to the manufacturers protocol to 40 cycles with annealing temperatures of 56°C for β-actin, 59°C for RFC, 56°C for PCFT/HCP1 and 55°C for DHFR, respectively. Quantification and melting curves of the amplified products were analysed using the RotorGene 6.0 software (Corbett Lifescience; Sydney, Australia). Melting curve analyses and agarose gel electrophoresis with ethidium bromide staining were performed to exclude non-specific products. Statistical analyses Statistical calculations were performed with GraphPad Prism 4 software (GraphPad Software Inc., San Diego, CA, USA). Analyses of the data from the rat study and the in vitro assay were performed by means of a one-way ANOVA followed by Dunnetts test for multiple comparisons of group means between animals receiving GTC or control diet. Analyses of the data from the human pilot study were performed by means of a paired Student&apos;s t-test for comparison of baseline vs. treatment and by means of an unpaired Student&apos;s t-test for comparisons between subjects receiving GTE or placebo. Reported values are means ± SEM and effects were considered significant at P&lt;0.05. RESULTS Dihydrofolate reductase activity in vitro Both pure EGCG and P60, at concentrations of 1000 for EGCG and 1060 nmol/L for EGCG from P60, respectively, time-dependently inhibited DHFR activity  Serum and liver folate concentrations in rats Feed consumption and final body mass (318.7 ± 4.8 g) of the Wistar rats were similar in all groups. Intake of diets containing 0.5% GTC over a period of 42 days significantly decreased the serum concentration of 5-CH 3 -H 4 folate compared to control rats, whereas the concentrations of H 4 folate remained unchanged (  Relative mRNA levels of reduced folate carrier and dihydrofolate reductase in rat duodenal mucosa The housekeeping gene β-actin was expressed at similar levels in all animals and no significant differences in the relative mRNA levels of RFC, PCFT/HCP1 or DHFR in the duodenal mucosa were observed  Plasma folate concentrations in humans Consumption of 670 mg of GTC per day or placebo did not affect plasma folate concentrations in healthy male volunteers. No significant differences in plasma concentrations of 5-CH 3 -H 4 folate were observed between the treatment groups at baseline (placebo, 16.3 ± 2.6 nmol/L; GTE, 19.1 ± 2.4 nmol/L) or after intervention (placebo, 15.5 ± 2.1 nmol/L; GTE, 17.6 ± 2.4 nmol/L). DISCUSSION Green tea is a widely consumed beverage in many countries and contains appreciable amounts of polyphenols. Catechins (flavanols) are the major subclass of bioactive compounds within the polyphenol fraction of green tea. Epidemiological studies associated a high dietary intake of catechins with a reduced risk to suffer from a variety of diseases (reviewed in 20), including certain forms of cancer (21). The underlying molecular and cellular mechanisms by which green tea catechins may mediate anticarcinogenic acitivty seem to be diverse: Cell culture experiments as well as studies in rodents indicate that green tea catechin may inhibit angiogenesis via a down-regulation of vascular endothelial growth factor (reviewed in 22). Furthermore it has been suggested that the anticancer activity of green tea catechins against different kind of cancers may find an explanation in direct targeting of lipid rafts (23). Recent in vitro studies have shown that epigallocatechin gallate (EGCG), the predominant catechin in green tea, competitively inhibits the enzyme dihydrofolate reductase (DHFR) (9, 13). DHFR inhibition is the mechanism by which so-called antifolates, such as the cytostatic drug methotrexate, inhibit cell division and reduce tumor growth (15, 24). Co-administration of folic acid and  the DHFR inhibitors methotrexate and pyrimethamine, respectively, reduced plasma folate concentrations in rats  The commercial green tea extract Polyphenon 60 (P60) used in the rat study and its principle bioactive ingredient EGCG inhibited DHRF activity time-and concentration-dependently in vitro  In order to study whether or not the effects observed in vitro bear a meaning for the more complex physiological processes in vivo, Wistar rats were fed for 42 days with diets fortified with increasing concentrations of green tea catechins (GTC) using a standardized green tea extract (P60). The diets contained 2 mg folic acid per kg, which is equivalent to twice the dietary recommendations for laboratory rats as given by the National Research Council (28). It is noteworthy that folates synthesized by the microflora of the large intestine are absorbed and may significantly contribute to blood folate concentrations (reviewed in 29). The diet used in this study was therefore formulated to provide a minimum of substrate to the intestinal microflora to limit bacterial folate synthesis. Only in those animals fed the highest concentrations of the green tea extract (0.5% GTC), did we observe a significant decrease in serum 5-CH 3 -H 4 folate concentrations as compared to the control group (  At a given substrate affinity and substrate concentration, the capacity of enzymatic turnover of folates as well as the amount of their carrier-mediated transport across cellular membranes is mainly affected by the amount of enzymes/carriers present at the tissue level. Because catechins are known to alter the gene expression for a variety of proteins (35), we quantified relative mRNA concentrations of the RFC, PCFT/HCP1, and DHFR in the duodenal mucosa of rats fed GTC. No significant differences in mRNA concentrations of RFC, PCFT/HCP1, and of DHFR were found between the experimental groups  The current findings suggested that GTC might decrease serum folate concentrations only if supplied at supra-nutritional doses. A 70 kg human would have to drink almost 100 cups of green tea infusion per day to match the highest dose fed to rats in the present study. Because such a human study would be unfeasible as well as unrealistic, we designed a pilot study with a standardized green tea extract to assess whether or not regular consumption of high doses of GTC might affect plasma folate concentrations in humans. The intake of 670 mg of GTC per day, which corresponds to about 20 cups of green tea, caused no significant differences in plasma concentrations of 5-CH 3 -H 4 folate between the treatment and placebo groups, both of which consuming a normal diet containing on average ~328 ± 26 µg folate/d. Insufficient dietary intake of folates for as short as 2-3 weeks has been reported to result in reduced blood concentrations of the vitamin (30). Our findings therefore suggest that green tea drinking is unlikely to affect plasma folate concentrations in healthy, free-living subjects and that a longer treatment period and/or even higher doses of dietary GTC may be necessary to induce changes in folate concentrations, if possible at all. Further human studies with GTC and a standardized supply of folic acid (in the absence of naturally occurring reduced folates) are warranted to investigate the influence of GTC on DHFR activity in vivo. In addition, the measurement of (oxidized) serum folic acid should be considered because folic acid has been found in serum of subjects consuming folic acid-fortified foods for 5 d (11). Based on the experiments presented here, it appears unlikely that daily green tea consumption, even at high levels, may affect folate concentrations in healthy humans.  Acknowledgement",augustin,https://core.ac.uk/download/357314651.pdf,,,,core
90989035,2010-03-01T00:00:00Z,"An appropriately designed motorway access control can decrease the total travel time spent in the system up to 30% and consequently increase the merging operations safety. To date, implemented traffic responsive motorway access control systems have been of local or regulatory type and not truly adaptive in the real sense of the meaning. Hence, traffic flow can be influenced positively by numerous intelligent transportation system (ITS) techniques. In this paper a contemporary approach is presented. It considers the design philosophy of an optimal and adaptive closed-loop multiple motorway access control strategy. The methodology proposed uses the artificial intelligence technique - known as reinforcement learning (RL) with multiple agents, and applies the Q-learning algorithm. One segment of the motorway network with three lanes in each direction and three motorway entries was designed. The detectors and traffic signals were placed at the entries (ramps). Traffic flows and traffic occupancy on the main line as well as the traffic demand on the motorway entries were taken as input model variables. The output variables referred to the travel speed on the corridor, the total travel time, and the total stop time. VISSIM micro-simulator and direct programming of the simulator functions were used in order to implement the RL technique. The peak hour was chosen for the time of simulation.
The model was tested in two phases. Its effectiveness was compared to ALINEA. It was observed that the proposed strategy was capable of responding both to dynamic sensory inputs from the environment and to dynamically changing environment. The model of the environment and supervision were not required. The control policy changed as response to the inherent system characteristic changes. It was confirmed that the strategy was truly adaptive and real-time responsive to the traffic demand on the corridor.
KEY WORDS: motorway access, traffic flows, control, strategy, artificial intelligence, Q-Learning, simulatio",Reinforcement Learning Technique in Multiple Motorway Access Control Strategy Design,,"University of Zagreb, Faculty of Transport and Traffic Sciences",10.7307/ptt.v22i2.170,"[{'title': None, 'identifiers': ['1848-4069', 'issn:1848-4069', 'issn:0353-5320', '0353-5320']}]",core
357196805,2003-01-01T00:00:00,"Correspondence and offprint requests to: Luigi Gnudi; E-mail: luigi.gnudi@kcl.ac.uk Abstract Background. Podocyturia is a marker of diabetic nephropathy, a possible determinant of its progression and a powerful risk factor for cardiovascular disease. A reduction in podocyte adhesion to the glomerular basement membrane (GBM) via downregulation of α3β1 integrin expression, the main podocyte anchoring dimer to the GBM, may represent one of the mechanisms of podocyturia in glomerular disease. This study investigated the role of mechanical forces and transforming growth factor beta1 (TGFβ1) in podocyte adhesion and integrin expression. Methods. Conditionally immortalized murine podocytes were exposed to mechanical stretch and/or TGFβ1 for 48 h. Podocyte adhesion, apoptosis and α3β1 integrin expression were assessed. Results. Stretch and TGFβ1 significantly reduced podocyte adhesion and α3β1 integrin expression, events paralleled by increased apoptosis. Blockade of β1 integrin, with a specific antibody, demonstrated a reduced podocyte adhesion indicating that β1 integrin downregulation was required for the loss of podocyte adhesion. This was linked to an increase in podocyte apoptosis. The role of apoptosis in podocyte adhesion was further investigated using caspase-3 inhibitors. Podocyte apoptosis inhibition did not affect stretch-and TGFβ1-mediated integrin downregulation and the loss of podocyte adhesion, suggesting that α3β1 integrin downregulation is sufficient to alter cell adhesion. Although stretch significantly increased podocyte TGFβ type I, II and III receptors but not podocyte TGFβ1 secretion, the combination of stretch and TGFβ1 did not show any additive or synergistic effects on podocyte adhesion and α3β1 integrin expression. Conclusions. These results suggest that downregulation of α3β1 integrin expression, by mechanical forces or TGFβ1, is per se sufficient to reduce podocyte adhesion. Apoptosis may represent a parallel important determinant of the podocyte loss from the GBM. Keywords: cell adhesion; podocyte; stretch; TGFβ1; α3β1 integrin Introduction Glomerular capillary stability depends on the co-operative function of endothelial cells, mesangial cells and podocytes along the glomerular basement membrane (GBM). The histopathological changes observed in the glomerulus of chronic nephropathies are characterized by alteration of the filtration barrier (predominantly podocytes and GBM) and by excessive extracellular matrix deposition  Podocytes adhere to the GBM principally via α3β1 integrin  Glomerular hypertension and transforming growth factor beta1 (TGFβ1) are important factors in the development of proteinuria in diabetic and other chronic glomerulopathies, with evidence showing that upregulation of TGFβ1 expression by a variety of stimuli  Materials and methods All materials and chemicals were purchased from Sigma-Aldrich (Dorset, UK) unless otherwise stated. Primary antibodies were purchased as follows: anti-α3 integrin (BD Biosciences, Oxford, UK), anti-β1 integrin, anti-TGFβ type II receptor (Santa Cruz Biotechnology Inc., Heidelberg, Germany), anti-TGFβ type I and III receptors (AutogenBioclear, Wiltshire, UK), Murine conditionally immortalized podocytes were kindly donated by Dr Peter Mundel (Albert Einstein College of Medicine, NY, USA) and cultured using standard conditions  Human lung adenocarcinoma epithelial cells (A549) were used as a source of laminin-10/11, as previously described  Cell number and viability Cells were harvested after treatment with 0.25% trypsin/0.5% EDTA and counted using a Coulter Counter (Beckman-Coulter, Luton Beds, UK). Cell viability was assessed using a 0.4% Trypan blue solution. Exposure to mechanical stretch and TGFβ1 Differentiated podocytes were seeded in equal numbers on silicon elastomer-base culture plates (Bioflex plates, Flexcell, Dunn Labortechnik GmbH, Germany) coated with 2 µg/cm 2 human extracellular matrix, a mixture of laminin, collagen IV and heparin sulphate proteoglycans (HECM, BD Biosciences), and cultured for 7 days. Podocytes were studied at different degrees of confluence: 450 000 cells per well (∼100% confluent), 300 000 cells per well (∼80% confluent) and 150 000 cells per well (∼50% confluent). Podocytes were exposed to repeated stretch/relaxation cycles by mechanical deformation using the FX3000 strain unit (Flexcell Int., USA) to mimic glomerular hypertension, as previously described  Differentiated cells were exposed to TGFβ1 (5 ng/mL, R&amp;D Systems, Oxon, UK) for up to 48 h. To assess potential interactions between TGFβ1 and mechanical stretch, experiments were further conducted on podocytes exposed to mechanical stretch (20% elongation), in the presence or absence of TGFβ1. Cells grown under identical conditions, but not treated with TGFβ1, served as controls. Exposure to tunicamycin treatment Differentiated podocytes were incubated with tunicamycin (10 µg/mL), an inhibitor of protein N-glycosylation, or with vehicle (DMSO, 0.05%), for 48 h. Cells were used for an adhesion assay. Cell adhesion assay Microtitre plates (96 wells, Bio-Greiner, Gloucester, UK) were coated with HECM (2 µg/cm 2 ) and human placental collagen IV (30 µg/cm 2 ) over a 2-h period at 37 • C. For laminin coating, A549 cells were seeded (3 × 10 4 cells/well) to near confluence, maintained in a culture at confluence for 3 days and subsequently lysed with 0.1 mol/L NH 4 OH as previously described  Apoptosis Following exposure to stretch or TGFβ1, podocyte apoptosis was assessed using FACS TM annexin-V-fluorescein isothiocyanate (FITC)/propidium iodide (PI), ApoAlert R Caspase-3 Activity Fluorescent Assay Kit (Clontech, CA, USA) and Apoptotic DNA Ladder Detection Kit II (PromoCell GmbH, Heidelberg, Germany) following the manufacturer&apos;s protocols. Staurosporine-treated podocytes (4 h, 1 µmol/L) and Triton X-100-treated cells (1 h, 1%) were used as positive controls, respectively, for apoptosis and cell death. To further investigate the link between α3β1 integrin and apoptosis on cell adhesion, podocytes were exposed to the anti-β1 integrin antibody for 30 min at 37 • C prior to the determination of their ability to adhere to HECM, and assessment of apoptosis. In parallel experiments podocytes at Pennsylvania State University on October 6, 2016 http://ndt.oxfordjournals.org/ Downloaded from α3β1 integrin and podocyte adhesion 2647 were treated with a solution of caspase inhibitor-VI (Merck Chemicals Ltd, Nottingham, UK) at 10 µmol/L for 1 h prior, and throughout exposure to stretch or TGFβ1, to investigate the effects of α3β1 integrin downregulation on podocyte adhesion independent of apoptosis. Real-time RT-PCR mRNA was isolated using the RNeasy Mini Kit from Qiagen (Crawley, UK). mRNA concentration was determined using the Ribogreen R assay from Molecular Probes (Leiden, The Netherlands). Total RNA (250 ng) was reverse-transcribed into cDNA using an iScript cDNA synthesis kit (Bio-Rad, Herts, UK). The level of gene expression was analysed by real-time quantitative RT-PCR performed with the TaqMan R gene expression assay based on real-time detection of accumulated fluorescence (Assays-on-Demand TM-ABI Prism 7900HT Sequence Detection System, Applied Biosystems, Warrington, UK). Gene expression levels of the target sequence were normalized to the expression of the housekeeping gene 18 S ribosomal RNA. SDS-PAGE and immunoblotting Podocyte cell lysate was separated by discontinuous SDS-PAGE, under reducing conditions. Immunoreactive bands were visualized after blocking for 1 h at RT with non-fat dried milk (5% w/v) in PBS/0.2% Tween-20 or TBS/5% Tween-20, and incubated with primary and secondary horseradish peroxidase antibodies following the manufacturer&apos;s instructions. Immunoreactive bands were visualized with enhanced chemiluminescence (Amersham Life Science, Buckinghamshire, UK) and quantified using Image J (NIH, Bethesda, MD, USA). Equal protein loading was verified with Ponceau-S staining and by analysis of the housekeeping protein GAPDH (1/200, Chemicon, Hampshire, UK). Glycosidase digestion Glycosidase digestion was achieved according to the manufacturer&apos;s instructions (New England Biolabs, Herts, UK). Briefly, cell lysates (20 µg) were incubated in a glycoprotein-denaturing buffer at 100 • C for 10 min. Denatured proteins were treated with either peptide-N-glycosidase F (PNGase F) or endoglycosidase H (Endo H) for 1 h at 37 • C, and subsequently resolved by 10% SDS-PAGE under reducing conditions, followed by immunoblotting with α3 or β1 integrin-specific antiserum as described above. α3β1 integrin cell surface expression (flow cytometry) Podocytes were detached using Trypsin/EDTA and left to recover in a fresh complete medium for 30 min at 37 • C with 5% CO 2 . Podocytes (2 × 10 5 cells) were then incubated in a cold blocking buffer (FACS buffer/5% goat serum) for 5 min on ice. Cells were incubated with a specific antibody to β1 integrin against the fully glycosylated cell membrane β1 isoform (Dr J. Marshall, St Bartholomew&apos;s Hospital, London, UK), or with a specific isotype control (Jackson Immuno Research Laboratories, West Grove, PA, USA) at 4 • C, in a light-protected environment, for 30 min. The cells were washed with PBS/2% FBS, incubated with a FITC-conjugated anti-rat secondary antibody (Jackson Immuno Research Laboratories) on ice, and then fixed in 1% paraformaldehyde prior to analysis using a FACScan (Becton Dickinson, Oxford, UK). Each labelling procedure included assays for cellular autofluorescence levels and negative controls stains using an isotype control antibody. The available anti-mouse α3 integrin antiserum was found unsuitable for FACS analysis studies, and therefore α3 integrin expression was only analysed by western immunoblotting. TGFβ1 protein determination The total secreted TGFβ1 was measured using a quantitative sandwich ELISA, according to the manufacturer&apos;s instructions (R&amp;D Systems). Latent TGFβ1 from each sample was activated by acidification and total TGFβ1 concentration was determined using a standard curve. Measured TGFβ1 levels were corrected for the cell number determined using a Coulter Counter (Beckman-Coulter, Luton Beds, UK). Statistics Statistics were assessed by SPSS 15.0 software. Data are presented as mean ± standard error of the mean (SEM), unless otherwise stated. Differences  between two groups were tested by Student&apos;s t-test. When more than two groups were compared, differences were tested by ANOVA followed by LSD post hoc pair-wise comparisons. Statistical significance was accepted at P ≤ 0.05. Results Stretch and TGFβ1 reduce podocyte adhesion to extracellular matrix substrates Exposure of confluent podocytes to mechanical stretch and TGFβ1 significantly reduced cell adhesion to HECM by 22 and 30%, respectively (P &lt; 0.01) (  When 80% and 50% confluent podocytes were exposed to mechanical stretch, most of the podocytes detached (70 ± 1.2%, and 74 ± 2.3% respectively, n = 3). An adhesion assay was performed with the remaining adherent cells and showed a 93 ± 2.1% (P = 0.002) reduction in adhesion in 80% confluent cells, and a 74 ± 2.01% (P = 0.01) reduction for 50% confluent cells (n = 3, in quadruplicate). In TGFβ1-treated cells, 80% confluent cells showed a 25 ± 4.36% (P = 0.035) reduction in cell adhesion and 50% confluent cells showed a 51 ± 4.53% (P = 0.027) reduction (n = 3, in quadruplicate). These data suggest an important role for cell-cell contact in the podocyte response to stretch or TGFβ1. To mimic the confluent podocyte layer in glomerular capillaries, all subsequent experiments were conducted on confluent podocytes. Stretch and TGFβ1 downregulate α3β1 integrin protein expression Under baseline conditions, a single band (135 kDa) was detected for α3 integrin  Stretch significantly reduced α3 integrin expression by 24% (P = 0.01)  Treatment with TGFβ1 resulted in a significant decrease in the α3 integrin subunit by 46% and β1 MF by 20% (P &lt; 0.01;  FACS analysis confirmed the change observed in β1 MF protein expression in cells exposed to mechanical stretch and TGFβ1 (β1: control cell versus stretched cell: 100 ± 2.56 versus 71 ± 3.09, P = 0.01; β1: control cell versus TGFβ1-treated cell: 100 ± 2.27 versus 43.40 ± 3.86, P = 0.001). Messenger RNA for both α3 and β1 integrin subunits was unchanged by mechanical stretch (α3: non-stretch 1.00 ± 0.01 versus stretch 1.12 ± 0.6; β1: non-stretch 1.02 ± 0.05 versus stretch 1.22 ± 0.13, arbitrary units, n = 10-14), while TGFβ1-treated cells showed a significant downregulation only for β1 integrin subunit mRNA (α3: vehicle 1.00 ± 0.02 versus TGFβ1 1.00 ± 0.01; β1: vehicle 1.04 ± 0.07 versus TGFβ1 0.79 ± 0.02; P = 0.01, arbitrary units, n = 7-10). α3β1 Integrin expression and glycosylation Upon deglycosylation, using the glycosidase PNGase (which hydrolyzes N-glycan chains of glycopeptides), a shift in the electrophoretic mobility of α3 and β1 integrin was observed. The molecular weight of the α3 subunit was shifted from 135 to 90 kDa, whilst the two β1 isoforms shifted from 130/110 to 85 kDa (  Treatment with PNGase F showed that stretch significantly downregulated α3 CP (CP, core peptide) and β1 CP by 40% and 70%, respectively (P &lt; 0.05) (  Stretch, TGFβ1 and podocyte apoptosis Stretch and TGFβ1 significantly increased markers of early apoptosis such as caspase-3 activity and annexin-V  Inhibition of β1 integrin results in an increase in podocyte apoptosis and a reduction in cell adhesion To further investigate a possible link between α3β1 integrin downregulation and apoptosis, we treated differentiated podocytes with a specific anti-β1 integrin antibody. Blocking of α3 integrin was not possible as a suitable antibody against mouse α3 integrin was not available. Blocking of β1 integrin resulted in a significant 42% reduction in podocyte adhesion (P = 0.001) (  Stretch-and TGFβ1-mediated reduction in integrin expression and cell adhesion are independent of activation of early cell apoptosis To examine whether stretch-and TGFβ1-mediated integrin downregulation resulting in reduction of podocyte adhesion would necessarily require cell apoptosis, experiments were conducted in the presence of an inhibitor of caspase-3 activity, a crucial enzyme in the cascade of molecular events leading to apoptosis. Under basal conditions, caspase inhibition reduced caspase-3 activity and annexin-V staining of ∼30% (P &lt; 0.001) but did not affect α3, β1 MF and β1 IF integrin expression or podocyte adhesion (  Stretch upregulates TGFβ type I, II and III receptor expression in podocytes To investigate the interaction between stretch and the TGFβ1 system in murine podocytes, cells were cultured in a complete medium and exposed to stretch for 48 h. TGFβ1 secretion and TGFβ type I, II and III receptor protein expression were studied. Stretch did not alter TGFβ1 secretion (control 10.21 ± 0.8, stretch 13.05 ± 1.7 pg/mL/10 6 cells) Under basal conditions, treatment with caspase activity inhibitor (Inh) resulted in about 30% reduction in caspase-3 activity and annexin-V staining (A) (Veh versus Inh, * * P &lt; 0.001), but had no effect on α3β1 integrin expression (B) or podocyte adhesion (C). Stretch-and TGFβ1-mediated increase in caspase-3 activity and annexin-V staining was inhibited by caspase inhibitor (D and E). Data are presented as percentage change from vehicle (Veh)-treated cells (control: C). C versus S or TGFβ1 without inhibitor ( * P &lt; 0.05). Each bar represents the mean ± SEM of multiple experiments performed in quadruplicate (n = 5-6). (  Discussion Podocytes, like other glomerular cells, are exposed to mechanical stretch and TGFβ1, and are vulnerable to injury. In conditions such as diabetes, podocyte loss (possibly secondary to reduction/impairment of adhesion to the GBM), resulting in podocytopaenia, has been implicated in the development of glomerular damage  Different studies have described podocyturia observed in proteinuric renal diseases as consisting of a mixed population of viable and apoptotic podocytes  The notion that α3β1 integrin downregulation is causally related to the loss of podocyte adhesion was supported by the finding that blockade of β1 integrin by a neutralizing antibody resulted in a significant reduction in podocyte adhesion to ECM. Our results are consistent with a number of previous observations suggesting a link between integrin expression levels and adhesion in podocytes  The mechanisms of α3β1 integrin downregulation by mechanical stretch and TGFβ1 appear to differ, and posttranscriptional mechanismsare implicated for both mechanical stretch and TGFβ1. Mechanical stretch was paralleled by a downregulation of the core peptides of both α3 and β1 integrin but had no effect on the glycosylation (maturation) of β1 integrin. Like mechanical stretch, TGFβ1 downregulated the α3  integrin core peptide but, in contrast, had no effect on the expression of the β1 integrin core peptide. TGFβ1 altered the ratio of fully to partially glycosylated β1 integrin isoforms in favour of the non-functional β1 IF isoform, suggesting an important role for TGFβ1 in β1 integrin glycosylation and maturation. This observation is consistent with the report of Bellis, who observed that TGFβ1 affects β1 integrin glycosylation in colon epithelial cells  The mechanisms by which TGFβ1 downregulates the β1 MF integrin isoform have not been fully elucidated in the present study. Previous work has suggested that the intracellular pools of immature α3 subunits are very small when compared to immature β1  A connection between α3β1 integrin and cell apoptosis is supported by the recent work of Chen and colleagues who showed that the use of antagonists against α3β1 integrin resulted in reduced podocyte adhesion and increased markers for apoptosis  Our study, while confirming that both stretch and TGFβ1 activate early markers of apoptosis in adherent podocytes in vitro  sufficient to induce reduced cell adhesion. Activation of early apoptosis is not necessarily required for the loss of podocyte adhesion, but reduced adhesion-induced apoptosis may play a crucial role on the fate of the detaching podocyte. This is supported by the fact that the presence of early (annexin-V, caspase-3) apoptotic markers was not paralleled by late apoptotic ones (DNA fragmentation), at least in still adherent podocytes (with reduced α3β1 integrin levels) studied after exposure to mechanical stretch or TGFβ1. Although apoptosis or &apos;cell death&apos; per se would result in a cell being unable to adhere, podocytes characterized by a reduction in anchoring receptors appear to manifest only signs of early apoptosis  One limitation of this study is that we did not observe sufficient detachment of podocytes exposed to stretch and TGFβ1 to study integrin expression and apoptosis in detached, floating cells. It is of interest to note that we observed DNA laddering (an index of late apoptosis) in the detached/floating cells of experiments where 80% and 50% confluent podocytes were exposed to mechanical stretch, as mentioned in the &apos;Materials and methods&apos; section. Future studies will have to address this issue, and possibly an in vivo system would be an ideal setting to answer this specific question. We further investigated the interaction between stretch and the TGFβ1/TGFβ-receptor signalling cascade and found that, similar to angiotensin-2 [56], stretch failed to increase TGFβ1 protein production in podocytes. However, under other conditions such as high glucose in podocytes  In conclusion, alterations in podocyte molecular anchoring mechanisms induced by mechanical forces and TGFβ1 are sufficient in mediating the reduction of podocyte adhesion to extracellular matrix substrates; parallel activation of apoptosis may represent an important determinant of the podocyte loss from the GBM. Our findings contribute to the elucidation of the mechanisms of the urinary podocyte loss observed in diabetes and other glomerular diseases.  Abstract Background. Increased transforming growth factor beta-1 (TGF beta) expression in the kidney is central not only to the pathogenesis of tubulointerstitial fibrosis but also in repair following acute injury. Recent work suggests that pro-inflammatory cytokines may determine epithelial cell responses to TGF beta in the contexts of acute injury and chronic wounding. Methods. In this study, we examined the effects of interleukin-1 beta (IL-1) on proximal tubular cell (PTC) response to TGF beta. Results. IL-1 induced the rapid activation of NF-κB in PTC. This was associated with inhibition of Smad2 and Smad3 signalling. NF-κB activation by IL-1 was transient, with a change from p65/p50 heterodimer to p50/p50 homodimer formation by 24 h and a switch to enhanced Smad signalling response to TGF beta. This was associated with IL-6 generation and prevented by IL-6 receptor blockade. Conclusion. In summary, IL-1 has a biphasic effect on PTC TGF beta signalling, with early NF-κB-mediated inhibition and delayed sensitization via an autocrine IL-6 loop. These results provide mechanistic insight into how C Th",exnephro.dvi,,,,,core
20901827,04/12/2008,"Abstract — Traffic congestion is one of the leading causes of lost productivity and decreased standard of living in urban settings. Recent advances in artificial intelligence suggest vehicle navigation by autonomous agents will be possible in the near future. In a previous paper, we proposed a reservation-based system for alleviating traffic congestion, specifically at intersections. This paper extends our prototype implementation in several ways with the aim of making it more implementable in the real world. In particular, we 1) add the ability of vehicles to turn, 2) enable them to accelerate while in the intersection, 3) give a better sensor model and communication-efficient heuristic to our driver agent, and 4) augment their interaction capabilities with a detailed protocol such that the vehicles do not need to know anything about the intersection control policy. The use of this protocol limits the interaction of the driver agent and the intersection manager to the extent that it is a reasonable approximation of reliable wireless communication. We then use this protocol to implement a new control policy: the stop sign. All three improvements are fully implemented and tested, and we present detailed empirical results validating their effectiveness. I",Turning the Corner: Improved Intersection Control for Autonomous Vehicles,,,,,core
2776809,2004-01-01T00:00:00,"In this paper, the results obtained by inter-comparing several statistical techniques for modelling SO2 concentration at a point such as neural networks, fuzzy logic, generalised additive techniques and other recently proposed statistical approaches are reported. The results of the inter-comparison are the fruits of collaboration between some of the partners of the APPETISE project funded under the Framework V Information Societies and Technologies (IST) programme. Two different cases for study were selected: the Siracusa industrial area, in Italy, where the pollution is dominated by industrial emissions and the Belfast urban area, in the UK, where domestic heating makes an important contribution. The different kinds of pollution (industrial/urban) and different locations of the areas considered make the results more general and interesting. In order to make the inter-comparison more objective, all the modellers considered the same datasets. Missing data in the original time series was filled by using appropriate techniques. The inter-comparison work was carried out on a rigorous basis according to the performance indices recommended by the European Topic Centre on Air and Climate Change (ETC/ACC). The targets for the implemented prediction models were defined according to the EC normative relating to limit values for sulphur dioxide. According to this normative, three different kinds of targets were considered namely daily mean values, daily maximum values and hourly mean values. The inter-compared models were tested on real cases of poor air quality. In the paper, the inter-compared techniques are ranked in terms of their capability to predict critical episodes. A ranking in terms of their predictability of the three different targets considered is also proposed. Several key issues are illustrated and discussed such as the role of input variable selection, the use of meteorological data, and the use of interpolated time series. Moreover, a novel approach referred to as the technique of balancing the training pattern set, which was successfully applied to improve the capability of ANN models to predict exceedences is introduced. The results show that there is no single modelling approach, which generates optimum results in terms of the full range of performance indices considered. In view of the implementation of a warning system for air quality control, approaches that are able to work better in the prediction of critical episodes must be preferred. Therefore, the artificial neural network prediction models can be recommended for this purpose. The best forecasts were achieved for daily averages of SO2 while daily maximum and hourly mean values are difficult to predict with acceptable accuracy",Modelling SO2 concentration at a point with statistical approaches,,'Elsevier BV',10.1016/j.envsoft.2003.10.003,,core
214599691,2011-04-07T00:00:00,"The work reported here has been motivated by the need for a generic spatial model to overcome the limitations of Cellular Automata (CA) regarding the rigid square-cell structure and limited neighbourhood configurations. A novel approach for spatial modelling technique is developed: the “vector-agent” in which the individual entity is represented by their real geometric boundaries (which can change over time) beneath an agent modelling structure. We show in this paper how the theory behind CA and agents can be combined to produce a generic and dynamic agent based on the vector data structure. This new paradigm has extended capabilities over the Geographic Automata (Torrens and Benenson, 2005) in terms of CA disunity and the abstraction of non-fixed-objects. Through computer simulation, different techniques and algorithms have been derived achieving a high degree of representational realism for a variety of phenomenaPublishedNon Peer ReviewedBarros, J. (2003) Simulating Urban Dynamics in Latin American Cities, Proceedings of the 7th International
Conference on Geocomputation, University of Southampton, UK.
Batty, M. (2000) Geocomputation Using Cellular Automata, In Openshaw, I. S. and Abrahart, R. J. (Eds.)
Geocomputation, Taylor & Francis, London, UK, pp. 95-126.
Batty, M. (2001) Cellular Dynamics: Modelling Urban Growth as a Spatial Epidemic, In Fischer, I. M. M. and
Leung, Y. (Eds.) Geocomputational Modelling: Techniques and Applications, Advances in Spatial Science,
Springer, Berlin, pp. 109-141.
Batty, M., Desyllas, J. and Duxbury, E. (2003) The Discrete Dynamics of Small-Scale Spatial Events: Agent-
Based Models of Mobility in Carnivals and Street Parades, International Journal of Geographic Information
Science, 17:7, pp. 673-697.
Batty, M. and Longley, P. (1994) Fractal Cities, Academic press, London.
Benenson, I. and Torrens, P. (2004a) Geosimulation Object-Based Modelling of Urban Phenomena, Editorial,
Computers, Environment and Urban Systems, 28, pp. 1-8.
Benenson, I. and Torrens, P. M. (2004b) Geosimulation: Automata-Based Modelling of Urban Phenomena,
Wiley, England.
Egenhofer, M. J. and Franzosa, R. D. (1991) Point-Set Toplogical Spatial Relations, International Journal of
Geographic Information Science, 5:2, pp. 161-174.
Egenhofer, M. J. and Franzosa, R. D. (1994) On the Equivalence of Topological Relations, International Journal
of Geographic Information Science, 8:6, pp. 133-152.
Goodchild, M. F. and Mark, D. M. (1987) The Fractal Nature of Geographic Phenomena, Annals of the
Association of American Geographers, 77, pp. 265-278.
Haklay, M., O'Sullivan, D. and Thurstain-Goodwin, M. (2001) So Go Downtown: Simulating Pedestrian
Movement in Town Centres, Environment and Planning B: Planning and Design, 28, pp. 343-359.
Hammam, Y., Moore, A., Whigham, P. and Freeman, C. (2004) Irregular Vector-Agent Based Simulation for
Land-Use Modelling, Proceedings of the 16th Annual Colloquium of the Spatial Information Research Centre,
University of Otago, Dunedin, New Zealand.
Hayes-Roth, B. (1995) An Architecture for Adaptive Intelligent Systems, Artificial Intelligence, 72, pp. 329-365.
Kenkel, N. C. and Walker, D. J. (1996) Fractals in the Biological Sciences, COENOSES, 11, pp. 77-100.
Laurini, R. and Thompson, D. (1992) Fundamentals of Spatial Information Systems, Academic Press, London.
Liu, Y. (2001) Modelling Urban Development with Geographical Information Systems and Cellular Automata:
A Case Study of Sydney since 1971, Unpblished PhD Thesis, University of Queensland, Australia.
Luck, M., D'inverno, M. and Munroe, S. (2003) Autonomy: Variable and Generative, In Hexmoor, H.,
Castelfranchi, C. and Falcone, R. (Eds.) Agent Autonomy, Kluwer Academic Publishers, USA.
O'Sullivan, D. (2000) Graph-Based Cellular Automata Models of Urban Spatial Processes, Unpublished PhD
Thesis, Barlett School of Architecture and Planning, University College London, University of London, London.
O'Sullivan, D. (2001) Graph-Cellular Automata: A Generalised Discrete Urban and Regional Model,
Environment and Planning B: Planning and Design, 28, pp. 687-707.
Peitgen, H., Jurgens, H. and Saupe, D. (1992) Chaos and Fractals: New Frontiers of Science, Springer-Verlag,
New York.
Rasmussen, S. and Barrett, C. L. (1995) Elements of a Theory of Simulation, Proceedings of ECAL'95, Lecture
Notes in Computer Science, Springer-Verlag, Berlin.
Rodrigues, A., Grueau, C., Raper, J. and Neves, N. (1998) Environmental Planning Using Spatial Agents, In
Carver, S. (Ed.) Innovations in GIS 5, Selected Papers from the 5th National Conference on GIS Research UK
(GISUK), Taylor & Francis Ltd., London.
Rodrigues, M. A. S. (1999) The Development of Spatial Intelligent Agents with Geographic Information
Systems, Unpublished PhD Thesis, City University, UK.
Rucc, J. C. (1994) Fractal Surfaces, Plenum Press, New York.
Russell, S. and Norvig, P. (1995) Artificial Intelligence: A Modern Approach, Prentice Hall International Inc.,
NJ, USA.
Russell, S. and Norvig, P. (2003) Artificial Intelligence: A Modern Approach, Second Edition, Prentice Hall
Series in Artificial Intelligence, New Jersey, USA.
Semboloni, F. (2000) The Growth of an Urban Cluster into a Dynamic Self-Modifying Spatial Pattern,
Environment and Planning B: Planning and Design, 27, pp. 549-564.
Shi, W. and Pang, M. Y. C. (2000) Development of Voronoi-Based Cellular Automata: An Integrated Dynamic
Model for Geographical Information Systems, International Journal of Geographical Information Science, 14:5,
pp. 455-474.
Torrens, P. and O'Sullivan, D. (2000a) Cities, Cells, and Complexity: Developing a Research Agenda for Urban
Geocomputation, Proceedings of the 5th International Conference on Geocomputation, University of
Greenwich, London.
Torrens, P. M. and Benenson, I. (2005) Geographic Automata Systems, International Journal of Geographic
Information Sciences, 10:4, pp. 385-412.
Torrens, P. M. and O'Sullivan, D. (2000b) Cellular Models of Urban Systems, The Centre for Advanced Spatial
Analysis (CASA), University College London, UK., Paper No. 22.
Voss, R. F. (1988) Fractals in Nature: From Characterization to Simulation, In Peitgen, H. O. and Saupe, D.
(Eds.) The Science of Fractal Images, Springer-Verlag, New York.
White, R. and Engelen, G. (2000) High Resolution Integrated Modelling of the Spatial Dynamics of Urban and
Regional Systems, Computers, Environment and Urban Systems, 24, pp. 383-400.
Wooldridge, M. (1997) Agent-Based Software Engineering, Proceedings on Software Engineering, 144:1, pp.
26-37.
Worboys, M. and Duckham, M. (2004) GIS: A Computing Perspective, CRC Press New York",Generic vector-agents,,,,,core
197995737,2005,"In this thesis, downhill driving strategies that co–ordinate the different brake actuators in heavy duty vehicles have been developed. As a starting point, the vehicle features affected by the retardation system are investigated. The main conclusions are that the retardation power demand will increase in the future and that, therefore, optimization of the brake systems will come to play a major role. In particular, strategies for the integration of foundation brakes, auxiliary brakes, and gear box have to be developed. Furthermore, these strategies must take component wear cost into consideration. Additionally, a thorough description of the current situation in terms of driver behaviour and existing systems for driver assistance is given. Optimal control and nonlinear programming have been used for the generation of open–loop optimal driving strategies. Two different methods have been employed for the generation of implementable, closed–loop, driving strategies. First, a method that utilizes neural networks and genetic algorithms is presented. Second, in order to further enhance the controller transparency, and the possibility for robust implementation, the control problem is divided into two differentmodes of operation. Linear quadratic control using gain scheduling is then utilized for the controller design and generation of actuator reference values. Comparison with the open–loop optimal strategies is also made. It is shown that transport efficiency (i.e. mean speed) and retardation economy (i.e. component wear cost) can be improved significantly, even compared to what skilled drivers can achieve, by integrating the whole retardation system. It is furthermore shown that there is a trade–off between component wear cost and transport efficiency that must be balanced in order to achieve good brake performance. The main parameters that affect the longitudinal control of the vehicle are the level of vehicle utilization (mass) and road slope. Algorithms for estimation of vehicle mass and road slope are therefore developed and presented. Additionally, a downhill driving strategy is implemented and verified in a real truck",Integrated brake control : downhill driving strategies,,,,,core
20967572,2006,"Traffic congestion and automobile accidents are two of the leading causes of decreased standard of living and lost productivity in urban settings. Recent advances in artificial intelligence and, specifically, intelligent vehicle technology suggest that vehicles driven entirely by autonomous agents will be possible in the near future. In previous work, we presented a novel reservation-based approach for governing interactions of multiple autonomous vehicles, specifically at intersections. This approach alleviated many traditional problems associated with intersections, in terms of both safety and efficiency. However, such a system relies on all vehicles being equipped with the requisite technology — a restriction that would make implementing such a system in the real world extremely difficult. In this paper, we augment the system such that it is able to accomodate traditional human-operated vehicles using existing infrastructure. Furthermore, we show that as the number of autonomous vehicles on the road increases, traffic delays decrease monotonically toward the levels exhibited in the system involving only autonomous vehicles. Additionally, we demonstrate how the system can be extended to allow high-priority vehicles such as ambulances, police cars, or fire trucks through more quickly without placing undue burden on other vehicles. Both augmentations are fully implemented and tested in our custom simulator, and we present detailed experimental results attesting to their effectiveness",Human-Usable and Emergency Vehicle-Aware Control Policies for Autonomous Intersection Management,,,,,core
21138457,2008,"This paper investigates an application of mobile sensing: detecting and reporting the surface conditions of roads. We describe a system and associated algorithms to monitor this important civil infrastructure using a collection of sensor-equipped vehicles. This system, which we call the Pothole Patrol (P 2), uses the inherent mobility of the participating vehicles, opportunistically gathering data from vibration and GPS sensors, and processing the data to assess road surface conditions. We have deployed P 2 on 7 taxis running in the Boston area. Using a simple machine-learning approach, we show that we are able to identify potholes and other severe road surface anomalies from accelerometer data. Via careful selection of training data and signal features, we have been able to build a detector that misidentifies good road segments as having potholes less than 0.2 % of the time. We evaluate our system on data from thousands of kilometers of taxi drives, and show that it can successfully detect a number of real potholes in and around the Boston area. After clustering to further reduce spurious detections, manual inspection of reported potholes shows that over 90 % contain road anomalies in need of repair",The pothole patrol: Using a mobile sensor network for road surface monitoring,,,,,core
23897894,2008,"This paper investigates an application of mobile sensing: detecting and reporting the surface conditions of roads. We describe a system and associated algorithms to monitor this important civil infrastructure using a collection of sensor-equipped vehicles. This system, which we call the Pothole Patrol (P 2), uses the inherent mobility of the participating vehicles, opportunistically gathering data from vibration and GPS sensors, and processing the data to assess road surface conditions. We have deployed P 2 on 7 taxis running in the Boston area. Using a simple machine-learning approach, we show that we are able to identify potholes and other severe road surface anomalies from accelerometer data. Via careful selection of training data and signal features, we have been able to build a detector that misidentifies good road segments as having potholes less than 0.2 % of the time. We evaluate our system on data from thousands of kilometers of taxi drives, and show that it can successfully detect a number of real potholes in and around the Boston area. After clustering to further reduce spurious detections, manual inspection of reported potholes shows that over 90 % contain road anomalies in need of repair",The pothole patrol: using a mobile sensor network for road surface monitoring,,,,,core
162408305,2007-01-01T00:00:00,"This paper focuses on the results of different consumer surveys conducted between 2004 and 2006 with regard to consumers ' perceptions and reactions concerning AI in Vietnam, (mainly in Hanoi). The main results observed are as follows: - A high proportion of consumers consider AI to be a food-related risk. However, over time, there has been a slight shift from a fear of consuming poultry to a fear of preparing it (slaughtering it). - AI has had a profound effect on poultry consumption, even outside peak crisis times, more in terms of the quantity consumed (approximately a third less in 2006) than in terms of the number of consumers (6% less). - Blood and internal organs are considered particularly risky, while eggs are viewed as being safer. Poultry from industrial farms is considered to be more risky than poultry from small farms. - Purchasing practices have also been affected by AI: in Hanoi, consumers declare that they prefer to buy poultry directly from producers that they know, or from supermarkets in the case of the wealthiest consumers. A high proportion still buy live poultry from market traders, but more consumers now ask sellers to slaughter it for them. With a view to lessening market shocks in the wake of the crisis while maintaining the priority of consumer safety, a number of measures should nevertheless be implemented: Risk communication should not over-emphasize AI as a food-related risk. - Reliable safe distribution channels should be promoted (with reliable quality signs and controls) in order to encourage safe production and poultry consumption. Otherwise, a market recovery will only benefit supermarkets and large-scale farmers capable of supplying supermarkets. - As numerous live birds are still slaughtered in urban market places, facilities should be provided for safe slaughter. At the same time, more attention should be paid to the provision of a real ""cold chain"" with a view to promoting the sale of slaughtered poultry. (Résumé d'auteur",Consumer perceptions and reactions concerning AI (Avian Influenza),http://agritrop.cirad.fr/539844/1/document_539844.pdf,'Food and Agriculture Organization of the United Nations (FAO)',,,core
326405776,2007-03-08T00:00:00,"International audienceThis paper focuses on the results of different consumer surveys conducted between 2004 and 2006 with regard to consumers’ perceptions and reactions concerning AI in Vietnam, (mainly in Hanoi). The main results observed are as follows: A high proportion of consumers consider AI to be a food-related risk. However, over time,there has been a slight shift from a fear of consuming poultry to a fear of preparing it (slaughtering it). AI has had a profound effect on poultry consumption, even outside peak crisis times, more in terms of the quantity consumed (approximately a third less in 2006) than in terms of the number of consumers (6% less). Blood and internal organs are considered particularly risky, while eggs are viewed as being safer. Poultry from industrial farms is considered to be more risky than poultry from small farms. Purchasing practices have also been affected by AI: in Hanoi, consumers declare that they prefer to buy poultry directly from producers that they know, or from supermarkets in the case of the wealthiest consumers. A high proportion still buy live poultry from market traders, but more consumers now ask sellers to slaughter it for them. With a view to lessening market shocks in the wake of the crisis while maintaining the priority of consumer safety, a number of measures should nevertheless be implemented: Risk communication should not over-emphasize AI as a food-related risk. Reliable safe distribution channels should be promoted (with reliable quality signs and controls) in order to encourage safe production and poultry consumption. Otherwise, a market recovery will only benefit supermarkets and large-scale farmers capable of supplying supermarkets. As numerous live birds are still slaughtered in urban market places, facilities should be provided for safe slaughter. At the same time, more attention should be paid to the provision of a real “cold chain” with a view to promoting the sale of slaughtered poultry",La perception et les réactions du consommateur à propos de la grippe aviaire,,HAL CCSD,,,core
235574532,2000-01-12T00:00:00,"An archive of the Magrath Trading Store News.The University of Lethbridge Library received permission from the Wes Balderson to digitize and display this content.MAGRATH
NEWS
Published Weekly since 1932 by
The Magrath Trading Company
35 cents
Wednesday January 12, 2000
A very significant event took place in
Geneva Switzerland this past November.
The World Congress of Families 11 was
held with representatives from around
the world, including 15 people from
Southern Alberta.
The purposes of this Congress were to:
1. rally organizations and leaders to
protect and fortify the natural family;
2. develop guidelines for the
formulation and implementation of
family-centered policies and laws;
3. raise worldwide awareness of the
objectives and results of the Congress;
4. create ongoing structures for mutual
cooperation and support
The Honorable Grant Hill, MP, was
one of the leaders who attended the
Congress. He has graciously accepted an
invitation to speak to our Magrath
Community about his experience in
Geneva.
This meeting will be held
Feb. 2 at 7:00 p.m.
in the Magrath School
(Band Room)
All are invited to attend.
BAND CALENDARS
""Spirit of Alberta Birthday Calendars""
There are a few of you
who haven't picked your's up yet!!
You've already paid for your calendar,
so why not pick it up!!
Doing your groceries!
Shopping at Home Hardware!!
Come to the Trading Company Office
to get yours now!!
Magrath Museum
Annual General Meeting
will be held
January 27, 2000
at 7 p.m.
in the lunch room
of the library museum building.
Anyone
who wishes to join
the Museum Association
is welcome!
No experience necessary!
We need You!
Call Donna Lybbert at 758-3896
or
Andrea Burrows at 758-3843.
&&A
New Books
Adult Non-Fiction
The Kid's Guide to Service Projects by
Barbara A. Lewis; Skin: An Owner's Manual
by Robert Buckman, M.D.;
Making Change Irresistible by Ken Hultman
Adult Fiction
The Last Dragon Lord by Joanne Berlin;
So Speaks the Heart by Johanna Lindsey;
Beyond Recall by Robert Goddard.
Juvenile
Creepy Riddles by Katy Hall, Peace Tales:
World Folktales to Talk About by Margaret
MacDonald, Now You See It - Now You Don't:
Optical Elusions by Seymour Simon; Karen's
Mistake by Ann Martin.
Children's
Cocoa Ice by Diana Appelbaum, Tanya and the
Magic Wardrobe by Patricia Lee Gauch; Here
In Space by David Milgrim.
******
The Magrath Museum wishes to thank the
community for their donations of artifacts,
photos and financial assistance during the
Centennial Yeai.
Keep your donations coming, our museum is
growing and improving each year.
Donors who wish to contribute S20.00 or more
can be issued a tax deductible receipt.
******
BINGO - BINGO -BINGO
Next Bingo is Thursday nite
January 20, 2000
at the Seniors' Centre.
Doors open 6:30 p.m.
Bingo @ 7 p.m.
Everyone Welcome.
******
General & Sub
Contracting
For all your Concrete
& Stucco needs
Garden City Gymnastic
classes and registration
begin January 18th
4:30 p.m.
in the small gym.
To begin with
age groups and hours
remain the same as last term.
******
MAGRATH HOSPITAL
The Magrath Hospital Committee held a
meeting December 15th to discuss the status of
the Magrath Hospital and what actions should
be taken in order to keep it functioning. At
this time the committee feels that the best
thing that we as residents of the hospital area
can do is to write letters stating the need for
medical and emergency care in our area. The
committee is scheduled to make a presentation
to the CHR Board on January 27, 2000.
Letters should be sent prior to that meeting to
the following:
The Honorable Halvar Jonson, MLA
Minister of Health
228, Legislature Bldg
10800 - 97 Avenue
Edmonton, AB T5K 2B6
Ron Hierath, MLA
503, Legislature Bldg
10800 - 97 Avenue
Edmonton, AB T5K 2B6
Mr. Frank Eden
Chairman of the Board
Chinook Health Region
960, 19th Street South
Lethbridge, AB T1J 1W5
Curt’s Construction
P.O. Box 535
Magrath, Alberta
TOK 1J0
CURTIS HATCH
Home: (403) 758-3759
Cell: (403) 308-4585
Before laundering, pretreat stains on clothing
with a mixture of three parts baking soda to
two parts white vinegar.
Mr. Garth Coleman passed away on Sunday,
January 2, 2000 at the age of 86 years.
Besides his wife Avilda, he is survived by his
children: Ross (Irene) of Preston, Idaho; Peggy
Ann (Blair) Meldrum of Christiansburg,
Virginia, Paul (Gwen) Coleman of Magrath
and step-children: Alan (Marie) Harker; Diena
(Randall) Meeks, Elaine (Grant) Fisher, Brent
(Sheila) Harker and Kevin Harker. He is also
survived by brother: Reed (Myrna) of
Magrath, sisters-in-law: Desmonia (Warren)
Harris of Magrath & Joanne Coleman of
Chilliwack, BC.
He was predeceased by his first wife, Adele;
daughter Brenda, sister Margaret Clifton and
brother Meade.
Funeral services were held on Thursday,
January 6, 2000 with interment in the Magrath
Cemetery.
Mrs. Catherine ""Katie"" Fisher passed away
Sunday January 2, 2000 at the age of 86 years.
Katie is survived by one daughter: Rose (Don)
Dunkel of Michigan; two sisters: Lorna Mosher
of Burnaby B. C., Elsa Kaytor, of Vancouver,
B.C. & one brother Steve Butte of Hixon, B.C.
Katie was predeceased by a brother Joe
Mirkovich and best friend and companion of
35 years Mike Parczen.
A Funeral Mass was held Friday, January 8,
2000, with interment in the Magrath Cemetery.
Mrs. Zelma Mary Strong, passed away on
Sunday, January 2, 2000 at the age of 81 years.
She is survived by two sons and one daughter­in-
law: Randall Strong of Lethbridge, Kendall
(Gail ""Perky"") of Calgary.
She was predeceased by her husband, her
parents: George & Leona Loxton and 2
brothers: George and Delby.
Funeral Services were held on Thursday,
January 6, 2000. Interment in the Magrath
Cemetery.
He who teaches his child to be thrifty &
economical has already bequeathed him a
fortune.
As the family of Garth Coleman we would
like to take this opportunity to thank all those
who have helped ease the sorrow at his passing.
We appreciate the many acts of kindness that
were performed on our behalf. A special thank
you to all the staff of the Diamond Willow
Terrace for taking such good care of our Dad
& helping make it his home. We would also
like to thank the medical staff of the Magrath
Hospital & ambulance for their response to his
medical needs.
Sincerely, The Garth Coleman Family.
Jessica (Reeder) and Jay Salmon
would like to invite you
to their wedding reception,
Saturday evening January 15th, 2000
at the Garden Place Chapel
(White Church)
All welcome
On behalf of myself and my family, I want to
thank the anonymous giver of their generous
gift. The timing was perfect.
With much appreciation. ""D""
Mrs. Fern Cook, long time resident of
Magrath, is now residing in Cardston at the
home of her daughter, Donna & Art Heninger.
Guests are welcome to drop in at 265 - 3rd
Avenue West. Telephone calls will be gladly
received at 653-1512.
Happy New Year to all of the wonderful people
in Magrath from Fern.
Thank you so much
Sincerely, Fern's Family.
ANGE-EMILE LABBE
•Drywall 'Boarding • Taping
• Texture Wall & Ceiling
• Small Renovations
• Commercial & Residential
BOX 803
MAGRATH, AB TOK 1 JO-PHONE
(403) 758-6876
Magrath Pee Wee Chiefs Hockey Club News Release Chiefs split weekend games
Friday night had the Pee Wee Chiefs home to the Lethbridge Warriors. A large Lethbridge fan contingent went away disappointed with a 7-2 win for the Chiefs. The win was helped by a three goal performance from L. Hofer. S. Johnson scored twice and added an assist, while S.Ragan and A. Wamboldt finished the scoring with singles. R. Yoshihara moved the puck around well, contributing 3 assists and J. Rudd managed 1 assist before leaving the game in the first period.
Saturday night the Chiefs met the Raymond Ice in Raymond. The wheels fell off early when the chiefs managed to give the Ice a 2 goal lead, putting the puck into our own net twice. The final score of Raymond 10 Magrath 6 leaves little to the imagination about how the rest of the game went. The only bright spot was a rowdy fan celebration for S. Nishikawa's hat trick. S. Johnson came up with two goals while L. Hofer dished up one. J. Rudd assisted 3 times while A. Wamboldt and S. Ragan also added assists.
The Chiefs play in Picture Butte Friday and are back home on Saturday night, Jan. 15 hosting Raymond at 7:45 p.m.
We would like to thank all the people in Magrath & area who supported the Magrath & District Minor Hockey Pasta Fundraiser. It was a great success! (We sold 374 boxes, a Lethbridge team recently sold 200.)
Special thank-you to Leonard Bourne for helping out and to Allen Clarke from ""Greg's Transport"" for giving Oodles of Noodles a reduction in the shipping costs. Magrath & District Minor Hockey Assoc. Sharon Ragan (Pasta Coordinator)
If You Ask Me.....
""It's hard to detect good luck - it looks so much like something you've earned."" -FredA. Clark
Friday in the Southern Alberta Girls Basketball Magrath 50 - Kainai 41.
Kelsey Helgeson 12 pts, Marcie Johnson 10 pts. J.V action Magrath 47 - Kainai 33.
Ginger Holladay 13 pts. Saturday in Boys Basketball Magrath 70 Medicine Hat 51. Jimmy Balderson 25 pts, Jared Kilkenny 14 pts. Darren Bevers 12 pts. Dylan Alston 11 pts.
TOWN OF MAGRATH PUBLIC NOTICE
1.
Business Licenses, as required by By-Law #897, are being issued at the Town Office for the 2000 business year.
2.
Dog Licenses are due to be renewed in January. Cost = $10 not neutered or spayed; $5 with proof of procedure being done.
3.
2000 Residential Water Charge = $27/mo. Water prepayment plan with the applicable one month discount is available to Town residential properties during the month of JANUARY ONLY. The 1999 water account must be paid in full before one can participate in the prepayment plan.
4.
Tax Prepayment Plan is available to property owners with taxes in a CURRENT STATUS. This plan is available only until January 31, 2000,
★“Young man,"" said the angry father from the top of the stairs, “didn't I hear the clock strike three when you brought my daughter home?” '
“Indeed you did, sir,” re­plied the boyfriend. “It was going to strike eleven, but I grabbed the pendulum and held it so it would not disturb you.”
As the father walked back to his bedroom, he was heard to mutter, “Now, why didn’t I think of that one in my courting days?”Magrath Rod & Gun
Measuring night will be held
January 21 from 6:00 p.m. - 9:00 p.m.
at the Club House.
All entries must be clean and tagged
with owners name.
New memberships now available from
John in Trading Company Hardware.
A A A A A A
How safe a driver are you? Try answering the
following questions .
1) What four things should you do if the brakes
in your car fail suddenly while you are driving?
2) What are the different techniques used when
parking a car uphill on a street with a curb or
on one without? .
3) How many drinks can a driver consume in
one hour before he starts taking risks?
Here are the answers:
1) Pump the brake pedal up and down; apply
parking brake while holding the release lever in
""off"" position. Shift to lowest gear. Steer car
off road and rub wheels along curb or bushes.
Turn off the ignition.
2) Front wheels should be turned left and
resting against the cur. Turn wheels right when
there is no cur.
3) The average driver will start to take risks
after just one or two drinks.
-How many make a dozen?
Twelve.
And how many make a million?
Very few!
PASSEY ELECTRIC
New Home Wiring
Renovations
Farm Wiring
Feedlois
7/ p
(4031752-4005
1999 Tree of Hope
Magrath Hospital Auxilians wish to
thank all the kind people, clubs,
businesses, and organizations of Magrath
and areas for their generous monetary
donations to our Tree of Hope Fund
Raising Campaign held in the months of
November and December 1999 and
finishing in January 2000. Because of
your great help, we have raised over
$5,000.00 to be able to purchase 2
electric beds for out residents at our
Extended Care Facility, for their
everyday comfort & enjoyment.
A very special Thank you goes out to the
48 volunteers, who so willingly donated
their time in this the very busy season,
who manned the table where the monies
were collected, without your assistance,
we would not have succeeded. Also,
many thanks to the Magrath Trading
Co. Hardware Dept, for letting us use
the space and many many thanks to
John Bourne for his great help as
always. And thank you, Sharon Owens,
for making the wonderful posters. Last
but not least, thank you Grace Toomer,
for your dedication to our Auxiliary, all
the hours you spent on telephoning,
making sure that everything went well,
and it did.
Again, A Very Big Thank You To All.
Magrath Hospital Auxiliary. 1-19.
AAA**A
-Wife: ""All men are fools!""
Husband: ""Of course we are, dear, we're made that way so
that women don't have to be old maids.""
Little Brad fell and cut his knee; When the
wound was cleaned & bandaged, his mother
gave him a pill to soothe him. After he had
swallowed it Brad asked, ""How will the pill
know which leg to go down?""
CLASSIFIED ADS
DEADLINE: TUESDAY 12 NOON PHONE 758^63.77
Less than 30 words------------$3.00
Small ad (2.5""X 3.5"")-------$8.00
Subscription------------------------$15.00/year
1/4 page---------------------------$10.00
1/3 page---------------------------$13.50
1/2 page---------------------------$20.00
Full Page—Copy Ready---------$40.00
Full Page—We do-------------------$55.00
Flyer insertion (your paper)....$40.00
To give away: german shepherd pups, 3 males, 1 female. Call 758-6567. 1-18
******
Found:trailer license plate, monday on 2 St. East. Call 758-6564 to claim. 1-12.
******
For sale: Piano, older upright $500.00. Call Regehr's 758-3614. 1-12.
******
For s«le: 6 mos old N64 with pair real N64 controllers(not no name brand) red, yellow, grey, blue $165, excellent cond. Call Susan Loose at 758-6328. 1-12
******
For sale: 1 piano, & 1 queen size waterbed. 758-6220. 1-18.
******
For sale: Travel trailer tandem 21', 1971, renovations, sleeps 4, $2,400.00 tele: Lowell Kendrick 758-3620 Magrath to see. 1-19.
******
HIGHLAND DANCE LESSONS Now available - boys & girls ages 5 & up. Beginner & advanced levels.
Phone Kandi Russell @ 758-3900. 1-18.
******
Mother of teenager:
“Her idea of a clean room is one where you can find the phone by the fifth ring. '’
For sale: Tractor, Mitsubishi Bull Diesel 23 hp, 4 wheel drive, 9 sp trans, diff lock/rear wheels, 3 pt. hitch, front end loader, 5' Bush Hog rotary tiller, 6' blade for leveling or snow plowing, 4'8'' Haban 3 blade finish mower, 21"" stroke log splitter, tractor has 2,685 hrs. Price $13,000.00—includes the above implements. Tele: Lowell Kendrick 758-3620. Magrath to see. 1-19
******
Need your feed barley moved?
Will also haul hay or straw. Also can haul your quota for you. Contact Sabey Trucking @ 758-3119. or 308-1944 ******
Jayco Contracting Do you need a handy man for all your building needs? I do everything from concrete slabs to circular stairs. Call Tim Sabey @758-3327 or 308-1209 .1-26.
******
♦Prevent rust by keeping your car clean - inside, outside & underneath. Set a lawn sprinkler under the car & turn the water on full blast. This washes off the salt & other chemicals that collect on the car's underside during the winter.
For all your cleaning needs, from hospital clean to a touch up, carpet to ceiling & everything in between. No job to big or to small. Call Wayne's Carpet & Upholstery Cleaning @ 758-6414. 12-22 ******
JEANNIE'S HAIR FASHION 136S - 1 St. West 4 doors South of Trading Co. 758-3379 Open Tues thru Friday Professional Haircare at pleasing prices ******
RICK'S PORTABLE WELDING Owner Rick Beres We weld everything from chairs to tractors Please call me for all your welding needs ""B"" pressure qualified. 758-6427
******
There’s one good thing about snow—it makes your lawn look as nice as your neighbor’s.
—Clyde MooreCANADIAN SECURITY SYSTEMS We sell, install, and service alarms, camera systems, safes.
*Also dead bolts and key locks Call Ross Moore 758-3945 for a free estimate.
******
Remember to Call Tai heating, air conditioning, refrigeration and appliance service.
Gas & Electrical Service and Trenching 752-3866. Tai Hancock mgr.
******
For rent: various spaces, in Magrath, ideal for small business. Call 758-3876.
******
For rent: main floor house, 3 bedrooms. Avail immediately 2000.
135 East-1 Ave. N. Magrath. $500 + 1/2 utilities, $500 d.d., abstainers, no pets.
Call Harold Murray 758-3325.
******
For rent: small 2 bedroom apart, avail, immediately. Call 758-3781.
******
For rent: one bedroom self­contained upstairs apart. Many renovations. $275/mo. plus utilities. Damage deposit of $275.00 required.
Phone 758-3409. 1-18.
******
Karen Filmore residence is for sale by owner 3 bedroom bungalow attached double garage, family room has wood floors & fireplace.
New lino, carpet
& paint upstairs.
Games room, 2 bedrms & large cold storage down.
$103,000.00 lowered to $98,500.00 contact: Kathy Dahl @ 758-3039 or Irene MacDonald @ 1(403) 257-4477. l.r.
******
House for sale 1550 sq. ft., 1/2 yr. old bung. Completely finished up & down, total of 6 bedrooms, 3 full baths, Irg f.r., main floor laundry, built in vacuum & appliances. Located on corner lot, 1 block East of school @ 180E Harkder Ave.
400 sq ft.shop is separate from home. Call Scott or Falene @ 758-3992.
Fast food for birds
Get some 100% cotton thread, a needles, popped corn & whole cranberries. Cut a 1 m(39 3/8"") length of thread. Tie a large know in one end; thread the other end through the needles. String the corn & cranberries onto thread & knot remaining end. Then hang the finished garland on a tree.
House For Sale by Owner 1456 sq. ft., open greatroom concept, sky lights, fireplace, main floor laundry, master bedroom has patio doors to deck & large ensuite with jacuzzi tub, walk out basement, appliances & draperies inc.
On 1 acre+ in Magrath, ideal for horse. 758-6805. 1-19.
******
*Fast food for the birds Pinecones, peanut butter, wild bird seed, waxed paper and some string or yarn are all you need. Lay two pieces of waxed paper on work surface. In the centre of one, spoon out a small amount of peanut butter; on the other, sprinkle bird seed. Roll each pinecone in peanut butter until it's coated, then in seeds. Spoon extra seeds into spaces. Tie one yarn end around tip of cone. Tie other yam end around a tree branch just outside your window.
Hot drinks can leave you cold. It's true they usually boost your body temperature a bit. but if you drink too many, hot beverages dilate blood vessels in the skin, which actually causes a slight heat loss.MEAT SPECIALS JANUARY 10-15
Fresh Whole Frying Chicken $3.38/kg $1.58/lb
Hot or Mild Italian Sausage $4.37/kg $1.98/lb
Marinated, Boneless Skinless Chicken Breast $8.77/kg $3.98/lb
Fletcher’s Sliced Bacon 500g $3.48
Fletcher's Wieners 375-450g $2.28
Fletcher's Bologna Chunks 750g $3.68
Fletcher's Smokies 450g $3.28
Western Family Thin Sliced Meats 70g .88
ATTENTION:
A meeting to hear about
ALBERTA FIRST
a new party to bring about
a political change
in the next provincial election.
If you are tired of the present
""give away""
come out and
hear about some alternatives.
Thursday,
January 13, at 7:00 p.m.
in the Trading Company Hall (Lions)
•J» *1* *1*
How come I can find two of everything
except socks?”
lîulilüj H
758-3992 ®
“Actually, I can take a peekaboo
Silence is a powerful weapon.
— Sir Wiltiasn Osler
MORE GROCERY SPECIALS JANUARY 10-15
ssdirect@telusplanet.net ww"".ssdirect.com
Kraft Cracker Barrel Cheddar Cheese 454g $5.48
McCain Deep Pan Pizza 500-530g $3.28
Tampico Punch (citrus or mango) 4 litre $2.48
Dairyland Sour Cream 250 ml .98
Dairyland Multipack Yogurt 8 x 125g $2.68
McCain’s Punch or Iced Tea 355 ml .88
Cadbury Chocolate Bars 3 for $1.78
Betty Crocker Fruit Snacks 110-175g $2.28
Magic Moment Pudding Snacks 4 pack $2.28
Kraft Handi Snacks 87g x 3 pk $1.48
Nature Valley Granola Bars 226-230g $2.48
McCormicks Wagon Wheels 350-400g $2.98
McCormicks Viva Puffs 300g $1.98
The Del Bonita 4-H Club held a meeting on
December 6th at the Del Bonita School, later
followed by a game of volleyball. On December
19th we carolled to all the seniors of the Del
Bonita Community, then we met at the school
and had a nice lunch, followed by our
Christmas party and a game of basketball.
The Club had each member bring $5.00 worth
of food for the food bank.
Our Annual Bottle and Battery Drive was
held on January 8th.
Our January 10th meeting was held at the Del
Bonita School at 7:00 p.m. It was decided that
our next meeting will be held on February 7th,
2000 at the Del Bonita School. We will hold a
Judging at Dave Newton's on February 18th,
right after school. We spent some time learning
the correct procedures on judging and then
adjourned the meeting.
This report was submitted by Kaycee Newton,
Del Bonita 4-H Club Reporter.
Computer Consulting, Sales, and Service
Box 389, 132 - Street SW, Magrath, AB TOK 1JO
Roger Davias voice 758-3577
General Manager fax 758-91*74
Magrath Golf Club Notice of Annual Meeting
- Meeting to be held Monday February 7, 2000
at 7:00 p.m. at the Magrath High School
- Nominations for executives to be forwarded to
Box 432, Magrath, AB. TOK IJO,
prior to February 1, 2000.
For more information contact: Ron Johnson @
(403) 758-3341.
PRODUCE SPECIALS JANUARY 10 -15
Texas Sweet Red Grapefruit
5 lbs.
$3.48
Fresh Limes or Lemons
2 for .88
Blood Oranges
$3.70/kg
$1.68/lb
Minneola Tangelos
$3.70/kg
$1.68/lb
Fresh Express Coleslaw or Garden Salad
lib
$1.78
SENIORS NEWS
Seniors Wednesday supper will be Wednesday January 12th, we will have Chinese food; January 19th, soup & sandwich or something else. These are at 5 p.m. at the Seniors Centre. Friday Pot Luck will be Friday January 28th at 5 p.m.
Following the Pot Luck supper the Seniors Annual Meeting will be held. Selection of officers will be made, please come and help the Seniors'
Bookings for the Senior's Centre, Call: Jack or Jean Butl","Magrath Store News (January 12, 2000)",,J. A. Ririe,,,core
214599579,2011-04-07T00:00:00,"Urban structures exhibit complex patterns made of heterogeneous and irregular objects. Few works in the computational urban modelling literature have considered and examined the real geometric boundary of the city’s objects. However, most of these works are driven by Cellular Automata (CA) as a spatial modelling vehicle. This model has had success, but also has its limitations regarding the study of urban dynamics in computer simulation. Extensive modification of CA or use of a different modelling paradigm should be considered. We argue here that, representational realism must be achieved in urban complexity. This paper is an attempt to fill this gap to address the rigid structure of CA: we present a novel technique called the “vector-agent based simulation”, which uses discrete irregular objects as an autonomous spatial entity beneath an agent modelling structure. Through computer simulation, this new technique has been applied to von Thunen’s theory of agricultural land use as a hypothetical environment for model verification. The findings demonstrate that our proposal can be a new paradigm for urban simulationPublishedNon Peer ReviewedBarros, J. (2003), ""Simulating Urban Dynamics in Latin American Cities"", Proceedings of the 7th International Conference on GeoComputation, University of Southampton, UK.

Batty, M. (1997), ""Editorial, Urban Systems as Cellular Automata"", Environment and Planning B, Vol. 27, pp.

Batty, M. and Longley, P. (1994), ""Fractal Cities"", Academic press, London.

Batty, M. and Xie, Y. (1994), ""From Cells to Cities"", Environment and Planning B, Vol. 21, pp. 31-48.

Batty, M. and Xie, Y. (1997), ""Possible Urban Automata"", Environment and Planning B, Vol. 24, pp. 175-192.

Batty, M., Xie, Y. and Sun, Z. (1999), ""Modelling Urban Dynamics through GIS-Based Cellular Automata"", Computers, Environment and Urban Systems, Vol. 23, pp. 205-233.

Benenson, I., Omer, I. and Hatna, E. (2002), ""Entity-Based Modelling of Urban Residential Dynamics: The Case of Yaffo, Tel Aviv"", Environment and Planning B: Planning and Design, Vol. 29, pp. 491-512.

Collier, N. (2003), ""Repast: An Extensible Framework for Agent Simulation"", Social Science Research Computing at the university of Chicago, Chicago, IL, USA, Available online at: http://repast.sorceforge.net, last accessed: 09-02-04.

Colonna, A., Stefano, V., Lombardo, S., Papini, L. and Rabino, G. A. (1998), ""Learning Cellular Automata: Modelling Urban Modelling"", Proceedings of the 3rd International Conference on GeoComputation, University of Bristol, UK.

Couclelis, H. (1985), ""Cellular Worlds: A Framework for Modelling Micro-Macro Dynamics"", Environment and Planning A, Vol. 17, pp. 585-596.

Flache, A. and Hegselmann, R. (2001), ""Do Irregular Grids Make Difference? Relaxing the Spatial Regularity Assumption in Cellular Models of Social Dynamics"", Journal of Artificial Societies and Social Simulation, Vol. 4(4).

Goldstein, N. C. (2003), ""Brains Vs. Brawn-Comparative Strategies for the Calibration of a Cellular Automata-Based Urban Growth Model"", Proceedings of the 7th International Conference on GeoComputation, University of Southampton, UK.

Haklay, M., O’Sullivan, D. and Thurstain-Goodwin, M. (2001), ""So Go Downtown: Simulating Pedestrian Movement in Town Centres"", Environment and Planning B: Planning and Design, Vol. 28, pp. 343-359.

Jiang, B. and Gimblett, H. R. (2002), ""An Agent-Based Approach to Environmental and Urban Systems within Geographic Information Systems"", In Gimblett, H. R. (Ed.) Integrating Geographic Information Systems and Agent-Based Modelling Techniques for Simulating Social and Ecological Processes, a Volume in the Santa Fee Institute in the Science of Complexity, Oxford Investment Press, Inc., New York.

Li, X. and Yeh, A. G. (2001), ""Calibration of Cellular Automata by Using Neural Networks for the Simulation of Complex Urban Systems"", Environment and Planning A, Vol. 33, pp. 1445-1462.

Minar, N., Burkhart, R., Langton, C. and Askenazi, M. (1996) ""The Swarm Simulation System: A Toolkit for Building Multi-Agent Simulations"", Available online at: http://wiki.swarm.org/wiki/Papers_on_Swarm, last accessed: 12-01-04.

Okabe, A., Boots, B. and Sugihara, K. (1992), ""Spatial Tessellations: Concepts and Applications of Voronoi Diagram"", John Wiley & Sons Ltd., England.

O’Sullivan, D. (2000), ""Graph-Based Cellular Automata Models of Urban Spatial Processes"", Unpublished PhD Thesis, Bartlett School of Architecture and Planning, University College London, University of London, London, UK.

O’Sullivan, D. (2001), ""Graph-Cellular Automata: A Generalised Discrete Urban and Regional Model"", Environment and Planning B: Planning and Design, Vol. 28, pp. 687-707.

O’Sullivan, D., MacGill, J. R. and Yu, C. (2003), ""Agent-Based Residential Segregation: A Hierarchically Structured Spatial Model"", Presented at Agent 2003: Challenges in social simulation, University of Chicago and Argonne National Laboratory, IL, USA.

Portugali, J. (2000), ""Self Organization and the City"", Springer, Verlag, Berlin.

Rodrigue, J. P. (2003), ""Transport Geography on the Web: The Free Educational Materials"", Available online at: http://people.hofstra.edu/geotrans/eng/ch6en/conc6en/vonthunen.html, last access: 21/04/04.

Rodrigues, A., Grueau, C., Raper, J. and Neves, N. (1998), ""Environmental Planning Using Spatial Agents"", In Carver, S. (Ed.) Innovations in GIS 5, Selected Papers from the 5th National Conference on GIS Research UK (GISUK), Taylor & Francis Ltd., London.

Russell, S. and Norvig, P. (1995), ""Artificial Intelligence: A Modern Approach"", Printice Hall International Inc., NJ, USA.

Sasaki, Y. and Box, P. (2003), ""Agent-Based Verification of von Thunen's Location Theory"", Journal of Artificial Societies and Social Simulation, Vol. 6(2).

Schelling, T. C. (1969), ""Models of Segregation"", American Economic Association Papers and Proceedings, Vol. 59(2), pp. 488-493.

Semboloni, F. (2000), ""The Growth of an Urban Cluster into a Dynamic Self-Modifying Spatial Pattern"", Environment and Planning B: Planning and Design, Vol. 27(4), pp. 549-564.

Shi, W. and Pang, M. Y. C. (2000), ""Development of Voronoi-Based Cellular Automata: An Integrated Dynamic Model for Geographical Information Systems"", International Journal of Geographical Information Science, Vol. 14(5), pp. 455-474.

Takeyama, M. and Couclelis, H. (1997), ""Map Dynamics: Integrating Cellular Automata and GIS through Geo-Algebra"", International Journal of Geographical Information Science, Vol. 11(1), pp. 73-91.

Torrens, P. and O’Sullivan, D. (2000), ""Cities, Cells, and Complexity: Developing a Research Agenda for Urban GeoComputation"", Proceedings of the 5th International Conference on GeoComputation, University of Greenwich, London.

Torrens, P. M. (2000), ""How Cellular Models of Urban System Work (1. Theory)"", Centre for Advanced Spatial Analysis (CASA), University College London, London, CASA working paper 28.

Turton, I. (2003), ""Modelling Landuse Development Using Multi-Agent Systems"", Proceedings of the 7th International Conference on GeoComputation, University of Southampton, UK.

Weiss, G. (1999), ""Multiagent Systems: A Modern Approach to Distributed Artificial Intelligence"", the MIT Press, England.

White, R. and Engelen, G. (2000), ""High-Resolution Integrated Modelling of the Spatial Dynamics of Urban and Regional Systems"", Computers, Environment and Urban Systems, Vol. 24, pp. 383-400.

Wilson, A. G. (2000), ""Complex Spatial Systems"", Pearson Education Limited, England.

Wolfram, S. (2002), ""A New Kind of Science"", Wolfram Media, Champaign, IL.

Worboys, M. F. (1995), ""GIS a Computing Perspective"", Taylor & Francis, UK.

Wu, F. (1998), ""An Experiment on the Generic Polycentricity of Urban Growth in a Cellular Automatic City"", Environment and Planning B: Planning and Design, Vol. 25, pp. 731-752.

Xu, H. (2003), ""A Model-Based Approach for Development of Multi-Agent Software Systems"", Unpublished PhD Thesis, University of Illinois at Chicago, Chicago, USA.

Yeh, A. G. and Li, X. (2001), ""A Constrained CA Model for the Simulation and Planning of Sustainable Urban Forms by Using GIS"", Environment and Planning B: Planning and Design, Vol. 28, pp. 733-753.

Yeh, A. G. and Li, X. (2002), ""A Cellular Automata Model to Simulate Development Density for Urban Planning"", Environment and Planning B: Planning and Design, Vol. 29, pp. 431-450",Irregular vector-agent based simulation for land-use modelling,,,,,core
187877562,2010-10-01T00:00:00,"La valorizzazione del paesaggio comporta una riattivazione delle risorse e delle identit\ue0 locali in territori complessi (variet\ue0 di paesaggi urbani, aree verdi interne,
spazi periurbani, aree agricole di prossimit\ue0, riserve naturali e parchi regionali). Il ""valore"" del paesaggio non \ue8 dato soltanto dalla qualit\ue0 dell'ambiente naturale e del patrimonio culturale, ma anche dal suo ""valore"" simbolico e da valutazioni estetiche ed emozionali. Il risultato sperato \ue8 l'individuazione di strategie efficaci di ricomposizione territoriale di spazi urbani e rurali e la loro  valorizzazione culturale e turistica.
La qualit\ue0 e l'evoluzione dei rapporti di interscambio materiale e simbolico tra le aree urbane e metropolitane e il territorio dei parchi (del Ticino, di San Rossore e delle Madonie) \ue8 uno dei temi centrali della ricerca.
In modo complementare, si analizzeranno gli aspetti psico-sociali della relazione tra natura e benessere soggettivo, poich\ue9 l'ambiente naturale \ue8 fattore di benessere cognitivo ed emozionale. L'obiettivo \ue8 analizzare i legami tra qualit\ue0 dell'esperienza provata in ambienti naturali e tipologia e organizzazione dell'esperienza stessa, ipotizzando l'esistenza di mediatori attivi nella complessificazione del S\ue9.
Le considerazioni emerse dalle analisi condotte in tali ambiti confluiranno nella proposta di buone pratiche innovative per la fruizione dei territori considerati.
L'adeguamento delle politiche di gestione delle aree protette alle condizioni di valorizzazione si basa sull'applicazione di principi, strategie, dispositivi e tecnologie
finalizzati alla ecoefficienza e alla sostenibilit\ue0. A lungo il parco \ue8 stato il soggetto esclusivo delle politiche di conservazione; di recente l'interesse si \ue8 spostato ai margini, dove si concentrano nuove risorse e potenzialit\ue0.
L'ambito di ricerca, partendo dallo studio delle fasce periurbane, mira alla definizione delle modalit\ue0 di fruizione, gestione e sviluppo dei sistemi di organizzazione tecnico-funzionale integrati agli spazi del parco (gestione dell'accessibilit\ue0, integrazione della rete di percorsi, organizzazione degli spazi e attrezzature di servizio per l'accoglienza e gestione di flussi turistici; produzione e distribuzione di energia rinnovabile, e a basso impatto.
Obiettivo della ricerca \ue8 un sistema di indirizzi, strategie e tecnologie per la valorizzazione delle aree di margine di contesti complessi, adeguati ai principi del recupero sostenibile, del ripristino ambientale e della fruizione compatibile del patrimonio naturalistico e storico-culturale. Il processo operativo valuter\ue0 le
soluzioni possibili per i diversi sistemi, verificandone la conformit\ue0 rispetto ai requisiti specifici delle norme locali e del bilancio energetico finale.
Nel piano di azione strategico per la gestione sostenibile del territorio, con riferimento alle aree protette si propone un sistema di indicatori a supporto delle decisioni, finalizzato alla valutazione di progetti di intervento su assetti costruiti nelle zone ad alta valenza ambientale, in grado di orientare i decisori verso una
valorizzazione delle vocazioni locali, considerando gli impatti ambientali. Gli indicatori per la valutazione di sostenibilit\ue0 saranno declinati (approccio del Life Cycle) alle trasformazioni dell'ambiente costruito in indicatori di carattere ambientale e sociale relativi agli impatti sull'ambiente (consumo di risorse ed energie, produzione di emissioni e rifiuti, impatti relativi alla salute, al comfort e alla fruibilit\ue0 di spazi e servizi). La matrice degli indicatori aggregati costituir\ue0 uno strumento di comunicazione dei risultati in grado di favorire la convergenza di dati integrati e sinergici, e indirizzato a decisori e tecnici degli Enti locali, per l'applicazione di procedure di analisi/valutazione e la gestione in un quadro sinottico multidisciplinare.
L'unit\ue0 operativa analizzer\ue0 la realt\ue0 territoriale attraverso alcune azioni fondamentali: Analisi delle componenti territoriali; analisi SWOT dei sistemi economici locali; analisi della domanda turistica, dei principali trend di sviluppo. La SWOT Analysis, evidenzier\ue0 i settori di intervento relativi ai problemi e alle opportunit\ue0 reali e potenziali di crescita dell'offerta turistica; svilupper\ue0 una banca dati territoriale (GIS). Si implementeranno sistemi di scelte (public choice), con una molteplicit\ue0 di criteri e alternative, riguardanti le ricadute delle scelte operative.
La valutazione quali-quantitativa dei sistemi territoriali e delle interrelazioni fra essi esistenti consentir\ue0 di individuare scenari di marketing turistico.
L'ottimizzazione di un piano di marketing \ue8 deriva dagli strumenti innovativi della ricerca: le scelte matriciali permettono di modificare i criteri e le alternative di piano. Nel differenziare le strategie di marketing nei contesti urbani e rurali tali metodiche rappresentano un punto di forza.Improving the landscape implies the reactivation of local resources and identities in complex areas (urban areas, green areas within the urban space, peri-urban areas, agricultural areas, natural reserves and regional parks).
The ""value"" of the landscape is not merely due to the quality of natural environment and cultural heritage, but also to its symbolic ""value"" and aesthetic/emotional features
The research will focus on the quality and the evolution of functional relationships and material or symbolic exchange between urban/metropolitan areas and the parks (Ticino Park, San Rossore, Massaciuccoli Natural Reserve).
We will further analyse the same territories looking at the psychosocial aspects of the relation between subjective well-being and nature, because natural environment was often found as a key factor for emotional and cognitive well-being.
The general aim is therefore to analyse the links between natural positive experiences and their main psychological features, hypothesizing the existence of specific buffers able to create self-complexity. The necessary adjustment of management policies of nature reserves to the conditions for the defence, recovery and value increase of environment is characterised by the application of principles, strategies, devices and technologies meant for environmental effectiveness and sustainability.
For a long time, the park was the exclusive subject for territorial preservation policies; recently, the focus shifted to margins, where resources and potentialities are
concentrated.
Starting from the study of the peri-urban belts between built-up areas and naturalistic areas, the research is meant to study and define procedures for the fruition, management and development of technical and functional systems integrated with park areas (manegement of access, integration of the network of different routes,
ORGANISATION OF THE SYSTEM OF SPACES AND SUPPORT to tourist services, SYSTEMS FOR PRODUCING AND DISTRIBUTING ENERGY FROM RENEWABLE SOURCES)
The goal of the research is identifying a system of tendencies, strategies and technologies for increasing the value of marginal areas of complex sites, following the principles of sustainable recovery, environmental restoration and fruition compatible with the naturalistic, historical and cultural heritage. The operational process
will weigh multiple solutions which are possible for different systems, checking their conformity with respect to the specific requirements of local regulations and the ultimate energy balance.
As regards the strategic action plan for a sustainable land-use management, in reference to the protected sensitive areas will be developed a system of indicators, organized in a decisions tool.
This tool will be used to estimate design and management plans for built areas of high natural and environmental value, in order to guide decision makers towards the enhancement of the specific vocations of places, considering environmental impacts.
The indicators for the sustainability assessment are framed, according to the life cycle approach, into indicators of environmental and social impacts (energy
consumption, emissions and wastes, the impacts on health, comfort and usability of spaces and social services). The matrix of aggregated indicators is a communication tool used for furthering synergy and integration of data, and designed for decision makers and technicians of local authorities, to be applied on assessment and management procedures, in a highly multidisciplinary overview.
The working group will analyze the local context through the key actions listed below: 
Analysis of spatial components; Consideration of strengths and weaknesses of local economic systems; Analysis of the main developmental trends of the touristic demand.
A SWOT Analysis will give us the chance to highlight the areas on which to act in order to solve the problems already existing and to seize the real opportunities and potential for growth or diversification and segmentation of tourism. Later we will develop a territorial database that will require the collection and intake in the GIS
and will be implemented systems of choices (public choice model) that use a variety of criteria and  alternatives.
This methodology will be useful to evaluate territorial systems and their mutual connections, allowing to plan new turistical marketing strategies.
The fundamental point for the optimization of a marketing plan is, in fact, already contained in the innovative instruments adopted by the research. Indeed, the peculiarity of the chosen matrix allows to change real-time either the criteria for the selection or the alternative plans.
The methods which have been described above are a fundamental strength in differentiating marketing strategies in urban and rural contexts","Responsabile scientifico nazionale del Progetto PRIN 2009 &quot;Ricomposizione territoriale e valorizzazione sostenibile degli spazi urbani e rurali: turismo e vocazioni storiche, culturali,

architettoniche, ambientali a confronto&quot;",,,,,core
21150430,2005,"Traffic congestion is one of the leading causes of lost productivity and decreased standard of living in urban settings. Recent advances in artificial intelligence suggest vehicle navigation by autonomous agents will be possible in the near future. In a previous paper, we proposed a reservation-based system for alleviating traffic congestion, specifically at intersections. This paper extends our prototype implementation in several ways with the aim of making it more implementable in the real world. In particular, we 1) add the ability of vehicles to turn, 2) enable them to accelerate while in the intersection, and 3) augment their interaction capabilities with a detailed protocol such that the vehicles do not need to know anything about the intersection control policy. The use of this protocol limits the interaction of the driver agent and the intersection manager to the extent that it is a reasonable approximation of reliable wireless communication. Finally, we describe how different intersection control policies can be expressed with this protocol and limited exchange of information. All three improvements are fully implemented and tested, and we present detailed empirical results validating their effectiveness. 1",Multiagent traffic management: An improved intersection control mechanism,,,,,core
20802616,14/08/2008,"The development of Intelligent Transportation Systems and the diffusion of urban traffic control and monitoring by Autonomous Agents have created very large databases of realtime and historic traffic data. Therefore, data mining approaches are necessary to design effective Decision Support Systems for traffic management and user information generation. In particular, the completion of urban traffic data in a spatial-temporal context represents a relevant problem particularly suited for traffic states estimation and forecasting, real-time traffic control, dynamic OD matrices updating and so on. As regards the traffic data completion, a constraints propagation approach has been proposed and implemented in the framework of the KITS European Research Project. However, more efficient and flexible methods have to be investigated and tested in order to support traffic data mining problems. 2 MULTILAYER NEURAL NETWORKS Neural Networks have been widely applied for traffic forecasting and they can be used for spatial data completion as well. To this aim, mobile detectors are able to collect necessary traffic data relative to those variables involved in the extrapolation. In this way it is possibl",URBAN TRAFFIC DATA MINING AND NEURAL NETWORK MODELS,,,,,core
24586579,2006,"1 Spartacus is the name of our robot entry at the 2005 AAAI Mobile Robot Challenge, which consists of making a robot attend the National Conference on Artificial Intelligence. Designing robots that are capable of interacting with humans in real life settings can be considered the ultimate challenge when it comes to intelligent autonomous systems. One key issue is the integration of multiple modalities (e.g., mobility, physical structure, navigation, vision, audition, dialogue, reasoning) into a coherent implementation. Such integration increases the complexity and the diversity of interactions the robot can have, as of analysis and monitoring of such increased capabilities. This paper reports on our solutions and findings resulting from the hardware, software and computation integration work on Spartacus, along with future perspectives regarding this initiative",Manuscript Click here to download Manuscript: AR2006.tex Spartacus Attending the 2005 AAAI Conference,,,,,core
101960181,2008,"In recent years, many emerging problems with urban transporta-tion system, including traffic congestion, have been direct results of the steadily increasing number of commuters in urban areas. To address these problems, many research projects have studied the behaviors of urban traffic systems, and researchers have proposed new and innova-tive traffic control schemes. These studies require accurate and realistic software models for simulating behaviors of urban traffic systems since using a real traffic network as a testbed is often impractical. In this paper, we describe a novel software simulator for urban traffic systems. Our simulator has several desirable properties. First, it gives an accu-rate and realistic representation of important elements of urban traffic networks. Second, it is specially designed for game-theoretic model-ing of traffic systems in order to address potentially different research questions. Lastly, it has a flexible framework for generating customized network topologies and running customized simulations. In particular, our simulator is particularly useful for studying long term driver be-havior through the application of multiagent reinforcement learning (MARL) algorithms. In this paper, we will first describe the urban traffic model used by our software simulator. Specifically, we will describe important com-ponents of real urban traffic systems and argue that the corresponding components of our model are reasonable and meaningful approxima-tions of their real counterparts. Secondly, we will describe the com-ponents of our model that allow game-theoretic modeling of urban traffic systems in order to address various research questions. Finally, we will demonstrate how our model is particularly useful for studying long-term driver behavior, especially during rush hour. 1 ",Building a Software Simulator for Urban Traffic Systems,,,,,core
30857249,2011-01-01T00:00:00,"Editors: Alexander Refsum Jensenius, Anders Tveit, Rolf Inge Godøy, Dan Overholt
Table of Contents

-Tellef Kvifte: Keynote Lecture 1: Musical Instrument User Interfaces: the Digital Background of the Analog Revolution - page 1
-David Rokeby: Keynote Lecture 2: Adventures in Phy-gital Space - page 2 
-Sergi Jordà: Keynote Lecture 3: Digital Lutherie and Multithreaded Musical Performance: Artistic, Scientific and Commercial Perspectives - page 3

Paper session A — Monday 30 May 11:00–12:30  
-Dan Overholt: The Overtone Fiddle: an Actuated Acoustic Instrument - page 4  
-Colby Leider, Matthew Montag, Stefan Sullivan and Scott Dickey: A Low-Cost, Low-Latency Multi-Touch Table with Haptic Feedback for Musical Applications - page 8 
-Greg Shear and Matthew Wright: The Electromagnetically Sustained Rhodes Piano - page 14
-Laurel Pardue, Christine Southworth, Andrew Boch, Matt Boch and Alex Rigopulos: Gamelan Elektrika: An Electronic Balinese Gamelan - page 18
-Jeong-Seob Lee and Woon Seung Yeo: Sonicstrument: A Musical Interface with Stereotypical Acoustic Transducers - page 24

Poster session B— Monday 30 May 13:30–14:30 
-Scott Smallwood: Solar Sound Arts: Creating Instruments and Devices Powered by Photovoltaic Technologies - page 28
-Niklas Klügel, Marc René Frieß and Georg Groh: An Approach to Collaborative Music Composition - page 32
-Nicolas Gold and Roger Dannenberg: A Reference Architecture and Score Representation for Popular Music Human-Computer Music Performance Systems - page 36
-Mark Bokowiec: V’OCT (Ritual): An Interactive Vocal Work for Bodycoder System and 8 Channel Spatialization - page 40
-Florent Berthaut, Haruhiro Katayose, Hironori Wakama, Naoyuki Totani and Yuichi Sato: First Person Shooters as Collaborative Multiprocess Instruments - page 44
-Tilo Hähnel and Axel Berndt: Studying Interdependencies in Music Performance: An Interactive Tool - page 48
-Sinan Bokesoy and Patrick Adler: 1city 1001vibrations: development of a interactive sound installation with robotic instrument performance - page 52
-Tim Murray-Browne, Di Mainstone, Nick Bryan-Kinns and Mark D. Plumbley:The medium is the message: Composing instruments and performing mappings - page 56
-Seunghun Kim, Luke Keunhyung Kim, Songhee Jeong and Woon Seung Yeo: Clothesline as a Metaphor for a Musical Interface - page 60
-Pietro Polotti and Maurizio Goina: EGGS in action - page 64
-Berit Janssen: A Reverberation Instrument Based on Perceptual Mapping - page 68
-Lauren Hayes: Vibrotactile Feedback-Assisted Performance - page 72
-Daichi Ando: Improving User-Interface of Interactive EC for Composition-Aid by means of Shopping Basket Procedure - page 76
-Ryan McGee, Yuan-Yi Fan and Reza Ali: BioRhythm: a Biologically-inspired Audio-Visual Installation - page 80
-Jon Pigott: Vibration, Volts and Sonic Art: A practice and theory of electromechanical sound - page 84
-George Sioros and Carlos Guedes: Automatic Rhythmic Performance in Max/MSP: the kin.rhythmicator - page 88
-Andre Goncalves: Towards a Voltage-Controlled Computer — Control and Interaction Beyond an Embedded System - page 92
-Tae Hun Kim, Satoru Fukayama, Takuya Nishimoto and Shigeki Sagayama: Polyhymnia: An automatic piano performance system with statistical modeling of polyphonic expression and musical symbol interpretation - page 96
-Juan Pablo Carrascal and Sergi Jorda: Multitouch Interface for Audio Mixing - page 100
-Nate Derbinsky and Georg Essl: Cognitive Architecture in Mobile Music Interactions - page 104
-Benjamin D. Smith and Guy E. Garnett: The Self-Supervising Machine - page 108
-Aaron Albin, Sertan Senturk, Akito Van Troyer, Brian Blosser, Oliver Jan and Gil Weinberg: Beatscape, a mixed virtual-physical environment for musical ensembles - page 112
-Marco Fabiani, Gaël Dubus and Roberto Bresin: MoodifierLive: Interactive and collaborative expressive music performance on mobile devices - page 116
-Benjamin Schroeder, Marc Ainger and Richard Parent: A Physically Based Sound Space for Procedural Agents - page 120
-Francisco Garcia, Leny Vinceslas, Esteban Maestre and Josep Tubau Acquisition and study of blowing pressure profiles in recorder playing - page 124
-Anders Friberg and Anna Källblad:Experiences from video-controlled sound installations - page 128
-Nicolas d’Alessandro, Roberto Calderon and Stefanie Müller: ROOM#81 —Agent-Based Instrument for Experiencing Architectural and Vocal Cues - page 132

Demo session C — Monday 30 May 13:30–14:30 
-Yasuo Kuhara and Daiki Kobayashi: Kinetic Particles Synthesizer Using Multi-Touch Screen Interface of Mobile Devices - page 136
-Christopher Carlson, Eli Marschner and Hunter Mccurry: The Sound Flinger: A Haptic Spatializer - page 138
-Ravi Kondapalli and Benzhen Sung: Daft Datum – an Interface for Producing Music Through Foot-Based Interaction - page 140
-Charles Martin and Chi-Hsia Lai: Strike on Stage: a percussion and media performance - page 142

Paper session D — Monday 30 May 14:30–15:30 
-Baptiste Caramiaux, Patrick Susini, Tommaso Bianco, Frédéric Bevilacqua, Olivier Houix, Norbert Schnell and Nicolas Misdariis: Gestural Embodiment of Environmental Sounds: an Experimental Study - page 144
-Sebastian Mealla, Aleksander Valjamae, Mathieu Bosi and Sergi Jorda: Listening to Your Brain: Implicit Interaction in Collaborative Music Performances - page 149
-Dan Newton and Mark Marshall: Examining How Musicians Create Augmented Musical Instruments - page 155
 
Paper session E — Monday 30 May 16:00–17:00 
-Zachary Seldess and Toshiro Yamada: Tahakum: A Multi-Purpose Audio Control Framework - page 161
-Dawen Liang, Guangyu Xia and Roger Dannenberg: A Framework for Coordination and Synchronization of Media - page 167
-Edgar Berdahl and Wendy Ju: Satellite CCRMA: A Musical Interaction and Sound Synthesis Platform  - page 173
 
Paper session F — Tuesday 31 May 09:00–10:50 
-Nicholas J. Bryan and Ge Wang: Two Turntables and a Mobile Phone - page 179
-Nick Kruge and Ge Wang: MadPad: A Crowdsourcing System for Audiovisual Sampling - page 185
-Patrick O’Keefe and Georg Essl: The Visual in Mobile Music Performance - page 191
-Ge Wang, Jieun Oh and Tom Lieber: Designing for the iPad: Magic Fiddle - page 197
-Benjamin Knapp and Brennon Bortz: MobileMuse: Integral Music Control Goes Mobile - page 203
-Stephen Beck, Chris Branton, Sharath Maddineni, Brygg Ullmer and Shantenu Jha: Tangible Performance Management of Grid-based Laptop Orchestras - page 207
 
Poster session G— Tuesday 31 May 13:30–14:30
-Smilen Dimitrov and Stefania Serafin: Audio Arduino—an ALSA (Advanced Linux Sound Architecture) audio driver for FTDI-based Arduinos - page 211
-Seunghun Kim and Woon Seung Yeo: Musical control of a pipe based on acoustic resonance - page 217
-Anne-Marie Hansen, Hans Jørgen Andersen and Pirkko Raudaskoski: Play Fluency in Music Improvisation Games for Novices - page 220
-Izzi Ramkissoon: The Bass Sleeve: A Real-time Multimedia Gestural Controller for Augmented Electric Bass Performance - page 224
-Ajay Kapur, Michael Darling, James Murphy, Jordan Hochenbaum, Dimitri Diakopoulos and Trimpin: The KarmetiK NotomotoN: A New Breed of Musical Robot for Teaching and Performance - page 228
-Adrian Barenca Aliaga and Giuseppe Torre:  The Manipuller: Strings Manipulation and Multi-Dimensional Force Sensing - page 232
-Alain Crevoisier and Cécile Picard-Limpens: Mapping Objects with the Surface Editor - page 236
-Jordan Hochenbaum and Ajay Kapur: Adding Z-Depth and Pressure Expressivity to Tangible Tabletop Surfaces - page 240
-Andrew Milne, Anna Xambó, Robin Laney, David B. Sharp, Anthony Prechtl and Simon Holland: Hex Player—A Virtual Musical Controller - page 244
-Carl Haakon Waadeland: Rhythm Performance from a Spectral Point of View - page 248
-Josep M Comajuncosas, Enric Guaus, Alex Barrachina and John O’Connell: Nuvolet : 3D gesture-driven collaborative audio mosaicing - page 252
-Erwin Schoonderwaldt and Alexander Refsum Jensenius: Effective and expressive movements in a French-Canadian fiddler’s performance - page 256
-Daniel Bisig, Jan Schacher and Martin Neukom: Flowspace – A Hybrid Ecosystem - page 260
-Marc Sosnick and William Hsu: Implementing a Finite Difference-Based Real-time Sound Synthesizer using GPUs - page 264
-Axel Tidemann: An Artificial Intelligence Architecture for Musical Expressiveness that Learns by Imitation - page 268
-Luke Dahl, Jorge Herrera and Carr Wilkerson: TweetDreams: Making music with the audience and the world using real-time Twitter data - page 272
-Lawrence Fyfe, Adam Tindale and Sheelagh Carpendale: JunctionBox: A Toolkit for Creating Multi-touch Sound Control Interfaces - page 276
-Andrew Johnston: Beyond Evaluation: Linking Practice and Theory in New Musical Interface Design - page 280
-Phillip Popp and Matthew Wright: Intuitive Real-Time Control of Spectral Model Synthesis - page 284
-Pablo Molina, Martin Haro and Sergi Jordà: BeatJockey: A new tool for enhancing DJ skills - page 288
-Jan Schacher and Angela Stoecklin: Traces – Body, Motion and Sound - page 292
-Grace Leslie and Tim Mullen: MoodMixer: EEG-based Collaborative Sonification - page 296
-Ståle A. Skogstad, Kristian Nymoen, Yago de Quay and Alexander Refsum Jensenius: OSC Implementation and Evaluation of the Xsens MVN suit - page 300
-Lonce Wyse, Norikazu Mitani and Suranga Nanayakkara: The effect of visualizing audio targets in a musical listening and performance task - page 304
-Freed Adrian, John Maccallum and Andrew Schmeder: Composability for Musical Gesture Signal Processing using new OSC-based Object and Functional Programming Extensions to Max/MSP - page 308
-Kristian Nymoen, Ståle A. Skogstad and Alexander Refsum Jensenius: SoundSaber —A Motion Capture Instrument - page 312
-Øyvind Brandtsegg, Sigurd Saue and Thom Johansen: A modulation matrix for complex parameter sets - page 316

Demo session H— Tuesday 31 May 13:30–14:30 
-Yu-Chung Tseng, Che-Wei Liu, Tzu-Heng Chi and Hui-Yu Wang: Sound Low Fun- page 320
-Edgar Berdahl and Chris Chafe: Autonomous New Media Artefacts (AutoNMA) - page 322
-Min-Joon Yoo, Jin-Wook Beak and In-Kwon Lee: Creating Musical Expression using Kinect - page 324
-Staas de Jong: Making grains tangible: microtouch for microsound - page 326
Baptiste Caramiaux, Frederic Bevilacqua and Norbert Schnell: Sound Selection by Gestures - page 329

Paper session I — Tuesday 31 May 14:30–15:30 
-Hernán KerlleÃevich, Manuel Eguia and Pablo Riera: An Open Source Interface based on Biological Neural Networks for Interactive Music Performance - page 331
-Nicholas Gillian, R. Benjamin Knapp and Sile O’Modhrain: Recognition Of Multivariate Temporal Musical Gestures Using N-Dimensional Dynamic Time Warping - page 337
-Nicholas Gillian, R. Benjamin Knapp and Sile O’Modhrain: A Machine Learning Toolbox For Musician Computer Interaction - page 343
 
Paper session J — Tuesday 31 May 16:00–17:00 
-Elena Jessop, Peter Torpey and Benjamin Bloomberg: Music and Technology in Death and the Powers - page 349
-Victor Zappi, Dario Mazzanti, Andrea Brogni and Darwin Caldwell: Design and Evaluation of a Hybrid Reality Performance - page 355
-Jérémie Garcia, Theophanis Tsandilas, Carlos Agon and Wendy Mackay: InkSplorer : Exploring Musical Ideas on Paper and Computer - page 361

Paper session K — Wednesday 1 June 09:00–10:30 
-Pedro Lopes, Alfredo Ferreira and Joao Madeiras Pereira: Battle of the DJs: an HCI perspective of Traditional, Virtual, Hybrid and Multitouch DJing - page 367
-Adnan Marquez-Borbon, Michael Gurevich, A. Cavan Fyans and Paul Stapleton: Designing Digital Musical Interactions in Experimental Contexts - page 373
-Jonathan Reus: Crackle: A mobile multitouch topology for exploratory sound interaction - page 377
-Samuel Aaron, Alan F. Blackwell, Richard Hoadley and Tim Regan: A principled approach to developing new languages for live coding - page 381
-Jamie Bullock, Daniel Beattie and Jerome Turner: Integra Live: a new graphical user interface for live electronic music - page 387

Paper session L — Wednesday 1 June 11:00–12:30 
-Jung-Sim Roh, Yotam Mann, Adrian Freed and David Wessel: Robust and Reliable Fabric, Piezoresistive Multitouch Sensing Surfaces for Musical Controllers - page 393
-Mark Marshall and Marcelo Wanderley: Examining the Effects of Embedded Vibrotactile Feedback on the Feel of a Digital Musical Instrument - page 399
-Dimitri Diakopoulos and Ajay Kapur: HIDUINO: A firmware for building driverless USB-MIDI devices using the Arduino microcontroller - page 405
-Emmanuel Flety and Côme Maestracci: Latency improvement in sensor wireless transmission using IEEE 802.15.4 - page 409
-Jeff Snyder: The Snyderphonics Manta, a Novel USB Touch Controller - page 413
 
Poster session M — Wednesday 1 June 13:30–14:30
-William Hsu: On Movement, Structure and Abstraction in Generative Audiovisual Improvisation - page 417
-Claudia Robles Angel: Creating Interactive Multimedia Works with Bio-data - page 421
-Paula Ustarroz: TresnaNet: musical generation based on network protocols - page 425
-Matti Luhtala, Tiina Kymäläinen and Johan Plomp: Designing a Music Performance Space for Persons with Intellectual Learning Disabilities - page 429
-Tom Ahola, Teemu Ahmaniemi, Koray Tahiroglu, Fabio Belloni and Ville Ranki: Raja —A Multidisciplinary Artistic Performance - page 433
-Emmanuelle Gallin and Marc Sirguy: Eobody3: A ready-to-use pre-mapped & multi-protocol sensor interface- page 437
-Rasmus Bååth, Thomas Strandberg and Christian Balkenius: Eye Tapping: How to Beat Out an Accurate Rhythm using Eye Movements - page 441
-Eric Rosenbaum: MelodyMorph: A Reconfigurable Musical Instrument - page 445
-Karmen Franinovic: Flo)(ps: Between Habitual and Explorative Action-Sound Relationships - page 448
-Margaret Schedel, Rebecca Fiebrink and Phoenix Perry: Wekinating 000000Swan: Using Machine Learning to Create and Control Complex Artistic Systems - page 453
-Carles F. Julià, Daniel Gallardo and Sergi Jordà: MTCF: A framework for designing and coding musical tabletop applications directly in Pure Data - page 457
-David Pirrò and Gerhard Eckel: Physical modelling enabling enaction: an example - page 461
-Thomas Mitchell and Imogen Heap: SoundGrasp: A Gestural Interface for the Performance of Live Music - page 465
-Tim Mullen, Richard Warp and Adam Jansch: Minding the (Transatlantic) Gap: An Internet-Enabled Acoustic Brain-Computer Music Interface - page 469
-Stefano Papetti, Marco Civolani and Federico Fontana: Rhythm’n’Shoes: a wearable foot tapping interface with audio-tactile feedback - page 473
-Cumhur Erkut, Antti Jylhä and Reha Di¸sçio˘glu: A structured design and evaluation model with application to rhythmic interaction displays - page 477
-Marco Marchini, Panos Papiotis, Alfonso Perez and Esteban Maestre: A Hair Ribbon Deflection Model for Low-Intrusiveness Measurement of Bow Force in Violin Performance - page 481
-Jonathan Forsyth, Aron Glennon and Juan Bello: Random Access Remixing on the iPad - page 487
-Erika Donald, Ben Duinker and Eliot Britton: Designing the EP trio: Instrument identities, control and performance practice in an electronic chamber music ensemble - page 491
-Cavan Fyans and Michael Gurevich: Perceptions of Skill in Performances with Acoustic and Electronic Instruments - page 495
-Hiroki Nishino: Cognitive Issues in Computer Music Programming - page 499
-Roland Lamb and Andrew Robertson: Seaboard: a new piano keyboard-related interface combining discrete and continuous control - page 503
-Gilbert Beyer and Max Meier: Music Interfaces for Novice Users: Composing Music on a Public Display with Hand Gestures - page 507
-Birgitta Cappelen and Anders-Petter Andersson: Expanding the role of the instrument - page 511
-Todor Todoroff: Wireless Digital/Analog Sensors for Music and Dance Performances - page 515
-Trond Engum: Real-time control and creative convolution— exchanging techniques between distinct genres - page 519
-Andreas Bergsland: The Six Fantasies Machine – an instrument modelling phrases from Paul Lansky’s Six Fantasies - page 523
 
Demo session N — Wednesday 1 June 13:30–14:30
-Jan Trützschler von Falkenstein: Gliss: An Intuitive Sequencer for the iPhone and iPad - page 527
-Jiffer Harriman, Locky Casey, Linden Melvin and Mike Repper: Quadrofeelia — A New Instrument for Sliding into Notes - page 529
-Johnty Wang, Nicolas D’Alessandro, Sidney Fels and Bob Pritchard: SQUEEZY: Extending a Multi-touch Screen with Force Sensing Objects for Controlling Articulatory Synthesis - page 531
-Souhwan Choe and Kyogu Lee: SWAF: Towards a Web Application Framework for Composition and Documentation of Soundscape - page 533
-Norbert Schnell, Frederic Bevilacqua, Nicolas Rasamimana, Julien Blois, Fabrice Guedy and Emmanuel Flety: Playing the ""MO"" —Gestural Control and Re-Embodiment of Recorded Sound and Music - page 535
-Bruno Zamborlin, Marco Liuni and Giorgio Partesana: (LAND)MOVES - page 537
-Bill Verplank and Francesco Georg: Can Haptics make New Music? —Fader and Plank Demos - page 53",Proceedings of the International Conference on New Interfaces for Musical Expression,https://core.ac.uk/download/30857249.pdf,,,,core
36204674,2009-01-01T00:00:00,"Urban Search And Rescue (USAR) is a time critical task since all survivors have to be rescued within the first 72 hours. One goal in Rescue Robotics is to support emergency response by mixed-initiative teams consisting of humans and robots. Their task is to explore the disaster area rapidly while reporting victim locations and hazardous areas to a central station, which then can be utilized for planning rescue missions. To fulfill this task efficiently, humans and robots have to map disaster areas jointly while co- ordinating their search at the same time. Additionally, robots have to perform subproblems, such as victim detection and navigation, autonomously. In disaster areas these problems are extraordinarily challenging due to the unstructured environment and rough terrain. Furthermore, when communication fails, methods that are deployed under such conditions have to be decentralized, i.e. operational without a central station. In this thesis a unified approach joining human and robot resources for solving these problems is contributed. Following the vision of combined multi-robot and multi-human teamwork, core problems, such as position tracking on rough terrain, mapping by mixed teams, and decentralized team coordination with limited radio communication, are directly addressed. More specific, RFID-SLAM, a novel method for robust and efficient loop closure in large-scale environments that utilizes RFID technology for data association, is contributed. The method is capable of jointly improving multiple maps from humans and robots in a centralized and decentralized manner without requiring team members to perform loops on their routes. Thereby positions of humans are tracked by PDR (Pedestrian Dead Reckoning), and robot positions by slippage- sensitive odometry, respectively. The joint-graph emerging from these trajectories serves as an input for an iterative map optimization procedure. The introduced map representation is further utilized for solving the centralized and decentralized coordination of large rescue teams. On the one hand, a deliberate method for combined task assignment and multi-agent path planning, and on the other hand, a local search method using the memory of RFIDs for coordination, are proposed. For autonomous robot navigation on rough terrain and real-time victim detection in disaster areas an efficient method for elevation map building and a novel approach to genetic MRF (Markov Random Field) model optimization are contributed. Finally, a human in the loop architecture is presented that integrates data collected by first responders into a multi-agent system via wearable computing. In this context, the support and coordination of disaster mitigation in large-scale environments from a central-command-post-perspective are described. Methods introduced in this thesis were extensively evaluated in outdoor environments and official USAR testing arenas designed by the National Institute of Standards and Technology (NIST). Furthermore, they were an integral part of systems that won in total more than 10 times the first prize at international competitions, such as the RoboCup world championships.This is a Ph.D. thesis originally defended at University of Freiburg.Artificial Intelligence & Integrated Computer System",Mapping and Exploration for Search and Rescue with Humans and Mobile Robots,,Freiburg : University of Freiburg,,,core
46947871,2007-01-01T08:00:00,"This dissertation introduces the design of a multimodal, adaptive real-time assistive system as an alternate human computer interface that can be used by individuals with severe motor disabilities. The proposed design is based on the integration of a remote eye-gaze tracking system, voice recognition software, and a virtual keyboard. The methodology relies on a user profile that customizes eye gaze tracking using neural networks. The user profiling feature facilitates the notion of universal access to computing resources for a wide range of applications such as web browsing, email, word processing and editing. The study is significant in terms of the integration of key algorithms to yield an adaptable and multimodal interface. The contributions of this dissertation stem from the following accomplishments: (a) establishment of the data transport mechanism between the eye-gaze system and the host computer yielding to a significantly low failure rate of 0.9%; (b) accurate translation of eye data into cursor movement through congregate steps which conclude with calibrated cursor coordinates using an improved conversion function; resulting in an average reduction of 70% of the disparity between the point of gaze and the actual position of the mouse cursor, compared with initial findings; (c) use of both a moving average and a trained neural network in order to minimize the jitter of the mouse cursor, which yield an average jittering reduction of 35%; (d) introduction of a new mathematical methodology to measure the degree of jittering of the mouse trajectory; (e) embedding an onscreen keyboard to facilitate text entry, and a graphical interface that is used to generate user profiles for system adaptability. The adaptability nature of the interface is achieved through the establishment of user profiles, which may contain the jittering and voice characteristics of a particular user as well as a customized list of the most commonly used words ordered according to the user\u27s preferences: in alphabetical or statistical order. This allows the system to successfully provide the capability of interacting with a computer. Every time any of the sub-system is retrained, the accuracy of the interface response improves even more",A multimodal and adaptive real-time human -computer interface for peoples with severe motor disability,,FIU Digital Commons,,,core
380927184,2003-09-01T00:00:00,"The development of the Virtual Reality Modelling Language (VRML) for the Internet has resulted in the emergence of a multiplicity of 3D web sites.  The metaphor used by these sites varies enormously from virtual galleries to virtual cities and style varies from abstract to reality.  Additionally these worlds are populated by virtual objects, some having reactive or interactive properties, including movement, audio, video, databases, artificial intelligence etc.  Perhaps the most stimulating embodiment of these new environments are those that offer the participant the opportunity to meet and communicate with other visitors exploring the same virtual space/world.  The Glasgow Directory is an established 3D web space,

with around 10,000 visitors per year.  The model represents approximately 10,000 properties in the city and is populated by contextual information on its culture and socio-economic topography.  This paper describes the background to this VR space, and suggests a set of design criteria for successfully deploying multi-user software within this and similar

environments. These criteria take into account lessons learned by ‘observing’ and analysing how participants interact with the existing system under different conditions and also what benefits they perceive on entering the environment via the multi-user interface.  These recommendations will hopefully be applicable to a wide spectrum of internet virtual environment builders and users",Visit VR Glasgow : Welcoming Multiple Visitors to the Virtual City,,,,,core
217046012,2009-04-01T07:00:00,"Creosote and pentachlorophenol (PtCP) are two commercial wood preservatives that are regulated by the EPA because of their toxicity to wildlife and humans.   Cresote and PtCP have been suspected of causing cancer in humans, but that claim has not been proven.  To observe changes in gene expression in organisms exposed to these compounds, a model system such as Saccharomyces cerevisiae (baker\u27s yeast) is used.  S. cerevisiae cells were exposed to a creosote concentration of 50ng/ml and to a PtCP concentration of  50uM.  Since creosote and PtCP were suspended in methylene chloride and ethanol, respectively, yeast cells were also exposed separately to the solvents as controls.  cDNA was prepared from a total RNA extraction of exposed and non-exposed S. cerevisiae cells and was hybridized onto microarray chips containing the entire yeast genome using Genisphere Array Kit procedures.  A total of twenty microarray chips were tested (seven creosote chips, seven PtCP chips, three methylene chloride chips, and three ethanol chips) for this study.  Analysis of the microarray data was done using MAGIC Tool software and Microsoft Excel to find statistical significance in gene expression.  Genes showing significant changes in genes expression underwent real-time polymerase chain reaction (RT-PCR) to validate that their change in gene expression was correctly measured in the microarray experiment and that it is due to genetic regulation.  Because creosote and PtCP have been indirectly linked to causing cancer in humans, clustering analysis in MAGIC Tool and BLAST analysis on the National Center for Biotechnology Information (NCBI) website compared genes with significant changes in gene expression to other genes in the S. cerevisiae genome and genes with the human genome.  In both experimental treatments, creosote and PtCP, genes with roles in cell cycle regulation, drug transport, transcription regulation, and response to stress had significant changes in gene expression.  RT-PCR analysis verified that the changes in gene expression could be validated.  Clustering analysis in MAGIC Tool revealed highly correlated gene expression in genes associated with mitotic controls.  BLASTn and BLASTp analysis confirmed that some genes with significant changes in gene expression had homology to human nucleotide and protein sequences.  Overall, the results of this DNA microarray study of S. cerevisiae cells exposed to wood preservatives are a sign of the necessity for more studies to be done by the EPA and workers\u27 health associations in order to establish job/health regulations.  The results of this study could be a starting point for R-1 institutions that concentrate in cancer studies",Gene expression of Saccharomyces cerevisiae exposed to commercial wood preservatives by DNA Microarray Analysis and RT-PCR,https://core.ac.uk/download/217046012.pdf,Digital Commons @ Longwood University,,,core
12953306,01/01/2006,"Software agents are more and more used to perform autonomous tasks in real world environments. This can be a single agent which is performing a task or a group of agents cooperating to perform a task. A system that consists of more than one agent is called a Multi-agent System (MAS). Multi-agent Systems is a research field within the research domain of Artificial Intelligence. One of the processes studied within MAS is cooperative problem solving (CPS). This process describes the problem of one agent that sees a goI for which it needs other agents to cooperate with to achieve this goal. Communication is essential during CPS processes because successful cooperative problem solving requires reliable one-on-group communication to attain an approximation of common belief among the the agents cooperating in a team. This will be shown during a brief introduction of cooperative problem solving. To attain an approximation of common belief it is necessary that the agents gain a certain level of group knowledge about the facts communicated. More specifically, the agents have to know the facts that are communicated, the agents have to know to whom those facts are communicated, and the agents have to know that the agents to whom these facts are communicated know these facts. A simple problem is the sequence transmission problem where one agent communicates a sequence of data to another agent while both agents gain a certain level of knowledge about this data. The sequence transmission problem becomes more complicated when one agent wants to communicate a sequence of data to a group of agents. To attain the desired level of knowledge gaining, somehow the group information has to be involved in the communication. In this thesis a general knowledge-based algorithm is presented that solves the sequence transmission problem for one-on-group communication. This general knowledge-based algorithm is correct for communication media where typical communication errors occur as long as the connection satisfies the fairness condition. It is shown that the agents from a group when using this algorithm for n cycles, gain depth n of general knowledge about the members of the group and about the facts communicated. The communication involved in the CPS process is a bit more complicated than the sequence transmission problem. The sequence transmission problem concerns the transport of data from one agent to one or more other agents while the CPS communication is more a dialogue between two or more agents. The requirements of reliability and gaining of knowledge are the same for both communication processes. A specific knowledge-based algorithm for CPS cornmunication is presented. This CPS algorithm is a modified version of the general algorithm adjusted for the specific demands of CPS communication. It is shown that the agents from a group when using this algorithm for n cycles during a CPS process, gain depth n of general knowledge about the members of the group and about the facts communicated. In general, software agents are connected to each other in a network. The most common network architecture for computers is the internet network architecture. For the CPS algorithm to be of use for software agents in a MAS environment involved in CPS processes, the CPS algorithm should be implemented somewhere in the internet architecture. In this thesis the feasibility of such an implementation is discussed. As a result of this discussion a design specification for an implementation is presented.",Knowledge-based Algorithm for Multi-agent Communication,,,,,core
15358413,2008,"A city modelled according to planning regulations usually presents a correlation between plots and building dimensions. To simulate the overall impact of building regulations over a large number of plots it is advantageous to use a computational tool to perform the task. Existing software can generate proxies of the would be reality, but do not accurately simulate the impact of alternative urban regulations. CityZoom is a Decision Support System for urban planning which not only provides CAD tools, but also allows users to evaluate and modify the city model according to different constraints such as solar radiation, luminance, terrain's pervious conditions, etc. By simulating specific urban regulations and addressing environmental comfort issues, CityZoom helps architects to simultaneously evaluate the different attributes of a particular design. Coupled with GIS tools, CityZoom allows users to perform multiple analyses over the existing database and use the resultant feedback to optimize proposed solutions. Future developments envisage giving CityZoom a more autonomous role, using artificial intelligence to optimize building shapes and dimensions",CityZoom: a Visualization Tool for the Assessment of Planning Regulations,,,10.1260/147807708784640144,,core
19606375,2011-10,"Burkholderia pseudomallei is a saprophytic bacterium which is the causative agent of melioidosis, a common cause of fatal bacterial pneumonia and sepsis in the tropics. The incidence of melioidosis is clustered spatially and temporally and is heavily linked to rainfall and extreme weather events. Clinical case clustering has recently been reported in Townsville, Australia, and has implicated Castle Hill, a granite monolith in the city center, as a potential reservoir of infection. Topsoil and water from seasonal groundwater seeps were collected around the base of Castle Hill and analyzed by quantitative real-time PCR targeting the type III secretion system genes for the presence of B. pseudomallei. The organism was identified in 65% (95% confidence interval [CI], 49.5 to 80.4) of soil samples (n = 40) and 92.5% (95% CI, 83.9 to 100) of seasonal groundwater samples (n = 40). Further sampling of water collected from roads and gutters in nearby residential areas after an intense rainfall event found that 88.2% (95% CI, 72.9 to 100) of samples (n = 16) contained viable B. pseudomallei at concentrations up to 113 CFU/ml. Comparison of isolates using multilocus sequence typing demonstrated clinical matches and close associations between environmental isolates and isolates derived from clinical samples from patients in Townsville. This study demonstrated that waterborne B. pseudomallei from groundwater seeps around Castle Hill may facilitate exposure to B. pseudomallei and contribute to the clinical clustering at this site. Access to this type of information will advise the development and implementation of public health measures to reduce the incidence of melioidosis",Groundwater seeps facilitate exposure to Burkholderia pseudomallei,,American Society for Microbiology,10.1128/AEM.05048-11,,core
211460615,2009-01-01T00:00:00,"Renewable energy sources (RES) are forms of exploitable energy which are derived from various natural processes, continuously regenerated, not exhausted or replaced [1, 2, 3]. According to the Hellenic Association of RES Producers the total installed RES power in Greece is estimated to 1000 MW, whereas there is potential for high increase, since in certain areas of the country the solar and/or wind potential exhibit very high values [4], The main task of the accomplished investigation is the mandatory increase of RES applications (with the well-known economic, social and environmental consequences) through the study of operation of the hybrid power systems which include PV generator and wind turbine (WT) with Permanent Magnet Synchronous Generator (PMSG). The above two energy subsystems are likely the most mature technologies of RES exploitation worldwide, which either when used separately (forming stand-alone systems) or when combined (forming hybrid systems) are compatible with the characteristics of Greek climate. According to the Hellenic Association of Photovoltaic Companies the installed PV systems in Greece in 2007 were 2,475 MW from which 68% were connected to the electrical grid while the rest 32% were autonomous. On the other hand, according to the Hellenic Wind Energy Association, in the last five years an average of 115 MW wind parks were installed in Greece, while a minimum value of 10000 MW should be installed until the end of 2020 in order to satisfy the European commitment Greece has undertaken [5,6]. In the beginning of this Ph.D. Thesis certain general information concerning RES is presented (i.e. categories, pros and cons, etc.), whereas their technological progress and applications in Greece and worldwide are described. The investigation is focused on electric power systems which include PV generator or/and WT with PMSG, and by obtaining relevant information from the existing experience and references of the international bibliography the best known types/forms of such systems, their particular characteristics, their most important applications, the points where further investigation and improvement is required, and the principal functions of their subsystems were described. In addition, the most important quantities which describe the solar and wind potential with respect to a site for installation were mentioned. Based on the above quantities, a systematic procedure of statistical analysis of the solar and wind potential of a possible installation site is proposed. This procedure was applied to a particular site, i.e. at the top of a building in the School of Engineering of Democritos University of Thrace in the city of Xanthi, proving/exhibiting its great capabilities, reliability and flexibility. Furthermore, the most accurate and acceptable mathematical models of various subsystems of electrical power systems which include PV generator or/and WT with PMSG are presented. These models are general and were picked from relevant applications of international bibliography. This is the reason why they were properly adapted for such use. At the same time, designs of such electrical power systems in stand-alone, hybrid, autonomous and connected with electric grid configurations are presented (via simulations). In order this investigation to be of more practical interest, special care was given to analyse and study designs of common RES applications (e.g. residential electrical systems, water pumping systems, small and medium scale grid connected systems), with the ultimate goal to achieve proper design and efficient/economical operation. In all proposed/studied power systems in this Ph.D. Thesis proper care was taken in order to simultaneously achieve, by using modem intelligent dynamic control methods (combination of fuzzy logic and neural networks), maximum power point tracking (with high speed, accuracy and stability) from the used electric power sources, and fulfillment of the particular requirements and demands of the autonomous electrical loads and the electric grid. Finally, the proper design of these power systems was confirmed via their techno/economic/socio-environmental assessment by applying a systematic procedure (using a properly developed software) to a practical system being installed in a building at the campus of the Polytechnic School of Democritos University of Thrace (DUTH) in the city of Xanthi. In this Ph.D. Thesis the following are conducted: at the beginning a review of the RES technologies is given; the technology of PV generators and WT with PMSG as well as their systems are fully described; a complete systematic procedure for solar and wind potential statistical analysis of a selected site with respect to a power system installation which includes PV generator or/and WT with PMSG is proposed; the modeling of the various subsystems of power systems which includes PV generator or/and WT with PMSG are presented; a method of modeling, control and simulation of designs of stand-alone and hybrid power systems which includes PV generator or/and WT with PMSG in autonomous and grid connected operation is given; and their proper design and operation through a proposed general methodology for assessing their techno/economic/socio-environmental features are presented. In any case a systematic effort has been made to farther develop and improve the technology of such RES systems with real contribution: to the desired deceleration of the rapid depletion rate of conventional fuels, to the associated avoidance of environmental pollution, to the economic utilization of the solar and wind potential of the planet, and to the improvement of the electric grid load power factor.Οι ανανεώσιμες πηγές ενέργειας (ΑΠΕ) είναι μορφές εκμεταλλεύσιμης ενέργειας που παρέχονται δωρεάν από τη φύση, ανανεώνονται συνεχώς, δεν εξαντλούνται και δεν αντικαθίστανται [1, 2, 3]. Σύμφωνα με τον Ελληνικό Σύνδεσμο Ηλεκτροπαραγωγών από ΑΠΕ η εγκατεστημένη ισχύς ΑΠΕ σήμερα στην Ελλάδα υπολογίζεται στα 1000 MW περίπου, ενώ υπάρχει δυνατότητα μεγάλης αύξησής της, αφού σε ορισμένες θέσεις της χώρας το ηλιακό ή/και το αιολικό δυναμικό είναι ιδιαίτερα πλούσια [4]. Βασικό κίνητρο της διερεύνησης που πραγματοποιήθηκε είναι η επιβεβλημένη πλέον αύξηση των εφαρμογών ΑΠΕ (με τις γνωστές οικονομικές, κοινωνικές και περιβαλλοντικές συνέπειες) μέσω της μελέτης της λειτουργίας υβριδικών ηλεκτρικών συστημάτων ισχύος που περιλαμβάνουν φωτοβολταϊκή (Φ/Β) γεννήτρια και ανεμογεννήτρια (Α/Γ) με Σύγχρονη Γεννήτρια Μόνιμου Μαγνήτη (ΣΓΜΜ). Οι δύο αυτές ενεργειακές πηγές αποτελούν ίσως τις πιο ώριμες τεχνολογίες εκμετάλλευσης των ΑΠΕ διεθνώς, οι οποίες τόσο όταν χρησιμοποιούνται χωριστά (σχηματίζοντας μεμονωμένα συστήματα) όσο και όταν συνδυάζονται (σχηματίζοντας υβριδικά συστήματα) ταιριάζουν ικανοποιητικά με τα ελληνικά κλιματολογικά χαρακτηριστικά. Σύμφωνα με τον Ελληνικό Σύνδεσμο Εταιριών Φωτοβολταϊκών οι εγκαταστάσεις Φ/Β συστημάτων στην Ελλάδα το 2007 ήταν 2,475 MW περίπου εκ των οποίων το 68% ήταν διασυνδεδεμένα με το ηλεκτρικό δίκτυο και το 32% ήταν αυτόνομα, ενώ σύμφωνα με την Ελληνική Επιστημονική Ένωση Αιολικής Ενέργειας τα τελευταία 5 χρόνια εγκαθίστανται στην Ελλάδα ετησίως κατά μέσω όρο 115 MW αιολικών πάρκων (Α/Π) περίπου ενώ θα πρέπει να εγκατασταθούν κατά ελάχιστο 10000 MW μέχρι το 2020 για να εκπληρωθούν οι ευρωπαϊκές δεσμεύσεις που έχει αναλάβει η χώρα [5, 6]. Έτσι στη συγκεκριμένη διδακτορική διατριβή (ΔΔ) αρχικά δόθηκαν κάποια γενικά στοιχεία που αφορούν τις ΑΠΕ (κατηγορίες, μειονεκτήματα-πλεονεκτήματα, κλπ.), και περιγράφηκε ποσοτικά η κατάσταση στην οποία βρίσκεται η τεχνολογική πρόοδος και εφαρμογές τους τόσο στην Ελλάδα όσο και διεθνώς. Η μελέτη επικεντρώθηκε στα συστήματα ισχύος που περιλαμβάνουν Φ/Β γεννήτρια ή/και Α/Γ με ΣΓΜΜ, όπου αντλώντας στοιχεία από την μέχρι τώρα εμπειρία και τις αναφορές της παγκόσμιας βιβλιογραφίας, περιγράφηκαν: οι πιο δημοφιλείς μορφές/τύποι τους, τα ιδιαίτερα χαρακτηριστικά τους, οι πιο σημαντικές εφαρμογές τους, τα σημεία όπου χρήζουν περαιτέρω έρευνα και βελτίωση, η αρχή λειτουργίας των υποσυστημάτων τους, καθώς και αναφέρθηκαν τα πιο σημαντικά μεγέθη περιγραφής του ηλιακού και αιολικού δυναμικού πιθανής θέσης εγκατάστασής τους. Με βάση αυτά τα μεγέθη προτάθηκε μια συστηματική διαδικασία στατιστικής ανάλυσης του ηλιακού και αιολικού δυναμικού η οποία υλοποιήθηκε μέσω κατάλληλου λογισμικού (υπάρχοντος και αναπτυχθέντος), που εφαρμόστηκε σε συγκεκριμένη θέση σε κτίριο της Πολυτεχνικής Σχολής (ΠΣ) του Δημοκρίτειου Πανεπιστημίου Θράκης (ΔΠΘ) στην πόλη της Ξάνθης, επιδεικνύοντας/αποδεικνύοντας έτσι την αξιοπιστία, ευελιξία και τις μεγάλες δυνατότητές τους. Στη συνέχεια, δόθηκαν τα πιο ακριβή και αποδεκτά μαθηματικά μοντέλα των διαφόρων υποσυστημάτων των συστημάτων ισχύος που περιλαμβάνουν Φ/Β γεννήτρια ή/και Α/Γ με ΣΓΜΜ. Τα μοντέλα αυτά επειδή είναι γενικά και προήρθαν από συναφείς εφαρμογές της διεθνούς βιβλιογραφίας προσαρμόστηκαν κατάλληλα για τέτοια χρήση. Παράλληλα, παρουσιάστηκαν μέσω προσομοιώσεων τέτοιας  μορφής ηλεκτρικά συστήματα ισχύος τόσο μεμονωμένα όσο και υβριδικά σε αυτόνομη ή παράλληλη λειτουργία με ηλεκτρικό δίκτυο διανομής. Μάλιστα για να έχει η μελέτη μεγαλύτερο πρακτικό ενδιαφέρον, ιδιαίτερη βαρύτητα δόθηκε στην ανάλυση και μελέτη συστημάτων που εξυπηρετούν συνήθεις εφαρμογές συστημάτων ΑΠΕ (π.χ. οικιστικά ηλεκτρικά συστήματα, συστήματα άντλησης νερού, διασυνδεδεμένα με το ηλεκτρικό δίκτυο διανομής συστήματα μικρής και μεσαίας κλίμακας, κλπ.), επιτυγχάνοντας σωστό σχεδίασμά και αποδοτική/οικονομική λειτουργία. Σε όλα τα μελετώμενα συστήματα της παρούσας ΔΔ ελήφθη κατάλληλη μέριμνα ώστε να επιτυγχάνεται, εφαρμόζοντας σύγχρονες τεχνικές ευφυούς δυναμικού ελέγχου (συνδυασμός ασαφούς λογικής και τεχνιτών νευρωνικών δικτύων), μεταφορά μέγιστης ισχύος από τις χρησιμοποιούμενες πηγές ηλεκτρικής ενέργειας (ΜΜΙ με μεγάλη ταχύτητα, σημαντική ακρίβεια και εξαιρετική σταθερότητα), καθώς και ικανοποίηση των ιδιαίτερων αναγκών και απαιτήσεων τόσο των αυτόνομων ηλεκτρικών φορτίων όσο και του ηλεκτρικού δικτύου διανομής. Τέλος, ο σωστός σχεδιασμός αυτών των συστημάτων επιβεβαιώθηκε με την αποτίμησή τους, από τεχνική/οικονομική/κοινωνικό-περιβαλλοντική πλευρά, με χρήση μιας ολοκληρωμένης/συστηματικής διαδικασίας η οποία εφαρμόστηκε (μέσω κατάλληλα διαμορφωμένου λογισμικού σε περιβάλλον Matlab) σε τέτοιου είδους πρακτικά συστήματα που βρίσκονται εγκατεστημένα σε κτίριο της ΠΣ του ΔΠΘ στην πόλη της Ξάνθης. Επιγραμματικά, στην παρούσα ΔΔ: γίνεται καταρχήν επιστημονική ανασκόπηση των τεχνολογιών ΑΠΕ, αναλύεται λεπτομερώς η τεχνολογία των Φ/Β γεννητριών και Α/Γ με ΣΓΜΜ και τα συστήματα που σχηματίζουν, προτείνεται μια συστηματική και ολοκληρωμένη διαδικασία στατιστικής ανάλυσης του ηλιακού και αιολικού δυναμικού επιλεγμένης θέσης για εγκατάσταση ενεργειακού συστήματος που περιλαμβάνει Φ/Β γεννήτρια ή/και Α/Γ με ΣΓΜΜ, περιγράφεται ο τρόπος μοντελοποίησης των διαφόρων υποσυστημάτων των συστημάτων ισχύος που περιλαμβάνουν Φ/Β γεννήτρια ή/και Α/Γ με ΣΓΜΜ, περιγράφεται μια εφαρμοζόμενη μέθοδος μοντελοποίησης, ελέγχου και προσομοίωσης μεμονωμένων και υβριδικών συστημάτων ισχύος που περιλαμβάνουν Φ/Β γεννήτρια ή/και Α/Γ με ΣΓΜΜ σε αυτόνομη και παράλληλη λειτουργία με ηλεκτρικό δίκτυο διανομής, και αποδεικνύεται ο σωστός σχεδιασμός και η λειτουργία τους μέσω μιας προτεινόμενης γενικής διαδικασίας για την τεχνική/οικονομική/κοινωνικό-περιβαλλοντική αποτίμησή τους. Σε κάθε περίπτωση έχει γίνει συστηματική προσπάθεια για την παραπέρα ανάπτυξη και βελτίωση της τεχνολογίας αυτών των μορφών ΑΠΕ με συμβολή: στην επιβράδυνση του υψηλού ρυθμού εξάντλησης των υφιστάμενων συμβατικών/ορυκτών καυσίμων, στον περιορισμό της περιβαλλοντικής μόλυνσης, στην οικονομική εκμετάλλευση του ηλιακού και αιολικού δυναμικού του πλανήτη, και στη βελτίωση του συντελεστή ισχύος του φορτίου του ηλεκτρικού δικτύου διανομής",Μοντελοποίηση και έλεγχος υβριδικού συστήματος (Φ/Β γεννήτριας και ανεμογεννήτριας μόνιμου μαγνήτη) σε αυτόνομη και παράλληλη λειτουργία με ηλεκτρικό δίκτυο,,'National Documentation Centre (EKT)',10.12681/eadd/20964,,core
,,,,,,,,
,,,,,,,,
,,,,,,,,
,,,,,,,,
,,,,,,,,
,,,,,,,,
,,,,,,,,
,,,,,,,,
,,,,,,,,
,,,,,,,,
,,,,,,,,
,,,,,,,,
,,,,,,,,
,,,,,,,,
,,,,,,,,
,,,,,,,,
,,,,,,,,
,,,,,,,,
,,,,,,,,
,,,,,,,,
,,,,,,,,
,,,,,,,,
,,,,,,,,
,,,,,,,,
,,,,,,,,
,,,,,,,,
,,,,,,,,
,,,,,,,,
,,,,,,,,
,,,,,,,,
,,,,,,,,
,,,,,,,,
,,,,,,,,
,,,,,,,,
,,,,,,,,
